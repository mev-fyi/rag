00:00:00.280 - 00:00:23.794, Speaker A: We're focused on having the pith product continuously expand. More publishers cover all financial market data. And so, like, if done right, then every month the number of symbols will go up, the number of users will go up, the number of publishers will go up, and the rewards is kind of one of the incentives and levers that, you know, that we have to make sure that happens.
00:00:24.134 - 00:00:45.590, Speaker B: All right, everyone. So on Empire, you obviously know that we talk a lot about the institutions coming into crypto, and that is why we are super excited to share that. We are hosting the digital asset summit. We've hosted this since 2019. It's coming up in London, March 18 to 20th. Don't miss your chance to get ahead of the curve. You can get 20% off with code Empire 20.
00:00:45.590 - 00:01:19.566, Speaker B: We'll see you in London. This episode is brought to you by Toku, the first comprehensive global solution for both token compensation and tax compliance. Toku makes implementing global token compensation and incentive awards simple. With Toku, you get unmatched legal and tax support to both grant and administer your global team's tokens across the entire token lifecycle. Make your token grants easy today with token. All right, everyone, welcome back to Empire. Really excited.
00:01:19.566 - 00:01:32.876, Speaker B: We have Mike Cahill. Spent time at Morgan Stanley, Nomura. Sibo jump was running, I think, special projects at Jump. Then today is CEO of Duro Labs and also contributor at Pith. So, Mike, welcome to the show, man.
00:01:33.060 - 00:01:34.740, Speaker A: Thanks so much. Excited to be here.
00:01:34.852 - 00:01:59.812, Speaker B: Yeah, so you guys obviously were going to talk. You guys just had this big token generation event. We're going to talk about pith and pith token. But I think first, there's like two setups to this conversation that will make the pith conversation more fruitful. One is the oracle problem in crypto. I would say the V one of Oracles has been dominated by chainlink the last couple of years. You guys are coming into that same space and maybe doing what I might call the Oracle V two approach.
00:01:59.812 - 00:02:16.104, Speaker B: Then there's this problem that exists in traditional capital markets where all of the data is monopolized with decentralized exchanges like the Nasdaqs of the world. So let's start there. Can you just give us the setup for what actually data looks like in traditional capital market structure?
00:02:16.884 - 00:02:48.594, Speaker A: Yeah, absolutely. So 20 years ago, financial market data was never really monetized. Basically, the exchanges would monetize their executions, and then it became more and more of a component of their revenue mix. And today it accounts for 20% of the revenue across the top six exchanges at $6.5 billion. That's for real time streaming financial market data that doesn't cover things like benchmarks. That's another multibillion dollar business.
00:02:48.594 - 00:03:46.164, Speaker A: So what does that model look like? So basically, when you're buying access to trade on, say, Nasdaq or NYSE, you will basically ask for a market data feed and an order entry feed. And so one will be over, like WebSocket or an itch or rest API connection. And depending upon what type of trading you expect to do, you're going to pay different amounts and you're going to have different types of access. So if you're like a, let's say a retail brokerage, you don't really need the most sophisticated market data. So, like Robinhood, if they're going to route orders to an exchange, they would say, all right, we're just going to get your most basic standard package, and the exchange is going to go, okay, we're going to give you a wholesale price for that. That's based on how many users Robinhood has. And if it goes up, you're going to pay a little bit more.
00:03:46.164 - 00:04:27.548, Speaker A: Then if you go up one rung and you have like a tiger or Appaloosa or Moore, someone like that, it's like a hedge fund. There's probably some sort of a human trader there, and he runs their smart order router. And from time to time, you'll have to intervene. He's going to want to have access to the full order book, look at it, place orders, watch them, cancel them, amend them, that sort of thing. And also from a historical perspective, they probably run some regression testing around performance of, say, like, twaps. And so they'll want to have all that data recorded and stored, and so that'll be a little bit more expensive. And then the third package is what high frequency trading firms have.
00:04:27.548 - 00:05:04.112, Speaker A: And so this is the most robust data. It's down to the nanosecond. And also the access to it is typically gated such that you pay for colocation. So you're really close to their server box in Secaucus or in Aurora, depending upon which exchange that we're talking about. And that's going to be the highest and sort of most optimized. And so over time, this product set has matured and it has become now this top level big contributor to the revenues. Now, the thing is, the trading firms are only ever buying this.
00:05:04.112 - 00:05:44.478, Speaker A: And so there has been some chagrin within those entities for a really long time because it's almost like they're giving something. The exchanges are monetizing the trades that just occurred. Traders are the only way that trades occur, or the bids and offers where traders want to trade. Traders are the only ones that can send in bids and offers. And so they're basically giving these exchanges all of this data, and then they have to buy it back. And for a long time, they were really upset with this, and many of them still are today. Some are more vocal than others, and that's just been the state of the world.
00:05:44.478 - 00:05:58.506, Speaker A: So all of the world's data has been kind of financial market data has been gated, monetized by big exchanges, and then traders are just sort of stuck on the bid side and can't really participate in this market and pretty unhappy.
00:05:58.610 - 00:06:01.794, Speaker B: What happened 20 years ago that let them do this.
00:06:01.954 - 00:06:52.194, Speaker A: It was the evolution of financial markets to become more electronic. Previously, it wasn't as necessary to have this segmentation of market data, and you basically used to have more human traders. So you'd be able to have, at a bank desk, for instance, you'd have like 50 human traders. And so you could say, all right, each one of those, they're going to have access to this particular order book or this software that allows them to trade. You'll pay for per se, just like the way that Bloomberg monetizes, and they're super expensive. And so because there were less and less traders, and most of the trade was coming through a single API, they really needed to change their business model. And there was also consolidation amongst the different exchanges.
00:06:52.194 - 00:07:37.166, Speaker A: For instance, CME has bought several other exchanges, including EBS, which was the large FX order book for a long time. And so this kind of lack of fragmentation and these large conglomerates taking up such a big percentage of the market share allowed them to really increase their exposure. The same thing happened for FX, it just happened later. So FX was like ten years lag to the equity markets, both on execution as well as financial market data. And so now it's become a big component of the FX markets. Basically for futures, it was pretty similar to equities. And that's why I think all markets sort of go through this evolution.
00:07:37.166 - 00:08:22.664, Speaker A: And like crypto today is in those, like, 20 years ago phase, where most of the exchanges within crypto don't charge anything yet because they're thinking about it as well. We actually probably need to grow the pie a bit first. And it's a good marketing tool, like having the binance price feed on trading view is a useful way for binance to get its name out. And the more people that are trading, you can extract the costs later. But for now, let's just think about this from a marketing perspective. The one outlier is probably Coinbase because they have such an institutional coverage model within the US and a lot of DNA from banks. They already have a pretty robust business model around financial market data, although I don't know that their enforcement is anywhere near the enforcement that Nasdaq or NYSE has.
00:08:22.664 - 00:08:32.084, Speaker A: So still people use it and there's some level of quasi commercialization around it.
00:08:33.064 - 00:08:50.443, Speaker B: Do you think they'll also, is that the natural evolution of crypto exchanges is right now they treat their data as like this top of funnel, pull in more users, get the binance brand awareness out there, but maybe fast forward three to five years they'll start monetizing their data. This becomes a big business line for them, like it does for NYSE or Nasdaq.
00:08:50.783 - 00:08:53.375, Speaker A: Yes, I would heavily predict that to be the case.
00:08:53.519 - 00:09:14.524, Speaker B: Yeah. Okay, so that's the. Okay, so one bucket here with pith. We have this problem in traditional capital markets where all the data is monopolized. Talk to me about the crypto Oracle problem and how some of that financial information streams into the apps like an aave or compound today. Let's go prepeth.
00:09:14.564 - 00:09:38.916, Speaker A: Yeah. Oracle 1.0 model was. The model was take the data from the Internet and put it onto the blockchain and do it in a way where you preserve trust. The Oracle 1.0 model looks a little bit like a bridge. It doesn't have any particularly proprietary or in network access to data.
00:09:38.916 - 00:10:32.796, Speaker A: It's just basically saying, look, there will be some endpoints on the Internet and we can use those endpoints if we have enough nodes. And those nodes will run a very simple aggregation where you get ten nodes who are replicating the price of coingecko on chain, and they'll do it every so often and it'll be good enough every so often, meaning they'll push an update every hour or 50 basis points. Now that works for crypto data because of the phenomenon I mentioned, it's immature in its life cycle. If crypto data was already protected, then that business model wouldn't work. You would have to go buy that data. And then buying data means that you're subject to the terms and conditions of it so you don't get uncontrolled distribution rights to it. There's this temporary time where you get this bootstrappy way to create a solution to the oracle problem.
00:10:32.796 - 00:11:10.424, Speaker A: The oracle problem is how do you bring exogenous data on chain in a trustworthy way? And it involves nodes that are going grabbing it from the Internet, replicating it, publishing it, but they add all this latency. It takes a long time for them to do this. Then because it's being published as a push on chain, you need to be gas conservative. And so you basically don't want to, you want to publish it as infrequently as possible. And so you basically end up with this very low resolution feed that doesn't track super well to the updated or high speed markets.
00:11:10.884 - 00:11:13.224, Speaker B: What does low resolution mean in this case?
00:11:14.284 - 00:12:11.904, Speaker A: So if you've predefined the epochs with which you're going to publish data on chain to 1 hour of 50 basis points. 50 basis points is basically the resolution. You're basically saying we're going to have a heartbeat that comes every 1 hour irrespective of the delta in that time period or it has to move 50 basis points. So if you think about just pixel resolution as being this 50 basis points requiring a print, anything below that is going to not be printed. That's where I think about low resolution. Financial markets go out to several decimal points and with things like leverage, those can be incredibly meaningful. And so there's been like the general evolution of markets as they become electronic is that they update more frequently and that there's generally more specificity or granularity with the prices.
00:12:12.724 - 00:12:31.064, Speaker B: So Oracle model 1.0 chainlink, all this info is available on the Internet, right? So then you have nodes scraping the models online, then they publish it to the blockchain. Why do we need to necessarily like improve the latency of oracle data today? Like compound doesn't need low latency, right?
00:12:31.724 - 00:13:07.488, Speaker A: They don't as is. So for the very first kind of proof of concept lending protocols, you don't really need to have very fast markets because you're not going to have really high loan to value levels anyway. So what you really need for, what you really need high speed oracles for is applications that are going to take advantage of a high speed oracle. So some of those would be like on chain perpetual markets. Now if you're running an on chain perpetuals market using a slow oracle, then it can be arbitraged very easily.
00:13:07.616 - 00:13:09.178, Speaker B: Like a synthetix for example.
00:13:09.226 - 00:13:50.504, Speaker A: Yeah, so synthetix is a great example. So synthetix was on v one using chainlink. They created a v two. They launched the v two at the beginning of this year and evaluated at that time the best oracles out there that would suit their needs. And the governance chose pith as the oracle to use for as their primary oracle for execution because it's the fastest and has the lowest latency. And that enabled them to really sort of leapfrog some of the competitors that had caught up during the time where they hadn't had that and became a very big force with regards to perpetuals on chain.
00:13:52.484 - 00:14:14.482, Speaker B: It's funny, I feel like this low latency of data in OG DeFi has actually been a, it sounds like a negative, but sometimes it kind of saves defi. Oracle is being slow to report. Volatility ends up sometimes on days like March of 2020. I'm sure Santi knows ten times more about this than I do because he's actually experienced it in depth, but sometimes it can actually help Defi in a way.
00:14:14.538 - 00:15:03.034, Speaker C: Well, I would probably make it a more precise statement, which is it's more. So the way you calculate the price feed, like, it's some of the, where mistakes have been made in DeFi is where you always, I think, want to have a constant and as like constant real time data feed. But the way you calculate the price is sort of like in a smoothed out manner, which is not taking a specific data point, but taking the average of the last five minutes, ten minutes, 30 minutes, an hour, and every, a lot of the mistakes that have been done and exploits have been because a lot of DeFi protocols, the price feeds that they're using, are just relying on a single data point and not a combination to triangulate to the best price. And that's where like compound was affected by this. Like even the, what I'm trying to say is some of the best DeFi protocols have made this mistake and have learned from it.
00:15:04.574 - 00:15:20.190, Speaker A: Yeah. And I think that the fact that there are way better ways to architect a lending protocol so that it's not subject to exploits, rather than just using a slow oracle. I know what you're talking about.
00:15:20.302 - 00:15:25.190, Speaker B: No need to evolve then. We need better and faster data. Yeah, of course.
00:15:25.262 - 00:16:15.204, Speaker A: Yeah, exactly. It's like, the point is, if the best thing we can come up with for our lending protocol in two years is compounds current instance, that'd be pretty depressing. So I'm pretty optimistic that there's going to be a use case for higher fidelity market data within a lending protocol. It's just they'll probably have different mechanisms that all defend against flash attacks or volatility spikes. Like you should be conservative with your ltvs or you should be conservative during periods of high volatility. So one of the things that pith also innovated was the aggregation is a tricky part. So, as Santi mentioned, right, if you're dealing with a single point, especially during high volatility, that's going to be a problem.
00:16:15.204 - 00:17:04.844, Speaker A: And when we're building the PIF network, we kind of had this first principles discussion, like, what is the price of bitcoin right now, if we have ten data sources, is it the average? It's probably not the best representation. The more data sources you get. Sure, it's a better representation, but still imperfect. For instance, how do you represent the fact that there is going to be anomalies based on the idiosyncrasies of funding rates or the access to a restricted currency market like kimchi premiums? So you actually do want to represent that in real time. So what we came up with was called confidence intervals. And pith is the only oracle that has these. So in real time, every price from PIF is aggregated together.
00:17:04.844 - 00:17:45.460, Speaker A: But along with the aggregate, each price has a confidence band. It says plus or minus price. So if the price of bitcoin is 37,000, it could be plus or minus $200. And this fluctuates at every single update and in periods of high volatility, what you could do, if you're building a smart kind of collateral evaluation, you could. Yeah, there you go. You could say, all right, if the confidence interval is quite wide right now, we're going to limit the amount of collateral you can contribute for a period of time. And those are smart ways to address this.
00:17:45.460 - 00:18:23.784, Speaker A: And then it becomes more dynamic, because with a lending protocol, you don't necessarily want to be the most conservative to all, like the 99th percentile. Incidents like that would just give you a pretty low leverage product. You want to assume that during 99% of time you can give high leverage because that's what would make your protocol more attractive. But you have defense mechanisms in place such that you don't get blown up by black swan events. And, you know, this is one way to do it. And another way to do it is using things like twaps. And, you know, all those stuff are available on pith as well.
00:18:25.884 - 00:18:28.264, Speaker C: What's the confidence on your confidence intervals?
00:18:29.604 - 00:18:32.892, Speaker A: That is a good idea. Very high confidence.
00:18:33.068 - 00:18:42.654, Speaker C: I'm sure you've had. I mean, I'm speculating here because, I don't know, but have you actually changed the way you think about and calculate confidence intervals?
00:18:45.034 - 00:20:05.228, Speaker A: We've changed the aggregation technique since the very beginning. So now the way that aggregation is constructed, it filters out outliers pretty well. So each one of the publishers basically gets three votes on what the price is. And so they get their, their price and then their price plus the confidence, plus, and then minus the confidence. So every publisher within the Pith network has to publish a price and a confidence, and then all of those votes are plotted, and you end up with something called a weighted median. And then the algorithm creates a Laplace distribution based on all of the inputs to come up with the aggregate confidence interval, and it tracks fairly well. It also has characteristics where if you have these outliers and you have a cluster of outliers, then the confidence will represent that cluster, but it won't necessarily skew the average price or the aggregate price to the cluster of bad prices, because you basically want the output to, in plain English, say, look, the real price seems to be, or where all the trading happened seems to be 36,000.
00:20:05.228 - 00:20:24.904, Speaker A: But there's some stuff that's trading plus or minus $1,000, which indicates high volatility. And that's really good information. That's better information than saying it's skewed up 1000 and then having some average. That shows you something that's kind of a bad price in both instances.
00:20:25.864 - 00:20:29.684, Speaker C: Yeah. So you, you're normalizing, but you're also paying attention to these anomalies.
00:20:30.824 - 00:20:32.128, Speaker A: That's right. Yeah.
00:20:32.256 - 00:20:40.724, Speaker C: Because that could indicate there's an exploit. There's a flashlight in particular, defi protocol. There's some sort of funky thing going on with your data providers that needs to be addressed.
00:20:41.024 - 00:20:41.784, Speaker A: Exactly.
00:20:41.904 - 00:20:48.912, Speaker B: Mike, can you actually, can we just zoom back out for a second here? Can you go back, like, take us back to jump? My understanding is pith came out of jump.
00:20:49.008 - 00:20:52.104, Speaker C: You were like, we were so happy going really deep, Jason.
00:20:52.144 - 00:20:56.728, Speaker B: I mean, as much as I like confidence intervals, I'm like, I gotta understand Pif before we go into these.
00:20:56.816 - 00:20:59.804, Speaker C: I wonder how many people dropped off over this, like, last few minutes.
00:21:01.144 - 00:21:01.632, Speaker A: It's funny.
00:21:01.648 - 00:21:08.888, Speaker C: If you're still here and listening, kudos. You get some pat in the back. But anyway, let's go back to the origin story, I guess.
00:21:08.936 - 00:21:19.404, Speaker B: Yeah, you guys flipped the model right now, instead of the exchanges owning the data, the funds are actually saying, hey, we've got this data, let's band together. But take us back to the days of JMP and how this kind of project came to be.
00:21:19.984 - 00:21:56.494, Speaker A: Yeah. So in 2020, I was working at jump, and we were involved in a lot of basically trading and partnering with a lot of projects. There was a lot of vesting as well. And there was a big thesis conversation. Looking at some of the bottlenecks around the markets. And when we did deep dives, we basically realized that there was two areas that were going to really constrain the short term growth of DeFi. And the first one was the solution to the oracle problem, which is Oracles 1.0.
00:21:56.494 - 00:22:38.706, Speaker A: We thought that with this low resolution data, it was never going to be able to do anything interesting from an application perspective. You could basically do lending protocols and that's about it. And that's really all it did. Like with DeFi 1.0. The other one that we thought was a big limitation was around bridging. But at the time we had basically, we talked to other trading firms and we realized that the solution for the Oracle problem was to get data directly from the source. And we combined these two problems together that we mentioned in the beginning.
00:22:38.706 - 00:23:20.474, Speaker A: The Oracle 1.0 wasn't that great. And number two, most of the data is owned by the exchanges. And we started talking exchanges and said if you guys were to publish your data on chain, that would create a potentially very interesting product for solving this other Oracle problem and it would potentially give you a new revenue source and combat that monopoly in the web. Three, space. And very quickly the idea resonated. So the first companies that joined into the network that were kind of founding members were Virtu, GTS, Myx, Elmax and a few others.
00:23:20.474 - 00:23:42.154, Speaker A: And that was really where the idea was born. So jump was a contributor, started doing a lot of the development, but really it was a collective. And then the collective grew quickly. So today there's over 100 publishers, all institutions, and the development is mostly done from dural labs, the company that I'm the CEO of now.
00:23:44.294 - 00:24:07.832, Speaker B: I remember talking to Colleen Sullivan from Brevin about this project early on when I think she was still at CMT and she said, look, the craziest thing about this is you have a bunch of firms who are hyper confident, sensitive with their own data, and they're hyper competitive as well. And now they're all collaborating and working together. How did you get jump and virtu, for example, to work together here?
00:24:07.998 - 00:24:40.710, Speaker A: Yeah, you would never expect. You think it's like cats and dogs. And we actually thought this when we'd gone out with the idea. It just so happens that there's two things. Number one, there's like a bigger cat, which is the financial market. Data is something that everyone deals with and is incredibly frustrated by. And number two, the innovations of blockchains allow distrusting people to have incentive mechanisms to collaborate on something that gets shared as a common good for other people.
00:24:40.710 - 00:25:13.274, Speaker A: That wouldn't be possible. There is no way that virtue jump Jane Street DRW would enter into any sort of an LLC for any kind of profit sharing or information sharing in a traditional sense. If there was a single shared, it was a single database owned by just one of those companies, it would never happen. It's all the innovations around blockchains that allowed this. It's not owned by anybody. No one has preferential access to it. The aggregation contract lives on the blockchain and everyone publishes to it.
00:25:13.274 - 00:25:32.226, Speaker A: So it's equitable access for all of the contributors as well as all the users. That was an important innovation. And then there was a model that was described at the time on how the publishers were going to be incentivized for providing their data, and that was what really resonated with people.
00:25:32.410 - 00:26:00.418, Speaker C: But like, okay, let's unpack that for a bit. Because of your jump, you're at the peak of the bull market. You think you have an edge. I don't know, but I'm speculating. You think you have an edge, you think you have better price feed, and that gives you a huge advantage over all the other cats and dogs out there. And so no matter how, like, I got to think, like, the economic incentive is kind of like, not there there to provide the data and get paid to contribute to this, like, Switzerland model of like a data feed. Because if you fundamentally believe that you have an edge, then it's sort of like an adverse selection problem.
00:26:00.418 - 00:26:26.364, Speaker C: All the underdogs do have an incentive to provide the data. And is then what you're saying that aggregation of that data was enough to convince the guy that thinks it has the most amount of edge on the table to finally come forth and provide the data, but it takes time. Or does that make sense? Because if I feel that I have the edge, no fucking way I'm going to contribute. I'm like, no, this is my data. Everyone else can not have an edge.
00:26:26.864 - 00:27:27.362, Speaker A: Yeah, I think there's a small nuance here as well, which is the time scale resolution that we need to focus on. The fastest time scale possible within blockchains today is quite slow for the traditional finance markets. The traditional finance markets, you're dealing with sub milliseconds, like nanoseconds in some of the markets in terms of the alpha and the resolution that you need to be competitive. The fastest times within blockchains right now, pith updates every 400 milliseconds. So the scale of magnitude there is in the thousands times category. So from their perspective, some of the data has already decayed or some of the alpha has already decayed for traditional markets. But now we're still at the fastest update times possible on blockchains because of the binning of around 400 milliseconds.
00:27:27.362 - 00:27:48.834, Speaker A: So for now, we have this paradigm. If blockchains were to become much faster and to have feature parity, then I think the model would need to change a bit. There would need to be some protection of the data, of the visibility of the data for a period of time before it gets released. But that's not yet a problem.
00:27:51.134 - 00:28:24.674, Speaker C: Okay. And so I just want to understand and go back to something that we've talked about earlier, which is, of course, you see the opportunity that Chainlink's not particularly addressing. But like, can you just crystallize? Like, what is the real advantage? Maybe from a, if you're like synthetix or some of these other protocols choosing their data provider, their oracle. I've always felt that the oracle space is not winner take all. You want to have redundancy. And so I was involved in the early days of Synthetix. They were running their own oracles and making the transition to chain link.
00:28:24.674 - 00:28:49.064, Speaker C: Seeing that happen gave me a finer appreciation of maybe you want to have not just one data provider, one oracle provider like Chainlink, but many others, because the risk of messing it up is really high. So do you have a sense of, like, the projects you're working with? How many of them are have chainlink and are working with Chainlink? Are you replacing Chainlink? Are you complementing Chainlink? Like, how do you think about that?
00:28:50.804 - 00:29:26.614, Speaker A: Yeah, in the case of synthetix, because I think that it's a good example. They still have chainlink as a backup. But the way that it works is it's a toggle to determine whether or not pith should be used or basically trading should be used. Right. So pith is the primary oracle. If you trade on synthetix, then you're leveraging a pith price. Now, if pith and Chainlink disagree over a time period, and it doesn't need to be super high resolution, it could be like the moving averages have gone off, out of whack.
00:29:26.614 - 00:29:40.284, Speaker A: Then they say everything off. Something's wrong here. I think that's a pretty good way to hold on to the side of the pool. The bad approach would be to just like average two oracles together.
00:29:41.424 - 00:29:44.764, Speaker C: That would just maybe wrong. Yeah.
00:29:45.224 - 00:30:22.974, Speaker A: Well, then you just end up with a bad product because you remember when search engines were coming out. At first it was like there was a bunch of search engines. And then there was like the aggregator, like dogpile would be like the search engine of other search engines. And then people realize actually the thing that you want is just the best search results. You don't want to have like a bunch of bad search engines included. And so I think that's probably the right way to think about like in oracle or primary oracle now I think a redundancy makes sense. And so PIF today covers about 25% of the applications that use an oracle.
00:30:22.974 - 00:31:01.002, Speaker A: The other 75% use different oracles. Chainlink is obviously the largest there. Pith is only gone cross chain eleven months ago from its first chain of Solana, and now it's on 40 different blockchains. So most of that growth has happened in the last eleven months. And basically it's been three new applications per week have integrated pith. So I would say that there's probably some redundancy. But if you do redundancy really well within a single protocol, I don't think that there's need for a separate one.
00:31:01.002 - 00:31:36.656, Speaker A: I think that PIF could be a redundant solution. There are sort of backups that are available as well. And PIF has a, if you click on a PIF price, you'll see the EMA, and then there's some stuff that are being rolled out in regards to putting together any kind of arbitrary twap. And so you can use those as good checks as well. Although I do think that competition within the oracle space is smart. And so if Chainlink does introduce new products, and those are things that require the pith protocol to react, that's probably good for the entire ecosystem.
00:31:36.800 - 00:32:02.016, Speaker C: Yeah, chainlink, we're looking at charts here for people seeing video, but chainlink, this is defi lana data, by the way. So chainlink secures or 366 protocols. You guys do little under half of that. 125 in terms of total value served, I guess, or secured, chainlink is roughly 15 billion.
00:32:02.200 - 00:32:03.964, Speaker B: What is this, like winklink?
00:32:04.744 - 00:32:07.640, Speaker A: Oh, that's the justin sun for.
00:32:07.832 - 00:32:10.924, Speaker C: Wait, jason, you can't switch it up. You're missing.
00:32:12.004 - 00:32:13.636, Speaker B: I was toggling between. So if you.
00:32:13.740 - 00:32:14.380, Speaker A: This is interesting.
00:32:14.412 - 00:32:20.156, Speaker B: Like, chainlink's 366 pith is 125 protocol secured, but chainlink has 14 times more.
00:32:20.220 - 00:32:41.860, Speaker C: Yeah, totally. I think that the real metric, I guess, is value secured. Maybe pith is a fraction. You know, chainlink is close to 15 billion. You guys are 1.6 billion. So walks us through like, what do you need to believe for you guys, to your earlier point, redundancy may not be necessary and you could just have one really good oracle solution.
00:32:41.860 - 00:32:46.304, Speaker C: If you guys believe that, then what would need to happen for you guys to flip chainlink?
00:32:47.324 - 00:33:14.752, Speaker A: Yeah. So the majority of that total value secured is consolidated around aave and compound. So it's mostly those two apps. And that's just sort of the nature of how value locked on chain is right now. I don't think that's the best metric for an oracle. I think it's a good one. It shows you which lending protocols are using an oracle, it definitely shows you something that's at risk.
00:33:14.752 - 00:33:57.572, Speaker A: So securing is an important component of it. The other component, I think that's important to add. Hopefully defi llama adds this at some point, but it's the total value traded that's secured every time someone's trading something that is using an oracle, like that volume kind of should be counted up. And in this model in particular, it makes more sense for basically two reasons. Number one, it's the primary users, like the synthetics of the world, they're the ones that are going to be most focused on low latency. The other element of it is that's actually the way that the protocol collects fees from the pith. Protocol collects fees.
00:33:57.572 - 00:34:09.104, Speaker A: So each time there's a trade, there's a fee that gets paid through the network. Today the fees are set to next to zero, but not zero, and governance can choose to update that. Now governance is live.
00:34:10.044 - 00:34:11.532, Speaker B: Sorry, Mike, can you say that one more time?
00:34:11.548 - 00:35:12.855, Speaker A: So when you do a trade on synthetics, what happens is I'll give you the whole workflow just so that you're just. So I can articulate that there's no latency as well, but basically you do a trade, you see a price on the GUI, and that price is actually reading off of pithnet and you click to execute and a message is sent from optimism to Pithnet saying, hey, we'd like to bring the price from this timestamp that Jason just tried to just execute it. It goes back, whatever, 400 milliseconds and sends that price over. And then you execute the trade and you pay a fee. And that fee has three components to it. There's the gas on optimism, there's the fee that synthetix charges you if you're a maker or a taker, and then there's a small fee that pith charges as well. And so every time a pith price is delivered to one of these 40 different blockchains, a fee is paid to pith.
00:35:12.855 - 00:35:46.856, Speaker A: And right now the fees are all set to effectively nothing. But they're set such that people are paying in pith is accumulating fees. There are, I think, over 2 million fee paying transactions per day. And so as governance chooses to figure out what fees are kind of appropriate, we'll see how that number of transactions changes. Like, it may go down a bit, it may go down a lot, may not go down that much depending upon how those dials are changed. Yeah.
00:35:46.960 - 00:36:20.538, Speaker C: So it really is around utilization, no different than like, TVL is kind of like a very crude metric. It's really, if you look at Uniswap, it's the best Dex, because it has a high civilization on the pools, because everyone's trading there. And so you care less about, you care less about how much value is being secured. You're more about the type of protocols, I guess, that are working with you. If you're like a high frequency type of defi application, then you're going to earn way more fees relative to a very low activity protocol.
00:36:20.706 - 00:36:45.754, Speaker A: Exactly. Yeah. Given this business model that I've just described, a lending protocol is not a very profitable client. Now, there are other business models that will make more sense for lending protocols that I can envision and could potentially be rolled out in the future and would actually work for helping them secure and perhaps monetize their protocol a little bit better.
00:36:46.694 - 00:37:03.954, Speaker B: When you think about revenue projections, how do you think about that? What's the market opportunity? What's your guys, I'm sure you can't give too specific of numbers here. When you think about revenue, what is the scale? How many zeros are behind the one? Fast forward a couple of years here.
00:37:05.264 - 00:38:33.658, Speaker A: Yeah, I mean, it's so hard to predict because there are so many assumptions, particularly around like what is the market opportunity for Defi? So, like on chain derivatives today are like sub 1% of binance futures, I think that in a year they'll be over 10%. And so that gives you a sense of like a first kind of really scaling and growing. Now, what are people going to be paying on those? And what percentage will go to market data for that one? It's typical because you'll start to basically find an equilibrium by starting pretty small. But if you zoom out and you just say, well, you asked me in the beginning, is this how the crypto markets develop? And that's my prediction is, yes, at maturity, you could say the revenue model for Defi will perhaps look in such a way where 20% of the revenue goes to the market data. And if the market data is all done via oracles, then perhaps it's 20% of the revenues of DeFi is decent enough as a bad heuristic because all the other heuristics are going to be highly speculative based on assumptions that have very low conviction.
00:38:33.786 - 00:38:39.094, Speaker B: Yeah, I want to get into the token in a second, but what about non financial data?
00:38:40.514 - 00:39:37.544, Speaker A: Yeah, I like non financial data. Conceptually, the thing that PIF has done really well has been take highly valuable data and incentivize users to make it available to everyone with a business model that will work. Things that are just like the public in the public domain, I think are a little bit less interesting. There's perhaps some requirements to just get them on, and if there's a big use case for, I don't know, weather data or things like that, then I can see that pith would eventually expand to it. But for the moment, they're just not highly valuable data sources. There's also no clear avenue of application sector that is really building something unique there. DeFi is really one of the biggest potential use cases for blockchains, and so financial market data is a linchpin.
00:39:37.544 - 00:40:21.404, Speaker A: Therefore, if you can be specialized and you can look like Bloomberg, you can kind of own that segment, as opposed to Google, which Google Finance is not used on Wall street anywhere. So I think that there's a lot to it. But that said, there's no reason why PIP data couldn't be expanded or PIP data sources couldn't be expanded to other types. There are other types of data, though, that I think could be pretty interesting to add in the future. Stuff like credit scores, if done in a way that preserves a bit of privacy would be pretty interesting. And then there's a couple of other things that we've kind of just like thrown against the wall that I think would be, would be pretty cool to add at some point.
00:40:23.024 - 00:40:54.640, Speaker B: All right, everyone, so we talk a lot about the institutions coming into crypto on empire. Santi and I are both headed out to London March 18 to 20th for blockworks, 8th ever Digital Assets Summit. Das this is an institutional buttoned up conference that we've hosted since 2019. I like to joke that it is probably the last remaining kind of suit and tie event in crypto. People are still wearing suit and tie. It's pretty funny, but you'll actually hear from a lot of the largest institutions in the world coming from standard Charter FIS, JPMorgan framework. Folks coming out.
00:40:54.640 - 00:41:25.658, Speaker B: Wintermute, Vanek, Goldman Sachs there are a couple big themes of this conference. One, bitcoin catalyst list, the having and the spot ETF. Two, a view from the buy side three, RWAs tokenization and stablecoins four, global regulatory frameworks five, institutional infrastructure, including banking and payments and six, the macro case for crypto. If you have anything to do with the institutional side of crypto, you have to be there. Santi and I got your back. We hooked you up with a 20% off code. It is Empire 20.
00:41:25.658 - 00:42:16.236, Speaker B: There is a little competition running internally at blockworks to see who can drive the most number of tickets. So help Santi and I out register with our code and you get 20% off. That is empire 20. This episode is brought to you by Toku, the first comprehensive global solution for token compensation and tax compliance. If you say yes to any of the following four questions, Toku is a no brainer solution for you. Number one, are you planning to launch a token? Number two, is your token already live? Number three, are you currently granting your employees or contractors vesting Token awards? And number four, are you trying to figure out how to take care of taxable token events for your team? If yes, you have to get in touch with the Toku team. Toku to high level makes implementing global token compensation and incentive awards simple.
00:42:16.236 - 00:42:49.714, Speaker B: You get unmatched legal and tax support to both grant and administer your global team's tokens. Toku navigates this across the entire token lifecycle, from easy to use token grant award templates, through tracking vesting to managing tax withholdings. Toku makes it simple for leading companies in the space, including protocol Labs, Dybx Foundation, Mina Foundation, Hedera, gnosisafe, gitcoin and many more. Reach out to Toku. That is toku.com empire. Toku.com
00:42:49.714 - 00:42:54.534, Speaker B: empire. Click the link in the description or dm me on Twitter and I'll get you connected to the team.
00:42:56.194 - 00:42:57.654, Speaker C: What's the role of the token?
00:42:58.714 - 00:43:02.346, Speaker A: The token is a governance token and.
00:43:02.530 - 00:43:03.654, Speaker C: Exactly me.
00:43:04.474 - 00:44:02.766, Speaker A: Yeah, so the PIF network is a decentralized oracle and it's designed to be run by the community. And so the token is the way to govern the network. And the token has now been distributed to over 95,000 wallets and users. Historically it was just a middleware b two B infrastructure network and it was a successful one. As we've sort of talked about. We've amassed over 100 publishers. There are 225 applications that are using, or 100 I guess on the Defi lambda numbers it's a little bit lower, but 125 applications that are using Pyth, and that has been done in a series of checks to confirm that this is the right idea.
00:44:02.766 - 00:45:05.034, Speaker A: There is this hypothesis that Defi needs low latency, high frequency oracles in order to be successful. The first way you're going to do that is, well, if you can get enough users within the network, or publishers within the network to create that data so that you don't need to go and ask Nasdaq or NYSE. So that was done. The second one was, well, if you've created this data, do people really care about it, or do they really want just slow data? So that was step number two, was when PIF launched on Solana and stood out there as the primary oracle. Over 90% of the applications use PIF. And number three was, can you build up a cross chain Oracle model that requires people to pay some fee, have price to be delivered cross chain? That happened earlier this year. So we basically passed these three proof points, and now it's a time to, you know, we've gotten out of this experimentation mode, and we can move on to letting the community decide how to govern things going forward.
00:45:05.034 - 00:46:10.174, Speaker A: So all the updates will get turned over, multisigital will be turned over to governance, and then governance will control things like which symbols to be added, which new publishers to be added. How should rewards work? Do we add new segments of data, such as weather, credit scores, things like that? Does the aggregation logic need to change? Should we have a new fee model for lending protocols and things like that? So the governance is going to be managed by users. And what has been really cool about this airdrop is that it's retroactive airdrop. Is that the end users really just got told about pith, perhaps for the first time, by seeing that they qualified for this airdrop, because PIF is this middleware b two B company. It's almost like Arm or Nvidia. Everyone knew about Nvidia because it had a stock and people could speculate on it. And arm people didn't necessarily know about until they had a stock.
00:46:10.174 - 00:46:33.912, Speaker A: Now, you can assume that there's people that want to trade different symbols on, say, a synthetix, but they don't have the ability to ask for the oracle to, you know, make those symbols available. They now can, they can participate in the governance. And so this is the, the way to invite them to the community and say, yeah, like, you have a bag.
00:46:33.968 - 00:47:13.374, Speaker C: And, okay, I want pith to service the protocol that I'm trading with. But what does the average DGen know about oracles and the way, like, like confidence intervals? Like, they have no business in understanding that, nor do you want them to be governing that candidly, like, this should just be a SaaS business as much as I'm a token maximalist. Like, I just don't see it, like, what needs to be governed? That the wisdom of the crowds, which is not so wise, or has been and Dao participation is terrible. Why even introduce that? And why not just keep executing as a team, run an incredibly profitable business model like Bloomberg has, and call it a day?
00:47:13.954 - 00:47:31.772, Speaker A: Yeah, I think that the ethos of it is to have it be decentralized. Now, I agree that if you do the decentralization or the governance wrong or poorly, then you ossify a protocol and you don't get it to develop any further.
00:47:31.908 - 00:47:53.204, Speaker C: It's like if United Airlines all of a sudden gave everyone points in the ability to determine if they should be using Rolls Royce as some other provider for jet engines, and the community somehow decides to use a shitty jet engine manufacturer. Well guess what, folks, I'm not going to fucking fly united after that. I'm exaggerating here, but you know my point, I'm just.
00:47:53.784 - 00:48:43.774, Speaker A: Yeah, exactly. But I mean, it goes to the, like, what is the utility of governance token? And, you know, theoretically if you're buying a share of arm, you get a say on the proxy and you get a vote for somebody. I think good governance will evolve into things like this. Like synthetix does a good job, they've got these, these councils, and so you end up creating an agile environment where people actually get to make decisions over the period of their tenure, and then they go and run for council again and they ship like they have new versions and there's a lot of, it's a dynamic protocol versus something like, say, Aave or compound, where the governance model is completely different and it doesn't move nearly as fast.
00:48:44.382 - 00:48:55.274, Speaker C: Are you as subsidizing part of the fees or incentivizing projects to work with you guys? And has that become easier because you have a token and you can subsidize or incentivize them to work with you in a kind of referral manner.
00:48:56.534 - 00:49:16.818, Speaker A: That's not something we're actively doing. But part of the retroactive airdrop did include protocols that were using pith. And so during the snapshot window there was an allocation that went to the end users and then there was allocation that went to the protocols, daos or treasuries themselves.
00:49:16.986 - 00:49:21.026, Speaker C: Yeah. So that's a very big number. I think you said 90,000 wallets or so.
00:49:21.210 - 00:49:29.514, Speaker A: Yeah, 90,000 wallets, and it involved 27 blockchains in terms of the usage.
00:49:29.634 - 00:49:32.778, Speaker B: So largest cross chain airdrop of all time.
00:49:32.946 - 00:50:15.884, Speaker A: Yeah. So basically, if you used an application on 27 different blockchains, you could have been eligible for pith, for the pith airdrop. And I think people real like what I started to see over the last week, since the tokens come out number one, so many more people heard of pith. It felt a little bit like the overnight success that was three years in the making. No one really appreciated the fact that there were so many protocols that were integrating, or that there were so many publishers in the network. The business model wasn't quite clear to people. The fact that there was these two dilemmas wasn't necessarily clear to people.
00:50:15.884 - 00:51:00.924, Speaker A: And so that was amazing. The other element of it is that because there are so many participants within the network, a lot of this stuff can be made to look quite easily, pretty quickly. The token distribution mechanism was, okay, well, let's just look at who's using these protocols. There's a lot of users. There wasn't a, okay, we're going to have this thing where people are in our testnet and playing around, and we're going to try and figure out which ones are sybils and which ones are airdrop farming. It was quite simple to be able to make it relatively simple, resistant, because we had a long history of users. And then the other thing was the token was then listed on a number of exchanges.
00:51:00.924 - 00:51:50.170, Speaker A: Well, the majority of those exchanges are publishers. In the fifth network, there are ten of them that are actually publishers from Okex bybit. There's many that are already part of this protocol, and then there's market making firms who are trading their token, and all those are in the network. So when you amass these participants, a whole lot is possible very quickly if you've done the legwork. And so I think that's what's been kind of most eye opening to me, is that all the work that was put in, especially during the bear market, has really now paid off because there's so much possibility, and it won't necessarily slow down. There's a good argument to be made where new segments of publishers may join the network, from asset managers to banks. And it won't just happen like the one off.
00:51:50.170 - 00:51:54.946, Speaker A: It's similar to the trading firms when they get the model. It happens really, really quickly.
00:51:55.130 - 00:52:34.914, Speaker C: Yeah. So I think a large part of what you're describing in the inception story was like this incredibly neutral Switzerland model to provide, dump your data and get some sort of compensation. If you're within, you're providing accurate data that was working. And it doesn't sound like, I mean, you were successful in convincing a lot of people, right, that wouldn't work otherwise. And you solve that coordination and incentive problem and you created all this value. So oftentimes you need a token to do that. But in this case you were successful pre token because you had all the major, sounds like you had a lot of the best data providers out there.
00:52:34.914 - 00:52:52.334, Speaker C: From their perspective, does the token matter? Is it a distraction from them? Do they get compensated in what is now a more volatile unit of account? Or do they like the idea of having this token and maybe participating in governance in some way, shape or form?
00:52:53.834 - 00:53:52.806, Speaker A: Yeah, I mean, I'd like to think that they'd be interested in participating in governance. Many of them, I think are. But the reality is that there's 22% of the supply that's reserved for rewards for publishers that has kind of been set already in the white paper, and that's really been the carrot, that's inflation. And so if fees don't pick up for a while, then there's these rewards that will be able to be used to incentivize them. And a lot of the publishers are motivated by this. Now if you just paid them in cash, then what they would try and do to optimize for maximizing their profit would be to go and connect to as many marketplaces or oracles as possible. And so having them incentivized with something that's like the PIF network token or governance token, they're a little bit more aligned with the long term success of the project.
00:53:52.806 - 00:53:55.342, Speaker A: And so there is optimization.
00:53:55.358 - 00:54:00.194, Speaker C: Would it backfire if the token value collapsed 90%? Does that backfire?
00:54:02.034 - 00:54:04.370, Speaker A: I mean, I mean, I guess it.
00:54:04.402 - 00:54:22.554, Speaker C: Also, like economically, like if, if the value of all the thing that you're securing and the swaps just goes down 90% as well, because everything's so correlated in crypto, then they're not making as much money. And so they may detract, they may leave, they may be still incentivized to work with as many providers to make up for the last. Kind of like monetization.
00:54:22.594 - 00:55:20.964, Speaker A: Yeah, sure. I think, I think there's like the getting the rewards right is a good idea, is basically my response to that is if you have inflation structured poorly, then you're going to have a problem with the emissions of the supply coming out too fast. If it's done too slowly, then you've got other problems where people aren't necessarily getting compensated for even their fixed costs, and they'll be kind of incentivized to walk away. So I don't have like a great answer for how it will work. This will be something that kind of gets proposed governance. We'll probably have a couple of different models that dural labs will propose, but I think there's a way for this to work and be equitable and have a long term economic model that survives for many years.
00:55:22.274 - 00:56:04.674, Speaker C: Yeah, that's one of the things that deepen is all the rage these days, but it's been around for such a long time. And I think one of the biggest criticisms that I have, and I've heard as well shared by others, is that the rewards are very simplistically modeled and should be more dynamic based on the contribution. It becomes hard. It's like milestone kind of fundraising. It's just like how do you like express that in a smart contract or like in a vesting like, reward mechanism? But I'm curious, in a perfect world, maybe share with us how you think about this reward mechanism and perhaps changes that could be done or that you may be proposing.
00:56:05.734 - 00:56:12.094, Speaker A: Yeah, I mean, I don't have really well thought out answers on exactly how.
00:56:12.254 - 00:56:15.474, Speaker C: It would work, but I'd say it's working fine now.
00:56:16.344 - 00:57:16.664, Speaker A: It is working fine now. Yes, it's kind of early days for the rewards, but basically I think the rewards should be used to incentivize markets that don't yet have a tremendous amount of demand, but eventually will. And if you can do that sparingly so that you're not overpaying for things to get on chain and you can do it so that it's constantly expanding the network, then the product set becomes better. And that's really what we're focused on. We're focused on having the pith product continuously expand. More publishers cover all financial market data, and so if done right, then every month the number of symbols will go up, the number of users will go up, the number of publishers will go up, and the rewards is kind of one of the incentives and levers that we have to make sure that happens.
00:57:17.084 - 00:57:40.474, Speaker B: Why did you guys, Mike, you guys could have done the token drop a different way. So when I hear about pith, and this has been a really interesting conversation, it's a very b, two b business. It's b, two b, two c in a sense, but really it's a b, two b business, I think why focus on the c part of this instead of just going direct, instead of really just focusing on the applications that, that have built on pith?
00:57:41.574 - 00:58:34.320, Speaker A: Yeah, I think it's within the ethos of crypto you have to take a view that there are going to be people that want to participate in open source projects. That's my view. And I think that it's the reason why jump virtue and Jane street are all doing this is because it's open sourced, that there's an incentive mechanism that they can contribute, they can step in and out of participation. And we wanted to open up the door to as many participants as possible. There are smart people who will begin to participate within pith governance that aren't necessarily publishers today. And I think that it's important to bring in the best minds and invite the best minds to be able to do this. So, in short, it's a cast the net as wide as possible.
00:58:34.320 - 00:59:26.268, Speaker A: See who shows up and can kind of join and really wants to contribute. Because we've been blown away with some of the people that have just learned about the pith network and now are looking for ways that they can contribute. So our awareness has gone up dramatically and we're seeing people that are basically becoming tribal around Pif. Like they're very bought into kind of the vision and the mission. We're getting lots of applications for any positions that we have open on Dura labs. I think that's really the idea around it. It's make this so that it can be distributed and it doesn't have to always be building up a huge treasury war chest that can become a huge company and then becomes extractive.
00:59:26.268 - 00:59:47.432, Speaker A: The ethos is build the best oracle. If that's the end goal of the mission and you don't need to build anything further, then great, then piffling lives as an oracle product and you can kind of leave it as opposed to, like, the exchanges, which is, well, what are you going to do this year? How are you going to increase your revenues by 5% so that Wall street is happy? That's not really the ethos of crypto.
00:59:47.568 - 00:59:56.032, Speaker B: Yeah, Mike, man, appreciate the conversation. I have a feeling this will be the first of many that we have about pith in the future and excited to see what you guys do. So thanks for coming on.
00:59:56.128 - 00:59:57.400, Speaker A: Thanks so much for having me, guys.
00:59:57.552 - 00:59:59.254, Speaker C: Thanks, Mike. Congrats.
00:59:59.384 - 01:00:16.858, Speaker B: Hey, everyone. Thank you so much for watching today's episode. Really hope you enjoyed it. We wanted to take a second to just remind you about our upcoming digital asset summit in London, March 18 to 20th. Santi and I got your back. Seats are limited and we hooked you up with a 20% off discount code. It is Empire 20.
01:00:16.858 - 01:00:28.394, Speaker B: If you heard it earlier in the podcast, there's a little competition running at block works to see who can drive the most number of tickets. So when you register for the digital Assets summit, make sure you use our code. Empire 20. See you in London.
