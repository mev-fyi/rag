00:00:00.240 - 00:00:38.776, Speaker A: Hey, everyone, if you have been listening to Empire, you know that Santi and I are fed up with unaffordable fees and frustrating transaction speeds that make the on chain experience basically unusable. So the arbitrum team reached out, and they showed us the platform. They showed us what you can do on Arbitrum. Whatever you're doing, you can experience frictionless transactions at lightning speed on Arbitrum. So head over to portal Arbitrum IO and check it out. What's up, everyone? Before we jump into today's episode, I'm excited to share empires for first ever security partner, Harpy is the best tool to prevent your wallet from theft in real time. Harpy is not just a security solution.
00:00:38.776 - 00:01:08.770, Speaker A: They are a peace of mind solution. But don't just take our word for it. Harpy is the only wallet security solution that protected 100% of its users from attacks like the ledger one in Q four, which was an off chain signature attack. To learn more about Harpy, click the link in the show notes or visit Harpy IO. Empire. What's up, everyone? Before we jump into the episode, little plug for digital asset Summit coming up in London, March 18 to 20th. Tickets are pacing so far ahead of schedule that we had to decrease the discount code.
00:01:08.770 - 00:01:32.750, Speaker A: So instead of Empire 20, it is now empire ten. Head over to the website digital asset summit, Das London, March 18 to 20th. Use code Empire ten and get 10% off your ticket. See you in London. All right, everyone, excited for this app today is we are talking Monad. We have Keone, who's one of the co founders of Monad. We got Santi, of course, we got John Char, co founder of dBA.
00:01:32.750 - 00:01:34.594, Speaker A: So, John Keoney, welcome to the show, guys.
00:01:35.214 - 00:01:36.514, Speaker B: Thanks so much for having me.
00:01:36.934 - 00:02:10.366, Speaker A: Yeah, yeah. Pumped to have you here. So I think the best place to start, there's a big conversation we had about kind of integrated versus modular, or I guess you could call it monolithic versus modular, a bunch of conversations that you can have about, like, the kind of technical optimizations you guys have made at Monad. There's a wide ranging conversation we can have here. Keone, I think the best place to start would be to pick on you, because you spend a lot of time actually working on Solana when you were at jump. And I'd love to hear kind of, like, the thought process behind, like, what happened in your head when you said, okay, we need to go build this. We have a conversation.
00:02:10.366 - 00:02:20.694, Speaker A: There's a decision that you can make. We could either go build something on the SVM. We could go build something on the EVM. We could do a roll up on the EVM. Like, how did you settle on building what you guys built with Monad?
00:02:21.634 - 00:03:45.504, Speaker B: Sure thing. Just to give a bit of background. So I've been working for, I guess 13 years now, about ten years in the high frequency trading space, and then started Monad about two years ago, along with two other co founders. So right at the start of 2022, but before that, or immediately before that, I was part of the crypto team at jump, and JMP is a big supporter of Solana. JMP was really excited about what Solana had done in terms of building a really performant integrated blockchain with high performance and low fees. And as a member of the team there, just spent a lot of time seeing applications that were getting built that couldn't be built in other environments where the low fees were an enabler and ultimately could allow developers to build applications that can cross the chasm from web3 into web two and have the potential for mainstream adoption. I think that normal people hate paying $3 ATM fees, so why would they be okay with paying $10 uniswap transaction fees, not to mention the slippage that they're going to experience interacting with liquidity? That's mostly not near top of book.
00:03:45.504 - 00:04:31.322, Speaker B: The space of solutions is still pretty nascent overall, and there's a need for much more performant EVM. EVM is the dominant standard for smart contract development. It's like the JavaScript of Web three, and there's just a need for much more performant EVM. So while I was part of the crypto team at JMP, supporting builders who are building in the Solana Space, among others, I just, along with my co founder James, who been working with since 2014, mostly as a high frequency trader, together, we just realized that there is this need for more performant execution and no one was building it. So ended up leaving jumping and decided to start Monad with his vision.
00:04:31.458 - 00:04:52.214, Speaker A: Nice. Why did you guys. Okay, so once you. Sounds like the first decision there is like, we need to go build something better. Second decision is let's go do it on the EVM. Then there's another decision that you can make, which is do we go build a roll up or not essentially, or do we go build like an l two? Or do you do what Monad did? Which is Monad is a EVM compatible l one. Right.
00:04:52.214 - 00:04:54.694, Speaker A: Walk me through that decision.
00:04:55.324 - 00:05:45.900, Speaker B: Yeah. I think that with any startup, the decision making about what to focus on should be driven by doing something that is unique in the space. So whenever there is just like a glaring need, and it seems like no one's building that, that's automatically something that's going to be more helpful to the space overall. So that was one thing, was that we could just see that known as really working on performant parallel execution. But I guess beyond that, yeah, I would just say that EVM is the dominant standard. EVM is. Most developers are building in solidity, or in some cases in Viper or other languages, which compile down to EVM.
00:05:45.900 - 00:06:47.794, Speaker B: 97% of all capital on chain, all TVl is in EVM apps. So there's this huge network effect also, on the research side, almost all the applied cryptography research is being done in the context of EVM. People don't think about the research part as much, but there's a huge network effect there as well. So there's this huge need, but there's not as much focus on the execution side. To come back to your original question about roll ups, why not build as a roll up? I think it's about going in an orthogonal direction that we thought made sense. We felt that the throughput of Ethereum at roughly ten transactions per second, or the throughput of rollups at about 100 transactions per second, that there was still a huge gap between that and what we felt should be possible given what we know about computers. So it was a very worthwhile effort to go down the path of making execution a lot more performant.
00:06:47.794 - 00:06:59.018, Speaker B: Um, and that improvement is actually orthogonal to the question of building out, um, a roll up mechanism and doing this fractal scaling thing. So it's just doing something orthogonal.
00:06:59.186 - 00:07:12.574, Speaker A: Yeah. How on the. Just coming back to the EVM conversation now, I have another question at the roll up. How much of the network effects are related to developer network effects versus user metrics versus user network effects?
00:07:16.194 - 00:08:14.344, Speaker B: I would say it's 50 50. I think that there is a lot of tooling that is kind of ingrained that users are familiar with metamask, ether, scan, the graph, these kinds of things that are common tools. I think that for developers it is much more appealing to build for the common standard, because then there's no vendor lock in effect. Developers who are building and maybe I should actually take a step back for a second and just say outright. Monad is a new EVM layer, one that's offering full bytecode EVM compatibility with really high throughput. So over 10,000 transactions per second of throughput, 1 second block times, and single slot finality. Monad is basically giving users and developers the best of both worlds between performance and portability.
00:08:14.344 - 00:09:04.424, Speaker B: Because right now, if you're a developer and you're building an application, you're having to choose between building in the portable way, which is building for the EVM. But then the environment that the application is deployed in has limited throughput and high transaction fees, like ten cents to a dollar at least. On Ethereum mainnet it's more like five dollars to fifty dollars even for a simple transaction. So you can go that route and go portable, but then things are expensive, or you can go the other route of perform it, and then transactions are cheap, transactions are plentiful, but now you've built for one very specific ecosystem, like Solana or Aptos or SWE, where it's really hard to move out of that ecosystem. So Monad is really giving developers the best of both worlds between those trade offs.
00:09:05.084 - 00:09:19.944, Speaker C: How much is the portability issue, like going to be an issue over the next, I don't know, 35 years? I appreciate from your standpoint, you could have maybe done SVM or something.
00:09:22.096 - 00:09:22.171, Speaker A: Like.
00:09:22.171 - 00:09:28.644, Speaker C: Is portability that an like issue that we're talking about today? Really focused, but it will be kind of largely irrelevant down the road.
00:09:32.624 - 00:10:22.354, Speaker B: I think from, from what we've seen in the past with technology, there is basically a common standard that develops and then it's, it's very, very hard to change it. There are maybe subtle tweaks, like, you know, so like think of HTML. There's like the different tags, like h one or Div or BR or what have you, and there's subtle tweaks to it, but the core is still the same. And it'd be very hard to come up with a new standard for websites because there's so many websites that are built already and the browsers are already built to support that. So yes, I think that the network effect is massive. Um, and it would just change would ultimately happen just through like incremental improvements like eips that introduce new op codes or new pre compiles.
00:10:23.294 - 00:10:40.074, Speaker A: John, what do you think about this? Um, I know you guys are a big investor in eclipse, um, which is kind of like focused on the SVM. And I mean, you can explain if you want to, but, um, what do you think about this idea of like compatibility? Um, and just like kind of monads decisions here?
00:10:40.904 - 00:11:20.736, Speaker D: Yeah, I think the two of them actually highlight really well the differences of the reasons of why should monad be an l one or an l two or whatever. I mean, one is just do what you're really good at and the Monad team, this is the background. They're really good at optimizing at this level of the stack. That's what they understand really well. The other part is that I think a lot of people still, to some extent, just underappreciate how much complexity and how much there is to deal with. When you're thinking about building a new l two and someone like eclipse, they're thinking about, okay, this is the vm we're using. We need to think about, okay, which da layer are we gonna use, and how do we plug into that? And how do we settle to Ethereum and do the proofing for all of this? That, on its own requires a gigantic team.
00:11:20.736 - 00:11:43.668, Speaker D: And then everything that Monad is doing requires a gigantic team, which is an orthogonal problem to everything that eclipse is solving. And so that's why you saw someone like them who, okay, we want to super perform in l two. What's the way that you do that in practice today is like, you just go take the SEM already. Like, hey, this is the one that the current implementation of it is really good. We can ship this thing today and it works. It's really good. The EVM is not there yet.
00:11:43.668 - 00:12:22.864, Speaker D: That's going to be multi year work of the type of stuff that monad is doing. So it's definitely a rational decision to do that. Part of it is also, I would say, the timing of it, particularly just when Monad started, I don't think there was any viable DA solution for them to have looked at at that time of Celestia and Eigen Da. And these things were so far off. Now, if you're building a super optimized DVM today, now it could be a consideration of, do we think about from the start, do we make this thing in l two using an alt DA? And that's what seems like mega ETH, to my knowledge. That's what they were thinking of doing, is like, hey, we have super optimized EVM, and we'll put it on, like Eigen Da. But like, when you're building with a constraint, particularly like Ethereum DA, like, that obviously just doesn't make sense in the first place.
00:12:22.864 - 00:13:09.104, Speaker D: As far as the portability across ecosystems, I definitely think the trend over time is going to be reducing those barriers at, like, all levels. The question is, I guess, like, how much does that reduce at the end of the day? Because in theory, at the limit, yeah, you would hope that I just deploy an application, and whether I put it on an EVM chain or I use neon on the SVM, and that just works perfectly well. Or I'm using stylus and arbitrum, that everything should hopefully work across all these ecosystems super well. In practice, we still did quite a meaningful amount of work before. That's actually a seamless experience working across these ecosystems. And there's a lot of work in between there and the, like, it's just an implementation detail. Like, we can fix this with engineering is, like, quite a ways off.
00:13:09.104 - 00:13:17.024, Speaker D: So I definitely think the trend is to reduce those barriers over time. But in practice today, like, there's obviously, like, very clear barriers still for developers to be working across different ecosystems.
00:13:17.324 - 00:13:54.524, Speaker A: Yeah, I was actually. I was thinking about that in light of the part about Da. Like, I was thinking about Antonio and DydX, and would they have actually gone out and built their own chain if the roll up, the roll up stack has gotten so much better in the last 18 months? Like, would they have made that decision if, uh, they were making that decision? If they were making that decision today, would they have done that? Or would they have just, like, leveraged all the great pieces of technology and I guess, Keoni, to, like, throw John's point over to you as a question, like, now we have Eigen, Da and Celestia. Like, would you go through this, like, hard battle of, like, creating the entire stack yourself if you were launching Monad today?
00:13:55.564 - 00:15:06.966, Speaker B: I think that roll ups are the new app chains. So if the pitch to developers in the past was, you can build a new chain where you have control over the VM and can make modifications to how the VM works, or run a specific program. In the case of DYDX, the order book internal to the client code, and maybe make it more performant that way. That was always the pitch. And so the roll ups are kind of like the more modern version of that, but with definitely some differences, like, from the current deployments that we've seen so far, the roll up sequencer is centralized, so that's a huge consideration for Dydx, I think, from what I understand. I don't want to really speak for them, but I think the decentralization aspect is important. It's not okay to just have one sequencer that's deciding all of the matching for various reasons.
00:15:06.966 - 00:16:00.652, Speaker B: So I think that that would be probably the reason why they would continue to use the app chain approach with Monad. Sorry, go ahead. Yeah, I was just going to say for Monad, I think we would still do exactly. Our team would still do exactly the same thing that we've done, because I still don't feel that the solutions are all mature. And I think there are opportunities at all levels of the stack, whether it's execution or consensus, or even the piece that integrates those two together. I think there's still need for very performant implementations at all levels of that stack. And for us, we wouldn't necessarily be willing to delegate that optimization to another piece, at least not in this current day and age.
00:16:00.652 - 00:16:20.794, Speaker B: There's still a huge amount of low hanging fruit. From an optimization perspective. It's really important for ultimately the end users of Monad, the users and the developers to have most performance system, so that we can get as much performance out of commodity hardware as possible and deliver really high performance decentralized network.
00:16:21.454 - 00:16:49.424, Speaker C: I want to maybe take a step back here, and for people that may be pretty uninitiated. Can you talk about this idea of parallelization? It's something that is a buzzword out there, but I think it's important to just lay the context of what has been. When we think about scalability like, you know, single threaded versus parallelized systems, like, and the choice that you guys made, I mean, I think it would be useful to just lay the foundation and then for the subsequent discussion.
00:16:50.524 - 00:17:48.628, Speaker B: Sure thing. So I guess I want to mention two major points here. The first one is talking about how optimistic parallel execution works, and how that contrasts to other approaches. And then the second thing I want to mention is the fact that the biggest bottleneck for execution is not the cpu cycles like, it's not the computation being done, it's actually the state access. So the actual bottleneck for execution is making it possible to access a lot of state for a lot of transactions in a very performant way, because I think that's really key to the actual performant parallel execution that's in Monad. But rewinding back to that first point, Monad does optimistic parallel execution. What that means is that the transactions in a block are linearly ordered the exact same way that they are in Ethereum.
00:17:48.628 - 00:19:00.808, Speaker B: And the result of running that linear order always has to be just the result from running those transactions one by one in the most naive single threaded way possible. With Monad, the optimistic parallel execution is basically a black box that strategically runs transactions in parallel, while always producing the same end result as if those transactions had been run one after the other. And I think that's the first important point to emphasize, because sometimes when people hear parallel execution, they're worried that it means that now a smart contract developer who's, when their contract is running like a function, is being called by some user that that function call might be affected by some other smart contract that's running in parallel. And that's just not the case. Like the end result is always as if those transactions were run independently one after another. That's the first thing I want to mention. Now how do we do that? So the algorithm is actually a very simple and intuitive one, and I'll try to describe it verbally and see if it sticks.
00:19:00.808 - 00:20:17.094, Speaker B: So, roughly speaking, optimistic parallel execution means running a bunch of transactions in parallel, as if they're starting from the same starting point from a state perspective, and then generating a bunch of pending results, where each pending result records the inputs and outputs for that transaction, and then committing those pending results in the original order that the transactions were originally defined as. We're going through the process of stepping through those pending results one by one, inspecting the inputs for that pending result and making sure that those inputs have not since mutated. And if they have, then we reschedule the work and immediately go re execute. So, as you can see from this algorithm, a couple of things jump out. One is that every transaction is executed up to two times, the first time in parallel, and then the second time, possibly when it's being run through again serially. But hopefully most of the transactions don't have to be re executed because the inputs have not changed. The only reason the inputs would have changed is because one of the other transactions that was being run in parallel mutated one of the inputs as one of its outputs.
00:20:17.094 - 00:20:38.134, Speaker B: But then the other thing to mention is that even in the event where there is re execution, that re execution is quite cheap, because the dependencies like the inputs for that transaction are already in cache because they've already been pulled into cache from the first execution, which was running a bunch of transactions in parallel.
00:20:39.014 - 00:21:29.208, Speaker C: Yeah, that makes a lot of sense in terms of, to give people an idea of how big of an improvement this is, how much of the transactions currently in Ethereum or other networks that are not running parallel execution would have this conflicting state that you'd have to rerun it sequentially, because when you think about, you have these hotspots, you might have an NFT mint going on, and concurrently there's, you know, people trying to do defi on an ongoing basis, and there might be a game that's running as well. But, you know, for the most part they're all kind of independent. They're not like conflict sharing state, if you will. But I'm curious if you, if you've done some quantification around how big of an improvement it is relative to what we have currently.
00:21:29.376 - 00:22:23.788, Speaker B: Right. So I understand where your question is coming from, and I guess the first thing I want to point out is that even in the event where there are many transactions that are conflicting, the parallel execution, this two phase process is still a significant improvement. And the reason for that is because state access is the biggest bottleneck. So this might be a good segue to talk about the second thing that I was alluding to, and then I'll come back to your question. But the point that I want to make is that with these transactions, they're actually pretty simple from a computational perspective. Like if you look at uniswap v two swap function, it's like doing a little bit of math. There's some arithmetic, some addition and division updating some balances.
00:22:23.788 - 00:23:01.792, Speaker B: The compute is actually very, very simple. The thing that's expensive about executing that transaction is that it has state dependencies. You have to go look up the balances of both of the balances of the two tokens for that pool. For ethereum, state is stored ultimately inside of a database called LevelDB, or they're migrating out another one called PebbledB, but it's a commodity database. So it's like a high level database used in many other applications other than blockchain. It has a tree structure underneath. That is how the data is actually stored on disk.
00:23:01.792 - 00:23:54.570, Speaker B: But it's like a very high level database for storing data. And the process of reading a value from state, like reading an account slot tuple from state, is quite expensive. And the reason that it's expensive is a couple of things. One is that the Ethereum Merkel tree data structure, which is how allstate is stored, is being embedded inside of another tree structure. So there's almost like quadratic amounts of lookups needed in order to navigate all the way down to one of the leaves in this merkle tree to go look up some value, that's one thing. But then the other thing is that these databases don't support asynchronous IO. What that means is that when we have many threads that are running in parallel, running many of these transactions in parallel, and they're all trying to go read something, some piece of state, they block each other.
00:23:54.570 - 00:25:09.048, Speaker B: And so effectively, the latency of looking up some value from state is basically sequential access, doing a call that costs 40 to 100 microseconds or more if there's in this case of this quadratic lookup because of the other underlying data structure. So it's just super expensive to go do a single lookup. And all these lookups are kind of blocking each other, so they're all cascading out the total amount of time to go look up something. So when you implement parallel execution, the bottleneck ends up being that state access. So with, you know, just to bring it all back with Monad, probably the thing that we've, our team has spent the most time on actually is building a custom state database called Monad DB, which natively stores Ethereum Merkel tree data on an SSD and basically unlocks the power of the SSD. Because ssds are actually very high bandwidth, they're slower than storing something in memory, but they actually, you could think of as a bottle with a big bottleneck. Actually, you can get a lot of data out of SSD as long as you have software that can appropriately make all those lookups in parallel.
00:25:09.048 - 00:25:26.084, Speaker B: So with MonadDB, there's full control over how the state is being stored on SSD. And then that ultimately allows these lookups, which are being done in parallel, because there's a whole bunch of threads running in parallel that can proceed much faster. So then this overall process of parallel execution can be much more performant.
00:25:27.704 - 00:26:11.624, Speaker A: Sticking with parallelization. John, there was this tweet by R 89 Capital said, it's hilarious to me that some eths haven't figured out that new l one s that parallelize the EVM are a long term vampire attack on Ethereum. And you responded, you said, look, I generally agree if the EVM alt l one isn't doing stuff to meaningfully improve the tech and upstream improvements, but there are a couple folks, like, if you are actually doing x, y and z, then it is basically a meaningful x. It is a meaningful improvement. I think you said something like that and you said, for what it's worth, from personal experience, Monad is the only l one that we've seen do all three. And I'd love to hear you maybe expound on that half baked quote that I just shared.
00:26:12.004 - 00:26:37.276, Speaker D: Yeah, sure. Yeah. Like, I, there's always this debate of like, what's cannibalistic to what and like, what's competitive with what in crypto. And at the end of the day, I mean, like got like a million chains. Like everything is competitive and like nothing is competitive to some extent. Like, yeah, if you, if you're of the view that like everything's going to sit on one chain at the end of the day, then yeah, then technically everything is cannibalistic to your chain. Um, I don't think almost anyone thinks that.
00:26:37.276 - 00:27:10.164, Speaker D: And certainly that's not the Ethereum view of the world. So in the Ethereum view of the world, it's like, okay, we're going to have a ton of chains. And so like, what are the chains out there that like add some level of value back to ethereum? Um, I'd say there's generally like three kind of different things that I see is like, one is like, you just directly pay fees back to the chain in some way. So that's what like, most ETH roll ups that like, use Ethereum da do today is like, they are literally just like paying ETH down to the base layer because like they are using the computational resources today in the form of call data. If you, they're literally just paying money to use the chain. It's pretty direct. Like, you add value back there.
00:27:10.164 - 00:27:54.094, Speaker D: The second thing I would say is that you are in some form indirectly, I would say more. So adding value back in the form of expanding the usage of ETH and the ecosystem itself, that would be something more like eclipse is a perfect example of that. They're going to pay almost nothing in direct revenue back to the chain because settlement costs are like, they're super cheap. They're going to pay da costs to Celestia because it's cheaper, but it expands the network effects of, hey, everything's going to be ETH based. Like ETH is gas, et cetera, ETH is money, all of that. So that very indirectly, but adds a very meaningful amount of value, particularly in the view of the world, where the value of ethereum isn't like a DCF, it's like ETH being used and the network effects around the ecosystem. That makes sense.
00:27:54.094 - 00:28:59.754, Speaker D: So Monad would not have either of those two as it's like kind of on l one. And that is presumably not using ETH directly in any form of ass or anything like that. So then the last thing is, are you, in some soft way adding to the network effects and the development around the ecosystem? And so a lot of the work that Monad is doing is fundamentally worked that anyone else who's building an EVM chain can at some point learn from and use in the future of. A lot of the stuff that Monad is doing is applicable to Ethereum and other EVM chains, client optimizations that these other teams can make. And so that comes down to, at the end of the day, it's kind of more difficult to measure this of like, how involved are you with these different communities and kind of adding value back what's the licensing all of this stuff that other people can go use what you're doing. And from personal experience, I mean there's a number of teams who are also working around some similar areas now where it's like avalanche is doing work around firewood, their databases, similar other forms of pipelining. I mean, Sei is doing parallel EVM stuff, polygon.
00:28:59.754 - 00:29:21.966, Speaker D: There's a ton of people who are doing this stuff. So it's just a subjective measure of is the team adding value back and valuable to the ecosystem? Because at the end of the day, anyone that's working on EVM, if you're doing it in a good faith manner, that's open source work. Well then other EVM chains are eventually going to use this stuff. At the end of the day, open solar software is like never going to be your moat eventually because everyone else is going to be able to eventually use it.
00:29:22.110 - 00:29:31.126, Speaker A: Yeah. Keoni, what are your thoughts on just like does Monad compete against Ethereum or not? Because I think I could probably make the argument both ways here and I'd love to get your take.
00:29:31.310 - 00:30:47.044, Speaker B: Yeah, Monad is definitely not competing with Ethereum. I think that Ethereum, one from a pure execution perspective, is focused on really high value, low frequency transactions. But we think that there's another mode that is really important as well, which is individually low value but high frequency or high count transactions. I think that. So for example, with Ethereum, Ethereum is delivering about a million transactions per day of throughput. So that's really not enough to support any single application that is able to cross the chasm to mainstream adoption and has hundreds of thousands or even millions of daily active users. Whereas Monad, with several different improvements that the Monad team has made, including parallel execution, custom state database, asynchronous execution, and high performance consensus, all these things put together is delivering a billion transactions per day, which then is much closer to being able to support a single application that has a lot of users or that has high interactivity.
00:30:47.624 - 00:31:07.162, Speaker A: How do you think about Keone? So Monad increases execution? I think most people think about that in terms of increasing the number of transactions. That's the first place you went. ETH does a million, Monad could do a billion. How do you think about this in terms of increasing the complexity of the transactions instead of just increasing the number of transactions?
00:31:07.328 - 00:31:14.574, Speaker B: Yeah, for sure. So the, the transactions per second is like very much like a simplification.
00:31:14.734 - 00:31:15.774, Speaker A: It's a nice meme.
00:31:15.894 - 00:31:51.734, Speaker B: Yeah. I think at the end of the day what is important is, well actually, the, the real measure is basically gas per second. So ethereum supports about 1.25 million gas per second. Gas is just a unit of work, unit of complexity. And so Monad is supporting a billion gas per second, which is about 1000 x. So you could have transactions that are way, way, way more complex from a gas perspective, you could just have both more complex transactions and many more of them on Monad than on Ethereum.
00:31:51.734 - 00:31:56.294, Speaker B: But at the end of the day, the capacity is basically a billion gas per second.
00:32:00.194 - 00:32:11.754, Speaker A: Do you want to get into? There's kind of like, when I think of Monad, there's like four big breakthroughs. We covered parallelization, Monad, DB, is there anything else you want to cover? Whether it's like BFT or deferred executioner or any of that kind of stuff that we might have missed?
00:32:11.874 - 00:32:28.894, Speaker B: Yeah, well, actually one thing I wanted to get back to was Santi asked about the collisions of transactions and what happens if you have people always, and it's a good question, like, what happens if someone just submits a whole bunch of transactions that are all tied to each other and you can't?
00:32:29.604 - 00:32:36.624, Speaker A: Just to make sure I understand Santee's question, is this about dependencies between transactions? Is that the same thing as collisions?
00:32:37.244 - 00:32:38.628, Speaker B: Yeah, I think that's his question.
00:32:38.756 - 00:32:39.548, Speaker A: Okay.
00:32:39.716 - 00:33:20.162, Speaker B: Yeah. So I think maybe first it would be helpful to just give an example of what a collision looks like. And then I'll also then try to explain why the combination of parallel execution and MonadB is ultimately able to make the problem of any collision much less severe than you might otherwise think. So maybe first of all, like imagine that there's three transactions. My USDC account has 100 USDC in it, and there's three transactions. In the first transaction, I send ten USDc to John. In the second transaction, like someone mints an NFT, totally unrelated.
00:33:20.162 - 00:33:59.560, Speaker B: And then in the third transaction, I send five USDC to Santee. So obviously at the end of these three, I should have 85 USDC in my account. So when parallel execution runs, the first transaction generates a pending result which has an input for that slot of 100 and an output of 90. Because my USDC went from 100, it became 90. The second transaction, let's not worry about that. The third transaction has an input of 100 and an output of 95. Because these three transactions were run in parallel, so they didn't really know about each other.
00:33:59.560 - 00:34:22.574, Speaker B: Does that make sense? Yep. Cool. Okay, so then we go and try to commit these three pending results. We just step through them one by one. So first we see the first result, which again has an input of 100 and output of 90, we commit that one, that's fine. Then we get to the second one, that one is fine, commit that. Then we get to the third one, and we inspect the inputs, and we check to make sure they're valid.
00:34:22.574 - 00:35:20.436, Speaker B: So we see the input on this third pending result is 100, but now that slot has become 90. So that pending result is now invalidated, and we're going to need to go re execute it. So when we go re execute it, it's re executed with an input of 90, and then the output becomes 85. I said before that the re execution is not a big deal because that slot is already in cache. The reason why that's important is because reading that slot from SSD is going to be the cost of an SSD lookup, which is 40 to 100 microseconds might not seem like a lot, but when you're operating that scale, that actually does matter, versus if that data is in cache, it's less than a microsecond, like 100 nanoseconds or so. Basically, that re execution is really fast. And I guess the other thing I would say is that there's actually another way of thinking about parallel execution that I think gives a good mental model for what's happening.
00:35:20.436 - 00:36:14.954, Speaker B: And that is that the first stage of parallel execution, which is running all these transactions in parallel, is basically surfacing dependencies for all the transactions and pulling them off of SSD into cash. Right. And because of MonadDB, all those dependencies can be surfaced and read from SSD very efficiently in parallel. So that first step is doing this like 4100 microsecond per that kind of cost really fast, pulling all that into cash, and then it is generating pending results, which are like, this is what happened when we ran that execution. But it's almost like the pending results don't even matter that much, because then when we step through those transactions again, if the pending result didn't, the inputs didn't change. We can immediately commit, but even if we need to re execute, that re execution is much, much, much more, much cheaper because the inputs are in cache.
00:36:15.114 - 00:36:20.694, Speaker C: Yeah. Is there a capacity constraint to how much you can cache per block?
00:36:21.314 - 00:36:36.714, Speaker B: Not in practice, because with monad, the hardware requirement is 32 gigs of ram, which is still a huge amount of memory. So there's not, in practice, that's not really a constraint. Got it.
00:36:37.334 - 00:36:56.954, Speaker C: And in terms of, I'd like to maybe transition a bit into the actual cost for these transactions and comparing it to other particularly Solana which is really optimizing for highest throughput and currently really low transaction fees.
00:36:59.294 - 00:37:35.924, Speaker B: Right? Yeah. I think that you could think of Monad as similar to Solana in terms of overall transaction costs. So fractions of a cent per transaction. Of course, the cost depends on the complexity of the transaction. But for a reasonable depositing into AAVE or swapping on uniswap, those are really simple transactions. We expect those to cost a 10th of a cent on Monad. I would actually add that the actual cost is probably just driven by almost an arbitrary parameter in the code base.
00:37:35.924 - 00:38:33.364, Speaker B: Like in Ethereum, there's basically, let's see how to say this. In Ethereum, the cost is actually driven by supply, demand, and because there's more demand for block space than there is supply, so there's an auction, and the auction clears at whatever causes those two things to equilibrate, which is a very high number. And that's how you get uniswap swaps that cost five to $50, depending on the activity level on the chain. So in Monad, my guess is that on day one, there's going to be way more supply of block space than there is demand. So the clearing price on that auction should actually be zero. But of course it won't actually be zero because then that would allow anyone to spam for no cost. So it's really just a de minimis cost to prevent spam and still enable any kinds of applications to thrive without cost being a consideration.
00:38:34.504 - 00:38:34.888, Speaker C: Yeah.
00:38:34.936 - 00:39:13.122, Speaker D: Quick follow up question on me on this. I'm curious, do you guys have anything decided yet on what the fee market from there will look like? Because for context for Lister, this is basically where Solana started as well, because they have so much throughput. We'll just set a low fixed number, and as long as supply is over demand, they don't have a problem and just, it's fine. And now there's a ton of TPs. Now Solana started to think about, okay, sometimes you do get really, really hot spots, and so we do need a few mechanism to move. So just curious how you're thinking about that. Over time, a few mechanism is set to change the price and what that looks like.
00:39:13.298 - 00:40:08.884, Speaker B: Yeah, I think it's still a pretty early, pretty early ideation on our team side, but, but you know, we're happy to sort of follow the research space and just learn from others. I think that one point I want to make about this sort of like congestion based pricing is that it's not always obvious that an account like a slot that a piece of state that's in high demand should actually be more expensive. It might actually be that it should be cheaper, because if it's cheaper or if it's in high demand, that means it's definitively in cash already. So I think the fee design system should ultimately reflect whatever the real cost is. But the real cost might be higher for something that's in demand because it causes more rescheduling, but also might actually be better. And I think we just have to go based on whatever the measurements are.
00:40:10.264 - 00:40:34.334, Speaker C: Can you expand on that? I just want to make sure I understand it. If there's a particular thing that's really in demand, and you said it's in cash, are you clearing the cash after every block or are you preserving that? I mean, I assume, like, there's finality, right? And like, at what point do you clear the cache or do you not? Like, I don't know if the question makes sense.
00:40:34.674 - 00:41:11.444, Speaker B: Yeah, I mean, the cash is really more of a. It would vary from node to node on how frequently it gets overwritten, depending on the exact amount of memory that's available. But yeah, there's not clearing of the way the caches usually work is something gets pulled into cache and then it's least recently used cache. So eventually it'll fall out of cache if it's not been used for a very long time. But yeah, it's not cleared at the end of every block or anything like that.
00:41:13.384 - 00:41:51.120, Speaker A: All right, I mentioned them in the pre roll, now I'm going to bring them up again. It's arbitrum. Santi and I are really fed up with these high fees, and we're really excited to have teamed up with Arbitrum for the next couple of months on Empire. As the leading Ethereum scaling solution, Arbitrum now powers hundreds of decentralized apps across defi, perps, nfts, gaming, and a whole lot more. The team has showed us everything in the ecosystem, both now and what's to come, and we're really, really excited about it. Arbitrum allows both daily users and developers to interact with Ethereum at scale with low fees and faster transactions. The way the team got me excited was through portal arbitrum IO.
00:41:51.120 - 00:42:20.596, Speaker A: So my call to action to you is to get started by visiting portal arbitrum IO. Go experience on chain like it was meant to be. For a lot of empire listeners, your crypto is not just another number on the screen. It's part of your future. I know Santi and myself feel that way. Our security sponsor of this episode. Harpy takes this responsibility seriously and is the only wallet security tool that shields users from both on chain threats and sneaky off chain signature attacks.
00:42:20.596 - 00:43:05.968, Speaker A: If you've ever been in that situation where you're moving quickly, you approve something on chain, you realize that the address might be dubious address, or you're really hoping that you could take that back. Harpy has you covered. Harpy can redirect your assets to your self custody vault, ensuring they remain completely under your control, safe and sound. With Harpy's always on monitoring, you're not just detecting threats, you're actively blocking and recovering compromised assets from malicious transactions before they can even confirm. On chain, Harpy is the only wallet security solution that protected 100% of its users, users from attacks like the ledger one in Q four, which was an off chain signature attack. So if you're serious about protecting your crypto investments, it's time to make the switch. Secure your wallet for free at Harpy IO forward slash Empire.
00:43:05.968 - 00:43:11.832, Speaker A: That's Harpy. H a r p I e I o. Empire. If you want it to be even easier, just click the link in the.
00:43:11.848 - 00:43:14.856, Speaker B: Show notes one of the things that.
00:43:14.880 - 00:43:29.514, Speaker C: People are really critical of a system like Solana is a higher hardware requirements. What are those for Monad on a relative basis, if you think about Ethereum being like Raspberry PI, Solana being on the other end of the spectrum, where does Monad lie?
00:43:31.174 - 00:44:15.614, Speaker B: Monad is much closer to Ethereum. I think the most notable difference is that the Ethereum Ram requirement is 16 gigs of RAm, whereas for Monad it's 32. But that's still much, much less than Solana, which I think is 256 gigs right now. RAm is really expensive. RAm is 100 x more expensive than SSD. For example, you can get a high quality two terabyte SSD for about $200, whereas two terabytes of ram is like $20,000 or more with Monad. The reason why the hardware requirements are low is because of Monad DB, basically making the SSD much more usable, much more able to serve a very performant workload.
00:44:16.314 - 00:44:20.618, Speaker A: So it's because of Monad DB that this memory is actually so low?
00:44:20.706 - 00:44:53.154, Speaker B: Yeah, correct? Yeah, because with. Yeah, because when you're trying to have very performant execution, you run into the bottleneck of state access. And so the two kind of paths to alleviate that one would be to jack up the Ram requirements so that more state is in cash. Then the other way is by doing all of the work to build MonaDB to make that SSD, like the data that's living on SSD much more accessible.
00:44:53.734 - 00:45:06.984, Speaker A: And are you going to do what Solana did and make it so that validators don't have to know the full state, so that new validators don't have to replay and catch up? Basically. I think that's my understanding of how it. How it works, I think.
00:45:07.924 - 00:45:11.508, Speaker B: Yeah, you're talking about state sync. That's correct.
00:45:11.676 - 00:45:44.678, Speaker D: Okay, I have a follow up question on this. I'm curious, actually, too, and it's another one of the major differences between the solenoid implementation and the modern implementation. I'm curious to get your take on. So it was something that you were touching on before. Most of the bottlenecks kind of fundamentally here stem from the fact of, for Ethereum, like the way that we store the state. You have it in a Merkle tree and you stick it in this database. And today, in practice, most of those databases are actually just not built to be doing this super efficiently of looking up to the Merkle tree, changing it, et cetera.
00:45:44.678 - 00:46:28.800, Speaker D: So one solution to that is what Monad is doing. And I think some of the work on Avalanche firewood is similar to that as well. Let's just optimize the database and make it for looking up and changing the sparkle tree. Another solution to that, or approach to it is what Solana does, which is actually just completely different of just saying, all right, screw it, let's rip out the merkle tree, and we get rid of that, and we just don't keep merkelizing the state. And there are some very different trade offs of that, of it makes it more difficult to build, like clients and stuff like that. But it does also just avoid a lot of these bottlenecks around, working around the merkle tree and needing to build the database for it. I'm just curious how you guys think about those trade offs compared to what Monad's doing.
00:46:28.800 - 00:46:35.524, Speaker D: And what do you see as the trade offs in there? What do you think is the right or wrong approach? Or when does it make sense?
00:46:37.024 - 00:47:38.964, Speaker B: I'm a big fan of SSD's and a big fan of merkle trees. So I think that the merkle tree is actually a very useful data structure for creating a commitment to all of the state in a very abbreviated way. And that it enables a lot of other things, like roll ups where, you know, the roll up is posting the state Merkel route back to L1, or bridges where the bridge is communicating a Merkel route from one side to the other side. And then that's a commitment to all of the state there. I think it's a really powerful construct and I'm not really very eager to throw it away, especially given that we have this solution that gives the best of both worlds in terms of offering performant access and offering full merkle tree support. But I could understand why in the past maybe it might have made sense to avoid this amount of work to build this database to alleviate this bottleneck.
00:47:39.504 - 00:47:57.504, Speaker C: To me it sounds like perhaps the biggest unlock here innovation is the Monad DB. Do you see a world where other teams, even Ethereum l one moves from Pebble, I think you mentioned, to Monad DB and unlock a big improvement?
00:47:58.244 - 00:47:59.464, Speaker B: Yeah, I think it's possible.
00:48:00.004 - 00:48:39.274, Speaker D: The other thing that Ethereum definitely does want to move to, and this is kind of, I would say almost like the third approach, instead of optimize for the merkle tree, get rid of the merkle tree, or just use a different type of state tree. And this is kind of, I haven't looked at closely, but I believe the approach that mega ETH is going, and also to some extent, what Ethereum plans to do in the future, is just to swap at some point from Merkel trees to verkle trees as just a more efficient form of having the state and unlock statelessness as well. I'm curious how you view that trade off and what your plans might be for that over time, particularly if Ethereum does decide to progress and go towards verkle trees.
00:48:40.134 - 00:50:05.110, Speaker B: Yeah, I think that merkle trees are actually quite a nice data structure because they can actually be updated very efficiently. If done with a good algorithm, you can update large parts of the merkle tree in parallel. I actually think that verkle trees are going in the wrong direction from a performance perspective. Um, so if, yeah, I think that I understand the, the goal, which is to sort of make it so that validators can be stateless, so that basically when nodes are validating the network in Ethereum, they could vote without actually having access to all the state. And then they would just ask other nodes for vertical proofs of the balances of various staking accounts and so on, so that they can have all the information that they need in order to vote. To me that feels like it's very much going in the direction of being able to support nodes that can validate while having very minimal requirements. So it's going down the, I can understand the vision of right now there's about 10,000 nodes in Ethereum that are participating in consensus.
00:50:05.110 - 00:51:07.608, Speaker B: And if you want to go from 10,000 to 10,0000 or a million or something, then maybe it's the case that node 200,000 to node a million are not going to be able to store all the state so that they can participate as a validator. I think that it is still very much, it's a huge change to the operation of the node, though, just to support this like 200,000th validator. At a time when Ethereum is also talking about how there's sort of quote unquote too many validators. Now they're not saying there are too many nodes, but they're saying there's too many validators. And there's like a lot of open discussions going on about just like how the huge number of validators actually creates other overhead on the system. Like it means that the signatures become huge because there's a million signatures and like things like that. So I guess I'm not sure that like it's, or in the very least like it's branchy.
00:51:07.608 - 00:52:14.062, Speaker B: There's basically a branch that's happening where one camp is saying like, yeah, we want to be able to support the 200,000th validator who doesn't have to have all the state, and we'll go through all this stuff to like introduce verkle trees so that the witness size for these other nodes being able to request witnesses to attest to these balances of these other validator accounts, it's a whole lot of work to do that. Then the other camp, which I would say the Monad team is much more in, is how do we make fundamentally the execution of the internals of Ethereum much, much, much more efficient so that we can raise the throughput of the system and, you know, still with like a very decentralized node set and very reasonable hardware requirements. So on, get way more performance out of the system. But you know, inherently that does mean making decisions that, that do prioritize performant execution. And I would say merkle trees are much closer to be able to being able to support performant execution. So therefore they're like pretty incompatible with the idea of like the verkle tree in that direction.
00:52:14.198 - 00:52:23.234, Speaker A: Keone, what do you think of the recent idea to raise the gas limit on Ethereum? I think it was from a couple of months ago. Vitalik commented on it as well.
00:52:23.534 - 00:52:36.494, Speaker B: Yeah, I think it's a good idea. I think that it would alleviate to a certain extent, some of the costs, the extreme nature of some of the costs. And I think this is just a personal opinion.
00:52:36.534 - 00:52:36.734, Speaker D: Right?
00:52:36.774 - 00:53:25.326, Speaker B: Like everyone, literally every single person in the Ethereum community has different opinion about it. But for me right now, it's true that Ethereum block times are 12 seconds, but the execution budget is a gas limit which is chosen to correspond to 100 milliseconds or so worth of execution time, which is only 1% of the total block time. That's crazy because it means that there's this shrinking factor of 100 x of execution throughput because of that gas limit that's being imposed. And for a twelve second block time, you could definitely imagine that budget going from 100 milliseconds to 200 milliseconds without really having a significant effect on the overall stability of the network.
00:53:25.470 - 00:54:07.614, Speaker A: Yeah, I'm going to pull us out of the weeds for a second here. There's this inherent friction in l one s and honestly, outside of crypto in general, just in protocols, um, which you, which is that one day you start a protocol and you innovate on it and you do everything differently and you figure out all these optimizations and then if it gets big enough, eventually there's actually pressure, there becomes pressure to not change things and to basically you need to ossify the system. And as that happens, you have to like slow down the rate of innovation. And I just be curious about like how you think about that as it relates to Monad, when that will happen. Like what, how do, yeah, how do you think about that in general?
00:54:08.274 - 00:54:40.044, Speaker B: Yeah, it's a good question. I mean, it's obviously a trade off. Um, Solana is a good example because I think they do ship improvements pretty frequently. Um, but then that creates more risk of outages. And so like the recent outage, that was like a month ago or so. Um, you know, that was like a new feature that got added and then. So, yeah, Solana is very much on the extreme end of the spectrum of trying to ship many features as fast as possible.
00:54:40.044 - 00:55:38.624, Speaker B: And sometimes it leads to liveness failures and that contributes to negative sentiment about the blockchain and its ability to serve as financial rails for the rest of the world. So I would say it's like it's a trade off, but obviously there's a way to do a lot of both. But that just comes from more effort, like more rigorous testing, more testnet environments that are running in parallel to mainnet, that are a complete replica of how mainnet looks. I think on Solana Mainnet, the real TPS is like 1000 or so transactions per second, whereas the Devnet is like 70 transactions of usage per second. So there's still more that could be done to make that dev environment more similar to the production environment. But it just requires work. It requires cycles, human cycles.
00:55:38.624 - 00:56:06.324, Speaker B: And sometimes when you're trying to go really fast, you're devoting more to speed, to the engines at the back of the rocket ship versus the ones that are steering it and keeping it safe. I don't know. I mean, the only answer is just to work harder and to, but to be very rigorous and to have rigorous practices with respect to like, rollouts and rollbacks. There are some, I think some learnings there that are very concrete.
00:56:07.504 - 00:56:26.910, Speaker A: Do you have a vision for the protocol? Like when we, we've had anatolian and he clearly loves Linux, I would say. And like he kind of looks at Linux and Linus as this kind of model for how to maintain it and how to be a founder in this open protocol. Do you have that with Monad?
00:56:27.062 - 00:56:58.034, Speaker C: You constantly hear him say, I want to bring Nasdaq on chain. This idea of single shared state, really valuable, doesn't include perhaps all the use cases, but that's something that he's very focused on. As you think about the mind share that you want to capture the type of use case or builder application that is perhaps most well suited for Monad. Is there something in particular that like you would impress on the builder or the user that wants to come to Monad?
00:56:59.174 - 00:58:07.224, Speaker B: I'm really focused on supporting developers that aspire to build things that have mass user adoption. So in my mind, I always have this idea of like people playing runescape and acquiring items and leveling up their character and earning achievements. And for a blockchain to be able to support that level of complexity of application, let alone that level of number of users, like literally millions of users, it's just going to require a much faster car than the solutions that we have right now. So I think that for any app to cross the chasm of adoption, probably for me the closest was stepn, because a lot of people just understood the premise. Go for a run record your steps, earn some achievements, have something to show off to your friends, earn some badges, Strava on the blockchain. That's something that can appeal to a lot of people, but you're going to need a lot of performance in order to support that.
00:58:08.304 - 00:58:26.644, Speaker A: John, where do you think Monad fits in, in this landscape of, like, we have l two s, we have Solana, we have these da folks, we have Celestia and eclipse now, and we've got Eigen da and all this kind of stuff. Like where does Monad fit in? And like your mental model of the crypto landscape.
00:58:28.144 - 00:59:44.104, Speaker D: The interesting part for me that I'm curious to see is kind of like, what ends up being the community and the kind of users of monad in particular. Like, how much overlap kind of is there with the Ethereum community versus how different is it? Like, that is something that, I mean, like, Solana was on the far end of, which seemed like a trade off at first, but I think ended up benefiting them. Clearly, in hindsight, is that you had a completely separate and completely unique community of, like, these are just the people who go build Solana and the SVM. And like, that's really difficult. But if you do make it through to the other side, like, okay, you have like, a unique community that like, really sticks it through and like keeps building. Um, Monad is in like somewhat of a middle ground of like, there is a meaningful amount of clearly, like, kind of bootstrapping off of like using the EVM's network effects and everyone is building around that, et cetera, while at the same time doesn't fit as cleanly into the mental model and architecture of someone who is like, entirely obviously relying on the Ethereum community of, like, we're just an Ethereum roll up that uses for dA, like we have ETH is gas, optimism, that kind of thing. We are very much built on Ethereum, and 100% this is the community that we want to reuse and build on top of.
00:59:44.104 - 01:00:48.036, Speaker D: Moneta isn't a middle ground where they clearly have done a great job. And this is probably the thing that I underestimated the most in the last year, actually, of having a quite unique community. Because my reaction, candidly, when I first heard of Monad, whatever it was like a year ago or so at this point, was just like, oh, it's another alt l one evm. Aren't we past that? Are people going to get interested in this thing? Even if the team's great, the tech's great and all of that? That sounds really hard to get people to care about that again, and was completely wrong about that. They've done an amazing job of that over the past year of getting a community of people who care about this thing. And I think that that is going to be kind of their place, if they're successful, is that they are able to get a genuinely unique community that really cares about Monad and isn't just, hey, we are recycling everyone that's in ethereum and just putting them on another chain that's faster and cheaper and does the same thing. It's going to be do we actually just get people who aren't just obviously copying uniswap and just ok, we go put it on Monad now it's a little bit cheaper and the users move over.
01:00:48.036 - 01:01:13.844, Speaker D: It's like, no, we have teams that go build on Monad from the start and are just like, we build here, this is our community and get really into it. Um, if Monad's going to be really successful like that, like that's what they're going to have to do. Um, and thus far, I mean like obviously still early days chains not live all that but like have done a great job of that. Um, and that, that's one of the things that I think Monad is understood like very well over the past year is like they need to do that for them to be successful and still like genuinely have like a unique community that like cares about and like builds there with Monad in mind.
01:01:14.144 - 01:01:27.104, Speaker A: Keone, how much of um, of uh, building an l one has been about community and like, or I guess maybe differently phrased, like how much of an l one do you think is about the tech stack versus just the social layer?
01:01:32.884 - 01:02:04.942, Speaker B: I think the tech stack can enable usages that maybe are unique and that the tech stack is kind of the backbone upon which the backbone of the beast, so to speak. But the social layer is like the biggest part. Probably 80%. I made that number up. That's obviously like how would you measure that even? But I think it's just measure by.
01:02:04.958 - 01:02:10.454, Speaker A: The number of people fighting to get into the monad. Monad private telegram chat.
01:02:10.614 - 01:03:13.430, Speaker B: Right? Yeah, yeah. There's certain KPI's. I mean, I think that for crypto overall, the thing that, that crypto has as a superpower, it really is community because like with any traditional tech startup, their Twitter will have like, you know, 100 followers and they'll put out a tweet and like two people will like it and it's, you know, it's like someone on the team and then their mom who liked the tweet. So I think there is actually like this incredible superpower from having all these people that care about the product and are eager to try it and eager to give feedback and eager to get involved. And there's a financial aspect for sure, but it also goes beyond that because people literally make really good friends with other people on the Internet just through their participation in community. And then the community is all built around a mission. I think that in this day and age, it's hard to meet new friends and it's hard to find things that you really believe in that are your mission.
01:03:13.430 - 01:04:22.264, Speaker B: And that's something that the crypto community overall has, is like, decentralization is really important mission, self sovereignty, really important mission. Owning your own data, owning your own assets and composability, like the idea of having open APIs where people can build an application and then someone else can just build another application that atomically utilizes that other app as a subcomponent, can build more powerful things by leveraging this composability aspect. These are all fundamental aspects of the technology that go beyond the pure tech and also are about philosophy and things that we value, things we believe in. I think that's the thing that makes me the most bullish about the space overall, is that, you know, like, we have the ability to change the world and then individual people who are just passionate, you know, like, if you're, it's similar to being like an apple superfan, but in crypto, like, you can actually change the outcome as an individual user. So I think that that's, that's incredibly powerful and that ultimately, for crypto to succeed, we have to really embrace that community aspect and use that as our superpower.
01:04:23.564 - 01:05:37.254, Speaker C: I'd say, like, not that my opinion matters much, but in a world where people have some fatigue of, like, the incremental cost to attract people is, I think, getting higher and higher as we have more l two s, as you have the aptos suis. And so there are people that are saying, look, do we need another l one? And I think we should always continue to explore the design space because we're nowhere near, I think, reaching a state where we can see billions and billions of users. But I think something that you said, which I definitely agree, I think it does start with really good tech at this point. Like, I would be hard pressed to think of a project that, like, doesn't have much tech and has, can attract community early on, like, if there's speculators for the airdrop, whatever, but sustain that community over time and attract the really good ecosystem of builders. I think that most teams, a lot of them, will probably fall short of the expectation. But if you have really good tech, I think medium to long term, you thrive and you win meaningful mind share. But that's probably going to take five years, another cycle or two to really.
01:05:37.294 - 01:05:38.074, Speaker A: Play out.
01:05:39.974 - 01:05:48.224, Speaker C: Because you're seeing right now projects that the airdrop craziness and is that community? No, not really.
01:05:48.524 - 01:06:22.264, Speaker B: Right, right. Yeah, I think that the thing that our team has done really well in terms of cultivating community and it's really not my doing. It's literally just all members of our team who have really stepped up and then individual community members who have really stepped up. It's kind of going against the grain in terms of not giving into whatever the easy KPI win is. So, like, I see a lot of people making the mistake projects making the mistake of, like, oh, great. Like, I have this many people in my discord. That's awesome.
01:06:22.264 - 01:06:47.174, Speaker B: Oh, there's more people joining. Like, even more awesome. Like, even more people chatting, volume of messages sent. Like, this is good. That's actually bad because it actually kills community. Because if you have a bunch of bots that are just chatting, then the people who actually know each other who are trying to hang out, you know, the bots totally, like, kill the vibe, and it's, like, almost. Anyway, that's the thing that's really unique about the Monad community is that, like, if.
01:06:47.174 - 01:07:01.114, Speaker B: If our team did any steering, I would say it was just to, like, be anti bot and anti low quality, like, contribution and just, you know, like, make it a place that's actually enjoyable to hang out in and enjoyable to hang out with your friends in.
01:07:01.494 - 01:07:33.240, Speaker A: John, what is your working model of. I want to ask you guys both this question, um, of basically what happens? How. How much of the world moves on chain? Cause I guess I'd say, like, okay, there's Polonia. If you guys have, like, read Polonia's blog, and that's basically, like, it's only going to be money and identity. And then there's the other end of the spectrum, which is, like, the crypto will eat the entire world. Everything will, every single thing and application will eventually move on chain. And you're seeing this, like, interesting thing play out with forecaster right now where, like, some stuff is on chain, some stuff is off chain.
01:07:33.240 - 01:07:41.794, Speaker A: I think it's making people maybe rethink what does go on chain, what doesn't. So I'd love to. I'll ask the same question to both you guys, but, like, what's your working model of this? And, John, I'll throw it to you first.
01:07:43.014 - 01:08:11.026, Speaker D: I have almost no idea. And I think pretty much everyone has almost no idea. Yeah, I think. And this is part of the thing that also makes everyone arguing over these architectures fun, is because I think you can make, like, a very coherent argument that, like, 99.9% of the important things in crypto will happen within, like, 100,000 tps. And I think you can make an equally compelling argument that it will take, like, 100 billion tps. To get like 99.9%
01:08:11.026 - 01:09:10.140, Speaker D: of value. And there's two just like radically different outcomes of like what is the type of stuff that you think we need to put on chain? And like what is the order of magnitude of like actual transaction count for that? I genuinely don't think that there's like any way you can reasonably come up with a correct answer within like orders of magnitude. Particularly because I mean, like the like, limited view is like, okay, only the things that touch money is the like small view of crypto. Like, only things that touch financial applications in some form is like an incredible amount of things that we probably don't even like, think about on a daily basis. Things that touched even to a minimal degree, and anything that touches identity and all these different things. So my general view is that we need to kind of build for, and the likely outcome is that it is just like an amount of compute and transactions that we can't honestly reasonably intuit about as a person of. We need to build for a million TPS.
01:09:10.140 - 01:09:51.617, Speaker D: I think you just need to build for, effectively infinity at a certain point is what you need to start thinking of the whole system as that is the direction we'll move to over time. And it'll just be much more background of. You're just not aware for so many of these things that everything is on chain. And the way that for something like forecaster of, it's not like every single button that you click is an on chain transaction, but there's a minimal amount of infrastructure that just makes sense to have for these things on chain in the background. And I think more of those things will start to make sense over time. And once it starts to be okay, there's just this background infrastructure for so many different use cases. This is an amount of transactions, compute, that is, you can't brace it about.
01:09:51.617 - 01:10:13.604, Speaker D: It's a lot of transactions. There's going to be a ton. I think the question is almost somewhat of how present to the user is that going to be. And I think a lot of it will probably be abstracted away from stuff like Farcaster. You don't need to know that stuff is on chain when you use Farcaster, for the most part, it's just like, it's just a social app. I think more stuff probably moves in that direction. And same with financial applications today.
01:10:13.604 - 01:10:25.704, Speaker D: It's clearly very present. But I do think that we move in that direction of over time of it's an obscene amount of transactions that can't really reason about. That means you're going to need a lot of chains, and a lot of it will be more background stuff over longer term.
01:10:26.164 - 01:10:27.624, Speaker A: Keone, what do you think?
01:10:30.484 - 01:11:25.594, Speaker B: I think that it's kind of like how a mollusk just grows into whatever the size shell that it has. So if we have an environment where we have capacity for several billion transactions per day, then we can actually enable apps that, that get to using tens of millions of transactions. And it's not a design consideration. But up to now, a lot of the design is really predicated on the assumption that transactions are very, very expensive. Storage is very, very expensive. Committing anything to this shared global database thing, it's really expensive. And so we have to be very, very stingy about what we commit, and then things will just evolve to take advantage of the space.
01:11:25.594 - 01:11:37.374, Speaker B: But of course, the first thing, like John said, is just to lay the foundation to make it really cheap, make transactions really plentiful, and then see what can come from that.
01:11:40.874 - 01:12:09.034, Speaker A: Nice. I have maybe one or two business model questions, actually, and then we can start to think about wrapping it up, unless Santi and John have anything else they want to talk about or ask you. KeOne but I guess my first one is like, blockchains are this weird blend of like, where you have a weird blend of two customers. Keoni, you have like the user as your customer in a sense, and then the developer is your customer who like, when you close your eyes and think about your customer of Monad, who is your customer?
01:12:11.814 - 01:13:31.824, Speaker B: So the ultimate customer is really the users, because the users are willing to pay some value for services. They're paying perhaps the money in the form of transaction fees or application specific fees or slippage or what have you, if they're trading slippage. But ultimately that's the ultimate end customer. But of course, as a platform, our job is to support developers who are then going and providing valuable services to those users. I guess in my head, I have this idea of you're building a mall and you want to the revenue of the mall, or basically the revenue of the mall is predicated on the revenue of the individual stores inside of it, which is ultimately driven by the value, like the customers, how much the customers are willing to spend when they come into the stores. But you could make the mall much, you can make it much easier to open a store in the mall, or you could make the roads going into the mall much better so that it's easier to bring more inventory in. Or you could have buses that bring users in.
01:13:31.824 - 01:13:39.930, Speaker B: So there's a lot of still things along the side that that one can do to help grow the businesses that are inside of the mall.
01:13:40.122 - 01:13:56.734, Speaker A: I've actually never thought about the mall analogy. I've always thought about almost cloud, where AWS would say that the end user organization is their customer, and then that end customer will basically market to the users, but it's not AWS's role.
01:13:58.674 - 01:14:52.138, Speaker B: Yeah, the other thing I want to mention is that in Ethereum users, like the end users, end up paying about $3 billion per year in fees just to the Ethereum network. And then some of those fees get distributed to validators, some of them get burned, so then contribute to deflation. But overall, it's like a $3 billion a year business, which is massive, but it's supporting about 300 million transactions per year. So people are paying on average $10 per transaction. So it's very catered toward, like, you know, usages where one transaction is worth more than $10 to that user so that they're willing to pay that $10 fee. So. But, you know, our thesis is that there's many more users who would be willing to pay some fee, but probably not $10.
01:14:52.138 - 01:15:09.074, Speaker B: Maybe they'd be willing to pay like $0.01 because they're getting like $0.10 worth of utility to do this transaction, so they're willing to pay $0.01. There's just this whole larger space of users that are currently not able to use applications right now. And so you couldn't build a business that then caters to those users.
01:15:11.254 - 01:15:27.124, Speaker C: Would you agree that perhaps where we'll see the most amount of innovation and optimizations going forward are on dynamic pricing and fee markets, less so on hardware, like other optimizations.
01:15:28.984 - 01:15:32.208, Speaker B: You'Re saying after Monad or just.
01:15:32.376 - 01:16:09.594, Speaker C: Yeah, just after Monad. In general, it feels like one of the more unexplored or things that we perhaps may not know today is because we don't have as many users, and when we have more users and hotspots. John, you made this great point. Solana's now figuring this out, which is they need to make some modifications, but it's one of those things you don't really know until you have 100,000 users. A million, a billion, 10 billion. I feel like that's more of my point, where we're going to have to probably reserve some pretty large wiggle room margin to make some changes on how fees are set.
01:16:11.174 - 01:17:43.456, Speaker B: I see, yeah, I think from the Monad engineering team's perspective, we feel like even beyond the initial launch of Monad, there's still a lot of other improvements to be made that are in the realm of either performance optimizations or additional features like support for new precompiles or opcodes that allow developers to build more powerful applications. So it's not, there's still a whole realm of engineering things that are not necessarily fee related. I think from the application perspective, I would expect to see applications that have more novel mechanisms of capturing more value. The extreme example is Uniswap, where it just seems from the teams public comms that there is frustration about the fact that there's a ton of users coming to Uniswap paying a lot in fees back to Ethereum directly, paying a lot of fees back to Ethereum indirectly in the form of MeV, like a lot of slippage, which then creates an MEV opportunity where then the searcher then goes and captures the value, but then pays most of it back to Ethereum. From the app developer. Like Uniswap's perspective, they would want to be able to capture more of that value that's currently all flowing to validators or to the ethereum network. But that's only possible if like, the baseline fee is much, much lower and the interaction can be much more complex in terms of like, frequency of updates by market makers.
01:17:43.456 - 01:18:05.954, Speaker B: So there's like a whole like technological improvement that needs to be made in order to enable a business model where then maybe Uniswap could capture more value either for their lp's or maybe for their token holders or maybe even for uniswap labs, like, who knows? But right now, some of which is so much is just being paid out baseline because of the way that the technology works?
01:18:07.534 - 01:18:28.314, Speaker A: Yeah Kiahn, is there one other ecosystem that you've studied that you think, whether it's like Sui or Aptos or avalanche or Solana, that you think has done a phenomenal job, or maybe I'd even push you one step further. If you weren't building Monad, is there one ecosystem that you would go build in?
01:18:33.174 - 01:19:03.230, Speaker B: Probably Solana. Honestly, I guess that maybe that's what you would have guessed. But just the overall ecosystems focus on performance I think aligns well with what I really value. I think there's still a lot of low hanging fruit for optimization in the Solana space as well. That's how you end up with a project like Firedancer, like nothing is, nothing is mature. Like there's still opportunities to add a lot of value in a lot of different places.
01:19:03.382 - 01:19:13.994, Speaker A: Yeah. John Keone. It's been great, guys. Any, any closing thoughts, final things that we missed that you feel like we should discuss, or should we call it here?
01:19:14.294 - 01:19:16.954, Speaker D: We hit most of it from my side. This is a ton of fun.
01:19:17.414 - 01:19:19.446, Speaker B: Yeah. Good here. Thanks so much.
01:19:19.590 - 01:19:24.866, Speaker A: Cool. Thank you, guys. Keoni, John. Appreciate it. Santi, as always. Be well, sir. Thanks, everyone.
01:19:24.930 - 01:19:25.530, Speaker B: Thank you, guys.
01:19:25.642 - 01:19:27.214, Speaker C: Really good discussion. Thank you.
01:19:27.834 - 01:19:54.668, Speaker A: Hey, everyone. Jason here. Thank you so much for watching today's episode. Wanted to take a quick second to thank today's title sponsor, Arbitrum. We know you are tired of on chain experiences that have unaffordable fees and frustrating transaction speeds, and that's why we partnered with Arbitrum. You can experience frictionless trades, lightning speed, and lag free transactions, all for pennies per transaction. Explore arbitrum's expanding ecosystem at portal arbitrum IO.
01:19:54.668 - 01:19:58.084, Speaker A: That's portal arbitrum IO. See you for the next episode.
