00:00:03.110 - 00:00:33.620, Speaker A: Good. So I think it's time for our second guest. So, yeah, we have Alex Buhaski from Zkynx, the CEO. I'm especially happy to have him as a guest today because Zksync is probably the highest profile, zero knowledge roll up today. Definitely, definitely top of the game. And, yeah, without further ado, Alex, thank you for joining us today.
00:00:34.550 - 00:00:35.860, Speaker B: Thank you, Mark.
00:00:38.250 - 00:00:47.480, Speaker A: Yeah, I'm not sure. Everything all right with the sound? Let me check with our team. Alex, can you say something?
00:00:48.090 - 00:00:48.840, Speaker B: Yes.
00:00:52.590 - 00:01:02.540, Speaker A: I have a little bit of. A little bit. Sound is a little bit weird. Yeah. Can you try to reconnect maybe just in case?
00:01:02.930 - 00:01:03.680, Speaker B: Yeah.
00:01:06.050 - 00:01:25.970, Speaker A: Let'S give it a moment. Let's make sure the sound is fine. So it's going to be complex topics. So it's good to hear everything. Right. I get a confirmation from the backend team that there was a problem with the sound. How about now, Alex?
00:01:26.310 - 00:01:29.286, Speaker B: Hello? Is it better now? Can you hear me?
00:01:29.468 - 00:01:30.230, Speaker A: Much better.
00:01:30.300 - 00:02:32.502, Speaker B: Okay, great. Thank you, Marek, for inviting me. And thank you, Bartek, for really well structured explanation and introduction into roll ups and escalating problem overall. And I actually wanted to talk a bit about this and compare different solutions. But you did such a great job that I don't have to do this anymore. I will rather augment the arguments and just make a couple of extensions and then talk about Zksync and how we want to achieve fully generic, composable smart contracts which are AVM compatible and Zksync. So the couple of things which I wanted to add throughout the session were I also believe that long term ZK roll ups will be the end game for scaling because of the properties that make them actually as secure as the underlying layer one.
00:02:32.502 - 00:03:22.482, Speaker B: So this is a unique solution in this regard. This is why we are all very excited. But I also think that in the near term, there are a couple of use cases which cannot be handled by optimistic roll ups. And some projects are better off waiting a little longer if there is a delay in release and jumping right on their ZQ roll ups. One of these cases is NFTs, because the withdrawals with NFTs cannot be mitigated with the techniques Bartek mentioned, which I'm, by the way, really excited with Maker and the diamond minting stuff which was just announced on Twitter. Looking forward to see more details about it. But with nfts, there is no way to accelerate withdrawal with some liquidity providers because all the nfts are unique.
00:03:22.482 - 00:04:22.890, Speaker B: So you really have to wait two weeks and not only to withdraw to layer one, but also to withdraw to any other l two solution if you want to move them around. So that's one case. The second thing where I'm really excited about ZK rollups compared to optimistic rollups is composability. What I mean by this is, on Ethereum, DeFi is really successful because you can have a lot of different protocols interacting with each other and increasing the value of each other's offering. And of course, you can have the same thing in all types of roll ups, in optimistic roll ups, in ZK roll ups. But with optimistic roll ups, the more value you lock in a single instance of an optimistic roll up, the more risks you expose your funds to. So if you have a lot of different protocols and billions of dollars of value, you will potentially invite some troubles.
00:04:22.890 - 00:05:42.054, Speaker B: Because what can happen is that the optimistic roll up will be attacked by the miners, not by the validators of the roll up itself. So what miners can do is to run a censorship attack, which we have not observed yet, ever on layer one. But we also never had an incentive for this. The way it will work is there will be a fraudulent transaction, and then all of the validators of an optimistic roll up will rush to submit a fraud proof, but the miners will just censor them, and they can be coordinated in a completely trusted way. So imagine an attacker comes along and says, I'm renting out 51% of the hash power somewhere on cloud services with a lot of processing power, maybe some gpus, maybe some fpgas, and I'm going to overthrow all blocks that include the fraud proof transactions to this particular optimistic roll up. I'm not going to do anything bad about any other transaction affecting any other account. So all other transactions will be fine.
00:05:42.054 - 00:06:23.970, Speaker B: I will include them and all miners that include those normal transactions, I will build on top of their blocks. Only if other miners include the fraud proof transaction, then I will revert their blocks. And as a miner, you face a dilemma. Either you become activist and you try to include the fraud proof transaction in your block, and you know it is going to be reverted. Or maybe you try to organize with other miners, but if you don't have 51% hash power, this might be problematic. You just might not be able to do this in the near term. Or you just comply.
00:06:23.970 - 00:07:41.260, Speaker B: And as a miner on proof of work system, the compliance with the attacker can be completely anonymous, because you just withdraw your hash power from the mining pool, which is legit, and you put it to the mining pool, which supports the attack. And the cost of this attack is actually pretty low. It's in the range of couple of like $50 to $100 million right now. So it's sufficient for the attacker to have this amount of money shown on some smart contract as a plausible commitment to the attack, so that they actually will perform it for one week, and then all the miners will just have to comply. So ZK roll ups are completely immune to this kind of problem. And NFTs attack with 51% censorship and yeah, generally much shorter confirmations. And the ability to actually verify every transaction by all of the full nodes of Ethereum, which we have currently around 10,000, is really exciting because this is a true scaling solution, as Patek said.
00:07:41.260 - 00:08:30.902, Speaker B: Okay, ZK sync. We started two years ago to work on ZK roll ups as a research project. We launched last summer. The first version, which does payments. Our payments have been integrated in a bunch of protocols in different wallets, and Gitcoin grants campaigns in golem payouts, which are going to actually start live tomorrow on the main net. But we started with payments because it was the most basic layer on which we wanted to build the generic functionality of smart contracts. We did not want to go into any specific protocol like building an AmM or building some exchange, because we don't want to compete with projects that will be building on top of Zksync.
00:08:30.902 - 00:09:06.520, Speaker B: We want to remain a neutral platform which will be turned into a decentralized protocol. Right now it's operated by a single sequencer, which does not have any impact on the security, because security is guaranteed purely by smart contracts and zero knowledge proofs. But it's not good from decentralization standpoint. And we're going to introduce multi validator consensus. And we've been working on bringing smart contracts to Ziki sync. And I'm going to talk a bit more about technology so that you understand what are the challenges and how we're solving them. I think it's pretty interesting.
00:09:06.520 - 00:10:23.760, Speaker B: So the end goal for Zksync is to support smart contracts which are EVM compatible and have all the properties of contracts on Ethereum. So if you have some code written in solidity and you have some dapps with some APIs that interact with this code, ideally you should not need to do any changes, should be able to just take this code, deploy it on zksync, and use it completely the same way you use it on Ethereum, and also interact with other contracts. And we believe that we are very close to this goal. We will actually show something in the near term. But we started with a slightly different approach, a year ago, we worked on a computing programming language called Zinc, which was a non Turing complete programming language as opposed to the Turing complete solidity. And the reason for this was that to make something zero knowledge proof friendly, you need to build a zero knowledge circuit. So you need to build a special function which you can prove under some zero knowledge proof system.
00:10:23.760 - 00:11:46.410, Speaker B: So, to recap what is a zero knowledge proof, you can have some function, f, that takes some arguments, X-Y-Z and produces some output, let's say r. And you can have a prover, one party, which tries to convince the other party, the verifier, that they have computed the result of this function, and the evaluation of this result is r, and the computation is correct without actually disclosing what x, y and z are. And this process is called the proving. You produce a small cryptographic proof, maybe a couple of kilobytes, size of data, which the verifier can take, apply some simple mathematical operations on it, and get a result of true or false, whether the proof is correct or not. So, interesting thing is, you can omit x, y and z, or you can omit just y and z. So this is going to be hidden. X is going to be known to both parties, and the function itself and the result also are going to be known to both parties.
00:11:46.410 - 00:12:54.400, Speaker B: So in place of this function, we can have something really simple, like a hash function. So I can prove to you that I know such x, that, let's say shah, two, five, six of the x equals some value. And if I produce the proof, you can check it, you will be sure that I actually know x. I could not computationally fake it. But we can do something a lot more complex. We can put a state transition function in place of this f, which will prove that we actually took a number of transactions, applied them to the previous state root, which is known. So we will have a state transition function, which takes r, the root hash of the blockchain, or of our roll up, which takes a set of transactions, and the new root hash, which will be the result of the application of all these transactions to our state.
00:12:54.400 - 00:13:35.910, Speaker B: And it produces the result of true. If this state transition is valid, and we can prove it. The problem is, these functions must be representable in the form of arithmetic circuit. What that means is they must be expressed in a sequence of simple mathematical operations, such as plus and multiply. And you can't really have loops there. It has to be one single function, which you can write in single line, so they are fixed size. You can represent complex algorithms.
00:13:35.910 - 00:14:43.806, Speaker B: There are ways to break them down. If you have loops of some fixed size, you can break it down in arithmetic operations and just write a long list of these operations and it will represent your circuit. And this is actually what we do, for example in case of shuttle 56. So for a proof system which is r one cs based, r one cs is a language to describe the proof systems. You take something like 25,000 constraints, so 25,000 linear equations to represent this single function. And if you have something more complex, like a state transition of a roll up, which makes just even simple payments, not even trades or contracts, then it will be like many hundreds or thousands of applications of some hash functions, some more complex logic, right? But it has to be fixed. And to make it fixed, the only option we had was to make a language which describes the program which is not variable.
00:14:43.806 - 00:15:12.346, Speaker B: So the program has to be of the constant length. The execution trace of the program had to be of the constant length. So we wrote this language called zyng. And it's interesting that for example the Viper language which is used by curve also has this property. It's also a non Turing compute language. So it was possible to take any program written in viper and translate it to zinc almost one by one, like line by line. You just rewrite it.
00:15:12.346 - 00:15:53.770, Speaker B: And then we built the compiler. This compiler would produce the virtual machine which being executed would produce these constraints, and then also the proof for a given set of data. So we did the demo together with curve, we demonstrated how this works. We built a testnet. It's actually possible to take programs, write them in zinc now and deploy this testnet. But then we discovered a very interesting way how we can do the contracts which are turing complete. And this opened the way for solidity to be brought to Zksync.
00:15:53.770 - 00:16:56.798, Speaker B: And the key component which was missing, which allowed us to finally do this, was recursion. So we use a proof system called plonk, which since early last year had the support of recursion. We implemented it on zksync. It's now live in production, which makes the cost of transactions on Zksync are the lowest among all existing rollups. And the way it works is you can create a zero knowledge proof that doesn't prove some functions, but instead proves that it verified some other zero knowledge proofs. For example you could take like this would be the proof of a single block of transactions which contains some fixed number of transactions, let's say maybe 500 or 1000 or something. And then you can have as many of them as you want.
00:16:56.798 - 00:18:02.260, Speaker B: And you can just recursively put them in a tree of proofs. So the bottom line will be our basic blocks, and then this line would be the proofs of the basic blocks combined together by many in the same block. And then you can have maybe some more layers if you need. And eventually you will have a single top level proof which verifies all of the underlying blocks and all of the underlying transactions, which is recursively converging here. So you can use it to create blocks of arbitrary size, even though a single block is of a fixed size. And this means the verification of zero knowledge proof of the single proof will cost some constant cost on layer one, which is around 500,000 gas for plonk, but it will be amortized among many thousands of the transactions. So the cost per transaction is going to be very low.
00:18:02.260 - 00:19:32.142, Speaker B: So how do we apply this to smart contracts? Specifically to solidity smart contracts. So the interesting thing about solidity is generally. Yeah, how can you create zero knowledge proofs of Turing complete execution? So it would be hard, because we cannot have a single recursive block for every operation, because we have many thousands of operations in the stack. Trace of an execution of a single smart contract on EVM. But we can have a single proof of a big block which contains many execution steps at each step, and we would have a fixed size of these steps. And at each step we would access memory load an instruction from the next memory point according to the program counter, and we would execute a single instruction. And the way we would execute a single instruction is we would create like, on each step we would have a conditional select with multiple instructions, and we would just check against each of them.
00:19:32.142 - 00:20:17.790, Speaker B: Is it an add? No. Is it a sub? No. Is it a move? Yes. Okay, so execute move. Right. So this means that the circuit would be pretty large, because we would have like, if we have, let's say 30 instructions and 1000 steps, we would have to have number of constraints proportional to the product of these two numbers, which might be pretty large, especially if you have a very long execution trace. The problem with solidity is that certain instructions in EVM are a lot more expensive than other instructions, so they are not homogenic.
00:20:17.790 - 00:21:58.330, Speaker B: The cost profile is completely different in operations such as add, which adds two numbers, costs something like two or three gas or one gas, maybe even. But an operation of s store, which you use to update a storage slot in storage. So this is one gas and a store is 20,000 gas. Right? So if we designed this circuit to contain each of the cell or each of the slots here would be as large as the most expensive operation. It would be prohibitively expensive, like we would not be able to support it, because it would mean essentially that if your program has 1000 execution steps, then you would have to pay as much money as you need for 1000s stores, which is out of order. But what we can do with recursion is we can have these instructions very simple and very small, and just limited to the most basic operations, such as arithmetic operations, memory access, and some basic hashes. And whenever you have a more complex operation, such as s store or ketchup hash, or shutter five six, or signature verification, we would just accumulate the requests for those operations in some special registers.
00:21:58.330 - 00:23:14.934, Speaker B: We would maintain the list of these registers, and then we would pass them to separate circuits which are specialized just for those operations. So we would have a special circuit which only does storage access, and we would have some fixed number of s store operations, which would be a lot less than normal execution trace. And then we would have a separate circuit just for ketchup hash, and just for shutter five, six, and just for signature and so on. And then we will recursively combine them together, just like we did over here. So instead of having simple blocks with all the same types of transactions, we would have simple blocks with these basic transactions, and a few specialized blocks with heavy operations, depending on how many of these heavy operations we actually need to use for a specific block of the roll up. So some blocks will contain a lot of storage accesses, but some will contain a lot of signature verification or hashes or something. So this is roughly how it works.
00:23:14.934 - 00:24:10.022, Speaker B: And we decided to go all in on solidity and completely shifted our focus to this. And we actually made huge progress since last couple of months. And we will have a public testnet probably within month, two, three months from now, maximum. And we're definitely launching this this year with EVM compatibility, full composability. So it will be possible to call one contract from the other and have a sequence of calls in a single atomic transaction. We'll have events, we will have all the things that you know from solidity, except maybe for some edge cases, such as some very complex cryptographic precompiles. Maybe we won't support them from the first version, but the rest we actually intend to support.
00:24:10.022 - 00:25:41.480, Speaker B: And it's going to work just the same way it's working on Ethereum, and we're super excited about this. So that's going to be, I'm not sure how deeply technical or not it was. I'm going to take questions, and I'm sure there's going to be a lot of interesting questions to discuss. Hello. So I'm not sure why I'm still live or Mark and Bartek are here. It. Yeah, I can also read the chat, so maybe we.
00:25:41.480 - 00:26:19.270, Speaker B: Yeah, it yeah, I know I'm live, so like. Yeah, would be great to have some questions here. So there's some problem with the video, but we will just post questions in chat. I will read them and try to give some answers. So what is the plong proof system you mentioned? It's called plong. Written like this.
00:26:21.480 - 00:26:22.710, Speaker A: And we're back.
00:26:29.880 - 00:27:10.710, Speaker B: Plunk is a proof system which uses polynomial commitments. It's an interesting hybrid system between snarks and starks. So it's actually a snark. It's a succinct, non interactive argument of knowledge. And it just uses a similar arithmetization technique as starks. So you can do very efficient specialized circuits, you can do hashes or some specific arithmetics on different curves in a very efficient way. So I just encourage you to Google plonk and you will find more information.
00:27:15.180 - 00:27:17.912, Speaker A: Awesome. We have another question coming in a second.
00:27:17.966 - 00:27:18.570, Speaker B: Now.
00:27:21.900 - 00:27:23.530, Speaker A: Let'S see. What's that?
00:27:27.520 - 00:28:35.456, Speaker B: So one, two weeks for optimistic roll up. How much it actually takes to compute fraud proof. So the fraud proof, you can calculate it instantly. The reason you need one or two weeks for withdrawals from an optimistic roll up is if the validator submits an invalid state, you need to give people enough time to submit the fraud proof and actually make sure that it's included by the miners in the block. And this one week is exactly to prevent the problem which I was talking about previously. It can happen that the validators will detect a fraudulent transaction, will submit it, but the miners will just ignore it. And by the way, I'm obviously biased because I'm working on ZK roll ups, but I still want to be a bit contrarian here and say that I actually think the miners should be allowed, or they are allowed essentially by the protocol, to not include transactions, to censor transactions at their will, because that's freedom, isn't it? So they should be incentivized to behave in a certain way.
00:28:35.456 - 00:29:13.500, Speaker B: And this is the beauty of blockchain and the beauty of capitalism. Overall, everybody is doing their selfish thing. But overall, because we have rules to punish crime and just being nice and bringing value is more profitable than trying to screw people up, because you're just going to make more of being productive, even if it's. If you're completely selfish and I think we should build systems that are resilient against manipulation, not try to rely on some social coordination, not try to build something which will be shaky.
00:29:17.220 - 00:30:08.708, Speaker C: I personally second that for sure. But there's this interesting caveat here in that when I personally think about this problem, if we had zero knowledge proofs a couple of years ago, maybe we could have asked all the miners to actually include zero knowledge proof right inside the block. It's just that it actually takes time to compute it and it will take space inside the block. It's just simply faster not to do it and rely on others to do the validation. Right. So there's this interesting trade off between relying on speed and crypto economic guarantees versus actually provers needing to compute this proof. Right, let's be realistic.
00:30:08.708 - 00:30:25.844, Speaker C: This does not come for free. Right? You need to actually do the heavy lifting when you compute this proof. It's much easier to verify, but to compute the proof, you need to run some bulky computing power, I believe. Right, this is true.
00:30:25.882 - 00:30:58.800, Speaker B: And this is, by the way, a very interesting point. We actually indeed need to run a lot more computations than is necessary for a transaction. So, like probably by a factor of 10,000. So for every simple operation in our blocks, we need to do 10,000 operations to produce the proof. However, number one, amortized cost of the zero knowledge proof per single transaction is completely negligible. It's in the range of 0.1 to 0.1
00:30:58.800 - 00:32:04.816, Speaker B: cent per transaction. So unless you need to do some super tiny microtransactions, nobody's going to notice. People are willing to pay a lot more. The second aspect of it is, can anybody run the prover? Will it not lead to a centralization where only a few select validators can be running a zero knowledge proof validation service? Because you need lots of this hardware and it's going to be like miners that you have to buy and keep it. And that's interestingly not the case, because with zero knowledge proofs you can, and we actually do this, we run them on demand in the cloud providers, and you can have a generic setup that works with any cloud provider, and you have a lot of them, they are competing. So you can have something that works on Google Cloud and Digitalocean and Microsoft Azure and different ones, AWS and so on, and you run them on demand. So you only run the server the moment you have a block.
00:32:04.816 - 00:32:37.548, Speaker B: You can spin it up in a matter of seconds and you shut it down in a matter of seconds. You run the server only literally to just compute. Maybe like 510 to 20 minutes to compute the proof of the block, and then you don't need it anymore. And the hardware where you run it can be completely untrusted. It's not like the validation service where you hold your private keys. Those ones have to be secured. Those are better off at your premises where you actually control the hardware, control the access.
00:32:37.548 - 00:32:57.110, Speaker B: But the proof generation, you can delegate it to anyone because they are self verifying. If the proof doesn't fit, then it won't be accepted by the contract, and it's really cheap to verify the proof. So for these reasons, I think that this is a non issue for ZK roll upS, just because it's negligible and anyone can do this.
00:32:58.760 - 00:33:26.290, Speaker A: Awesome. I would like to continue on the topic, but I think we have a few more questions. So let's start with the questions and see what the question is. ZK roll up should be faster than optimistic roll up. What are the TPS? Oh no, not the TPS discussion please. Like the worst discussion you can have with technical people on blockchain is the price of bitcoin. But the second best comes how much is blockchain doing?
00:33:29.300 - 00:34:22.864, Speaker B: Unless you're one of the l two projects. We love TPS. We love talking about TPS because of course we have the largest TPS. We like to measure that both optimistic and ZK roll ups are limited by the bottleneck is the size of ethereum block for call data. It's not the computation behind the proofs or the hardware requirements for your full node there. We can have unlimited tps for both optimistic and ZK rollabs, but we can only include a limited number of public data chunks in the Ethereum call data. And so the question becomes, how much data do you need per transaction in each case? In a ZK roll up case and an optimistic roll up case.
00:34:22.864 - 00:35:23.552, Speaker B: And then you can take the Ethereum block limit of currently something like 12.5 million gas. You know that one single byte of data costs 16 gas and you can make a simple calculation and you will know what's your TPS, right, because the block is produced every 13 seconds or so. So interestingly, for ZK roll ups and optimistic roll ups, they might have slightly different profiles of costs, because for optimistic roll ups you have to post the same inputs as you do on Ethereum today. So exactly the same input for transaction as on Ethereum. Plus the signature. Plus if you're using a single round optimistic roll up like optimism does, you need to put the new root hash of the transaction arbitram doesn't need that because they use truebit style verification with multiple rounds so they can skip the hash.
00:35:23.552 - 00:36:05.388, Speaker B: So I'm going to write this down here. So optimistic, you put Ethereum input, signature and root hash. Root hash is four bytes. A signature is 65 bytes. Ethereum input depends on the transactions of a simple payment. It can be something like, I think 100 bytes. Now signatures could be abstracted away and they could aggregate the signatures with BLS.
00:36:05.388 - 00:36:29.640, Speaker B: But none of the projects that are currently building optimistic roll ups doing this for now. So let's just assume this is what you get there. For ZK roll ups, you have two options. Option one is to include only this part, only the ethereum, the transaction input. This is TX input. So this is option one. So this is Ek.
00:36:29.640 - 00:37:54.752, Speaker B: Option one is just this here you have from, to maybe it's like 40 bytes, 40 bytes here. Then it's going to be already smaller than current versions of optimistic roll up by about factor of two, three. Now the second option for ZK roll ups, which optimistic roll ups do not have, is to only publish the outputs, the outputs, meaning like for each of the changed slots, you publish the key, so the account id, the slot number, and then the value which you put on the slot. So depending on how many slots you update for in a transaction, and on the size of key and value, it can be somewhere from a few bytes to maybe 40 bytes, maybe 60 bytes, let's say 64 bytes. But the interesting thing is you don't have to publish it for every single transaction. You have an option to publish it only once at the end of the block, just for the latest state of the update. So imagine if you have a uniswap and a lot of users interact with it, and they make updates and you update a single trading pair.
00:37:54.752 - 00:38:25.790, Speaker B: Then at the end you will only need to publish one of these updates, and not every single transaction. TPS currently are in the range of like 2000 tps. For EZK roll ups. For simple payments, let's measure everything with payments. For optimistic roll ups, I heard the number of something like let's say 400 for payments. For contracts it will vary because the inputs will be different.
00:38:26.980 - 00:38:49.190, Speaker A: I have one question that kind of adds to that. If ethereum comes with shards in the version two, obviously we can multiply that by number of shards. But is that sharded VK rollout or can we kind of move things around?
00:38:51.020 - 00:39:47.716, Speaker C: Alex, if I can take this, obviously extends optimistic. Yeah, I mean devitalik, he posted quite detailed analysis of the expected TPS using if one and the future if two. And it's not just about charts, it's about the cost of storage. So if two really gives us much more space to actually put all the transaction info. So I think with the current solutions on if one, we can expect like two orders of magnitude of the speed up. So instead of 15 tps, you'll end up with maybe 1000 or a few thousand tps, depending on the actual solution. On if two, Vitalik's estimation is that that's going to be over 100,000 dps.
00:39:47.716 - 00:40:33.370, Speaker C: Right? So that's like a significant. But again, that's because we will be able to put a big chunks of data on if two, not a state, again, just data, which is write once read type of data. Right? And data is cheap. I mean, the way we should all think about data is that logs are super cheap, storage is cheap. What's really problematic is the state access, right? We should do everything to sort of minimize the state size, whereas just storing logs, we can store as much as we want because it's like almost zero cost.
00:40:33.820 - 00:41:32.328, Speaker A: Yeah, I think it's worth adding that state is something that is stored in specially designed database based on Merkel trees, which we talked about on one of previous meetups. While the call data is basically the history of all transactions, and it can be stored like in the cloud on very cheap storage, while the Merkel tree data needs to be stored on very fast SSD drives. And today, the biggest bottleneck in scaling Ethereum is the speed of those databases. Not anything else, not the consensus algorithm, not anything else. And Alex, I think I actually heard it from Alex for the first time, which is super unintuitive when you come from outside the blockchain space, if you don't know internals, how it works, it came as a shock to me. The speed of the database is the bottleneck of scaling Ethereum as of today. So that's super interesting.
00:41:32.328 - 00:41:45.244, Speaker A: But I want to ask, following a question, if we have really a lot of this call data, and it grows really big into many terabytes, then it's becoming difficult to run a full node. So it kind of adds up to centralization. So it's not like.
00:41:45.442 - 00:42:22.904, Speaker C: No, it's not, because it's not about the size of a call data. I mean, you can easily download terabytes of data in hours, whatever. This is not the problem. The problem is the state size and validation time of all the transactions. That's literally the problem, right? If you wanted to do this, because with the ZK roll ups, you don't have to, because you've got to prove that the state is correct. But with any other construction, if you really wanted to run the full node, you have to run through all the transactions, downloading terabytes of data. I mean, think about it.
00:42:22.904 - 00:42:32.508, Speaker C: How much time does it take right now to do the full node sync on Ethereum? Right. That's probably going to take days or maybe weeks.
00:42:32.674 - 00:42:33.996, Speaker A: Weeks probably.
00:42:34.178 - 00:42:43.570, Speaker C: Downloading terabytes of data, that's going to take hours on fast Internet connection. So that's not the bottleneck at all.
00:42:44.980 - 00:43:02.440, Speaker A: Eight k moving super fast as long as you have. Wonderful. Let's take another question then. Well, I guess it might be that one we might address to Alex. What is the major roadblock now for ZK, for zero knowledge?
00:43:04.460 - 00:43:24.076, Speaker B: Honestly, we don't have a roadblock now. It's just a matter of engineering. So all the fundamental questions of research are solved. We know what to do, we have a roadmap, we'll publish it soon. And yeah, it's just a matter of finishing the.
00:43:24.258 - 00:43:28.060, Speaker C: I think there's so much optimism right behind Zike.
00:43:29.200 - 00:44:24.210, Speaker A: Well, there's a lot of optimism behind Zikai. Well, I want to say I'm super excited, and I've been in Ethereum space for almost four years now, and remember they announcing L2 and all the stuff and waiting forever for those things to happen. And finally, end of the last year, we had first things working with ZUkc being one of the most notable examples. And it's not solidity compiled back then, but it was really exciting to see things working now. Optimism seems to be just around the corner now. We have a great news from Zik, again with the solidity compilation. So if anyone has any doubt yet if it's worth joining the blockchain space now, if you're not in the blockchain space now already, then I think that those break proofings are happening just now.
00:44:24.210 - 00:44:49.900, Speaker A: I think we have at least one more question, that one we already, I think, discussed. Let's see if we have anything else. The talk was fantastic. Yes, thank you. Question. You mentioned some solidity functions might not be available in early versions. What are the use cases that might be impacted by this?
00:44:50.350 - 00:45:22.450, Speaker B: I think this is like EAP 1962, which by the way, we created and we introduced and implemented these are like some heavy pre compiles for elliptic curve operations to verify other types of snarks or make pairings over elliptic curves of different types, but you don't really need them because you can implement them with your knowledge proofs. So what we will support eventually is native support for recursion, where you will be able to write your own pre compiles for anything.
00:45:22.600 - 00:45:33.222, Speaker A: So that gives me a perfect excuse for the question I wanted to ask earlier, but I want to allow other people to ask questions first. Can you build a ZK roll up on ZK roll up? Can we build layer?
00:45:33.286 - 00:45:48.794, Speaker B: Absolutely, you can do that. And what this gives you is shielded transactions, which will cost essentially the same as normal ones. So the cost of privacy will be like, maybe you will pay double the cost of a simple transaction.
00:45:48.922 - 00:45:53.520, Speaker C: And of course you can build a Zika roll up on top of optimistic roll up and vice versa, right?
00:45:56.370 - 00:46:35.520, Speaker B: Not really, because for optimistic roll ups you will have to post the proofs and they're very big for the proof systems like plonk are around like I think 1.5 or 2 kb, which is a lot if you have to put it on solidity, on call data. If you use something like starks for transparency, then you have to put the 100 data and this is going to be very expensive for optimistic collapse. With ZK roll ups, you don't have to publish all of that, you omit it. You just verify it recursively and then you can be sure that transactional is correct, but you omit the date itself.
00:46:37.250 - 00:47:23.142, Speaker C: But again, I kind of feel that given the high gas prices and all the upcoming changes, especially with 52 as well, you have to expect to pay more and more for state update and potentially even the state rent will be introduced, whereas posting data like logs will become cheaper and cheaper to the point that it will be literally free almost. Right. This is how you scale potentially, right? You sort of turn the blockchain into this huge data store with just state commitment rather than the full state. And the actual computation will be for sure eventually done off chain on another layers.
00:47:23.206 - 00:47:23.482, Speaker B: Right.
00:47:23.536 - 00:47:27.360, Speaker C: There's just literally no point of doing a lot of computation on l one.
00:47:28.050 - 00:47:28.800, Speaker B: True.
00:47:32.770 - 00:47:33.950, Speaker A: Sorry, go ahead.
00:47:34.100 - 00:47:36.686, Speaker B: No, I just want to say we.
00:47:36.708 - 00:48:18.666, Speaker A: Have a few more questions. I propose to do two more questions because I think let's do at least one more question and then we can go to discord, hopefully. This is actually interesting one, because in case of many competing ZK technology system, and that goes to optimistic as well, I guess. Could there be a serious risk down the line of competing technologies beating up gas prices? So how often before we go to that, how often do you publish zero knowledge? How often do you publish approve on the l one? For the l two maybe?
00:48:18.768 - 00:49:04.730, Speaker B: Let's start with, well, you can do it as soon as you have enough transactions right now, we do it every couple of hours. But you could also, at a very high flow, potentially do it every five minutes or even every minute. So this can actually happen that l two solutions will consume quite substantial portion of ethereum block size for proofs or for call data. But you always have to measure it amortized against a single transaction in the block, which will be relatively low. Otherwise nobody would use the tilted solutions.
00:49:06.510 - 00:49:07.174, Speaker A: Okay.
00:49:07.312 - 00:50:20.210, Speaker C: And again, I don't think you should be concerned so much about the gas price on the base layer. If you sort of assume that eventually base layer will become this global settlement layer, right? I mean, this cost will be spread across different roll ups and users. We hope that eventually users won't need to pay much. And from the end users perspective that's going to be like it used to be three years ago, right? And by the way, if any of you follows other kind of side chains, if you like, or blockchains, like copies of Ethereum advertising low gas fees, you probably will notice that one of the reasons for the low gas fees on these blockchains is because there's not that much traffic on them, right. Once they get to Ethereum state, they will be facing this unfortunate dilemma. Either they will centralize or they will have to raise gas fees. There's just no easy solution to this, right? And the reason why ethereum is so expensive, the true reason, is because there's just so much demand.
00:50:21.050 - 00:50:21.800, Speaker B: Sure.
00:50:25.130 - 00:51:03.170, Speaker A: We have two more questions that all kind of relate to each other, so let's try to put two questions at a time. It's all relating to peace on the blocks, to inordinate levels. Just freezing out competitive, less funded system. It seems like this could lead to a massive increase in gas prices. So that's kind of continuation on previous one, but I think it's worth mentioning because the next one is also related. How low are the fees on L2 and how are the fees of operation on L2 covered?
00:51:05.350 - 00:51:39.120, Speaker B: I think we already covered that. So I said that zero knowledge proof part is ineligible and the costs are in a range. You can compute. I can say that for a simple transfer on ZK sync, you would have to pay something like 500 guests on chain part compared to 60,000 guests. Like 40 to 60,000 guests for ERC 20 token transfer. For optimistic roll ups, it's probably a couple of thousand gas per transfer.
00:51:40.130 - 00:52:31.310, Speaker A: I think it's also worth mentioning that I think there are fees on every single roll up that are used to cover the fees on l one, right. Because it seems there are a few questions around how do you going to pay all these fees on layer one? Well, we're going to earn with lower fees on L2 we're going to earn for our in the future. I think it's the image that we can start seeing. It might be obvious or not, but it seems that in the future we might have ethereum. That is basically a security layer for L2. And we don't really do many things on layer one. We don't really run transactions, maybe a very big one, maybe like moving funds between layers, but we basically submit those proofs and everything happens.
00:52:31.310 - 00:52:45.620, Speaker A: Or majority of the work happens. Or L2. Right. So Ethereum becomes kind of decentralized central bank, the one, the source of security rather than a place where you would go and actually withdraw money or do any business.
00:52:45.990 - 00:53:25.040, Speaker C: But if we actually do consider this previous question, what happens when the block size on l two goes to this inordinate level? I think we will probably reach again the scalability limit and the gas prices will indeed be high. However, we're talking to orders of magnitude higher than today. Right? So now it's a question of can we get to orders of magnitude more transactions and users of ethereum blockchain? And if that happens within a year, I certainly would be the first one to be super happy about that. Right? Because all of us are looking at a very bright future if that happens.
00:53:26.290 - 00:53:45.938, Speaker A: Okay guys, so last question. Last question. I wanted to ask that question. Someone else asked this question. So here comes this question. How can composability between roll ups be handled? So now the question comes, I'm on one roll up doing my business, now I want to do my business. On the other roll up, I have to go through the main chain.
00:53:45.938 - 00:53:49.240, Speaker A: That's probably expensive. Can we do better than that?
00:53:53.930 - 00:53:59.290, Speaker C: Well, a short answer is that you have to start, learn and love asynchronous transactions.
00:54:02.750 - 00:54:05.322, Speaker A: Alex, anything you would like to add?
00:54:05.456 - 00:54:47.180, Speaker B: I agree, you can have composability the same way you're used on Ethereum. And I think it's going to be a deal breaker for many use cases. Actually, for most use cases, there are a few use cases where you don't care about this. If you just want to make at market trade, then some people just put an order overnight with some good price and they wait for this order to be executed. This will obviously work. You can just send this request and then whatever chain executes it faster, you will send it there and send it back. But for things like composable defi, you really want them to be on one chain, otherwise it just won't work.
00:54:48.430 - 00:54:56.042, Speaker A: What about the cost of migrating? Because I imagine the Ethereum might be quite expensive and might be even more.
00:54:56.096 - 00:54:58.560, Speaker B: It will be way more expensive. Yeah.
00:55:01.010 - 00:55:09.040, Speaker A: We have those chip transactions, so I have $50, $100. I want to just move it to another thing, but then it costs, I don't know, $10.
00:55:09.650 - 00:55:59.220, Speaker C: No, you'll be using state channels to do that between roll ups, I think, and liquidity providers that will sort of allow you to actually move liquidity over and they will maintain position on l one. So for small users, I don't think there's going to be a problem. If you're like a whale, yes, you will pay those fees, but even with today's prices, because we have to put everything into the context, right. Even with today prices, it's actually cheaper to trade on the base layer if your trade is significantly large, because decentralized exchanges will sort of charge fee on your transaction anyway. Right. So for big liquidity providers, the current fees are still quite low, interestingly enough. Right.
00:55:59.220 - 00:56:33.082, Speaker C: And they might actually provide liquidity for state channels that will allow you to cheaply move your liquidity across chains. I think that's definitely possible. And I would expect such rails to exist for, let's say, what we call retail users, if you like. Right. People just having regular accounts and just wanting to move, let's say, I don't know, a few hundred dollars from one roll up to another. Right. There's literally no need for you to sort of go back to l one and go to another l two.
00:56:33.082 - 00:56:54.930, Speaker C: It's almost like having chips from one casino and you wanted to play in another casino and you sort of meet a guy that would do swap with you because maybe they want to go the opposite way. Right. You wanted to go from casino a to casino B. There's somebody else going the opposite direction. Why go to l one where you could simply swap the chips?
00:56:58.230 - 00:57:34.174, Speaker A: I know for a fact that the biggest problem for Wales today in exchanging their money on Ethereum is not the cost of transaction, but impairment loss. Something to talk about on previous meetup. It's that the markets are not big enough. They're moving the markets when they're exchanging amounts that are like in hundreds of thousands of dollars or millions of dollars. So indeed the price of the fee is not far from the problem. And they hate going back to centralized exchange to do that. And a lot of them do.
00:57:34.174 - 00:57:36.000, Speaker A: Some just split into.
00:57:36.690 - 00:58:16.540, Speaker C: And all of us not being whales, we also experience some interesting side effects the one nice one is what we call a drunk whale effect, which is a whale that does something really silly while potentially being drunk. Right. And that can cause, like, a flash crash on one particular exchange. They can rip through the whole order book with one trade and whatnot. Right. And that will sort of create this huge amount of arbitrage in the ecosystem, raising the gas price for at least few minutes before everything sort of settles again. Right.
00:58:18.350 - 00:58:20.870, Speaker A: Well, the joy of using blockchain.
