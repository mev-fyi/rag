00:00:00.960 - 00:00:36.839, Speaker A: I'm Mariano, I'm the head of engineering of C Labs and I'm going to be talking today about like how you can scale roll ups using Igenda. So first things first, Guess something about me. I'm from Argentina, I'm not really part of Igen or Iganda. So I'm not going to be speaking here from like ok, it's my company, I'm going to try to shield it. Actually I am more of a customer. I'm part of celo. Celo it's L1 chain.
00:00:36.839 - 00:01:53.963, Speaker A: It's a chain that it's basically in its own journey to become an L2. It's basically an EVM chain that I guess one of the most interesting properties to me is that we kind of modify, we introduce new types of transactions so you can actually pay gas using RC20s so for example you can use unpaid gas using Tether, USDC or Celo Mentos families of stablecoins which is like coz euro or even like local currencies in different countries. As I said, one big thing that basically is relevant for Solo today is like we are in our journey to become an alert too. We started talking this around a year ago. It's been quite a journey because we wanted to basically assess all the different technology that was available and see what was the best stack options for that for us. And that's why I think I can really go and speak about you like different options, different traits and how you make that decision. Just recently we introduced our first Test net for DL2.
00:01:53.963 - 00:02:30.641, Speaker A: It's called Dango. Something interesting is like, I guess it's not that easy to. It's not like we are launching a new chain, we are migrating an existing chain. So imagine from the user perspective, the user won't really notice. It's like one day they'd be connecting to nodes that are running on L1 and next day it will be an L2 and for them it will be exactly the same. So same state, same everything. So doing that particular migration is actually particularly tricky and that's part of what we need to do.
00:02:30.641 - 00:03:08.025, Speaker A: And Dango is our first experiment of doing that. Actually later this year, like later on September we'll be migrating actually our first test. Net that it's live. That's a testnet that has many years of history. We expect. I have some people here that probably are not going to agree too much with me, but we expect that maybe sometime next end of the year will be able to get to Mainnet. Okay, so that's enough about Celo, let me talk about Igain in particular I'm going to be talking about Igain and op.
00:03:08.025 - 00:04:07.247, Speaker A: So you know optimism, optimism has its own stack, it's called OP stack and that's actually what we're using. We're using OP stack and we're using Eigenda. And I'm going to try to talk you about how you can use those two and how they work together. So first things, what is Eigenda? So first thing is like you need to understand what is Eigen. Now I know how many of you know Eigenlayer but Eigenlayer, it's a risk taking solution. So the idea here is that you can use the staked if for Ethereum proof of stake to run other services. Now you know that most of these services have this idea of economic security that some nodes are doing some job and if they fail or do something bad they get slashed.
00:04:07.247 - 00:05:11.597, Speaker A: And so the idea is like by reusing the stake, by reusing all that, if that it's stake for Ethereum they can start running other service and they can get slashed for running those services in a bad way. Now one of those service is Eigenda. So Eigenda is, I guess they call this service AVS and Eigenda it's actually also Eigensolution for data availability. Why do we care, why do we care about data availability? I don't know how many of you work on infrastructure but or you've seen that we have roll ups. Roll ups used to be I guess cheaper but not as cheap as they could. And one of the main blockers was the data, like the data requirements. A roll up basically takes execution outside of Ethereum but the data still goes to Ethereum and that's why Ethereum was becoming this huge bottleneck.
00:05:11.597 - 00:06:05.703, Speaker A: Ethereum has plans to solve this problem. So you heard about product sharding which is already live and you probably heard about dank sharding which is something that's going to come later. The whole idea is Ethereum is becoming this kind of huge settlement layer or data availability layer where all these roll ups or layer 2s are going to send their data. Now that's maybe interesting future but we don't know when or how it's going to arrive or if we arrive and it will actually be able to cope with all the demand for space. So having off chain data availability solutions is actually an interesting idea. And for that you can mention Celestia for example that you probably also know and Ikda would be another one. Now I'm going to do a step back and try to explain.
00:06:05.703 - 00:07:01.781, Speaker A: So we have a common framework, how does a rollup work? So how does this work? No, so we have Ethereum. Now you have like blocks on Ethereum you have the sequencer, you have a sequencer, you have a validator. These are like roll up rows that are basically have a job within running a rollup. No. And you have the L2, these are like the blocks of the L2. So what's going to happen is you can have this person, the sequencer that is going to start creating blocks now it's going to go define, okay, I'm going to get these transactions, I'm going to put them into a block and I'm going to say okay, this is a new block. No, now it's not only going to do that, it's going to get the data for those transactions and it's going to send those to Ethereum.
00:07:01.781 - 00:08:03.325, Speaker A: Now that thing called their TX data. Why do you need to do that? The whole idea is like I guess with chains in general, you know that whenever you are I don't know any node of a chain and you want to sync and you want to learn what's the canonical chain, you need to go and get all the blocks and how do you know it's canonical? In proof of take because it's signed in a rollup it's because it's there and it's there signed by the sequencer and has some kind of maybe other authentication properties. So full nodes will go and basically read the TX data from there. That's how they will be able to sync to your roll up. Now there's I said that it's only sending the TX data. So again if you go back to analyze an Ethereum block, what you'll find Ethereum block has the transactions and it also has what we call like the state route. The state route is the result of applying all those transactions.
00:08:03.325 - 00:09:32.977, Speaker A: And so in a roll up basically that has been divided into two jobs. Now one job is to get the transactions out there, okay, these are the transactions that we need to execute in this order. So that's one job, that's the sequencer job and then there's another which is the validator that's going to say okay, and this is the result of executing those transactions. Now you could argue that this is not entirely needed all the time. I guess if you're running your own node and you know the transaction that you need to execute, you can execute them yourself and you get the result. No, but this is actually become quite important if you want the L2 to communicate with the layer one in this case Ethereum, because the validator is going to compute that state route, it's going to send it to Ethereum and then Ethereum is going to, I guess Ethereum smart contracts are going to be able to say oh, because the state route, the result of the L2 is this, I can do this other thing on the layer one that's basically bridging or, or what they call in this lingo, it's like a validating breach because in the time of bridges, basically we are validating the whole state transition. Now how does this change when you go and try to do off chain da? So when you try to do off chain da, well, you need to introduce another actor here which is the DA provider.
00:09:32.977 - 00:10:38.575, Speaker A: And so now this thing that I would say that you have like free jobs is now divided in free jobs. So the sequencer is going to be responsible to sending the data commitment, but it's going to send the data to the VA provider. What does that mean? It means that basically, guys, basically the Ethereum is going to still be responsible of giving you the order in which you need to process all this data. Imagine this is not like just one time. You are doing this for every block or for every batch of blocks. And so Ethereum is what they call like consensus layer because it's basically giving you the order of all the pointers to data. But then you need to go get appointed to the data and ask the DA provider to okay, what's the data that I need to process? Okay, so Celestia off chain.
00:10:38.575 - 00:10:42.083, Speaker A: Sorry, is Celestia DA off chain?
00:10:42.139 - 00:10:43.795, Speaker B: That's what you mean by off chain.
00:10:43.955 - 00:11:18.935, Speaker A: Celestia is also off chain. Yeah, I'm here talking more of a general like off chain da. I'm going to be talking about Eigen da, but Celestia will be the same. So this general process will be the same. Every time you see an off chain da, now you are going to send the data elsewhere, but then you're going and you're going to get like a commitment of that data from the DA provider and you're going to send that commitment to Ethereum. You can imagine that that commitment is like a pointer to the data. Okay.
00:11:18.935 - 00:12:16.259, Speaker A: Before getting into Eigenda, another thing I want to mention from the OP stack is what they call op plasma, or I guess they used to call OP plasma. They are now calling like oplda. And the idea is that this is a protocol for off chain optimistic da. What do I mean by this? So I guess this is more or less the same that I just show. The sequencer at some point is going to go and post a blob. So this transaction data to the DA provider in this case going to be called like the plasma server and the full node when it wants to sync, when it wants to basically get all the transactions, it's going to go to the same plasma server and going to say hey, give me the blob for that key. And by key I see is like this DA commitment I was talking before.
00:12:16.259 - 00:12:57.985, Speaker A: So the sequencer post a blob to the DA provider or the plasma server. This is something like this and it's actually an HTTP protocol. So the idea is that you're going to basically do like a post and you're going to send okay, this is the blob. And as a response you're going to have like this kind of X encoding commitment. So you're going to basically get okay, this is the commitment of the blob. You can use this commitment to later go fetch it, which is what the full node will do. So it's very basic, it's not really telling much what is the optimistic or what's the interesting part here? It's what they call like challenges.
00:12:57.985 - 00:13:35.043, Speaker A: So there's an attack that you can do through these systems. Now imagine we know that. I know it's very hard to attack. Ethereum has a ton of economic security so would be impossible to. It's quite hard to go and change those commitments to do like a reorg or to do like a, like I guess, yeah, a reorg, like a double, what you call, people call like double spend attack. But in this case it's just trying to change the history of Ethereum. But what if the DA provider at some point decides not to serve the data? No.
00:13:35.043 - 00:14:19.855, Speaker A: So you have a DA commitment and say hey, give me the blob for this commitment and the DA server says okay to you. I'm not giving that or to you yes or I don't give it that to anyone. No. What happens then? That's actually what they call data withholding attack. Actually in optimistic roll ups that could be quite serious because you know that optimistic roll ups have these. I haven't explained that. But basically when you send the state commitment there's a challenge period of seven days in which, in which anyone or some parties can go and say okay, that state rule that you said, it's actually incorrect.
00:14:19.855 - 00:15:10.271, Speaker A: Now what happens if the DA provider decides to withhold data for seven days. So no one is actually able to get the data, execute it and challenge the results. And so that way would be basically a way to generate a hard fork or do a double spend attack on the L2. So what OP people thought was, okay, we're going to create something that's called a challenge. The idea is like when you are syncing, when you're a full node, you start getting and you start querying the DA provider, the plasma server. And if the plasma server is not giving you the data, you're going to send a challenge to a smart contract on Ethereum and you're going to say, hey, I can get the data. So put it here, put it on the L1.
00:15:10.271 - 00:15:30.035, Speaker A: No, again the whole idea is like I'm going to be paying for the cost of that as a node. So you can do this grief in a task, but what will that generate is like the DA server will need to either post the data there.
00:15:32.415 - 00:15:32.751, Speaker C: Or.
00:15:32.783 - 00:16:25.231, Speaker A: It could choose not to. But if it choose not to, the L2 itself will reorg. It basically would say, okay, there's a challenge. And this challenge was uncontested. And so we're going to rewind the story of the L2 to the point where basically that data was not available. So this requires a smart contract and this requires a very subtle but important thing. When you create this kind of challenge contract you need to be able to say, hey, I ask for this key for this DA commitment and you show me the data and how do I know that the data you show actually belongs to that commitment? To actually do that you need to be able to, let's say the simple version would be like, the commitment should be like the hash of the block.
00:16:25.231 - 00:16:58.065, Speaker A: So I could have a smart contract that basically hashed the whole data and say ok, it's the same. Cool. So let me talk about Eigen first. So Eigen, the idea is that the solution, it's an off chain. The a solution has these kind of free steps. It's going to be something that's called encoding, that is going to be doing disperse and attestation and finally has something that they call bridging. The idea is like this is designed to be horizontally scalable.
00:16:58.065 - 00:18:06.477, Speaker A: And so the idea is like you're going to talk, maybe I'm going to show it here first you're going to talk with what they call like a disperser. So Disperser is a service that they run and then these Ethereum validators that are part of eigenva can run what they call like these DA nodes. And so the disperser is going to get your blob is going to do Azure encoding into that, it's going to put all of that into a KCG commitment to a polynomial commitment. And it's going to send chunks, this part of this blob to the different DA nodes. And so why does this scale is because you can actually have as many DA nodes as you want and that's going to give you a lot more bandwidth that you can basically use. So this scales with that. And so the first part that I was mentioning like this encoding and it's the Azure encoding and then getting all of that into polynomial commitment and I guess generate all the chunks and send all of those to the DA nodes.
00:18:06.477 - 00:19:05.085, Speaker A: Now the DA nodes are going to get a chunk and it's going to return a signature saying hey, I saw that chunk, I'm holding it, I'm going to store that. That's going to be part of a BLS signature. And so the disperser then is going to got all those BLS signals, going to aggregate that. And that's part of what they're going to give to you, I guess the client and say, hey, here's a BLS signature, here's the proof that your blob has been stored on Eigen. Now if you are like talking to a disperser and using HTTP or any wallet, Utah that's okay for you, you that data. But what if you are a rollup or what if you are a smart contractor? You want to kind of know that actually that happened. So they do another thing for that.
00:19:05.085 - 00:19:54.495, Speaker A: That's what they call bridging. So when you go and call the, when you go and call the disperser, the disperser is going to get your blob. It's going to store it using this process that I explained. And the second thing it's going to do is going to add that into a batch. Now it's going to basically have as many blobs. It's going to have like, I know, let's say, I think it's for 10 minutes more or less, it's going to accumulate all those blobs, it's going to put all that into a merkle tree and it's going to call all of that a batch and it's going to send that batch or the batch header with like the batch root to Ethereum. And thanks to that you can basically go and prove that your blob has been attested.
00:19:54.495 - 00:20:39.613, Speaker A: It's basically part of a batch on Ethereum. And by doing that you can basically use that knowledge from smart contracts on Ethereum. Okay, that's again not necessarily what you will use, but it's actually what it's going to give you. Certainty that has been attested, has been stored, going to be available. Okay, Final thing that I guess I wanted to go and talk is how you mix all those things together. Now I talk about op, lda, I talk about Inda, how all these things work. So this is basically what I describe about op, RDA or plasma.
00:20:39.613 - 00:21:16.003, Speaker A: And this is what I've been describing about eigenvalue. Now the way it works is like again, you still have the sequence. So the sequence is going to talk, what with this plasma server, there is actually one that exists, it's called eigenproxy that implement that standard. The eigenproxy is going to be talking to a disperser which is part of Eigenda and Eiganda is going to be doing the DA nodes, going to be talking to the DA nodes, the same thing as a full node. You will be able to go reach the eigenproxy and going to get a blob. No. And you're going to get all the data.
00:21:16.003 - 00:22:43.615, Speaker A: Now for DA commitment, it's not going to be the hash, as I explained as a simple case, it's going to be more of like a serialized data structure that contains like the blob, the batch where this was stored, the batch header, and some marker proof to show you that you can actually verify that actually exists. And again, if, if at some point it failed, you could always go and do the challenge and say, okay, show me the data, show me the data that you said you had, but you are not willing to give to me and you can create a challenge again. For that you will need a specific kind of contract that is able to understand the DA commitments of Igen, which is I guess in construction right now. Now I say this seems very simple, but it's actually not that simple. Right now when you actually, the disperser, I guess as I said, puts everything into a batch, it's also going to consider that your blob is fully, fully confirmed only when that batch has gone to Ethereum. That actually takes some time. And so the way they work, it's basically you post a blob, they give you a request ID and then you can actually go and query this person as many times you want to see if your request is done or what's the set of your request.
00:22:43.615 - 00:23:32.295, Speaker A: So you can imagine that first phase is going to be Azure encoding and then send that to The DA nodes, that's probably good enough for some use cases. Then it's going to tell you, okay, I already include that into a batch and the batch close. No. And finally it's going to send that batch to Ethereum and it's going to tell you, okay, it's done in Ethereum. Here's a transaction or even more, it's a transaction on a block on Ethereum that it's already finalized. So that actually takes some time. So a few things that I wanted to mention here that I guess are kind of missing pieces that we are working on either by ourselves or with I N D A or with op.
00:23:32.295 - 00:24:35.987, Speaker A: So as I said, the challenge mechanism, the OP plasma initial version relies on the idea that DA commitments are hash of blobs. Here is quite different. And so there's a need to actually create this kind of challenge mechanism for this to work. The second is like as I said, the disperser takes actually some time to do the writes, several minutes. And so we are working actually in making many or concurrent writes because you don't need to actually wait for that. They also working on actually having all of that in a faster fashion. Another thing that I didn't explain too much, but if you see this diagram, you see that from a system perspective like the Disperser seems to be like the central point of failure, right? Like they run the Disperser and the Disperser is down.
00:24:35.987 - 00:25:17.185, Speaker A: Well, everything is down again. It's a server, it's a service that you can have all your technology and ops to make it fault tolerance replicas and everything. But it's still going to probably be run by a single organization. And so the idea they're also part of Igen is to go and decentralize this and so you could potentially run your own Disperser. This is not something you can do now, but would be important for the future if you want this to really, really work. And finally this is more of our own addition which is called like a backup service. And I'm probably going to be discussing now when I talk about like failure scenarios.
00:25:17.185 - 00:26:12.925, Speaker A: So when we, I guess as I mentioned at the beginning, we are working on, we are working on integrating OP with Eigenda and ran Cell on top of that. And so one of the things we start doing was try to understand how this could fail. What would be our response? What would be the response of our system in those scenarios. So I guess you have two typical buckets now you can talk about write failures. So imagine you are the sequencer, you create your own transaction batch, you Want to talk to the service, you want to write that. Let's say the disperser is down and so you are not able to communicate with that or disperser respond but what doesn't respond doesn't really verify no. So they give you a DA commitment at the DA commitment doesn't match what you expected.
00:26:12.925 - 00:27:07.865, Speaker A: Those are for us. Maybe the similar thing for us is the same thing like write fail and there's two type of solutions you can have or I guess free. So you know like chains have like I guess two options right? When it comes to true failure they could say to prefer liveness and so they say okay, I'm going to continue working even if I can guarantee this is going to be final and I'm okay with reorgan. So that's for example like how Ethereum tends to do they prefer liveness over safety or you can prefer safety and say okay, I'm not able to progress because the conditions are not met and so I'm going to stop now. Depends on what kind of system you want to build. You can say okay, what would be my preference? So in this scenario, okay, you can write, so you can stall. You can say okay, I'm going to stop.
00:27:07.865 - 00:27:43.885, Speaker A: You can choose liveness too. You can say I'm okay with reorgan when this happens with this change. Or you could also have a secondary server or mechanism to write. For example, you can go and send the data directly to Ethereum as call data or as blobs. Or you could have a secondary DA service for these kind of things. Now that's okay. This actually is more challenging.
00:27:43.885 - 00:28:24.937, Speaker A: So what happens if a read fails? What happens if the disperser goes down or the disperser sends something that doesn't verify? So the first thing is like well you know that you have a challenge mechanism. So with the challenge mechanism if read failures is going to generate a reorg so that's actually solved there. But maybe that's not what you want. I guess in our case we prefer to have safety over liveness. So we want to have like we call one block finality or reorg resistance. And so we don't want to reorg at any cost. So in our case what we are thinking is actually have a bit of a backup service whenever you write.
00:28:24.937 - 00:28:43.635, Speaker A: I guess we will probably write that ourselves at least as a training wheel for this system. So you know that you'll still be able to read whenever you want. And I guess I'm on time and this is all that I wanted to show. Yeah.
00:28:46.935 - 00:28:55.085, Speaker B: Thank you so much Mario, we have time for just one question. If anyone's got a question, please feel free to give it now. Yes.
00:28:56.025 - 00:29:20.015, Speaker C: So I'm curious to why you chose to use a data availability service now that you know on L1 we are in a blob world and columns use 444 blobs which are quite cheap and there's a separate fee market from the call data. Is there still benefits to like using the data availability service over blobs?
00:29:21.955 - 00:30:34.125, Speaker A: Yeah, so I think it's probably a cost and the problem we see how we see is like I think right now with product sharding the costs are pretty low and it's not being the bottleneck anymore. At the same time we are seeing all these L2s posting and starting to be coming on Ethereum and so it's very hard to gamble or no, it's like okay, are the costs going to is like the supply of blow up space is going to be as it is now or because the demand is going to grow so much the costs are going to increase again. So when you are trying to make these kind of long term decision you are saying okay, always a bit of a gamble, you say okay, well maybe the man is going to increase but then it's going to arrive. Dan Chard in full dunkshardi and we're going to be okay again. But still a bit of a bet, no. So I guess it would be more of like a conservative approach where we say okay, we know that we can have actually much cheaper dea in a way that is going to always work without taking that gamble. The whole thing is like again on these systems you could eventually change if you wanted.
00:30:34.125 - 00:31:15.101, Speaker A: The nice thing about that, I haven't really described that from this RDA or plasma solution is like whenever you say a DA commitment you basically you send like a DA commitment of type X now so you could basically when you are reading those DA commitments, imagine it's basically a full note reading from a queue and reading messages. And so you say okay, this is a blob DA commitment. I'm going to read it this way. This is an Eigen DA commit, I'm going to read it this way and you can do all of that. And so eventually you could basically have a multi DA system or with follow ups as I said or you could switch. So I think that's interesting too from a system perspective.
00:31:15.293 - 00:31:17.445, Speaker B: Awesome, thank you very much. Another round of prize.
