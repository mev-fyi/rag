00:00:02.880 - 00:00:28.393, Speaker A: I think that's the first time anyone ever actually got my name correct ever on stage or elsewhere other than like Belgium and Holland. It's the last talk of the day, so I'm going to try and maybe like keep it a bit short. It's also, well, Nikolai is laughing at me because there's a lot to get through. I'm going to change the format a little bit. You can ask me questions in the middle if you want. I'm really going as deep as you guys want or as not. I kind of just want to make it interesting for you.
00:00:28.393 - 00:00:45.941, Speaker A: I hate speaking about things that are obvious. It really pisses me off. When in the crowd I know this and I know other people don't. So just set the pace. Tell me if you need some more time or something or if you're interested in a topic, we can go for that. The idea is about starknet. If you're all familiar with the network, it's one of the major L2s.
00:00:45.941 - 00:01:23.635, Speaker A: It's zero knowledge rollup. We're going to go through the major developments that are coming over the next year. I'm not telling you any dates, I'm not making any promises, but I am telling you exactly what the state of play is right now. Who's involved, how you can get involved, if you're interested, includes you guys. And yeah, some of the minor details of the upcoming protocol changes. The main thing is starknet is decentralizing. Is everyone here familiar with Zero knowledge roll ups? Can I get like a show of hands? Like, is this a very basic architecture? Do I need to go through the designs? Okay, I'm going to go.
00:01:23.635 - 00:02:11.079, Speaker A: Most people are. Then I won't go too deep into this, but all the, all the rollups, I think it's still all are completely centralized. These different security techniques to make sure that they are correct in the data that's put there. So in the starknet case, we have a centralized sequencer, we also have a very expensive prover in the background which is running and producing a bunch of proofs that what this sequence is saying is correct. And then those proofs of the correctness are being sent to Ethereum which is doing the validation. And that's giving you all of the basic crypto properties, piece of sensitive resistance and basically correctness and availability and liveness. So you kind of trust one person and then you get your trust back later when the watchdog comes and verifies all the work.
00:02:11.079 - 00:02:33.685, Speaker A: That's the basic principle. That's reducing it a bit. If you start making Actions based on what the sequencer says. And then the sequencer turns around and just says, lol, no, there's nothing you can do about it. And this is the case with all of the sequencers. It's only once the data is on A one that you are actually safe and then you're relying on other trusted substance. So the question is, why do you decentralize? And this is a basic crypto question.
00:02:33.685 - 00:02:55.905, Speaker A: I'm not going to get into it. You're all here because of this reason. But normally these are two of the main properties, not all of them. But you have censorship and correctness. In the rollups case, we actually only care about censorship. Or another way of looking at censorship is liveness, depending on how you want to look at it. So we don't need to work on correctness.
00:02:55.905 - 00:03:28.495, Speaker A: This kind of changes the picture a bit. You just want to make sure your transaction gets in and nobody else is blocking you and not saying like, okay, well the state has come and talked to me and said you're not eligible to be on this network. Right. So which parts of this picture do we actually want to decentralize? Well, Ethereum is fine. Obviously we want to decentralize a sequencer, but to a certain extent we also want to decentralize the prover. We want everyone to have access to the ability to prove blocks. We don't want this to be in the hands of a single entity.
00:03:28.495 - 00:04:02.631, Speaker A: This was a major problem with early ZK proving systems. The math was there, the early implementations were there, the infrared to run this at scale and at a reasonable price was not there. It wasn't public information. So I think what the rollups really have to do now and to make ZK really successful is to make sure that these proving systems are as public as possible, that people can run on their own personal hardware, that they know exactly what the constraints are, what the costs are and so on. Right. And as these networks decentralized, we'll start finding out this information. Everyone with me so far? Yeah, cool.
00:04:02.631 - 00:04:35.095, Speaker A: I'm going to stop doing that. All right. A lot of what I'm going to cover here is covered in Ilya's talk, who is the designer of this protocol and is very detailed. What I'm going to add to that talk is some of the more high level explanations of it and then more like what we are doing today, who is working on which parts, what state within, what are the trade offs that we're trying to fight with. Ilya's talk will give you a very good overview of what the actual structure of the system will look like. He's also really funny. I recommend watching his things.
00:04:35.095 - 00:04:48.319, Speaker A: Is a cool one. Right. We're going to conceptualize starknet into four layers. And we are doing this intentionally. We're trying to make them as simple as possible. In each layer we have the proposal schedule. We've got the L2 consensus.
00:04:48.319 - 00:05:04.527, Speaker A: And then the part that's unique to ZKRollups is you have the proving system. And then someone needs to put all those proofs onto the L1. Right. It's basically the diagram I showed you before. But we're going to tackle each one of these separate protocols. The first one, I'm not going to do any diagrams. You're just going to have to imagine mostly proposed selection is very simple.
00:05:04.527 - 00:05:24.483, Speaker A: We use some randomness from the L1, probably the block hash. I think we've settled on the block hash now. I say this very confidently. I'm not the one doing designs. I just look at the designs. But we will be likely choosing the set of proposers to epochs in advance. We're using Tendermint.
00:05:24.483 - 00:05:49.669, Speaker A: I'll get to that in a second. This is actually quite long and there's a really good reason for that, which I'll get to. Everything is kind of changed to decentralized approving. So you get all the normal consensus stuff. I'm not going to go too deep into those parts because you can just literally just watch the thing about Tendermint. But I'm going to go through all of the changes that are there to make sure the proving is properly decentralized, that the costs are aggregated across users very well and that this is like a functioning system. Cool.
00:05:49.669 - 00:06:11.405, Speaker A: So we are using Tendermint. This was actually a really hot topic. A bunch of people were like, let's do dags. Dags are the hot new thing. And no offense, Cosmos and informal systems. Tendermint is amazing. And actually we decided to stick with Tendermint as a bunch of different research teams because we know its performance in the real world and you've got to factor in that with ZK rollups.
00:06:11.405 - 00:06:44.915, Speaker A: All of this stuff is so new. The ZK part itself has issues that we're trying to squeeze into these protocols. The last thing you want is an experimental consensus system underneath it just to make sure you have it all. So I think going down the faster consensus mechanisms is something that we definitely wanted to do in the future, maybe two years from now. But let's first get a working decentralized ZK rollup going and Then we go back to patching up the holes. Usual properties, you've got the 1/3 security, you've got attributability. If people try to fork the network, you can slash them.
00:06:44.915 - 00:07:14.725, Speaker A: Very long discussion going on right now about whether or not you should or should not be slashing nodes in this network. And I'm very confused by that one and it's a very interesting one. So I recommend reading the literature about it afterwards. And then the size, like we're going to kind of take Celestia playbook here. You have a fundamental trade off, right? If you're decentralizing the system, you are actually just like slowing it down. You're making your whole thing more latent, right? One of the trade offs with the rollups is that you just have one node. It is just doing thousands of transactions for you.
00:07:14.725 - 00:07:34.437, Speaker A: You don't have to worry about it too much. It's really easy. The moment you decentralize it, you put network latency in there and you slow down the whole network. You have to set the amount of time in which you can build a valid block and then you have time to vote on it. If you pack all of this stuff really closely together, you can get basically the same speed. But doing that is actually really hard. So you are going to introduce latency issues.
00:07:34.437 - 00:08:17.113, Speaker A: And in the main big issue with tendermint is that one, the slowest proposer is kind of like a major source of delay in your network. And two, your 1/3 plus 1 slowest validator is also a major delay in your network. Right? Because then you can't come to consensus for a few rounds or whatever, or it takes too long, you miss around. So the network will be configured to expect really, really high performance characteristics from the nodes. You do this by just saying the blocks are huge, the times are fast. You're going to start missing rewards if you're not keeping up with the network. Is this okay? Is this censorship resistant now? You put it in only a very few number of people.
00:08:17.113 - 00:08:48.035, Speaker A: I will talk about how we handle that later and why it's still actually quite good in censorship conditions. But right now you definitely have some amount of censorship resistance. Classify some with different metrics as you like. And you also have. Yeah, I forgot what I was going to say next. But you have some nice properties from this. Reasonable liveness guarantees is sort of going to say most cloud centers have actually less than 100 copies, right? Yeah, so I covered this already.
00:08:48.035 - 00:09:04.171, Speaker A: I'm not going to get into the tendermints. But does everyone know roughly how these kind of protocols work. You all vote in one round. You're like, hey, I just seen a block. Let's vote, everybody. No, okay, I'm going to say, go watch the talk. But roughly.
00:09:04.171 - 00:09:26.417, Speaker A: Some guys send a block out and say, I think this would be the next block. And then you kind of say, oh, I also had the same block, or I've seen this block before, or I've seen your vote for the block. And then you say, I'm going to send out to everybody that I've seen this block. And then everyone says, oh, two thirds of the network have seen this block in the last bit of the round. And you say, okay, cool. At least 2/3 of the network has seen the block and committed to it. We all agree that's it.
00:09:26.417 - 00:09:50.005, Speaker A: That's tender meant in a very quick nutshell. The thing is that in the round, you might not come to consensus, you might not get the 2/3 vote. And you say, there's probably some major latency in the network or there's an attacker. And so I'm going to dump this block on the ground. Then I'm going to start a new round. And we try to come to consensus on a new block from a new proposer in a new round. Zindermin works normally.
00:09:50.005 - 00:10:34.675, Speaker A: Starknet changes a little bit. We say that in the first round you can come to consensus on a block, and in the second round you can only come to consensus on an empty block. Super weird property like causes weird jitter in your network. And the question is, why the hell would you do this? And this is kind of like one of the really cool things about ZK systems or one of the really annoying ones is you have this relationship between sequencing transactions and proving them. You can sequence them and lock onto them and then prove them afterwards. ZK proofs are embarrassingly parallel. Every block I can prove after I've already come to consensus on it.
00:10:34.675 - 00:11:25.155, Speaker A: But we have this problem now that as I keep sequencing transactions, if the prover is lagging behind and I don't have a proof for a block a long time ago, I suddenly need to reorg that block if there's an issue. So we actually want to slow down the network so that we have fewer transactions that you're capable of reorging or doing anything nefarious with. By having the proofs basically block the rate at which Transactus can enter the network, this keeps these two things in balance. Does that all make sense now? The other thing that you want to do is one proof of one blog Posted on Ethereum is actually too expensive. It's not that much in itself, but if I'm doing it for thousands of blocks, it becomes very expensive. And the whole point of Starks is that when we aggregate them all together, it becomes much cheaper. Using recursion of the proofs, I proof that two proofs are correct all the way up at the top.
00:11:25.155 - 00:11:55.735, Speaker A: And so on Starknet, we have 30,000 blocks roughly in a batch. I think that might have changed in the last update, but before that, that was roughly the number. So I take 30,000 L2 blocks and just send them all off in one go. And I get some nice cost savings because of this aggregation. But now you have one person who'd be responsible for proving 30,000 blocks. That is really bad. Can anyone think of why? Yeah, it's expensive for the guy.
00:11:55.735 - 00:12:14.917, Speaker A: That's not the main reason. What if one of the blocks can't be proven? Yeah, well, it's more that he spends a bunch of money, can't prove one of the blocks. So the whole proof is valid. Exactly. And now we need to reorganize it on the blocks. Right. There's no.
00:12:14.917 - 00:13:06.915, Speaker A: No one saw this coming. So the aggregation of two blocks together is a really easy thing to test. The full test of the correctness of the execution engine and the prover is really, really hard. So if there's one little proof right in the middle of this thing that is unprovable, I suddenly wasted a whole bunch of effort and I need to reorder the network for a crazy amount of time. But nobody wants this. So what we're doing now instead is every block gets proven in another block in the chain, which means that we're constantly producing small blocks and we're constantly producing small proofs for them, which means we're consistently reassured that the proof will be able to be generated, basically. So there's a specific parameter in here called the K parameter, which I don't think anyone's called the K parameter before, but starknet's K parameter is what we use, and it means that K blocks ago needs to be proven in my block.
00:13:06.915 - 00:13:28.935, Speaker A: Otherwise it's an invalid block and we come to consensus on no block, we move on. This creates a very strange structure. This is one of Ilya's favorite drawings. It's a reference to that. This is a blockchain. Imagine each of these little squares is a little block, and it goes through. We've put this zigzag pattern in for a reason, because it kind of represents rounds.
00:13:28.935 - 00:14:15.701, Speaker A: And then you basically have These different proof chains that like overlay it. The different alignments in each of these strands, the block that is in that chain proves K blocks ago the correctness of that block. Right? Which means that one block should prove every K block in a modular fashion all the way through the history. And if they don't come to consensus, you just have an empty block, then your proof will technically contain the empty block proof. Which should be quite cheap. In this way we're consistently getting proofs for blocks. We know that they're doable and we're distributing the cost of doing the proofs because every single person who produces a block is responsible for producing the proof.
00:14:15.701 - 00:14:36.531, Speaker A: Keigo. And this is the reason why we've got roughly two epochs on the like proposer selection. Because they need to know way in advance. I need to prove a block, it blocks. Ok? Right. So I need to start producing the proof long before it's even my job to start sequencing my transactions. Does it all come together? It's all making rough sense.
00:14:36.531 - 00:15:15.441, Speaker A: Cool. How much time we got left? Right? There's one last bit. Who the hell posts all this stuff to L1? We don't have an answer for that. Partially because it's really simple and maybe it's not and we need to do a bunch of work on it. But effectively you would just choose one entity somewhere in your system, probably one of the proposers and you say, okay, you've got so much time to choose three blocks or the heads of each of the proven strands, put them all together, stitch together all the state transitions. Because you've just proven State Transition 1, State Transition 2, State Transition 3. You need to also prove that the ending of state transition one is the start of state transition two.
00:15:15.441 - 00:15:38.943, Speaker A: You need to do all the stitching all along. If you're understanding this from just my rough explanations, you should really go talk to Ilya. He also asked this talk people to come work with them and then you post it on. This proof should be really simple in some sense. Like the structure of it is very simple. So we can be pretty sure that it won't fail. And this is the whole point of speeding up these small little proofs.
00:15:38.943 - 00:16:17.955, Speaker A: All the complexity of proving is hidden in these small blocks. And then it's easy and cheap and verifiable that this intermediate step won't fail. It's a really important property because if that fails, then we are back to the problem of having 30,000 block reorgs, which would be friends. Q I think I covered all this in the discussion. All Right. This is the question, what if I can't prove a block? What do we do? Like, we come to consensus on the K blocks ago, there was a block, it was unprovable. And try to come to consensus, no block comes.
00:16:17.955 - 00:16:41.301, Speaker A: There's a specific thing with ZK proofs, which is solvable. I'll talk about it later. But it's really hard to differentiate between a proof that is hard to generate and that's just taking a really long time to come. Kind of like a network of failure. Or a proof that just doesn't prove. Right. Proofer just says, nope, you can't get a proof that the prover said no.
00:16:41.301 - 00:17:13.379, Speaker A: Or maybe you can, but that's for the next slide. But you get the idea. So suddenly we're just getting empty block, empty block, empty block, empty block. And then you say, okay, well, I've had 10 empty blocks. Maybe we should reorg. And so one of the big discussions happening right now is how many blocks should you wait. How big of a reorg is acceptable? How many resources or fees should you lose for producing the blocks? Whose responsibility is it to say, I think that this network is not producing a block that is provable? This is like the disaster scenario for Zkit improved systems.
00:17:13.379 - 00:17:44.423, Speaker A: This is what everyone's terrified of. Has it happened before? Yes, it has happened before, but it's usually a bug on the executioner side that produces State update and then you can go and patch it and then do the proof, and so it's solvable. But there are definitely cases where if the prover is like, got a mistake in it. Proofs are very hard to get right. So it could take days for someone to say, like, oh, I finally found the block of this proof. So now you've got like a liveness issue for days. Not.
00:17:44.423 - 00:18:28.565, Speaker A: Okay, so the solution would be to just reorg that block out. But then the question is like, how long do you want to make that reorg window and stuff? Or this is something that nethermind research is working on and we're trying to see if it's feasible. I think it was one of the coolest little things that we've worked on in a long time. I'm not going to explain the whole diagram, but this is courtesy of Benisa. The idea is we use two ZK systems in conjunction, and we assume that the likelihood of a mistake in both provers happening at the same time is significantly smaller than. This is not public work. There will be a blog post about this so you guys can find it out.
00:18:28.565 - 00:19:22.911, Speaker A: So thanks for later, but you're more than welcome to take pictures of it. Does that make sense? So I actually use two ZK systems from two different systems. I'm not going to name any names, but you could imagine RISC0 or something else is another prover. And what you do is. So if I understood you correctly, those two provers that run in parallel, they could have different times taken the different time taken for computing proof for the same block. Why is that? I'm not saying that we run the same two provers on the same thing and there's a reason why we wouldn't do that. No, I'll get to that.
00:19:22.911 - 00:19:50.855, Speaker A: But this has been a proposal by a few other people as well. I think it's fairly good. But the problem is that if the bug is actually in part of the stack virtual machine itself, so you don't solve the problem. Right, agreed. And that's not a good solution. That's our conclusion. In our system, what Denise is proposing very specifically, is that you have one prover which is proving the blocks, and then you have another prover which can prove mistakes in different parts of the stack.
00:19:50.855 - 00:20:37.675, Speaker A: So you say if there was a bug that caused that the state transition done by some optimized version of the runtime, which is what we do, we separate the actual execution from the proving. So we do the execution, we get the state differences, then we re trigger the execution for the prover, knowing what the state differences are. And this is how we parallelize. So now you have two different execution engines with different properties and no formal semantic proof of their equivalence. And then in that they produce traces which are then checked by a different system called a trace checker, if that has a bug, then the prover will fail to produce a proof. Or the proof might be producing a proof on a wrong trace, which means that the prover has a bug. So you can kind of make a taxonomy of all the failure cases and you can wrap them all up in an alternative.
00:20:37.675 - 00:21:14.129, Speaker A: Each one of these pieces you can make quite small on say on risk zero or something. And each one of them you can prove that there was a bug at that point in time and therefore raise a warning to the network and say, hey, this block is categorically unprovable. Here is the proof, which is quite cool. So you got this two bits together. It also means you can make sure that the execution is as fast as possible, that the proving for the execution is as light as possible, and then you can leave the proving of the incorrectness to be as robust as possible. You don't have to worry about the price of that as much if you can decouple these two things as well, so you have more design space anyway. This may or may not be part of the protocol.
00:21:14.129 - 00:21:44.583, Speaker A: This is one of the big current things. Cool. How much time do we have again? Five minutes. Validator set? Yeah. So the 100 nodes, basically the log of the validator set will be maintained on the L2 itself because it's cheap to store all the information there and do the updates. But you will basically be able to trigger a transaction on L1 to deposit into the validator set on the L2 and that will be force included in the network. Which means that if you have assets on L1, you can rescue a malicious L2.
00:21:44.583 - 00:22:28.761, Speaker A: If you don't have assets on L1 and the L2 goes malicious, then you fit the major failure case and you'd have to fork the network to another contract, which is seriously big problem for any loss. This is probably the stickiest issue. And this is not the case for stacking. This is the case for all of them. How do you deal with this? If you have a malicious L2 network, how do you control the L1 contracts? If you have a malicious ownership of the L1 contract, there's literally nothing you can do because all of your ERC20 tokens are knocked up in Ethereum, which is the whole point of Vitalik's Alphabet. Don't overload Ethereum's like consensus. This is the area that needs the most research in my opinion, outside of the consensus part.
00:22:28.761 - 00:23:03.215, Speaker A: We will build a consensus system, it will work, but this is the one where you need to really think hard about how to fix it. Great. I think, yeah. One of the reasons we don't like DAG protocols is because the MEV properties, we don't know what they are. There's not really great literature on what they are. So that's a question to you, the audience, if you're interested in this research topic and you can say what happens in the DAG based system when my incentive to actually disseminate steps might not be the same? It's just like getting the networks move forward. It would be interesting to know what that is.
00:23:03.215 - 00:23:32.953, Speaker A: Great. Where are we? Nethermind, which is our company, and we've got some of the engineers here in the room are working on the consensus implementation. Informal systems which design Tendermint. And it's like the guys behind Cosmos, they are working with starkware a little bit on designing their Tendermint implementation. All three of these entities are Working on sequencers to be as fast as possible. This is all ongoing work. Nethermine's implementation is super early.
00:23:32.953 - 00:24:02.433, Speaker A: I think informal systems is quite a bit ahead of us. But it means that we're really gunning to start getting this stuff in next year. So we're talking about like, I don't know. Personally, I want starknet to be the first decentralized, truly decentralized ZK rollup. Maybe the first decentralized rollup other than Tyco. Yeah. Do you know something I don't? That's my goal.
00:24:02.433 - 00:24:14.457, Speaker A: This is not a commitment from Stark Foundation. Never mind. Or anyone else's. Yeah. But I said it was my personal goal, so. Right. I think we might be a little bit over time.
00:24:14.457 - 00:24:50.921, Speaker A: But since it's the last session, if you guys don't mind, I've got two more minutes. This is probably the biggest part and I think this should probably have been like mainly the point of the talk. But staking is coming to starknet and it's coming very soon. You might ask yo, but you still haven't finished the consensus part. Why are you doing staking? What we want to do is we want to already start lining up the actors and basically get everything done in the same way that Ethereum did where they launched most of their staking infra before actually doing the consensus change. It's a really difficult and complex transition and it involves having a lot of buy in from different people. So we're sort of like getting the buy in now.
00:24:50.921 - 00:25:12.999, Speaker A: This is part of the application here. What does it look like? It's a little bit like Celestia. So you've got a set of stakers who are responsible for doing the actual operations for the network. They will have some minimum buy in. We are considering 20k. Anyone can delegate to them. If you don't want to do the technical parts of the staking and just maintain the security network with your tokens.
00:25:12.999 - 00:25:41.169, Speaker A: There is a particular curve which has been designed by Noam, I think. Yeah. So tokens will be minted to the stakers. You can find all of this on the community forum. I'll have a link at the end. The amount of rewards decreases the higher the staking set is. This is to try and get a natural equilibrium of tokens locked in the protocol and tokens that are liquid to pay for the actual transaction fees.
00:25:41.169 - 00:26:20.579, Speaker A: On Starknet, I think the games the aim is between 1.8 and 2% a year return. This basically says that if you have everybody in protocol, this curve is slightly off. Actually they've changed the numbers a little bit. Then maximum you'll be minting around 4% total extra dilution. The blue curve here is the total amount minted over the year. The red curve is the amount of rewards per person per unit of time for a particular amount of staking, like total asset stake at the bottom here.
00:26:20.579 - 00:26:58.997, Speaker A: Does that make sense to everybody? So this keeps it in equilibrium and makes sure that the total mint a year doesn't range more than whatever the lowest value is here to up to 4% due to the mid amount. And the goal is to get surrounded by 2%. What are the responsibilities there to be announced? And I actually can't talk about this, but they will be done incrementally. So if you decide that you want to be staker and not delegate, you want to be sure that you can implement whatever the protocol asks of you within the lockout period, because if you can't, you're losing that money. Maybe that's a bad idea. Actually. There's no slashing, so you might not be using money for most of those things.
00:26:58.997 - 00:27:29.585, Speaker A: So just be sitting there not earning any revenue. It is every staker's choice to decide what the distribution of rewards is between the delegates and the stake. And so you have an open market of stakers and breaks and things like this. Cool. I am Yorick. My telegram handle is mempoolsurfer. The person who's actually really knowledgeable about this is Denise Dinesky and this is her email and she's more than happy to answer any questions or get you involved in the conversations.
00:27:29.585 - 00:28:07.491, Speaker A: We're trying as hard as possible to get these things out in the open, so it's really easy to follow where they are. I think stacknet is one of the most amazingly designed protocols with some really beautiful things underneath it, and it's made by the Stackware team and they have been working day and night to get this done and get this out to people, and we are now working to help them get it out to more people. And so we're doing a few initiatives around this. Every two weeks we have a call with all the full node development teams to talk about where the status is. I would like to start doing this as well for the consensus one once we're in a more stable state. But in the meantime, this is an open call. You can come join if you want to ask questions.
00:28:07.491 - 00:28:34.475, Speaker A: You'd have to first message me. And then there's a community forum where most of the updates are posted for different technical changes and discussions that are happening. We're trying to shorten the window between design decisions, community forum postings and this sort of conversation that's happening. And this is nethermind. But I'm not going to give the whole spiel because we are over time. Thank you so much everybody for coming. I hope you learned a lot about L2s and we shall see you at another topic.
00:28:34.475 - 00:29:08.293, Speaker A: I'm sure I just gave them nod to like turn off the camera. Thanks for. Hello. Yeah, yeah, we can hear you. So I would like to thank you for this presentation. If I would like to be a staker, what are estimated hardware requirements and connection speed? Yeah, this is embarrassing. I actually don't know.
00:29:08.293 - 00:29:40.985, Speaker A: That answer is a very important thing. So the infrastructure has been run by starkware exclusively up until now and they're talking about the decentralization. If you look at the Juno full node, you can definitely see how much the storage requirement is, which McLeod didn't know what the current full node sizes and DB. All right. Okay, cool. And then blanket statement, probably top of the line execution. You said 170 gigs by the way.
00:29:40.985 - 00:30:13.515, Speaker A: Yeah, I can't give you a good answer, but it would probably just be as big as possible. Pick your top 8 OS server right now. Actually not that big because the demand on the network isn't that high. Like we have a peak TPS as of last week. The 400 transactions a second, which is not close enough to what we need it to be, but it's definitely going in the right direction. But the actual transaction usage is much lower than that. So you don't need top of the line right now, but I'm sure it will be quite high.
00:30:13.515 - 00:30:36.933, Speaker A: I would say bare metal, top of the line something. Sorry, not a very good answer, but you should ask if you're interested in this, make a post in the discussion forum and call out Starcore directly and ask if they could share their specs. That would be cool. I think now is the right time to start talking about this stuff. Any other questions? Nope. Okay. And thank you.
00:30:36.933 - 00:30:38.725, Speaker A: Cool. Thank you very much. Thank you.
