00:00:00.160 - 00:00:17.374, Speaker A: And when I read the math behind zero knowledge proofs, it took me a while, like a few full days of staying in my man cave and no distraction, and I still couldn't understand, like 80% of it. So all that to say, this is really at the bleeding edge of computer science.
00:00:17.494 - 00:00:54.102, Speaker B: And maybe let's go a little bit deeper, just for fun. Welcome to Good Game, a podcast for crypto insiders with your host Imran and Chow. All right, welcome to good game. Today we're going to be talking about roll ups, specifically zero knowledge roll ups, or ZK roll ups and optimistic roll ups. And today we have our guest, who also works with us. He's our head of research, Mohamed Fuda. And Mohamed Fuda, his background is quite deep.
00:00:54.102 - 00:01:19.102, Speaker B: He did his PhD in electrical engineering. He's worked on research within alliance and also has worked with me on the research side for other venture funds. His background is great because he understands and has built products both from a technical perspective and then also helped a lot of our founders in regards to how to think about roll ups and their technical strategies moving forward.
00:01:19.198 - 00:01:33.232, Speaker A: Just add to that Fuda is also a former founder himself. Basically, all the things you guys have ever heard from me and Imran on the technical side that are interesting and insightful, pretty much all come from Fuda.
00:01:33.328 - 00:01:54.280, Speaker B: That's right. And today we bring him on to discuss rollups. And how should founders be thinking about the types of rollups there are in the space, along with why rollups and why layer twos? And finally to talk about how does zero knowledge or zero knowledge proofs fit into this entire roll up space. And before we get started, chow, do you have anything to add?
00:01:54.392 - 00:02:44.640, Speaker A: I really want to share some of the empirical data that we've been seeing, both the public data as well as the private data that we have. Remember, ten episodes ago, we talked about the competitive landscape of the various layer ones and layer two s, and we shared a bunch of data that we have that typically people don't have in the public, which is the alliance applications data. So we're able to see sort of the live, real time developer activity of various ecosystems. And we said that these datasets tend to be really good leading indicators of how the layer ones will perform in the near future. So some of this data, some of the behind the scenes comments we've heard from founders, I'd love to share those.
00:02:44.672 - 00:03:50.374, Speaker B: And maybe just to add, we're starting to hear more about rollups as a service. And so should founders be thinking about deploying their own roll up or app specific roll up versus on a general roll up chain. And so, just to get started, what is rollups? How did it get started? Well, I think it started around it could be traced back to 2014. Joseph Poon and Thaddeus worked on Lightning Network, which is a scaling solution for bitcoin. And the idea was similar, which is like, could we allow more block space to be used for other types of activities while we do general transactions on a layer two like service? And so the basic concept of the lightning networks was first started. And as Ethereum started to gain popularity around 28, a person by the name of Barry Whitehat introduced something called the ZK rollups. And the idea was, could we use zero knowledge proofs as a way to offer succinct transactions in such a way that would reduce the bloat for the layer one Ethereum chain.
00:03:50.494 - 00:03:51.598, Speaker A: What year was this?
00:03:51.726 - 00:03:58.518, Speaker B: This was in 2018. This was around the last bear market, if you remember. OMG Network and or plasma group.
00:03:58.646 - 00:03:59.406, Speaker C: Yeah, yeah.
00:03:59.470 - 00:04:07.438, Speaker B: They had raised via ICO at least OMG raised via ICo to offer, I believe it was first state channels. Right. Correct me if I'm wrong, Buddha.
00:04:07.486 - 00:04:07.670, Speaker A: Yeah.
00:04:07.702 - 00:04:09.830, Speaker B: Or was it plasma channels? Sorry, plasma channels. Yeah.
00:04:09.862 - 00:04:23.816, Speaker C: There was many competing solutions proposed at this time. There was steady channels. There was plasma. Yeah, plasma was a hot thing back then. And then everything was kind of put to the shelf to scale. Ethereum through sharding. So Ethereum to win zero.
00:04:23.960 - 00:04:24.768, Speaker B: Yes, but.
00:04:24.856 - 00:04:38.816, Speaker C: And Ethereum to win zero itself, it changed to this, like kind of skip charging completely and like focus on roll up as a roll up centric view. So, yeah, like it's a roll up came as a series of evolvement of different ideas.
00:04:38.920 - 00:05:02.248, Speaker A: If I recall correctly, optimistic roll ups also came around the same time, the same era. And it just so turns out that all these state channels, plasma, even sharding on the layer one, these never came to fruition, or at least widely adopted. And it seems like over time, the two remaining scaling efforts are ZK rollups and optimism.
00:05:02.296 - 00:05:02.968, Speaker C: Rollups.
00:05:03.096 - 00:05:23.820, Speaker B: Yeah. So a group by the name of Plasma group or Plasma, which was led by Jingle and a couple of the team members, which is now called optimism, started with optimistic roll ups or optimism. And so what is roll ups? Maybe Fuda, if you want to give a quick primer on what are rollups and why do we need them in the space?
00:05:23.932 - 00:06:26.072, Speaker C: Okay, so, like, roll ups and plasma and side exchange also share the same philosophy, which is that you have limited capacity on the l one. The l one cannot have many transactions. It has limited throughput so the concept here is let's take some of this transaction and execute them off each chain or in a different chain that we can actually be faster, we can have more capacity. And once we execute this transaction on, off a chain, let's tie them back to the l one so that no one can steal this money when you take it off a chain. So the roll up in simply like you take a bunch of transactions, you execute them in a roll up. And when the roll up executes transactions and do all the activities that you want to do, but every block they kind of push back a proof in the case of the Corolla, and they also push a compressed version of the transaction data back to the l one. And then you can actually scale Ethereum this way.
00:06:26.072 - 00:06:38.412, Speaker C: And today, actually we achieved this vision because rollups now actually have three x throughboot of Ethereum. Like the universe of rollups in general have three exercises of Ethereum itself.
00:06:38.508 - 00:06:39.988, Speaker A: It's not very much, to be honest.
00:06:40.036 - 00:07:15.442, Speaker C: Three X. Yeah, like we just, we are just starting. Like we have the first two Zk roll ups launched in last two weeks. So like we are very, very early in this game. But according to Vitalik's review, this will keep growing and Ethereum eventually will be just a layer to settle up the proofs that are boast from the l two s. So Ethereum may not be even used for smart contract anymore, or like for applications like Uniswap. Everything will move to the l two, and ethereum in the long term will be the settlement layer for this all.
00:07:15.538 - 00:07:47.834, Speaker B: L two s. And just to double click on what Fuda mentioned around what rollups are, rollups are essentially a way for transactions to be computed that isn't using the layer one resources. And what it does is once it computes, what it does is once it's finished, it sends the verification proof and it will submit it to the ethereum layer one. So then Ethereum layer one knows that this transaction has been completed off chain and the result of that would then be published so that ethereum layer one can verify that transactions have been completed.
00:07:48.154 - 00:08:00.706, Speaker A: Just to dive a little bit deeper on that. So basically, the way that the layer two interacts with the layer one, you said two things. One is posting the proof on chain and the other one is posting a compressed set of transactions on chain.
00:08:00.810 - 00:08:02.394, Speaker C: Yes, that's correct.
00:08:02.554 - 00:08:11.338, Speaker A: And that will incur some cost on the layer one because these two things need to be stored on a layer one between these two things, which one incurs more costs.
00:08:11.426 - 00:08:41.142, Speaker C: As of now, the data storage, because the amount of data you store to the l one is higher than the amount of compute you need to verify the proof. And something like optimistic roll ups doesn't have actually the cost of validating the proof. So in optimism, for example, all the cost comes from storing the data. So an optimistic relapse, it's only data cost. So yes, here's the largest factor. That's why Ethereum community is trying to move to this blob space to save or reduce the cost of this data storage.
00:08:41.278 - 00:09:09.788, Speaker B: Yeah, and maybe we can touch on data availability layer, blob space, which is EIP 4844. A bit later, we can go into Celestia and Eagan layer. In regards to just the roll up space, there's optimism and there's arbitrum. And so in terms of infrastructure, they're a bit different. And so, before we dive deep, what is the difference between an optimism or optimistic roll up and a Zk rollup?
00:09:09.876 - 00:09:29.084, Speaker C: So let's start with what ciao mentioned here. You have two things. You have to have to send a proof to the l one, and you have to store the data. So you have two variables. Actually, changing these two variables will give you three kinds of scaling solutions. Optimistic roll ups, Zika roll ups, and validiums. So I made it even more confusing.
00:09:29.084 - 00:09:52.226, Speaker C: So an optimistic roll ups, you assume that everything is correct by nature, you are optimistic. So you post the data from the execution data from the l two to the l one, and you say, we are optimistic. Correct. But we will have to give some time to make sure that if something wrong happened, we will have to fix it. So this is called fraud proof.
00:09:52.330 - 00:09:54.690, Speaker B: And what do you mean by when something is wrong?
00:09:54.802 - 00:10:15.632, Speaker C: The sequencer or the entity that is responsible for ordering the transaction on the l two and posting this data on l one can cheat, can either block some transactions, can front run, or like, or can even change the execution and say, yeah, this user sent me all this money, something like that, and submit something like that. And then.
00:10:15.728 - 00:10:16.552, Speaker A: Or double span.
00:10:16.608 - 00:10:51.618, Speaker C: Yes, exactly. So any kind of attack that can be done by sequencer can be challenged using the floor proofs. And this is the first kind, we are optimistic. But there is a way to recall a wave recalls the other way. Try to avoid that, which is like Zika proofs or Zika roll ups. Say no, we will use the math, like very specific kind of math, zero knowledge math to be correct by construction, which means that once we execute the transactions, we use zero knowledge to create a proof. This proof cannot be generated unless all this transaction were executed correctly.
00:10:51.618 - 00:11:21.374, Speaker C: And once we have this proof, we will send this proof to the l one, store it in the l one so everyone can validate it, and then also post the data. So the curl ups are the most secure because it shares a proof of correctness, which is a proof, and it also has a data availability on the l one. So validium is different from the QR labs because they try to reduce the cost but also reduce the security. So they send only the proof to the l one, but they store their data somewhere else.
00:11:21.484 - 00:11:23.418, Speaker B: And this could be a centralized server, right?
00:11:23.506 - 00:11:38.554, Speaker C: Yeah, AWS, ivfs, it doesn't matter, but it's not in the l one anymore. So this can reduce the cost significantly, but has less security guarantees than Zika roll ups. So in my opinion, Zecho roll ups is the most secure scaling solution for Ethereum right now.
00:11:38.634 - 00:12:05.052, Speaker A: It's the most secure. But also given that optimistic roll ups need a seven day or a week of fraud challenge period, it's also the probably comparatively the best user experience. Because with the ZK rollup, when you withdraw your money from ZK rollup back to the Ethereum layer one, you don't need to wait that long. Whereas on an optimistic roll up you do need to wait a long time.
00:12:05.108 - 00:12:19.080, Speaker C: Exactly. But at the same time, it's very complex to build a ZCO roll up, right? So you take the effort on the engineering side, you make something very complex, but has better user experience, which is shorter withdrawal delay.
00:12:19.192 - 00:12:32.952, Speaker A: But then what is the trade off there? Because right now we've mentioned two advantages of ZK rollups compared to optimistic roll ups, the trustlessness and the wait period. But then what's the downside? There's got to be a trade off somewhere.
00:12:33.048 - 00:13:11.514, Speaker C: The trade off is complexity of building a secure ZQ roll up, because you will have to go into very specialized kind of math and you will have to construct what we know as Ziki circuits, which we may touch on today as well. So it's a very complex process, very tedious, it has a lot of security assumptions. This is one. The other thing is that you need to actually optimize the proof. Like the proof is you have additional penalty. Here you post another proof to the chain and you need to actually do some computation to validate this proof. So this proof has to have certain parameters, it has to be small in size because you pay for storing on Z one.
00:13:11.514 - 00:13:21.558, Speaker C: It has to be easy to verify this proof. So this is the main downside for Zyn Corps, the proof part that you need to validate this proof and store it.
00:13:21.646 - 00:13:46.454, Speaker B: And just to add some context, there are different types of proving systems, right? As you mentioned. And these proving systems are essentially ways they're different trade offs in regards to how they compute the zero knowledge proof and then being able to store it onto, let's say a layer one. There are probably like, what, five different or popular zero knowledge proving systems. There's ZK snarks, right?
00:13:46.494 - 00:13:47.118, Speaker C: As one.
00:13:47.246 - 00:13:53.398, Speaker B: There's pickles, there's airstarks, plancks and yeah, I think those are the ones, right?
00:13:53.486 - 00:14:46.548, Speaker C: Yeah, actually like there is now, because Ziki, since the invention of proofs or introduction into practical environments in Zcash in 2016, actually, ZK zero knowledge is a very hot area of research, and literally every few months we come, we find a new paper that implements a new kind of proving system. So as of now, we probably more than ten popular and in production proving systems. And it will grow from here, it's not gonna consolidate. But as a large categorization, we have snark, succinct, non interactive proof of arguments of knowledge. And this talk, which is again scalable, transparent arguments of knowledge. So these are two major categories. Snorts started by from zcash developers, and storks started from the founders of Starkware.
00:14:46.548 - 00:15:03.968, Speaker C: And from there, people have iterated on each side, and snarks have now many implementations. They have growth 16, they have blanc, they have hallow, they have blankish, they have, as you mentioned, Bekles. So there are so many variants that we can spend hours speaking about every one of them.
00:15:04.156 - 00:15:25.032, Speaker B: So the way this would work is just as you mentioned, if I'm using a zero knowledge proof or, sorry, a zero knowledge roll up, I would submit a transaction. This transaction would then go to a proving system, which would verify the transaction, then generate a proof using the proving systems that we just talked about.
00:15:25.208 - 00:15:25.800, Speaker C: Yes.
00:15:25.912 - 00:15:34.218, Speaker B: And then it submits the proof or the end of the computation results onto the ethereum layer one as an example.
00:15:34.346 - 00:16:14.934, Speaker C: Yes. So this is the main concept of proving like that. You have a complex process called proving, and you have a simple process called verification. The proving takes a lot of computations and do a lot of math on them to create a very small proof. And once you have a small proof, anyone can do very little computation to make sure that this computation that you proved is actually have been done correctly. So yes, for the roll ups, the proving is done by the roll up itself. They do a lot of computations of each in this proof, but once it's generated, they post it to the l one and the smart contract and the l one will verify this proof to make sure that l two execution was correct.
00:16:15.014 - 00:17:11.214, Speaker A: And by the way, for our listeners, if everything we talked about in the last five minutes sounds confusing and very abstract, it's not that you're stupid, it's just that this stuff is extremely at the bleeding edge of mathematics and computer science. I remember back in 2016 or something in that era, Zuko, who is the founder of Zcash, said that zero knowledge proofs, or ZK snarks, is moon math. And the very few people in the world actually understands how it works. And including Zuko himself, he said he didn't understand the math behind his own creation of Zcash. So if everything we just talked about sounds confusing, it's not the end of the world. I personally have a background in pure math. I studied a bunch of number theory, group theory, all that stuff in college.
00:17:11.214 - 00:17:31.164, Speaker A: And when I read the math behind zero knowledge proofs, it took me a while, like a few full days of like, you know, staying in my. In my man cave and no distraction, and I still couldn't understand like 80% of it. So all that to say, this is really at the bleeding edge of computer science.
00:17:31.284 - 00:17:55.728, Speaker B: And maybe let's go a little bit deeper, just for fun. Yeah. And Fuda, you and I have chatted a lot about the differences between the different proving systems. Let's talk about the two most popular ones, right? ZK snarks and ZK starks. What are the differences between the two? And why would I want to choose one over the other?
00:17:55.816 - 00:18:41.494, Speaker C: Okay, so to understand the difference, let's actually start by discussing the characteristics of a proving system. What do we need from proving system? Like, how can we judge this proving system is better than the other, right? So there are three main characteristics. One is that you try to. The proving is complex already, right? So try to make this proving process a little bit faster, because while you are doing the proving, your customer is like, the verifier is waiting for the proof to be generated, right? So you try to make the proving process faster. The second part, you need to store this proof somewhere in the roll up case. You need to store this proof on l one, and you will be more if you are storing more data. So you try to make succinct, you need to create a proof that is very small inside, right? So this is the second characteristic.
00:18:41.494 - 00:19:08.486, Speaker C: The third characteristics are, is that you need to verify this proof. You need to do some computations to verify this proof. And this computation has also. It consumed gas on the l one. So you don't, you won't make this verification process is very simple. So these are the three main characteristics of my opinion about upbringing system. There are many other characteristics like trustlessness and like trusted setup or not trusted setup, programmability, privacy, even if you are dealing with private information.
00:19:08.486 - 00:19:37.048, Speaker C: But let's just focus on the three main characteristics. So at a high level, snarks are very efficient in creating succinct proofs. They can create very small proofs in size, a few kilobytes, and they can be verified very quickly. So that is the main advantage of snorts. Earlier version of snarks like Zksnark that was developed by Zcash was not required. Something called trusted setup. That initial group of people will come together to create some secret randomness.
00:19:37.048 - 00:20:04.240, Speaker C: And this is not very favorable. It adds a security requirement. So that's why it wasn't very popular in recent versions of snarks. Actually like Bloc or hello doesn't require this trusted setup anymore, so it improved. But the problem with snarks is that they require a lot of brewing. Brewing complexity is too high. On the other hand of the comparison, storks stocks have two main advantages.
00:20:04.240 - 00:20:25.968, Speaker C: The brewing can be very fast and they by nature trustless. They don't require trusted setup. Their main issue is that they require, they generate larger proofs, maybe hundreds of kilobytes. So the cost of that is higher to store this higher. So this is kind of very high level comparison between snarks as a family of algorithms and snorts.
00:20:26.096 - 00:21:07.890, Speaker B: And just to add a little bit more context, you mentioned trusted setup. And a trusted setup is essentially a way to deploy a proving system that is untampered with. Because the problem here, which could be the attack vector for a malicious party, is to set up a proving system that could be altered so that you could change the transaction in a way that would be beneficial for the malicious actor. What you need to do in this case is set up a trusted setup, which would allow a party of people that come together, that generate a random code, and the random code has to be verified between the parties. Once it's aligned, then the proving system would then get deployed.
00:21:07.962 - 00:21:12.018, Speaker C: That's a very good summary of the task, is the process. It's essentially creating a secret.
00:21:12.186 - 00:21:14.134, Speaker A: Was this generated by GPT four?
00:21:15.634 - 00:21:23.556, Speaker C: We wish it would be super fun, but would give user output the secret. Writing like here is a secret.
00:21:23.700 - 00:21:37.692, Speaker B: Yeah, but yeah. So we have the understanding of the proving system, we have trusted setup, and then maybe one more area that we hear a lot about is ZK circuits.
00:21:37.828 - 00:21:56.752, Speaker A: So people hear the words ZK circuits and also the prover and the verifier. Like there's all these terms that people tend to be confused about. Let's define those Fuda, just so people have enough background knowledge to understand further literature in this space. What are ZK circuits? Prover and verifier.
00:21:56.928 - 00:22:39.350, Speaker C: So, the simplest way to examine a ZK circuit, we'll have to tap into your memory from high school algebra and remember something called equations. When you have x plus y equals zero, or x squared minus two, x equals zero, something like that. So this is an equation. A Ziki circuit is a very simple explanation of Ziki circuit is that you have a bunch of these equations, like millions of them, and all of them have to be consistent. Once they are consistent, you can actually generate a proof that all these equations were consistent. That is what a ZP circuit is. It's a bunch of mathematical representation or arithmetic equations that, that have equalities.
00:22:39.350 - 00:23:22.962, Speaker C: And you have to make sure that all these equations are satisfied. And once they are satisfied, you can actually create a small proof showing that. So why their ZK circuits are useful? Because using math, you can create a small proof that will be equivalent to validating millions of equations. So this is very simple, like Azki circuit. So how we use them now? So now we use smart contracts, programs, or any good, you can develop good and rust solidity, whatever. So you have a smart contract, which may be hundreds of lines of code. So the advancement happened that we can actually convert these hundreds of lines of code in any high level programming languages, like solidity.
00:23:22.962 - 00:24:01.708, Speaker C: And using some arithmetization or like some arithmetic tricks, convert this code into a bunch of mathematical equations. Once you do this, then you have converted your code, which cannot be proven. You cannot make sure that the code has been proven in its original format. So you perform a kind of transformation, you convert the high level code into a bunch of mathematical equations, which we call Zk circuit. And now, voila, ZK circuit. This Zika circuit can be proven. So this is what most, most of the work happens, like this conversion process, converting the equations or that code into a circuit.
00:24:01.796 - 00:24:09.020, Speaker B: So zero knowledge. So ZK circuit is essentially like a middle layer that communicates between the prover and then the application that's behind it, right?
00:24:09.092 - 00:24:24.638, Speaker C: Once you have a ZK circuit, once you have this bunch of equations, the proverb comes in, the prover comes in to make sure that all these equations are consistent and then gives you the proof. So the proverb actually acts on or works on the ZK circuit.
00:24:24.726 - 00:24:27.582, Speaker B: Got it. So let's wrap this all up.
00:24:27.718 - 00:24:28.994, Speaker A: Imran, you look confused.
00:24:29.334 - 00:24:47.874, Speaker B: Not confused. I just want to make sure that our listeners understand all of these different components and how they all work together from end to end. So maybe, Fuda, walk us through. Let's say I submit a transaction. I'm a user, and I just submitted a transaction. I'm yoloing into some shitcoins.
00:24:49.064 - 00:24:51.016, Speaker C: Okay, don't do that.
00:24:51.120 - 00:24:55.624, Speaker B: Explain to me what happens in the background within a ZK roll up.
00:24:55.664 - 00:25:46.238, Speaker C: Yeah, so the beautiful thing about ZCO roll ups is that this, all this moon math, in Zooko's terms, is very abstracted and hidden from the user. So someone, some clever teams of engineers have done insane amount of work to abstract all this complexity away. So what happens now? Let's take. I will just take ZK sync error as an example, which is the new product of matterlabs. So, Zika sync, actually, and the engineering team at Zika sync have done an amazing job building a Ziki circuit that represents the Ethereum virtual machine, the EVM. So now it's a virtual machine, which means it can execute anything, right? So they build that as general enough circuit to execute essentially any solidity code, or any code for this matter. It's assuring complete circuit, essentially.
00:25:46.238 - 00:26:10.004, Speaker C: So now this circuit sitting at the back end on a server at Ziki sync. And now developers will come and build smart contracts using solidity, or even reuse the smart contracts from Ethereum. Someone can get the uniswap code and deploy it. Done. So you are there. Users. You connect your wallet, point it to the Zksync era network.
00:26:10.004 - 00:26:36.862, Speaker C: You use metamask as usual. You send the same ethereum transaction that you were going to send on the l one, but now you send it on the l two. After that, the magic happens. But it happens somewhere else. The magic happens that this solidity code will be converted to the equivalent circuit. This circuit will be proven Zksync servers. Originally the proof, the proof will be posted on the l one, the smart contract, and the l one will validate it and done.
00:26:36.862 - 00:26:41.942, Speaker C: As a user, you didn't see any of this happening. You interacted with, you did a bunch.
00:26:41.958 - 00:26:44.974, Speaker A: Of moon math, but you don't know what you did. That's the magic behind.
00:26:45.054 - 00:27:04.338, Speaker C: Exactly like chow. This is exactly chat GBT. Right? You go type a prompt. Right. Do you know what's happening under the hood? Does anyone know what's happening under. Does anyone understand the transformer model? No. That is the beauty of engineering, that you build very complex engineering systems and just abstract it.
00:27:04.338 - 00:27:27.816, Speaker C: The user doesn't know anything about it, but it works beautifully. So in this case, AI and ZK are very similar. You can connect to Zksync or Polygon, ZKVM, submit your transaction. Very similar to Ethereum. And the moon math happens, and you get more throughput, you get lower fees, but you don't know what happened under the holy.
00:27:27.920 - 00:27:29.256, Speaker B: I love the term moon math.
00:27:29.360 - 00:27:30.688, Speaker C: It's cool. Yeah, right?
00:27:30.856 - 00:28:39.090, Speaker A: By the way, Vitalika wrote an article, I think, a few months ago, he actually talked about this moon math term and where it came from. And he said that, yes, a few years ago, it was indeed considered moon math, you know, back in 2016. But at the pace of innovation in the whole ZK space, like, ZK is moving so fast, and there's so much research going on, and there's more and more people who now have the knowledge to understand it and have the expertise in such a way that ZK is almost no longer a moon math because so many people understand it today. And this has been only, what, seven, eight years since the first application of ZK in crypto. And Vitaly says something to the effect of, if you have, quote unquote, intermediate knowledge of math, you will be able to understand zero knowledge proofs. I assume that intermediate knowledge of math means you have a undergraduate degree in pure math and you've taken at least first year, second year course in some kind of number theory, group theory, Galois.
00:28:39.122 - 00:29:32.904, Speaker C: Like, these courses, it was one left because, like, actually, like, the roots of zero knowledge proofs. And cryptography was, like, a very niche in computer science department, and because it has very hard cryptography, no one touched this except people who are, like, really, really deep in math. So, like, it was like the geeks of the geeks who, like zero knowledge proofs. And the funny, here is a funny story. The first implementation of zero knowledge proofs, which was Zcash, was not developed by a company, and instead the company, the electric coin company, that is the name of the company behind Zcash, which is a fun name as well, working actually with researchers at university to build it together, because no one will understand how. So Matthew Green, she's a professor in computer science, was actually one of the co founders, because this is very hard to implement. So they worked with the university to make it happen.
00:29:33.844 - 00:29:45.564, Speaker A: Let's bring all this theory back to reality. Zksync, starknet, Polygon, zkevm, arbitrum. These are the four hottest things at the moment.
00:29:45.604 - 00:29:50.812, Speaker C: NZK, scroll and linear from consensus. So everyone is bringing a ZKVM.
00:29:50.868 - 00:30:24.400, Speaker A: Now, two of them launched last week, Polygon, Zkubm as well as Zksync era. One of them launched their token two weeks ago, and that was arbitrum, which brought the entire chain down. That was a very typical crypto and there was some drama around the Dow. But anyway, everyone's talking about these things. What is the current state of these three or four ZK rollups and what is the trade offs that they are making between each other?
00:30:24.512 - 00:30:56.346, Speaker C: So, arbitram and optimism, just for clarity, are optimistic roll ups so they don't follow this moon math, Zika sync, Boligan, Ziki, VM and these are the two main net zcorolapse. So because optimistic roll ups were first to market, like arbitram is now what, one and a half years old? And like optimism is another about a year old as well. So they, so far they are number one and two in activity. Arbitrum number one. Optimism is number two. Ciao. If you can find the l two bit.
00:30:56.470 - 00:30:57.174, Speaker A: Yeah.
00:30:57.474 - 00:31:44.356, Speaker C: While you are doing that. So as we discuss optimism and arbitram and number one and two, Zika sync era and bullying, ZVM are the first general purpose Zika roll ups. They are not the first Zika roll ups in the sense, on the contrary, actually, ZK sync had a product launched two years back, which now it's called zklite, which is, which was a Zika roll up, but it only had payment functionality. It didn't allow general smart contract similar to loop rank, which has one functionality, which is trading, and many other limited functionality, they correlapse. So the innovation that happened last two weeks, that we finally have a mainnet that can have general purpose applications.
00:31:44.460 - 00:32:37.078, Speaker A: And by the way, the reason for that, so you said Zksync light was launched one or two years ago, which was very specialized ZK rollup. And the reason for that is because it's very difficult to write all these ZK circuits exactly for a general, for a set of general purpose, turing complete functions and programs, you're gonna have to write very specialized circuits for each like, quote unquote set or type of functions. And that's why it took them a long time to finish writing all these different circuits. But back to the original point. So the state of everything, right, like you mentioned, arbitrum optimism, which are the op, optimistic roll ups, as well as Zksync, Polygon, Starknet, which are the ZK rollups.
00:32:37.126 - 00:32:37.662, Speaker C: Yeah.
00:32:37.798 - 00:33:23.926, Speaker A: And today, if we look at the TVL of these various roll ups, we can see that arbitrum is by far the number one. Arbitrum is actually three times as big as number two, which is optimism in terms of TVL. So arbitrum has $6 billion in TVL, and optimism has around $2 billion. And it's also worth noting that on this list, you see Zksync era and Zksync Lite. So Zksync era and Zksync Lite. One of them is the general ZkVM and the other one is the specialized one. They both have around order of magnitude, $100 million in TVL.
00:33:23.926 - 00:34:08.898, Speaker A: But Zksync Arrow is the one that launched two weeks ago, and I'm actually very impressed with their growth so far. So I'm going to click into this so that everyone can see the growth of the TVL on Zksync. So this dot here, this green dot is March 24, which was the date where Zksync era was launched. So we start from zero, and it's been growing very consistently until today, to about $100 million in TVL. Very impressive growth. On the other hand, Polygon, Zkevm also launched around the same time. Right.
00:34:08.898 - 00:34:55.776, Speaker A: And Polygon is actually down here with only less than $3 million in TBL. Let's assume for a second that this data is correct, because I haven't, haven't been able to find any other datasets. So let's assume that this is correct. By the way, we talked about this many times on our podcast, which is that the quality of data on chain data in crypto is just not there yet. So we should always take everything that we see with a grain of salt. But anyway, let's assume this is correct. Polygon now has $3 million in TVL, which is 30 times less than Zksync era, although still it's been growing pretty consistently.
00:34:55.776 - 00:35:51.894, Speaker A: And actually, I was talking about this with some of our founders, and they were wondering why Zksync era and Polygon EVM, despite launching at the same time, one of them is growing much faster than the other. And they said they think it's because Zksync era, or Zksync in general, has been around for a much longer time. And so developers trust them more than the Polygon ZkeVM polygon, despite being around for a very long time. The ZKE EVM is their first project into ZK rollout, because previously, the polygon chain, it was basically a sidechain. It had nothing to do with ZK rollups. They think that people trust Zksync more than polygon. In reality, I think what's happening is, is something really dumb, which is the fact that Polygon has launched a token already, whereas Zksync has not.
00:35:51.974 - 00:35:53.554, Speaker B: I think it's as simple as that.
00:35:55.014 - 00:36:06.446, Speaker A: It's really simple as that. Like people are just pumping their assets into Zksync in anticipation of a future airdrop. And so people are really just farming.
00:36:06.470 - 00:36:08.954, Speaker C: The token arbitrage playbook, right?
00:36:09.254 - 00:36:47.070, Speaker A: Yeah. By the way, this is a really funny lesson for a lot of founders. This is actually a really good reason for not launching a token, because there's an anticipation for future token launches when you don't launch the token for the same reason. I think that, or at least for a similar reason, arbitram and optimism. Despite launching around the same time, Arbitrum has been well ahead of optimism. And I think part of the reason was because of the anticipation of token launches. Fuda, you seem unconvinced.
00:36:47.070 - 00:36:47.670, Speaker A: What are you thinking?
00:36:47.702 - 00:37:36.932, Speaker C: Arbitram launched quite before optimism. So by the time optimism came to mainnet, like, there was already a ton of activity on Arbitram. And if you remember back then when Uniswap was scaling to l two s, they communicated the plans to launch an optimism. But because optimism was not ready, they made a quick vote, and the community actually voted to launch Uniswap on arbitram first. So Arbitram was first mover, and honestly, at the technology level, zero is still the first mover because at least they implemented the fraud proofs, which optimism is still working on. So, yeah, I think also ZK Cenk, but so it's time to market, which also placed in the Ziggy sync Ziki EVM from Polygon. Zika sync was first to market.
00:37:36.932 - 00:37:58.804, Speaker C: They had actually a product. And if I'm a developer and want to deploy my protocol, I would likely go to Zika sync era because I will be the first mover there. I will get more attention on that. But if I'm considering polygon system, I will launch some bollinger us because that's where the liquidity is. So there are different variables.
00:37:59.624 - 00:38:47.316, Speaker A: Speaking of first mover advantage, I want to mention a quick anecdote which was mentioned by Anatoly, the founder of Solana. So Anatoly came to our cohort and spoke in front of our founders a couple weeks ago, and he talked about how Solana really out executed all their competitors in the 2020 era, despite having raised one or two orders magnitude less money than all their, like, do you remember, like, there was products that raised like $100 million at a $600 million valuation, like in the 2018 era without a product, whereas Solana, guess how much Solana raised. Solana raised like less than ten, that order of magnitude, like one or two order of magnitude less money than all their competitors.
00:38:47.340 - 00:38:52.620, Speaker B: And they got like 100 passes before they got a. Yes, I remember they got 100 passes.
00:38:52.772 - 00:40:01.390, Speaker A: And I actually know this from a lot of VC's who pass on Solana saying, so one of the VC's says, anatoly, you seem like someone who is a little bit too laid back for a founder, for entrepreneur. And there's other people who said, you only raised like $5 million. How are you going to compete with those who raise $100 million? So they pass on Solana for these reasons. But anyway, the story that Anatolians told us was that they outperformed, out, executed everyone else simply by being the first to the market. Like all his competitor, all the competitors, Solana was still building or was still on testnet marketing for like 100,000 tps, where Solana just didn't care, just launched the mainnet in 2020 and got some of the major market makers on board, including jump and FTX. FTX, obviously, in hindsight, was a bad, I guess, incident for Solana. But it's undeniable that FTX played a pivotal role in terms of bootstrapping the critical mass of developers in the early days of Solana.
00:40:01.390 - 00:40:28.090, Speaker A: So back to our point, the first advantage is actually ridiculously high. And same story with Polygon. Polygon, back in the day didn't have very good technology. It was AZK rollup. It was not even optimistic roll up, but it was the first to the market. And the marketed themselves as the scaling solution for the Ethereum community, even if they're not relying on security. The underlying trustlessness, assumptions of Ethereum in it.
00:40:28.090 - 00:40:30.210, Speaker A: Ethereum l one. But they were first to the market.
00:40:30.242 - 00:41:03.836, Speaker B: It goes back to Amir Haleem's quote, if you remember that Vinod Khosla gave him, which is like, if you're a startup founder and you're building in the space, your goal is to survive, no matter what. And if you survive long enough, then luck can play to your advantage in a way that'll help you become successful. And so I feel like it's also partly that, right? It's like, you know, if your first mover's advantage and you continue to stay and build long enough, luck will play to your advantage as you're, like, scaling up and growing your user base with. I mentioned recursion. And so what is recursion?
00:41:03.980 - 00:41:18.692, Speaker C: So you mentioned that Zika sync used snarks, which specifically planck and boligon Zika EVM uses. Actually both snorts and stuff. So you ask it a very valid question.
00:41:18.788 - 00:41:19.164, Speaker B: How.
00:41:19.244 - 00:41:56.774, Speaker C: How can you use two different proofing systems? Yeah, so Boligon ZkVM uses the concept of recursion, which is you take some data or some transaction, you generate proofs for each one, and then you create, you aggregate all these proofs and create a proof for the proofs, so you can create this recursive loop. So what ZK EVM is doing is that for every transaction they generate a stork, and like, then they aggregate these torques into another stork. So they have two layers of stork. But the problem with the stork is that it has larger proof size, right?
00:41:56.814 - 00:41:57.318, Speaker A: Yeah, that's.
00:41:57.366 - 00:42:20.438, Speaker C: We discussed this earlier. So as the final stage, they use a snark to prove the stocks, which voila, makes this proof size smaller, and they can submit this small proof on the l one. So, Boolean ZKVM has done really good tricks in engineering to do recursion between stocks and snarks, to be able to kind of check all the boxes by engineering.
00:42:20.486 - 00:42:41.310, Speaker A: So is this the same idea behind the so called layer threes? I don't know if you've seen this narrative, but it seems like layer threes is also recursive proof. So that the layer threes whose proofs to the layer two, that layer two verifies them and posts a proof of those proofs to the layer one.
00:42:41.462 - 00:43:07.286, Speaker C: Yes and no. So yes, the concept is correct. When you recurse, when you do recursive proofing, you can do that. But the fact, the fun fact is that you can actually do this in the layer two. Why do you need a layer three if you can do recursive proofing? So you don't need a layer three. So I think layer three is more to add more features that are not available in l two. Let's say Zksync now can do proving, right, but it doesn't support the privacy.
00:43:07.286 - 00:43:34.074, Speaker C: So what if you want to add privacy, you will not go and rebuild the l two. Instead you go build an l three that has privacy, and it will create the proofs that maintain privacy. And now Zika sync can actually roll this into their own proofs. But again, just to be clear, not all proving systems support recursion. So there is still a lot of work to be done to assess the viability of l three s is the.
00:43:34.114 - 00:43:43.394, Speaker B: Idea between this mashup as to what is it just for data compression? Like, why would someone want to, to use both Starks and Starks? Is it just primarily for data compression?
00:43:43.554 - 00:43:47.498, Speaker C: Yes. You mean your question is specific to Bollinger ZKVM, right?
00:43:47.546 - 00:43:51.082, Speaker B: That's right, yeah. Why the mashup of both? Is it just because if you use.
00:43:51.098 - 00:43:57.290, Speaker C: A stark alone, like if you. They will have very fast moving, but the proofs will be too big.
00:43:57.402 - 00:43:57.714, Speaker B: Oh.
00:43:57.754 - 00:44:11.580, Speaker C: Then you do another step of snarks to that, will increase your brewing time a little bit, but not by much, but will lead to a much smaller proof size. So that's why the mashup, and it's.
00:44:11.612 - 00:44:14.308, Speaker B: Only two minutes, right. To do the computation.
00:44:14.356 - 00:44:33.320, Speaker C: Yeah, they have done a really good job. So, like the most recent claims of Boligan team is that half a million of gas will be done of transactions will be approved within 2.5 minutes. And they are trying to push this to sub two minutes. So.
00:44:33.512 - 00:44:51.328, Speaker B: Okay, so that's a good understanding. So Zksync uses zksnarks and polygon. EVm, or Zkvm uses snarks and starks. Yes. One is EVM compatible, which is Polygon, and the other is, I believe, bytecode compatible. Right?
00:44:51.456 - 00:44:52.024, Speaker C: Yes.
00:44:52.144 - 00:44:55.424, Speaker B: Okay, so let's talk about the differences between the two.
00:44:55.544 - 00:45:04.540, Speaker C: Yeah, we haven't touched on the topic of EVM compatibility. Actually, Vitalik has a really good article here, and he created some of the terminology was which we are using. So talk about.
00:45:04.572 - 00:45:06.396, Speaker B: Yeah, what EVM is and then.
00:45:06.500 - 00:45:36.874, Speaker C: Yeah, so Vitalik defined four different kinds of z key EVM, or like circuits that can be EVM compatible or not compatible. Okay, so let's start. What is EVM? EVM is the ethereum virtual machine. So that's a virtual machine that can execute any code, but it was created with a specific syntax and specific logic. So you have a chance here. When you are trying to build the z core app, you have an opportunity. You can tweak your circuit to be exactly compatible with the evm.
00:45:36.874 - 00:46:10.960, Speaker C: So this is 100% zq vm equivalent or compatible. So Vitalik calls this type one, type one evm. On the other end of the spectrum, maybe when you try to do this equivalence to the evm, your throughboot gets affected. So instead you can build actually a very efficient z key circuit that are general. You can, let's call ZK VM. Not will drop the e here. So you have ZK VM that can execute any logic, but it's way faster because it doesn't tie to the, it doesn't commit to the EVM instruction.
00:46:10.960 - 00:46:35.652, Speaker C: And this can be very fast. So this is what Zksync went for. Zksync went for this and sorry stockinette specifically they went for this type four. They build their own circuits that are very general, but it can be very performed. It can have thousands of tbs. Like the target is at least 2000 tbs. But the problem is that this is not compatible directly to solidity.
00:46:35.652 - 00:47:17.214, Speaker C: Like developers who develop for solidity, they will have to learn a new language. So what they think done, and I think they have done great in this regard, that they created a lot of software layers or middle layers that can actually take solidity code, high level solidity code and compile it to the architecture that works for them to their own SQL circuit. So now actually you can have a solidity code that you wrote for ethanium before and deployed in the casync era. Yes, it's not compatible to EVM. So you cannot just have the byte code and run it in ZKPM. But the Zki sync error will take the solidity code and compile it to the correct architecture.
00:47:17.334 - 00:47:32.774, Speaker B: So we talked about the TVL, right? The change in TVL. Do you think builders are choosing Zksync because it's just much more faster? I mean, what's the throughput on polygon ZkVM? Is it three x faster than a layer two? A traditional optimistic layer two?
00:47:32.854 - 00:47:45.360, Speaker C: So Boolean ZkVM can have like hundreds of TBs. Like tbs in the hundreds, maybe a thousand. But I think Zika sync will always be faster because Z optimizes circuit for this reason.
00:47:45.472 - 00:47:47.112, Speaker B: Like how much faster?
00:47:47.248 - 00:48:09.436, Speaker C: I would say, like again, both are trying to improve, but I would say as of now it's like between around three x or three to x five x faster than Zkvm. But anyway, none of them actually are running at maximum throughput right now. So if we go back to the chart that Xiao showed up from l two p, like both of them are running at sub ten TPS.
00:48:09.580 - 00:48:10.860, Speaker B: So interesting.
00:48:10.932 - 00:48:13.500, Speaker C: The theoretical maximum is not there yet.
00:48:13.612 - 00:48:53.436, Speaker A: Right now if you look at this website called l two fees dot in fo, which is another pretty popular website that people look at, again, I don't know how accurate the data is. This one should be a lot easier than TvL to get read, but polygon zkevm, the fees on Polygon ZK EVM to send ETH is $0.23 right now. And to run the same transactions on Arbitrum one is $0.15. So arbitram is actually cheaper than polygon ZK. And then the data doesn't have ZK sync error, but it does have zksync lite. I suppose zksync error might be a little bit more expensive than Zksync Lite.
00:48:53.436 - 00:49:00.428, Speaker A: Fuda, what do you think? Is that the right intuition or because ZK sync error tries to do something more general purpose?
00:49:00.556 - 00:49:04.588, Speaker C: I think it will be in the same, around the same ballpark of Ziggy sync Lite.
00:49:04.716 - 00:49:20.756, Speaker A: Okay, so let's assume that it's same ballpark as ZK sync Lite. It's only $0.10. So ZK sync Lite today is cheaper than polygon ZK Evm, if that answers your question, Imran. But again, there's so many other factors at play here.
00:49:20.860 - 00:49:21.556, Speaker B: Yeah, agree.
00:49:21.660 - 00:49:37.782, Speaker A: The number of transactions that are happening today haven't reached the capacity yet. So that's one factor. The other factor, I think, Fuda, you mentioned this earlier, which is that the more transactions you run on a ZKVM, the cheaper it is for each transaction. Because there is a fixed cost, right?
00:49:37.838 - 00:50:21.006, Speaker C: Yes, it's a counterintuitive, because we always go to the mentality of like, more transaction means more competition for block space, which means, naturally, more fees, right? But because roll ups actually have to generate proofs, and these proofs are fixed cost. Like, once you send a proof, this proof can prove one transaction or can prove a thousand transactions, right? So the more transactions you put in the proof, the cost of this l, one cost will be amortized or be shared between this thousand transactions. So at a certain scaling limit for the curl ups, the more transactions you get, the cheaper the cost will be until you get to this limit, and then it will be kind of fixed cost from that from now on, my.
00:50:21.030 - 00:51:09.424, Speaker A: Takeaway from looking at this data is today, the layer two s are all in the same ballpark in terms of transaction fees, and they're all about five to ten times cheaper than Ethereum layer one. But they're all more expensive than Solana. This chart doesn't have Solana, but Solana transaction fees is basically zero. So even the Ethereum layer twos today are one or two orders magnitude more expensive than Solana. And again, I want to share the insight from Anatoly's talk. By the way, I'm not shooting Solana versus layer two, or vice versa, just sharing the things that we've heard. The insight as to why Solana works and is cheaper and is really, really cheap, is because Solana builds the parallelization into the VM.
00:51:09.424 - 00:51:51.124, Speaker A: There's built in parallelization, meaning when the developers write a Solana code, they have to, if they want to write it according to the rules of Solana, they have to parallelize some of the transactions. Whereas on Ethereum, the way Ethereum layer two scale is basically parallelization via l two s. So what that means is on Solana you have a bunch of transactions, but they are parallelized natively, whereas on Ethereum, you have a bunch of transactions, but they all parallelize because there's a bunch of independent layer twos. This is the insight behind Solana being really, really cheap.
00:51:51.244 - 00:52:35.244, Speaker B: Another interesting thing about Solana is they launched something called the account compression program. I don't know if you saw that, but essentially this program is used to. Right now, all the data is stored on chain. Now they have a compression program where you could store the data off chain, and then it produces a Merkle root or merkle tree that would be then stored on chain via. It's called like a fingerprint. And then whenever the smart contract has to call that data, it can use this account compression program to do so. And so an interesting experiment they launched recently was being able to mint a billion nfts within a minute and actually did it, which I thought was very interesting.
00:52:35.374 - 00:52:38.448, Speaker C: I think this is similar to the concept of validium, when you have the data of a chain.
00:52:38.496 - 00:52:39.088, Speaker B: Exactly.
00:52:39.176 - 00:52:54.216, Speaker C: And they can now reduce the cost of storing the data on a chain. But, yeah, like the interesting. The discussion on fees between Solana and l two s is interesting because Solana, like, the counter argument is that Solana doesn't have a fee market. Like, you cannot compete for priority in transactions.
00:52:54.320 - 00:52:55.760, Speaker B: I think they started a fee market.
00:52:55.872 - 00:53:09.772, Speaker C: They started, but it's not willing yet. So, yeah, they intentionally, like small fee for everyone, which, according to Anatolia, it was a minimal fee. Like, other than that, it doesn't work anymore. Under that, it doesn't work anymore.
00:53:09.828 - 00:54:03.690, Speaker A: Yeah, we talked about TVL and fees. There's another data set that I wanted to share, which is our alliance application data, because it's a pretty good indicator of where the developer activities are. So we said arbitrum is ahead of everyone else in terms of TVL, in terms of fees. Same ballpark. Maybe Zksync in theory, will be cheaper than polygon, and ZK rollups in general will be cheaper than optimistic rollups. But in terms of developer activity, what we observed over the last few months is that there was a very big event which was the collapse of FTX in November last year. And ever since that, we see that arbitrum and optimism are growing in terms of developer activity at the expense of Solana.
00:54:03.690 - 00:54:38.118, Speaker A: Now, the rate of change that the delta isn't that big. So it might be a few percentage increases for arbitrum and optimism and a few percentage decreases for Solana, but this is a trend we've seen so far. But still, arbitrum, optimism, Solana are in the same ballpark. They're still way ahead of everyone else. And Polygon is still also up there. So I would say today, polygon, Solana, you know, they're followed by Arbitran, which is also slightly ahead of optimism. But then after these four, there's a long gap.
00:54:38.118 - 00:54:54.978, Speaker A: There's a big gap between these four and the rest, including ZK sync, Starknet, and all the other alt layer ones and all the other up and coming layer twos. So this is what we're seeing on the developer activity side.
00:54:55.106 - 00:56:05.882, Speaker B: Well, I guess the question is, I feel like the crypto ecosystem, they swing in certain narratives, right? So there was a narrative early in the last bull market where everybody was building on Solana, and then layer twos weren't as robust as they are today. Now, because of the downfall, as you mentioned, and other infrastructure issues, the pendulum swung back to layer two s, and now everyone's building on layer twos. And people are now looking at Solana and saying, like, who wants to build on Solana anymore when we have all of these layer twos and just going through all of the. Just the infrastructure around our roll ups? You have roll ups, then you have data availability, you have elydiums, and the list goes on. Now, we have shared sequencers, which we can also talk about. And so the question then becomes, how much is too much for founders building in this space? And would founders then find the ease of use of using Solana much more, from an onboarding perspective, much easier versus going through the roll up space? We also have rollups as a service. You have all of these different complex components within the Ethereum side, and then on Solana is just very easy to be onboarded.
00:56:05.938 - 00:56:13.894, Speaker A: And then there is a whole story about the roll ups as a service becoming a real threat to cosmos. This whole space is very deeply intertwined.
00:56:14.014 - 00:56:21.598, Speaker B: Well, let's talk about rollups as a service. Okay. Do you want to run down what roll up as a service is?
00:56:21.726 - 00:56:22.366, Speaker A: Go ahead.
00:56:22.470 - 00:57:13.452, Speaker C: The concept is very simple, actually. It goes back to, like, maybe Ziki lite. Ziki Lite was like, Zika roll up as a service, essentially, but it's only for payments, right? It was cheaper, it was faster, it can be better, essentially. So the concept of Ziki, of roll ups as a service is that instead of using a general purpose roll up like optimism, arbitram Ziki sync or the Polygon Ziki VM, and competing with other application using the same system, why not go and build your own blockage? And your blockage in here is your own roll up that you own, all the resources you control, the sequencing of transaction, you have the whole space. And the examples that are left today, actually, like DydX, immutable, X, Rainify, all these are actually rollups as a service. And Storkware was the one that not.
00:57:13.468 - 00:57:16.380, Speaker A: Necessarily roll up as a service, but rollup as an app chain.
00:57:16.492 - 00:57:44.920, Speaker C: As an app chain. So, like, roll up as a service is just taking this step further by making some development frameworks that it's actually very easy to launch your own rollup. So there are many competitors now. Most of them are actually focused on optimistic ob stack. Some of them are focused on having ZK roll up as a service. But the concept is that a developer will. It will be five minutes integration work.
00:57:44.920 - 00:57:56.192, Speaker C: Five clicks and you get your own roll up with its own browser, with its own RPC node out of the box. This is the concept of roll up as a service.
00:57:56.368 - 00:58:36.388, Speaker A: Remember, almost a year ago, we said on this podcast that roll up as a service and cosmos, the two visions are converging. Indeed, today they're converging. And matter of fact, I think the roll ups as a service on Ethereum is a massive threat to the whole Cosmos ecosystem, because we know for a fact that there are several fairly mature defi protocols that were previously looking at Cosmos to launch their own app chain. Today, they're looking at a roll up as a service or roll up as an app chain to launch their own app chain on Ethereum, as opposed to Cosmos.
00:58:36.516 - 00:58:48.360, Speaker C: You can also have both. By the way. You can have a roll up as a service on top of Cosmos, like Celestia. So you can have everything if you want. As you mentioned, everything is interwind.
00:58:48.532 - 00:59:32.414, Speaker A: Again, back to Imran's previous point, like, how much is too much? Right? Like, this is way too much for new founders. Like, what the hell is going on? And how do you choose a chain to build on top of? I think there is actually a few simple rules or some simple trends that we're seeing. So roll up as a service is probably not something to think about when you're new because these are for mature products. Okay, roll up as a surveys and cosmos. They're all for mature projects, but we're seeing some trends between arbitrum and optimism. Again, these are things that you probably can find online, but things that we hear behind the scenes. Arbitrum seems to be the hub for Defi, the likes of GMX and Imran.
00:59:32.414 - 00:59:43.868, Speaker A: You've tried a bunch of derivative trading protocols on both arbitrum and optimism, and you can tell us which one is better. But it seems like Arbitram has most of the volume. At the very least, that's where the user.
00:59:43.916 - 00:59:59.564, Speaker B: Yeah, I mean, the volume on, well, both open interest and liquidity. The UI UX on products are just so much more robust on Arbitrum than it is on optimism. I've used a couple of products in optimism and I mean, it's okay, it's not amazing.
00:59:59.644 - 01:00:49.428, Speaker A: So Arbitrum is a defi hub. The defi roll up optimism seems to be attracting a bunch of gaming developers due to their, I guess, connections with some of the OG gaming projects and their friends, the Ethereum foundation, the Xerox Parc, Dark Forest, these guys. So it seems like a lot of new gaming developers are choosing optimism just because everyone else is choosing optimism. That's a very interesting trend. We're also hearing things about Starknet. It seems like some of the Starknet developers are a little bit discouraged by their migration towards Cairo 1.0, which is taking a while, number one, because they acquired a bunch of developers to build on top of Starknet.
01:00:49.428 - 01:01:32.854, Speaker A: But the current version of Starknet is just not very cheap. It's only a quarter of the fees of Ethereum layer one. They're going to migrate to the next version, which they call Regenesis, but it's taking a while. And then also developers would have to rewrite their whole code. So there are some talks there, some disillusionment in that side of the world. And then you have Polygon, which is, we mentioned before, the beast in terms of BD in crypto, they're by far the best BD team in the entire layer one, layer two space. But yeah, so these are some of the, I guess, data points that founders can think about when they think about choosing which.
01:01:32.854 - 01:01:59.748, Speaker A: Oh, and there's also Solana, which we talked about last time with Amir. And the reason why helium is picking Solana over any of the Ethereum layer twos is simply because Solana is two orders magnitude cheaper. There's simply no way for a proof of physical work project where a user earns, let's say $5 of tokens per day and having to pay $0.25 in fees, that just eats into too much of the margin.
01:01:59.876 - 01:02:28.806, Speaker B: He also made a direct comparison to AWS, if you remember. He goes, what's the point of me running my own validators, my own protocol, launching my token, et cetera, if I could just use something off the shelf similar to AWS. I think that's where the roll up as a service model is, I guess, going in terms of making it easier for any founder to build their app. Don't worry about the infrastructure, we'll take care of it. Just launch using our product seems to be the case for me.
01:02:28.870 - 01:02:44.550, Speaker A: It seems like the roll ups and the out layer ones are all specializing. That's the point I'm getting at. Or they may not be doing it intentionally, but some groups of developers and founders are gravitating towards one over the other.
01:02:44.702 - 01:02:51.050, Speaker B: But the issue with this obviously is cross roll up communication and fragmentation of liquidity.
01:02:51.122 - 01:02:56.466, Speaker A: I know where you're getting at the sequencers.
01:02:56.610 - 01:02:57.082, Speaker B: The sequencers.
01:02:57.098 - 01:02:58.442, Speaker A: Decentralized sequencer network.
01:02:58.578 - 01:03:24.006, Speaker B: Yep, there has been some talks. I just saw two announcements this just the past two days. One's called Astria. I forgot what was the name of the other startup. But essentially they're creating a way for sequencers to be decentralized within different roll up space. Now you have rollup as a service. I'm hearing this question a lot actually, which is we have a couple rollups as a service startups in our current batch.
01:03:24.006 - 01:04:16.702, Speaker B: One is called Caldera, which is specifically around the optimism framework, and you have Snapchain, which is primarily around the ZK framework. When we talk to these founders, they're like, well, our customers are asking us for our service, but they also want to understand, like how do they decentralize their sequencers? They don't want us to own the sequencers, obviously, and us to run the transactions, but they also don't want themselves to do it. They want their customers to know that these transactions are batched and being executed trustlessly. So how do we solve that, Imran? I mean, there's some trade offs that I've talked through, but now with the launch of Austria and others, they could solve that issue. And so maybe Fuda talk us through, why do we need decentralized sequencers, and how important is it for maybe cross chain comms and messaging and et cetera.
01:04:16.838 - 01:04:43.318, Speaker C: Let's start with a board sequencer. Why it's called the sequencer. Why is like the sequencer like, what is a sequencer? Yeah. So like in any blockage chain, you have the validator, which do two tasks. First to order all the transactions, and then to make sure they are correct. Right. In the l two s, you don't need the second board, you just need to order the transaction because the l one will validate them if they are correct or not, or do the consensus.
01:04:43.318 - 01:05:18.922, Speaker C: So that's why the terminology is different for the l two s. So sequencer is what? The entity that organizes the transactions of the l two s. So the problem is that decentralizing sequencer will come to the same problem that we have in the l one's, which is that you need to achieve consensus between different players, and this is too much work to implement. So most l two s have already their hands full with the development of their core technology, whether it's optimistic roll up or zk roll up. So every one of them is using a centralized sequencer, and the entity runs one server that organizes all the orders, all the transactions.
01:05:18.978 - 01:05:26.130, Speaker B: Isn't that an issue? Or is it just that we trust optimism and arbitrum and, like, we shouldn't have to worry.
01:05:26.202 - 01:05:35.102, Speaker C: It's an issue, but it's not on the heron fire issue. Right. You need to implement the product first. Right. You don't, you cannot go and build everything at the same time, because the.
01:05:35.118 - 01:05:46.150, Speaker A: Issue is, if the sequencer cheats, you can get your transactions blocked, censored, but you won't lose your money. Your money is still yours. So that's why it's not a hair on fire problem.
01:05:46.302 - 01:06:07.574, Speaker C: Exactly. So you can actually sometimes in arbitram, or like, you think you can actually claim your money back on the l one, even if the sequencers are dead or like off a chain, offline. So anyway, the problem with that model is very simple. And you mentioned, Imran, that every rop is its own universe. It doesn't speak to anyone. You lose composability. Right.
01:06:07.574 - 01:06:16.078, Speaker C: So optimism team actually came with this idea of, can we actually allow different roll ups to speak to each other and have atomic compensability?
01:06:16.246 - 01:06:18.478, Speaker B: What is it called? Bedrock? Bedrock framework.
01:06:18.566 - 01:06:21.798, Speaker C: Bedrock is the implementation of optimism as an OB stack.
01:06:21.886 - 01:06:22.514, Speaker B: Yes.
01:06:23.854 - 01:07:01.482, Speaker C: The optimism team came with this concept of subarichain. What's a subarachnoid? That everyone will build their own ob stack roll up or Ob roll up. But you will share the sequencer. Like a sequencer will be sequencing all these roll ups, and they can order transactions from different roll ups together so that you can actually send money from roll up one, receive it on roll up two in the same transaction. So that's why shared sequencer can allow actually this atomic compatibility. But as of now, optimism suggested AstraZenec idea. They haven't implemented it, but many teams, many smart teams found this idea and just run with it.
01:07:01.482 - 01:07:35.252, Speaker C: So Astria is one of them. Like they are trying to build this concept as to offshore sequencer, but so far they are not focused on Ethereum, just to proclaim. Astria is focused on using Celestia as data availability and focused on the civilian roll up space, which we didn't touch on today, but many other solutions. There is a company called Espresso which is trying to do this for Ethereum. So the concept is simple. We will have this network of sequencers, maybe all of them share a token and they will be slashed if they.
01:07:35.308 - 01:07:36.628, Speaker B: And yet another token.
01:07:36.756 - 01:07:45.834, Speaker C: Of course, like the concept of our space, you scale by adding more blocky chains and each blockchain is a token. So like more blocky chains, more tokens.
01:07:48.214 - 01:07:49.446, Speaker B: Wait wait, wait, wait.
01:07:49.590 - 01:07:58.754, Speaker A: The way our space scales is by creating more blockchains and the way we acquire more users is by launching more tokens. That's how space works.
01:07:59.614 - 01:08:08.714, Speaker B: So a roll up for a token, sequencer, token mainnet, token, data availability, token. Okay, got it.
01:08:09.794 - 01:08:14.534, Speaker C: So that Celeste and that availability is a token. Celestia has its own token. So like. Yep.
01:08:16.114 - 01:08:24.522, Speaker A: By the way, I think the only token that is really, really, really, really needed, absolutely necessary, critical to the product is the Ethereum layer one.
01:08:24.618 - 01:08:29.794, Speaker B: Yes, yes. I mean, really, you could probably run everything else without a token, right?
01:08:29.914 - 01:08:31.962, Speaker A: I mean, arbitram ran without a token.
01:08:32.098 - 01:08:35.494, Speaker C: That is a piece of agglayer. Iggy layer is trying to use the same token for everything.
01:08:36.404 - 01:08:45.772, Speaker B: Wait, we talked about all like roll ups, but we didn't really talk about data availability. And we mentioned this word a few times and so maybe this is a plane to it.
01:08:45.868 - 01:08:48.116, Speaker A: No, no, I mean, I really don't.
01:08:48.140 - 01:08:49.180, Speaker B: Want to go down this, by the way.
01:08:49.212 - 01:08:54.064, Speaker C: I don't mind. I can keep talking about that stuff for hours, but like, that's would be too long.
01:08:54.644 - 01:09:03.480, Speaker B: So there's two types of data availabilities, right? There's firms or companies that are building the space. Celestia and eganlayer explain to us what is data availability?
01:09:03.592 - 01:09:40.484, Speaker C: So we agreed that every roll up need to make that transaction data available because if the sequencer goes offline, how would we make sure what happened? What is the activity, what is the history of activities that happened on the, or the roll up? Right. So you need to make this transaction that available somewhere, right? So this is what is we called as data availability. Where is this data will be available. So if you decide to compare this data and put it on an l one, then you have this best kind of data availability. Because we assume that ethereum l one is always up, anyone can access it. So this is the best kind of data availability. But Ethereum is still expensive.
01:09:40.484 - 01:10:00.084, Speaker C: So other chains came like Celestia, saying we don't need to use Ethereum. Ethereum has execution and has storage. What if we create only a blockchain that only does a storage? It doesn't do execution. So Celestia as a concept is trying to do, we are building a blockchain just for that availability. Any roll up can just throw their.
01:10:00.124 - 01:10:04.708, Speaker B: Data at any layer one too, right? I mean, it could be any, it can be anyone.
01:10:04.756 - 01:10:15.132, Speaker C: But essentially it's made for layer twos, because layer two is where you need data availability. You cannot have an l one with its data, with its data sitting somewhere else. It wouldn't work. So l one s have to have their data.
01:10:15.228 - 01:10:15.972, Speaker B: Oh yeah, you're right.
01:10:16.028 - 01:10:37.870, Speaker C: But l two s can put their data somewhere else. So Celestia came with this concept of let's separate storage from execution. Celestia will be only full storage. Execution can happen somewhere else. It can happen in Ethereum as well. It can happen in something like fuel, it can happen somewhere else. But as a roll up, you will bust your data into Celestia and they give it a name, surveillance roll up.
01:10:37.870 - 01:10:45.374, Speaker C: And you can decide also to have this data availability layer on your AWS. And this goes back to the validium concept.
01:10:45.454 - 01:10:57.036, Speaker B: What I like about Celestia is that they have something called data availability sampling, where they actually like shard the data into different nodes. So if one node goes down or something happens, they can still retrieve the data.
01:10:57.180 - 01:11:21.010, Speaker C: Yeah, this is like more discussion about how to show also that, like the data is available. Like no one can withhold the data because in celestia you have validators who will maintain this data. And if you need access to this data, you need to access the validators to give you this data. How do you make sure that no validator can withhold part of the data? So the data sampling is an approach to reduce the dependency on certain layers, to withhold the data.
01:11:21.162 - 01:11:45.002, Speaker A: So in this vision, this separate data availability vision, you have three components. You have celestia or another DA layer. You have the roll up and you have the Ethereum layer one, then the layer two, the roll up will do the execution, the DA layer will store the transaction data. And the Ethereum layer one, what does it even do? Does it just verify the proof?
01:11:45.098 - 01:12:10.318, Speaker C: Exactly. It will verify the proof or achieve consensus that the execution was correct. So how would you make sure that, yes, that transaction data is fixed, is stored somewhere, but how we make sure that this transaction actually got executed? And what is the final state of that prolap after the execution? So consensus is actually validating the state or the state route. Like where are we after executing these transactions?
01:12:10.446 - 01:12:27.654, Speaker A: The trade off of this vision is making is trustlessness, right? Because now you're storing the transaction data in a new chain, a new DA layer. You're no longer relying on the trustlessness of Ethereum, so you add a new trust assumption.
01:12:27.774 - 01:12:34.520, Speaker C: Yeah, Vitalik has a really good argument against this separation of data availability. It reduces the security significantly.
01:12:34.662 - 01:12:35.196, Speaker A: Yeah.
01:12:35.300 - 01:13:00.804, Speaker B: So it's similar to like kind of the world we live in today with Amazon, right? You have database, you have compute, you have storage and all of the other infrastructure it provides. And similarly, it seems like with Ethereum rollups, data availability and all of the other things that are coming around, it's similarly like kind of like breaking that into its own core components and it has infrastructure behind that powers it. So that's very efficient for the end user. Is that the right analogy?
01:13:00.884 - 01:13:47.484, Speaker C: I would say it is, but the problem is that when you go to WS, all of this are offered as a services in the same platform, right? It's a clicker of the button. You commission a database, you get EC two nodes, you are doing all of that right now, you have to use different chains. Each chain requires payment in its own token. So there is a lot of developer friction. So you will essentially need over time, like platforms that abstract all of that, like just, or this is actually can be also the roll up the service, because you kind of try to abstract this complexity. So once you launch your roll up, we will take care of everything. We will use whatever availability layer that you want, we will pay them, we will use whatever consensus layer that you want and we'll pay them.
01:13:47.484 - 01:13:52.304, Speaker C: So this can be part or it can be specialized layer that actually platform as a service.
01:13:52.644 - 01:14:45.880, Speaker A: This whole data availability layer, as well as the previous thing, which was the decentralized sequencer network, these seem still very much far out. Like I've been speaking with people who are really in the ways of building these things, and they all tell me it's way too early. And so I think the reason why we're talking about these things is because you should be aware of the future. But these are not hair on fire problems yet. Like for example, like arbitrage and optimism are not even they're still running a centralized sequencer. But we want to talk about the future. What other visions of the future do you, do you see on the roll up side? Because one thing we talked about, which something we discussed recently and seems like you and I are on the same point, are on the same page, which is that Zk rollups will probably need to specialize in the long term.
01:14:45.880 - 01:14:48.684, Speaker A: Do you want to talk a little bit about what this vision means?
01:14:49.104 - 01:15:39.484, Speaker C: If this is space to grow and really get mass adoption, and people are using our products or this blockage in any everyday use cases, then the throughput of any l one or any general purpose l two will not be enough. So this is the thesis. Like if we got adoption, no matter how many l two s you will need. So eventually you will need every app or every protocol to run on its own space that can have full potential. It can have a thousand transaction per second at zero fee or almost negligible fees, which means that you need to specialize. So it is the same thesis as epic chains. Everyone will meet their blockchain essentially, but in some extent rollups reduce a lot of complexity around having your own chain because you can commission the security and data availability from someone else.
01:15:39.484 - 01:16:35.244, Speaker C: Our thesis here is that if you are running a successful product and you have a lot of users, you will try to specialize. You will have your roll up. And if it's a ZK roll up, which I think will be the standard of trust, because now it's correct by construction, you will have to optimize your ZK roll up. What does it mean? It means that instead of using a general purpose circuit, a general purpose circuit that can do any instructions, you will just create a specialized circuit or a specialized ZK circuit that will do this functionality only and they cannot do anything else. This will lead to much faster grooving time, much smaller proofs and lower fees for your users. And it comes and then comes a question about availability. Let's assume that the blue space will be implemented at this time and you will not be too much cost for the data storage.
01:16:35.244 - 01:17:01.446, Speaker C: So that means that if you are running an order box exchange or a blend chain game, then that requires at least 1000 dps. Then you can build this, you can have someone who will give you create the circuit for you and voila, you have your own space, you have the security of ethereum as an l two or even l three and your product is running in a decentralized fashion.
01:17:01.550 - 01:17:07.622, Speaker B: Do I hear CK circuit as a service? Yes, it's a joke, but that's in.
01:17:07.638 - 01:17:27.330, Speaker C: A sense what as they call up as a service should do on the long term. Now they can just use any tech stack available like Zika sync or polygon or whatever, but on the long term they will have to really go into the weeds and like ah, you need this application. Here is a circuit that does that and only that.
01:17:27.482 - 01:17:55.708, Speaker B: So the Zk circuit communicates with the proving system, right. And is the idea, different proving systems would allow different types of Zk circuit implementations and that would then be the types of applications that would be in need of those types of guarantees. An example of this trustless setup versus trusted setup, one being proving speed or the cost of storing the data, et cetera. Is that the idea?
01:17:55.876 - 01:18:13.882, Speaker C: Yes, and we're already seeing this. We have a company that is building a Zkvml. ZK machine learning, right? There's roll up or like l two will focus on executing machine learning models. So you will have kind of AI on a chain, which is a very.
01:18:13.938 - 01:18:33.120, Speaker A: Special type of compute, which is a bunch of matrix multiplications versus a general process EVM, which can do all sorts of things. Right. So in that case, it's basically a roll up for this, a bunch of matrix multiplications. And they would write very specialized circuits for that kind of computes.
01:18:33.312 - 01:18:38.008, Speaker C: Yeah, this circuit will not be useful for anyone else, but it's very, very useful for this use case.
01:18:38.176 - 01:18:57.012, Speaker A: Yeah. And there's also like actual production examples like loopring. You mentioned order book based decentralized exchanges. They will probably need some kind of specialized ZK circuits at some point versus taking it from starkware. Right. Like as a white glove service. Right.
01:18:57.012 - 01:19:06.212, Speaker A: So point being, we think that ZK rollups will specialize over time in order to provide faster and cheaper computes.
01:19:06.348 - 01:19:28.430, Speaker B: Would that mean then rollups as a service could be threatened? Because I feel like every app will want to have. I mean, if the edge is providing a quality product with UI UX speed, all of that would then the idea then mean that founders should be thinking about ZK circuits? Or is this the future? And rollups as a service is just a short term hair on fiber problem.
01:19:28.542 - 01:19:45.062, Speaker A: Rollups of the service, they can aggregate all these different specialized roll ups, ZK roll ups as well. So it's a layer on top of the, a bunch of different roll ups, including optimistic roll ups, including ZK rollups, including like specialized ML roll up, mlzk et cetera.
01:19:45.158 - 01:19:50.194, Speaker B: Got it. Cool. Well, I think we've covered a lot today. Any final words?
01:19:50.534 - 01:19:53.710, Speaker A: I feel like we can easily break this down into, like, ten episodes.
01:19:53.862 - 01:19:55.150, Speaker B: Yeah, maybe we should.
01:19:55.262 - 01:19:59.958, Speaker A: We should. At some point, we could spend a whole episode, just talk about ZKmL.
01:20:00.046 - 01:20:00.422, Speaker B: Yep.
01:20:00.478 - 01:20:02.022, Speaker C: Yep. That would be fun.
01:20:02.158 - 01:20:09.902, Speaker B: Great. Well, thank you so much for joining us. Hit subscribe if you haven't. We'll talk to you guys soon. Thanks for listening to. Good game. Don't forget to subscribe.
01:20:09.902 - 01:20:10.854, Speaker B: We'll see you next week.
