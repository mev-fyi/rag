00:00:00.240 - 00:00:16.865, Speaker A: And when I read the math behind zero knowledge proofs, it took me a while, like a few full days of staying in my man cave and no distraction and I still couldn't understand like 80% of it. So all that to say this is really at the bleeding edge of computer.
00:00:16.905 - 00:00:20.377, Speaker B: Science and maybe let's go a little bit deeper just for fun.
00:00:20.481 - 00:00:23.485, Speaker C: Okay. Fun for me not.
00:00:25.625 - 00:00:56.677, Speaker B: Welcome to Good Game, a podcast for crypto insiders with your host Imran and Chow. All right, welcome to Good Game. Today we're going to be talking about roll ups, specifically zero knowledge roll ups or ZK roll ups and optimistic roll ups. And today we have our a guest who also works with us. He's our head of research, Mohammed Fuda. And Mohammed Fuda, his background is quite deep. He did his PhD in electrical engineering.
00:00:56.677 - 00:01:19.109, Speaker B: He's worked on research within alliance and also has worked with me on the research side for other venture funds. His background is great because he understands and has built products both from a technical perspective and then also help a lot of our founders in regards to how to think about roll ups and kind of their technical strategies moving forward.
00:01:19.197 - 00:01:33.273, Speaker A: Just add to that. Futa is also a former founder himself. Basically, all the things you guys have ever heard from me and Imran on the technical side that are interesting and insightful pretty much all come from Futa.
00:01:33.369 - 00:01:54.297, Speaker B: That's right. And today we bring him on to discuss rollups and how should founders be thinking about the types of rollups there are in the space, along with why roll ups and why L2s. And finally to talk about how does zero knowledge or zero knowledge proofs fit into this entire roll up space. And before we get started, Chow, do you have anything to add?
00:01:54.401 - 00:02:44.615, Speaker A: I really want to share some of the empirical data that we've been seeing, both the public data as well as the private data that we have. Remember 10 episodes ago we talked about the competitive landscape of the various layer ones and L2s and we shared a bunch of data that we have that typically people don't have in the public, which is the alliance applications data. So we're able to see sort of the live real time developer activity of various ecosystems. And we said that these datasets tend to be really good leading indicators of how the layer ones will perform in the near future. So some of this data, some of the behind the scenes, you know, comments we've heard from founders, I'd love to share those.
00:02:44.655 - 00:03:50.385, Speaker B: And maybe just to add, we're starting to hear more about like rollups as a service and so you Know, should founders be thinking about deploying their own roll up or app specific rollup versus on a general roll up chain? And so just to get started, what is rollups? How did it get started? Well, I think it started around. You could, it could be traced back to 2014. Joseph Poon and Thaddeus worked on Lightning Network, which is a scaling solution for Bitcoin. And the idea was similar which is like could we allow more block space to be used for other types of activities while we do general transactions on a L2 like service? And so the basic concept of the Lightning Networks was first started. And as Ethereum started to gain popularity around 28, a person by the name of Barry Whitehat introduced something called the zkrollups. And the idea was could we use zero knowledge proofs as a way to offer succinct transactions in such a way that would reduce the bloat for the layer 1 Ethereum chain.
00:03:50.505 - 00:03:51.633, Speaker A: What year was this?
00:03:51.769 - 00:03:58.545, Speaker B: This was in 2018. This was around the last bear market if you remember. OMG Network and. Or Plasma group.
00:03:58.665 - 00:03:59.441, Speaker A: Yeah, yeah, yeah.
00:03:59.513 - 00:04:07.441, Speaker B: They had raised via ICO at least. OMG raised via ICO to offer. I believe it was first state channels. Right. Correct me if I'm wrong, Fuda.
00:04:07.473 - 00:04:07.657, Speaker A: Yeah.
00:04:07.681 - 00:04:09.875, Speaker B: Or was it plasma channels? Sorry, plasma channels, yeah.
00:04:09.915 - 00:04:38.839, Speaker C: There was many competing solutions proposed at this time. There was steady channels, there was plasma, Plasma was a hot thing back then. And then everything was kind of put to the shelf to scale Ethereum through sharding. So Ethereum 2.0, yes, but and Ethereum 2.0 itself, it changed to this like kind of skip sharding completely and like focus on rollup as a rollup centric view. So yeah, like it's rollup came as a series of evolvement of different ideas.
00:04:38.947 - 00:05:02.975, Speaker A: If I recall correctly, optimistic rollups also came around the same time, the same era. And it just so turns out that all these state channels, plasma, even sharding on the layer one, these never came to fruition or at least widely adopted. And you know, it seems like over time the two remaining scaling efforts are ZK rollups and optimism roll ups.
00:05:03.095 - 00:05:23.873, Speaker B: Yeah. So a group by the name of Plasma group or Plasma, which was led by Jing and a couple other team members, which is now called optimism, started with optimistic rollups or optimism. And so what is rollups? Maybe Fuda, if you want to give a quick primer on what are roll ups and why do we need them in the space.
00:05:23.969 - 00:06:38.429, Speaker C: Okay, so like the rollups and plasma and side exchange also share the same philosophy which is that you have limited capacity on the L1. The L1 cannot have many transactions, it has limited throughput. So the concept here is let's take some of this transaction and execute them off a chain or in a different chain that we can actually be faster, we can have more capacity. And once we execute this transaction on off a chain, let's tie them back to the L1 so that no one can steal this money when you take it of exchange. So the roll up is simply like you take a bunch of transactions, you execute them in a roll up and the roll up executes transactions and do all the activities that you want to do. But every block the kind of pushback of proof in the case of ZKrollaps and you also push a compressed version of the transaction data back to the L1 and then you can actually scale Ethereum this way. And today actually like we achieved this vision because rollups now actually have 3x the throughput of Ethereum like that universe of rollups in general have 3x capacity of Ethereum itself.
00:06:38.517 - 00:06:40.845, Speaker A: It's not very much to be honest. 3x.
00:06:41.005 - 00:07:16.191, Speaker C: Yeah, like we just, we are just starting like we have the first two ZK rollups launched in last two weeks. So like we are very, very early in this game. But according to Vitalik's view, this will keep growing and Ethereum eventually will be just a layer to settle up the proofs that are post from the L2s. So Ethereum may not be even used for smart contract anymore or like for applications like Uniswap. Everything will move to the L2 and Ethereum in the long term will be the settlement layer for this all L2s.
00:07:16.263 - 00:07:47.785, Speaker B: And just to double click on what Fuda mentioned around what roll ups are, roll ups are essentially a way for transactions to be computed that isn't using the layer 1 resources. And what it does is once it computes, what it does is once it's finished, it sends the verification proof and it'll submit it to the Ethereum layer one. So then Ethereum layer one knows that this transaction has been completed off chain and the result of that would then be published so that Ethereum layer one can verify that transactions have been completed.
00:07:48.155 - 00:08:00.747, Speaker A: Just to dive a little bit deeper on that. So basically the way that the layer 2 interacts with the layer 1, you said two things, one is posting the proof on chain, right? And the other one is posting a compressed set of transactions on chain.
00:08:00.851 - 00:08:02.403, Speaker C: Yes, that's correct.
00:08:02.579 - 00:08:10.459, Speaker A: And that will incur some cost on the layer one, right? Because These two things need to be stored on the layer one. Between these two things, which one incurs.
00:08:10.507 - 00:08:41.158, Speaker C: More costs as of now, the data storage. Because the amount of data you store to the L1 is higher than the amount of compute, you need to verify the proof. And something like optimistic roll ups doesn't have actually the cost of validating the proof. So in optimism for example, all the cost comes from storing the data. So an optimistic relapse, it's only data cost. So yes, here's the largest factor. That's why Ethereum community is trying to move to this blob space to save reduce the cost of this data storage.
00:08:41.286 - 00:09:09.869, Speaker B: Yeah and maybe we can touch on like data availability layer blob space which is EIP4844 a bit later and we can go into like Celestia and Egan layer. In regards to just the roll up space, there's optimism and there's arbitrum. And so in terms of infrastructure they're a bit different. And so before we dive deep, what is the difference between an optimism or optimistic rollup and a ZK roll up?
00:09:09.957 - 00:09:29.109, Speaker C: So let's start by what Chau mentioned here. You have two things. You have to have to send a proof to the L1 and you have to store the data. So you have two variables. Actually changing these two variables will give you three kinds of scaling solutions. Optimistic roll ups, Zika roll ups and Validians. So I made it even more confusing.
00:09:29.109 - 00:09:52.267, Speaker C: So in optimistic roll ups you assume that everything is correct by nature, you are optimistic. So you post the data like from the execution data from the L2 to the L1 and you say we are optimistic, it's correct, but we will have to give some time to make sure that if something wrong happened, we will have to fix it. So this is called fraud proof.
00:09:52.371 - 00:09:54.715, Speaker B: And what do you mean by when something is wrong?
00:09:54.835 - 00:10:15.629, Speaker C: The sequencer or the entity that is responsible for ordering the transaction on the L2 and posting this data on the L1 can cheat, can either block some transactions, can front run or like or can even change the execution and say yeah this user sent me all this money, something like that and submit something like that and then.
00:10:15.717 - 00:10:16.549, Speaker A: Or double span.
00:10:16.597 - 00:10:51.627, Speaker C: Yes, exactly. So any kind of attack that can be done by sequencer can be challenged using the flow proofs. And this is the first kind. We are optimistic. But there is a way to recall a wave recalls the other way, try to avoid that which is like ZK proofs or ZK rollups say no, we will use the math like very specific kind of math, zero knowledge math to be correct by construction. Which means that once we execute the transactions, we use zero knowledge to create a proof. This proof cannot be generated unless all this transaction were executed correctly.
00:10:51.627 - 00:11:21.399, Speaker C: And once we have this proof, we will send this proof to the L1, store it in the L1 so such everyone can validate it and then also post the data. So the Core Labs are the most secure because it shares a proof of correctness, which is a proof and it also has the data availability on the L1. So validium is different from securelaps because they try to reduce the cost but also reduce the security. So they send only the proof to the L1, but they store that data somewhere else.
00:11:21.517 - 00:11:23.435, Speaker B: And this could be a centralized server, right?
00:11:23.515 - 00:11:38.595, Speaker C: Yeah, AWS, IBFs, it doesn't matter. But it's not in the L1 anymore. So this can reduce the cost significantly, but has less security guarantees than Zeke roll up. So in my opinion, Zeke Rollups is the most secure scaling solution for Ethereum right now.
00:11:38.675 - 00:12:05.037, Speaker A: It's the most secure. But also given that optimistic rollups need a seven day or a week of fraud challenge period, it's also probably comparatively the best user experience because with the ZK rollup, when you withdraw your money from ZK rollup back to the Ethereum layer 1, you don't need to wait that long, whereas on an optimistic rollup you do need to wait a long time.
00:12:05.101 - 00:12:19.105, Speaker C: Exactly. But at the same time it's very complex to build a ZQ rollup, right? So like, so you take the effort on the engineering side, you make something very complex but has better user experience, which is shorter withdrawal delay.
00:12:19.225 - 00:12:32.937, Speaker A: But then what is the trade off there? Because right now we've mentioned two advantages of ZK roll ups compared to optimistic roll ups, right? That the trustlessness and the wait period. But then what's the downside? There's got to be a trade off somewhere.
00:12:33.041 - 00:13:14.831, Speaker C: The trade off is complexity of building a secure ZQ rollup because you will have to go into very specialized kind of math and you will have to construct what we know as ZK circuits, which we may touch on today as well. So it's a very complex process, very tedious, it has a lot of security assumptions. This is one. The other thing is that you need to actually optimize the proof. Like the proof is you have additional penalty here, you post another proof to the chain and you need to actually do some computation to validate this proof. So this proof has to have certain parameters. It has to be small in size because you pay for storing on the L1 it has to be easy to verify this proof.
00:13:14.831 - 00:13:21.559, Speaker C: So this is the main downside for zkorops, the proof part that you need to validate this proof and store it.
00:13:21.647 - 00:13:53.375, Speaker B: And just to add some context, there are different types of proving systems, right. As you mentioned and these proving systems are essentially ways there are different trade offs in regards to how they compute the zero knowledge proof and then being able to store it onto let's say a layer one. There are probably like what, five different or popular zero knowledge proving systems. There's zk, snarks, right as one, there's pickles, there's airstarks, Plancks and yeah, I think those are the ones. Right?
00:13:53.455 - 00:14:59.117, Speaker C: Yeah, actually like there is now because ZK since the invention of proofs or introduction into practical environments in Zcash in 2016 actually ZK0 knowledge is a very hot area of research and literally every few months we come we find a new paper that implements a new kind of proving system. So as of now we probably more than 10 popular and in production proving systems and it will grow from here. It's not going to consolidate. But as a large categorization we have Snark, succinct non interactive proofs arguments of knowledge and the Stork which is again scalable transparent arguments of knowledge. So these are two major categories. Snorts started by from zcash developers and Starks started from the founders of starkware and from there people have iterated on each side and Snarks have now many implementations. They have brute 16, they have prong, they have Halo, they have Blonkish, they have as you mentioned, Beckles.
00:14:59.117 - 00:15:03.905, Speaker C: So there are so many variants that we can spend hours speaking about every one of them.
00:15:04.265 - 00:15:25.057, Speaker B: So the way this would work is just as you mentioned, if I'm using a zero knowledge proof or sorry a zero knowledge roll up, I would submit a transaction. This transaction would then go to a proving system which would verify the transaction, then generate a proof using the proving systems that we just talked about.
00:15:25.241 - 00:15:25.841, Speaker C: Yes.
00:15:25.953 - 00:15:34.243, Speaker B: And then it submits the proof or the end of the computation results onto the Ethereum layer one as an example.
00:15:34.379 - 00:16:14.945, Speaker C: Yes. So this is the main concept of proving. Like that you have a complex process called proving and you have a simple process called verification. The proving takes a lot of computations and do a lot of math on them to create a very small proof. And once you have a small proof, anyone can do very little computation to make sure that this computation that you proved is actually have been done correctly. So yes, for the roll ups, the proving is done by the rollup itself. They do a lot of computations of each chain to generate this proof, but once it's generated, they post it to the L1 and the smart contract in the L1 will verify this proof to make sure that L2 execution was correct.
00:16:15.025 - 00:17:11.215, Speaker A: And by the way, for our listeners, if everything we talked about in the last five minutes sounds confusing and very abstract, it's not that you're stupid, it's just that this stuff is extremely at the bleeding edge of mathematics and computer science. I Remember back in 2016 or something in that era, Zuko, who is the founder of zcash, said that zero knowledge proofs, or ZK Snarks is moon math. And the very few people in the world actually understands how it works. And including Zuko himself, he said he didn't understand the math behind his own creation of zcash. So if everything we just talked about sounds confusing, it's not the end of the world. I personally have a background in pure math. I studied a bunch of number theory, group theory, all that stuff in college.
00:17:11.215 - 00:17:30.685, Speaker A: And when I read the math behind zero knowledge proofs, it took me a while, like a few full days of like, you know, staying in my, in my man cave and no distraction and I still couldn't understand like 80% of it. So all that to say this is really at the bleeding edge of computer.
00:17:30.725 - 00:17:34.157, Speaker B: Science and maybe let's go a little bit deeper just for fun.
00:17:34.261 - 00:17:36.905, Speaker C: Okay, Fun for me.
00:17:39.335 - 00:17:55.743, Speaker B: Yeah. And Fuda, you and I have chatted a lot about the differences between the different proving systems. Let's talk about the two most popular ones, right? ZK Snarks and ZKStarks. What are the differences between the two? And why would I want to choose one over the other?
00:17:55.839 - 00:18:34.895, Speaker C: Okay, so to understand the difference, let's actually start by discussing the characteristics of a proving system. What do we need for a proving system? Like how can we judge this proving system is better than the other? Right. So there are three main characteristics. One is that you try to. The proving is complex already, right? So try to make this proving process a little bit faster because while you are doing the proving, your customer is like the verifier is waiting for the proof to be generated, right? So you try to make the proving process faster. The second part, you need to store this proof somewhere in the roll up case. You need to store this proof on L1 and you will pay more if you are storing more data.
00:18:34.895 - 00:18:54.895, Speaker C: So you try to make succinct. You need to Create a proof that is very small in size. Right? So this is the second characteristic. The third characteristics are, is that you need to verify this proof. You need to do some computations to verify this proof. And this computation has also it consumes gas on the L1. So you don't, you won't make this verification process very simple.
00:18:54.895 - 00:19:32.575, Speaker C: So these are the three main characteristics of my opinion about a broken system. There are many other characteristics like trustlessness and like trusted setup or not trusted setup, programmability, privacy, even if you are dealing with private information. But let's just focus on the three main characteristics. So at a high level, snarks are very efficient in creating succinct proofs. They can create very small proofs in size, a few kilobytes, and they can be verified very quickly. So that is the main advantage of Snorks. Earlier version of Snarks like zksnarks that was developed by zcash was not required something called trusted setup.
00:19:32.575 - 00:19:57.521, Speaker C: That initial group of people will come together to create some secret randomness. And this not very favorable. It adds a security requirement. So that's why it wasn't very popular in recent versions of Snarks. Actually like block or hello doesn't require this trusted setup anymore. So it improved. But the problem with snarks is that they require a lot of proving.
00:19:57.521 - 00:20:26.025, Speaker C: Proving complexity is too high. On the other hand of the comparison stocks have two main advantages. The proving can be very fast and they by nature trustless. They don't require trusted setup. Their main issue is that they require they generate larger proofs, maybe hundreds of kilowatts. So the cost of that is higher to store this as higher. So this is kind of very high level comparison between snarks as a family of algorithms and stores.
00:20:26.145 - 00:21:07.875, Speaker B: And just to add a little bit more context, you mentioned trusted setup. And a trusted setup is essentially a way to deploy a proving system that is untampered with. Because the problem here, which could be the attack vector for a malicious party, is to set up a proving system that could be altered so that you could change the transaction in a way that would be beneficial for the malicious actor. And so what you need to do in this case is set up a trusted setup, which would allow a party of people that come together that generate a random code. And the random code has to be verified between the parties. Once it's aligned, then the proving system would then get deployed.
00:21:07.955 - 00:21:11.979, Speaker C: That's a very good summary of the task process. It's essentially creating a secret.
00:21:12.147 - 00:21:16.419, Speaker A: Was this generated by gpt4 we wish.
00:21:16.547 - 00:21:23.533, Speaker C: It would be super fun to. But changing would give you the output. The secret writing like here is a secret.
00:21:23.669 - 00:21:37.693, Speaker B: Yeah, but yeah. So we have the understanding of the proving system, we have trusted setup. And then maybe one more area that we hear a lot about is ZK circuits.
00:21:37.829 - 00:21:56.761, Speaker A: So people hear the words ZK circuits and also the prover and the verifier. Like there's all these terms that people tend to be confused about. Let's define those futa just so people have enough background knowledge to understand further literature in this space. What are ZK circuits, prover and verifier.
00:21:56.953 - 00:22:32.135, Speaker C: So the simplest way to explain a ZK circuit we'll have to tap into your memory from high school algebra and remember something called equations. When you have x plus y equals 0 or x squared minus 2, x equals 0, something like that. So this is an equation a ZK circuit is. A very simple explanation of ZK circuit is that you have a bunch of these equations, like millions of them. And all of them have to be consistent. Once they are consistent, you can actually generate a proof that all these equations were consistent. That is as what Tzatziki circuit is.
00:22:32.135 - 00:23:09.849, Speaker C: It's a bunch of mathematical representation or arithmetic equations that that have equalities. And you have to make sure that all these equations are satisfied. And once they are satisfied, you can actually create a small proof showing that. So why the RZK circuits are useful? Because using math you can create a small proof that will be equivalent to validating millions of equations. So this is very simple, like as a key circuit. So how we use them now? So now we use smart contracts, programs or any code. You can develop code and rust, solidity, whatever.
00:23:09.849 - 00:23:51.763, Speaker C: So you have a smart contract which may be hundreds of lines of code. So the advancement happen that we can actually convert these hundreds of lines of code in any in high level programming languages like Solidity and using some arithmetization or like some arithmetic tricks, convert this code into a bunch of mathematical equations. Once you do this, then you have converted your code which cannot be proven. You cannot make sure that the code has been proven in its original format. So you perform a kind of transformation. You convert the high level code into a bunch of mathematical equations which we call ZK circuit. And now voila, ZK circuit, this can be proven.
00:23:51.763 - 00:24:01.685, Speaker C: So this is what most of the work happens, like this conversion process, converting the equations or the code into a circuit.
00:24:01.765 - 00:24:09.021, Speaker B: So zero knowledge. So ZK circuit is essentially like a middle layer that communicates between the prover and then the application that's behind it, right?
00:24:09.093 - 00:24:24.663, Speaker C: Once you have the ZK circuit, once you have this bunch of equations, the proverb comes in, the prover comes in to make sure that all these equations are consistent and then gives you the proof. So the prover actually acts on or works on the ZK circuit.
00:24:24.759 - 00:24:27.519, Speaker B: Got it. So let's wrap this all up.
00:24:27.647 - 00:24:29.075, Speaker A: Imran, you look confused.
00:24:29.415 - 00:24:47.915, Speaker B: Not confused. I just want to make sure that our listeners understand all of these different components and how they all work together from end to end. So maybe Fuda walk us through. Like let's say I submit a transaction, I'm a user and I just submitted a transaction. I'm yoloing into some shitcoins.
00:24:49.095 - 00:24:51.031, Speaker C: Okay, don't do that.
00:24:51.143 - 00:24:55.639, Speaker B: Explain to me what happens in the background within a ZK rollup.
00:24:55.687 - 00:25:46.277, Speaker C: Yeah. So the beautiful thing about ZK rollups is that this, all this Moon math in Zoku's terms is very abstracted and hidden from the user. So someone, some clever teams of engineers have done insane amount of work, right, to abstract all this complexity away. So what happens now? Let's take, I will just take ZK Sync Era as an example, which is the new product of Matter Labs. So Zksync actually and the engineering team at Zync have done an amazing job building a ZK circuit that represents the Ethereum virtual machine, the evm. So now it's a virtual machine, which means it can execute anything, right? So they build that as general enough circuit to execute essentially any Solidity code, or any code for this matter. It's a Turing complete circuit essentially.
00:25:46.277 - 00:26:12.185, Speaker C: So now this circuit sitting at the back end on a server at zksync. And now developers will come and build smart contracts using Solidity or even reuse the smart contracts from Ethereum. Someone can get the uniswap code and deploy it. Done. So you are the user. You connect your wallet, point it to the Zksync era network. You use Metamask as usual.
00:26:12.185 - 00:26:39.745, Speaker C: You send the same Ethereum transaction that you were going to send on the L1, but now you send it to the L2. After that the magic happens, but it happens somewhere else. The magic happens that this solidity code will be converted to the equivalent circuit. This circuit will be proven Zksync servers originally. The proof, the proof will be posted on the L1. The smart contract in the L1 will validate it and done. As a user, you didn't see any of this happening.
00:26:39.745 - 00:26:41.897, Speaker C: You interacted with, you did a bunch.
00:26:41.921 - 00:26:45.481, Speaker A: Of Moon math, but you don't know what you did. That's the magic behind exactly.
00:26:45.553 - 00:27:18.363, Speaker C: Like Ciao. This is exactly chatgpt, right? You go type a prompt, right? Do you know what's happening under the hood? Does anyone know what's happening under. Does anyone understand the transformer model? No. That is the beauty of engineering, that you build very complex engineering systems and just abstract it. The user doesn't know anything about it, but it works beautifully. So in this case, like AI and ZK are very similar. Like you can connect to ZKsync or like Polygon ZKVM submit your transaction.
00:27:18.363 - 00:27:27.753, Speaker C: Very similar to Ethereum and the Moon Math happens and you get more throughput, you get lower fees, but you don't know what happened under the hood.
00:27:27.889 - 00:27:29.273, Speaker B: I love the term Moon Math.
00:27:29.369 - 00:27:30.729, Speaker C: It's cool. Yeah, right.
00:27:30.897 - 00:28:39.115, Speaker A: By the way, Vitalika wrote an article, I think a few months ago, he actually talked about this Moon Math term and where it came from and he said that yes, a few years ago it was indeed considered Moon math back in 2016. But at the pace of innovation in the whole ZK space, like ZK is moving so fast and there's so much research going on and there's more and more people who now have the knowledge to understand it and have the expertise in such a way that ZK is almost no longer a Moon math because so many people understand it today. And this has been only what, seven, eight years since the first application of ZK in crypto. And Vitali says something to the effect of if you have quote unquote intermediate knowledge of math, you will be able to understand zero knowledge proofs. I assume that intermediate knowledge of math means you have a undergraduate degree in pure math and you've taken at least first year, second year course in some kind of number theory, group theory Galois.
00:28:39.155 - 00:29:32.945, Speaker C: Like these courses, it was one worth because actually like the roots of zero knowledge proofs and cryptography was like a very niche in computer science department and because it has very hard cryptography, no one touched this except people who are like really, really deep in math. So like it was like the geeks of the geeks who like zero knowledge proof and the funny. Here is a funny story. The first implementation of zero knowledge proofs, which was zcash was not developed by a company and instead the company, the Electric Coin Company, that is the name of the company behind zcash, which is a fun name as well, working actually with researchers at university to build it together because no one will understand how. So Matthew Green, she's a professor in computer science, was actually one of the co founders of Zika because this is very hard to implement. So they worked with the university to make it happen.
00:29:33.885 - 00:29:45.509, Speaker A: Let's bring all this theory back to reality. ZKsync, Starknet, Polygon, ZKEVM, Arbitrum, these are the four hottest things at the moment.
00:29:45.557 - 00:29:50.813, Speaker C: NZK, scroll and Linear from Consensus. So everyone is bringing a ZK VM.
00:29:50.869 - 00:30:24.409, Speaker A: Now two of them launched last week, Polygon, ZKEVM as well as ZKsync era. One of them launched their token two weeks ago and that was Arbitrum which brought the entire chain down. That was very typical crypto and there was some drama around the Dow. But anyway, everyone's talking about these things. What is the current state of these three or four ZK roll ups and what is the trade offs that they're making between each other?
00:30:24.537 - 00:30:56.359, Speaker C: So Arbitrum and Optimism, just for clarity are like optimistic roll ups. So they don't follow this Moon Math, zksync, Polygon, ZKVM and these are the two main net ZCO rollups. So because Optimistic roll ups were first to market like Arbitrum is now what, one and a half years old and like Optimism is another about a year old as well, so far they are number one and two in activity. Arbitrum number one, optimism is number two. Ciao. If you can find the L2 bit.
00:30:56.487 - 00:30:57.271, Speaker A: Yeah.
00:30:57.463 - 00:31:44.351, Speaker C: While you are doing that. So as we discuss Optimism and arbitrary number one and two, Ziki Sync Era and BullyCon ZVM are the first general purpose ZK rollups. They are not the first ZK rollups in the sense on the contrary actually ZK Sync had a product launched two years back which now it's called ZK Lite which is. Which was a Ziko rollup but it only had payment functionality. It didn't allow general smart contract similar to Rubring which has one functionality which is trading and many other like limited functionalities. They correlates. So the innovation that happened last two weeks is that we finally have a mainnet that can have general purpose applications.
00:31:44.463 - 00:32:37.079, Speaker A: And by the way the reason for that. So you said ZKsync Lite was launched one or two years ago and which was the very specialized ZK rollup. And the reason for that is because it's very difficult to write all these ZK circuits exactly for a general for a set of general purpose Turing complete functions and programs. You're going to have to write very specialized circuits for each quote unquote set or type of functions. And that's why it took them a Long time to finish writing all these different circuits. But back to the original point. So the state of everything, right? Like you mentioned, Arbitrum, optimism, which are the OP optimistic roll ups, as well as Zksync, Polygon, starknet, which are the ZK roll ups.
00:32:37.127 - 00:32:37.687, Speaker C: Yeah.
00:32:37.831 - 00:33:31.527, Speaker A: And today if we look at the TVL of these various rollups, we can see that Arbitrum is by far the number one. Arbitrum is actually three times as big as number two, which is optimism in terms of TVL. So Arbitron has $6 billion in TVL and optimism has around $2 billion. And it's also worth noting that on this list you see Zksync ERA and Zksync Lite. So Zksync ERA and Zksync Lite, one of them is the general ZKEVM and the other one is the specialized one. They both have around order of magnitude $100 million in TVL. But Zksync Arrow is the one that launched two weeks ago and I'm actually very impressed with their growth so far.
00:33:31.527 - 00:34:09.033, Speaker A: So I'm going to click into this so that everyone can see the growth of the TVL on Zksync. So this dot here, this green.is March 24, which was the date where Zksync ERA was launched. So we start from zero, and it's been growing very consistently until today to about $100 million in TBL. Very impressive growth. On the other hand, Polygon ZKEVM also launched around the same time. Right.
00:34:09.033 - 00:34:55.800, Speaker A: And Polygon is actually down here with only less than $3 million in TBL. Let's assume for a second that this data is correct because I haven't been able to find any other data sets. So let's assume that this is correct. By the way, we talked about this many times on our podcast, which is that the quality of data on chain data in crypto is just not there yet. So we should always take everything that we see with a grain of salt. But anyway, let's assume this is correct. Polygon now has $3 million in TVL, which is 30 times less than Zksync era, although still it's been growing pretty consistently.
00:34:55.800 - 00:35:51.911, Speaker A: And actually I was talking about this with some of our founders and they were wondering why Zksync ERA and Polygon evm, despite launching at the same time, one of them is growing much faster than the other and they said they think it's because Zksync ERA or Zksync in general has been around for a Much longer time. And so developers trust them more than the Polygon zkevm. Polygon, despite being around for a very long time, the ZKEVM is their first project into ZK rollout because previously the Polygon chain, it was basically a sidechain. It had nothing to do with ZKrollups. They think that people trust Zksync more than Polygon. In reality, I think what's happening is, is something really dumb, which is the fact that Polygon has launched a token already, whereas zksync has not.
00:35:51.983 - 00:35:53.475, Speaker B: I think it's as simple as that.
00:35:55.015 - 00:36:36.995, Speaker A: It's really as simple as that. People are just pumping their assets into zksync in anticipation of a future airdrop. And so people are really just farming the token Arbitran. Yeah, and by the way, this is a really funny lesson for a lot of founders. This is actually a really good reason for not launching a token because there's an anticipation for future token launches when you don't launch the token. And for the same reason I think that or at least for a similar reason, Arbitrum and optimism. Despite launching around the same time, Arbitrum has been well ahead of optimism.
00:36:36.995 - 00:36:47.607, Speaker A: And I think part of the reason was because of the anticipation of token launches. Futa, you seem unconvinced. What are you thinking?
00:36:47.631 - 00:37:35.607, Speaker C: Arbitrum launched quite before optimism. So by the time optimism came to Mainnet like there was already a ton of activity on Arbitrum. And if you remember back then when Uniswap was scaling to L2s, they communicated the plans to launch on Optimism. But because optimism was not ready, they made a quick vote and the community actually voted to launch Uniswap on Arbitrum first. So arbitrary was first mover and honestly at technology level zero is still the first mover because at least they implemented the fraud proofs which optimism is still working on. So yeah, I think also Zksync. But so it's time to market which also place in the ZK Sync ZK EVM from Polygon.
00:37:35.607 - 00:37:58.465, Speaker C: ZKsync was first to market. They had actually a product and if I'm a developer and want to deploy my protocol, I would likely go to ZK Sync era because I will be the first mover there. I will get more attention and that. But if I'm considering bullying system, I will launch some Bulligan BOS because that's where the liquidity is. So there are different variables.
00:37:58.545 - 00:38:47.291, Speaker A: That's speaking of first mover advantage, I want to just mention a quick anecdote which was mentioned by Anatoli, the founder of Solana. So Anatoly came to our cohort and spoke in front of our founders a couple weeks ago and he talked about how Solana really out executed all their competitors in the 2020 era despite having raised one or two orders magnitude less money than all their. Like, do you remember like there was products that raised like $100 million at a $600 million valuation. Yeah, like in the 2018 era without a product. Whereas Solana, guess how much Solana raised? Solana raised like less than 10. That order of magnitude, like one or two order of magnitude less money than all other competitors.
00:38:47.323 - 00:38:49.227, Speaker B: And they got like a hundred passes before they got a.
00:38:49.251 - 00:40:01.315, Speaker A: Yes, I remember they got, they got a hundred passes. And I know that, I actually know this from a lot of VCs who pass on Solana saying so one of the VC says Anatoly, you seem like someone who is a little bit too laid back for a founder for entrepreneur. And there's other people who said you only raise like $5 million. How are you going to compete with those who raise a hundred million dollars? So they pass on Solana for these reasons. But anyway, the story that Anatolia told us was that they outperformed out executed everyone else simply by being the first to the market. Like all his competitors, all the competitors Solana was still building, was still on testnet, you know, marketing for like, you know, 100,000 TPS, where Solana just didn't care, just launched the mainnet in 2020 and got some of the major market makers on board, including Jump and ftx. FTX obviously in hindsight was a bad, I guess, incident for Solana, but it's undeniable that FTX played a pivotal role in terms of bootstrapping the critical mass of developers in the early days of Solana.
00:40:01.315 - 00:40:30.167, Speaker A: So back to her point, the first mover advantage is actually ridiculously high. And same story with Polygon. Polygon back in the day didn't have very good technology. It was a ZK rollup. It was not even optimistic rollup, but it was the first to the market and they marketed themselves as the scaling solution for the Ethereum community, even if they're not relying on security. The underlying trustlessness assumptions of ethereum minute, Ethereum L1, but they were first to the market.
00:40:30.191 - 00:41:00.891, Speaker B: It goes back to like Amir Haleem's quote, if you remember that Vinod Khosla gave him, which is like, if you're a startup founder and you're building in the space, your goal is to survive. No matter what. And if you survive long enough, then luck can play to your advantage in a way that'll help you become successful. And so I feel like it's also partly that. Right. It's like, you know, if your first movers advantage and you continue to stay and build long enough, luck will play to your advantage as you're like scaling up and growing your user base. Fuda mentioned recursion.
00:41:00.891 - 00:41:03.899, Speaker B: And so what is recursion?
00:41:04.067 - 00:41:20.217, Speaker C: So you mentioned that ZK Sync used snarks, which specifically Plonk and Polygon ZKVM uses actually both snarks and stuff. So you asked a very valid question. How can you use.
00:41:20.331 - 00:41:20.605, Speaker B: Yeah.
00:41:20.645 - 00:41:21.949, Speaker C: Two different proofing systems.
00:41:21.997 - 00:41:22.485, Speaker B: Yeah.
00:41:22.605 - 00:41:58.373, Speaker C: So Polygon ZKFM uses the concept of recursion, which is you take some data or some transaction, you generate proofs for each one and then you create, you aggregate all these proofs and create a proof for the proofs so you can create this recursive loop. So what Polygon is doing, Polygon ZKEVM is doing is that for every transaction they generate a stork and then they aggregate these storks into another. So they have two layers of stark. But the problem with the stork is that it has large proof size. Right? Yeah, that's. We discussed this earlier.
00:41:58.469 - 00:41:58.845, Speaker B: Yeah.
00:41:58.925 - 00:42:20.755, Speaker C: So as the final stage, they use snark to prove the storks, which voila, makes this proof size smaller and they can submit this small proof on the L1. So Boligon ZKVM has done really good tricks in engineering to do recursion between stocks and snarks, to be able to kind of check all the boxes by engineering.
00:42:21.055 - 00:42:41.319, Speaker A: Is this the same idea behind the so called layer threes? I don't know if you've seen this narrative, but it seems like layer threes is also recursive proofs. So the layer threes post proofs to the L2, the L2 verifies them and posts a proof of those proofs to the layer one.
00:42:41.487 - 00:43:05.173, Speaker C: Yes and no. So yes, the concept is correct. When you recurse, when you do recursive proofing, you can do that. But the fun fact is that you can actually do this in the layer 2. Why do you need a layer 3 if you can do recursive proofing? So you don't need a layer three. So I think layer three is more to add more features that are not available in L2. Let's say ZK Sync now can do proving.
00:43:05.173 - 00:43:33.547, Speaker C: Right. But it doesn't support the privacy. So what if you Want to add privacy, you will not go and rebuild the L2. Instead you go build an L3 that has privacy and will create the proofs that maintain privacy. And now ZK can actually roll this into their own proofs. But again, just to be clear, not all proofing systems support recursion. So there is still a lot of work to be done to assess the viability of L3s.
00:43:33.651 - 00:43:43.419, Speaker B: Is the idea between this mashup as to what is it just for data compression? Why would someone want to use both Starks and snarks? Is it just primarily for data compression?
00:43:43.587 - 00:43:47.483, Speaker C: Yes. You mean your question is specific to Boligon zkbm, right?
00:43:47.539 - 00:43:51.043, Speaker B: That's right, yeah. Why? Why the mashup of both? Is it just because if you use.
00:43:51.059 - 00:43:57.331, Speaker C: A Star clone like if you they will have very fast improving but the proofs will be too big.
00:43:57.443 - 00:43:59.571, Speaker B: Oh, then you use so you do.
00:43:59.603 - 00:44:11.197, Speaker C: Another step of snarks to that will increase your brewing time a little bit, but not by much, but will lead to a much smaller proof size. So that's why the mashup.
00:44:11.261 - 00:44:14.293, Speaker B: And it's only two minutes right to do the computation.
00:44:14.349 - 00:44:32.425, Speaker C: Yeah, they have done a really good job. So like the most recent claims of Polygon team is that half a million of gas will cost will be done of transactions will be approved within 2.5 minutes and they are trying to push this to sub 2 minutes.
00:44:33.505 - 00:44:51.369, Speaker B: Okay, so that's a good understanding. So Zksync uses Zksnarks and Polygon EVM or ZKEVM uses Snarks and Starks. Yes, one is EVM compatible, which is Polygon and the other is I believe bytecode compatible, right?
00:44:51.497 - 00:44:52.041, Speaker C: Yes.
00:44:52.153 - 00:44:55.425, Speaker B: Okay, so let's talk about the differences between the two.
00:44:55.545 - 00:45:03.541, Speaker C: Yeah, we haven't touched on the topic of EVM compatibility. Actually Vitalik has a really good article here and he created some of the terminology which we are using.
00:45:03.613 - 00:45:06.397, Speaker B: So talk about what EVM is and then.
00:45:06.501 - 00:45:36.923, Speaker C: Yeah, so Vitalik defined four different kinds of ZKE EVM or like circuits that can be EVM compatible or not compatible. Okay, so let's start. What is evm? EVM is the Ethereum virtual machine. So that's a virtual machine that can execute any code, but it was created with a specific syntax and specific logic. So you have a chance here. When you are trying to build the ZK roll up, you have an opportunity. You can tweak your circuit to be exactly compatible with the EVM.
00:45:36.923 - 00:46:12.809, Speaker C: So this is 100% ZKEVM equivalent or compatible. So Vitalik calls this type 1, type 1 EVM. On the other end of the spectrum, maybe when you try to do this equivalence to the evm, your throughput gets affected. So instead you can build actually a very efficient Z key circuit that are general. You can let's call ZKE vm. Not you have ZKE VM that can execute any logic, but it's way faster because it doesn't tie to the, it doesn't commit to the EVM structure. And this can be very fast.
00:46:12.809 - 00:46:39.605, Speaker C: So this is what zksync went for. ZK Sync went for this and sorry, stuck net Specifically they went for this type 4. They built their own circuits that are very general, but it can be very performant. It can have thousands of TBs. Like the target is at least 2000 TBs. But the problem is that this is not compatible directly to solidity. Like developers who develop for solidity, they will have to learn a new language.
00:46:39.605 - 00:47:17.263, Speaker C: So what ZK done, and I think they have done great in this regard that they created a lot of software layers or middle layers that can actually take solidity code, high level solidity code and compile it to the architecture that works for them to their own ZK circuit. So now actually you can have solidity code that you wrote for Ethereum before and deploy it in ZK Sync era. Yes, it's not compatible to evm. So you cannot just have the byte code and run it in ZKVM. But the ZKsync error will take the solidity code and compile it to the correct architecture.
00:47:17.399 - 00:47:32.791, Speaker B: So we Talked about the TVL, right? The change in TVL. Do you think builders are choosing ZKsync because it's just much more faster? I mean what's the throughput on Polygon ZKVM is it 3x faster than a layer 2? A traditional optimistic layer 2?
00:47:32.863 - 00:47:45.409, Speaker C: So bullying ZKVM can have like hundreds of TBs, like TBs in the hundreds, maybe a thousand. But I think ZK Sync will always be faster because Z optimizes circuit for this reason.
00:47:45.537 - 00:47:47.129, Speaker B: Like how much faster?
00:47:47.257 - 00:48:09.491, Speaker C: I would say like again both are trying to improve but I would say as of now it's like between around 3x or 3 to x 5x faster than ZKVM. But anyway none of them actually running at maximum throughput now. So if we go back to the chart that Ciao showed up from L2 bit like both of them are running at sub 10 tbs.
00:48:09.643 - 00:48:10.827, Speaker B: So interesting.
00:48:10.891 - 00:48:13.515, Speaker C: The theoretical maximum is not there yet.
00:48:13.635 - 00:48:49.573, Speaker A: Right now if you look at this website called l2fees.info, which is another pretty popular website that people look at. Again, I don't know how accurate the data is. This one should be a lot easier than TVL to get read. But polygon zkevm, the fees on polygon zkevm to send eth is 23 cents right now and to run the same transactions on arbitrum 1 is 15 cents. So arbitram is actually cheaper than polygon zk. And then the data doesn't have zk sync error, but it does have zksync lite.
00:48:49.573 - 00:49:01.173, Speaker A: I suppose Zksync Error might be a little bit more expensive than Zksync Lite. Futo, what do you think? Is that the right intuition? Because zksync error tries to do something more general purpose, I think it will.
00:49:01.189 - 00:49:04.573, Speaker C: Be around the same ballpark of ZK Sync Light.
00:49:04.709 - 00:49:20.733, Speaker A: Okay, so let's assume that it's same ballpark as Zksync lite. It's only 10 cents. So Zksync Lite today is cheaper than Polygon ZKEVM, if that answers your question, Imran. But again, there's so many other factors at play here.
00:49:20.829 - 00:49:21.541, Speaker B: Yeah, agree.
00:49:21.653 - 00:49:37.803, Speaker A: The number of transactions that are happening today haven't reached the capacity yet. So that's one factor. The other factor, I think Futa, you mentioned this earlier, which is that the more transactions you run on a zkevm, the cheaper it is for each transaction. Because there is a fixed cost, right?
00:49:37.859 - 00:50:20.727, Speaker C: Yes, it's a counterintuitive because we always go to the mentality of like more transaction means more competition for block space, which means naturally more fees. Right? But because roll ups actually have to generate proofs and these proofs are fixed cost. Like once you send a proof, this proof can prove one transaction or can prove a thousand transactions, right? So the more transactions you put in the proof, the cost of this L1 cost will be amortized or be shared between this thousand transactions. So at a certain scaling limit for the CURE labs, the more transactions you get, the cheaper the cost will be until you get to this limit and then it will be kind of fixed cost from that from now on.
00:50:20.811 - 00:50:57.629, Speaker A: My takeaway from looking at this data is today the layer 2s are all in the same ballpark in terms of transaction fees and they're all about five to 10 times cheaper than Ethereum layer one. But they're all more expensive than Solana. This chart doesn't have Solana, but Solana transaction Fees is basically zero. So even the Ethereum layer 2s today are one or two orders magnitude more expensive than Solana. And again, I want to share the insight from Anatoly's talk. By the way, I'm not showing Solana vs. Layer 2 or vice versa, just sharing the things that we've heard.
00:50:57.629 - 00:51:51.121, Speaker A: The insight as to why Solana works and is cheaper and is really, really cheap is because Solana builds the parallelization into the vm. There's built in parallelization, meaning when the developers write a Solana code, they have to, if they want to write it according to the rules of Solana, they have to parallelize some of the transactions. Whereas on Ethereum, the way Ethereum layer 2 scale is basically parallelization via L2s. So what that means is on Solana you have a bunch of transactions but they all parallelize natively. Whereas on Ethereum you have a bunch of transactions but they all parallelize because there's a bunch of independent L2s. This is the insight behind Solana being really, really cheap.
00:51:51.233 - 00:52:35.247, Speaker B: Another interesting thing about Solana is they launched something called the account compression program. I don't know if you saw that, but essentially this program is used to. Right now all the data is stored on chain. Right now they have a compression program where you could store the data off chain and then it produces a merkle root or merkle tree that would be then stored on chain via. It's called like a fingerprint. And then whenever the smart contract has to call that data, it can use this account compression program to do so. And so an interesting experiment they launched recently was being able to mint a billion NFTs within a minute and and actually did it, which I thought was very interesting.
00:52:35.391 - 00:52:38.463, Speaker C: I think this is similar to the concept of Validium when you have the data of a chain.
00:52:38.519 - 00:52:39.103, Speaker B: Exactly.
00:52:39.199 - 00:52:54.207, Speaker C: And they can now reduce the cost of storing the data on a chain. But yeah, like the interesting, the discussion on fees between Solana and L2s is interesting because Solana, like the counterargument is that Solana doesn't have a fee market. Like you cannot compete for priority in transactions.
00:52:54.311 - 00:52:55.727, Speaker B: I think they started a fee market.
00:52:55.831 - 00:53:09.763, Speaker C: They started but it's not will ingrain be it. So yeah, they intentionally like get small fee for everyone, which according to Anatoly it was a minimal fee. Like other than that it doesn't work anymore. Under that it doesn't work anymore.
00:53:09.819 - 00:54:07.703, Speaker A: Yeah, we talked about TVL and fees. There's another data set that I wanted to share which is our alliance application data because it's a pretty good indicator of where the developer activities are. So we Said Arbitrum is ahead of everyone else in terms of tvl, in terms of fees, same ballpark. Maybe Zksync in theory will be cheaper than Polygon and ZK rollups in general will be cheaper than Optimist rollups. But in terms of developer activity, what we observed over the last few months is that there was a very big event, which was the collapse of FTX in November last year. And ever since that we see that Arbitrum and optimism are growing in terms of developer activity at the expense of Solana. Now the rate of change that the delta isn't that big.
00:54:07.703 - 00:54:38.105, Speaker A: So it might be a few percentage increases for Arbitrum and optimism and a few percentage decreases for Solana. But this is a trend we've seen so far. But still Arbitrum, optimism, Solana are in the same ballpark. They're still way ahead of everyone else. And Polygon is still also up there. So I would say today, Polygon, Solana, they're followed by Arbitron, which is also slightly ahead of optimism. But then after these four, there is a long gap.
00:54:38.105 - 00:54:55.035, Speaker A: There's a big gap between these four and the rest, including Zksync, starknet and all the other alt layer ones and all the other up and coming Layer Twos. So this is what we're seeing on the developer activity side.
00:54:55.155 - 00:56:03.519, Speaker B: Well, I guess the question is, I feel like the crypto ecosystem, they swing in certain narratives, right? So there was a narrative early in the last bull market where everybody was building on Solana and then Layer Twos weren't as robust as they are today. Now, because of the downfall, as you mentioned, and other infrastructure issues, the pendulum swung back to Layer Twos and now everyone's building on Layer twos and people are now looking at Solana and saying like, who wants to build on Solana anymore? When we have all of these layer Twos and just going through all of just the infrastructure around our roll ups. You have roll ups, then you have data availability, you have Validiums and the list goes on. Now we have shared sequencers, which we can also talk about. And so the question then becomes how much is too much for founders building in this space, Right. And would founders then find the ease of use of using Solana much more from an onboarding perspective? Much easier versus going through the roll up space. We also have roll ups as a service, right? So you have all of these different complex components within the Ethereum side.
00:56:03.519 - 00:56:05.847, Speaker B: And then on Solana is just very easy to be onboarded.
00:56:05.911 - 00:56:13.895, Speaker A: And then there is a whole Story about the roll ups as a service becoming a real threat to Cosmos. This whole space is very deeply intertwined.
00:56:14.015 - 00:56:15.835, Speaker B: Well, let's talk about roll ups as a service.
00:56:17.735 - 00:56:18.515, Speaker C: Okay.
00:56:19.055 - 00:56:21.391, Speaker B: Do you want to run down what roll up as a service is?
00:56:21.463 - 00:56:22.351, Speaker A: Futa, Go ahead.
00:56:22.463 - 00:57:13.393, Speaker C: The concept is very simple actually. It goes back through like mini BZKlite. Ziki Lite was like ZK rollup as a service essentially. But it's only for payments, right? It was cheaper, it was faster, it can be better, essentially. So the concept of ZK of rollups as a service is that instead of using a general purpose rollup like Optimism, Arbitrum, ZK Sync or the Polygon ZKVM and competing with other applications using the same system, why not go and build your own blockchain? And your blockchain here is your own roll up that you own, all the resources, you control, the sequencing of transaction, you have the whole space. And the examples that are left today actually like dydx, Immutable X ray of high, all these are actually roll ups as a service. And starkware was the one that not.
00:57:13.409 - 00:57:16.361, Speaker A: Necessarily roll up as a service, but roll up as an app chain.
00:57:16.473 - 00:57:56.239, Speaker C: As an app chain. So like roll up as a service is just taking this step further by making some development frameworks that it's actually very easy to launch your own rollup. So there are many competitors now. Most of them are actually focused on optimistic OB stack. Some of them are focused on having ZK roll up as a service. But the concept is that a developer will, it will be 5 minutes integration work, 5 clicks and you get your own rollup with its own browser, with its own RPC node out of the box. So this is the concept of a roll up as a service.
00:57:56.407 - 00:58:36.379, Speaker A: Remember almost a year ago we said on this podcast that roll up as a service and Cosmos, the two visions are converging and indeed today they're converging. And matter of fact, I think the roll ups as a service on Ethereum is a massive threat to the whole Cosmos ecosystem. Because we know for a fact that there are several fairly mature defi protocols that were previously looking at Cosmos to launch their own app chain. Today they're looking at a roll up as a service or roll up as an app chain to launch their own app chain on Ethereum as opposed to Cosmos.
00:58:36.507 - 00:58:48.835, Speaker C: You can also have both. By the way, you can have a roll up as a service on top of Cosmos like Celestia, so you can have everything if you want. As you mentioned, everything is interlined again.
00:58:48.875 - 00:59:32.395, Speaker A: Back to Imran's previous point, how much is too much? This is way too much for new founders. What the hell is going on and how do you choose a chain to build on top of? I think there is actually a few simple rules or some simple trends that we're seeing roll up as a service. Probably not something to think about when you're new because these are for mature projects. Okay, Roll ups and surveys and Cosmos, they're all for mature projects. But we're seeing some trends between Arbitrum and optimism. Again, these are things that you probably can't find online, but things that we hear behind the scenes. Arbitrum seems to be the hub for Defi the likes of GMX and Imran.
00:59:32.395 - 00:59:43.819, Speaker A: You've tried a bunch of, you know, derivative trading protocols on both Arbitrum and Optimism and you can tell us which one is better. But it seems like Arbitrum has most of the volume at the very least and that's where the user.
00:59:43.867 - 00:59:59.573, Speaker B: Yeah, I mean the volume on, well, both open interest and liquidity. The UI UX on products are just so much more robust on Arbitrum than it is on Optimism. I've used a couple products on Optimism and I mean it's okay, it's not amazing.
00:59:59.659 - 01:00:49.477, Speaker A: So Arbitrum is the, is a Defi hub. The Defi roll up optimism seems to be attracting a bunch of gaming developers due to their, I guess, connections with, you know, the, some of the OG gaming projects and their, their friends, you know, the Ethereum foundation, the Zero X Park, Dark Forest, like these guys. So it seems like a lot of new gaming developers are choosing optimism just because everyone else is choosing optimism. That's a very interesting trend. We're also hearing things about starknet. It seems like some of the Starknet developers are a little bit discouraged by their migration towards Cairo 1.0, which is taking a while, number one, because they acquired a bunch of developers to build on top of Starknet.
01:00:49.477 - 01:01:26.013, Speaker A: But the current version of Starknet is just not very cheap. It's only a quarter of the fees of Ethereum Layer one. So they're going to migrate to the next version, which they call Regenesis, but it's taking a while. And then also developers would have to rewrite their whole code. So there are some talks there, some disillusionment in that side of the world. And then you have Polygon, which is we mentioned before, the beast in terms of BD in crypto. They're by far the best BD team in the entire Layer one, L2, space.
01:01:26.013 - 01:01:59.715, Speaker A: But yeah, so These are some of the, I guess, data points that founders can think about when they think about choosing which. Oh, and there's also Solana, which we talked about last time with Amir. And the reason why Helium is picking Solana over any of the Ethereum L2s is simply because Solana is two orders magnitude cheaper. There's simply no way for a proof of physical work process project where a user earns, let's say, $5 of tokens per day and having to pay 25 cents in fees, that just eats into too much of the margin.
01:01:59.835 - 01:02:28.815, Speaker B: He also made a direct comparison to aws, if you remember. He goes, you know, what's the point of me running my own validators, my own protocol, launching my token, et cetera, if I could just use something off the shelf similar to aws. And I think that's where the roll up as a service model is, I guess, going in terms of making it easier for any founder to build their app. Don't worry about the infrastructure, we'll take care of it. Just launch using our product seems to be the case for me.
01:02:28.895 - 01:02:44.563, Speaker A: It seems like the roll ups and the outlier ones are all specializing. That's the point I'm getting at. Or they may not be doing it intentionally, but some groups of developers and founders are gravitating towards one over the other.
01:02:44.719 - 01:02:51.027, Speaker B: But the issue with this obviously is cross roll up communication and fragmentation of liquidity.
01:02:51.091 - 01:02:58.435, Speaker A: I know where you're getting at. The sequencers. The sequencers Decentralized sequencer network.
01:02:58.555 - 01:03:24.009, Speaker B: Yeah, there has been some talks. I just saw two announcements. This just the past two days. One's called Astria. I forgot what was the name of the other startup. But essentially they're creating a way for sequencers to be decentralized within different rollup space. And so now you have rollup as a service and I'm hearing this question a lot actually, which is we have a couple rollups as a service startups in our current batch.
01:03:24.009 - 01:04:08.383, Speaker B: One is called Caldera, which is specifically around the Optimism framework. And you have Snapchain, which is primarily around the ZK framework. And when we talk to these founders they're like, well our customers are asking us for our service, but they also want to understand like how do they decentralize their sequencers. They don't want us to own the sequencers, obviously, and us to run the transactions, but they also don't want themselves to do it. They want their customers to know that these transactions are batched and being executed trustlessly. So how do we solve that? Imran, I mean, you know, there's some trade offs that I've talked through, but now with the launch of Austria and others, they could solve that issue. And so maybe Fuda.
01:04:08.383 - 01:04:16.719, Speaker B: Talk us through. Why do we need decentralized sequencers and how important is it for maybe cross chain comms and messaging and et cetera?
01:04:16.847 - 01:04:21.791, Speaker C: Let's start by the board. Sequencer. Why it's called the sequencer? Why is the sequencer?
01:04:21.903 - 01:04:23.087, Speaker B: What is a sequencer?
01:04:23.191 - 01:04:59.003, Speaker C: Yeah, so in any blockchain you have the validator which do two tasks. First to order all the transactions and then to make sure they are correct. Right. In the L2s, you don't need the second board, you just need to order the transaction because the L1 will validate them if they are correct or not or do the consensus. So that's why the terminology is different for the L2s. The sequencer is what the entity that organizes the transaction of the L2s. So the problem is that decentralizing sequencer will come to the same problem that we have in the L1s, which is that you need to achieve consensus between different players.
01:04:59.003 - 01:05:18.887, Speaker C: And this is too much work to implement. So most L2s have already their hands full with the development of their core technology, whether it's Optimistic rollup or ZK rollup. So every one of them is using a centralized sequencer and the entity runs one server that organizes all the arranges, orders all the transactions.
01:05:18.951 - 01:05:26.127, Speaker B: Isn't that an issue? Or is it just that we trust optimism and arbitrum and like we shouldn't have to worry.
01:05:26.191 - 01:05:33.463, Speaker C: It's an issue, but it's not on the hair on fire issue. Right. You need to implement the product first. Right. You don't. You cannot go and build everything at the same time.
01:05:33.519 - 01:05:46.183, Speaker A: Because if the, the issue is if the sequencer cheats. Yeah, you can get your transactions logged, censored, but you won't lose your money like your money is still yours. So that's why it's not a hair on fire problem.
01:05:46.319 - 01:06:16.771, Speaker C: Exactly. So you can actually, sometimes in Arbitrum or like ZK Sync, you can actually claim your money back on the L1 even if the sequencers are dead or like off a chain offline. So anyway, the problem with that model is very simple and you mentioned Imran, that every rope is its own universe. It doesn't speak to anyone. You lose composability. Right. So Optimism Team actually came with this idea of can we actually allow different rollups to speak to each other and have atomic composability what is it called?
01:06:16.803 - 01:06:18.435, Speaker B: Bedrock. Bedrock framework.
01:06:18.515 - 01:07:01.487, Speaker C: Bedrock is the implementation of optimism, the obstacle. Yes, the Optimism team came with this concept of subarchain. What's the subarachnoid that everyone will build their own OB stack, roll up or OB rollup, but you will share the sequencer. Like a sequencer will be sequencing all these roll ups and they can order transactions from different rollups together so that you can actually Send money from rollup 1 receive it on roll up 2 in the same transaction. So that's why short sequencer can allow actually this atomic composability. But as of now, optimism suggested astronomical idea. They haven't implemented it, but many teams, many smart teams found this idea and just run with it.
01:07:01.487 - 01:07:35.237, Speaker C: So Astraea is one of them. Like they are trying to build this concept as to offshore sequencer, but so far they are not focused on Ethereum just to proclaim. Astraea is focused on using Celestia as data availability and focused on the Silverian roll up space which we didn't touch on today, but many other solutions. There is a company called Espresso which is trying to do this for Ethereum. So the concept is simple. We will have this network of sequencers, maybe all of them share a token and they will be slashed if they.
01:07:35.301 - 01:07:38.105, Speaker B: And yet another token of course, like.
01:07:38.885 - 01:07:46.093, Speaker C: The concept of our space, you scale by adding more blockchains and each blockchain is a token. So like more blockchains, more tokens, they.
01:07:46.109 - 01:07:49.429, Speaker B: Will go wait, wait, wait, wait.
01:07:49.557 - 01:07:58.785, Speaker A: The way our space scales is by creating more blockchains and the way we acquire more users is by launching more tokens. That's how space works.
01:07:59.685 - 01:08:08.671, Speaker B: So a roll up for a token, sequencer token, mainnet token, data availability token. Okay, got it.
01:08:08.823 - 01:08:14.595, Speaker C: Yeah. So that availability is a token. Celestia has its own token. So like. Yep.
01:08:16.135 - 01:08:24.551, Speaker A: By the way, I think the only token that is really, really, really, really, really needed, absolutely necessary, critical to the product is the Ethereum layer one.
01:08:24.663 - 01:08:29.791, Speaker B: Yes, yes. I mean really, you could probably run everything else without a token, right?
01:08:29.903 - 01:08:31.887, Speaker A: I mean Arbitrum ran without a token.
01:08:32.031 - 01:08:36.389, Speaker C: That is the case of Agile Layer. Agile is trying to use the same token for everything, kind of.
01:08:36.517 - 01:08:45.821, Speaker B: Wait, we talked about all like roll ups, but we didn't really talk about data availability and we mentioned this word a few times and so maybe this is explained to us.
01:08:45.933 - 01:08:46.861, Speaker C: No, no.
01:08:47.053 - 01:08:49.189, Speaker B: I mean I really don't want to go down this rabbit hole, by the way.
01:08:49.237 - 01:08:54.065, Speaker C: I don't mind. I can keep talking about that stuff for hours. But like that's that would be too long.
01:08:54.645 - 01:09:03.504, Speaker B: So there's two types of data availabilities, right? There's firms or companies that are building the space. Celestia and Egan layer explain to us what is data availability.
01:09:03.624 - 01:09:40.515, Speaker C: So we agreed that every roll up need to make the transaction data available. Because if the sequencer goes offline, how would we make sure what happened? What is the activity, what is the history of activities that happened on the E2 or the rollup? You need to make this transaction data available somewhere. So this is what we call as data availability. Where is this data will be available? So if you decide to compress this data and put it on an L1, then you have this best kind of data availability. Because we assume that Ethereum L1 is always up, anyone can access it. So this is the best kind of data availability. But Ethereum is still expensive.
01:09:40.515 - 01:10:00.109, Speaker C: So other chains came like Celestia saying we don't need to use Ethereum, Ethereum has execution and has storage. What if we create only a blockchain that only does storage, it doesn't do execution. So Celestia as a concept is trying to do. We are building a blockchain just for that availability. Any roll up can just throw their.
01:10:00.157 - 01:10:04.069, Speaker B: Data at, I mean any layer one too, right? I mean it could be any, it.
01:10:04.077 - 01:10:15.133, Speaker C: Can be any one, but like essentially it's made for L2s. Because L2s is where you need data availability. You cannot have an L1 with its data with data sitting somewhere else. It wouldn't work. So L1s have to have their data.
01:10:15.229 - 01:10:15.973, Speaker B: Oh yeah, you're right.
01:10:16.029 - 01:10:37.919, Speaker C: But L2s can put their data somewhere else. So Celestia came with this concept of let separate storage from execution. Celestia will be only for storage. Execution can happen somewhere else. It can happen in Ethereum as well. It can happen in something like Fuel, it can happen somewhere else. But as a roll up you will bust your data into Celestia and they give it a name, Suvarian Rollup.
01:10:37.919 - 01:10:45.351, Speaker C: And you can decide also to have this data availability layer on your aws. And this goes back to the Validium concept.
01:10:45.423 - 01:10:57.053, Speaker B: What I like about Celestia is that they have something called data availability sampling where they actually like shard the data into different nodes. So if one node goes down or something happens, they can still retrieve the data.
01:10:57.189 - 01:11:21.035, Speaker C: Yeah, this is more discussion about how to show also that the data is available. No one can withhold the data because in Celestia you have validators who will maintain this data. And if you need access to this data, you need to access the validators to give you this data. How to make sure that no validators can withhold part of the data? The data sampling is an approach to reduce the dependency on certain layers to withhold the data.
01:11:21.195 - 01:11:45.017, Speaker A: So in this vision, this separate data availability vision, you have three components. You have Celestia or another DA layer, you have the roll up and you have the Ethereum layer one. So then the L2, the roll up will do the execution, the DA layer will store the transaction data and the Ethereum layer 1. What does it even do? Does it just verify the proof?
01:11:45.121 - 01:12:10.299, Speaker C: Exactly. It will verify the proof and achieve consensus that the execution was correct. So how would you make sure that yes, the transaction data is fixed, is stored somewhere, but how we make sure that this transaction actually got executed? And what is the final state of the prolab after the execution? So consensus is actually validating the state or the state route, like where are we after executing these transactions?
01:12:10.427 - 01:12:27.651, Speaker A: The trade off of this vision is making is trustlessness. Right. Because now you're storing the transaction data in a new chain, a new DA layer, you're no longer relying on the trustlessness of Ethereum, so you add a new trust assumption.
01:12:27.763 - 01:12:34.513, Speaker C: Yeah, Vitalik has a really good argument against this separation of data availability. It reduces the security significantly.
01:12:34.659 - 01:13:00.789, Speaker B: Yeah, so it's similar to kind of the world we live in today with Amazon, right? You have database, you have compute, you have storage and all of the other infrastructure it provides. And similarly, it seems like with Ethereum roll ups, data availability and all of the other things that are coming around, it's similarly like, kind of like breaking that into its own core components and it has infrastructure behind that powers it. So that's very efficient for the end user. Is that the right analogy?
01:13:00.877 - 01:13:35.749, Speaker C: I would say it is. But the problem is that when you go to ws, all of these are offered as a services in the same platform, right? It's a click, I have button, you commission a database, you get EC2 nodes, you are doing all of that right now. You have to use different chains. Each chain requires payment in its own token. So there is a lot of developer friction. So you will essentially need over time, like platforms that abstract all of that. This actually can be also the roll up as a service because you can try to abstract this complexity.
01:13:35.749 - 01:13:52.345, Speaker C: So once you launch your rollup, we will take care of everything. We will use whatever availability layer that you want, we will pay them, we will use whatever consensus layer that you want and we'll pay them. So this can be part or it can be specialized layer that actually platform as a service.
01:13:52.725 - 01:14:27.165, Speaker A: This whole data availability layer as well as the previous thing which was the decentralized sequencer network. These seem still very much far out. Like I've been speaking with people who are really in the weeds of building these things and they all tell me it's way too early. And so I think the reason why we're talking about these things is because you should be aware of the future. But these are not like hair on fire problems yet. Like for example, like Arbitrum Optimism are not even they're still running a centralized sequencer. But we want to talk about the future.
01:14:27.165 - 01:14:48.765, Speaker A: What other visions of the future do you see on the rollup side? Because one thing we talked about futa, which something we discussed recently and seems like you and I are on the same point are on the same page, which is that ZK rollups will probably need to specialize in the long term. Do you want to talk a little bit about what this vision means?
01:14:49.065 - 01:15:39.509, Speaker C: If this is space to grow and really get mass adoption and people are using our product or this blockchain in any everyday use cases, then the throughput of any L1 or any general purpose L2 will not be enough. So this is the thesis, like if we got adoption, no matter how many L2s you will need. So eventually you will need every app or every protocol to run on its own space that can have full potential. It can have a thousand transactions per second at zero fee or almost negligible fees, which means that you need to specialize. So it is the same thesis as Abby chains. Everyone will need their blockchain essentially. But in some extent, rollups reduce a lot of complexity around having your own chain because you can commission the security and data availability from someone else.
01:15:39.509 - 01:16:46.005, Speaker C: Our thesis here is that if you are running a successful product and you have a lot of users, you will try to specialize, you will have your roll up. And if it's a ZK rollup, which I think will be the standard of trust because now it's correct by construction, you will have to optimize your ZK rollup. What does it mean? It means that instead of using a general purpose circuit, a general purpose circuit that can do any instructions, you will just create a specialized circuit or a specialized ZK circuit that will do this functionality only and they cannot do anything else. This will lead to much faster proving time, much smaller proofs and lower fees for your users. And it comes and then comes a question about data availability. Let's Assume that the proof space will be implemented at this time and you will not pay too much cost for the data storage. So that means that like if you are running an order box exchange or a blend and a really only chain game, then that requires at least 1000 TPS.
01:16:46.005 - 01:17:01.433, Speaker C: Then you can build this, you can have someone who will give you create the circuit for you and voila, you have your own space, you have the security of ethereum as an L2 or even L3 and your product is running in a decentralized fashion.
01:17:01.529 - 01:17:07.641, Speaker B: Do I hear CK circuit as a service? Yes, it's a joke, but that's in.
01:17:07.673 - 01:17:26.171, Speaker C: A sense what as they correlate as a service should do on the long term. Now they can just use any tech stack available like ZK Sync or Polygon or whatever, but on the long term they will have to really go into the weeds and like ah, you need this application here is a circuit that does that.
01:17:26.283 - 01:17:55.701, Speaker B: And so the ZK circuit communicates with the proving system. Right. And is the idea different proving systems would allow different types of ZK circuit implementations and that would then be the types of applications that would be in need of those types of like guarantees. Right. So an example of this trustless setup versus trusted setup, one being proving speed or the cost of storing the data, et cetera. Is that the idea?
01:17:55.893 - 01:18:13.923, Speaker C: Yes. And we're already seeing this like we have a company that is like building a ZKV ML, ZK machine learning. Right. So their rollup or their L2 will focus on executing machine learning models. So you will have kind of AI on a chain which is a very.
01:18:13.979 - 01:18:33.097, Speaker A: Special type of compute, which is a bunch of matrix multiplications versus a general persist EVM which can do all sorts of things. Right. So in that case it's basically a roll up for this, a bunch of matrix multiplications and they would write very specialized circuits for that kind of compute.
01:18:33.281 - 01:18:38.001, Speaker C: Yeah, this circuit will not be useful for anyone else, but it's very, very useful for this use case.
01:18:38.153 - 01:19:06.203, Speaker A: Yeah, and there's also like actual production examples like loop ring, you mentioned order book based decentralized exchanges. They will probably need some kind of specialized ZK circuits at some point versus taking it from starkware. Right. Like as a white glove service. Right. So point being, we think that ZK rollups will specialize over time in order to provide faster and cheaper computes.
01:19:06.339 - 01:19:28.415, Speaker B: Would that mean then roll ups as a service could be threatened? Because I feel like every app will want to have, I mean if the edge is providing a quality product with UI UX speed. All of that would then the idea then mean that founders should be thinking about ZK circuits or is this the future and rollups as a service is just a short term hair on fiber problem.
01:19:28.495 - 01:19:45.087, Speaker A: Rollups as a service they can aggregate all these different like specialized roll ups ZK roll ups as well. So it's like a, it's a layer on top of the a bunch of different rollups including optimistic roll ups, including ZK roll ups including like specialized ML roll up MLZK et cetera.
01:19:45.191 - 01:19:50.205, Speaker B: Got it. Cool. Well I think we've any final words.
01:19:50.505 - 01:19:53.697, Speaker A: I feel like we can easily break this down into like 10 episodes.
01:19:53.841 - 01:19:55.137, Speaker B: Yeah, maybe we should.
01:19:55.241 - 01:20:00.193, Speaker A: We should. At some point we could spend a whole episode just talk about zkml.
01:20:00.369 - 01:20:02.033, Speaker C: Yep, that would be fun.
01:20:02.169 - 01:20:10.825, Speaker B: Great. Well thank you so much for joining us. Hit subscribe if you haven't. We'll talk to you guys soon. Thanks for listening to Good Game. Don't forget to subscribe. We'll see you next week.
