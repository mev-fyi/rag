00:00:00.250 - 00:00:04.190, Speaker A: Welcome to good game. Your no BS Insights for crypto founders.
00:00:07.570 - 00:00:13.534, Speaker B: Can I get a quick gauge of sentiment? Bullish, neutral or bearish cosmos? Because I'm, I'm hearing conflicting things.
00:00:13.572 - 00:00:17.150, Speaker C: Again, bearish short term, neutral long term?
00:00:17.570 - 00:00:18.506, Speaker A: I'm neutral.
00:00:18.618 - 00:00:33.858, Speaker B: Well, can you actually be bearish short term than neutral long term? Because I kind of agree with Zaki that they have a twelve month window of opportunity. So if they don't succeed within the next twelve months, quote unquote succeed, do they really have another shot at dethroning?
00:00:34.034 - 00:00:49.478, Speaker C: I guess I'm slightly optimistic of them executing something interesting, but still not fully convinced that even if they execute, it's going to be powerful enough to actually have some particularly like long standing ecosystem overvalued.
00:00:49.494 - 00:01:23.890, Speaker A: Prop looking for your next startup idea in crypto? Check out our request for startups list and get inspired at Alliance XYz ideas. Welcome to good game. This topic is going to be a very interesting topic around the modular thesis and more or less to understand how that's going to affect the Ethereum ecosystem, how it's going to affect founders users, and then more importantly how it's going to affect its other blockchain networks like cosmos. But before we get started, we have a new guest. His name is Dimitri. Dimitri, do you want to give a quick background?
00:01:23.970 - 00:01:48.782, Speaker C: Yeah, sure. Happy to be on. Yeah. Research partner at one KX, generally focused on research. Do a lot of writing in the space across number of topics. I've been more towards the infrastructure side of things, app chains, interop portfolio support, things around token design, go to market strategies, BDM partnerships and general investment. DD before that led investments and research at a crypto family office called Volunteer Investment Group.
00:01:48.782 - 00:02:04.798, Speaker C: Did around 25 investments with a firm there and before that spent a year with a phone called Coin fund for a year also doing research and DD. So overall investing in crypto since 2017, life pre crypto, much more boring. Spent six years working at a bank across some growth strategy and innovation goals.
00:02:04.974 - 00:02:16.600, Speaker A: And Dimitri, when people said they've been around since 2017, you've actually been around since 2017. I remember reading your articles back in the day. I think you were still at Berkeley from what I remember.
00:02:17.210 - 00:02:17.730, Speaker C: Yeah.
00:02:17.820 - 00:02:19.162, Speaker A: You went to Berkeley, right?
00:02:19.296 - 00:02:50.718, Speaker C: Yeah. First piece was on token standards, just around when 1155s are coming out just after cryptokitties, other standards that are coming out like nine nine eight. And interestingly, now we are seeing a reemergence. I think it's almost like a token standard. Summer between 4337, some cool ones like 6551. Turns out this is actually an interesting, but a market for teams as well. It's your crypto equivalent of regulatory capture.
00:02:50.814 - 00:03:26.350, Speaker A: 6551 is interesting, but let's not go off on tangents because it's so easy in crypto. But let's talk about the modular thesis. There's been a lot of content around the modular thesis, and there's this new narrative of why we need to have our own data availability layer, our own roll ups, et cetera. But it could be pretty much broken down to a few functions, right? Consensus, data availability, execution, and maybe even system of bridges, or bridges are like the three to four that I think are the most important when we think about modularity. So maybe diving deep, maybe Fuda or Dimitri, do you want to talk about what is the modular?
00:03:26.690 - 00:03:59.186, Speaker D: Yeah, sure. Like, I will start, but I will let you, Dimitri, also add your thoughts. So modularity is kind of a new thought in our space because monolithic chains are suffering now. So like each chain, when Ethereum started, no one would speak about modularity. What modularity? I'm speaking about, like, everyone is trying to capture users. So all chains started at monolithic. The chain does everything possible, right? Like, it does execution, it does consensus, it does data availability or data storage.
00:03:59.186 - 00:04:33.890, Speaker D: It does settlement of the transaction. So all the four main function of a blockchain have been in one monolithic block. And this actually led to. It was a good design choice at this point because you cannot just build five pieces in barrel. So as Ethereum started to get users and the network started to get crowded, people started to think, okay, we need to actually start unbundling these pieces to achieve better throughput. And we saw this unbundling actually at a very interesting level. The first modularity happened at the node level.
00:04:33.890 - 00:05:19.506, Speaker D: Like some protocols, like flow, for example, said we can actually unbundle these four pieces into different kind of nodes, but all of them are in the same network. So the flow network has four different kinds of nodes, all using the same token, but still four different functionalities. It was recently when Srimh changes the roadmap to roll up centric vision, that we started actually having discussions about, oh, we can actually have these four different pieces as four different chains. And we are seeing actually some chains, like Celestia, coming to fill only one of these pieces, like data availability and consensus. And we are seeing roll ups coming to fill the part at the top, which is execution. Roll ups will do execution. We can do everything else on the L1.
00:05:19.506 - 00:05:32.162, Speaker D: So of course, there is so many designs of roll ups. So I will let Dimitri take it from there. If you have any thoughts or like you also Dimitri can walk us through what different kinds of roll ups or modular designs that you can see.
00:05:32.296 - 00:06:20.306, Speaker C: Yeah, no, I think that makes a lot of sense. I agree with you. It came from a need of scalability. I don't think that you could fit the entire global economy on top of one monolithic infrastructure. I also think about it as blockchains are fundamentally software controlling hardware. And so as you scale, I think what helps as a mental model is how do you create more specialized hardware that does a specific thing? And the kinds of hardware requirements that you might need for DA is very different than what you might need for execution or in the flavor of a ZK roll up. If you have approver, that hardware looks very different than if you have a normal or like a regular e two consensus client.
00:06:20.306 - 00:07:02.820, Speaker C: So a lot of the work around modularity, I think, also revolves around how do you start to specialize your hardware to actually achieve more scalability. And yeah, I think there's a lot of interesting flavors. I think we are seeing it mostly around DA and execution. I think there's a lot of very interesting experiments that we'll see around da layers being live between Celestia, polygon, avail, Eigen, da, all very cool ideas. 4844, Dank, sharding. I think in practice, we're seeing the most traction right now with the ones that are actually live between optimistic and zk rollups. A question of whether you use crypto economic versus cryptographic security.
00:07:03.270 - 00:07:11.062, Speaker A: Before we dive deeper Chow, any questions that we want to answer by the end of the episode that we're looking to learn more about.
00:07:11.196 - 00:07:41.786, Speaker B: The main question that has been bugging me for a while is how much complexity is too much. Because on our last episode, we talked about Solana, which is the antithesis of modular architecture. And obviously there are trade offs. But the nice thing about Solana is that, at least conceptually, it's simple. It's not easy, but it's simple. Consensus, execution, data availability layer, they're all done at the same layer. There's only one layer, one blockchain.
00:07:41.786 - 00:08:16.458, Speaker B: Ethereum started this way, but Ethereum is moving to more modular. And with all these different layers unbundled, is this too complex from an engineering perspective and also from the perspective of builders? Picture yourself as a builder who is new to this space, and you see a bunch of names. Arbitrum, optimism, Ethereum, polygon, Eigen layer, Celestia. What the hell do you do with all these things? Right? So the question is, is this too much complexity. That's the main question I want to answer.
00:08:16.624 - 00:08:53.926, Speaker A: Got it. So maybe for some of our audience members that don't know what the modular thesis is, maybe we should talk a bit about what is consensus and what does that mean for Ethereum, right? Like ordering of blocks to data availability, where does the transactions get stored? And is that transparent execution environment like with EVMs? And then I also like to touch on bridges, because I think bridges will become an important part of our conversation towards the end. So maybe talking about each of these Lego blocks and what are the startups that are building in these Lego blocks, and then maybe we can dive deeper from there.
00:08:54.028 - 00:09:33.682, Speaker D: So I can take first step at that, because it can go along. So let's start from the application layer, because application layer is very obvious. Let's say that you are trading in uniswap, for example, you are changing PB token for USDC or something, right? So you as a user emit a transaction, you create a transaction. I want to sell this token, right? Someone else will come and buy this token or the liquidity provider position will change, right? So this is a transaction. This transaction affected your balance only, right? So it affected your individual state. We call this state, like your balance on the chain. What you hold is called a state.
00:09:33.682 - 00:10:01.626, Speaker D: So when you emit a transaction, you change your own state, but at the same exact block. Imran Chow, Dimitri, all of you are emitting transactions, right? So we need to kind of settle all this transaction. So after execution comes settlement, we need to update, execute these transactions and update everyone's state. We call this settlement. So everyone knows their new state after that. But do we agree on that? We have to agree on that. The network as a whole have to agree on that.
00:10:01.626 - 00:10:31.202, Speaker D: That's why validators, each validator in the network have to reexcute a transaction. And all of us have to agree on the new state. And once we agree on the new state, we call this consensus. So we achieved consensus on the new state. So the new state now has to be stored somewhere, because any newcomer to the network or any new transaction need to know what is the state of the wallet before it starts. So now we need to store this state somewhere for other people to be able to read it. And this is the data availability where this data exists.
00:10:31.266 - 00:10:33.042, Speaker A: And right now, where does it get stored?
00:10:33.186 - 00:11:14.494, Speaker D: Yeah, like now everything is stored on Ethereum. It is stored in the main blockchain. We have something called cold data, which is kind of, let's consider the state as a memory, your computer memory, your hard disk you throw your data on your hard disk and now the data stays there for any people to come and read from it, right? So at a higher level, modularity means that you have execution layer to execute transactions. You have settlement to know each one's balance, every individual balance. After this execution, then you have consensus that we already have to agree on the new state and data availability at the bottom, which is like we need to store this data somewhere to be able to retrieve it and know what the state of the blockage chain at. So this is a very quick description of the four layers.
00:11:14.622 - 00:11:41.390, Speaker A: Got it. Let's talk about like we touched on this a bit, which is data availability, right? Which is we have a couple of things happening on the macro side, right? You have like Deng sharding from the Ethereum side, you also have like egan layer and others that are building within the data availability layer. So maybe let's talk about what that means for founders. I know there's a concept of restaking as well, but what does it mean for the end roll ups and people that are building in the space?
00:11:41.460 - 00:12:40.186, Speaker C: For me, I think it's a matter of cost structure. The interesting part is we're finding this out in real time. For every dollar that is spent on gas, when you split that out, what proportion goes to DA, what proportion goes to execution and settlement. And so the economics here are very interesting to observe in real time. And what we don't know that again we'll find out is, is it $0.80 on the dollar that is going to be paid for DA for actually storing this, or is it $0.20 or is it less? In my perspective, it's primarily a cost and security question, where, for example, you have one construction called a validium where DA is stored amongst a separate set of nodes, and it's usually a smaller subset.
00:12:40.186 - 00:13:43.678, Speaker C: It's usually cheaper to store for a lot of teams. Do they need e l one equivalent DA security? If so, there's going to be a higher willingness to pay. But if they're fine with it being stored somewhere cheaper, then maybe it's a cost question for them. Maybe they could reduce the overall transaction costs for those application developers. My sense is for DFI protocols on ETH L two, often your DA is your weakest link because you need them, for example for fraud proofs. So if it's the weakest link and you want ETH l one equivalent security, then a dFI protocol might say I need ETH l one equivalent da as well. So then maybe they might be more willing to pay the cost for 4844, but for a game, maybe they're fine with a lidium construction.
00:13:43.678 - 00:13:47.622, Speaker C: And so for them, they might use a cheaper DA option.
00:13:47.756 - 00:14:34.658, Speaker B: I have an anecdotal data point related to what you just said about cost structure. So I talked to a very specialized ZK roll up project recently, and they told me that the data availability accounts for about 80%, very roughly 80% of the cost. And the remaining 20% is everything else, like execution, consensus, blah, blah, blah. And the reason why he came up with this number is because the alternative of putting DA on Ethereum is to put the DA on, let's say, like at the extreme, put it on AWS. And if you put the DA on AWS, you can basically save 80% of the cost of ultimately settling on Ethereum. So just an interesting data point.
00:14:34.744 - 00:14:36.580, Speaker D: Was this an optimistic roll up?
00:14:36.950 - 00:14:55.078, Speaker B: No, it's DK. So the remaining 20%, a big chunk of it, is actually the ZK prover. So it's a combination of zk proving and consensus, but the exact breakdown is very hard to estimate. This is more of an intuition based estimate.
00:14:55.174 - 00:15:20.974, Speaker C: That's interesting. That's very surprising for me, actually. I thought it was going to be the other way around. I thought that DA might actually end up being relatively commoditized outside of ETH L, one equivalent DA. Because as more of these solutions come up, it feels like a more commoditized product to me than having globally secure settlement layer that could secure trillions of TDL.
00:15:21.022 - 00:15:47.562, Speaker B: That's really interesting, but you're talking about the future. In the future, this stuff will be very commoditized. What I'm talking about is the current state of things. So today, to settle the data from a roll up onto Ethereum layer, 180 percent of that is DA. Our views are incompatible with each other. But also, EIP 48 44 will reduce the 80% of DA down by an order of magnitude, one or two orders of magnitude. At least that's what they think will happen.
00:15:47.616 - 00:15:48.186, Speaker A: Why is that?
00:15:48.208 - 00:15:55.680, Speaker B: Right? I mean, that's the goal of EIP 48 44. You probably know this better than I. Yeah, okay.
00:15:56.050 - 00:16:31.106, Speaker D: Exactly what you mentioned, Chad. The cost of storing that data of transaction from a roll up to Ethereum, is 80% of the cost, right? So what if you can somehow take this data and store it somewhere else and save this 80%? So this is the whole philosophy, not just behind EIB 44 48. It's also Eigen, Da, and Celestia. All of them have the same exact goal. Let's put this data somewhere else. And the design choices are different. So Igenda says, you can do this today through Ignda and use restaking.
00:16:31.106 - 00:17:05.842, Speaker D: So the same italian you're using for staking will be used to secure the data. So this is one design, the other design, Celestia says, no, you can give us this data for you to skip, and you will pay us using our own token. So crypto economic security. So that's one. The third way is, as Dimitri mentioned, you can just store it in AWS. Who cares, right? Like put it in a validium, put the data on AWS. Ethereum like data blobs concept is that you will create this data space as a separate or a side blockage chain to Ethereum that only focuses on data.
00:17:05.842 - 00:17:17.398, Speaker D: It will not do execution, it will just do data. So it is the same concept as Eigen da, but it's a kind of Ethereum foundation sponsored. It's part of the process here.
00:17:17.484 - 00:17:28.826, Speaker A: I thought Dang sharding was more for temporary memory or temporary data, whereas Eganda and others are more long term data. Is that true or no?
00:17:28.928 - 00:17:32.202, Speaker D: Okay, so what do you mean by temporary data?
00:17:32.256 - 00:17:41.578, Speaker A: Like a. Like it will flush out at a certain time period, it will flush out. And then people can continue storing data within Ethereum.
00:17:41.674 - 00:18:30.526, Speaker D: To some extent, yes, dimmetry, you can add more color into that. But we have two concepts of memory on any node, which is the active data, your account balance, my account balance at the exact moment, and your historical data. Let's put historical data to the side. The idea of data blocks is that when you transact on a roll up after a bit, after two weeks or so, your state doesn't matter anymore. You don't need to keep the data availability anymore because the data probably has progressed and you can go to archival nodes to get this data. So yes, the datablops concept that you will save the data for some time, and then after two weeks or a month or whatever amount they decide on, the older state will disappear, but you're still keeping the current state. Dimitri, do you agree?
00:18:30.628 - 00:19:19.840, Speaker C: I think that conceptually makes sense. I think also just goes back to Chow's question around complexity, where there's different flavors of doing this. I think it ultimately comes down to what extent does this benefit end users or application developers? It's largely a question of cost. From one perspective, if it is significantly cheaper for devs or users, then it might be worth actually going through and implementing this design pattern. I kind of think about it as if you want to build a car. Like the usual analogy when building a startup, you don't do the wheels and then the chassis and then the steering wheel. You start with a scooter and then a bicycle and then a car.
00:19:19.840 - 00:20:00.598, Speaker C: And I think these DA constructions are finding ways to potentially reduce transaction costs. I don't know which flavor is going to be preferred, but I like the idea of for a startup or like an application developer, when thinking about which da solution to choose from, what pain points are they trying to solve for and then what additional complexity are they taking on? And if the cost benefit or the security benefit is worth, yeah, maybe you should opt in for the complexity.
00:20:00.694 - 00:20:56.442, Speaker B: It almost feels like at some point there needs to be a AWS type of product for developers where you have a nice front end that gives you a few options to choose from as a developer. You can choose which DA, for example, the Ethereum foundation version of DA, or the Celestia or Eagleayer, et cetera. And then there's another option for another drop down, literally a drop down on the UX, for which roll up. Is it optimistic roll up or zk roll up or something else. And that's the whole idea behind Caldera, for example, that you guys invested also went through our program. So moving from data availability to the roll up side of things, what is the current landscape that you're seeing? Because there's obviously arbitram optimism, a bunch of app chain roll ups, and then there's shared sequencer, a lot of complexity, a lot of names. What's the landscape?
00:20:56.506 - 00:21:43.982, Speaker C: I think in general we're kind of seeing the usual suspects around arbitram optimism, zk sync, starkware that have their flavors of either optimistic or Zk. I think there are some interesting ideas around how l three s might look like both on starkware and the hyperchains that zk sync is working on, you can conceptually do an l three on top of an optimistic roll up. I think arbitrum has been working on that. I believe they call it orbit. I think these are very cool constructions. I like the idea of giving developers the opportunity to have their own block space. It feels like a lot of applications are exploring that right now.
00:21:43.982 - 00:22:41.822, Speaker C: Most are probably still on. A lot of these monolithic l two s, I'll call them, because of user eyeballs, effectively liquidity. I think to have a vibrant, application specific l two ecosystem or L three ecosystem, you probably need to solve for things like bridging and cross roll up atomicity. And that's where a lot of the ideas around shared sequencers come in. Most of these roll ups today I think all of them today is single sequencer. And part of the exploration that's happened in the last year is, well, what if we decentralize the sequencer? Or what are even the issues here? And a lot of folks are saying, okay, well, censorship resistance is an issue, liveness is an issue. What if someone finds where Arbitran's single node is and they blow up the data center? People would not be happy.
00:22:41.822 - 00:24:04.330, Speaker C: So do you need to build in some redundancy in the system? And then I think a lot of the thought evolved and said, okay, well, is there really a benefit to decentralizing a single roll up? Maybe you could have some very vanilla flavor of like round Robin where you have, I don't even know if you needed to be stakeweight, but having multiple options to actually settle to l one. But you don't solve a lot of the issues around bridging and crossroad communication. And the notion of shared sequencers is probably relatively more novel. It's the idea of can you split, I suppose, transaction ordering from transaction execution. So introducing another layer of modularity with the benefit of if you do have a shared sequencer set that is providing ordering for multiple roll ups, what can you do there? The cool benefit there, it might be the most clear benefit is that bridging and atomic composability become slightly easier. It almost comes for free if you have a fully overlapping cheer sequencer set. So I think it's a very interesting direction, like things that Astria espresso radius are working on.
00:24:04.400 - 00:24:38.962, Speaker B: Dimitri, I probably have like ten questions from the last two minutes, and some of them are for myself and others are for the audience. I think this requires going back to the basics of defining what is, for example, l three, l two, what is atomic city, atomic composability and bridging, et cetera. But let's start with the first one you mentioned, l three and an L two. What is exactly an L three? This is actually a question for me. What is exactly l three? And why would you as a developer do an L three versus doing an L two app chain?
00:24:39.106 - 00:25:19.214, Speaker C: Yeah, so that is still up in the air. I think all layers are blockchains. They're just blockchains with two way trust. Minimized bridges is the way I frame it. And what l two s do is they post their estate like their settlement transaction to L1. The L three design pattern posts that to the L two. For the Zk variant, you would have the L three post the validity proof to the L two, and then that is recursed into the validity proof down to L1 for optimistic.
00:25:19.214 - 00:26:15.346, Speaker C: It's a bit cloudier where I suppose you have the state route posted to L two and then that's posted to L1. I see a bit less of a benefit there because the L three security on an optimistic roll up still falls back to the fraud proof window for the L two going back to L1. So I'm not sure you get much of a security benefit there. I think the zk variant with recursion is a bit more interesting why someone would do it. I think ultimately it's around customizability and somewhat business model, potentially to have their own token, for example used as the gas token. I think that's often interesting for teams. I think this is also a good opportunity for the reemergence of enterprise blockchain, but they could actually be their own L two s or L three s.
00:26:15.346 - 00:26:25.014, Speaker C: It gives you access to the l two liquidity and user base, and it makes bridging easier amongst the different L.
00:26:25.052 - 00:26:45.470, Speaker A: Three s. Is this like a similar thesis to base, where base is developing their own chain within the optimistic bedrock framework and they're bringing their own users into this kind of permission Kyc chain, but that's ultimately bringing in a whole new set of users that wouldn't have been able to come before. Is that how you're thinking about it?
00:26:45.540 - 00:26:48.702, Speaker D: But basis and l two. Basis and still an l two.
00:26:48.836 - 00:26:51.520, Speaker A: Say it again. Basis and l two. That's right.
00:26:52.610 - 00:27:15.254, Speaker D: I think L three is more complex, kind of, it's kind of replicating what is happening between L two and L1. Another level up. So L three to L two. But there is actually a very nice distinction here that was actually commented really well, or captured really well by Vitalik himself when he said like, actually ls three currently feels like a marketing term. Everyone is saying I'm building as an LSD, which is different.
00:27:15.292 - 00:27:17.414, Speaker B: That's what I feel too, same.
00:27:17.612 - 00:27:55.234, Speaker D: You know why? Because the data availability, the funny thing that it's a marketing term, because exactly the terms that we have been discussing since the beginning, which is data availability, right? If you are rolling the data from the L three to the L two, and then the l two is rolling its data back to the l one, then the L three data is rolled back all the way to the l one. So you are paying the exact same cost. So you cannot have actually any saving or any advantage actually of building an l three. So I define l three in a funny way. I call it, I think, a chow. I had discussion with you before the true l three and the fake l three. The true l three, that ends its relation with life to the l two.
00:27:55.234 - 00:28:27.120, Speaker D: Like the data availability is only posted to the l two end of the story. It's not rolled down to the l one, then. Yes, it depends on the security assumption of the L two to live. So it's a layer on top of L two, the fake L three, which actually, what people say right now is that, yeah, we just build on top of starkware or maybe zookeesync or arbitram as an LSD, but essentially our data still will live on the L1. So it's actually an L two that is marketed as L three for the community to give them some grant or something.
00:28:27.730 - 00:29:14.666, Speaker C: Yeah, it's interesting, there might be some cases where it makes sense. If you have, for example, an application specific l two that is a certain gaming ecosystem, then what you might have is potentially l three s with their own games that potentially leverage the ip of the L two. And those games can have their own economies, but then they use that l two as some connectivity back down. That is largely theoretical, like a communications layer, communications layer, shared liquidity, and interop layer. But yeah, it's definitely still in. The idea made is to actually figure out where this makes most sense. To your question on atomicity, it's an all or none transaction.
00:29:14.666 - 00:29:58.826, Speaker C: So either you execute both things or multiple things, it could be a chain, and if any one of those fail, then everything reverts. This is particularly important in deFi, particularly around liquidations. Atomic what? Flash loans are a feature that you get from atomicity. It's fascinating because you can effectively have infinite leverage with zero credit risk. I suppose. So it's a really important concept around keeping the Dfi ecosystem kind of balanced across different roll ups. You can have composability, meaning you can interact with different l two s, but you might need either a bridge that goes between the l two s, or you go back down to L1 and read one.
00:29:58.826 - 00:30:14.418, Speaker C: L two reads the state that was posted from another l two to that l one, and then they take an action. But if you wanted to, say, do a flash loan across two different l two s, that's where the notion of atomic composability would be particularly important.
00:30:14.504 - 00:30:20.014, Speaker B: Dimitri, can you define atomic composability in the context of roll ups?
00:30:20.142 - 00:30:52.130, Speaker C: Yeah. So if you have, say you have in the world where you have uni chain and maker, chain and maker is not a cosmos chain, it's an l two, and there's a liquidation on maker chain, you would want some actor, some liquidation bot, to take the collateral, take it to unichain, swap it, and then deposit it back to maker chain. And so that all needs to happen atomically.
00:30:52.230 - 00:30:58.138, Speaker B: But what does atomically mean exactly? Because on the layer one typically means within the same block.
00:30:58.234 - 00:31:15.314, Speaker C: Within the same block, yeah. I suppose you're assuming that the different l two s have some alignment between their block times. It's an alignment, I suppose, within the same blocks that are mined, I suppose, on each of the different l two.
00:31:15.352 - 00:31:28.882, Speaker B: S. And today they're not synced. Right. But something like a shared sequencer might potentially enable the syncing. Or you're skeptical.
00:31:29.026 - 00:32:01.790, Speaker D: That's what I was going to say. Even if they are synced, atomic composability is not possible simply because you need to kind of transfer an asset from the roll up one to roll up two, do an action and transfer the asset back into the roll up. Rob one in one chain, in one block, in 1 second, in one fight, whatever time period, this is not going to happen. The security guarantees gets broken. Once you move from one chain to the other, security guarantees is broken. Done. What you can do, actually using shared sequencer is ordering transaction.
00:32:01.790 - 00:32:10.086, Speaker D: And this has different use cases. It's not liquidations, it's not flash loans, maybe arbitrage. And I think we discussed this before.
00:32:10.188 - 00:32:12.962, Speaker A: Would this be considered like asynchronous composability?
00:32:13.106 - 00:32:37.040, Speaker D: Yes, it's not atomic. We can call it non atomic. We can create a new term. Now, given that we are all here, we can say non atomic composability. Right? So it's a composability in something that you can force some order of transactions to be run on different chains, but they are not atomic. You cannot move assets between these two chains unless you have pools of capital sitting. Let's say that you are liquidating something in here.
00:32:37.040 - 00:32:54.834, Speaker D: I will use the same example that Dimitri gave. Like you have the maker roll up and you have the uniswap roll up. If you have infinite pools of capital on both side chains, then you can kind of use this liquidity. But moving the same asset back and forth breaks atomic compatibility for sure.
00:32:54.952 - 00:33:05.318, Speaker B: Okay, so for audience, how does a share sequencer work, actually? What is the goal of it? So first of all, what is the problem that it's trying to solve? And two, how does it solve it?
00:33:05.404 - 00:33:46.918, Speaker C: Liveness and censorship. Resistance is what I hear about the most around. Again, if you have a single sequencer, it is fairly nontrivial for that sequencer to censor your transactions. That sequencer can go offline, and so then all the users on the l two are forced to do a manual exit back to L1. They could do it, but I can imagine the UX for that is probably pretty awful. So ideally you want to reduce, I suppose, the possibility of that happening. There can be other ways of doing it.
00:33:46.918 - 00:34:37.618, Speaker C: I suppose for a given l two, you can always checkpoint back to the last commitment to L1. So maybe you can have another l two sequencer that is spun up that effectively like restart state from the last checkpoint. But the sequencer bridge admin keys or like multi stick keys are probably still going to be controlled by a single entity. So that still gets difficult, I suppose. Yeah. The idea of a shared sequencer is to split the notion of ordering and execution. You can still have a single roll up operator do the state transition and then post that state transition down to L1, but it's not focused on the actual transaction ordering.
00:34:37.618 - 00:35:04.010, Speaker C: That can be a separate group of nodes effectively that do it. And yeah, I think, like I mentioned before, the nice thing that you get for free is because that ordering layer can touch multiple roll ups, it can give soft guarantees of inclusion across different roll ups, and it can also effectively act as a bridging mechanism between those different roll ups.
00:35:05.070 - 00:36:30.934, Speaker D: Curious, actually, when you were talking, I remember the recent tweet from Kyle Samani, when he was actually speaking about a different kind of shared sequencer, which he called it centralized shared sequencer, which is like exactly that. It's like you have a single sequencer, but instead of this single centralized sequencer, but instead of it operating only on one roll up, it's operating on multiple rollups. And the discussion was around how would we progress to the thing that you were describing, Dimetri, which is decentralized shared sequencer, something like Astria or espresso or radius, when you have actually a network operating as a sequencer. Right? So this can be a decentralized shared sequencer, but there is actually people now actually thinking of, can we actually enable composability between some kind of composability, obviously between different chain by just making it a shared sequencer that is centralized and everything, but it's like shared. So this is what came into my head and made me laugh, because people are trying to go for the easiest solution possible to solve the problem, which is like, oh, we are breaking consume visibility by having multiple roll ups. So let's solve it by having a sequencer layer that is shared but still centralized. So they are not focused on decentralization anymore.
00:36:30.934 - 00:36:33.350, Speaker D: So this is just kind of funny.
00:36:33.430 - 00:37:15.302, Speaker B: Okay, but from my understanding of what you guys described, Dimitri and Futa, there's actually two problems. One is breaking the composability of having too many roll ups, and the other one is decentralization, which currently is not the case because they all use one centralized sequencer. So these are two distinct problems that may be solved with a decentralized shared sequencer network. But the problem of decentralization isn't conceptually the easiest solution, that the roll up just recruits more sequencers rather than talking to a decentralized sequencer network. Why can't arbitram just recruit their own sequencers as opposed to creating a new astria or express or whatever?
00:37:15.436 - 00:37:24.666, Speaker D: It will not solve the composability. Essentially, they can decentralize sequential and be happy, but they will not solve the composability issue.
00:37:24.768 - 00:37:40.926, Speaker B: Correct. But if I were arbitram, my incentive, my goal is to be the only roll up that wins everything. I don't care about composability. I actually want optimism to lose so that they become irrelevant and then composability doesn't matter in the first place.
00:37:41.108 - 00:38:12.614, Speaker D: Yes, but they will still need central, they will need a shared sequencers for all the roll ups. That user tech stack, that's what the optimism super chain is all about. Optimism now is not a single chain. And arbitram is actually moving in the same direction. They said we will open our ticket stack for anyone to use. And you know what? We will give you, even arbitokens, as a grant to build another l two in our ecosystem. So arbitram and optimism now is moving in some direction.
00:38:12.614 - 00:38:23.774, Speaker D: They want to have many roll ups within their ecosystem. And for composability between this sub roll ups or whatever, you still need a short sequencer. It's a must.
00:38:23.972 - 00:39:42.806, Speaker C: But the difference is it's the RB, for example, like shared sequencer or the op labs one that is operating across all of the roll ups that are building within their ecosystems and chow. What you're referring to is an incentive misalignment problem, or an incentive alignment problem where the walled gardens are being recreated. Not full walled gardens, but it seems like there's not an incentive to actually achieve atomic composability between different roll ups because you actually want keep DeFi on RB because you're able. That's why I'm a bit torn on the, I don't want to say like viability, but go to market. I think is going to be interesting on this. Again, this goes back to complexity to what extent do you need to decentralize everything? Go back to your end users. Which Persona actually needs these properties today? Maybe there is enough of an ecosystem within a certain roll up that they have less of a need for that.
00:39:42.806 - 00:40:29.838, Speaker C: So are you creating a solution for a problem that doesn't exist there today? I think applications are probably more concerned with low fees, higher throughput, predictable gas costs, interesting business model ideas around introducing pre compiles that have not been included in E L1 yet that enables their user base or use cases to do novel things. I feel like that's more important today than actually figuring out a way to solve for the theoretical of liveness and censorship resistance. That's probably more of a contentious statement. Know, I might get some DMs on later, but I think it's like an.
00:40:29.864 - 00:41:11.106, Speaker B: Order of operations follow up question. I feel like the two of you send me very mixed signals. And in fact, Fuda, you said a couple things earlier that are seemingly conflicting, and it's probably just me misunderstanding you guys, but it seems to me that, Dimitri, you think atomic composability is possible, and Futa, you think that it's not possible. But my understanding of what you just described as shared sequencer, if you have a shared sequencer node that is used by both arbitrum and optimism, that handles the consensus and transaction ordering of both, why is atomic composability not possible? I'm probably just misunderstanding this, but I want to get to the bottom.
00:41:11.128 - 00:41:16.930, Speaker D: Let's clarify. First your statement. Dimitri, do you believe that atomic composability is possible using short sequences?
00:41:17.090 - 00:41:17.846, Speaker C: Yes.
00:41:18.028 - 00:42:02.334, Speaker D: Okay, I am the opposite scam then. I don't believe that atomic composability is possible. So you got this right way, Chow. I can explain my reasoning, and I kind of did. But atomic composability for me means that you can take a flash loan from an application, move this flash loan, take a million USDC from one chain, move this 1 million USDC to another chain, do something with this 1 million USDC, then get whatever, do arbitrage, move back the one USDC to the first application. So you can do this now on Ethereum l one. And you can do it in one transaction, not even one transaction.
00:42:02.334 - 00:42:19.734, Speaker D: You run a smart contract that has all these transactions and all of this happens and done. Voila. Right. So this is possible because of two things. Because you have the ability to order transactions. This is the fact that you have a smart contract that contains these transactions in order. This is one.
00:42:19.734 - 00:42:49.070, Speaker D: The second one is that you have liquidity. You can move this liquidity within the simple chain, your estate is known to every player, to every validator, right? Maybe we can add a third one that. Watch the definition of atomic. If any of this transaction failed, then all of the smart contracts will revert and you don't lose any money. So let's take these three properties and examine them again. In the case of two different roll ups with a short sequencer. So we're assuming they have a short sequencer.
00:42:49.070 - 00:43:34.674, Speaker D: So you can take flash loan from a roll up one, you won't move this liquidity to the roll up two, can you do this in the same clock, in the same transaction as executing the transaction? I don't think it's possible today that you do a transfer of asset between different roll ups and do things in that and go back. So today, for the foreseeable future, I don't think it's possible. The other part that is breaking the atomic composability for me is that you can do some of that. But if the last step failed, you have no guarantee whatsoever to revert everything. Whatever has happened, has happened. You need just to be so. Can the short sequencer guarantee that? I doubt it.
00:43:34.674 - 00:43:39.540, Speaker D: Yes, they can order transaction, but who knows, right?
00:43:40.150 - 00:44:13.066, Speaker C: It's interesting. Can you do that if, for example, you have two different roll ups which share the same sequencer set with a rule base that each. So they all have access to the same mempool, they have each other's state transition function in the logic. And there is inclusion criteria there, meaning you need to have one to go through in order for the other to execute within the shared sequencer logic.
00:44:13.098 - 00:44:54.842, Speaker D: Yeah, but now you recoubled like the shared sequencer we started by. The shared sequencer separates execution from ordering. Right. Now what you said now is exactly recoubling both. Recoubling consensus to execution, that if this doesn't execute, then consensus have to fail, kind of, or like, you cannot proceed. So the idea of shared sequencer, that you take this ordering piece, put it aside and separate it from consensus, we don't need or settle. Yes, in the way I describe it, it's possible, but you kind of walked back on the first assumption, which is like now, you cannot progress in the chain unless you do what the shared sequencer told you to do.
00:44:54.842 - 00:45:09.102, Speaker D: So something like that. So it's kind of tricky. I'm sure there is people in the space way smarter than me, and they will figure out what to do it. But as of now, with my limited knowledge, I don't see it happening in the next couple of years, maybe talking.
00:45:09.156 - 00:45:14.100, Speaker A: More about the shared sequencers, unless. Is there any follow up to this?
00:45:14.950 - 00:45:22.420, Speaker D: I think. Enjoying the debate. Go ahead.
00:45:23.990 - 00:46:00.506, Speaker B: I do have a follow up question, which is a direct result of your disagreement, which is bridging. So you two disagree on whether or not atomic composability is possible today. So then the natural question is what is the ideal bridge across roll ups in several dimensions, in terms of composability, in terms of fees, in terms of latency. These are the things I think are the most important factors for the end users. This is a question that I have not been able to answer myself. What is the future of cross roll.
00:46:00.538 - 00:46:48.670, Speaker C: Up bridges to your point on atomic composability? Also, I don't think it's possible today. I guess I was more saying there might be ways to do it in the future, but I think these constructions are still in progress. So still TBD for quite some time. I've been thinking about bridging in the context of l two s for quite some time. I think within a given l two ecosystem, there might be less of a need for a third party bridge. I think a lot of these l two ecosystems between their roll ups and potentially l three s, they will probably use homegrown messaging systems. I think that's probably the safest for them.
00:46:48.670 - 00:48:15.002, Speaker C: We've seen this in other areas, like Avalanche, like rolled out their own messaging system. Obviously IBC, I think you can have ways to use al two, say different l two s can read from the fraud proof or Litig proof that's posted to the l one and then use the DA as the bridging mechanism where one l two can check the fraud proof against the state route on some da and say, okay, this is valid. And so that l two takes an action on their side. So two different l two s can use l one to bridge and then different l three s can use the l two as effectively. The summer layer where it becomes harder is if you're going across different l two or l three ecosystems, or if you're going across chains, I think you for sure need a third party bridge if you are going off ethel one that's unavoidable for anything else. And then you have all of your flavors of transport verification layers that do that. But I think a realization for me relatively recently is that within a certain l two ecosystem, you probably might not need a third party bridge as much as we thought.
00:48:15.002 - 00:48:29.306, Speaker C: There can be use cases where you can have operators front liquidity between two different l two s on the liquidity layer. But on the transport and verification layer. I think that could happen without a third party bridge.
00:48:29.418 - 00:48:44.654, Speaker D: I will have a follow up question to this one. So do you think the IBC style bridges is the way to go for interroll up bridges? Or what would be the ideal bridge between different roll ups? Let's say ethereum roll ups?
00:48:44.782 - 00:49:15.546, Speaker C: You need finality for IDC. It's hard on. I mean, you'll have a finality gadget, but that'll give you some baseline. It was like every 50 blocks or something. So that's like the fastest, I suppose, that an IBC type construction would work. I guess that's like, to Imron's point in the beginning, like, what did cosmos do better? It baked in interop from day one. I don't think that EF has any roadmap to think about ecosystem wide interop.
00:49:15.546 - 00:49:36.498, Speaker C: Ideally there would be standards on this, but I think they've just kind of left it to roll up teams to figure it out by themselves. And I think that's why you have this mush of dozens of projects that are trying to do it on their own. I think that's still an ethereum problem that has not been solved.
00:49:36.594 - 00:50:11.310, Speaker A: Okay, I want to go back to the shared sequencer topic specifically. Around right now, you have centralized sequencers that are ran by L2s. Arbitrum runs one, optimism runs one. What is a path to decentralization? So an example is like you mentioned Austria and espresso, right? Are the two decentralized sequencer networks. What do you think is their go to market strategy? How do you think they're going to, one, get alignment with those L2s? And then how do you think about deploying additional sequencers in a way that's composable with all the L2s that are out there?
00:50:11.380 - 00:50:16.190, Speaker C: I think it's hard. I think if Arbitrum wants to decentralize.
00:50:17.170 - 00:50:23.550, Speaker A: Sorry, is it opt in as well, or is it something that they can do on their own, or do they have to get alignment with those L2s?
00:50:23.630 - 00:51:04.810, Speaker C: I assume it would be opt in. I think the thing that would push them, really push them, is some regulatory pushback. I think that's going to be a concern. Are you considered a money transmitter if you're running a single sequencer? I'm not a lawyer. How does the CFTC look at you if you are at specific l two that's running perp decks? Should you be sharding the order book amongst a sequencer set? That's basically. It is a benefit to Dydx. On Cosmo set, that order book is distributed.
00:51:04.810 - 00:51:44.958, Speaker C: You can't do that on an ethel two unless you decentralize the sequencer, which is, I guess, different from having a shared sequencer. But I think that would be, in my opinion, the thing that leads them to want to do this sooner rather than later. So within a given ecosystem, I can imagine RV is like working on this. I presented this last year on some preliminary ideas. I don't think you need it to be stakeweight. I think just like simple round robin around who gets to post l one, they capture Mev fees. There's some business model there.
00:51:44.958 - 00:51:57.678, Speaker C: It becomes harder. If you're trying to say, hey, RB sequencer, go. Also order transactions from optimism roll ups. I don't know how they make friends with each other there.
00:51:57.764 - 00:53:09.702, Speaker A: So primarily you'd think it's going to be a regulatory push. It could also be community push. I know right now Arbitrum makes quite a lot of money from running a centralized sequencer, whereby they give that back to the community, or some portion of the fees back to the community, or they put in a dow where they reinvest back into the community. I'll be very curious to see how that actually plays out in the long run, especially if you have these shared centralized sequencers. They have to one, try to get opt in from those networks or those L2s, and then over time they have to decentralize in a way that it forces some sort of composability or asynchronous composability between the different L2 networks. So like, let's say Austria and the others were able to get opt in from, let's say optimism, arbitrum and others, and they run these decentralized sequencers. The question that I would have is like one who's running the centralized sequencers or decentralized sequencers within Austria and espresso? How does that look like and how does that work? Are they really decentralized or are they running in the back end? I'm curious on how are they doing that today? And then if that was to happen, then would this create this kind of shared composability network where they're able to do maybe not composable transactions, but some sort of asynchronous composability?
00:53:09.846 - 00:53:57.990, Speaker C: Yeah, I mean, that feels nice to think about from an ecosystem perspective. I have not dug into Astria or espresso enough to understand, implementation wise, who runs it, whether it's like a permission set, a whitelisted set, or if it's fully permissionless. I can imagine. It's probably easier to just have whitelisted parties in the beginning and minimum viable. Start with five to 30 and then see how far you get there. You probably need to offer some benefit to. If it's like RB foundation and op foundation that are running for each other, why would they participate? I can imagine the community argument, but I don't know.
00:53:57.990 - 00:54:01.882, Speaker C: I have not spoken to anyone there, so I don't want to speak out.
00:54:01.936 - 00:54:36.130, Speaker D: I read a little bit about Astria and Espresso and I think they have a network of sequencers and they kind of randomly select which sequencer will sequence the next block over different chains. So it depends on VDFs verifiable delay functions that you have to stake the token of the network. And then based on how much you stake, you will get a number of slots, but you don't know which slots you will get beforehand. You will have to run VDFs to know the slot and VDFs are. Yeah, it's a consensus problem. As any problem. The advantage is that it's a simpler one.
00:54:36.130 - 00:54:52.220, Speaker D: You just need to pick one from a pool so you don't execute anything. You don't get consensus on execution. You just. Okay, I have ten. I want to pick one of ten randomly. So this shared sequencers network actually do consensus. There is no way around it.
00:54:52.220 - 00:54:56.314, Speaker D: But it's a random one and a similar one, right?
00:54:56.352 - 00:55:08.574, Speaker C: Because espresso has a variant of hot stuff, they use that and then the VDF is for the selection of who executes it at the end. Or is that VDF based to select.
00:55:08.772 - 00:55:30.920, Speaker D: If we are ten sequencers who are participating? Like who us will be doing the next block or next order. This is where the VDFs come in to select one of us to take this slot. Got you a lot of discussion. In short, sequencer, it's a very interesting topic. We focused on this one a lot.
00:55:31.290 - 00:55:35.926, Speaker A: I think we can touch on. Well, yeah, sorry, I was going to.
00:55:35.948 - 00:56:06.260, Speaker B: Say we have three options now. At least three. We can either branch from a shared sequencer into MeV on L2, which is a whole interesting topic of its own. Or we can talk about Eigen layer, which may be related to everything we've talked about so far, or we can also talk about Cosmos. I know Dimitri has a lot of thoughts and strong opinions about Cosmos, and so do we. We do. What do you guys want to talk about.
00:56:12.230 - 00:57:04.462, Speaker C: For Cosmos? I suppose I've been trying to figure out where you compete as like a cosmos chain. And if you're trying to be a sentiment layer for Defi. It's really tough to compete with ETH as a sentiment layer. And if you're trying to be a gaming chain, it's really tough to compete with the culture, or maybe the BD, rather, on Polygon. And then if you're trying to do art and NFTs and PFPs, it's really hard to compete with the culture on E. And so I feel like a lot of that. And if you're trying to do interesting things at the base layer side, Atmos had this really cool flavor of 1559, where a portion of the gas fees burned would go to the developers.
00:57:04.462 - 00:57:53.442, Speaker C: Canto has some interesting public good stuff there. I would argue a lot of that can be pre compiles on L two s. So a lot of the features which you're building an entire chain for and paying a ton for security can just be a feature on, ideally an L two with a decentralized sequencer set. That probably gets you very close to Cosmos, but you still get the benefits of having ecosystem cultural alignment, liquidity alignment. I think where it probably does make sense is if you truly need a very high degree of customization that you cannot get. Again, we don't have decentralized sequencer sets for sequencer or shared sequencers live. This is like all conceptual, but you can bake that into the Tenderman SDK support set out of the box.
00:57:53.442 - 00:58:19.858, Speaker C: So if you actually need that today, that's probably a good option for you. So I almost view it as an order of operations around where teams launch. And historically they've launched on L1. I think now we're seeing that flip a bit, and teams are launching on L two s first as a go to market. I've seen them launch on other EVMs. In some cases, like GMX actually started on BSE. I didn't know that until very recently.
00:58:19.858 - 00:59:00.582, Speaker C: So you can have migrations, and I think once you get to a place where maybe you've outgrown the capabilities, but you need these capabilities for a better experience for your end users, then maybe it can make sense to have your own cosmos chain. So I think it will make sense potentially for more teams over time, assuming those applications grow to scale where they need it. But there's a tension there between having that ability on a Cosmos chain and potentially l two s having that ability through pre compiles and decentralizing sequences.
00:59:00.726 - 00:59:34.898, Speaker B: Dimitri, I want to touch on the three verticals that you talked about at the beginning. How does cosmos compete with in defi in gaming and in NFTs. So, nfts, I strongly agree with you. The culture is something that cosmos simply cannot compete with Ethereum culture wise, or even Solana. Solana and Ethereum both have a very strong NFT culture, and culture itself has very strong network effect. That's very hard to cosmos to overcome. Defi, I think for the most part, I agree with you.
00:59:34.898 - 01:00:27.978, Speaker B: We know for a fact that many blue chip defi protocols are looking to launch their own app chain, and they want to do so on an Ethereum roll up as their own app chain as opposed to a cosmo zone. This is the thing that really pushed me to change my mind on Cosmos in the last six months. Over the last six months, Cosmos is the only thing that I had a 186 months ago, we did an episode where that was when they published the Cosmos 2.0 Vision, Imran, remember that? And we're extremely bullish. And then recently, the move of Ethereum blue chip D five protocols onto their roll up app chains is causing me to not feel bearish on Cosmos, but certainly feel that Cosmos is feeling a huge threat. It's facing a huge threat from Ethereum in Defi.
01:00:28.094 - 01:00:29.862, Speaker A: Could we dive a little bit deeper on the defi side?
01:00:29.916 - 01:00:31.080, Speaker B: I can't share the.
01:00:31.690 - 01:00:45.386, Speaker A: No, no. What I mean is there's some comments that I have specifically regarding Defi. Yeah, go ahead. There's a set of pros and cons for Cosmos and Ethereum. Right. Dimitri mentioned pre compile. Right.
01:00:45.386 - 01:01:40.154, Speaker A: So, like getting some subsidized smart contract computation and then being able to allow some sort of apps being able to run that's cheaply ran versus, let's say, on a layer one. Similarly with Cosmos, you could kind of do the same thing. Right. There are some advantages to running on Cosmos, such as if you think about threshold encryption or if you think about DyDX order matching engine that's on the validator set and then using its own currency as a way to find value in the sense of what the token can do. And so I do think there's some advantages, like if you look at what DX is doing, there's a reason why they moved from. They could have done an app chain, but they ended up doing their own cosmos chain. So I guess what I'm curious more about is what pushed them to do that move.
01:01:40.272 - 01:01:45.262, Speaker B: Imran, you tried uidx after they moved to Cosmos, right? You tried the product.
01:01:45.396 - 01:01:49.070, Speaker A: I have not, no. I've only used it on starkware.
01:01:49.650 - 01:01:50.446, Speaker B: Okay.
01:01:50.628 - 01:01:55.526, Speaker A: But I have not used it because I'm just curious.
01:01:55.578 - 01:02:25.658, Speaker B: What wallets do you actually need to use DyDx on Cosmos? Do you have to use Kepler, or can you use metamask? Because the thing that really boggles my mind about this DyDx decision is once they've moved to Cosmos, they basically have a much smaller user base that already have a wallet installed, assuming you have to use a cosmos first wallet like Kepler instead of metamask. But I haven't tried the product either, so I don't know.
01:02:25.744 - 01:02:37.440, Speaker A: Yeah, I'm just curious. Roll ups people generally use, like l two and l three s, or whatever you want to call it, as a way to scale. But then if you want more customizability, then you want to do your own zone, right?
01:02:38.530 - 01:03:27.758, Speaker C: For now, my understanding, they move for three reasons, maybe to you. One, I think it was difficult for them to build in Cairo. Pretty painful. The second is that potentially a regulatory thing around sharding the order book. And then a third, I think is a pretty natural business model for the market makers who each run that order book, where you kind of get, like Round Robin Mev, where I think those market makers can potentially have a more crypto native business model through transaction ordering. And you potentially don't need to ever even turn on a fee switch if you're able to capture fees through Mev. So those are, in my head, what the three reasons are.
01:03:27.758 - 01:03:42.882, Speaker C: And I think again today, they did need to do that, because you cannot do that on any east l two, because we still have no idea how to fully decentralize a sequencer on ETH L2. But maybe that will change within the next year or two.
01:03:42.936 - 01:03:57.606, Speaker A: I also thought that the reason why DyDX moved was because of the open source nature of the Cosmos SDK. There's just a lot of flexibility and ways to kind of communicate with the rest of the open source community around how they should be building their product.
01:03:57.788 - 01:04:55.494, Speaker D: Yeah, I think the DyDX decision actually came at a timing that at this exact moment of time when they launched, when they announced it a year ago, roughly, yes. Cosmo zone gave you a lot of customizability that is not always impossible at l two, and to some extent, not possible today. Still on L two, but a year after that, actually, many of the options that forced DyDX to do this transition, maybe, except for the MEV around robin part that Dimitri was speaking, are actually happening on the L two. And let me tackle that, because this is some of the interesting stuff that is happening due to modularity so far, either as an L two, either you use EVM and you write solidity code, or you have to go to Starquare and write Cairo. Right. So this was your options a year back. Actually, now I am seeing so many projects using an L two with a custom execution environment.
01:04:55.494 - 01:05:38.914, Speaker D: You can write quas, you can write rust, you can write whatever language you can want and compile it and use it as an L two. So this wasn't possible before. So the developer experience that wasn't possible before as an L two can be possible now. What enables this Buddha, the separation between execution and consensus, that was enabled by optimism, tweaking their bedrock infrastructure to separate execution from consensus. So you don't need the EVM, you separate Gogeth or get the execution engine from the consensus. So you can replace get with anything else. So you can use any state machine, essentially.
01:05:39.042 - 01:05:45.830, Speaker C: And we are seeing many projects it compiles down to MIPs or wasn't.
01:05:46.670 - 01:06:05.710, Speaker D: It can compile up to anything. Once you have a state, you just push the state on the L1. Done. It's like you can do whatever. We are seeing people trying to bring Solana virtual machine on top of Ethereum as an L two. I am seeing so many teams building wasn. I'm seeing even projects that say you can build a stick machine using any language.
01:06:05.710 - 01:06:08.018, Speaker D: So this flexibility is interesting.
01:06:08.184 - 01:06:09.874, Speaker B: And actually, these are not production ready.
01:06:09.912 - 01:06:10.210, Speaker C: Right.
01:06:10.280 - 01:06:19.430, Speaker B: Most of these are still work in progress. Are there actually any developers who's actually writing non solidity, aside from Carol?
01:06:20.330 - 01:06:28.920, Speaker D: But Cosmos wasn't ready. Also, to run arbitrary code like Cosmosm is very new, unless you.
01:06:29.290 - 01:06:33.126, Speaker B: Cosmosm was two years ago, right? It was not that new.
01:06:33.228 - 01:06:34.278, Speaker A: About a year ago, I think it.
01:06:34.284 - 01:07:07.406, Speaker D: Was a year ago, came into production ready environment. It was under development for the last two years. So I think DyDx moved because they would intended to bake their code in the node validator code. So as a precompile, you don't need to write a smart contract, you just write your code as part of the node code. So that was possible before in Cosmos, that was impossible as an L two. But I foresee that within a year or so, this will be possible. You can write whatever code you want as an L two and you can run it.
01:07:07.406 - 01:07:11.846, Speaker D: And this will take one big advantage from the cosmos ecosystem back to the.
01:07:11.868 - 01:07:16.070, Speaker A: L two, which is what Cosmos has as an advantage. Right?
01:07:16.220 - 01:07:22.410, Speaker D: Yeah. And Imran, I think you mentioned that the cosmos people are seeing that as well, right?
01:07:22.480 - 01:07:30.474, Speaker A: Yeah. There was a tweet, Zaki tweeted a couple weeks ago. I don't have it in front of me. But pretty much said that they have about twelve months.
01:07:30.512 - 01:07:31.034, Speaker B: I have it.
01:07:31.072 - 01:07:32.206, Speaker A: Yeah, once you read it.
01:07:32.308 - 01:07:53.250, Speaker B: Here's Zaki's tweet. I think Cosmos social capital has about twelve months to do something unique and differentiated. Otherwise we get swallowed by east flavored variants of cosmos originated ideas like roll ups and eigen layer. I feel intense urgency. The hour is late.
01:07:54.070 - 01:07:55.922, Speaker A: That sounds kind of scary, man.
01:07:56.056 - 01:08:03.250, Speaker B: When I saw this, I was really shocked because Zaki is like, what? At least top ten people in cosmos.
01:08:03.330 - 01:08:04.914, Speaker A: He's one of the co founders of Cosmos.
01:08:04.962 - 01:08:06.294, Speaker B: Is he a co founder or.
01:08:06.412 - 01:08:07.126, Speaker A: I thought he was.
01:08:07.148 - 01:08:10.170, Speaker D: Yeah, he was one of the leader lead developers in the cosmos.
01:08:11.230 - 01:08:51.640, Speaker B: Today he's certainly the top five people in the cosmos ecosystem. And when I saw this, I was pretty shocked. But the funny thing is we actually independently came to this conclusion before he tweeted this from our last few. Like, we talked about this several times already, that cosmos is facing a huge threat from Ethereum. But also, interestingly, this is a bit of history, but it seems like arguably Cosmos is the OG or really the creator, the inventor of modular architecture with all these various independent cosmo zones. Of course, I know you're rolling your eyes already, but no.
01:08:54.330 - 01:09:21.310, Speaker D: It'S a different kind of modularity. If you ask any developer, why do they love Cosmos SDK? It's because the modularity of the SDK, the tendermint consensus engine, is separate from the IPC consensus engine. It's separate from the execution engine. So the modularity was actually baked in the SDK so you can take the SDK and build whatever with it. So it's a different kind of modularity. It's not the staking modularity, but modularity within the SDK. It was great, actually.
01:09:21.310 - 01:09:24.046, Speaker D: And yeah, I agree, they are the OGs.
01:09:24.238 - 01:09:27.342, Speaker C: That's like vertical modularity instead of like horizontal.
01:09:27.486 - 01:09:28.180, Speaker D: Yes.
01:09:28.550 - 01:10:02.262, Speaker B: That really begs the question, because does the first mover advantage really well, I guess. Okay, it's very arguable who's the first mover, but Cosmos certainly the first mover in the modular architecture. Cosmos and Polka dot, both of them actually started in the 2017 18 bear market. And then Ethereum later on converged. The vision of Ethereum converged to Cosmos, not the other way around. Arguably again, but Ethereum's network effect is so strong that all the developers went to Ethereum.
01:10:02.326 - 01:10:27.726, Speaker D: Actually, I will argue again, is that it's actually Cosmos that is moving to the shared security platform that Ethereum was believing in from day one. Cosmos started with the segregated security. Everyone is responsible, every zone is responsible. For an own security. Right. And I would say the most interesting piece of Atom V two vision was the interchange security. And now the opposing proposal from osmosis.
01:10:27.758 - 01:10:35.654, Speaker A: Which is, can you explain interchange security? Because I think there's some new changes. I don't know if you've heard about the latest, which is mesh security.
01:10:35.772 - 01:10:36.006, Speaker B: Okay.
01:10:36.028 - 01:10:46.540, Speaker D: Mesh security is actually an opposing view, and it was led by osmosis. It doesn't came from the cosmos atoms. Okay. Yeah, that's a big divergent. Okay.
01:10:46.990 - 01:10:51.854, Speaker A: But I think that's the model they want to go towards now, which is mesh security.
01:10:51.972 - 01:11:45.790, Speaker D: You will find some interesting community discussions. I predict a municipal war around which one is better, but let's go with that. ICs, or interchange security, is that instead of you as a new project, building their own cosmo zone, and instead of you recruiting your own validators to have your own security or segregated security, you can just recruit some of Atom's own validator, the atom validators to validate on your chain, and you pay them the inflation rewards of your token to this validator. So this kind of centers atom as the hub, as the cosmos hub. It gets the cosmos hub as the central security hub for the whole network. And this kind of resembles Ethereum's vision that the Ethereum exchange. The L1, is the security source for everything else.
01:11:45.790 - 01:12:28.202, Speaker D: So the goal here was to accurate value to the atom holders. So atom token holders will benefit from being the source of security for every cosmo zone. So actually it was a shift from the direction that Cosmos started for to shared security. The missed security is very similar in concept, but instead of everyone going to recruit security from the cosmos hub or the atom zone, no, we will build collateral, like we build bi directional relationship. Osmosis can recruit securities from celestia, for example. And Celestia gets a relationship with injective. And injective have a relationship with whoever, right? Evmos or something.
01:12:28.202 - 01:12:53.950, Speaker D: And by having this bilateral relationships, we will end up by having a distributed network of security that everyone is secured essentially by everyone. So these are two competing visions. And you will find people in the cosmos community advocating that missed security is the thing. That is the real goal. Some people will say ICS is the real thing. So that's why I'm expecting some fun dynamics.
01:12:54.030 - 01:13:06.274, Speaker B: I kind of get why osmosis is pushing for this vision because this will accrue value to their token, whereas the cosmos core developers want to push for the other vision because that accrues value to the atom token.
01:13:06.402 - 01:13:06.886, Speaker D: Yes.
01:13:06.988 - 01:13:13.590, Speaker A: There begs the question about the Cosmos hub. What is its role within the entire cosmos ecosystem?
01:13:13.670 - 01:13:24.030, Speaker D: The OG guy whose hub is the OG guy in the cosmos ecosystem, there's no canonical.
01:13:24.450 - 01:13:36.702, Speaker C: The cosmos hub is the one where there's the most connectivity and the most liquidity. There's no canonical. It's a Game of Thrones around who gets to be the de facto cosmos hub.
01:13:36.766 - 01:14:00.540, Speaker D: But it lost the seat twice, by the way. I would argue that Terra deceited atom for a while and then osmosis unseated atom for a while. So terra collapsed and disappeared. So it's out of the story. Osmosis is still kind of a competitor, kind of, to the Game of Throne game on Cosmos. So we'll see. It's interesting.
01:14:00.990 - 01:14:03.420, Speaker C: It's a fun reality TV show.
01:14:04.110 - 01:14:05.100, Speaker D: It is.
01:14:05.870 - 01:14:27.630, Speaker B: Back to your original analysis of gaming, NFTs and Defi. So I said I agree with you on NFTs and Defi. I think gaming, it might be a little bit too early to draw a conclusion. I think everything is so experimental. There's no game that actually works. There's some network effect around optimism. Like the Zero X Park guys mud.
01:14:28.530 - 01:14:32.500, Speaker A: But it's still theoretical stuff that they're building.
01:14:32.870 - 01:14:37.282, Speaker B: Exactly. It's too early to say. Cosmos is out of the question.
01:14:37.416 - 01:15:08.014, Speaker C: Yeah. On the web 2.5 games, it's probably a question of distribution partners where if you do want to bring in traditional game studios, maybe there is a play there around having a really strong game studio just ship crypto enabled games. I don't want to call them crypto native. It's more basic. There's some assets that are on chain but not fully full game state. I think what Argus is building is very cool.
01:15:08.014 - 01:16:03.630, Speaker C: That's more of probably the web3 flavor everything fully on chain. I think a lot of that is also around. To what extent are you aggregating the brains that are actually building this? And it's a very different concept and mental model of what a fully on chain game is and what the notion of an autonomous world is. And they're also still competing against mud on optimism, Dojo on Starknet, two very interesting frameworks. And then where are the builders? And how do you get the attention of the people who are crypto native? They know how to build these autonomous worlds and they are willing and able to do it on an ecosystem outside of an E L two. And maybe there are. I guess to your point, it's still an outstanding question.
01:16:03.630 - 01:16:16.422, Speaker C: It's more of a thing that the cosmos projects need to prove. Like the burden of proof is on them, on whether you can amass enough of the intellectual firepower to build something unique and interesting there.
01:16:16.476 - 01:16:24.550, Speaker A: Well, speaking of mud, there's also Argus labs, right? That's ran by Scott Sunardo, which is building.
01:16:24.620 - 01:16:25.926, Speaker B: They're on Cosmos. Yeah, that's right.
01:16:25.948 - 01:16:40.426, Speaker A: They're on Cosmos. Yeah. I haven't dug deep into what they're building exactly, but taking a quick look at the Twitter, some sort of EVM based shard, not sure exactly what that means and haven't read through it yet.
01:16:40.448 - 01:16:58.238, Speaker B: But for context, they're the creators of Dark Forest, which in my opinion is the closest thing to giving us a magical moment similar to DeFi summer. It's not at the same level as Defi summer, but it was pretty magical.
01:16:58.414 - 01:17:05.730, Speaker A: Yeah. Okay, so they're introducing world engine, which is a sharded roll up SDK built to horizontally scale on chain games.
01:17:06.230 - 01:17:27.930, Speaker C: Yeah, I mean, that's been very stealth about this. I think now is being more public. Yes. Sharded roll up with a shared settlement layer, and that settlement layer is probably going to be some cosmos chain and there's probably some customizability there. Probably a game engine that's baked in. I haven't spoken to Scott about it. I'm not sure what the details are.
01:17:27.930 - 01:17:48.542, Speaker C: And the question is, can you execute that in a standalone cosmos chain or can you execute that with an l two and instead of shards you have l three s that settle onto that l two and then you bake in custom mud pre compiles in every instance.
01:17:48.686 - 01:17:49.282, Speaker A: Interesting.
01:17:49.416 - 01:17:59.166, Speaker B: Can I get a quick gauge of sentiment? Bullish, neutral or bearish cosmos? Because I'm hearing conflicting things again. Dimitri, bullish, neutral.
01:17:59.358 - 01:18:03.830, Speaker C: Yeah, bearish. Bearish short term? Neutral long term?
01:18:04.250 - 01:18:05.186, Speaker A: I'm neutral.
01:18:05.298 - 01:18:24.170, Speaker B: Well, can you actually be bearish short term than neutral long term? Because I kind of agree with Zaki that they have a twelve month window of opportunity. So if they don't succeed within the next twelve months, succeed, quote unquote succeed. Do they really have another shot at dethroning Ethereum?
01:18:24.250 - 01:19:20.798, Speaker C: I guess I'm slightly optimistic of them executing something interesting, but still not fully convinced that even if they execute, it's going to be powerful enough to actually have some particularly long standing ecosystem or value prop. I don't consider myself an ETH, Maxi. I'm more of like an ETH realist around in the context of history or the course of years or decades. I don't know. Try not to fight momentum. You know, just like, like lean into where things are, are heading and just try to build on top of that and and not have, like, not try to step on each other's toes and build something for end users that will onboard the most retail, the most institutions, because that's where if we don't get that, the whole space dies. That's still my north star around.
01:19:20.798 - 01:19:45.478, Speaker C: How do you 100 x, the users, the liquidity, the developers, the applications, and if you see something has the most inroads to the real world, that's your largest market as an entrepreneur. So why do you want to take unnecessary platform risk if you could avoid it? Unless there's a really good reason to.
01:19:45.564 - 01:20:43.802, Speaker B: Personally, I don't know what to make of cosmos, but I want to share one counterargument that haven't been said to the bearishness towards cosmos. The counterargument is that yesterday Imran and I spoke with a founder from Cosmos and the guy, I love the guy, a missionary builder and super smart. And the vast majority of the conversation revolved around whether or not he's bullish on Cosmos in the long term. And why? Because that's the biggest risk for any Cosmos builder, is the ecosystem wide risk, not the product specific risk. And the answer that he gave, that I really like is the fact that he said pretty much anyone who has ever written solidity code, and Cosmosm says that Cosmosm is a ten x better developer experience. And anecdotally, I heard the same thing from independent sources. I haven't verified myself because I never wrote any code in cosmosm, so I can't comment on it.
01:20:43.802 - 01:20:47.594, Speaker B: But this is a thing that I've heard a few times now.
01:20:47.792 - 01:21:00.478, Speaker A: Also, the community. He mentioned that he came from the Berkeley community, and everyone in the Berkeley community is like pro Cosmos, even the.
01:21:00.564 - 01:21:01.182, Speaker D: People that are coming.
01:21:01.236 - 01:21:02.720, Speaker A: And Demetri, you went to.
01:21:03.250 - 01:21:06.798, Speaker C: Oh, yeah, I went for sure.
01:21:06.884 - 01:21:10.346, Speaker B: I forgot he called Cosmos the Berkeley chain.
01:21:10.538 - 01:21:11.280, Speaker A: Yeah.
01:21:11.970 - 01:21:42.122, Speaker C: It's going to be painful. Yeah, I guess, because you write like rust, I think, has a nice tailwind because it's used by multiple chains, I would argue, like JavaScript. It's awful, but people use it. There's a lot of tooling around there. There's a lot of security firms that are working on solutions to make it safer, I think. Yeah. If I'm to give maybe an area where also they could compete, it's around hardware resource provisioning networks.
01:21:42.122 - 01:22:24.090, Speaker C: They kind of have like a cache on there. But can you basically have a way to coordinate a lot of machines to provide some compute, storage, GPU, CDN, whatever. And if you do that, then you need a way. Well, you by nature have a lot of nodes, and you likely need some state machine. You need a way to meter resources based on very specific things that those machines do. I think of like the Jensens of the world, though they built on substrate. I think you could have that variant be built on Cosmos, where they provide GPUs for AI model training.
01:22:24.090 - 01:23:01.794, Speaker C: Very computationally expensive, a lot of cost savings that you can get, not because of the decentralized network itself. The redundancy actually makes it more expensive, but just the supply and demand, just having more machines online that's globally distributed relative to ones that you could stuff inside a single data center. So that's also a pretty unique case, because even if you. I don't think you could get to l two sequencer decentralization that is that broad, where if you need hundreds of nodes, like thousands of nodes, that's particularly where you're probably better off with the cosmos chain.
01:23:01.842 - 01:24:02.794, Speaker A: I was listening to one of Sonny's podcasts, and he talked about the differences between Ethereum and Cosmos. And one thing he said was, Cosmos takes an app first approach, whereas maybe Ethereum takes more of an infrastructure first. That, and he tied it to a very clear example, which is like Google built out search, right? And then from there they work backwards into figuring out the infrastructure that can continue to help support search. Same thing with Facebook as an example. Then he drew the example to Cosmo and said, like, look, we build up this very modularized infrastructure so that anyone can build any type of app that's fully customizable to the app level, which Ethereum can't do today, all the way today. And that is the edge that Cosmos has versus Ethereum. Obviously, Ethereum is starting to converge on the same path, but the question is, are they fully there? From a parity perspective? And what your thoughts are typically around.
01:24:02.832 - 01:25:16.818, Speaker C: That thesis, I would ask, what burden are you putting on your developer and what needs do they have? I would argue in many cases, if you just want to create a monkey PFP, you don't need to have a whole chain for that. Or if you just want to create, like a marketplace, you probably don't need a full chain. So it probably goes back to the type of application. My sense is there's a much smaller pool of application devs that are willing to go through that pain of setting everything up. Like launching a network is very difficult, and if you are a smart contract dev, maybe you don't want to deal with that. So I think that customizability exists and is useful for devs, but probably not as many as we thought, and a lot, probably just want to deploy a smart contract and not worry about how do I bootstrap a validator? How do I pay for security and worry, oh, if it's reflexive security, how is that going to break my app? I think that's where there's a smaller pool of applications where that makes sense.
01:25:16.984 - 01:25:32.886, Speaker D: Yeah, I agree with Dimitri on this one. And to this comment, Emran. I think Cosmos is built to make everything easy. But they missed the part about value capture. Like where is the value capture in this? Don't create Linux again. Linux is a very versatile tool. Everyone can use it.
01:25:32.886 - 01:26:07.570, Speaker D: Everyone can tool. But they didn't figure out a value capture mechanism. So ethereum in this case is more similar to Microsoft or Apple when they have built, it's still open source and everything, but they have built something that will keep getting the user back and keeping locking the user. And now Microsoft is following Linux essentially right by WSL. So I think from a long term perspective, the approach from Ethereum may work better over like 2030 years.
01:26:07.720 - 01:26:44.080, Speaker A: All right, well, I know this is probably one of our longest recordings. I think this is the longest recording of all the sessions that we've done. And I know we could have kept going on maybe to close out this conversation. Could there be a world where both can coexist? Both there could be thousands of mini apps that run on Ethereum and maybe some larger. You mentioned Dimitri, like infrastructure heavy apps that could run on. Could there be a light at the end of the tunnel for Cosmos where they can both coexist? I'm curious to hear your opinions on.
01:26:44.450 - 01:27:32.006, Speaker C: I mean, again, it depends on the use case and the market timing. I think Cosmos was a decade too early. In general, I think they were solving for the right things, but at the wrong time. And I think more applications as they hit scale and they need a lot more customizability probably makes sense. And different flavors where you have my sense of the application that might make a lot of sense are the hardware, resource provisioning networks. I think that as a sector has also been very difficult because the UX to their buyers hasn't really been addressed well. And they've been trying to find what kind of customers make sense.
01:27:32.006 - 01:28:10.158, Speaker C: Like for example, there's this CDN project that was going to the YouTubes of the world and they were saying, oh, we could reduce your CDN cost by 80%. And that customer said, well, it's a cost, but it's not a cost center for me. I would double my CBN cost if it meant increasing market share or user base. So I think that flavor of blockchain app still hasn't found the use case or the customer base. That makes sense. And I think they're still in the idea maze. I think you can compete on cost if you have at least some equal performance uptime and UX.
01:28:10.158 - 01:28:51.702, Speaker C: So I think when they have more feature parity there, I think you could have these distributed network of nodes and you can do a lot of cool stuff around customizing it. I think we're still excited about that sector. It's just been a longer time for a lot of those applications to hit PMF, but I think there's something there. So I think that flavor works particularly well for Cosmos chains and the Dy DX flavor. So I think it just might be like a more niche segment. But I feel optimistic that they might find something. But you just need to be mindful that it's not just the Cosmos devs that are looking at these things.
01:28:51.702 - 01:28:57.362, Speaker C: It's also folks that are building their counterparts on ETH roll ups.
01:28:57.506 - 01:29:42.002, Speaker B: I have a similar feeling. For the most part, I don't think Cosmos and Ethereum can coexist in the long term. One of them will crush the other. But if Ethereum crushes cosmos, I think cosmos is probably not dead. It's going to remain some niche. And what I'm seeing now is you have these chains that are for the most part infrastructure chains, like Axelr, Thorchain, despite being a cross chain Dex, I consider that as infrastructure because it's really about bridging. Right? Thor chain band protocol hits an oracle and there's a bunch of these middle layer infrastructure stuff that are built on cosmos.
01:29:42.002 - 01:30:05.050, Speaker B: And that seems to be the niche that cosmos has found so far. These products would prefer to build on cosmos than on an Ethereum roll up app chain. And probably for good reasons. I don't know what the reasons are, but there's probably really good reasons why the specific group of projects all gravitate towards cosmos. So that's my view on things long term.
01:30:05.130 - 01:30:40.394, Speaker D: I agree with this view. By the way, some chains or some applications would prefer to have their own infrastructure, everything under the control. If DyDX kept growing the way the same rates that you have been growing, I think it's better for them to stay as an abbey chain, have their own validators even. Why not games? If they become really big, then why not? But overall, smaller applications or like the liquidity hub can be. I think Ethereum will win overall over a long enough time, and we talked.
01:30:40.432 - 01:30:58.990, Speaker A: A lot about this chow, which is like, is cosmos too decentralized for its own good, versus Ethereum, which is decentralizing factor of groups. Right. You have the research group Lord Vitalik that's at the helm. Right. Then you have all different research groups. Then you have these communities that are underneath it. That's kind of propelling Ethereum forward.
01:30:58.990 - 01:31:25.560, Speaker A: Right? Compared to Cosmos. I don't even know who to look at. I look at Zaki and maybe so. And then they just got a grants program that just started a couple months ago. Right before that, they didn't even have a grants program that you could apply to. So I feel like a part of this slowness against Ethereum is also partially how it's organized from a social construct perspective. Cool.
01:31:25.560 - 01:31:29.762, Speaker A: Well, thank you, Dimitri, for joining us. It was a great chat.
01:31:29.826 - 01:31:31.478, Speaker C: Yeah, thanks for having me. This was fun.
01:31:31.564 - 01:31:39.380, Speaker A: For those that haven't hit subscribe. Hit subscribe. Thanks again for joining us. Thanks for listening to good game. Don't forget to subscribe. We'll see you next week.
