00:00:02.890 - 00:00:30.630, Speaker A: Hello, everyone, I'm Josh, I'm the co founder at Astria. We're building a shared sequencer network. The talk I'm going to be giving today, somewhat similar to like the two previous talks, we're going to have boxes and arrows. We're going to define a lot of terms and argue that a lot of them are kind of similar things, but it's called how super builders unlock shared sequencer superpowers. We're going to start by defining a very high level understanding of the MEV supply chain. We're just to define it as this. A user talks to a builder, there could be a searcher in the middle, but we're going to try to keep the numbers of boxes small.
00:00:30.630 - 00:01:14.942, Speaker A: So user goes to builder, builder goes to relay, relay goes to proposer, proposer goes back to relay, relay goes to network. That's going to be the starting point for how the MEV supply chain is going to work. And we're going to also define a bunch of terms. You don't have to agree with these definitions, but these are going to be the ones we build the talk on. So what is a sequencer? We're going to get that a sequencer is an entity that's making a commitment to an ordered list of transactions at a specific block height that is intentionally very similar to what a preconfer would be defined as. What is a shared sequencer? We're just going to say a shared sequencer is a sequencer that is used by multiple roll ups. And then we're going to say, what is a lazy sequencer? And we're going to say a lazy sequencer is a sequencer which does not execute the state machine of any roll ups.
00:01:14.942 - 00:01:49.134, Speaker A: These are important terms. When we say it doesn't execute the state machine, if we go back write a sequencer, it's making a commitment to an ordered list of transactions. It is not necessarily making a commitment to a state route that is the result of the execution of a given state machine over said list of transactions. Right? So that's what we're going to be building on. If we look at this, this is a rough high level of how, kind of like blockchain flow, right? You have some unordered transactions, you order the transactions, you then execute the transactions. You're going to get a state DB and you're going to get a state root. This is what I would argue as a lazy thing.
00:01:49.134 - 00:02:23.974, Speaker A: All you are doing is taking the unordered transactions and then you are ordering them into a block. Why would we do lazy sequencing. Fundamentally, it's because of resource requirements. If we are assuming we are using a shared sequencer, which is kind of like our kind of work, it is infeasible for all of the sequencing nodes, the validators, whatever, of this shared sequencing chain to store the state DB of multiple roll ups. Right. For every roll up you add to the shared sequencer, you are increasing the amount of state and resources that are necessary for this kind of sequencer set. So we use lazy sequencing.
00:02:23.974 - 00:03:03.314, Speaker A: Get around that, we will go into how you are resolving the inherent kind of problems of that what is a builder? This is the definition of a builder that Stefan put in the original kind of mevboost writeup. It is a party specialized in the construction of Ethereum execution payloads using transactions received from users and searchers. We for us are not specifically focused on Ethereum, but it is useful to say it is a party specializing construction of execution payloads. Right. What is a superbuilder? We're going to use a simple definition. A superbuilder is a builder which constructs execution payloads for multiple roll ups, right? So similar to how we have a shared sequencer. Shared sequencer is a sequencer to be used by multiple rollups.
00:03:03.314 - 00:03:44.286, Speaker A: A super builder is a builder which creates execution payloads for multiple rollups. Again, what is a relay? Again, we're using Stefan's original definition from the MeV booth thing. It is a party specialized in DOS protection and networking who validates and routes execution payloads to proposers. If we go back to the original kind of MeV supply chain, Doc, that's kind of the framework we're going to be talking about optimistic relays. I'm going to define an optimistic relay as a relay that does not validate the execution bailout. That's not strictly true. If we look at the optimistic relay Allah, I'm forgetting the name of the Justin Drake one, but it's in the PR, the validation actually done asynchronously.
00:03:44.286 - 00:04:20.286, Speaker A: But theoretically you could have a block that is sent to the optimistic relay that is accepted tight enough to the timing window that the out of band asynchronous validation is not done, and therefore you could theoretically get a payload in that is not valid, and then you have staking and slashing. That happens because of that. So we're going to think about optimistic relays very intentionally. An optimistic relay is going to be somewhat similar to a lazy sequencer in that both of these things do not do the execution of these payloads. Why are we doing optimistic relays fundamentally, it's because of latency. And as many of the other speakers have said, latency is very important in this. Right.
00:04:20.286 - 00:04:59.050, Speaker A: We're playing timing games here. If you can move the execution either completely out of the process or into an asynchronous process, you can have tighter timing on submitting these execution payloads. So I'm going to make the argument like shared sequencers and incentivized relays. I gave another talk earlier this week kind of on this topic. We're not going to focus on the incentivized bit much on this talk, but the idea is that shared sequencers are very structurally similar to relays. One of the things from the other definitions is that a lazy sequencer is very similar to an optimistic relay. So again, going back to our Mav supply chain user builder relay proposer network, right, this is what Asteria's architecture at a high level looks like.
00:04:59.050 - 00:05:27.778, Speaker A: We're going to assume we have roll ups and we have like a little roll up box and a large roll up box that we run on a modular, kind of like sidecar architecture. The thing that is kind of key here going to be the roll up. Just think it's a guest node, op, guest, whatever that is. The EVM, a user is going to be assumed to interact with the roll up node itself. The core thing we're going to be focusing on for this talk is the composer piece that is in between the roll up and the sequencer node. Don't think about that relayer. I did not have time to update my diagrams, to not have that.
00:05:27.778 - 00:06:14.162, Speaker A: But I know that can be confusing when we're talking about relays and relayers and whatever, but we're focused on roll up to composer to sequencer. I'm going to make the argument that a composer is equivalent to the searcher or builder part of this. When we kind of came up with the concept of the composer, that's kind of like an internal, kind of like technical name, really what this is. We would call it the Mev box. And that was kind of constructed because again, in this lazy sequencing design, we wanted to make sure that when we were constructing this kind of novel system, the connection between the roll up node and the sequencer was not like hard coded. So we are not putting modifications into the actual, say, geth code such that it knows how to submit a transaction to the sequencing node. We are assuming that there is actually like a middle layer there, and we've constructed APIs that way.
00:06:14.162 - 00:07:03.518, Speaker A: And that is, again, because we define this as like the MEV box, we assume that there will be a more sophisticated kind of actor playing that role of relaying transactions from a RPC node that a user interacts with to the sequencing node. And so I'm also going to argue that a sequencer in this case is an optimistic relay. Again, it is lazy sequencer, right? It is not doing the execution of this. So what we have in this construction is we have users talking to a roll up RPC node. We have this MEV box, this like generic MeV actor that is responsible for relaying transactions from that RPC node to a lazy or an optimistic relay. And so very intentionally, this looks similar to what we see in the Ethereum layer, one kind of PBS MEV supply chain. Today, though, this is kind of specifically focused on like roll up architectures here using a shared sequencer.
00:07:03.518 - 00:07:32.486, Speaker A: So I'm going to argue that kind of the flow here at a very, very high level for kind of how users interact with blockchains generally. So you have a user, it sends a transaction to an RPC, that RPC is going to do a stateful check on a transaction. And I say stateful check. That is intentionally a generic term. If we look at the Cosmos SDK, there is a concept of like a check TX that literally does both stateless and stateful checks. As you're saying that transaction is valid. The RPC's job fundamentally at a high level, is to get that transaction to a block producer.
00:07:32.486 - 00:08:00.754, Speaker A: Again, we keep that intentionally, very generic. How that a lot of different kind of implementation details for that happens across different architectures. But fundamentally, the RPC's job is saying, a user gave me a transaction. My job is to get that transaction into a block producer's hands. And the block producer's job, obviously to produce the block, but then to also propagate that block to the rest of the network. If we look in maybe like an MEV supply chain thing, how this would maybe practically be implemented is that a user sends a transaction to a searcher and or builder. These could be like integrated.
00:08:00.754 - 00:08:54.646, Speaker A: They could not. A builder is going to do a stateful check on the transaction and or bundle. And maybe it's like the transaction, the bundle, the block, right. But fundamentally, in this case, and especially, we're assuming an optimistic relay or a lazy sequencer, the builder is the entity which is actually doing the stateful verification of, again, this transaction, this bundle, the entire block made up of a combination of transactions or bundles. This is kind of fundamental to the reason we are comfortable using a lazy sequencing and or like optimistic relay design, because the question is always going to be, what is the guarantee a user gets that the kind of transaction was both included but also executed? And our assumption is that economically, it's not desirable for a builder to actually be submitting these blocks that it has executed, that it knows are going to kind of revert or not go through. So we have an entity that is responsible for doing that. And so I'll touch on that a little bit later.
00:08:54.646 - 00:09:35.630, Speaker A: The builder's job, right, is to get this transaction to the relay, right. We have other diagrams for Kaido's slide, right? And the relay's job is to get this block to the network that has some problems in it, where we saw with the low carb crusader attack. But fundamentally, that is the kind of job, kind of stack we're going to assume and how we assume it works in with a shared sequence architecture is very similar to that. This is like a very large kind of like extended diagram. But this is fundamentally what we think like a theoretical architecture could look like. We can reduce some of these boxes, but fundamentally a user, again going to interact with a roll up RPC node, that roll up RPC is going to have some mem pool. A searcher is going to be the entity that retrieves the transaction from the rollup's mem pool.
00:09:35.630 - 00:10:16.610, Speaker A: It is going to then build this block. Whether or not we have builders being super builders, or whether superbuilders are yet another entity that is aggregating bundles from builders itself, that is very much an implementation detail. We assume the superbuilder is going to get a block to the shared sequencer. Proposer, remember, the definition of the shared sequencer is that the shared sequencer is an entity which is doing sequencing from multiple roll ups at a time. So we could call this block a super block. But we assume that fundamentally what a shared sequencer is doing, it is having multiple roll ups share a tick or a block time. So we assume that you have one block for all of these roll ups going through a single network at the same time.
00:10:16.610 - 00:11:04.946, Speaker A: And that is done by this nebulous super builder entity. I'm going to just end with kind of three kind of potentially controversial kind of statements about equivalents here. So what are they like? Atomic multi roll up execution can be done via the simulation of multiple roll ups. And then again, referring to all the kind of definitions and the different parties in the system, we assume that the entity that will be doing this, we're just going to tag that as like a super builder, right? It is this entity that we presume is incentivized through their own means to create singular bundles, right? These like super blocks or whatever, meta blocks, whatever you want to call them, that include transactions, bundles, whatever, for multiple other roll ups. And it is simulating those against each other. Again, I am not like a builder expert. There are many people in this room more experience in how the specifics of that knapsack problem goes.
00:11:04.946 - 00:11:55.360, Speaker A: But we assume that these entities are economically incentivized. And the key thing is that they are willing to pay the resources to run these kind of simulations over multiple chains. And we want to abstract that job out of the kind of network sequencer role, because we want these sequencers, validators, whatever you want to call them, to be lower resourced actors. And that is why we did the lazy sequencing in the first place. But if we want to be able to get this atomic kind of cross roll up execution thing, we need some entity that is willing to actually do the full simulation, including the execution of each of the state machines on all the roll ups together to create this singular block. We assume the builders are theoretically the party that is kind of incentivized to do that. Again, shared sequencer enables multiple roll ups to share a single block that is kind of like the core base of all of this.
00:11:55.360 - 00:13:11.586, Speaker A: When we talk to people about what are the pros cons, trade offs of using a shared sequencer, I think one of the fundamental things that people need to accept if they want to use a shared sequencer, that they're going to have to accept like a shared block time, right? Because that is fundamentally like the base primitive that a shared sequencer is kind of like selling. If you want to have this kind of atomic composability, you need to have a shared kind of, I'll say like a temporal domain, such that all of these roll ups actually have one kind of concept of time at which they kind of progress, right? And that goes to kind of a shared sequencer could be thought of as a settlement layer. And that term is controversial. What does that mean? But you can think of a shared sequencer as this settlement layer for intents, right? If we see an intent, we assume that is a transaction. That transaction could be across multiple roll ups, right? You could theoretically have a UI, and that UI is representing a single interface for a user, and it constructs a transaction or an intent. And that intent could touch multiple roll ups. And that intent could say, hey, I want know buy on roll up a, I want to sell on roll up B, how am I going to settle that? Intent, you're going to assume that a searcher and a builder are going to actually construct a block for a single shared sequencer such that it lands this payload in the single temporal domain of the shared sequencer.
00:13:11.586 - 00:13:51.154, Speaker A: And so that's kind of our argument for shared sequencers. Thoughts on economics here. I will actually argue that I did not, unfortunately, have time to put together a lot of concrete thoughts. I think fundamentally, one of the important things to think about, like what are the situations in which you might want to have multiple roll ups versus like one roll up? What are the situations in which we would see this kind of arbitrage opportunity between multiple domains be desirable? And what are the opportunities where we want it to be atomic? Right. We want to be one kind of shared tick. I think people, again, knowing more than me in this room, but we've already seen the kind of atomic arbitrage opportunities in MeV kind of decay already. And people are kind of seeing more profitable opportunities.
00:13:51.154 - 00:14:25.600, Speaker A: They're being able to outbid the purely atomic kind of arbitragers by kind of being able to take probabilistic risk. Right. And a little bit of kind of like directional risk and timing risk by holding one leg on like a separate domain. So we do think that kind of the game of working cross domain in building as like an arbitrager has already been kind of explored and is relatively standard. Again, this is not necessarily my area of your expertise here, but we do. Again, there are more thoughts on this, but not kind of like in a thing. Happy to kind of have Q and A on this.
00:14:25.600 - 00:14:46.790, Speaker A: Okay, references if people are interested. This is going to have just a bunch of links to a bunch of different stuff. Shared reference link between. A talk I gave earlier this week. This is going to go into a lot of kind of like how. I'll just say like, order flow, supply chain goes across multiple domains. I evaluate kind of like cosmos and Solana and Ethereum.
00:14:46.790 - 00:14:52.740, Speaker A: Lot of good talks that go into more depth on each of these kind of individual things. Thanks, everyone.
