00:00:02.410 - 00:00:47.642, Speaker A: This is project, one of our most ambitious data based projects yet. And the goal was to bring some more sophistication to the way that we look at builder strength and the competitiveness of the MEV boost auction. This is joint work with Tavas and Malesh, who work with me at SMG and Shun, who's one of Malesh's colleagues at Rice. Just a little agenda. I'm going to talk about what we did, which is called structural estimation. It's kind of a technique from the empirical auction literature and economics. I'm going to talk a little bit about kind of that first, which is how does an auction theorist look at the data set? Oh, yeah, sorry.
00:00:47.642 - 00:01:49.680, Speaker A: Okay, is this better? Good. So I'm going to talk a little bit about structural estimation, and then I'm going to talk about our model and our results. Then I'm going to talk about what does this mean for improving validator revenue. So the data set that we look at is just the bids, basically, and what that means in the context of the MEV boost auction is the winning bid doesn't necessarily reflect the value that the winner had for winning the block. So they may bid less than their value, either because they have some order flow that other people don't have, or because they're subsidizing they might bid more than the fair value. We want to explain those things away with game theory and then use the bids to get a more accurate view of what the fair value is in the system. So that's the main idea, is basically use game theory, make some assumptions, and use those assumptions to get the bids to tell us more than just what the winning price was or what the bids were.
00:01:49.680 - 00:02:58.920, Speaker A: This is kind of an active area of research in economics, which is kind of why we started looking at this was basically, there's a ton of papers in econ that try to do auction empirics. The main limitation for them is that they don't have very many observations. And so when they try to do more complicated analysis, they have to either restrict their data set even more to get a bunch of like, for example, on eBay, there's tons of auctions, but there's only a few for any given item. Like if you want to look at auctions for the same model of a dell laptop, you can do that, but then you've restricted yourself down to only on the order of thousands. So you can see, I just have some selected papers here, and the size of the data set is in the thousands or even in the hundreds for many of them. Whereas for us, with Medpoost, this is a great part about medboost is that we have every 12 seconds almost, we have a new data point coming in. And so we're already over a million auctions, and it's growing like 200,000 observations a month at this rate.
00:02:58.920 - 00:03:26.442, Speaker A: Okay, so this is basically a picture of how an auction theorist views an auction. At the top, you have bidders. Each bidder, instead of having a value, they have a distribution of possible values. Every instance of the auction, they draw a value. So think of this like you're playing poker. You have a bunch of different possible hands from Ace, ace to deuce, seven offsuit. That's your range.
00:03:26.442 - 00:03:44.546, Speaker A: That's the distribution up here. Each time you play a hand, you draw two cards, you draw your value. In this context, it's like you see the private order flow that you got. You see all of the other things. You see the price movements, et cetera, and you construct your block from that. So that's your fair value for the block. You know that.
00:03:44.546 - 00:04:17.226, Speaker A: And then what we actually observe in the data is the bids. And actually, we may have some information about the fair value, too, because we can see only the block that won. We can see what went into it. But mostly what we observe is just the bids. And that's what we're going to be trying to do here is go from these bids back to what the distributions are. And I think a lot of people have tried to do this by looking at what's in the block. And that only tells you about the winning block.
00:04:17.226 - 00:05:00.038, Speaker A: It doesn't tell you how competitive things were. And there's some reasons that that might not be perfectly accurate as well as we saw in Danning's talk earlier. So just a little review of this is what the auction looks like. It's open English. It happens over the course of 12 seconds, and bidders are submitting their blocks over time. They're kind of outbidding each other if they think that they have a higher value block than what the prevailing bid is. And in the structural context, we are going to make an assumption about this that says if you're not the winning bidder, then your value is less than the winning bid.
00:05:00.038 - 00:05:19.134, Speaker A: Basically, you don't stop bidding until the value is higher than what you can match. And that is not that strong of an assumption. I guess the place where it would break is latency. And I would just say, in econ, all assumptions are wrong. Some are useful. All models are wrong. Some are useful.
00:05:19.134 - 00:05:56.794, Speaker A: So this is just an assumption that we have to make in order to get anywhere. And it's a relatively weak assumption. And what it allows us to do is kind of construct the distributions of the order statistics. So what are the order statistics? If you roll two dice and then you look at the lower one, that's called like the first order statistic. And if you roll two dice and you look at the highest die roll, that's the second order statistic. So here, these come up a lot in auctions, because obviously the highest bid is the winning bid. The second highest bid tends to tell you a lot about what the revenue is going to be in the auction.
00:05:56.794 - 00:06:50.110, Speaker A: So, a really competitive auction is one where the second highest bid is close to the first highest bid. And so once you make this assumption, you can kind of back out what the distributions of these second highest values are, and you can use that to do some inference. And that's exactly what we do. The only other thing that we're going to add, which I think is useful to think about, is that there's common and private components of this. So there's a bunch of data in the public mem pool, there's a bunch of private transactions that actually get submitted to all builders. And then there are other private flows that don't go to all builders, or trading alpha that doesn't go to all builders, or latency advantages or whatever, that don't go to all builders. So those are in the CT, the common component, and the epsilon T, the private component, respectively.
00:06:50.110 - 00:07:43.086, Speaker A: And the other thing is we're going to look at, because we're very interested in this at SMG, is what's the difference between independent builders who don't have a trading arm, and integrated builders, who do have a trading arm. So we're going to look at trying to estimate the difference across type. So, I guess I'm going to skip through most of this math because it's more interesting to look at the results. But basically, we're going to model it as log normal, which is another assumption that we're making here. We use these moment conditions. If you ever taken an econometrics class, you'll know that this is like basically 90% of what you do is find these moment conditions and then run an optimization problem on it. And to model the common component, we tried to go and find some good signals of that.
00:07:43.086 - 00:08:05.062, Speaker A: Like for example, we use the previous twelve second ETH USD price movement on finance. If you run a regression, a linear regression, that explains about 10% of the variance in the winning bid. So the price moves a lot. There's usually arbitrage. That means the block is more valuable. We also use the base fees because we thought that might do something in our data set. We also have some mempool data.
00:08:05.062 - 00:08:38.174, Speaker A: So we have like the number of transactions in the mempool. We can use lagged gas prices. We can use block gas. We can also do some things with the winning block as well. Here's the results, and then we'll look at an example. So it's more clear, but in bold, that coefficient is the coefficient on binance ETH price movement in Bips, basically. So what we see is, for integrated builders, that coefficient is higher, significantly higher than it is for non integrated builders.
00:08:38.174 - 00:09:15.034, Speaker A: It's not zero for non integrated builders because they also have other people submitting to them. And they would submit the arbitrage bundles to the neutral builders as well. But we do notice a difference, and we kind of expected to see this between the coefficient on finance price movement for the integrated and for the non integrated builders. So how does that look? What is our estimate for a specific block? So, in this block, base fee was 30.2 guay. The price movement was 2.81 bips.
00:09:15.034 - 00:09:53.370, Speaker A: Not very much. The winning bid was from Rsync. They paid. The second highest bid among non integrated builders probably was titan was zero, two ETH. Okay. And our predictions, obviously, we're not going to have perfect predictions, but we're pretty close here. We predicted that builders have values distributed according to zero point 126 times this log normal distribution.
00:09:53.370 - 00:11:12.238, Speaker A: And the non integrated had bids that were distributed with a lower mean and a slightly higher variance. So the general finding is that integrated builders value distributions have a higher mean and slightly lower variance than non integrated builders did for typical values of x. I would attribute the low variance part to basically the strategies for non integrated and alpha for non integrated builders are kind of more different than they are for the integrated guys. I think the integrated guys are pretty similar. Okay, so what does this mean for, how do you make more money with your validator? If you know this information, how is this useful? Well, the econ literature basically suggests that when some bidders are stronger, meaning they have shifted or stretched valuation distributions relative to others. Our classic result of revenue equivalents, which says no matter which auction you run, you're going to get the same revenue actually breaks. And what it suggests is that in these situations, with specific types of stronger bidders, a first price auction will actually be higher revenue.
00:11:12.238 - 00:11:59.594, Speaker A: So the implication is you're a validator, you can run whichever type of auction you want as long as you have the relay implemented for you. Maybe you should explore running a first price seal bid instead of an open english auction if you want to increase your revenue. Last thing here is my data wish list, which is because I knew there was going to be some relays here and I just wish we knew which relay won. I think that's the big thing missing from the data. It would help us with timing games research as well so we can kind of get it at consensus. We run some nodes and we can get it from the logs there, but that's a very limited subset. So it'd be great if we as an industry could add this to the data set and I think it would make it like by far the prevailing data set for this kind of auction.
00:11:59.594 - 00:12:05.780, Speaker A: Empirics work because there's so much you can do with it. Cool. I think I had some references here but not a big deal.
