00:00:00.730 - 00:00:25.878, Speaker A: Hello and welcome everybody. Today we have another fantastic guest from the trail of Bits team. As you know, I'm a huge trailbits fanboy. Absolutely love the tooling that they make love, the content that they make love, all the security and auditing and solidity, non solidity, just evm stuff and even non evm stuff that they put out. We are here with Alpharush, aka Troy, who is on the trail of bits team on the blockchain team. Troy, how you doing?
00:00:25.964 - 00:00:27.158, Speaker B: Doing good. How about yourself?
00:00:27.244 - 00:00:35.142, Speaker A: I'm doing well, thanks for. Don't you tell. Why don't you introduce yourself, tell everybody who you know, what you do, what you're working on.
00:00:35.276 - 00:00:58.574, Speaker B: Yeah, I'm a security engineer at Trailbitz on the blockchain team, so that includes both smart contracts and off chain components as well. I've been there a little bit over a year now and I guess I am associated with my Twitter account, I guess, what else? And I am a contributor at this point, probably core contributor to slither and also try to work on our other tools, too.
00:00:58.692 - 00:01:31.078, Speaker A: Awesome. Well, Troy, really excited to have you here. And before we actually started recording, Troy and I were talking about a lot of the different things we want to cover, maybe fuzzing, testing philosophy, contract languages, compilers, non solidity, security, how to view your relationship with security. And just all these different things really go underappreciated when we're looking at security and we're looking at the different ways we want to approach smart contract audits. So, Troy, we're going to jump into all of that here. It's going to be a lot of fun. So Troy, why don't we actually start by talking about testing? Right.
00:01:31.078 - 00:01:49.566, Speaker A: That's something that I know you're very passionate about. You've mentioned to me that you have a lot of different opinionated testing philosophies, which are awesome. Yeah. What's the deal with testing? Is there a one size fits all testing methodology that when we're building our smart contracts we should engage with?
00:01:49.668 - 00:03:00.754, Speaker B: Yeah, I don't know that I would call it one size fits all, but I think there's kind of a toolbox that you can have available to you and use across various different projects. And I think we've seen there's a lot of books and articles about software development lifecycle and all these things that people have come up with over the years. Obviously, foundationally you have unit tests, which is basically just that. You do a very specific thing like this function does this. And I think that that's obviously the bare minimum, when you get into things more like then you start asking yourself like, okay, how can I give myself higher guarantees about my testing? You start looking at things like test code coverage, so you'll look at stuff like statement and branch coverage. And so I think that's kind of the bare minimum in any software development project, but especially when you're handling lots of money that you would try to attain fairly high test coverage. But something that I think is becoming more common over probably the last decade is fuzzing, which in traditional software, right, you use fuzzing primarily to identify crashes related to memory corruption, stuff like that.
00:03:00.754 - 00:03:30.000, Speaker B: And unfortunately it's a little bit more difficult in smart contracts because there's not memory crashes, or at least as long as the silly compiler is doing its job, there aren't. And so what you have to do then is you have to define things in your code that you always want to hold true. And you can kind of use this random transactions to your code and see is there a way for this important property of my code that I say should always hold true? Can it ever?
00:03:30.770 - 00:04:14.042, Speaker A: Let me actually just jump in here for a second, because there's a ton of alpha right there and I want to kind of catch everybody up to speed real quick. So you said a couple of things. So number one, you said bare minimum needs to be unit test and then code coverage. And you were talking about branch statements and whatnot, right? So for those of you who are a little bit less familiar, when Troy's talking about branches, he's talking about like if you have code that can split off and do different things, you want to make sure that your tests still cover all those different branches or use cases that split off. For example, if you have an if statement, if you have a conditional if x is zero, do so. And so you want to have tests that cover that first if. But you also want to have tests that cover the else case, right? So he's saying unit tests in code coverage, including these branches, are absolute minimum.
00:04:14.042 - 00:04:28.830, Speaker A: And then now he's talking about this thing called fuzzing and variance and assertions and all these other keywords. Troy, actually, before we dive too deep in that, want to give the people just kind of a high level overview what is fuzzing and then what is an invariant?
00:04:28.910 - 00:05:55.870, Speaker B: Yeah, so fuzzing is where you take random inputs and run them through your program and you look at the output. And for some predefined thing you ask, is this bad? So in the case of memory crashing, you have something like an address sanitizer and you would say is some area of memory access that shouldn't have been. And that kind of gives you this feedback of whenever some random input goes through my test, something bad just happened. We're going to consider that a failure and warn the user in the case of something like a smart contract system, sort of the canonical example that is given a lot when you're explaining what we call like property or invariant based fuzzing, and a property or invariant, it's just something literally that shouldn't vary. So regardless of whatever the starting state or all these things that have occurred over the lifespan of your smart contract, no matter what some user sends to your contract, it should still hold true after their transaction. And a good example of this would be an ERC 20 token, right? If you deploy your ERC 20 token contract and it has a total supply, you could write a test that says a fuzz test that says no user should ever own more tokens than the total supply. And let's say you have an overflow.
00:05:55.870 - 00:06:08.814, Speaker B: If the fuzzer generates sequences of transactions that cause this overflow, then that invariant or property of users not holding more than the total supply would fail.
00:06:08.942 - 00:06:44.410, Speaker A: Awesome. Yeah, thanks for that. Correct me if I'm wrong, but basically, to summarize here, fuz testing is going to be providing random data, random inputs. And I know we didn't mention this, but you were kind of talking a little bit about stateful fuzzing, where you also do random function calls, basically to break some assertion, or as you put it, invariant. Right? And the invariant is going to be some assumption about the code that should always hold true. So invariant, some assumption about the code that should always hold true. Fuzzing, the process of providing random data to your functions to test some invariant.
00:06:44.410 - 00:06:58.274, Speaker A: And in the example that you gave ERC 20, the invariant is going to be, a user should never have more than total supply, and then the fuzzing is going to be doing a whole bunch of random function calls with random data to try to break that invariant. Is that correct?
00:06:58.392 - 00:07:41.486, Speaker B: Yeah, you mentioned stateful fuzzing. So within this, there's a lot of, I guess, subcategories which you can get into, and we've already discussed coverage. So I would say that's an important distinction that I think people should understand as they adopt these tools, especially with discussions that have popped up recently about the differences between foundry and echidna. It's very helpful to know that how your tools work and what guarantees they're giving you. And so when we discuss something like state. We're talking about stateful coverage guided fuzzing. What that means is that all of these random sequences of transactions actually will.
00:07:41.486 - 00:08:03.446, Speaker B: So in the context of the EVM, right, they'll commit the state to the database. So if a user transfers their ERC 20 token and the next transaction will reflect that update. And the important quality of having coverage guided buzzing is that oftentimes if you're just randomly generating inputs, I want to.
00:08:03.468 - 00:08:07.410, Speaker A: Jump in for a second. So coverage guided fuzzing. Wait, what's that? Yeah.
00:08:07.500 - 00:09:24.542, Speaker B: Okay, so we mentioned here that fuzzing is like a random process, right, of you come up with inputs and you could literally just sit here and hash something and cast it to an int and say, throw this at my function, you're very quickly going to see sort of this decay in effectiveness. After several fuz runs, you're going to start seeing like, oh my fuzzer isn't covering those branches, those conditions. It might not even get past that require statement in my solidity code because it's not intelligent enough to get past that. And so what coverage guided puzzling does is that it expresses that some test cases are more valuable or more rare than others. So an input that exercises that branch, it goes through that require statement, it would say this test case is more interesting than the other ones. Let's spend time running variations of this test case by instead of just randomly generating an input, we're going to use the discovered input and we're going to mutate it. So we'll swap the bytes, we'll modify something about this.
00:09:24.542 - 00:10:02.746, Speaker B: And then through that you get this kind of nice effect that it's affectionately like evolutionary, where it's like, as you do this longer and longer, and you save these inputs, like we mentioned, with unit test coverage, you want to get that high. You also want to get coverage guided fuzzing. You want the same effect, right? So over time, you start to sort of saturate the state space, which just means all of the possible paths through your program or the various states that it can reach. You start to get more and more assurance as you do this more frequently.
00:10:02.938 - 00:10:32.470, Speaker A: So basically we're talking about, okay, tests. We obviously need tests. We need unit tests. We need to get a good amount of coverage. We need to do some fuzzing, which is this random stuff. And then we should ideally be looking at this coverage guiding fuzzing, where we're using kind of the outputs of previous fuzz runs or previous test runs as the input for future test runs. When we run like a fuz test, we say, oh, we have some tool that goes, oh, this was kind of weird.
00:10:32.470 - 00:10:43.226, Speaker A: We have this weird little quirk that kind of happened when we did this fuss test. Let's use that to make smarter decisions about the random data that we use in the future. Is that correct?
00:10:43.328 - 00:11:37.786, Speaker B: Yeah, that's a great way to put it. And why this matters is that I think developers are always focused on efficiency, right? You want really fast feedback, and also in the context of security, we want really high guarantees. And basically the benefit of coverage guided fuzzing is that all cumulative work that you've done up to a certain point, it benefits testing in the future. So if you're saving these inputs, you literally save them to a disk file and you can reload them when you start your fuzzer again. Any modification that you've made to your code, even changing conditions, can still more rapidly explore the program than if you were to just take a random fuzzer and run it sequentially. Right. Because you're now having, like I said, it's this evolutionary process of we're able to very quickly explore the program.
00:11:37.786 - 00:11:52.522, Speaker B: And on startup of the fuzzer, you will very quickly cover those previous paths. And now your fuzzer, all that compute that you're using is actually going towards doing something more interesting rather than just doing something random.
00:11:52.586 - 00:12:15.398, Speaker A: Now, you mentioned kidna as well, and I know you guys recently put out the hybrid fuzzing tool with optic. Now, when you're talking about coverage guiding fuzzing, is that something that Echidna can do right now, or is that something that optic and echidna have to. Actually, in another interview for those who are watching, we also talked with Jocelyn, who's the, what is head of engineering, director of engineering at.
00:12:15.564 - 00:12:16.086, Speaker B: Yeah.
00:12:16.188 - 00:12:45.918, Speaker A: Who also talks a little bit about this too. But the difference between foundry and echidna, foundry does this more very random. It does a little bit of, a little bit of smart fuzzing where it looks at different properties, whereas echidna or echidna optic do more this guided, smarter, looking smarter symbolic execution hybrid mix for fuzing. What are some of the main differences then between echidna, this tool that I just threw out there, optic, which is kind of this hybrid fuzzing thing, and then foundry?
00:12:46.094 - 00:13:25.630, Speaker B: Yeah, well, just very quickly, I guess I mentioned efficiency earlier. Basically with all these testing methods, we're basically saying for the fewest number of tests and the least amount of time, how much of the possible program can we explore? And so that's always a question because if you want high guarantees about your code. You basically want to, before deployment, know all of the things that it can do. And that's sort of what testing does is that it can find bugs, but it's not necessarily going to give us, we don't have a total guarantee that it won't still have bugs, but we can find as many bugs as possible using testing.
00:13:26.370 - 00:13:47.640, Speaker A: I just want to highlight that really quickly what you just said. You said we want to use these tools to find bugs quickly and efficiently, right? So we want to use the tools. You're basically saying we want to use the tools that are going to allow us to do that the best, the fastest. Right? So if there's some tool that is very easy to use, very quick to use, that allows us to get to the same result quickly, that's what we want to use. Right. That's kind of what you're saying. Awesome.
00:13:48.170 - 00:14:48.758, Speaker B: And in mean, we see this with projects like there's this project called OSS Fuzz where people can upload fuz tests and Google provides this sort of fuzz runner for open source projects. And all of the work is cumulative. So every time something like OpenSSL, they want to do a release, right, when they update their code on what's a spuzz, for instance, if there was a previous input that caused a crash, it's going to be checked into this corpus and should that bug be reintroduced, it will presumably be found again. Or even just like when you're testing new code, right, with a fuzzer. If your fuzzer is working on exploring this old code that you have already tested a bunch, it might not reach this newly introduced code as quickly. So that's kind of what we mean when we are concerned with efficiency. To get back to your question about how optic and Echidna fit together.
00:14:48.758 - 00:16:07.346, Speaker B: So Echidna, as far as I know, since its release, I could be wrong on this, has always been coverage guided. I think that pretty much if you don't have any tool for a smart contract language or some platform or anytime you're doing a security review, you could obviously just write very quickly like a script that does random input generation and kind of throw it at another program. But it's pretty standard in the fuzzing community to aim for having coverage guided by default. So what this entails is that in the context of the EVM, right, you have the bytecode, which in the interpreter loop of the EVM, each bytecode instruction is a certain program counter. And so the way that you track coverage is like, have we seen this byte, that is an instruction at this program counter before. And so when a kidnap sees a new program counter that it hasn't previously seen, it will save that input as interesting and then spend time mutating it. What happens when you combine the coverage guided fuzzing with optic, which optic is just sort of a wrapper around a symbolic execution engine.
00:16:07.346 - 00:17:29.594, Speaker B: And what it does is that when you're like, say you have run echidnet, right? You have a corpus on disk. What optic does is it will load that corpus and it will symbolically execute that sequence of transactions. So what that means in practice is that when you're evaluating these transactions, rather than return something that is a concrete input, like an integer, you just have an expression, like a mathematical expression, saying, these are the constraints of the program at this point. What it will do is it'll say, we've run the sequence of transactions symbolically. We now have this really long constraint, and a kidnap hasn't been able to reach this point. Can we give this to an SMT solver and ask it to generate a sequence of transactions that could reach this point? What you're doing here is that rather than trying to fully symbolically execute a program, you're spending most of your time doing the fuzzing. Or that's the goal at least, and trying to spend at least the least amount of time using an SMT solver, because it's just a very computational intensive activity to do.
00:17:29.632 - 00:18:04.966, Speaker A: And now I know we introduced a lot of terms there, like corpus SmT solver, symbolic execution. For those of you who are like, what the hell are they talking about? Definitely check out that interview with Jocelyn. We go over a lot of that, and please leave comments on those videos. If anything, there is confusing because, yeah, there's a lot of stuff going on here. I don't want to dive too deep into here, because we covered it a little bit with Jocelyn, but that makes a lot of sense. Akin is doing all this extra stuff with optic, right? It's trying to do this symbolic execution. It's trying to just get way smarter about what random inputs it's going to give.
00:18:05.068 - 00:19:02.850, Speaker B: Yes, exactly. It's basically saying, with coverage guided fuzzing, you may have things that are just like, even then, still just if you choose one random input value, it's very likely that even the other process might not find it. And so there's a couple of ways that we get around this. One of them is extracting constants from code with slither. So we literally look at the source code and we tell echidna, hey, this value appears in the source code. Maybe it's crucial to reaching some point in the program. And if we can't do that, then we take a mathematical representation of the program and we ask a solver, sort of this black box thing that we give a formula and say, can you find some input that satisfies these conditions? There's also some other interesting things that it does, but I don't know that's worth getting into.
00:19:02.940 - 00:19:34.910, Speaker A: I was going to say, I think for the most part, that's already like a really good dive of like, hey, here's what it's doing. It's doing all this extra stuff, making these fuzzing inputs, I keep just saying, way smarter, which I know is really not a good explainer, but from a really high level, that's essentially what it's doing, just giving much smarter inputs. So I do kind of want to back up. We're getting a little bit in the weeds here with symbolic execution and kind of really advanced fuzzing stuff. But I kind of want to back up to the testing philosophies again. Right. Because you mentioned you have very strong opinions on testing philosophies.
00:19:34.910 - 00:19:56.646, Speaker A: So I think we've covered some really good stuff in my mind, actually. This needs to be the new normal, right? The new normal is unit tests have good test coverage, and then you need to define your systems and variants and do your fuzz and staple fuz tests to do. So to me, that's like the new normal. But I'm curious what other testing philosophies and whatever other insights you have on testing.
00:19:56.758 - 00:19:59.900, Speaker B: Yeah. Sorry for getting into weeds there.
00:20:00.830 - 00:20:04.922, Speaker A: Good man. You're good. That's what we're here for. We're here to be nerdy.
00:20:04.986 - 00:20:42.666, Speaker B: Yeah. When we're talking about what tools do we have in our toolbox to try to have high assurance about our program? I think there's a couple of other ones. The primary one, I guess, would be static. And this goes back to something we talked about earlier, which is that I guess I'm going to have to introduce some new terms here, unfortunately. So the things that we've been discussing, unit testing and fuzz testing, they fall under this umbrella of dynamic testing, which dynamic just means like, you're actually doing something. Right, like dynamic stretching. You're moving around, trying to get warmed up.
00:20:42.666 - 00:20:47.100, Speaker B: Right. That's what it means. Static stretching. You're sitting still.
00:20:47.790 - 00:20:53.946, Speaker A: Oh, I love this analogy, by the way. I've never heard this, but, yeah, this makes a lot of sense. Dynamic stretching. Versus static stretching.
00:20:54.058 - 00:21:49.834, Speaker B: Same thing with dynamic testing. You have this really nice property of like, okay, anytime a test fails, assuming our test is correct, right. We found it because we actually ran the code and we know for sure. But the trade off is, and these things that we've been talking about, especially when you get really advanced with things that are really intensive, like fuzzing, it very practically requires time and resources. Then we start to consider like, okay, well, if these dynamic testing things are sometimes expensive, you have to have a long running fuzzer, for instance, or we can only find bugs with them. How can we verify the absence of bugs and also do it efficiently? So that's where you get into this, what it was called, static analysis. So that's the paradigm there is dynamic analysis versus static.
00:21:49.962 - 00:22:16.422, Speaker A: Running some tool real quick, just looks at your code, doesn't execute anything, and just looks for stuff that's possibly out of the ordinary. If I may jump in, slither, for example, it'll look at your code. It's kind of hard code to look for reentrancy bugs. When you make an external call and then change some state after that external call, it'll look for that, it'll call that out and go, hey, this code smell, this is bad practice. Don't do that. Right. It doesn't run any code.
00:22:16.422 - 00:22:22.380, Speaker A: It just sees that you're doing something in your code that you probably shouldn't be doing and lets you know, yeah.
00:22:24.190 - 00:23:04.422, Speaker B: The trade off you're making here is that you're basically being less restrictive about, are all of the results true bugs? Are these things that could actually occur in practice or are these just things that look like bugs? Because like you said, it follows a pattern. How this is typically explained is that it's an over approximation. So you have this kind of like fuzzy bound around your program of like what could potentially happen just by kind of doing something. It's a fuzzy boundary around what your code can actually do.
00:23:04.476 - 00:23:07.254, Speaker A: Right, fuzzy boundary. Now we're using all this.
00:23:07.292 - 00:23:08.982, Speaker B: Yeah, I shouldn't use that word, I guess.
00:23:09.116 - 00:23:13.274, Speaker A: I'm kidding. It's a blurry boundary. The line is blurred. Yeah.
00:23:13.312 - 00:24:25.326, Speaker B: And so the unfortunate thing is that you will get false positives and you'll find things like, this isn't a bug. Right. This can't happen for some other reason. And if you were to write a test case trying to do that, it would be fine because your program can't actually do that, but because you're using a technique that it makes that trade off to basically be, I guess the thing is I don't know that it's a bad trade off to be making, because the thing that's nice about stack analysis is that it basically verifies the absence of this class of vulnerability. So for instance, and I'm talking theoretically here, because in practice people who write tools make mistakes or just to decisions that make developers lives easier. So for instance, when we work on detectors in slither, we might allow bugs to be missed in order to present less results to the user, if that makes sense. Because if you have to filter through all these false positives, is it more worthwhile to try to increase the true amount of bugs or do we want to present just all of the results? So that's a very hard thing in practice to do.
00:24:25.326 - 00:25:13.006, Speaker B: There's all sorts of, and the biggest thing is just that it takes time, right? You have to get developer feedback. But the nice thing is in practice, when you actually do create an analyse that follows these guarantees about over approximation, if you run this tool on your code, then you know that, hey, you mentioned reentrancy, right? So if I never write to a variable after making external call, I can verify the absence of this type of reentrancy from my code. Not to say that there aren't other bugs, but we've verified the absence of this one in particular. So when you take this as like a holistic approach of like, and this goes back to the whole, I guess.
00:25:13.108 - 00:25:27.406, Speaker A: Adding to the whole just to jump in. So we have about 20 minutes left here. I think we could probably wrap up static analysis just because I think we've done a good job covering it. But yeah, sorry, go ahead and finish your thought. Unless I totally derailed.
00:25:27.438 - 00:26:07.486, Speaker B: No, I was just going to say that the other aspect of having efficient testing would also be just recognizing that each method is suited to finding different types of bugs. And so when you're using these tools, I think you can kind of treat it as like an everything but the kitchen sink. Like literally just use as much things as you can, see what falls out. And that's the ultimate way to have high assurances about what your code does, is you want to use the wide variety of testing methods and you want to be using the testing methods to their full effectiveness by making sure they're efficient and not wasting time doing something spurious.
00:26:07.598 - 00:26:20.242, Speaker A: Troy, if you could have a one liner, one line, that's all you get. As a piece of advice for people who are looking to test their smart contracts, what would that one outliner be?
00:26:20.376 - 00:26:37.754, Speaker B: It'd probably be write down what you expect your code to do. If you are very thoughtful and take the time to say, this is what my code does, I think that it will have a profound benefit for yourself, your teammates, and anyone helping you, like third party firms and such.
00:26:37.872 - 00:26:54.686, Speaker A: You hear that, guys? Troy said write good documentation. All right. He said write good docs. The joke in the community is nobody reads docs, but nobody reads docs because a lot of docs are not written very well. Write good docs. Write exactly what your code does. Absolutely love that.
00:26:54.686 - 00:27:17.914, Speaker A: So, Troy, so we have like 20 minutes left here. We've talked a lot about kind of testing different philosophies and testing a lot of the different tools that are available for web3 testing right now. I know there's a number of other things we had on our plate, like contract languages, compilers, non solidity, security, how to view the relationship with security firm. Which path do you want to go down? What do you want to talk about next?
00:27:18.032 - 00:27:57.986, Speaker B: Yeah, well, on the topic of efficiency, I think that when you're working with security firms. Right. And this also goes into the non solidity thing, too. All the things that we've discussed, we sort of delved into the high level of what these things are, and then we talked a little bit about EVM specific things or solidity specific things. But in general, these testing methods apply to other areas. So if you're working on a rust project, we have this thing called test fuz that we will use to do coverage guided fuzzing of rust programs. And so I think those same things apply in any domain.
00:27:57.986 - 00:28:35.562, Speaker B: There's all sorts of tricks of the trade to make it work on different platforms, whatever. But when you're working with a security firm, I think that's one of the values that they can provide, is sort of giving guidance on these kind of things. Because if all you walk away from a security review is a list of bugs and you do not have any idea, how can we prevent these bugs in the future? How can we catch these bugs earlier? How can we prevent them from being reintroduced then? It wasn't as effective of a security engagement as it could have been, which is really difficult to do, but that's the goal.
00:28:35.706 - 00:28:57.222, Speaker A: I love that, actually. Pretty much every security professional that I've spoken with has kind of that exact same mentality, that exact same vice. The relationship that you have with your security form, or if you're a security auditor, you're an up and comer. The report isn't the product. The report is part of the product. The product is this security journey right. It's a.
00:28:57.222 - 00:29:20.746, Speaker A: The report, yes. It's list of bugs. But are they're hiring you to help them be more secure, not just for this audit, but for all code in the future. So everything that Troy's been talking about, testing philosophies, different tools. Right. These are some of the knowledge that you should be considering giving to your client. And as a client, these are some of the things that you should expect from your security professional.
00:29:20.746 - 00:29:29.534, Speaker A: Right. Hey, what can I do to stay safer? What can I do to make it so that we don't get hacked for $200 million again? So. Absolutely. I love that.
00:29:29.652 - 00:29:41.542, Speaker B: Yeah, I think we were talking earlier about pay, paying security firms like $50,000 to try to formally verify your code.
00:29:41.596 - 00:29:42.006, Speaker A: Right?
00:29:42.108 - 00:30:34.566, Speaker B: Which I think that's one thing that I see frequently happening, is this thing where people will have like three day audits, right. Simultaneously, the same people who want three day audits, they also want formal verification. And so when we were talking about this really important thing of understanding what code does, I'm not going to claim to be the best security auditor ever. And so maybe it's just me, but it takes me a very long time to actually find deep seated vulnerabilities of something that's just like a logical issue. Right. Those are things that probably take longer than three days to find manually, let alone to find with automated tooling, I would think would take a lot longer. So whenever you're balancing this trade off of cost and efficiency, right.
00:30:34.566 - 00:31:36.922, Speaker B: You really need to make sure that you are allotting time to do the really valuable activities, and only if you just have a lot of time and money to start investigating things that are very time intensive. But there's a lot of prerequisites to, if you're going to do formal verification, you should have all of these things lined up and have a really good idea of what your system does. And the great thing about this is all these things I've mentioned up until this point are freely available. You don't need a security firm to do them for you, and you can benefit from them. And when you get to the point where you're actually paying people money to do something, you can focus on things that matter. And so I think that's something that's maybe not well understood or just until you've been working in this industry for a year or two and you've worked with multiple firms or you've been on lots of smart contract projects, maybe you just don't have enough experience under your belt to see. But I think that's valuable to know.
00:31:36.976 - 00:31:58.738, Speaker A: I think that makes a lot of sense. And yeah, I think that's really good feedback for everybody who's listening. So I do want to pivot though, talk about either, both different smart contract languages and compilers, and also non solidity security, because I know that's something that you're passionate about as well. But actually before we jump into that, just straight up, is viper the underrated goat, or like what?
00:31:58.904 - 00:32:02.558, Speaker B: Yeah, I personally have not used Viper.
00:32:02.654 - 00:32:04.174, Speaker A: No, just kidding.
00:32:04.222 - 00:32:10.230, Speaker B: I do like Charles a lot. I've talked with him some. I think that he, Charles Cooper is.
00:32:10.300 - 00:32:29.526, Speaker A: The main dev behind Viper, by the way, for those who don't know. Really just phenomenal dev. I sat behind him at a hackathon and anytime I see somebody like ripping around them, just like opening 18 files in the span of 20 seconds, I'm like, oh, okay, this person's insane. So, yeah, Charles, really good dude.
00:32:29.638 - 00:33:29.754, Speaker B: Yeah. And I think he has a lot of great ideas. I think there's this kind of unfortunate reality of, I don't know if it's that the industry is super small or it's very hard to fund things that don't necessarily have a profit thing, but whenever we're talking about tools and languages, it comes down to that. These things take, I would say at minimum, it probably takes five years to build a language. Right? And the same thing is true of static analysis tools are fairly involved as well, because there's a lot of overlap between stack analysis and compiler. So whenever we're talking about building tools and how to make these things better, the thing that I'm really passionate about is like code reuse, because it's very time intensive, costly, and requires a lot of labor. And the way that you get a very good language and good tooling is having a lot of people looking at the, reusing it and slowly sifting out these really nasty bugs.
00:33:29.754 - 00:34:46.706, Speaker B: Right. And so when we're talking about new smart contract languages, that's something that I think Charles has talked about. And that I would like to see is having lots of code reuse. And so what this looks like, practically is, in the traditional programming world that we have LLVM, which is like most languages, target this shared code format and then you can lower it to different architectures and you have this really nice property of any tooling or any optimizations that you want to perform, sort of benefit the whole ecosystem as a whole. And I think this has been a shortcoming in the smart contract languages, but at the same time it is like as I said a very time intensive and expensive thing, so it's very clear why it hasn't happened. But when we see these up and coming languages like Viper, Faye move and stuff, I do think there's this trade off to balance of, like, if you're targeting a different vm than the EVM, it probably makes sense to make a different language. But if you're targeting the EVM, right, I do think at some point all the things to optimize code for a stack machine could probably be reused.
00:34:46.706 - 00:35:59.134, Speaker B: And the benefit of this would be that debuggers, analysis tools, optimizations would all be shared between languages. And there's sort of this running thing on Twitter where as soon as you get into this solidity, the thing that you tweet is the weird things that it does, the weird thing that happens, weird code generation things, weird. You sort of become like the shaman of solidity. And that's considered knowing a lot, which I think it's good because in some ways people learned the EVM to this way. But at the same time I think that it is symptomatic of how the compiler was built and choices that were made architecturally, and that in order for developers to. I guess what I'm saying is instead of focusing on these kind of things, it would be nice if we were more focused on things that were higher level related to building the code itself, versus having to get in the weeds of understanding some intricacy about the code generation process.
00:35:59.252 - 00:36:23.398, Speaker A: Right. I feel like maybe an example is like, all right, like modifiers and solidity. A modifier is just tacked on to every single function you use it for. And so if you're trying to make some really gas efficient code, maybe it doesn't make sense to use a modifier because it's just going to repeat that chunk of code and over and over again, which you probably don't need to do. Right, and you're talking about like solidity shaman. A solidity shaman is going to know that. Spending a lot of time learning solidity is going to know that.
00:36:23.398 - 00:36:33.610, Speaker A: But you're saying, hey, it would actually be better if we kind of focus as a community on, hey, how should a language look like? As opposed to just, this is what one looks like, I'm going to learn the ins and outs of it. Is that kind of what you're saying?
00:36:33.760 - 00:37:28.026, Speaker B: Yeah, there's also this trend of liking rust, and I think that there's good reason for it. There's a lot of things where having a philosophy that your compiler drives you to write better code and gives you feedback. I think that's something that's lacking. And I guess, like I said, the unfortunate thing is there are teams trying to do this, but they're doing it in a way that you sort of get this walled garden effect of like, we're not going to share in a lot of the benefit of people writing new languages if they do it in a way that forces you to work with their tooling or something like that. Which of course I'm not saying that you should make. There are reasons to not trying to compile solidity to BPF. I'm not sure that makes something like that.
00:37:28.026 - 00:38:26.526, Speaker B: But there probably are good places to reuse code, especially when it comes to this developer tooling I think would be a great place to have. For instance, Go has first class support for fuzzing in the language. And also most compiler systems have build systems and package management. Is there a reason that the Ethereum community could not come together and have a shared something like cargo or something like PiP or. Not to say that these things are solved problems or easy, but that we could get to a better place where some of these things are very critical for security too. Like if you're importing open Zeppelin and the version is this, there's a lot of footguns that just by upgrading the version of the library that you could be safeguarded from. But it's very hard to tell if you're vendoring packages from open Zeppelin, which version are you relying on or something like that.
00:38:26.526 - 00:38:51.606, Speaker B: Or even just testing solidity. Right. In order to test solidity, it's just historically been like an act of Congress. It's very difficult and it'd be nice if there were something like rust, like you could do a test in line and it would compile and run on the EVM. I dare to say that lots of money could have been saved by just very simple things like having these things in place.
00:38:51.788 - 00:39:17.146, Speaker A: Yeah, makes a lot of sense. We need more open source tools, more good open source tools, more communication about these tools. I 100% agree. So we only have a couple of minutes left here, Troy. So I did want to just briefly touch on non solidity security, although. Or like even. Maybe even non EVM security, although we probably don't have time to get into the weeds, the weeds of them over at trillibits.
00:39:17.146 - 00:39:38.146, Speaker A: I know you guys have done some deep dives on different chains, kind of the Nakamoto coefficient of different chains, which we don't have to go into. We just talked briefly about just non solidity security. We talked a little bit about rust. Do you have any thoughts on either non solidity or even just non EVM security in the blockchain web3?
00:39:38.248 - 00:40:00.810, Speaker B: Yeah, I would say that if you're looking to dive in, I think that people underestimate the knowledge that's like independent of sility, that you probably want to learn to be proficient at these things because things like go ethereum and clients and people need security researchers working on those.
00:40:00.880 - 00:40:13.920, Speaker A: I think about that all the time. Like auditing, for example. Everyone's just looking at the solidity, but nobody's looking. Hey, does this make sense from a client perspective? And I feel like that's why Mev went under the radar for so long, because nobody was looking at.
00:40:14.530 - 00:41:22.994, Speaker B: I mean, and that's something that I think is nice about working at Trailblaz is like, I primarily worked on solidity for a decent while, but over time I've gotten to get the experience of working on things that are in go and rust and sort of closer to the metal of these chains. You mentioned consensus and stuff like that. And I think it sort of has this nice feedback loop of like, if you're reading through an Ethereum implementation, you're going to understand solidity a lot more. And also, just that it also has, I think it's good for the client teams because it's like a lot of these things, there's mistakes. If you were to ask yourself how to build a blockchain from first principles, there's a lot of things that you can do wrong and end up in a position where you're making the same mistakes that someone else made before you. And I would also just say the attack surface is just infinitely more. There is this whole thing where you have the whole defi composability, and because you depend on another defi project and they depend on something else, you have some of that risk, I would say, is in similar vein of complexity.
00:41:22.994 - 00:41:52.218, Speaker B: But when you actually have a network and you have a file system and stuff like that, there's a lot of things that can go wrong because these are general purpose programming languages and not like domain specific language. So there's a lot more to understand. But as I said earlier, the same techniques can be applied. Fuzzing and go and rust. Like I said, go has native support for it, and then it's fairly easy to set up fuzzing and rust for test fuzz.
00:41:52.314 - 00:42:15.842, Speaker A: Got you. Yeah. Thanks for all the insight there. So we have maybe 30 seconds left. Let's wrap it up here, I guess. Yeah, Troy, is there any last bit of advice you want to give to anybody who's thinking about getting into security, who's thinking about trying to make their smart contract code a little bit better? Just any advice to people who are becoming more and more security minded?
00:42:15.986 - 00:42:48.958, Speaker B: Yeah, I would just say that I myself, I'm interested in all these sort of academic and fancy things, and I didn't go to college for computer science, so I don't really understand these things either. So I think it's important to just really try these things for yourself and you sort of start to understand and build an intuition, very practically, of what are these things doing and why is that helpful and why is that giving me higher assurance? I don't think you can trade that for anything.
00:42:49.124 - 00:43:11.430, Speaker A: Gotcha. Makes sense. Well, Troy, aka Alpha Rush, thank you so much for being here. Thanks for giving all this insight, all this alpha on testing compilers, different types of security, the relationship to have with the clients and auditors when doing security focused journeys. Thank you so much and we will see you all next time.
00:43:11.580 - 00:43:12.914, Speaker B: It's great talking to you, Patrick.
00:43:12.962 - 00:43:13.940, Speaker A: Thanks. All right.
