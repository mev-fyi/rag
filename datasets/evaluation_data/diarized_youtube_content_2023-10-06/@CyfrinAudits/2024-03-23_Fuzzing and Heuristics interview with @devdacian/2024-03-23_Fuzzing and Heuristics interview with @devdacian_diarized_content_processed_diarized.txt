00:00:02.730 - 00:00:29.814, Speaker A: Gm. Gm. And welcome back to another fantastic cypher interview here today we actually have a special cypher interview. Well, they're all special. We actually have a cypherin security researcher themselves here to speak a little bit about fuzzing, how to think about approaching different audits, different security reviews, halmos, how important fuzzing is. Some fuzzing research. You've probably read some of this guy's deep dives before.
00:00:29.814 - 00:00:33.506, Speaker A: Daishan, thank you so much and welcome to the podcast.
00:00:33.698 - 00:00:36.454, Speaker B: Hey, Patrick, great to be here. Thanks for having me, man.
00:00:36.652 - 00:00:56.880, Speaker A: Yeah, absolutely. So I guess I know the answers to some of these, but just from your perspective, how did you get into Web three? How did you get into web3? Security. Like I said before, you are a security researcher. You do a lot of deep dives on different concepts in security. Researching in web3, how did you get here?
00:00:57.330 - 00:01:19.890, Speaker B: Sure. Yeah. So I have a software engineering background. I used to program for soulless multibillion dollar corporations. I used to build distributed high performance transaction processing systems for them. But I knew someone in my other life, and he used to run like, trend following strategies on different instruments. So he used to trade like, futures stocks as well as cryptos.
00:01:19.890 - 00:01:40.894, Speaker B: And for his crypto strategy, he needed to get the data quicker. So he knew that I was a programmer. So he asked me if I could hook up to some APIs for him, and I was like, yeah, just send it to me and I'll just do it like as a little homework exercise. So I did it for him. It didn't take me very long, but he was like really happy. He's like, what can I pay you? And I'm like, no, man, don't worry about. It's just a cool little programming size for me.
00:01:40.894 - 00:02:17.320, Speaker B: And he's like, oh, no, I really want to pay you. Like, can I tip you some bitcoin? I'm like, yeah, all right. So he tipped me some bitcoin, and it was from that that then I did some research and I found ethereum, and then for me, then I found the EVM and smart contracts. And that really finding about smart contracts really switched like a light on for me. I guess I saw the potential of blockchain much more than just sort of sending and transferring tokens. But now that you could write computer programs that live on the blockchain, that really sort of got me going and piqued my interests and then, so I just started researching solidity smart contracts and then from that smart contract security, and, yeah, now I'm here.
00:02:19.310 - 00:02:23.610, Speaker A: Love it. So when was this? When did that person send you the bitcoin?
00:02:24.110 - 00:02:27.260, Speaker B: It was like late 2021.
00:02:29.070 - 00:02:42.766, Speaker A: So relatively recent compared to a lot of the other people who got into security. So somebody sent you bitcoin. You were like, wow, this is really cool. Let me check this out. Oh, my goodness. There's this whole other world with the EVM. Let me dive into this.
00:02:42.766 - 00:02:45.886, Speaker A: Boom. Now you're in love with security research.
00:02:46.068 - 00:02:58.230, Speaker B: Yeah, but I didn't dive in straight away. So once I found the EVM, the smart contracts, like, I bought some shitcoins and I shitcoined for a little bit, and then it was like around October 2022 that I started to study solidity.
00:03:00.410 - 00:03:19.580, Speaker A: But what was it about the securities part that got you interested? Right, because thinking from a software developer point of view, oftentimes software developers like to build and then security researchers or white hat hackers like to hack. So how did the security aspect pique your interest? How did that transition happen?
00:03:20.350 - 00:04:09.660, Speaker B: I have had an interest in security from when I was younger and also in my software life. I did build stuff, but I ended up transitioning more into a role of performance optimization, where my role actually became reading and understanding other people's work in a distributed system, reading and understanding all the layers and then thinking about how to refactor a certain service or a certain process to speed it up ten x 100 x to make it reach its performance targets. And I found that I actually really enjoyed that. I really liked reading other people's code, understanding how it worked, and then sort of rewriting it. And so security, in security, I kind of do exactly that. I get to read other people's code, understand how it works, but instead of rewriting it, I get to do something even more fun, which is figuring out clever, interesting ways to break it.
00:04:10.510 - 00:04:36.420, Speaker A: I can hear the smile when you say clever, fun, interesting ways to break it. And let's get into that a little bit then. So you've talked a lot in your deep dives and on different interviews that you've done about this mentality that you take to break code bases. And with this mentality, you found some really cool findings. So from a high level, what's the approach that you take when you're looking at code bases and thinking about how to break these?
00:04:37.110 - 00:05:16.420, Speaker B: Sure. So from a high level perspective, I guess, first of all, is that I trust nothing and I verify everything. And that's kind of the way that I am in my normal life as well, is that I don't really trust what people tell me. But I would take sort of a very critical view of most things. And I'm always, like, checking for where's the bullshit at, basically. So with smart contracts developers, they might be really confident, they might have big names, but I always approach the code base from a critical perspective. And then what I want to do is I want to read the code, and I want to understand what the code does, but I'm reading the code from a point of view of constantly challenging what the developer is trying to do.
00:05:19.350 - 00:05:47.580, Speaker A: Trust nothing, verify everything. I like that a lot. So with the mentality, where does fuzzing come in? And just to kind of 180 real quick, right here. The reason I bring that up, obviously, is because a lot of people have seen your deep dives on fuzzing. We've seen a lot of your research on comparing the different fuzzers. So how does the fuzzing play into this? Is that a big part of what you do? Is that a small part of your process? How does that come into play?
00:05:48.110 - 00:06:23.986, Speaker B: Sure. So for me, I would be 90% a manual auditor, but fuzzing is a tool in my toolkit that I bring out on almost every audit, and I use it for a number of different purposes, but fuzzing is probably around 10% of what I do. So in terms of fuzzing, I would use both invariant or stateful fuzzing as well as stateless fuzzing as well. So when it comes to fuzzing, in terms of invariant fuzzing, I think about invariants that should apply to different contracts. Right. And so I break these up into two major categories. So the first would be white box invariance.
00:06:23.986 - 00:06:54.238, Speaker B: Those are invariants. They're sort of system and contract properties that I can derive usually from the documentation or the white paper without actually reading the code, without the internal implementation details. They're sort of like more high level invariants. And then I think in terms of black box invariants. So those are implementation specific invariants. They're invariants that I derive from actually reading the code and seeing how things are implemented. And then once I have those two groups, as I read the contract, I also think about invariance in regards to the contract's lifecycle.
00:06:54.238 - 00:07:17.158, Speaker B: So, for example, most contract have two or three lifecycles. So almost every contract has an initial state. That's when the contract is being set up, it's being configured. Then there's an active state. That's when the contract is actually alive and functioning as it's intended to. And then some contracts optionally have a final state. That's when the contract has fulfilled its purpose, and there's nothing more to do.
00:07:17.158 - 00:07:27.050, Speaker B: And so I try to think about which properties should hold true during each of those states, and then I try to code up those invariants.
00:07:28.190 - 00:07:51.966, Speaker A: Is the fuzzing or the coding part that you do in regard to these invariants? Is that the only tool that you use? I guess maybe I'm framing this poorly. Do you stop thinking about these invariants after the fuzzing is done? Once you do your fuzzing, are you like, okay, cool. Checked out these invariants.
00:07:51.998 - 00:07:52.354, Speaker B: They're good.
00:07:52.392 - 00:07:54.530, Speaker A: I don't have to think about these invariants anymore.
00:07:54.950 - 00:08:28.122, Speaker B: So I think about invariants throughout the audit. So even if you're not going to fuzz the contract using a fuzzer, it's still very helpful to switch at times to an invariant thinking mindset. So when you read the contract, try to ask which properties should hold true, and then try to break them yourself, or try to read the contract from the point of view of breaking them. So this is something that I do as part of the audit. Writing the fuzzer is just an extension of that. Sometimes I can break the invariant purely mentally, and I don't even need to write the fuzzer. I can just go straight away and write the POC.
00:08:28.122 - 00:09:15.466, Speaker B: But other times, then I want to run the fuzzer, because the fuzzer can also find things that I would never find. I can give you a couple of examples from the previous audit that we did, which was swell. From that audit, the fuzzer found a couple of invalid states that actually would be very impossible to find as a human being, as a human auditor. And one way that I do my fuzzing as well is my fuzzing is constantly evolving. So if I'm doing a fuzzing campaign over, say, three days or five days as part of a greater audit, I'm actually running and developing the fuzzer every day. And the way that I develop my fuzzer is I'm writing the bare minimum code possible to get the fuzzer to as organically as possible, perform a piece of functionality. And then I'm checking the coverage.
00:09:15.466 - 00:09:43.506, Speaker B: So I'm checking the coverage to see where the fuzzer got. And then I'm doing a bit more. I write another handler, wrap another bit of functionality, trying to make it as organically as possible to mimic the natural system. Then I run the coverage again. So I'm constantly looking at my coverage, and I'm incrementally adding more and more coverage. And what actually happened on the previous audit is twice. The fuzzer got stuck where I didn't expect it to get stuck, and it was that incremental approach and then seeing it where it got stuck.
00:09:43.506 - 00:09:53.900, Speaker B: And then you asked the question. I'm always constantly asking questions, how did it get stuck there? And that led to two quite interesting findings of invalid states that the system shouldn't have been able to enter.
00:09:55.790 - 00:10:04.958, Speaker A: Interesting. So it's almost like, as engineers and security researchers, we like to think ourselves very pragmatic, but this almost sounds like.
00:10:04.964 - 00:10:29.400, Speaker B: A dance that you're doing in a way. I'm dancing with a system and I'm constantly probing it and prodding it. And my goal is to incrementally increase the coverage. That's the goal of the buzzer for me. And then once I have the coverage where I want it to be, which is at a high percentage, where I'm as organically as possible exercising the system, then I will settle in and do a longer fuz run, for example.
00:10:30.490 - 00:10:57.246, Speaker A: Interesting. So we do not trust the developers. You don't trust anything somebody tells you. You verify everything. We're constantly thinking about invariance and the different properties of the system that must hold. And we use that knowledge to build fuzers and really test and really push the limits of the system and use these tools as fuzzing to give us more feedback into how to continue reading the code. Very interesting so far.
00:10:57.246 - 00:11:12.370, Speaker A: So what are the best fuzzers? And I bring this up because obviously you have a repo out there, I believe, which actually compares all the different fuzzes out there. What is your fuzzing tool of choice? Why do you choose what fuzzer?
00:11:13.030 - 00:12:00.658, Speaker B: Sure. So at the moment, my favorite fuzzer is Medusa. That's just my opinion though. And I think that it's great that we have multiple top fuzzers that are under active development. I think all those teams are doing a really good job and I think it's great that we can compete in the market and try to become better and we can all win together. But I would say that just in my experience, just as an auditor, and I just want to put a disclaimer out there that I'm not a PhD, I'm not producing academic papers or academic benchmarks, I'm just a simple auditor in the trenches, getting findings for my clients, our clients. So from that point of view, day to day, Medusa seems to perform the best for me.
00:12:00.658 - 00:12:18.920, Speaker B: It seems to be very quick, very fast. It's easy to develop for which foundries as well, and so was a kidnote. But it also seems to perform very well in terms of getting findings quicker and getting findings that the other fuzzers sometimes can't get.
00:12:19.530 - 00:12:52.094, Speaker A: So, Medusa, this is that trilla bits fuzzer. I feel like Medusa is like the secret alpha. Nobody uses Medusa. Everyone's still using foundry or echidna, which are both fantastic fuzzers. But, yeah, Medusa being this experimental fuzzer, it's really interesting to see the results. And for me, it's personally really interesting to see trail of bits trying to one up trail of bits with them trying to make a better fuzzer, even better than their already fuzzer, their incumbent fuzzer, Echidna. So, 100% agreed.
00:12:52.094 - 00:12:53.940, Speaker A: Great suit the competition in the market.
00:12:54.390 - 00:13:14.780, Speaker B: But I will just say, though, that I do use all three, and the reason for that is because I have found things that Medusa can't solve, but echidna and foundry can, for example. So I do try to use all three. And so that gives us a lot of flexibility, where we can have three fuzzers for our clients. And, of course, I also sometimes use hamos as well.
00:13:15.950 - 00:13:30.590, Speaker A: Can we talk a little bit more about how you use all three? I know the answer to this question. Is there a framework that all three of these fuzzers can be used in to make it easier and you don't have to rewrite three different fuzzing specs?
00:13:31.250 - 00:14:02.582, Speaker B: Sure. So, yeah, when I first created my fuzzing repo, I just kind of wrote my own contracts for that, and I did have a decent amount of code reuse, so I didn't have that much duplication, but there was still a bit of duplication to use all three. But now there is a new framework. It's called chimera, which is really cool by a vigiano shout out to him. And, yeah, it's a really cool framework, and it lets you use as much or as little of the framework as you like. So it's very flexible. It's like, sort of like a swiss army knife.
00:14:02.582 - 00:14:16.810, Speaker B: And it makes it easier to write code that runs across multiple fuzzers with very little duplication of the code. Yeah. So I'm a big fan of that one. And that's sort of what I use now going forward when I want to run multiple fuzzes.
00:14:16.970 - 00:14:21.722, Speaker A: Yeah. Why use one when you can just use all three for the same amount of effort?
00:14:21.866 - 00:14:25.090, Speaker B: That's right. Especially if the cost is so minimal.
00:14:26.310 - 00:14:39.510, Speaker A: So you mentioned hamos a little bit earlier. How often do you use hamos. How easy it is for you to use hamos. How does Hamos fit into your approach to your dance with these code bases?
00:14:40.170 - 00:15:38.022, Speaker B: Yep. So hamos, for me, I use it in similar places where I would use stateless fuz testing. That's something that we haven't talked about that we should as well, but mainly so Hamos. I would use hamos when, for example, if we are doing gas optimizations, which we do as part of our audits as well, and then sometimes we would optimize a function, so we would significantly refactor or rewrite a function, because it's perhaps important, but also inefficient the way that it's currently implemented. So then I would use hamos to have both those functions sort of running against each other, just to verify that the outputs are still the same. So that way we can be confident that, hey, we've improved, we've sped up this function considerably, but it's still behaving from an input output perspective the same way that it used to, because fuzzing can help you with that. But fuzzing can't guarantee that, whereas symbolic testing can give you perhaps a greater guarantee.
00:15:38.022 - 00:15:45.900, Speaker B: But of course, me, I don't trust anything, but Hamos does give me more confidence that the behavior is the same.
00:15:48.350 - 00:16:19.522, Speaker A: If you're unfamiliar with a lot of these terms, by the way, everybody, you should 100% go over to cypress updraft. We teach about Hamos. We teach about fuzzing. We don't teach medusa, but it sounds like we probably should. We do teach foundry fuzzing, but Dacian, just for the people who are unaware of Hamos or symbolic testing, why is it so important to do Halmos to do this symbolic testing? Or I usually call it like equivalence checking for doing these gas optimizations, how does that fit in? What is Halmos?
00:16:19.666 - 00:17:03.918, Speaker B: Sure. Okay, so another way to do it is to use stateless fuz testing. So that's, for example, imagine you have two functions. You've got an original version and a fixed or optimized version, and you just run the stateless know, pumping different inputs into those functions and comparing the outputs. But can that guarantee that those functions will behave the same for the entire input set? Well, it can't, because it's not going to run the entire input set. So the way that Hamos works at a high level is that it effectively transforms the code into a mathematical representation, and then it can effectively prove the behavior through a mathematical proof. And so it's a method of symbolic execution.
00:17:03.918 - 00:17:15.910, Speaker B: So the idea from a high level point of view, is that homos, in a way, it does allow us to exercise both functions with the entire input set and have confidence that the output it produces is identical.
00:17:16.810 - 00:17:24.300, Speaker A: So why not just use Hamos instead of all this fuzzing? If Hamos could prove stuff, and fuzzing is just this random walk.
00:17:25.230 - 00:18:12.726, Speaker B: So Hamos doesn't work for everything. So there's a limit perhaps, to the complexity can take. So it doesn't work for everything. And also, one thing that I found with Hamos, one of Halmos's selling points, is that it integrates really well with foundry, which is great. But one thing that I found is that there is a slight difference in the behavior. So, for example, when you run the foundry fuzzer, if you're running a stateless fuzzer, comparing two functions, the fuzzer will stop on, for example, an underflow or an overflow, but Hamos actually won't. So one trap that you can fall into is that you've created your optimized function, but perhaps your optimized function will actually revert for a particular input, whereas the original one didn't.
00:18:12.726 - 00:18:31.920, Speaker B: But if you run Hamos, then it'll look like, oh, it's all good. But actually, if you run the foundry stateless fuzzer and it finds it, then it'll show you that actually the behavior is different in that case. So I do use both, and especially because I can use both from the same contract, so there's very little cost to running both.
00:18:33.890 - 00:18:57.542, Speaker A: Gotcha. Makes a lot of sense. Cool. So we've been talking a lot about the tooling that you use station, and for me, it's always been really interesting because I love doing those deep dive comparisons on tools. I've ripped through maybe 15 or 20 smart contract development frameworks. About 75% of them no longer exist. I love doing the comparisons as well.
00:18:57.542 - 00:19:46.390, Speaker A: But what's interesting is we've spent maybe 20 minutes so far kind of just talking about fuzzing and formal verification tools, even though, like you said, it's only 10% of your time, the vast majority of your time, the bulk of your time is doing the manual review. And the last 10% is really kind of this final. I don't want to say putting a bow on it, because that's definitely underselling the value of these tools, but it's kind of the last leg, right? It's this last important cross. Your t's, dot your I's steps that you take. Let's go into this bigger bulk of what you do this 90%. How are you approaching coming to a code base? How do you get started? How do you begin asking these questions? What is the mentality that you think most auditors or security researchers should have to be successful?
00:19:47.290 - 00:20:43.414, Speaker B: Yeah. So the first thing that I do when I approach a code base is I'm usually going to go and read the white paper or the docs that they might have on their website. And as I'm reading it, say, on the left, I've got the browser with the text, and on the right, I've got a big notes file, and I'm constantly taking notes. And so one thing that I'm doing as I'm reading is I'm constantly taking notes about who the actors are in the system, what are the entities, the objects which might correspond to contracts. And especially I'm always thinking and looking for where is value stored in the system, and how does value flow through that protocol? And so I'm taking all these notes, and this way, I'm trying to build a mental map of how the system works for me. I don't actually create any graphs at all. I know that other auditors, they create graphs and that, but I build up a whole mental map.
00:20:43.414 - 00:21:04.180, Speaker B: So I build a giant graph, a giant machine in my head. I push everything else out of my brain, and I push the protocol into my brain. And then once I've done that, I go to the code, and I'm reading the code, and I build up this giant mental model of the whole system in my brain. So that's the first key part of it for me.
00:21:06.870 - 00:21:40.220, Speaker A: To me, that sounds very daunting, right? Getting all the context in your head can feel overwhelming, right? If we're all secretly giant AIs, right? AIs have these context windows. They can only hold so many tokens before the models start to break down. It's keeping the context of these large protocols. I mean, you mentioned swell already. Swell isn't a small code base. How were you able to keep the context of all that code base in your head doing the security review?
00:21:40.750 - 00:22:22.140, Speaker B: Well, I guess for me, like comparing smart contracts to traditional software, smart contracts are actually very small. So in traditional software, we would work with code bases that are tens of thousands, if not hundreds of thousands of lines. And that they might not be just in one programming language, but they would be in multiple programming languages in multiple layers. You might have your database application layer service bus multiple front ends in a whole distributed system. Right? And that scales out. So from that point of view, for me personally, smart contracts are actually very small. They're actually much easier to read and understand than a traditional sort of service orientated architecture or distributed system.
00:22:23.870 - 00:22:43.280, Speaker A: Fair enough. Makes sense using this approach, right? Using this approach of going to the code base, getting all the context in. What's the next step? Are you asking questions? Are you ripping through each function, running in the fuzzer? What's the next step here?
00:22:43.970 - 00:23:16.342, Speaker B: Sure. Yeah. So for me, in my first pass through the code base, I'm trying to just build up that mental mapping and then I'm annotating the code with questions, right. So for me, pretty much all of the findings that I have, they're either two things. So the first thing is just the easy stuff that I spot right away. So usually in every private audit that I've done at Cypher, in the first day or two, I will get a bunch of just easy findings. And they're just things that hopefully one day they'll be completely automated.
00:23:16.342 - 00:23:38.430, Speaker B: That would be great. So that I wouldn't even have to. Hopefully, you know, more and more developers, they'll do your courses, they'll read my deep dives, and they won't make these silly mistakes. Right. But this is just easy stuff that I just spot right away. But this is just as a result of having a large knowledge base. I've read just thousands of articles, thousands of audit reports.
00:23:38.430 - 00:23:57.270, Speaker B: I've gone into Solidar and I typed in precision loss, and I read like 200 findings. I've typed in slippage, read 300 findings. I've done a large amount of research on a lot of different areas, and I've internalized a lot of vulnerability classes.
00:23:57.770 - 00:24:14.970, Speaker A: You've literally gone to solid it, typed in precision, and then gone through 200 results, and then gone to slippage, posted into solid it, and then gone through the results. You've literally just gone through hundreds of these audit findings from other people.
00:24:15.120 - 00:24:56.730, Speaker B: Yeah. And that's how I wrote my deep dives and that's what I do. And I've had people message me on Twitter because they saw that I talked about this in another interview and they messaged me and they said, like, dacian, did you really do that? Man? Did you really read them all? And I'm like, yeah, I read them all. That's the thing, Patrick. For me, this is fun. I do this because it's fun for me. It's fun for me to read 200 findings on slippage or precision loss or any other topic, because it's fun for me to systematize the knowledge and to see the little permutations to see the differences and then to build them up into a system that I can then internalize and I can become, in a way like an AI, where I can just read the code and just automatically spot many different types of common mistakes.
00:24:56.730 - 00:25:47.338, Speaker B: But this stuff is the easy stuff. This stuff is not that exciting for me anymore because it's too easy. Right. But this is just how it is, though. So in that first pass through, I'm picking up all this easy stuff, and then I'm asking questions of the code base, and then as I build up that context, then all of my more interesting findings, which is the hard stuff, they're all a result of me answering those questions that I've raised and why. Questions? Why do I ask questions? Well, I'm constantly asking questions of the code base. I'm interrogating the code base because questions, they prompt your mind to explore, they prompt an open mind, and they prompt a challenging mindset where you're not going to trust the developer, you're not going to trust the code, even if it's an amazing developer from an amazing company or whatever.
00:25:47.338 - 00:26:08.370, Speaker B: Doesn't matter. By asking questions, you're putting yourself in that critical mindset, that challenging mindset. And then by asking questions, you're forcing yourself to then answer those questions. And it's part of answering those questions that you'll learn a lot more about how the protocol works and that eventually you'll find those more interesting, those harder vulnerabilities.
00:26:09.910 - 00:26:47.360, Speaker A: It's always so funny to me, listening to you talk about a lot of the legwork that you've done to get as good as you've become and continue to get better and better where you're like, yeah, ripping through 200 findings on solid it. Yeah, of course I did that. What do you mean? Why haven't you almost like, yeah, that's the prerequisite to doing that. There's no shortcuts. But it's even funnier to me because this kind of daunting chore for a lot of people or this seemingly daunting chore to you is almost like, well, of course I did that. I want to be the best. I want to get better.
00:26:47.360 - 00:27:05.950, Speaker A: Finding holes in billion dollar code bases is really freaking exciting. Why would you not get excited? Why would you not want to go through solid, it almost seems like you have a leg up in what you do simply because you find so much joy in doing these seemingly difficult tasks.
00:27:06.110 - 00:27:32.526, Speaker B: Well, that's right, Patrick. I do this, number one, because it's fun. That's the biggest reason. And for know, I have the most fun when I push everything else out of my, you know, when I'm doing my audits, I'm not thinking about my wife, my child. I'm not thinking about politics, Joe Biden, Donald Trump. I'm not thinking about the World Economic Forum or any of this crazy stuff. Right? When I'm doing audits, everything is pushed completely out of my mind.
00:27:32.526 - 00:27:59.822, Speaker B: And I am like 100% in the zone, in the code. And that's actually when I'm most happy. That's when I'm most fulfilled. So for me, this isn't a job. For me, it's amazing that I can get paid great money to do know, because this is like the best, I'm having the best time of my life. Like, smart contract, security. It's what I was born to do, Patrick.
00:27:59.822 - 00:28:24.640, Speaker B: It's what I was made know. I just can't believe it took me so long to find it. But think about it. If you want to compete with me, think about that. Can you compete with that? Do you have the same motivation that I do? Do you achieve your ultimate peak happiness by grinding out 200 findings on solid it? By grinding through a code base? Is that peak happiness for you? Well, it is for me.
00:28:25.730 - 00:28:51.110, Speaker A: So we need to teach people to almost have more fun so that just the emotional response you have right there and then. Yeah. Having worked with you for a while now, I've seen this a couple of times, and I always, always love seeing it, like, oh, my God, this is so much fun. I'm in peak happiness. Screw wef. We're going to keep using swell here. I'm about to find a bug in swell in this private audit for them, or we're going to make them more secure.
00:28:51.110 - 00:29:00.810, Speaker A: And they were audited by these six other auditors before, and I found this bug and they didn't just like, you almost go like, super sane.
00:29:02.030 - 00:29:14.990, Speaker B: Yeah, it's great fun for me. I'm at peak happiness, peak fulfillment when I'm in the zone and I'm just 100% focused on the protocol and I've filled up my mind with it. That's Peak joy. I'm at peak joy.
00:29:17.250 - 00:29:43.874, Speaker A: So this is what everyone needs to strive for. Listening to this podcast, by the way. Go for peak joy. Find your zen and then go Super Saiyan as daisha does. So back to the mentality that you have for these, though. You're saying you're asking all these questions, right? And there's some low hanging fruit that you knock out of the park. And basically you said one of two constructs are what give you all your findings.
00:29:43.874 - 00:29:46.280, Speaker A: One of them is the low hanging fruit. What was the other one?
00:29:46.970 - 00:30:32.914, Speaker B: That's the hard stuff. That's where I'm asking questions of the protocol. And I have a lot of heuristics that I use as well. So there's a lot of things that I know the types of mistakes developers make because I was a developer, and then I spent a lot of time reading other people's code as well. So yeah, I know a lot of things to look for. I have a large library of heuristics, and many of them I've internalized so that I'm not doing it consciously anymore, but it just happens automatically. But then there are also like when I study audit reports or study findings that other people have made, I try and build new heuristics from them, and then when I build new heuristics, I add them to my list, and then I consciously implement those new heuristics in my audits until they become unconscious.
00:30:32.914 - 00:30:46.940, Speaker B: Right. Because for me, the way that competency works is first it's conscious, first I have to learn something, then I have to apply it over and over again. And once I've applied it enough, then I've internalized it and it just happens automatically without even thinking about it.
00:30:48.270 - 00:30:49.130, Speaker A: Go ahead.
00:30:49.280 - 00:31:06.660, Speaker B: Yeah. So all these questions that I ask of the protocol, a lot of these questions, they just come automatically now, because it's just from experience, from grinding out all these things, from having this almost second nature heuristics internalized, that I know to ask all these questions as I'm reading the code.
00:31:08.310 - 00:31:11.780, Speaker A: What are some of the most important heuristics that you keep in mind?
00:31:12.950 - 00:32:06.162, Speaker B: Sure. So one of the most important heuristics is that I'm looking for symmetry and asymmetry. So what does that mean? Well, it means that, for example, it's quite common for developers when they're writing code, that they will actually write the same code in multiple places when they shouldn't. That's something that can happen quite a lot. And even if sometimes they'll write a function and they should be reusing that function everywhere else. But then perhaps there's a bigger team of developers working on the same code base, and one guy, maybe the new guy, he doesn't know about this function that he's meant to use, so then that he'll implement the same thing again. So I'm looking for these things where there should be symmetry, it should all be the same, it should all be calling that one function, but maybe a developer has implemented it differently in one area, and there's an asymmetry there.
00:32:06.162 - 00:32:41.002, Speaker B: And for me, that's a big red flag. If I see asymmetry where there should be symmetry, that's a major red flag. On that same topic, there's also patterns that I look for. So, for example, I can give you an example in swell. So from swell, they had an off chain bot that could do a number of things, but the off chain bot can be paused as well. So there's this pattern that develops in the code base where whenever the bot does something, there's a check to see if it's paused before it does that thing. So as I'm reading the code, I'm internalizing this pattern.
00:32:41.002 - 00:33:14.650, Speaker B: I see this is a coding pattern that they're using in this code base. So then I automatically go into hunter mode now, where I'm switched on and checking for is there an OD place out? And it turned out, for example, that in a swell audit, there was an OD place out where the bot could do, actually something quite serious when it was paused, which it shouldn't have been able to do. Right? So I'm looking for symmetry, asymmetry. Is there symmetry? And then there's a spot where there's not symmetry. And then I ask the question, why isn't there symmetry in this spot? What is the consequences of a lack of symmetry in this spot?
00:33:18.900 - 00:33:49.370, Speaker A: Interesting. So this is almost more like a code smell, right? Since you know how software engineers work, you're saying, okay, where did they have this pattern that repeated? And then do they break that pattern somewhere? And maybe that's going to be a tip that there might be something wrong. It doesn't necessarily mean there is something wrong, but it's almost like you're not necessarily looking for, like, a bug here. You're just looking for some type of inconsistency. Almost.
00:33:50.060 - 00:34:39.500, Speaker B: That's right. Yeah. So you call it a smell, I call it a heuristic, and I have plenty of heuristics. Another thing that I do is I'm looking for state transitions, right? So if you think of, in traditional computer science, a smart contract, and the EVM is just a finite state machine, right? So that's just sort of a big computer program that can just move from one state to another, and it's deterministic, meaning that if you provide it with the same inputs, it'll always move from state A to state B, for example. Right? So I view the smart contract as a state machine, right? So as I'm reading the protocol. I'm trying to understand what are the valid state transitions? And then I'm looking for invalid state transitions. I'm looking for unhandled state transitions, particularly in, for example, if there's a multistage process.
00:34:39.500 - 00:35:17.690, Speaker B: Right? So if there's a process where you might have step one, step two, step three, I'm going to automatically go and see. Can I get to step two? If step one hasn't been performed, is the developer actually validating that every single step has occurred correctly, that all the state has been correctly updated? And are they checking that? Because quite often you'll find that they're not. Quite often the developer will just assume. They will just assume that the step has been completed when it actually hasn't, or they won't validate correctly that it has. And that gives you an invalid state transition. It lets you transition from one state to another when you shouldn't be able to.
00:35:19.980 - 00:35:27.480, Speaker A: Makes a lot of sense. What, are there some other really important heuristics that you try to keep in mind when attacking code bases?
00:35:28.460 - 00:36:31.560, Speaker B: Sure. So another really good heuristic that gets me a lot of nice findings, which are very sort of hard to find, is do many small operations result in the same effect as one large operation? Right. So this has gotten us some really nice findings in our private audits, some really nice highs. And so this is a very simple heuristic, but quite often there'll be weird things that happen at large or small numbers. And that's another heuristic that I also use. Like what happens if I use a really big number or a really small number? But imagine that there's a process, and so you can call that process and do one big operation. What happens if you call that process doing, say, ten smaller operations? Well, if the inputs of the smaller operations add up to the inputs of the one large operation in most protocols, the effect, and when I say the effect I'm talking about, the resulting state changes should be the same in most protocols.
00:36:31.560 - 00:36:42.130, Speaker B: But you'll often find that something actually goes wrong when you do those multiple smaller operations compared to the one big operation, which you may not find just by reading the code itself.
00:36:45.400 - 00:37:21.284, Speaker A: Interesting. So we have a couple more different heuristics that we put in here, or that you think about when you're approaching these. How does this compare to. So Hans on our team as well, he's a big fan of the checklist. Right. And if you, on Solidit XYZ, there's even like a checklist tab, which if you're doing a private audit or a competitive audit, it's basically a list of vulnerabilities to walk through and look for in all of your competitive audits. How does this heuristics compare to this checklist mentality? Are they the same thing? Are they slightly different? How does that compare?
00:37:21.432 - 00:37:52.456, Speaker B: Yeah, so I think the checklist is great. I do use that as well. But the heuristics, these are general principles, whereas the checklist tends to be specific to different solidity things or different protocols. And that's very good as well. It's great to know those things. For me, a lot of those, they're the easy findings that I pick up straight away, whereas the heuristics, they're general principles for the types of errors developers make that can help you find all sorts of errors. Another heuristic that I mentioned briefly is the big numbers or small numbers.
00:37:52.456 - 00:38:23.510, Speaker B: Right. So for example, in the current private audit that I'm just doing, just yesterday, I found an interesting issue where, for example, there's a function that has some math in there and it's meant to run for a particular sort of integer input set. Well, actually, I used stateless fuzzing for this one. I just wrapped the function in a standalone stateless foundry fuz test. And what do you know, it actually reverts due to overflow for a large section of the input space that it's meant to work for.
00:38:26.680 - 00:38:27.664, Speaker A: Boom finding.
00:38:27.712 - 00:38:46.170, Speaker B: Boom. Boom finding, that's right. But that's just a simple heuristic, right. What happens with small numbers? What happens with large numbers? Often with small numbers, particularly in systems where there's like a share calculation, often with small numbers, you can find some precision loss, like some rounding down to zero precision loss or something like that.
00:38:46.560 - 00:39:15.732, Speaker A: So whereas the checklist is very granular, right. The checklist is going to say, is there, actually, I have the checklist up right here, maybe, is there validation on the size of input data? Right. Is there zero address checks? Right. Very like things you can literally checklist. Yes, they've done that. No, they haven't done that. You're saying, it almost sounds like the heuristics are kind of more these abstract things.
00:39:15.732 - 00:39:41.644, Speaker A: Right. Like what you're saying one of your heuristics is like check for information asymmetry. It's very difficult for you to be like, yes, checked. There's no asymmetry anywhere in the code base. Right. That would be very od to tell to a protocol. I mean, I guess hypothetically would be possible, but it almost sounds like heuristics are more these high level approaches, checklists is more these very granular approach to securing a code base.
00:39:41.644 - 00:39:42.444, Speaker A: Is that right?
00:39:42.562 - 00:39:55.440, Speaker B: That's right. And so the checklist, they can help you find very specific vulnerabilities, but the beauty of the heuristics is they can help you find vulnerabilities that are general, generic. They can show up in all sorts of forms.
00:39:57.300 - 00:39:58.816, Speaker A: Got it. Makes a lot of sense.
00:39:58.918 - 00:41:01.732, Speaker B: I'll give you another good sort of rule. For example, another good heuristic is when you see a function, ask the question, what happens if I call this function with a nonexistent identifier? Right? So say if it's like an ERC 721, what happens if I call the function with a token ID that doesn't exist? Or if the function takes, say, an address, can I call that function with an address that I control, for example? Right. Another good heuristic that I use is from the test suite. So one thing that I do as well is I read the test suite that the developers give us. And why do I do that? I think the test suite is often underlooked by auditors, but the test suite can be really helpful for finding areas to focus on. So we've had test suites, for example, that if you just run the coverage, it's like, boom, 100% coverage, right? Amazing. Or like 90%, 95%, 97%.
00:41:01.732 - 00:41:50.832, Speaker B: It looks really good. Like, if you just run the coverage, looks amazing. But if you actually go and read that test suite carefully, you might find, for example, what do I look for on the test suite? I've got test suite heuristics here. So one of my test suite heuristics is I'm looking to see if the actual state changes have been validated by the test suite. So one thing that is very common to find is that the tests that developers write in this industry, they do not validate all of these state changes that take place. Instead, they might just call the functions, and if it doesn't revert, it's all good. Or they might just validate the events that were emitted, or they might validate the output parameters that are returned by function calls.
00:41:50.832 - 00:42:37.936, Speaker B: But actually, the most important thing to validate is the actual changes in state that should have occurred. And if I see that those changes in state haven't been validated, then I know straight away that that's one area that I want to focus on. Another thing that I look for, another heuristic I look for in the testing suite is I'm looking for integration tests between important contracts. So you can have a testing suite that has 95% coverage because you're testing all those contracts in isolation. But what happens when you have them all integrated together? Well, that's when you can find that things actually don't work as they should. Right. Another thing that I'm looking for in testing suites is does the testing suite actually do anything different to the production code? So we've seen this before in the testing suites.
00:42:37.936 - 00:43:18.450, Speaker B: For example, when the testing suites, they're calling functions that never get called in the protocol code, right? So that means that in the test harness, it's doing something different than actually happens in the protocol. What's the consequence of that? You ask that question and it leads to findings with that as well. I'm looking for functions in the protocol code that never get called. So why is this function in there if it never gets called? And it's funny because I found this exact scenario where in one of our private audits, there was a function in the protocol code that never got called, but the test suite called it. So the test worked, but the protocol never called it. So if you actually tried to do the thing on the protocol, it failed. Right.
00:43:18.450 - 00:43:43.576, Speaker B: Similar with variables. So, for example, with another heuristic I use for contracts is, for example, I'm looking for storage variables that get written to but never get read. Right. So what's the point of writing to it if you don't read it? Well, probably, it's probably meant to be read somewhere, but the developer just forgot about it. So what's the consequence of that?
00:43:43.758 - 00:44:12.660, Speaker A: So we have this large list of heuristics, and I actually love the fact that you're talking about looking at the test suite. I feel like so many security researchers almost ignore that as noise, which to me has always been kind of OD, because it's like the test suite is there to test the code base. Right. And why not use that to your advantage? But it sounds like you have this large list of heuristics. Is this list public anywhere, or is this kind of an internal dation thing that you're constantly updating?
00:44:13.240 - 00:44:48.830, Speaker B: Yeah, it's an internal data thing that I'm constantly updating. I think on solidit, ages ago, I did start to put a heuristic section on solidit, and then for our recent company offsite, I did make a PowerPoint as well, with more heuristics as well. But it's mainly at the moment just an internal data thing. I think in the future I may do a big deep dive or a big article on it, but right now it's just an internal data thing and that we're highlighting a little bit today.
00:44:49.360 - 00:45:22.104, Speaker A: Cool. So we're running a little bit short on time. We have maybe 510 minutes left here. I have two main questions that I want to get through, and these might be super quick or they might take forever. What are some of your favorite findings that you found in some of your audits, your security reviews, and would love to keep it tailored to ones that are already public. I know you're working on and have worked on some that are not quite public yet, so let's not talk about those. Yeah.
00:45:22.104 - 00:45:26.730, Speaker A: What are some of your favorite findings that you're most excited about, you're most proud of?
00:45:27.680 - 00:46:23.150, Speaker B: Probably the number one finding that I've had at Cypher was the delegated flashlorn exploit on Dexidao. And why was that my favorite? Because that protocol had been audited by a number of companies, and they explicitly had put in place a number of protections against this attack. So they were aware of this attack, and they had tried to mitigate it multiple ways. And it took me about a week to just relentlessly attack it from all different angles. And finally, I found the needle in a haystack. I found an unhandled state transition. I found something that I could call which they hadn't expected, and I was able to execute this beautiful attack, get around, bypass all of their protections, bypass all of their mitigations, and completely own them.
00:46:23.150 - 00:46:32.976, Speaker B: That was great that we found it in the audit. Right. So that was my all time favorite at the moment.
00:46:33.078 - 00:47:10.700, Speaker A: And why was it your favorite? It was your favorite because it was audited by a ton of auditors before you. So there's always this mentality, and I've heard a lot of people say this, it's not fair to compare sequential audits, because the knowledge from a past audit might have influenced a future audit, but at the same time, when it's a bug, a whole bunch of other people miss you, can't help but be like, yes, I'm so amazing at what I do. So you're excited because it was audited by a ton of people before it took you a week to find it. And it sounds like the exploit itself was sufficiently complex.
00:47:11.360 - 00:47:42.432, Speaker B: Yeah, it was beautiful. And to their credit, one of the previous orders actually found a more simpler version of the exploit. And so Dexi took that and put in more and more mitigations. Right. But I was able to bypass all of their new mitigations and execute the exploit using a beautiful customer attack contract, actually two beautiful customer attack contracts. And just find I was able to find the needle in the haystack, these unhandled state transitions. By seeing the system as a state machine and seeing how I could squeeze.
00:47:42.496 - 00:47:51.704, Speaker A: Wedge my way in, you can't help but just hear the excitement in your voice when you're talking about like, oh, man, it was autobiography, people.
00:47:51.742 - 00:47:52.696, Speaker B: And it took me a week and.
00:47:52.718 - 00:48:46.760, Speaker A: I found the needle and then boom, my last and probably most important question, although I'd be happy to listen to you go on about all the different findings that you found that you're excited about. Actually, can we train the industry to have this almost fun first approach? Right? Like, obviously what you do is very competitive, right? You can hear it. By the way, the first thing you said was, hey, it was audited by a lot of other people. And there's that competitive spirit here. You were talking earlier before, you're like, how are you going to compete with me when this is like, instead of me playing video games all day, I'm like auditing code bases and I get the same high from it. How do we train the rest of the industry, and can we train the rest of the industry to almost take this fun first approach to make all this very difficult and serious work much more enjoyable, right?
00:48:46.830 - 00:49:42.700, Speaker B: So I guess the first thing, Patrick, is I actually believe that anyone can do what I do, right? So I'm actually not special at all. The only difference between me and a bunch of other people is, I don't know, like 500 to 1000 hours of study and application, right? So I believe that anyone can do what I do if they're willing to put in the work that I put in or more. So from that point of view, it's just a matter of, is it fun for them? Right? Because if it's not that much fun, they probably won't be willing to put in the work. Money by itself is a motivator for some people, but at some point, money loses its appeal, right, because there's only so many things you can buy, right? And for me, I actually live a pretty simple life. I'm never going to buy a lambo. I have no interest in buying a lambo. It's a depreciating asset, usually.
00:49:42.700 - 00:50:16.228, Speaker B: And then I got to pay for servicing and insurance. It's just a waste. I have no need for boats or any of that crap like the crypto bling and stuff. Money is not a main motivator for me. But if you can make it fun, if you can sort of trick yourself to make it fun. Make it exciting. If you can enjoy the process of learning, of acquiring knowledge and then learning to weaponize that knowledge and use it, if you can sort of gamify that in your mind to make it really fun, then you can go and put in the 500 hours, you can go and put in the 1000 hours, and you will want to do it.
00:50:16.228 - 00:50:44.464, Speaker B: If it's fun, you want to do it, and then you can do everything that I do and probably better. If you're willing to work harder than me, then you can probably be even better than me. Right? It's just a matter of time and effort. And so I think that anyone can do what I do. And I'll tell you my secret, right? This is the grand secret, right? What do I do? What does it come down to? I fail repeatedly until I succeed. That's it. That's what I do.
00:50:44.464 - 00:51:04.036, Speaker B: I just fail and fail and fail and fail and fail. But I never give up, right? So I view every failure as what? As an opportunity to increase my knowledge of the protocol. As an opportunity to learn, to grow. I don't get discouraged. Failure doesn't discourage me. I don't get depressed. No.
00:51:04.036 - 00:51:29.420, Speaker B: I get excited. Why? Because with every failure, it brings me that much closer to the next success. So I'm never giving up. I'm going to continue to improve. I believe, Patrick, that I'm just at the beginning. So I believe that I'm basically a noob in this space, that I'm at the beginning of my evolution. I'm nowhere near Super Saiyan just yet, but I'm getting there.
00:51:29.420 - 00:52:03.930, Speaker B: I'm getting there. And so I believe that I'm going to continue to grow. To continue to increase. And a year from now, two years from now, I will be super saiyan mega level ten. And I think that everyone else can do that as well. Right? So I do believe that we can definitely onboard the next thousand smart contract developers, the next 10,000 smart contract auditors. It's all about just reaching people who think this is fun, who really enjoy working in this space, and who are excited and motivated and they're happy to put in the work and to achieve success.
00:52:05.580 - 00:52:38.820, Speaker A: I almost want to clip that last 60 seconds and stick it on every motivational TikTok or YouTube short out there or play it on replete when I wake up in the morning, because that is absolutely phenomenal. So we have about maybe 60 seconds left here. Daishan, is there anything else that you want to say to any of the developers or security researchers out there listening, trying to get better, trying to improve.
00:52:42.840 - 00:53:23.692, Speaker B: I want to say to you, Patrick, thank you so much for all the work that you do. Right? Thank you so much that you create so much free educational content for everyone. I've tried to do that in a small part with sharing all my research publicly as well, but I think that what you do is just amazing. And that's such a large part of why I joined Cyphern, because we're so aligned on making our research freely available to everyone. We live in a time where it almost doesn't matter where you are, you can be anywhere in the world. Everything that I learned in this space, I learned for free online. Literally, you just need an Internet connection and like a laptop and some time.
00:53:23.692 - 00:53:50.796, Speaker B: And everything that I've learned, you can learn for free. And even better, because Patrick, updraft didn't exist when I started, right? I had to hunt around, doing out of date courses, reading out of date materials, trial and error. But now you can go to updraft and get the latest and greatest right there for free. It's just amazing. Absolutely amazing. So what can I say? All I can say is, if you want to get it, then go get it. You go make it happen.
00:53:50.796 - 00:53:59.020, Speaker B: If you want it, then go get it. The tools are there, the info is there. We've made it all free for you. It's never been easier.
00:54:01.140 - 00:54:20.052, Speaker A: I'm about to run through a brick wall. If you want it, go get it. It's there. Phenomenal way to close this out. Well, Dacian, thank you so much. I'm going to go hit a PR at the gym because I'm filled with determination from this podcast right now. Thank you so much.
00:54:20.052 - 00:54:30.564, Speaker A: Thank you everybody for watching and have a phenomenal rest of your day or whatever it's going on in your life right now. Thank you all. And thank you so much, Dacian, for being here and sharing all this.
00:54:30.682 - 00:54:37.790, Speaker B: No worries. I loved it. Patrick, thanks so much for having me. Peace out, everyone. Good luck and I wish you all the best and I hope that you're going to be the next big success.
00:54:38.480 - 00:54:39.850, Speaker A: Absolutely. All right, take care.
