00:04:42.820 - 00:05:24.510, Speaker A: And we are live. Welcome everyone to Acde one seven three. So a bunch of Dancoon discussions today. So around Devdet tens, I think there's been some new analysis done as well on the cl block and blob receiving and processing times, some updates on KZG. And I think once we go through that we can chat about what we want to do next for Denkun. And then finally Carl will give us an overview of the roll call series that kicked off last week. And there's an EIP around empty accounts that Dano wanted to discuss.
00:05:24.510 - 00:05:40.210, Speaker A: But I guess to start we've launched Devnet ten and Perry, I saw the DevOps team put together a pretty comprehensive doc analyzing how things have gone so far. Do you want to take a few minutes to walk through that?
00:05:41.540 - 00:05:46.320, Speaker B: Yeah, maybe banmas can first set the stage and then I'll continue after with the analysis.
00:05:46.400 - 00:05:54.852, Speaker A: Yeah, sounds good and I'll share. This is the doc that I've just shared in the chat. Is Barnabas here?
00:05:54.986 - 00:06:24.244, Speaker C: Yes, I'm here. Yeah, I can begin. I was just also looking for the doc. So basically we launched with 330,000 validators. So this is just slightly above the churn limit of four. So we started out with a churn limit of five, and we wanted to test to make sure that the churn limit thing is working the way we want it to. And in epoch 256 we have hit the nub.
00:06:24.244 - 00:07:13.024, Speaker C: And at that point we had about 5000 validators in the deposit queue and a few thousand in the exit queue. And a few epochs later, I think in epoch 260 we started to see that the churn has indeed went down from five to four. So that's working. And we had a couple of open issues that should be now closed. So we found a bug during epoch 32 to 35 during a non finality incident that I have caused by mistake that Prism couldn't think back after the chain has started finalizing again. But this issue might have been solved already. I'm not sure.
00:07:13.024 - 00:07:38.170, Speaker C: The issue has been closed by Prism and there was another issue by Teku regarding some mass deposits where the teku nodes were not voting on the correct deposit head. Also seem to be closed by now. And we have some upsetting Mev related issues that maybe we can discuss a bit later.
00:07:41.180 - 00:07:42.120, Speaker A: Awesome, thanks.
00:07:42.190 - 00:08:16.288, Speaker B: Yeah, to continue on that, we did the Devnet ten in sort of two phases. So the first phase was roughly 24 hours. So we had the Dankoon fork running. We targeted a stable blob production, so roughly three. We weren't spamming the network to any crazy number. We had arm machines as well this time around, and we basically just wanted to see how the network behaves if there's nothing chaotic going on. There was no blobber, there was no bad block.
00:08:16.288 - 00:09:05.952, Speaker B: It was just blobs, as you would expect in a regular network. You can see that as the baseline analysis and things look to be really good. We do see some spikes here and there and potential places for optimization. Most of the charts in the analysis are aggregated across clients, but I've also linked all the dashboards, and there's often a filter on the top where you can choose your own client. So if client teams want to dig a bit deeper into how their particular client performs, and if they notice something that's more of a client specific issue, I would suggest hunting those grafana dashboards. If you don't have access to them, please reach out and we'll create an account for you. The most important ones, I think, are the block and blob analysis.
00:09:05.952 - 00:09:12.180, Speaker B: So that kind of correlates how many blobs exist in a block. And when things are being propagated.
00:09:13.880 - 00:09:14.244, Speaker A: As.
00:09:14.282 - 00:09:44.940, Speaker B: A TLDR, this was just the baseline. Towards the end, you can see blob performance of a client pair and just say, every client is including blobs. So fundamentally, we're good. What we then did was last evening we started the blobber as well as spamming blocks. Sorry, spamming blobs to a much higher degree. Also just a side point. Throughout the entire baseline analysis, we had TX files running, and that means there was transaction load.
00:09:44.940 - 00:10:34.492, Speaker B: You can see very often 100 plus transactions in the network. So last evening we started the blobber. So we're targeting more six blobs all the time. Of course, that's impossible to always hit six blobs, but we're trying as often as possible. You can see there's some outliers in CPU or RAM usage, but in general, things still look quite good. We don't notice anything specifically different on our machines, which is also a great sign. Network use did indeed go up, and we're seeing extremely spiky behavior, which is also probably the point that Peter mentioned, I think, at the last ACD or the call before, where there's really burst use of the network, which we're yet to completely debug.
00:10:34.556 - 00:10:34.832, Speaker A: Why.
00:10:34.886 - 00:11:12.910, Speaker B: But, yeah, once in a while you do see extreme bursts. As a result of the blobber, we seem to have knocked out at least the prism node, one prism node, potentially more. But Terrence has already posted a message on interrupt saying they've identified the issue and they're patching it. Besides that, there was a nimbus node that was also knocked offline, but it seems to have healed before we got to it. So that's a good sign. We're seeing an increase in the depth of the reorgs, which is not a great sign. And we're yet to debug if that's a result of the blobber, or if that's a result of something else.
00:11:12.910 - 00:11:56.872, Speaker B: In general, blobs on average, are still being included at a great rate, but the extremes have widened. I would recommend looking at the heat map for that analysis. And besides that. Yeah, I think besides that, we're still seeing stuff within tolerances. The trend that we're seeing very often is that everything is happening as we're expecting. It's just our tolerances are being reduced. And there is a note that the depth of the Riog seemed to be, for some reason, higher on teku nodes than the other ones.
00:11:56.872 - 00:12:25.830, Speaker B: So maybe someone from the tech team can also look into that. And just one last point, regarding MeV. We got the Mav workflow working completely. There are blobs being included. We only tested this with Lodestar nodes, so in the Mav analysis, you would only see loadstar nodes. For now, we're rolling out Mav on all other nodes. But the fact that we're seeing blobs included indicates that MeV workflow probably works fine now.
00:12:25.830 - 00:12:38.730, Speaker B: And yeah, I think there's some follow up analysis from G Eleven tech and from Enrico from the techo team. So I'll also maybe ask if they want to say something about that.
00:12:40.540 - 00:12:57.340, Speaker A: Yeah, amazing work, Perry and Barnabas. And thanks so much for putting these docs together. It's really helpful to have it on one. And Enrico, Gizinder, either of you want to chime in with your analyses?
00:13:00.020 - 00:13:27.492, Speaker D: Yeah, I can share what I gather two days ago and actually yesterday, and I updated also what the data from three nodes to this morning. So I can share maybe my screen, so I can give you a quick recording.
00:13:27.556 - 00:13:35.690, Speaker A: Stopped recording in progress. Sorry about that. Should be good.
00:13:36.640 - 00:14:41.340, Speaker D: Yeah. So I'm sharing here what the aggregated view of the data I gather. So, I gather this data from only three taco nodes, because I'm using the debug logs that we produce specifically for blocks timing and blobs timing. So, what we are seeing here is that we have this graph with the timing at which we first see something related to a particular block route that could be the block itself or one blob that we saw. And on top of it, we have the computation time, which is the amount of time that passes after we see the first block route to complete the old set of blobs and block. And this is taking in account the validation. So this is post gossip validation.
00:14:41.340 - 00:15:37.840, Speaker D: So what is interesting here, maybe this is the relationship between zero and the six. So as you can see, we got a pretty trend going up. And most importantly, we have this percentile table that says so the graph represents the mean. So it's roughly, you see in the 0.5 percentile and matches accordingly. But I'm also reporting some more higher percentile. And we can see that roughly at nine we can get around 200 millisecond kind of each blob added to the block.
00:15:37.840 - 00:16:10.710, Speaker D: So up to having pretty high wide timing around 1 second compared to maybe five millisecond when blobs are. Yeah, so we got. Yeah.
00:16:12.440 - 00:16:20.808, Speaker E: So there's not a, like this. This is generally in line with the data we saw with loadstar rate. So not like a big surprise or.
00:16:20.814 - 00:16:21.544, Speaker A: Just haven't seen data.
00:16:21.582 - 00:16:39.824, Speaker D: Yeah, yeah. It's just confirming that trend is very similar, what has been reported by Lodstar. So removing the constant timing of the validation that maybe Lodstar is not reporting, but the trend is very similar.
00:16:40.022 - 00:16:54.416, Speaker E: And these absolute numbers are fine and safe. Which attempt or which method you try to use to then extrapolate these numbers to mainnet where you may or may not have concern.
00:16:54.528 - 00:17:15.630, Speaker D: Yeah, that's the real deal here. So we got this baseline that presents our testnet here and then how we translate this to an empothetical main net behavior. This is where interpretation and ideas may vary. Definitely.
00:17:17.920 - 00:17:39.330, Speaker F: I don't think you can, unless at the very least I think you would need to send non empty blocks on the test net to understand what the actual baseline is. Because I think you cannot get this comparison if you just compare to empty blocks. Like that's not what mainet does.
00:17:39.800 - 00:17:47.750, Speaker A: And were these blocks empty or was this taken from Devnet ten where we had the transaction fuzzer on?
00:17:49.000 - 00:18:44.200, Speaker D: Yeah, this data is taken up to this morning and is taken from denab activation. So the first blocks that are empty is just because I think the blobs transaction were not yet produced. And after a while blobs has been starting to come in and you start getting these blobs also with different numbers. I don't know when these blocks were starting being full. And I can definitely filter out and starting gathering this data from a particular slot could be maybe interesting to remove the zero blocks that are also empty.
00:18:44.940 - 00:19:01.180, Speaker F: Yeah, because I think that makes a big difference. In real life, blocks aren't propagated that fast because they are more than a few kilobytes big. So this pushes our baseline down arbitrarily.
00:19:03.360 - 00:19:15.920, Speaker D: I'll chat with Perry to set a reasonable slot to start printing this graph and I can update you afterwards.
00:19:16.420 - 00:19:17.970, Speaker F: Right, that would be interesting.
00:19:18.900 - 00:19:27.056, Speaker G: As per his input, the blocks aren't empty, so they should be full and from the beginning.
00:19:27.248 - 00:19:29.220, Speaker D: From the dynamic activation?
00:19:29.640 - 00:19:36.890, Speaker B: No, definitely not from the dynamic activation. I think it's a couple of hours after because first we want to validate that the four point work.
00:19:40.810 - 00:19:49.980, Speaker G: But that still would be only very small data of empty blocks compared to the entire data set that you have used. I think.
00:19:52.030 - 00:20:06.830, Speaker D: Yeah, it's just a couple of hours, maybe those numbers will not move that much unless all the zero blobs block are always empty.
00:20:09.410 - 00:21:21.314, Speaker G: Yeah, I think it would be nice to see what is the actual propagation of the full actual propagation latency data of the full main net block. Because full block, which is similar to a main net block because load star on my main net node I saw that it takes around 3.5 seconds for 95 percentile of blocks to show up. So yeah, we need to figure out and also on the main net, there is an additional factor of MeV blocks whose proposals might be delayed anyway because of the communication between the builder and the validator. And so yeah, maybe even on the mainnet blocks I might need to filter out for the blocks which might not be MEV based, to do an actual analysis. But even if that is the case, then our major concern will be to figure out how to resolve the MEV flow so that we don't have an extra latency added over there. So the entire concern is that whatever is the latency diff, it shouldn't have an additive effect on the blocks.
00:21:21.314 - 00:21:45.710, Speaker G: And if it does have an additive effect on the blocks, for example, Lodestar would need to optimistically import the block in the sense that we need to optimistically run validations even before all blobs show up, which we don't do as of now, but seems like Lighthouse does it, and it definitely seems like a good strategy. So we might you're not currently running.
00:21:45.780 - 00:21:48.510, Speaker F: The execution client when you haven't received all the blobs?
00:21:50.530 - 00:21:56.580, Speaker G: Yeah, Lordsar is not doing that, but seems like Lighthouse is not sure whether other cls are doing it or not.
00:21:57.030 - 00:22:51.170, Speaker D: Yeah, uteku is doing that. So when we see the block, we already start doing the block import process or the state transition, and also contacting the yelt for doing the new payload API while dealing with blob site cars and also doing some partial KZG validation. If we in the meantime, we receive the subset of the blobs that we expect, and we only do the final import once, once all the blobs have been received. So there is a level of parallelism in teku up to three different streams. One is the state transition el importing and keyzg from blobs.
00:22:54.760 - 00:23:07.370, Speaker G: Yeah, Lordsa will sort of need to improvise this flow and come to what you guys are doing so that we don't see an additive effect. If there is an additive effect of blowbladency on the main net.
00:23:10.770 - 00:23:11.166, Speaker A: I mean.
00:23:11.188 - 00:23:28.870, Speaker F: There will be an effect, right, because it just consumes more bandwidth. And there will be an effect because you now need to receive all these things. And if one is delayed, it's just slower. But yeah, the question is how big that effect should be. And I think that's just much more complicated to know.
00:23:28.940 - 00:23:29.222, Speaker A: Right.
00:23:29.276 - 00:23:32.886, Speaker E: And extrapolate, trying to extrapolate, even in.
00:23:32.908 - 00:23:49.558, Speaker G: The current data, we see an additive effect, even when, for example, the bandwidth is perfect. If, for example, everything was parallel, then all the numbers of blob is equal to zero and blob is equal to six. They should basically align. Or maybe blob is equal to one and blob is equal.
00:23:49.584 - 00:24:17.720, Speaker E: No, I don't agree with that, because depending on the message, you're going to sit somewhere different in the relative mesh. So you might be one hop on message zero, and you might be four hops to get message five, because they're following different paths in the mesh. So by virtue of that, by having more messages, you have a higher chance of being a longer hop distance on one of those messages than others.
00:24:19.770 - 00:24:24.858, Speaker G: But would the higher chance translate into two x delay, which is what we are saying.
00:24:24.944 - 00:24:28.300, Speaker E: Well, it's not delay, it's two x to get the last message.
00:24:30.270 - 00:24:31.018, Speaker D: Yeah.
00:24:31.184 - 00:24:32.780, Speaker E: Rather than to get.
00:24:43.460 - 00:24:44.172, Speaker A: Right.
00:24:44.326 - 00:24:48.710, Speaker F: And then also this is assuming infinite bandwidth, which is also not true.
00:24:58.540 - 00:24:58.904, Speaker C: As.
00:24:58.942 - 00:25:43.290, Speaker D: You think, that things are not going in parallel. But taking my graph in mind, since the first scene is also going up in time, is actually a proof that things are actually sent in parallel. Because even the first blocks or blobs that we receive, in case of six blobs, we are getting delay there. So it's definitely the bandwidth that is kind of split between blobs and blocks. So the first message that is completely sent to the peer is getting more time because bandwidth is used more.
00:25:44.060 - 00:26:19.140, Speaker G: Actually on Devnet ten the bandwidth is not an issue because one gbps symmetric upload download is there. And so with your analysis it might mean that actually things are not being studied in parallel. And also, for example, Pawan mentioned that lighthouse first transmit the block and then transmit the blobs, and I'm not sure whether it's also transmitting the blobs in parallel. So I think all the CL clients should go and see whether there is parallelism in terms of the data transmission.
00:26:21.800 - 00:26:45.016, Speaker A: Yeah, this is something that we are exploring, that I'm exploring on Lighthouse if it's doable on lip PTP. I'm not the lip PTP expert, so I've been looking into the code and seeing if just calling publish on blocks and blobs in multiple threads, would that result in a concurrent publish over lip.
00:26:45.048 - 00:26:46.380, Speaker B: P two p as well?
00:26:46.530 - 00:26:55.360, Speaker A: So I'm checking that out and I should probably have results sometime by tomorrow, but it might take time because I'm not the lip PTP expert.
00:26:57.460 - 00:27:08.480, Speaker D: If it was completely sequenced from all clients, I would not expect the first scene to move. For me, the first scene should be completely constant, correct?
00:27:08.550 - 00:27:31.528, Speaker G: First scene should be constant because there is no bandwidth issue involved. And as far as loadstar is concerned, there is some serialization that is happening till the point of all the block and blob data being converted into bytes and handed over to the network layer. So there is a serialization factor, for example, which loadstar like engines can't do away with because we have only one main thread running.
00:27:31.694 - 00:27:44.440, Speaker D: You mean before even start sending anything, you serialize everything and then you start. So this is adding something that is function of the number of things that you have to send even if you send it in.
00:27:44.610 - 00:27:59.084, Speaker G: So that is because of the underlying node js concurrency model, in which basically async tasks start after the current whatever it can execute in serial, and then basically the handovers occur.
00:27:59.132 - 00:28:09.850, Speaker D: That's clear. So I think every client should double check the level of parallelism in gossip sending out it.
00:28:31.820 - 00:29:18.070, Speaker A: I don't know that there's a specific like that we have enough data here to come to a conclusion, but what do people feel is the right next step? So there's some comments in the chat about potentially running this on more nodes, potentially comparing it to large call data blocks. It might be worth also running this on a testnet and seeing in an environment where there are transactions in blocks by default and there's like a larger distribution of nodes. Yeah, I don't know how the people feel about next steps here.
00:29:21.240 - 00:29:36.200, Speaker G: I think we will definitely need to test it on a real testnet and then only make some conclusions about it because having it in a data center will not get us near to the real deployment conditions that might exist for Mainet.
00:29:38.720 - 00:29:53.890, Speaker B: Definitely agree with that one. Also, the setup we use typically doesn't have web3 signer, it doesn't have stuff like vouch we don't use any custom things. We're just basically running native clients and that's typically not how mainnet is run.
00:30:00.740 - 00:30:01.680, Speaker A: POTUS.
00:30:03.060 - 00:30:37.000, Speaker H: So one worry about going to testnets. I mean, certainly this has to be tested and there's a danger that we might need to go to bundling blocks with blocks. But one thing that worries me is that we have only so many testnets to fork and we are seeing very serious bugs deal on clients. Prism has had terrible bugs on every client has port one way or another on their own fork. So I would want to wait a little bit before we port testnet until the clients are a little more stable.
00:30:37.660 - 00:31:16.948, Speaker A: Got it? Yeah. So we basically have three testnets, right? Like we have Gorli, which is deprecated and this will be the last fork we have on it. Then we have sepolia and Hosky. So yeah, we want to make sure for sure that by the last one, things that are going live are effectively what's going to go on Mainnet. But yeah, how do people feel about stability in general at this point? Terence, besides stability, there are still things that we haven't tested, such as the MeV builder and the relay and client copat.
00:31:17.044 - 00:31:19.236, Speaker F: And that is arguably probably the biggest.
00:31:19.268 - 00:31:32.956, Speaker A: And most important part because 90% of the main client uses mev boost. So I think we probably will need to spend some more time testing it. I know we started on low star, but yeah, spend some more time testing.
00:31:32.988 - 00:31:34.800, Speaker F: It before moving to testnet.
00:31:39.090 - 00:32:00.120, Speaker B: I think the only counterpoint I have to that one is at least in all the previous fox, we only tested the circuit breaker before we agreed on Mainnet. And we still have like a couple of weeks to test out Mav workflows even before Gorli, for example, if we were to agree on it.
00:32:07.820 - 00:32:13.210, Speaker A: Yeah, I'm curious, how do other clients feel about this?
00:32:20.120 - 00:32:46.190, Speaker F: I'd be interested in seeing everything on a bigger test net. And yeah, we'll continue to use Devnet nine until we get another testnet up and running because there's still things I think we can figure out. And if we can, for example optimize our upload a bit. We could see if enricos numbers change on devnet ten.
00:32:50.720 - 00:33:00.560, Speaker A: So when you say sorry, you said see another testnet, but do you mean you want to move to something like gordy? Or you want to see another devnet like Devnet ten or devnet eleven?
00:33:02.740 - 00:33:04.300, Speaker F: Yeah, I was thinking Gordy.
00:33:04.380 - 00:33:04.636, Speaker B: Okay.
00:33:04.678 - 00:33:08.708, Speaker F: Honestly, we would continue to use devnet ten until.
00:33:08.874 - 00:33:09.396, Speaker A: Got it.
00:33:09.418 - 00:33:10.310, Speaker F: That was running.
00:33:11.640 - 00:33:30.940, Speaker C: We were actually thinking about shutting down devnet ten by Monday. So maybe we can have like Devnet eleven with smaller number of validators just for running it for a longer period of time, but not really for stress testing because running these 100 nodes is pretty expensive.
00:33:34.200 - 00:33:44.570, Speaker F: Okay. Yeah, that works as long as we have something. Because we're still digging into logs and understanding how this works.
00:33:47.980 - 00:33:58.504, Speaker C: We can definitely launch something maybe even bigger than Devnet nine for devnet eleven. And then we could have that until the point that we would fork.
00:33:58.552 - 00:33:59.150, Speaker A: Good.
00:34:01.200 - 00:34:02.830, Speaker F: Okay, that sounds good.
00:34:09.640 - 00:35:45.934, Speaker A: Any other client teams have thoughts on what they'd like to do for testing? For all the eight other teams, do people feel, I guess they agree that they need more time before running to a first testnet? Or are people more comfortable and potentially ready to move forward with Gordy Sooner? Yeah, I think I'll just call on the teams. I don't know, geth. How do you all feel about just overall readiness on the El side?
00:35:45.972 - 00:35:48.994, Speaker F: This is not really where I think the big decisions are being made.
00:35:49.032 - 00:35:49.966, Speaker A: So I don't really feel like it's.
00:35:49.998 - 00:36:07.720, Speaker F: My place to say let's go forward. But yes, from guess I think we're okay. More or less the same situation. On Nethermind's side, I think we are quite comfortable, but I think it's not on execution side.
00:36:13.080 - 00:36:17.716, Speaker G: Yeah, for Raregon it's the same. We are comfortable with going with the.
00:36:17.738 - 00:36:24.490, Speaker D: Fork on Gurley, but we can wait as well if there is need to wait more.
00:36:27.660 - 00:37:20.200, Speaker A: Okay, same at Besu. We're comfortable with it. Not in a rush to move. Think. Okay, so clearly on the CL, then looking at the MeV pipeline and a bunch of other smaller client specific issues, the two things. So yeah, maybe it makes sense to decide the next steps on the CL call next week. And I guess until then, if we are launching a new devnet, then the main thing we should be doing is making sure that all the clients are set up with the mev pipeline.
00:37:20.200 - 00:37:42.260, Speaker A: Is there anything else that we want to make sure we do on the next Devnet? I see a lot of thumbs up and comments around shadowforks as if I don't know if DevOps folk want to give an update on shadowforks.
00:37:45.560 - 00:38:00.600, Speaker B: Yeah, we can do Shadowfox. That's absolutely no problem. For our set. We just need to discuss which network it would be. Gurley just has a big state, so that would be the most expensive, and I guess either Holski or Sepoya would be the cheapest.
00:38:04.710 - 00:38:06.418, Speaker A: What's the most roll up?
00:38:06.504 - 00:38:06.850, Speaker F: Sorry.
00:38:06.920 - 00:38:08.420, Speaker A: Oh, sorry. Please go ahead.
00:38:09.270 - 00:38:29.610, Speaker H: Yeah, so for Shadow Forks, I wonder if there are already roll ups that are actually testing on for a four four. I mean, as soon as we have a deployment of actual clients that are sending block transactions at a more or less realistic pace, like an actual roll up batch posting, then it would be nice to have those tests.
00:38:37.680 - 00:38:48.530, Speaker B: I think at least optimism is using gurley, if I'm not wrong. So I guess if we do a Gurley shadow fork, then we can also ask them if they can point some load at us.
00:38:53.550 - 00:39:43.530, Speaker A: Okay. And there's a comment in the chat saying there's an op stack roll up that briefly ran on Devnet nine, so we could likely get that either on Devnet Eleven or on a shadow fork, whatever is the simplest there. Okay, so yeah, Devnet Eleven basically getting the mev pipeline set up with all the clients getting a roll up implementation running, and then obviously every client sort of fixing the issues they have with their own implementations. Is there anything else we want to see out of Devnet eleven before moving to Gordy?
00:39:52.030 - 00:39:52.634, Speaker D: Good stuff.
00:39:52.672 - 00:40:01.142, Speaker G: So I'll add importing the block and running validations even before all the blobs show up. So that is my to do.
00:40:01.296 - 00:40:26.390, Speaker A: Got it. Okay. And is it realistic to get this done in the next week or so, such that by next Cl call on Thursday we have understanding of how all those things went.
00:40:34.660 - 00:41:08.472, Speaker B: Yes, we can plan for just sort of summarizing on the schedule. We'd want to get rid of Devnet nine because it's using the old KCT set up. And I guess at this point all the tooling has moved on and everyone's using different folks. So we'd like to turn that off. Devnet ten can stay up as long as you guys decide. I think we were defaulting to shutting it off on Monday. After we collect all the data that we need, we'll start Devnet eleven, which would just be a longer running, small scale devnet.
00:41:08.472 - 00:41:23.250, Speaker B: So if anyone wants to actually test out tooling, et cetera, they can expect it to last for a while. And we can do a curly shadow fork. We plan for that early next week. So that by Cl call we'll have some more data. Does that sound okay?
00:41:28.160 - 00:41:30.350, Speaker A: Yeah. Does anyone disagree with that?
00:41:37.920 - 00:42:30.380, Speaker B: And is there any sort of request on the size of the girly shadow for. And we'd probably want to keep the. It depends. We're going to have to see how long we're still going to keep accepting girly traffic, because in the past, I think we had like roughly three days worth where we stayed, peered to the canonical chain before it no longer mattered. But yeah, I guess we can just do a mid size network. It would teach us more about the processing time, but I doubt it would teach us much about the networking. And the alternative is doing something at the scale of Devnet ten, just as a shadow folk.
00:42:35.200 - 00:42:50.000, Speaker A: Okay, yeah, I think that makes sense. Barnabas has a chat comment about can we choose a Gordy date now? And he was asking for November 9.
00:42:52.820 - 00:42:56.336, Speaker C: Next week. We can postpone it if it's really rushed.
00:42:56.528 - 00:43:56.600, Speaker A: Yeah, and I guess to give context. So I think generally what we try to do with Testnet forks is have the blog post out at least a week before the fork happens. So the blog post means all the clients have a release that people can go and download that's fairly well packaged. If we want to fork Gordy on the 9th, it means basically mid next week we would need the client releases out with all the clients. Does that feel realistic to people? And basically by the time the CL call happens next week, we would be a week away from the fork. Yeah, it would be kind of the absolute laylist. We could postpone.
00:44:01.000 - 00:44:02.820, Speaker D: It's fine for Aragon.
00:44:07.690 - 00:44:16.810, Speaker A: Yeah. How about on the CL side? Because that seemed to be where there was most concerns about timelines.
00:44:22.180 - 00:44:26.770, Speaker H: I'm speaking for myself, but I would say that Prism is not ready to work.
00:44:28.100 - 00:44:48.450, Speaker A: Got it. And I guess the alternative, basically, if we do this same thing a week from now, it's like instead of the 9th, then you're talking about something like the 15th or whatever on Gordy at the earliest, give or take.
00:44:49.620 - 00:45:13.690, Speaker H: So again, speaking for myself, not representative of the team, but I see very large and deep changes still being pushed in the branch. We are storing blobs on DB directly. There's no gachet for blobs. We are moving that to file storage. These things are like deep changes. These are not just one liners. I don't see how this is going to change in a week or so.
00:45:14.540 - 00:45:29.580, Speaker A: Got it. And do you have a feeling for the time for just larger changes to be done and the overall code base for prism to stabilize?
00:45:31.120 - 00:45:36.496, Speaker H: I do expect this to be of the order of weeks, certainly not months, but one or two weeks.
00:45:36.598 - 00:46:33.950, Speaker A: Yeah, got it. Any other teams have thoughts, concerns about this? Okay. And yeah, there's a comment. So saying that we probably shouldn't fork if Dev connect is going on. So this means it's unlikely. If we don't do like the 9th, which seems pretty early for teams, then it means we probably have the fork happen like in the week or so after dev connect at the earliest. Yeah.
00:46:33.950 - 00:47:17.256, Speaker A: So that means. Yeah, basically the week after Devconnect is having Gordy around somewhere between around November 22. I don't know if that starts to be okay. So Ansgar has a question. If we fork after Devconnect, do we have a shot of shipping this by the end of the year? I mean, historically, I think the closest we forked testnets apart has been like two weeks or so. So if we fork after Dev connect, that's November 22. Two weeks after that is November 6.
00:47:17.256 - 00:48:20.640, Speaker A: Two weeks after that is December 20. So we can probably get all three testnets done before Christmas. But it seems unlikely you can also shove main net into that, especially if we want to see, especially if we want to see things on testnets for more than a week. I think if you really wanted to push everything in 2023 and you're forking the first test net after Devconnect, you'd have to have something like a week between each different test nets, which probably means that the same client releases are used for different test nets. So that seems hard, given it doesn't give you the opportunity to fix any issues that you do see on testnets between one and the next. Yeah, there's a lot of chat going on. I don't know if anyone else wants to chime in on the call directly.
00:48:20.640 - 00:49:45.510, Speaker A: Okay, I guess, yeah. Let's see how things evolve in the next week and talk about it on the CL call next week. But I think clearly we won't be forking Gordy on November 9. And then if we're also not going to be forking the week after that because of dev connect, then it means we will be doing it towards the end of November at the earliest. And that also will give more time for client code bases to stabilize. Any other comments, concerns, thoughts around Dencoon testing and fork schedule? Okay, if not, Carl, you had an update on the ceremony? Yep.
00:49:45.670 - 00:50:22.774, Speaker F: So as you all know, we're now using the final output from the KZG ceremony, which is great, but a very important component is of that is for everyone to verify the contributions. So if you participated in the past and you know you participated, but your output's not in the final ceremony, then something clearly went wrong. And we have ways of proving that, but this is not something we're seeing. But it's important to check this. So there are two easy ways for you to do this. One is the same website that most people use to contribute ceremony, ethereum.org. You should just be able to go there.
00:50:22.774 - 00:50:52.270, Speaker F: You can paste in your credentials used to contribute, and that should allow you to verify the ceremony and claim a pop. Then there's a second alternative method, which is a rust script for doing the same verification. It performs slightly more in depth checks and is a little bit easier to obviously audit that the code that you see is being run. So if you really want to go in depth, I don't recommend doing that. So please everyone verify the ceremony.
00:50:54.610 - 00:51:20.374, Speaker A: Thank you. And as a heads up, you need to be on desktop to do this. A bunch of people have asked this question. Yeah, thanks, Carl. We were going to talk about next steps for Denkoon after, but I think that's pretty clear. We sort of went over that. But you had another item.
00:51:20.374 - 00:51:23.660, Speaker A: Carl, do you want to give an overview about roll call?
00:51:25.070 - 00:52:05.878, Speaker F: Yeah, sure. So roll call is something that myself, Anskar and Yurv have been working on. The idea is it's a neutral platform for coordination between a bunch of the roll ups and l two s. So there's a process that much like we have Eips and erCs. The new addition here would be rips, which is basically changes specific to layer twos. And this ideally allows them to have a little bit more standardization between them. There are many things that they've implemented which are slightly incompatible with one another.
00:52:05.878 - 00:53:19.280, Speaker F: So hopefully we can bring everyone on the same page and possibly even make some slight changes to the EVM if people particularly want, but at least try to do it in a standardized way that everyone can see and work on. And then the second thing is, the idea is to have it be an API between. Metaphorical API between l one and l two. So obviously there's lots of things happening in the l two s where people bring up points that they're interested in, but it's not obvious that whether this is just like one particular team that's interested in something, or whether this is something that'd be helpful for the whole ecosystem. So helping l one and all our governance processes to see have a bit more insight into what the needs are from the l two side of things, and then also from the reverse side of things is to help l two see when things are needed from them. So a great example of that is what we're talking about testing on Devnets by posting data blob data. So I'll go back and I'll bring this back to them, the mini l two s, and see if we can get some more of them to test.
00:53:19.280 - 00:53:56.300, Speaker F: The idea is just to help these processes become slightly easier to participate in. Everyone here is of course welcome. It's not supposed to be exclusionary or whatever, just help separate concerns. So if you want to participate, jump. There's a roll call call which happens monthly and you should be able to see that coming up in the PMS channel. The PMS repo, much like ACDe and ACDC, and also the layer two channel in the ETH R and D discord, has been renamed to roll call for participation if anyone wants.
00:53:59.250 - 00:54:03.680, Speaker A: Thank you. Any questions? Yeah, please.
00:54:04.770 - 00:54:33.800, Speaker F: Sorry, I didn't want to jump in, but I just want to mention we also have an in person event doing Devconnect. So if you're in town and you have some free time on, that's the Wednesday, I'll put the link in the chat and there might be some topics you can have a look. And if there's some topics that you think you might be interested in maybe joining the conversation. Yeah, reach out to us and that would be great to have some Taiwan people involved as well.
00:54:40.800 - 00:55:06.680, Speaker A: Awesome. Yeah, thanks a lot. Okay, the last thing we had today, Daniel wanted to bring up EIP 7523, which is about prohibiting empty accounts on post merge networks. I don't know Dano or Peter, if either of you are on the call to give some context.
00:55:07.980 - 00:55:10.170, Speaker I: I'm on the call. I don't know if Peter is.
00:55:12.540 - 00:55:12.952, Speaker A: So.
00:55:13.006 - 00:56:08.844, Speaker I: A quick summary of this EIP. So the scope really is mostly going towards some of the test data in the reference test and formalizing some behaviors for clients that meet the standards. But what EIP 75 23 says is that if you're either the main net chain after the merge hash, or if you are on a chain that has the Genesis post spurious dragon and has no empty accounts at Genesis, then if you follow the rules of the specification, then empty accounts will never be persisted between blocks. So based on that, clients can make assumptions that they'll never see empty blocks and the whole swath of corner cases that interrupt. As to what do you do with an empty block if you do a revert when you touch an empty not empty block, empty account. If you do a revert when you touch an empty account. If you touch an empty account, you delete it from the state.
00:56:08.844 - 00:57:07.970, Speaker I: That whole bit of logic that was put in after the Shanghai attacks no longer applies. And this does have impact on some client design if you're writing some state systems and evms that can behave more efficiently if they don't have to care about empty accounts, and they can run quicker if this EIP is presumed. So I guess the temperature check here is what do people think about clients having modes where they're only working post burritos? Dragon with no empty accounts? And if that's favorable, how they would feel about updating all the reference test cases that are postporious? Dragon to not have empty accounts and not test for empty accounts, but only to test in the legacy test cases. So those are the two questions I want to get before I push this any further elsewhere, to make sure that this is something that the rest of all core devs is cool with.
00:57:16.930 - 00:57:47.020, Speaker F: Not everyone at once, I think. I didn't get the full context, but are you aware that with Virgo trees, at least at the moment, there will be a difference between things like positions in a state that have never been written to versus position in a state that have just been overwritten with zeros? I just want to make sure this is not creating a conflict with that.
00:57:47.870 - 00:58:31.190, Speaker I: So if you're following the EVM specs right now in producing it, accounts would only get overwritten with zeros if self destruct happened. And since we're deprecating that and possibly removing it, I guess there was one question about doing it within the transaction, writing it out, and I think we settled on it wouldn't get persisted to disk if it was a transient storage option if it never actually hit the disk. So that is one corner case. But just the general premise is that you would never write an empty account in normal operation if things are operating correctly, aside from self destruct in the band mode.
00:58:33.690 - 00:58:37.174, Speaker C: Okay, I'm not sure I understand fully what your change means.
00:58:37.212 - 00:58:43.340, Speaker F: I just wanted to make sure that the new state structure with vocal tries would be taken into account.
00:58:44.270 - 00:59:19.654, Speaker I: Right? It shouldn't impact it. I mean, zeros on states will still be written. I mean, that changes nothing about writing a state. It only has to do with at the end of execution if you find you have an empty account. If you start with an empty account, how you'd handle it differently than if you end with an empty account. So as long as there's no empty accounts in the system, then following the rules of EVM ensures that aside from self destruct, you won't be writing empty accounts to disk. And as far as pulling it off a disk, I think that's know, maybe we need to drill into this with the vertical account.
00:59:19.654 - 01:00:01.170, Speaker I: But I don't see how it would affect anything different because the real issue comes in. There's one corner case where if you read an account that's empty, you do a transaction that touches it, which would then market for deletion from the system, and then you revert that block, and then the touch is undone and the deletion never happens. That's a real corner case that this is getting rid of. So I think that is separate from what's writing to disk. It's almost entirely within the evms to make the assumptions. And the real impact here is I want to go into the test code, and there's a few tests that still put empty accounts into post merge test cases and get those updated.
01:00:05.910 - 01:00:41.514, Speaker A: So I guess on one hand it's worth taking some time to just sanity check the vertical and self destruct compatibility issues. But assuming we did this, how do we reflect this in the chain activation history? So we have eips now that we've retroactively activated since Genesis. This wouldn't work in this case because of spurious dragon. So would we say this EIP was activated as part of the Paris fork retroactively in a way, or something like that?
01:00:41.552 - 01:00:58.420, Speaker I: So it's written into the specification. There's two cases that matches. The second is any chain which has no empty accounts in this post spurious fork. And the first one is the main net chain whose merge block has the hash, which is the merge hash. So it's written into the spec that the effectiveness activates at the merge right now.
01:00:59.510 - 01:01:27.580, Speaker A: Okay, so yeah, we can figure out how to represent that. The reason why I ask about this is like somebody who's writing your client from scratch. Do we want this to show up as part of the merge eips so that they can know when they've get to that point? Like, okay, I can assume there's no longer empty accounts or is there a better spot for it to show up?
01:01:28.830 - 01:01:50.820, Speaker I: Right. And I think that's exactly the question that we need to have. What is the appetite for writing accounts that can't do the full history of Ethereum that only start at the merge or only start with a certain point, with a certain set of data from a fork? Do we need to keep all historical ways to generate blocks in future clients or even current clients? And that's a separate pandora's box to open that I don't think we have nearly enough time to talk about.
01:01:52.870 - 01:01:58.790, Speaker A: So I guess where's the best way for people to discuss it? Should we just use the Eth magicians thread of this EIP?
01:01:59.290 - 01:02:00.994, Speaker I: There is an Eth magicians thread?
01:02:01.042 - 01:02:01.206, Speaker A: Yes.
01:02:01.228 - 01:02:03.320, Speaker I: I think that's the best place to drop your issues.
01:02:04.330 - 01:02:48.040, Speaker A: Okay. Any other comments, questions, concerns about this? Okay, yeah, thanks a lot, Daniel. So yeah, people can use this east magicians thread. That was the last thing we had on the actual agenda. Is there anything else anyone wanted to discuss before we wrap up? Okay, well, if not, we can close out here. Thanks everyone for coming on. And I'll see pretty much all of you on the testing call next Monday.
01:02:48.040 - 01:02:50.730, Speaker A: Yeah, talk to you all soon.
01:02:51.260 - 01:02:54.264, Speaker F: Thanks, Tim. Thank you.
01:02:54.382 - 01:03:09.310, Speaker A: Bye. Thanks all. Thanks. Sa.
