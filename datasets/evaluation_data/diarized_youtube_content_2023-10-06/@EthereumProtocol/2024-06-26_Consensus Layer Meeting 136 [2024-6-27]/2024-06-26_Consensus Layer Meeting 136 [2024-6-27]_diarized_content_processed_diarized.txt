00:00:27.895 - 00:00:57.235, Speaker A: Sa. Sa.
00:01:35.105 - 00:01:56.789, Speaker B: Okay, we should be live and from here. Let me grab the agenda. There we go. It's there in the chat. So, yeah, everyone, this is. Oh, wow. My computer is not happy.
00:01:56.789 - 00:02:20.005, Speaker B: There we go. Sorry about that. Call 136 and yeah, there are some things to do first before we get into Electra, so let's just hop in. The first one, let's see, is Yoram here handle or. Yeah, maybe this is.
00:02:20.125 - 00:02:20.605, Speaker C: All right.
00:02:20.685 - 00:02:21.013, Speaker D: Yeah.
00:02:21.069 - 00:02:24.865, Speaker B: Hey, if you'd like to give us an update on your work, that'd be great.
00:02:25.605 - 00:02:29.105, Speaker A: Absolutely. Thank you. Is there any chance I can share my screen?
00:02:30.525 - 00:02:31.825, Speaker B: You should be able to.
00:02:32.125 - 00:02:49.155, Speaker A: Oh, I can, I can, Yeah. I see the. I see the. I see it now. Thank you. Sorry, let me do this differently. Okay.
00:02:49.155 - 00:02:55.395, Speaker A: Can everyone see my full screen?
00:02:56.935 - 00:02:58.075, Speaker B: I can, Yep.
00:02:58.695 - 00:03:14.793, Speaker A: Excellent. All right. Hello everyone. It's a great honor for us to be here. My name is Jorge Arce. I'm a blockchain cryptography researcher at Nevermind. And today we're going to be talking about our work allowing validators to provide client information privately.
00:03:14.793 - 00:03:52.567, Speaker A: This is a work supported by a grant by the Ethereum Foundation. First, a little bit of background that we're going to be assuming in the interest of time and because I'm sure the agenda is backed. Also looking at the. At the audience, we are going to be sort of like assuming the following points are familiar. If not. I would. If there's anything you would like a quick refresher on regarding the things I'm about to say, we are going to be posting a link in the chat to the full research deliverable, which goes into more detail into these things.
00:03:52.567 - 00:04:51.975, Speaker A: We're also going to have a QR code at the end for those watching this in the form of a recording or on YouTube. So we will assume you have heard about the current client diversity estimation methods, things like block print, ether nodes and surveys such as supermajority.info or so we assume you have some familiarity with their current limitations and weaknesses. Also, you might have heard of a proposal by ETF streamer, which I think is also on this call, by the way, to provide the necessary changes to the protocol to allow validators to post their client diversity data on the Graffiti field. This is a solution that has already been discussed by the community. It's like a lightweight approach to having us get more client diversity data. And so that one is also part of the background.
00:04:51.975 - 00:05:59.133, Speaker A: Cool. So in light of those things, especially if you have heard about them before and you have heard about some of their Limitations. It might come as not a surprise to you that we're interested in modifying the Ethereum protocol to allow validators to report this data in a way that is, let's say, more canonical, more direct, of course, without compromising the core functionality of the protocol, the performance of the protocol and so on. Moreover, we explored the possibility of making such mechanism anonymous, that is making it so that the data is shared in a way that is decoupled from the validator index. And if we're able to pull this off, can we maintain accuracy, security, privacy and performance? And how much? So this is what we're looking into today, the results of the, of the research and once again we'll be having the link to the deliverable in the, in the chat. We'll be sharing a QR code at the end for you to read this and more. I'm more carefully.
00:05:59.133 - 00:06:48.913, Speaker A: This is more of. If you're interested, this is more of a sneak peek into the main ideas. So for method one, client diversity data on the Graffiti field, where each block proposer writes an encoding of the el en combination on the graffiti field. By default, we're not going to discuss a lot about this here. We refer you to the relevant e research discussion. We will say that at least for the goals as we defined them before, one challenge that this method has is it becomes quite difficult to anonymize such graffiti field reports because they are coupled to the identity of the validator. And it's very hard to work around this using any kind of cryptographic building blocks because you just have very little size or very little space in the Graffiti field to work with.
00:06:48.913 - 00:07:38.735, Speaker A: So one of the first things we came to realize is that in order to use any of the, let's say, standard techniques for privacy or anonymity that would come to mind, you would need a channel where you can share data that allows for sharing for larger data blocks than just a Graffiti field. So how could that look like? Let's go into our second method. We're calling this share data through the gossip network plus use nullifiers for privacy. What we're going to do here, what we're going to propose here, is a generalized crawler method of sorts. So instead of using the request response domain of peer to peer, which is how crawlers currently work, they query notes and they get back information from them. We're looking into flipping that on its head and using Publish Subscribe. We'll see some advantages to this.
00:07:38.735 - 00:08:26.885, Speaker A: But how would that work? Like, how would that look like? And how do we make sure we're not just breaking the network by proposing something like that. Well, consider the following first iteration. Have validators encode their client or be able to encode their client diversity data into a client data object. Right now, for this first iteration, we won't consider privacy. We will add it in a few slides from now we will explain how the privacy comes into the picture and have the protocol periodically and randomly select a sample of validators that will be published in their client data. Much like how the protocol currently chooses a sample of validators periodically for sync media duties or for attestation aggregator duties. Let's have the validators in the sample gossip this client diversity data on a dedicated client diversity topic through gossip sub.
00:08:26.885 - 00:09:38.005, Speaker A: If they happen to not be subscribed to the topic at the time, they can use the fan of mechanism to share the data. And so for the notes that are subscribed to the client diversity topic, they can gather these reports, they can verify them independently, aggregate them and publish them on their own. Similar to how crawlers work nowadays, in the sense that every single crawler can provide sort of like their own dashboard, their own visualization of the results, and anyone else can also play the role of a crawler and verify the accuracy of this information. So the object that we're looking to share, this client data container, so it would have the slot number the encoded client data. Let's say we're encoding the client data in 32 bytes, similar to the client data on the Graffiti field proposal. We're going to have a signature for the validator in order to see whether it actually belonged to the sample at the time or not. And the validator index, if we do it like this, the container size is roughly 144 bytes, which is about 50% of a current Ethereum attestation.
00:09:38.005 - 00:10:44.966, Speaker A: So by itself it's not a terribly large data object that it's being sent, but we need to think about how many of those are flying around the network at a given time. And we definitely want to use samples to avoid network overload. We don't want to have the entire validator set be sending these objects at a time. We want to be working with samples now, how large should that sample be? And this is a general observation that applies to, we made early in the research and hope it would apply to all the methods for assessing client diversity. So if you consider, in order to consider the statistical significance behind this, if you use the standard statistical theory for surveys like this is essentially what we're running a survey on the validators we Find out that with with a confidence level of 95%, which is standard to these kinds of applications, the following error rates are going to hold our error margins. If you choose a 9.6K validator sample, then your estimations of client diversity are expected within a 95% confidence level to have up to a 1% error rate.
00:10:44.966 - 00:11:24.755, Speaker A: If you multiply that by 4, I.e. if you run with a 38K validator sample, then the error rate goes down to 0.5%, which is not too shabby. Now if we, let's say gather one client data object per slot and following in the footsteps of the client diversity on the Graffiti field method, or you could rearrange it to be 32 per epoch, like sort of like batches like this, to allow the clients or the validators time to prepare the data, send it, so on and so forth. Then you would be reaching these targets for sample size in 1.33 and 5.33 days respectively.
00:11:24.755 - 00:12:14.939, Speaker A: So this might be a bit counterintuitive until you round the numbers, but thankfully the you can get to statistical significance pretty quickly and you don't need your samples to be huge to do that. Further supporting the point that we can do this without like imposing unreasonable demands on the peer to peer network. Now let's talk about the privacy behind this all which we said we were going to leave for later. Let's go into how we're going to use nullifiers for privacy. And as a general observation, if we want to make our data client diversity data private, you can go about it in one of two ways. You could hide the client data itself without touching the identity of the validator in the let's say client data object. Or you could hide the identity but keep the client data itself public.
00:12:14.939 - 00:13:20.225, Speaker A: If you do any of these two things, you would be able to break this connection right between identity and the data that is being shared. The thing is, to do this, to follow either of those two approaches, you're going to be looking at different cryptographic primitives. So if you want to hide the client data, you will require some kind of homomorphic encryption scheme, such as the one used by private voting protocols that are known. However, using homomorphic encryption requires a trusted Decryption authority or Trusted Encryption committee for that matter. The appeal behind going about it the second way, which is hiding the identity. Instead of a plain BLS signature that you use to identify the validator, you can use a zero knowledge proof of the validator's identity plus a nullifier to prevent double counting of these client data reports. If you choose the latter, you would have a zero knowledge proof where you're essentially showing that the validator belongs to the sample that was chosen by the protocol without revealing the identity of the validator itself.
00:13:20.225 - 00:14:45.101, Speaker A: And the sample can also be kept the sample itself can also be kept private and not just be a list of validators. Similar to how happens with attestation aggregators nowadays. This anonymous client data object would be slightly bigger because of the zero knowledge proof would be roughly the size of an ethereum attestation, like 296 bytes if we're assuming a growth 16 proof. Still, given the number of attestations of these anonymous client data objects that would be flying around the peer to peer network at a time, we think that the requirements on the network would be minimal. Now to disclose a challenge about using this technique, it has similar weaknesses to those of many privacy blockchain privacy cryptocurrency projects in the sense that if a sufficiently motivated attacker is listening to the peer to peer network and conducting traffic correlation analysis, then they can use these heuristics to get IP addresses for the node and sort of like triangulate and match a validator index to the client data. If you want to address this at the protocol design layer, we would require further research into mixed approaches or specialized routing strategies such as Dandelion. We know such initiatives have been undertaken before and haven't discussed any research and such.
00:14:45.101 - 00:15:33.675, Speaker A: Some have also been adopted by other privacy cryptocurrency projects, but going too deeply into this direction would be out of the scope of the current project. This is more of a follow up that we'd be interested in looking into. Also, I think it begs the question of whether we expect these kinds of attacks to happen in the first place for client diversity data. That's a point where we would actually be very interested in hearing what the community has to say. Like how much of a concern would that be for method 3? Dedicated voting scheme for client Data collection Let's take the other road in the crossroads. I presented you guys some slides ago. Instead of hiding the identity, let's hide the client data itself.
00:15:33.675 - 00:16:12.795, Speaker A: And think about this as a generalization or a stronger alternative to the survey approach. You're going to be leveraging a blockchain to publish votes in a censorship resistant and publicly auditable fashion. You're going to be needing a homomorphic encryption scheme and a committee of trusted encryption authorities for decentralization. And you can choose these building blocks in a variety of ways. An example that could be pretty lightweight and Leveraging existing infrastructure and design choices would be, well, we need a blockchain that is low cost in order to be publishing these votes. We don't want people to be paying gas for publishing these votes. So a proposal would be using a proof of authority blockchain for dissent.
00:16:12.795 - 00:17:34.885, Speaker A: Ideally something already existing like Cholesky testnet could be a good solution here. And as for a committee of decryption authorities that would be in charge of gathering, aggregating and publishing the end result, we're going to need some parties that are trusted by the community at larger by the protocol. So we could propose, for example to have the Ethereum client teams themselves running nodes for description authorities with the understanding that this design would require collusion of super majority of these encryption authorities in order for any data to be privately leaked. Now here's a rough diagram of the process. We're still going to be using peer to peer for validators to publish this data to a dedicated gossip subtopic. The reason for that is more direct alternatives such as like using an RPC channel to send the data to the decryption authorities we think are not acceptable because they could lead to IP leaking of the validators. So if we have the validators gossip their data through the peer to peer network as before, the encryption authorities gather these votes and publish them and do the whole aggregation and encryption decryption or decryption process on a voting smart contract on a low cost blockchain, where here we propose the example of Cholesky testnet for that.
00:17:34.885 - 00:18:26.451, Speaker A: Now the object that you're going to be dealing with as it turns out, is going to be a lot heavier than the one we were working with before. Because at least on a naive implementation, let's consider that first you would require a homomorphic encryption ciphertext per each of the different client choices that the validator can make. Each of the different execution client software choices, each of the different consensus client software choices. If you were to do it like that, and you imagine you have 20 client choices in total, then the ciphertext would add up to 2 kilobytes, the voting data would add up to 2.4 kilobytes. However, we can use compression. We can fit up to three plain client data objects into one cipher text, such that when you add up the votes, when you do the voting aggregation, you don't run into out of bounds or overflow issues.
00:18:26.451 - 00:19:31.899, Speaker A: If you do it like this, then this brings the data needs down, let's say to roughly the data objects are going to be 1000 bytes, 1 kilobyte and the smart contract itself would be looking at storing something like 10 megabytes of storage. We're running out of time. Unfortunately. For our comparative analysis of the solution with regards to privacy, trust assumptions and adding complexity, I will refer you guys to the full report where we attempted to sort of like rank the considerations we have just made in terms of severity or in terms of how easy they could be addressed with further work. So for example, for the dedicated voting protocol, there's no way we can see to remove the trust assumptions. You're always going to be needing a trusted party to take care of the aggregation and decryption, which is why it's highlighted in red. And as a, let's say, conclusion, we can see a trade off between the highest degree of privacy guarantees in which not even a party that's listening to a peer to peer network can de anonymize the client data.
00:19:31.899 - 00:20:20.575, Speaker A: This is the case of the dedicated voting scheme, but it requires the trust assumptions on a decryption community. So there's this trade off, sort of like between the highest possible privacy guarantees and the need for trusted parties to come in. Okay, so let's wrap it up. Here's the QR code leading to the ETH Research link where you guys are more than welcome to post any comments, concerns, observations about what we have discussed working on this project. We have Ahmed Bitar, and in no particular order, we have Ahmed Vitar, an Ethereum core developer who is here in the call as well, and Ahmed Ramazan, blockchain, cryptography researcher and yours truly. So that would be it. Thank you very much for your time and attention.
00:20:20.575 - 00:20:34.315, Speaker A: We are Nevermind Research. Not sure we have time for any questions or comments. If we don't, we can bring it over to Eat Research where we would be very happy to continue the discussion if. Yes, we're more than welcome to talk about those.
00:20:34.945 - 00:20:35.805, Speaker D: Thank you.
00:20:36.905 - 00:20:43.645, Speaker B: Great, thank you. Do you mind dropping a link to the E3 search post in the Zoom chat?
00:20:45.625 - 00:20:49.049, Speaker A: Yes. Oh, I'm very sorry, I thought it was already dropped.
00:20:49.097 - 00:20:53.765, Speaker B: I might have missed it, but either way, just a place to continue the conversation.
00:20:54.305 - 00:20:58.445, Speaker A: Absolutely. Now let me, let me do it again. Or maybe Ahmed can. Oh, there you go.
00:20:59.955 - 00:21:01.891, Speaker B: Yeah, perfect.
00:21:01.963 - 00:21:02.187, Speaker D: Yeah.
00:21:02.211 - 00:21:17.735, Speaker B: No, thank you. That's very interesting research. Yeah, there were some questions in the chat, but yeah, maybe just looking at the time, we'll move on to the next agenda. Unless there are any comments right now.
00:21:27.725 - 00:21:32.465, Speaker A: We'Ll look at the chat and we can continue the conversation on E3 Research. Thank you everyone.
00:21:32.965 - 00:21:44.505, Speaker B: Sounds great. Thanks Next up, Peter has an update he would like to give us. I think Peter's on the call.
00:21:46.605 - 00:21:47.397, Speaker A: Yes.
00:21:47.581 - 00:22:50.625, Speaker D: Great. Hey, so I don't really want to drag this out. So basically the reason I kind of posted my proposal and the summary on the chat is that you guys are aware of it. It's something that we've worked on. But so just to do a very, very tldr, we've been on the guest team, we've been investigating a possibility of creating execution layer witnesses and using them to cross validate executions. Basically what we are trying to figure out is whether it would be feasible, possible, et cetera, to have one client run an Ethereum block, create a witness out of it, and then have other clients cross validate the execution. So the idea, my original idea where I started from was this client diversity issue.
00:22:50.625 - 00:23:40.855, Speaker D: Not necessarily the client diversity, rather the slashing conditions. That way back in, I think end of last year, Geth had a much larger market share and there were these really loud horror stories that if Geth were to do something bad, meaning have a consensus fault, then it could have dire consequences. Which are all true. And back then what I was thinking about is how could we solve this issue without necessarily forcing everybody to switch away to some other client. And this isn't basically my problem isn't necessarily people switching away from Geth. Rather this whole client landscape is pretty dynamic. I mean, we have clients, for example, REF is the hotshot upcoming new client.
00:23:40.855 - 00:24:55.731, Speaker D: And. But people have, I kind of feel that people have weird incentives of choosing one client over the other client. For example, people might want to run rest. However, since it's new you, the risk of a consensus fault is higher. So depending on if you're a validator or you want to run it in production, it's like, yeah, are you willing to foot that risk or not? What happened if basically you run an archive node breath and it just goes off consensus and you act upon bad data? Or there are a lot of different scenarios, or if you're a validator and all of a sudden you start attesting and validating bad blocks, et cetera, whether it's Geth Rest or whatever other client. And basically what I, what the idea was that obviously all such instances require a full node which runs the consensus, runs everything. But what if we were to actually also create the witness while running a block and have other execution clients just statelessly validated? Because in theory, our hunch was that it should be very, very cheap.
00:24:55.731 - 00:26:05.691, Speaker D: So once one of your nodes runs a block and produces a witness it should be relatively cheap for to validate it with another client or basically validate it with all the other clients. And the document that I posted is basically a summary of our basically what we arrived at specifically that we've implemented this whole witness creation in Geth which is fairly optimized. I can tell you a number that it's about a 20% performance hit the block import. So if your block import is about 100 milliseconds then aggregate creating the witnesses maybe another 20 milliseconds extra. So I think that's very, very cheap. And that's kind of the only component that we have optimized heavily because it's something we need elsewhere too. But we've also implemented running these witnesses through Gethsemane and basically having state having code in get a statelessly validate the witness and that code actually runs the exact same code that a live validation runs.
00:26:05.691 - 00:27:02.515, Speaker D: So there's no special black magic to have witnesses run specially. So basically we can guarantee that it's the same code. And that part is currently basically takes the same amount of time. So running a witness takes maybe again 120 milliseconds. That is not something that we have optimized. The reason why it matters is because it's the witness is a fairly dumb format and so it's a lot less optimal than what GET has at its basically what GET can do either way, what we've what the document kind of details on the EL side is how to create these witnesses and that it is actually feasible to create these witnesses fast and that it is feasible to run these witnesses relatively fast. We haven't optimized it.
00:27:02.515 - 00:28:30.257, Speaker D: I'm hoping we can make it even faster. And then the question is that if I ask can create witnesses and EAs can statelessly verify witnesses, then what can we do with them? And the first thing that came to my mind, and basically that was my starting point, is to allow multiple EAs to cross validate each other for a validator or for a production node. And the question is then where do we integrate this and I mean this infrastructure, where do we take our witness and give it to other clients to cross validate? And there's a lot of possibility for custom infrastructure and pluggability everywhere. But one place that kind of looked interesting to me is to move these whole witnesses and cross validation into the engine API. The reason I kind of thought about that is because what the engine API currently does is that you have the payload method which basically gives EL a block and EL says whether it's correct or not. So we could extend that method to also return a witness, not just the response. And then you have the fork choice Update which asks geth2 or el to create a new block in case you're the next validator, sorry, that you're the next block producer there.
00:28:30.257 - 00:29:29.581, Speaker D: You can easily also add a flag saying that, well, get me the next block plus its witness. So it kind of naturally fits in creating these witnesses really elegantly fits into how the engine API works currently. And then you would have one more method on the engine API which kind of would be similar to new payload, but instead of actually integrating that payload locally, it would be like, I don't know, stateless payload, where you just get the payload and the witness run it and we can reply whether it is valid or invalid without actually doing any modification. Anyway, the tldr, you have the details written both in my doc and on the GitHub issue too. So I don't really want to go into very, very much detail. But the idea was that the engine API seemed very, very elegant for two reasons. One of them is that this way we don't need to invent new infrastructure.
00:29:29.581 - 00:30:53.365, Speaker D: So we have already had the communication between EL and CLS done. CLS, many CLS support talking to multiple ELs at the same time. So it would be fairly trivial to just extend it so that the CL sen uses one execution layer client as the main driver and uses others as the verification ones. So it's fairly simple. So to do the cross validation logic and the other thing that this kind of allows us to do is a very, very big step towards stateless Ethereum. Because if we think in a Verkal world where you have Verkal trees and some block producer generates not just the Verkal block but also the Verkal witness, then you would have to use again a similar API where the block producer needs to somehow give the witness, whether it's empty or vertical witness to the cl, the CL needs to propagate it in the network and then on the other side, when the CL gets the block and the work co witness and it would need to deliver those down to the EL to verify. So the infrastructure proposed for modifying the engine, I mean my proposal for the engine API would be useful, I would say probably 100% as a stepping stone towards the stateless Ethereum.
00:30:53.365 - 00:31:59.773, Speaker D: So that's why I was kind of proposing this change, to see how much against you guys are on modifying the engine API. My proposal that I wrote up added new methods to the engine API Simply because that way we can actually merge this code into geth and anyone can play with it without touching the production endpoints so that we don't mess with anybody's code. And then any consensus layer client can actually try it out with get and see if it makes sense or not. If it makes sense, then we can think about a more proper integration. One thing that we did notice is that the engine API is encoding currently is JSON and we have huge hex blobs in it, which for the witnesses. The problem is that witnesses are quite large and the JSON encoding and hex encoding is slow. This doesn't necessarily relate only to witnesses.
00:31:59.773 - 00:32:49.485, Speaker D: So the same issue happens for blocks too when you transfer the blobs. Currently we have one megabyte worth of blobs. If we want to raise the number of blobs available, then that number goes up and eventually the blobs themselves will hit latency issues due to JSON and hex encoding. So one thing that in my proposal is not covered, but we should think about is also, and that's independent of my proposal. So whether my proposal exists, accepted or not. I do think another issue is we should change the the ink, the transfer protocol for the engine API from JSON to probably SSE or anything binary really, I don't care. And that would probably completely eliminate the encoding latency for the engine API, by the way.
00:32:49.485 - 00:33:28.355, Speaker D: So basically that was just a really rough rundown of what we've worked on. It's actually functional in G. That's why I brought it up now. The question now is whether there's an appetite to have it included in one way or another. As for the performance numbers, as I said, witness creation is kind of final ish, as in we probably can't really make it a lot faster, but propagate. I mean encoding, decoding the witnesses on the engine API and witness verification will probably. I'm hoping we can make that fast.
00:33:28.355 - 00:33:50.455, Speaker D: So while looking at the numbers, do consider them in that respect too. Either way. So that's. You'll find a lot more data and numbers and everything in my documentation. It's just an overarching overview of this thing. Yeah, so that was a brain. I didn't really prepare too much for it, so hope it was kind of understandable.
00:33:52.835 - 00:33:59.845, Speaker B: Yeah, thanks. No, it's super cool and I like how it lays the groundwork for stateless clients. Guillaume, you had your hand up.
00:34:02.905 - 00:34:27.365, Speaker E: Yeah, I had a question because there's been a spec for witnesses in using the execution payload. Like that's Been around for two years. I don't see an upgrade path from this system to this. And so that was my first question. And the second question is, wait, so.
00:34:27.705 - 00:34:50.675, Speaker D: The Witness format in this pack is completely opaque. So I mean, it's just a binary blob of data. Yeah. Understand that you don't care about it. So changing one binary blob of data when running in MPT to Verko blobs when running, when passing the part for point, I mean that should be fine.
00:34:52.215 - 00:35:17.161, Speaker E: Yeah, but that's not what I was saying. Yes, there's indeed the format type, but there's also the way it's passed. There are some assumptions in Verkal that are meant that like the block should be executed, should be received at the exact same time as the. As the Witness. From what I see, this is. This is more off band and I don't see think it can. It's future proof of work is what I'm saying.
00:35:17.273 - 00:35:18.685, Speaker D: What is off band?
00:35:19.985 - 00:35:21.689, Speaker E: You have several calls, right?
00:35:21.777 - 00:35:23.353, Speaker D: So no, you could.
00:35:23.529 - 00:35:25.445, Speaker E: Well, you define three calls.
00:35:26.265 - 00:35:53.723, Speaker D: Well, yes. One of them is when you produce a block. Then you get the block and the witness together back when you currently in MPT world. Yes. So the block production logic would be exactly the same for MPT or Verkal is the same thing. Just give me a block and you get the block back and the witness. So that's covered on the other side in Verkal.
00:35:53.723 - 00:36:08.269, Speaker D: You can if you want. I mean, I define this. I know. Execute stateless something something. If you have a stateless client, then that is what you would use. And that's all. And that stays the same thing.
00:36:08.269 - 00:36:34.631, Speaker D: Whether you verify MPT statelessly or verify Verkal statelessly. That's still the same thing. I give you the block and the witness and you run it. Done. The third is just the new payload with Witness. Which is the way. Well, if you have a full node where you want your own node to create the witness that you can cross validate with other client, that might be something that won't be needed for Verkl.
00:36:34.631 - 00:36:42.275, Speaker D: That's the cross validation thing that we can use today. But you would just delete that.
00:36:44.095 - 00:36:58.415, Speaker E: But wave. So from what I understand, you're not forced to produce that witness during block production. You could use the old call instead of like producing a block with the witness.
00:37:02.035 - 00:37:36.027, Speaker D: Well, yeah, the idea is that if you want to do cross validation currently, then you would make a witness. If you don't want to do that, you won't make a witness because it's Pointless to dump that on your note. So it just gives you an option. Maybe in Verkal if you always have to do you always need a witness? Then in Verkol you would just say that while you will always make a witness. So I don't think this is the methods. I think the methods will mostly stay the same thing. I don't really see them changing at all.
00:37:36.027 - 00:37:45.255, Speaker D: Just maybe before Verkal it makes sense to call one method and after work that method just loses its point. But otherwise it's the same thing.
00:37:49.875 - 00:38:03.025, Speaker E: Yeah, okay, I guess I have to dig a bit deeper, but yeah, I. In any case it will require a lot of changes compared to the actual implementation of urql but okay, thanks.
00:38:11.845 - 00:38:55.703, Speaker D: I guess. All in all here the question is whether there's anybody seeing anything super obviously wrong with trying to get it through the Engine API and the way the Engine API proposed methods look like. Definitely we can go with a starting point where the methods are separate the way I suggested just so that we can merge it in and we can iterate on it and then if it turns out to be valuable we can figure out a tighter integration. If not, then not. As for the performance issues raised and the sizes, those are valid. That's. We need to look into how we can optimize them.
00:38:55.703 - 00:39:20.649, Speaker D: I think it should. Yes, we need to do. We need to see what the worst case would be and how expensive that would be. Yes, it's definitely something that needs a bit of work both in geth itself to make the missing pieces fast and. But still the Engine API should be. Should be turned into a binary one. That's.
00:39:20.649 - 00:39:25.005, Speaker D: I would say that's mandatory long term whether this thing gets it or not.
00:39:28.185 - 00:39:29.925, Speaker B: Mikhail, you had your hand up.
00:39:30.705 - 00:40:14.239, Speaker C: Yeah. Just a quick comment on how the Engine API spec could be handled. We used to have an experimental folder in the spec and usually which is if we need to extend. If you. If it is supposed to be an extension of say new payload method, then just the spec can be created in this experimental folder and the version of this method could be some arbitrarily high number. So not to clash with the ongoing work on the current hard work. Yeah and this probably if it's used better.
00:40:14.239 - 00:40:50.775, Speaker C: So you can just by leveraging this approach just extend the existing methods and experimentally implement those in different clients and after this scope for any hard work we can move this back to. To the hard work spec. So that's one of the ways to handle this on the SPAC level. But yeah, we can iterate on this in the PR that line has created.
00:40:51.915 - 00:41:14.805, Speaker D: So just to add a quick memo, none of this requires a hard fork. So basically adding at least the way I designed it with a couple of new methods. Those methods can be added to ELS whenever and they can be used by CLS whenever and there's no synchronicity requirement across them.
00:41:15.545 - 00:41:22.365, Speaker C: Yeah, I see. Yeah, if. Yeah, okay, let's just discuss it.
00:41:23.145 - 00:41:24.085, Speaker D: Yeah, sure.
00:41:28.225 - 00:42:46.477, Speaker F: My main concern is prioritization because Prague we already made it very big and there's a vertical coming that would kind of make this partly obsolete. So the part that we're definitely worth investing is the CL logic of supporting working with multiple clients in this way. And the other part is definitely working on removing JSON from as a layer of transportation engine API. Right. To move to something faster. But other than that, I kind of have a concern that until this is implemented in enough clients and have good enough tooling for people to actually run it, and there's also concern of increased latency that would it be acceptable on average machine, which is not obvious, then we will be already shipping Prague and focusing on Verkal, which should be our priority in my opinion. Bigger priority because it comes like this Setup can be cheaper on Verkel because you don't need a leading client.
00:42:46.477 - 00:43:21.625, Speaker F: You can do cross verification in parallel, which basically you avoid the additional latency. You still have to wait for the slowest client, but at least it's not serialized waiting on two clients. So that's my question. Is it important or urgent enough to focus on it as beyond parts of it? This is a stopgap solution that will be obsolete, and we know it will be obsolete, so that's the question.
00:43:22.565 - 00:44:11.197, Speaker D: So with the obsoleteness I kind of both agree and disagree. So the reason I kind of tried to spec it to fit into the engine API was specifically to make as little part of it as possible obsolete. So basically updating engine API to binary will be needed anyway. The updates to the engine API to be able to pass witnesses back and forth will be needed anyway. Clients supporting running stateless execution and gathering witnesses will be needed anyway. The only thing that would be different is what the content of those witnesses are. But you still need to be able to gather try nodes.
00:44:11.197 - 00:44:35.905, Speaker D: Whether they are Verkhol mpts doesn't matter. And the other thing is you still need to be able to execute a block based on a soup of trie nodes. But Verkal or mpt, it doesn't matter. So I'm not so sure that there's relevant amount of Data that's going to get thrown out or work. It's.
00:44:37.205 - 00:44:48.327, Speaker E: Sorry, the super trinode part. Do you assume the same format of mpt, like the same storage format for MPT and Verbo? Because we had this conversation.
00:44:48.511 - 00:45:17.195, Speaker D: No, no, it doesn't. No. Basically what I'm saying is that basically you just have a partial subset of the try. So in MPT2 you will. An MPT witness will contain, I don't know, a thousand try nodes in no particular order because MPT doesn't require it. For Verkal, if Verkal does require it, then you would have it in some specific structured format. But it's still just a bunch of try nodes that the witness contains.
00:45:17.195 - 00:45:28.395, Speaker D: So it's. You still need to, while executing the block, you still need to gather that and put it into the witness. So I don't really see that much difference.
00:45:33.415 - 00:45:34.355, Speaker B: Aman.
00:45:35.695 - 00:47:01.341, Speaker G: Yeah, so I have like two points here. I want to say that first in this approach that Peter's suggesting, you provide a false sense of security to the network because like in the case that a DOS attack is done on a majority client, then the liveness of the network is still affected in a. In a severe way. Of course my assumption was that we introduced multiple clients just to guarantee the liveness of the network. So this basically goes against the whole principle of multi client network because it just deletes it. Whereas a solution like utilizing a multiplexer, something similar to voucher or something like that, would actually guarantee that liveness without stateless execution. And if this is aimed for smaller operators or solo operators, then in the chat we already like kind of devised that the latency from this will render this very not profitable for the solosticker.
00:47:01.341 - 00:47:29.085, Speaker G: Whereas large operators can easily utilize something like a multiplexer to run multiple clients concurrently without enduring the latency issues. So another thing is that when Burkle is coming up, like Lukasz said, I don't see why we would invest time into something like this at the current stage.
00:47:32.345 - 00:48:06.045, Speaker D: So thanks for those to answer to try to answer them. So one of them was the dos. So the DOS is relevant. But in my opinion Ethereum network being DOS and losing liveness is the smaller of the problems. Consensus faults are much bigger problems. That's where the nasty stuff happens. So the fact that Ethereum doesn't finalize for half an hour or an hour is a lot less of an issue than basically having two chains.
00:48:06.045 - 00:49:18.207, Speaker D: So for sure if it does affect a bit anyway, period. So that was the DOS issue, the latency issue, as I Kind of mentioned the problem is that basically the engine API is basically most of the latency currently in this whole cross validation is the engine API encoding. So passing the witness back and forth to geth will consume 200 milliseconds of latency just to pass the thing. Basically it passing the witness through the engine API back and forth takes more time than running the block twice. So it's like as long as that's not fixed, it's kind of unfair to say that the latency is too high because this is not the fault, this is not the reason why the latency is high. The engine API is the reason. And with regard to small versus large validators, yes, I kind of agree that if you are a large operator, then you are expected to run arbitrarily many nodes.
00:49:18.207 - 00:50:10.895, Speaker D: But if you are a smaller operator, I think being able to run one node and whatever that is your preference and cross validate with the other kind of lets people choose nodes that they like. For example, I can easily run a RET archive node and know that even though it's a bit problematic because it hasn't been battle tested, if there's another mine node cross validating it, you have much higher guarantees. Whereas if I'm a home staker, maybe I will just be too afraid to run a ref node because. Yeah, what happens if it goes bad? Yes, Kim, yeah, that's.
00:50:11.475 - 00:50:29.193, Speaker E: I mean I'm changing topics a bit, but not completely. In case the engine API doesn't like this proposal does not get accepted to go through the engine API, is there a backup solution to propagate the witnesses somehow? Like let's say only, only guess wants to do it.
00:50:29.369 - 00:50:30.325, Speaker A: Would you.
00:50:30.625 - 00:50:34.085, Speaker E: Do you have a backup plan to propagate those witnesses?
00:50:37.105 - 00:51:13.805, Speaker D: Not that I would prefer. So basically the this whole thing, just if other ELs decide that they want don't want to do it then. I mean geth playing by itself doesn't really make much sense. However, being able to run with stateless and generate witnesses does have a lot of potential use elsewhere too. So it's, I mean it's not really thrown out work. As for creating another channel outside of the Engine API, that one kind of really feels like wasted effort. So I probably wouldn't do that.
00:51:18.075 - 00:51:38.455, Speaker F: So my suggestion would be let's start with changing NG API to binary format and see when we deliver it. See where is Verkle if it's just around the corner or further away and I think just doing that will take us quite a long time. So that's a problem for Me.
00:51:43.465 - 00:52:26.733, Speaker D: I mean, I completely agree that we can definitely focus on that. On that part. I don't know. Honestly, converting the engine API to SSD seems like somebody has to go in and define basically just annotate the those five data structures with the SSD magic numbers that are needed there, the max sizes and whatnot. But after that's done, is there a lot of extra work? I don't know, I'm just questioning it. Basically I'm just saying that I don't think that if there is a legitimate effort to make it happen, I don't think it should be that complicated.
00:52:26.869 - 00:53:09.001, Speaker H: Trademark Last time that SSC like something simple there was proposed to align the withdrawal routes in Capella there was like more the problem that people wanted to go farther than that. So with ssc the problem is that if we are changing the Engine API to ssc, don't we just also want to transform DEL block headers to SSC so that we can just send them natively? Like do we want this intermediate step? I guess on engine it's less of a problem because it doesn't go on chain. But yeah, that's one of the aspects. Like you can choose how far you want to go there.
00:53:09.193 - 00:53:53.135, Speaker D: No, don't open that kind of forms. Basically keep everything as is currently. Just replace the JSON with SSE anything else. Basically that's why the previous effort to get SSD into EL got shut down because basically it just blew up into less SSD this and that and that and that and then it just became a monstrosity monstrous effort. So my $0.02 would be just replace the JSON itself and the binary encodings themselves with SSE and leave the way the headers and everything are formatted the way they are currently changing headers to sse. That's kind of a consensus change that gets complicated.
00:53:55.435 - 00:54:27.145, Speaker G: One suggestion is to go gradually with something like Message Pack because actually The JSON RPC 2.0 spec actually supports Message Pack and Message Pack actually sends all bytes as row without Hicks or base 64 encoding. That adds overhead. So I don't know if there is any reason why not to go with something like Message Pack which already has libraries to cover all programming languages.
00:54:29.255 - 00:55:28.335, Speaker D: I just don't see the reason why that's a good idea. It's probably a bit simpler than sse, but long term, I mean we want els to speak SSE long term. If we want to switch, if we want to move more EL consensus types to sse, we need to leak SSE somehow into ELSA and This seems like doing the Engine API seems like a very trivial thing because it's a very simple. Just a few things and then that's a nice gateway drug to have EL start speaking it. Whereas if we go with Message Pack, I mean, you still need to add Message Pack and all the encodings and whatnot to els. So SSE or Message Pack doesn't really lower the effort, but it doesn't get you further. It doesn't get you further to having consensus SSC stuff.
00:55:33.475 - 00:55:42.535, Speaker B: Cool, thanks for that, Peter. Is there a place people should go to continue the conversation? I see you linked to gist from the GitHub comment.
00:55:42.692 - 00:56:16.985, Speaker D: So I would probably, in that GitHub comment, there's the proposal and I would probably go there as a starting point or if somebody wants to do a more live discussion, then we can maybe discuss one aspect or the other on Discord or wherever. So probably to just do a brain dump of ideas. I would dump it below the proposal just so that it's kind of there and to go back and forth wherever you guys prefer.
00:56:20.005 - 00:56:23.905, Speaker B: Okay, yeah, Discord sounds like a good place for that. Great, thank you.
00:56:24.725 - 00:56:25.585, Speaker D: Thank you.
00:56:27.765 - 00:57:20.429, Speaker B: Okay, so next up we will turn to Electra and the main thing here is DevNet1. I believe it's targeting Alpha 3 of the specs which has been cut and yeah, I guess I'll just open it up. Is there anything anyone would like to discuss with respect to DevNet1? I think people mainly are heads down implementing the spec. Is there anything anyone would like to discuss in particular? Otherwise we'll move to a few other updates for future devnets. Okay. Perry has a comment that they're waiting on EL readiness, so.
00:57:20.517 - 00:57:21.145, Speaker D: Great.
00:57:22.165 - 00:57:44.775, Speaker B: I assume that means everything's progressing smoothly. Okay. Mikael had a pr. I believe this was to follow up from some changes to EIP 6110 following the work at Interop. Mikhail, do you want to give a few words about this?
00:57:45.715 - 00:58:24.705, Speaker C: Yeah, thanks, Alex. Yes, so this is the follow up to the breakout session we had during the Interop. And yeah, this PR basically implements the logic we discussed there. And yeah, just, just go quickly through the mechanics. So the Eth1 bridge deposits remain the same, so they are processed instantly. And if there is there it created instantly. And yeah, the balance is queued to be processed later and to be passed through the activation churn.
00:58:24.705 - 00:59:33.835, Speaker C: On the opposite, the deposit requests, which are new deposit, which are new way to actually deliver deposits to the consensus layer, they are added to the pending deposit sku. They are not processed anyhow and then this queue is being processed on the processing. This actually done for the main reason was to rate limit the deposit processing during the epoch. So we can put a limit which list PR does there is the max band and deposits break processing. And yeah, if number of deposits exceed this limit it just, you know, breaks aborts and yeah, the rest will be processed during the epoch transition. It also ensures the logic implemented in this PR ensures that it won't breach deposits will be processed before deposit request. So no withdrawal credentials from Tran attack as possible.
00:59:33.835 - 01:00:37.345, Speaker C: So there is a strict order of new validators that are created. And yeah, it also waits for deposit request position to be finalized in the queue. As we discussed, this would be easy to implement if we already have a queue. And yeah, to prevent some complexity and potential bugs around managing the PUB key cache in the implementations. So what else? Yeah, refactors do the refactor of the process pending deposits method because it gets more complicated and this refactor actually adds one computational inefficiency. The lookup of a validator index in the validator set is happening twice for each deposit. This is done for SPAC readability to improve the spec readability.
01:00:37.345 - 01:02:02.703, Speaker C: But yeah, if it is going to be a concern we can do something about it and do it once and do some workaround. But I just thought that probably all clients will use the cache to do this lookup so it will be really cheap. Also the other thing to probably think about is whether we need max lending deposits per epoch processing to be even to even exist because we have a natural limitation which is the churn limit. Yeah and there is the potential worst case scenario attack that is described in this in the description to pr. So just if you want to take a look, please take a look at it. And yeah, probably it would be great to not just introduce another constraint and leverage on the churn limit. Yeah, that's basically it about the logic that this PR introduces and it would be great to get as much feedback till the next week probably because I'd like to start working on tests and if anyone wants to help with the write and test this functionality, this new stuff.
01:02:02.703 - 01:02:05.795, Speaker C: So yeah, please welcome. Yeah, that's basically it.
01:02:09.855 - 01:02:53.295, Speaker B: Cool, thanks. Okay, we'll move on to the next agenda item then. Thank you for that Mikael. So right, yeah, if anything we will time box this but I was going to ask if anyone wanted to review the Blob base G spike that I believe happened last week. I think it led to some turbulence with roll ups and kind of touched many layers of the stack. Not sure if there's anything we want to discuss now. If not, we'll handle that in other places.
01:02:53.295 - 01:03:30.625, Speaker B: Okay, sounds like no. In that case let's move to peerdos. So I guess first off, I don't know if anyone here was on the breakout. I'm not sure if there are any updates from that call that are worth sharing with the group. Here.
01:03:34.645 - 01:03:36.789, Speaker D: We have Peer Desk, devnet one.
01:03:36.917 - 01:03:39.985, Speaker H: Online now with three different SIA clients.
01:03:41.735 - 01:03:46.515, Speaker B: Okay, awesome. Does the network look healthy?
01:03:48.255 - 01:03:49.195, Speaker H: Sort of.
01:03:50.615 - 01:03:58.495, Speaker D: We had some blob spamming and some technodes were unable to produce blocks.
01:03:58.615 - 01:04:04.635, Speaker H: All the blocks that they produced were orphaned. So I think they are already taking a look.
01:04:09.785 - 01:04:53.885, Speaker B: Okay, great. There was a comment to look at essentially these two PRs. Let's see, let me grab the link. So essentially this was to uncouple how we communicate the blob limit between the EL and cl. There was a PR that I opened that would basically have the CL drive the max BOB count for example over the engine API. I think one open item there was to consider including the max BOB count in the EL header to facilitate optimistic sync. I haven't added that to that PR yet or added it anywhere else, but that was an open item that I think we kind of agreed we needed.
01:04:53.885 - 01:05:32.735, Speaker B: Another thing was this PR3813 to the consensus specs repo. I don't know if Dencrad's on the call, I don't see him. But essentially this was going to go even a step further and take the essentially the base fee calculation away from the EL and have it handled all in the cl. I think this encapsulates things quite nicely. Yeah, so I guess for now I'll just call those two out. I think ultimately we want to do something like this for Petra. So yeah, please take a look if you haven't.
01:05:32.735 - 01:05:36.975, Speaker B: And yeah, just consider this a call for review.
01:05:37.355 - 01:07:11.207, Speaker C: Mikael, I think that for both PRs whatever we do on the CL side in terms of that affects the EL block validation. We will have to to include this into the EL block because as you've said because of the optimistic sync because it's not always the case where every where a payload is sent by the CL VI a new payload. So yeah will need to be self contained in terms of validating the block. And the other thing is that we have this target parameter as well and if we don't want to make this target parameter a product of max, like say target is half of a max, then we will need to include as well into the block header. And if it is the case then probably it is better to. In my opinion it would be better to have a couple of parameters like in the and cross PR having this base fee computed by the CL and validated by the CL and the basically Blob gas limit. One potential problem here is that EL will not be able to compute the base fee for Blob Gas.
01:07:11.207 - 01:07:31.769, Speaker C: And if this is required by EL like for you know, to filter out transactions in the mempool then it could be a problem. But Yale could use this parameter from the block. From the parent block of the block. It is. Yeah. From basically the block it has as the hat. So probably it's not that bad.
01:07:31.769 - 01:07:38.005, Speaker C: It's not a problem. Yeah, that's just my thoughts on those two PRs.
01:07:44.355 - 01:09:06.335, Speaker I: Yeah, my opinion is that solution one is more simple in terms of it is keeping, you know, it is keeping. Maintaining separation of concerns in the sense that CL is only telling what the blob limit is and the pricing is calculated by el. And now in future we can have for example a contract like 7702 which transactions can be sent and that can itself decide what the pricing is. So there are many ways you can enhance this further in future using EL rather than having these calculations being done at EL and rather than these calculations being done in CL. And also updating the formula in EL is not really difficult and the only thing that is relevant is that okay, target will have to assume is half the maximum blob limit. But I actually don't see that you know how fancy we are going to go with respect to Target and the max limit and most probably, you know, things are good to be kept simple and straightforward.
01:09:11.635 - 01:09:53.374, Speaker B: Yeah, I tend to agree. I think I will wait until Dunkrad's here and he can maybe motivate his PR a bit more. But yeah, I do, I definitely see this like violation of separation of concerns across the layers which I don't like. I do think his thinking was essentially that there's like at the fork boundary. So if we change the blob count there's like this weird discontinuity in the base fee calculation and like the CL then would have more context to be able to like you know, compute the fee change smoothly. So I think that was one important point there. But yeah, that all being said.
01:09:55.074 - 01:09:55.386, Speaker C: This.
01:09:55.410 - 01:10:29.625, Speaker B: Will be an ongoing point of development. So yeah, please take a look and we will revisit in the future. Let's see anything else on pure dos. Otherwise we'll move to an SOC Update. Okay Etan, I think this is you. Would you like to give us an update on SSE work?
01:10:30.805 - 01:10:31.545, Speaker C: Sure.
01:10:32.725 - 01:11:48.355, Speaker H: So there are still these two EIPs. One is on the consensus side to update all the SSE container definitions that frequently change such as beacon state, beacon block, body and these to a format where the generalized indices no longer change for any given field across future forks. Just for context, this is used by smart contracts that rely on EIP4788 and pull in Beacon data. Right now they need a redeployment whenever a gendex changes and with the CIP it's no longer necessary. We have full implementations from Lodestar, Nimbus and TECU that have a definite running in cortosis. Krantine, Lighthouse and Prism are still implementing it, but there is no real opposition at this time to give it a try for inclusion into Electra. So I would just like to ask one more time if there is any opposition still against EIP 7688 in Petra Devnet 2?
01:11:53.055 - 01:12:01.475, Speaker B: Well, I think right off the bat at least I can say Petra is quite big. So we need to very seriously consider making it even bigger.
01:12:02.025 - 01:12:53.875, Speaker J: But yeah Andre, go Just one comment on tech readiness. So we took a short path to be able to be compatible with the DevNet, just essentially be compatible with profile definition. But being fully compatible with the entire new SSD spec of the stable container turned out to be a little bit more complex. Still working on it on full compatibility. And yeah, just say that one thing is being compatible with the DevNet for switching to profiles and another thing is to have a full implementation of the SSD library fully compatible with the specs.
01:12:56.945 - 01:13:18.165, Speaker H: Sure, yeah. I mean for this EIP like the stable gendit test for the beacon state struct for the consensus structures, only profile support is necessary. So like what you did for the devnet, I'm talking about including this scope only this scope into the Devnet 2.
01:13:27.515 - 01:14:02.395, Speaker J: Yeah, but including only that it will be anyway tricky for us because the implementation is still undergoing and we normally go with master branch with devnets. So I'm it will force me to merge to master an incomplete implementation which is still working on. So my preference at least will be to complete the implementation and then.
01:14:04.095 - 01:14:04.455, Speaker B: Have.
01:14:04.495 - 01:14:07.275, Speaker J: Everything ready for full support.
01:14:09.655 - 01:14:23.085, Speaker H: What is the timeline on Devnet 2 like? We are still ahead of Devnet 1, so for Devnet 1 it's definitely too early. Right. So two or three would it like could it be a good target?
01:14:25.465 - 01:14:36.805, Speaker J: Yeah, that's a good point. I don't have an idea, but yeah. So maybe DevNet 2 could be good. DevNet 3 will be safer.
01:14:45.395 - 01:15:00.655, Speaker B: Okay. And in any case, I think people are mainly focused on Devnet 1. Devnet 2 will probably be some time off, so. Yeah. Again, this is a nice update. It sounds like we can make a call on a future AC dc.
01:15:02.715 - 01:15:36.455, Speaker H: Okay. Yeah. And for the SSC transactions like that's the El eip Ethereum JS has a prototype now and we are working on making a client demo as well that consumes that data so that it verifies, for example, transaction inclusion proofs so that if you have a wallet and it shows your transaction history that you know that it's accurate. That one I posted into the Light Clients channel on this. It's still early access.
01:15:43.515 - 01:15:44.695, Speaker D: Okay, thanks.
01:15:54.875 - 01:16:32.627, Speaker B: Okay, cool. I think that was everything on the agenda for today. Is there anything else that we would like to discuss? Otherwise we'll go ahead and wrap up. Okay, cool. Thanks everyone. Let's go ahead and call it. Thanks.
01:16:32.691 - 01:16:33.019, Speaker E: Bye.
01:16:33.067 - 01:16:33.815, Speaker G: Thank you.
01:16:34.315 - 01:16:35.235, Speaker C: Thank you. Thank you.
