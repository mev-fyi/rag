00:00:01.840 - 00:00:48.374, Speaker A: Hello and welcome everyone to roll call number four. That's issue 1013 in the Ethereum PM's repo. I'll drop a link in the chat. Yeah, let's get started. So, as I'm sure you know we. But we skipped the last call that was scheduled for just after the go live of EIP 4844. It seems like that went very successfully, both from an l one perspective and then from any of you here deploying blobs on the l two side of things.
00:00:48.374 - 00:01:33.964, Speaker A: I was going through the base fee recently. It's also the blob base fee that is has been interesting to see. It seems like we're still just hanging under the threshold or the target of three. So blockbase fees nice and low, particularly now that the descriptions have died down, which is, which is really cool to see. And excited that this seems to be bringing down the costs for many people. Another exciting thing that's sort of happened since our last roll call is we've seen rip 7212.1 real precompile go live on several chains.
00:01:33.964 - 00:01:51.344, Speaker A: Congrats to everyone who's deployed that. This is really awesome. And yeah, I'm excited to see the direction this goes, but I think it's a big unlock from a UX perspective, as you're all aware of, I think.
00:01:52.204 - 00:01:56.544, Speaker B: Well, rip seven to twelve is also scheduled for Petra.
00:01:58.764 - 00:01:59.428, Speaker C: Yeah.
00:01:59.556 - 00:02:00.584, Speaker A: Wait, is it?
00:02:01.844 - 00:02:04.224, Speaker B: I've seen it in the mega thread.
00:02:05.084 - 00:02:05.452, Speaker D: Okay.
00:02:05.468 - 00:02:32.204, Speaker A: I don't know the scheduled. I mean that's also. Yeah, yeah, it's being considered. I mean, would be cool. And it's definitely something we can push on the l one side and see if there's interest to have it deployed on the l one side as well.
00:02:34.224 - 00:03:01.544, Speaker B: So I guess from roll up perspective, everyone that wants to add SGX, and I think there are several, at least Tyco does. I think scroll mentioned it as well. This would help on gas costs, like reducing just the SGX verification from a million to 100,000 or something.
00:03:04.724 - 00:04:27.684, Speaker A: Okay, that'd be pretty cool. Yeah, that definitely goes from the round of the realm of barely feasible to definitely doable within a block. Okay, any other things people want to mention on 7212? Great, moving on. The next topic is the BLS pre compile on layer one that I'm sure you guys everyone has seen lots of discussion on in our chats. Alex was going to pitch it, but I believe he's unable to make it. The idea here, and that the reason this is important to discuss in this forum is for the chains that are targeting EVM equivalents particularly on the ZK side, it could be quite complicated to have the support the BLS precompile. EiP 25 37 and so.
00:04:29.424 - 00:04:30.000, Speaker E: This is.
00:04:30.032 - 00:04:45.814, Speaker A: Something that will happen on l one mainnet. So the question is just like, is there something that the ZKl two s feel like they can implement, or.
00:04:47.634 - 00:04:48.114, Speaker E: Is.
00:04:48.194 - 00:05:03.264, Speaker A: It something that we need to figure out a standard for having to work around not supporting on the l two side of things? Is this just something we can solve with gas being different on l two? Does anyone have any thoughts on this?
00:05:11.004 - 00:05:39.174, Speaker B: Well, taco is not live, so we have less constraint, but we are adding support to the BLS twelve 381 precompile. I mean, once you support bn 254, modifying to support BLS 12,381 is not that hard, at least for addition and scalar multiplication, hashing to curve is a bit different.
00:05:46.274 - 00:05:50.538, Speaker A: And the reason hashing to curve is.
00:05:50.706 - 00:06:16.444, Speaker B: Less on what's so there are two reasons. One is that there is no hashing to curve for bn 254, so we need to write the code from scratch. And the other one is was. Even if it was, it uses bls 12,381 uses SWU algorithm, while bn 254 uses SVDW.
00:06:17.924 - 00:06:59.604, Speaker A: Okay, all right. And then the other thing that I think many people have seen discussed, and what has been discussed in the, in the chats was in relation to whether we should do the subgroup checks and which function should have subgroup checks. I mean, that's a larger discussion and I think will be solved largely the l one side of things. But is there anything that's like, does this have any effect on those trying to implement it in ZK crypto?
00:07:02.304 - 00:07:15.974, Speaker B: So the way subgroup checks are done is through a scalar multiplication, but a very short one. So once you support scalar multiplication in ZK, subgroup checks are kind of free.
00:07:17.994 - 00:07:24.774, Speaker A: Okay, so you're not worried, there's no concerns about the additional costs of.
00:07:26.514 - 00:07:26.866, Speaker D: It'S.
00:07:26.890 - 00:07:34.414, Speaker A: Less about whether it's feasible, it's more about just the marginal cost of how frequently it gets done. Yeah, and that's not too bad then.
00:07:35.154 - 00:08:38.434, Speaker B: On the gas costs. I guess there is a tension between, like if we want people to verify Zk on chain for cheap, we need the gas cost to be very low. But at the same time for L2, if people want to use ZK bridges, maybe layer three and everything, we need to be able to do those like it should be able to reflect prover costs on L2. But this is only an issue for type one rollups, and the only one that's type one at the moment, like there is none typo will launch at type one. We'll add some precompiles to, well, syscalls, accelerators, coprocessors, whatever people name it.
00:08:40.174 - 00:08:40.486, Speaker A: To.
00:08:40.510 - 00:09:47.194, Speaker B: Make sure it works. But yeah, it's there is attention there for sure. And this is also why I raised EIP 7667 from Vitalik, which was about raising the gas cost of hash functions by ten x to reflect a bit the cost in ZKE evm in type one zkvm of proving hash functions. But the difference for elliptic curves operations is that if you use like aztec, goblin, planck or cycle fold, you can actually reduce the cost of elliptic curve operation proving. While you cannot do this with hash functions, there is a lot of research with lookup, singularity, binus and things like this, but nothing in production yet.
00:09:51.294 - 00:10:40.974, Speaker A: Okay, I think that's helpful. Anyone have any other things that like to raise on the BLS pre compiled particularly on with relevance to l two? Nope? Okay, great, moving on. So the next one is EIP 7623, which is a me at increasing call data costs, and I believe Tony wants to speak about that over to you, Tony.
00:10:42.674 - 00:11:44.664, Speaker D: Yes, thanks a lot Carl. Hi everyone, let me quickly drop some links here in the chat. Yes, so in the chat you can see the link to the EAP itself, then the latest analysis, and also you find this GitHub pages page where you can look for functions that some stats on different functions and check which functions are essentially affected by the ERP. But to give you some background. So ErP 7623 is basically proposing to raise the costs for call data. So basically increasing it from four gas per zero byte and 16 gas per non zero byte to 48 for non zero bytes and 16 I think now, yeah, or twelve. I would have to check it again, but definitely increasing it for both.
00:11:44.664 - 00:12:37.104, Speaker D: And the rationale behind this change would be to just lower the maximum possible block size. And I thought it might be important to also introduce this EAp specifically here in the roll up call. Because in the end it's the roll ups that are currently the biggest call data consumers. And although most roll ups have already switched towards using blobs, there are still some rollups that are dependent on cheap call data. I guess until picture, most of the roll ups might have moved to using blobs anyway. But yeah, there are still some edge cases where certain rollups might still need to use call data, especially if you need that call data inside DBM. If you want to do something with it, then those roll ups might be affected.
00:12:37.104 - 00:13:30.924, Speaker D: And although the ERP is currently not considered for Pacdra, I think there's still a way for the EAP to make it into the update, so I'm super curious about opinions on it. Basically what the EAP does is it introduces a floor price. When you use a certain amount of gas on EVM computation, you don't hit that floor price, which means you get the call data at the same price as of now, which is four and 16 guests per byte. But if you're a pure data availability consumer on layer one, then you might have to pay more for that. And this allows us to reduce the block size from currently 3.5 megabyte, including blobs, now to around 1.9 megabyte.
00:13:30.924 - 00:14:05.234, Speaker D: So it's almost half the block size here. And why do we want to reduce the maximum possible block size? That's one question you might ask. And the reason is because the current block size is quite large. So 3.5 megabyte is arguably quite large. And while the current block size is around, the average block size is around 100 to 200 kb. So there is a big room between, let's say, 200 megabytes.
00:14:05.234 - 00:14:59.334, Speaker D: And in order to get to such a 3.5 megabyte block, you would basically create one single transaction with a lot of call data and get it into a block. So there is no real use case except for creating these very, very big transactions that, yeah, I'm not aware of any use case, and there weren't such big transactions that really consumed the whole gas that is available in the block within one transaction. So in my opinion, it makes sense to make sure that the block size is reduced a bit. And, yeah, paving the way for further scaling, because as you might have seen, there are already, there's a draft eap, I think, for increasing the blob count. And there is also pumpdegas.org, which is kind of a community effort to increase the gas limit.
00:14:59.334 - 00:16:03.962, Speaker D: And I think this should be taken very seriously because I think the community has voiced a demand to increase the gas limit. And, yeah, we can discuss if there is room. So probably the cortex can tell us if there is room to increase the gas limit without changing anything in the call data, because, of course, the bigger the gas limit, the more call data you can get into one block, and the bigger the blocks are. So, yeah, curious what people think about that. I think there were already some cordovs saying, okay, we shouldn't naively increase the block gas limit because we already have very large blocks. And then, of course, the question is, how do we best deal with the block size EAP 7623 is one attempt. I've not heard about other attempts to achieve the same goal.
00:16:03.962 - 00:16:08.974, Speaker D: But of course there might be different ways towards that goal.
00:16:09.754 - 00:16:10.210, Speaker A: And.
00:16:10.282 - 00:16:37.344, Speaker D: Yeah, super curious what people think, especially if you're representing a roll up, if you're working for a roll up. I'm super curious if this would even affect you, if this would affect you as a roll up, because most roll ups, I guess, would be unaffected entirely. Yeah. So curious what people think. Even if you're for it, against it. I'm curious to hear about different opinions.
00:16:49.964 - 00:16:51.932, Speaker A: Assuming you've switched over to.
00:16:52.108 - 00:17:11.954, Speaker F: Yep, yeah, no, I mean, the barrier roll up will choose between which is most cheaper, either call data or either blob. So if you increase the call data cost, at the very end, more roll ups will move in order to put the data availability to developer, I guess.
00:17:15.854 - 00:17:32.834, Speaker D: Which is good, right? Because the roll updater has kind of the better place for roll up data is, are the blobs. Right. So this might be a positive aspect to kind of strengthen. I mean, this kind of.
00:17:33.734 - 00:17:57.614, Speaker F: Yeah, kind of forcing to switch, which is, I mean, it's not good now because at the very end, no, not good because you are forcing drops to move to blobs. But I mean, if it's not forcing in a short time, I think it's better, maybe in the midterm, it would be better for roll ups in order to adapt perfectly to the blocks.
00:18:05.034 - 00:18:25.004, Speaker A: So this, the threshold at which you do sufficient compute such that the call data cost isn't at this higher level. Would we be reaching that threshold with, for example, a fraud proof? What does this mean? That fraud proofs would become more expensive? Do you have any insight into that?
00:18:26.464 - 00:19:32.452, Speaker D: Actually, you would have to tell me how much call data such fraud proofs need, because I can tell you that it's around 97% of the transactions would be unaffected. So this means there are not many transactions currently that would be affected. I know that there are some very big Merkel proofs that might be affected by that, because certain Merkel proofs, they don't use a lot of EVM operations, EVM computation, but they require very big call data inputs that are then of course, used inside EVM. Yeah, I'm not sure about fraud proofs. I guess as soon as you use the EVM, as soon as your transaction involves the EVM to a certain extent. So I think it's two x. So you have to spend two x the amount on EVM gas, two x the amount of gas on EVM computation compared to call data then you're already in that range where you don't hit the floor.
00:19:32.452 - 00:20:04.084, Speaker D: And currently it's around. It's as said, 97% of the transactions of last week would be unaffected. And those 3% of the transactions that would be affected were executed by 1% of the users. So the number of users that are affected is very small. Currently I think scroll hasn't switched to using blobs, what I saw. So they would still be, they would. If we would ship that today they would be affected.
00:20:04.084 - 00:21:07.840, Speaker D: And also starknet. So I'm not sure if someone from Starknet is here, but I think also the stark proofs, they are very big and they also need that data, the call data inside the AVM. So not sure if there is something to optimize or if there is another way for them. But as soon as you require a lot of call data, but you only touch the EVM a little bit, then you are affected. Also maybe to add that most of the affected uses today would be those that add messages to their transactions. So if you withdraw from some exchange and the exchange lets you put in a message in your call data, you might have seen that then you're affected, but only very negligible because the messages are usually very small. So this is not a big concern.
00:21:07.840 - 00:21:13.844, Speaker D: It's more those that have very big call data and still needed inside the VM.
00:21:15.984 - 00:21:44.824, Speaker C: So what about future use cases where you're using a lot of CK proofs that you need to include in the call data? Like not, you know, they're different. That, you know, sufficiently different that it's, it's, that it's. It's not good for aggregation and recursive proving. So what do you do then?
00:21:46.364 - 00:21:52.984, Speaker B: Are you thinking of ZK coprocessors like Axiom, Rodotus or Lagrange?
00:21:53.884 - 00:23:15.084, Speaker C: No, just in a regular, let's say like a defi use case where you want to do trades, right? Or more complex trades and you want to make sure that the trades are compliant and you're using zero knowledge proofs as assertions that sender recipients, et cetera, are proper, are properly KYC'd, that the assets that are being traded are not hitting any asset policies. Maximum volume of trading per day. Suitability, for example, is the, is the account purchasing that suitable for that type of asset? That's a compliance thing? If that account belongs, if it's a highly speculative asset and the account belongs to a 95 year old that's not suitable. You have to have these ways to prove that. And the easiest way to do that without revealing information if you want to do it on chain. Whether that's l one or l two doesn't matter. Is using gks.
00:23:15.084 - 00:23:35.704, Speaker C: So there are use cases where you could have 810 different ZKs for a single deFi trademark.
00:23:37.484 - 00:23:56.264, Speaker D: Okay, yeah, I see. Yeah. Usually if you use snarks, for example, then in all the cases I saw so far, you're not affected, because it's just so much EVM computation that you need to verify snark that there's a difference for starks. So I think when it comes to.
00:23:57.244 - 00:24:12.674, Speaker C: It depends on the inputs, right? I mean, inputs could be, they can, depending on what the inputs are, how many there are, that will drive call data size, things like that.
00:24:14.334 - 00:24:55.574, Speaker D: Right. But yeah, as you say, it's dependent on the input. How much input such use cases would eventually need, I don't know. If you look at tornado cache, for example, today, the inputs are small enough to not hit the floor, because I think the native cash has 500k gas, one transaction. So deposit or withdrawal is 500 gas. While the inputs, I think you have, I don't know, five, six parameters that you need in your call data. So this is still not enough to causing hitting the floor.
00:24:56.444 - 00:25:16.424, Speaker C: Yeah, I'm looking at use cases where they're there. Unless you're going to like a semi private approach where the input is just the root, the root hash, you could easily have 40, 50, 60 input parameter.
00:25:19.564 - 00:25:21.156, Speaker D: Right. And that's also.
00:25:21.220 - 00:25:34.094, Speaker C: Right because you. Right, because you have different keys and you know, when you decompose. So it's like, it's, it's, it's that there are use cases where you have, where you can have large number of public inputs.
00:25:35.034 - 00:26:04.332, Speaker D: Yeah, I see, I see, I see that point. It's very similar to the big Merkel proof point where you also need to provide the whole path. Yeah, I think it's a, it's a very individual thing. So one would have to look into the individual transaction to check if they would be affected or not. Maybe to highlight those use cases would not become impossible. They would just become more expensive. Yeah.
00:26:04.332 - 00:26:33.592, Speaker D: And for example, with starcs, I think it was around 30% more gas. So as stark proof would be 30% more gas, for example. And of course, this could be compensated by increasing the blob count, by increasing the gas limit, by just making the whole thing cheaper. But this is, of course, a little. Yeah, this is an unknown, so I can, we cannot really tell, but that's a no.
00:26:33.608 - 00:26:49.164, Speaker C: I just wanted to bring that up as a, as another use case. Which is kind of general. What if you're using, what if you're dealing with a lot of, a lot of snarks, which I think we're going to see more and more of.
00:26:52.964 - 00:27:52.024, Speaker D: Yes, I agree to that. Yeah, I, to highlight that again, there's this. I think the third link I posted shows you all the functions and there you can see which functions are currently affected. And you can see it's functions like commit batch, which is a function used by scroll, which is currently affected and would be increased by 100%. And others are like there are min functions but they are rarely used. For example, there are other functions like verify merkle, which takes four lists, two lists of un 265 which might be affected, and of course the verify freaking which is the function that starkway uses. So on that link, if you're a rollout operator, you can check if functions that you call if they would be affected.
00:27:52.024 - 00:28:41.146, Speaker D: But so far what I have seen, it's only starkware that will still be affected because all the other roll ups can kind of flee towards using blobs. I see Anska wrote something. Let me check. Yeah, Ansker wrote, now that we have blobs, they call data increase sims. Inevitable. Expensive, right? Yeah, I agree. Anska CK proofs in general, snarks are usually very small, like tornado cache, and they come with the verification, takes a lot of evm resources.
00:28:41.146 - 00:28:57.094, Speaker D: So snugs that we like, we know them today, like tenant care for example. They wouldn't be affected at all. So they are way above this threshold using enough EVM operations to do that.
00:28:59.714 - 00:29:24.344, Speaker B: Regarding your option for full 2d pricing for cold data with a market similar to EIP 1559, I assume this one is not scheduled because this is too complex to implement for Petra, which is less than six months from now, I assume.
00:29:26.604 - 00:30:12.034, Speaker D: Yeah, so this was also the main feedback I got so far regarding that point. That kind of having a separate fee market for col data might be just an overkill for the, for the thing we want to achieve, which is basically just lowering the maximum possible block size to prevent those attacks. And yeah, so of course having a separate fee market would be also a very clean solution, but maybe just over engineering the whole thing or too complex. I think Ansko once had this comment like why don't we just have a separate fee market for every opcode then, which might be the next step?
00:30:12.574 - 00:31:03.134, Speaker B: I think one roll up, like two years or three years ago all roll ups were discussing 2d fee market. If you search on ETH research, there might be several instances. So there might be for it. But the burden of implementation is on the execution layer, and we need buy in from them for this. And then you need also wallets to implement this. So I think six months is way too low. And if it would be very interesting, but if we want something for Pektra, we need a smaller solution.
00:31:03.134 - 00:31:10.474, Speaker B: And then the question is, is it good enough or we need yet another artwork to fix this again?
00:31:11.494 - 00:31:53.034, Speaker D: Right? Yeah, I agree. Yeah, fully agree. And it would also be, the question would then be, do we then achieve the goal of lowering, of reducing the block size? Because imagine all roll ups switch towards blobs, then call data becomes cheaper again. And suddenly with cheap call data, one could again create those very big blocks that might cause some troubles, especially thinking of smaller stakers that don't have the bandwidth to keep up. But this is super interesting. Would be cool if you could send me a link or something. I would love to see that discussion.
00:31:53.034 - 00:31:55.734, Speaker D: Seen that so far.
00:31:58.834 - 00:31:59.814, Speaker A: Anskar?
00:32:02.354 - 00:32:47.186, Speaker E: Yeah. On this topic, I just wanted to briefly mention there's nothing right actionable right now, but I was also working on like multi dimensional fee markets like a year and a half ago or something for a while. And in general, kind of more repricing flexibility. I think there's a lot of pent up demand around doing this, especially as L2 structurally become less and less similar to l one. Especially, say, in just throughput numbers, you might have seen the kind of, for example, base has been very active on meaning about wanting to like 400 x the throughput. But I know that a lot of L2s are working on similar plans in the background. Once you do that, you basically just hit very different constraints than l one.
00:32:47.186 - 00:33:40.474, Speaker E: And so basically having the exact same pricing schedule, even if you stay on a one dimensional pricing scheme, just doesn't make sense. So basically, the question on how can we have more flexibility and having different pricing schedules for different vms without breaking interoperability of apps, how can we maybe introduce multimensional pricing? There's a lot of just pent up ideas and research and everything. I would expect that over the next one to two to three years, this will somehow lead to some more fundamental revamp of the fee market and some actionable things for maybe just for their twos, maybe also for their one. And so for me, in my mind, basically, we are in this space where right now we are just trying to have like small minimum, minimum intrusive adjustments that basically get us to this more fundamental rework. And I think basically Tony CIP is perfect for that.
00:33:56.834 - 00:34:03.334, Speaker A: Great. Anyone else want to mention anything on this EIP before we move forward?
00:34:08.634 - 00:34:35.704, Speaker D: Maybe if someone is very strongly against it, especially including it into packtra. I will also present the DAP and propose it to include it into Pakstra tomorrow in the OCADF call. So yeah, just feel free to join and raise your voice there. If you don't want to see it into Paktra or contact me, I'm happy to discuss it.
00:34:44.064 - 00:34:45.124, Speaker A: Thanks, Tony.
00:34:45.864 - 00:34:58.244, Speaker B: Maybe ping people from ZK coprocessors. I'm not sure how they use cold data, but they might need a lot of data as well.
00:35:02.984 - 00:35:05.044, Speaker D: Okay, yeah, thanks. Thanks for that.
00:35:14.644 - 00:35:28.940, Speaker A: Great. And then I believe Mami also wanted to talk about EIP 7667, which in the ip to increase the gas cost of hashing precompiles. Mami.
00:35:29.092 - 00:36:34.334, Speaker B: Yeah, so Vitalik proposed two weeks ago to increase gas costs of hash functions, obviously for type one roll ups. That's great, because hash functions have a disproportionate proving cost. But I'm wondering, should we just say, yeah, all roll ups agree with this. That's basically the kind of consensus I'm looking into because I assume other Dapps developer, for example, for example, the one that uses Merkel trees on chain, we say, oh no, this becomes too expensive for us and there will need to be a discussion. Arbitrage in terms of do we increase two x, three x, five x, or do nothing, for example.
00:36:48.234 - 00:37:10.664, Speaker A: I think the thing I'm a little unclear on is why, like from an l one perspective, more than obviously is this benefit for type one l, two s? But from now one perspective, why is it something that needs to be done now as opposed to in the future?
00:37:16.884 - 00:38:12.804, Speaker B: So that was asked by WGM elements. And what did Vitalik reply? Yeah, making things unviable and so that they come to other solutions before, instead of them building something and then one year later, it cannot be, it's dead in the water. So I guess it's managing expectations. Personally, I have no opinion on when, when to do this.
00:38:28.924 - 00:38:43.944, Speaker A: Yeah, I mean, I can definitely see the argument for wanting to do it sooner from the perspective of setting expectations that would prevent issues where people are unduly impacted, such as Tony was mentioning earlier with, with his EIP.
00:38:45.964 - 00:38:46.524, Speaker E: Where certain.
00:38:46.564 - 00:38:59.224, Speaker A: Use cases suddenly get become radically less possible or more possible. Anyone else want to raise any additional points on this EIP?
00:39:03.564 - 00:39:18.924, Speaker B: No, that's all just raising awareness and seeing if there is some kind of consensus on roll ups or if people maybe don't care neutral.
00:39:21.904 - 00:40:11.204, Speaker C: Would it not make sense to offer the option? I mean increase the cost on the l two rather than the l one. Right. And for, for, you know, hack check and offer an alternative on the l two like poseidon two. Because from a smart contract point of view, if you're using hashing, hashing is hashing is hashing as far as the logic goes. Obviously from a proving point of view, there's a huge difference. So if you just make it, you just introduce different pricing for, hey, if you use CAC Jack, well, it's going to cost you like ten x more or 20 x more than if you're using like Poseidon two. We also have a pre compile for that.
00:40:11.204 - 00:40:15.848, Speaker C: Tell us about that.
00:40:15.936 - 00:40:33.424, Speaker B: I think that's, that's a very interesting point. I think you should add it. Maybe it should come with a positon hash function for, based on both BN 254 and maybe one for BLS 12,381.
00:40:35.724 - 00:40:46.944, Speaker C: Yeah. Doesn't Poseidon to work very effectively for both?
00:40:49.044 - 00:41:04.702, Speaker B: Yeah, but maybe we should have an eip that implements both so that people are not, again, oh, there is only Poseidon on BN, so I'll build only BN.
00:41:04.798 - 00:41:09.274, Speaker C: Oh yeah, of course, of course, of course. Yeah.
00:41:13.014 - 00:41:15.274, Speaker A: Is the idea to have this thing as a pre compile?
00:41:16.734 - 00:42:26.934, Speaker C: Yeah, so you get, so you give options, right? You give options for the smart contract devs. It's like, look, you do, this cost you more money. It's more like what you're used to. But if you're writing code for an l two, you know, and, and by the way, it would not hurt if you had, if you had pre compiles for like Poseidon two on Vn and Bls on the l twelve, because that would make, that would make, that would make life easier for, you know, for Zk type applications. Because then you can compute Poseidon hashes on one that you might need for public inputs, et cetera. So there's also a general upside if you want to make the l, one more Zk friendly to support, you know, something like Poseidon on Vn and BLS.
00:42:28.874 - 00:42:37.974, Speaker A: I imagine the issue of getting this kind of thing on zero one will be the relative novelty of Poseidon.
00:42:41.634 - 00:43:01.428, Speaker C: I just picked Poseidon out of the hand. You would have to discuss which is the, which is the, which would be the type of hash function that would be, that would be supported because that basically would become a forcing function. Right, sure.
00:43:01.476 - 00:43:07.784, Speaker A: But I mean, it's a more general issue of arithmetic hash functions.
00:43:10.044 - 00:43:10.452, Speaker C: Yes.
00:43:10.508 - 00:43:21.864, Speaker A: Like there are lots of novelty and the like. These are things we have to maintain in perpetuity on the one side.
00:43:23.404 - 00:44:02.724, Speaker C: Yeah, yeah. It's doubtful that, that, that's going to go away, but point, right? Yeah, sure. If you, if you want to. And I said you can do two, they're not, they are not couples. Right. You can do just the stuff on the l two and not do it on the l one and do the l one later. But for offering something like that on the l two would definitely lessen the pressure on the l two s if you gave the developers the options.
00:44:04.564 - 00:44:30.344, Speaker A: Yeah, no, agreed. All those things are relevant, and that's largely why we set up this calls to be able to ship these kinds of things on all twos that might be necessary, but the l one isn't necessarily really all willing to ship. Great. I think that brings us into general discussion. Does anyone have any other points that they wish to raise?
00:44:33.744 - 00:45:25.614, Speaker C: Yeah, so I just wanted to bring up a point since we had discussed like l two transaction fees. So the l two standards working group over at Oasis has come up with a proposal for an API spec. And I thought it might be a good time to have a breakout session on that with like the different client teams to take a look at it and see what they think, because it would be aligns with like, you know, separating out the fees and the different components, etcetera, execution fee, data fee, priority fee, what already has been discussed. So I just want to propose that, see if people would be interested.
00:45:32.914 - 00:45:45.174, Speaker A: Yeah, I mean, actually, I imagine that's the kind of thing that there is demand for, and you're always welcome to host your own record on this. Any other topics?
00:45:52.094 - 00:46:52.364, Speaker E: Maybe just to briefly mention to what Andreas just said. I think basically that, just to add, I think basically the two separate aspects of the actual protocol changes, like having some sort of consistent transaction type for this and then the API changes around this. And I think ideally an ideal kind of approach would combine the two. But of course any protocol changes would require much more buy in from the different teams to move to new kind of transaction type system. So I think on the roll call side, a breakout would probably be good once we can kind of have some sort of feeling for how to best tackle the actual kind of in protocol changes as well. But I'm definitely open to also having a, at some point having some sort of breakout session on more. The, the API changes standalone, otherwise, because otherwise maybe it's not a good idea to have been blocked by a need for foreign protocol changes.
00:46:57.424 - 00:47:24.374, Speaker C: Yeah, I think they, I mean the APIs are simple enough that they should, you know, all that information should be available, so it's I don't think that there would be, like. I mean, it might be different for each client, but I wouldn't perceive that there should be, like, any protocol changes on the client side.
00:47:25.194 - 00:47:25.578, Speaker A: Right.
00:47:25.626 - 00:47:33.214, Speaker C: The data is being computed today, so it's just a question of exposing that.
00:47:37.654 - 00:48:04.574, Speaker E: Right, yeah. I'll have a look and see whether that makes sense, to have some sort of roll call kind of organized record. I can chat with you about that, but then we can maybe have something announced next roll call and have it then kind of in the cycle after or otherwise. Alternatively, maybe have something more on the oasis side, and then we just highlighted on roll call or something. Either. Either way, basically. But next roll call.
00:48:04.994 - 00:48:05.306, Speaker A: Yeah.
00:48:05.330 - 00:48:22.894, Speaker C: So we're basically ready to say, hey, this is a proposal. What do you guys think? We're at that point? So it's like, with, like, metal apps and octane and optimism and so forth.
00:48:26.934 - 00:48:28.434, Speaker E: Sounds good. Sounds good.
00:48:28.854 - 00:48:30.114, Speaker A: Okay, cool.
00:48:33.134 - 00:49:43.658, Speaker E: And then, Karl, because you were asking about other topics, so there was just one topic I briefly wanted to mention, and that's more, like, on a meta level, just for people to kind of maybe think a little bit about kind of the general kind of future role of roll call. So Carl and I and Joao, we've been kind of talking about this as well, over the last few months, and maybe just for context for other people. So the way I see it is that Roko kind of, of course, started as a bit of an experimental project, and I think by now I see three separate distinct kind of categories of uses for this process shaping out. And so the first one would be just general connective tissue between layer one and L2. So that's like, a little bit like what we saw today, the conversation around the core data, EIP, BLS, eips, that kind of thing. So, basically, both representing layer one towards L2 s and the other way around representing L2 s towards layer one, making sure that everyone is kind of tied into the respective kind of governance processes, and we have less friction there. So that's kind of.
00:49:43.658 - 00:50:10.878, Speaker E: That's kind of number one. I feel like we already kind of on a good path there in terms of filling, filling that kind of gap a little bit. Then the second one is what I would say is maybe more general, kind of. I would say research. That's things like fee markets, everything. I feel like there that we have the breakout calls, which were a little bit more experiments. Of course, we haven't really followed up on those yet, because general feedback was that weekly calls was a bit much.
00:50:10.878 - 00:50:53.292, Speaker E: So the question is do we keep that up in the future? Basically, where is there a need for, say kind of cross layer to research collaboration? So that's a little bit something where we are still kind of evaluating. And so if people have feedback, like were these initial breakout calls we had a few months ago, were they useful? Do people, would people want to basically start that back up? And that's definitely something that we'd be interested in hearing about. And then the third one, that's I think the most ambitious side of the future for roll call would be. What I would say is, on the one hand, let's say just like L2 ercs or something. That's of course something we can do at any time. That's something like this fee market API change or something. But I think even more interesting of course would be the eye piece.
00:50:53.292 - 00:51:40.984, Speaker E: So potential future extensions of the EVM on there too. I think there. The situation we are in right now is that so far it is shown to be an, that was of course predictable. It should be relatively hard to ship any kind of changes that meaningfully deviate from layer one. Just because for now there is no good process of having support for these kind of changes in clients for L2s. For now we are really limited to these ips like the r1 precompile, which happened to be so tiny implementation wise, that geth in this case just opted to support this even while it's not actively targeting layer one yet, or at least not accepted for layer one. But of course that's an anomaly.
00:51:40.984 - 00:52:23.744, Speaker E: Most interesting. Layer two ips would require deeper changes and it's not clear yet in what form these could be shipped. So I think for now, basically the roll call is a little bit, I don't want to say blocked, but basically we're in a position where we yet have to figure out a good path. We have some ideas there. It's maybe too early to really discuss those, but that's definitely topics for future breakout calls. So far we have talked to individual client teams about how kind of a path forward to giving more options, more and more possibilities of these kind of more ambitious ips in the future. We're also over the next few weeks going to start talking more to some of you all on the L2 team side.
00:52:23.744 - 00:53:05.484, Speaker E: And so I just wanted to bring it up already just to. Because I think these calls for now, it's also like if you look, there's been attendance, it's relatively low and everything because right now it's a little bit still in this experimental stage. And so we're kind of actively working on charting a path forward to figuring out exactly kind of the shape of roll call. I just wanted to mention it a of course, if someone has feedback right now, but also more just to think about it, and we'll basically kind of reach out, talk to teams and. Yeah, I think the ambition is basically to figure out how this process can be the most useful for you all. So just wanted to give that as some context.
00:53:16.424 - 00:53:43.414, Speaker A: Sounds good. You don't have anything else they want to add to that or phrase more generally as discussion before you wrap up? Okay, great. Thank you, everyone, for attending. I will see y'all in the chats. Thank you.
00:53:43.574 - 00:53:44.354, Speaker C: Cheers.
00:53:45.214 - 00:53:46.566, Speaker D: Thank you. Bye bye.
00:53:46.710 - 00:53:47.766, Speaker F: Thank you. Bye.
