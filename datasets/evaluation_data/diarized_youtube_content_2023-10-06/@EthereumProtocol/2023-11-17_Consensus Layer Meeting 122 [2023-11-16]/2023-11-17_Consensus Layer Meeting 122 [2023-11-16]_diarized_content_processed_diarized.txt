00:04:02.010 - 00:04:53.954, Speaker A: Welcome to awkward Devs commensurator, call one 2222. This is issue 900 in the pm repo. Hopefully short and sweet, but we shall see. So first and foremost, usually we have testing devnets, but instead we have blobsidecar networkingupdates status because testing and devnets and other things are generally blocked on the the consensus layer rework that is currently underway. So I'd love to get like a general status update. And then we have one issue posted by Enrico around when to serve certain messages that we can talk about most core devs, that's the name of somebody on the zoom. Which teams are behind there.
00:04:53.954 - 00:05:02.582, Speaker A: Just so we know. All teams. Okay, a lot of teams. Cool.
00:05:02.636 - 00:05:05.830, Speaker B: Yeah, 40 people here I think.
00:05:05.980 - 00:05:19.164, Speaker A: Oh shit. Hello. Do you have a video? Your depth, your depth of field is very.
00:05:19.362 - 00:05:23.950, Speaker C: That's probably just right.
00:05:27.220 - 00:05:27.970, Speaker A: Nice.
00:05:30.660 - 00:05:31.410, Speaker D: Cool.
00:05:32.740 - 00:05:45.670, Speaker A: So I would really like to hear from the various consensus layer teams on just a status update on the rework, the networking rework with respect to the spec changes a couple of weeks ago. Who wants to get us started?
00:05:48.670 - 00:05:49.900, Speaker C: I'll get started.
00:05:54.210 - 00:06:06.622, Speaker E: So we're pretty much done with development. I think we would need to the end of next week in order to get through review and finish up testing.
00:06:06.686 - 00:06:08.580, Speaker C: But yeah, that's about it.
00:06:10.310 - 00:06:11.060, Speaker A: Next.
00:06:15.980 - 00:06:52.070, Speaker D: From the tech team, work is in progress. We implemented the new gossip validation. We are migrating all the data structures that are needed to be migrated out. We are working on the builder flow and yeah, working progress on track with timings for Lodstar. I aim to complete this in this week, hopefully next week. Lordstar should be ready.
00:06:54.360 - 00:07:09.790, Speaker F: For prism, I think for the normal path which is without builder. I think end of next week is fairly doable, but with the builder probably need one more week, which is probably fine because builder takes a while to set up anyway.
00:07:11.760 - 00:07:19.004, Speaker A: Enrico, real quick, when you said on track, what timings did you generally mean? Kind of, yeah, I think, I guess.
00:07:19.042 - 00:07:23.040, Speaker D: A couple of weeks seems like reasonable.
00:07:27.060 - 00:07:37.890, Speaker A: Loadstar. Yeah, I already apologize. Nimbus.
00:07:43.340 - 00:07:49.580, Speaker B: Does anyone from Nimbus? I guess we got no one from Nimbus.
00:07:49.660 - 00:08:38.976, Speaker A: Yeah. All right, cool. So it sounds like we are not ready to plan devnets, but we're ready to kind of pre plan talking about it again. So at least let's bring it up on Acde next week, get another status update and that'll inform whether we're kind of working on some proto devnet the following week or maybe the week after. Obviously DevOps. Folks, if you want to keep a closer eye on the pulse, maybe you can do some pre work if a team or two want to start experimenting next week, but I'll leave that up to you.
00:08:39.158 - 00:08:51.556, Speaker B: That sounds good. Ideally, I think it would be very good if you could launch something next week like Devnote twelve next week. But if we're not going to have more than two clients ready, then maybe.
00:08:51.578 - 00:08:53.590, Speaker A: A week after that.
00:08:54.280 - 00:09:00.692, Speaker B: But like end of November is probably like 29th to 30th. November is probably more reasonable.
00:09:00.836 - 00:09:08.120, Speaker C: What's the status on Hive test? Do we already have them updated? And other clients that are already testing on Hive?
00:09:09.340 - 00:09:44.836, Speaker G: Hey yes, so we're basically ready on the mock builder and on the blubber side. So we're ready to start testing the builder, I mean the normal file, the builder and the blubber stuff. I've added some interesting test cases on the blubber. I will try to share them as soon as possible on the r and D. Basically just accubocating block headers and all that stuff should be ready to go. So yeah, we are basically ready. Just adding more test cases that will be ideal over the course of the next week.
00:09:44.836 - 00:09:47.910, Speaker G: But the basic stuff is ready.
00:09:50.760 - 00:10:18.820, Speaker B: Sounds good. So if we can have some client branches ready to go by end of next week, then we can do hype tests a week after that on Monday, Tuesday. Then we can launch on Wednesday around the 20 eigth. And if that goes smoothly, maybe we can fork early in mid December. Sounds right.
00:10:22.470 - 00:11:03.498, Speaker A: I'm not going to commit to a date, but I do think that certainly next week we can start preparing for Devnet twelve, whether that's all clients or a couple. Okay, anything else related to this item? Okay, cool. So the next one's kind of related, at least to some of the edge cases that come up in this. Enrico, I did just toss a comment on there, but can you give the quick on this and we can discuss?
00:11:03.664 - 00:12:33.458, Speaker D: Yes, sure. So I was thinking about highlighting difference that will be introduced by the NAB in terms of usage of block by root and blobs by root RPC request. Because before the NAB, what happens is that if a node misses a block sent by gossip, the moment in which the node will start using RPC is when first it receives an attestation voting on that block, or even if he receives the next block building on top of it. In both cases, most of the network probably has already imported that block. So if all the clients actually serves only blocks that are fully imported, it's most likely that that node will be successful in retrieving the block by route. But with the NAB, a node will be able to be aware of that something of a block or a blobs that he missed via gossip way before that. This is because we can receive a bunch of set of messages that all related to the same block route.
00:12:33.554 - 00:12:39.450, Speaker A: It's like there's a native race condition, rather than like an accidental, sometimes race condition.
00:12:41.630 - 00:12:43.180, Speaker D: What do you mean by that?
00:12:43.630 - 00:12:49.662, Speaker A: Like built into the protocols, we're sending messages that have linked dependencies at the same exact time.
00:12:49.796 - 00:14:28.080, Speaker D: Yeah, exactly. So this is perfectly a consequence of splitting and decoupling blobs and blobs and block, which means that potentially the node could decide to do a by root request both on blobs and to blocks a little bit before that. But if the client still serves those RPC requests only for blocks and blobs that are fully imported, there is a high chance that those early blocks, those early by root requests probably fails. So this is related to a logic that we already implemented in Teco, which is a logic that tries to recover when something is missing and the attestation due is coming. And now it's time to attest. So we still give some time to the gossip layer to receive the missed message, but at some point we try immediately to do the byproduct request. So I was thinking if clients could start serving messages that has been validated via gossip and not fully imported yet, I think the taco behavior could receive some benefits on doing that.
00:14:28.080 - 00:14:55.160, Speaker D: So it will be some early recovery and give them more chance to not to attest to the right thing. From a security perspective, I think like receiving things that are validated via gossip or actively requesting for a particular message. It doesn't feel to me that there's any particular difference.
00:14:59.370 - 00:15:00.166, Speaker A: Right.
00:15:00.348 - 00:15:40.274, Speaker F: I guess I have a question. So do clients wait until they have all the blocks and the blobs? Assume there's blobs for the block before everything, then start running the state transition function. Because theoretically you can do everything in parallel, like a pipeline, right? Because you can do the consensus first, and then the execution can also be in parallel, the consensus, and then after that you just wait until you have all the blobs, because that will save you at this couple of hundred milliseconds. This is what prism does.
00:15:40.472 - 00:16:43.430, Speaker D: Teku does that as well. So at some point you could have three pipelines in parallel. One is the state transition, because we start doing the block import as soon as we receive the block. Then there is this pool of blobside cars that are building up gradually, and then at some point we also engage the execution layer. So there will be blobs blob validation with the retransition and execution layer that does the payload validation. But it doesn't mean that at some point you really need those blobs to fully import the block. So if you have the block and three out of four blobs and you're still waiting, but you could have some time to get the missing blob earlier and actually import everything before attestation queue.
00:16:44.410 - 00:17:11.600, Speaker A: My intuition here is that being able to request the message and the responder of the message sending it to you upon the same conditions that they would have forwarded on gossip is not increasing the attack vector and then potentially removing it from such message responses in the event that it failed full validation. I think it's worth that optimization to keep the network kind of lubricated here.
00:17:12.370 - 00:17:41.020, Speaker D: I also think that with blob's number rising, maybe the chance that one of those are missed maybe rises. But this also valid from the block perspective. So we received all the blobs, but we missed the block, and we can do an RPC by root for the block itself.
00:17:50.450 - 00:17:51.760, Speaker C: In White House.
00:17:52.290 - 00:18:54.340, Speaker E: I think we already do have this behavior where we start serving blobs or the block prior to import via RPC, but when we're requesting, we actually are delaying our by routes request to sort of optimistically listen on gossip, because we've noticed that it's more likely that we just end up receiving it on gossip later than we complete an RPC request in time to actually attest. So generally I support this sort of, I guess, clarity and how we should behave in RPC. But yeah, I'm not sure how necessary it is in terms of testing in time, but it seems better to try to seed the network more quickly, I guess, even if we don't have blocks and blobs fully verified together.
00:18:56.790 - 00:19:37.410, Speaker A: Yeah, I mean, I guess there's two things here. One is how a node should heuristically kind of decide when to switch from. I'm going to rely on the gossip to actually I should go find these messages, and I'm not certain if that should land in the spec, but the clarity on is it okay to serve messages that have passed the gossip validations only to potentially remove them from things you might serve in the event that it fails full validation? I think that that is my opinion is that's the way to go. And if we want to make sure it's clear in the spec, I'm definitely cool with that. Does anybody dissent from that opinion that if it passes gossip validation while you're doing full validation, it's okay to serve.
00:19:39.190 - 00:19:45.940, Speaker E: So I will say it is more complicated to implement, but I do think it's better.
00:19:48.650 - 00:19:56.520, Speaker H: Mikhail is here. I just have a question which case we try to optimize. So as far as I understand.
00:19:58.570 - 00:19:58.946, Speaker C: A.
00:19:58.988 - 00:20:23.958, Speaker H: Remote peer sends you a block and you assume that this peer has also received a block, right? And this block passes gossip validation, but then this peer must have sent you this block as well. So we are optimizing for the case when this push from that peer push of the block has failed and we are requesting it by root.
00:20:23.994 - 00:20:24.482, Speaker A: Right?
00:20:24.616 - 00:20:42.310, Speaker H: Am I understanding this correctly? So the peer tried to send you this block via gossip, but did not, but failed to do this only in this case, this by root request will make any difference in my opinion.
00:20:46.170 - 00:20:47.366, Speaker A: Here. Give me 1 second.
00:20:47.468 - 00:21:12.714, Speaker D: Yeah, I was also thinking about doing that. No. Okay. Yeah, that's the optimization. So if for some reason you got some kind of networking glitch that prevents you to get those messages, one of.
00:21:12.752 - 00:21:46.214, Speaker A: Those which these are different meshes. I didn't understand the push versus pull thing here, but if I receive message a and b on the meshes I'm expecting, and then they have a dependency which is message c, and I didn't get message c, it's a matter of who's going to respond to my query for message c. Is it people that have only fully validated message c? Or is it people that have validated the gossip conditions of message c? Is that excellence? Are we on the same page?
00:21:46.412 - 00:22:17.520, Speaker F: I think there is a difference, right, because you can pass gossip, but you don't verify the state transition function, right? So I think there is three levels to it. The first level is you pass gossip. The second level is you pass stage transition function. And then the third level is that you have everything, right? And I think it's probably better to serve only if you pass stage transition function because say, if today, like an attestation is bad in the block, you will still serve it versus you actually verify it.
00:22:19.250 - 00:22:55.340, Speaker A: So I mean, we definitely can't gate on the third because then it would be very difficult to recover because only nodes that have seen everything would be able to actually respond. I'm arguing we should only gate on the first because arguably those conditions made it safe from an antidos perspective to forward the message to your peers on gossip. So it's arguably safe if somebody requests it from you. Obviously you could add the additional full validation, but I don't think it really buys you anything with respect to security and just takes more time.
00:22:56.130 - 00:23:00.750, Speaker F: It's also easier to implement if it's just the first, I guess from prison.
00:23:02.050 - 00:23:03.760, Speaker A: Sean said the other way.
00:23:04.850 - 00:23:21.860, Speaker D: Okay, for Tegu is the same. We have this pool of things that has been validated and passed the gossip validation. So we only have the first. And I was thinking about serving only that set of data.
00:23:22.890 - 00:23:42.570, Speaker C: For prism. The problem is on where they are stored. So if we go to an approach where the things that haven't been completely seen and validated are stored differently than those that were already seen and validated, then when you're getting a request by root, it becomes a little trickier to be looking in two different places for blobs to serve.
00:23:45.150 - 00:23:58.320, Speaker F: But what I don't understand is that if today you pass gossip, you should be able to run state transition function right after. Right. So there is not that big of a difference. And then the state transition function is quite fast anyway.
00:23:59.970 - 00:24:21.398, Speaker A: Yeah, but if you're talking about 200 milliseconds to run it, and you're talking about a network that's trying to heal repair itself in these four second windows, it could be meaningful. Obviously, that's just kind of an intuition, and I don't totally know the impact of those, on those requests, but yeah.
00:24:21.484 - 00:24:22.760, Speaker F: That'S a fair point.
00:24:24.910 - 00:25:53.478, Speaker C: So one idea that we had a long time back was you guys are like making a kind of binary, whether to run the checks or not. And one idea would be to not make this binary, but say, okay, every second request, we run the state transition function before we distribute the block or not. We can use the peer scoring and say, okay, for peers where we have a higher confidence that those are good peers, we forward the block directly. We forward their block directly without checking it. And for peers that are lower scored, the probability, like the way we implemented it back in the day was not ongoing ethereum, it was on a different project, was that, I don't know, normal peer would have like a 50% chance. And on every block you receive you would evaluate it whether like you would generate a random number between one and two or whatever, one and 50. And if it's below 50, then you would evaluate it before you send it out.
00:25:53.478 - 00:26:35.780, Speaker C: And the more confident you became in that peer, the higher the probability is to send the block out without verifying it. What this would give us is basically in the happy scenario, if there's no bad blocks on the network, no bad agitations, we are in this fast mode where we just propagate everything without checking it. And if we start seeing agitations, we will check stuff and kind of prevent the propagation of those bad things.
00:26:36.230 - 00:27:00.970, Speaker D: Yeah, I think that will apply on the gossip side for us, because I think we decide to gossip thing only when we pass it, the validation of the gossip, but we don't try to do the state transition. And then after that to say, okay, let's disseminate the message, it will be too slow.
00:27:01.310 - 00:27:44.690, Speaker A: Yeah. I mean, to be fair on gossip, we do have antidos checks, and those are primarily around the proposer signature. And so I think it's okay to remap that to the request response here. I do like the design space we were talking about with respect to gossip, Marius, because certainly very interesting. But I don't think that we need to hoist that into the gossip. Could this could show up in spec or may respond to queries rather than a must. But I do think that it makes sense to clarify this one way or the other.
00:27:44.690 - 00:28:04.570, Speaker A: Enrico, would you be willing to open up a pr against the byroot to at least show a version of this? And then we can try to land it in spec next week?
00:28:10.380 - 00:28:12.712, Speaker D: Sorry, Danny, you're saying to me.
00:28:12.846 - 00:28:13.864, Speaker A: Yeah, I said, would you?
00:28:13.902 - 00:28:14.632, Speaker D: Yeah, sure.
00:28:14.766 - 00:28:15.064, Speaker C: Yes.
00:28:15.102 - 00:28:33.390, Speaker A: You're sure? Sweet. That is the last item on the agenda. Does anyone else have anything they want to talk about today?
00:28:43.470 - 00:29:09.874, Speaker C: If there's nothing else on the consensus layer? There was a proposal from people yesterday about building stateless clients on the execution layer in order to help with client diversity. If we have still some time left, it would be nice to just get this thought out there. Yeah.
00:29:09.912 - 00:29:12.934, Speaker A: Does somebody want to give the TLDR? I'm happy to.
00:29:12.972 - 00:29:27.980, Speaker C: If not, I can give the TLDR. Basically, the worst case scenario is if a majority client has some bug, and we finalize this bug, and everyone.
00:29:29.870 - 00:29:30.234, Speaker A: Who.
00:29:30.272 - 00:30:40.994, Speaker C: Runs this majority client will get slashed and cannot participate in the network anymore. And the idea behind this proposal is basically you not only run one client, but you run all of the clients. But you run most of the clients in a stateless mode. So whenever you receive a block, you will have one client that you run in a stateful mode, and you will execute this client in the stateful mode. And executing the block on it will generate a witness. And you take this witness, you put it somewhere, memory mapped file or whatever, and then you call all of the other clients in stateless mode with this witness and verify that they also execute the same block correctly. And if they execute the same block the same way, or if, like a boron n out of m, executed the block correctly, then and only then would you attest to it.
00:30:40.994 - 00:31:58.518, Speaker C: And this stateless execution is very easy to implement in execution layer clients. You basically just need to have a different database that you can feed this witness to, and your EVM will just take the state out of this different state database. And yeah, that's basically it. Because all of this can be done in memory. This witness database can be a memory database, the witness data itself can be a memory map file. Doing this should be extremely quick, and it will just require some additional cpu time. And what this would give us is that every client would execute the block on all different execution layer implementations before attesting to it, which would prevent us from finalizing a bad block in a majority client.
00:31:58.518 - 00:33:21.730, Speaker C: That's basically the proposal. And so what we would need to do for that is for every execution layer client to implement a way to get the witness for a block and a way to execute a block in stateless mode with a given witness. And the cool thing about it is, stateless execution is something that we want to target with Verkel anyway. And so if we were to build this right now, as I said, it's not hard to build, but most of this code can also be used for worker. And so we would already have stateless clients. These stateless clients, they are not really feasible for syncing the chain or these kind of things, because the witnesses in Memphis Patricia tries are kind of big, or can be big in some cases, but because we are not sending this data over the network, we only have it within our node, within our computer, it's not a big problem. So yeah, that would be the proposal.
00:33:21.730 - 00:34:17.270, Speaker C: I think we are going to implement it. I think Bisu was also already looking into it, and I think some other clients are also convinced now hopefully to also join. That's it. An interesting, it's possible that yields is fast, is fast enough that you could do this sort of verification against yields. So potentially you could actually check, have validate is checked, as well as the proper execution. Clients think the block is valid, that the stepification thinks the block is valid, which could be another layer of protection. And it might also work with ethereum JS actually.
00:34:17.270 - 00:35:32.010, Speaker C: So like even the clients that cannot sync mainet right now because the state is so big and sloads and s stores are so heavy, we can still use the ABM to verify the state transition on the execution. Can I ask a question? Sorry, I'm not super familiar with the statelessness stuff, but do you prove the validity of the inputs with respect to the prestate hash? Because I'm thinking this stateless client doesn't have any continuity, so it will just verify anything. Like if you could give it crap inputs with valid execution and it would say yeah, that's fine. So how does it know that it's executing a block, that it's really part of the chain? So you execute the state transition and you mark all of the try nodes that you touched or that have been modified during the state transition, and you prove all of them to the state root of the prestate and that's it. And you get to post the machine as well. The client that generates the witness. All other clients run on the same machine in terms of security.
00:35:32.010 - 00:36:26.830, Speaker C: But I mean, say if get has a bug in its database, can that break the guarantee somehow? If it generates a valid proof, no corrupt data, then it wouldn't be able to generate a valid proof. But not what the stateless client doesn't know what the old state route is though. So it could just generate, generate proof to an old invalid state route. No, the stateless client would need to have the old state root. So the stateless client does keep track of something between each execution. You could just say, hey, the state root for the last block was right, okay, but then that's part of the trusted input. You have to trust the death is not going to pass as bad.
00:36:26.830 - 00:36:33.208, Speaker C: It's not a real issue for me.
00:36:33.234 - 00:36:34.608, Speaker E: I'm still trying to understand, is it.
00:36:34.614 - 00:37:01.464, Speaker C: Possible to do this without the state witness because you already trust your client to get the right thing, send it over. So it's kind of important to do the verification because you want to make sure you're running the code that's going to calculate the state route at the end of the block. There was something wrong there, but it's not quite the same code because you are reading and writing to a different back end for the data.
00:37:01.582 - 00:37:04.376, Speaker E: And so there's some more overlap than.
00:37:04.398 - 00:38:11.440, Speaker C: Having zero state proof data, but it's not 100% there are bugs that could happen on the real client that wouldn't happen. I also think that you could consider not actually doing the statement verification and just telling the client this was the pre state, this is all the accounts and key storage and what their prevalues were. And here is the post value, every account and storage being revoked. And that wouldn't catch a bug if there was a bug in the vertical or vertical tree, but it would catch any bug in UVM. And I suspect in practice I feel like almost all of the attack surface and the scope of bugs is actually UVM. So if we implement it in a way that it doesn't catch a small category of bugs, like bugs in the implementation of the local train or bugs in some of the block building. It catches as much as reasonably possible.
00:38:11.440 - 00:38:15.516, Speaker C: We could cut some corners or bugs.
00:38:15.548 - 00:38:19.810, Speaker A: In like stateful caches and things like that. Hey, I just want to interject and give Lukaj a second.
00:38:23.060 - 00:39:40.008, Speaker I: Yeah, can you hear me? Okay, so while it's technically all this is correct, I'm not that favorable for this proposal because it has few things. Firstly, it increases latency. Generating the proof and then checking with other CL clients will increase latency on block validation. So we are already increasing latency with cancun potentially, and this will increase it even more. Secondly, I agree it's very easy to implement in terms of EVM and executing EVM in the client, but all the things related to integrating this between the clients and integrating this with CL clients is not that easy. And I think the estimations here are too optimistic. And I would prefer to put this effort to actually finalizing vertical trees.
00:39:40.008 - 00:41:04.390, Speaker I: And if someone isn't looking into vertical trees very carefully, there was tremendous progress in vertical trees this year, to the point that I think it's close to completion. And vertical trees would allow us to do it better because we would have these things in protocol. So we could have multiple clients like doing this at the same time without introducing any latency more than just the vertical trees, because there might be a bit of latency just by distributing the weaknesses, but it's something we probably want to do regardless. So putting a lot of effort, and I think it would be a lot of effort into that just to throw it away in few months after. I think it might be just the wrong way. And the last thing if I can finish is how would the chain behave if, for example, we would have bug in one of the clients, and most of validators would run this client and get, for example, and they wouldn't agree. Would the chain just stop at all or would it just not finalize? What would be the proposed solution here?
00:41:06.300 - 00:41:18.840, Speaker C: I think the proposed solution would be for the chain not to finalize and people just not attesting to the blocks. So the chain will finalize.
00:41:20.620 - 00:41:56.390, Speaker A: There's a lot of nuance to how to potentially do that. I mean, the safest version of this, to find some balance is like doing rather than n of n, do n of m, where if one fails or something, you can still continue forward, but if you have over some high threshold of failure, then you don't continue forward and try to find the balance between kind of safety and an available chain, but just not a testing is a potential major security issue in itself.
00:41:58.440 - 00:42:01.910, Speaker C: Yeah, and the other things that.
00:42:03.820 - 00:42:04.136, Speaker A: As.
00:42:04.158 - 00:42:47.130, Speaker C: I said, I think Proto said today that they already have it and they implemented it in a day. So I think it's not that much effort and it should be implementable. That's part of it. I think the guest team is definitely going to implement it and then we can see how much effort it is and then we can also see how much overhead that is. Like we don't know, we don't know how much cpu it is, how big the witnesses will be. Collecting the witness is extremely easy. So collecting the witness because you have to go through the tree anyway.
00:42:47.130 - 00:43:02.796, Speaker C: It's just whenever you touch something, you also record the touch into a map and then at the end you put out the map. So it's very easy to collect the.
00:43:02.818 - 00:43:18.080, Speaker I: Witness, but no, proving the witness is harder. If you go through the snapshot to load things, it's one call to the database. If you want to prove it, you have to go through the entire tree. So you increase latency.
00:43:19.620 - 00:44:13.600, Speaker A: Another thing to consider here is there's maybe a second order effect to at least keep in mind by going down this path, is that this might help with resilience with respect to multi client state transition, but it might actually reinforce fragility in other portions of the stack. If, for example, you have less of a reason to switch from a majority client because you get the kind of the EVM state transition resilience, then all of a sudden all of the other things become like potential DOS vectors with respect to database or crashes on P to P and things like that. If there's less of an incentive to diversify on these other things, you might actually end up with more fragility in other parts of the stack than if there were a theoretical healthy diversity across clients.
00:44:15.300 - 00:45:33.964, Speaker C: To push for this, because it's like weird guess and this will. Yeah, but I think it's the right thing to do for the healthy health of the network. And like what I think the most important thing is that we're not finalizing any invented state. This is way more important than if get has like 50 or 60% of the network. I think this is the biggest nightmare scenario, and with this proposal we have a good chance of making this worst nightmare scenario quite significantly better. Just as a heads up, it doesn't need binds from all of all the teams, but I think it would be really good for the whole of ethereum that we have these modes. Just on the topic of latency I think we could also just only do the proof once per epoch because the cross client check.
00:45:33.964 - 00:46:44.690, Speaker C: Because the main thing is you don't want to finalize an invalid block and that's just dependent on the target vote of the outer station, right? So if that target block has already been checked with all the clients, you can sort of attest to whatever head block you like. If that head block is invalid, it's not going to count towards the Casper FFG weight and you're not going to find allies. So this could be something that we just need to run at the beginning of the epoch, on the epoch transition, which is like a 32 x reduction in the number of times it has to happen. Again, I don't think that latency will actually be an issue at all because this is just cpu and at least most of the latency comes from fetching data from the disk. I think the best way forward is for some teams to implement it that have the capacity to do it and then just to benchmark it, see the numbers and see if the full approach with proving everything and being as secure as possible. If that is not feasible, then we can reduce the security.
00:46:49.560 - 00:47:06.330, Speaker D: I think I agree with Lucas. I think that in general, well, I think in general it's a good idea, but I would do it with Verco because then we can agree on the standard API and we don't have a custom API built just for that.
00:47:09.680 - 00:48:19.856, Speaker I: But like I said, for generating the proofs, you throw out the snapshots, you need to traverse the tree all the time for each read. So that increases latency in itself. So that's basically even the first processing is increased latency a lot, and then depending on how the proofs are moved and distributed, which will have some latency and then there will be some execution on the other verifying clients. And for example, if we have a very compute heavy block which can be attacked vector, again it might be a problem. Right. But again, feel free to benchmark. I'm just stating my concerns and that I think the effort is underestimated to make it actually happen on the production level because it's very easy to prototype it.
00:48:19.856 - 00:48:44.970, Speaker I: We had it in Beamsync, we have it on vertical trees right now. And I agree that just the EVM part is simple, you just need to substitute the data source. But I disagree that it's easy on the protocol level to deploy it on the network. Okay, it's not protocol even, but you know what I mean.
00:48:50.220 - 00:48:52.104, Speaker C: Sorry, was someone else trying to talk?
00:48:52.302 - 00:48:53.530, Speaker A: No go.
00:48:56.240 - 00:49:28.170, Speaker C: If we want to have the situation where we can continue producing blocks but just stop attesting. The CL currently has no way of understanding that from the ER. We just have an idea of whether a block is valid or not. And if a block is valid then we're producing blocks and attestations for that block. We almost need a concept in the engine API of like this block is sinking or unsafe for a testing, but go ahead and produce something on top of it if you want to.
00:49:28.700 - 00:49:35.770, Speaker A: But then the implications there of having like an attestationless work choice is something to think about.
00:49:37.980 - 00:49:38.730, Speaker C: Yeah.
00:49:41.280 - 00:50:41.310, Speaker A: I would motion to the fact that I think we all at least understand the proposal and can think about it and Geth and others can do r and D and bring back to Alcor devs what they learn rather than spending another 45 minutes talking about it here. Is that reasonable? Cool. Thank you Mars other items for today. I'm currently in a hotel bathroom so it's definitely not here. Anyway, hope you all are all well. If you are in person, enjoy. And if not, see you all very soon.
00:50:41.310 - 00:50:51.150, Speaker A: Take care. Bye guys. There is the awkward Devs executionary call tomorrow next week us thanksgiving. All right, bye bye.
00:50:51.230 - 00:50:52.766, Speaker C: Bye guys. Bye.
