00:00:03.370 - 00:00:44.540, Speaker A: Okay, so SSZ breakout room, I think high level. The thing we need to figure out here is how do we move forward? How do we move forward with this and when? I guess we do it. If we decide to move to SSD. Yeah, I don't know. Ethan, do you want to maybe take a minute or two and sort of recap both your proposal and Vitalik's and we can go from there?
00:00:45.470 - 00:00:54.030, Speaker B: Sure. So this all started kind of by me and Vitalik at the same time. Roughly.
00:00:58.690 - 00:01:00.080, Speaker A: I think we can.
00:01:03.750 - 00:02:34.830, Speaker B: Sorry. Okay, so we both started to push for moving those Merkel patricia trees in the execution block header to SSE. Vitalik made this post that I just linked, where he essentially discusses a way how to achieve this by creating a SSC union that has on one end a selector that tells you what type the transaction originally had, and on the other branch it has a specific type, depending on whether it's 4844-155-9293 or a legacy transaction. And that essentially gives you the full design space. But it may be challenging to use because these trees are mainly used by light clients on, for example, smart contract on a different chain, an IoT device, or some zero knowledge circuit. And for these, they don't really tend to care about those specific encoding details. They just want to know whether a transaction was sent to a specific contract and whether it was successful.
00:02:34.830 - 00:03:57.910, Speaker B: And that's why I explored an alternative approach that's currently EIP 6404, where instead of using the union, I created a super type that's the same way how it's done on JSON RPC. They have a general transaction type that can be used to represent any transaction. And the advantage here is that regardless of what type it originally was, you can access it using the same data structure. So I also created this drawing here, where I have the union based approach on the right side and the normalized transaction on the left side, just so that we can visually see how the different transaction types would be represented in both Vitalik's and mine approach. And this is essentially the point that we want to discuss, whether we want to use a approach based on unions, or whether we want to use an approach based on this generalized or normalized transaction.
00:04:02.110 - 00:04:15.054, Speaker A: Thank you. Yeah, I guess, does anyone want to sort of make a case strongly for one approach or the other? And we can sort of go from.
00:04:15.092 - 00:05:49.322, Speaker C: There'S, I think the main part of my case for unions, it's just the instinctive argument that it feels more natural doing it with one super object. Just feels like it's using a functionality for something other than what that functionality is meant to be used for. Right? Because if there are transactions that do have different types, or if there are objects that have different types, then that's just what unions are for. And if we try to put everything into one object, it feels like we're shoehorning one thing into a design that's really not very well suited for it. And the risk from this sort of stuff is that there are unknown unknowns. And in the future, especially as more transaction types get added, there's a risk that it might even become more confusing of what's actually going on in a transaction and which field should be, or how to keep changing the scheme to accommodate a larger number of set of things in the future. So I'm willing to be convinced towards something closer than model.
00:05:49.322 - 00:06:20.950, Speaker C: I don't feel extremely strongly about this, but it just feels like unions are by default the right way to do it. And I'm not convinced that some fairly niche use cases involving generalized indices are quite enough to be decisive to push us in the other direction, especially when the same field in different types of transactions could possibly even end up having different meanings.
00:06:25.890 - 00:07:24.370, Speaker D: I think I can make an opposite case, which is basically that the union is a more complex construct because it has this open ended property to it where you can keep adding types. Right. And I think that actually works against us in some ways. Like adding a new transaction type isn't, I'd call it an extreme option in the sense that we've been at this for five years now, and we should be getting closer to where we want to be in terms of transactions, and at the same time, inventing new ones has a big cost for the whole community. Right. We're seeing that now with legacy transactions, like we're never ever going to be getting rid of them, actually. So because of hardware wallets, because of all kinds of issues and smart contracts.
00:07:24.370 - 00:08:09.840, Speaker D: So I kind of feel the extends. Sure, we could be designing for a future where we create lots of transaction types, but is that an objective? If it's not, then the single type is kind of simple. It's very compact and easy to reason about. Also from an implementation point of view, because you remove one form of dynamism and you also remove this case where in your current code you kind of have to guard against the union taking a branch that you don't understand.
00:08:16.310 - 00:09:01.490, Speaker C: I think that's fair, though. I think the thing that's just worth keeping in mind is that, well, so from a compactness point of view, actually, this is probably my more substantive objection. Right. The issue is that if you have a union, then the type specific overhead is just going to be an extra four bytes. But if you have this structure where you have one transaction type and it just has a bunch of fields, and the average transaction is only using half of the fields, then that means that you would need to add an extra 20 bytes into the SSD representation just to independently specify that all of the different fields are missing.
00:09:03.990 - 00:09:51.310, Speaker B: It doesn't affect the network format. It is only for the transactions route, transactions route in the block header where the transactions would have to be normalized. But of course if you would later on create a new dev p, two p that for example uses the normalized types for purpose of exchanging the transactions as they are included into a block, maybe you could use the SSC format there as well. And then I agree that there could be this overhead of maybe an extra 20 bytes or so, but only for the transactions that do not use all the fields.
00:09:53.410 - 00:10:00.340, Speaker C: Yeah, sorry, but the average transaction is going to use far from all fields, right?
00:10:02.150 - 00:10:33.920, Speaker B: Currently the average transaction, I guess, is a 1559 transaction. And the overheads for that one would be the extra zero to specify the number of blobs that it has, and the list like an extra four bytes to indicate the offset of this blob version hashes field. So it's an extra eight bytes to represent a 1559 transaction in SSC. With this.
00:10:38.210 - 00:11:01.670, Speaker C: Um, one other actually, can you just remind me one concrete kind of very specific case? So how do pre eip one, five five and legacy transactions get represented in terms of the gas price and the base fee? Where does the facts that they are pre 1559 get represented? In this structure.
00:11:03.450 - 00:11:20.540, Speaker B: There is a TX type field that remembers the original encoding. So you can still recover the original encoding, you can recover the originally signed got it. And recover the id.
00:11:21.330 - 00:11:54.470, Speaker C: Got it. Okay. Actually, I think I having a TX type definitely makes a decoding cleaner. And on the serialization stuff, to be clear, I don't consider my own argument necessarily decisive there. Right. Because we have compression and places where data is expensive, like roll ups are going to use compression anyway, and you can easily custom compress away like 20 bytes of nothing if that appears all the time in a transaction. Hmm.
00:11:58.670 - 00:12:57.034, Speaker E: So I guess I just want to say that I also tend to fall in the union camp for a handful of reasons. One, I think it is very similar, as Vitalik said, to how we think about the transactions. Even with this unified transaction type, we still have the concept of many different types of transactions, and yes, there are places where we end up normalizing the transaction into a single message that maybe goes through the EVM. But there are a lot of times where we have to deal with transactions on an individual basis. Dealing with the legacy transactions and 1559 transactions in the MEm pool is slightly different. Just because there is no priority fee for legacy transactions, we have to make concessions for that. We're having discussions about four, four, four transactions and whether they should have blobs, whether they should not be allowed to not have blobs, how to deal with them pool there.
00:12:57.034 - 00:13:56.830, Speaker E: So I think that we have this situation where we're already dealing with transactions independently of each other, and the unified transaction doesn't get away from that. We still have many functions and types related to it that we have to determine, okay, if it's this type, then I calculate this signature in this way, et cetera. So I think for that reason, just from a mental model, it doesn't make as much sense to have the normalized transaction. Then I also think that the main reason we seem to want this normalized transaction is to have static general indices for all of the values of a transaction. And I don't even know if that's a good commitment that we can make, because the reality is we don't know what the next five or ten years are going to lead us to. And it's very likely that we need to change the transaction type. Again, we don't necessarily want to do it, but I don't think that we can reasonably make the commitment to the community that this general index is not going to change in the future.
00:13:56.830 - 00:14:27.580, Speaker E: And so if we can't make that commitment, then we need, one, to not have this unified transaction type because we don't get the value that it's trying to provide. And two, we need to provide another mechanism for getting general indices. So we eventually need to have some sort of pre compile or opcode that says, I want the general index for this type and this fork. That's something that I think needs to exist. And I think that's the right way to get general indices, not making a commitment that the general indices won't change.
00:14:31.470 - 00:15:25.530, Speaker D: Well, one thing is that we've kind of avoided the union type because it does represent this complexity where it actually over generalizes. Because if we do assume that we might add one or two transaction types, the union is absolutely overkill, right? Because it allows for and forces us developers to think about an infinite number of new transaction types that might appear. And at the same time, when we do create new transaction types. It's true we might get more wild with them if we have unions, but then we don't have to think as much about the existing transactions, which means.
00:15:25.600 - 00:15:26.220, Speaker B: That.
00:15:28.190 - 00:16:26.702, Speaker D: We'Re sending a signal to the community, basically that anything can happen here. And the example I would bring up is like intel processors, which like from 80, 86, from 40 years ago, they kind of still work. The instruction set is fine. I think part of their success has been this fantastic record of backwards compatibility. And if we now say that, all right, infinite amount of transaction types, don't worry guys, that doesn't tell a story of ossification or anything, or tells rather a story of expected anything. And if that's what we want, indeed, if we want to design for this thousand transaction typed world, then, then the union makes a lot of sense. But it does also create this complexity in the code today.
00:16:26.702 - 00:16:30.880, Speaker D: And that's how we didn't use it.
00:16:31.730 - 00:17:16.106, Speaker C: Right. It's a good point. I think one thing that we could do is go through the same exercise we went through with EOF, where we realized that, wait, the path we take actually does depend on having understanding the exact, or at least a probable structure of what changes we're likely to do in the future. So here, what future transaction types are we even likely to introduce? And I think my answer for this is, I can think of a few. Right. So one is new transaction types for contract creations. So like a transaction being able to direct, to make a create to possibly something specific to EOF code because of how EOF is going to try to ban introspection.
00:17:16.106 - 00:17:47.514, Speaker C: And so you need to do some more specific stuff. Third would be a transaction type that's designed around account abstraction. Whenever we get to transforming four, three, seven from being an ERC into something that's more enshrined. And we need an actual transaction type for user operations. Yeah. Aside from that, four would be if we go even further on multidimensional gas. Right.
00:17:47.514 - 00:18:31.960, Speaker C: So if, for example, we go full on and add four dimensions of gas, where one is computation, another is witness size, a third might be data and a fourth might be storage, then transactions might have to have four gas prices and maybe four priority fees and something like that. How many use cases is that? Like about four. I don't know if we can think of any more. So I don't know, maybe that can help understand what future we're concretely trying to build around. Do people have any other future transactions they have to have in mind?
00:18:36.490 - 00:18:37.240, Speaker B: Go.
00:18:38.190 - 00:19:33.850, Speaker F: I just wanted to point out that we can think of it on multiple layers of abstraction. As an Ethan is pointing out also on the chat, this 6404 is only about calculating the transaction route and what representation used for that. And we definitely have different representations for networking, for example for legacy transactions, right? We could have different representation, I don't know, for block storing in blocks, we could have different representation for Ng API. It probably doesn't make sense to have that many different representations. But yeah, we can think of this as different potential problems and have a solution tailored to that problem. Still, we probably don't want to have too many of those solutions because it's.
00:19:40.610 - 00:20:05.000, Speaker D: Yeah, I wanted to add one potential use case, which I'm not quite sure about, but a kind of zcash like transaction where we don't actually reveal some of the information that I think might be a major upheaval. But it's certainly a case that maybe stresses this theory of not wanting to change transactions too much.
00:20:05.770 - 00:20:43.940, Speaker C: Well, that one is fascinating, right? Because it feels similar to ERC four three seven aggregation, where we start talking about like, oh, instead of having a signature for every transaction, we're going to talk about one merged signature for all the transactions. And because that's the way to save space. And if we start doing base layer ZK for privacy, then we're going to start getting into similar kinds of collective stuff. I think there's a chance that that stuff can be pushed into existing frameworks, but there's also a chance that it is going to look different.
00:20:46.390 - 00:21:12.110, Speaker G: I like to mention something else. I totally support this idea that there should be a kind of pre compiled header for contract for giving you generalized instances according to the fork. This is unfortunately not applicable in other blockchains where things like Merkle proofs or other commitment proofs may be used over the SSD types of ethereum.
00:21:18.370 - 00:22:12.480, Speaker C: Speaking of Merkel proofs, actually one thing to keep in mind is that I think we should start thinking about early is that eventually the hash function is probably going to be changed from SHA 256 to maybe Poseidon, maybe RC, something that's more stark friendly as Ethereum at some point in the future is going to redesign itself around or reoptimize itself around the snark world. And so one other topic that is maybe relevant here, maybe relevant in the context, and definitely relevant in the context of figuring out how to future proof Merkel proofs is what we need to do to make that happen. Does that mean that we need to make a pre compile? Would we commit to SSD forever and we just make a pre compile for here is whatever the current SSD hash function is. Does that mean doing other things?
00:22:16.710 - 00:23:04.030, Speaker D: To answer that, I actually have one more idea, which is basically we could just leave the union for later. We could always introduce layer over whatever we decide now. That includes whatever we decide now as one of the union types, so that when we do indeed have a change that is expensive for whatever reason, we deal with it then. Because that even plays well with the tree structure in general. Bit uglier maybe, but that's one option. If we don't feel strongly right now, I don't think it's kind of irreversible. We just call whatever we come up with now in the legacy legacy transaction.
00:23:30.990 - 00:23:33.820, Speaker A: I'm not sure what's the best way forward here.
00:23:36.110 - 00:23:37.260, Speaker D: It's a mess.
00:23:37.710 - 00:23:38.460, Speaker A: Yes.
00:23:39.390 - 00:23:43.020, Speaker D: Can we flip a coin or something? I don't know if the next hash is even.
00:23:46.590 - 00:23:47.340, Speaker A: It.
00:23:48.230 - 00:24:40.846, Speaker B: Yeah, I mean, just want to highlight this from the chat. So this interface, this transactions route, I just want to double check whether this assumption is correct. As part of the core Ethereum, we treat it more or less in an OPAC way. There is a way how we create it that's standardized by spec. But ultimately the only ones who really care about what is part of this transactions route are consumers. They right now use the JSON RPC API or the EVM pre compiled if they are consumers on Ethereum itself. But what we are trying to create here is a way so they can validate that the answer that they are getting is actually mean.
00:24:40.846 - 00:25:50.630, Speaker B: If you already live on top of Ethereum as a smart contract, you don't need to validate because you can just ask the EVM, hey, give me this transaction value if that's important. But if it's a smart contract on a different blockchain, if it is a small wallet like metamask that tries to validate, hey, is this transaction amount correct? Or if it's an IoT device, those are the consumers of these transactions rule. And that was my main motivation there, to make it as easily possible, as easy to consume as possible. I mean, all these transaction types, they exist. The normalized transaction does not seem to restrict any future transaction types besides them just having to find a representation in both the JSON RPC and transactions route three. So yeah, maybe there are other users of transactions route, but my current assumption is it's just those essentially.
00:25:51.610 - 00:26:16.750, Speaker E: I mean, I don't really understand what you mean by consumers are just the user of transactions route. Are consumers the only users of the state route? Are consumers the only users of the receipts route or uncles, I think they are used for both consensus validation and by users.
00:26:21.250 - 00:26:52.300, Speaker B: Yeah, I mean this vertical discussion for the state route primarily the reason there is to make it more parallelizable to update as well, which is a service to user in the end for receipts route transactions route. Yeah, sure. I mean they are used in consensus to agree on things as well. That's true.
00:26:53.390 - 00:27:02.750, Speaker E: I guess I'm just like wondering what is the specific thing that you're trying to point out by saying that consumers care about the transactions route.
00:27:05.090 - 00:27:38.490, Speaker B: That's just that those light clients that are not really part of the core protocol, they live outside. But either we put the complexity on them by making it more general than necessary with the union, or we can try at least to cut the complexity and contain it within the core so that at least they don't have to deal with any of these legacy types.
00:27:43.400 - 00:28:16.210, Speaker E: Yeah, there's just a lot of things to think about here. But I don't think that there's that much of a difference between having a unified transaction object and a union. We're talking about one additional level of the try. And if we have the union then the transactions already have static general indices for the different values. It's just that you have to determine which type you're interacting with.
00:28:21.540 - 00:28:46.520, Speaker C: One thing that might help I think concretify things here is do you have a list of, or could you come up with a list of use cases for why someone would want generalized index proofs of particular values? Just to get a feel for both the scope of the issue and to what extent. Some of those specific use cases actually would be served by different approaches.
00:28:48.620 - 00:28:58.780, Speaker B: I don't have them prepared right now, but I could work on them but would have to be the next call, so to say. Or in the chat.
00:28:59.360 - 00:29:31.210, Speaker C: Yeah, or over the next couple of days. As I said, I'm not sure we need a call on it. As I said, I'm not dead set on unions in a way that I would feel really sad that they would come. I just think it's one of those irreversible choices that I want to send you a bit of exploring to make sure that we settle on the right one. But if you come up with use cases, I'm happy to take a look. I'm also happy to try to think of some myself.
00:29:33.340 - 00:29:50.940, Speaker B: Okay, I will prepare. I can prepare an IoT a wallet and maybe with Sahari's help, a Ck use case so that we can explore how they would behave.
00:29:54.900 - 00:30:19.560, Speaker E: One other thing I think is kind of strange about the unified type is that we only have a fixed number of additional fields that we can add right now with the pdfs that you generated, I think that's five additional fields that we could add. So that sort of sets a bound on what we can change. Without modifying the general indices.
00:30:20.620 - 00:30:56.860, Speaker B: I was thinking about extending SSC containers so that you can specify a maximum capacity, so you can maybe say it's like 64 or 128, and it would just zero extend until there. The same way how the lists work. So, yeah, there is a limit, but you can make it large enough to fit the design space.
00:31:06.260 - 00:31:59.490, Speaker A: One question I have is, how critical is it that we make this decision for Cancun? My understanding is that it is, and that we don't want to ship 4844 with a transaction type that in the fork after we end up sort of overriding. But is that correct, or is it possible that we ship four four four with the current design, but then in the fork after that we decide to change it? I'm trying to get, if we aren't sort of making this irreversible decision, is this something we need to figure out in the next month or two, or is this something we potentially have a year to think about?
00:32:04.790 - 00:32:29.760, Speaker E: I personally think it would be a shame to ship 4844 with some idea about how SSD is going to be integrated into the execution layer and then have a pretty big change in that in a later fork. So I would really like to have a pretty clear idea of how we're going to do this migration now or in the near future.
00:32:33.970 - 00:33:52.690, Speaker A: Yeah. Does anyone disagree with that? Okay, so I guess this probably implies, like, in the next month or so, we should try and align honest spec, especially if it affects refactoring old transaction types. Does that seem, like realistic to people? I'll take the silence as no objections. In terms of next steps. Is it Ethan, looking at some of the potential use cases, you mentioned, sharing that with the folks here and making a decision off of those use cases. Or is there something else that we should be sort of investigating in the next month or so?
00:33:58.320 - 00:34:08.584, Speaker B: There was also the way how transactions are. It's sort.
00:34:08.642 - 00:34:14.640, Speaker A: Is this what Roberto is bringing up in the chat? So, like, the hash collisions concerns?
00:34:15.380 - 00:35:41.500, Speaker B: No, the hash collisions between ShA 256 and Kejock. I guess they are not really maybe a theoretical thing, but I don't see any risk there personally. Talked to a couple of cryptographers, but I'm talking about whether, how we want to derive the signature of a transaction, because you need to find like hash the transaction in a way, and then you sign that hash and you kind of need to put the transaction type in there. You need to put the fact in that it is a transaction that you are signing and not a regular message and maybe something that tells you, by the way, this is the type five of the Ethereum spec and not the type five of the binance chain, so that it doesn't conflict with each other. So that's another open thing related to SSC in some way, because in Vitalik's proposal, you can at least have the transaction id be the part of the SSC tree, which is not possible in the normalized case. So I need this additional transaction hash field. But yeah, it's sort of related, but also kind of isn't.
00:35:41.500 - 00:35:42.370, Speaker B: Not sure.
00:35:54.610 - 00:36:44.080, Speaker A: Okay. And I guess, yeah, I feel like. I don't know in terms of next steps. Ethan, do you have the bandwidth in the next couple weeks to look into those things and potentially update your eip to sort. Yeah, look into those different edge cases. It probably makes sense to have another conversation about this in, I don't know, maybe two weeks, and then assuming we're happy with that, we update your EIP and go forward for that. With that.
00:36:46.370 - 00:37:27.274, Speaker B: Yeah, sounds good. I mean, just providing, collecting and providing those use cases. Maybe someone else also has another use case that they want to try implementing them with the normalized TX, implementing them with the union, just to see how they differ from each other. I think that's the best way to decide on these. I think this image already helps a lot that I prepared just this graphic that shows you how exactly do those types differ. But yeah, having some code would be even better. Two weeks sounds good for me.
00:37:27.274 - 00:37:28.182, Speaker B: Yeah.
00:37:28.336 - 00:37:52.100, Speaker A: Okay. And I can schedule another one of these breakouts, like literally two weeks from now at the same time. And then tomorrow on awkwardevs, we can tell people who maybe are not here that this is happening and they should pay attention in the next two weeks if they want to get involved. Yeah. Does that make sense?
00:37:54.710 - 00:38:21.230, Speaker B: Yeah, I think so. And also regarding Danap timeline, EIP 4844 forces execution to implement SSE anyway. So I think regardless of whether this will be a union or a normalized transaction, it should not risk delaying that release too much. It's just transaction route computation.
00:38:23.730 - 00:38:31.200, Speaker A: Okay, cool. Anything else people wanted to cover?
00:38:34.530 - 00:39:16.410, Speaker E: I think maybe one thing to think a little bit about is where are we going in terms of the peer to peer propagation of this information. I know that a lot of these things are still represented in the 20 718 manner. But I don't think that for the long term we really want to have a bunch of different serialization formats of transactions. It doesn't sound nice to serialize transaction for the try in a certain way, serialize it on the disk a certain way, serialize it over wire a certain way. So it might be useful to think about these proposals in the context of what if this is what's going over the wire to peers.
00:39:34.210 - 00:40:12.060, Speaker B: So for the main pool, I don't think you can change those too much from 20 718 because you always have this use case where someone signed a transaction, puts it in a box, maybe for legacy or whatever inheritance someone else will later broadcast it. I mean, you can wrap it, I guess, but the serialization in that specific RLP format probably still needs to be supported in some way, either on JSON RPC or on the mempool topics even.
00:40:14.270 - 00:40:30.050, Speaker E: I don't think it needs to be supported outside the signing hash. We could today totally move the El peer to peer stack to SSE. And if you have that rop transaction in a box, it should be a function to convert it from rop to the SSC format.
00:40:33.170 - 00:41:44.260, Speaker B: Right? Also for the transaction id. But yeah, sure, it kind of has this problem that you need to upgrade the network in a coordinated way, because regardless of it's I mean, if it's a union and you add another case to the union, it may no longer be the case that you can still load the previous specs format. For example, if a field changes from variable length to fixed length, or vice versa, it can make them incompatible. But yeah, sure, I will think about this network if it's possible to change the network to SSC as well. Cool.
00:41:45.670 - 00:42:01.140, Speaker A: Okay, anything else? Okay, thanks everyone. Then yeah, let's continue to chat on the type transaction channel and have another one of these calls two weeks from now.
00:42:02.390 - 00:42:04.000, Speaker E: Thanks Tim, thanks all.
