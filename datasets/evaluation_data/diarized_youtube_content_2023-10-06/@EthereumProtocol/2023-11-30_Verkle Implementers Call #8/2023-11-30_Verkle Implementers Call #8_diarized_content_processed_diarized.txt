00:00:02.570 - 00:00:50.640, Speaker A: Okay, welcome everybody to virtual implementers, call number eight. Starting off today with a round of client team updates. Then we will touch on the relaunch of the Kalstenin testnet. We have a proposal for a new performance optimization from Guillaume and Ignacio, I believe. Then assuming we have time, we will touch on the issue around the block hash opcode that Holger and others have recently been discussing. And lastly, touching on the vertical proof verification. Pre compile some potential new features to that eip that Guillaume created a few weeks ago.
00:00:50.640 - 00:00:58.000, Speaker A: Okay, starting off with client team updates. Guillaume, you want to start off there?
00:00:59.250 - 00:01:38.426, Speaker B: Sure. No real updates. I mean yeah, we relaunched the Calston and Testnet, so we're going to talk about this later. And apart from that, yeah, we've been identifying some bugs like Geth has some bug with debug trace transaction, so we're aware of that. We're also looking at a potential bug in displaying in the witness where the code leaf number would be incorrect. But yeah, that might be a dora problem. So we just found out today and otherwise I think that's pretty much it.
00:01:38.426 - 00:01:42.640, Speaker B: If Inaccio has some extra things like go.
00:01:45.650 - 00:01:47.680, Speaker C: No, no, I think that's fine.
00:02:01.470 - 00:02:03.530, Speaker A: Anybody else would like to give an update?
00:02:04.750 - 00:02:57.690, Speaker D: I can give an update on Ethereum javascript. So during Devconnect in Istanbul, we were able to begin syncing the old continent with the help of Geshinder, who's done most of the hard work on that. We are not building vertical state locally, but we're statelessly executing blocks and then verifying the resulting post state with the post state included in the blocks. So we're troubleshooting the blocks that have proven to be problematic. We've updated to custom and number two over the weekend, I think on Sunday. So yeah, we're just going to go block by block and see wherever the post date doesn't match what we're getting locally and make the appropriate fixes.
00:02:59.630 - 00:03:43.290, Speaker B: Yagion sorry, just a quick remark for everybody who would be interested who's trying to join the testnet. So there's no guarantee like we're still discovering if there are problems with the testnet. So if you spot any problem, write us because it could definitely be our fault. And another thing I wanted to add is you could also try to join a testnet first by executing blocks, but just verifying the state route and not verifying the gas, and then only verify the gas consumption afterwards. So do it in several steps so that we can identify bugs one by one instead of having like 15 different.
00:03:43.360 - 00:04:07.518, Speaker D: Possible sources for bugs about the gas. I know you mentioned that when we were in Istanbul, but since we're verifying, for example, the post date balances, then surely that's affected by gas usage because one of the accounts is the one paying for the gas and therefore if we don't have proper gas consumption, the balance isn't going to match.
00:04:07.684 - 00:04:21.190, Speaker B: Good point. Yeah, I forgot about this. So. Yeah, forget about that. But still, if you have problems, if there's a bug, let us know because we could be responsible. Maybe what guest does is incorrect.
00:04:22.170 - 00:04:23.142, Speaker D: Yeah, good to know.
00:04:23.196 - 00:04:23.800, Speaker E: Thanks.
00:04:24.650 - 00:05:09.786, Speaker F: Yeah, just to chime in here. And that was one of the issues that we sort of figured out on Ethereum JS stateless syncing that our gas schedule was not correct. And once basically we got some part of it right in an incremental way. Basically we were able to execute few of the blocks on Lodestar. Basically. I also refreshed the Lodestar branch and also figured out a bug around transition. So now on kurtosis, Lordstar and Lighthouse play well for the electra transition.
00:05:09.786 - 00:05:18.100, Speaker F: And yeah, so that has also been tested out. So plus one for kotosis as well.
00:05:20.310 - 00:05:29.030, Speaker B: Sorry. Yeah. Question, when you say you resolve the bug at the transition, is that genesis time transition or post genesis transition?
00:05:30.090 - 00:05:38.950, Speaker F: So it's not transition on the ELN, it's transition on the Cl fork transition from Capella to Electra.
00:05:39.770 - 00:05:40.520, Speaker C: Right.
00:05:40.970 - 00:05:50.118, Speaker F: The state upgrade of Lodestar was not matching with that of lighthouse. So there was a bug there and that was construct.
00:05:50.294 - 00:05:58.670, Speaker B: But did you test both cases like Genesis and anywhere later, like at a non zero epoch?
00:06:00.130 - 00:06:18.660, Speaker F: Yeah. So transition. I tested at epoch one with kotosis for genesis. Kotosis does not have a genesis state generator yet for the electra. So when that will be sort of completed then I can test with that too.
00:06:27.100 - 00:07:05.076, Speaker G: Okay, I can share on Bezos side. We are still trying to implement vertical tree into Bezos, so we are moving forward. Currently we are trying to import the first blocks of the genesis to verify the state route. No, we have some issue with the commitment. So we have a novel state route Guillaume shared last week, dot generating from guests to help us checking the tree structure and the different commitments. So it helped to fix some issues, but we are still some problem. So we are working on that.
00:07:05.076 - 00:07:46.908, Speaker G: We are also some performance issues, but we have some ideas to make it faster. It will not be really a big deal. We can fix some issues quickly without a lot of modification. So we are also working on. So for the next step regarding Bezo, I think it will be to pass the first block to have a valid state route. When we'll have this valid state route, we'll be able to try to join the testnet. When I'm saying joining the testnet just to import the different block to see if we can match all of the state roots for the block in the testnet.
00:07:46.908 - 00:08:26.044, Speaker G: And after, when that will be okay. We'll also have to work on tool generation and validation. But we are for the moment not really on this topic. Yes. Regarding integration to bonsai, I think for the moment it's working well. We don't have to refactor a lot of things, so it's really good news. I'm just starting a small refactor of the code to help to simplify the integration and maybe in the future to use directly the leaf of the vertical tree as a flat DB.
00:08:26.044 - 00:08:38.090, Speaker G: So just some ideas like that we are trying to implement. It's all for me.
00:08:42.530 - 00:08:56.278, Speaker H: Yeah, I can go next. So on nethermine side, I don't have any significant update. The last testnet and this testnet we've got out that there were some bugs in gas calculation just causing some issues and.
00:08:56.444 - 00:08:57.720, Speaker B: Yeah, that's it.
00:09:11.100 - 00:09:11.656, Speaker C: Okay.
00:09:11.758 - 00:09:14.040, Speaker A: If no other updates.
00:09:15.820 - 00:10:01.850, Speaker B: I would like to make one last update from Aragon. So I am starting to work on vertical, trying to implement that into Aragon's very early stages. Hopefully I'll have something concrete by the next meeting. Nice. One thing I wanted to add, so I don't see him in the list, but Ben Edginton from Tiku just wrote me at the start of the talk of the call that they decided they were going to work on vertical, so we should have something by. He said within two weeks they should have something. So then we would have three cls, which would be nice.
00:10:07.920 - 00:10:13.810, Speaker A: Very nice. And thank you, Samnath, for joining from Aragon. That's exciting to hear as well.
00:10:18.170 - 00:10:18.774, Speaker B: Cool.
00:10:18.892 - 00:10:22.520, Speaker A: Anybody else? I think we've covered everybody.
00:10:26.670 - 00:10:27.420, Speaker E: Okay.
00:10:28.110 - 00:11:01.330, Speaker A: If not, we can move on. Next up, just a few quick notes on the relaunch of Kaustinan, which we've already covered. And thanks to the ops team for putting this all together. A couple of notes. The legacy name of Kowstonin testnet has been dropped in favor of Verkel devnet x and verkel gendevnet. In this case it's vertical gen two. This means the genesis state is verkel.
00:11:01.330 - 00:11:33.760, Speaker A: There was no state transition from Merkel to Verkel. In the near future. The goal will be to deploy a testnet with a Merkel genesis and then do the conversion over to Verkel. One last note, this current testnet is still built on top of chapella. So in order to sync for anyone that would like to sync in the future, you have to make sure that Dankun related code is just not being triggered here. Anything I forgot to mention there, Guillaume or anyone else?
00:11:35.010 - 00:12:19.066, Speaker B: Right, there was something I wanted to add, but I just don't remember. I think you said vertical gen devnet like you said vertical gen two. It's actually vertical gen devnet two, but kelstin and Testnet, like the old address, still works. And in fact we'll try to keep it going. Like as we update testnets, we will keep the calstin and name to follow the latest. But yeah, if you want to target a specific testnet, obviously use the canonical address, but we'll give you an indication of what the latest one is. One thing I wanted to add is that we currently have a.
00:12:19.066 - 00:13:14.266, Speaker B: Is that the right one? Yeah, so we have this landing page, we have an explorer, actually a regular block explorer that is currently not working very well, but we started having this data. I'm not sure why Google Meet doesn't update, but yeah, if you click on Explorer here it should show you the list of blocks and transactions. But we're currently stuck at 3000 and not all the transactions are included. So it's still work in progress, but we're getting there. So that's quite exciting thing because we will be able to help debug much more easily. Like if we can get the transactions and our content, that's going to be helpful. So that's early, but if you are thinking at the beginning, it's actually usable.
00:13:14.266 - 00:13:24.500, Speaker B: If you find your problem much later, like after block 3000, it's not going to help you just yet, but hopefully we can get it. Yeah, that was it.
00:13:29.110 - 00:13:41.670, Speaker A: Cool, thank you. Moving on then. Diom, I believe you wanted to discuss your new proposal for the performance optimization for the conversion.
00:13:42.410 - 00:14:04.506, Speaker B: Right, give me a minute. Too many windows, I don't see it. Extension. Oh, right, I'm not looking. Right. Okay, so here we go. So yeah, it's a proposal.
00:14:04.506 - 00:14:06.240, Speaker B: Can you guys see my screen?
00:14:11.360 - 00:14:13.100, Speaker A: Okay, yeah, we can.
00:14:13.250 - 00:15:04.590, Speaker B: Excellent. So yeah, it's a bit of an optimization whose proposal is to increase the speed of the transition. And it's done by noticing that there are two special cases that we could optimize. So we could change the structure of the tree a little bit to optimize the computation time by reducing the amount of computation that is needed. So just as a quick reminder, this is the structure of the vocal tree, you have the root branch node, then you have a series of other branch nodes, and then you get into the extension node, and the extension node has, the first value is one. Then it's the encoding of the stem. And then you've got two serialized subtree root nodes, c one and c two.
00:15:04.590 - 00:15:50.728, Speaker B: And for example, in the case of an EOA, of which there are many in the state, you always have version zero right now. So maybe the version will change in the future, but currently it's always zero. And at the time of the transition, it will always be zero. You always have the balance, and the balance is also 32 bit, little indian number. But you will never use those 32 bits because the whole issue of issuance of Ethereum does not fit in 32 bits. It does fit. It's definitely not even close to filling 32 bits.
00:15:50.728 - 00:16:20.436, Speaker B: Sorry, 32 bytes. The nonce itself is a 64 bit number, so it's also going to fit in 253 bits. And then the codecatchack, yes, unfortunately, does not fit in 253 bits. It's a 32 byte value. But we know what it is. It's well known, like the empty code. And the code size is always zero, and then everything else is empty.
00:16:20.436 - 00:16:57.250, Speaker B: So that gives an opportunity for optimizing this approach. And what I would propose is, when we detect that this is an EOA, we would use instead the following structure. So we would still have an extension node, but no value node. We would set c one and c two to zero. We would change the marker from one to three. And the reason why we don't use two is because two is used somewhere else. I can't remember it right now, but it's definitely used somewhere else.
00:16:57.250 - 00:17:33.420, Speaker B: So we would put the balance inside the polynomial at the next degree, at the next available value, available location. Sorry. And same thing with the nons. And then we would set zero for another use case that I'm going to talk about next. But what you have to notice is that in the general case, because the value is potentially 32 bytes. Sorry, 32 bits. Again, it doesn't fit inside a single field element, so you have to break it in two.
00:17:33.420 - 00:18:46.816, Speaker B: But in this case, you know that it fits in 253 bits because the nonce is once again 64 bits maximum. And the balance itself, well, technically, it could go all the way to 32 bytes. So 256 bits. But, yeah, it's definitely not going to happen. Like, there's more than 100 orders of magnitude more room for growth before we hit this use case. So in all likelihood, it will never fill more than 253 bits, and as a result, it could fit into a single field element. So we would save on the evaluation of twice as many values, because here we would have to break all those values into two field elements and then evaluate as many polynomial value, like evaluate the polynomial at as many location as you find, whereas here it's only two extra locations, and then you don't have to evaluate anything below the extension node.
00:18:46.816 - 00:19:37.376, Speaker B: So you save quite a bit on the amount of evaluation, and as a result, it should make things faster. There's also another case that I believe, although I haven't confirmed, is still quite likely. It's the case of a single slot. So the typical case is when you have an ERC 20 contract, and they decide to store the values into a hash table and a solidity map. And this is a terrible approach for Verco, because all your gas model is based on groups and grouping of consecutive values. So you will find very often values that are alone in their own group. And this is already something we see on the testnet.
00:19:37.376 - 00:20:40.650, Speaker B: So this is why I believe it's going to be likely, although I haven't seen that with a real payload. But so what you would do in that case is use the marker four instead of three or one, and you would still set c one and c two to zero. Balance and nons would also be set to zero. And then the next values will be set to the lower 16 bytes of the slot value, and then the next 16 bytes of the slot values. So you still have two evaluations here, but you're saving on one extra evaluation, because you don't have to evaluate c one or c two and serialize them before you evaluate this lot. So this one, I'm less sure that it would be positive, but yeah, I think if we do this and if there's buy in, of course I don't want to decide this on my own. I think it would make sense to try that.
00:20:40.650 - 00:20:49.080, Speaker B: We don't have to do it now. I was not planning to work on it this year, but I think it would save some time during the conversion.
00:20:49.160 - 00:22:00.832, Speaker I: Yeah, Goti, just a quick remark. I mean, I'm not sure about the later optimization, but if you go back to the case with the version of three. Can I go back one slide? There is one thing here that makes me a bit cautious of this optimization, or something to keep in mind. I mean, it costs a lot of extra additional complexity, which is of course the downside. But I'm not sure how big the upside really is, because if you have a leaf node that is, can you go back one more slide? Even if you have a leaf and, well, basically the leaf node below the extension of this form, then actually computing the Pedersen hash for this special case is already much faster than the general case. The reason being that in order to evaluate the Pedersen hash, you would want to pre compute. I mean, you have your fixed Pedersen basis and you actually compute your pedersen hash by using pre computed values.
00:22:00.832 - 00:23:02.090, Speaker I: And one of the ideas when we did this kind of thing is that the number of precomputed values, because of the presence of the extension nodes, we already have much denser precomputed values for the first four or five entries because of the extension nodes. So something that is already contained in the only few first few, where all but the first few of the values are zero, can already be made much faster by making just an appropriately shaped size of the pre computation table for the Patterson hashing. So you don't gain as much as you would do if you were. So there's basically some other way of doing this optimization that does not add as much complexity. This one probably helps you more, but on the complexity versus how much you gain, trade off, it's probably not as good without taking this into account.
00:23:02.860 - 00:23:35.840, Speaker B: Okay, yeah, that's possible. I'm still planning on gaining. Once again, this is just for the conversion, right? I agree it's a bit more complex, but if we save, let's say, even one person of the time per EOF, and we have millions of EOF, that's going to have an impact. But okay, fair enough. I could try to implement that and see how much of a gain there really is. Inacio.
00:23:37.960 - 00:24:06.780, Speaker C: And I have a question. When you say this proposal is for the conversion, are you thinking about only applying these rules for the converted state, or is this like a general thing that will continue to be used for new EOA addresses after Berkeley trees are fully migrated? Because this is actually, in theory optimizing every case of eoas, not really the state transition.
00:24:07.520 - 00:24:26.930, Speaker B: Yeah, so what I meant was that it would have a noticeable impact. It should have a noticeable impact during the conversion, then it doesn't really matter as much. But if it works, we would have to support this structure forever, so why not use it as well in the future?
00:24:29.480 - 00:24:52.312, Speaker C: Okay, I have another question regarding the single storage slot case, if you want to scroll down. So I think that here, are we missing, like which index is the corresponding storage slot in this if node? Because here we are only going the value.
00:24:52.366 - 00:25:11.072, Speaker B: Not really. That's a good point. Yeah, that's missing. So that might be actually an extra evaluation and kind of completely, then it would not make any sense to use that thing, although the index is only a single byte, so you could actually stuff it somewhere in v two n or v two n plus one.
00:25:11.126 - 00:25:11.440, Speaker E: Yeah.
00:25:11.510 - 00:25:29.716, Speaker B: So we could still rescue the feasibility or the advantage of doing this this way. But yeah, I'm not really a believer of this one. I just want to try it and see if it works. But yeah, I wouldn't be surprised if.
00:25:29.738 - 00:25:58.864, Speaker E: It brought nothing a single account thing has. Also the problem that it induces, because there might be another value written to that same tree, and then you would have to make a proof for the transition. I don't know right now, but I feel like that could be more complex and that would be quite annoying. Complexity, right.
00:25:58.902 - 00:26:43.292, Speaker B: That's a good point. There are some rules, like the way the spec is drafted. It says there's a series of rules, and if this rule change, then you have to fall back to the old concept. So I don't know if it would actually cause a problem for the proof, because you would have a proof of absence for any child of c one, c two, and then you would update this to be zero. So, okay, technically you would update this to be one, exactly like we do deletions in leave values, and then we would have some extra children. So. Yeah, that's a bit of a corner case.
00:26:43.292 - 00:26:51.232, Speaker B: That's not really nice. Yeah, well, all of these things just add up.
00:26:51.286 - 00:27:53.030, Speaker E: That's just something to remember here. Another thing to consider on the whole proposal is. Okay, so when Vitalik designed it, the idea was that we designed the whole thing as a universal 32 byte key to value table. So I guess we also have to be careful if we're breaking that abstraction. His whole point was, if we ever need to do this again, we should try to make it easier. And basically, if, for example, we could keep the keys and values, then the transition would not leak outside. The pure commitment and storage code.
00:27:57.160 - 00:28:04.010, Speaker B: Would not leak. What would leak outside? You mean the keys and values would not leak outside?
00:28:05.020 - 00:28:29.010, Speaker E: No, I'm saying the whole thing was designed as an abstract key value store. Yeah, I was wondering if this breaks it, but yeah, probably it doesn't actually break it. Okay, this objection might not be that important.
00:28:29.940 - 00:28:52.920, Speaker B: So one thing that indeed Vitalik suggested and is missing from this model, but it can be added, was that we should also put the pre image at this level, so that if we need to do another conversion in the future, we already have the pre image. Yeah, that's doable. It's just missing from this picture.
00:28:53.580 - 00:28:54.330, Speaker E: Right.
00:28:57.900 - 00:30:00.060, Speaker C: One extra thing to mention is that doing this kind of change should also have a little impact in the proof generation, in the witness generation. Because now in all the cases for leaf nodes, we are assuming that the extension marker is one. I think this can be easily fixed by a new extension status present. One for extension marker one and other for three, another for four. So I think it shouldn't be that bad. But today we don't have all these separate cases for lift nodes, so it will add a bit of extra complexity in the witness generation too. And also, one last thing, I think in this extension marker four, we can simply remove this t one, c two balance and non being zero, because four already set up a new format so we can move.
00:30:00.060 - 00:30:04.030, Speaker C: I mean, all these things aren't really adding anything.
00:30:05.940 - 00:30:42.990, Speaker B: Yeah, so the reasoning for that, but I agree it's not very strong, is that if you want to do like express this as constraints, you could just confirm that this first column is four and then if it's four, then the other two are zero. But it's true that the column being four is already enough of a constraint. So that was the reasoning. But yeah, I agree, it's not very thought throw. It's just an intuition. Kareem, you had a question.
00:30:44.080 - 00:30:57.890, Speaker G: It was just regarding the size of the storage. Do you have some number? How much would be the difference regarding if we are saving the empty, empty storage route and if we are not saving that?
00:30:58.740 - 00:31:16.756, Speaker B: No, I do not have numbers so far. I'm just expressing the idea because it's going to be quite a lot of work before I can even get numbers and benchmarks. So I'm just trying to get some early feedback to see if it's worth spending a month on this or not.
00:31:16.938 - 00:31:24.440, Speaker G: Because I think it will be also another advantage to do that to save some space transition.
00:31:24.940 - 00:32:02.230, Speaker B: Right, but that's really not, I mean, this is something you could get anyway if you just detect that pattern at the moment you've write it to your database. And even if we don't do this structure, this tree structure, this is what guest would do, for example. So just realize, oh, okay, I'm actually storing an account here, so I will not bother storing the empty code hash, for example. So this is a database level optimization that is definitely very interesting, but doesn't need to affect the structure of the tree proper.
00:32:11.160 - 00:32:19.396, Speaker H: If we just think about the transition, like while transition, we are only doing a batch of leaves for one block.
00:32:19.428 - 00:32:20.010, Speaker B: Right?
00:32:20.940 - 00:32:48.400, Speaker H: Overall, I understand that this could result into huge difference in performance but I feel that this will not result, not give us a lot of performance improvement. When you just think about a small batch of leaves, like the amount of leaves we want to convert for one block and aggregated, it would make a difference, but it won't make much of a difference for one particular slot of transition.
00:32:48.920 - 00:33:07.720, Speaker E: Generally we should be thinking in relative improvements, not in absolute improvements. Right. When you said, oh, maybe this is only 1% improvement, I was like, well, I'm pretty sure we can still find bigger improvements elsewhere that don't introduce this amount of complexity.
00:33:09.260 - 00:33:12.860, Speaker B: Yeah, understood.
00:33:13.520 - 00:33:19.564, Speaker E: If you claim those 1%, I would be like, well, that really doesn't seem worth it.
00:33:19.762 - 00:34:14.590, Speaker B: Okay, fine, then I still think it's worth trying because, yeah, it's not going to work for sure. If in your batch you have a huge contract like cryptokitty or gas token or whatever, clearly your batch will find itself in the worst case. But if you have a series of smaller accounts, I'm sure it's going to have a big impact. So I guess ultimately now we need to try it. I understand there's potentially some cases, or maybe a majority of cases that would completely annihilate the point of doing this, but, okay, I guess at this point now it needs to be tried and see if it works or not.
00:34:20.980 - 00:34:36.150, Speaker I: How do you even get your 1% number from? Or why even in the best case, assume that every contract actually has this form. What's basically your estimate? Even in the best case, how much this actually would give?
00:34:37.000 - 00:34:50.250, Speaker B: I have no estimate. I just say 1% like that. If it's 1%, I still think it's worth it. If it's less than that, definitely not. If it's more than that, yeah.
00:34:52.620 - 00:35:12.764, Speaker E: Why do you think 1% is worth it? That's just noise to me, because it's noticeable. So you're saying if it's going to take us like 100 hours to transition and this makes it 99 instead, that would be an important improvement to prioritize.
00:35:12.892 - 00:35:18.950, Speaker B: It's not going to take us 100 hours to transition. It's going to take us way more than that.
00:35:19.480 - 00:35:37.128, Speaker E: But even then, the relative thing matters. Do you not think if it would be worth more? Like spending ten more days on optimizing, that seems much likely that it gets more than 1%. I don't get it.
00:35:37.294 - 00:36:04.944, Speaker B: Okay, but do you have any other suggestion to optimize? Because right now we're scraping, as far as I can tell, we're scraping the bottom of the barrel here. So if you have any ideas, do share them. But yeah, I think 1% of what we estimate to be 700 hours. So that would be 7 hours. That's noticeable. 7 hours.
00:36:05.142 - 00:36:31.770, Speaker E: It would be worth it if it didn't create any extra complexity. But this creates extra complexity that leaks. That's not just like inside the code, but that's actually part of the specification. That seems like a bad trade off to me. I mean, you can test it, it might be more than 1% sure. Like if it's 10% then it's something different. But at 1% I would give it a very clear no.
00:36:33.520 - 00:37:31.630, Speaker B: Okay, understood. Carlos. I ignore a bit why we have this kind of index at the beginning, aside from coming one, but how worth it it is to include this in the sense from the ZKBM perspective, and I'm not sure from the IPA one, but looks a bit weird. Also, if we have different cases for that, we either need to have padding for all of the possible cases and we need to check the worst one always. And for the IPI perspective, you still should do the same exact operations everywhere, because otherwise you need a different proverb key and verifier key to commit to your proofs. So I'm not sure whether is it worth it to have this prefix in multiple cases for any reason or anyone can explain.
00:37:37.680 - 00:37:38.284, Speaker E: It.
00:37:38.402 - 00:38:07.770, Speaker B: Okay, yeah, I think that boils down to the same thing. Okay, let's say one person is not enough, but let's say if we get 10% and clearly it's worth it. ZKVM is one thing, but right now we're talking about user experience for actual users. So clearly I'm not working on the ZKVM. That's something I'm quite happy to pay if the user is willing to do this.
00:38:09.340 - 00:38:33.360, Speaker E: Yeah, I would also be careful with that. Like 10% is clearly worth it. I think we have to remember that the transition is a one time thing, so overly optimizing for that doesn't seem worth it. I would say if it's 10% of the performance in actual operation, that would be a better argument.
00:38:38.900 - 00:38:44.972, Speaker B: All right. Yeah. Okay, I guess now the question is, yeah, go team.
00:38:45.126 - 00:39:39.220, Speaker I: Well, I mean, to reiterate what I just said, mean like what Dankra said, I mean, there is probably better optimization targets for the actual transition. Like as case in point would be optimize your MSM computation for the special cases that appear. Well, that will give you almost some back of the envelope calculations are just that. It will give you almost everything that this proposal gives you at the cost of basically not needing to change anything. In the specs. Well, what this, however, would probably help you with is actually that your witnesses are a bit easier to generate because essentially the depth of your tree is more shallow. If you do this, then I think targeting this for the transition is probably not the right target.
00:39:39.220 - 00:39:48.136, Speaker I: It's maybe interesting for the witness generation, which is more for the operational thing that comes later and not for the.
00:39:48.158 - 00:39:59.260, Speaker E: Transition, and it potentially saves one commitment for these witnesses. Right? Is that right? So that could be actually quite substantial.
00:40:05.360 - 00:40:18.368, Speaker B: Okay. Yeah. Okay. Thank you. I would like to move on because I don't want to hog the whole call for this. There are still a few things on the agenda, but thank you for the feedback. It's duly noted.
00:40:18.368 - 00:40:46.190, Speaker B: And I guess I will spend some time next year just trying to figure out if it's really worth it. If it's indeed not saving anything, it's not worth bothering doing it. But if we get into, let's say, 10%, 20%, whatever, we can have this conversation again. But yeah, once again, thank you for clarifying all those points.
00:40:53.630 - 00:41:44.860, Speaker A: Okay, moving on. Sorry, bring up my notes. So, block hash opcode. How do we get statelessness to play nice with it? Which is an opcode that can be used for those that don't know to query the hash of the past 256 blocks. But in a stateless world, it cannot be assumed, of course, that clients will have these past 256 blocks. So for a stateless execution of blocks, that would mean a client would always need to wait around an hour, approximately. A few potential solutions include mappings in the witness, though this will of course add a nontrivial amount of extra data to the block size, something like 32 bytes times 256.
00:41:44.860 - 00:42:30.220, Speaker A: Another potential solution is we can move the mapping into the state tree. We can get more into this as a potential solution. And then lastly, before we get into discussion, Holger suggested as a potential option worth discussing, what about lowering the block hash range from 256 to something like 50 blocks or less, which would mean stateless clients only have to wait ten minutes or less. Though this likely also will break existing contracts. So, yeah, just by way of introduction, I guess I'll hand it over there. Guillaume or anyone else, any thoughts on this?
00:42:33.720 - 00:43:03.420, Speaker B: I do have a comment that actually, you don't need to put every single block hash. You need to only put, like, if you choose to put the block hashes in the witness, you only need to put those block hashes that you are accessing during block execution. So I know there's a bit of a finicky aspect, because how do you prove that these block hashes are correct. But yeah, ultimately you only need to pass those block hashes that are required for execution.
00:43:06.820 - 00:43:15.410, Speaker E: We could simply put them in the state. Right? Like we could have an address where we just always have a.
00:43:18.440 - 00:43:18.964, Speaker B: Sort of.
00:43:19.002 - 00:43:43.930, Speaker E: Walking history of the last 1256 block hashes. Yeah, we could do that address would write one location every time, and whenever you read from it, you would add a proof. So reading would be quite cheap. It's basically just like reading a warm address.
00:43:46.560 - 00:44:05.776, Speaker B: Currently you need 256 values, so you would want to put 256 hashes. And that's what Ignacio said about ring buffer. I get it. You will only replace one location at a time and circle through, right?
00:44:05.878 - 00:44:06.576, Speaker E: Exactly.
00:44:06.758 - 00:44:07.392, Speaker B: Okay.
00:44:07.526 - 00:44:17.590, Speaker E: That's like the standard in the beacon chain spec. We have a few things like that. I think we also have like a history of live block hashes. That works like that.
00:44:21.000 - 00:44:40.456, Speaker B: Okay. That works into the witness, you will only add the latest block hash or the previous block hash, and then any extra block hash that you read and.
00:44:40.638 - 00:44:42.284, Speaker E: Yes, correct.
00:44:42.402 - 00:44:50.290, Speaker B: What would be the extra, like the stem for this? Could we pick something like, no, not zero, because people could write there. What happens if someone.
00:44:50.980 - 00:44:52.690, Speaker E: Well, you can't write at zero.
00:44:54.260 - 00:44:55.836, Speaker B: Yeah, exactly. You can't write. So there's.
00:44:55.868 - 00:45:02.004, Speaker E: You would have to have write a pre match. But yeah, we could just reserve an address for this.
00:45:02.202 - 00:45:02.950, Speaker B: Okay.
00:45:03.400 - 00:45:45.488, Speaker E: But I'm not sure. I think Vitalik tried this for the. He was like, oh, we should establish that we can just put stuff into the state. When we introduced the EIP 1559 like mechanism for the blobs and people, we didn't like it. And I think it's basically replaced by just putting it all into the block header, which, I mean, I think. I don't actually think that is the right decision, but that was the practical outcome that people really didn't like. This extract complication of putting stuff into the state.
00:45:45.488 - 00:46:15.304, Speaker E: But here it's a bit different because it's like a lot more data, right. It's like actually a few kilobytes. So it wouldn't be practical to just add all of them to every block header. For sure. You could also simply add one pay doesn't commitment to all the block hashes to the block header. That would be the alternative if you didn't want to add it to the state, and then you would just prove it from there. So it wouldn't be at a random state location, but like out of band.
00:46:15.304 - 00:46:17.020, Speaker E: An extra pay doesn't commitment.
00:46:19.840 - 00:46:29.040, Speaker B: Right? Yeah. So that would be an extra block. You would. So you would pass a second proof to open the Pederson commitment.
00:46:30.980 - 00:46:37.368, Speaker E: It's actually not an extra proof. You can open any number. Our multi proof is universal.
00:46:37.404 - 00:46:37.556, Speaker B: Right.
00:46:37.578 - 00:46:56.730, Speaker E: It's just the proof for opening many different payers equipments, right? Yeah, but it would be added to the witness, so it's kind of weird if we added it to the block header. I feel the more elegant version is to add it to the state.
00:46:57.260 - 00:46:58.330, Speaker B: Yeah, I agree.
00:47:00.940 - 00:47:26.530, Speaker E: Yeah. It would be quite cheap. Although I guess it's a bit attackable because people could just try to make that particular address as deep as possible into the tree somewhere, which we know has limits. It's not going to be terrible, but yeah, it's going to make our witness a few hundred bytes bigger. I guess. It's not the end of the world. Stick to that.
00:47:31.160 - 00:47:47.156, Speaker B: You say it's attackable, but if you write the new block hash at the end of the block, then someone could try to. Okay, someone could try to write to similar addresses.
00:47:47.188 - 00:48:19.716, Speaker E: I mean, they would just basically make the tree much deeper. That would be the attack. Like, you try to write lots of stuff that's as close as possible to that address where we're writing stuff. And then instead of being depth four or so, which would be probably like, if you just had a random address, it would suddenly. I don't know, what's the best they can do? Probably like 60 bit, which is 64, would be like maybe depth eight or so instead. So that's not a big deal if they do.
00:48:19.738 - 00:49:01.730, Speaker C: That's also, I was thinking, it seems like adding it to the state seems to be the most elegant solution. But whenever. I think this means that for a stateless client to validate a new block, it should also have the Previous block hash from somewhere where. Now, we were only thinking about this because I guess in this current block, the previous block hash is the thing that gets included in this ring buffer. Right.
00:49:04.420 - 00:49:13.510, Speaker E: Okay, wait. Okay, but you have the previous block hash because each block contains the previous block hash anyway, right?
00:49:14.040 - 00:49:16.740, Speaker C: Yeah, but when you are starting from scratch.
00:49:18.280 - 00:49:24.250, Speaker E: No, block n contains the block hash of block n minus one.
00:49:25.900 - 00:49:30.760, Speaker C: Right. But when you are a status client and you start from scratch.
00:49:31.100 - 00:49:31.850, Speaker B: Yeah.
00:49:32.640 - 00:49:43.372, Speaker C: You should have the previous state root from a trusted source because you haven't reverified a block. You are kind of starting now.
00:49:43.426 - 00:49:50.670, Speaker E: Okay, fine. So let's assume that you get block hash number n.
00:49:52.400 - 00:49:52.956, Speaker B: Yeah.
00:49:53.058 - 00:50:10.070, Speaker E: Okay, you're. You start verifying block n plus one. N plus one will have the block hash of n. Yes. So you can use that to update it.
00:50:10.780 - 00:50:48.710, Speaker C: Right. So your starting data for validating this n plus one block should be the state root of n block n, and the block hash of state n of block n. Until now, we only expected the stateless client to get the state root of block n from somewhere. But this also adds the condition that you should also have the block hash of state of block n. Right. Because you will need that value to mutate the tree block n plus one.
00:50:49.240 - 00:50:52.932, Speaker B: But if you have an old block, it will be also part of the.
00:50:52.986 - 00:50:53.348, Speaker E: Sorry.
00:50:53.434 - 00:50:57.350, Speaker B: If you need to access hash, it will be part of the witness, right?
00:51:00.140 - 00:51:02.010, Speaker E: No, he's not talking about that.
00:51:02.860 - 00:51:21.900, Speaker C: The thing is that for this M plus one block execution, you need some data that is outside the EVM world is a writing the tree from a block hash value that you don't really calculate in the EVM.
00:51:22.800 - 00:51:53.240, Speaker E: Okay, so ignatio, you do need the block hash. I think you do start from the block hash, because otherwise block n plus one could build on the state that you were given, but not on the block that you were given, which is also wrong. That would also be an incorrect chain to follow. So in all cases, I think the correct thing is actually not to build on a state route. The correct thing is to build on a block hash.
00:51:53.740 - 00:51:54.152, Speaker B: Yeah.
00:51:54.206 - 00:51:55.930, Speaker C: Okay. Yeah, that makes sense.
00:51:57.020 - 00:52:09.740, Speaker E: And actually, if we want to see blocks in complete isolation, not depend on the previous one, then we should just put the old state root and the new state root into the block.
00:52:11.280 - 00:52:11.788, Speaker C: Right?
00:52:11.874 - 00:52:12.220, Speaker E: Actually.
00:52:12.290 - 00:52:49.336, Speaker C: Okay. When you evaluate block m plus one, and if you take as your starting source of truth the state root of that block, it is true that that state root actually is like the state of the world on multiple potential blocks that have the same state root. But this condition actually constrains not like every potential block that led to this previous state, but actually a concrete block, which is interesting.
00:52:49.438 - 00:53:32.090, Speaker E: Well, the problem is rather that the following block can just only build on the block route. Sorry, on the state route, but not on the actual chain, which is invalid. So the correct thing is we want a block that builds on the actual that builds on the chain. And yeah, there's always some things. I think maybe we probably need the complete header, to be honest. Because, yeah, you always want to verify that some conditions between two different blocks are fulfilled. And even with statelessness, you still need the header chain for that.
00:53:32.090 - 00:53:50.860, Speaker E: Like, yes, you do always want the condition that the post state root of the previous block equals the prestate root of the next block. Right. That's a fundamental thing you want. And there are more conditions like that. For example, there are all these EIP one five nine values that need to be updated correctly.
00:53:54.820 - 00:54:16.630, Speaker C: Another thing I wanted to touch on is if there is any other opcode that has this kind of stateful assumption apart from blow hash. Because maybe we have the same problem. Right. So I guess we need the same kind of solution for that one.
00:54:17.560 - 00:54:18.790, Speaker E: Sorry, for what?
00:54:23.020 - 00:54:28.040, Speaker B: For post Denkun, you also need to be able to provide the last blob hashes.
00:54:30.700 - 00:54:35.868, Speaker E: Wait, is there an opcode for that? I don't think so.
00:54:35.954 - 00:54:40.350, Speaker B: I'm pretty sure there's an eip for it.
00:54:43.520 - 00:55:08.528, Speaker E: Yes, I know that. For the blob hashes, the way I specified it, they are per transaction. Like, you can only access the one that corresponds to your. To the transaction where it was included.
00:55:08.704 - 00:55:09.430, Speaker B: Okay.
00:55:09.900 - 00:55:13.480, Speaker E: I am not aware that that has been changed.
00:55:14.220 - 00:55:23.804, Speaker B: That's not what I remember reading, but. Okay, I'll look it up, see if it's really a problem. I don't think it's stateful boundaries, so.
00:55:23.842 - 00:55:25.820, Speaker F: It should be within the same block.
00:55:35.600 - 00:55:52.710, Speaker E: It would be worthwhile for someone. I mean, yeah, we were like. I also wasn't aware of block hash, so probably it would be worthwhile for someone to go through all the opvotes and see if there's anything else that acts with this historical stuff.
00:56:07.820 - 00:56:16.156, Speaker A: In the last two minutes. Do we want to quickly touch on the vertical verification pre compile or you want to save that for next time?
00:56:16.338 - 00:56:30.296, Speaker B: Yeah, I think it's getting a bit. I have to add some update to it, so let's skip it for, like, it's only in two weeks, so we can touch that then. Okay? Yep.
00:56:30.348 - 00:56:35.510, Speaker A: And reminder for everybody we will switch to every two weeks going forward, like guillon just mentioned.
00:56:38.840 - 00:56:39.444, Speaker B: Okay.
00:56:39.562 - 00:56:44.010, Speaker A: Anything else in the last minute, otherwise we can end there.
00:56:49.870 - 00:56:50.730, Speaker E: Okay, great.
00:56:50.800 - 00:56:53.690, Speaker A: Thank you, everybody. Carlos.
00:56:54.430 - 00:56:55.180, Speaker B: Sorry.
00:57:00.190 - 00:57:02.460, Speaker A: Yeah, no worries. Okay, thanks, everybody.
00:57:03.710 - 00:57:11.920, Speaker B: Thanks. Bye. Thanks, everyone. Bye. Um.
