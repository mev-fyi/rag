00:00:00.330 - 00:00:25.046, Speaker A: You okay? Let's get this started. 15th. Implementers call for Ford. Four. Four bunch of spec updates to go over quickly today and then we can discuss client statuses and anything else people feel we should bring up, I guess. Yeah. To start.
00:00:25.046 - 00:00:31.014, Speaker A: Danny, you were saying you want to quickly cover Frida blobs decoupling you?
00:00:31.212 - 00:01:13.682, Speaker B: Yeah, things are looking good. Simulation results as discussed on ACVC last week actually look really good in terms of delivery times and network load. We're closing in on it. I think we generally have agreement on how everything is going to be structured and we're just kind of nitpicking. The intention would have to have this done on Thursday for any last commentary on ACde and then a release on Friday with this and other minor changes. So if you haven't taken a look and you intend to, now or tomorrow is a good time. So kind of definitely in the last little cleanup phases, nothing structurally we expect to change.
00:01:13.682 - 00:01:15.940, Speaker B: So kind of last call. Check it out.
00:01:17.590 - 00:01:27.400, Speaker A: Cool. Should we bring it up on awkward devs explicitly Thursday or do you just think it'll be merged by then?
00:01:28.090 - 00:01:37.114, Speaker B: It should be merged by then and maybe on all core devs. I'm going to just say we're cutting a release tomorrow with this.
00:01:37.312 - 00:01:37.722, Speaker A: Yeah.
00:01:37.776 - 00:01:42.270, Speaker B: If anyone has problems with that, they should speak.
00:01:42.420 - 00:01:46.720, Speaker A: Yeah. And I assume this doesn't change anything on the Yale side, right?
00:01:48.290 - 00:01:58.370, Speaker B: Correct. It does not. So it's not perfectly synced for the call and I don't expect any issue. So release on Friday.
00:01:59.430 - 00:02:04.290, Speaker A: Cool. Any other thoughts? Comments on freedom lobs?
00:02:06.890 - 00:02:15.670, Speaker C: No, I just say great job getting this wrapped. I feel like we're almost exactly on the schedule we set in Interop, so pretty impressive.
00:02:16.490 - 00:02:21.980, Speaker A: Yeah. Do any clients have this prototyped or implemented yet?
00:02:23.870 - 00:02:39.550, Speaker C: Prism has started it, but I stopped because of seems like there's a few moving pieces, such as different signing domain and such. But yeah, I can pick it back up this week. Tim, site is getting very close, so yeah, very, very excited.
00:02:39.970 - 00:02:40.720, Speaker A: Nice.
00:02:42.930 - 00:03:11.290, Speaker B: And we'll have pop in his simulation framework, which actually runs like a full client, I think generally biases to running lighthouse. He's going to take the old version and the new version when they have it and also run sims on it to further validate. Every simulation framework has its own caveats or corners being cut, so just to continue to validate that this is the right direction.
00:03:13.870 - 00:03:46.790, Speaker A: Nice. Anything else? Okay then. Yeah. Ethan, you linked to a bunch of different small spec updates. Yeah. So I'll just sort of go through these. But the first one was basically dealing with zero blob transactions and how it relates with E 68, the announce EIP.
00:03:46.790 - 00:03:51.020, Speaker A: Yeah, you want to get some quick context there?
00:03:51.870 - 00:04:31.522, Speaker D: Yeah, sure. So right now, when we use a type five transaction, we always require it to have a blob while it's in the mem pool. We do not enforce that when someone includes it into consensus with zero blocks, that's fine, but we cannot add it to the mempool right now. Type four is not used. Type five is the blob transaction. Type two is the 1559 transaction. Type one is 29 30, and type zero is legacy.
00:04:31.522 - 00:05:44.734, Speaker D: So three and four are unused. So so far, those transaction types, they were always a superset of what was previously possible. So I think it would be nice if we could keep that with the type five transaction. And I made this small proposal for the E 68 dev P, two p. That essentially adds the number of blobs to this announcement packet, and then only if it has blobs, it would actually bundle this network wrapper containing the blobs, and otherwise it would just go through like every regular transaction. And this essentially avoids us needing separate transaction types for distinguishing these transactions that end up in the exact same way in the consensus execution payload. And also going forward, if we have additional access that we need to support, it avoids us having to double all the transaction types every now and then.
00:05:44.734 - 00:06:08.710, Speaker D: So we don't need a blob transaction with additional extra feature, a blob transaction without extra feature, a non blob transaction with feature, and a non blob transaction without feature. Yeah. Wondering if there were any thoughts on this already or just adding this number of blobs to the announcement.
00:06:19.820 - 00:06:59.700, Speaker A: Yes, sorry, I lost Martin, go ahead. Yes, it's just like technical thing. It's like, in current implementation of if 66, it's like encoded, like all RI of types, then all ari of the same length of sizes, and all ari of transaction hashes. So it would be not possible to add number of blobs only to the ones which has blobs. So you just will need to add all the array with cetera.
00:07:00.760 - 00:07:12.490, Speaker D: Sure. But that sounds doable as well. Or I'm not even sure if the transaction type itself is still needed, actually, or if it's just the number of blobs that would be needed.
00:07:13.420 - 00:07:45.760, Speaker A: Yeah, probably we can just replace transaction type with number of blobs. And. Yeah, I think transaction type was needed only to not ask about blob transaction with blobs, which is heavy. Okay.
00:07:47.810 - 00:07:57.970, Speaker C: Do we want to make a decision on that? Do we know any clients have implemented e 68 already and this would have to be undone.
00:08:02.470 - 00:08:05.300, Speaker E: Yes, never mind implemented 68 already.
00:08:10.470 - 00:08:29.834, Speaker A: And Ethan, does this change also depend on the higher level SSZ decision we make? Basically on the SSZ breakout, that call.
00:08:29.872 - 00:09:08.040, Speaker D: Is tomorrow, and it doesn't really depend on it. It would allow, for example, to eliminate this complexity of having a RLP transaction without blobs and an SSE transaction without blobs. Currently, in Vitalik's proposal, there is an additional transaction type for an SSE transaction without blobs. And if the 4844 type five transaction would already support that, this extra type is not needed, but it can be accommodated anyway. It doesn't matter.
00:09:08.730 - 00:09:10.726, Speaker A: Got it. Lucas.
00:09:10.918 - 00:09:31.120, Speaker E: So I somewhat don't like it, because now this number of blobs is creeping out everywhere, right? It's creeping out in networking, it creeps out in transaction pool. Wouldn't the size be a better limit how to handle things? And based that on the size of transaction, because blobs would be counted in the size, right?
00:09:37.210 - 00:09:44.520, Speaker A: Yeah. And by size, there's two things we can do, right? Like we can look in kilobytes, and we can also look at gas or data gas, right?
00:09:47.050 - 00:10:26.930, Speaker E: True. So if we have a transaction type of blob transaction, and its size is bigger than something, we know that it has blobs, probably. Like we could be sure, right? Can we be sure it has blobs? That's my question here, because I'm not as sure that creeping out this blobs, which is something special for this transaction type everywhere, is like that. Good design. I'm hesitant on that. Again, would be good to consult someone who also wrote ETA 68. That would be, I don't know, Marius.
00:10:34.730 - 00:11:16.710, Speaker A: Okay, so does it make sense to follow up with Marius offline about this? Yeah, it does feel like this whole the transaction pool and the transaction types are hard to get everyone on the same page about. Yeah, I'm not quite sure what's like the dependency, basically the dependency list between those things and how we get to a decision that people are happy with.
00:11:17.400 - 00:11:22.500, Speaker C: Yeah, I agree. This is starting to seem like the highest risk domain in our execution.
00:11:24.620 - 00:12:05.760, Speaker D: The transaction types, they are used in a much bigger scope than just a mempool. I mean, they determine how to serialize a transaction, how to sign a transaction, and how to create a transaction id. So it's an entirely different aspect of transaction processing. So having anything separate would already solve this. It can be like what Alex has said in the chat. It can be a flag that says, this transaction should be actively requested instead of gossip. It could be number of blobs.
00:12:05.760 - 00:12:33.150, Speaker D: It could be something based on size. I'm not sure if you can do it based on gas. Because the gas cost is only known after you execute the transaction. And it may change dynamically depending on the order of transactions, how they are bundled into the block. But there are multiple approaches, and I think anything that is not based on transaction type can be used.
00:12:39.000 - 00:12:40.448, Speaker A: Okay, Mophie.
00:12:40.624 - 00:13:40.310, Speaker F: Yeah, I think this point of using the size was brought up at Bogota. And I think if I recall the conversation, the decision for using the transaction type was that we could more accurately capture the full cost of validating, doing some quick validation of the transactions, particularly in the mempool. I know the mempool discussion is going taking a different direction, but assuming we're still using the regular mempool, then you want to use the transaction type because it fully captures not just the cost of transmitting these transactions over the network, but also verifying them. Whereas you could have a large transaction. But if it's not a blob transaction, then the cost would be much less than transmitting like a blob transaction, even if it's smaller than a regular transaction. That kind of makes sense.
00:13:41.480 - 00:14:22.740, Speaker D: Yes, if you think about a transaction of type five as only having blobs. That's true, because then you can use the transaction type to determine if it has blobs. But if you want to make the transaction type generic, like to allow it to support zero blobs, then the transaction type doesn't tell you whether it has blobs anymore. That's the problem. Probably not the size then, but instead some extra flag or number of blobs.
00:14:24.280 - 00:14:31.748, Speaker A: Can't you use data gas? Please go ahead.
00:14:31.914 - 00:14:33.430, Speaker D: No, sorry.
00:14:36.430 - 00:14:50.270, Speaker A: Yeah, so I guess the size doesn't work if you can have zero blobs in the transactions. Is that right? Because you can't. But you could still use that to determine for any transactions whether it has a blob.
00:14:51.570 - 00:15:02.530, Speaker G: But we don't want to. Like, that's okay. I don't understand. Isn't that exactly what we want? That zero bub transactions are handled exactly like normal transactions?
00:15:02.950 - 00:15:21.466, Speaker A: All right, so you say if a type five would. Smart. Sorry. Yeah, I was thinking about size larger than. But yeah, you can do the opposite and say if the size is smaller than this, you know, there's no blobs. Actually, I don't know that that's true. Because actually, I think this is not true at all.
00:15:21.466 - 00:15:34.080, Speaker A: Because you can have huge call data transactions, right? So you can have a transaction with call data, you just won't be gossiping yet. But you can have like a big transaction full of call data. And so I don't think size.
00:15:34.850 - 00:15:39.102, Speaker G: And then maybe that transaction should be considered just like a Block transaction.
00:15:39.246 - 00:15:43.780, Speaker A: Right. But I guess, and this is a different mempool design than, say, what Peter proposed. Right.
00:15:45.990 - 00:15:47.540, Speaker G: But it makes more sense.
00:15:49.670 - 00:16:48.600, Speaker A: Sure. I think that might be worth bringing up on the ACDE call this week. Because, yeah, you could say that the big call data transactions get treated the same. And then maybe the proxy can be something like whichever transaction is too big for you to gossip, for which there's already a check in the mempool you treat somewhat differently than the transaction that you can gossip. I don't know from the El folks here. Does anyone see an issue with using size versus type or data gas or something? Because if you want to do just blob transactions, and blob transactions can have zero blobs, you can also do something like if the data gas is bigger than zero, basically.
00:16:50.570 - 00:17:13.790, Speaker E: Well, yeah. It also depends if we're just being informed about hashes or they're actually being transmitted. This is like unrelated topic, like different message. Right. So it's hard to infer that information from this information. It starts getting confusing and ambiguous.
00:17:15.010 - 00:17:22.678, Speaker A: Okay, so what do you think would be the cleanest way to make the distinction based on how it's created?
00:17:22.714 - 00:17:25.170, Speaker E: I don't have the answer currently, sorry.
00:17:25.240 - 00:17:29.060, Speaker G: What is ambiguous about it if you simply make a size cut off?
00:17:33.110 - 00:18:05.920, Speaker E: Well, the ambiguous thing is because we have a broadcast transaction, right? Which broadcast the transactions. And right now all the transactions are broadcasted. Now you're saying some might not be broadcasted. And the criteria here would be size, potentially, which might be fine, but then it's like we would have to guarantee that this is in spec, right? That this size. Or do we want it in spec? Don't we want it in spec? It starts. Gets ambiguous for me a bit.
00:18:07.330 - 00:18:10.240, Speaker G: Sorry, I don't see that. It seems very clear to me.
00:18:10.930 - 00:18:14.350, Speaker E: Okay. But for me it isn't.
00:18:14.870 - 00:18:20.818, Speaker G: Why not? What's the problem of just defining a size? How is it more difficult than defining something else?
00:18:20.904 - 00:18:31.560, Speaker E: Will we define it in protocol as a protocol rule that we don't broadcast transactions above the size? Or will we?
00:18:31.930 - 00:18:41.580, Speaker G: There's already a rule like that, right? Right now, transactions over 128 kb are not broadcast already, is it?
00:18:42.110 - 00:18:47.562, Speaker A: Well, get does that. I don't know that other clients, it doesn't matter.
00:18:47.616 - 00:18:56.400, Speaker G: So this can be the same as this rule that gets has. So I don't see what's typical about us.
00:18:59.490 - 00:19:17.590, Speaker A: Marcinia? Actually not broadcasting. Blob type transaction is defined in spec of if 68. So it's just when we are using if 68, it means that we are not broadcasting blob type transactions.
00:19:20.510 - 00:19:35.390, Speaker E: Yeah, but I think that here is we are going back to blob transactions are zero blob transactions are the same as normal transaction. When some we should broadcast, some we should not. And again, it's becoming a mess, in my opinion.
00:19:36.610 - 00:19:39.374, Speaker G: Why is it becoming a mess? You still haven't clarified that.
00:19:39.412 - 00:19:39.614, Speaker A: Yeah.
00:19:39.652 - 00:19:46.158, Speaker G: If you have these two, some sort of statement that doesn't make any sense to me. It's very clear to me, but it's.
00:19:46.174 - 00:19:53.378, Speaker A: More of like, you now have these two rules. Do we need two rules about what you broadcast? Basically. Right.
00:19:53.464 - 00:19:55.730, Speaker G: Why are there two rules? It's a single rule.
00:19:56.630 - 00:19:58.526, Speaker A: Well, right now it's like larger than.
00:19:58.568 - 00:20:00.738, Speaker G: X bytes you don't broadcast.
00:20:00.914 - 00:20:25.114, Speaker A: But right now we have this other rule in each 68. That is any type five transactions we don't broadcast. Right. And that might be fine. Basically, it would probably be cleaner if either we just keep that as is. But that leaves you with the problem of like, zero blob transactions, which you could maybe treat more effectively.
00:20:25.162 - 00:20:32.930, Speaker G: And I'm simply saying, simply change that rule to instead of being type five transaction, being transactions greater than 128 kb.
00:20:34.070 - 00:20:35.090, Speaker A: Marius.
00:20:37.190 - 00:21:18.080, Speaker H: Yeah. I think we should disallow zero block transactions. I know I'm quite late to this conversation, and I think so, yeah, we can disallow broadcasting of transactions over whatever, but the thing that not having zero blob transactions allows us is to have them really separate from normal transactions. And this comes with a lot of advantages in the code, not having.
00:21:21.730 - 00:21:22.094, Speaker A: These.
00:21:22.132 - 00:21:51.260, Speaker H: Kind of transactions, just having these distinct types that don't all do the same thing. And this is kind of the problem that we have in the engine API right now is we have a new version, but it supports all of the old stuff as well. And it's a real mess because we have to filter out, we have to special case a lot of things.
00:21:51.790 - 00:21:57.930, Speaker G: But Marius, does that advantage still exist if we also want SSD transactions?
00:22:01.070 - 00:22:01.820, Speaker H: Yes.
00:22:02.670 - 00:22:14.450, Speaker G: How? Like, I mean, you know, like, what is the big difference between a normal transaction and a type five transaction?
00:22:15.510 - 00:22:18.030, Speaker H: Type five? You mean blob transaction?
00:22:18.190 - 00:22:28.310, Speaker G: Yes. Well, the only major one I can see is that it is SSC.
00:22:29.610 - 00:22:32.390, Speaker H: No, we have to verify the blobs.
00:22:34.090 - 00:22:36.870, Speaker G: Sure, if there are zero blobs, then there's nothing to verify.
00:22:37.310 - 00:22:50.014, Speaker H: Yes, but either we have to allow the zero blops transaction in the normal transaction pool, which would.
00:22:50.052 - 00:22:50.800, Speaker G: Why not?
00:22:51.410 - 00:23:10.580, Speaker H: Yeah, that would mean we have to filter out all of the blob transactions that we get broadcasted, and we have to allow them to be broadcasted to us to then filter out which transaction type they actually are.
00:23:14.090 - 00:23:17.160, Speaker G: Why do you have to filter the transaction type? Like. Sorry.
00:23:18.330 - 00:23:25.110, Speaker H: Because we have to put the blob transaction into a different transaction pool than the other transactions.
00:23:26.590 - 00:23:31.770, Speaker G: Well, I'm suggesting that the difference would be by size rather than by whether they have blob transactions.
00:23:36.190 - 00:23:53.730, Speaker H: No, you cannot do that. Yes, you can do that. The problem is that one transaction is verifiable in, like, nanoseconds, and one transaction takes at least milliseconds.
00:23:55.030 - 00:24:00.900, Speaker A: So the blob transactions are much longer. Yeah, but only large call data transactions. Right.
00:24:04.510 - 00:24:05.260, Speaker H: Sorry.
00:24:07.150 - 00:24:12.270, Speaker G: But Marius, a zero blob transaction can also be verified in the same time as a normal transaction.
00:24:13.170 - 00:24:13.920, Speaker A: Yes.
00:24:14.530 - 00:24:36.980, Speaker H: I'm just saying it's way easier and cleaner to have these transaction types in a different pool. It makes the implementation way easier because we don't have to have these special checks for everything.
00:24:38.150 - 00:24:54.700, Speaker A: And does that still hold true? This is not the case if you allow zero block transactions. Right? Because then if you have zero block transactions, you sort of have this small transaction slipping in your big transaction pool, is that right?
00:24:55.070 - 00:24:56.010, Speaker H: Kind of.
00:24:56.160 - 00:24:56.426, Speaker A: No.
00:24:56.448 - 00:25:07.680, Speaker G: But why? It doesn't. Because the small transactions would be treated just the same as a normal transaction. And there's also no special check to do because it has zero blocks. Yeah.
00:25:08.930 - 00:25:14.740, Speaker H: What we could do is have them in the big transaction pool. But the big transaction pool will have different.
00:25:15.750 - 00:25:18.050, Speaker G: I'm saying they should be in the small transaction.
00:25:20.950 - 00:25:22.680, Speaker H: That's not good.
00:25:23.690 - 00:25:24.680, Speaker G: But why.
00:25:27.290 - 00:25:44.570, Speaker H: Is not expensive? I'm answering Tim's question. It's not expensive. It's just like. I think it's very unclean to have. What's the advantage of having zero blob transactions?
00:25:46.110 - 00:26:08.420, Speaker D: The advantage is that with every future transaction type, we won't have to define two of them. Like one with blob and one without. And they are allowed on consensus. So anyone who wants to do a regular SSC transaction without blobs can just use the same type as anyone with blobs. Right.
00:26:09.270 - 00:26:13.550, Speaker G: Also, they would immediately give us SSC transactions for everything.
00:26:13.640 - 00:26:17.846, Speaker H: Yeah, but we can have SSC transactions. That's not a problem.
00:26:18.028 - 00:26:29.660, Speaker G: Right, but why does an SSC transaction have to be anything different than a blob transaction with zero blobs? That seems to be the natural thing to introduce them.
00:26:33.150 - 00:26:44.590, Speaker H: Yeah, I think it's just I don't see a point in having zero block transactions. It makes everything more complicated.
00:26:48.690 - 00:27:05.160, Speaker A: If this is just something that's also done at the gossip and networking level, we can change it pretty easily after the fork, right? I would lean no.
00:27:09.050 - 00:27:28.874, Speaker H: Depends on whether we verify these transaction rules and consensus. Like, if we get a block, do we verify that the block for transactions is correct? But it's very easy to change after the fact.
00:27:28.992 - 00:28:32.900, Speaker A: Yeah, I don't know. Going with the simplest one now, where, prolo, you have this comment about the list. That's another can of worms. But at the very least, it seems like the value of zero blob transactions might not be worth the implementation complexity. And if the transaction pool is one of the big things we're worried about, I don't know. It feels like just not having them, at least banning them from the transaction pool and potentially letting block builders use zero block transactions if they want, might make sense, I guess. Does anyone feel strongly we should have zero blob transactions? And as I understood, the best argument is, like, if a roll up just wants to use one address to do sort of everything, and they're not going to publish blobs with every transaction.
00:28:32.900 - 00:28:49.930, Speaker A: Actually, that's not even true. So it's like, if they need to use only one transaction type, they could still use one address, but then sign a type two transaction if there's zero blobs, right?
00:28:51.020 - 00:29:07.070, Speaker H: So regarding the argument that consensus doesn't verify that blobs are not zero, we can just make that rule, right? Like, there's nothing prohibiting consensus from doing this.
00:29:16.080 - 00:29:24.240, Speaker A: Okay, anyone disagree that we should not have the zero block transactions in the pool?
00:29:28.960 - 00:29:36.530, Speaker D: Okay, then I think that we should not have and disagree. What is the result there?
00:29:37.700 - 00:29:47.750, Speaker A: So what I'm saying is like, okay, I would like to propose we ban zero blob transactions from the transaction pools. If you have an objection, now is like the time.
00:29:51.320 - 00:29:58.644, Speaker G: It's not a kill I want to die on. I still don't understand any real reason for it.
00:29:58.762 - 00:30:01.430, Speaker A: Well, we get simplicity in the at least.
00:30:01.920 - 00:30:08.350, Speaker G: I just can't see it personally, how it gives us simplicity. But yeah, fine, if that's what people think.
00:30:10.240 - 00:30:27.940, Speaker D: What I would like to understand is if this announcement, instead of the transaction type, just says, just contains a bit, I contain blobs, or I don't contain blobs. Wouldn't that just allow the zero blob transactions without any complexity?
00:30:30.280 - 00:30:38.150, Speaker H: I think we already sent the size of the transaction in e 68.
00:30:39.180 - 00:31:04.750, Speaker D: Yeah, sure, but the size some people don't like because it mixes up the call data transactions from the blob transactions. Right. But instead of the transaction type, if you include a bit there that says this transaction contains the blob wrapper or it doesn't, then you don't really care about the transaction type anymore. I think.
00:31:12.240 - 00:31:12.990, Speaker A: It.
00:31:15.280 - 00:31:16.300, Speaker D: Anyhow.
00:31:16.600 - 00:31:19.728, Speaker A: Yeah, I guess you sort of added some complexity.
00:31:19.824 - 00:31:57.040, Speaker H: Yes. I'm weakly against the transaction pool rule without having it in consensus. I think it makes most sense to have this rule because otherwise someone can just modify their guest version and send these transactions. So if we have this rule in the transaction pool, it should, in my opinion, also be part of the consensus rules.
00:31:58.180 - 00:31:59.884, Speaker A: Okay, Lucas.
00:32:00.012 - 00:32:25.544, Speaker E: So I'm against adding another flag if there are blobs in the transaction or not. So the idea of transaction type was to give you information about transaction. Right. So it can give you this information and potentially be forward compatible with even more information about future transaction types that we don't even think about now. Right. So that's the advantage of having the transaction type. And this is what you're saying.
00:32:25.544 - 00:33:05.524, Speaker E: That transaction type is kind of irrelevant for gossip right now. And I don't like this idea as an idea. I'm not saying we should ban totally zero block transactions. I'm inclining to it, but I'm not 100% convinced. But this was what transaction type gave us, like, additional metadata about transaction that we can actually define a lot of rules against, on. And with Zelebr transactions, we kind of, like, break this encapsulation of information. Let's say, I don't know, not what word I want to use, but not.
00:33:05.562 - 00:33:22.100, Speaker D: Sure if you know what I meant.
00:33:22.460 - 00:33:56.770, Speaker H: So is it okay if we postpone this decision till next week? I kind of came into this call way too late, half an hour too late. Only because there was no one from Gathia. And I would like to think a bit more about all of this from our perspective and talk to the guys about it.
00:33:57.380 - 00:34:09.144, Speaker A: Okay. And do you think it makes sense to decide this on this call next week, or should we try and maybe bring it up on the execution layer call Thursday so that we have more folks from.
00:34:09.182 - 00:34:13.400, Speaker H: Yeah, we can talk about it on the execution layer call.
00:34:13.550 - 00:35:05.742, Speaker A: Okay, so I'll add it to the agenda. Yeah. And I guess the trade off is, like, if you allow zero blobs or not. If you don't allow them, do you want them in consensus, blocked in consensus, or at the mempool? And if you do allow them, how do you discriminate between either blob and blob? Less transactions or big and small transactions, and where that lives in terms of eat 68 and transaction pool rules, those seem like the three things to sort of figure out. I'll add that to the awkward apps agenda and we can discuss it there. Okay. I suspect this was probably the longest one.
00:35:05.742 - 00:35:21.860, Speaker A: We have a couple more spec things that Ethan, you had on the agenda. So the SSZ list for EIP 6495. Do you want to give maybe a quick update here?
00:35:22.470 - 00:36:00.782, Speaker D: Yeah. Right now in 4844 blob transaction, there is a field for the destination. It's an address field. And right now it's specified as a union that can either be none or it can be an address. The problem is that SSC unions are not standardized yet fully. Like they are still being discussed how they want to be serialized or merklized, or whether they are actually something that's even needed. So my proposal there was to use a list with up to one address instead.
00:36:00.782 - 00:36:22.760, Speaker D: Like if it's either it can be there or it can be an empty list or. I also made this new SSC optional type that can be implemented in like 2 hours, that is fully tested, that could also be used. Just so we don't accidentally introduce this union discussion and get blocked on that.
00:36:30.100 - 00:36:34.980, Speaker G: I'm really against introducing a new type in order to save one byte.
00:36:36.920 - 00:36:47.130, Speaker D: No, you could also do the list with one that also saves the byte. It's not about the byte. It's just about the union not being standardized yet.
00:36:47.660 - 00:36:54.484, Speaker G: How is it not standardized yet? I think it's been in the SSC spec for two years. I don't think it has changed.
00:36:54.612 - 00:37:24.180, Speaker D: It is unused and there is no test. We are currently discussing it for Vitalik's proposal about SSC transactions, whether we want to change how unions get merklized. So the hash tree root stuff may still change. Yeah, it's not about the byte. If it's about the byte, you could just use the list as well. It also saves the byte.
00:37:27.780 - 00:37:43.910, Speaker I: So we have implemented what's currently in the SSC spec. So I feel like I'd be in favor of just using that. Even if it's not necessarily agreed upon. It seems like as likely to change as doing something else.
00:37:58.950 - 00:38:30.410, Speaker A: Does anyone else have thoughts on this? Because I know we have the whole SSD call. Is it tomorrow? Yeah, literally tomorrow. This. I guess I'm trying to understand if there's other parts of the design space that are worth considering. Here's.
00:38:45.780 - 00:38:49.504, Speaker D: A detail. We don't need to spend too much time on it.
00:38:49.622 - 00:38:56.690, Speaker A: Yeah. Okay. And I guess there's a pr that people can discuss on as well.
00:38:58.660 - 00:39:04.070, Speaker C: Is there a decision we would make on this or do we want to just leave it open and say we'll get to a decision by next week?
00:39:04.620 - 00:39:11.892, Speaker I: Do people have the union type that's in the spec implemented right now in nimbus?
00:39:11.956 - 00:39:21.790, Speaker D: We don't fully implement it. We only have the case where it's abused as an optional. But we don't have the mercurialization for the full union type yet.
00:39:23.760 - 00:39:48.260, Speaker C: I think for prison last year there was like a renaissance packathon post by proto using just for the initial part of the merge there was union type. We have that implemented. I think the problem was it's more like there is no spat test. So we're not sure if our implementation aligns with other clients.
00:39:50.440 - 00:40:13.230, Speaker D: Yeah, the theme is from Forteg is in our code base because of the initial merge implementations then has been left out. But it's still in our SSD libraries. But it's never tested and I don't know how easy to know.
00:40:15.120 - 00:40:23.650, Speaker I: Yeah, if people think that just using a list of size one is less likely to cause headaches, then that's okay with me.
00:40:27.460 - 00:40:38.550, Speaker G: Adding the testing should be easy, right? I mean, I don't feel like that should be a major obstacle if we have all the infrastructure, right. Just adding a test for the union should be quite easy.
00:40:50.880 - 00:41:19.670, Speaker D: Yeah. I mean, one more thing is tomorrow in the SSE call, there is another discussion about unions. So maybe the information from there, if we see that unions as a whole, as a concept, are actually used not just for an optional, then I agree that we don't need a separate type for optionals, actually. But yeah, maybe postpone this one as well. I mean, it's a really quick change anyway.
00:41:21.020 - 00:41:36.168, Speaker A: Okay. Yeah. Let's see on the SSD call tomorrow what comes out and how we can get back to this one next week. Okay. 16 million blob capacity. I'm not sure what this is about, but also you, Ethan.
00:41:36.344 - 00:41:59.480, Speaker D: Yeah. Currently in the same blob transaction type, we support up to 16 million blobs per transaction. I just wonder if this is just a random number or if there is an actual design space, why this is supported like more of an understanding question.
00:42:00.590 - 00:42:11.660, Speaker A: So this is basically in transaction type five as they currently are described. The max number of blobs is set to a constant that's like 16 million. Is that right?
00:42:12.530 - 00:42:15.550, Speaker D: Yeah. Let me look it up real quick how it's named.
00:42:16.130 - 00:42:32.370, Speaker G: This is the list type, right. I think generally we don't want to use the list max as the actual maximum for things. It should be rather like a theoretical upper limit.
00:42:34.150 - 00:42:34.994, Speaker D: Exactly right.
00:42:35.032 - 00:42:41.750, Speaker G: So for serialization, it doesn't matter at all the max list length, it only matters for mercurization.
00:42:43.290 - 00:42:44.040, Speaker D: Correct.
00:42:46.250 - 00:42:51.130, Speaker G: And it's so that all the leaves are the same depth.
00:42:51.950 - 00:43:22.420, Speaker D: Exactly. And it also affects the length of the Merkel proof. I mean, that one is not. I mean, if there is a design space that we actually want a lot of blobs in the future, then there is another limit that an SSE object that you serialize, it cannot be more than 4gb. So 16 million gets it to multiples of gigabytes. So even the theoretical limit is very unlikely to do that.
00:43:24.390 - 00:43:45.340, Speaker G: Yeah, I mean, 16 million does seem like a lot. I wouldn't argue that we have to. I mean, we definitely should make it like a large number, but something in the thousands is probably enough for any time until we have to change it fundamentally again anyway.
00:43:46.670 - 00:44:27.570, Speaker A: Yeah. Dankrad, do you want to just do a quick change to the spec, to some generous upper bound beyond what full dank sharding would give you? And we can just lower that constant by a couple of orders of magnitude, probably. And. Cool. Okay, and then next one. So you had this hash tree root based signature, but a comment saying it might be better for the SSV breakout. Do you want to maybe give some quick context there?
00:44:28.100 - 00:45:35.140, Speaker D: Yeah, that one was opened by light client, and Vitalik is also behind it. Essentially. Right now in 4844, we are using the ketchup of the SSZ encoding to determine the hash that is being signed, and also the transaction id. And the idea there is that we want to go to a world where we don't need the Kejock hash anymore there. And we also can be based on hash tree root to be more in line with how it works on consensus. So there are some troubles there, especially for the signature hash, because that one needs to be protected against malleability if there is a future transaction type that encodes similarly that it doesn't end up with the same hash. But besides that, it's mostly a discussion for tomorrow, how we can optimize this transaction id, at least to be part of the actual SSC tree.
00:45:38.300 - 00:46:05.800, Speaker A: Unless anyone has really strong opinions. Now, I would probably save this for tomorrow just because we only have ten minutes left, but anything people feel is really important to bring up right now. Okay, and then Sean, you had a spec item about the beacon APIs. Do you want to give some quick context there?
00:46:06.970 - 00:47:08.620, Speaker I: Yeah, so it's just a proposal for how we want to support signing over individual blobs in the validator client via the standard APIs. So I guess the notable features are one that blobs will just be like blinded by default because it seems like there's no real point to send large clumps of data to the validator that aren't interpretable by the validator. And then in addition to that, the get request by the validator is a combined block and blob to avoid a race between the head of the beacon chain changing between requests and then the posts back to the beacon node by the validator are individual blog requests and then the existing block request. Just generally looking for feedback. People can take a look. That's it.
00:47:11.790 - 00:47:14.540, Speaker A: Cool. Any comments on this? Now.
00:47:18.610 - 00:47:29.630, Speaker F: Just a question. So you're coupling the block and blobs in the beacon API. We also going to have the old just blobs endpoint.
00:47:35.030 - 00:48:03.050, Speaker I: Right now with blocks. We have a separate block endpoint for just like telling the beacon node to create a block for me to sign and propose versus get a beacon node in the. Sorry, get a block from the beacon nodes database. So I think we'll have the same sort of separation with blobs where the endpoint that Jimmy has a pr for will still exist to get like the full unblinded blob.
00:48:10.830 - 00:48:25.550, Speaker A: Okay, anything else? Okay then any client team have some updates they want to share?
00:48:33.120 - 00:49:02.390, Speaker D: Yeah, from Bessu. We are now feature complete and we are asking to join the devnet for Testnet. Yes, so pending for join the testnet and then working to refine our implementation and merging it to main piece by piece.
00:49:05.560 - 00:49:09.750, Speaker A: Nice. Anyone else?
00:49:13.080 - 00:49:31.550, Speaker I: So in Lighthouse Poon's been working on the decoupling work in gossip and then we're also just working on things like blob pruning and then the ability to separate your blobs database from your blocks database, stuff like that.
00:49:34.400 - 00:49:44.130, Speaker C: For prison. We're also working on decoupling and making our dbs more reasonable. Just basically wash and said.
00:49:48.380 - 00:50:21.910, Speaker D: From techu last week on Friday released a new image for Devnet four with some fixes around sync logic plus lobs handling on DB. So so far so good. We are synced from genesis and we are okay. And we were waiting for the confirmation to actually starting decoupling and yeah we are starting working on that.
00:50:25.600 - 00:50:55.430, Speaker A: Sweet. Anyone else? Okay, anything else anyone wanted to bring up before we wrap up?
00:50:59.980 - 00:51:12.350, Speaker C: Kim, going back to your comment around the transaction pool, it seems like we have a clear next step on the zero blob transaction question. Do we want to set a clear next step on the transaction pool design?
00:51:13.060 - 00:52:13.540, Speaker A: Well, I guess they're sort of related, right? Like, if we don't allow zero blob transactions, then we can have a design that's based on that. I feel like we need to make the zero block transaction decision first, and then from there, we can also discuss the transaction pool more broadly on all core devs. Yeah, cool. And I've added it already. Yeah, I've added it already to the agenda. Anything else? And, yeah, we'll have the SSD call as well tomorrow, so I can post the agenda in the chat here if anyone wants to join that. Yeah.
00:52:13.540 - 00:52:22.932, Speaker A: Otherwise, I think we can wrap up. Yeah. Thanks, everyone. Thank you, guys. Thank you. Thank you.
00:52:23.066 - 00:52:24.960, Speaker C: Bye, guys. Bye.
