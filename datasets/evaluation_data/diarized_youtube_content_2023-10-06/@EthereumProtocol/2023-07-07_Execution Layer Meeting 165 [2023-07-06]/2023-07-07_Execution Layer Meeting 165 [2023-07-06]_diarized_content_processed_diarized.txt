00:01:19.080 - 00:01:42.230, Speaker A: Hey, everyone, this is all core devs execution layer meeting one six five. This is issue eight one five. My chat's going through. No. Can someone share? Oh, it is. Okay. Anyway, couple of technical difficulties, but we can keep moving.
00:01:42.230 - 00:02:11.670, Speaker A: Anyway, this is issue eight one five on the PM repo and I am subbing in for Tim Baco today. All right, looks like we have. Is somebody from Dab. Sorry. Yeah, cool. Okay, so the first thing up is we do have an SSD impact analysis that was done by Dadab. If you all want to take it away, you can screen share and give us a look.
00:02:17.490 - 00:03:00.170, Speaker B: Sure thing. Thanks, Danny. Let me just share my screen. All right, so thank you for hosting this meeting and inviting me here. So over the past three weeks we've conducted another study. This one has to do with the migration of the RLP format to SSZ of some MPT commitments which are stored in the receipts and transaction route respectively. So this is obviously Joy's work with some of my colleagues.
00:03:00.170 - 00:04:05.170, Speaker B: One of them is on this meeting as well. Yeah. So what was the original scope of the study? So the issue with making changes to execution layer data structures in the block header is that there are some protocols that might actually be using these and performing proofs on top of these. So these two eips, so that's 64, 66 and 64, four, are migrating the encoding from RLP to SSD. Now, what is the impact of such a change on these protocols? We've tried two different approaches for completeness. So in one approach we've analyzed all smart contracts deployed on Ethereum. We have a data set for that on contract library.
00:04:05.170 - 00:05:14.440, Speaker B: So the static analysis and the other approach employed dynamic analysis. So we actually looked at the transaction behavior and tried to determine whether there were smart contracts that are actually doing verifying proofs over these data structures. And we already had an idea that perhaps decentralized bridges might be doing this sort of thing. But a couple of years ago we also saw historical blockchain events, oracles and other weird stuff that cannot be discounted. We found and examined these projects and tried to determine what is the provenance of this RLP encoded data? Is it coming from L2? Is it coming from layer one itself? Now note that in this meeting we're deciding, discussing whether to make changes to layer one but not L2. So if L2 changes, then.
00:05:18.490 - 00:05:18.866, Speaker A: It.
00:05:18.908 - 00:06:19.690, Speaker B: Shouldn'T affect decisions that are being made on layer one. Yeah, and then of course there's other things. Like for instance, is some of the data being relayed via centralized keepers and things like that. So if there's some element of centralization through which some of these data is being passed through, then perhaps even if the changes are being made, the centralized systems can be upgraded. And of course all of this study has been done on recently used or high balance or part of known protocol contracts. Now, if we determined that the protocol was affected, we explored upgradability options. So, to get into a bit more detail about the effect of the changes.
00:06:19.690 - 00:07:04.618, Speaker B: So here's the theorem block header. Here's some of the fields. There's three routes there. There's a state route, which is a root hash global state, also coded using a Merkle tree. Transactions routes and receipts route. Again, so these are routes of mercury containing transactions and receipts respectively. So 64, 66 is changing the encoding of the transactions from RLP to SSZ, and in case of the receipts as well.
00:07:04.618 - 00:08:07.070, Speaker B: So this will obviously make a big difference to the hashes that are going to be calculated. So if there was some smart contract which is actually reverifying these, then those will actually be broken. So throughout these three weeks, we've determined that actually there are three protocols that will be affected by this. So one of them is layer zero, and in this case layer zero is also being used by Zkbridge in some way. So both of these protocols are affected. Specifically, one of the light nodes is being used, and it's going to be affected. In a case of telepathy, which is doing very complicated things, that is also going to be affected.
00:08:07.070 - 00:09:25.666, Speaker B: Now, in all three of these protocols, we've also found out that there are upgradability options, and in all cases only EIP 64 66. So the receipts route had an impact. Now, the estimated impact is subjective. For instance, the subjectivity comes from the fact that how can we determine whether the protocol is easy to upgrade? We're not exactly sure how easy it is, but we've seen that, for instance, if they're using certain software engineering patterns, we know that they're upgradable. How often is the protocol used? And then, especially on the more complicated protocols, there's alternative configurations. And in some configurations the protocol is affected, and in some it isn't. Now, most importantly, before we started the study, we already had an idea that perhaps some of these decentralized bridges, like an optimism or polygon, there is a possibility of them being affected.
00:09:25.666 - 00:09:57.060, Speaker B: And indeed these bridges were being flagged up time and time again in our analysis because they were doing proofs of RLP encoded commitments. But in actual fact, these are not being affected. I mean, these were out of scope because in both cases, for instance, polygon and optimism, the data was coming from L2 and being verified on layer one.
00:09:59.670 - 00:10:13.026, Speaker A: Do we know when data goes from layer one into L2? Are they utilizing the layer one commitments, or is that a blind spot?
00:10:13.218 - 00:10:44.938, Speaker B: So what we did is we looked at the protocols that were deployed on Ethereum, but then most of these larger protocols, they would have similar contracts on other chains. So when we found that they were actually doing verifying these kind of proofs on layer one from L2 data, we also checked on the other chains whether a symmetric thing is also happening.
00:10:45.124 - 00:10:45.666, Speaker A: Right.
00:10:45.768 - 00:10:54.980, Speaker B: And in this case, we didn't find that that was happening. But say, for instance, in the case of layer zero, we did find that that was happening.
00:10:55.510 - 00:10:57.160, Speaker A: Okay, thank you.
00:10:57.930 - 00:12:38.706, Speaker B: Okay, so I'll go into the two different approaches we used. So we used static program analysis, where we literally tried to find, just by looking at analyzing smart contract bytecode just to be like fully complete, whether the bytecode we're implementing RLP decoders and using that, say, for instance, in Merkel tree proofs. Now, it looked a bit daunting at first, but we actually did manage to mechanize an analysis for identifying these. And the main insight from this is that all of these RLP libraries, if they want to decode RLP correctly, they need to actually contain some constants, such as the ones that I'm selecting here, because these are actually needed, because you need to do comparisons or you need to subtract these constants to get, for instance, length fields and things like that. So what we did is we tried to see whether there's like patterns such as, for instance, the variable is being compared to a constant in either direction and things like that. So in multiple different ways. And it turns out that this analysis yielded quite a few contracts, and I'd say most of them were, they weren't actually false positive.
00:12:38.706 - 00:13:55.040, Speaker B: Indeed, having a combination of those constants being used in expressions such as comparison expressions, and then using that to decide which part of the loop to execute and things like that. It was basically an RLP decoder. Now, one other thing we did is because Ethereum bytecode doesn't have variables, what we did is the analysis was performed on a lifted version of the bytecode. So in contract library, we have this constant data set of decoded smart contracts and, sorry, decompiled smart contracts, and they're decompiled up to various levels. And one of those levels is tree address code, which is very faithful to the original semantics of the bytecode, but it contains variables instead of stack operations. And so once you have that, and it is easier to perform comparisons, looking at certain patterns in the code. Okay.
00:13:55.040 - 00:14:47.258, Speaker B: And of course there's other things you need to do. So the compiler generates a lot of code which needs to be optimized and things like that. And we had analysis passes to de optimize some of the code. It wasn't work which was conducted for this, but it is a data set that we're constantly analyzing for security vulnerabilities and things like that. Okay. So we've also tried a second approach to find affected contracts. And the reason why we wanted to try two different approaches is so that we can get kind of like more confidence out of the data set and be sure that each approach has its own blind spot.
00:14:47.258 - 00:16:24.160, Speaker B: But if you combine two approaches together, you get better results by dynamic analysis. What we actually tried to find is identify contracts, specifically some functions within those contracts that received our lp encoded data as part of their call data, as part of their arguments. Now how do you do that reliably in order not to get a lot of false positives? Because what's special about RLP encoded data? And we will see in the next slide, assuming that the RLP encoded data is of type bytes, then because of the way ABI encoding works. So in Abi encoding, you'd have the first four bytes being the selector, and then you've got chunks of 32 bytes being individual arguments. Now, when you have something of type bytes, apart from having a pointer to where the bytes start right before the actual data in that byte array starts. So I'm putting my cursor here. Right before that, you have a length, you have the actual length of the array in the next 32 bytes and chunk and so on.
00:16:24.160 - 00:16:39.250, Speaker B: And it also happens to be the case that RLP encoded data also contains a length field, and it's funkily encoded because you've got the first byte gives you the length.
00:16:44.070 - 00:16:46.382, Speaker A: Sorry, that's just somebody unmuted. I muted them.
00:16:46.536 - 00:18:12.400, Speaker B: All right, no worries. Yeah, so the first byte is actually the length of the length, and then you've got the actual length here. Of course, there's like differences here. But if you go through, as we did, like all the call data over a million blocks, and you check for these heuristics like this, we ended up getting a pretty good data set of around 1000 smart contracts that were actually doing RLP decoding. Now, there was also a source of false positives in this data set, because there are many contracts such as, for instance, uniswap, flash swap, or things like that, that just relay the call data from one contract to another without actually interpreting it. And so what we did there is we actually assessed for every smart contract, for every public function that it implements, the number of times it actually is receiving RLP encoded data in its argument. So if it was receiving more than RLP encoded data, and more than 10% of the time that it was being invoked, then we put it into the data set to be inspected later.
00:18:12.400 - 00:19:35.210, Speaker B: So then what we did is we combined these two data sets and we found out that it's mostly the same contracts that were being flagged. And we inspected the most popular contracts out of these data sets. And a lot of contracts and big protocols were flagged, as I said before, like polygon and optimism, BitTorrent and many other protocols. And then obviously there were false positives as well. But I'm just saying like contracts or protocols that are decoding our LP encoded data, but most of them we found out by painstakingly going through each one of these contracts, one by one trying to understand the protocol. There was always some reason why this protocol would be unaffected. And these are the three reasons, right? So one of the reasons was that the protocol didn't really do MPT proofs over this RLP encoded data, even though the data was RLP encoded, such as the case of for instance when you have flash loan contracts or things like that which relay call data arguments.
00:19:35.210 - 00:21:08.150, Speaker B: So the second case, which is the most popular case, is that a contract was actually doing proofs, but the provenance of this data was coming from L2 chains or other side chains. And then when we looked at the same protocol again on other chains, we didn't find asymmetric relationship there. And then finally the protocols were doing proofs over MPT commitment, but they were unrelated to in scope imps ieps, such as for instance the state route or custom data structures that were being created from centralized systems. And so these wouldn't be affected. Yeah, so the important unaffected protocols that we found, so these are unaffected were polygon bridge, optimism, and Bittorrent bridge. So BitTorrent bridge was mostly used between Tron, I believe, and we didn't find a symmetric path where the implementation on the opposite chain. So the chain to which they were bridging to what's doing it.
00:21:08.150 - 00:22:08.890, Speaker B: Like similar proofs as was happening on Ethereum. Interestingly, there was another protocol called telepathy, which we marked as being affected, but in many cases it was being flagged up as doing RLP encoded proof commitment checks, but these were on the storage route. But then there was part of that protocol doing that on the receipts route as well. Okay. All right, these are the affected protocols. So I'm going to give an example here on layer zero. This did exactly here on layer zero, it is validating one of the proofs, and the data is stemming from receipts root.
00:22:08.890 - 00:22:37.406, Speaker B: In the second case, Zkbridge was integrated into layer zero as an oracle, and in some cases, we found that it was being configured with the ultra light node in layer zero. So that's also affected. And in telepathy, that was more complex. Yeah, indeed.
00:22:37.438 - 00:23:31.910, Speaker C: So again, hi, I'm Elias. I was one of the collaborators that worked on this report with Neville. Indeed, telepathy was a very different beast than the other bridges in the sense that it actually uses consensus layer data. But it turns out that they depend on the historical summaries field, on the historical summaries buffer, which contains commitments from execution layer, and which of course, right now are a function of receipts route. So when this changes, they will also be affected because they assume the data in the execution layer blocks RNP encoded. So changing that will affect the smart contract.
00:23:34.090 - 00:24:41.850, Speaker B: Yeah. And of course, it was also being flagged up for things that were unrelated as well. Okay, so in conclusion, just from what we found out, we think that the impact is overall it's moderate, but in all three cases, we found upgradability possibilities. Now, I got to stress that RLP encoding and then reproving MPC commitments happens a lot on chain, and it actually took quite a bit of time to go through and filter out cases where this was going to be affected versus not. Most instances were indeed unaffected. In all three cases that we found in layer zero, for instance, the default inclusion library can be upgraded and ZK bridge. If layer zero fixes this, then it's going to fix ZK bridge.
00:24:41.850 - 00:25:08.520, Speaker B: But they can also implement their own proof validator, which might add some technical debt. And again, in telepathy, they can also upgrade the inclusion proof logic. So thank you. Do you have any questions? And also you can find the full report. I'll share it with you on the chat, and also this presentation as well.
00:25:11.130 - 00:25:11.734, Speaker D: Awesome.
00:25:11.852 - 00:25:29.740, Speaker A: Thank you. I have a quick question. Given the tools, analysis, and some of the kind of heavy filtering you had to do to remove what we think are false positives, what are the chances we miss something? What's the confidence level on completeness here?
00:25:31.570 - 00:26:15.020, Speaker B: Yeah, so the reason why we did two different approaches is in order to maximize this. Now, if you want to be even more rigorous than this, theoretically you would have to go through every single chain that we know about and repeat this on all the different chains. Now the problem with that is that you will flag similar contracts, but they will not be exactly the same. Perhaps. And so again, you have to do a lot of inspections of all of these contracts. We did a little bit of this. So specifically, we looked at a sample of.
00:26:15.020 - 00:27:04.230, Speaker B: So we ran one of the dynamic analysis, which was easier to run on multiple chains. We ran this on a sample of BSc data. We ran a sample on phantom data as well. But essentially what was happening is that these protocols were deploying similar contracts on different chains. If you find a contract which is doing that on Ethereum, you're most likely going to find the same contract doing it on other chains. And it's probably the case that there weren't protocols bridges that were being developed for other chains with no similar symmetric contracts on Ethereum.
00:27:07.690 - 00:27:11.920, Speaker A: Got it. Other questions. It.
00:27:28.790 - 00:27:40.900, Speaker E: If we're talking about other chains and coordination. Right, because this is not a blocker for us or anything. Do you have the tools for this analyzes available for them?
00:27:42.870 - 00:28:08.320, Speaker B: Yeah. So part of the tool chain is open source, but another part is closed source. So we can give you accounts, for instance, on one of the tools we have internally called watchdog, and some of the analysis can be repeated again. So on the other chains we're limited. But on the Ethereum chain we fully support that.
00:28:09.570 - 00:28:23.330, Speaker E: And second question. So if I understand the conclusions, the conclusions are there is no blockers, but it needs a bit of coordination when doing this upgrade with all affected parties.
00:28:23.830 - 00:29:00.474, Speaker B: Yeah, I would go with these three affected parties. I think Layer zero already knows. Some of the developers have read this already. And then the other two, we can contact them as well. And I guess the news will spread as well. We'll also put this up on our Twitter account. Maybe you can do the same thing on some of your accounts, but I don't think we'll find completely different protocols.
00:29:00.474 - 00:29:04.160, Speaker B: They will be similar to what these protocols are doing.
00:29:16.650 - 00:30:03.562, Speaker A: It great, thank you. Any other questions? You okay? I appreciate it. We dropped the link in the chat for the full report. Check it out. Obviously, this isn't something we're making decision on today, but this is really good information to help us make decisions in the future. There were a lot of discussion points in the chat. Does anybody want to surface? Anything for discussion now? Okay, great.
00:30:03.562 - 00:30:17.600, Speaker A: Thank you for the detailed analysis. Next up are just general Cancun. Cancun updates, any testnet updates that we want to relay or other updates relevant for today.
00:30:19.730 - 00:31:09.040, Speaker F: Yeah, we have Devnet seven up and running since last Friday, and the testnet has been finalizing. We've also submitted a bunch of blobs to it and triggered at least one or two issues in the testnet. I think some of them have already been patched and some are still open. There's an issue tracker, so please refer to that for more details. And besides that, once we have all the patches put in, we're going to just continue spamming the network to see how we'd perform with three, six blobs being full all the time. So far we've had it for short durations of time, but we haven't had it for a longer duration of time. And there's a message from Justin from the security team about a deposit processing issue.
00:31:09.040 - 00:31:16.160, Speaker F: If someone from Teku could have a look, that would be great. Or I think Teku and Prism could have a look, that would be great.
00:31:18.070 - 00:31:23.570, Speaker A: What layer of the stack are we seeing? Were the couple of issues networking consensus?
00:31:26.070 - 00:31:33.942, Speaker F: I'd say it was networking issues. I think Marius has a lot more information on that and if he wants.
00:31:33.996 - 00:31:36.360, Speaker A: To expand on that.
00:31:36.970 - 00:32:08.110, Speaker G: Yeah, so Gath had an issue with RP encoding of the transactions on the networking layer, so transactions weren't relayed correctly. That was fixed now. So that was the only thing. Transactions between guest nodes were relayed, but not with the others. And the others implemented this correctly.
00:32:12.390 - 00:32:19.250, Speaker F: I think there was also an issue with the message size, right. With too long messages being requested.
00:32:20.410 - 00:33:52.740, Speaker G: Yeah, but there's not really something that we can change on our end. Basically we can just say, hey, give us messages, give us these messages, and someone on the other side has to respond. There was an issue in gath where we would not tell the other nodes the correct size of the messages. But I don't think anyone is really has implemented this part where we really use the new e 68 mechanism to say, to filter messages and say, okay, we now want blob messages, or we only want messages that are smaller than x. So that's something that still needs to be implemented. I think some clients even might even still send blob messages over other ETH protocol versions, I think especially ethereum js because they don't have e 68 yet. So either they are not sending any blob transactions or they're using e 66, which is.
00:33:55.110 - 00:34:33.650, Speaker A: Not good. Okay, thanks for the updates. Any other cancun updates? Yeah, never mind. Has an issue. We almost have a fix. As an image deployed soon it is connected to message size tool which was mentioned by. So the fix is.
00:34:36.760 - 00:34:59.870, Speaker D: Yes, we have a mechanism of limiting size of transactions which we are sending, but we had a bug in calculating size like length of blob transactions and it's already fixed. Will be merged in a few minutes and probably image for Devnet Seven will be updated today as well.
00:35:06.430 - 00:35:25.700, Speaker F: Assuming that all the fixes are in and Devnet Seven is relatively stable, do we then start planning? Would you guys prefer that we start planning the next one with all the other eips? Or should we continue just keeping Devnet Seven up and stressing it more? I'm not sure which approach we want to take right now.
00:35:29.190 - 00:35:57.580, Speaker A: I know on the consensus layer at least a couple of themes have voiced the desire to move towards the full feature set for the next testnet. But in terms of the timeline, if we want to stress Devnet seven and there's some iterative pure four eight four updates that people want to do, then that makes sense. So preference for when we move to do so. But I don't know if there's a time preference for when we move.
00:35:59.310 - 00:36:32.290, Speaker G: So we don't have 4788 yet in the code and I think at least one or two other eips. I would rather use this time to stress test definite seven and in parallel work on getting those other things in and not wait with the stress testing until the next definite.
00:36:33.210 - 00:36:52.090, Speaker F: How about we add all the tests required for the next Devnet onto hive so that we can already, anyone who's ready can already start testing and making sure hive is green and in the meantime we keep Devnet seven around at this rest test. Devnet seven, would that be a good doesn't block anyone approach?
00:36:52.670 - 00:36:54.126, Speaker G: Sounds good to me.
00:36:54.308 - 00:37:06.450, Speaker A: Yeah, I think so. We don't need to be setting the date for the next test net, but allowing clients to be in the position to move there. Mario?
00:37:07.030 - 00:37:19.560, Speaker D: Yeah? Do you mean with the high test? Do you mean the EIP tests or for example the dev peer to peer issues tests or both of them?
00:37:21.850 - 00:37:30.200, Speaker A: Yeah, I guess ultimately both starting with consensus and then moving to Debp to P makes sense. Unless you think other way.
00:37:32.250 - 00:37:42.330, Speaker D: Yeah, we are still working on having all the AP tests. I think in a couple of next weeks we should have them ready for testing.
00:37:43.250 - 00:38:00.580, Speaker A: Got you. And I believe the eips are going to be dominantly kind of consensus tests rather than dev PDP tests in terms of the workload on your end, contrary to work for.
00:38:01.190 - 00:38:19.270, Speaker D: Yeah, definitely. I think four was like one off because of the different transaction formats that we used in the header versus the peer to peer. So that's not going to be an issue for the other APC.
00:38:25.920 - 00:38:50.610, Speaker A: Yeah, so, Barnabas, just to echo that, obviously, unless major issues are found with Devnet seven and pure four testing, the intention is that the next Devnet would be full featured for Dankoon. But we're not setting a timeline for that next testnet. We're going to allow people to develop the full feature set while stress testing devnet seven.
00:38:57.150 - 00:39:05.500, Speaker F: Talking about issues with 2436, we only had three, six for a short duration. So I think it's too early to make a call on either one.
00:39:13.380 - 00:40:03.980, Speaker A: Cool. And on Monday, we have a four for four call we can spend some time talking about if there's networking stuff that surface looks like Nimbus is having issues, but we don't really have clarity on if on our small testnet three, six is changing the impact. But we will continue to bring that up either on the call on Monday, if we have time, I mean, if we have information, or on the consensus layer call next week. And Barnabas, just for your information, the latest consensus layer spec release and tests does have the full feature set on the consensus layer. So if you're building the spec for that Devnet, you can point to whatever the latest release is.
00:40:04.590 - 00:40:05.580, Speaker G: Great, thanks.
00:40:15.870 - 00:41:21.790, Speaker A: Okay, other Dankoon discussion points. Did someone unmute? Moving on. There's an item from myself, a should override builder flag being introduced into the engine API. This was discussed on the consensus layer call last week and general consensus that getting this flag into the engine API is high priority for them and very easy. It's because it can be introduced and always sets a policy execution layer and a no op if they feel like Lincoln's layer. So you can kind of introduce the flag and then add feature functionality to it over time. But Mikhail, can you give us the information on this and what people are thinking?
00:41:23.440 - 00:42:08.776, Speaker D: Okay, so I'll just quickly give the context on this flag and then the discussion on whether included into Cancun. So should override builder flag. The proposal is to add this flag to the get payload response. It's basically true or false. This is the way the EL to communicate to CL that there is some censorship is happening in the network and CL should make its own decision about that. So the way it's supposed to work. So y'all will have some heuristics and to find the evidence of censorship transactions censorship in the network.
00:42:08.776 - 00:42:50.244, Speaker D: And according to these heuristics made a decision and returned this flag with the build payload. So we favor client diversity for implementing these heuristics. So they're completely unspecified for this reason. And also if El does not implement any of the heuristics, it should always return false. So as Dennis said, it should be a no op and also consensus layer. Client may also ignore this flag even if it is set to truth. So it's completely free process for both parties and consensus.
00:42:50.244 - 00:43:44.380, Speaker D: They may use other sources to feed it into their decision making process about censorship. So the main reason why this flag should be returned by El is that El has information like Mempool and Reorgs, which is a pretty big chunk of data that it does not want to expose via any API, and it doesn't make sense to be exposed. So that's the way it is supposed to work. And getting back to the cancun, so we want to include this into Cancun. As Danny said, the clients are on board with that. And considering that the engineering complexity is really low, and also we have a prototype in Gaeth of those heuristics. Give more details on that.
00:43:44.380 - 00:43:56.210, Speaker D: So considering that the complexity is low and the utility is quite high of that flag, so the proposal is to make it into Cancun. That's basically it.
00:43:56.900 - 00:43:57.840, Speaker A: Lucas.
00:44:00.180 - 00:44:56.530, Speaker E: So my question might be premature, maybe Marius should come first. But the question is what's the status of research of this heuristics? And are there, like many of them proposed? Are they in any way tacked on historical data? Right, against historical data. And my question stems from actually client diversity. So think that a client would implement this in a way that would flag a lot of false positives, right? And then stakers might be discouraged to this client because they would get lower rewards. So it may affect this kind of things too. So hence my question.
00:44:59.080 - 00:46:01.770, Speaker A: I suppose in theory that is possible. If you had a client that always said true, and you can override that, and your cl listened to the El in the extreme, obviously you would have lower words. That said, I know there are discussions of a number of simple heuristics, for example, transactions sitting in the mempool for x time that certainly could have been included profitably in a block where these end up looking like pretty deterministic. But this is primarily to elevate the El to have a voice in the fallback functionality. And there's any number of reasons, even beyond censorship, that you might perform such fallbacks. But are there any write ups or deeper discussions of potential heuristics here? I know pothos has thought about them.
00:46:07.920 - 00:46:59.740, Speaker G: So I have not really thought about it. I just implemented this very simple heuristic. If a transaction has been reorganized, that someone is censoring it. This is just the first heuristic that I. That I thought about. So it's not back tested. It's not really just something to prove that it's actually possible to implement this and show that it's very easy to implement those heuristics.
00:47:03.760 - 00:47:15.920, Speaker E: Okay, let's say someone reor transaction three times, you managed to find this out. And so for how long do you return the slack?
00:47:18.340 - 00:47:19.780, Speaker G: Forever afterwards.
00:47:21.320 - 00:47:24.340, Speaker E: Okay. That's probably also not optimal.
00:47:24.680 - 00:47:36.010, Speaker G: No. As I said, this is very much a draft of this concept. It's just to make sure that this concept kind of works.
00:47:36.700 - 00:47:58.224, Speaker A: Yeah. And defining optimality is certainly up to the client. Right. Like, if you have any heuristics go off, you could flip a coin. Also, good idea to return true false. There's all sorts of ways, if you are concerned about that, to allay those concerns. Go on, sir.
00:47:58.342 - 00:48:08.460, Speaker E: Good idea from Ben that if we have empty blocks, fairly empty blocks, that could take transactions from the transaction pool, but didn't.
00:48:08.540 - 00:48:08.832, Speaker B: Right.
00:48:08.886 - 00:48:18.180, Speaker E: So someone is trying to sensor the block space in general, not certain transactions. Good idea for a heuristic.
00:48:18.680 - 00:48:41.150, Speaker A: Yeah, I mean, I guess the way I think about it is primarily the El has a lot of information. This gives a way for the El to give an opinion, to voice something about that. And because it's very wide open, then you have the faculty to decide how, when and if to voice that.
00:48:44.080 - 00:49:16.580, Speaker E: Yes, I'm totally agreeing with the concept. Right. The concept is great, and we should implement it ASAP. And it's very simple because we can right now, just return false for some time. Just wanted to highlight that this opens quite not trivial problems that may have better or worse solutions that, in my opinion, need some, maybe could be helpful to have some research on. That's what I meant.
00:49:16.740 - 00:49:17.640, Speaker A: Agreed.
00:49:21.260 - 00:49:42.192, Speaker E: One thing that I noticed also about the code, I'm not entirely sure about it, but I was thinking that if someone posts a transaction with a bit of a low fee or a low priority fee, and that's the reason why it's not included with the current approach that Marius has.
00:49:42.246 - 00:49:47.010, Speaker A: This might flag as censoring when it's actually.
00:49:49.620 - 00:50:09.610, Speaker G: Like this only tracks. If a block gets reoccured out and we see three blocks in a row getting reorganized out, and they contain the same transaction, then we suspect someone is doing.
00:50:10.220 - 00:50:14.860, Speaker E: We're talking about only reorgang, not being included.
00:50:15.440 - 00:50:19.624, Speaker G: Yes. The heuristic that I implemented.
00:50:19.672 - 00:51:08.808, Speaker A: Yes. So the main question here is, is anyone opposed to getting this into the engine API spec it is a single boolean flag in one method that, other than implementing the engine API, you don't have to do anything immediately. This would probably be slated for that full feature, Devnet, that we're talking about called Devnet eight. Anyone opposed to that? We have support from the consensus layer teams. Marius. Right now, we do not have eips for the engine API. The engine API is more of an implementation detail around the outer consensus spec.
00:51:08.808 - 00:51:17.324, Speaker A: But I'm not sure if there was. This is sarcastic or not. Docred, sorry.
00:51:17.362 - 00:51:19.870, Speaker H: What are we going to do with this flag when we have it?
00:51:23.070 - 00:51:36.250, Speaker A: In the event that the execution layer says true, the consensus layer can decide to fall back to local building instead of map boost or any sort of outer block.
00:51:39.790 - 00:51:40.586, Speaker G: So.
00:51:40.768 - 00:51:47.226, Speaker H: But then Marius's approach of detecting it doesn't make sense because he is detecting it using Reorg blocks.
00:51:47.258 - 00:52:02.046, Speaker A: Right, right. Okay, sorry. Go debate. We can debate this. Right? Because if the attacker has the ability to reorg, then they're going to just reorg it again. We can debate the technique.
00:52:02.158 - 00:52:30.800, Speaker H: Yeah, I think set of lag only makes sense if you can detect an attack and you have a remedy for that attack. Yeah. If we can detect block builders are not including it, then that flag might be interesting. But if we're detecting something completely different and extrapolating from that. Oh, censorship is going on, then it doesn't make sense to affect this part of, I don't know, the two that don't seem to be in alignment there.
00:52:31.650 - 00:52:36.014, Speaker A: There are many other heuristics that do not involve reorgs. You're right. There is a nuance to the Reorg one.
00:52:36.052 - 00:52:42.260, Speaker H: Right. But I'm simply saying it definitely has to be one of those, otherwise it doesn't make sense to me.
00:52:47.360 - 00:53:04.064, Speaker A: Agreed. I think the most obvious ones are monitoring mempool with respect to what was included in the main chain. And that does not have to do with reorgs and can allow for something there might be.
00:53:04.102 - 00:53:17.172, Speaker H: In that case, you have to make a judgment about the tips. But I think you can say, if transactions with lower tips are getting included, then you could say a censoring attack.
00:53:17.226 - 00:53:18.570, Speaker D: Is probably going on.
00:53:25.150 - 00:53:32.540, Speaker A: But I do think you and I agree that there are techniques that are valuable here, and relatively simple.
00:53:40.400 - 00:54:27.370, Speaker G: I think this heuristic was kind of naturally grown, and in the beginning, it was just a log output. If we have this, if we see this case. So basically, just an indicator that there's censorship going on. And for that one, the heuristic was fine. I agree that for falling back through local block building. This heuristic is not really doesn't say anything. As I said, the heuristic was something that was thought up in like five minutes just to see that the concept works.
00:54:34.560 - 00:55:08.000, Speaker D: Thanks a lot for the discussion. Recapping if there is no opposition on the call, if anyone listened to us and has an opposition to this tiny change, please express it in the EIP. Sorry, in the PR? Yeah, we have no eip for that and I will be aiming to merge this on next Monday. So that's the plan.
00:55:08.610 - 00:55:25.580, Speaker A: Great, thank you Mikhail. Okay, next up, four, seven, eight. Bounded storage has been merged. Alex, do you have anything you want to discuss here?
00:55:28.110 - 00:56:02.280, Speaker I: Not really, mainly just calling it out. We've kind of had a little bit of back and forth on the design and the PR that was merged is kind of the last or sort of the product of all those conversations. So basically it's storing blockers from the beacon chain and the storage is now bounded using like two ring buffers and. Yeah, I'll just point you to the PR if you want to see the details. A few people have implemented it and said good things, so sounds like this is the way forward for this EIP right now.
00:56:06.170 - 00:56:11.740, Speaker A: Great. And this had no impact on the consensus layer spec, right?
00:56:13.470 - 00:56:20.250, Speaker I: Yeah, that's been handled other places. There's support for it in the engine API. So that's all set.
00:56:20.400 - 00:56:20.942, Speaker D: And then.
00:56:20.996 - 00:56:28.800, Speaker I: Yeah, all this is really doing was just changing some of the implementation details of the actual pre compile, but otherwise same basic idea.
00:56:31.520 - 00:57:10.010, Speaker A: Great. Any questions for Alex? Thank you. That's all we have on the agenda for today. Are there any other discussion points, announcements, concerns at this point? Okay, thank you everyone. I believe Tim will be back for the next version of this call in two weeks. Talk to you all very soon.
00:57:11.100 - 00:57:12.010, Speaker G: Thank you.
00:57:12.540 - 00:57:14.430, Speaker D: Thanks Danny. Thanks. Five.
