00:00:57.900 - 00:01:18.976, Speaker A: Okay everyone, this is consensus layer call. One, one, one. I'll drop the agenda in the chat. It's p. M. Issue eight, nine, Danny can't make it, so I'll be moderating and yeah, we can go ahead and get started. So first up, up.
00:01:18.976 - 00:01:50.890, Speaker A: Well, yeah, I think Mrs. Call will be Daneb, and then we'll touch on some more research forward looking issues. So, to get started with Deneb, I think essentially we just want to understand what things are finally going into the spec, the spec release soon. So I guess we'll just go to the agenda. There's this issue for EIP 6988. I think Mikhail has been driving this, let's see.
00:01:55.820 - 00:03:16.804, Speaker B: About 6988. So the problem this EIP is about preventing that a slashed value is elected as a proposer, which is basically prevents us from having a lot of empty slots in case of mass slashing. And one of the problems that we have encountered during working on the cap was that basically this eap breaks the invariant of a proposer shuffling for an epoch, so it can change throughout the epoch. And that's apparently a problem for their clients and for probably some other tooling and some other pendencies. So I've made a pr which suggests the change proposed by depline. It basically stores the proposer shuffling in state. The footprint is not that big, and it seems like pretty not that big in terms of like engineering complexity to keep it stored in this state.
00:03:16.804 - 00:04:05.220, Speaker B: So there is the update proposer shuffling, which basically accepts the state and computes the shuffling for the next epoch. And then we read proposer indices from this vector. That's the change. And yeah, it's called every epoch processing. This function is called every epoch processing. What's not that nice about the introducing of this function is that if we are bootstrapping from genesis, for instance, we have to call this function before our state becomes usable and before we are going to process any further slots and propose any blocks. Otherwise it will just not have the proper shuffling in it.
00:04:05.220 - 00:04:41.890, Speaker B: But I don't think it's a big issue. And the main question here is whether, considering this additional complexity, whether we want to include this change into the NEB, if anyone took a look at this PR, it would be, yeah, if we can discuss it now. So let's do it now, or I would keep it for a couple of days more to take a look into this pr and make a final decision on that EIP, whether it goes in the nub or not.
00:04:43.780 - 00:05:13.450, Speaker A: So we do have a precedent of having some initialization function for the changes of state. So yeah, I agree with you. I don't think that's like a big problem. It says in the description it only increases the extra state that we have to track by 266 bytes. So not too wild there. I know there's been some back and forth on this, trying to achieve the aim of the slashing invariant. Yeah, it's good to see this here.
00:05:13.450 - 00:06:18.150, Speaker A: Personally, it feels a little bit late to me to add something that is kind of this big, because it is kind of like a change to the mental model of the protocol in the process. I don't know if anyone else is taking a look. Yeah, I'll take that as a no. Maybe we have the pr open for another few days and I don't know if anyone feels strongly they should voice support or not support, and we'll go from there. I would imagine it's a bit too late to decide this on the next CL call, but if that's the next time we get to discuss it, that might be the next time.
00:06:20.440 - 00:06:27.110, Speaker B: Yeah, probably we can briefly mention it on the next Yale call, if it makes sense. I don't know.
00:06:28.360 - 00:06:43.850, Speaker C: So I think here it would make sense to do a study like we did for removing the self destruct to understand the implications of breaking the dependent root, which may not be that bad, but I think that's the main question we have to answer here. But definitely it feels tight for the net.
00:06:46.560 - 00:06:56.770, Speaker B: Yeah. So definitely if we want to explore breaking this dependent root thing. Yeah, I would say that this is too late for the node also.
00:06:58.820 - 00:07:02.240, Speaker A: Sorry, could you explain a bit what you mean by breaking the dependent route?
00:07:03.780 - 00:07:37.790, Speaker C: So this doesn't show up in the spec, but in the bitcoin APIs all the duties include a dependent route. And the idea is if the dependent route of that specific epoch is not changing because there is a reorder deeper than a number of slots, you are good and the duties should be the same. So I know a few clients at least. I think Loadstar and Ally house rely on that. We can get around that and just poll that it will be good because that's exposed. Who else may be using that dependent route because we don't know.
00:07:38.560 - 00:07:49.884, Speaker A: Right. So I thought partly why we have the EIP for this is to basically maintain as a variant that the shufflings wouldn't change. So it seems then that you'd still have that guarantee for the dependent route.
00:07:49.932 - 00:07:50.530, Speaker D: Right.
00:07:51.380 - 00:07:55.970, Speaker C: Well, my point is, if no one is using dependent route, maybe we can just break it.
00:07:56.740 - 00:08:08.920, Speaker A: Oh, you just mean like get rid of it entirely yeah, well, okay, sure. But my point then is that it sounds like this change doesn't have any bearing on that, or am I not following?
00:08:10.220 - 00:08:26.430, Speaker C: So the EIp six, nine, eight is a reaction to the simpler one that just adds an extra condition on the compute proposal. If we don't mind breaking the embarrassion, that's a much simpler change.
00:08:27.760 - 00:09:01.240, Speaker A: I see. Okay, so you're saying we could not do this, just make the few lines change in the other pr and then loses guarantee. Okay, yeah. When I was talking about changing the mental model, I think that having shufflings be, because we're talking about that they would change possibly every slot, right. So that to me seems even more chaotic than what we have today. So I think we should not go in that direction.
00:09:06.540 - 00:09:34.850, Speaker B: I know maybe this invariant also used by some other tooling, I don't know, and can have some implications. But as for me, not having proposals shuffled and sort in the state is a more clear, more clear solution, like breaking the invariant if it is possible.
00:09:35.380 - 00:09:51.990, Speaker C: Another point is if you have to process old blocks, you need to retrieve the shuffling. So yeah, having to process the block up to that specific lot, that's the cost. But I don't know, I will look into it.
00:09:52.780 - 00:09:58.250, Speaker B: You would have a state anyway, so it's not going to be a problem.
00:10:03.800 - 00:10:25.470, Speaker A: Okay, so I think we should stay focused on essentially what's going to go into the final TM Deneb release. And so it sounds like there's still a few design questions. We have some more sort of research we want to do around this particular feature, which suggests to me that we table it for now. Does that sound good?
00:10:27.120 - 00:10:28.510, Speaker B: Makes sense to me.
00:10:33.120 - 00:10:57.910, Speaker A: Okay, next up on the agenda, there is another issue, this one for extending the engine API for Cancun. I think also Mikhail opened this one and. Yeah, anything Mikhail we should know. I went the other day and added stuff for 4788, which we'll get to later in the call. And generally it looks good when I looked at it.
00:11:03.100 - 00:12:01.764, Speaker B: The spec consists of the blob extension spec that everyone is familiar with and also with the parent beacon block route, which is for the other eap. And last one change that is in this pr, in the proposed Cancun specification. For now it's not. Probably the last one which will be on the Cancun is the deprecation of exchange transition configuration. I would just like to quickly, briefly go through it. So I think that the spec, first of all, I think that the spec is just enough in terms of tooling to deprecate this gracefully. So basically the spec says that first execution layer clients must not surface any error messages to the user.
00:12:01.764 - 00:12:50.410, Speaker B: If this method is not called. If we remove this requirement, I mean, if we remove this error message, then we can break the dependency between Cl and El. So we remove this on El side and then continuously clients can remove this method entirely. So actually, how I see the procedure of removal of this thing is that El clients remove it right away, as soon as possible. Remove the error message, not the methods. Methods should still exist because otherwise consistently, clients will surface error because of this method will be responded. So Yale clients remove the error message, we wait for Cancun or.
00:12:50.410 - 00:13:26.752, Speaker B: Yeah, we wait for every El client to remove this message. And then consistent client also releases the software without this method being called. And then after Cancun, everyone can just remove it at any point in time. So we will use Cancun as the point of coordination for software upgrade. Yeah, this is the way of how to do it gracefully. So maybe we can just cut the court and remove it right away. But then users will see some ux issues by potentially seeing error messages.
00:13:26.752 - 00:13:44.350, Speaker B: If one client stops supporting it, the other one will complain. Yeah. So there are two potential ways to do this. I would prefer the first one, which is the graceful one, but maybe there are some other opinions on that.
00:13:49.460 - 00:13:55.890, Speaker A: Yeah, I mean, this graceful approach sounds better. And was that what you have in this PR or something else?
00:13:56.500 - 00:14:18.490, Speaker B: No, it's not described in the PR because this, we are into the spec. And so I'd say that spec is not here. So probably we should just rise this on El call and ask el clients to start removing this error message, if they're okay with that. How does it sound?
00:14:24.630 - 00:14:42.780, Speaker A: Yeah, it sounds good to me. I got a plus one in the chat. Let's make a note to bring this up on the call next week. But otherwise. Yeah, sounds good. I know this is something we've been wanting to do for a while, so makes sense to go ahead and do it.
00:14:43.470 - 00:14:44.220, Speaker B: Great.
00:14:55.940 - 00:15:26.520, Speaker A: Okay. That was pretty straightforward. Okay, so there's a list of vips here that we essentially. Well, that are on the slate for going into Daneb. And let's see, we'll just take them in turn. So the first one is 70 44. This refers to essentially changing how we process voluntary exits so that they once made, are sort of valid forever.
00:15:26.520 - 00:16:07.770, Speaker A: And this was essentially ux improvement because before they expired, and that was not so nice for people, especially in custodial staking services. Yeah, so it looks like it's been merged into the specs. Already, and I think it's on here just to call it out. I don't think we really need to rehash it, but I guess it's worth calling out. The next one is 70 45. So this one wants to change how we process attestations. And I know that this was around the confirmation rule that some of the fork twist people have been working on.
00:16:07.770 - 00:16:40.950, Speaker A: Does anyone here want to give an overview of this? The PR itself basically just says that you have, I believe, the current and also previous epoch to include an attestation on chain, whereas before it was just one epoch worth of slots. So there's basically like slots per epoch rolling window, and now it's basically extending back out to sort of rounding down, so to speak, to both epochs, if that makes sense. So I don't know if there are any of the confirmation rule people on the call.
00:16:45.480 - 00:18:11.804, Speaker B: I can try to give an overview on that. It's not only about the confirmation rule, it's also about some other properties in the protocol that we want to maintain under some edge cases. So essentially this PR removes the constraint and gives us the guarantee that attestations that has been produced in the previous epoch will be includable until the end of the current epoch, which is important for the confirmation rule and for some other things. As I already mentioned, this PR also fixes the incentives part because previously there was the same constraint on the number of slots equal to number of slots in the epoch for the target, as far as I remember to give the rewards for that. So now proposal will be rewarded for attestation if it even comes from the beginning of the previous epoch, while the book has been proposed in the end of the current epoch. So this part is also fixed. And yeah, I would be pretty much in favor of this eap of this change to be in the node as I think it is straightforward.
00:18:11.804 - 00:18:38.120, Speaker B: There is also the p two p part as well. So attestations will be propagated. So the propagation also changed from the slots equal to number of slots in debug to the previous epoch and the current epoch. All the essentials produced in these two are free to be propagated.
00:18:42.320 - 00:18:59.970, Speaker A: Okay, thanks for the overview. Yeah, I mean, I don't really see anything blocking inclusion. So has anyone looked at this in terms of implementation? That'd probably be the only reason we wouldn't move ahead with it if there was some issue.
00:19:10.890 - 00:19:11.398, Speaker D: It.
00:19:11.484 - 00:19:16.300, Speaker A: So has anyone looked at implementing this? I'll assume no if no one speaks up.
00:19:26.150 - 00:19:40.460, Speaker E: Okay, I have a quick question. Does this affect the aggregation subscription in p to p at all. Like which subnet you're supposed to be subscribed to for how long?
00:19:47.770 - 00:19:50.360, Speaker A: Yeah, I think this is mainly a state transition change.
00:19:52.010 - 00:20:05.702, Speaker E: No, it's a peer to peer change as well. And it extends the time that somebody is allowed to send out stations on a particular subnet in peer to peer.
00:20:05.766 - 00:20:10.510, Speaker A: Right. But I don't think it changes anything about the aggregation structure.
00:20:12.290 - 00:20:45.740, Speaker B: And that's actually a good question. And the question is that in the current before this change, how does the aggregation works in terms of subscription? Whether aggregators stay for a longer time to expect some anticipations from the past or not. Because the spec does not seem to say anything about this hedge case. I mean like even with the 32 slots as it is today.
00:20:51.910 - 00:20:58.740, Speaker E: Yeah, I think that might be a gap actually. I think we're kind of unsubscribe early.
00:21:00.790 - 00:21:01.346, Speaker B: Yeah.
00:21:01.448 - 00:21:12.280, Speaker E: And then the wrong aggregators will be listening to these attestations and they won't aggregate them anyways. They will be lost anyway. So I think that's merit at least thinking about.
00:21:13.310 - 00:21:43.540, Speaker B: Yeah, so I mean it does not break that much, this change in terms of aggregation. And I think that there is a probabilistic function like whether there is the right aggregator. So there is still a probability that somebody subscribed to the right subnet. The current subnet can aggregate those or not. Maybe I'm wrong.
00:21:44.390 - 00:22:29.940, Speaker E: Well that's the thing that nowadays we will only be subscribing to one aggregate subnet per beacon node and not per validator anymore. Yeah. To be honest, I don't know. I'm raising the question because I have no idea. But it feels like the kind of thing that would in practice possibly break this or not break it. It would just render it pointless to be propagating these attestations unless the aggregation pipeline is in tune with this change.
00:22:31.590 - 00:22:49.270, Speaker B: Yeah, so again it doesn't seem to break anything but it's probably the problem that we are discussing now already exists for some all the destinations.
00:22:59.190 - 00:23:50.138, Speaker A: Okay, well so it sounds like we should do a little digging to look into this a bit further and I suppose we'll discuss this on the next call. I don't think even if we needed to change the subscription logic that it would make this quite as complex or anything. So I would lean towards considering it included. But yeah, definitely something we should take a look at. Okay, so we'll make a note about that. The next one up for discussion is 4788. So I opened a pr for this.
00:23:50.138 - 00:24:33.110, Speaker A: Basically there was like a feature for this and now it's been in this pr, migrated into Daneb formally. There was some feedback from some of you, so thank you for that on the PR. And yeah, this one's pretty straightforward change. We've been discussing this for a while. I think it's pretty much ready to go. There was some feedback here that I can get to, but it won't be super substantial in terms of the spec or implementation, I suppose. Any final thoughts? Otherwise, we'll get this ready and merge it for the Denav release.
00:24:33.110 - 00:24:43.250, Speaker A: I guess it is worth calling out that there was also an engine API change for this in the PR that we discussed earlier from Mikhail.
00:24:51.820 - 00:26:19.270, Speaker B: One thing that I commented out in the PR, and that at the first glance can look a bit od, is that we are duplicating the parent picking block route on the CL site. So the other way around would be not introducing this into the execution payload on CL site, because we have the parent root in the outer structure, in the beacon block structure, and then just CL passes this data into EL, but El includes this parent beacon block root into the execution payload, into the actually execution block. But yeah, there was a comment about it and a bit of a discussion, and I think that having this block root inside of payload on CL site might make sense, really from the live client perspective, and probably in some other places it could be useful. So I think it's fine to keep it there, considering that the complexity, data complexity isn't really little here. So I'm just emphasizing that because probably somebody else may take a look at that and have a similar thought on that.
00:26:22.090 - 00:27:16.854, Speaker A: Right, yeah, thanks for bringing that up. So you could say very strictly that we're like duplicating some data in the block with this extra parent route. And while that's true, the reason it's there is because we essentially want to have this sort of symmetry, almost, that whatever we have in the execution payload in the block is what is passed to the El, and the El needs us there, definitely. So I think it's easier to reason about if we just put it there, as Mikhail pointed out, we could save 32 bytes. But, yeah, I don't know if anyone here has a strong opinion either way, and perhaps if you do, you can take it to the PR. Unless you want to talk about it now, we gotta. Seems okay in the chat.
00:27:16.854 - 00:28:12.810, Speaker A: Thank you. Okay, so those are the prs here on the agenda. I think the intention is to essentially get them merged into a genev release in the next week or two, and that would sort of be our final DNEB spec from the Cl side. So very exciting to see. And yeah, everyone here, please take a look at everything we've discussed. If there's any final comments, especially beyond what we've discussed on the call so far, let them be known. So with that being said, the next item here is discussing the blob count.
00:28:12.810 - 00:29:17.052, Speaker A: Perhaps this was mainly just a note to call this out that I think as spec, the EIP says that we essentially have a target of two blobs and a max up to four. There's been some conversation between different researchers and devs about bumping that up to, say, target three blobs, max of six blobs. And, yeah, basically, I think the call out here is just to say that people on this call and others are looking at all the data that we can and thinking about potential ramifications. Please just keep doing that and join the conversation to the extent that you're able to do so. And there's a note here, we're trying to make an informed mainet decision in the next two weeks. So, yeah, if you have thoughts or feelings about this, you should bring them up soon. I would imagine this will be a topic on the next execution call.
00:29:17.052 - 00:29:25.232, Speaker A: But yeah, this is just a reminder that it's happening. At least it's a topic to happen, a discussion point that it may happen.
00:29:25.286 - 00:29:25.890, Speaker B: So.
00:29:27.860 - 00:29:30.530, Speaker A: Don'T let it fall off your plate, so to speak.
00:29:35.810 - 00:30:23.230, Speaker F: Yeah, and I also wanted to mention for those who weren't on the Monday call, that I did more tests at 768 kb. I'm going to share the dashboards in the chat for anyone who's interested in looking at these as a summary. We didn't see any problematic behavior, any instabilities. We did have some problems with our own node syncing, as far as I know. So the blocks we created, some of them, while we hit the average, were more like 1.5 megabytes and zero megabytes alternating, which is actually a stronger stress test. But yeah, that was our endpoint.
00:30:27.830 - 00:30:53.930, Speaker A: Okay, great. And yeah, thanks, tankrat, for leading the charge on those experiments. It seems like from what we've seen, there isn't any immediate issue with the bigger blob size. So that's a strong argument to make them bigger. Okay, great. There's some data in the chat. Angar is asking how into the process can we make this decision.
00:30:53.930 - 00:31:07.940, Speaker A: Yeah, I mean, I would say the sooner the better. I don't think we want to be like two weeks out from the fork and being like, oh, let's change this. Just because it'll ripple into, I think, a bunch of things.
00:31:14.440 - 00:32:01.120, Speaker E: I'll mention one thing, which is only weekly related, but it kind of came out of looking at graphs around this. What's been happening over the past six months is that we've gone from practically no reorgs at all to a few per hour, and we can kind of. There's no great answer to why this is happening. There's a couple of theories. It looks a little bit like it's growing with the number of validators. It definitely became worse after the complexity increase in capella.
00:32:04.980 - 00:32:21.304, Speaker F: Well, what about the late block reorgs? Wouldn't that be a strong reason why we're seeing this? Because before that, even if you get your block in as late as 11 seconds, your probe is still going to.
00:32:21.342 - 00:32:21.930, Speaker B: Be.
00:32:23.660 - 00:32:32.090, Speaker F: On the chain, whereas now we reorg those blocks. So maybe it's not that surprising that this has gone up. Two clients have introduced that.
00:32:34.460 - 00:33:11.284, Speaker E: Yeah, it's possible, let's say. But if you look at the graph, it's kind of growing. So that could be explained by more people starting to use newer versions of the clients, but it kind of looks very similar to the validator set growing. I don't want to draw any conclusions here, really, I'm just highlighting it. But when looking at block experiments, this is like an interesting thing that has changed over the past few weeks. Sorry, months. And it definitely became worse with capella.
00:33:11.284 - 00:34:26.400, Speaker E: Like, on the spot where we switched to capella, it's markedly higher on average. And why am I mentioning it right now? Because, well, we're packing more and more stuff into the first 4 seconds before we're supposed to send an attestation. And my gut feeling is that it might actually be an excellent time to rebalance the timing of sending the attestation and the aggregate. And I just wanted to sort of feel out the call whether anybody strongly opposed to, say, I'm just going to pull some numbers out of my nose here, but let's send the block at 6 seconds. Sorry, let's send the attestation at 6 seconds, and send the aggregate appropriately in the middle between that and the next block, instead of at 4 seconds. Has anybody explored this and found strong reasons not to do it or to do it? I'd be kind of curious, because that would help, certainly help reorgs.
00:34:30.110 - 00:34:58.440, Speaker F: So I'm strongly in favor of this, and I also have the same feeling that this is exactly one of our big problems. We put one, the first third of the sort. I think the interesting question would be, if people have data on when attestations are arriving in the second, third and when aggravations are arriving. Yeah, if anyone has that data, that would be very interesting.
00:35:00.570 - 00:35:06.470, Speaker E: Well, we have histograms that tell us when all the attestations come.
00:35:06.620 - 00:35:08.074, Speaker B: Can you share that?
00:35:08.272 - 00:36:01.998, Speaker E: Yeah, I can share it in the consensus dev channel later. The general trend is that there are two things in the spec. There is a rule that allows us to send attestations as soon as we've observed the block. Clients are generally not doing this. There's like a large concentration of attestations coming in shortly after the four second mark and then it's spread out. But there's a fairly large number of attestations coming in the second after the four second mark. So as a client, Dev, if we were pushing the timing back, I would still look into sending.
00:36:01.998 - 00:36:18.720, Speaker E: I would strongly suggest that people implement this feature where we send the attestation a little bit earlier to spread out the traffic. That would help. And I am going to post the exact numbers in the consensus channel, but just got feeling based. They're pretty good.
00:36:24.770 - 00:36:29.780, Speaker A: Sorry, what do you mean it's pretty good? Just the ones that you gave as actual numbers?
00:36:31.750 - 00:36:47.666, Speaker E: No, as in we're supposed to send the attestation at the four second mark and like by 5 seconds most of the ones that we're going to be sending have been sent and received.
00:36:47.698 - 00:36:49.222, Speaker A: Right, okay. Right.
00:36:49.276 - 00:36:58.300, Speaker E: But again, this is just me looking at the graph. I'm going to pull it out in a slightly different format to give better numbers.
00:36:59.070 - 00:37:46.250, Speaker A: Sure. Data would be helpful. I mean, I also agree, I think this is something worth strong investigation and I don't know, my concern with say, deploying this in Danab is just delaying the fork. This is like a somewhat involved change. That being said, I do think it's pretty mean. It sounds like we should probably spend some resources looking into this ASAP. I don't know if anyone has any thoughts on Daneb and its relation to this change, but it sounds like we probably just want some more data first.
00:37:47.980 - 00:38:24.224, Speaker D: I just wanted to bring up. Go ahead, Mike. Sure. Yeah. Just from the relay perspective, some of the issues around relay stability were around that four second deadline and in particular getting all the relay checks done in time to hit that four second deadline. Like if a validator sends assigned header to the relay at t equals two, then the relay really only has 2 seconds to get the block published in time. So yeah, I think with bigger blocks that timeline is going to get even tighter for relays.
00:38:24.224 - 00:38:31.530, Speaker D: So 6 seconds might help in terms of stability there too, so just thought that was worth bringing up.
00:38:34.140 - 00:38:44.670, Speaker E: For relays. How much of that is like trying to post a block as late as possible to make more profit versus starting the work on time?
00:38:45.280 - 00:39:00.352, Speaker D: Well, the relay can't start the work until the validator sends assigned header. Right? So I guess the validator could play like some timing games. There was a paper from Casper and the rig group about this.
00:39:00.486 - 00:39:01.696, Speaker E: Let me get the link for it.
00:39:01.718 - 00:39:08.020, Speaker D: It's called time is money. I think the takeaway is generally validators aren't playing these timing games.
00:39:08.600 - 00:39:10.420, Speaker E: I'll post a link in the chat.
00:39:11.400 - 00:39:55.250, Speaker D: But the relay still has some amount of latency that it has to do. For example, simulating the block takes on the order of two to 300 milliseconds, and then receiving all the bytes might take like another 200 milliseconds. So the latency starts to add up quickly, especially if someone calls get payload later in the block. But yeah, the relay can do stuff like, oh, we'll reject any get header requests past 3 seconds, but if they have a valid header that they sign, the relay is kind of like obliged to try and get the block published, even if the signed header isn't received until like t equals 3.5 or something like that.
00:39:58.920 - 00:40:00.390, Speaker E: Cool. Thanks for the.
00:40:04.840 - 00:40:15.880, Speaker D: Terrence. Yeah, I just want to call out that Prism also has the release that's coming either today or early on Monday, that we do find like an issue.
00:40:16.030 - 00:40:19.044, Speaker G: Where that if today there's a late.
00:40:19.092 - 00:40:33.528, Speaker D: Block, and then for the subsequent slot after the late block, that prism will delay, have some additional latency on block production, and therefore it may cause issues with the relayer.
00:40:33.544 - 00:40:34.980, Speaker H: And that's why if you're a prism.
00:40:35.000 - 00:40:48.070, Speaker D: Validator, you may see more reorg or not Reorg, or your block proposed block will have a higher chance of getting reorg. But the fix is coming. So hopefully that should improve things a little bit from the presumption side.
00:40:52.520 - 00:41:21.804, Speaker A: Okay, that's exciting. Yeah. So, zooming out a bit, I think this is actually really important that we look into changing these subseconds or, sorry, these. So these sub slot second timings. So, yeah, let's keep this thread going and we'll see. Again, I won't speak to when exactly it's included, but I do think it's very important for us to dig in here. Yeah.
00:41:21.804 - 00:41:50.900, Speaker A: POTUS. POTUS. Do you have your hand up? We can't hear you. You're muted if you're speaking. Can't unmute. I'm sorry. Maybe try to get it working and just speak up when you can figure that out.
00:41:55.670 - 00:41:56.250, Speaker D: It's.
00:41:56.350 - 00:42:11.420, Speaker A: Yeah, I'm not sure. There's some messages in the chat, but I'm not sure what. Oh, I can. I don't know if I can. Maybe. Tim, do you know if you have the ability to unmute people?
00:42:12.510 - 00:42:13.562, Speaker E: Let me check.
00:42:13.616 - 00:42:14.570, Speaker B: I cannot.
00:42:16.610 - 00:42:21.920, Speaker A: I'm getting that you are the only one. I don't know what that means.
00:42:23.570 - 00:42:26.640, Speaker B: I mean, why couldn't he unmute? People have been talking.
00:42:30.380 - 00:42:34.008, Speaker A: Yeah, but I think Danny's the host. Technically, he's just not here.
00:42:34.094 - 00:42:36.120, Speaker E: Wow. Danny just shadow banned.
00:42:41.520 - 00:42:43.630, Speaker B: See if login yet fixes it.
00:42:44.640 - 00:42:47.384, Speaker A: Yeah, we need censorship.
00:42:47.432 - 00:42:49.790, Speaker D: Assistant Zoom. Yeah.
00:42:51.920 - 00:43:13.636, Speaker A: Wait. Okay, he's back very credibly now. Yeah. I don't know. I can chat with you. I'm not sure who the actual host is. Yeah.
00:43:13.636 - 00:43:24.790, Speaker A: Okay. Sorry. If you want to send a message in the chat, we can try to go about it that way, but otherwise I will keep moving things along.
00:43:27.820 - 00:43:30.970, Speaker G: Oh wait, I can probably be unmuted now.
00:43:31.340 - 00:43:32.904, Speaker A: Yeah, we can hear you.
00:43:33.022 - 00:43:33.736, Speaker H: Oh good.
00:43:33.838 - 00:44:47.388, Speaker G: So I just want to mention this thing on the subsecond that it's not so triggered and it's not so clear that you can actually take out time from the first part of a slot because aggregation becomes a problem and becomes an actual problem. We have a very large validator set that is getting larger and aggregating is taking over 2 seconds on a normal computer, especially if you're subscribed to all subnets. I've been monitoring this because we changed in prison the way we are aggregating an aggregated attestations. And on a normal nuke like mine, it would take up to 4 seconds to aggregate all an aggregated attestation if you are subscribed to all subnets. So that means that you cannot really realistically be a good aggregator if you are hosting more than 32 keys on a nuke. So what happens is that very large validators can run on very faster software hardware. But I think for small home takers you are not going to get good aggregation if you reduce the middle part of the slot.
00:44:47.388 - 00:45:25.150, Speaker G: So that leads us to shifting everyone and taking only seconds from the last part of the slot and the last part of the slot. I think it is safer to take some time out of it. But the problem is now with the reorg feature that we place a bet before the end of the slot on whether we are going to reorg or not. And I'm afraid that we're going to see a lot of split views if we make this last part very smaller. So I think that it's going to take a long time to actually get these numbers correct and getting good experimentation that actually vouches to how long we can increase the first part.
00:45:26.320 - 00:45:32.560, Speaker F: So what I don't understand is why is the relevant number all subnets? Why isn't.
00:45:34.500 - 00:45:53.956, Speaker G: The amount of assessations that you need to aggregate is going to depend on how many subnets you're subscribed to. And if you run more than 30 validators, you are going to be subscribed anyways to all subnets. So if you're running on a home computer, you can't really realistically run more.
00:45:53.978 - 00:45:58.720, Speaker F: Than two or three. You wouldn't be an aggregator on all of those subnets, though.
00:45:58.890 - 00:46:18.140, Speaker G: Well, but you are an aggregator quite often, and you are going to get all of these many more attestations, unaggregated attestations that you are going to need to aggregate. So by subscribing to all subnets, you're going to be getting a lot or more of unaggregated attestations.
00:46:18.960 - 00:46:19.324, Speaker B: Right?
00:46:19.362 - 00:46:31.616, Speaker F: But you cannot do all that work in parallel, right? Like presumably if you are running tens or hundreds of validators, you don't just have one cpu to run.
00:46:31.718 - 00:47:00.248, Speaker G: Yeah, the bottleneck is even we are parallelizing it and the bottleneck is in the BLSt library and it doesn't matter. Like I see Terrence asking whether or not clients are aggregating all at once or as they come. I benchmark this and it doesn't matter. It doesn't make much difference. Lighthouse aggregates as they come and prism just aggregates them all at prescribed times. Because the number of additions that you make is exactly the same. It doesn't really change anything.
00:47:00.248 - 00:47:34.090, Speaker G: True. So I truly don't think that we can't really subtract time from the middle part of the slot. We can measure it and we can try to benchmark it, but I would clearly see a degradation if we subtract parts from the middle part of the slot. And I think measuring the split views that will come from subtracting parts from the last part of the slot is a pain. So I think that we should keep our minds open that if we are forced to increase the first 4 seconds, then we may need to increase the slot.
00:47:34.510 - 00:47:38.590, Speaker F: I didn't understand why you can't parallelize it. Sorry, that doesn't make sense.
00:47:38.660 - 00:48:03.246, Speaker G: No, we can parallelize it. We are parallelizing and anyway, we are getting 4 seconds marks. So the typical aggregation for my computer is about 200 milliseconds. But then from time to time, when there's a missed block, for example, and you need to aggregate more, then you get up to 2 seconds and I'm running only three validators.
00:48:03.358 - 00:48:06.158, Speaker F: Why do you need to aggregate more when there's a missed lot?
00:48:06.344 - 00:48:21.690, Speaker G: Well, because you get a lot of attestations that weren't included before. If there's a missed slot, you have the aggregate the attestations from the previous slot. Then you need to aggregate with the attestations from this slot to include more attestations. And my computer takes up to 2 seconds.
00:48:22.030 - 00:48:26.518, Speaker F: That I don't understand because the previous ones you could already have done in the previous slot.
00:48:26.614 - 00:48:34.414, Speaker G: No, but you still need to aggregate with the current one. There's always late attestations as well.
00:48:34.612 - 00:48:38.174, Speaker F: Sure, but these, like you can just add, right? You've already aggregated the one.
00:48:38.212 - 00:48:50.210, Speaker G: Well, now there are. The algorithm to add is not so simple. It's simple to add when you only have one bit. But then you need to start aggregating aggregates and it's not trivial.
00:48:51.510 - 00:48:54.100, Speaker F: No you don't. I don't understand.
00:48:54.550 - 00:48:55.266, Speaker G: Yes you do.
00:48:55.288 - 00:48:55.860, Speaker F: Okay.
00:48:57.030 - 00:48:58.450, Speaker G: Coverage algorithms.
00:48:59.850 - 00:49:04.818, Speaker F: I had like 200 aggregated signatures. Now I received ten more that were late.
00:49:04.994 - 00:49:19.386, Speaker G: Yeah, but then the problem is that you're receiving ten more that had like two of them are with intersection with 100 that you had before and you cannot, you had a group of seven or ten that have different intersections.
00:49:19.578 - 00:49:22.718, Speaker F: Okay. Why do you have to aggregate the aggregates? I didn't know.
00:49:22.884 - 00:49:24.970, Speaker G: Because you want to have a better block.
00:49:25.130 - 00:49:25.840, Speaker F: Yeah.
00:49:31.170 - 00:49:39.854, Speaker E: Compared to nimbus, at least that sounds a bit more. Your numbers seem on the high end of things.
00:49:40.052 - 00:49:40.366, Speaker F: Yeah.
00:49:40.388 - 00:50:12.310, Speaker G: So I'm giving you the worst cases. So my computer takes very little. So the biggest chunk is in aggregating the one bit ones because we take them all at 8 seconds and we aggregate them all at ten. I'm sorry. We take them all at 4 seconds and we aggregate them at eight. And that one takes on my three validators. It's taking about like 25 milliseconds normally, but it gets to 2 seconds on bad locations.
00:50:13.710 - 00:50:48.630, Speaker E: So I'm looking at some numbers here actually, and specifically I'm looking at the delay from the start of the slot when attestations and aggregates arrive. And this was basically the number that I was asked for before. And just eyeballing it like it's in the 97% range that both attestations and aggregates are in 2 seconds after when they're.
00:50:49.130 - 00:51:23.182, Speaker G: Well, but this is a different issue. So I'm talking about different things. So in order for us to submit the aggregate at 8 seconds, what we're doing is start aggregating before so that the aggregate is already ready at 8 seconds. At 8 seconds is our deadline to submit the aggregate. What we are doing now is before 8 seconds. And this is adjustable by the user. We aggregate them all and you need that before to have enough time so that by 8 seconds you can actually send an aggregate.
00:51:23.182 - 00:51:30.980, Speaker G: Because at 8 seconds we're going to send whatever the node has because that's the deadline always going to be early.
00:51:31.830 - 00:51:55.574, Speaker F: I still feel like you're misrepresenting the issue because if nodes with two validators can manage in that time, then I feel like that's fine, that's great, because those are just going to get their aggregations in and we don't need the others, we don't need everyone to aggregate, we just need someone to aggregate.
00:51:55.622 - 00:51:55.978, Speaker B: Right.
00:51:56.064 - 00:52:04.218, Speaker F: And so people who run more validators actually need larger machines to run them, then that's not the end of the world in my opinion.
00:52:04.394 - 00:52:19.010, Speaker G: Well I do think that this is centralizing force. If we're going to have people that be able to be homestakers can only stake like one or two validators in our classes we're seeing 4 seconds.
00:52:21.750 - 00:52:25.970, Speaker F: You were talking about someone who subscribes to all validators.
00:52:29.370 - 00:52:49.740, Speaker G: That's correct. But this is the way that when we submit our clients with default values, we are typically looking at the worst situation, which is a validator that runs at least 30 keys and there's many of this. And those are going to be subscribed to all subnets and this is what needs to be our default. So our timing is going to be this regardless of whether you're a homesaker or not.
00:52:51.970 - 00:53:07.460, Speaker F: I think it's okay to require someone who is running 30 validators, which is like what? Like a few million in capital. I think they can afford a machine with a few more stpus to make those fast enough.
00:53:07.830 - 00:53:39.242, Speaker E: Something to note here though is that aggregation is not incentivized. And so we rely on aggregation, but we don't actually incentivize it. The only implicit incentive is that you aggregate your own view. I mean there's another point which is that when you're running 30 validators, you're not aggregating 30 subnets, you're aggregating much fewer. So there's 16 aggregators for every subnet. So your chance.
00:53:39.296 - 00:53:46.320, Speaker F: Yeah, we're really talking about a node that's running 1000 validators or something that would actually need to aggregate all of these.
00:53:47.730 - 00:53:59.406, Speaker E: Yeah. My point is that with 16 the number is fixed. The number of aggregators per subnet that.
00:53:59.428 - 00:54:09.590, Speaker G: The blocks are pulled. If you have many more aggregates there are worse aggregates like what a validator with only two subnets would have, then you're going to fill the blocks with less attestations.
00:54:12.810 - 00:54:15.750, Speaker F: Why would a node with two subnets have worse aggregates?
00:54:16.570 - 00:54:33.130, Speaker G: Because they see less attestations. Nodes that subscribe to less subnets they get less peers. Yeah, but smaller nodes see many less peers and many less unaggregated attestations than larger nodes that are getting much more peers.
00:54:33.290 - 00:54:33.934, Speaker F: Is that true?
00:54:33.972 - 00:54:40.426, Speaker G: So my node is a much worse aggregator than the node in our prism like kubernetes.
00:54:40.458 - 00:54:49.202, Speaker F: I don't understand why would, if I'm subscribed to Subnet one, why would subscribing to Subnet two as well make me see more attestations on subnet one?
00:54:49.256 - 00:55:14.314, Speaker G: No, it's also dependent on the number of peers that you have. If you're a home staker like myself on a bandwidth of a home that I am restricting my number of peers to be, I don't remember if it's 30 or 50 now by default. Then I see many many less attestations that someone running with 200 peers on a cluster and we do want to aggregate those.
00:55:14.512 - 00:55:16.620, Speaker E: That's not how the protocol works.
00:55:17.070 - 00:55:18.250, Speaker F: Yeah, that's not how it works.
00:55:18.320 - 00:55:37.282, Speaker E: The number of peers is completely irrelevant. The only thing that's relevant is the gossip submesh and that one is kept at, let's say 812 depending on if you look at average or max. So you can be subscribed to eight peers and see the exact same traffic that if you're subscribed to 200 pairs it doesn't really matter.
00:55:37.336 - 00:55:37.940, Speaker G: Right.
00:55:41.030 - 00:55:55.286, Speaker E: Your aggregate, when you're creating a block is created from listening to the aggregate channel, not from listening to the attestation channel typically. So the aggregators are doing that work for you basically. And there's 16 of them, right?
00:55:55.388 - 00:56:07.290, Speaker G: No, but I'm talking about the aggregating the one bit attestations, which is when you are an aggregator, not when you're aggregating aggregates. The largest chunk for us is aggregating one bit attestations.
00:56:10.030 - 00:56:48.890, Speaker E: Yeah, and the risk of you being one of those guys is pretty small. That's the 16 aggregators that exist per subnet and you're never subscribed to all the subnets. And again, this is not a function of how many peers you're connected to. That is completely irrelevant. So I think this deserves some more investigation and I think we should take it offline and we can go through the flow, but it certainly merits investigation, like in depth investigation of all these issues if we're going to change these timings.
00:56:50.670 - 00:57:02.990, Speaker G: In the channels, these benchmarks that we have, because we changed these algorithms, because of these numbers that we were seeing both in my clusters and phone computers and we were seeing very large times for aggregations.
00:57:05.880 - 00:57:06.630, Speaker D: Yeah.
00:57:09.000 - 00:57:10.710, Speaker G: I'll post the pr as well.
00:57:11.080 - 00:57:43.970, Speaker E: Yeah, I'm just saying that your peer count. Not relevant. That's not how the protocol works, but changing the timings. The other thing I wanted to say actually, was that we don't have to put all the time from the point where we do the attestation or the point that we do the aggregate. We can, we can play around with that a little bit. It doesn't have to be evenly divided or now it's evenly divided and we want to divide it in a different way. But it doesn't have to be.
00:57:45.940 - 00:57:50.492, Speaker F: Six. It would even be whatever plausible to make this flexible.
00:57:50.556 - 00:57:51.120, Speaker B: Right.
00:57:51.270 - 00:57:56.740, Speaker F: To not have a 100% clear division between adaptations and aggregation.
00:58:00.520 - 00:58:12.970, Speaker E: Yeah, flexible. I don't know, because that feels like something that somebody could exploit, but it could be two and six or it could be three and three and whatever. Right.
00:58:19.080 - 00:58:19.830, Speaker A: Right.
00:58:22.540 - 00:58:45.790, Speaker F: Even a 1 second increase on the first third I think would be huge, because in my observations there are already, under normal networking conditions, slots like 3.5 seconds in. It's not rare to see that. So if you add one more second, that's going to have a huge benefit to the network already.
00:58:53.230 - 00:58:53.546, Speaker D: Yeah.
00:58:53.568 - 00:59:21.640, Speaker A: I think we can all agree it would make sense to have more breathing room on the front end. And so the question now is just what are those numbers? What should they actually be? There's a lot of data people have been referencing. I think moving this to the consensus dev or some async channel is a great idea. Obviously this is important and we should keep looking into it, but, yeah, we need a lot more investigation before we can just say, oh, let's make this 6 seconds, let's make this 2 seconds, or however it ends up.
00:59:23.690 - 00:59:24.534, Speaker D: Yeah.
00:59:24.732 - 00:59:44.870, Speaker A: So let's keep the conversation going, but we'll take it offline from here. So next up, we do have an agenda item to discuss this proposal. Mike, do you want to talk about the max effective balance change?
00:59:45.400 - 01:00:40.496, Speaker D: Yeah, sure. And actually this kind of flows really nicely from the previous discussion because the goal of the proposal, I'll link it in the chat, is to reduce the validator set, which hopefully would help with aggregations kind of not only reducing the validator set as currently, but also slowing down the rate of growth for the new incoming validators. So I'll just give kind of a high level overview of the proposal. We have a few docs that I'll link that kind of outline the pros and cons, and then maybe we can just open up to discussion as far as some of the design decisions go. So yeah, the kind of TLDR of the proposal is increasing the max effective balance. So this doesn't change the 32 ETH minimum balance to become a validator, but it allows validators to go above that. We've kind of proposed 2048 ETH as a potential upper bound.
01:00:40.496 - 01:01:32.980, Speaker D: We don't want to make it infinity as far as how big a validator can get, but yeah, going up to 2048 we think could be like a reasonable choice. Some of the benefits we outline from the roadmap perspective, we talked about how kind of slowing the growth of the validator set will be important for single slot finality. Daplion brought up the point that intentionally being blocked on the current validator set size, though that is like a little more of an under debate thing. We also talk about the benefits for the current consensus in PTP layers, so that's kind of what we were just discussing. As far as aggregations taking a really long time. There's a post from Aditya on unnecessary stress on the PTP network. I'll link that here.
01:01:32.980 - 01:02:30.120, Speaker D: So he wrote that before we published the modest proposal. But yeah, just kind of talking about some of the numbers on the PHP layer of how many messages are being passed around, and all these kind of unnecessary bloat from all of the validators. And then we also talk about some of the benefits for the validators. From the solo staker perspective, it gives this kind of like auto compounding benefit which people seem really interested in. The kind of key takeaway here is that with the current 32 E max, the sweep just takes all your rewards and withdraws them. So any solo staker just would have to redeploy that capital somewhere else to earn any yield on it, versus if we increase the max effective balance, they immediately start compounding that eth. So they're earning rewards on more than just the 32 ETH that they kind of initially deployed.
01:02:30.120 - 01:03:28.952, Speaker D: We also talk about the potential benefit for larger node operators who wouldn't have to run as many validators. This is kind of like a big part of the consolidation would depend on the larger validators actually doing the consolidation. There's some risk associated with it because the slashing conditions would result in potentially like higher slashing if they accidentally double a test or they double propose. So there's some risk associated with it. But in general, for large validators like Coinbase operates something like 70,000 validators, that 32 e cap kind of artificially inflates that number for them. And various staking operators have expressed interest in reducing the number of validators that they run, or consolidating their validators into higher stake, but fewer of them. So yeah, I guess that's kind of the high level overview.
01:03:28.952 - 01:04:15.960, Speaker D: A few of the big questions, I think are the design trade off of the UX versus the complexity of the spec change. So in the proposal we link to kind of a minimal view spec pr. Let me get the link for that. And it's super tiny. It's 58 lines added, 21 lines removed. So the goal here was to show how small the spec change could be. But this has some, I guess some UX inefficiencies in terms of if someone wants to get the auto compounding effect, they have to actually withdraw and deploy with a new withdrawal credential with the two prefix instead of the zero x one prefix.
01:04:15.960 - 01:05:37.910, Speaker D: There's also this idea that I guess staking pools wouldn't be able to migrate or to consolidate without pulling their validators out and redeploying them with a 2048 validator. So they'd have to deploy 120 48 validator, they'd have to exit 64 32 e validators and then deploy a single 2048 one. So I think it's worth discussing a bigger potential change to the spec that makes the UX better and more desirable for people. And then I guess the other big question that's come up, and Doncrat has mentioned this a number of times, is how do we actually get the consolidation to happen? If we make the change, it's only worthwhile if it actually results in a meaningful difference in the validator set size. And if the UX is so bad and there's kind of no incentive for them to do it, the big stakers might not do it. There might be some social capital that they gain by doing something that is healthy for the ethereum network and improves the overall PTP layer, but it's not totally clear that they would take advantage of the consolidation and make it worthwhile. So I guess those are the big questions in my mind.
01:05:37.910 - 01:05:51.640, Speaker D: Happy to open up the discussion here and also take questions in the Discord channel later if that's useful, but that's kind of the high level. Thanks.
01:05:51.710 - 01:05:54.424, Speaker A: I have a question just on your last point.
01:05:54.462 - 01:05:54.616, Speaker D: There.
01:05:54.638 - 01:05:59.320, Speaker A: Is the auto compounding not enough of an incentive to sort of migrate to this regime?
01:06:00.300 - 01:06:42.490, Speaker D: Well, so for big stakers, they can kind of take advantage of compounding by just, they have the automatic withdrawal sweep. Right. And then they can just deploy. So for Coinbase, they're running like 60,000 nodes. That gives them enough to deploy like nine new validators every day just through the withdrawal sweep, so they don't have to really, the auto compounding doesn't benefit them as much as it benefits the little guys. There could be a case to be made that because the withdrawal sweep takes a long time, like it's 40 days or whatever, that capital is effectively dead while it's waiting to get out. And so the auto compounding might help them.
01:06:42.490 - 01:07:03.040, Speaker D: But, yeah, I think other than that, it's not obvious that it's strictly financially better for them to consolidate. Oh yeah, sorry, the activation churn. Yeah, thanks, POTUS. That's the 40 day thing that I was meaning to reference.
01:07:07.380 - 01:08:11.856, Speaker H: Especially since for solo stakers or small stakers, one of the bigger benefits was auto compounding. Yeah, I'm with you on improvements to user experience. Make it easier for solar stakers, at least this current they proposed. This was brought up, but the max effective balance being 2048 is so high, a small staker would almost never experience a withdrawal. We have to manually withdraw the whole thing at this point to have a withdrawal. So I definitely do think that UX is improved or needs to be improved in order for small stakers to take advantage of this. And then another point I noticed in deposit processing or applied deposit, basically people can top up their, if they, if they opt into being a compounding validator, let's say they can top up their validator with a deposit.
01:08:11.856 - 01:08:34.940, Speaker H: And there is like a limit that you can only top it up by 32 e, but it seems to be a limit per deposit. And so it seems like they could, I mean, they don't have to really wait the activation queue, they just wait the 16 to 24 hours deposit queue.
01:08:35.840 - 01:09:06.688, Speaker D: Yeah. So this is actually something we tried to cover in the spec, because you're right, if they are able to kind of get around the activation queue, then all of the kind of churn invariants are broken. The way we got around that is just to say, if you top up past 32 e, you can't top up past 32 eth, basically. So, yeah, that's kind of like a brute force way of doing it. But yeah, this did come up and it should be covered.
01:09:06.864 - 01:09:16.616, Speaker H: Isn't that a limit per deposit? Like you could submit a deposit for 32 e, wait for that to process, and then submit another deposit for 32.
01:09:16.638 - 01:09:33.840, Speaker D: E. So basically how it works is if we see that the deposit changes the effective balance to be greater than 32 e, then the effective balance of that validator, then we just say, okay, the effective balance is only 32 e. So even if the deposit is processed.
01:09:34.980 - 01:09:50.420, Speaker G: How do you implement this thing? You're going to have a validator. You need to keep track of how much part of this balance in this validator was added in this way or was added correctly. Are you sure about this?
01:09:50.570 - 01:09:57.064, Speaker D: No, it's just a mechanism to stop them from topping up past 32 e. We don't care.
01:09:57.102 - 01:10:03.290, Speaker G: How do we keep track of this? So how is this mechanism you don't allow.
01:10:05.820 - 01:10:18.300, Speaker C: So the logic is if this public key is known and the effective balance is already a 32, we ignore the value deposited. So it's effectively burned.
01:10:18.880 - 01:10:19.630, Speaker D: Right.
01:10:21.060 - 01:10:34.580, Speaker G: So you're declaring that deposit to be an invalid deposit. The contract is already deployed. That sounds like something that is going to lead to a lot of people being burned.
01:10:36.280 - 01:10:37.510, Speaker H: That's kind of true.
01:10:38.920 - 01:11:00.376, Speaker D: Yeah, we talked about this. I think. Why would someone top up past 32 e? People can burn Eth in a lot of ways. Why would they top up past 32 Eth? If we tell them explicitly that that ETH is going to be burned, I agree that it would be something we'd have to call out. But I also don't think that that would be a normal pattern.
01:11:00.488 - 01:11:12.300, Speaker H: At the very least, you'd have to, in the tooling in the front ends that deal with topping up a balance, tell them that you're going to burn it for sure.
01:11:12.370 - 01:11:12.844, Speaker A: Yeah.
01:11:12.962 - 01:11:24.500, Speaker H: Because a lot of people aren't going to know this. A lot of people didn't know that there was a leaf o queue for the PoS activation change. Like, people just don't know these things about the protocol.
01:11:30.520 - 01:11:52.430, Speaker D: Yeah, I mean, we could also maybe think about ways that the spec, like we tried to make the spec PR super minimal and maybe there's a way that we can make deposits part of the churn limit and say, okay, this is what we have the invariant of like one over two to the, it's like one over 65,000 per epoch can't change or whatever.
01:11:55.440 - 01:12:20.028, Speaker F: This sounds extremely drastic. I think this is no go. And to many, this has actually happened a lot of time that people accidentally topped up above 32 e. So I think there's no way we're going to do that. So drastic for a small mistake. But I also don't understand why would you want to stop people from topping up?
01:12:20.214 - 01:12:22.496, Speaker H: You're skipping the activation queue.
01:12:22.688 - 01:12:31.930, Speaker G: Yeah. The problem is if you get a lot of deposits like this, then you get a big change in the validator set quickly. But I think we can just churn this and that's it.
01:12:33.020 - 01:12:36.410, Speaker H: That or especially something that would help.
01:12:36.780 - 01:12:42.840, Speaker F: Then deposits just needs like the activation queue just needs to be replaced by deposit queue.
01:12:43.000 - 01:12:54.850, Speaker G: Right. And I think we can do this at the same time that we get rid of all of this e one voting, because anyways, we're thinking on churning those so we can just mix these two things at the same time.
01:12:56.500 - 01:13:21.600, Speaker H: True. In terms of user experience, I know this is historically unprecedented, might add a lot of complexity, but another complete other route is, is it possible to add a beacon transaction to combine validators.
01:13:28.280 - 01:13:35.480, Speaker A: So I think the proposal right now is trying to keep things as simple as possible and adding new types of operations is definitely be more complexity.
01:13:37.980 - 01:13:52.430, Speaker D: Yeah, I think that's kind of the trade off we keep circling around here is like spec change complexity versus UX. But I think it's worth kind of exploring many different avenues and seeing what makes the most sense.
01:13:53.040 - 01:14:12.130, Speaker G: Mike, I think everyone more or less agreed about getting rid of Eth one voting, and there's already a spec towards that. So as long as we move towards putting the churn on the deposit queue instead of the activation queue, I think we can just mix these two prs into one.
01:14:14.420 - 01:14:15.830, Speaker H: Yeah, that's a good idea.
01:14:16.440 - 01:14:26.324, Speaker D: Yeah, that's helpful. I haven't been kind of keeping up with the e one voting thing, so I'll have to do a little research there, but that sounds promising to me.
01:14:26.522 - 01:14:31.130, Speaker C: I mean, the latest spec doesn't have a queue, but it can be brought back.
01:14:33.100 - 01:14:41.310, Speaker G: That was just a matter of terminology. Right. Because you still have the churn. So the issue of the queue being on the state or not in the state I think is minor here.
01:14:51.300 - 01:15:09.620, Speaker H: And the other, is it also like pretty heavily planned or leaned into that we're going to enable execution layer with initiated withdrawals.
01:15:15.260 - 01:15:23.530, Speaker D: Yesterday, I think it seems like. Yes, that will happen. I'm still not totally clear on the relationship between that and this, though.
01:15:24.460 - 01:15:30.140, Speaker H: It's just that if we also combine that with this, it would drastically improve ux.
01:15:32.240 - 01:15:41.216, Speaker D: Yeah. How do the execution layer withdrawals impact the churn? Maybe that's a question for offline, but.
01:15:41.398 - 01:16:00.900, Speaker H: It'S not the churn. It's just the fact that if you're a small staker, you basically never withdraw. If you have one validator, you have to generate 2048 e off of that one, even if it's compounding. So you need another way of initiating withdrawal.
01:16:02.840 - 01:16:11.210, Speaker D: Right, but I was just asking, if you do an execution layer triggered withdrawal, do they have to go through the withdrawal queue too?
01:16:12.220 - 01:16:14.570, Speaker H: Okay, they would have to, yeah.
01:16:16.780 - 01:16:18.216, Speaker C: The point is that, yeah, the proposal.
01:16:18.248 - 01:16:21.870, Speaker A: Is for exits, and it would just move into the execute like it does today.
01:16:23.360 - 01:16:35.644, Speaker C: The point here is, if we can do partial withdrawals triggered from execution, then we could get rid of partial withdrawals. Automatic partial withdrawals.
01:16:35.692 - 01:16:41.632, Speaker A: And you're saying you would do that to add this back into the proposal now that max talking about.
01:16:41.686 - 01:16:42.290, Speaker D: Well.
01:16:44.020 - 01:16:46.260, Speaker H: I was actually talking about partial withdrawals.
01:16:48.040 - 01:16:48.356, Speaker D: Yeah.
01:16:48.378 - 01:17:11.950, Speaker C: So the point is, today, if you're a solo staker, you have one moderator. You need to at least capture some value to pay for expenses and what's or not. If we disable partial withdrawals such that you have automatic compounding with the feature that Mike is presenting, there has to be some way for you to extract a value, fractional value of your validator without having to exit the full thing.
01:17:12.720 - 01:17:19.550, Speaker D: Right. So execution layer initiated partial withdrawals, not full exits, basically.
01:17:19.940 - 01:17:22.160, Speaker H: Although we would presumably need both.
01:17:22.310 - 01:17:23.010, Speaker D: Right?
01:17:25.620 - 01:17:29.728, Speaker F: I mean, that would be cool. Then we could get rid of needing an extra address.
01:17:29.814 - 01:17:30.208, Speaker B: Right.
01:17:30.294 - 01:17:44.900, Speaker F: We could just switch all validators to this functionality, and everyone just withdraws whenever they want. So that seems a lot cleaner than the current proposal.
01:17:45.880 - 01:18:00.270, Speaker G: One problem I see with this is that you need to bound the amount that you can actually withdraw on a partial withdrawal. Otherwise you're going to get a large change in effective balance in one slot, which might be a problem.
01:18:08.880 - 01:18:35.970, Speaker D: But wouldn't the partial withdrawal have to go? So the proposal is written to rate limit the activation and the withdrawal queue based on stake rather than number of validators. So if the execution layer partial withdrawal goes through the normal withdrawal queue, then that rate limiting should be fine. Right? Like, we just have to make sure that the rate limiting is correct.
01:18:40.740 - 01:18:53.720, Speaker H: Yeah, but it ends up, as currently written, there is no limit for partial withdrawals, only full withdrawals, because that actually affects the validator set. But when you actually have.
01:18:53.870 - 01:18:57.720, Speaker G: The proposal is only to trigger exits, not to trigger withdrawals.
01:18:58.460 - 01:19:17.996, Speaker H: Right, but if we do enable partial withdrawals this way, now you can withdraw even more than 32 eth and still not technically exit your validators. So you've withdrawn a validator's word without going to the exit queue.
01:19:18.028 - 01:19:18.224, Speaker E: So.
01:19:18.262 - 01:20:02.540, Speaker D: Yeah, sounds like some more details to work out, but this is potentially promising. I guess the one thing about this is it would change the default behavior. Part of our design goal for the first spec was like if people don't want to change anything, then we wanted to leave that there. So that's why we left the one credential alone. But if we change everyone to compounding with these execution layer triggered partial withdraws, then a lot of workflows would have to be updated. I don't know if that's a big enough reason to not do it, but it's a consideration.
01:20:15.940 - 01:20:31.510, Speaker A: Okay, well, thanks for bringing this up Mike, and hopefully that was helpful feedback. Thanks everyone for the conversation. Is there anywhere, Mike you'd want to drive further feedback like, I guess just to the e three search post?
01:20:31.960 - 01:20:47.470, Speaker D: Yeah, e three search post or I think Danny suggested that discord, the Pos consensus channel could be a good place. So yeah, I think I should be pretty easy to get in touch with, but would be happy to hear more feedback. So thanks everyone.
01:20:52.640 - 01:21:00.690, Speaker A: Okay, great. Are there any closing or final comments for this call? Otherwise we'll go ahead and wrap up.
01:21:03.680 - 01:21:08.830, Speaker I: I wanted to bring up an update on the testnet call we had just before this.
01:21:09.760 - 01:21:10.750, Speaker A: Yeah, please.
01:21:11.680 - 01:21:54.590, Speaker I: Yeah, so we had the first Holsky testnet call about an hour ago, and we're going to have the next one again on June 29 with a couple of asks. I can link the summary over here, but one of the big questions that we're still open is the current idea is to start with about a million and a half validators so that we have significantly more than Mainnet and we don't have to rush to immediately make deposits to keep ahead. We're just not sure if all clients think that they'd be ready for such a big validator set of Genesis or such a big genesis state. So just looking to hear some thoughts on that.
01:22:05.430 - 01:22:12.150, Speaker C: Will we ever reach that in Mainet? What percentage of total its supply stake will that represent?
01:22:14.490 - 01:22:40.460, Speaker I: I mean, we're at 600,000 now with a queue of about 100,000. So we're already at 700,000 in a couple of months. So we double of what we have right now. We're also open to starting with a smaller number, like a million, but that won't give us as big a difference.
01:22:41.310 - 01:22:51.120, Speaker A: So has anyone on this call tried a genesis state that big? I would say if we can get away with the million and a half, may as well.
01:22:53.090 - 01:23:02.100, Speaker C: What I mean a million a half is 40 million eth staked. So 30% of total supply. If we do that in many, that will be pretty crazy.
01:23:03.910 - 01:23:14.310, Speaker A: Would it? I don't know. I think if you talk to some of these liquid staking people, they want all the stakes. I don't think the numbers are completely.
01:23:14.380 - 01:23:16.486, Speaker C: Stake in the beach and chain and no one can do anything.
01:23:16.668 - 01:23:24.700, Speaker I: I think when we started Prata, if someone had said 15% of all eats was going to be staked, we would also thought it was crazy. But we're already here.
01:23:25.630 - 01:23:26.090, Speaker D: Yeah.
01:23:26.160 - 01:23:33.040, Speaker C: I'm not opposed like seeing the difficulties we had in Pratt. I will rather start with a big one so we can optimize the clients and get done with it.
01:23:37.750 - 01:23:38.500, Speaker B: Yeah.
01:23:39.350 - 01:23:42.130, Speaker D: I also support the million and a half solder.
01:23:45.270 - 01:24:12.640, Speaker G: I have a suggestion as well too. I'm not sure how hard is it to do, but one of the things that we're seeing on Mainet is that we now have some validators that are exited. Even if we start with a large number of deposits, we may increase even the validator slice by just adding validators that are already exited at Genesis. So that we start with. We don't need to have a large number of validators sending attestations, but the slice itself is still large.
01:24:13.810 - 01:24:15.246, Speaker C: That's a great point.
01:24:15.428 - 01:24:17.470, Speaker I: Yeah. We can also take that into account.
01:24:17.540 - 01:24:24.542, Speaker D: Thanks. Yeah.
01:24:24.596 - 01:25:10.000, Speaker I: Besides that, we're looking for client teams involvement to run at least majority of the validators. There's a Holsky planning doc that's been shared on the chat already that states what the requirements would be like, what you can expect to get away. In terms of machines. It is a bit of investment. In terms of money, we're looking for commitments, like solid commitments from client teams by the 29th. And if not possible, then we're going to look at node operators to help us get to the one and a half million or 1 million or whatever number we decide on. So please talk to your infrastructure teams and so on and try and get back to us before the next call.
01:25:16.710 - 01:25:19.650, Speaker A: I'm sorry, you said that was the 29 June.
01:25:23.110 - 01:25:23.860, Speaker D: Okay.
01:25:29.790 - 01:25:32.214, Speaker I: Yeah, I think that's it on the whole ski topic.
01:25:32.262 - 01:25:32.860, Speaker D: Thanks.
01:25:34.670 - 01:25:59.300, Speaker A: Yeah, thanks for bringing it up. It's very exciting to see progress there. Okay, anything else? Otherwise we'll wrap up a few minutes early. Okay, I'll call it. Thanks, everyone.
01:26:00.310 - 01:26:01.726, Speaker D: Thanks, Alex Divine.
01:26:01.838 - 01:26:03.730, Speaker B: Thanks, everyone. Bye, Alex.
01:27:04.350 - 01:27:04.840, Speaker D: Thank you.
