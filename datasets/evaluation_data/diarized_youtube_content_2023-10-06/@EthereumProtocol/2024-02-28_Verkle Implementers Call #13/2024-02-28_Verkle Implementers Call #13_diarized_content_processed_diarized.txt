00:00:00.570 - 00:00:01.120, Speaker A: It.
00:00:01.970 - 00:00:55.120, Speaker B: Okay this is virtual implementers call number 13 and issue nine five, eight in the PM repo we have a few agenda items today just a few actually might be a quick call after we go through some updates Guillaume has some updates on the shadow as well as the testnet if we have time and pavel is here we have agenda item that we did not get to last week the proposal for code packaging and last up on the agenda we have gagender on EIP 2935 block hash so yeah anyone have any updates they'd like to start with?
00:01:07.810 - 00:03:27.290, Speaker A: Yeah I guess we'll start yeah so there was a shadow fork we're going to talk about this in more details in a few minutes. That involved some reworking of the kurtosis tooling so I think pari is on holiday this week so just mentioning that there was some DevOps work involved and they did a very good job to speed things up so that really helped otherwise yeah I'll let Ignacio talk about the investigation he did also Ignacio worked on the replay so if Ignacio you want to say something about this but yeah just before we were discussing with other teams to try to investigate bugs on the testnet I did some work on the transition tool for testing so there was some back and forth with Mario who's also not on that call today and yeah I think at this point it's clear that we need to start working on I'm saying we geth needs to work on the rebase of vertical on top of Cancun because we had the holy ski shadow fork working but yeah now we need more data to keep testing to keep pushing and that will come with sepolia and Mainnet and I think at this point it's clear that we need to get that. Otherwise I think in the next week I will work on that rebase it's not going to be done in a week but at least make some progress on that because there are also some developments in guests on the guest side. We're going to reactivate archive mode hopefully in the next release and that's a good point. That's a good moment to also integrate more oracle code and I want to start investigating the chunk fill cost that were part of the EIP 4762 that we're not integrating on the testnet currently. I don't think we should integrate it for the time being still but I would like to spend some time on that. And yeah Ignacio did two things so I'll give him the floor.
00:03:33.110 - 00:04:31.510, Speaker C: Yeah we get a quick update. So apart from some fixes that we did for costing. Regarding bugs that were present that we discussed in the previous call. Yeah, I'm working on some have this replay of importing a chain now on rebase fixes that we did on costine end in the last weeks. So there are some problems there that I have to investigate. But apart from all this main stuff, I try to make some progress on trying to answer this question about if in Berkeley trees we have a due respect regarding making a branch deeper than expected. All this was motivated by the plug hash discussion and if using ring buffers is a problem or not, and things like that.
00:04:31.510 - 00:05:39.550, Speaker C: So I think I just wanted to make some more. I wouldn't call formal approach, but trying to see if this is really a problem or not, and put this problem to rest. Hopefully if we have to do something about it or not, because it's very confusing to people if developers have to do something about it, or we have to change the purple tree design in some way or not. So what I did, I will just talk very briefly about this, because I don't want to get into details. I share in the chat hack MD document that I shared with Godfrey and Kev some days ago, so they didn't have much time to review it yet. But I got some feedback and the idea here is just to do some semi naive attack on this kind of dos vector. Because I think that the summary of this topic is that we have to answer two questions.
00:05:39.550 - 00:06:35.226, Speaker C: The first one is what is like a practical result that an attacker can achieve on producing a branch of a particular depth. So let's say that the expected worst case depth that an attacker can achieve is 13 or 15 or something like that. So we need have an idea of what is that number. So that's one question. And the second question is, we need some idea from execution client teams if updating a branch of this depth is really a big performance hit for them or not. So I tried to answer both questions in this document. I did some kind of poc code to generate a branch of a given prefix.
00:06:35.226 - 00:07:45.670, Speaker C: And I have some numbers on different cpus, which are obviously not really that powerful compared to what an attacker could have. And also I created a benchmark for get into to understand better how much time it takes to update the roots of the tree on different branches with different depths, as to try to have a sensation of what kind of depths start to feel as somewhat uncomfortable. And I did this in two setups. One in my desktop cpu, which is a semi modern cpu, let's say, and on a rockpi B, which is like a really low hardware. Yeah. So the bottom line here is, I was just chatting with Godfrey that was giving me his feedback now. And it seems like a reasonable number to expect that an attacker can achieve is a depth of 13 or 16 in that range.
00:07:45.670 - 00:08:00.460, Speaker C: So that's a good ballpark to have in mind. And now it's more like trying to answer the question if having branches of this depth is really a problem for clients or not.
00:08:03.710 - 00:08:05.770, Speaker D: How do we get 13 to 16?
00:08:06.670 - 00:08:08.540, Speaker C: That's very high.
00:08:09.570 - 00:08:52.360, Speaker D: Depth of ten would be 80 bits. And, I mean, 80 bits seems like a very. I think you can get higher using collisions, but collisions are not going to give you the ability to attack a specific address. You can create random ones that are. But if you want to attack a specific contract, say that stores this ring buffer, then you have to get a pre image. And I would think that ten would be hard. And I think I expect attackers to get, at most, to something like eight.
00:08:53.130 - 00:09:04.960, Speaker C: Right. Maybe Godry wants you to talk about this, but I think that was exactly his conclusion. I think that this 13 to 16 is considering a collision, not really like a specific target of address.
00:09:05.490 - 00:09:12.766, Speaker D: Right. But that is not relevant, then, to attack something that we want to store in a state at a specific address. Right?
00:09:12.948 - 00:09:32.390, Speaker C: Yeah, correct. I mean, if it wants to attack a really hot branch, it shouldn't be reasonable to expect that kind of depth. So, yeah, I think that's a good point to clarify. So maybe a more specific attack to a branch is around ten and 13.
00:09:34.730 - 00:10:19.720, Speaker D: Yeah, I mean, ten is very hard. I think, realistically, yes. If we want to be strict and say we want 128 bit security, then it could be up to 16, I guess. But realistically, this attack is not going to bring down ethereum. It's just going to make blocks slightly larger or slightly slower to process. So is someone going to invest trillions of dollars to attack it? No, I think maybe we can set the cap at $10 billion or something. And then I think ten is really the max that I could ever expect.
00:10:21.450 - 00:10:48.926, Speaker C: Cool. Yeah, I mean, this is exactly the kind of, let's say, conclusions that I was trying to find. So that's pretty helpful. I think it's still worth that each client do this kind of benchmark to see how much time it takes to update the root node on different branch depths, just to be sure, because get. Maybe using a library is very optimized, but, yeah, this is very helpful, I think.
00:10:48.948 - 00:10:53.810, Speaker D: Godfrey will, is this still only about the block hashes?
00:10:56.150 - 00:10:59.250, Speaker C: Not really. This is like a more general concern.
00:11:00.070 - 00:11:09.810, Speaker D: Okay, I see. Because for the block hashes, I think it's not really a concern because we have to use that branch at most once per block.
00:11:09.890 - 00:11:10.182, Speaker A: Right.
00:11:10.236 - 00:11:19.430, Speaker D: Because we can batch all of the transactions together, and so both the size and the access times are pretty bounded, I would assume.
00:11:21.130 - 00:11:32.030, Speaker C: Yes, well, yeah, I got freeze right in the chat, but maybe we can keep with the updates and jump back to this topic if you want to keep discussing.
00:11:39.860 - 00:11:40.512, Speaker A: Cool.
00:11:40.646 - 00:11:42.080, Speaker B: Any other updates?
00:11:46.820 - 00:11:48.770, Speaker E: Yeah, hello, I can.
00:11:52.100 - 00:12:10.920, Speaker A: Outside the Constantine library, which includes all the matrix for vertical, is complete and is on par with. And in parallel, we are also starting the integration into the Nimbus client.
00:12:14.400 - 00:12:16.510, Speaker E: Yeah, that's it. Thank you.
00:12:22.660 - 00:12:38.230, Speaker F: Yeah, I'll just give another reminder that the pedison hash, this serialized commitment method that we're using in pedison hash, is being changed. This is a breaking change for.
00:12:40.380 - 00:12:40.696, Speaker A: I.
00:12:40.718 - 00:12:50.040, Speaker F: Think Nimbus needs to update and I believe, nevermind everyone else, I've sort of pushed a branch or a pr to do the update.
00:13:00.020 - 00:13:23.220, Speaker G: For Ethereum Js, we have worked a little bit on developing a pre image manager, which Gabriel, if he wants to talk more about on Dorsa side, I can also pick up the rebase to Dennis so that I can support Guillem in his rebase. And yeah, that's.
00:13:26.380 - 00:13:26.852, Speaker A: Yep.
00:13:26.916 - 00:14:00.050, Speaker H: As Ishinder was saying, we've developed pre images manager to be able to handle pre images saving on the Ethereum Js client, which is necessary for the transition. We initially had a PoC that through testing we noticed was missing some things that happen at the block level and the transaction level, like withdrawals pre images as well as the coinbase. So that's been addressed and it should be merged into our production Ethereum Js client this week.
00:14:11.270 - 00:14:40.670, Speaker F: One thing, while we're on Ethereum Js, we had a meeting, and I think one thing which is going to stop the full integration for Ethereum Js is the fact that proofs are being serialized on the vertical tree sort of layer. I think it was brought up a while back. Are we ready to sort of only make proofs serializable and deserializable on the IPA, sort of on the cryptography layer.
00:14:44.610 - 00:14:48.130, Speaker A: So you mean like having opaque proofs?
00:14:48.790 - 00:14:54.740, Speaker F: Yeah. So only the cryptography can deserialize and sort of make sense of what it is?
00:14:58.710 - 00:15:35.470, Speaker A: Yeah, we could. It's just that it's such a nice tool to debug. I would rather push this, make this change as late as possible. But we could, we just have to write something that deserializes like if you take Dora, for example, the Dora the Explorer, which is the explorer, the block explorer we've got on take. I just take the JSON and I dump all this information. Right, but you mean the cryptography. That would only be the cryptography.
00:15:35.470 - 00:15:47.522, Speaker A: Yeah, I guess we could do this. You would just need to provide a. I don't know. Yeah. Do you have a write up of what you would like and then we could make it happen?
00:15:47.576 - 00:16:23.390, Speaker F: I guess I have a write up, but essentially everything that we considered the proof is now just an opaque byte. So like ClcR, the things that I think wouldn't be considered part of the proof would just be like the keys and values. Because the cryptography doesn't need to make. Well, it does use it, but I think Veracle also uses it. So that's not sort of considered a part of it. I can write up an issue for it just to be more concise.
00:16:24.210 - 00:16:28.160, Speaker A: Yeah. If you don't mind, then we can just refer to it and implement it.
00:16:45.430 - 00:16:46.770, Speaker B: Any other updates?
00:16:52.370 - 00:16:53.694, Speaker A: Cool. Okay.
00:16:53.812 - 00:16:57.810, Speaker B: Up next, Guillaume with the shadow fork update.
00:16:59.510 - 00:18:09.400, Speaker A: Yeah, so that's going to be very short. There was a shadow fork of holesky that was like the simplified version of it. We were not running any transactions, but it was about testing that we could perform an overlay transition with the right real life data. So it revealed a lot of issues, namely that at least in GEF, but I'm sure in every other client, there's a reintrancy problem. You have on the one hand the miner that produces blocks, and on the other hand you have the API engine that can potentially preempt and cause some issues. So this got fixed with a simple lock, of course, which is not as efficient as I would like, but at least the proof of concept is there. More tests need to be done, more refining of the locking system needs to be done, but at least it's proven to be working.
00:18:09.400 - 00:18:44.580, Speaker A: So there's a branch, it's a pr that's on guest. On the guest repo, more exactly, my guest repo, that's called. I'll actually just copy paste the pr in the chat, but it's called post genesis transition, and that's the branch that you want to look at if you're interested in looking at it. But there will be more updates to that branch as we keep testing Shadowforks. Yeah, that's pretty much it. Unless there are questions.
00:18:56.720 - 00:19:01.692, Speaker B: Cool. Did you want to keep going with any updates on the testnet as well?
00:19:01.746 - 00:19:51.660, Speaker A: Guill? Yeah, right. So the testnet. So I will share my screen for this one, where is the window? So as you know, there's this page that describes the current specs and status of the testnet. So we found some bugs thanks to all those teams that tried to join it and get their clients to work with it. So the testnet is currently at ten times what other teams have reported bugs at at least. So I assume most clients are actually just 10% of the way. I mean, I'm happy to be wrong of course, but I think that's my understanding.
00:19:51.660 - 00:20:44.248, Speaker A: And we had some bugs, so we fixed none of them on the testnet. They will all be fixed during the relaunch. I think currently it's quite valuable to keep this current testnet running and finding more bugs, namely because Adrian from open Zeppelin has released his. He's got a lot of test contracts that he has deployed around block 70,000. So I would like to encourage people to try to keep thinking, even though they see bugs, at least they are understood they are being fixed. I could go over them. Most of them have been fixed already in the branch, so it's just awaiting a redeploy.
00:20:44.248 - 00:21:59.728, Speaker A: We have two. One of them is understood, but guess has no fix for that, but it's well understood. And there's a new one that has been reported by Gabriel from beso this morning, and there's an understanding. It seems to be stemming from the fact that it's coming from when the contract is deployed and it has all the gas it needs to execute the init code, but then it runs out of gas while charging for the gas cost related to the contract creation that occur when you deploy a contract. Some witness costs, some vertical tree related costs are charged before the init code is executed, and some are charged after. So this is all in the spec. If the contract creation runs out of gas during that second time that gas is charged, apparently there's a bug, or at least there's a disagreement on the spec, on the interpretation of the spec between guests and other clients.
00:21:59.728 - 00:22:57.784, Speaker A: So we want to keep investigating this one, but I assume it's just a matter of agreeing on what needs to happen and fixing it. And yeah, otherwise on this block 6842, there was a weird interaction in which gas would charge the gas at the wrong time and the init code would actually, sorry, it said self destruct. That would actually use the gas instruction to decide where to send the balance. So it's a very peculiar corner case, but it's a valid one. So we found that guest was charging the gas at the wrong time because of it. So that's great. So this one is not fixed yet, but it's just about charging the gas at the right time.
00:22:57.784 - 00:23:26.320, Speaker A: So it will be fixed this week. So that's pretty much it for the bugs. Yeah, like I said, there's no relaunch plan yet but I think for the relaunch we would also want to integrate. So fix those bugs obviously, but we would also want to integrate the change that Kev talked about last time, which is, if I'm not mistaken, instead of serializing the commitments we would map them to field. Is that correct, Kev?
00:23:29.220 - 00:23:39.190, Speaker F: Yeah, from the outside you're just still mapping it to 32 bytes. It's just that you're using the group to field or map to field method instead.
00:23:42.910 - 00:24:15.680, Speaker A: Yeah. So I would like to add this when we relaunch. Just want to make sure everybody is okay with it. I don't expect any opposition, but you never know. Okay, well, I guess that's it for the testnet. Cool.
00:24:15.830 - 00:24:30.070, Speaker B: Okay, next up we have the proposal for code packaging in vertical. I believe some folks from Ipsalon, Pavel as well as Guillaume and others have been discussing this for the past few.
00:24:36.460 - 00:25:31.476, Speaker E: Hi everyone. I think it's my first appearance here so I think I wouldn't call it proposal yet. It's kind of the exploration at this point and the original of this is thinking what are the alternatives to having this code layout that is currently used in Vercos which is one byte of metadata in the chunk and 31 bytes of actual code. And all of that is needed to be able to have localized jump this analysis. Right. So to be compatible how EVM works. And this document which is the link is in the agenda.
00:25:31.476 - 00:25:34.410, Speaker E: I can resend it the chart if you want.
00:25:40.160 - 00:25:40.524, Speaker A: Yeah.
00:25:40.562 - 00:27:21.496, Speaker E: So mostly this document tries to have alternatives which are kind of splitting this metadata and the code into separate sections to that we use some small pieces of uf but I think the of is the last important part, all of that. So I think conceptually we try to think what are the other options to keep this metadata somewhere? And this document mostly proposes to have this metadata being proceed the actual code. And there are three different kind of encodings that we can use which one of them is the same as is currently used. So we just move this one byte separate section and the actual code then can use like 32 bytes of payload. So whenever you have to verify a jump instruction you need to reach out to this other section and check if the metadata there actually confirmed that the jump is valid. Yeah, we kind of listed three different encodings for it. One is a bit set, which single bit informs that the destination is a valid jump desk.
00:27:21.496 - 00:28:17.372, Speaker E: This is more or less how the current VM works. It just produces this bitmap on the way for the whole code. The other one is to put this one byte of metadata in the section accordingly. And the third one is we can actually squeeze this one byte into, I believe, six bits because that's enough to have all of that. So this gives us some savings. So in general, this is kind of playing with the trade off between the actual code size and the computation needed for that. And yeah, I think, I'm not fully sure what to do with this next.
00:28:17.372 - 00:29:15.868, Speaker E: There are some comments about the document that I plan to incorporate in the document itself, but I think conceptually that's more or less the idea. And I think they come up the question, for example, is it worth it to do it? I think two main benefits of this. One is that the execution is more similar to what we currently have. I think you already know that there will be at least two different mode of execution. One is for init code, one is for the deployed code. So we already have two variants of VM, and they will behave differently depending on the context. So at least maybe that simplifies the implementation for the execution.
00:29:15.868 - 00:29:59.150, Speaker E: And second aspect is that the metadata is actually only needed when you have a jump instruction. So in the current veracle trees design, you kind of waste this one byte of metadata if you fall through from one chunk to the next one, and you don't perform any jumps there because you can kind of figure out the offset by yourself using the context of the previous chunk. So I'm not sure how to actually simulate the different modes of operation and which one is better, but I think that's for later.
00:30:07.710 - 00:30:26.100, Speaker A: So it was this method, this last point, you said there could be some kind of happy path in which there would be pretty much no jump in the code or as little as possible, and then you would charge less gas because you wouldn't have to go and read the analysis. Right?
00:30:27.270 - 00:31:18.862, Speaker E: Yes, but I think it's a bit difficult to justify that. Yes, strictly. It's not like the code size that is actually deployed doesn't contribute directly to the gas overhead for accessing code, but to kind of say which one is better, I guess we would need to analyze the execution of existing contracts somehow. I think that's doable, but I think requires some infrastructure to do it. Or you can just launch different tested. I'm not sure exactly how to do it, but you need kind of dynamic analysis of the execution. It's not enough to just see the code.
00:31:18.862 - 00:31:31.820, Speaker E: You just need to know what are the offsets of the jumps and how frequently they happen. And then we can compare the cost of different variants of.
00:31:35.390 - 00:32:01.220, Speaker A: I mean, not that it's a complete, perfect indicator of what future use would be, but we have this code that Ignacio is maintaining at the moment where we replay blocks, historical blocks. We could do this and get this kind of information if we implemented some approach that you describe, which doesn't sound very difficult on paper at least.
00:32:01.910 - 00:32:17.160, Speaker E: Yeah, that sounds really good for analysis. I think if you have something that this proposal can be applied to, I think that would be one of the next steps for it.
00:32:18.650 - 00:33:00.996, Speaker A: Yeah. So unfortunately that code needs to be rebased. I don't know, in, as you can tell, how ready it is, but yeah, we might need a couple of weeks to get it to work, but we could do that. So that's the first question answered. I had another question just actually, it's not really a question, it's more like to confirm I understood correctly, you described those three approaches, like either putting the bitmap or putting the offsets in a section, or putting six bit offsets in a section. Those are interchangeable. Like if I pick the bitmap, for example, I don't have to store the offsets.
00:33:00.996 - 00:33:01.930, Speaker A: Is that correct?
00:33:03.020 - 00:33:03.770, Speaker D: Yes.
00:33:04.700 - 00:33:40.260, Speaker E: They differ by additional kind of computation. You need to get the information. The bit set is the straightforward access. You just need to read this one bit from there. For the others, you just need to do something similar you are currently doing, which is this. I just call it like localized jump, this analysis. So mostly perform this analysis on this 32 byte chunk.
00:33:40.260 - 00:33:43.700, Speaker E: Know where the valid jump tests are in the trunk.
00:33:44.200 - 00:34:41.850, Speaker A: Yeah. And I wanted to make a calculation because the bitmap would be the best. I mean, yes, it's more computationally intensive, but because reading from the tree means increasing the gas cost, if we have to store the bitmap next to it, next to the code, it would be better for us to just store the bitmap. But, yeah, one question I had. Sorry, I'm brain dumping here. If that extra section, the jump list section, was spread over several leaves in the tree, would there be any issue with this? I'm thinking of storing the section corresponding to a group of leaves close to the tree. Close to where in the tree they are, basically.
00:34:41.850 - 00:34:46.810, Speaker A: I'm just trying to see if we can optimize the gas cost here.
00:34:49.580 - 00:36:10.444, Speaker E: Yeah, I'm not sure I fully understand the question, but all these jump test sections are kind of random access, mostly when you need to decide if a jump test in a given code chunk is valid. So for the given code chunk you need to do a lookup in the jump test section and a specific offset. So I think in most cases it means you need additional chunk to load from the jump test section and whenever the same code chunk will be reused for later jumps. This is up to checking empirically with the simulation tool. Like how often do we actually reuse the chunk? So the chunk for the bitset for example, has how many? 32 times eight code positions, right, right. 206. If you jump a lot within the 256 instructions or like code offsets, then we use the same trunk.
00:36:10.444 - 00:36:34.590, Speaker E: If you jump a lot with big offsets then maybe it won't. So yeah, to me this will increase the gas cost but a bit, right? To my experience it's more likely that the gas cost will be increased with this proposal, but I think we need to verify it.
00:36:35.680 - 00:36:49.040, Speaker A: Okay, cool, makes sense. Yeah, I have more fine detailed questions, but I guess I'll ask them offline. No need to. I assume other people want to ask questions so I'll leave the floor.
00:37:01.940 - 00:37:17.300, Speaker B: Any other questions or follow up on this? And thank you pavel for giving the overview. If not, we can also continue at async and discord.
00:37:22.150 - 00:37:22.900, Speaker A: Okay.
00:37:25.910 - 00:37:31.540, Speaker B: Then moving on to the last agenda item, EIP 2935.
00:37:36.010 - 00:38:20.530, Speaker G: Hey guys, so I have prepared a short presentation to help navigate the discussion and I'll be presenting it. Just getting through the mechanics of sharing the screen.
00:38:21.060 - 00:38:22.050, Speaker B: No problem.
00:38:32.950 - 00:38:35.730, Speaker G: Josh, could you share it? I'm not getting the permissions.
00:38:37.590 - 00:38:47.220, Speaker B: I just shared. Oh, you want me to share my screen?
00:38:49.930 - 00:38:50.678, Speaker G: Yes.
00:38:50.844 - 00:40:00.290, Speaker B: Okay sure, give me 1 second. Sorry. For some reason it's only letting me share my entire screen, which does not work. Well, maybe we can just kind of each go through this document. Do we need to screen share? I think we just maybe gotten will be right back. Okay, it's back. Did that fix it for you?
00:40:07.780 - 00:40:09.330, Speaker A: Otherwise I can share.
00:40:11.140 - 00:40:30.240, Speaker B: Yeah, maybe if you don't mind just there we go, cool. Sorry about that guys.
00:40:53.960 - 00:40:54.710, Speaker A: Back.
00:40:58.680 - 00:41:02.180, Speaker G: Thought he was sorry it was muted.
00:41:03.020 - 00:41:04.570, Speaker B: Okay, there you go.
00:41:06.860 - 00:42:30.290, Speaker G: Okay, so this is a short presentation to come up with consensus for moving for the way forward for dealing with block hash. So the current status is if we can go to the next slide. So current status is that we have a proposal which says that so we can basically update number two hash on the start of every block. So basically we have a serial storage and on the fourth block we also write the last 256 ancestors so that we can instantly switch to the new logic to resolve the block hash. So the salient features of the current proposal are that this is stateless and vocal friendly, that the accesses in execution witness can help the stateless client resolve the block hash and there is no weird waiting period. And since this is a serial storage, there is no real wraparound so perpetual history can be stored since activation and the block number is not going to wrap around for this particular time which is quite large and slot is even bigger. And currently we have this active in austin n four.
00:42:30.290 - 00:43:47.560, Speaker G: So we have had this discussions, two discussions on it in the last vic as well as last ACD. And the main discussion points are that why is nora being ring buffer like 4788 and what should be the correct implementation method? Should we just do a client native implementation like how it's being done right now? Or we should do a system contract. And depending upon what implementation we choose, there are a few things we need to decide upon. Next slide so the current proposal is for the serial storage. So the points are that there is perpetual history but it's still small compared to what the current state is and offers providing proving capability against any history trap. There are no wraparounds and simpler logic and it is resistant to branch poisoning attacks because there is a new branch every 256 slots as per the worker logic. But as ignatio mentioned, it might not be a real problem.
00:43:47.560 - 00:44:51.142, Speaker G: And then the second proposal that has been brought forward is that have a ring buffer like 4788. So we might just need 256 slots for the ring buffer and basically poisoning since poisoning is not a real reference and this can just be kept loaded and updated in memory. But even with a serial solution we can keep last to 56 loaded and updated in memory as well. And 4788 can be used to construct proofs because there are beacon block accumulators. So you can pick up beacon block header and construct proof against beacon block header because it has the history accumulator and then you will have to sort of create proof against latest execution block.
00:44:51.206 - 00:44:52.010, Speaker B: Header.
00:44:53.730 - 00:45:13.490, Speaker G: In the beacon block. So it might be a longer proof but yeah, it is still possible. So yeah, we'll want to have this discussion that how do people here want to wane between these two options? So please give your thoughts.
00:45:27.850 - 00:46:32.426, Speaker A: Yeah, if no one else is talking, I guess I'll start. I think yeah, the ring buffer might actually be the best option at this point because I know the state doesn't grow as much as we. This is not going to be a significant increase in the state compared to the actual day to day operations of the chain, but it still makes, it doesn't help. So we have the conversion on the one end and we have the realization that branch poisoning might not be such a big problem, at least targeting a given branch. Yes, we haven't actually gotten back to Gotti's points, but we should actually try to cover them. But I still think if it takes ten billions to just slow down the network a bit, I mean, this is what duncrot was saying. I think it's totally worth it.
00:46:32.426 - 00:47:05.670, Speaker A: And there's been a lot of pushback on ACD about not having a ring buffer. I think we could at least at the beginning, go for the ring buffer. And if it turns out our fears materialize, all we have to do is just keep increasing the count and not use a ring buffer anymore. So there's an easy way out of this problem if it turns out it's really materializing.
00:47:08.650 - 00:48:07.850, Speaker C: Yeah, I would say that it will be useful to decouple this discussion about branch poisoning with ring buffers. I think it's like, still think it's not like the correct framing, because if it is a problem, not using ring buffers won't be really a solution. I mean, sounds like a workaround for block hash, but if branch poisoning is a problem, it's like a much bigger problem in general. And regarding this ring buffer versus perpetual history, I have a question. Is there a clear argument on why it is useful to store the history at the execution layer in the tree compared to this other option that is possible today with the bitcoin accumulators? Any concrete argument on why we need this log history execution layer?
00:48:10.910 - 00:48:16.090, Speaker A: My understanding is that there is none and that's why the EIP went stagnant.
00:48:17.070 - 00:48:18.380, Speaker C: Cool. Okay.
00:48:21.170 - 00:48:28.160, Speaker G: So if we are going for ring buffer, in that case, what is the size of the ring buffer that we want? 256.
00:48:32.230 - 00:48:32.980, Speaker A: Yeah.
00:48:35.910 - 00:48:36.660, Speaker G: Okay.
00:48:38.150 - 00:48:41.010, Speaker A: Basically keeping the old behavior of block hash.
00:48:45.540 - 00:48:50.470, Speaker G: Sounds good. Anyone has any differing views on this?
00:48:56.880 - 00:49:27.292, Speaker C: I have another question. So I think this perpetual history IP was mostly motivated by, I think like the idea of clients pruning their databases regarding history. Was that like the original motivator for this? Maybe if there's someone interested in that topic and have an opinion if this IP is still needed or something. Or maybe I'm wrong and it's not the original motivation.
00:49:27.456 - 00:50:34.130, Speaker G: I think 4788 has sort of solved that problem, so I guess we don't really need it anymore. All right, we can move to the next discussion point. So what is the implementation method that we should go for next slide please. So basically the current implementation is that just implement it like a pre compile. So client write native code to update the storage and they resolve the block hash again in the special way. So there is no system contract accesses and there are no charges. And we can basically keep the gas charge for block hash for what it is or we can change it to any other static value.
00:50:34.130 - 00:51:40.472, Speaker G: So how to explain. So one step behind and if we are choosing this then there are two ways we can use to escape the 158 cleanup which is basically on the fork block. We switch nuance of the special address to one by doing an irregular state transition. Or we can just deploy a get bytecode so that people can also call the contract if they want to from their contracts. Or basically block hash opcode can also be resolved natively. The second part basically can give us two benefits. So if there is a system contract then it looks like that we are using our own system for implementing new features, which is sort of elegant and the gas charges will be per the actual excess of reading them.
00:51:40.472 - 00:52:15.830, Speaker G: So basically the confusion about what is the right gas charge won't be there. But the drawback is that system, the clients might need to introduce system contract execution capabilities, especially if for example there is a special gas pricing that needs to be done over there, the generated address will be used. So there are no 158 cleanup considerations over here. Yes. So please Wayne, how you feel about this?
00:52:19.880 - 00:52:45.230, Speaker C: I have a question regarding this. So I think this question was also existed in 4788, right? I know if someone remembers the argument of why they decided to deploy a contract compared to a client native solution because it sounds like the same discussion, right? Or it has something different in this case.
00:52:47.680 - 00:53:35.310, Speaker G: Yeah, it is the same discussion in the sense. First of all in beacon block header I think there was no backward compatibility issue. So it was just like something a new feature that needs to be implemented even though clients can basically transition execution to this system contract when they want to resolve the opcode. But as such I don't really have any particular reason for whether we should go for system contract or client native, except the fact that the gas charges would basically be a mess to deal with or basically we need to figure that part out at least.
00:53:37.520 - 00:54:39.472, Speaker A: Yeah. If I may add to that, the biggest problem with this question to begin with, the fact that we asked the question is that until Veracle happens, or until at least we have witnesses in blocks or available, it doesn't matter what you choose to do, you don't have to call the contract. So for example, geth does not, if I'm not mistaken, doesn't call that code, it just updates the state directly. We cannot have that anymore because if you actually call the code, you will find the code in the witness. Or you need to fake the fact that you have called the code and that's a bit tacky. Or you could also say, well, this part of the code will never end up in the contract because we expect even stateless clients to have a copy of this code, which is what Matt suggested to me. That can be done.
00:54:39.472 - 00:55:45.110, Speaker A: But honestly, yeah, it feels like a special case that's not really useful. And while I don't exactly remember the connection with the reason why they went for this system call, sorry for calling actual code, I sort of get the feeling that this was to be a bit more generic. At least that's what I got from conversation with people. It's just my intuition, don't quote me on that. But yeah, I think even if we pretend that there was code execution when there is not, or if we do code execution where there's no need to be, we kind of break this purpose altogether. So I think we should actually enshrine, I mean, this is what I would be pushing on Acde saying, well, you know what, deploying code was a bad idea. When we switch to vertical, let's assume everybody has their own implementation, that they go and update the contract, that they don't call any code.
00:55:45.110 - 00:55:51.320, Speaker A: That will definitely get some pushback. But I think that's the right choice.
00:55:54.140 - 00:56:11.180, Speaker G: I think for beacon block header, the ring buffer is a bit bigger as well. So if we only have to store photo 56, I think we don't really need to do code execution to evm code execution to serve the block hash.
00:56:16.180 - 00:56:53.528, Speaker C: Yeah, I mean, this point, if we have a contract, that means that all the witnesses in every block will have this code, which is somewhat annoying. And working around that seems like an exception. To assume that all stateless clients have the code and how to rely on that in the witness is very messy. So I think that sounds like a really good argument to maybe try to avoid system contracts in this case if possible. I wasn't having that in my rather.
00:56:53.614 - 00:56:56.088, Speaker B: Yeah, right.
00:56:56.174 - 00:57:44.910, Speaker A: And just to add some detail, you actually have to pass some of the data in any case, some of the tree in any case. Because if someone, another attack, or actually pretty much the same attack, someone starts inserting values even if they don't do it on purpose. But there's a new level being added. You need to be able to handle this insertion in the tree. So yeah, it makes things really tacky. Like if you want to omit this approach, omit the fact that if you want to omit the ring buffer from the proof, you will still need to go through a lot of hoops to keep the tree in the proper shape. So yeah, that's why I would advise not doing it.
00:57:49.210 - 00:58:14.400, Speaker G: Okay, so in the case that we are going for client native implementation, then what is the best way to resolve 158 cleanup? We do by setting the no ones to one or we actually deploy a small contract that can at any way resolve the get calls to it. I mean, even if we won't use it, if someone wants to use it in their contract execution, they are free to use it.
00:58:16.690 - 00:58:38.726, Speaker A: Yeah. So my personal take on that would be indeed to just exactly do what you said, which is just a get so that other contracts can access that data if they need it. I don't know why they would do this, but they could. And yeah, it solves the problem of the nons. Indeed. Yeah.
00:58:38.748 - 00:58:56.410, Speaker G: So in that case we'll get a special address and that is something that we can update in the EIP along with the bytecode for get. That's all from here. Thank you.
00:59:01.580 - 00:59:12.250, Speaker B: Thank you. Cash under. That was great. Okay, well perfect. We got through the agenda. Thank you everyone for joining. See you next time.
00:59:14.380 - 00:59:15.130, Speaker A: Thanks.
00:59:15.860 - 00:59:16.270, Speaker G: Thank you.
