00:00:01.760 - 00:00:02.994, Speaker A: All right. Shall we begin?
00:00:06.454 - 00:00:07.234, Speaker B: Yeah.
00:00:09.454 - 00:00:44.164, Speaker A: From my side, I ran the old five tests with our client. The main difference was the timestamp on some of the. On some of the items, looked a bit into it, and also tried to compile the sinus fresh, like fresh node for the hive tests. But somehow hive tests died on me. Clarified. You any success in there, in running it?
00:00:45.704 - 00:00:50.404, Speaker C: I'm not yet seen. I've made some fixes, but I haven't tried those yet.
00:01:00.924 - 00:01:19.076, Speaker D: Yeah, I think I've updated five branch. Maybe that. I think that issue will go away now. So. Yeah, that's. As for me, that's basically it. I don't have a lot more to update.
00:01:19.076 - 00:01:38.784, Speaker D: I also, I am looking into implementing the beacon root override for ethcall, which of course I can then reuse for multipol. Yeah, I think that's about it.
00:01:39.364 - 00:01:45.344, Speaker C: You were just mentioning that with block override you could set those already. So do we need to have those separate?
00:01:51.414 - 00:02:13.253, Speaker D: I would say yes, you can change them via the state override. But I don't know, I feel like. I definitely feel like having the blood override is more accessible for users. I guess somebody wanted to do this.
00:02:17.313 - 00:02:23.973, Speaker C: I guess then we run into issues. You said both of the overrides and then you can decide again what you do in that kind of case.
00:02:24.713 - 00:02:59.334, Speaker D: Yeah, I think. I mean, yeah, I would say what override takes priority in this case because the user explicitly opted into it. But I'm not entirely against leaving it out and leaving it for state override to handle. Yeah. It's just my preference is that. Yeah, because it is a block override. Like, it is a field in the block that can be overridden and it's better to support it like directly.
00:03:00.514 - 00:03:04.774, Speaker C: Do you know some really use cases why you would need to override that kind of things?
00:03:06.614 - 00:03:20.994, Speaker D: So I would guess the people who would use this vip would want to simulate it. I'm guessing it's staking contracts mostly.
00:03:23.654 - 00:03:53.784, Speaker C: I was wondering if it's a big use case, then we could maybe make it more user friendly. But if only few people will use it, then they probably could use the state override methods of way of doing it. I wonder how difficult it is to set with the state. Or I guess just an array and then you set the array fields. So it sounds like difficult to set, but something that would be a bit annoying to set to figure out that kind of things.
00:03:54.724 - 00:04:40.416, Speaker B: This is for the beacon root, right? Yeah. So I think we might want to verify, but I believe because it's a circular array all you'd have to. The offsets for each of the items are well defined and known in advanced and permanent. So I think it's like an 8000 item circular array. And so you know exactly which 8000 slots are like today and they will always be those same slot numbers. And so you just literally need a mapping, like a hard coded mapping of the item at index n is slot address, whatever. So yeah, so I don't think it's going to be too hard.
00:04:40.416 - 00:04:44.964, Speaker B: If my understanding of the implementation is accurate, I don't think it would be too hard to do a state overrides.
00:04:51.084 - 00:04:53.224, Speaker D: Yeah, that's fine with me.
00:04:55.324 - 00:05:16.824, Speaker B: It's also possible it's been a long time since I've looked at this in solidity. But are arrays contiguous in memory? So if you know the position of the first item in the array, you can calculate the position of the nth item by just adding n times word width. You know that offender?
00:05:17.714 - 00:05:19.094, Speaker D: No, not me.
00:05:19.434 - 00:05:28.534, Speaker B: Okay, maybe we're looking into if that's the case. That makes it even easier because then you only want one address if they're contiguous. I don't know if they're contiguous though. I feel like in solidity maybe they're not contiguous.
00:05:30.314 - 00:05:38.174, Speaker C: But with eight call you said that you are adding the t is there at the moment, but with eight call you don't want to add the state overrides right away.
00:05:39.754 - 00:06:01.834, Speaker D: We do have the state working. So I guess whatever we decide for multiple, I will also apply it to default. So if we decide not to add the block overrides the beacon routes to multiple, then I will also refrain from adding it to each call because I want them to be consistent.
00:06:03.894 - 00:06:18.764, Speaker B: I mean, I would be totally fine with not adding it to ETH call, but adding it to ETH simulate, so that way people are encouraged to move. But I'm also fine with just not adding it. I think state overrides are simple enough.
00:06:23.064 - 00:06:24.040, Speaker C: Go ahead.
00:06:24.232 - 00:06:40.714, Speaker A: On a related note, Sina, when we talk about the hive tests branch, do you mean we shall take stuff from your multicol branch or shall we take stuff from your like master Branch?
00:06:45.374 - 00:07:07.514, Speaker D: So I merged from like master into my branch and now I think the next step is that Hillary merges my branch into his. And then. Yeah, it's possible to run those health.
00:07:09.014 - 00:07:12.534, Speaker A: But yeah, I was just wondering about the branch name. Is it multicol?
00:07:12.574 - 00:07:18.674, Speaker D: Yeah, it's multicolored. Yeah, just against killer is fork.
00:07:55.874 - 00:07:58.454, Speaker A: All right, do we have any more topics for today?
00:08:01.454 - 00:08:03.110, Speaker C: Decide what we do with that.
00:08:03.262 - 00:08:07.434, Speaker D: Yeah, I'm still thinking about it. So, so the.
00:08:12.374 - 00:08:15.714, Speaker C: One option, should we just write a blog post how we can do it?
00:08:18.494 - 00:08:51.834, Speaker D: Yeah. So I think in this case the timestamp also should be overridden because that's how the contract works. It uses the headers timestamp to find the index in the ring buffer to store the root contract. The root, basically. And it's not possible to predict the times in advance if it's not over with them.
00:08:57.374 - 00:09:18.654, Speaker B: So you're saying the execution client gets the timestamp from the consensus client and then it uses, and it gets the beacon root from the consensus client and then it uses the timestamp to calculate the, essentially the slot index and then model the size of the ring buffer. And then that's the index.
00:09:19.154 - 00:09:25.334, Speaker D: I think the index is simply timestamp modulo ring buffer size.
00:09:28.234 - 00:10:22.664, Speaker B: I would assume timestamp divided by twelve modular ring buffer size, though maybe that works out to be the same. I don't know. Anyways. Yeah, so, and so if we have blocks that have timestamp overridden, they're going to look into that ring buffer in a place that, that timestamp would have shown up in a slot. But now if those timestamps aren't evenly divisible by twelve, which is necessary for this to work, weird things may happen and stuff may break. And on top of that, there's the big question of, you know, they go by timestamp.
00:10:23.684 - 00:10:24.704, Speaker D: What was that?
00:10:26.604 - 00:10:31.100, Speaker C: So how does this go by timestamp and not by block numbers?
00:10:31.132 - 00:10:31.784, Speaker D: I guess.
00:10:32.504 - 00:11:09.024, Speaker B: Sorry. The consensus client gives the execution client timestamp and beaconroot hash. And then if I understand correctly, the execution client then derives the slot number, effectively, or rather derives the position in the ring buffer from that pair of information. And the beaconroot is useless. So all it has to go on is the timestamp. And so it uses the timestamp to figure out where in the ring buffer the thing should go. And I'm guessing that this is predicated on the assumption that timestamps are always 12 seconds apart or it would not work properly.
00:11:09.564 - 00:11:15.144, Speaker C: Well, I guess by some constant amount and on some other networks it's more than 12 seconds.
00:11:17.204 - 00:11:29.444, Speaker B: Yeah. Is it literally just timestamp divided by ring buffer size or is it timestamp mod twelve divided by ring buffer size? Which one of those is it?
00:11:35.704 - 00:11:38.684, Speaker D: No, it's timestamp modulo.
00:11:39.904 - 00:11:40.480, Speaker C: Sorry, sorry.
00:11:40.512 - 00:11:48.604, Speaker B: Timestamp modular ring buffer size. And that's it. There's no block length in there. There's no 12 seconds or twelve or anything.
00:11:51.804 - 00:12:01.184, Speaker D: There's no twelve or anything. This. Wait, but I'm just saying, like, wait, what is it? This basic. This is, this is the pseudocode. Let me post it here.
00:12:01.924 - 00:12:09.464, Speaker B: So if that's correct, that means this is a weird ring buffer because it doesn't insert linearly into the ring, right?
00:12:11.004 - 00:12:11.784, Speaker D: Yeah.
00:12:12.444 - 00:12:34.396, Speaker B: I mean, this. This is off my head. My math may be very wrong here, but I'm kind of just doing this in my head. Like you're going to insert randomly into the. Into your ring and over time you will eventually get the last, you know, 8000 items. But I don't even know if this guarantees they don't replace each other before you hit 8000. And I'd have to.
00:12:34.396 - 00:12:37.464, Speaker B: I'll do the algebra to verify, but. Feels weird.
00:12:38.404 - 00:13:01.404, Speaker D: I'm also. It seems also a bit strange to me, honestly. I was reading the code for this contract for the first time today and I haven't fully got it. Oh, wait.
00:13:12.204 - 00:13:14.264, Speaker B: Anyone happen to know the EIP number for this?
00:13:15.444 - 00:13:17.064, Speaker D: 4788.
00:13:22.884 - 00:13:25.020, Speaker B: Maybe if we're lucky, someone mentioned the rationale.
00:13:25.052 - 00:13:25.624, Speaker C: Why?
00:13:36.504 - 00:13:54.608, Speaker D: Actually, there seem to be two ringbuckers. Two?
00:13:54.776 - 00:14:05.944, Speaker B: I only see one. Oh, wait. I say the timestamp ring buffer and the root ring.
00:14:07.724 - 00:14:08.544, Speaker D: Yeah.
00:14:20.124 - 00:14:22.304, Speaker C: I hope we didn't find a conscious bug.
00:14:24.924 - 00:14:39.314, Speaker B: I'm guessing this is pretty well tested. My faith in the core devs to actually test the consensus important things is reasonably high. As much as I rage against the core dev process on Twitter, I'm actually reasonably confident in their processes today.
00:14:58.634 - 00:15:11.774, Speaker C: Do we have any other discussions? I guess one discussion. What about, what was the fundamental box? What do we do with those.
00:15:21.374 - 00:15:32.766, Speaker B: Phantom blocks? What was the issue with phantom blocks? Oh, should we drop them or not? Yeah.
00:15:32.790 - 00:15:34.754, Speaker C: So Cena doesn't like them.
00:15:35.374 - 00:16:21.934, Speaker B: I'm. I'm still leaning more towards dropping them with Cena. I know I originally pretty strongly against dropping them, but I'm slowly being convinced that they're just iteratively making things more and more complicated. And, you know, after talking with Bantag, I think last time we didn't really come up with any use cases that would be significantly impacted negatively by dropping them. Like, all the use cases should be using timestamp, and the ones that use block aren't use cases where this would be an issue. They're like short timeframe things, like three blocks, four blocks, five blocks, in which case you can simulate four or five blocks. The times we need long range.
00:16:21.934 - 00:16:31.530, Speaker B: I'm just not convinced that there's really any good use cases for long range need of this. And so I'm tempted to leave it out. Yeah.
00:16:31.562 - 00:16:55.730, Speaker A: Just in theory with this approach, say user wants to simulate 1000 blocks, he or she would just put into the request empty bodies and correct timestamps. Or what would be the user facing dilemma there, how he or she would need to use esimulate to do that.
00:16:55.922 - 00:17:08.316, Speaker B: Yeah. So they would just, presumably they would just put in a bunch of empty blocks and each block can just use the defaults for literally everything. I think we even have a default for the timestamp. Right. I think it's increment plus something from the previous block.
00:17:08.340 - 00:17:09.044, Speaker C: Plus one.
00:17:09.204 - 00:17:55.882, Speaker B: Plus one, yeah. So user could just have defaults for literally everything. So you would just need an array of 1000 empty blocks basically, and we would autofill everything in between. But it does mean that on the client side it would actually iterate through all thousand of those and generate 1000 blocks and 1000 block headers. And so all the costs associated with those thousand would be incurred, unlike phantom blocks where if you did the same thing with phantom blocks, you would not actually generate any client costs for the 999 in between. You would just skip to the end and only generate the first and last or something. And so there is, if someone's doing this, this is why the costs now scale linearly with number of blocks of empty blocks versus if we drop phantom.
00:17:55.882 - 00:18:08.674, Speaker B: If we keep phantom blocks, then the cost scale are just constant from the number of actual blocks you have, regardless of how many phantom blocks. Which is why we basically have to constrain the number of blocks that you can include. If we don't already.
00:18:10.694 - 00:18:14.754, Speaker D: I believe we have limit for that. It's 256.
00:18:15.214 - 00:18:15.654, Speaker B: Okay.
00:18:15.694 - 00:18:19.794, Speaker D: So yeah, basically you would not be able to simulate longer ranges than that.
00:18:22.474 - 00:18:28.094, Speaker C: But that is the client side specifications so that somebody can override that if they want to make longer ones.
00:18:31.594 - 00:18:35.054, Speaker B: Assuming the client in question provides configuration for that. Yeah.
00:18:36.714 - 00:18:37.494, Speaker D: Yes.
00:18:37.874 - 00:18:40.894, Speaker C: I thought that was the idea, that that's a config setting.
00:18:43.794 - 00:18:44.106, Speaker B: Yeah.
00:18:44.130 - 00:19:21.984, Speaker A: Then the question would be, do we still want to say we have this like range, range limitation set on user in like 250 blocks by default or flexible set by the node operator? Do we generate this empty configs for the user to keep the API as it is now, so that user can write block number one, then block number 50, then block number 100 and we'll just silently generate empty blocks for that user? Or shall we force the user to insert empty blocks without any contents?
00:19:22.964 - 00:19:55.712, Speaker B: Interesting thoughts there. I have two thoughts on it. One, I do like the makes the API nice and easy to use even if we are still generating them. That's on the positive side, I like the suggestion. On the negative side, I do worry a little bit about user not realizing the load they're putting on the client. Whereas if the user has to explicitly include each of the blocks in a row, it kind of makes, it brings the forefront of their mind that, oh, you know, the client's actually doing something for every single one of these that I send it. Even if it's just the defaults, it's still doing work.
00:19:55.712 - 00:20:00.088, Speaker B: I don't know if that overrides the convenience. I do like the convenience of this proposal quite a bit.
00:20:00.136 - 00:20:05.964, Speaker A: As a user, do you really care? Whatever does the client thingy, do you just want results?
00:20:07.984 - 00:20:16.194, Speaker B: I mean, it depends on the user, I guess. If you're the person that's also running the node, then you probably care if you're not the person writing the node and you probably don't care. Someone else cares.
00:20:19.574 - 00:20:26.474, Speaker C: But good point. Generate anything, any blocks automatically. I would just then force user to input all the blocks they want to generate.
00:20:31.214 - 00:20:39.234, Speaker B: So essentially we would have a hard coded requirement that each item in the blocks array must contain a block number that is one higher than the previous item in the array.
00:20:41.074 - 00:20:48.374, Speaker C: Oh, it doesn't need to contain, but if you don't specify it, then it's automatically incremented. That should be enough.
00:20:50.674 - 00:20:53.894, Speaker B: So you think there should be something in each position?
00:20:56.754 - 00:21:04.054, Speaker C: Yeah, if you generate 100 blocks in future, then you need to have 100 blocks in the queue.
00:21:05.714 - 00:21:32.794, Speaker D: And to this point I think it would make it more clear what exactly will happen in the case that there are gaps. Well, yeah, basically if there are gaps, then there is some assumption to be made by the user, like what behavior will happen for those missing bots. But when explicitly they have to provide everything, then they will know that. Yeah. What's in there?
00:21:36.694 - 00:21:39.634, Speaker B: I'm fine with either. Whatever you guys decide.
00:21:40.054 - 00:21:54.794, Speaker A: I think it is a little bit comedic that we allow user to skip nearly all parameters when they set block headers or like transaction stuff, but we force them explicitly to fill in the blocks with empty contents.
00:21:57.044 - 00:22:11.664, Speaker C: Yeah, I also wouldn't want to get rid of the phantom blocks, but if that's what everyone wants them, that's what they have to do. But yeah, I would also like to kind of be able to override that rock number. I don't feel it's in a way any different setting than anything else.
00:22:12.844 - 00:22:16.064, Speaker A: Can we have a vote on do we keep them or not?
00:22:17.524 - 00:22:18.584, Speaker B: I abstain.
00:22:20.484 - 00:22:21.944, Speaker A: I want to keep them.
00:22:22.944 - 00:22:24.404, Speaker B: Keep phantom blocks.
00:22:25.584 - 00:22:26.364, Speaker A: Yep.
00:22:26.664 - 00:22:36.164, Speaker B: Oh, sorry for that one. Very weekly vote to get rid of them account. My vote for like, point one. So tiebreaker.
00:22:43.864 - 00:22:49.724, Speaker C: I don't know about the complexity, so I cannot vote for that, but I would want to keep keep them.
00:22:52.644 - 00:22:57.704, Speaker D: For removing them. So I guess Lukash is the vibrator.
00:22:58.124 - 00:23:15.224, Speaker E: Ah, all the power to me. Well, to be honest, I don't know if so. I really like them, but if there is no use cases, then we can remove them, right? I like not entirely sure if they're worth it then.
00:23:16.704 - 00:23:20.844, Speaker C: Was the compound was use case? Was compound was using that somehow.
00:23:21.184 - 00:23:47.974, Speaker B: I talked to van tag and the place where compounds using them isn't a place where it would be negatively impacted in a significant way by this, by removing phantom blocks. Like, you don't actually need phantom blocks to provide their use case. I don't remember what the details were. I remember we looked over it and it was like, eh, you don't really need phantom blocks for that. You can just do this instead. It's very simple and arguably better anyway. Yeah.
00:23:47.974 - 00:24:37.566, Speaker B: The one use case that is there is something that annoys me, which is open zeppelin. The people who make, they're like an auditing firm, and they make some kind of core contracts that a lot of people in the ecosystem use because they're very robust and secure, well tested, et cetera. They actually have a timer contract that you can then has a timer interface that has two implementations. One is block number timer and one is timestamp timer. And I'm very annoyed that exists at all because as an auditing firm, they should know better than to create something that people shouldn't use. But it does mean that there may be people out there using the block number version of their timer, despite all the auditors and ecosystem telling people generally not to do that. But you never know when you put something out there into the wild, people use it for things that not supposed to.
00:24:37.566 - 00:24:41.714, Speaker B: That's the one place that I worry, again, just because we don't know what's out there.
00:24:43.534 - 00:24:49.194, Speaker E: And what is the for removing argument that it's more complex?
00:24:49.894 - 00:25:11.684, Speaker B: Yeah, it's just as lots of small bits of complexity are added all over this feature because of phantom blocks. There's no single thing that just makes phantom blocks really complicated or impossible. It's just lots of little things, lots of if statements here and there for dealing with phantom blocks. And so the idea was we can get rid of it, get rid of some complexity in the client's implementations.
00:25:16.184 - 00:25:43.824, Speaker A: Do I understand correctly that basically the use of the phantom block would be if you create a DAP for example, to vote in the general elections and that vote have a timer, like, please vote until this certain time. Then on the like, after block, at time x, you want to see the votes casted and you can get results. And before that you cannot.
00:25:45.484 - 00:25:59.324, Speaker B: Not quite. So everything you said correct, except for instead of the vote being time based, it's block based. So the vote finishes, you know, in 1 million blocks, rather than saying the vote finishes in one month.
00:26:01.184 - 00:26:28.468, Speaker C: Even in that case, you can override the contract in the multi care. And that's what we are doing in the interceptor, that we are overriding the contract with our contract that bypasses the time fee. So we don't need to use the multicolored feature for the, since we can just override the contract. And I guess in the other cases where the block, block number is used, we could just override the contract. But that always required you to understand the contract more better when you do the simulations.
00:26:28.636 - 00:26:29.384, Speaker D: Yeah.
00:26:29.804 - 00:27:25.190, Speaker B: So basically, the situation you described, Oleg, is almost exactly correct, except for the switch from timestamp to block number. And the place where that's particularly useful is if you want to build a tool that allows end users to fast forward time, so to speak, without having to know anything about the internal implementation details of the contract. Contracts are interacting with. So again, using R tool Interceptor as an example, user goes to adapt website, interacts with it, and then clicks a button in R tools UI that says fast forward time by one year and then continue interacting on the website. The website will now see time has moved forward by a year and, you know, present things appropriately, but the user doesn't have any clue how the contracts implemented. They don't know if the contracts implemented using timestamps or block numbers or anything else. And so our tool would fast forward both the block number by, you know, the time one year times twelve second, or divide by twelve second increments, and it would update the timestamp on the block by a year.
00:27:25.190 - 00:27:53.118, Speaker B: Whereas if we don't have phantom blocks, we would have to, we could still do the same thing. So we still have our fast forward time by a year button in our UI. User can still click it, but we would only fast forward the time. The block would only move forward by one. And so if the contracts that the DAP is using are expecting blocks to move forward over the course of a year by more than one, then the DaP may not function as the user expects. This is, again, it's kind of an edge case of an edge case, sort of. And so it's questionable whether this matters.
00:27:53.118 - 00:27:54.554, Speaker B: That's the gist.
00:28:00.694 - 00:28:05.794, Speaker A: What was the latest, like, complexity issue that broke the camel's back?
00:28:06.734 - 00:28:08.714, Speaker B: I believe it was beacon, right?
00:28:11.514 - 00:28:57.554, Speaker D: I think, yeah, we did discuss that. Lukash had the idea that maybe we can solve it by simply ignoring the problem and it will be fine. But that just made me think about this and question, like, do we really need this? And to me, it felt like it's basically. It's not intuitive how some things should work. In this case, for example, the block hash. We know that that's a tricky thing when it comes to phantom blocks, both the block hash opcode and the block hash in the header.
00:28:58.774 - 00:29:00.030, Speaker B: And then there was the base view.
00:29:00.062 - 00:29:34.524, Speaker D: Where we on this call. Me and Micah had a different understanding of how basefree should. Should work for phantom blocks. Like, he thought that basically, phantom blocks are average blocks with normal number of transactions in them, whereas I thought that phantom blocks are empty. And so I just felt like there are some unintuitive things about them for the user as well as, like, in the implementation side.
00:29:34.904 - 00:29:43.804, Speaker A: What would be the main difference between the case when they are empty and when they are filled with, like, mean transactions?
00:29:45.224 - 00:30:02.514, Speaker D: So basically, if the phantom blocks are interpreted as empty blocks, then the BP should go down. By the time you reach block 100, it should be very little. However, if they are average blocks, then the base grid should roughly remain the same.
00:30:06.014 - 00:30:06.390, Speaker B: Yeah.
00:30:06.422 - 00:30:39.524, Speaker E: So, as. As you made me your leader and made me decide, then let's remove phantom blocks and let's just move. Move forward, because I agree, it's like, it's a lot of things to be specified that could go one way or the other, and it's both are similarly intuitive. And if there are known use cases that are known, then let's just drop it and simplify things a bit.
00:30:40.104 - 00:31:04.664, Speaker A: So then the behavior of a phantom, like, not a phantom, but empty block, would be averaging the previous block. And we'll get to the same point that Mika had with, like, the block that you are actually doing any work on is the average block of the, like, initial block that was just before the start of east. Simulate.
00:31:06.564 - 00:31:10.324, Speaker B: What you're talking about for the base fee calculation.
00:31:10.484 - 00:31:14.444, Speaker A: Yeah, for the base fee. For the, like, everything, it would be empty blocks, right?
00:31:14.484 - 00:31:14.848, Speaker B: If the.
00:31:14.876 - 00:31:15.832, Speaker E: There wouldn't be phantom.
00:31:15.888 - 00:31:16.368, Speaker D: Yeah.
00:31:16.496 - 00:31:37.724, Speaker B: Yeah. So if there's no phantom blocks for the base fee calculation, you literally just calculate base fee exactly the same as consensus rules tell you to, which means look at the previous block. How full was it? Calculate the base fee accordingly. So if you did do, like, 200 empty blocks, in a row your base fee would drop very significantly. And if you didn't want that, then you would need to specifically specify a base fee of something else.
00:31:38.224 - 00:31:47.004, Speaker A: Oh, poor users. It would really, really suck for them, I think, because they would expect it to be like real leash.
00:31:47.744 - 00:31:55.764, Speaker E: But the current consensus is that this is not really a useful pattern to begin with.
00:31:56.344 - 00:32:32.874, Speaker B: Yeah, exactly. I think the idea is that most, the vast majority of people will probably just do like some number of blocks in their simulation and all those blocks have content in them. They're all meaningful to the user for some reason or in some way. The idea here is it seems unlikely that people are going to be, or rare that people are going to want to do big jumps in terms of number of blocks. And so the pattern where a user provides an array of 200 empty items is in theory unlikely to be a common pattern or something that users will want to do. That's our hope at least.
00:32:41.194 - 00:33:05.434, Speaker C: So I guess we agree that we are removing then. I guess we need to agree then. How does the impact on the things we discussed about can you still kind of leave blocks out? And I would vote that then you have to specify all of the blocks and you cannot have gaps in there. But then can you still specify some of the blocks? And then if some of the block numbers out, and then we would calculate the block numbers for all of them.
00:33:07.054 - 00:33:55.004, Speaker A: So just one more thing. I think that what we are currently doing would lead to quite scary behavior from the end users and recalls to simulate, because they would need to simulate the thing that normal user would have it in his mind, the min block. Here she would need to get the latest block contents, then here she would be able to create latest, then content, then 1000 empty ish blocks. Then he or she would need to put into the empty block stuff he or she copied from the like latest. And then he or she would be able to do the real testing that. Really, really.
00:33:56.484 - 00:34:05.504, Speaker B: I think the idea is that we don't expect anyone will actually do that thousand empty blocks. Like there's no reason, no good reason to actually include empty blocks in your simulate.
00:34:06.284 - 00:34:23.484, Speaker D: And in case that is needed to simulate average blocks for Bayesvi purposes, then we can override Bayes fee in the empty blocks to stay the same as latest block, I guess.
00:34:24.584 - 00:35:21.704, Speaker A: Yeah, but as you want to cheat and like make API to do this cost free, you explicitly set to zero every of these empty like x blocks that you allowed to do and set the correct one right before the block that you actually will be emulating to. Like to make it actually work for you. So it would lead to quite dirty API use cases just to like bypass the absence of phantom blocks because users would want to probably think as if block were full. And by the way, when we test our feature currently, we try to fill it not with maximum amount of transactions per block. We fill it with like two or three or five or something like that. And like presume that next block will be assuming a lot of details that were in the, in the base block, don't we?
00:35:26.764 - 00:36:09.974, Speaker B: I think the idea is we would, if we drop phantom blocks, we would remove all of the tests that test the phantom block behavior. So all of our tests would turn into that common use case that I just showed there where you have latest, followed by one or more blocks that have interesting things in them. And that's it. Maybe we have one or two tests for the empty block case just to make sure we cover it. But presumably that's the, it's not something that we think users want, I guess is the argument here. If we think users want it, then I think that is a strong argument for keeping fandom blocks. I'm just, when we looked into it, we couldn't find any good use cases for, for that at all.
00:36:11.274 - 00:36:30.454, Speaker A: My main problem is that as soon as we start to think that blocks in the middle are empty at all, to simulate that they are filled, we would need to fill them with some random fast stuff. It makes this API much more complicated for use.
00:36:32.954 - 00:36:35.334, Speaker B: Maybe I'm not understanding why would you want.
00:36:35.704 - 00:37:04.124, Speaker A: So the main issue is that, as you said, if the block is empty and there is no transactions in it, something goes down and it deviates from the mean case. And when you simulate 100 blocks ahead, you don't want to have like something deviated too much from the mean case. You want to have that mean case like in place. And if you would need to simulate, to feed it data to simulate this mean case, it would be quite complicated.
00:37:04.504 - 00:37:35.210, Speaker B: Right. So the argument is that no one wants to do that. No one wants to simulate many blocks into the future. The idea is that all use cases can be accomplished by simulating, you know, just one block or maybe two blocks, maybe three blocks. But it's very finite and each block has a purpose and something in it. And it's interesting. Do you, is that the part that you disagree? You think that there are use cases for people simulating far blocks that are in the distant future or distant.
00:37:35.210 - 00:37:36.514, Speaker B: Maybe an hour?
00:37:36.674 - 00:37:45.814, Speaker A: Yeah, dapps, dapps connected to each other every voting scenario. When you can have voting like happening on a long periods of time.
00:37:46.274 - 00:37:46.586, Speaker D: Right.
00:37:46.610 - 00:37:48.654, Speaker A: So all those cases with revolting.
00:37:49.034 - 00:38:12.446, Speaker B: All those cases, though, are timestamp based. And you can put the timestamp very far in the future. It's only the block number that you can't. So you can do, like, you know, a year later, timestamp, it's just, it's a year later, next block, as if for some reason you had. So we can see this in the El. So anytime there's a slot dismissed, the El jumps forward by 24 seconds instead of 12 seconds, right. So the timestamp moves forward in one block.
00:38:12.446 - 00:38:32.974, Speaker B: The time stamp moves forward 24 seconds instead of the normal twelve. And so the idea is that if you want to simulate something happening in the future, you just pretend like there was, you know, 10 million missed slots, essentially. And so, you know, block n is timestamp x and block n plus one is timestamp x plus a million or something.
00:38:36.874 - 00:38:43.414, Speaker A: Wouldn't this confuse, like, half of the potential contracts on the chain?
00:38:44.874 - 00:39:34.664, Speaker B: That is the risk. So there is the risk that there are contracts out there potentially that are using block numbers when they should be using timestamp. Now, all the professional auditors and experienced people in the ecosystem that give advice to newbies and write tutorials and all that all, say, use timestamp these days. But a long time ago, people used to use block numbers as a proxy for timestamp because there was some misinformation around how accurate timestamps were on Ethereum for a couple of years early on. And so there definitely are contracts out there that use block number when they really mean timestamp. We don't believe any of the contracts that people use on a regular basis these days still have that problem. The ones that did have that problem didn't generally hit off because they generally also had other problems.
00:39:34.664 - 00:40:06.324, Speaker B: And so the theory that we're working off of, or that we hope is true if we remove phantom blocks, is that anyone who cares these days is using timestamp for all those things and not block number. And in theory, almost no one should use block number in their contracts at all. There's very little reason to, aside from doing proof things and stuff, we could be wrong. That's the big risk. If we're wrong on this estimation. And it turns out that, oh, apparently everybody's using block number. I don't think we're wrong on that, but it's possible.
00:40:06.624 - 00:40:22.134, Speaker A: Besides block number, it was said that something diminishes when you fill in with empty blocks with, like, no transactions. In the mean case, when you have phantom blocks. This something won't diminish that much.
00:40:24.154 - 00:40:51.214, Speaker B: I think even right now if you provided a block with some content followed by two empty blocks where you just had like an empty object in that place in your API call, followed by a third block that had content, I believe we would still diminish today even if we kept phantom blocks because you are providing a block. And so they're not actually phantom blocks. In that case, they are now real blocks, just with all defaults.
00:40:54.434 - 00:41:08.624, Speaker A: And the last dumb question, if we change the time between two blocks, two dramatically, shall that block still be validated if validation mode is on? If we like don't chain?
00:41:09.684 - 00:41:43.764, Speaker B: Yes, I believe that there is no constraint in the EL that consecutive blocks need to be within a certain distance of each other because the, like I said earlier, ultimately the CL could have, you know, 1000 missed slots and the EL needs to be able to recover from that, which means it needs to accept the next block in the chain being, you know, 1000 times 10,000, 12,000 seconds in the future or whatever that number is. And so I believe that for consensus reasons on Mainnet, the ELS currently have to support unbounded increases in time.
00:41:55.244 - 00:42:14.424, Speaker C: I guess the latest block number is bit difficult because if you want to specify the block numbers then it might not actually know what is the current block number. But I guess we can always allow that you leave the block numbers out so you can put the latest and then you can simulate 100 blocks and you don't specify the block numbers and they will be always based on the latest block.
00:42:15.324 - 00:42:41.294, Speaker B: Yeah, that's my assumption is that if you don't include the block number in any of your blocks, then in fact we have two options here. If we drop phantom blocks, we can just remove block number as an override field all. Or we can leave it in but always require that it is plus one from the previous. And if you leave it out then we automatically fill it in with plus one from the previous. There's no option anymore for having block numbers be not the previous block plus one. If we remove.
00:42:42.514 - 00:42:54.154, Speaker C: I thought we already decided that we are dropping them. So I would have started to move forward and that we are dropping and how that would modify the multi call behavior and what we need to agree there.
00:42:54.494 - 00:43:05.074, Speaker B: Yeah, so I don't think we have to change anything on this. Do you think that something will require a change here other than just having an assertion early on that the blocks are consecutive?
00:43:05.494 - 00:43:19.854, Speaker C: I think we had to agree that if you are, do we need to specify all the blocks or not? Or can you leave them out and then generate something, something there? And if it generates something there, then we should agree. What are we generating there?
00:43:20.234 - 00:43:54.294, Speaker B: I see. So if we leave them out, if we allow for leaving them out, then I believe we should just use all defaults using the defaults that we have documented already for every field. And that includes block number. And the default for block number should be block previous block plus one. As for whether we should leave the allow them required them always. Should we go with Oleg's suggestion earlier and allow for, just allow for missing blocks and then we just backfill them automatically? I'm okay with that, but like I said, I'm on the fence and I'm okay with you guys deciding otherwise.
00:43:57.634 - 00:44:22.304, Speaker C: I think that it's useful that you could be able to specify the latest and then produce block based on the latest block number, but that would always happen on top of that. And you wouldn't then need to specify the block numbers at all. And if you want to specify the block numbers, then I feel they should be ahead of the latest block by some amount and then we will generate between there.
00:44:23.564 - 00:44:46.084, Speaker B: I see, yes. So the first part I believe is already the case. If you leave the block number out, it will auto generate or default to latest plus one or, sorry, previous plus one, where previous for the first block in your list is latest if you don't specify a block tag for the other one. Like I said, I don't care either way if we allow people to have blank spots in their array or not.
00:44:46.464 - 00:44:57.564, Speaker A: So yeah, my main question would be, when we move to the next block, it appears so that there are some fields that we do not copy from parent. Yes.
00:44:59.664 - 00:45:34.714, Speaker B: Yes, specify those. So any field that we can calculates based on as a function of the parent, then we would do so. So the base fee is an example of this. The base V is a function of parent blocks, base fee, and parent blocks fullness. And so for those ones, we will, we can, the default, if you don't include a base V override is we will just calculate it according to consensus rules. So we follow all the normal main net consensus rules for the ones that are don't have a way to derive it from the parent or derive otherwise. Derive it.
00:45:34.714 - 00:45:49.698, Speaker B: I'm fine with just following what we previously did. Oh, I think I see what you're saying. You're saying should we copy from the base block or copy from the previous block? What is the current behavior like if.
00:45:49.706 - 00:45:50.814, Speaker D: You just include the most.
00:45:52.714 - 00:46:12.684, Speaker B: Okay, there's always copy from the parent. In other minds, when we're just doing a copy, that feels slightly more correct to me. But is there an argument for sourcing from the base block instead?
00:46:15.544 - 00:46:19.804, Speaker C: I would generate empty blocks. I think it would be more clear in this situation.
00:46:21.624 - 00:46:34.374, Speaker B: What do you mean blocks? I think we're talking about if you use all defaults for a block, so you have some block and it's just empty object in the API. Right? So you say fill in everything for me.
00:46:34.874 - 00:46:43.374, Speaker C: If the current block number is ten and I want to correct the block number 100, then we have to generate the blocks between there. I would generate empty blocks there.
00:46:44.714 - 00:47:28.924, Speaker B: Yeah. So if we go with Oleg's suggestion of allowing, I think we're talking about two different things here. So there's two topics we're discussing. One is what are the default values for all of the fields? And two is should we require the user to explicitly specify every single block or do we allow them to have gaps and we backfill those gaps? Oleg's suggestion was to backfill the gaps. Let's settle on that one first. Is anyone against backfilling the gaps? So if you say build on top of latest and the first block in your array is latest block number plus five, do we just generate four empty blocks followed by your block or do we give you an error?
00:47:32.024 - 00:47:38.964, Speaker D: I mean, mildly against, but not a strong.
00:47:47.084 - 00:47:53.144, Speaker C: What do I guess it would do? Would you see that like that? It would error on that case.
00:47:55.444 - 00:48:01.304, Speaker D: Yeah. Just say block number two, height.
00:48:03.044 - 00:48:12.884, Speaker C: I think the block specifying block numbers becomes really useless if they always have to exactly match because the blockchain can move kind of forward at any time.
00:48:13.464 - 00:48:16.404, Speaker D: Yeah, sure. Yes, that I agree.
00:48:20.264 - 00:48:26.444, Speaker C: Then I feel we could even remove the block number override altogether and you just over build on top.
00:48:27.304 - 00:48:28.404, Speaker B: Yes, we could.
00:48:30.024 - 00:48:35.004, Speaker C: I mean, you can specify the starting point where you're building but you couldn't specify anything else.
00:48:36.164 - 00:48:53.984, Speaker B: Correct. And we can leave it in there. We could leave it in there for completeness just so the user gets some validation that their thing they're building is correct. Maybe, but that's questionable value.
00:48:54.444 - 00:49:00.370, Speaker C: But they're still returned. So the user can get them returned and they can see what by the block numbers they were built on.
00:49:00.532 - 00:49:29.704, Speaker B: Yeah, true. Okay, so it sounds like Oleg wants, if I understand correctly, Oleg wants to pretend blocks eight through twelve in my little example in chat there exists and just fill them in with all defaults. And Sina has a very light preference to require the user pass blocks eight through twelve even if they're just empty objects and it sounds like, Larry, you were slightly siding with allow missing and backfill, but maybe change your mind.
00:49:30.644 - 00:49:48.744, Speaker C: Well, I think if you want to simplify the thing, then I would remove the block number overrides altogether and just, just allow the setting where the simulation start. Or they can set latest or some other number, but then all the blocks are built on top of that. I think that was, that would be the simplest solution.
00:49:55.304 - 00:50:15.484, Speaker B: I don't have strong opinions. Any strong opinions, Lucas? Nope. Anyone want to concede to move things forward, or should we offer some other mechanism of deciding, like tossing coins?
00:50:16.024 - 00:50:24.794, Speaker D: Yeah, I mean, I think Oleg was having a stronger feeling on this, so I'm happy to do the back feel.
00:50:25.494 - 00:51:08.974, Speaker B: Okay, let's go with that for now. If it turns out that during implementation, it's complicated or hard or something, we can reevaluate, but that, let's just move forward. So we will. If the using that example I have in chat, if user passes block number seven, or, sorry, they say latest block number is seven. And the user, the first block in the array they pass to the RPC is block number 13. Then we'll backfill blocks eight through twelve using all default values as though the user passed five empty objects in that array prior to the block number 13. We also still validate as well, prior to that, that the block number the user is providing is not 256, not more than whatever the client side limits are.
00:51:08.974 - 00:51:18.694, Speaker B: So before you bother to backfill, make sure you check. See this user isn't trying to backfill 10 million blocks to dos the client.
00:51:19.634 - 00:51:20.194, Speaker D: Okay.
00:51:20.274 - 00:51:22.534, Speaker B: And then moving on to Oleg.
00:51:23.234 - 00:51:37.374, Speaker A: Yeah, and then we come to the result. When the user gets his result, what does he see? He sees only the ones he submitted, but he sees every piece of empty trash we just generated.
00:51:38.054 - 00:51:58.914, Speaker B: I would say all the trash for two reasons. One, it makes it clear to the user that what they did is actually generating blocks, so it gives a little feedback to them. And two, I think, I suspect this will be easier client implementation wise, so you guys don't have to track which of the blocks were actually provided versus which ones are backfilled.
00:52:00.814 - 00:52:11.304, Speaker C: That is awesome. That I don't really like since you provide two blocks and then you get hundred blocks back, which is, sounds quite confusing. And I would expect to get two blocks back when I'm specifying two blocks.
00:52:17.564 - 00:52:37.704, Speaker B: I mean, if you specified two blocks, you got back 100. Would this perhaps encourage you as a developer to stop submitting blocks with 100 unit gaps and just submit to blocks instead? Like, if you only want two blocks back, just submit two blocks.
00:52:38.604 - 00:52:49.384, Speaker C: It probably would make me make a Google search for this and then find some mygas and repost about like why this method is returning 100 blocks instead of two blocks.
00:52:50.124 - 00:52:52.984, Speaker B: That sounds like it's working, working as intended then.
00:52:58.944 - 00:53:05.324, Speaker C: But yeah, I feel if you are doing the black feeling that then we should return them. But I can see that the users can be confused about.
00:53:08.344 - 00:53:51.212, Speaker B: I feel a little more strongly about this than the previous topic. I do feel like that if we're going to put work on the client to generate these things, we should be letting the user know via some mechanism, some way of giving feedback to the user that hey, the client's doing a bunch of work here. If you want your client to work less, stop doing this. And if we just silently do it, I worry people might think that we're doing some sort of phantom block thing or virtual block thing and not actually generating them. And by giving them that feedback, it hopefully will encourage them to be a little nicer to their nodes and their own computer hardware if they're running their own. It also lowers their bandwidth cost because I mean, the user has to pay half the bandwidth cost, right, for any RPC response. And so the user is constantly making requests for 256 items.
00:53:51.212 - 00:54:00.574, Speaker B: We're giving them 256 full blocks. This encourages them to, hey, you know, if you want to lower your bandwidth costs, just simulate two blocks instead of 256 blocks.
00:54:01.634 - 00:54:17.778, Speaker C: I feel if Uniswap will adapt this and start using this, and then they implement it in that way that there are certain hundred blocks, or the RPC providers, they will start optimizing their clients with the phantom blocks so that they don't actually generate them less work for.
00:54:17.786 - 00:54:18.894, Speaker B: The core devs, right?
00:54:20.094 - 00:54:24.474, Speaker C: I think they would complain to core devs, like why this is so inefficient.
00:54:25.574 - 00:54:50.134, Speaker B: Go talk to Uniswap and tell them to stop making crappy software. So I feel reasonably strongly about returning all the blocks. Does anyone feel strongly opposed to that or is everybody kind of weakly opposed? Like, I'm willing to keep fighting on this, but if no one else wants to fight me, then I'll just take the win.
00:54:56.114 - 00:54:58.894, Speaker D: No, I also think we should return all the blocks.
00:55:03.674 - 00:55:24.074, Speaker B: Let's return all the blocks then. If, if someone feels strongly about it and they just don't want to fight me right now, feel free to bring this back up in telegram or discord and we can discuss more. But I do want to make sure we're moving forward and we're not getting stuck on little things that people don't have strong opinions on like the last one or this one.
00:55:30.614 - 00:55:31.030, Speaker C: Okay.
00:55:31.062 - 00:55:57.684, Speaker B: And then I think last thing in the last couple minutes defaults. So it sounds like maybe, do we have all the default behaviors logged? So if a user doesn't provide a given value, it sounds like Nethermind maybe is always using the previous block, whereas Geth might be using some from the previous block, some for from the base block. We should definitely make sure we are on the same page on that.
00:55:58.224 - 00:56:07.704, Speaker D: I pulled up our default behavior. We are taking the Coinbase difficulty and gas limit on the base block.
00:56:09.604 - 00:56:16.184, Speaker B: Is there any reason why we shouldn't just take those from the previous block in case they were overridden?
00:56:19.124 - 00:56:24.904, Speaker D: I think I did it this way because of time stone blocks.
00:56:26.084 - 00:56:30.244, Speaker B: Okay, so without phantom blocks, we can just switch to always default to parent.
00:56:32.584 - 00:56:42.164, Speaker D: Well, because if every block is inheriting from the base, then these fields cannot.
00:56:45.384 - 00:56:49.044, Speaker B: We can override them. In the current API, we can.
00:56:49.384 - 00:57:00.304, Speaker D: We actually can override them. So I don't know. I don't know what's the right solution. I think all of these three fields can be overridden. So.
00:57:02.164 - 00:57:05.464, Speaker B: My vote is just to take from the parent from default then. Are you okay with that?
00:57:07.044 - 00:57:08.744, Speaker D: Yeah, that's fine.
00:57:09.124 - 00:57:30.424, Speaker B: Okay. It sounds like that's what another mind's already doing. So let's, let's do that. So now that the phantom blocks are gone, all the defaults are either computed like base V via normal consensus rules, or if there's no way to compute them, then we just copy the parents where the parent may have been overridden. It may not have been, but we just, whatever was in the parent block, we just copy that.
00:57:32.004 - 00:57:44.944, Speaker A: I wonder if there is any specific test case that can prove that there were no phantom blocks and all the blocks are real. Besides fee getting lower in between blocks.
00:57:45.604 - 00:57:50.314, Speaker B: Like, is there any externally visible difference between real?
00:57:56.454 - 00:57:56.886, Speaker D: No.
00:57:56.950 - 00:58:29.524, Speaker B: Assuming that defaults copy from the most recent non phantom block in the chain, which sounds like not what Geth was doing previously, but I mean, something could have changed. So there's the base feed. There's also, isn't there, the new blob base fee? So two base fees, I guess, that are both functions of the parent, the hash. So all these blocks would have actual hashes, like legitimate real hashes of the block header. That's probably the most noticeable difference.
00:58:32.224 - 00:58:46.254, Speaker A: So basically the cheapest operation will be, and otherwise the cost would be just to compute the empty block hash and stick other fields with data. And that's basically.
00:58:47.594 - 00:59:02.386, Speaker B: Yeah. So you could. Yeah. So just see what the block hash of each return block is. And that'll tell you if it's been actually filled out with defaults. It also tell us if all our defaults are the same or not. Like, you can.
00:59:02.386 - 00:59:25.524, Speaker B: Basically, any test can just compare the hash first and only continue to look at other things if the hashes mismatch in theory. Sorry, Kalari. You lost all your battles today. Okay. Anything else? Sounds like flair had to go. Anything else?
00:59:30.584 - 00:59:32.124, Speaker A: Yeah, let's continue, I think.
00:59:32.884 - 00:59:44.784, Speaker B: Okay, sounds good. I am sorry that not everybody got what they want today, but I'm glad that we are making good progress. I think it's important at this point that we don't get too stuck on little thing disagreements here and there.
00:59:47.884 - 00:59:49.204, Speaker D: Okay. Thanks, guys.
00:59:49.364 - 00:59:56.484, Speaker B: Yep. See you guys next week. Or in chat. Bye.
