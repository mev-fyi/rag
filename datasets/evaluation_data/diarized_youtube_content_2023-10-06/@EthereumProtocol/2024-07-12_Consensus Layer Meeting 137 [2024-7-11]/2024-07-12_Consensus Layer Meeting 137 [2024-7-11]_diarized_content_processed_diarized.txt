00:00:26.795 - 00:00:27.335, Speaker A: Sa.
00:02:26.625 - 00:02:29.285, Speaker B: Okay, I think we're good there.
00:02:32.945 - 00:02:56.525, Speaker C: And here's the agenda. Yeah, issue. Yeah. Okay, great. So everyone, this is consensus layer call 137. It is issue 1096 in the PN repo. And yeah, let's go ahead and get started.
00:02:56.525 - 00:03:49.655, Speaker C: The agenda is pretty light today, so hopefully, yeah, we can get through everything. So first off, I want to start with Electra in particular, DevNet1. I think most clients are ready for DevNet1. Does that sound correct? Everyone getting some thumbs up? Okay, I'll assume your thumbs up mean means your client is ready to go. So then otherwise I think last time we were waiting on some EL clients to be ready. That being said, I think there are a couple that are ready for Devnet one. So I would suggest we go ahead and launch the Devnet as soon as we can.
00:03:49.655 - 00:04:23.575, Speaker C: I think Pari and Barnabas are out this week, so they are usually the ones doing that. But that being said, I think we can launch it as soon as we're ready. Does anyone oppose that? I think even if there are other Yale clients who aren't quite ready, they can just join when they are. Okay, there's a ball pit. We can't hear you. Oh, there's Barnabas.
00:04:31.245 - 00:05:03.485, Speaker A: Hey, maybe I can quickly join in. We are currently testing with Tecu, Nimbus, Lodestar and Grandin as the consensus layer clients and netamind and Geth as the execution layer clients. And when looking at the interrupt chat, I've just found a small issue with Nimbus that should be looked at. But if any other client is ready, please share your client image that we can add that to the testing. Apart from that, all other clients are fine so far.
00:05:06.785 - 00:05:27.975, Speaker C: Okay, great. Sounds good. Cool then. Yeah, I mean, I would suggest we start DevNet1 next week. If. Yeah, that's not possible, then I think definitely by the. Well, so it'd be the next ACD next week? Yeah, I think as soon as possible.
00:05:27.975 - 00:05:30.975, Speaker C: And yeah, we'll keep picture. Moving along.
00:05:35.435 - 00:05:36.215, Speaker A: Wow.
00:05:37.115 - 00:06:25.425, Speaker C: One second. Okay. Yeah. Anything else on PEKA or Devnet one that we should need to discuss right now? Nope. Okay, great. In that case, we will turn to pure dos. So I don't know if anyone here is on the breakout call the other day, but it sounds like things are moving along.
00:06:25.425 - 00:06:36.585, Speaker C: I think there were some issues with the Devnet that they had, but the plan is to move into the next Devnet. Anyone have any PeerDos updates they would like to discuss?
00:06:38.845 - 00:07:07.635, Speaker B: Yeah, so we have Peerdos DevNet 1 running now and it's currently forking away from every client, from every other client. So we really don't have a good devnet one. We were thinking about a relaunch and on Tuesday we discussed that we are going to be planning for a relaunch by the end of the week. So hopefully by tomorrow we should have client fixes for all the different issues that have arise on DevNet once.
00:07:11.625 - 00:07:12.805, Speaker C: Okay, great.
00:07:22.345 - 00:07:22.961, Speaker A: Cool.
00:07:23.073 - 00:08:30.045, Speaker C: So otherwise I have a small update. I had a PR to start to uncouple having the Bob counts set on the CL&EL. After some discussion there, what we landed on was that essentially the check even right now for the maximum in the EL is redundant just given the way the engine API works and the checks the CL already does. So the plan there is to actually keep the maximum check just with the CL that does not need to be sent across the EL because it kind of already happens indirectly with the version hashes. There's then the question of the target. The target, if we want to have the CL sort of drive, this value will need to be sent across the engine API in the same way and also be included in the block header. So that will look like an EIP that presumably we would include inpectra or sometime in the future.
00:08:30.045 - 00:08:39.845, Speaker C: And yeah, I have had not. I have not had time to get that together. But yeah, just a quick update there. That's where things are at.
00:08:43.185 - 00:09:25.885, Speaker A: I have a question on that topic related to this and this PR and dancrads pr. Could you please give us quick summary on why can we compute the Bob Gas price on the CL and do the validation and send this value to the el. Right, yeah, because in this case we would not need to add another field to the block header as far as I believe we would just repurpose the excess field to keep this price. I don't know if it could work.
00:09:25.925 - 00:10:16.825, Speaker C: But yeah, yeah, no that, that sounds right. I mean the main pushback with so there is this PR that shall I dropped it's suspects issue or PR 3800. So this one is just basically handling like these max and target values, leaving all of the fee accounting and calculations with the EL itself. There was another PR from Dencrad to basically even go one step further and pull the fee calculations into the CL as well. We can do this. I think it violates the sort of separation of concerns across The EL&CL that we have and generally is nice. So from what I heard is that implementers with peer dos kind of preferred the max and target routes rather than oystering everything into the cl.
00:10:16.825 - 00:10:18.905, Speaker C: But it's an option.
00:10:22.245 - 00:10:27.205, Speaker A: I'm just trying to understand where is the separation of concerns would be broken.
00:10:27.245 - 00:10:43.613, Speaker C: In this case because it's more like the. You know, it just seems to make more sense to have like fees and accounting and all of this within the el. Because that's like the CL doesn't really care about the Blob base fee, for example, whereas the. Quite a bit.
00:10:43.789 - 00:11:34.825, Speaker A: Yeah, but I would argue that actually CL cares because CL handles the networking for blobs and blob propagation. So from that perspective, CL care about these targets. And this is why actually our intention is to have these marks and target given by the CL rather than defined on the elite. And also. Yeah, but from the transaction propagation perspective, here is just. Yeah, just one could be one problem. But I don't know how much this price is used for in the Blob transaction Mempool though probably it is important to have this value at hand to build a block and propagate transactions.
00:11:34.825 - 00:11:40.515, Speaker A: In this case, yes, it must be on the EL as well known in advance.
00:11:43.575 - 00:12:02.915, Speaker C: Right. I mean, so the EL definitely needs the target so it can compute the base fee. The base fee is helpful in the mempool and block validation in all these different places. I would say the CL actually doesn't care about the target, it only cares about the max again, just even more for like DOS concern reasons.
00:12:03.855 - 00:12:06.495, Speaker A: Yeah, I see. Okay, cool. Thank you.
00:12:06.615 - 00:13:28.543, Speaker C: That's how I'm thinking about it. So I'll keep working on the consensus specs, PR and also any ip. And yeah, we will keep those moving along. If other people feel strongly about, you know, this other option of having the base view calculation pulled all the way into cl, we can explore that as well generally from what I've heard. Again, we want to leave that with el. Okay. Anything else on Peer dos? Okay then next up we had an update on fork Choice testing is Ericsson 49.
00:13:28.543 - 00:13:35.905, Speaker C: That's the GitHub handle. Are they on the call? I don't.
00:13:36.685 - 00:13:39.525, Speaker B: Yes, yes, I am on the call.
00:13:39.685 - 00:13:42.065, Speaker C: Okay, perfect. You had a presentation?
00:13:42.565 - 00:13:58.475, Speaker B: Yes, I put the link in the chat. Can you see my screen?
00:13:59.575 - 00:14:00.395, Speaker A: Yep.
00:14:02.295 - 00:15:20.015, Speaker B: Okay, great. So we have been working on for choice compliance testing. It's work supported by Ezekiel foundation grant actually. And okay, so we have implemented a test generator and generated test suites. There is a link to a more detailed report on eth research and I quickly go overview of what have been done. So actually there is more work but we have implemented initial phase this generator and the compliance test suites and our focus Was on ease of adoption, minimize surprise to client teams and in particular we keep the same for choice test format without actually there is only one small changes. We added new check which can be ignored initially and next step will be to work on implementation specific coverage guided fuzzing and using these test suites as seed corpus.
00:15:20.015 - 00:16:25.637, Speaker B: And eventually we expect up to 1 billion of test vectors but it's probably upper bound. Okay, so test generator is implemented now we have a draft consensus specs pull requests to put it into the repo. Okay, as I said, it outputs test cases using standard test format. We have also simple Python test runner which can execute tests using the official workchoice specs. The test generation is parameterized so there are several parameters and we can adjust them and generate more tests depending on your needs. It's currently one problem is that test generation currently is slow, about 10 seconds per test. We know the problem, it's basically a consequence of compatibility.
00:16:25.637 - 00:16:46.525, Speaker B: So we are relying on the existing for choice testing infrastructure and because of that one aspect is that it's kind of slow. However it can be run in a multiprocessing mode. Okay, I hand over to Mikhail to talk about.
00:16:48.905 - 00:17:29.965, Speaker A: Thanks Alex. So this slide elaborates on the ease part that Alex has mentioned. So we have added just one more check or into the folks test format. So the test generator produces the focus test in the usual format and yeah, with just one additional property which is called viable for hand routes and weights and yeah, it can be ignored at the beginning so it just can be skipped. This check can be skipped. Yeah, so that's the extension of the format. So it should be pretty easy to integrate this one.
00:17:29.965 - 00:18:19.209, Speaker A: Next slide please. Yeah, so now let's a bit circle back and yeah a bit described how this test generation seed works. So we have actually three models written in minutesync, which is the constraint solver language. This is one of the models. It gives us an idea of why do we use it and how it works. I mean the constraint solver in this particular case. So this block cover model covers the various sets of predicates that are in the filter block tree to define whether the block is viable for hat.
00:18:19.209 - 00:19:17.725, Speaker A: So on the right we see the excerpt from the spec. So the predicates are kind of flags which indicates whether the voting source epoch is equal to the justified checkpoint epoch, whether it is whether this value +2 is greater than the current epoch, greater or equal to the current epoch. So we actually give a bunch of a different set of those predicates. One can be false, the other true and all this kind of stuff. So we give this vector of predicates to this model as the user input to this model. And this model produces the solution which is written on the bottom. So this is just numbers and some information that will be used by test instantiator downstream which will produce the test, the focus tests out of this data and some other.
00:19:17.725 - 00:20:03.055, Speaker A: So here what you see is just one solution. But we can ask constraint solver to give us all potential solution, all possible solutions with certain constraints on the next slide. Yeah, we have a couple of other models which used to produce similar information for supermajority link trees and for block trees. Yeah, and the next slide please. So the test suite config looks like this. There are instances. Instances are those solutions from the model given by the constraint solver that we have already seen.
00:20:03.055 - 00:20:35.731, Speaker A: There is also the randomness seed that is passed on the test on the test instantiated input and the number of variations and number of mutations. Number of variations is the number of variations with different randomness seeds. And this is the initial one. So the first one will be derived from this one. Yeah. And next slide please. So how the randomness seed is used.
00:20:35.731 - 00:21:38.295, Speaker A: So this kind of like the block tree instantiator that consumes the supermajority and block tree model outputs and some randomness seed. And for instance, in this instantiator we take this data and we use the we leverage on the existing POC choice test helpers from the PI Spec test suite and produce the fully functioning, fully featured focus tests. And just wanted to mention how this just wanted to show how this randomness is used here. So this particular instantiator works in two steps. First step is to create the trio supermajority links given some certain model constraints. And the second step would be to create to build a block tree based on the most recent justified checkpoint. So and on every.
00:21:38.295 - 00:22:25.379, Speaker A: On these two steps, we use randomness to actually partition validator sets and subsample validator sets that will be building the blockchain during the certain period of time. For instance, during step one, we create partitions validator partitions to be active during a certain epoch. So we just use randomness to sample one or more validator subsets. So it can be two. As you can see, in the third epoch we used two validator subset because we need to build to parallel two forks. Two parallel forks. Yeah.
00:22:25.379 - 00:23:24.219, Speaker A: And yeah, the next on the next step, this same randomness used to attach to different block tips and also used to flip the. We also flip the coin when we need to whether this slot will be empty or not and for other aspects of the test as well. But yeah, this gives some intuition behind how randomness used. And so with the given supermajority link and block tree outputs from the model, we can apply different seeds and actually get different test cases with different FOC choice with different weights for each viable head. And yeah, different and some other properties like empty slots and invalid messages and so forth. Next slide please. Now to the test suites.
00:23:24.219 - 00:23:25.451, Speaker A: Alex, do you want to give.
00:23:25.563 - 00:24:03.353, Speaker B: Yes, yes, I just. Switching on microphone. Okay, so again, more details can be found in the link. We currently implemented three test suites like tiny, small and standard. Tiny is like for demonstration purposes and small is for initial adoption and probably for smoke testing. And standard is like for main testing. And yeah, okay, standard is 13,000 and small is 10 times less and so on.
00:24:03.353 - 00:25:02.081, Speaker B: We also planned to generate extended test suites. However, currently, since test generation is low and there is no kind of demand for it, we did not generate it. But we can do that in future if needed. There are also links to tests in TAR format, links to the generated tests and yeah, okay, so a couple of words about test groups. We split all tests in six groups. Block tree, it does main testing kind of focus on trees. Block trees of varying shapes and block weight is more focused on producing block trees with variation in weights kind of to cover get weight functionality.
00:25:02.081 - 00:25:44.455, Speaker B: And also have a shuffling group which focus on shuffling mutation operators. And shuffling basically means that we shift events kind of delay block or make it appear later or drop and duplicate. You can also duplicate messages. Also have a tester thrashing group of tests and invalid messages. Yen work cover which covers various combination of predicates from filter block trim method Yag I hand over to Mikhail again.
00:25:45.035 - 00:26:24.661, Speaker A: Yeah, so we also did with this test suite. We also did integrate it in Taku. So yeah, there was a question about how fast it is to run those tests. So it's like the standard suite with 13,000 tests takes about 20 minutes on my old Mac to run, which is yeah, quite fast. So we have found several issues of different kind. Like there were two HK bugs in the tester session testation processing in Taku. So yeah, but they are super HK edgy and yeah, unlikely to happen ever.
00:26:24.661 - 00:27:25.473, Speaker A: But still. Yeah, what what's interesting is the next one is that there were a couple of issues a couple of failed tests that are related to attestation processing optimizations. So the spec does not nothing about optimizing attestation processing but clients do those optimization to prevent spam on the mainnet we had the incident a few months ago, several months ago so. And yeah so actually some of those attestations were not processed by techo, which is fair, those optimizations are pretty legit. But this is. This caused the divergence from the test result from what's given by the SPAC and what happened in techo. Also in techo folks choice test executor deferred attestations from the future that is coming from the future for later processing and applies them later on.
00:27:25.473 - 00:28:46.085, Speaker A: It's kind of happened implicitly but it does not differ blocks and to say that on the mainnet run it would defer blocks and process them as well. But the folks test executor triggers those parts of the functionality that would not defer blocks. So we have this kind of to deal with this somehow. Also we ran those that test suit with the coverage which just standard coverage metric and yeah the following functionality appeared to be not covered by the suit. So obviously that was expected that execution payload invalid status and all that is related to it kind of like remains uncovered because we do not model this in our test suite. And also there were prote array pruning threshold is quite large to be hit by our test because we produce those tests with minimal configuration. So it would require like mainnet configuration to to hit these yeah functionality.
00:28:46.085 - 00:29:50.735, Speaker A: And yeah, on the next slide we kind of like summarize and open questions to all client developers based on our experience with Taco. So how to deal with the first question is how to deal with the messages from the future. So we have several options here and ideally we would like to yeah, if all clients defer those messages for later processing then probably this is the way to go. So the suite can do this and expect that the client will implicitly apply the corresponding message when the store time allows to do this. So that would be the best option I guess. But there are others to think about and I believe it's pretty much dependent on the fog choice test run or implementation in each client and all the fog choice implementation at all. So the other one is how to deal with legit optimization and client implementations.
00:29:50.735 - 00:30:34.179, Speaker A: So option there are two options here. So the first one is to just ignore those tests by each client implementation separately. The other one is just exclude them from the suite somehow. But honestly we don't like the second one because different clients may have different optimizations and some will work, some will not work. So excluding them all would be kind of like meaning that we do not test some functionality in some client because the other client does not protest the statistation for yeah, legit reasons. So those are open questions to think about when you. Yeah.
00:30:34.179 - 00:30:51.695, Speaker A: When this test suite will be being adopted. Next slide please. Yeah, so basically that's, that's all we have so far and happy to address questions.
00:30:54.885 - 00:31:05.265, Speaker C: Super cool. I have a quick question like how hard is it or easy is it to write these constraint models? Like did you find that part was like pretty straightforward.
00:31:07.805 - 00:31:09.145, Speaker A: Question to Alex.
00:31:11.285 - 00:32:18.325, Speaker B: It's probably writing models not very difficult, but making them useful to generate tasks can be tricky. Actually we originally thought about using some kind of test generator would just give us kind of best coverage of state using standard coverage metrics like branch or statement coverage. But in case of our choice it's not very interesting actually. I mean it's easy to get. And so we come up with this solution to use model based coverage. So for example, we have several predicates in filter block tree and we want to cover various possible assignments of true and false values to these predicates. And of course some of them are not probably possible, they can be excluded by some checks in code.
00:32:18.325 - 00:33:35.435, Speaker B: But we want basically to have a test case for each combination of predicate which is possible to instantiate. And similarly for blocks and for trees we can say restrict us to have like say a tree of eight blocks and just enumerate all possible combinations of such trees, probably with additional restrictions like maximum child count and so on. And so these models, we use these models basically to instantiate models, okay, instantiate this, enumerate all possible variance solutions to this model. And basically when we generate a test case for each instance, this will give us coverage that we want and for writing, I mean it's not difficult, just we need to have some experience with constraint programming, constraint satisfaction programming. It's actually quite convenient, I would say.
00:33:38.855 - 00:33:42.315, Speaker C: Gotcha. Thanks Anskar and Perez.
00:33:46.855 - 00:34:53.375, Speaker D: Not, not Anska here or Anska with a different German accent. So regarding the letting people or forcing people to forcing the test to leave our tests that people cannot pass. So we kind of have a similar problem with the execution layer tests where we have some tests that we cannot pass anymore because we for example we removed the fork choicer that was needed for pre merge. So all of the tests that kind of require you to to pick between different proof of work chains are stuff that we cannot test anymore and we just ignore them. So I would also go the route of telling clients like this is the test suit and if there are tests in it that you cannot pass because of your optimizations, you should just ignore them. Locally for your client but not remove them from the test suite.
00:34:55.725 - 00:35:29.755, Speaker B: Yeah, sure. Yeah, it's definite there will be some kind of cases where clients do not pass tests. But that does not mean that this is dangerous because for example, if some message is early delayed, but there are multiple copies of blocks and digestations arriving in practice, so I think many failures won't be real problems and we need somehow exclude them. Definitely.
00:35:37.455 - 00:35:55.935, Speaker E: I'm just wondering how hard is it to change the test generator for other versions of for choice I particularly have in mind 7732 but also for peer Das, if they eventually go to block slot voting, how hard would it be to adapt it?
00:35:56.715 - 00:36:28.815, Speaker B: Okay, we can generate basically we are currently generating for the nap, but we also try generating to Altair and Capella and using mainnet and okay, we're generating using minimal profile but generating mainnet is possible is just very slow. And for other forks, basically it should be working. Hopefully it will work. You just need to change.
00:36:29.195 - 00:36:45.467, Speaker E: So these are forks that change the way that we. That change the for choice logic actually. So it's not about mainet or minimal or not the form of the blocks, but actually the way we count attestations and the way we weight the nodes.
00:36:45.651 - 00:37:14.547, Speaker B: Yeah, I mean it's still probably work because we just generate these blocks and trees and just run the same actions. So I kind of don't know details. Maybe it will require some adaptation in a good. If you're happy, if you're lucky, it just works. It will just work.
00:37:14.571 - 00:37:56.263, Speaker A: Yeah, I can add more on this probably. I will try. So yeah, as Alex said, it might need to even. Yeah, some folk choice changes, some folk choice surgery might require adjusting those models, those constraint solver models that we were mentioning in the beginning. But for this block slot particular case, the empty slots are kind of the product of the randomness seed that is given to the generator. So it probably will be enough just, you know, to do nothing and this will probably be enough having the empty slots.
00:37:56.439 - 00:37:58.703, Speaker E: Yeah, but Misha, that's very hard to believe.
00:37:58.759 - 00:37:58.951, Speaker A: Right.
00:37:58.983 - 00:38:04.465, Speaker E: So on the block slot the weight would be assigned to a different. To a different node.
00:38:05.565 - 00:38:06.305, Speaker A: Right.
00:38:07.165 - 00:38:16.105, Speaker E: When you're asking for weights on one of these optional tests, I mean the result will be very different on one model or the other model. Yeah, it's a matter of like changing.
00:38:16.885 - 00:38:38.689, Speaker A: Yeah, it's not going to be a matter of changing a test generator because test generator uses the spec actually, so it directly calls to the spec. And if the spec has this change, if the spec has the block slot for choice in it. Then the generator will just use it and produce the outputs correct. Compliant with this pack.
00:38:38.857 - 00:38:39.817, Speaker E: Ah, very nice.
00:38:39.921 - 00:38:40.345, Speaker C: Very nice.
00:38:40.385 - 00:38:41.137, Speaker A: I think I got it.
00:38:41.201 - 00:38:41.761, Speaker C: Thanks.
00:38:41.913 - 00:38:55.639, Speaker A: Yeah, but if there is some change that requires to remove some of the predicates or add new predicates, then we would need to. Yeah, that's just an example. Then we would need to adjust the model that. Yeah. Works with the.
00:38:55.817 - 00:39:02.255, Speaker B: And probably to improve coverage. You also may want to introduce changes.
00:39:09.195 - 00:39:16.335, Speaker A: So are there any other questions?
00:39:24.645 - 00:39:29.021, Speaker B: Okay, if there will be questions, you can write us.
00:39:29.213 - 00:39:57.151, Speaker A: Yeah, just ask us directly in discord whenever. And also we're keen to see these adopted by client different client implementations. We have a PR in Taco. The PR is listed in the references slide to this presentation. So also those test suits are public and we have this PR to the consent specs repo. So please. Yeah.
00:39:57.151 - 00:40:12.835, Speaker A: Find a time to try it out to run it. So the presentation slides. Yep. Thanks everyone for that. For your attention.
00:40:13.425 - 00:40:14.325, Speaker B: Thank you.
00:40:15.625 - 00:41:01.535, Speaker C: Yeah, thank you both. Super cool. Okay, that was everything on the agenda. Is there anything else we would like to discuss? Otherwise we can wrap up early today. Okay, let's go ahead and call it. Thanks everyone. I'll see you next time.
00:41:02.235 - 00:41:03.579, Speaker A: Thanks all. Bye bye.
00:41:03.667 - 00:41:05.575, Speaker B: Thanks guys.
00:41:06.335 - 00:41:07.055, Speaker A: Thanks. Bye.
