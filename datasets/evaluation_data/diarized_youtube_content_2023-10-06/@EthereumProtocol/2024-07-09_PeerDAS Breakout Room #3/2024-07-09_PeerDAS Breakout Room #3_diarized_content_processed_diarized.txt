00:02:35.850 - 00:02:36.790, Speaker A: Hello, iPhone.
00:02:42.010 - 00:02:48.950, Speaker B: Hello. Oh, it's a. What are you doing here, Antonia?
00:02:49.650 - 00:02:50.510, Speaker C: I mean.
00:02:51.010 - 00:02:52.130, Speaker D: I mean I.
00:02:52.290 - 00:02:55.030, Speaker A: In order for me to understand, the more the better.
00:02:56.490 - 00:03:01.480, Speaker B: Oh, I see. I didn't know you attended these like breakout room calls.
00:03:02.260 - 00:03:06.760, Speaker A: I do not usually, but again, for me it helped me understanding context.
00:03:07.620 - 00:03:10.560, Speaker B: Right, that makes sense.
00:03:12.180 - 00:03:13.720, Speaker A: Are you Kev, by the way?
00:03:15.620 - 00:03:23.240, Speaker B: Yes, I think I'm the only one who's I change it. Yeah, my iPhone just usually says iPhone.
00:03:25.950 - 00:03:28.158, Speaker A: I have the same issue by the.
00:03:28.174 - 00:03:31.810, Speaker D: Way, when I connect from my. From my iPhone in zoom.
00:03:33.710 - 00:03:38.890, Speaker B: Yeah, I'm just going to leave it. I don't really know how to change it on my phone.
00:03:41.950 - 00:03:42.262, Speaker E: But.
00:03:42.286 - 00:04:02.770, Speaker B: Yeah, this should be good. Hello Shawe.
00:04:04.150 - 00:04:06.610, Speaker F: Hello. Hi Kev.
00:04:07.430 - 00:04:10.730, Speaker B: Hey Mikael.
00:04:14.350 - 00:04:16.570, Speaker F: You are an iPhone, you know?
00:04:17.650 - 00:04:30.030, Speaker B: Yeah, my phone doesn't have the name when I joined by Zoom. I don't know why to change.
00:04:37.970 - 00:04:42.670, Speaker F: Antonio. Hi Mikael. Did I pronounce it correctly?
00:04:43.140 - 00:04:44.560, Speaker C: Hash away. Yeah.
00:08:08.500 - 00:08:34.300, Speaker F: Hello everyone, thank you for joining the call. Just posted agenda and meeting notes in the chat. So let's start with the client updates. Anyone want to share the progress in the past two weeks please?
00:08:39.480 - 00:09:25.180, Speaker D: I can start. So for the last two weeks, yes, we added a new pr where data columns are activated from the EIP 75944 epoch. Before data columns were activated at Deneb Epoch. It's not anymore. The case also about sampling. We did only verify that the root and the colon index was corresponding to the expectation when sampling. We not checked at all the KZ proof or Kangenja sidecar proof.
00:09:25.180 - 00:09:53.140, Speaker D: And now we test them also. Yes, we added some unique tests for robustness and currently we have some issues with an appeared as Testnet one where we cannot sync past a given point and we are working on it. Basically this is the updates from the last two weeks.
00:09:57.850 - 00:10:01.990, Speaker F: Thank you. And then Lighthouse.
00:10:04.170 - 00:10:43.696, Speaker C: Hello everyone. I've mostly been out last couple of days, but last week I spent most of the time working on sync. We fixed an issue with our RPC limit and previously it was preventing us from serving Dylacom's by range request. And that's working. So I think, I believe Lighthouse was serving by range requests for a while until it broke. And then I've also got Lighthouse to sync work on our branch but we haven't deployed it yet. There are still some minor issues that we wanted to fix before we deploy this.
00:10:43.696 - 00:11:43.732, Speaker C: It's very slow at the moment, so we're still looking to sync. But in terms of serving data, House is able to serve data now, but the latest Devnet one, it's currently broken because the change with disabling the inbound rate limiter kind of review a bug in our sampling process. So one of our Lighthouse supernova actually ended up sending a large number of sampling request with the other super node and eventually causing that one to be out sync as well without the rate limiter. So for one from our teams been looking to that, I think that should be a pretty easy fix. And we're not sure if we still want to revive Devnet one or just relaunch it. Want to see what people's thoughts are directly.
00:11:43.756 - 00:11:56.680, Speaker E: Meter was re enabled, by the way, so I was hoping that we would see something else, but we just have too many forks at this point. I don't know if we can still recover from the stage.
00:11:57.660 - 00:12:57.710, Speaker C: Yeah, I think it's possible, but it will probably require a bit of hack because we still have all that validators being active. It might be easy to just relaunch, but I think depending on what everyone's preference is, because I think some people are still debugging issues with sync. So maybe one leave it on? I'm not sure, but yes, today I've been trying to use a tool, a manual httpsync tool that Michael from our team created. It's working the past, but I haven't been able to get it to work on the lighthouse in Devnet one. I could try to run on other clients if we want to. So what it does is it basically downloads blocks from an up to date node and then push those blocks to another node, which in turn will cause a big reorg. So then they'll eventually be on the same chain.
00:12:57.710 - 00:13:34.596, Speaker C: It's kind of a hackath. So that's what I was thinking. It might be easy to relaunch, but yeah, we can try if you want to. But on the lighthouse side, we're looking into fixing the sampling bug. Other than that, we have tested Kev's rust KZG library, which is like a replacement alternative to CKZG. It's worked pretty well, and I done a bit of testing on it. The metrics so far seems pretty comparable to CKCG at a high level when the conditions.
00:13:34.596 - 00:13:54.370, Speaker C: Great. So we're looking to merge that as well. Yeah, I think that's pretty much it. From life outside, we will continue to focus on getting sync to work and to just try to stabilize our das branch. Yeah, that's it from our side.
00:13:59.510 - 00:14:06.446, Speaker F: Thank you. Barnabas, go ahead.
00:14:06.478 - 00:14:44.000, Speaker E: Yeah, Jimmy, you mentioned that the rate limiter buck could be very easily fixed. Would it be possible to get that fixed ASAP, then we would have relaunch with Devnet two, let's say. Because this is. Yeah, it's hard to recover from this point. And then because this rate limiter really limits capabilities of other people thinking to the devnet because we don't have too many nodes, so other peers are able to serve the state, but it's just not fast enough.
00:14:44.990 - 00:15:16.130, Speaker C: I see. I was wondering if we could raise the rate, limiting the rate limits to a very, very high limit instead of just disabling it completely, just in case again, we might kill it. Again. I think if we don't set a limit, there's still a chance that we will kill it. It's better if we just raise the limit to a high enough limit. High enough for people to download stuff. Yeah, but we can talk offline.
00:15:16.130 - 00:15:19.350, Speaker C: I can send you the flags needed to raise the limit.
00:15:20.530 - 00:15:21.310, Speaker E: Yep.
00:15:29.530 - 00:15:37.360, Speaker F: Thank you, Jimmy. Any other kind updates? Load star?
00:15:39.500 - 00:16:09.588, Speaker G: Yeah, I can go for load star. So basically last week I was able to sync load star to the Devnet one head when basically the APIs were working. And I was also able to stay synced. Stay synced with the hedge for some time. And this was basically using supernodes. And. And right now, yes, I have not baked in any validation slash roof validation, even though.
00:16:09.588 - 00:16:48.498, Speaker G: So. But I am trying. Not right now. Trying to do is basically change the sync mechanism so I can also sync, so that Lordstar can also sync from known supernovas. And through our various mechanisms we have ranging by root and when we don't have the parent, so we do some unknown blocks in our range uses the root bus. There are some mechanisms that we have to pull the blocks and the data for it. So I'm sort of getting it working.
00:16:48.498 - 00:17:28.480, Speaker G: And then load stars can basically take part, take part in the Devnet itself. Once that is done, I believe lobster can also very well serve solve the data columns because, I mean, on the local testnet, I'm just using lots of Lordstra to Lord Star lost our convert to do all the debugging and to fix all the issues. And so if basically this, this goes well, then I will also sort of integrate the validation and proof so we can basically then participate in the Demnet.
00:17:36.380 - 00:18:01.170, Speaker F: Thank you, Daniela. Client updates. Okay, then let's talk about the deathnet one. I think one of us. Could you give us an overview?
00:18:02.550 - 00:18:38.720, Speaker E: Yeah, sure. So last week, when I disabled the rate limiting, that's when really the whole network just went haywire. And right now we are participating. We have six different forks, so each client is sitting on its own fork. It's. Let me send you to the URL. So we have prism on their own forks, Lighthouse on its own fork, and Taku is the only one that is able to agree with itself.
00:18:38.720 - 00:19:11.860, Speaker E: But every other client just seems to fork away, even from its own client implementation. So I think a relaunch is worth it. We can probably do that once we have a rate limit, a higher rate limit option from Lighthouse, and when Lodestar is ready to be onboarded, maybe end of the week. Sounds reasonable.
00:19:17.880 - 00:19:23.780, Speaker G: Yeah, we'll be ready by another week to join the Lebanese.
00:19:30.160 - 00:19:31.060, Speaker F: Jamie?
00:19:31.800 - 00:20:10.390, Speaker C: Yeah, I was just curious for the question for prism, because I think Prism's supernode also died around the same time when the lighthouse supernode died. So I wonder if we know what happened there. Was it because if Lighthouse somehow does the prism node, Prism would probably have some sort of rate limiting to prevent that from dying. So does anyone know why? Like, what happened to the prison supernotes? I think it would be great to have, like, at least two clients supernote working before we do a relaunch, because I think having just only one supernote client is a bit risky.
00:20:10.770 - 00:20:20.070, Speaker E: No, we should have deku, and there's some super nodes enabled as well. So we had six super nodes, not just lighthouse.
00:20:20.490 - 00:20:29.460, Speaker C: Yeah. Yeah. Well, I was wondering what happened when. When Lighthouse disabled rate limiting. Did the lighthouse node kill all the other supernodes as well?
00:20:32.080 - 00:21:00.400, Speaker H: So I think for prism, we have introduced a few changes over the last week to our rate limiting strategies. So I think when Lighthouse died, it kind of triggered a few bugs. So I'm trying to track down the bugs right now so that we would effectively be able to serve, you know, all the columns, all the nodes, rather than simply rate limiting everyone, hopefully by the end of the week, we have the fix in.
00:21:09.980 - 00:21:10.800, Speaker F: Keith.
00:21:13.620 - 00:21:29.420, Speaker B: Yeah, just trying to understand. So each client is on their own, and is it because of a different reason, or is it that the rate limiter got removed and then they just started disconnecting from each other?
00:21:38.680 - 00:21:52.290, Speaker H: I think we had a few bugs. So one of the bugs is our rate limiting strategies. So we have these custom limiters for data columns, and I think the other bug manu is trying to track down, maybe he has more info on that.
00:21:54.950 - 00:22:06.610, Speaker D: Actually, the bug I'm tracking down right now is a bug about syncing when you start a new node, trying to connect to the existing network. So it's a different bug.
00:22:07.550 - 00:22:10.678, Speaker H: Okay, great.
00:22:10.814 - 00:22:18.050, Speaker B: But I guess did all of this, the forks start when the rate limiter was removed or were they on their own forks forehand?
00:22:19.270 - 00:22:21.970, Speaker E: It happened when the rate limiter was removed. Yes.
00:22:22.390 - 00:22:38.690, Speaker B: Okay, I see. And okay, so this implies that if you have a network with just lighthouse nodes or a network of just prism nodes, then once the rate limiter is removed, then it will also fork away from each other and it's just the tecu nodes that don't do that. Is that correct?
00:22:41.070 - 00:22:43.052, Speaker E: That's what it looks like currently, yes.
00:22:43.246 - 00:22:55.660, Speaker B: Okay, I see. Is there any way I can get more context on the rate limiter issue if it seems to affect every node?
00:23:04.600 - 00:23:50.850, Speaker C: Yeah, I'm actually not sure what's the root cause of it, but I think when Lighthouse got restarted, one of the supernodes started sending out like huge number of sampling requests and that kind of overwhelmed the others lighthouse super node. So that means both Lighthouse supernode stopped serving other stuff. So I don't know if that's the reason why the rest of the network started having issues. So it basically lost the two at the same time when it was turned off. I think it's not really a direct issue with the rate limiter. It was just a review about sampling code. Right.
00:23:52.550 - 00:24:06.580, Speaker B: Cool. Yeah. I mean at least, yeah, it can be sort of reproduced of just like lighthouse or just prism nodes, which means that it's probably easy to test locally in debug, I think. Or at least easier than testing across multiple nodes.
00:24:22.880 - 00:24:51.442, Speaker E: I think it might have also been because of the ENR change. There was like some ENR issues post the launch of the different clients and then we rolled out an update to Lighthouse and with the fix and I think that also had some issues. Yeah.
00:24:51.466 - 00:25:30.320, Speaker C: Another thing I wanted to raise, I think maybe we have a spec discussion session later, but kind of related. ENR is not always available, so it means if other nodes connect to us, we don't know their super nodes. So it's not a super reliable way to identify supernodes. So lion's got a pr to consensus spec to add a metadata b three query. Maybe we can talk about this later, but I think that would help. Maybe. I'm not sure if we want to make this part of the Devnet two spec.
00:25:30.320 - 00:25:32.280, Speaker C: I think it might be useful.
00:25:49.790 - 00:26:23.700, Speaker F: So I wonder if. So if we want to restart, I mean relaunch Devnet to us this inspect or we have like more time to add more features to it or not. I mean, so Jimmy, you mentioned that having a extra request response API is nice to have or it's like a must here.
00:26:29.080 - 00:27:11.010, Speaker C: I think it depends on how nodes are like depending on how clients are identifying supernotes. I think EkU looks at peers that are subscribed to the subnet. I think that would still work if we're purely looking at ENR. And if ENR is not available, we just fall back to the default, then that might not be enough, which is the previous behavior in Lighthouse. So if other clients are all just looking at subscribe peers on the subnets, then that's fine. I think we can live without having the metadata query for now.
00:27:21.350 - 00:27:51.300, Speaker E: Is it 3821, how much implementation would this add for client teams?
00:27:59.810 - 00:28:09.270, Speaker H: It wouldn't be too much, but it would be a bit of work. So if we do have it as a requirement for Devnet two, I don't think we could launch it by end of the week.
00:28:24.980 - 00:28:40.360, Speaker C: Yeah, sounds reasonable. I think. I think having a Devnet to test is probably still like a bit more useful. So maybe we can leave that to the next Devnet if we can launch relaunch a new Devnet next week.
00:28:56.830 - 00:29:15.620, Speaker E: Okay, so let's aim for Devnet to relaunch with the same spec as the previous one with all the client fixes and hopefully at the end of the week. And they should probably include Lodestar if they are ready as well.
00:29:29.880 - 00:30:07.830, Speaker F: Sounds good. Any other thing we want to discuss with. Definitely. Okay, if now, then we can start with the state discussion. So we have two items on the agenda. The first one is that pop has a new proposal here to make.
00:30:09.410 - 00:30:59.610, Speaker I: Yeah. The is to decouple the network subnets from the core protocol. Like for example, when we think of the attitude subnet, right? We have the committees. It's like first of all, when you send the attitude, when you work harder, you send it to the committee, right? You don't send it to, you don't think that you send it to the subnet. You send it to the committee. And the community is just like an abstract group of people. And yeah, I want to do the same with as well because I don't want, when you send the columns, I don't want you to think that you send it to the subnet, but you send it to, to like a group of people.
00:30:59.610 - 00:31:54.270, Speaker I: Yeah. So, so that, so that, so that we can, we can decide the underlying network, like in the way, in any way that we want. Does that make sense? So it's like when you clearly when you take a custody, you take a custody of the column in the subnet, but I just renamed the subnet to something else. Like two custody groups is like when you custody the column, you custody the custody group instead of a subnet. Yeah, yeah. Please take a look at the PR.
00:32:01.170 - 00:32:12.350, Speaker F: Thank you. Pop. So it's to make the peer desk part more general, more abstract.
00:32:17.300 - 00:32:22.720, Speaker I: Yeah, it's like make the network layer and the core protocol like decouple to each other.
00:32:38.540 - 00:33:13.890, Speaker F: So it looks nice and we, I need more time things to, to help review it, to give feedback on it before we merge it. And it seems to be the target of Devmay three if possible. Yeah. Any feedback or suggestions on this PR?
00:33:21.230 - 00:34:07.960, Speaker J: Yeah, from my side, it's just, just what I wrote in the, in the PR as well. Is that so conceptual decoupling is I think a good idea. The, my concern was that we, it seemed to me that the, the only reason we are actually grouping the columns, I'm just trying to use this item here. So why were you grouping the columns? Is because of the presumed inefficiencies in the, in the topic based gossip, subtopic based distribution. So, so we are decoupling it but we are doing the whole thing just because of that. That's how I understand it now. But I have to look into it in more detail.
00:34:07.960 - 00:34:15.480, Speaker J: And for me even if we would do it just because of that, maybe decoupling makes sense. But yeah, I have to look more in detail.
00:34:24.390 - 00:35:35.150, Speaker F: Thank you. Any other suggestions? Feedback? Okay if not then please comment on the PR itself. The next one we have. Yeah, we like to revisit the, the block, the block limits here again. So I think it, I think from the previous course AC D and P records that we seem to prefer the one that not handle gas free computation in the Cl side. So this PR and has some other steps to do like changing the engine APIs and also needs el client to support it. Yep.
00:35:35.150 - 00:35:52.160, Speaker F: And any you mean suggestion feedback on this pr like how much we want to include it in picture.
00:36:00.940 - 00:36:02.840, Speaker E: Wouldn't this be a peer destroyer.
00:36:05.870 - 00:36:06.158, Speaker C: Or.
00:36:06.174 - 00:36:29.010, Speaker E: Would you want to include it in Petra anyway, whether Pyradus makes it in or not, I mean it would be good to have it in Pectra because then like if we do peer dust we don't need to fork the EL doing to increase the number of blobs.
00:36:34.000 - 00:36:47.940, Speaker F: Right. That's the logic of being part of the picture is it will make peer desk update cleaner.
00:36:53.400 - 00:36:59.130, Speaker I: It seems to me that this issue is not related to peer desk. Right. It's just, it's like modulated to four.
00:37:07.190 - 00:37:19.090, Speaker F: Right. But just giving us a foundation of like increasing blood limit easier in the future.
00:37:27.080 - 00:37:37.820, Speaker E: Last week we didn't get any pushback from the. So I feel like it could be pushed into picture even.
00:37:42.160 - 00:37:56.430, Speaker H: Yeah, like implementation wise, it'd be a lot easier for us if it's like with Vectra. We've been doing some work on trying to implement the separate activation epoch after Electra, and it's just a lot cleaner that way.
00:38:15.620 - 00:39:14.900, Speaker F: Good. So I suppose the next step is having an ip and changing the engine API interface, and that we might also push another push again, again in the call on Thursday to make sure the EO people are aligned with it. And another item that stokes that the message is do we realistically see ourselves changing the flop target to anything other than max divided by two? If we do, then we should also have the cl send target along with max values. Any stores on it?
00:39:21.160 - 00:39:40.390, Speaker E: I think it would be better to send both. That's going to give us a lot more flexibility down the line. Even if we just, even if we don't change anything in the foreseeable future, I think it's better to send both.
00:39:55.290 - 00:39:57.350, Speaker F: Thank you. And anyone else?
00:40:00.210 - 00:40:30.430, Speaker I: I'm not sure, but I think the idea of dividing it by two is from the EIP. Like the idea is from the EIP 1559, right? I think. I think the idea is adopted from that past EIP. Maybe. Like what is the reason for dividing it by two in the EapDev one five f nine? Maybe. Even if anyone know the reason, maybe we can answer that question like easier.
00:40:31.810 - 00:40:38.150, Speaker C: There's no specific reason that it has to be by two. It's just, yeah, it's just a parameter choice.
00:40:39.410 - 00:40:44.034, Speaker B: I mean, I think the idea is that it means it's robust to even.
00:40:44.202 - 00:40:46.750, Speaker G: To up to half of the proposals.
00:40:47.730 - 00:40:51.780, Speaker B: Wanting, for example, to boycott 15 by.
00:40:51.820 - 00:40:59.960, Speaker C: Fixing the block size and therefore the gas, and expect the power to piece instead.
00:41:00.460 - 00:41:03.116, Speaker G: So basically we lose some robot to.
00:41:03.148 - 00:41:06.520, Speaker C: This attack if we increase it above one half.
00:41:30.190 - 00:41:46.050, Speaker F: Any other suggestions? If not, then we can start the open discussions. Anything we'd like to discuss today?
00:41:55.110 - 00:42:29.720, Speaker B: I have two things. The first one was regarding metrics. I spoke to Bonibus about this, I think, last week or yesterday. Is there a time where, I think where clients are looking to sort of implement all of the necessary metrics in order to sort of get a good view on what the p two p network looks like? Maybe this is like too far in the future. I just wanted to get a handle.
00:42:36.310 - 00:42:52.090, Speaker H: I think for us, we have a set of metrics for all topics that we apply for, so I think it's relevant for peer to us too. Are there any specific metrics that you're particularly looking for?
00:42:53.990 - 00:43:04.890, Speaker E: I posted a discord message. We created a list of, of the essential metrics that we probably see we would need.
00:43:06.230 - 00:43:07.078, Speaker H: Oh, okay.
00:43:07.174 - 00:43:08.410, Speaker A: All right, cool.
00:43:13.070 - 00:43:18.770, Speaker H: Yeah, I think we have most of this, a few of them, but maybe not all.
00:43:33.400 - 00:43:38.740, Speaker B: And I guess lighthouse has most of them as well. Is that right, Jimmy?
00:43:41.400 - 00:44:47.074, Speaker C: Yes, I think we do have most of them, although some of the names might not match exactly, but I think we have most of them. I think the challenge now is to actually gather the metrics, because we do actually need a good size network, a lot of nodes if you want to really get useful metrics for bandwidth. For example, if a full node proposer want to see how much the proposing block and blobs take, then we need at least like enough number of mesh peers in each subnet as possible to get some realistic metric that's going to potentially look like in practice, if we only have one peer, which is a supernode, then it's probably not going to be a realistic representation of what's going to happen in practice. So we kind of need a large network to get real metrics. I'll just lock supernote. Right.
00:44:47.162 - 00:44:59.150, Speaker B: That makes sense, I guess for load star, we're still trying to sync, so the metrics aren't really a priority at the moment.
00:45:01.010 - 00:45:02.150, Speaker C: Yeah, correct.
00:45:02.970 - 00:45:03.790, Speaker B: Cool.
00:45:08.460 - 00:45:12.080, Speaker G: I mean, metrics can become priority. Maybe next week, I think.
00:45:14.020 - 00:45:29.560, Speaker B: Okay, cool. That makes sense. I can't see any of Salius. I don't know if you're on.
00:45:31.860 - 00:46:08.780, Speaker A: Yeah, hi guys. Do you hear me? Yeah, so we actually kind of doing experiment with peer dust development. So we have a bunch of fellow guys from fellowship, and essentially they are trying to deliver something. So we are a bit, I would say, out of sync of the whole thing and likely will not participate in the next devnet. So, yeah, I cannot comment any further. Likely.
00:46:11.000 - 00:46:48.050, Speaker B: Okay, yeah, that's great. Thank you. Okay, I don't think tech is here. Maybe I can just ask offline. Yeah, I guess my second question was, Tecu has merged our rust implementation for the cryptography. Is it possible to allocate some nodes that sort of switch, that use the clive flag to enable it so we can see a network of both implementations?
00:46:50.870 - 00:46:57.620, Speaker E: Yeah, could you please put a message about it and the testing and then I can get to that?
00:46:58.320 - 00:46:59.540, Speaker B: All right, thank you.
00:47:08.640 - 00:47:24.350, Speaker C: Lighthouse is potentially going to merge that pr soon as well. So we potentially going to switch to the rust library as the main one, temporarily at least. So, yeah, I think Lighthouse might be running that next devnet.
00:47:25.050 - 00:47:35.630, Speaker B: Okay, cool. Maybe we can sync offline because I would like to figure out how to collect metrics on the local devnet. I think I was messing something up locally.
00:47:46.620 - 00:47:49.600, Speaker E: Have you tried to use kurtosis, by the way?
00:47:50.300 - 00:48:00.800, Speaker B: Yeah, I'm using kurtosis. The metrics that I was getting from the endpoint seemed a bit off, so I think maybe I was just doing something wrong.
00:48:27.190 - 00:48:29.810, Speaker F: Any other topics to discuss today?
00:48:36.270 - 00:49:24.980, Speaker C: I've just got something just to mention. It may not be 100% relevant, but lion mentioned an approach to decentralized blob building, which is also mentioned in Bancroft's post. Lyons wrote a hackmd page about it, and Michael and I have created a VR to the execution API spec. It's being approved. I'm not sure if it's merged yet. If anyone's interested, it's been implemented in ref, but it's on microspork. So if anyone wants to experiment with it, if we have some bandwidth, we're likely going to experiment with it soon as well.
00:49:24.980 - 00:49:34.470, Speaker C: But yeah, we've got the spec pr open on the execution API repo. Yeah, that's it.
00:49:47.570 - 00:50:24.400, Speaker A: I wanted to. Yeah, can I get a minute here? Yes, I wanted. Yeah, so, I wanted to ask, what is the current situation in optimizations of the specific cryptography function? So, previously, it seems that most of the time was spent in msms. And is there a big change in this, or this is still the slowest part generally.
00:50:26.540 - 00:50:28.960, Speaker B: You'Re asking about the cryptography in particular?
00:50:30.510 - 00:50:33.270, Speaker A: Yes, I mean, for the cryptography, the.
00:50:33.350 - 00:50:45.450, Speaker B: Specific cryptography, yeah, the main part would sort of always be in the msms, because the msms are group operations, whereas everything else is just like field operations, right?
00:50:45.830 - 00:50:58.580, Speaker A: Yeah, and it's still the same. I mean, there was now some new, like very different algos introduced that reduces the usage of msms.
00:51:00.680 - 00:51:39.280, Speaker B: Not as. I mean, there is a technique to reduce the usage of msms by sort of amortizing, you sort of batch across different msms. So in Pippin, maybe this is something for offline, but essentially you can batch the inversions for group additions across different msms. That's the only technique that I know that we're not using at the moment. It's sort of a standard technique used in pipengine nowadays. So you do addition affine formula and then you batch across the inversions that are needed.
00:51:40.260 - 00:52:33.280, Speaker A: Okay, so we just got merged GPU CUDA optimizations for msms in rusty CZG, but unfortunately, at the moment, we don't have that specific functions ported to rust KZG. And even so, the result is that even for smaller msms, it makes sense to use. I mean, it is faster to calculate on GPU. And I was just thinking, is there, I mean, was there some discussion on the potential on trying to use GPU's here at least on some nodes, like a supernodes or so? And if yes, then we likely will. Once we have specific functions, it will be very easy to hook up these CUDA msms.
00:52:35.540 - 00:53:08.772, Speaker B: So I guess my opinion on this is that currently it doesn't seem to be a bottleneck at the moment. So GPU is not currently needed. Cuda in particular is for Nvidia GPU's. So I'm a bit more hesitant on adding those in because then now the super nodes need to have an Nvidia GPU, right. If it was like web GPU, I think that might be a bit more acceptable since that works on every GPU. But yeah.
00:53:08.956 - 00:53:31.280, Speaker A: Yes, maybe I can ask differently. So for sure there is some vendor locking and so on, but let's say there are some nodes that runs on GPU's and are doing faster msms on compared to high end cpu. So is it going to be a good thing for the network?
00:53:35.470 - 00:54:18.970, Speaker B: I mean, yeah, I mean, I guess generally I'd say yes, because you'd be creating the cell that proves faster, right? Yeah, but instead of going to GPU, wouldn't it just make sense to just add more threads to it? Initially I did some benchmarks and for, I think it was eight or 16 threads, we go down to 100 milliseconds. And this is not parallelizing the FFT, like the FFT algorithm, this is just parallelizing the MSM part. Like, in my opinion, I don't think it's currently a bottleneck, but maybe if it becomes a bottleneck, then I think maybe GPU should be considered.
00:54:19.670 - 00:55:11.510, Speaker A: Yeah. So my initial impression actually is that it is possible to, to kind of stack a lot of small msms like we have in Ethereum. The msms are quite small. And this is at least from my view, it seems it's significantly faster than even if you load a high end cpu fully. But this is the initial observation from the results from the benchmarks we got. So anyway, we will keep working in towards this direction, and once we have something working, I think we can just join a network and see is there any changes in metrics or is there any effect of actually doing msms faster?
00:55:13.170 - 00:55:37.220, Speaker B: Yeah, that sounds great. Yeah. If you have any preliminary numbers, it'd be good to know how much the GPU is making proof recovery or computing proofs faster. So then we can sort of say you can get a GPU or you can get like eight threads and you get sort of the same performance. So we can make comparisons.
00:55:38.200 - 00:55:38.940, Speaker A: Yes.
00:55:42.280 - 00:55:59.170, Speaker J: I just wanted to say if it plays in, if we have larger pop counts, because the construction as we are doing it now on the full column, so on the full data is getting more and more expansive as we have more and more blobs, which.
00:56:01.110 - 00:56:01.398, Speaker B: If.
00:56:01.414 - 00:56:36.050, Speaker J: You have the faster deconstruction, then we have nodes which can do faster deconstruction and feed those in. So feed those channels, basically feed those topics which are deconstructed which were not propagating well. So having nodes in the network which do faster reconstruction is definitely helpful. With the current blob count, I think it's helpful, but it's not, you know, it syncs work without, with more blobs, it can be timelines are extending.
00:56:39.670 - 00:56:55.230, Speaker B: Yeah, I agree. I just think that it is a sort of another requirement to say that you now need an Nvidia GPU. So we should sort of weigh off the trade offs between more threads and Nvidia GPU.
00:56:56.250 - 00:57:18.440, Speaker J: Yeah, so I would change to 2d instead of scaling in that dimension, but because in any 2d case, you would not need that actually. But again, whenever you have faster deconstruction, it's good for the network. So it's a benefit for the network, it's benefit for energy consumption and for anything else. So, yeah.
00:57:45.670 - 00:58:04.450, Speaker F: Any other topics for today? Okay, then, thank you for attending. Let's talk soon on Thursday. Bye bye.
00:58:06.830 - 00:58:09.454, Speaker C: Bye. Bye bye.
00:58:09.502 - 00:58:10.210, Speaker H: See you.
00:58:13.750 - 00:58:14.030, Speaker B: Bye.
