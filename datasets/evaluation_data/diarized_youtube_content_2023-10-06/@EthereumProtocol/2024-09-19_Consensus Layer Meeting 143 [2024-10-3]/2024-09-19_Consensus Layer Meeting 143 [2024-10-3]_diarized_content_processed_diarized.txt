00:00:06.200 - 00:00:37.815, Speaker A: Okay, we should be live. Cool. Everyone, this is consensus layer call 143. There's the agenda here in the chat it is issue 1158 on the PM repo. And yeah, we have a number of things to talk about today, both with Electra and then also scaling the bops. So let's go ahead and get started. So first up, I think it'd be helpful just to touch on DevNet 3.
00:00:37.815 - 00:00:53.495, Speaker A: I think the last time we checked in it was going pretty well, but in the meantime I think there have been a few bugs pop up and the last time I looked for participation wasn't super good. Are there any updates on DevNet 3 anyone would like to give?
00:00:57.375 - 00:01:14.355, Speaker B: Yeah, so I think the bezel team's identified what the issue is and they're currently working on a fixed image. Daniel mentioned that it should be ready by tomorrow so we can deploy the fix tomorrow. He's on the call. I guess he'll expand on that.
00:01:16.215 - 00:01:47.613, Speaker A: Yeah, in the end. Just what Paris said. So we had a bug where we did not warm up the address at the beginning of a transaction for 7,702 accounts. So accounts with delegated code. This led to a wrong gas calculation on our side. So this issue I've created a pr. I need to add a bit more of tests but yeah, as far as.
00:01:47.613 - 00:02:01.585, Speaker A: But tomorrow we should be ready. Okay, cool. And that was the only outstanding thing on DevNet 3 that we were tracking?
00:02:03.005 - 00:02:22.925, Speaker B: Yeah, there was one Lighthouse Nethermine node that had an issue. I think right now we've basically chopped it down to hardware issue on the VM we got in the cloud provider. So we've started desync and I have to check up if it's fine now, but besides that I don't think we found anything specific.
00:02:24.305 - 00:03:01.375, Speaker A: Okay, cool. Okay, that sounds like we're good on DevNet 3. Then we'll wrap up those items and hopefully things restore to normal, then we can move on to Devnet 4. So yeah, there's a number of things here. I think the first thing to kick us off, I'll just call out this document from 8 Panda Ops, the DevNet Force specs. There's a section here on open prs. I think this is the best spot just to see what remains before we can get to DevNet 4.
00:03:01.375 - 00:03:42.643, Speaker A: On the CL side, I think the main thing is that it says here listed Alpha 6 for the specs release. I think it will end up being Alpha 7, but if someone feels differently. Let's discuss. There are a number of PRs since Alpha 6 that again, I think we'll go into Alpha 7 and yeah, from here it'd be nice to sync on some of them. So I think the biggest thing at this point is this execution request structure and how we're handling them. So it'd be good to resolve that. I think there was some work on discord around how to handle this.
00:03:42.643 - 00:04:00.295, Speaker A: From what I saw there were like a number of issues around a number of different things. Would anyone here be able to give us an update on that? I do think there is a resolution to get to at least a design that we like. So it'd be good to agree on that particular design.
00:04:06.565 - 00:05:09.687, Speaker C: I mean I can give the update. So basically the open issues that popped up were that, well we. The main thing is that some of the cls decided that it's a bad idea to like having to compute the request hash on the CL side and they want to leave this to the Elite. In response to that we are changing the engine API to pass the request instead of the request hash calculated by the cl so the EL will calculate it. We have also changed the commitment to resolve, like the request commitment in el, we have changed it to resolve a potential hash collision and the commitment is very simple. So Even if the CLs don't have to compute it, it is absolutely possible for them to compute it because it just show 256 of some bytes which they have. So it's not like anything complicated.
00:05:09.687 - 00:06:02.967, Speaker C: There's no rlp, there's no NPT in that commitment. So that should be okay if they want to compute it, but they don't have to with the spec changes. And then finally there was some. We are still kind of debating if we should make the system contracts behave one way or the other, but it's irrelevant for the cls and I think this is basically the result. Today Porters from Prism has raised some concerns regarding the ordering of requests, but these are in fact already resolved in the latest specification, at least the one that we are targeting right now. So I think we are basically done with the design right now. It's just all very messy because the specs are still being updated and the whole thing is just a bit like in the air right now because of the last minute changes that everyone wanted.
00:06:02.967 - 00:06:07.355, Speaker C: But it's basically good now from my perspective.
00:06:09.135 - 00:06:26.955, Speaker A: Okay, thanks Felix. The one thing with the contracts, like we did have an RFP for an audit and so I think we should resolve that as soon as possible. Was there one direction you were leaning? Because I know there's an issue with the typebyte either being emitted from the contract or it handled somewhere else.
00:06:27.675 - 00:07:17.535, Speaker C: Yeah, I mean the changes to the contract are very small. It's not like a big change to the contracts at all to put this by. And we don't have to put it either. And I think officially we are just soliciting proposals right now and the proposals will be in by mid or end October and then the work on the audit will actually start. I don't think people will really seriously start the audit before they have been selected as as an auditor. So I think we used a couple days left to add these opcodes or potentially I have personally have some plans to make some refactorings in the contract but just to try it out if it's like a better approach. But it's not.
00:07:17.535 - 00:07:33.295, Speaker C: Again, it doesn't really affect the state of things like the functionality of the contract at this point is kind of locked in. We just have to make the final call and it doesn't change the output from the EL at all. This is just internal stuff basically.
00:07:35.235 - 00:07:38.975, Speaker A: Right. Potus, do you have something to say?
00:07:40.835 - 00:08:51.217, Speaker D: Yeah, just the abstract problem on this API changes and by the way, as Felix said, I mean we were already implementing and I was reviewing a PR for Prism without knowing that there was a commit of today changing completely how the commit was. So this is what drove that discussion. So after this latest changes of today it seems that the CL is receiving the requests in the same ordering, the canonical ordering that the beacon block has them the CL can return. Now the CL and the EL will be speaking the same ordering, which is what was bugging me. And the only thing that I would try to come up with an agreement which I don't really understand why we are disagreeing on is on the way of sending a hash. I think it shouldn't be a problem for us to compute a hash in whatever Mechanisms helps the EL. If it's only like hashing SHA256 bytes that we already have and we already have the ordering and it doesn't require to include new encoding like rlp, I don't see why we wouldn't do this.
00:08:51.217 - 00:08:54.325, Speaker D: I mean in the end over the JSON.
00:08:55.305 - 00:09:59.093, Speaker C: The main difference is that instead of the like on the cl, on the CL side in the CL block we are using the hash tree root to create the commitment for the requests. While on the E side we compute this request hash which is not quite the hash pretty root but it's kind of similar to the hashtag root. It doesn't I mean, an argument could be made for just using hashtag root on the el. And this is what Matt has also alluded to at one point. But I think it's not necessary because there's no need to create proofs about requests because the requests are just an output that follows exactly the transactions in the block. So basically the request hash on the EL side is just a checksum of the activity that happened within the block which is fully triggered by the transactions. So it's just this, the value of this commitment for the EL is very low because it's literally just an identifier for like something happened or not happened.
00:09:59.149 - 00:10:44.925, Speaker D: I sort of understand. But so that the thing is the current status I think is the worst of both words because we're sending data over the hot path, which is block validation. We're sending data JSON over the hot path, which is block validation. And I would want to just send 32 bytes instead of this. Of course for us a hashtag root will be trivial because it's already implemented for us and it would mean work for the EL. But if it takes us to like computing a SHA256 of bias that we have already, I would be willing to do that work to save the sending the full request. I think we should come into agreement with this.
00:10:44.925 - 00:10:49.725, Speaker D: I don't mind this, but sending the full data I think is the worst possible outcome of this.
00:10:50.345 - 00:11:13.255, Speaker C: No, I mean this is where this is something that has to be resolved between the cls. So there were some people who did not like the fact that they would have to compute two different hashes. Even though the hashes are pretty trivial, but they still wouldn't want to compute like two different commitments. On the CL side, hashes are microseconds.
00:11:13.375 - 00:11:20.835, Speaker D: Sending bytes over JSON is milliseconds. I think this is just a no brainer that we don't want to increase the JSON side.
00:11:23.335 - 00:11:43.025, Speaker C: Yeah, I mean, I don't know. I'm fine with either. It doesn't change the EL implementation all that much. If we receive the request, we computed over the request. If we receive the hash, we just put the hash. It doesn't matter to us in the end, like how it's sent. This is either way totally works.
00:11:43.025 - 00:12:26.893, Speaker C: And it's a very cheap operation. If you feel very strongly about sending the hash, then, well, it has to be computed on the CL side. It's a trivial thing to compute, honestly. It's just you take the SSE lists of the requests that are in the beacon block and you have to write them one BY1 into SHA256 and collect the upcoming hashes and then basically hash concatenation of these hashes again. So it's like a two level tree kind of thing. It's very trivial and all the bytes exist, so it's not like it doesn't require any encoding. I don't know.
00:12:26.893 - 00:13:07.005, Speaker C: I can look into the hashtag as well, but I think the hashtag root will be more complex. It would be more operations. Maybe it's fine to make a definition for the el. I don't know if this is really, really something that you guys must have, then I will look into it and figure out how to specify it in a way that doesn't require introducing like we don't want to introduce SSE in the EL with this change. We are basically walking a very fine line here between like dealing with SSE but also not dealing with SSE too much because it doesn't appear to be time yet to change it. It's not for this fork to introduce SSZ into the els.
00:13:08.865 - 00:13:28.775, Speaker D: And I think just SHA of each list and just SHA of that would be trivial for us to implement. It would be three hashes. This is, I presume, collision resistant. That's it. It's actually four hashes. I'm sorry, four hashes is much better than sending this over JSON.
00:13:29.195 - 00:14:05.691, Speaker C: Yeah, just let's. Okay. This part of the spec is really not that complicated. I think the main stuff we have is just like the design is pretty much locked in right now. It's just that with this thing, maybe we can give it another two days or something to resolve it in chat because I don't think we will be resolved today. Or can we maybe quickly get a show of hands from the other CEOs what they think about this issue? I'm seeing in the chat some people writing just send the stuff in JSON. It's fast.
00:14:05.691 - 00:14:12.975, Speaker C: Also like not everyone seems to believe it's a problem to send the full requests in new payload.
00:14:13.715 - 00:14:47.129, Speaker A: Yeah, the concern for me is just having like yet another. It's more conceptual where it's like you have to then think about a new way to compute the commitment. And granted in this case it is very small. So if this is the best thing we can do, then let's do that. But yeah, I think it'd be nice to avoid like yeah, another bespoke commitment scheme. Mikael, I just wanted to add that I think it's nice separation of concerns. The CL sends the data and the L computes the commitment.
00:14:47.129 - 00:15:26.465, Speaker A: This was already mentioned. I also want to add that if for some reason we decide that this is not optimal for, I mean like data transmission takes too much time, we can switch from. In the next version of new payload, we can switch from. From the entire data to the commitment. I would not just do the preliminary optimization here. Okay. I mean, to Felix's point, it seems like from the CL so far they're leaning towards flat request, meaning just sending the data over with no hashing on the seal other than maybe Prism.
00:15:26.465 - 00:16:03.735, Speaker A: How do other clients feel about this? Or it's fine if you haven't had time to take a look, but an update would be nice. Okay, I'll assume that means no one's had time to look. Okay. Yeah. Thanks for the input, everyone. Oh, go ahead. Yeah, one last bit.
00:16:03.735 - 00:16:21.803, Speaker A: The PRs that are to the consensus spec and to the engine API, they are now updated with sending the entire data. And also there was previously this byte prepending each list. The type byte, it's now dropped as well.
00:16:21.859 - 00:16:23.539, Speaker B: So there is no type byte.
00:16:23.627 - 00:16:56.005, Speaker A: And the type should be derived from the index of each requests, each byte sequence. So if it's 0, then the type is 0 and so forth. Okay, thanks. Okay, I wanted to make a decision on this today. I sounds like we won't fully resolve every question, which is. Okay, I think we should try really, really, really hard to have this completely settled by next week's call. So.
00:16:56.005 - 00:17:28.011, Speaker A: Yeah, what's the right place to continue the discussion? There was like a very long thread on the R and D discord. Is that the right place? Felix or Mikael? You guys are. Yeah, okay. I think it was the JSON RPC API channel if I'm not mistaken. So yeah, please weigh in there. Otherwise some decision will be made. And yeah, you know, maybe just for like broader context.
00:17:28.011 - 00:17:59.349, Speaker A: Like I think at this point we should aim to have like pectro specs frozen by defcon. That's like a month. If we can do it sooner, that's great. But yeah, I think at this point we should be very focused on that goal. I can message you after eth dreamer about it. Okay, so that was, I think the biggest thing. There's also a number of like downstream things to how we answer this question.
00:17:59.349 - 00:18:27.411, Speaker A: So yeah, it is quite important to resolve as soon as possible. Otherwise. Okay, so there were a few other things for Petra. So in particular there is this one. I think it's actually, let me just double check the number here. 3918. This was an update to how we Handle requests for validators to switch to compounding.
00:18:27.411 - 00:19:04.903, Speaker A: And we're going to leverage the execution layer consolidation request mechanism for that. I put a link to the PR here. I think it's in a pretty good place. I think we're in a place to merge it later today. But yeah, if there's any final feedback or comments, now would be the time. Okay, I'll assume not. So yeah, I'll take a final look after.
00:19:04.903 - 00:19:25.377, Speaker A: But I think we can merge that one today, which would be nice. Great. Yeah, Mikel's just saying everything's been addressed and yeah, I took a look the other day. I think we're in a good place with that one. Downstream of that there were then some implications for. I think this is 4:3. Oh no, that was something else.
00:19:25.377 - 00:19:54.495, Speaker A: Sorry, let me grab the number. It was 3818. Let me grab this link here. And again, this was handling how requests, how deposit requests are queued now, particularly in the beacon state. There were some performance issues there. So this is like really important to resolve. I say it's downstream of 3,918 because some of this is touching on like consolidations and deposits and their interplay and all this.
00:19:54.495 - 00:20:17.235, Speaker A: Yeah, I guess I'd have a question for Mikael. Do you feel like this is moving along pretty well and do you agree with my assessment that. Yeah, I mean my redesign needs a few updates after three eight, one nine. Sorry, 3918. Yeah, it's. I mean like the updates on the client side. Right.
00:20:17.235 - 00:20:46.251, Speaker A: Okay, I guess. Then is the PR ready to review or. No PR is ready to review. There are a couple of things from recently inputs. A couple of inputs in the pr. I would say that they're not significant, so I will review them and apply some suggestions. Yeah, it's ready to be reviewed.
00:20:46.251 - 00:21:17.691, Speaker A: It's ready to be merged after these recent things are addressed. Thanks. And yeah, and I think that we need these to get merged sooner because it entails engineering work on consistent client side. One is for the net for. Right? Yeah, definitely. I mean we did a test here and it caused issues with like a high deposit count. So this should be the PR that resolves that and yeah, I would agree.
00:21:17.691 - 00:21:48.135, Speaker A: It's definitely very important for Petra. So. Okay, maybe we'll leave that there for now then and let me just see if there anything else. We covered the requesting. Okay. Anything else anyone sees on like at least for devnet4 picture. Otherwise we'll move to a few other PRs that might go into picture but I think need more broad Discussion.
00:21:48.135 - 00:22:42.685, Speaker A: Okay, so then I think the really only remaining things up for discussion perpetra would be these two PRs on the specs repo, there's 3900 and 3787. So these are both touching the attestation format at various parts of the stack. Let me just grab links here in case that's helpful for anyone. But yeah, this is I think kind of the biggest open question for Pector at this point, so it'd be good to go ahead and make a decision today ultimately. Yeah. So these are refinements of how we're handling the attestations following the attestation. EIP and Pectra already I think it was.
00:22:42.685 - 00:23:02.305, Speaker A: I always get this number wrong, but I think it's 7549 in any case. Yeah, I guess we'll start again with another temperature check maybe. Arn the Duck, I think you're kind of pushing for these if you want to make a case for them.
00:23:06.645 - 00:23:54.655, Speaker E: I can just summarize some of the discussion. So I think there are three options. Basically we can delay everything till the next artwork. There exists something which I would call like a complete conversion. This includes changing the simple attest, the single attestation format, the attestation format, and possibly any downstream APIs. Personally, I'm not that afraid of this option because it's kind of a limited option, it's kind of a well known option. But the feedback has been that this would cause a lot of engineering effort in many clients.
00:23:54.655 - 00:25:52.057, Speaker E: So the third option that I think is maybe the most reasonable to pursue right now is to is to go for a limited implementation that addresses specifically only the security advantage on the gossip channel of the single attestation and then leave other changes for the next hard fork because of two things. First of all, the difference that the full set of changes brings can kind of be implemented in code. They don't really depend on the spec that much. Like if you want two types for, you know, on chain attestations and network aggregates, then that's perfectly fine to code your client this way. And then in the future, if we decide to do the full thing in the spec as well, all that really changes is a constant, which is the size of the list. So if we focus only on single attestation, the benefit that it brings is this idea that we can check the signature before computing a shuffling. And based on experience from past outages both on mainnet and on testnets, every time we run into a case where a shuffling has to be computed, we've kind of seen instability and my suggestion is thus that anybody that or the way that we would introduce these changes to really keep it tight and focused on the gossip channel so that if clients want to implement it in such a way that they just translate to attestation and then change nothing else, that would be an entirely minimal change.
00:25:52.057 - 00:26:16.655, Speaker E: Also to client code bases like it should be at least for Nimbus we're fine with both options. I think it would be sad to delay the single attestation change to a hard fork in the future because this is like a thing just waiting to happen and when it happens we'll. We would regret it.
00:26:20.115 - 00:27:09.305, Speaker A: Yeah, thanks. Yeah, thanks for the update. Right, so what I think that would mean then is going ahead and closing 3787 for now and then focus on. Yeah like you said, just these like networking level concerns of 3900. So yeah I think there's been some other input across different client teams. How do we feel about this approach? I'm not sure if there's a refinement of the 3900PR to make it clear to just scope it to the gossip layer. Maybe just a note on the PR would be helpful but yeah I hear the security arguments around again this DOS vector and I think that one has a lot of weight.
00:27:09.305 - 00:28:28.593, Speaker A: I lean towards doing that for Petra but I would love to hear other clients input. Yeah E Dreamer, you had your hand up first so yeah, so Lighthouse's opinion we've had time working on this and I mean he said that we could have a spec compliant version pretty quickly at least like on the networking level but we wouldn't necessarily have all the changes propagated through the client and I guess we could push that to a later time and I guess it just depends on timing. If we're trying to ship pec day picture as fast as possible then he would like slightly lean towards no, but I feel like if I. I feel personally like if that I agree with. Yeah, I think it's worth doing. Even if we could have a spec compliant version and propagate the changes later but to get it to get the fork done and deal with the gossip or deal with the DOS attack I would feel it's worth doing. Yeah, that makes sense.
00:28:28.593 - 00:28:29.565, Speaker A: Enrico.
00:28:31.555 - 00:29:17.305, Speaker F: Yeah, so we were discussing about this for a long time and internally we're still leaving the door open for the single attestation super minimal implementation and I haven't had the time to complete my first spike over the simplest simple approach. We think that it should be a minimal even if not, super clean, might be ugly, but we could have something that is good to go relatively quickly, but we'd like to confirm that later.
00:29:18.325 - 00:30:28.775, Speaker A: Okay, Terrence, if it's minimum like now, I wonder if we even should have the validator changes, because I was originally the proponent of that. It just naturally feels right to have the validator gossip the single attestation. But that of course, is more invest. That of course, is more invasive in a way. So I guess the questions to everyone here is that would it be better if today we take out the validator side changes? I am personally in favor of it, but I'm also open to not having that as well. But I guess one way or another, I think the security thing is quite important and I am personally supportive of this change. Right, so how do you see it? Perhaps even just in Prism, if we have the validator guide changes, does that imply you need to touch your validator clients and all those APIs, or can you just still limit it to gossip? If there is a validator change, then yes, validator client will change and then the API will also have to support that.
00:30:28.775 - 00:30:31.235, Speaker A: Right. Okay, Arnoduck.
00:30:35.135 - 00:31:59.091, Speaker E: Like, this is more of a point of order, but in theory we can keep that change and still keep the API because the two formats are interchangeable in the sense that the full attestation is a superset of the simple and the single attestation. So we could actually get away with not changing the, you know, BNVC communication protocol, even if the spec reasons in terms of, you know, single attestation. It to me, that mainly sends a signal whether we want to pursue the full change in the future as well. And in that case, I would leave the language as it is. If, like, let's say that today we decide that we do 3787 for the next hard fork as well, we aim towards it at least. Then when we introduce those changes, we could also discuss changing the Beacon API. But again, like, I'm happy to change the Beacon API today as well, or, you know, for the next DevNet or whatever.
00:31:59.091 - 00:32:16.495, Speaker E: To me, that is kind of a separate discussion. Like we can, we can decide on these two things separately in the spirit of not ballooning this change beyond where it absolutely has to go to solve the security issue.
00:32:18.595 - 00:32:33.435, Speaker A: Right. So, yeah, I mean, I think that makes sense in theory. And then my question would be, yeah, how to handle this in practice? Because there's now going to be some, like, extra spec communication that people will need to be aware of. I'm open to ideas on how to handle That.
00:32:41.335 - 00:33:37.655, Speaker E: I mean, in one way, like, I'm happy to propose to like make a PR for the Beacon API as well. And then we can concretely see what it will entail if we today agree on shipping simple attestation on the gossip network. I think that's also an important step for clients to see like how much change it, like how much work it would be to expand that change to the Beacon API as well. And I'm happy to make that PR to the Beacon API as well so that we can see how much it changes the language of the Beacon API so that we can decide that, you know, in two weeks or four weeks or whatever it will be for a future. Ah, somebody says some. This is already on the Beacon API spec. Great.
00:33:37.655 - 00:34:24.595, Speaker E: I haven't really been following there. To me, like, I really think about this change from the point of view of the network and to me, single attestation and the proposed aggregate attestations, those are envelopes and just like, you know, aggregates on the gossip channel today, they have an envelope already which is called. I don't remember what it's called. It's called something weird, which is why I don't remember it. But this is all like just, just for the purpose of simplifying communication between clients on the P2P layer. And we shouldn't, you know, force that into context where it doesn't belong.
00:34:27.815 - 00:34:28.319, Speaker A: Right?
00:34:28.407 - 00:35:11.210, Speaker F: Enrico, just a comment on the PR about the API changes. So that specific pr, I think, is not just something that is impacting the communication between VC and vn, because it's also considering changing the event of the event stream, which I think could have some other downstream effect outside VC and vn. I might be wrong here by just raising the flag that if we go down that path, maybe someone else could say, hey, you're changing something else here, right?
00:35:11.242 - 00:35:19.795, Speaker A: I mean, there's at least two line and stuff that would use those channels, so it would break them if there's a breaking change. Yeah. On the doc.
00:35:22.255 - 00:36:23.891, Speaker E: My personal opinion would be to not change the Event API actually, because imposing this change on, you know, third parties that just observe the event stream. I think that's completely unnecessary. I feel there is a weak, a small argument for updating the Beacon API. It's not as strong as updating the gossip channel. But the thing is that, you know, when you have a list, you have to compute that list and you have to verify that it's correct, that it only contains one element, that the element is in the right spot, and so on. So if we Change the Beacon API to speak this same language when signing attestations back and forth. I think we'll just end up with a simpler API that has fewer ways to fail.
00:36:23.891 - 00:36:47.245, Speaker E: So it's just one less thing to check for. So, and in the BCV and communication, and because the bcv, the BNBC communication is something limited to, you know, client implementers and not passive observers or tooling, I feel that that's kind of different than changing the event stream.
00:36:53.465 - 00:37:43.505, Speaker A: Okay, so to move forward, it sounds like we think 3900 is like in a good enough date. If that's not the case, we should discuss that now. But okay, so we have that one, I guess. Yeah. Arnoduck, if you want to review this 472PR on the Beacon API, just to make sure it aligns with what you're thinking, that'd be helpful. And then what I would suggest to move forward here is that we sort of, you know, soft include 3900 for pector devnet 5, not devnet 4. That gives people a bit more time to implement just the very strictly scoped gossip layer implementation.
00:37:43.505 - 00:39:08.043, Speaker A: We'll have a little more time to understand if that's actually a pretty well scoped change that given the security considerations pulls its weight, then we have another ACD cycle to think about, you know, formally putting it into Devnet 5. Does that sound like a good path forward for everyone? We have a plus one and a thumbs up. Does everyone feel like they understand what they need to do here with respect to their client implementation? Because I think that's like my biggest concern at the moment is just like there's not like a super direct way, I think, to make it super clear just from the spec what we're talking about. Okay. There's journal agreement on this, so let's move ahead with that. And also, yeah, just to be clear, it sounds like we're going to ignore 3,787 for Petra, which should get us to spec freeze sooner, which is very good. Cool.
00:39:08.043 - 00:39:48.685, Speaker A: So, okay, I think that was everything on Petra. Is there anything else anyone would like to discuss? Otherwise we can move to talking about some blobs. Okay, great. So for the blobs, I keep asking and the situation doesn't change too much, but I will ask one more time. Are there any updates for the Parados devnets? We're still working on it, yeah. Okay, thanks. I think.
00:39:48.685 - 00:39:53.185, Speaker A: Yeah, people have been working on it and yeah, it's just moving along as fast as I can.
00:39:53.805 - 00:39:56.545, Speaker E: Yeah. So basically we launched the devnet and.
00:39:57.525 - 00:40:00.517, Speaker A: We have been on finalized for well.
00:40:00.541 - 00:40:18.891, Speaker E: Over a week at this point and today it's been turning to the worst and we're considering to do a relaunch with exactly the same spec. We had a bunch of different client fixes for different bugs. We have a lot of debugging going.
00:40:18.923 - 00:40:21.055, Speaker A: On and we had a good call.
00:40:21.515 - 00:40:27.215, Speaker E: Just on Tuesday in the breakout room. So I think we're on track.
00:40:30.115 - 00:41:32.205, Speaker A: Cool, thanks for the update. Okay, another relevant, really exciting thing is that we did finally merge this engine get Bob's V1PR. So what this does is essentially lets the CL query their local mempool for blobs and it unlocks a lot of cool use cases in particular very likely supporting blob propagation. You can imagine if I am a proposer and I don't have the best uplink, I can essentially leverage my peer's mempools to do the availability check for me. So really cool to see this merged and yeah, I think immediately I'm just curious about implementation status. Are there any CLs who have not implemented this yet? We are implementing it but we have not completed yet. I see there's a get open pr so I'm looking forward to testing that.
00:41:33.755 - 00:41:45.095, Speaker F: No, for tech. We merged yesterday and it will be part of the next release going out soon.
00:41:50.355 - 00:42:33.035, Speaker A: Cool. So yeah, I mean I wanted to bring it up because I think it's especially in light of last week's conversation, I think it's like a really critical thing to at least make it easier for all network participants to handle the blobs as soon as possible. So yeah, I would consider this top priority to get merged into your clients. My understanding is that there's kind of a couple phases of this. Like the first one is just implementing it and being able to like read and then separately there might be another phase for the right path or like when you go to propose. Maybe it's a bit early then to like discuss all of that, but I guess just something to get on your radar. I believe this is something we can do even without a hard fork.
00:42:33.035 - 00:43:03.625, Speaker A: So if we get this together even in the next few months, there might already start to be impacts on reorgs and all sorts of things like that. Consider this super exciting and very important to implement asap. There was a related suspects pr. I don't know if we want to discuss this now. I think the main open question oh, you have something about this or this other PR?
00:43:05.605 - 00:44:13.785, Speaker B: Well, I'm not 100% familiar with the other PR. More like the general topic I just wanted to briefly remark that it's not super urgent, but once this is rolled out, I think we should at least look into the possibility of explicitly prioritizing broadcasting the block over the blobs, because for now what we will still do both for the local builder them itself, but then also the individual nodes as they receive both the block and the blobs, that they will rebroadcast those kind of with equal priority. And so they all kind of compete for upload bandwidth, which especially for the local builder, often is the crucial bottleneck in terms of timing. And given that basically with the mempool, we expect that in most cases, basically all the blobs will actually already be known across the network. Really, the block itself is the only kind of crucial missing piece of information. And so I personally think that there's at least a good case to be made to explore whether we might want to put a small delay on the blob broadcasting so that the block itself usually is basically already fully submitted by the time we start clogging up the upload bandwidth with blops.
00:44:16.005 - 00:44:51.295, Speaker A: Yeah, thanks. Yeah, and this is what I meant a second ago when I said like the second phase to this, my understanding at least is that that's directly the intention because again, that directly helps again say the solo stickers or nodes that are under resourced relative to others on the network. So again, I think that's super important to figure out as soon as possible. And related to that there was this again related PR. This is 3864 on consensus specs, repo and. Right. I think the question here is essentially like.
00:44:51.295 - 00:45:02.147, Speaker A: It's almost like a may or must around node behavior. If you. Let's see. Let me just double check. Actually, I don't know. Enrico, you open this?
00:45:02.211 - 00:45:59.093, Speaker F: Yeah, I can. Yeah, I can. I can summarize very quickly. So essentially, is the PR saying that on the spec we are saying there is a new way of getting the blobs and if the client wants to leverage that new way of getting data, what should do? Essentially, there are two things that PR focus on. One is the must around publishing the reconstructed blobsite car based on the blobs received by from DL, which seems like something that has been discussed over several channels and we all agreed on. And so this saying that if you rebuild the blobsidecar, you have to publish over the P2P gossip. This is one thing.
00:45:59.093 - 00:46:42.585, Speaker F: The other thing is we're more nuanced over the interaction between this publish and the gossip rules. We were arguing that the client should take care of updating the equivocating caches. While when we do that, to close the door of eventually additional blob sidecar coming over the P2P that was essentially equivocating the blob that you reconstructed and sent over the network. So it was kind of making sense to us to say, clients just make sure that you update those so you don't have you close the door over these additional blobs.
00:46:49.285 - 00:46:59.045, Speaker A: Yeah, thanks. Ginger in the chat says that it should be a MAY rather than a must. Any other clients have a take on this?
00:47:02.745 - 00:47:10.125, Speaker F: Well, to really help the network, I think you, you really should do that. So.
00:47:12.065 - 00:47:33.295, Speaker A: Yeah, does anything. It should be required to rebroadcast. Okay. I mean.
00:47:36.035 - 00:47:37.435, Speaker D: There is no actual way.
00:47:37.475 - 00:47:44.975, Speaker E: To enforce the mask, but yeah, I mean, language wise, may or must, it doesn't really matter because most of the clients will do that.
00:47:45.515 - 00:48:12.855, Speaker F: You cannot even enforce the, the dissemination of the, of the things that you receive. I mean, it's not enforceable. You're just participating as a good actor into the, into the network. Like is a must that you should disseminate messages that you, that you receive. The same way you should publish this at the same, at the same level of criticalness to me.
00:48:19.485 - 00:48:19.821, Speaker D: Right.
00:48:19.853 - 00:48:50.955, Speaker A: I mean, the spec here is for like honest behavior. So if we think it's better for this to be honest, which I think it is, then that's how we handle this. But yeah, maybe zooming out just a bit. I think this is something we should resolve soon, but I think there's a little less time pressure, especially given implementations are in progress. Yeah. So anything else for this PR we should discuss right now? I think that was some helpful feedback. Otherwise.
00:48:50.955 - 00:49:37.379, Speaker A: Yeah, there are a few more things on the agenda. Okay. Yeah, I kind of just wanted to leave some space to discuss again our strategy for raising the bobs. My understanding from the call last week was essentially that we have the core pectoral EIPs. There are a batch of, say, like three or four EIPs that we were considering to include to address the BLOB count and how we implement that. So, yeah, I mean, there was a lot of discussion, I think, around solo stickers and nodes on the network and what we should really be supporting. One option.
00:49:37.379 - 00:50:21.205, Speaker A: Well, yeah, so I think there were like kind of two immediate things we could do to start to address the concern again of these under resource nodes. One of them is this engine. Get bobs to be one. Which is why, again, it's really exciting to see that moving along another one would be essentially having a way for a node to specify when they go to build a block. Essentially like a node local blob max. And I either didn't see or I missed a conversation of this route last week so I wanted to leave some room for that. Now I think there was some conversation in other channels and it seems like people were kind of divided.
00:50:21.205 - 00:50:43.175, Speaker A: Yeah, I don't know if there's anything to update there. My sense I think was people generally kind of prefer the skip lobs route and then kind of see what that buys us in terms of headroom before thinking about other options.
00:50:49.435 - 00:50:49.971, Speaker B: Okay.
00:50:50.043 - 00:51:27.649, Speaker A: Onscore has an interesting proposal, custom min priority fee which could kind of start to do the same thing. Everyone is very quiet on this topic. I guess everyone said their piece other places. Okay. Yeah, I mean I think ultimately there's, you know we're going to need more data to actually make decisions here and we have actually really quite had time for that. So. Yeah, that's fine.
00:51:27.649 - 00:51:50.135, Speaker A: And in that case that was everything on the agenda today. Final call for anything else we would like to discuss. Sure. Perry's asking what data we would need. Yeah. Okay. Portis.
00:51:51.275 - 00:53:15.031, Speaker D: Yeah, so on that I do have an opinion. So I think if we still need to check if it's true and if Tony's data that points to less reorg since Denkun starting with client optimizations I think that would point to stakers being able to handle the maximum blob on regular sync on regular gossiping. If that data, if we discriminate that data for home stakers and that data is still true and reorgs have not increased a lot. Although Tony posted something that does make this clear then I think tip, I mean sinking at the tip in gossip is not a problem and we should be thinking on what would increase with a target increase without a maximum increase. So we already have data that proves that stakers can handle the maximum and if we increase the target, what will change effectively change will be transactions in the mempool. We'll have more broad plan on the mempool at gossip but we will also have much more time for syncing and I think what this affects is the speed at which a node can sync when there is a period of non finality. I think this is a crucial measure that we want.
00:53:15.031 - 00:53:31.355, Speaker D: We want to have a client after two weeks of non finalization and check if it can actually catch up to head or not. I don't even know if this holds today and it seems that it's hard to check to test this short of making Cholesky not finalized.
00:53:33.055 - 00:53:40.815, Speaker A: Right. So this isn't even really a node or a client issue. This is more just a particular node in their bandwidth. Right? That's what you're concerned about?
00:53:41.955 - 00:54:30.285, Speaker D: What I'm concerned about is this is the fact that there's going to be actually more blobs in each block that we have and more blobs in the mempool floating around. More transactions with blobs in the mempool floating around. And if we are in a period of non finalization while we're syncing, a node that is trying to catch up while it's syncing is not going to fast forward to the next finalized checkpoint is going to request all blocks with all blocks from their peers and it's going to execute them all. So this I think is the critical thing that will effectively change with a target increase. And I think we need to measure this. We need to measure whether or not we can handle this situation today. If we have some slack into handling this situation today that would allow us to increase the target.
00:54:32.435 - 00:54:34.135, Speaker A: Right. I mean it seems like.
00:54:36.275 - 00:55:02.981, Speaker B: I do want to make one point that I also brought up in the blob metrics chat. If mainnet had stopped finalizing two weeks ago we would also by now have had a lot more forks. So I don't think this version of like saying the client and it syncs to head even would exist. You would probably have a. Most users would just checkpoint sync to the fork that we socially agree on is essentially the canonical.
00:55:03.093 - 00:55:48.505, Speaker D: I don't think that's true. So on devnets you've seen this because clients are like experimenting with new software. But on testnets we've shown that if we stop finalizing because say like percentage of the nodes go offline we are very good in our fork choice is quite stable and we want fork. But in any case like the fork situation has to be strictly worse the non forked one. So I'd be happy if we at least get the data of what happens in the non forked one and we just need to sync. I believe that our clients are like stable enough that if 33% of the network goes offline we are not going to start forking which is simply not going to gather enough attestations to finalize which is what did happen in early.
00:55:52.495 - 00:56:09.035, Speaker B: Yeah, yeah that's fair I guess then the question is is there any situation where we can even connect this data? I mean I guess we can coordinate non finality on wholeski but that won't be that trivial or easy, I guess.
00:56:10.655 - 00:56:28.945, Speaker D: I know for Priest it's very hard to simulate this. It would be nice if other clients have analyzed if they could simulate non finalization on their own clients without making the actual network non finalizing. But we've tried on Prism and it seems that it's incredibly invasive to change this.
00:56:32.845 - 00:57:41.307, Speaker E: I can only speak to the effects of what happens. Like even if the network mostly follows one or what tends to happen is that more and more forks kind of appear because, you know, somebody's halfway through syncing creates a block. And these forks, they're actually legitimate from you know, a fortress point of view. And then the number of them kind of just keeps growing if you're online and if you're not online, then obviously you just see the one main kind of event. But I know in Nimbus we don't really have a good rule for when to call the number of forks because there kind of exists none. The best we have is kind of, yeah, let's just, you know, put at least recently used cap on the number of forks that we track and that's it. And that's maybe the best thing we can do in this case of long known finality.
00:57:41.307 - 00:57:53.775, Speaker E: But it's kind of not entirely satisfactory. And we haven't really tried this since, what was it Pratter that died last and we tried to revive it. Didn't go very well.
00:57:59.915 - 00:58:19.405, Speaker D: Yeah, this was actually my concern that Gurley. We actively tried to save it and we couldn't. Clients could not sync to head. So it's not really clear to me that we are in a situation today that we're robust enough to sync and increasing the target would certainly make it harder.
00:58:27.385 - 00:58:36.575, Speaker A: And maybe this is obvious, but we can't just have an ephemeral devnet that we spin up because we went more realistic networking parameters.
00:58:39.155 - 00:59:03.961, Speaker D: Yeah, devnets would suck for this. I think that the idea, the ideal situation would be if we could fork a client to do exactly what it would do if it wasn't finalizing, but then keep the network finalizing. If we have a solution to that, if some see if this is not so hard to do for other CL plans, that will be the best scenario, I think.
00:59:04.113 - 00:59:07.445, Speaker A: Why can we have a devnet and just take say a third of the validators offline?
00:59:10.025 - 00:59:38.245, Speaker D: Yeah, but then you don't really get the data of what it takes. I mean we have done those experiments and we do finalize bug on small devnets. We have done those experiments of attacking the network on devnets. But whenever this happened on an actual running testnet we couldn't get back to syncing. So I think the P2P side of testnets is very different than the P2P side of DevNet. Topology apparently affects a lot.
00:59:38.545 - 00:59:42.605, Speaker A: Yeah, are in the dark.
00:59:46.345 - 01:00:57.155, Speaker E: I think I can articulate one thing which is when it would become a no brainer to increase it, which is actually that we have kind of in flight a lot of attempts to lower bandwidth and as we deploy them. So for example this ETHGET blobs thing, we kind of should see a decrease in general in bandwidth usage and if we consider what we're using today, kind of a informal ceiling or informal minimal requirement of what you need to run in Ethereum node and if we decrease from there, that is obviously room to increase block counts in the future. So that, that's kind of like a way to say at least like where we're definitely happy to increase the block counts. And then of course there exists the gray area in between where we might just do it and hope for the best. I don't know. That depends on other concerns. More.
01:00:57.155 - 01:01:01.795, Speaker E: Yeah, non technical concerns.
01:01:29.545 - 01:01:39.125, Speaker A: Okay. There's a little discussion in the chat. Yeah. Any other points anyone would like to bring up for today?
01:01:52.195 - 01:02:37.985, Speaker B: Yeah, maybe just one last question I would have on the block front. It looks to me really that like maybe we are leaning towards kind of making this decision basically as late as reasonably possible within the kind of the fork cycle to get as much real world data as possible also on these kind of new PRs that are being deployed and everything. I do wonder though, at what point would it make sense to try to already have a blob throughput increase part of the DevNet so that it would be ready to go. It seems to me like easier to revert back to not having that and removing it back out than to basically add it rushed very late into the devnet process. So I'm wondering whether people might be willing to consider having like an optimistic addition to the devnets for blob blood throughput increase.
01:02:44.245 - 01:03:42.285, Speaker D: I think these changes are symmetrical so it's just changing about by a couple of constants. So it's exactly as hard to add than to subtract. I'm not involved in devnets but I would personally avoid making a change that I might need to revert it, especially if it's the same change. But having said so, we really need to commit to shipping these changes as soon as possible, have a release immediately as soon as the get blobs v1 gets merged I can say that my personal node, I enabled QUIC and it made a huge difference already on my own bandwidth there are some changes that are already being shipped that are in the released clients as optional that already change the bandwidth consumption. If we start advertising our users to use those changes and we commit to shipping everything immediately, I think in two weeks, three weeks times, we might have already some data.
01:03:46.505 - 01:03:48.165, Speaker A: Perry, you had your hand?
01:03:49.225 - 01:04:26.545, Speaker B: Yeah. Just to mention one thing, I don't think it is as simple a change. It's not a constant change for a lot of clients. They include the blob count at compile time, so they have to first figure out how to change how they consume the library, which is a bit of work they have to put in in the future as well. The question is if they put it now or later. And then the second question is if we want to include 7742 and if we do, that means engine API changes and all of the stuff that that implies as well. So I do think it's important to make the decision earlier rather than later because it isn't just the constant change that has to happen.
01:04:28.625 - 01:05:03.365, Speaker A: Yeah, from my understanding, if people want to accelerate this for Pectora, it sounds like 7742 implementation would be the thing to target. Let's see. Well, there's a 7623 question, but you know, assuming we'd be okay without that, then like the most minimal thing would be 7742 and then any IP to raise the target or even consider raising the maximum. So yeah, if you are keen to push Bob's long 7742, I think is the place to focus for now. POTUS.
01:05:07.105 - 01:05:08.125, Speaker D: Apologies.
01:05:10.185 - 01:05:46.453, Speaker A: All good. I mean, I guess one question that hopefully is easy to answer. Would it be helpful to have an EIP that does actually propose bumping the target, say to four or something like that? Because that is something where like we do talk about the set of EIPs that we would possibly put in the Petra. There isn't an EIP for an actual target increase and that is something we'd want. Yeah, Barbis, I think we should make.
01:05:46.469 - 01:06:09.665, Speaker E: A decision whether we want to actually increase the blob with 7742 or if we're going to just play around with some constant and change the constant everywhere. And if we do decide to Change it with 7742 then, and we're actually considering an update of the blob count in Pactra, then we need to include 7,742 in Pactra.
01:06:10.515 - 01:06:51.965, Speaker A: Yeah, I mean, my read is most people or if not everyone's in favor of 7742. So I think that's going to happen. Yeah, Goodginder was asking. There was. I believe Perry had an eip. There have been a couple floating around, but yeah, if someone wants to like polish one and start entering into the discourse, I think that could be helpful. Although it is more of a minor point.
01:06:51.965 - 01:07:37.665, Speaker A: Okay, great. That is a target increase already. Yeah, I forget exactly what this EIP does, but yeah. Anything else? Otherwise, I think people are pretty heads down on Devnet 4 and yeah, we will keep pushing things along. Okay then. Thanks, everyone. Oh, one last thing.
01:07:38.125 - 01:07:38.437, Speaker C: Yeah.
01:07:38.461 - 01:08:11.005, Speaker B: There's one more thing I did want to mention. We do want to do a named testnet for devcon. It would be nice if clients could maybe plan a release with a release flag for the name Testnet. We'll basically be reusing whatever we use for Devnet 4, so it. Does that sound okay for clients? If not, then we can also work around it and just have custom flags. But I think having a name release would make it really easy for people to interact with it at DEVCON.
01:08:12.105 - 01:08:15.085, Speaker A: Will DEVNET 4 be ready by DEFCON?
01:08:20.185 - 01:08:21.685, Speaker B: I would really hope so.
01:08:23.665 - 01:08:36.409, Speaker A: I would also hope so, but I feel like there are still a number of spec things that are kind of in flight. Yeah, I mean, perhaps I can go just aim for that. Yeah.
01:08:36.497 - 01:08:47.885, Speaker B: The main idea is that there's a couple of VIPs, like 7702 that would make sense for people who are at hackathons to test out, and this could be the place that they could easily test it out.
01:08:48.675 - 01:09:03.843, Speaker A: Yeah, no, I mean, I think it makes a lot of sense. I guess my question was just if we want to look for dev3 or devnet4 in terms of specs, devnet4 would be great. Yeah.
01:09:03.859 - 01:09:10.855, Speaker E: Because the contracts will change between DevNet 3 and DevNet 4. So ideally we want to target DevNet 4.
01:09:14.045 - 01:09:40.665, Speaker A: We still have five weeks. That's a long time. We could have done that five by then. Okay then. Yeah, let's. Let's plan on DevNet 4 for a Defcon testnet. There's been no opposition, so I would say it's thumbs up to Perry's question.
01:09:40.665 - 01:10:12.785, Speaker A: Okay, anything else? Going once, going twice. Yes. Everyone Keep focusing on DevNet 4. As Barnabas said in the chat. Okay, cool then. Yeah, I'll see you around. Thank you.
01:10:12.785 - 01:10:17.205, Speaker A: Thanks. Bye, everyone.
