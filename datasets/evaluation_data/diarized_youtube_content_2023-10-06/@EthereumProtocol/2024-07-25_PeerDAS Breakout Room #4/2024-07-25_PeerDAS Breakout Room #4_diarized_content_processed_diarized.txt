00:03:37.470 - 00:03:38.250, Speaker A: Hello everyone.
00:03:41.830 - 00:03:42.610, Speaker B: Hello.
00:03:43.030 - 00:04:09.220, Speaker A: Today she can't make it to the meeting today, so I'll be feeling on her behalf. Yeah. Welcome to the peer Das breakout call. I think we don't have much items on the agenda today, so maybe we just start with a few client updates. Does anyone want to go first?
00:04:12.480 - 00:04:13.940, Speaker C: Yes, I can start.
00:04:15.240 - 00:04:15.576, Speaker A: Yes.
00:04:15.608 - 00:05:04.640, Speaker C: So for prism about pure desk now we run reconstruction in parallel. Before when we wanted to reconstruct the matrix, we reconstructed it blob by a blob, which took some time. Now we did it, we do it in parallel. And also for supernode, when for supernode we used to consider that a block was available only when we receive the whole 128 data columns. Right now we consider the block is available if we receive only 64 columns. Of course 64 verified columns. Why 64? Because when we have 64 we know we are able to reconstruct.
00:05:04.640 - 00:05:26.210, Speaker C: This is it. And also yes, we fixed important memory leak we had from the beginning in our peer desk branch. It was a cache issue actually. And now there is no memory Nic anymore. Yes, it was mainly that for the last week.
00:05:31.110 - 00:06:13.130, Speaker A: Cool, thanks Manu. We can go next for Lighthouse. So in the last two weeks we've been mainly working on stabilizing our Das branch. We've had a few issues in Devnet, one that we still haven't solved yet. One of them is the peer Das KCG library that we run into a stack overflow issue. Dev is looking into this at the moment and at the moment we have to do a workaround to increase the stack size just for now. But we're hoping to get this fixed properly this week.
00:06:13.130 - 00:06:47.830, Speaker A: And then other than that we also investigating our sink issues and trying to focus on getting our das branch changes onto our main branch. Yeah, so a bit of refactoring work, a bit of tech debt fixing, and we're trying to make sure that we can serve the data column by range request reliably, hopefully before the next Devnet. So that's our focus right now. Yeah, that's it for Lighthouse.
00:06:51.930 - 00:07:48.580, Speaker D: I will talk about Teco. So we are passing our alpha free reference tests. We had to all logic before Devnet, but the structures were not updated. So we have updated it. We did a lot of work around observer deadlocks, potential deadlocks around client stability, and we plan to continue on with sites because it's a bit prototype and buggy and we want to improve stability and that tests to make the clients more stable. Also we finally merged sampler. It was 3d for a long time, but we decided to refactor it and we want to combine custody and sampler, but we will do it maybe in a month.
00:07:48.580 - 00:08:16.110, Speaker D: And currently I'm working on loyal Sampleregh. Both samplers will only lock at the moment. We are not planning to turn on for choice to stop imports with one trailing slot. I'm not sure in current testnet if it makes sense. That's all for Turku.
00:08:20.780 - 00:08:30.280, Speaker A: Great, thanks. All right, sorry. Please go ahead.
00:08:31.820 - 00:09:21.800, Speaker B: Yeah, so for Nimbus, we're currently mainly solidifying the code around the P two P interface. Last week for Devnet two we tried to have a super node variant, but on some further interrupt testing we discovered some issues with syncing. It looked like our normal node is sort of sending out some bad range requests. And turns out we had some issues in maintaining the valid custody peer set, so we are currently working on that. Apart from that, the last two weeks we have also collaborated with Kev to initiate the rust peer dust crypto backend integration, and I think we are passing some of the fixture tests. I think if Kev is here he can okay, he's not here. He could have probably elaborated more on that.
00:09:21.800 - 00:10:33.110, Speaker B: We had to refactor some of our reconstruction logic because of the new and recent developments of the peer dust spec in general. And speaking of peer dust spec, for the past couple of weeks it was sort of a blocker for us to use CKCG's Das branch because some of the non peer DAS code was conflicting with our base branch. So we were kind of using a fork of the Das branch and we were passing all crypto tests from there. And now due to the recent merge, we are in process of bumping up CKCG in order to be compatible with the base branch and as well as the peer dust branches in general. We are also investigating ways to maintain the least amount of diff from the base branch, but I think that can be only executed to completion when the fork logic is finalized or in general if we have a finalized fork epoch for the next Devnet. And this week we mostly started working on sampling, so looking forward to finish that up until the next peer does breakout. So that's all from members.
00:10:37.040 - 00:10:41.380, Speaker A: Thanks Anak. Does anyone else want to give an update?
00:10:42.960 - 00:11:02.650, Speaker C: Yes, odors from Grandina team so a few folks from fellowship program are working on Tildas. I know they did some progress. However, as far as I know we still have some sinking issues. Not that big update so far.
00:11:08.990 - 00:11:11.518, Speaker A: Oh thanks Elias, I can go for.
00:11:11.534 - 00:12:04.876, Speaker E: Lots of so basically we were able. So we successfully refactored all our sign, as well as unknown unknown block sinks or partial block signs where only the root was known or we're not. All data columns were available in the corset. So basically overall the entire thing is sort of working now. And we also added ability to be a supernode through param. So if one can specify that cli param, then we can also act as a supernode. Right now we added some tests for the block production data column synthesis part, and so we are passing that.
00:12:04.876 - 00:13:04.410, Speaker E: So we should also be able to do block production in the test. Apart from that, we will now sort of add sampling onto it. And I think one more thing that we should do before the next devnet start is we should basically also include the metadata custody pr. So that, because I think that is sort of a missing link as of now, because if there is an incoming connection, we assume it's custody to be minimum basic custody. So for the next devnet, let's basically have this sort of a complete thing where we have this complete data and we know what our peers offer us so that we can do a better syncing and have a good stable devnet.
00:13:13.040 - 00:14:26.230, Speaker A: Cool, thanks Kajinda. I think there were some comments around the meta v three spectr. I'm not sure what the status is. Maybe we I'll add action to follow up on that. Pr has everyone gone yet? Any other updates? Cool, looks like we don't have DevOps today. I guess there was a discussion on Discord last week regarding Devnet two launch, just in case if any of you miss it, it's been postponed and all the clients are focusing on getting the clients more stable and reliable before we launch Devnet two. So there's no Devnet two now, but maybe we can start thinking about what we want to include in the next Devnet at some point.
00:14:26.230 - 00:14:39.520, Speaker A: Does anyone want to talk about any Devnet? One issue that we still data are still outstanding before we move on to the next Devnet.
00:14:47.060 - 00:15:15.300, Speaker C: During the last Devnet, I guess some clients have had issue with initial syncing, at least prism had, and some other clients as well. I guess I think we should have. Clients should have a correct initial shrink for the next Devnet. So it will be easily possible to add a new nodes after the devnet start.
00:15:21.280 - 00:16:03.840, Speaker A: Yeah, great. I think we should also make sure that we can serve the data column RPC endpoints reliably. I think that was like one of the issues that we had in the previous Demnet as well. Anything else to add for the next stepnet? I guess we have syncing and we also have metadata v three. I'll just write some notes and see if anyone else want to add any more items for next stepnet.
00:16:07.060 - 00:16:25.440, Speaker B: I wanted to basically cross check with every one of you all when should the peer does get activated because for Nimbus it's a bit problematic to activate it at Deneb. So if any one of you all have any alternate input that would be really great.
00:16:35.860 - 00:16:41.400, Speaker C: Is it because you, you have peered us as a separate fork?
00:16:43.190 - 00:17:16.869, Speaker B: Yeah, that's how we were working it out because we had the assumption that it would be a separate fork when we started working on it and it's currently much diffed with Electra. So there is no way we can merge or. Yeah, I mean we can rebase but again that would be significant refactoring and maybe we might miss out on the next Devnet as well if we do that. So a bit doubtful on that. Confused as well. Yeah.
00:17:21.649 - 00:18:10.290, Speaker E: So the way loads are sort of has handled is that, I mean so the electra current electro branch is separate and the peer does branches sort of for Dan app. And what we have done is basically I for example have created a fork in that branch. And so what I do is using the config and epoch and epoch variables, I basically schedule it on that particular epoch and that seems to be working out fine for me on a dynamic genesis. So what exactly is the issue that you are facing?
00:18:13.990 - 00:19:03.090, Speaker B: So what we were doing initially is we were picking up the Purdas fork version and epoch and we were like at the very beginning we were replacing it with Electra but then we realized that the Devnet starts at Deneb so we were replacing it at Deneb and since we were replacing it it was like the logic to have some of the peer does functionality going was sort of like if I am at a fork version greater than equal to Deneb, but I'm just having too much of a blob activity as well. And that is sort of making like just using up more resources in general. So that is why I was asking.
00:19:03.130 - 00:19:03.750, Speaker F: Yeah.
00:19:06.130 - 00:19:12.270, Speaker E: So you mean you are doing blob API calls and all that as well?
00:19:13.730 - 00:19:28.616, Speaker B: Yeah, like we, we were doing like if I don't obviously comment out stuff we would do because we went with a design like that. But yeah, there is no way to have it backward compatible at this point.
00:19:28.768 - 00:19:39.180, Speaker E: Yeah, but you would sort of need to do the same thing whenever we merge it in electro and activate it at a certain epoch.
00:19:41.120 - 00:19:55.190, Speaker B: Yeah, but like if we merge it at Electra I have like a, we have a separate logic where we can specify that if my fork version is greater than equal to Electra, then do all the peer does stuff like RPC and all of that.
00:19:56.530 - 00:20:04.950, Speaker E: But you right now here, you don't introduce a new fork, you just are replacing Dene Fork. That is what you're saying.
00:20:05.370 - 00:20:06.710, Speaker B: Yeah, yeah, yeah.
00:20:07.370 - 00:20:18.560, Speaker E: All right. Maybe you should just add a new fork. I mean to simplify your life. You can just add it as electro and use that, the variables.
00:20:19.100 - 00:21:07.808, Speaker B: Yeah, right, right. Yeah. But like again, that would, that would probably, I don't know about if we were to refactor and like have a less diff, lesser diff from Electra. But yeah, I mean that would do. But like, I think from our perspective, if we activate pure dos, if we, even if we have a Deneb genesis and we activate peer does at a slightly later epoch, that would be helpful because I think while running kurtosis, I was checking the config and it just says that 4k epoch of peer does as well as Deneb is zero.
00:21:07.904 - 00:21:27.300, Speaker E: So yeah, one of the ways basically this can be sorted out for you is that let's say in first few epochs, there are no data columns, blobs. So that way basically you could configure for example to activate your stuff at Epoch one.
00:21:28.000 - 00:21:35.260, Speaker B: Right. And thereby keeping the backward compatibility as well. Oh, for blobs, I guess. Is it?
00:21:36.480 - 00:21:45.740, Speaker E: Yeah. So maybe when then when the next devnet is launched, maybe for a few epochs we don't start any block TX's.
00:21:47.000 - 00:21:50.500, Speaker B: Right, exactly. Exactly. That would be good.
00:21:57.040 - 00:21:57.392, Speaker D: Yeah.
00:21:57.416 - 00:21:58.740, Speaker B: Dustin has a question.
00:22:01.040 - 00:22:03.820, Speaker E: So Justin, what do you mean by fake denim start?
00:22:10.400 - 00:22:11.860, Speaker C: Am I audible?
00:22:15.680 - 00:22:16.460, Speaker E: Yeah.
00:22:17.560 - 00:22:18.420, Speaker A: Okay.
00:22:19.760 - 00:22:23.660, Speaker C: I mean that peer dos will never exist.
00:22:25.850 - 00:22:31.790, Speaker A: Never. It won't. Read the port. Yeah, it's fake.
00:22:39.330 - 00:22:41.710, Speaker B: I think there's some audio issue.
00:22:47.210 - 00:23:15.040, Speaker F: Yeah. So this is mostly an artifact of how interop was done because we're working on peer Das and electron parallel. So this is why we build off the NAB for now. The, I mean, yeah, I agree. Like it will never be deployed during NAB, but the ultimate goal would be to have it after Electra. I do not know when that will be. I think Devnet two is still early to do that.
00:23:16.830 - 00:23:36.990, Speaker E: But how will sort of this basically change anything? I mean basically in my eyes it just rebasing peer dash on top of Electra when that happens. So does that basically impact things in any real sense for us?
00:23:37.030 - 00:23:59.220, Speaker F: No. So we've already worked it out. We can build it on top of either Dnab or electra. But from what I understand, this is, it's not that simple for Nimbus because they have to do a lot of stuff to get it to work. But right now if you would configure peer Das on Electra, prism would be able to do it.
00:24:01.680 - 00:24:20.880, Speaker E: Also, we shouldn't rush towards rebasing it on top of Electra because that code base is sort of moving and it would then require multiple rebases as we move along. And I think it would be best that we rebase on top of Lekktra towards the end to avoid spending those depth cycles.
00:24:23.860 - 00:24:45.590, Speaker F: Yeah, I don't know about the other clients. I think Electra is not in a stable state yet where we can comfortably build on top of it because there still hasn't been a long running lecture Devnet. So for testing purposes, you know, it makes sense to go with the net for now.
00:24:53.730 - 00:24:55.110, Speaker A: Yeah, I think that makes sense.
00:24:57.450 - 00:25:18.880, Speaker E: I think for Nimbus maybe we can, we can make sure that, you know, when we launch the devnet, we don't start drop spamming from, from the beginning and maybe from few talks down the line, depending upon what epoch Nimbus is comfortable with. And from that point on, I guess everyone should be on board.
00:25:19.340 - 00:25:30.390, Speaker B: Yeah, I think, I think that is. Yeah, that can be like the most minimum workable way out. Yeah, sort of. Yeah. Agreed.
00:25:41.410 - 00:25:41.906, Speaker A: Cool.
00:25:41.978 - 00:25:57.330, Speaker B: But in general that would, that would still make like, that would still make our code base kind of non mergeable, but. Yeah, but. Okay. Like it should work enough for testing. Yes.
00:25:58.030 - 00:26:27.910, Speaker E: Yeah, I mean, yeah. We should not basically worry about merge ability as of now because we know that Electra is basically quite big and it's changing and there are other few other things that are being pushed, try to being pushed on electrolyte stable containers for Cl. So I think we should not just worry about merging it into lectures yet and just focus to get the peer dot functionality working properly.
00:26:31.450 - 00:26:33.750, Speaker B: Yeah. Okay. Seems okay to me.
00:26:43.250 - 00:27:28.900, Speaker A: Anything else let's attempt if not, I thought it was to talk about, but I think there are two main big spec changes coming. One is Francesco's fork choice and the other one is pops decoupled subnet change. I'm not sure if client team has had a chance to look at it yet, but I feel like it would be, it would be great if we can have those two inspect changes finalized soon, soon, so we can prepare mano.
00:27:29.720 - 00:29:04.290, Speaker C: Yeah, actually I have a question about validator custody right now. If node doesn't custody the good amount of colon so we can don't score him. For example, if the Ethereum node record announced, I don't know, four subnets, let's say four columns. And if the nodes actually doesn't reply to, let's say, request by a root for the given columns, we can don't score the node, and at the end the node will be, the peer will be ejected from the peer set. But with validator custody, I'm not sure how we can ensure that a node really sets in its node record the good amount of custody currents. For example, if a node had, let's say, 100 watt validator attached, so normally he should study all the currents, and so the node should say on its record the node has 128 custody subnets. And if the node instead of that just announced its custody for subnets, I think we don't have really a way to ensure the node isn't lying and so we cannot penalize the pair in such a case.
00:29:04.290 - 00:29:25.470, Speaker C: I don't know if you see what I am saying or not, and if, yes, if you have an idea of how to solve this issue.
00:29:27.210 - 00:30:01.050, Speaker A: Yeah, I think I get what you're saying. So you're saying that we can't really enforce the validators, to be honest about how many validators they have attached to the beta node. Yeah, so yeah, I'm not sure if there's any way to enforce that, but I guess I think we want order client to be advertising the custody subnet camp based on the spec, but I don't know if there's any way to enforce that. Anyone else have an idea?
00:30:22.800 - 00:30:28.100, Speaker C: Well, I will write a comment on the pull request and we will see.
00:30:33.320 - 00:32:00.970, Speaker A: Cool, thanks Banner. Anything else on spec changes? Yeah, again, I think it'd be good if clients that hasn't look at the PR. It would be great to have a quick look because I think there are quite significant changes that we need to think about soon. Yeah, that's it for spec discussions. Anyone else have anything to talk about? I think everyone is still focused on testing with ketosis, right? Is not having Devnet a blocker for anyone? Cool. I guess it's fine with ketosis. I think there was one spec PR in the last breakout call from Alex, the one to uncouple the blob limit per block across CEO and Yale.
00:32:00.970 - 00:33:02.030, Speaker A: I think there's an ERP created for that change. It's been raised in the last Acde. I think the outcome was that client team should review it before the next ACDC, just in case if anyone's not aware any open items to discuss. I guess it's a pretty short one with no definitions great. In that case, I think we can finish the call early. Yeah. Thanks, everyone.
00:33:02.030 - 00:33:06.990, Speaker A: We'll see you in the next call in two weeks. Thanks.
00:33:07.970 - 00:33:08.910, Speaker C: Thank you.
00:33:09.210 - 00:33:09.706, Speaker D: Thank you.
00:33:09.738 - 00:33:09.970, Speaker F: Bye.
00:33:10.010 - 00:33:10.650, Speaker B: Thank you.
00:33:10.770 - 00:33:11.710, Speaker C: See you guys.
00:33:12.490 - 00:33:12.930, Speaker A: See ya.
