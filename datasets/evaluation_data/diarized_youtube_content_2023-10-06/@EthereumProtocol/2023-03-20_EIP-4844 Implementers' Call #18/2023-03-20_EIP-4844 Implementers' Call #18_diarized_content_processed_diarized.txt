00:00:01.170 - 00:00:49.330, Speaker A: You okay? Welcome to 44 four, call number 18. Couple things on the agenda today. Some spec stuff around SSZ, around the fee market, and then we can chat about work time teams are at and what makes sense in terms of potentially setting up a Devnet. I guess first on the SSD front. So we discussed this on all cordes last week. The thing that came out of it is it seems really unlikely that there's a wholesale move to SSD that happened in Cancun alongside 4844. So given four eight four is already a pretty big chunk of work and moving everything to SSD also would be quite significant.
00:00:49.330 - 00:02:01.740, Speaker A: Teams felt like it probably makes sense to split those up. That said, because we are introducing some SSD in 4844, we still sort of have this open question of like what's the right format to use? So there seems to be this disagreement between whether we optimize for the storage slides on a full node of transactions, or whether we optimize for the proof verification of transactions. And that results in two different SSZ encodings. We didn't get to a solution or agreement about that on all cordes, which I suspect is still the thing that's kind of blocking for us. But yeah, that's roughly where things are at. So it would be good that whatever we end up doing with four four is forward compatible with sort of broader SSZ roadmap, but we don't have consensus on that quite yet. Yeah, I don't know if anyone has thoughts on that.
00:02:11.340 - 00:02:20.280, Speaker B: Yeah, it's going to be hard to reach consensus, I guess, without a lot more eyes on that sort of more broad proposal, I suppose.
00:02:24.780 - 00:02:51.700, Speaker A: How big of a change is it to change the SSD port? Because I suspect a lot of the engineering work is to just add SSD support in the El clients period. But then if we only make this decision a month from now on, all core devs, how big of a change is it to change the 4844 devnet at the time to a different transaction structure?
00:02:52.040 - 00:03:26.220, Speaker B: Yeah, it shouldn't be too bad. I mean, I think the way it's specified right now, what's nice about it is we don't have to bring in the entire SSV protocol because it doesn't involve the mercurialization. But I think most people have implemented the mercurialization anyway and they're freaking consensus layer work. So bringing it in isn't all that bad. But yeah, I think that's the main thing. If it's just tweaks of the, assuming we're going to bring in localization anyway, and it's just tweaks of the layout. I don't think it's a whole lot.
00:03:26.220 - 00:03:38.900, Speaker B: It does affect. Well, it used to affect. I don't know if it still does, given the latest tweaks, but it used to affect a little bit on the consensus layer side. Had to understand kind of where to extract the commitments in order to verify, but that's pretty minor.
00:03:40.440 - 00:04:18.846, Speaker A: Okay. Any other thoughts? If not, then, yeah, I think the path forward is like, I would leave the four four spec as is. And potentially when we change this, then we make the change in Ford 44. But I just don't think. Yeah, realistically, there's a way we can come to a decision. It's like a coin flip. I don't know.
00:04:18.846 - 00:05:01.830, Speaker A: That is like an objectively better or worse approach. And. Yeah, wait until client teams have spent more time on it, figure out a decision. If we're lucky, we won't have to change things. If not, we just make the transaction format change once we have a broader SSD consensus. And I guess, Terrence. So the SSZ proposal, I assume this would be the transaction type as it's defined in the EIP, right? I'm trying to pull up the EIP right now.
00:05:01.830 - 00:05:57.360, Speaker A: Yeah. So new transaction type. So this is basically specific transaction type. And there's some other discussions happening around potentially changing the withdrawal route on the El to SSE and stuff like that. But again, it's like, once we make all those decisions, I don't know that the withdrawal route would affect this transaction type. I don't see how. But, yeah, we can retrofit this EIP to just be in line with things, but, yeah.
00:06:04.800 - 00:06:08.536, Speaker B: Okay. Did, did I hear that right? So just stick with the existing specified.
00:06:08.568 - 00:06:56.540, Speaker A: Yeah, until. Yeah, until there's a change, if there is one. Basically, unless someone here has, like, a strong objection, but doesn't seem like we do. Okay, so stick with the current transaction type and we'll know once there's a decision made on the broader SSD discussion. Okay, next up, Terrence, you posted this eat research post. Roberto, you independently sent it to me as well. The fee market analysis.
00:06:56.540 - 00:07:11.276, Speaker A: Unfortunately, haven't had time to read it yet. I don't know. Terrence or Roberto. Do either of you want to give a quick overview and share your. Yeah, sure. Yeah, go ahead. Sorry, Roberto.
00:07:11.308 - 00:07:12.384, Speaker B: No, you go ahead, Terrence. It's fine.
00:07:12.422 - 00:08:31.580, Speaker C: Okay. Sorry about. So I think overall, it's a very good post and I highly encourage people to read it because this is kind of one of the lesser known design space that people have been working on. And it's definitely something that we do want to get right initially because I feel like we don't get this initial mini data guest set correctly, then we may be encouraged spamming applications and then this is something that unfortunately we definitely want to get it right as early as possible. So basically the summary of the post is essentially saying that given the layer two demands today, that it will take one to one and a half years for the usage to reach the target data gas price threshold, basically. So given that, whether it's worth raising the minimum price because we want to deter spamming applications, basically applications that's using data guests for the wrong reason, and then because it's hard once they start using it, then it's going to be hard to move them away later.
00:08:31.650 - 00:08:32.028, Speaker A: Right?
00:08:32.114 - 00:09:02.950, Speaker C: And then another thing the post pointing out is that it kind of assumes the layer two, their current usage is in elastic, but I will argue it's more elastic than people think because of layer two users paste data gas. And the data, I guess is also a function of, as a function of the layer one code data. So that's my main argument. Other than that, the post is great with definitely one more I sound.
00:09:08.140 - 00:09:39.090, Speaker B: Yeah, that's a good summary. Terence, I left a little comment, I guess with some of those points at the bottom there. It's good food for thought though. I don't know if doncrat is here, but I know in the past he suggested raising the minimum as well, so he would be a good person to hear from on this topic. I don't think I have a strong opinion. I did push back a little bit though. I mean, I think ideally we'd want the market to be able to determine the appropriate minimum price.
00:09:39.090 - 00:10:19.810, Speaker B: But one concern I have with current proposal, it's a little under specified. Not that proposal, but EIP 4844 itself doesn't actually specify what happens if there's a lot more blobs kind of pending in the mem pool than there's available space like an EIP 1559. We know that we have a priority fee, we can sort by that, and nodes are incentivized to do so. The obvious option with 44 four is to sort by max fee per data gas. But since there's no priority fee, there's no incentives for nodes to actually do that. But then again, I guess there's not really incentives for nodes not to do that either. So perhaps that's fine.
00:10:21.540 - 00:10:22.290, Speaker A: Right?
00:10:23.380 - 00:11:05.740, Speaker B: I guess Mophie's pointing out that the priority fee applies here too. That's true. But I guess that's on regular gas. But I guess it's still used to rank. Yeah, it applies here too. Because even though there are two separate markets, it's the same person that's selling, I guess, the product. So you can just variety fee we use for compute gas and bump up your data gas to get shoved in.
00:11:05.740 - 00:11:25.010, Speaker B: Got it. That makes sense. So if I'm competing for blob space when we're in the saturated mode, the 1559 priority fee kind of can serve that same purpose. Is that the argument?
00:11:45.460 - 00:12:04.196, Speaker A: And I guess the, the biggest difference, I'm skimming the post now. The biggest difference, I guess with 1559 here is that when we deployed 1559, the blocks were full already. So it was a question of like. Yeah, Mophie, do you want to answer to Roberto?
00:12:04.388 - 00:12:07.176, Speaker B: I missed this question. I lost audio for a couple.
00:12:07.358 - 00:12:08.090, Speaker A: Okay.
00:12:08.400 - 00:12:32.852, Speaker B: Oh, I was just repeating your point to confirm that I got it right, which is that the priority fee from 1559 can kind of serve the same purpose for the saturated blob case. One could just bump that up. So that's sort of the prioritization rule. Yeah, totally got you.
00:12:32.906 - 00:12:33.510, Speaker D: Thanks.
00:12:36.440 - 00:13:13.170, Speaker A: Right. And then from the post. So one of the challenges is because the blobs will not be full, the price will be, I don't know if I want to say artificially low, but it'll be lower, at least for a while. The price would remain somewhat low because blobs won't be fully utilized. And this is different. The blocks where prior to 1559, they were fully utilized. So it took a couple of minutes to just find the right market price again.
00:13:13.170 - 00:13:18.080, Speaker A: Yeah, proto.
00:13:19.060 - 00:13:44.036, Speaker D: So I think it's a very strong statement to say that the blobs won't be full for the utilized today. The only reason why roll ups use the amount of data that they do is because we're already max out the gas in Ethereum versus the demand that there is in fees.
00:13:44.228 - 00:13:44.970, Speaker A: Right.
00:13:45.660 - 00:14:26.820, Speaker D: And right now, these roll ups, including optimism, they're paying well over 90% of their fees towards data already. So we know that users are willing to pay like a premium to pay for data now if there was more data to provide to them and at a lower cost. I think it's very reasonable to think that those fees that they are currently being utilized for less resources would go towards utilizing all those resources. Fitting the blobs.
00:14:31.540 - 00:14:45.048, Speaker B: Yeah, that's a similar point that I had made in there. Do you feel, Proto, that the end result is that when this does go live, we're not going to be in a situation where we're kind of min pricing for a year, a year and.
00:14:45.054 - 00:14:45.770, Speaker D: A half.
00:14:50.460 - 00:14:52.170, Speaker B: The TLDR, I guess.
00:14:53.200 - 00:15:30.100, Speaker D: I think a year or even half a year is already quite long. I do expect it to be unstable for at least a week or longer where it is quite a jump in going from the current amount of data to the different type of data. So it will take a while for usage to adapt to. Oh, we can make more and more transactions or we can call this contract more often. These type of things that have to be like they find a way in the market, but I don't think it will be instant.
00:15:33.100 - 00:15:37.610, Speaker B: Right? But we don't need it to be instant, right?
00:15:39.980 - 00:15:41.050, Speaker D: Pretty much.
00:15:42.860 - 00:15:51.740, Speaker B: I guess what I'm trying to ask more concretely to you, do you feel that the proposal is fine as is with the minimum one data gas fee?
00:15:53.360 - 00:16:27.930, Speaker D: So personally I'm more of a maximalist of markets where markets can price these things. I do believe that there's a certain lower bound for safety reasons. So if people are concerned that the markets don't find a right price for this, you can set at least some minimum that makes sense for the protocol to sustain this. But beyond that, I'm in favor of just not biasing the fee markets too much.
00:16:29.100 - 00:17:36.044, Speaker A: I wonder if there's a way to set the minimum. I get like to do some sort of mapping towards what's the real economic cost of running this data. Obviously we can know the relative cost with gas prices, but is there some minimum? But again, that's hard though, because you're back to the east USD exchange rate. Like we could set a minimum that targets roughly the real world processing fiat cost of sending over the data. But we're going to set this minimum in ETH and so that's kind of anchored to whatever the ETH price is when you deploy the EIP. Yeah, and I guess this is also something that you could imagine. I guess there's two ways you can go about it.
00:17:36.044 - 00:19:06.340, Speaker A: One is either you trust the market to start and assuming that the minimum was too low, you can then also change the transaction pool rules and stuff without doing like a hard fork to make it slightly harder to get these minimum p transactions in. But you can't do the other approach. Right? If we set a high minimum in the protocol, there's no way to then change things if we think it's too high rather than a hard fork. I don't know, this sort of makes me lean towards not having a minimum or keeping the minimum of like one way because worst case, clients could just each hard code in their transaction pool that they're not going to accept transactions with a one way price or something like that, I guess. Does anyone feel strongly that we should have this higher minimum?
00:19:15.580 - 00:19:44.260, Speaker D: I think a higher minimum is fine as long as there's a case for why this minimum base for certain security guarantees. Because I do think with the target mechanism, we know and understand the throughput, and this type of resource is bounded to a limited amount of time. So all that combined, I think it's very reasonable for us to take low fees on these plumps.
00:19:47.320 - 00:19:50.950, Speaker A: Sorry, are you arguing that we should raise the minimum or leave it as is?
00:19:51.640 - 00:20:45.160, Speaker D: Arguing that we shouldn't need the minimum, given that the resource limiting is bound like the whole thing. We know how much bandwidth it will take if it stays under the target, we know how much storage it will take, and we know that it is a bound to this after like 18 days, that this data starts to get pruned. So even if there's a spike in throughput, this bike will pay for it with the price adjustments. And if there is a consistently below target throughput, then sure, the blobs are cheap, but it is within our model of what kind of resources are going to be used for this. And after 18 days, it's all gone again. So this is not a persistent cost to ethereum.
00:20:51.470 - 00:21:18.650, Speaker A: Yeah, I'm happy to not change the minimum to get as is. Does anyone disagree strongly with that? Okay, anything else on the specs? Okay then.
00:21:21.900 - 00:21:24.750, Speaker E: Sorry, I just going back to the minimum thing.
00:21:26.480 - 00:21:30.044, Speaker A: The thing I don't understand is given.
00:21:30.082 - 00:21:51.910, Speaker E: We have such a small number of blob transactions that are going to get into blocks, and maybe this is just me, I need to understand the better 1559 mechanism. But do we have any concern that the 1559 mechanism won't work as well here because of that small number of transactions, won't we just immediately go up to a really high price or stay at a very low price?
00:21:52.680 - 00:22:21.630, Speaker D: So the precision of the mechanism is smaller. There's discrete steps of fill blobs, so it doesn't adjust like a smooth curve like ep one five f nine, but the curve is there. It does adjust the way you would expect it to adjust. And so, over a long enough time frame, if the usage targets some amount of blob throughput, it will find like a price that reflects that.
00:22:22.320 - 00:22:30.770, Speaker A: Doesn't the curve also look at the target usage since the introduction of blobs, rather than just the last blob? Right.
00:22:31.300 - 00:22:52.470, Speaker D: It's sticky. Yes. So when you start pruning blobs, it doesn't take in account the removal of these blobs. But overall, if you look at 18 days worth of blob data, this is quite insignificant, this small change in.
00:22:55.240 - 00:22:55.652, Speaker A: Whether.
00:22:55.706 - 00:23:35.940, Speaker D: Or not the excess is met. If the excess is truly too expensive, then you'll just see empty blocks for a little while and we'll go back down the price for the blocks. Yeah, this discussion has been had before in EIP about the, I think in the original EIP that changed it from this base feed like mechanism. I think you'd have to dig up the pr number. But when we refactored the fee market, we had this whole discussion about how the price adjustment should account for the pruning.
00:23:37.640 - 00:23:46.552, Speaker E: Yeah, I don't know, I guess I'm nervous about the one way thing. It just feels like we're leaving a lot of value on the table for.
00:23:46.606 - 00:23:49.444, Speaker A: Ethereum that we could be capturing.
00:23:49.572 - 00:23:51.656, Speaker E: And it feels to me like we.
00:23:51.678 - 00:23:53.112, Speaker C: Should be doing something like what Tim.
00:23:53.166 - 00:24:00.110, Speaker E: Suggested, where we do an analysis of how much does this actually cost a network or something to set a minimum gas price.
00:24:01.440 - 00:24:44.700, Speaker A: Yeah, so the problem with that analysis though is you can get some fiat cost, but then the network doesn't know that, right? Like the network only knows it doesn't even know the price of eth. Right? Yeah. And then the other thing is, I think to me the most convincing argument in that post is if the cold start problem actually is like a year long, right? Whereas if we're talking days to weeks or minutes to hours, then I don't think it matters if we're talking for a year, then yeah, it probably does make sense to have a minimum price because we do hard for more than once a year. We can change that minimum price roughly on that schedule.
00:24:46.880 - 00:25:40.300, Speaker D: Yeah, proto so another thing for context, or as example, would be to think about the Shanghai denial surface attacks. Not the one for the hard work, but like the original ones. Then there was this type of dos. I believe you can still see it when you're using full sync and note and see the slowdown for a bit. This gas usage is persistent, whereas these blobs after 18 days were dumb. So even if they're misused at a low gas price, sure, price will creep up. Maybe it creeps up too slowly, but it will creep up and the data will get removed eventually and won't be like an obligation to Ethereum anymore.
00:25:40.300 - 00:25:47.100, Speaker D: There's a very different type of long term cost than the gas resource.
00:25:59.160 - 00:26:45.270, Speaker F: Sadius yes, hi, I was thinking one thing that we may wait a bit until this 4844 is deployed on some testnet and some potential users starts to use it. Because in my understanding there may be some users even not roll ups, that are going to use these blob transactions. Maybe there is actually a much bigger usage than we expect currently, or we are 100% sure that there will be under usage for the first year or something like that.
00:26:45.960 - 00:26:57.200, Speaker B: I think a lot of people are questioning that assumption that there'll be no low usage for a year. Certainly I would be surprised if there were more users popping up taking advantage of this feature.
00:26:57.380 - 00:27:30.150, Speaker F: Okay, so even now there isn't much other greater ways to use these blob transactions. Okay, but if I understand correctly, what could happen is that, well, somebody will spam at the full capacity of what network should handle. So in worst case, well, nothing bad happens. Or am I wrong?
00:27:33.960 - 00:27:40.260, Speaker B: Yeah, I mean there's a max four blobs per block that kind of handles the spam case pretty effectively.
00:27:40.420 - 00:28:02.540, Speaker F: Yeah, the worst scenario is that we have some temporary spam which doesn't generate much revenue for the network in terms of fees, but it kind of tests the capabilities of the network. And this is the worst case scenario.
00:28:03.200 - 00:28:35.800, Speaker A: And I'll push back on that, even it doesn't generate fees because if you have a lot of spam, then it starts competing for blob space and so the price starts going up and so it does generate a lot of fees. This is more, I guess, on the research post, but it's like spam is almost like a weird way to put it, because it's like if the transaction price is low, people are like bidding it up, but then once it gets bid up, what's the difference between that and real usage? They're like paying for the transactions.
00:28:38.480 - 00:29:03.590, Speaker F: Yeah, and this is in any case, it's a temporary. I mean, the blob data will be purchased after a while. I understand this discussion in general, this is interesting, but I personally don't see too much of damage if it happens that the fees are too low, at least from my point of view.
00:29:09.970 - 00:29:24.500, Speaker A: Yeah, so I guess what makes sense as an extent, the post is still quite new on e research. We can see if there's more conversation on it, but it doesn't seem like it warrants an urgent change to the spec.
00:29:27.670 - 00:29:42.962, Speaker B: Yeah, I agree. So I already left my thoughts on there. I would love to see other people that have made orthogonal or similar points to kind of just maybe we'll just continue the discussion there. It doesn't seem like there's a rush to do any need to do anything though.
00:29:43.116 - 00:30:33.784, Speaker A: Yeah, I'll link the recording of this call once it's up on the thread so people can see this discussion as well. Cool. Any other spec related issues? Okay. If not on the client side. So on the last call, we sort of discussed that we probably had two to four weeks of implementation time, and obviously with Shanghai coming out, it wasn't clear if we wanted to get it that stood up before the Shanghai releases were out or wait until that's done. So we have kind of this good foundation to build off. Yeah.
00:30:33.784 - 00:30:38.810, Speaker A: Does anyone have thoughts on this or just want to share their progress over the past couple of weeks?
00:30:39.980 - 00:31:10.708, Speaker C: I should share a quick progress on the prism side. We are still working on decoupling blocks and blob. I started running localnet last week and then I ran into a few issues with gaff. I believe it's fixed, so I need to retry it. But yeah, I think majority of resources are in Chappella right now. So I think after one week we will have more people looking into this. But yeah, I think we're probably on track for multi client testnet maybe in two weeks from today.
00:31:10.708 - 00:31:15.670, Speaker C: I think that's a good target from. But yeah, that's, that's where we are.
00:31:16.780 - 00:31:24.200, Speaker A: Cool. And would that use basically that would build on top of the Chappelle releases, right, I assume.
00:31:24.700 - 00:31:25.930, Speaker C: Yeah, exactly.
00:31:27.740 - 00:31:33.932, Speaker B: Terrence, did you figure out you said you were seeing an all state route? I guess. Is that still right?
00:31:33.986 - 00:31:37.628, Speaker C: I haven't tested yet. So, yeah, that's all my TD today.
00:31:37.794 - 00:31:39.150, Speaker B: Okay, great, thanks.
00:31:40.320 - 00:32:09.224, Speaker G: So on loadstar side, Lordstar is sort of ready in the sense that without including the changes to the blob signing and blocks and blob signing endpoints, basically where right now we are just independently fetching and signing and publishing the blobs. So without the latest discussed changes in the signing endpoints, we are sort of.
00:32:09.262 - 00:32:09.850, Speaker A: Ready.
00:32:12.460 - 00:33:08.600, Speaker G: To basically sync with any other client that is out there locally. I have been able to run a devnet where there have been two nodes which basically were running sync, they were blob transactions, and then there was a third node that could sync from request response and then would also accept blobs from gossip once it got synced. So we are sort of right now in the review stage of our work. And definitely when the pr for blob signing endpoints is sort of clear with respect to beacon APIs, then we can get that into the LordSa work as well. But even without that, we are sort of ready to do a multi client devnet or testnet.
00:33:12.670 - 00:33:13.420, Speaker A: Nice.
00:33:15.630 - 00:33:34.800, Speaker C: So for Lighthouse, we've made a lot of progress on our decoupling implementation, but we still don't have something that's fully workable. I'd say that's our goal for this week, is to get something that we can actually start testing with locally. Yeah, it's more or less it for us.
00:33:42.740 - 00:33:50.224, Speaker A: Intermind looks like has an implementation that works with cluster in pair with Cluster.
00:33:50.272 - 00:33:54.552, Speaker D: But we of course would be happy.
00:33:54.606 - 00:34:50.390, Speaker A: To participate in some active client network with this update. CKZG work is going well. We've just been focusing on cleanup and stuff for the past week. Reminder to client teams that the audit will start pretty soon. Currently it's planned for April 17, but we're asking that client teams finalize their code like a week or two before then. So if you want a date before April 5 would be great. On Peco side, we are not yet in full speed on 44 four.
00:34:52.200 - 00:34:52.564, Speaker C: But.
00:34:52.602 - 00:37:13.160, Speaker A: We are progressing a bit, hopefully full speed this week after releasing the main net for Shanghai. And yeah, we're working currently mostly in the syncing and blocking port and we are starting working on the signing of blobs in the VC side, but we still need to start working on the beacon node APIs for designing, waiting for things to settle there. Yeah, that's an update for us. Anyone else? Okay, so I guess, does it make sense to keep working on the implementations, maybe start some smaller, some smaller devnets in the next two weeks if we can, and then maybe try to use the next call to coordinate the launch of a longer lived Devnet? I'm not sure what number we're at and I feel like by that point as well, maybe that longer live Devnet also ends up being sort of Shanghai or, sorry, a cancun devnet that we can start potentially adding other stuff to in the future. But yeah, it seems like taking the time to polish implementations and then putting together something that's bit longer lived would work with the different teams. Okay, cool. Anything else anyone wanted to chat about? Okay, well, yeah, we can wrap up here.
00:37:13.160 - 00:37:24.690, Speaker A: Thanks everyone. And yeah, we'll have the same call in two weeks. Bye everyone. Bye. Thanks, bye.
