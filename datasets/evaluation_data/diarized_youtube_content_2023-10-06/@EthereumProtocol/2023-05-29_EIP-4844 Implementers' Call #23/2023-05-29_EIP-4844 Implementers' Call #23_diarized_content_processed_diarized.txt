00:00:00.860 - 00:00:04.130, Speaker A: You okay?
00:00:04.500 - 00:00:31.080, Speaker B: Welcome everyone to our 23rd Ford four. Four implementers. Call. There was a couple spec items added in the comments. I added us to the agenda. I think the main thing we should try and do today is align on the spec for Devnet six. We can also get a quick update on Devnet five as well.
00:00:31.080 - 00:00:32.810, Speaker B: But yeah, hopefully if.
00:00:34.620 - 00:00:36.088, Speaker A: We head out.
00:00:36.094 - 00:00:49.950, Speaker B: Of here with a clear understanding of Devnet six, teams can get working on that. But I guess to start. Mikhail, is Mikhail on?
00:00:50.500 - 00:00:51.504, Speaker A: Yes. Okay.
00:00:51.542 - 00:00:56.480, Speaker B: Yeah. You wanted to bring up the engine API versioning?
00:00:58.820 - 00:01:59.030, Speaker A: Yeah, I have to bring it up because there is a time to spec out cancun spec for engine API and I think this versioning stuff is very important to be resolved to be solved before that or to come to conclusion on to come to decision and consensus on what we will do about method versioning in particular. So I would share my screen for. Yeah, go for it. Yeah. To make it easier to give people the context. Okay, can you see my screen? Yes. Okay, so what the question is about currently we have, yeah, that's for example with three methods that we will use for eight, four.
00:01:59.030 - 00:03:07.492, Speaker A: And we can see here that this method supports all execution payload data structure versions that we have up to date. And why is that here? Basically because when we were discussing this back in time, we made a decision that it probably will be convenient for concept player client once it see the new version of any engine API method, just to stick with it and use it for communicating any previous payload or whatever method call it is. So any previous data structures and so yeah, that should be convenient. Reduce complexity on Cl site, which is actually questionable. I know that at least taco and Lighthouse uses this, so they really use V three if it is available for Bellatrix. Sorry, I'm sorry. For Paris, for Shanghai and for concelos.
00:03:07.492 - 00:04:14.936, Speaker A: So they use V three method. I've heard that Prism doesn't use this feature and they just use V two and V one and V three methods separately for every fork. So that's questionable. And tell me if I'm wrong, but probably it's not that provide that much convenience to Cl site, but it brings more complexity to the spec. And by the spec I mean not just writing these statements here, but also testing. Mario can have some input on that because once we have v three methods introduced, we basically have to test all these things that for Shanghai, for instance, there is a withdrawal field, for canfun there is the excess data gas field, and for Paris the payload corresponds to this. All the errors are responded correctly by execution layer.
00:04:14.936 - 00:05:20.592, Speaker A: Client that's the spec complexity also involves testing and we have like El client complexity. So for instance, get code we have here, sorry, here we have like v two method and we have v three method. Basically this v three method is not stack compliant at this stage because these v three methods will, to comply to this stack, it has to support v two, and it will have to incorporate this logic and recognize that the given payload is from Shanghai, check withdrawals and so forth, instead of just saying that, okay, here is the v three method. It only accepts Cancun payloads, and this payload must contain excess data, gas and withdrawal fields. Otherwise it is invalid. So here we see another complexity. The other example is with besu for get payload method.
00:05:20.592 - 00:06:34.620, Speaker A: They also do some checks. So what to do about that? We have two options here. Basically we have this issue and both options are described here. So the original proposal was as follows. If the data structure version is just the only one change that we actually have for a method, for a new version of a method, but we don't change the behavior. All that changed is we add a new version of a payload, for instance, and this new version is backwards compatible, which is almost always the case, and will almost always be the case backwards compatible with the previous versions. We can just say that, okay, we are not adding a v three while we're just modifying v two and saying that v two now supports the v three payload and so forth.
00:06:34.620 - 00:07:36.480, Speaker A: That's one option here. The other option is to make a step back and say that, okay, there is the v three method that just supports execution payload with three, and that's it. I'm kind of leaning towards the latter because it's just more clear. One method, one responsibility, and less complexity on the outside, and probably not that much problems for CLS to deal with that. But the main obstacle here is that some cls already uses this logic and uses this approach to versioning, relies on it. They will have to change their client, their engine API, clients code. So that's basically it.
00:07:36.480 - 00:07:49.620, Speaker A: This is what I was going to bring up for discussion today and describe the problem. So, any inputs on that, any opinions?
00:07:53.960 - 00:08:37.692, Speaker C: Yeah, just wanted to confirm that prism doesn't use this feature, so it doesn't really affect us. I will probably prefer what Mikhail said. Whatever is the easiest path for the spec to implement. I think we will prefer that. I guess another complication here is that what happened if someone sends a v two payload but with some blob version hash, right, so you have some kind of weird hybrid scenario. Do you consider that as invalid given that it's a valid v two payload, but also someone made a mistake and entered some weird blob version hash. So it does get pretty tricky.
00:08:37.692 - 00:08:43.350, Speaker C: So, yeah, we kind of prefer the simpler approach, which is the later one.
00:08:51.350 - 00:08:52.370, Speaker A: Lucas.
00:08:55.210 - 00:09:50.630, Speaker D: So I was designing this versioning code on nevermind side, and we have it like in steps. So first step is trying to, when we get the execution payload, we are checking if it's correct payload and calculate its version. And then in the second step, we get the version that we got it to, the version of the new payload method. And there we can check some checks if this version is enabled and based on that correctly, and based on that, we calculate. So it's not that complicated code, actually. So I think we do not need to revise that. But it needs to be like, if you separate those two concerns and do not copy paste it, then it's manageable.
00:09:55.140 - 00:10:08.740, Speaker A: Yeah. You mean so you can reuse the validation logic. Right. But you will have to discern. Right. You will have to understand which payload it is. I mean, like based on timestamp based on the fork trigger.
00:10:10.940 - 00:10:25.944, Speaker D: So we decide which payload it is based on the more of. Yeah, you have to correlate all those.
00:10:25.982 - 00:10:26.184, Speaker A: Right.
00:10:26.222 - 00:10:42.210, Speaker D: But we kind of decide based on what values it has. So, for example, if it has withdrawals, it's execution payload v two, and then we check timestamps based on if the shank I was also activated, et cetera, things like that.
00:10:42.580 - 00:10:50.292, Speaker A: So you go like the other direction, right? Yeah, I see. Oh, yeah. Interesting.
00:10:50.426 - 00:11:15.550, Speaker D: We check data and based on that, we decide, okay, this is trying to use payload v two. Okay, so now we can check the timestamps. Right, so this is payload v two. Is payload v two correct. In engine new payload, this version, which we also just put the version number there.
00:11:16.320 - 00:11:16.780, Speaker A: And.
00:11:16.850 - 00:11:30.880, Speaker D: Okay, if payload version is bigger than the engine new payload version, then it's wrong. Right. It has to be smaller or equal. And then we can also check the timestamps depending on the version.
00:11:32.180 - 00:11:51.160, Speaker A: Yeah. Interesting. Okay. And Terrence made a valid point that we have with three. Right. We have blob version hashes here, but blob version hashes doesn't come alongside to v two. Right.
00:11:51.160 - 00:12:30.820, Speaker A: And, yeah, there's probably something, I don't know if it adds more complexity, but we can't in this case, v three is not anymore. It seems like it's not possible to use it only if the second parameter will be null. So this kind of, like, weird thing, some incompatibility and. Yeah, to address Matt's question. I think that we can also delete old versions. You mean like data structures or method versions?
00:12:32.200 - 00:12:55.150, Speaker E: Both, I guess. For us, we don't necessarily implement every single data structure. We kind of do it based on what is there. And so if the version blob hash is there, then we would consider this to be v three. But I think, I guess I'm referring more to the methods. We don't have to support new payload v one after Cancun, right?
00:12:57.200 - 00:13:12.850, Speaker A: Kind of, yes. You don't have to. Definitely don't have to support new pay law to be two after Cancun. If we find a way to pass something here in this parameter. Right. Let's just meet this. Yeah.
00:13:12.850 - 00:13:51.148, Speaker A: This is why there is a proposal of like, do not introduce a new method because you can extend the old one. Right. And avoid removing the old method because it just becomes unusable. So do not add a new one. That's, I think, more reasonable, but yes, changeable. Yes. So that was the idea to deprecate v two, for instance, also, am I correct that?
00:13:51.234 - 00:14:01.330, Speaker D: Sorry, I'm not sure if I understood the proposal. It's just using v three for everything right now. So rename it like two. Can you.
00:14:03.300 - 00:14:56.800, Speaker A: Clear event for all the blocks for archival sync? Let's say the proposal, there are two options. So we can leave it as is, three options. Okay, so we can leave it as is. And in this case, we will have to do something. We'll have to either support v two, keep supporting it because v three is not backward compatible, or find a make it v three backward compatible. So backwards compatibility brokes here as I can see it with this new render. So the other option is just to get back to one method, one data structure version concept, as we had at the very beginning.
00:14:56.800 - 00:15:24.220, Speaker A: So there will be just payload v three in this payload three method execution. Payload v three is the only structure that is supported, which makes the El code cleaner. But we will have to do something with the Cls that already relies on this method, supports all previous versions thing. They will have to change their logic.
00:15:27.280 - 00:15:32.888, Speaker E: Can you not just pass null in for the blob version hashes?
00:15:33.064 - 00:15:39.170, Speaker A: Yes, that's the option. But to be honest, I don't like it.
00:15:40.260 - 00:15:41.010, Speaker F: Yeah.
00:15:46.340 - 00:16:23.324, Speaker A: I don't think it is a clean way of doing it. But anyway, why? To be honest, why do you don't like null? No, we can do it. Why not? I mean, if it's null, it means, okay, so this is one of the ways to make it backwards compatible. But that's the kind of, if we want to support backwards compatibility. I think that this is the only way to make it so.
00:16:23.362 - 00:16:32.370, Speaker D: I somewhat agree that we don't need like double versioning because now we are versioning both execution payload and the method and suffice to version only one of that.
00:16:36.500 - 00:16:50.490, Speaker A: I know that Marius was one of the proponents to get back to one to one relations between met version and data structure version, but not on this call.
00:16:57.430 - 00:17:01.134, Speaker D: Or just have one method version.
00:17:01.182 - 00:17:01.394, Speaker A: Right.
00:17:01.432 - 00:17:04.530, Speaker D: And version by the execution payload.
00:17:09.850 - 00:17:11.400, Speaker A: Sorry, say it again.
00:17:13.210 - 00:17:21.770, Speaker D: So yeah, either have three methods, each one take one execution payload, or have one method and version based on which execution payload you're getting.
00:17:21.840 - 00:17:22.074, Speaker A: Right.
00:17:22.112 - 00:17:23.482, Speaker D: So either one or the other.
00:17:23.536 - 00:18:02.180, Speaker A: Probably, yeah. If we say that we get back to the one to one relations. So we will leave V two as is, I guess we will not change it. But for V three we will just stack it differently and move onwards with other forks. It's probably having these checks here, like a lot of checks here. It's probably like will not happen. So consider.
00:18:02.180 - 00:18:36.430, Speaker A: I doubt that we'll have ten forks that will change the payload structure. Probably we will, but not more than ten. So probably it's not that terrible, but still. So that's kind of it. I think that we don't probably need to come to any solution right now.
00:18:40.230 - 00:18:51.170, Speaker B: Yeah. Should we just use the pr to keep discussing it? And I guess, at what point does this block us from having a solution?
00:18:51.250 - 00:19:46.450, Speaker A: Yeah, so if it is possible, I would really get back to one method version, one data structure version approach. But if it's really a complexity and if it really delays the nerve delivery. So that's probably that the other option, like which trying to make this method backwards compatible will be the one that we go with. I would say that. And to understand that, I think that we need to hear from Cl clients. We need to hear from Cl clients, client developers to understand what's the complexity. Know.
00:19:49.160 - 00:19:56.340, Speaker C: I also don't think all the Cl representative are here today.
00:19:56.410 - 00:19:56.644, Speaker A: Right.
00:19:56.682 - 00:20:02.890, Speaker C: So I think that maybe this Thursday, is this ACDC, maybe. Yeah, we can also bring up there.
00:20:05.180 - 00:20:05.832, Speaker A: Okay.
00:20:05.966 - 00:20:07.112, Speaker B: And let's try to get.
00:20:07.166 - 00:21:02.920, Speaker G: I was in mute. Sorry guys, just trying to comment that. Yeah, I can confirm that ego is actually leveraging this transparent way of dealing with back compatibility. So there would be some changes on our side to go back and have one, one relationship, definitely. But we are about to start working on this new v three with the additional parameter and would say that we probably go for the null version of it for the backup readability. So we're probably going to stick with the if this discussion was not happening, we were just passing null and then start passing version and version transactions when d three actually enables.
00:21:09.970 - 00:21:28.440, Speaker A: Okay. Yeah. Is it like more complex, I don't know, to use this one to one thing instead of what we currently have?
00:21:29.690 - 00:22:02.578, Speaker G: Yeah, at the moment I can't tell right now because now the thing is so embedded in our structure that kind of fully transparent everything. So to evaluate what would be the impact, it takes probably some time, maybe it's easy, but I can't be sure at the moment how it would be cool.
00:22:02.744 - 00:22:11.140, Speaker A: It would be great if you can evaluate it before they call on Thursday and just give some feedback on that.
00:22:18.210 - 00:23:18.180, Speaker H: The very common problem is that some clients send vitri version when should not be available. But some clients respect this rule. I mean v three can be sent by some clients break and con, right? And yeah, maybe we could agree on some rule, at least in these terms. So for example, we all can agree that we do not need to check future fields and v three is acceptable precancoon for example. And it will be a preparation step for the rest of the peer because we have no agreement on this part at least yet.
00:23:22.980 - 00:23:26.480, Speaker A: So what do you mean by the preparation?
00:23:28.020 - 00:23:42.390, Speaker H: I mean that your peer potentially will lead to one version of API with like fields appended to this version and so on, right?
00:23:45.080 - 00:23:53.850, Speaker A: Yeah, there is no APR actually. It's just, you mean the proposed change or how it is spec out now.
00:23:56.220 - 00:24:05.468, Speaker H: Specific implementation, according to your advice, means that we will have just one version of the API, right?
00:24:05.634 - 00:25:06.030, Speaker A: Yeah, if we decide to have one to one relations between method data structure versions, then yes, it will just accept one version and with every modification to execution payload, for instance, we will just introduce a new version of new payload method and the previous versions will have to be supported as well because if somebody wants to run a sync in lock step then we need previous versions as well because to support previous forks, but as it currently specified and as it currently works, you don't need to support previous versions. Yeah, but considering that some CL clients do not rely on that, so we have to support them either way. So yeah, that's not true.
00:25:09.380 - 00:25:35.050, Speaker H: Does it mean that after it becomes a reality, if I send some field with some default value from a feature fork, right, prep this fork before this fork, it will not lead to any error like that.
00:25:37.420 - 00:25:38.650, Speaker A: You mean currently.
00:25:41.260 - 00:25:44.360, Speaker H: After change you are proposing.
00:25:50.560 - 00:26:37.230, Speaker A: If you submit with some new value using the method version, the existing method version, and just add some new value, this, what do you mean? Which is not specified? Yeah, it should be ignored, I guess. I don't know how. It depends on the clients. This particular method will be used for a particular data structure that is used by some fork. Starting from this fork and onwards. Not every fork is changing the data structure, but some extra field will just be ignored by this method, I guess.
00:26:38.880 - 00:26:47.100, Speaker H: But for now such fields are checked for absence. So if they exist, it's a problem, it's an error.
00:26:48.560 - 00:27:05.910, Speaker A: No, I don't think that clients following that are doing these checks. Okay.
00:27:07.320 - 00:27:19.160, Speaker B: Just also to be kind of mindful of time because we're already halfway through the call, does it make sense to table this now and discuss async and bring it up on awkward as Thursday?
00:27:21.420 - 00:27:23.050, Speaker A: Yep, makes sense to.
00:27:31.260 - 00:27:36.370, Speaker B: Ok, so Matt, you had the data gas used?
00:27:39.060 - 00:27:39.712, Speaker A: Yeah.
00:27:39.846 - 00:27:57.910, Speaker B: Okay, so let's do data gas used and then we can go over the list for Devnet six of all the other open question, but that seems like the biggest one to agree to. Yeah, Matt, you want to give some quick background on that one?
00:27:58.360 - 00:28:05.640, Speaker E: Yeah, I think a lot of people are probably aware. We talked about it on, I guess, was it awkward devs last week?
00:28:05.710 - 00:28:06.330, Speaker A: Yeah.
00:28:07.020 - 00:29:06.156, Speaker E: So we shut a little bit about just a TLDR again. Currently whenever you're executing a block and you're computing the cost of the transactions and you need the base fee, you just pull the base fee out of the header and you use that to compute like the total cost of the transaction. For excess data gas it's slightly different. You need to get the excess data gas value from the parent header, pull it out of the parent header and then use it. And then at the end or sometime during executing the block, you have to compute the excess data gas based on the parent header and the number of blobs in the current block and validate that the new header that you have, that you're validating has the correct access data gas. So it's kind of the same computation between both of them. There's this new exponential mechanism for calculating the actual value, but the idea is the same, really.
00:29:06.156 - 00:30:21.684, Speaker E: The main difference is that instead of the value that you're utilizing in the block being in the header itself, it's in the parent header. And so this is different than the base fee. And so it means that whenever we were implementing this, we noticed, Peter mainly noticed, that there's a lot of changes that had to be done to the code just to support like plumbing through the parent header in places where it wasn't really necessary before. And so from that we decided to make a proposal to actually make it so that the actual excess data gas value that's in the header is being used for that header's block for validation rather than digging back to the parent. And to do that, we need to add an additional field to the header data gas used, because the excess data gas is sort of this neat trick that by using the number of blobs in the current block, you get to avoid the amount of data gas used in the header because it kind of exists, that information exists as part of the block body. So to have the same functionality that basically has, which already has gas used in the header, we have to add data gas used. Whenever we talked about it last week, I think I was pretty ambivalent on it.
00:30:21.684 - 00:30:45.390, Speaker E: But after a couple of conversations with people, I'm feeling like more that this is the right thing to do, like bringing alignment between how the base fee is calculated and used and how excess data gas is calculated and used, because it's a small confusing difference for a mechanism that's very similar. So that's kind of a bit of background. I'm curious what other people have thought in the last few days about this.
00:30:49.600 - 00:30:55.976, Speaker F: My intuition is that I also don't have a very strong opinion, but my.
00:30:56.018 - 00:30:59.904, Speaker A: Intuition is that it's not really the.
00:30:59.942 - 00:31:20.484, Speaker F: Same and that it's a different mechanism. And you just call a function to derive the correct value. So it doesn't feel like it has like, I mean, it's efficiently different that I don't think it should cause confusion. I mean, adding an extra header here, there's definitely like, I don't know if it's such a trivial cost in the.
00:31:20.522 - 00:31:21.110, Speaker A: End.
00:31:23.560 - 00:31:28.804, Speaker E: I don't think there's much a difference between adding one thing to the header and adding two things to the header.
00:31:28.932 - 00:31:39.470, Speaker F: Well, because you're only thinking about the complexity of doing the change. But for light clients, it means like one extra field that they have to download as part of the header chain, right?
00:31:40.160 - 00:31:43.660, Speaker E: I mean, it's like an extra eight bytes.
00:31:47.460 - 00:31:48.800, Speaker A: How big is the header now?
00:31:48.870 - 00:31:54.690, Speaker E: Without it, it's like 300 ish, 350 maybe.
00:31:55.060 - 00:31:55.810, Speaker A: Okay.
00:31:56.180 - 00:32:12.920, Speaker E: Oh no, it's probably bigger than that. Yeah, they have to download some more data. But I don't know, it's like an over optimization in my opinion. We're adding additional complexity that's confusing to a lot of implementers. Just to save some hypothetical.
00:32:14.060 - 00:32:16.890, Speaker F: What is the additional complexity that I don't understand?
00:32:17.420 - 00:32:59.780, Speaker E: Well, right now, whenever you are verifying these headers, right now, whenever we're syncing the headers, we don't have the parent header readily available. We're kind of just utilizing whatever the active header that we're trying to retrieve. And the reason is because for the base fee we have in the code a specific place where you verify the base fee. And in that part of code you have the header. And then when you go back to the syncing, you don't have the parents anymore. You're just focused on that one header and downloading these chains of headers. And so the complexity is just like, it's a different, the values are utilized.
00:33:00.600 - 00:33:04.728, Speaker F: But 1 second, you can't verify the correctness of data gas used from the.
00:33:04.734 - 00:33:06.170, Speaker A: Header anyway, can you?
00:33:06.860 - 00:33:13.384, Speaker F: No, you need to block for that anyway. So I don't see what. That doesn't seem like an argument to me.
00:33:13.422 - 00:33:15.420, Speaker E: Well, you need the parent. You need the parent header.
00:33:16.080 - 00:33:22.412, Speaker F: No, but even if you have the parent header and the header, you can't verify that access data gas is correct.
00:33:22.466 - 00:33:33.200, Speaker E: Well, of course you have to still validate the data gas used amount just like you have to validate the gas used amount. It's making the mechanism the same as the base fee.
00:33:34.100 - 00:33:42.932, Speaker F: I think that argument is quite weak because we want to change the base fee anyway, right. Ultimately we want to have it all be the exponential one.
00:33:43.066 - 00:34:09.100, Speaker E: Yeah, but this is orthogonal to using exponential or not. This is about when do you do that computation? Do you do the computation for the block that you're currently validating or do you use the one in the parent header? And right now they're not in line. And so if we update base v to be. Yeah, but I agree, it's still not updated.
00:34:09.920 - 00:34:21.760, Speaker F: How would it not be in line then? Both would use this mechanism. No, if we did update it, then we would update the base fee mechanism to be the same as what we're using now for blobs.
00:34:23.380 - 00:34:40.512, Speaker E: Again, though, the actual way that you compute the base fee value, it's orthogonal to using this exponential value or not. This is a totally separate argument versus where do the input.
00:34:40.656 - 00:35:01.900, Speaker F: It's not because this mechanism that we are applying here you cannot use without the exponential mechanism because then base fee is history dependent. The only reason we can derive everything from excess data gas is because of the exponential mechanism. So it's not orthogonal.
00:35:04.900 - 00:35:10.530, Speaker E: Yeah, I don't know if I follow that. Are you proposing to get rid of gas used?
00:35:12.260 - 00:35:36.040, Speaker F: Yeah, we could basically, we could make the mechanism for the classical eIP one five f nine be exactly the same as the one that we're now using for the blobs. That's why I'm saying it's not such a strong argument if we agree that we want to adapt that one anyway, because it's the more elegant mechanism.
00:35:36.780 - 00:35:37.400, Speaker B: I think.
00:35:37.470 - 00:36:04.370, Speaker E: I agree it's the more elegant mechanism. I just don't see why there's any different, like why there's a huge difference here between computing using the exponential, exponential mechanism on the parent header versus the current header. Because right now the base fee and the current header is the base fee for that block. But are you saying that you think we should move the base fee to mean the base fee for the next block also?
00:36:09.780 - 00:36:11.040, Speaker A: It. Right.
00:36:11.190 - 00:36:19.896, Speaker F: Well, I don't know. Honestly. No. We don't even need, like, what I'm saying is you don't even need the base fee to be part of the.
00:36:19.918 - 00:36:21.210, Speaker A: Block header, for example.
00:36:21.980 - 00:36:40.280, Speaker F: It's not a necessity once we move to. To me, base fee is simply a derived value. It happens that there should be a function that given some inputs, say current and parent block header, outputs the base fee.
00:36:40.360 - 00:36:40.990, Speaker A: Right.
00:36:41.520 - 00:37:08.000, Speaker F: And that function, you can just treat it as a black box. I mean, that's simply like a spec implementation detail what exactly that computation is. And if you have that function, then if you use the mechanism that we use for the blobs now, you don't need to store that base fee anywhere in the blob. Like it's simply unnecessary in the header.
00:37:08.160 - 00:37:11.416, Speaker E: Do not store anything related to the base fee in the header then.
00:37:11.598 - 00:37:12.810, Speaker F: Basically, yes.
00:37:14.140 - 00:37:17.400, Speaker E: So why do we need to have excess data gas in the header?
00:37:17.820 - 00:37:22.200, Speaker F: Well, you do need that because that's an accumulator. That's like the total amount.
00:37:22.270 - 00:37:25.460, Speaker E: But you need that for base fee too, right?
00:37:25.550 - 00:37:26.796, Speaker A: Exactly. Okay.
00:37:26.818 - 00:37:29.820, Speaker E: So you would replace the base fee for gas with this accumulator.
00:37:30.160 - 00:37:32.556, Speaker A: Exactly. Yeah. Okay.
00:37:32.658 - 00:37:53.348, Speaker E: Again. Right. But then the question is, when you're calculating what the cost, the actual cost of the transactions are, the debate here is not should we use this exponential mechanism that puts the accumulator in the header? The debate is should we use the header value itself or should we use the parent header value?
00:37:53.514 - 00:37:57.700, Speaker F: Right, I see a change a bit.
00:37:57.770 - 00:38:16.492, Speaker D: So the problem is that when we are dealing with headers, we go through the parents, right. And it's easy to have a header, parent header to do some validation. But when we are dealing with bodies, we generally are dealing with the same header. And now we will have to pull parent header to do something there.
00:38:16.546 - 00:38:16.764, Speaker A: Right.
00:38:16.802 - 00:38:20.030, Speaker D: That's the problem which currently is not there.
00:38:27.780 - 00:38:51.640, Speaker E: This is why I was saying I think it is orthogonal to changing the mechanism to use exponential, because it's not like we can still replace it with the exponential calculation. Have the accumulator in there. It's more of a question. Is the value in the header, what's used for that header's block? Or does that block rely on the actual value from the parent header? That's the debate.
00:38:54.060 - 00:39:20.664, Speaker A: Yeah. And I support this change because currently we need to, as what Peter said, we need to wire parent header into more places where it was needed previously. And it's confusing and error prone. So I think we should make this change. Yeah.
00:39:20.722 - 00:39:24.530, Speaker B: Does anyone disagree that we should make the change?
00:39:27.780 - 00:39:53.240, Speaker F: I'm not strongly against it, but I feel like, I think the total size of the header, we might undervalue that, how important that is because currently, clearly we don't have very good light client supports. But as we're improving it in the future, the total size of the headers will become more important, in my opinion.
00:39:53.980 - 00:39:58.680, Speaker A: And even like several of relatively easy.
00:39:58.750 - 00:40:04.670, Speaker F: Changes would add more gas. Important.
00:40:07.760 - 00:40:10.464, Speaker B: I'm wondering if there's still some old.
00:40:10.502 - 00:40:12.972, Speaker A: Fields in the header.
00:40:13.036 - 00:40:16.130, Speaker B: Should we reuse like an old field to do this instead?
00:40:16.580 - 00:40:17.424, Speaker D: Let's go.
00:40:17.542 - 00:40:20.480, Speaker E: Let's use the uncle's hash.
00:40:24.280 - 00:40:30.310, Speaker B: But I don't know, does this actually make a difference?
00:40:30.840 - 00:40:44.324, Speaker F: Again, that's a different question. We're now updating the mechanism so that we need eight extra bytes in the header. And technically yes, if we can get rid of old values, that's also nice. But replacing.
00:40:44.372 - 00:40:57.564, Speaker B: No. Okay. Yeah, it doesn't make a difference. Well, I guess because those old values are like zeros now, which I assume is like one byte. So it's like maybe we save one byte by also.
00:40:57.602 - 00:40:59.470, Speaker F: We can simply remove them in the future.
00:41:00.260 - 00:41:05.170, Speaker B: No, there's some weird reasons why it's really hard to do that. Because of hard.
00:41:07.060 - 00:41:11.216, Speaker A: We can do anything. Yeah.
00:41:11.238 - 00:41:11.616, Speaker I: I don't know.
00:41:11.638 - 00:41:28.520, Speaker E: I mean, we are always in this trade off space of trying to trade off space efficiency and then also on the other end trading off consistency. And I think here the space cost doesn't really outweigh the benefit of having a more consistent protocol.
00:41:32.700 - 00:41:53.680, Speaker F: I see the argument as stronger that it might be more difficult to pass. Always like parents and headers themselves. I can see that argument. Consistency with the normal base fee mechanism I find very weak. Because to me it's just a function. It's just two different functions.
00:41:56.740 - 00:42:21.240, Speaker E: Well, of course, the theoretical mechanism, there's no problem with it, but whenever the code is all written around how the base feed is interpreted and then we sort of add this new thing that has a slightly different requirement, then you have a lot of other things that fall out of that around the sync and around verify and stuff. So that's why I'm saying the consistency there makes the change simpler for clients and less error prone.
00:42:32.600 - 00:42:47.130, Speaker A: And a question. If we don't add this field to the block header, we'll have to change our state transition function from depending on the state and the current block and add a parent block header as well. Right.
00:42:51.250 - 00:42:52.734, Speaker E: I didn't catch that.
00:42:52.932 - 00:43:06.100, Speaker A: I mean like you have to pass parent block header to a state transition function to execute a block. Right. You can't do it because you can't compute the cost of data gas for sure.
00:43:06.470 - 00:43:06.834, Speaker B: Yeah.
00:43:06.872 - 00:43:33.850, Speaker E: I mean technically you have to pass the parent block to the state transition function anyways. That's why in the theoretical sense this isn't really that big of a concern. It just is like more practically when you start implementing these things and the way that the clients have already been designed to handle this stuff, they're not designed to do all of their functionality. Also, assuming they need the parent Header, like there's times where you can only reason about the current block with its header.
00:43:35.070 - 00:43:37.820, Speaker A: Oh yeah, you have to pass it because you have some.
00:43:40.110 - 00:43:41.260, Speaker E: Validate the base.
00:43:41.590 - 00:43:42.642, Speaker A: Yeah, of course.
00:43:42.696 - 00:44:00.760, Speaker E: And so that's why originally it was like oh, this isn't that big of a problem because you already have to have the parent header. It just turns out that the way that a lot of clients are implementing this stuff, there are times where it's okay to just not have the parent header. You've already verified some things and so now you can just make assumptions from the header that you have.
00:44:01.210 - 00:44:46.520, Speaker A: Yeah. So currently parent header is not that deep in the block execution abstractions rate. That's what it's about. Yeah. If it extends the time to work and client developers will have to write additional tasks to does this change or whatever it increase the complexity of delivering the node general. So think that makes sense to add this field, the header, that's one of the arguments.
00:44:58.720 - 00:45:01.036, Speaker B: So do people have a strong opinion?
00:45:01.228 - 00:45:02.370, Speaker A: So I guess.
00:45:06.500 - 00:45:16.950, Speaker B: My feeling is we should go ahead with the change even though it makes the header slightly larger, not try to reuse an existing field if it's not going to be a meaningful saving.
00:45:17.320 - 00:45:20.004, Speaker A: And when we're at the point where.
00:45:20.042 - 00:45:26.420, Speaker B: We'Re bottlenecked by clients and header sizes, we can revisit all these decisions.
00:45:26.500 - 00:45:27.130, Speaker A: But.
00:45:28.940 - 00:45:35.450, Speaker B: It seems like today it's worth that cost. Yeah, we can just zk everything at that point.
00:45:38.320 - 00:45:38.780, Speaker A: Yeah.
00:45:38.850 - 00:45:40.990, Speaker B: Does anyone disagree with that?
00:45:47.540 - 00:45:48.290, Speaker A: And.
00:45:50.340 - 00:46:00.150, Speaker B: Matt, did the current pr you share basically do this? It adds a new field with the data gas, right?
00:46:00.600 - 00:46:01.540, Speaker A: That's correct.
00:46:01.690 - 00:46:03.920, Speaker B: Okay, so this is what we would merge.
00:46:04.080 - 00:46:15.610, Speaker E: Yeah, I'm going to rebase it. We got Unscar's pr merge, so there's some conflicts. I'll rebase it though, and then we'll get an approval from an author and then. Yeah, let's merge it.
00:46:16.220 - 00:46:17.290, Speaker A: Sounds good.
00:46:18.940 - 00:46:44.676, Speaker B: Okay, we only have ten minutes left. Barnabas, you had the Devnet six spec. Doc, we just resolved the biggest open pr on it. Do you want to maybe walk us through which ones you think are most important to resolve now on the call, or if there's not, that we should resolve on the call, at least we can flag the ones people should try.
00:46:44.698 - 00:46:48.580, Speaker A: And look at async. Yeah, sure.
00:46:48.730 - 00:46:59.416, Speaker I: So basically we have more open prs maybe G Lementech can talk about, because he's the one that have opened them.
00:46:59.598 - 00:47:03.972, Speaker A: He probably knows better priority.
00:47:04.116 - 00:47:04.970, Speaker I: I do.
00:47:15.290 - 00:47:17.494, Speaker B: Who is that? Sorry, that you said knows it better.
00:47:17.532 - 00:47:19.820, Speaker I: Than you in there.
00:47:26.290 - 00:47:54.600, Speaker A: Hey, guys. So I think it's the same as what we discussed in ACD. Call one is regarding ndns of pre compiled inputs. And the second one is refactoring of the network payload transaction. Payload. So these are the old prs that we discussed over there.
00:47:58.090 - 00:48:20.240, Speaker C: I think we also need another one that, from what we discussed a few minutes ago, that if we're adding the data use field on the El side, that change also will have to reflect on the Cl side as well. So that means the execution payload and the addition payload header will have that field. And the bigger state will also change as well. And the bigger block as well.
00:48:25.870 - 00:48:34.670, Speaker A: Sorry, I just want to add that. Also the engine APIs.
00:48:40.550 - 00:48:48.326, Speaker B: Right, okay, so we need a corresponding pr on the El spec. Sorry, Cl and engine API spec.
00:48:48.348 - 00:48:49.400, Speaker A: That makes sense. Yeah.
00:48:55.230 - 00:48:59.100, Speaker I: I cannot those. If they are ready to the list.
00:49:00.590 - 00:49:02.570, Speaker B: I assume they don't exist, right?
00:49:02.640 - 00:49:07.918, Speaker I: Yeah, but I will let them if one creates them.
00:49:08.084 - 00:49:08.800, Speaker A: Cool.
00:49:10.690 - 00:49:34.550, Speaker I: Also, regarding five to turn off if there's no opposition against it, hopefully in a day or two. I feel like we test everything there and should be moving on to net six very soon. Are there any opposition?
00:49:48.760 - 00:49:49.510, Speaker B: Okay.
00:49:55.650 - 00:50:04.000, Speaker I: Regarding timeline for Devnet six, some feedback from client teams. They think early.
00:50:12.150 - 00:50:15.380, Speaker A: Do we want to talk about timelines for Devnet six?
00:50:17.830 - 00:50:32.570, Speaker C: I mean, with the latest discussion, if we wanted to add data used, then we kind of back to square zero because we kind of need the consensus layer spat changes now and then we also need to redo a lot of the things.
00:50:32.640 - 00:50:32.922, Speaker A: So.
00:50:32.976 - 00:50:49.840, Speaker C: Yeah, I don't think next Monday is viable unless we don't include data use for deafness six and have a deafness seven. That's up to everyone here. But if you want to add data use, then that's another probably one to two weeks.
00:50:50.530 - 00:52:01.462, Speaker B: I would personally rather, at this point we should be looking to get to production, right? And I'd rather we take an extra week to implement the right thing than just say we have one more. And similarly, you know, Mario and others have been working on all these test suites. I think we should be treating this as like, okay, let's start writing the code that we're going to ship on testnets in the next couple of months. And obviously, yeah, if we change the spec because we think there's a better approach, then we should treat this as like a production change and get it done before we deploy Devnet. And I do think there's probably value in having one more call it like spec frozen 4844 exclusive Devnet, like we were saying on awkward devs, even if it takes a bit longer. But then after that one, assuming that there's no more major spec changes, I would then look to start adding all the other stuff in like what would that be? Devnet seven, basically. So like the self destruct stuff and all of that on the El and whatever else on the Cl.
00:52:01.462 - 00:52:47.510, Speaker B: But I think one solid board four four exclusive Devnet makes sense. Cool. Okay, so obviously in like six minutes we won't be able to resolve everything, but Barnabas, we can use your doc as the canonical thing. Keep adding prs to that. We obviously then have the awkward devs in the next two Thursdays where we can bring up. And let's use that to coordinate on Devnet six. And even if it takes more like two, three weeks to launch rather than one or two, if we can get like a relatively final spec, I think that's really good.
00:52:49.320 - 00:53:03.710, Speaker I: That sounds good. So maybe we can specify what we actually want to have in Devnet six by Thursday and then try to aim to launch a couple of weeks from that point of time for everyone.
00:53:06.160 - 00:53:07.100, Speaker B: Mario?
00:53:09.520 - 00:54:02.910, Speaker A: Yeah, I just want to take time to just mention the testing changes that required for next. So we have a tracker for all the changes for the execution spec changes. The main requirement that we need right now from clients is that we need the get implementation for us to be able to fill tests before we can release this iteration of tests for the next Fnet. That's basically our only requirement from you guys to fill the tests. Other changes to the specs are minor in my opinion, for the tests for the actual tests, but yeah, to regenerate we need the get implementation. So yes, that's basically it.
00:54:09.440 - 00:54:10.190, Speaker B: Awesome.
00:54:11.680 - 00:54:59.416, Speaker A: I can also comment on Hive. We have this pr here. There are many remaining changes that I need to introduce, but I wanted to prioritize the execution spectrum because that's consensus test for the NAS devnet and that was one of the main issues in the previous Devnet. So we want to prioritize execution spectrums and we're also working on another Hive simulator that was discussed last meeting here, which is this one. This is basically the beacon API and peer to peer simulator, but I think that one is going to take a little while longer. I don't think it's going to be ready for next Devnet, but the beacon API changes. Sorry.
00:54:59.416 - 00:55:06.620, Speaker A: The engine API changes in the Hive simulator and the execution spec test should be ready for next Devnet.
00:55:07.200 - 00:55:43.336, Speaker B: Nice. Sweet. Anything else? Okay, well, thanks everyone. Yeah, let's wrap up, get the spec for the devnet by hopefully this Thursday. And yeah, we can bring up also the engine API discussion on the consensus layer.
00:55:43.368 - 00:55:44.300, Speaker A: Call Thursday.
00:55:46.080 - 00:55:47.870, Speaker B: Yeah, thanks a lot, everyone.
00:55:48.560 - 00:55:53.480, Speaker A: Thanks. Bye.
