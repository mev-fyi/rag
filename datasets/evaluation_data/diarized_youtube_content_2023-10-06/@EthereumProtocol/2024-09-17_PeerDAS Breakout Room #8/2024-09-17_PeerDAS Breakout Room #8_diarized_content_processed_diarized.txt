00:03:40.865 - 00:03:41.405, Speaker A: Long.
00:04:58.135 - 00:04:59.395, Speaker B: Hello everyone.
00:05:01.575 - 00:05:08.785, Speaker A: Hello, hello, hello.
00:05:13.765 - 00:05:14.585, Speaker C: Hello.
00:05:41.895 - 00:06:51.561, Speaker A: Yeah, I think shall we can make it to the call today. So I'll help with moderating the call. Maybe we get started in one or two minutes when everyone's here. Seems like we've got enough people, maybe most of the usual crew. So maybe we can start and just save a bit of everyone's time. And we can probably start with client updates. I can quickly go with Lighthouse for Lighthouse.
00:06:51.561 - 00:07:48.375, Speaker A: In the last two weeks we have mostly just been working on interrupt testing and debugging issues with other clients. I think from initial testing it looks like most clients, I mean the ones that I've tested with at least seems to be okay with interacting with each other. And the issue is whenever we have issues with resources, like whenever we have six block sidecars, it's more likely that we missed a block and then we fail to recover. That seems to happen a few times for a few clients. I think we probably want to just make it a bit more robust in that. But it's most likely a resource issue. So we're mostly doing local laptop testing and maybe we're not assigning enough cpu.
00:07:48.375 - 00:08:12.381, Speaker A: So I'm not sure if it's a real issue. But it will be ideal if we can recover from those scenarios. And I think on top of that with the decentralized block building, I think that could also solve the issue if a node's not able to compute proof quickly. Super node, essentially it would help with that. Sorry, yeah.
00:08:12.493 - 00:08:26.385, Speaker D: I guess the question is when I was trying it on Kubernetes, I was still running into the same issues and there was no resource constraint test. So there's probably something else in the mix, right?
00:08:27.725 - 00:08:37.904, Speaker A: Yeah, probably. Have you looked at the. The proof computation time? I think it would be interesting to look at the metrics whenever that starts to happen.
00:08:39.244 - 00:08:43.924, Speaker D: Ah, okay. So it's sort of like a spike that's unable to be done within a certain amount of time.
00:08:44.044 - 00:08:50.716, Speaker A: Okay, yeah, probably like try to compute proof for six blobs and if that happened too late then do you have.
00:08:50.740 - 00:08:54.024, Speaker D: A metric I could maybe use or a dashboard or something?
00:08:55.324 - 00:09:10.367, Speaker A: I think Ketosis itself comes with a peer DAS dashboard that's only for Lighthouse. That's built by catia. I've been using that myself. I think we'll probably need to add a few more things there, but I think that that should be good enough for now.
00:09:10.471 - 00:09:12.875, Speaker D: Then I'll rerun it and have a look there.
00:09:13.815 - 00:09:14.263, Speaker E: Cool.
00:09:14.319 - 00:09:42.895, Speaker A: Yeah, thanks. And we've also been working on making the Max blob a Block configurable. Gonna open PR for that. It's under review, that should be ready soon. It's been tested, so that's also a good, good thing. And we've also been working on capturing metrics for distributed block building and we're looking to have a blog post out soon. So maybe we can share some of the metrics that we found there.
00:09:42.895 - 00:09:50.255, Speaker A: That's all from Lighthouse, maybe Prism. Want to go first go next?
00:09:51.035 - 00:10:47.557, Speaker C: Yes, sure. So during the last two weeks yes, we had them issue with big CPU usage in a small devnet. Basically the nodes needed a lot of CPU to look at other nodes in the network and if there is not enough, no need in the networks the CPUs consumption keeps running and running which does not help for sure for block reconstruction if needed. So we fixed this issue. Also we went from mix between subnet sampling and peer samplings to subnet sampling only as defined by the latest specification. So we removed the peer sampling and we increase the number of subnet subscription. Yes.
00:10:47.557 - 00:11:51.515, Speaker C: Also we hadn't some issues when reorg appeared. Sometimes Prism nodes were not able to rejoin the main fork. The issue was it was happening only after a few hours of test. So what I did, I added a new hidden flag in Prism in order to ignore data columns sometimes from time to time coming from the network. So for example, every, I don't know, every 15 slots a given node will ignore all the data currents and this node may create a reorg a few slots later. And so like that we are able to test if everything is going all right in case of a reorg. And with that I have a pull request where tonight I was able to run devnet only with Prism Supernode and full node during a little bit more than 10 hours.
00:11:51.515 - 00:12:34.065, Speaker C: I mean, I stopped after. So yes, it's basically that. And now I want to test again for a very long time. So let's say 10 or 12 hours with Prism and other clients with stress tests. So like with some colons withholding when we propose a block and also when with some columns ignore when Prism node receives some columns and it ignores them on purpose. That's pretty it for the last two weeks maybe Teku.
00:12:35.895 - 00:14:04.215, Speaker F: Yeah, so Taco we have refactored access layer and Dartmouth stink like a prototype now also we added parallelization to two tasks. Proof calculation during block construction during proposal when we do column when we are making com side cars and the boots mostly depends on number of threads but it's like five times on my laptop. And also we added parallelization to KZG verification in availability checker we did some changes to debug logging so it's less noisy now and also improve other login in few things but the main thing is that we identified like four reasons of why techo fails to join canonical chain on any splits due to block delays or availability check fails and so we found four issues and we already fixed one of these issues and free still to to go and we don't want to join any devnet until these issues are resolved and I think we could finish it during this week. That's all.
00:14:09.395 - 00:14:16.335, Speaker A: Nice, thanks. Thank Dmitry see Solius maybe granting and go next.
00:14:17.085 - 00:15:17.205, Speaker G: Yes hello. So the guy from Fellowship program he was working on fixing some issues and it seems that the one issue that we still have is for some reason we got downscored by Lighthouse and eventually get disconnected from Lighthouse. However I think it's slowly getting the state where it seems it makes sense to start testing. So I think I'll give Parry a link to that branch and I think after Pari does some initial testing we can see maybe we will be quite ready to join the network even if there is some issues a bit Lighthouse downscoring so yeah, that's the update so far.
00:15:21.725 - 00:15:52.455, Speaker A: Great, thanks Al. Do we have Nimbus here or no? Star it doesn't look like it. Yeah, maybe we can go to Devnet Updates. I think Perry's been helping us with testing. Perry, do you want to give an update?
00:15:52.795 - 00:16:52.085, Speaker D: Yeah, yeah. Most of the work has been spent on local testing. I think the only real find last week has been the Lodestar bug with the endiness bug. But besides that, the one we're all talking about with potentially proof, computation, et cetera is still an open topic and just surfacing some things from the chat, I think there's two things left to try. One would be yeah, just trying what we did last time, but paying attention to the metrics that you mentioned. Second would be trying a network with zero blobs just to see if it's still breaking at some point and then GUMI allows fine grain blob spamming so we can ensure that the network only has one slot, one blob per slot or two blob per slot and so on. So I guess we can increase that number to see if the behavior at 1 blob per slot versus 6 blob per slot is actually different.
00:16:52.085 - 00:17:11.015, Speaker D: And then I guess that will also give us a bit more information to try and debug what's going on here. And also just another short update Barnabas is back this week so I will be switching back to full time Petra and he will be doing gear das as of this week again.
00:17:19.875 - 00:17:20.795, Speaker A: Cool, thanks.
00:17:20.915 - 00:17:21.171, Speaker D: Thanks.
00:17:21.203 - 00:17:34.255, Speaker A: Flowery. Yeah, I know you've been doing some kubernetes testing. Does the time that it takes to crash produce with that or is it the same?
00:17:35.045 - 00:17:45.305, Speaker D: It seems to be roughly the same. So it tends to happen between epoch 12 to 17. There's no real exact number.
00:17:49.325 - 00:18:13.563, Speaker A: Right, thanks. Yeah, for me, when I was testing, I think it lasted until seven or 800 slots before it started forking. Yeah, I really don't know if it's a resource issue, but I feel like we should be able to recover from those issues if it is a resource issue. But yeah, we should look into it a bit deeper. Yeah.
00:18:13.619 - 00:18:37.855, Speaker D: And one more sort of ask is that it seems like the metrics that Katya has built are really helpful for trying to debug this, but as far as I know it's not present in all clients yet. There should be a PR open to the like there's a metric spec. Maybe client teams can spend some time implementing that as well so that we have a bit more visibility into what's going on.
00:18:41.955 - 00:18:48.375, Speaker A: Nice. Do you have the link to the spec PR for the metric?
00:18:48.755 - 00:18:55.557, Speaker D: Yeah, just looking for it, but I think Katya is also on the call in case she has it a bit more ready than I do.
00:18:55.731 - 00:18:57.645, Speaker H: Yeah, yeah, I can share the link.
00:19:12.585 - 00:19:19.481, Speaker A: Cool. I see Barnabas is back. Have you got something to share or. Too soon?
00:19:19.513 - 00:19:21.005, Speaker I: I got married last week.
00:19:23.985 - 00:19:26.165, Speaker H: Really? Congratulations.
00:19:30.905 - 00:19:39.345, Speaker I: So, yeah, I. I wasn't really up to date with the all the previous changes, but it sounds like everyone was still debugging so I was.
00:19:39.385 - 00:19:39.965, Speaker C: Good.
00:19:46.145 - 00:19:47.645, Speaker A: You didn't miss anything.
00:19:49.595 - 00:19:58.255, Speaker H: I will also add a gossip sub and request metrics into this list maybe today.
00:20:07.635 - 00:20:08.375, Speaker A: Nice.
00:20:10.155 - 00:20:28.345, Speaker I: Do you plan to open a PR for all of these changes? Maybe it's going to get a bit more visibility that way because right now I feel like this is just a fork of the repo. I think if you would have an open PR with the draft or even whatever, then people can refer to that.
00:20:29.005 - 00:20:34.425, Speaker H: Yeah, sure. Then I will end with the other metrics and open pr.
00:20:38.045 - 00:21:08.525, Speaker B: So if I can comment on the metrics. We had a chat with Katia in person yesterday and I mentioned that we should distinguish between metrics that we suggest for clients to have because they are interesting and metrics that we need to standardize with the same meaning. So the ones that you talk about, Barry, would it be important for the metrics to be identical or we just need them to exist in order to debug each client individually.
00:21:12.115 - 00:21:53.309, Speaker D: I think I have a loose preference for trying to make it such that all like there's one metric name and it means one thing across the clients. Because we have the situation today that everyone has different client, different metric names for everything. And that makes it extremely hard to get a global view of how the network is looking without looking at six different dashboards. Or the approach we've taken is building an exporter, at which point we don't need the client to implement the metric anyway. So I will lose preference for trying to standardize it. But I think what you're trying to imply is that there's like for example, the verification time for a column. That's something that we don't need running all the time on mainnet.
00:21:53.309 - 00:22:11.385, Speaker D: Right. So it's just increasing cardinality for no reason. In that case, maybe it makes sense to only do it when there's debug logging enabled or a debug metric flag or something like that, such that we can expand the cardinality set for users that want it and by default we're not expanding it.
00:22:13.405 - 00:22:14.145, Speaker B: Right.
00:22:14.645 - 00:22:29.845, Speaker H: So I'm currently trying to implement metrics with the same names, Lighthouse and Techo. And we will have an example so I can share it later and we will see if it really works.
00:22:29.885 - 00:22:30.465, Speaker A: Good.
00:22:33.235 - 00:23:01.165, Speaker B: Right. So what I was meaning is even if you want to go to the approach of having six dashboards, if clients don't even have any metric at all of the thing you want to measure, then too bad. So that was. That would be kind of the first step, just hey, clients, we need to measure these things. Please can you have a metric? And the second one would be to standardize. So for Katy and maybe Pari, you can comment. There was a, an effort a long time ago to standardize metrics and it didn't work out.
00:23:01.165 - 00:23:42.645, Speaker B: So if you guys want to do that, yeah, you have to be much more push, pushy. I would say. Just keep in mind that it will never be a priority. So if you want this to succeed, you have to both make the spec simple enough that it doesn't take too much time on the clients and it's something that could be feasible in terms of prioritization. And then you need to be like really, really rough in a way. So be on top of the things, have a way to enforce these standardizations somehow across versions and basically be on top of everyone. Or otherwise I would just abandon the effort.
00:23:43.305 - 00:24:07.135, Speaker D: Yeah, totally, totally agreed. Also, just a bit more context on that earlier standardization attempt, I think that was my first three months at the ef and I think I have a lot more experience and ability to crack the whip now a few years later than I did back then. It's definitely something that's on the plate and there were just other battles to fight back then, but I think it's worth fighting right now. So I would, I would push for a standard.
00:24:08.155 - 00:24:44.805, Speaker B: Got it. I think related to that somehow we have the Bitcoin API spec, but I don't think anyone is tracking compatibility besides users just randomly having compatibility issues between clients. So that would be kind of an example that yes, we have a spec, yes everyone agrees on the spec, but there is no enforceability or enforcing outside of maybe us forgetting to look at it every once in a while. So that would be something that would be valuable. If there is a spec. Do we have an automated way to make sure that LTS complies?
00:24:45.945 - 00:25:22.005, Speaker D: Yeah, that's been an open topic as well. So the execution layer has this approach with using Hive for spec compliance, but the issue, so not necessarily spec compliance, but making sure that the API is compliant, it's just no one currently owns it. So I was kind of hesitant to add the Beacon API into a stack that no one is owning anyway. But yeah, if someone is interested in working on that, happy to help support you, but otherwise I guess once work changes slow down a bit, we would definitely take it up. It's. It's a standard we want to help fix, but the stack to test exists.
00:25:27.705 - 00:25:29.405, Speaker A: Solis, you've got your hand up.
00:25:29.985 - 00:25:56.119, Speaker G: Yeah, so. So regarding the metrics. So couple of ideas that may help to, to push this this thing. So. So one way, which is a very hard way, is just to implement the required functionality in the clients and open pr. So I'm pretty sure that the PR will be decent. You would get this accepted fast and that would.
00:25:56.119 - 00:26:54.035, Speaker G: But of course it's hard to do that to add the metrics to all the clients by yourself. And another option would be try to show that the metrics help to helped debug this functionality because we sometimes actually spot some quite interesting bugs or issues when checking just dashboards and comparing. Especially if the dashboards are exactly the same. It's not always. But for peer dos you likely can do the exactly same dashboards for all the clients and by checking the visual differences between the dashboards between the different clients could be a good way to to debug the peer dos. And so far it seems the debugging of the peer dos was Quite a big challenge for the client. So maybe you can push through this angle somehow.
00:26:56.095 - 00:27:08.885, Speaker D: Yeah, definitely. Definitely makes sense. I think this is a good use case for why the standard works and will I guess help people also see that we should standardize other stuff. Thanks.
00:27:10.665 - 00:27:29.285, Speaker A: Yeah, I think it's a good time to do it as well because I think like what Dimitri mentioned that we can't change the metrics once it's been introduced, otherwise it'll be a breaking change for users. So we tend to avoid that in Lighthouse as well. So given peer as it's new, so it's our opportunity to standardize something early on.
00:27:36.555 - 00:27:48.775, Speaker H: So please review my PRs ready for review metrics PRs so we can go further with it.
00:27:54.715 - 00:28:01.645, Speaker A: Cool. Anything related to DevNet. I guess we need more testing before we can decide on when to launch, right?
00:28:03.065 - 00:29:03.925, Speaker D: Yeah, exactly. I'd like to get to the bottom of whatever's going on with this spontaneous combustion of devnets before we launch a public one. And one point I do want to bring up was it was mentioned last week that a few of you at least have local machines as the bottleneck. So if you guys want, I think we're happy to provision like a more beefy machine. But one thing we did want to mention is we don't want this to be like a permanent thing because I do think local testing is valuable and all of your independent client teams should have some sort of infrastructure to help you guys fix it. But at least for this specific issue, if it just gets the ball rolling, we're happy to set up a machine and then give you guys all access and you can run your kurtosis configs there. And I guess the main thing we wanted to know is is that something that's useful or is it not at all and you guys are happy with the status quo.
00:29:12.545 - 00:29:15.245, Speaker A: Anyone want to say something?
00:29:18.865 - 00:29:24.645, Speaker I: I know, Jimmy, you run a MacBook Air. Hopefully now you have a beefier machine.
00:29:26.585 - 00:29:50.625, Speaker A: Yeah, I don't, but I did request one internally. So just for peer DAS testing, I've got a temporary machine as well. Cool. So I guess it would be by request if anyone nears these machine. We could reach out to DevOps, I guess.
00:29:52.485 - 00:30:15.033, Speaker I: Yeah, but we don't want to make this like every running for a long time. So ideally each of the client teams should handle this internally. But if someone needs something quick like by end of today, then we can do something quickly. But yeah, just let us know.
00:30:15.209 - 00:30:21.205, Speaker D: Yeah, it's more to just help us get to Devnet too faster and figure out whatever the Bottleneck is until then.
00:30:27.865 - 00:30:48.655, Speaker A: Awesome, thanks. So that's definite update. Any spec questions anyone? Yeah, I don't think we have any agenda items today. If anyone wants to.
00:30:50.755 - 00:30:58.095, Speaker D: Jimmy, maybe you want to bring up the network debugging stuff we were talking about and potential like debug API.
00:30:59.995 - 00:32:19.947, Speaker A: Oh yeah, those are little rabbit hole that I went to last week. Kind of like a little side project there. Because last week we were debugging a few issues between clients and we're looking at the RPC request sent between clients and I kind of got interested in trying to build a debugging tool to see to visualize the P2P traffic between clients. But I ended up finding out that's like almost impossible because of the way we encrypt the network packets with noise protocol. So it seems like it's quite hard to do with just capturing the network packet and decrypting it. But we had a bit of discussion with Parry and we think this thing could still be useful. But an easier way could be to implement like debug events at the events API, I guess and potentially just spit out like all the RPC requests going out and come in and then we could have some visibility on the network activity.
00:32:19.947 - 00:32:29.815, Speaker A: But yeah, I kind of wanted here if anyone feel that this would be interesting because it's kind of like a little side quest I went for but ended up failing.
00:32:33.915 - 00:32:57.685, Speaker C: If the tool knows in advance the private key of the private key of the peer. Like I know in some clients you can specify a given private key and so if the tool know this private key as well, normally it should be able to decrypt the message coming between peers.
00:32:59.265 - 00:33:44.625, Speaker A: Yeah. The problem is the messages are not encrypted with the node ID private key. It's actually encrypted with a key that it's encrypted with a symmetric key that is derived between two to peers when they establish a connection. Yeah, so it's in memory and there's no way we can get the private key used for computing that symmetric keys. So it's a bit hard with encryption. I feel like the easier way would be to disable encryption during testing or potentially spit out debug events, but only if it's useful.
00:33:45.845 - 00:34:24.551, Speaker D: So there's. I'm hugely in favor of the debug events also just thinking about it a bit longer since the last discussion about it. Main reasoning is that there's scenarios where the sort of having access to this RAW network event stream is also useful. There's a bunch of researchers who work With Talon, the security team who are trying to do some formal modeling of the P2P layer. And currently their approach is to use trace logging and then using a log ingester to understand which peer sent which message to whom. And it feels like there's enough use cases for people to build on. Stop.
00:34:24.551 - 00:35:21.195, Speaker D: This is something that will never be enabled by default anyway. It's probably really deep in the debug space that if client teams can actually support it, it might be helpful long term for peer tasks. Essentially, the moment you get a message, you push it to a debug event stream, slash network or whatever, and then you forget about it. And then we can build tooling around that expectation to handle more complicated stuff like visualization or understanding what's going on, et cetera. And this would also kind of give us access to invalid messages that show up on the P2P layer, which would never show up on the beacon event stream, because that's post validation. I guess my main question on the topic, is this some fundamental reason why this is a bad idea? Or clients are architected in a way that would make it impossible? Or we're talking about like a few months worth of work to get this to work.
00:35:32.545 - 00:36:21.505, Speaker J: I want to echo that. I think traditionally, like, this kind of local debug thing is how debugging is happening instead of going like, you know, the hardcore way of disabling encryption. I think nobody really does the disabling encryption thing because it means that clients need to go to the critical security path and add conditional code that like, you know, disables encryption or uses a null key or something under testing scenarios. And I think that is usually considered very risky behavior because, you know, if you mess it up, you end up with no encryption in real life. So I think this kind of local debugging thing is how usually things are done, least as far as I know.
00:36:34.525 - 00:37:15.215, Speaker A: Yeah. I feel like Prairie's idea of using the debug events could be useful because we could use it on real networks as well, even for like Hosky or mainnet. Yeah, the decrypt the encryption, the not encrypting idea is not great. But yeah, I don't think it's a huge risk because you can't really talk to any other peers without encryption unless the entire network is not encrypted. Yeah. But I do think that the debug event is useful. Yeah, it shouldn't be too hard to build, but I don't know if we want to have this.
00:37:15.215 - 00:37:24.095, Speaker A: I think it's probably worth having a spec first and then we can discuss Perry, are you volunteering?
00:37:26.515 - 00:37:53.505, Speaker D: Ideally I would like to not, but happy to help get the ball rolling at least. Yeah, it can have like some basic PR up and running and I guess we can like shed on that and outcome could still be that it's a bad idea and there's a smarter way to do things, but I guess the cost of trying is low.
00:37:56.725 - 00:38:24.041, Speaker A: Yeah, I agree. Yeah, happy to collaborate on that as well if I can help. But if there's no like objections, I think that's a good start I guess. Be good to hear some feedback though. Do people find this useful? Besides testing and besides DevOps and testing, yeah.
00:38:24.073 - 00:38:36.945, Speaker D: And I think just in general we're doing so many things to get peerdas shipped faster that making testing and debugging faster is also a way to ship pure DAs faster. So I guess my selling point is always going to be there.
00:38:47.085 - 00:39:00.785, Speaker A: Yeah. Banabas, I think the reason why we want to have debug events is that we want to see the actual data being sent or the actual request being sent.
00:39:01.185 - 00:39:37.245, Speaker I: Yeah, I totally get it. Like I totally understand why we want the Debug API as well, but at this point I would really want to have the general metrics done before we go deep dive into the debug API because I feel like this debug API would potentially be a much deeper rabbit hole than we expect. Also, we don't necessarily need the debug API from every client, but the peer desk metrics would probably be a higher value, low hanging fruit potentially.
00:39:42.505 - 00:39:51.205, Speaker A: Yeah, so I misunderstood what you mean. Yeah, but yeah, I do agree it would be ideal to have both, I guess.
00:39:53.265 - 00:40:23.825, Speaker I: Yeah, yeah, yeah, we totally need both eventually. But like the next thing to focus on, I feel like we should push for the metrics and then once we have a good implementation of all the metrics then. Yeah, or if anyone else has some free time then they can work on the debug API as well. But I just feel like we have maybe one or possibly two devs per team and that's why it might be hard to push for both at the same time.
00:40:26.325 - 00:40:40.757, Speaker A: Yeah, I think the debug. Yeah, I think the debug matrix is not necessarily like part of peer does. It could help with other areas as well. It's not really a peer DAs specific thing.
00:40:40.941 - 00:40:43.985, Speaker I: Yeah, yeah, yeah, for sure.
00:40:50.495 - 00:41:16.017, Speaker A: I have a question regarding spec. I think there was a discussion on Discord last week regarding the sampling without PsyMP PR. Are we agreeing to something for DevNet2 or are we just. Just Aiming for the current behavior for.
00:41:16.041 - 00:41:42.575, Speaker C: DevNet 2, Prism implemented the subnet samplings. So 8. So Prism is now subscribed to 8 subnets and custodies for subnets and PRISM does not do any more peer sampling.
00:41:49.675 - 00:41:56.659, Speaker A: I see what I mean now. So do you advertise those subnets as well in your metadata?
00:41:56.827 - 00:42:25.665, Speaker C: No, only. So in the metadata and in node record, it's only four subnets which are advertised. However, we are subscribed to eight subnets and in order to consider a block as available, we need the eight columns basically to be available to us. I mean like the specs states.
00:42:28.605 - 00:42:34.767, Speaker A: So where do you get the four from? Is that based on the number of samples in the configuration?
00:42:34.911 - 00:42:36.195, Speaker C: Can you repeat please?
00:42:37.615 - 00:42:42.527, Speaker A: Where did you get the four number from? So you add four more subnets, right?
00:42:42.591 - 00:43:14.405, Speaker C: Yeah, actually you just. Actually the spec is very clear. It's exactly all the function you have to run are really written for you. You don't have to think at all. Yeah, so basically before you have the four subnets, you should custody coming from a function like custody subnets, something like that. And you call this function with argument four and now you just call this function with argument eight. That's all.
00:43:17.225 - 00:43:24.165, Speaker A: Okay, thanks, that's cool. Yeah, we haven't implemented that in Lighthouse, so we'll try to implement it in the next.
00:43:24.585 - 00:43:29.605, Speaker C: Yeah, it's free and it took me maybe five windows to implement actually.
00:43:30.505 - 00:43:37.565, Speaker A: Well, nice. Can you do online health too? That's cool, that's great.
00:43:39.865 - 00:43:56.245, Speaker C: I have a question about this. I'm not sure currently for the four extra columns, we do save them. We do save them into our database as well. I guess it's not mandatory to save them because we are not supposed to custody them.
00:44:00.455 - 00:45:09.955, Speaker E: Yeah, I guess this period of it would be that you only keep the things that you custody. But I mean, if you're participating in the subnets and like you're like people will not make a distinction between the fact that you're custodying a subnet or not casting a subnet, like as part of just you being in the subnet, like if there were to be peer sampling, then people would only ask you for columns, your regular peers, if they knew that you were customing them. But while you're just in the subnet, you're subscribed to subnets, people might still ask you for historical date, like historical columns. So I think you should still keep them. But yeah, I don't know, I guess in principle you could rotate those subscriptions, but then you'd have to sync up well, yeah, I think it's probably just easiest to keep them. I would assume to just to not have to deal with this problem of people asking you for historical columns that you don't have.
00:45:11.375 - 00:45:39.255, Speaker C: Yeah, so I see. Just for information Prism Never ask, will never ask from Codons up here does not advertise our as custody. I mean we. We do not. We, of course we know which peer is subscribed to which subnet, but we will only ask for columns which are advertised through either not record or through either metadata.
00:45:40.115 - 00:45:44.403, Speaker E: I see. And this is even for syncing like you. You join a subnet. Okay.
00:45:44.539 - 00:45:44.787, Speaker C: Yeah.
00:45:44.811 - 00:46:04.145, Speaker E: Yeah. Okay. But I guess, I don't know is this. Not sure if this is specifying in the spec. So I think if we want this behavior, we should really clarify it in the spec so that there's no issue around different clients doing this differently. And you know, some client drops the data and some clients ask for it or things like that.
00:46:06.165 - 00:46:35.701, Speaker C: I mean, I guess it could be up to each client to decide to keep or not to keep those columns in the database. But I really think that a client should not ask even for inserting should never ask to another peer a colon which is clearly not advertised as a custody column. Basically, we should not A node should not rely on the subscribed subnets of its peers.
00:46:35.893 - 00:46:57.155, Speaker E: Okay. I mean, yeah, that makes sense. I think we can. I mean, I think if we agree that that's the right thing to do, then that thing we should specify. You're right. Like I think the keeping or not keeping sure clients could decide to do it either way. Maybe we can have a recommendation not to keep them.
00:46:57.155 - 00:47:16.335, Speaker E: Possibly. I think that. I mean, if there is no behavior of asking for that data, then maybe there's no reason to keep it. But yeah, so I guess how do other people feel like is this something that we should specify? Do other people feel as strongly as that being the correct behavior?
00:47:30.075 - 00:47:45.655, Speaker A: The Lighthouse also only ask peers that has advertised the column count in the metadata. So if they're not advertised, then we never query the peer for those columns.
00:47:46.435 - 00:48:00.175, Speaker E: I see. Okay, then yeah, maybe let's just add a note somewhere in the spec that makes this explicit that this is how it should work. Unless there is one already. Maybe I'm just missing and it's already there, but I'll take a look.
00:48:03.965 - 00:48:46.865, Speaker A: Great. Thanks, Francesco. Any other spec questions? So I guess we're targeting for this change for Devnet 2. I think that was the previous consensus on discord, I guess. Cool. Then we move forward. I guess there's no other spec question than any other open questions anyone?
00:48:48.445 - 00:49:08.935, Speaker J: I'm curious if people are happy with the direction the proof computation have gone and are going. Like are people? Okay, it was raised at least two weeks ago and I think, you know, I think today is the first meeting for the distributed proof computation. Are people happy with that?
00:49:17.955 - 00:49:19.735, Speaker B: Too early to tell for me.
00:49:24.625 - 00:49:53.775, Speaker J: Okay, I'm saying because it feels like we've moved to the distributed proof computation approach over the let's use the previous slot to pre compute things approach. And I think that's a sensible choice to make and better long term. I'm just wondering if, I don't know, people feel uncomfortable about it or something.
00:50:01.595 - 00:51:21.045, Speaker A: Yeah, I think it's a good idea. I come back to the call later today but I think the pre computation is the way forward. I guess we've got some numbers on the optimizations that we've done in Lighthouse with fetching blobs from el and the proof computation time for 16 blobs actually looks pretty good and by I think with pre computation we could even make it even better. So yeah, I think I'm in general in support of precomputation distributed block building. I think there are potentially two phases. One is the free computation where nodes could participate in group building of non lot on the Yale and there's another one where we could like after the blocks have been proposed. If it wasn't pre computed then the other nodes could also participate in distributed building of the blobs that they hear about.
00:51:21.045 - 00:52:04.095, Speaker A: So I guess there's two part of distributed block building. Lighthouse has implemented the later part where we fetch a block EL after the blocks been proposed and that's pretty straightforward. I think it's probably the less complicated ones and with pretty big gain as well. But precognutation could bring us even further. I think we're hoping to share some metrics in the next week. Whenever we already but we've tested with 16 blobs. It seems like it's.
00:52:04.095 - 00:52:06.255, Speaker A: It's working pretty well.
00:52:11.875 - 00:52:12.187, Speaker D: Here.
00:52:12.211 - 00:52:15.655, Speaker E: You mean distributed block building, right? Not pre computation?
00:52:17.595 - 00:52:45.645, Speaker A: Yeah, that's right. Yeah but even with precomputation that that's also like distributed block building as well. But just before the proposal the proof building part. Any other open questions?
00:52:49.635 - 00:53:07.135, Speaker D: Yeah, I just wanted to get a temp check on how people are feeling about splitting backtrack into two forks. I think it'll be a bigger topic on ACD this week in case people have concerns or whatever that might need some time to think about before acd.
00:53:10.355 - 00:53:27.069, Speaker B: Yeah, I Wanted to bring it up. If we split we are definitely going to ship the next fork. So Fusaka in Q1 20, 26 is that too late for Pierre does I.
00:53:27.077 - 00:53:33.705, Speaker D: Don'T actually think it'll be 26 to be honest. I actually think that we'll ship faster if we split the fork into two then just one.
00:53:35.045 - 00:53:41.065, Speaker B: So how are you going to prevent Usaka from just swallowing a bunch of random EIPs?
00:53:42.745 - 00:54:11.415, Speaker D: Yeah, that's the hard part that I've also been bringing up on act. I genuinely think we just have to get better at saying no to more EIPs and I think a big problem of that is also just people not wanting to be the asshole. But I guess more voices saying no more eips is a good thing. But yeah, definitely a point you should also bring up on acd.
00:54:15.675 - 00:54:36.505, Speaker I: Yeah I think Fusaka scope could probably be just limited to EOs and peer desk without anything else and then that way we could pro possibly push it for like Q3 next year. So like two smaller forks rather than one massive one.
00:54:37.325 - 00:54:45.905, Speaker D: Yeah I think the argument I've also heard being used is that we've kind of already scoped the fork so we're not going to open the scope forking discussion again.
00:55:03.935 - 00:55:05.955, Speaker A: Any other comments on splitting?
00:55:08.495 - 00:55:15.435, Speaker I: Does anyone heavily oppose this idea of leaving P out of picture?
00:55:21.855 - 00:56:03.615, Speaker E: I mean I'm opposed if it means that it ends up being bundled with other stuff that it really doesn't need to be bundled with. I already feel slightly. I'm like not so at ease with the fact that it's going to end up being bundled with EOF and I hope that it doesn't have to be that way. Like if UF ends up being delayed for whatever reason I think we should just say we just do a fork with peer does because I think it's just like strategically much more important but other than that. Yeah, if we can like really you know still keep in mind that it is a very important thing that we should ship as soon as we can after this kind of initial smaller fork then I don't have any problem with that. I think.
00:56:13.555 - 00:57:01.335, Speaker A: Yeah, yeah I grew up Francesca. I'm kind of concerned that we lose the momentum appeared as because I think we've been moving pretty well. I think the my current concern with I mean at the moment I'm kind of agreeing that we we don't duplicate as in the first spectrum fork because just feel like we're not ready yet for Q1 2025 and I guess that brings to the next question That I have in my mind, I really wanted to understand what are the remaining features that we have in order for us to ship peer DAs, because we do have a few more spec changes and I want to understand what's the minimum viable features that we need to implement before peer das is shippable.
00:57:03.275 - 00:57:05.535, Speaker E: Which spec changes are you talking about?
00:57:06.155 - 00:57:26.045, Speaker A: Yeah, like things like data custody and custody groups. Those we haven't worked on them yet and I kind of want to understand are those like required for the first iteration of peer DAs and is there any more things that we should include before we can shoot peer das?
00:57:27.185 - 00:58:09.759, Speaker E: So I think, I mean if, you know, this presented a problem, we can also not do custody groups. I don't think it's a, it's an urgent thing. It's more just like a kind of cleanup thing, I guess. Validator custody. I feel somewhat strongly that we should do it because I really think that it will actually make the rollout much safer. But I mean really the basic validator custody, not any other. I don't plan for there to be further iterations on it for now.
00:58:09.759 - 00:58:52.175, Speaker E: And I think this basic thing of just saying you subscribe to one subnet per 32 ETH, that you have basically one extra or you custody one extra subnet per 32 ETH. So I'm hoping that that shouldn't be too kind of much more to implement on top of the existing custody infrastructure. Other than that, I don't think that there's anything else. I think the only things it's more just this distributed block building and et cetera, et cetera, et cetera kind of optimizations for trying to get local block building as viable as possible with as many blobs as possible. But in terms of the spec, I don't think that it should change.
00:58:59.485 - 00:59:11.625, Speaker D: I guess that's the question then, right? Is sharded mempool, is it on the critical path or is it a nice to have that? Yeah, it doesn't matter if we don't have it before we want to shift us.
00:59:11.965 - 00:59:43.535, Speaker E: Right? Yeah, that's. That is the other question. I, I mean I think it's. Yeah, I think we should probably not consider it to be on the critical path like just because, I don't know, it seems. Seems a bit late for being confident that we can do it something like that. I mean, I don't know. There's ways to do it, I think, which are not necessarily like a huge lift, but yeah, there's like basically hardly any kind of effort that has been started in that direction.
00:59:43.535 - 01:00:29.563, Speaker E: Except some preliminary thinking. So I think we should not consider it to be in a critical path and just accept that it might mean that we cannot get the full benefits of doing charting on the cl. But I think we can still get some benefits and we can, as we kind of have the. Well, as we can test. Peter does better and better. We can understand maybe what he means in terms of blob count. Like what's the impact of the not having a sharded mempool on the blob count that we can actually support? And if, you know, if it turns out that we cannot increase the blob count by a lot without doing some form of Yale sharding, then yeah, then so be it, I think.
01:00:29.563 - 01:00:55.615, Speaker E: Or I mean, maybe eventually we can reassess and decide, okay, let's wait a bit longer until we can do this. But yeah, I think maybe this is also something that we'll have a bit more clarity soon because I think we're going to. I mean we've been talking about it and we want to talk to Felix about it more. And I mean whoever else on the Yale is interested. So maybe this is one thing that. Yeah, we'll know a bit more in a few weeks.
01:01:02.675 - 01:01:46.995, Speaker A: Great, thanks Francesco. You just add to that from my testing with 16, 16 blobs, I think the er, bandwidth doesn't seem too bad. So yeah, if we're just increasing to 16, it's probably not like a blocking issue, but if we increase it to something like 64, then we probably will need it. And also want to understand whether we need all of these optimizations. Like do we need all the features of distributed block building in the fork or is this something that we can do after the fork? Because it doesn't really impact the consensus.
01:01:51.935 - 01:02:34.545, Speaker E: I mean my personal opinion is that we could ship peerdos without this distributed block building infrastructure or like maybe with like a minimal amount of it, maybe just with pre computation and. Yeah, or like anyway, without this infrastructure being able to support like a very high blob count for two reasons. One reason is that I don't think we're going to increase the blob count either way by so much. Like, you know, if we just, I don't know, double the blob count, that's still. Or, and I mean we could even do less than that. But that, that's still. We're not talking about some crazy high low numbers.
01:02:34.545 - 01:03:18.533, Speaker E: So that's one thing. And yeah, the other thing is that it doesn't really, in terms of revenue, it's barely a factor. Like it's you. You can be like it basically doesn't like impact at all profitability for, for someone to not include a lot of blobs. And even from the network perspective it doesn't matter too much if like a small percentage network like home stakers don't include a lot of blobs. Like if they only included up to six or eight or whatever it is, that would still be fine. Like from the network's perspective just because with the 1559 mechanism you still recover the throughput that you lose in like these slots where you have like a lower resourced proposer.
01:03:18.533 - 01:04:08.365, Speaker E: So again my personal opinion is like this shouldn't be also a blocking issue. Like we can go ahead ship like a conservative version of peer DOS where there's not going to be any crazy blob count increases and kind of iterate from there and improve the distributed block building infrastructure, improve the el mempool we need and try to do things in this way like iteratively and not trying to have a perfect thing where we have a super high block count and local block builders can also propose like as many blobs as possible from day one. That just doesn't seem to me like a super, neither realistic nor super useful goal. And again, I don't think that this is going to kill local block building. But again, this is my personal opinion.
01:04:14.865 - 01:04:32.495, Speaker A: Great, thanks Francesca, that's really helpful because we're on time now. If there's any further discussions we can continue on Async, on Discord, but that's all for today. Thanks everyone. I'll see you guys in two weeks.
01:04:35.755 - 01:04:36.535, Speaker C: Thanks.
01:04:36.915 - 01:04:37.443, Speaker J: Thank you.
01:04:37.499 - 01:04:38.695, Speaker E: Thank you, thank you.
01:04:38.995 - 01:04:40.123, Speaker H: Bye bye.
01:04:40.179 - 01:04:40.895, Speaker D: Thank you.
01:04:42.195 - 01:04:42.975, Speaker B: Bye.
