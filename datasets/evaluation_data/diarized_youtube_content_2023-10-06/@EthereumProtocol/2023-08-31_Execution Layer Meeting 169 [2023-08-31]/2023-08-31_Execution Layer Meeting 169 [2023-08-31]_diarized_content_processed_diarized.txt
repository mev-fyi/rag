00:00:01.130 - 00:00:15.550, Speaker A: Okay, we probably have enough people to at least get this started. Yeah, there's someone from guests. Anyone from Nethermind. Vitalik, have you joined Nethermind?
00:00:17.410 - 00:00:18.990, Speaker B: I have not joined.
00:00:22.210 - 00:00:53.390, Speaker A: Saw. Okay, we have at least. Let's get this started. Welcome everyone, to acde one six nine. So some dencoon stuff today. Getting some updates on Devnet eight, and then we can continue the conversation around the Testnet ordering. Then Andrew wanted to bring up the vertical try and state expiry strategy so we can spend some time discussing that.
00:00:53.390 - 00:01:14.450, Speaker A: And those are basically the two big things after that. I think Ethan just had some updates on SSD, wanted to share. And that's pretty much it, I guess, to kick it. Yeah. Barnabas, do you want to give an update on Devnet ape?
00:01:15.590 - 00:02:07.860, Speaker C: Devnet Ape has onboarded all the different clients and everything looks very healthy. We have a few of the pairs that are not really working. Yesterday we had a smaller issue with Nethermind. Hopefully someone from the Nethermind team is here to let us know what has happened. I'm not quite sure, to be honest, but we see quite some drop. But then they released a new version and after I rolled it out, everything went back up again. We have a few pairs, as mentioned, mainly Eradon and Ethereum js that are struggling with a few of the cos we have now a beacon chain explorer also onboarded, and we have a light beacon chain explorer also.
00:02:07.860 - 00:02:10.680, Speaker C: I will share the link in a second.
00:02:12.810 - 00:02:22.890, Speaker A: Sweet. Thank you. Does someone from Nethermind want to give some context on the issue that was fixed?
00:02:23.550 - 00:02:48.660, Speaker D: So I'm not entirely sure because it wasn't involved, but as far as I know, the experimental transaction pool stuff was merged and was causing problems and it was like, missed. That was merged, basically it. So the transaction pool part that we have separate for it, for transactions, was causing problems. It's not yet ready.
00:02:49.350 - 00:02:56.390, Speaker A: Got it. So was the fix just to revert that change rather than having a full working transaction pool?
00:02:56.810 - 00:03:02.440, Speaker D: Yes, I think so. I think margin will handle that soon.
00:03:03.290 - 00:04:07.250, Speaker A: Got it. Thank you. Any of the other clients want to share context on any issues or fixes they've made this week? Okay. And I guess before we go into the test nets themselves, it seems like we probably still want to get more testing done on Devnet eight. Does it feel reasonable to try and get. To try and get all of the pairs working on Devnet eight and then once we have that move to Devnet nine, do the last changes with the 4788 spec and hope all the pairs sort of work on that pretty smoothly.
00:04:12.220 - 00:04:20.440, Speaker C: I personally think that's reasonable and I'm just curious if we had an agreement on how to deploy the contract.
00:04:24.400 - 00:04:56.020, Speaker A: Yeah, we decided we're just going to use a normal transaction. And if I remember correctly, the reason was it was pretty split between that and the custom deployment logic. But we figured a normal transaction was easier to implement and just start with that. And then if for whatever reason there's like issues with using the normal transaction, you can always add the logic around the infork deployment.
00:05:04.930 - 00:05:07.200, Speaker C: Will there be any hype test for this?
00:05:09.570 - 00:05:12.190, Speaker A: Good question. I don't know, Mario.
00:05:19.060 - 00:06:02.764, Speaker E: What do you mean by just so there is going to be like a particular address that needs to be sent from. It's like a synthetic transaction in that it's reverse engineered from a particular. Actually don't know exactly how it works. I'm not going to explain it, but there will be a particular address that needs to be funded to do the deployment. And there'll need to be a gap, a big enough gap between Genesis and the fork on that testnet such that we can do that. I think we normally have such a gap, but it's just going to be something that we manually need to make sure is done in that interim and not to just for this test.
00:06:02.802 - 00:06:03.052, Speaker A: Net.
00:06:03.106 - 00:06:21.842, Speaker E: Don't just deploy it to a particular memory location at Genesis, we want to do the actual transaction. It's mentioned how to do so in the actual spec. 4788. Yeah.
00:06:21.976 - 00:06:22.610, Speaker A: Okay.
00:06:22.760 - 00:06:25.060, Speaker E: That's why it has a particular address.
00:06:26.470 - 00:06:34.554, Speaker A: Got it. And I assume that the client teams can. Oh, sorry, go ahead.
00:06:34.592 - 00:06:35.980, Speaker C: Implement quite quickly.
00:06:36.430 - 00:06:40.254, Speaker E: Just one person has to do this, right?
00:06:40.292 - 00:06:41.630, Speaker A: This is not in the client.
00:06:42.450 - 00:06:54.930, Speaker E: One person, maybe Matt or other know, write a script or just do it themselves if it's not worth to be scripted. It's just a couple manual steps.
00:07:01.900 - 00:07:13.080, Speaker C: But I assume for stuff like Devnets we would need to have this then deployed on the actual epoch of Denkun.
00:07:14.220 - 00:07:25.840, Speaker E: No, it must be deployed before. So that's why I said there needs to be an adequate stretch between Genesis and Dankun because a manual transaction has to make it on chain.
00:07:30.330 - 00:07:31.190, Speaker A: Mario.
00:07:32.490 - 00:07:32.998, Speaker D: Yes.
00:07:33.084 - 00:08:22.040, Speaker A: So I guess we can just add the expected address in the testnet. Genesis, Testnet, Genesis and also tests just to match what the address is currently in the EIP. And yeah, instead of sending the transaction. And that's the reason I asked if we are doing a normal transaction or the transaction because with the synthetic transaction we have a deterministic address everywhere. It's just easier. But yeah, for the test I assume we're going to update the address to the final address we are seeing in the EIP for Devnet nine, if that's okay with Barnabas and Paris. Yeah, we can do that ASAP if you guys want.
00:08:22.040 - 00:09:01.780, Speaker A: Sir, I was just going to say, yeah, as long as it doesn't affect any. So I guess as soon as you change it in Hive, those tests will start breaking for clients. And obviously we shouldn't retroactively change this on Devnet eight, but going to Devnet nine, we should make sure that hive reflects this address. Yeah, I will just send a message, and when we get sufficient, go ahead. From every client, I will just update the hive test.
00:09:05.000 - 00:09:34.060, Speaker E: Myra, can you make sure that there are hive tests where this contract is not deployed in time to that address? That's something that almost certainly won't happen, but it's something that can happen in this technique. So I just want to make sure that we do test the kind of like no op at that address. It should, quote, just work, but we should have tests.
00:09:34.400 - 00:10:32.082, Speaker A: Yeah, absolutely. There was a question by Barnabas in the chat, asking if client teams are ready for Devnet nine. And I guess based on the comments by Nethermind, do we want to figure out the full blob transaction pool on devnet eight and potentially test that on devnet eight before we deploy devnet nine? Or would we want to move to devnet nine sooner rather than later so that we can have the final 4788 spec and then maybe do a bit more non consensus changes on that devnet? Yokim, hi, sorry, I just want to.
00:10:32.136 - 00:11:04.140, Speaker E: Ask about ERP 4788 for definite nine. Do we also want to add the beacon contract as a varm address or not address? A warm address? So like it is sort of a pre compiled, but do we want to add it as warm or not? I'm not sure if this is the right place to ask this question, but do we want this or not? Because I know this question has been raised before on the r and D.
00:11:08.950 - 00:11:12.518, Speaker A: So this is the deposit contract, whether we should always treat it as a.
00:11:12.524 - 00:11:25.220, Speaker E: Warm, sorry, sort of beacon block roots. So the new contract, Matt says, I think we decided.
00:11:26.040 - 00:11:26.548, Speaker A: Yeah.
00:11:26.634 - 00:11:30.170, Speaker E: So okay then. That's fine then. Yeah.
00:11:41.530 - 00:11:54.102, Speaker A: Okay. And I guess so. Back to the devnet nine. Oh, shit. I was muted for everyone on the stream. Sorry about that. Everyone watching the YouTube.
00:11:54.102 - 00:12:27.960, Speaker A: I can try and swap with the zoom recording later today. Back to Devnet nine. So do we want to get it live as soon as possible or do client teams want to make sure that things are working that we have like the blob transaction pool on Devnet eight, that we maybe spam Devnet eight, send some bad blocks on it and all of that. Yeah. How do people feel about moving to the next one versus doing more testing here?
00:12:35.080 - 00:13:14.480, Speaker E: Assuming there's no spec change other than the way this contract is deployed, I'd rather be very confident going into devnet nine rather than saying, okay, well, we hope these things are going to work. And so in that case, I'd rather iron out the things that we know are issues. Otherwise, if we have a high chance of issues in devnet nine, then we certainly will have a devnet ten. And I'm not saying we shouldn't have a Devnet ten, we should have the amount of devnets that make sense. But I think at this point we can attempt to have devnet nine be really solid.
00:13:17.840 - 00:13:32.100, Speaker A: Yeah, I would also lean towards that. Unless people think there's some reason to test the four seven, a lake change much sooner is FLCL.
00:13:34.680 - 00:14:39.800, Speaker E: It looks like the current devnets. Well, clients have several quite critical issues. Like we have different opinion about how to calculate block hashes, which is quite a basic stuff. Right? And we are observing payloads, new payloads with some kind of wrong data appear. So maybe we could at least make it more stable and fix all of that. Because the network had multiple forks, as far as I know now it seems more stable, but this issue still exists and I think it will cause a lot of troubles if we do another network. Maybe we could spend more time on stabilization.
00:14:39.800 - 00:15:13.730, Speaker E: I also just want to echo, I believe, and I think most people on this call believe that the deploy method for 4788 is like almost a no op for clients. And so I don't think we need to rush to test that spec change. It really doesn't manifest as a change sim. It's obviously something we have to test, but stability and reduction of additional overhead and Devnet seems to be the preference.
00:15:17.890 - 00:16:25.490, Speaker A: Okay, so yeah, that seems pretty widely agreed upon. So let's move forward that way. And obviously we'll check in on both the CL call next week and this call in two weeks about Devnet eight testing. And we can use that to inform when we launch Devnet nine once we're kind of feeling confident with the Devnets, whether that's after Devnet mine or potentially if we need more after that. We started discussing the ordering of testnets on last week's call, and I think it would be good to start to align on the order just because we're going to want to announce the launch of Holski, the deprecation of Gordy and all of that. So even though we don't need to talk or decide on dates or anything now. Yeah, I like to just get a feeling from what do people think is the right order for them? My proposal was starting with Holski because it's basically new.
00:16:25.490 - 00:17:10.420, Speaker A: No one will be using it, and I assume there'll be like a pretty controlled set of operator at Genesis. So it's not a huge group to coordinate to upgrade the testnet. Then I would probably do Gordy. If this is the last fork we're going to do on the network, I guess the risks of breaking it are slightly lower in a way. And it's also good because most of the applications, and especially the l two s, run their test on Gordy, so it gives them a tiny bit more time to test. And I would finish with Sepolia just as a final sort of dry run before mainnet. And by then we'd obviously expect to no longer have any sort of issues.
00:17:10.420 - 00:17:20.740, Speaker A: Yeah. Does that generally make sense to people as an ordering? Do people have preferences or different preferences than that?
00:17:36.310 - 00:17:37.662, Speaker E: Makes sense to me.
00:17:37.816 - 00:18:28.950, Speaker A: Okay, yeah, no objections. So if that's good, we'll probably have a blog post announcing that host key went live once it's actually been launched and it's relatively stable. And that same blog post will also mention that this is the last for Gordy. And finally, as well, there's been some folks working on ephemera, which is a testnet that keeps restarting a genesis for stakers to test their setup. So we'll start mentioning that as well. But it's still semi in development, so we're not going to use it as one of the main test nets or anything, but we'll at least let people know that they can test validators on it and have a quicker iteration cycle.
00:18:31.530 - 00:18:33.720, Speaker E: Is the cycle one week or two weeks?
00:18:35.130 - 00:18:44.940, Speaker A: I always forget. I think it's on the website. Let me check. I can't find it super quick. I don't know, is Mario on?
00:18:46.910 - 00:18:49.180, Speaker E: It's not a good job. I can figure.
00:18:57.850 - 00:19:09.180, Speaker A: And then Barnabas, you had a pr, I guess just changing the time on Holski for basically removing Cancun until we figure out the time, is that right?
00:19:10.430 - 00:19:40.130, Speaker C: That's correct. So apparently everyone would have an issue overwriting the cancun time if it's already set once. Okay, so this is something that maybe some clients will have to change, but the still open. I would just want all the know approve it before I merge it in, because it can potentially cause some issues, especially for argon users.
00:19:47.570 - 00:19:52.750, Speaker A: Sweet. Anything else on testnets or dancun?
00:19:55.330 - 00:20:32.700, Speaker C: One more thing that I also think that Holski should be the first one going into Denquin, and we should really stress test the three and six blobs that we have set up. So it's going to be a very good test to see. After that we could adjust if needed then to two to four. Or if it's deemed reasonable then I think we can leave it as it is at 1.5 million validators. If that can handle it, then should be good for the minute too.
00:20:34.270 - 00:20:48.240, Speaker E: The heads up adjusting down would be a hard work. And so if we did deploy 306, I'd rather just leave it as is or reboot the net than having a hidden hard fork for that test.
00:20:54.080 - 00:21:01.052, Speaker C: I'm not saying that we should like hard fork holesky, I'm saying that we could probably adjust it down for Gurley or Sapolia.
00:21:01.196 - 00:21:02.690, Speaker E: Yeah, got it.
00:21:03.300 - 00:21:30.700, Speaker A: Right. But would that mean. I know on the Cl I assume you can probably set all of this in the presets. On the Cl, I don't know if on the EL this would mean we have to maintain two versions of the code paths based on the network. So I don't know how easy it is for the ClS to support different blob counts on different networks.
00:21:31.280 - 00:21:32.430, Speaker E: You mean the.
00:21:33.280 - 00:21:38.110, Speaker A: Sorry. Yes. I assume it's easy on the Cl. I don't know if it's easy on the El.
00:21:43.120 - 00:21:48.952, Speaker E: It seems like a configuration parameter, but sometimes things go deeper than that unexpectedly.
00:21:49.016 - 00:21:49.630, Speaker A: Yeah.
00:21:50.240 - 00:21:55.790, Speaker C: Is there any reason we wouldn't want to fork oscillator also down to two four.
00:21:59.300 - 00:22:19.350, Speaker E: Be exceptional code path at that point because you need conditional logic to change the configuration dynamically. And so I would definitely not advocate for even if it's not the parameterization we go with. I think you either keep pulse at a higher or you get rid of pulski and start over.
00:22:22.550 - 00:22:50.446, Speaker A: I think. Justin, you were going to say something about the El side. I think Danny's got me covered. I was just saying that for Basu at least, it's more of a protocol schedule concern than a configuration concern for us. So it would be work, but it would be non trivial work, but it wouldn't be hard. Okay, but it would be better to keep whole ski at three six no matter what, even if we make all the other testnets and main net two four.
00:22:50.548 - 00:22:51.200, Speaker B: Right.
00:22:52.210 - 00:23:00.514, Speaker A: Rather than having a special fork just on whole ski that goes from three six to two, four. It would be easier. I wouldn't say it's better, but yeah.
00:23:00.712 - 00:23:43.998, Speaker E: Okay, so just a heads up, we do have the schedule to talk about next week again, the main net parameterization, some new information presented about big blocks on main net. And just to reopen the conversation as it's a default in our parameterization. But we've all agreed to talk about it again, so we can obviously talk about it more here. But before even deciding if Holste is going to go 2436, we should decide if we want main net to be such before we go to the. So next week is good too. Sorry, I was wearing my hat. When I wear my hat, you can't hear me.
00:23:43.998 - 00:23:45.950, Speaker E: It is not a tinfoil hat.
00:23:47.170 - 00:24:02.920, Speaker A: Yeah, it's better without the hat. Cool. Okay, so I think I got the gist of that. We're going to talk about it more on the Cl call next week. And there's some interesting data based on seeing larger blocks on main net right now.
00:24:10.210 - 00:24:22.190, Speaker E: Yeah, please join, because it's their call. I think we're easily talk about this for 20 minutes, if not a bit more, with some more data to discuss. Sweet.
00:24:26.200 - 00:25:06.350, Speaker A: Anything else on Denkoon or the forks or the testnets? Okay, next up, Andrew, you wanted to bring up the whole strategy around vertical and state expiry. Do you maybe want to take a minute to chat about what questions you wanted to get into? And then we have, I believe, guillaume, Josh on the call, been working on Vercol, and then Vitalik, I think is also here. So yeah, we can probably hear from.
00:25:08.560 - 00:25:12.960, Speaker F: Sure. And I'm sorry, my connection is unstable, so I might break.
00:25:13.030 - 00:25:13.650, Speaker A: But.
00:25:15.460 - 00:26:45.500, Speaker F: Briefly speaking to my mind, state expiry makes sense because, well, for obvious reasons, that with bigger state, the sync time goes up, but also even disregarding the sync time with each access, the biggest estate generally, roughly speaking, the cost is logarithmic. And also there are just so many dust accounts, things that are irrelevant, suboptimal to keep, never to never clean the state. I'm a big fan of both Velco and state expiry. And Vitalik's proposal makes a nice plan how to achieve both. As far as I can tell, the only problem is that address expansion that was designed to help with the state expiry, which might be technically challenging to have this address expansion, but to my mind that's a separate discussion. If we decide to have state expiry, then we need to maybe rethink whether we achieve that with address expansion or somehow else. But I would rather have both Verco and state expiry.
00:26:50.770 - 00:26:53.550, Speaker A: Got it. Thanks, Guillaume.
00:26:57.210 - 00:27:44.600, Speaker D: Sorry, can you hear? I mean, I agree. The question that I'd like to understand, or at least the point I'd like to understand if you're making or. Yeah, I'd like to understand the point you're making. Sorry, are you saying we should have vertical or stateless and state expiry at the same time? Because the current upgrade path does not preclude getting state expiry later. Right. So my understanding is that you want both at the same time. And I have an issue with this, because state expiry has been effectively abandoned as a research topic for the last two years.
00:27:44.600 - 00:27:47.910, Speaker D: That would push.
00:27:48.810 - 00:27:49.494, Speaker A: Waiting for.
00:27:49.532 - 00:28:40.146, Speaker D: It would push Verco back for at least a year, if not more, and we have some kind of ticking clock in the meantime, which is the state is growing, and that means the more we wait, the more the conversion will take time. So I understand that if we switch to that scheme that Vitalik proposed, and I think has a lot of things going for it, namely that you don't really need to do the conversion live. So it would be actually not such a big problem. Like, it would not be such a ticking bomb. But the way I see it is there's been efforts to get it implemented. There's been several in 2019, if I remember correctly, and also in 2020. They've both been abandoned.
00:28:40.146 - 00:29:37.980, Speaker D: So, yeah, I don't see at the current state of research, which, like I said, is abandoned, it's more like we're writing a blank check, and we hope that there will be state expiry implemented in the future. Yeah. This way, if it turns out this is abandoned a third time, we find ourselves with having to perform the overlay transition, except it's done a year later, so the amount of data that needs to be translated is even higher. So, yeah, basically, the question is not whether, in my opinion, it's not whether we should choose between statelessness and state expiry, but if we should do them at the same time. My favorite answer is no. But if you think differently, I'd like to understand.
00:29:46.560 - 00:30:00.720, Speaker A: Oh, I think Andrew dropped him back while you were talking. Are you back, Edrew Andrew?
00:30:02.980 - 00:30:09.908, Speaker F: Yeah, sorry, I'm back, but I missed the last part. So is there a question?
00:30:10.074 - 00:30:28.196, Speaker A: Yeah, I guess the question is trying to understand, are you arguing for state expiry to happen eventually, or for it to be coupled with the transition to vertical tree and say the same hard fork?
00:30:28.388 - 00:30:59.700, Speaker F: All right. No, I want it to happen eventually, but it affects vertical, because if we decide that it happens eventually, then we can follow Vitalik's proposal and not bother with translating parts of the merco. Patricia, try with every block. That's kind of how it affects the code. It actually simplifies.
00:31:14.600 - 00:31:16.790, Speaker A: Oh, you're, you're really breaking up.
00:31:22.040 - 00:31:41.500, Speaker F: It simplifies our transition to Velco. Yes, sir. So if we do it eventually, it just needs to do this mute translation. And just if state expiry happens eventually, then we can introduce.
00:31:50.010 - 00:31:57.450, Speaker A: Yeah, I think I understood the gist of what you tried to say, but you're breaking up quite a lot. Dancrad, I assume that's what you wanted to respond.
00:31:58.510 - 00:32:59.962, Speaker G: Sure. So I think it's definitely still open that we will do state expiry eventually, but I think we should also. So there is also basically the option that statelessness goes well. Like we achieve all the goals with Verco, and very few nodes in the end opt to have the state, because we find a good way of providing a state through other means. For example like secured RPC node with light clients proofs, as well as, I don't know, like something like portal network. And in that world, it seems like state expiry would become at the very least much less urgent, and would be like a potentially long term future thing. And in that world, carrying around the old Merkel partition tree would make Verkel in many ways significantly worse and more complicated.
00:32:59.962 - 00:33:47.200, Speaker G: And that is why eventually we opted like that this is pretty inelegant, and that it would be best to just do the full vertical. And even in the state expiry world, that's still better, because there's no old MPT that is still carried around. So I think we should stay with this current roadmap, which is simply to have a full transition to virtual that makes no assumptions about future state expiry, which is still possible. And the question to do state expiry eventually, which is still quite a difficult problem, especially because it's coupled with address space extension. Yeah, so we make no prejudgment about that, whether it happens or not.
00:33:49.010 - 00:33:51.950, Speaker A: Got it. Vitalik?
00:33:53.170 - 00:35:36.030, Speaker B: Yeah, I guess I thought I would just give a bit of a background as to why some of we first thought about doing virtual and state expiry at the same time, and then why we ended up switching and some of the various nuances around all of those issues. So at the beginning we thought about vertical trees, and vertical trees are important because they allow us to have witnesses that have bounded size, which makes stateless clients possible. It actually also reduces worst case bounds for Zke evms by quite a bit, because if you have a 400 megabyte witness that would also be at 400 megabytes of hash, you get a ZK sadark. So it's something that we probably have to do anyway. But the challenge with vertical trees is one, the transition process, and then two is that they don't solve the full problem because there's still a few nodes that have to have the entire state, right? So with the transition process, the issue is basically that converting a 50 plus gigabyte Merkel Petricia tree into a 50 plus gigabyte vertical tree in a live network is just friggin complicated. This is something that the research team literally agonized on for more than a full year. I remember last year at DefConnect, it was basically the topic at the research event, and it's basically as much r D effort as the entire rest of the virtual roadmap put together.
00:35:36.030 - 00:37:01.046, Speaker B: Probably just the process of how to do all that transition. Like it's literally comparable to the merge in terms of complexity in some ways, right? And so it ends up breaking a lot of assumptions, like either you have to do some really complicated thing where nodes stick to states, state routes at the same time, or you have to bring in trust assumptions or nodes that are fast syncing during the transition process will have to have custom code to download two trees and so on and so forth. And so with state expiry, like the natural attractiveness of this is basically that, that whole complexity goes away because you basically say, well, the epoch zero, or I guess the era zero tree, since the epoch means something else. Now the era zero tree is a Merkle Patricia tree. Then we start era one, usate goes into era one, and then after one year usate goes into era two. And actually we don't have to keep the MPT around forever because what we could do is once we're in era two, then we can actually go and just do an offline computation and just in place swap the MPT route of era zero with a vertical route. And that actually ends up being significantly easier than doing a live transition.
00:37:01.046 - 00:38:35.738, Speaker B: Right? But basically just the form factor of state expiry does make it significantly easier to do these two at the same time. And it basically removes the transition complexity. And it does of course add all of these other transition complexities involving address based extension. And I think the problem of address based extension itself might even be complicated enough to have by itself been decisive in terms of putting things off in terms of the research agenda, right? Because just to give an idea of this problem. Basically right now we have 20 byte addresses, but then if we're going to have these new eras, then the whole point is that objects that are created during these new eras would be in spaces that cannot collide with the existing 20 byte address space. Because if you see one of those objects in the state, then you know that it's actually new, and you know that you don't need to scan proofs for absence of some other edit record in the past, right? But the problem is that pretty much all existing contracts make huge assumptions all over the place about addresses being exactly 20 bytes long. And getting around that would just require an unprecedentedly insane ecosystem level wide amount of work.
00:38:35.738 - 00:40:03.090, Speaker B: And if we have the capability to do that amount of work, then personally there's like five other things that are higher priority in terms of the good that they could do for the Ethereum ecosystem, right? Getting everyone to switch over to account abstraction, getting all the DApps to support ERC 1271 and ERC 6900, getting everyone to use to be standards compliance to a non metamask wallet would stop breaking. Just like this incredibly long list of stuff that requires that same level of ecosystem level wide coordination. And moving from 20 byte addresses to 32 byte addresses is like literally that level of a task. It's crazier, right? Like even ERC 20 contracts would have to be redeployed, right? There's huge amounts of code that, for optimization purposes, assume that an address is 20 bytes and try to pack something together with an address within one storage slot. Now, there is one other possibility that's kind of less well known that I suggested, which is address space contraction. And there, the idea basically is that we ban one over two to the 32 of the address space from being used right now. And then we start using that in order to create address spaces for new errors.
00:40:03.090 - 00:41:16.702, Speaker B: And if we do that, then that actually completely solves a problem, but that has this one really important, big sacrifice to it, which is that counterfactual addresses that are multi user stop being secure. So there's this fairly small but somewhat difficult to understand a subset of applications that are kind of small right now, but might become significant in the future that stop working. But it's the sort of security weirdness that I think rightfully sort of perks security people's ears up in terms of just bad things that could happen, right? Security practitioners and devs and cryptography people just don't have experience dealing with hashes that are pre image resistant but not collision resistant, which is effectively what we get if we use address based contraction because the hash part of an address would only be about 15 bytes. So that's basically in terms of address based contraction. For links. I wrote about this on ether research. I think if you just search on ether research and maybe azure space contraction or something, you might be in there.
00:41:16.702 - 00:42:14.702, Speaker B: Right. But basically there's these two proposals and I think things kind of got stuck because one of them is just very hard and the other of them is imperfect. In a different way, though, one thing that's worth noting is that we probably can't keep kicking the candy on the road on this forever because as of today, the security that we have on address collisions is two to the 80. And we are over the next a couple of decades going to get into the regime where doing two to the 80 compute power to create an address collision is going to start to become open to more and more people. I mean, technically it's doable already, right? Like in its history the bitcoin network has done something like, I think two to the 93 work or something like that, right? No thanks Tim, though. Yes, that's the one. So it is something that we have to address anyway, but it's like a very thorny problem.
00:42:14.702 - 00:43:51.002, Speaker B: And like extension and contraction both have their issues. And one of the other kind of things with vertical trees is that if we do a vertical tree transition and we bite the bullet and we actually do the complexity of implementing just a pure virtual transition as a transition, then I think the downsides of delaying state expiry by literally another decade go down by quite a bit. Right? And the reason why they go down is because even though it's true that the state size is going to keep blowing up and at some point people are probably going to complain and ask to increase the gas limit. And at some point the gas limit is going to go up to like 40 or 50 million or something and the state size will grow up faster and the state will be a couple of terabytes. But the portion of nodes that would actually have to have the state is going to be much smaller. And even validators like the nodes participating in proof of stake would be able to be stateless with the combination of vertical trees and the proposed builder separation, including both enshrined PBS and the current mev boost based stuff, plus a couple of fairly, I think, small modifications that are totally manageable. You'd have to do a little bit of extra work to make inclusion list work because you'd have to make transactions carry proofs with them through the mempool.
00:43:51.002 - 00:45:47.074, Speaker B: But that's totally doable, right? But the point is that with that infrastructure, the number of nodes during the state goes down by quite a lot. And the amount of people who suffer if the size of the state literally blows up to four terabytes, is going to be much lower. And so if Virgo trees exist and the urgency of solving the state problem just decreases quite a lot compared to the urgency of Manok, all of the various other stuff that the EIP enthusiasts have down the pipeline, plus the urgency on the developer side to kind of just be cautious and continue to improve client quality and make the existing chain work more efficiently and all of those things. So I think just kind of summarizing all of those different considerations put together, it does feel like a bit of a binary decision in that either you do state expiry basically with the vertical transition, or if that doesn't happen, then it's okay to just wait a decade or at least five years and do state transition kind of, or do state expiry fairly far down the pipeline. After that, it's the middle path, like doing state expiry somewhere like one year or two years after vertical trees. That probably is unfortunately a bit of a worst of both worlds, right? So I think that's roughly the consideration, and I think it just depends how much we are concerned by the issues involving a growing state. Another thing, actually, I feel like I might have sort of misspoken a bit, right.
00:45:47.074 - 00:47:24.980, Speaker B: When we talk about things like complexity of state syncing. Doing state expiry also will require changing a lot of that code, right? It's not like doing the state transition, or it's not like doing a virtual plus state expiry transition at the same time is easier than doing just a vertical transition, right? I think maybe it's definitely much less hard than doing the two transitions separately, but it's still more work than doing one of those transitions, right? So we're not going to save time, I think realistically, by going straight to state expiry. So the question is just how much? Or do we value feeling like we have the state problem completely solved versus being okay with this kind of kind of equilibrium where we rely a bit on proposal builder separation and the portal network and some smaller number of state holding nodes and all of those kinds of things. Does doing SC plus VT together mean we don't need ASD and C? No, you still need address space extension or contraction if you do state expiry and vertical trees together. If you want to do state expiry you need to solve the address based problem at that time, regardless of what format you do it in. Right. So if we want to do a state expiry now, then basically we have to just bite the bullet on one of the two address based solutions and commit biting that bullet basically, literally now, so that developers could start preparing for it.
00:47:26.790 - 00:48:06.160, Speaker A: Got it. And so I guess I'm trying to understand. You said the middle ground is the worst. I'm trying to understand why would doing them together actually be. I guess we wouldn't need the live transition by doing them together, which simplifies it. From what I understand of the work that's been done on vertical trees so far, it seems like we're actually doing okay with regards to designing a live transition and improving on that. I don't know how far along we are, but definitely more than zero.
00:48:07.090 - 00:48:15.122, Speaker B: Right. But we're doing okay. But just as a fraction of complexity of the entire Verkaling project, it's surprisingly high. Right?
00:48:15.176 - 00:48:27.720, Speaker A: Right. Yeah. But I guess we've already sort of accepted to bite that bullet and go down that path versus address space extension is basically in the same spot. It was like two years ago.
00:48:28.970 - 00:48:30.280, Speaker B: Right. That's fair.
00:48:30.970 - 00:48:33.080, Speaker A: I just want to make sure that's actually the case.
00:48:33.770 - 00:48:45.500, Speaker B: I guess one of the variables here that I don't know is what percent of the cost is sunk. Are we talking about 80% of the work being done? Are we talking about like 45%? Somewhere in between? Something even lower?
00:48:47.310 - 00:48:58.480, Speaker A: Yeah. Guillaume, do you want to maybe walk us through. I know we covered this a couple of calls ago, but do you want to maybe sort of rehash where we're at with Vercol and specifically the.
00:49:01.650 - 00:49:54.130, Speaker D: Yeah, well, I mean, okay. The problem is always that there's exactly what just happened when some design people agree on the design and then people just start paying attention and come up with a new design. But the way I see it, which is really just. I'm speaking for myself here. We have the implementation of Oracle tree in two clients, two and counting. There's Ethereum js also that has made some progress, and I don't exactly know where Besu is exactly these days, but they also started working on it regarding the transition. At some point, there was an agreement on the overlay method.
00:49:54.130 - 00:50:38.978, Speaker D: This has been implemented, this has been tested. We have numbers. It looks like it's at least a workable solution. We are still discussing premitch distribution, and there's been some back and forth about some details of the transition. Like should we keep the MPT frozen or should we keep writing to it? This is a discussion for another time. Is it completely done? No, this is definitely more than 50% done. I would say 90% done.
00:50:38.978 - 00:50:45.860, Speaker D: Of course, there's the question of, like I said, pre image distribution. So I think at this point.
00:50:48.390 - 00:50:48.798, Speaker A: There'S.
00:50:48.814 - 00:51:29.540, Speaker D: Got to be a good reason to change the transition method. But, yeah, like you said, last 10% is 90% of the work, obviously, and that's quite true. Although, yeah, I think we're closer than that. Okay. Like I said, some people will disagree for sure. I think we have a pretty clear path, and now it's just about implementing the last few details and getting people to try it out.
00:51:33.060 - 00:51:51.030, Speaker A: Got it. Thank you. And I assume, just to be clear as well, I'm not aware of anyone who's worked on address based extension or compression significantly in the past year or two. But does anyone know?
00:51:52.440 - 00:52:15.630, Speaker B: One thing I think I want to stress is that it might be worth revisiting just for kind of future compatibility reasons, in the context of EOF, because there may well be decisions that we can make with EOF while EOF is not yet released, that might make it significantly easier for us to do address spacings in the future.
00:52:17.280 - 00:52:20.960, Speaker A: Got it. Andrew?
00:52:22.340 - 00:52:51.530, Speaker F: Yeah, I just wanted to note that we don't have any to make any decision now, but I think I don't agree with Guillaume's argument that workload trees is mostly done. So how I heard it then we should ship it. But we should ship what strategically makes sense, right? So we need to think about what we do strategically about state expiry. And based on that, that might shape how we ship virtual. And that's all. Right.
00:52:52.780 - 00:53:26.180, Speaker A: That's my point. Right. Yeah, I think that makes sense. And I guess it's maybe just sharing the mental journey that the vertical people have been through and how. Yeah. Fits in those parts of the roadmap. And obviously, if the explicit assumption is like, we just put state expiry on hold for a long time because we rely on epbs, basically on the builders to provide the states and whatnot.
00:53:26.180 - 00:54:20.172, Speaker A: Yeah. Maybe it's just worth making that a bit more explicit. Sorry, Josh, let me just read your question. Right. Yeah. So how much extra value do we get by shipping state expiry at the same time, rather than only getting vertical ASAP? I don't know if anyone has thoughts on that. I guess maybe one last question for Guillaume, Denkrad and folks.
00:54:20.172 - 00:54:36.710, Speaker A: What's the best place for people to just chat about stateless and vertical on a more regular basis? There's the vertical tri migration channel on Discord. Is that the main spot that people should follow?
00:54:37.800 - 00:54:42.600, Speaker B: I think there are state expiry and address space extension channels somewhere in there too, right?
00:54:42.750 - 00:55:04.190, Speaker A: Oh yeah, there's a space expiry channel. That state expiry is pretty dead. Do we still have the address space extension shadow? Oh yeah, we do. Okay, so those three address space extension hasn't been used in over a year. Yeah.
00:55:08.390 - 00:55:37.950, Speaker B: But yeah, I mean I do think that there is value in just like putting a team to think about the address based issue because it has a lot of future optionality value. And as I mentioned, even if we never do state expiry, the whole 80 bit address collision complexity thing is like a bit of a long term taking time bomb that's worth starting to figure out how to diffuse.
00:55:41.490 - 00:56:00.420, Speaker A: Yes, if you're listening and you want to take this on, please reach out. And yeah, we can probably help set up a grant and put you in touch with the folks who looked at this in the past. Andrew, you still have.
00:56:01.050 - 00:56:17.850, Speaker D: Sorry, I just wanted to finish answering your question. There's also a vertical implementers call where we discuss this kind of stuff. The next one is next Tuesday I think so. Yeah. That's usually a topic we talk about the transition.
00:56:21.970 - 00:56:55.120, Speaker A: And Andrew, did you want to add anything? No, no. Okay. Anything else on this? Okay, yeah, I think this was good to bring up and get everyone a bit more context around where things are at. Next up, Ethan, you wanted to share some updates on SSC?
00:56:56.900 - 00:57:55.330, Speaker H: Yes. So SSE has been standing still for a while, but I could finally find some time to update it from last time. When we discussed it, we essentially had two different approaches. One based on SSC unions where each transaction type, so to say, has its own branch in the object, and the normalized one where each transaction gets converted to a unified representation. So now what I did is essentially combine those two approaches into something called a partial container. So how this works is that essentially it behaves like the union when it comes to serialization. It is very close to it, little bit different in the header, but the rest is exactly the same.
00:57:55.330 - 00:59:09.616, Speaker H: But it also ensures that when there are common fields in the transactions, which there are a lot, for example, the amount is present in every transaction type so far, or like the destination, those common fields, they now mercalize always at the same location in the tree with this new approach. So there can be holes now in this partial container. So if one transaction type doesn't use a certain field in the Merkel tree, it simply becomes a zero. And this ensures that we essentially get all the benefits of the union without requiring any clients that verify Merkel proofs to update each time there is a new transaction type, like they can just continue to keep the same verifier forever unless they start caring about the new fields. So yeah, that's the latest approach there. I have also extended the specifications for networking. So with the previous approach, the SSC transaction was only something that gets created as part of the execution payload.
00:59:09.616 - 00:59:55.560, Speaker H: But with this latest specification, you can also directly sign SSC transactions and put them in the mempool and everywhere. Because of this flexible container, it is possible with the SSC transaction to just mix and match your own fields. I have somewhere is a police car. It's not for me, somewhere else. So I have made sure that there is like a little function that checks the invariance so that no new combinations are possible. So for example, if you include a block, then you also have to include the destination and you have to include the priority fee. So no new transaction types.
00:59:55.560 - 01:00:30.630, Speaker H: But everything that's possible right now, with the exception of the replayable legacy transaction, is now also possible with SSC transaction. Exactly in. Yeah, so just want to raise some awareness there to check out the eips. I have put them in the PM issue on GitHub and would appreciate reviews. And also if there are any questions, the typed transactions channel on Discord is the one for questions.
01:00:36.260 - 01:01:22.500, Speaker A: Thanks. Yeah. Anyone have any questions or comments they want to bring up now? Okay, well, yeah, thanks, Ethan, for the update. And then I think that was basically it. Was there anything else anyone wanted to cover? If not, I'll just give a quick shout to eels. So, the Eels team officially put out a blog post announcing the python spec for the EL this week. So the spec's been live for a while, but I think now it's called it production ready.
01:01:22.500 - 01:02:12.768, Speaker A: There's also a PR from Guillaume about the self destruct removal that he opened and that you can contrast with the original EIP. So that shows kind of a nice side by side implementation in both eels and in a regular eIP. So if people want to use eels to write Eips, you can, and you can even link to it in your eip and the bot won't stop you. And similarly. So the yellow paper came up a bunch on Twitter in the past few weeks. People were linking to it, so I figured it was worth a quick update. But just as a heads up, the yellow paper hasn't been updated since Berlin.
01:02:12.768 - 01:03:07.740, Speaker A: So if you look at it today, you basically see the Berlin spec. So if someone wanted to update it and add support for London and Paris and all the difficulty bomb forks in the meantime, there's the option of a grant for that and we've linked this in the yellow paper repo and also are pointing people towards eels there as the up to date spec for Mainnet. I don't know, guru, did you want to add anything more around eels? No, not much. You covered most of it on new vips. Welcome. We would also like some feedback on how easy and smooth the process is and we like to continue to work on solving any issues that people might encounter. So feedback very much appreciated.
01:03:07.740 - 01:03:30.640, Speaker A: Sweet. Anything else before we wrap up? Okay, well, thanks everyone and talk to you all, whatever the next call is. Testing call on Monday. Yeah, thanks.
01:03:31.330 - 01:03:32.538, Speaker D: Thanks, bye.
01:03:32.714 - 01:03:33.550, Speaker B: Thank you.
01:03:33.700 - 01:03:35.434, Speaker C: Bye bye bye.
01:03:35.562 - 01:03:37.180, Speaker E: Thanks, bye. See.
