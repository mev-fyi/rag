00:00:01.610 - 00:00:39.366, Speaker A: Yeah. Hello everyone. Welcome to the second Roko breakout call today. The idea is to talk a little about fee markets and specifically, of course on the L2 side, the concentration, what might change in the future, what should ideally change and how we could do that. And I thought for structure it might be good. I prepared just yesterday evening like a little 1 minute, two minute kind of brainstorming presentation. Just kind of my thoughts, the way I structure, kind of how I think about it.
00:00:39.366 - 00:01:17.720, Speaker A: And maybe I start with that. And then we go into like a quick round and have the different teams that are on the call kind of present, of course, just briefly kind of mention how they think about it, if that sounds good. So I'll briefly share my screen if I am able to do that. Let's see, does anyone see my screen? Awesome. Okay, sure. Yeah. Again, as I was saying, super quick.
00:01:17.720 - 00:02:09.894, Speaker A: So basically the idea is, the way I kind of thought it might be, might be a good idea to structure this, would be to kind of have it separated into three topics. So one of them is just taking the EVM with EVM equivalents, right. The old topic comes with many kind of problems because EVM was originally designed for layer one, the layer one context. And so just the simplest thing in terms of fee market is just to try and kind of do some repricing. Then the question was specifically is in the L2 context, how to best charge for the portion of the fee that go to layer one, settlement cost and debt availability. And then the third one, that one was the last minute addition. So I don't have much thoughts on that.
00:02:09.894 - 00:03:34.666, Speaker A: But the question basically just being is kind of the model of congestion pricing that we use on l one that resulted in the AP 1559, even the right way of approaching this in the L2 context. So basically does 1559 make sense in the L2 context? So on the gas cost side, basically the idea is in general there might just be somewhat different relative kind of cost or kind of maximum availability of different compute resources on L2 than on layer one, say bandwidth versus compute versus storage access, these kind of things. So how to ideally kind of price that all more appropriately. And then of course, especially on the ZK side, there might be some specific operations that are just vastly more expensive. And so ideally, of course those also wanted to be repriced. Challenges of course is that if you make vast changes about pricing, then a lot of existing contracts that people would just want to port over to your network either just don't work at all anymore because they made some assumptions about pricing or at least they are now way inefficient because they're not specifically compiled and optimized for that gas schedule. And of course you also require client modifications.
00:03:34.666 - 00:04:43.462, Speaker A: Is there some sort of way of making this very easy, in a modular way of doing these modifications? And then of course, again, kind of, if you need contracts to be specifically compiled with that in mind, how do you have language and tooling support these kind of things? And just to mention kind of potential solutions or things that might help with this would be of course one, the IP process for standardization. If there was some specific kind of core here that many teams would find useful, then there would be maybe a way to make it more feasible to have that adopted. And then in specific, and I know we have some people from ipsilon on the call as well. So once we get to this in the open discussion section, that'll be great. There's this proposed feature called UF, so kind of version bytecode that one of its goals is to remove kind of gas introspection from contracts. And that way it would at least make contracts more able to run on instantiations of the EVM that have somewhat different pricing under the hood. So that was kind of topic one.
00:04:43.462 - 00:05:51.514, Speaker A: Again, I'm just going to go quickly through them just to kind of have something that we can then refer back to. So the second topic, kind of l one settlement fees in a way, basically kind of thinking about it as like a very basic version of a two dimensional fee market. Specifically because the level of l one fees that you charge users, right, that is completely independent of the current congestion level of the l two. And with the annoying exception of ZK roll ups that actually commit to state this, you can already predict the exact kind of settlement cost, or at least kind of proportional to other transactions of a transaction by looking at it without even executing it, which is nice. I didn't want to insult you by saying makeshift mechanisms, but right now every l two kind of has their own kind of way of how to charge for that. But it's not necessarily ideal. And so ideally you would want to have some sort of 2d solution there.
00:05:51.514 - 00:06:52.094, Speaker A: Although one thing we could talk about, maybe this is just not an urgent thing, maybe this is just fine as is. But basically, ideally, 2d solution standardized across L2s and then the big one that everyone always mentions. Context. Okay, yeah, people always mentioned in that context is kind of how hard it would be to have wallets and whatnot support a new transaction type. One idea here would be to, I mean a, with ERP 44 now we have a transaction type that has two dimensional pricing. So we could copy that and just repurpose that blob pricing for the l one piece instead. And then again, kind of the idea would be like with leveraging IP, the nice thing is, if there was such a transaction type would be completely optional, right? So as long as wallets want to keep using the old ones, that's perfectly fine.
00:06:52.094 - 00:07:45.506, Speaker A: But then maybe over time we could see a slow adoption of that transaction type, if it makes sense to introduce that. And then the last topic, just to briefly mention it, congestion pricing. So basically, just the idea being that on layer one we kind of arrived at 1559 after seven years of iteration, just because it's a pretty good fit for our model of kind of congestion pricing, right, where basically we don't actually charge for transactions to reimburse other users running full nodes. It's really purely to kind of decide who gets into the limited box space we have on the level two sets. It's a bit more complicated. Of course, you don't have quite fixed throughput because there's more kind of variable batch frequency. Of course you still have some sort of congestion sequencer approval, kind of choke points, chain growth, all these kind of things.
00:07:45.506 - 00:09:06.410, Speaker A: So it's also not like congestion is not a factor at all. And possibly you want to kind of charge above clearing rate, clearing price kind of prices. At least that could be an option. And so question is, does it maybe make sense to kind of try to unbundle 59 and adapt it, or only pick some of the aspects for the two contexts? In particular, kind of the way I think about it is kind of 59 is kind of the combination of these three things, kind of the base fee oracle, the way of kind of making the revenue go to the protocol, and this kind of nice second price auction aspect for users. So is there maybe some subset of that we could do in the lateral context in a different way? So those were just kind of my thoughts when I was preparing for this call. Yeah, and then I guess the idea could be now I'll stop sharing for now and maybe we can refer back to these slides later during the open discussion, but to maybe just go through the teams that are on the call and just have your very briefly kind of mention. Is fee markets a topic you have thought about? To what extent have you thought about that in the past? What kind of, maybe beyond what I mentioned just now? Kind of our thoughts you have, and just kind of to get us started.
00:09:06.410 - 00:09:19.038, Speaker A: So yeah, if maybe someone from the teams here could just raise their hand and then we go around. Ed, if you want to start. Sure. Yeah.
00:09:19.124 - 00:10:30.710, Speaker B: Hi everybody, I'm Ed from off chain labs. We're the main developers of arbitrum. For us, the top priority for standardization is to standardize some way of expressing the pass through l one, cost settlement and data availability, to have some transaction format and API calls that allow that to be expressed instead of having to force everything into a one dimensional model. It's not clear, I think, whether the EIP 4844 format is quite a fit for that, because it seems to be assuming that what you're paying for is data per byte. And I think it's a little bit more complicated than that. In pricing. One thing to consider, I think, is if we're going to be adding a dimension to gas pricing, and maybe two dimensions, one for blobs, one for pass through data, and then of course the original gas, I think we should consider going the next step and creating a standard for multi dimensional gas generally.
00:10:30.710 - 00:11:11.150, Speaker B: As far as the issues that Anskar mentioned on the slides, I think several comments. First of all, for l one fees, the sort of pass through fees, I think there are two key issues there. Two key challenges in pricing. One is that the price per unit is determined by someone else, by the l one, or by the foreign da service, and not decided by us. So it's a different kind of problem. It's more a cost accounting problem than it is a price setting problem. And second, we can't in fact predict the exact settlement cost of a transaction, the exact pass through cost that it will impose for two reasons.
00:11:11.150 - 00:12:02.386, Speaker B: One is compression, because transactions, this transaction will be batched and compressed together with data that we haven't seen yet. And second, because of time delay, that the transaction will be posted onto l one for data availability, but that won't happen until a bit later. And so the relevant l one price is not the current l one price, but whatever the l one price is going to be when it gets posted, which we don't exactly know. So I think there are some different issues there. And so I think these sort of pass through costs are fundamentally a different beast than other forms of gas. In terms of congestion pricing. I think we see that the problem on l two is closer to the l one problem perhaps than maybe some others do, and the mechanism that we currently use is pretty close to 1559.
00:12:02.386 - 00:12:48.100, Speaker B: The main difference is there's a minimum base fee that is designed to ensure that costs can be covered. But it's an active research problem, and we're doing active research and maybe innovating in the future in this area, how exactly to set the primary gas fees in a way that better meets the sort of mutual goals of the chain. But we think that the problem of congestion pricing for l two s is pretty closely aligned with the congestion pricing problem for l ones. As far as repricing of instructions, we put a lot of value on being compatible in terms of the number of gas units used by each operation with Ethereum. And so we would expect to continue to move together with Ethereum on that issue.
00:12:49.030 - 00:13:18.564, Speaker A: That's it. Thank you, that was very helpful. Peter, you want to go next? Yeah, sure. Hey, this is Peter from scroll, just to reflect on Asgard's slides. So in our case, in scroll we do have this new dimensional fee for the settlement fees, called the album data fee. And yeah, this is quite similar to optimism. From what I know.
00:13:18.564 - 00:14:06.598, Speaker A: I think arbitrum is a different mechanism, so it might be hard to standardize it. But I agree there's value in standardizing the mechanism here for tooling support. And I also wanted to mention the issue of compression that Ed already mentioned that we cannot exactly predict what is the contribution of today cost of each transaction. But I think even with compression, some heuristics work fairly well here. Apart from this, I think a multidimensional fee market could also be interesting to be used for proving fees so that we can directly charge for the proving costs for each transaction. So we're very excited in exploring that in the future. Great, thanks for that.
00:14:06.598 - 00:14:27.250, Speaker A: Do we have any other teams on the call? Maybe if you're on the call from a team, but you're maybe not the femarket expert or something. Could be still be interesting if you want to briefly say, hey, we've maybe thought about this a lot, or not yet so much. I don't know the details, but something that if anyone hears.
00:14:28.410 - 00:15:30.600, Speaker C: Hello, my name is Carlos, I'm from Polygon ZKVM team. Okay, basically, first I want to explain a little bit. What we do is we introduce kind of a new field in the transaction, which is excitement. It's selected by the sequencer and it is called the effective gas price. And basically what we could do is to just reduce the gas price that we charge to the user in order to charge a fair gas price, which includes the layer one, data availability cost and the L2, the standard Ethereum gas. I would say then I think that one challenge that we have ahead of us is how to price the zero knowledge resources that a transaction consumes. We have the finite km of six GK resources in our case.
00:15:30.600 - 00:16:05.330, Speaker C: And I think for example, these resources are the Kiki machine, binary state machine, arithmetic estimate machine steps and so on. And here one of the difficulties that I see is how to standardize these GK resources between different projects. Since we have different improving systems, it's not clear and not an easy path in order to do that. That's basically my thoughts.
00:16:11.530 - 00:16:26.842, Speaker A: Great, thanks. Yeah, it really seems that there's some specific issues on the ZK side that are especially kind of interesting as well. Do we have any other teams? Anyone else?
00:16:26.976 - 00:17:59.026, Speaker D: Yeah, hi, my name is Elias, I work with the Kakarotz at KVM. Yeah, we've just started to think about the fee markets, as you may know. May not know kakao Zkivm relies on the Starknet OS. So like the underlying Starknet client, and currently starknet uses first in, first out, so no few market, but we are actively looking into it, especially because we've chosen to kind of go the no repricing route, kind of what I believe Ed says for arbitram, which is even though for us under the hood kitchack is not priced the same way as l one, we still use the l one price for lack of better solution for now. So there is a disconnect in this, there's also a disconnect in congestion. So I think we're very interested in multidimensional gas, but yeah, we're just getting started in understanding the ins and outs and the challenges of pricing correctly. So for now everything is included one dimensionally in the gas price, the l one data availability, the shared proverbial costs, and the KoVM steps like emulating the EVM encourage.
00:17:59.026 - 00:18:06.666, Speaker D: So it's a bit rough, but it works. But yeah, that's it.
00:18:06.768 - 00:18:30.100, Speaker A: Thanks. Thank you. Anyone else? Okay, looks like that might have been.
00:18:31.670 - 00:19:48.650, Speaker E: Hi, no, this is Andreas Frein from. I'm co chair of the L2 standardization working group in the EA Oasis community projects, and we've been working with off chain labs and Zksync, Mattis optimism, also linear and a few others on a transaction fee specification, not at the technical sort of like interface level, but at the level of actually how you define all the different terms and what are the requirements around the fee structure. So multiple fee components and we just published a draft and we're calling out sort of like the challenges, especially around the data fee for posting to twelve one. So they're just simple requirements that towards the end user developers, et cetera.
00:19:50.750 - 00:19:51.114, Speaker A: When.
00:19:51.152 - 00:21:02.500, Speaker E: You'Re displaying a fee must be met. So I think that's a first step. But from a technical point of view, I think there's the opportunity to think about particular the data fee in particular, because it varies so greatly across l two s, and in order to provide a sort of like a more stable fee towards end users is to do option pricing. So you're basically pre buying and then follow that up with a hedging strategy in order to smooth out pricing. So thinking of treating gas just like any other commodity you would use in terms of oil or gas. But I can post the link to the.
00:21:06.940 - 00:21:20.940, Speaker A: Yeah, I do remember kind of in the chat. Yeah, seeing that Google couple months ago. That's a really good point. I did not know that you just published something. Also, I'm not sure if you are in the roll call telegram chat, but if you are.
00:21:21.010 - 00:21:22.076, Speaker D: Yes, I am.
00:21:22.258 - 00:22:08.344, Speaker A: Yes, would be great if you could also post the link over there as well for people who are not on the call. And I definitely agree with what you said. Especially. Also I think, for example, things like option kind of basically smoothing out the cost to pay for settlement, I think that's maybe probably out of scope for this call. But it's definitely something that I could imagine emerging maybe as a kind of extra service or layer in between that I think would be very valuable. And I even remember when we were preparing for shipping for it. For that there was briefly a discussion, do we want to have an options market embedded natively in the protocol? We decided against it for complexity reasons.
00:22:08.344 - 00:22:15.710, Speaker A: But anyway. Yeah, Alexandro, I see you also have your hand up. Do you want to go?
00:22:17.360 - 00:22:54.376, Speaker F: So yeah. Alexander liner. So just to briefly sum up how we handle the fees concretely, we just apply a multiple of what it would have costed on l one, and we apply this multiplier. So on average it would be something like one over 15. I'm not sure about the exact number. I don't have the direct knowledge of that. Depending on the variation of the cost on Ethereum, we may change that multiplier.
00:22:54.376 - 00:22:56.030, Speaker F: And so that gives us.
00:22:59.600 - 00:23:00.124, Speaker A: A very.
00:23:00.162 - 00:23:54.850, Speaker F: Rough way to price l one data, but at the same time it has the benefits of being, from a user's point of view, exactly the same thing as if you sent it on Ethereum, which I believe has some value. But we believe this will be even more rough once EIP 4844 comes out, because then we will have to accumulate for the imprecision of as we send data. But since we will send it on blobs and the cost of sending it in a blob will be variable versus other upcode, and this will become a less acceptable solution. So we are actively looking into what could be a valid multidimensional fee solution without compromising too much on the user experience for that.
00:24:01.840 - 00:24:21.210, Speaker A: Thanks. Oh yeah, one more. Bert, you want to connect? No, I see that you unmuted, but I cannot hear you in case you're speaking.
00:24:28.080 - 00:24:29.390, Speaker G: Can you hear me now?
00:24:29.920 - 00:24:31.276, Speaker A: Yes, go ahead.
00:24:31.378 - 00:24:31.644, Speaker H: Okay.
00:24:31.682 - 00:25:09.912, Speaker G: Hi, I'm Bert with block native. We provide gas estimation. So I've been working on improved pricing mechanism for blobs based off of the exponential formula. Long story short, it considers recent deviations a lot more than the current algorithm. And so I think this will be beneficial. Well, it will be beneficial after, like large spikes of usage, then the equilibrium will return a lot faster. Maybe not an issue with blobs right away, but with 1559, like the really large spikes, it takes an hour for the price to come down.
00:25:09.912 - 00:25:35.760, Speaker G: So it's basically a proportional integral controller. Like the current blob equilibrium is an integral controller on the log space of gas prices. So it adds basically a current error and just corrects things faster. So the simulations look pretty good. I'll go ahead and post it. I'm still working on analytical solutions, but I'll just post it on the forum and see what everybody thinks.
00:25:40.420 - 00:26:08.620, Speaker A: Sounds good. Thanks for that as well. It sounds like, I'm not sure if you already have something to share. If so, you could also put it in the chat. But if you're publishing something soon, then yeah. Also, ideally, if you then put the link in the roll call chat, that would be helpful. Okay, do we have anyone else? I guess now we had quite a few people, otherwise we can give it a few seconds.
00:26:08.620 - 00:26:43.268, Speaker A: Good. And then we can move to the kind of the discussion section. Let's see. Of course, might be a bit of a challenge with so many people, but let's try. Yeah, it sounded like from listening to all these summaries, that probably the l one pass through is the one where we will have the most kind of to talk about. Maybe we still kind of stick to the order in the agenda. So that would mean that we first talk about just general repricing.
00:26:43.268 - 00:27:37.240, Speaker A: It sounded like a lot of people here, a lot of teams, were basically saying that they were pretty committed to not deviating from l one pricing at all. I think that seems like a very pragmatic choice on the optimistic side. On the ZK side, of course, that is very costly because now you have to kind of artificially subsidize the catch and these kind of things. Yeah, I don't know how to kind of start the conversation, maybe for ZK teams, to what extent do you think it's going to be sustainable to basically stay on the l one pricing schedule forever and just subsidize these things? Or versus to what extent do you basically have feel some sort of pressure to try and move to world where it's easier to reprice things versus l one evm?
00:27:59.890 - 00:28:53.138, Speaker F: So far we adopted this approach because it allows you to use the gas pricing mechanism of ethereum. The question for now does not matter too much, because in the way linear work currently is, we have a limit in the number of k track blocks that we can process at once in a block. If a transaction does more ktch than that, then the transaction is simply not executed. And so the limit is set in such a way that it will be practical for any purposes. For any purposes. So in practice it does not limit too much the number of use cases. But in the future we would like to improve on that.
00:28:53.138 - 00:29:12.530, Speaker F: But what we have that even with this limit that is in practice never really reached. We are not paying the most because of ketchak. I don't know if what I said was clear.
00:29:15.060 - 00:29:45.050, Speaker A: Yeah, that makes sense. I'm not quite sure who there there was was some other ZK team that said that maybe kind of a proper multi dimensional pricing model would be the better solution than just repricing in the existing dimension. So we can also touch on that once we get to the next topic of multi dimensional stas. Do I say that.
00:29:49.100 - 00:30:58.690, Speaker D: I'm from ZK Singh? And in our case, things like Keshek, they work fine. But if we were to try to have the same opcode pricing as an EVM, then there would be definitely some problematic opcodes. And yeah, multilingual pricing I would say is definitely the best long term way to go. But I was just as a random thought, thinking that maybe, and generally interested in the opinion of others about such approaches. If we charge during transaction the same way as an EVM, like the internal schedule is the same as an EVM, but at the end of the transaction, after the main execution is over, post charging for whatever execution was too expensive. So for instance, if catch act was very expensive compared to actual ad op codes, maybe. And if the transaction used like 1000 of catch ups, then maybe at the end of the transaction we could charge 1000 guests more in addition to whatever was used in the main execution part.
00:30:58.690 - 00:31:20.328, Speaker D: This way, inside execution, the behavior is fully EVM equivalent. That is, all security environments are preserved, but at the end the user will still pay for whatever happened. Yeah, sorry, never mind, never mind. Understand why it won't work.
00:31:20.414 - 00:31:33.420, Speaker A: Okay, yeah, I think we can maybe talk about the multidimensional side of this once we get to the next section. But Ed, did you want to say something?
00:31:33.490 - 00:31:54.420, Speaker B: Sure, yeah. I just thought it would be useful to unbundle a couple of different things here. One of them is, do people charge the same number of gas units per EVM operation as the EVM? And the second is whether the price of gas is different.
00:31:54.570 - 00:31:54.932, Speaker A: Right.
00:31:54.986 - 00:32:16.030, Speaker B: It might be that some team wants to make ketchak require a different number of gas units, but still use one dimensional gas pricing, or alternatively, you could use two dimensional gas pricing. But I think the term pricing sometimes actually is incorporating those two separate notions, gas units per operation versus gas price.
00:32:18.880 - 00:32:36.416, Speaker A: Yeah, thanks. I think that's really important to make that very clear when talking about this. Basically, yeah. In this section, I was specifically thinking about just charging a different amount of gas units for operations than l one. Right. And so basically roll ups that want to do this. I mean, right now I don't think anyone does that.
00:32:36.416 - 00:33:32.020, Speaker A: I think right now everyone literally just copies the evms. Is that's the point of EVM equivalents. The question is, is there a need to change that in the future to enable tools to be able to more easily do that? And maybe in a standardized way, or is alternative, more elegant solutions like multi dimensional pricing the way to go? Or maybe neither of the above. Right. I see that axic is on the call, given that kind of, the epsilon team has been working a lot on UF, and that uf kind of comes with kind of this removal of gas introspection, which I guess also then is important in the multidimensional context. But maybe here. Ready? Axie, could you maybe very quickly kind of talk about that portion of UF and give a quick kind of uf status update?
00:33:35.720 - 00:34:36.568, Speaker H: Sure. So it's not only me on the call from Epsilon, but Andre is as well here. Maybe the quick status update is that the UF specification is quite stable and has been for quite a while. Most of the changes going on are kind of minuscule or concentrate on one or two specific parts, but the general design is kind of stable. There was one larger discussion we had regarding the general design, but we decided against making bigger changes there. And we do have a EOF implementers call which has been bi weekly, but starting this week it's weekly and we moved the time of it to not be conflicting with the roll call. So basically it starts after this call, and anybody is more than welcome to also participate there.
00:34:36.568 - 00:35:58.800, Speaker H: To learn more about the a on a weekly basis. The specific solution we can offer to this problem? Well, yeah, maybe because Ansgar said to give like a short intro. So EOF is a larger set of changes to the EVM, which introduces a number of different features which the EVM currently is lacking. It introduces versioning, introduces structure where code and data is separated. It introduces subroutines and a lot of basically transforms all the jumps to be static instead of dynamic, and also introduces a number of different changes to reduce introspection capabilities. The two introspection capabilities I want to highlight here is one, is code introspection. In the EUF paradigm, it is not possible to inspect the code of any contract in platform, which means that in theory code becomes upgradable because you could transform and replace account code, because the code is not able to introspect itself or other contracts.
00:35:58.800 - 00:37:10.200, Speaker H: In theory this is possible. The more important piece, however, is what Anskar mentioned is the gas introspection. And the way we limit that is we do change all the call instructions to not take a gas argument, and we also remove the gas instruction and anything related to it. The actual gas rules remain the same because we really wanted to make this more like a step by step approach. And so we do still have the 63 of 64 rule, but with not having this inspectable, that could also be removed in the future. So with not having the gas argument in any of the call instructions and not being able to inspect the remaining gas and passing all the gas forward, this means that the only place to limit gas usage is at the transactional level. And we think that this would solve most of repricing problems because you could just change any of the pricing.
00:37:10.200 - 00:37:59.370, Speaker H: And the single point where you have to make an adjustment is at the transaction execution level when the user sets the complete transaction. You know, because I didn't have any slides here, I hope this on a high level made some kind of a sense. But lastly, we're also launching a UF website which would explain this in much more detail, which can be there's still updates needed, but it will be@evmobjectformat.org and we have a combined specification explaining all these different parts of EOF, but we do have individual eips for the individual features. So that's like in a high level intro.
00:38:02.800 - 00:38:43.540, Speaker A: Awesome. Thanks so much, Alex. Just to give a little bit of context. So basically this change has been proposed on Mainet for a while. It's still unfortunately not 100% certain if and when it will reach mainet in this context specifically. Of course it would be relevant if it also or alternatively comes to is shipped on the l two side and we can talk about some more on the multidimensional pricing side if it turns out that that would be really useful. It could even be that this could be a candidate to ship this via the IP process.
00:38:43.540 - 00:40:24.864, Speaker A: Nice thing is that most l one clients have already implemented support for it, or at least are close to it. So it could be something that actually could be easy to maintain even as an l two only feature or something. And of course the reason why it's relevant also say for multidimensional pricing is that it now allows us to change how we count for gas on an ongoing basis. So instead of one gas counter, there could be multiple gas counters and everything, which just isn't really possible in the count EVM because you only have this one gas opcode that expects a one dimensional answer when it asks for the count gas on the repricing side itself. I assume we'll get back to this in a few minutes on the multidimensional side, but on the repricing side itself, is there like something else that people would want to touch on? Then I would say we move on to the multidimensional side. I didn't really mention the kind of general question of multidimensional pricing in my notes, so maybe we can have two sections. First we talk specifically about the l one settlement data charging, and then we talk about the more general multidimensional pricing, also say for proving and these kind of things.
00:40:24.864 - 00:41:05.650, Speaker A: The reason why I didn't kind of call it multidimensional also was, I think Ed mentioned that as well. Is that kind of the l one settlement is special in that it is an externally kind of determined price that you don't set yourself. So in a way it doesn't seem to be quite a proper two dimensional problem. It's more of like a special case of just external costs for transactions. And also, it sounded like my assumptions were a little too simplistic around the fee. They are being predictable from the outside of a transaction because of both compression and then of course the non data cost portion of settlement cost. So point taken on that.
00:41:05.650 - 00:41:34.488, Speaker A: Yeah, I don't know, how do we best get started on the conversation here? It seemed like there is definitely more for demand on this topic for standardization. Yeah, I don't know, maybe we'll just open it up for general discussion. Okay.
00:41:34.574 - 00:42:43.148, Speaker B: I'll go first. I think there is probably stronger agreement on the need for transaction format support and API support for communicating with users and applications about the l one fees or l one costs. I think there's more probably alignment around that than there is alignment about the particular way that these fees would be calculated or passed through. And I think that there's less alignment about the second thing for two reasons. One is different kinds of systems have different balances of costs, right? So ZK systems have higher proving cost, for example, but also because there are differences in design approach among different roll ups about how they handle this. So, as an example for arbitrum, it's very important to us that the chain almost never reorgs, and we also make it a goal that the pass through pricing mechanism be fully trustless. Some others have different goals.
00:42:43.148 - 00:42:58.550, Speaker B: Maybe they worry less about Reorgs because they're trying to prioritize something else, for example. So I think if we can standardize on formats and APIs, I think we can make progress more quickly than if we try to standardize everything.
00:43:04.700 - 00:43:31.330, Speaker A: Yes, it's a very good point. I think getting the standardization boundary right of what to send us and whatnot is very important. So it sounds like basically what you're saying is just how to charge for it. That's the part, basically like just the mechanics of how to expose it to the user. That's what you'd want to standardize. And then how you actually come up with a fee, that's what you basically would leave up to each o two.
00:43:31.780 - 00:43:32.850, Speaker B: Yeah, exactly.
00:43:35.140 - 00:43:58.170, Speaker A: Okay. And I mean, of course, question there is whether people generally feel similarly about this. And then also, of course, I think specifically a good question is, do people agree that having a specific transaction type for this, standardizing on a transaction type would be the main area of focus for standardization here.
00:43:59.280 - 00:44:04.220, Speaker B: I think you would also want to standardize on APIs for things like gas estimation.
00:44:05.680 - 00:44:10.780, Speaker A: That's a very good point, yes. Transaction type and APIs. Carlos?
00:44:12.180 - 00:45:05.376, Speaker C: Yeah, I think that adding a new transaction type, and I'm guessing here it's just a questionnaire, it's about just adding a new field, like L2 gas or something like that. Layer two max gas. Allow it to church, to the user somehow. And later on, each project could have kind of a table with its own gas consumptions in L2. So it's a stand up for all the L2s, but later on, each L2 will search whatever they want. So basically, this could be an approach that we could follow. And I totally agree about doing an eips, modifying estimate gas or whatever in order to add more data or in order to retort more data from the RPC view.
00:45:05.376 - 00:45:07.410, Speaker C: That's for the wallet, basically.
00:45:16.990 - 00:46:02.300, Speaker D: I'm not so sure about the upside of having a new transaction type, because it's nice to display to the user that the breakdown of where the gas cost, the gas price comes from, whether it's pass through, whether it's, I don't know, premium, whether it's proving costs, and so on. But then at the end of the day, they're going to be all summed in the overall gas price, and this is still what is going to be sent to the l two operator. I mean, there is an upside, I guess, but if the main goal is to give more data to the user, why not just modify estimate gas so that they can know the whole breakdown of their operation rather than.
00:46:10.390 - 00:46:55.860, Speaker A: Yeah, I have some thoughts on this, but let's first hear from the people with their hands raised. Peter, I feel like we might not be on the same page on what we are charging the user. Is it Eth directly or is it in gas units? Like in our case for Alan data fee, we directly deduct that from the balance. So in that case, we might be okay without a new transaction format or maybe with a new transaction type that just comes with like a maximum data fee. But for us specifically, the API standardization would be more important. So estimate data fee, that's one API, and the other one is to add the data fee to the transaction receipt. But I feel like there are some rollout projects who do not charge in ETH, but in some gas units, so that might be different.
00:46:55.860 - 00:47:12.330, Speaker A: Yeah, that's right. Andreas? Yes.
00:47:16.140 - 00:48:24.264, Speaker E: If you actually look at the spec that was published. So we're breaking things out into at least three components for now. And there's the l two fee, so that can have different components based on the l two. So there is the opportunity to break things out further, because if you think about further decentralizing, like ZK, EVM, or anyone in sequencer improver, then you have two entities that really have costs, right? The sequencer and. And the prover. So if you have multiple sequencers, potentially also multiple provers, then there's really a fee competition. And the key question is, what are you charging the transaction sender in the end? A, that it's transparent and known ahead of time.
00:48:24.264 - 00:50:01.180, Speaker E: Like when you fill up a car at a gas station, right? You're not driving up to the gas station and going like, what am I going to pay right now, no. You look at it and you see a price and you know exactly how much you're paying for the unit of gas, right? And then it's up to the provider to ensure that that price meets their cost needs, right? So that's where a market is really important. Even a market within an l two, if the l two is decentralized, right? You can have multiple markets. You can have a market within the l two, and then you have a market for the data fee for the pricing for that. I think that is, you don't need a new transaction type. You just need to have an effective mechanism that allows to produce a known price to the end users for a given time period, right? And then display what are the drivers? Or where does that come from? Is it an oracle, is it a fee market? Is it the sequencer sticking a thumb in the air and seeing which way the wind blows?
00:50:06.740 - 00:51:50.960, Speaker B: I think it's fundamental in this space that the cost of providing l two execution versus the cost of l one pass through services. Those two costs fluctuate independently, right? We don't control how much the l one is charging us for our chains or our users for those services. And so I think we do need the ability to adjust something about transaction cost to account for the fact that there is a two dimensional cost structure. And the closest analogy to the way that the model that users are accustomed to on l one is you use gas estimation to tell you roughly what it will cost. But gas estimation also gives you a hint of how much spending you should authorize, right? Because the other important part of it is the user signs the transaction, actually authorizing the fee being taken from them. And so if you want a scheme where users, where you don't take fees from users unless they've authorized it, and where the cost of pass through services can vary from transaction to transaction, then you have what's fundamentally a two dimensional gas market. And I think having two dimensional gas estimation and two dimensional authorization from the user in the alternative is to really change the semantics so that, for example, different transactions that are in the same block l two different gas prices, and that, I think, causes other forms of challenge.
00:51:50.960 - 00:51:57.350, Speaker B: For example, it destroys the notion that there is a current gas price.
00:52:00.810 - 00:52:35.626, Speaker A: Right. And just to very briefly, by the way, give some context here. So right now, basically, I'm aware of three different ways how l two s charge for l one settlement cost. So that is the arbitrary model of, and maybe, correct me if this is wrong, but I think basically just adding it to the general gas consumption with like a dynamic kind of ratio that you set at the time of execution and just charging it against the same gas limit. Is that right? Yes. Okay. And then there's the optimism model and all the op chains.
00:52:35.626 - 00:53:36.722, Speaker A: And I think some other people also use that. For example, Peter, you mentioned, I think that you do that as well on scroll, where basically you just don't expose that to the user at all. You independently just take that out of the balance. And then there is the third model that I know linear uses. And I might be, if I didn't misunderstand that polygon. ZkVM also does that, where basically you don't have an explicit separate pricing, you just basically charge a little extra and set that so that it roughly kind of accounts for what you expect to pay on l one. And so the reason why there might be an interest in actually standardizing this in the form of an explicit extra dimension and a transaction type, and not just a way of making standardizing the estimation, is to make sure that this is both under the user control.
00:53:36.722 - 00:54:13.710, Speaker A: So of course, the trade off right now is, let's say, on the optimism side, the user just has no way of limiting, of expressing any intent about this. So you have no way of limiting the cost on this side. I've done this in the past where I sent a transaction on optimism and I was like, oh, nice. The interface tells me at most four cents and then it charged me $0.20 because of the settlement cost. And that's of course not ideal. So basically having that be both under user control while, and of course in arbitram, it is under user control, but in arbitram it has this other kind of trade off of now messing with this existing mechanism of the gas element.
00:54:13.710 - 00:54:43.260, Speaker A: And in order to have both of these at the same time, you need a separate dimension. So that's kind of just the context for why there might be demand for not just having the estimation mechanism standardized, but actually do a new thing here. Yeah, I would want to move on a little bit. Andrews, if you have a comment on that narrow point, do you want to?
00:54:43.950 - 00:56:09.880, Speaker E: Yeah, I think that the very notion of price variability from transaction, of not knowing a price before you buy, I think that very fact, even though that's what it has been, I think it's just not going to fly. People want to know what it's going to cost them, and that's only fair. And by the way, from a regulatory point of right, even though some people might not really be interested in that for, for users in the US, for example, with action that Congress is currently considering, fees must be known and published ahead of time and you need to commit to that. You can say, oh, it's going to be a fee, but a fee is known. So saying, you know what, sorry, we can't do that is just not going to fly, it's just not going to work. So I think we need to fundamentally rethink the whole notion of pricing and really treat it as a commodity. And then there are known mechanisms for how to deal with that, that we don't have to reinvent the wheel for that.
00:56:09.880 - 00:56:28.460, Speaker E: You can actually go to the Chicago Mercantile Exchange and know it's like ask them, know, put out options and just buy it. Right? It's a cost. Right.
00:56:29.230 - 00:57:28.878, Speaker A: I would say just because I do agree with what you're saying, basically, I think it's because we have so many people on the call, I think it's important that we kind of try to separate the different layers of the conversation. And I think right now we are mostly talking more like the mechanics of how to do that. But then I agree, once we kind of, when we talk more on the say, what then ends up kind of what is the user experience? Right. And then I think what you're saying very much, I would subscribe to that. So basically, kind of the predictability and everything on the kind of the more pricing mechanics, I would be curious if we kind of continue on that path of exploring a transaction type and figuring out how to best make that work. Do people generally think that. I understand not everyone thinks that there's necessarily a need for it, but people generally think this is a reasonable approach of going about it.
00:57:28.878 - 00:58:19.354, Speaker A: The nice thing, of course, is that it's forward compatible. Like if a wallet doesn't immediately adopt it, even though I do think that people are maybe a little, kind of have a bit ptsd from everyone on this call, probably experienced that of trying to have a new feature and then because it's just one layer, two shipping the feature and a lot of tooling wallets, not interested in supporting it. I think hopefully this will change a lot with these kind of common core of standardized IP features where as a wallet, I now have a lot of interest in supporting this as soon as possible. But even if not right, the nice thing with the transaction type is you can always send the old transaction type and we've seen that with 1559 as well on main net. That's fine. It's an opt in kind of transition. Is there anything specific, any specific concerns that people would have with the new transaction type or challenges of that rollout.
00:58:19.354 - 00:58:36.866, Speaker A: And then as a second question also, Ed, I think initially you mentioned that you think, let's say the four to four approach of basically, because it's just treating data like transaction type repurposing, that might not be an ideal fit because it just could only account for data. Maybe if you could also elaborate on that a little bit.
00:58:36.968 - 00:58:37.814, Speaker B: A little bit about that.
00:58:37.852 - 00:58:38.150, Speaker A: Right.
00:58:38.220 - 00:59:46.170, Speaker B: So it's kind of implicit in the way that 4844 transactions are expressed and priced, that you're paying for all of the data in a blob. The data, it comes in units of blobs, and you're paying for the whole blob because the size of the blob is not in the, first of all, the Blob data is not in the transaction, but also the size of the blob is not in the transaction. So I think you would need to, whereas of course, we want to be able to charge for different sizes of data, different data footprints for different transactions. So to me, that's the main issue. The transaction type fundamentally has a built in assumption that you're always posting a full blob and that the cost does not depend on the contents of the blob. And I know arbitram and some other, and someone else mentioned it as well, that uses heuristics to estimate the compressibility of a transaction even in advance of knowing how it will be compressed ultimately.
00:59:46.670 - 01:00:34.954, Speaker A: Right. So what I'm hearing you say, basically is that, and this comes back to the question of what to standardize. Right? So basically we need a way of just mapping, taking a transaction and mapping it to a one dimensional value of amount of l one space that it will have to take up. Say that could be the data itself, the compressed data, but also maybe take into account the execution, that portion that's necessary for that. And then the question is, does it make sense to even map that into a one dimensional value? You could almost imagine, okay, but I want to separately account for the portion of the execution for settlement versus the data and everything. But of course, then at least to the extent that we want to expose that in a transaction type, it seems very, very valuable to map that to one value. So then the question is, is that mapping something we would want to standardize or just leave up to each l two? The intuition would be to leave it up to each l two.
01:00:34.954 - 01:01:27.180, Speaker A: But then, of course, if we don't standardize it, it's harder to then standardize the rest, say, the estimation of what it even means to have, say, one settlement gas. If the meaning of a settlement gas is up to each l two, there should at least be some sort of rough standardization where say roughly, say each settlement gas would be one byte of one data or something like that, right? Because if we don't standardize it at all, then kind of that field now becomes own away almost meaningless, at least across different l twos. So what exactly to standardize in this mapping to one dimension question mark. And then once we have that, I guess the rest is relatively straightforward, right? Because then you just take the limit and the max price that the user set for that dimension. You multiply and then you charge. Or are there other complexities in that?
01:01:28.590 - 01:02:17.580, Speaker B: It seems like if you want to follow the model of the l one gas pricing, that you would want to have a notion of what is the current price of settlement gas. But then the number of units of settlement gas consumed by a transaction would vary depending on the transaction itself. And maybe that is how many catch acts it does. Maybe that is something about how large it is or how large it's heuristically estimated to be after compression or something like that. It seems like different roll ups could have different policies for saying how many settlement gas each transaction took. But the notion that there is a current settlement gas price seems like.
01:02:19.310 - 01:02:19.674, Speaker A: Seems.
01:02:19.712 - 01:02:22.670, Speaker B: To align with how people think about l one gas.
01:02:25.250 - 01:03:02.138, Speaker A: Right? And then we would not need some sort of 59 mechanism because that's an external price, right? So basically, yeah, that's right. Do you think in terms of standardization on how do you actually derive the current price? This seems also something that's probably not, that doesn't make sense to standardize. Is there like basically some sort of small portion of that to standardize, some standardized way of predicting it or some standardized way of accessing that information? Because again, I assume different L2s have different strategies at the point of inclusion, determining what actual level to charge at.
01:03:02.304 - 01:03:29.730, Speaker B: I guess what I would propose here is to have a standard gas estimation API. Each l two could have its own policy for deciding how many settlement gas to charge to a particular transaction. But then you have a gas estimation API that someone can call to get the best estimate of how many of those gas it's going to require.
01:03:33.650 - 01:03:34.062, Speaker A: Right?
01:03:34.116 - 01:03:35.600, Speaker B: And then you could also have.
01:03:37.410 - 01:03:37.774, Speaker A: The.
01:03:37.812 - 01:03:57.190, Speaker B: Transaction format be like l one gas, where the user can say what is the max settlement gas price I'm willing to accept and what is the maximum number of settlement gas units I'm willing to be charged for. Again, pretty much aligned with the model of regular l one gas.
01:03:58.970 - 01:04:26.638, Speaker A: Right. And that makes sense. And I would say, by the way, that is mostly what I had in mind in terms of repurposing the photo, Fortune's action title, just meaning that that also comes with these two extra fields. They have different meanings, semantic meaning, of course, in the block case, and you're right that basically we can't too closely match that semantic meaning. But I think basically just having a new transaction type that just has two extra fields for a separate dimension, that's something that we could more or less copy or there's precedent for at least.
01:04:26.724 - 01:04:46.280, Speaker B: Yeah. And then of course, next question is, should there be more than two? Do you want a format that allows more than two types of gas based on this zero one infinity principle, that maybe it makes sense to allow more types if we're going to take this step at all.
01:04:46.730 - 01:05:06.206, Speaker A: Yeah, let's maybe first wrap up the kind of the l one pass through specific kind of conversation, and then unfortunately already is only 20 minutes left on the call. But that we can then talk about. I think that the main remaining topic is the kind of the dimensions beyond that. Andresia, if you hand up, yeah, I.
01:05:06.228 - 01:05:28.340, Speaker E: Think it would be about the gas estimator. So if we specify the API, what is returned could be sort of like a commitment to that price. So you can have different providers of the gas estimator that will commit to the price that the user is going to be charged. Exactly that price.
01:05:31.340 - 01:06:04.804, Speaker A: Yeah. And actually, by the way, just to mention that, because I think when we talk about this, it's always kind of important to kind of separate the responsibilities. Now, almost in my mind, this is something where this could be a service that l two s offer or third parties offer on top of l two s. Basically just like committing, giving fixed commitments to future prices, which I think, I totally agree with you that it's super important for ux, but I think it's not yet necessarily clear which layer of the stack that responsibility would lie with.
01:06:04.842 - 01:06:46.550, Speaker E: Well, if you're operating a paymaster for meta transaction or even just for account abstraction, the cost there, it's like, who are you promising that to? Then you introduce, again, variability. That is problematic in terms of promising inclusion. You should have a price where a transaction is either guaranteed to be included or to know that it's not going to happen.
01:06:46.920 - 01:06:47.332, Speaker A: Right.
01:06:47.386 - 01:07:04.760, Speaker E: I think that's really important. Also it's like just providers for meta transactions or the account abstraction that's not native.
01:07:06.220 - 01:07:55.398, Speaker A: Right. Because in interest of time, just to kind of wrap this topic up so you can move to the general multidimensional. That was mostly kind of me talking with Ed here. Is there any other team that has kind of follow up thoughts on the specific l one pass through and the viability of doing that with the nutritional action type? And I see that we have the typical fate of a 90 minutes call, that after 60 minutes. Many people have to jump off or had to jump off already. Okay, sure. I actually think I personally would be really interested in kind of following up on this.
01:07:55.398 - 01:09:11.742, Speaker A: Maybe even if we could kind of have some sort of async kind of working group follow up, or maybe even just a collaboration with arbitroma, with a few teams that are specifically interested in this, to kind of try and see if we can get that to an IP reasonably soon. Because to me, honestly, that seems like a big win. That's not that hard to get to with, of course, the level and details around what exactly to standardize. Okay, then I do think kind of in the time left, and maybe we can briefly also talk about this kind of condition pricing question. But I think mostly the big remaining question is about general kind of multidimensional pricing. And again, the reason why I didn't even kind of bundle that with l one pass through in the first place is that l one pass through is almost like not even a proper second dimension, even though kind of in terms of how it's exposed to the user, it would look like that. So the question kind of being a, in general, there's of course just a few inherent extra dimensions to the EVM, that if we were to restart the blockchain from scratch, of course we would try to just have inherent, we would have compute gas and we would have kind of state access gas, and we would have bandwidth gas and all these kind of questions.
01:09:11.742 - 01:09:50.440, Speaker A: So to what extent l two s would just really, really want to do that kind of thing, versus just l two specific types of extra dimensions, like on the Zk side, the proving dimension, and then kind of, yes, I don't know. I would just be curious with l two s that have specific thoughts on extra dimensions for the future, unless we lost both of the people that raised those points early on the call.
01:09:54.090 - 01:10:44.360, Speaker B: I think this is something that we would be interested in exploring, and really for the same reasons that multidimensional gas could be valuable to l one, that there are good theoretical arguments that with multidimensional gas, you can, at least in some important cases, get more throughput or have lower costs with the same throughput. So it may be more efficient for users and it may also be that l two s run up against those limits sooner than l one because we have more throughput and are trying to push the throughput envelope more so than l one is. So it seems like a good topic to discuss, at least from where we sit.
01:10:53.280 - 01:11:50.450, Speaker A: That makes sense. And I think, unfortunately we lost most of the people from the ZK side already. But of course there's also the specific kind of extra circumstance of the prover. In general, the way I've been thinking about is basically distinguishing static versus dynamic dimensions. So static dimensions being ones that you can basically completely predict the consumption from the outside of a transaction, because those are just, they're not conceptually different, but they're just in terms of implementation, they're much easier to add dynamic dimensions where you basically have to now have a second gas counter running, or a third or fourth or something while you do the execution. That's harder to do in terms of backwards compatibility. That's now where we can basically look back to uf that we talked about earlier, where basically it is relatively hard to do that using the existing kind of evm with existing introspection capabilities because you have access to that gas counter, which inherently is a one dimensional value.
01:11:50.450 - 01:13:06.840, Speaker A: So something like that might require, might be dependent on a change like UF that would first remove that introspection. Yeah, I guess one question could be. Okay, first question, do we still have anyone on the ZK side on here that might have specific thoughts on the prover dimension? It's a bit of an open ended question. Okay. Because then I think, just pragmatically speaking, it seems if we want to kind of move forward with something, I think Ed, you briefly asked the question, should a transaction type that introduces an extra l one pass through dimension, should that kind of try to already be forward compatible with multiple dimensions? Yeah, I don't know. I would just be curious whether people have thoughts on that. It seems maybe that adds a lot of extra complexity for something that probably only is relevant in the medium to long term.
01:13:09.820 - 01:13:32.720, Speaker B: To me that's the main question is how much extra complexity do you add by making it extensible? If we think that it's something we're going to want to do eventually, and it doesn't add much complexity now, then it's worth doing. But you might need to do the exercise of actually trying to specify it to know how much pain it brings in the immediate term.
01:13:36.580 - 01:14:29.460, Speaker A: Yeah, and it's a good point that it's probably worth at least doing that. I would also say that, of course, with any of these standardizations across multiple l two s. It's important that semantically, then everything is identical. And so the nice thing with the one pass through dimension is that it's relatively easy to scope that in a way where semantically, at least to the extent that the fields that are in the transaction have identical meaning. Meaning it's kind of this extra kind of l one dimension and kind of gas of sorts. And then it's up to the l two s to fill that with more specific meaning with extra dimensions. Then the questions becomes, how do you kind of keep that as well? It would basically require the entire multidimensional pricing to then be a standardized project, which might not be a bad thing.
01:14:29.610 - 01:14:56.350, Speaker B: But you would at least need a standardized way to express multidimensional pricing. Even if you can imagine, for example, that you have gas types which are numbered and zero is execution gas and one is settlement gas. And then there's a range reserved for sort of l one, and then there's a range that l two s can use for their own experimental purposes. Something like that.
01:14:57.200 - 01:15:41.130, Speaker E: Well, I mean, if you're introducing a new transaction type, you might as well go all in and just simply replace a number by an object, right? If you're expressing that through a JSON object, for example, the fee of the price, then every l two can then list the price component and variability and what have you not, based on their own needs. And there's an aggregate top value and then you have the components listed and you just need to publish the schema and you're good.
01:15:43.600 - 01:16:08.050, Speaker A: Right. And that's probably something that we would want to get to the question. I mean, there were specific pragmatic limitations in terms of how just transactions are structured today. And I think we can't go too wild. Presumably we're kind of stuck with the very simple way of doing it. But yeah, I think basically what?
01:16:08.420 - 01:17:28.750, Speaker E: Well, the l two s could go wild. The l two s could go wild because you can do an abstraction, right? Because they have the freedom, they can abstract out the underlying sort of like l one compatible transactional structure and they can wrap that into a more dynamic or more flexible l two structure. That is then where you separate out sort of like what's required with regard to the execution versus the fees and the pricing. So if you're thinking it in terms of different abstraction layers and you have a wrapper, then if we standardize that wrapper, then the l two s have the flexibility to express their implementation specific needs much more flexibly, without impacting, without having to tightly couple that to the execution, to the transaction execution structure that's required, which is much more closely tied to the l one.
01:17:29.920 - 01:19:06.732, Speaker A: Right. Two narrow points on this one is just that, just pragmatically speaking, I think in the past we've talked a lot about how l two s are very hesitant to move too much away from how l one works, just because that means that now you have to just maintain kind of deviating client behavior and so say different ways of passing a transaction object would, I think for now be probably out of scope of what l two s would be comfortable with doing. So we kind of have to pretty narrowly follow how l ones kind of today handle transaction objects. And then also, unfortunately, of course, because the majority of the cost for l one settlement comes specifically from just the data size of the transaction object, keeping that as thin as possible and efficient as possible really is super high priority. So I think there's a trade off basically between kind of cost, just like settlement cost, and expressivity of the transaction object. I think kind of coming out of this call, my main takeaway is that probably that's the one actionable item here, right? That we should just have some sort of very specific kind of follow on effort to just work on an ip for 2d or potentially multidimensional also transaction type that, which I don't know, that's very encouraging to me and I'll kind of try and make that happen afterwards. Yeah, I'm more than happy to spend the last five minutes talking about kind of this question of is 1559 the right model for L2s? Of course, that's a big topic for a short amount of time.
01:19:06.732 - 01:19:40.750, Speaker A: Maybe just to briefly, we can briefly just touch on it just to kind of get the temperature check. Just before that, is there any remaining comments or points on this general question of l two multidimensional pricing? Of course, the general multidimensional pricing, that's going to be a big topic. Yeah. I never know how long to wait with these questions, but okay.
01:19:44.160 - 01:20:25.150, Speaker D: Yeah, I just wanted to say like one small thing. I'm not sure if it has been already mentioned on this call or not, but in the rip about account obstruction for the native account obstruction transactions, there is already a field for builder fee. And if I recall correctly, in the rap they were said that potentially this field could be used to, for instance, users to provide how much eth are they willing to pay for the pub data or whatever else. So probably whatever efforts are done with regard to multidimensional fees, they should be made compatible with this rep already there.
01:20:29.790 - 01:21:27.830, Speaker A: Yes, that's actually a really good point. I thought about this briefly earlier in the call, but yes, that actually, given that there's the general intent to move in the medium term to a four three seven style account abstraction transaction model, of course any of these changes would be quite short sighted if they don't take into account that long term transition and how that all fits together. I haven't really thought about it enough to have a specific opinion on this other than this is important to take into account. Yes, very good point. Okay then. And if people have to leave, then cost already, please feel free to. But for those remaining for the last five minutes.
01:21:27.830 - 01:22:00.450, Speaker A: Yeah, I would be curious to just briefly talk about general kind of 1559. There's of course something, it only came to mind yesterday evening, so I haven't really thought about that much. I'd just be curious. Of course, I assume that's a topic L2s have thought about a lot. Given that you all have to make these practical decisions. Yes, I'd just be curious to hear a bit more how L2s think about 1559 versus other types, like condition pricing, non condition pricing, unbundling, potentially these different aspects of it.
01:22:16.040 - 01:23:09.210, Speaker B: Okay, so from an arbitrary point of view, we use something that is similar to, but not the same as 1559. Basically a version of the exponential mechanism to do congestion pricing with a minimum price. So in a multidimensional setting, if we're thinking about resources and limits in a multidimensional way, there may be other approaches and we're continuing to do research. I think we may evolve how pricing works, but the basic spirit of 1559, which is tracking use versus target and then adjusting prices to try to reach equilibrium by an exponential like mechanism, seems in principle to be a good approach. But we don't currently and probably won't do exactly 1559.
01:23:21.960 - 01:24:14.096, Speaker A: Right. Are there any other. Okay, now the calls really has shrunk, so I assume we might not have anyone else left on the call that has opinions on 59 like mechanisms fault. Right. Then it's probably not worth at this point to go into too much more because we also are running up on time then. Yeah, always a good fun topic for a separate call at some point, but yeah, then it's probably a good ending point here. Thanks everyone for coming on.
01:24:14.096 - 01:24:46.744, Speaker A: Especially, of course, those of you that made it all the way to the end, and I think that was a productive conversation. And especially if we maybe can actually follow up on that idea of one pass through transaction type, then we actually got somewhere. And, yes, that's all from my side. Thank you all and have a nice rest of your day. And just to mention, by the way, thank you. Just briefly mention it's still wrong on the calendar. I'm trying my hardest to fix this on the protocol calendar, but the next roll call will be in two weeks, not in one week.
01:24:46.744 - 01:25:03.280, Speaker A: Then in next week, we'll have one more breakout call on the general kind of concept of standardizing pre compiles across L2s. And then the week after, there will be another main roll call. So keep that in mind. Awesome. Bye for now. Bye bye.
