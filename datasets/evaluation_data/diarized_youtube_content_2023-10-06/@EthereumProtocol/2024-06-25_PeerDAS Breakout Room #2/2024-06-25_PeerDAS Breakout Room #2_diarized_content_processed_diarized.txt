00:01:16.620 - 00:01:17.160, Speaker A: It.
00:03:38.670 - 00:04:01.100, Speaker B: Hello everyone, is Knutc too harsh for many people? Can you hear?
00:04:02.520 - 00:04:04.620, Speaker C: Hello? Yeah it's good here.
00:04:05.720 - 00:04:34.020, Speaker B: Hello George. Yeah, so I posted the agenda and meeting notes in the chat so I will re edit some spec items. So I think today we have banabas here and we can probably talk about the definite status. Firstly do you think one of us.
00:04:36.640 - 00:04:44.414, Speaker C: I think we can only start talking about Devnet status once we have client releases. Someone's ready.
00:04:44.462 - 00:04:45.450, Speaker B: Oh I see.
00:04:45.750 - 00:04:57.490, Speaker C: And we were saying that client releases should be doing office three release. I'm not sure if anyone or if all of them are ready to go.
00:05:02.750 - 00:05:10.650, Speaker B: Does any client wants to share your new release updates for the peer desk part?
00:05:13.750 - 00:05:18.934, Speaker D: So is this an official release or the offer three?
00:05:19.062 - 00:05:28.970, Speaker C: Yeah, or like I just need a branch. I don't need an official release, I just need a working branch. I think for prism it is already working.
00:05:29.630 - 00:05:31.730, Speaker D: Oh okay. All right cool.
00:05:34.950 - 00:05:40.090, Speaker C: We could also just launch a devnet without all the clients included if that's what we want.
00:05:49.270 - 00:06:17.256, Speaker B: So is it time to, for countries to make a commit about when will the peer desk branch be ready? One of us which kind software that you, you don't are not available for, you know for the Defnet is there.
00:06:17.408 - 00:06:24.780, Speaker C: Lodestar, Lodestar Nimbus and Grande was not ready last I checked.
00:06:25.640 - 00:06:36.740, Speaker B: I see anyone's in this co want to share the updates?
00:06:39.720 - 00:07:52.816, Speaker E: I can share the updates for Prism since last two weeks. Yeah we worked a little bit on reconstruction right now when a node received 50% of the colon he then had the reconstruct immediately all the metrics but waits for 3 seconds into the slots before sending back the colons. The node should have, should have received but columns the nodes did not actually received before we were resetting all the columns immediately. Now we wait for 3 seconds into the slots. It is about reconstruction. And we did, we made some progress on sampling as well. Before about sampling we were just iterating over peers until a peer gave us the column we need, which is to quote famous researcher like if we toss a coin until we like the result.
00:07:52.816 - 00:08:43.040, Speaker E: And we implemented last week lossy das, incremental Das. For that we use actually the function getextended subcount which is not yet, not yet merged, but which seems to work quite well. And so yes we implemented lucida incremental Das and what we have to do yet is to sample nodes, to sample sphere entire right now with sample spheres one by one. And especially if we, once we, once we meet super node basically we sample all the columns from the supernode which is not a good way to do, but we are working on parallel samplings. Yep, it is an update for prism for the last two weeks.
00:08:47.460 - 00:08:51.320, Speaker B: Thank you. Anyone else.
00:08:54.260 - 00:09:27.390, Speaker F: I can give an update on Lighthouse? This is Jimmy. I got the team right this time. So on the lighthouse side, we have implemented the latest spec. So including a passing test for the spec one 5.0 alpha three. We've updated the custody subnet, count Enr key and format to match the latest spec. So I think we are probably devnet one ready.
00:09:27.390 - 00:09:55.210, Speaker F: Except for syncing. We're currently looking to a range sync issue. Hopefully we have that fixed before we launch Devnet one. And we've also made some improvement to reconstruction. We've made just a bit of improvement on the. Making it like non blocking so we can continue to process other gossip data columns coming in while we perform reconstruction. So that's work in progress currently.
00:09:55.210 - 00:10:22.310, Speaker F: We're doing a bit of testing on that. Yeah. So I think with Devnet one ready, the branch is the same branch part of us. We're still working off the Das branch. Sam Bradshaw can link it later as well. And I've been running a local Devnet for about four days now and it's for four nodes and it still seems to be working okay. It's continuing testing a bit as well.
00:10:22.310 - 00:11:18.650, Speaker F: Also we've got Kev building a KCG library integration for us. So he's got it integrated into lighthouse. So we're probably going to try to test it out. So just as a fallback from CKCGPG, we can do a bit of testing on the new Rus peer Das KZG library and see how it goes. And then we also plan to work on checkpoint sync in the near future. What else? I think if I get some time, we'll probably try to do a bit of experiments on the fetching blob from the El man pool so we could help with reconstruction from the supernote. Michael from our team has been doing a bit of experiment on that.
00:11:18.650 - 00:11:26.590, Speaker F: Hopefully we can put that over to Pierdaz soon. Yeah, I think that's all from Lighthouse. I'll pass on to the next team.
00:11:30.620 - 00:11:35.560, Speaker B: Thank you. Any other client updates today.
00:11:39.740 - 00:12:00.010, Speaker G: I could do quick update for techo. We have updated to be compatible with Alpha three spec and we rework das sampler now because it had some issues with implementation. Will match soon, in a few days. I hope that's all.
00:12:02.790 - 00:12:04.570, Speaker C: The branch is still correct?
00:12:06.270 - 00:12:08.382, Speaker G: Yeah, the branch is the same, yeah.
00:12:08.486 - 00:12:09.250, Speaker C: Okay.
00:12:22.550 - 00:12:52.860, Speaker B: Any other client updates on your desk? Nope. Okay. Now we can move to the spec discussion or is there any other questions? Issues of the death Knight one?
00:12:55.560 - 00:12:59.580, Speaker C: Looks like Brandon just joined in. Maybe they have a status update.
00:13:01.440 - 00:13:33.190, Speaker H: Yeah. Hi guys. Sorry for later joining late. So we still have some issues not solved. And my question is, how ready are the other clients right now? Probably it was disclosed early in the call. Maybe somebody could do a very short summary, presumably. Okay, so three clients already, right?
00:13:33.610 - 00:13:34.390, Speaker C: Yeah.
00:13:35.490 - 00:13:37.202, Speaker H: Okay. Okay. Okay.
00:13:37.226 - 00:13:37.458, Speaker E: So.
00:13:37.514 - 00:13:38.790, Speaker C: So I think.
00:13:39.290 - 00:13:44.990, Speaker H: And do you have a data in mind for Devnet one?
00:13:45.910 - 00:13:53.126, Speaker C: I mean, we can launch without all clients being ready and then clients that are not ready can join in later. So that's not.
00:13:53.158 - 00:13:58.926, Speaker H: Yeah, but. But was it discussed the potential date for launching with.
00:13:58.998 - 00:14:05.330, Speaker C: I mean, based on the feedback now, I think we can even launch it today or tomorrow.
00:14:06.910 - 00:14:17.640, Speaker H: Okay. Okay. For me, it's okay if I. It will be decided to lunch only with three clients and the rest will join. We support that idea.
00:14:37.190 - 00:15:23.756, Speaker B: Any other questions about. Then let's move to the stack discussion. So the most interesting discussion that is related to the peer, that is the block. Sorry, block max limit. How to pass that number to El. We talked last time, so I guess since it would be a. It will also be an El update.
00:15:23.756 - 00:16:20.410, Speaker B: So we will also talk about it on the Thursday code. But so far there are two open prs in the space. One is 3800 that Alex Stokes proposed, which is. It passed max block parameter to Yale. And the second one is downgrade, proposed yesterday. This one will calculate the best rate per block gas in this CEO site and then pass that number to the eo. Now, I don't know if people have the chance, had a chance to review it or nothing.
00:16:31.220 - 00:17:04.510, Speaker I: I have seen both the peers and I would basically favor passing Max blob gas limit and sort of then having target half of it and El doing the rest of the competition. So basically this is, this is like the first solution. And for the second solution, I mean Cl computing and passing broadcast. I am basically less inclined to it. That is my opinion on the matter.
00:17:25.660 - 00:17:31.360, Speaker A: So photos had a comment that we will have to do math with U 256. I'm not sure if that's a problem.
00:17:46.150 - 00:17:48.650, Speaker B: Sorry, the line could repeat again.
00:17:53.070 - 00:18:03.090, Speaker A: That we just had to do math with UIN 256. And I don't. I don't think consensus clients have that, do they?
00:18:09.960 - 00:18:15.540, Speaker D: Currently not. It's nowhere in the spec that we have to handle unit 256.
00:18:38.370 - 00:19:15.890, Speaker I: And anyway, this particular pr should be orthogonal to peer das. And I mean, I idly would basically favor any of the two solutions to go in tekra. So that basically peer does can use it. So pdas basically is orthogonal to it, right? So but yes, the activation can of increase of the blobs can happen according to the plan that we can bake in pdas. But idly this pr should go in pectoral.
00:19:18.590 - 00:19:22.910, Speaker C: Any reason this should be in pectra and not together with a fork in peer does?
00:19:27.330 - 00:19:33.990, Speaker D: It's not possible to be in the fork with peer dust because it's technically a consensus change. So it has to happen in pectora.
00:19:36.250 - 00:20:09.300, Speaker I: Yeah, first pls is not a fork. And then I think it's better that it should go just with pectoral so that whatever is the blob rollout limit, even without peer it can be independently rolled out or again paired with PRS. PRS is just totally orthogonal feature with respect to this particular prince.
00:20:13.080 - 00:20:16.260, Speaker C: So which solution is actually preferred? The client teams?
00:20:26.680 - 00:20:37.020, Speaker D: I think one is better because you wouldn't have to do the blob guest arithmetic in the CL, you could leave it to the El as it is right now.
00:20:50.850 - 00:20:56.630, Speaker I: I would also favor one line. What do you think? Which solution do you like.
00:21:02.210 - 00:21:04.150, Speaker A: On chain boring by validators.
00:21:12.180 - 00:22:00.420, Speaker I: That actually can happen in solution one. For example, if there is an El contract, like for example we have for 7702, where the gas fee etcetera is sort of decided by the contract because I mean there is this particular formula. But if there is an El contract where you send a transaction through for the blob and that can decide the block whatever the transaction has to pay. So whatever is the logic, whether it's a limit raised by some on chain voting or by some upgradation of the contract, I mean that would be particularly nice and would sort of be consensus hard fork free.
00:22:19.640 - 00:23:45.310, Speaker B: Any other feedback? All right, so completed on the Thursday coast agenda as well to have a deeper discussion. And meanwhile, please take a look at the PR and maybe add some comments emoji signal like your favorite solutions here. Okay, so the next thing that I like to check is printed, sorry, downloading the link. So it's this by Chaba. So I added some text and so far the correctness looks good to me and I wonder if people in favor of merging it or is there any issue with this PR?
00:23:50.690 - 00:24:02.150, Speaker E: Yeah, just for information, prism already implementing this get extending sample counts function and yeah, it works well for us.
00:24:05.050 - 00:24:08.370, Speaker B: Could you use it in your here, there.
00:24:12.710 - 00:24:18.730, Speaker E: It is currently in the period branch and we use it for incremental das slash lucidas.
00:24:28.470 - 00:25:07.550, Speaker B: Any other feedback? Oh, okay. Invest no issue. I plan to merge it before Thursday. Thank you any other spec discussion, any space, what issues that we want to discuss today for the cryptography part as well.
00:25:08.410 - 00:25:32.150, Speaker J: I also have another PR, which is the. I think it's a 92. Let me see. No, 94, which is this one. Yep. Thanks. So that's about the reconstruction and cross seeding.
00:25:32.150 - 00:26:10.250, Speaker J: So that goes into the direction that was mentioned at the very beginning of the core. I was adding sinks on delaying the construction and on seeding based on the construction. But obviously I think it can use some, some input based on experiences. What can be recommended there? What should be left to implementation to the point that it's not even part of the, of the spec. But please, please check.
00:26:16.360 - 00:27:12.090, Speaker B: Thank you. Yeah, I think that VR is also like ready to merge. Nice to have. So if there's no objection, I can also merge it before Thursday. Okay, any other big discussion today? Okay, and then any other open discussion about pure desk?
00:27:18.630 - 00:27:37.100, Speaker F: Yeah, I have a question about Devnet. Would it be beneficial if we increase the blob count just to test out how the bandwidth looks like with peer Das? That would help.
00:27:38.840 - 00:27:41.100, Speaker C: The El change. For that.
00:27:44.920 - 00:27:58.180, Speaker F: Maybe we could change the hack code constant in the El. I'm not sure what's the best approach. If we don't make any changes in El, does it just give us wrong gas prices?
00:28:04.450 - 00:28:17.150, Speaker C: Maybe? I'm not quite sure, to be honest. We could try to increase it, but do we even have now a variable that could be changed config var?
00:28:20.250 - 00:28:33.270, Speaker F: I believe it's currently a preset value. So I think some clients support changing preset, but some clients don't. Lighthouse does it. So we could do a custom build for that test.
00:28:34.410 - 00:28:43.230, Speaker C: I would actually first move that to a config variable and then potentially discuss if we want to change that.
00:28:44.850 - 00:29:01.080, Speaker A: Yeah, this is an indication we shouldn't make the same error with blobs. So we should make the data structures have some value that's a very high upper bounden and then have as a config the actual limit of columns. Same we were doing with lots.
00:29:08.180 - 00:29:42.930, Speaker C: Could we get a pr for that in the consensus pack? And then I can make a adjustment for that in our genesis generator. And is it actually configurable by every client then should we make that a spec requirement? Basically? Like do we want this to be something for Devnet one or should we launch Devnet one today without it? That's basically the question.
00:29:44.190 - 00:30:07.450, Speaker D: I think it'd be better to launch Devnet one without it. If we do have it. It would take a bit of work for us to make it like a config variable, like unless we're okay with delaying Devnet one by like a week or so.
00:30:12.030 - 00:30:20.380, Speaker A: Yeah, I got, we can, we can just launch that in one with this and then do tests with higher counts later. So no need to wait for this.
00:30:22.840 - 00:30:40.570, Speaker C: All right, then I will spin up Devnet one with similar validator setups as Devnet zero. And then let's see how stable that's going to be. And then for Devnet two, we could probably have this adjusted and hopefully have like a new spec for it. Ready.
00:30:55.990 - 00:31:46.322, Speaker A: Another topic that I think is relevant to mention. So Michael Sproul has an initiative to help peers access blob data before they can do. Now the trick is that instead of expecting blob to come through gossip, you can just go to the main pool of blobs in the execution client and just fetch them from there. He has a proof of concept for that with both of modification on Lighthouse and ref because we need some new engine API route to expose blobs to be tested that way. But in our tests, it shows that we can retrieve blobs really, really fast, maybe like in 20 milliseconds or 30 milliseconds after we get the block at the block.
00:31:46.346 - 00:31:46.970, Speaker D: Sorry.
00:31:47.130 - 00:32:33.980, Speaker A: So this can be, this can be a game changer for basically people that are in locations with high latency like Asia Pacific or constraint ions in home stakers that don't have too much bandwidth and they already have the blocks. This can significantly help them make the corporate boats and not be like penalized for high bandwidth. So I think that can also apply to peer ras, because if you get all the blobs, then you can consider the thing available, even though you don't have the columns at the moment. You can eventually compute from there. And yeah, I think that's going to help a lot to equalize the playing field. People with different language.
00:32:42.680 - 00:33:05.190, Speaker D: Yeah, I think that's a really good idea. So I've been looking at the blob of my local node and I do see a lot of blob transactions floating about. So you know, it is, it makes a lot of sense to just use, you know, these transactions rather than, you know, waiting for it to come true. Gossip. So I'll be for this idea.
00:33:33.940 - 00:33:35.680, Speaker B: Any other feedback?
00:33:43.230 - 00:34:10.660, Speaker J: I think it sounds very good. This is the one that was discussed also in the discord, as I understand. My question is what information you use there to construct it from the mempool. So how the node puts it together, it was not clear to me. Maybe it's just mine owns.
00:34:13.040 - 00:35:12.400, Speaker A: So if you get the bitcoin block, you have a list of commitments that you can use. So we still haven't settled in if we are going to index by transaction hash or the version hash. But if, I mean, I hope we do version hash because otherwise we need to make consensus plans aware about LP and coding to tell them. Tell the yale hey, give me these transactions. I think it will be easy that you just compute the version hashes from the commitments and then tell the yell hey, give me the list of blobs for this list of version hashes that I got from the blob and then from that for the net straightforward, you get the blocks. For PFAS, you would have to compute the extensions and then do all the groups to get all the sidecars. So I'm not sure if there is a way to like let's say that this feature will be used, I would imagine will be more interesting for people that don't have a high cost of requirement.
00:35:12.400 - 00:35:21.720, Speaker A: So having to do all the work for two columns just doesn't sound too good. But that's an optimization, it should still be faster.
00:35:26.750 - 00:35:28.610, Speaker J: Much thanks.
00:35:33.550 - 00:36:27.200, Speaker A: Which, sorry, just to clarify, the way I want to implement this in lighthouse is at least for trdas. So you would ask the El for the blobs, and then when you catch the blobs you will optimistically, whatever that works means to you, consider that block to be a viable head before you get the columns. And then on the background you will compute the column sidecars. So there will be some window of time that you have already attested, but you still don't have your constituent requirements. So hopefully that time Delta is less than what peers will be asking for you on PC. But yeah, if for some reason it takes too long, you would be announcing to the network that you have some data that you don't have and could be penalized or downscored or delay something. But I don't think that should be too much of a problem.
00:36:35.460 - 00:36:47.480, Speaker G: If some clients are going to do this and others not, there could be an issue that we will not actually check. Is it, was it actually distributed or not?
00:36:51.940 - 00:36:53.600, Speaker A: Sorry, I didn't got that.
00:36:56.380 - 00:37:22.250, Speaker G: Say, if Lighthouse and Prism will use this trick here and data was not actually distributed, all lighthouse and Prism will think that it was distributed, but other clients which did not implement it will not see data available.
00:37:22.910 - 00:37:37.540, Speaker J: No, because it will be distributed actually, just not by the one who was supposed to distribute it. So it will get onto the gossip sub, right? So. So it will, it will be.
00:37:39.560 - 00:37:39.872, Speaker B: The.
00:37:39.896 - 00:38:07.540, Speaker A: Way for this mechanism to fail is if you propose a block which includes a block transaction, and you don't rebuild this block transaction. That's the only way for this trick to fail. In that case, when you have your list of version hashes and you ask the yale for them, then you will get an error because the yale has not received this block transaction, and then the trick will not work. But if all the blobs have been made available and distributed on the main pool, then data is available.
00:38:09.400 - 00:38:16.260, Speaker J: So you're kind of decentralizing the calculation of the. Of the extended structure.
00:38:18.480 - 00:38:23.860, Speaker A: Exactly. Because if you have the blob, you can always compute the extension.
00:38:33.150 - 00:39:30.490, Speaker J: But when it comes in on gossip sub, when it comes in in the sampling, you are always validating it against the commitment. So it cannot be something not formally part of the structure. The notes which are not doing this trick don't even know that this trick is being done. You just don't see the difference because you are having a few neighbors. One of them will do this or will get it from someone who is doing it, and then it comes in on gossip sub, like if it would have been sent out by the block producer. It's binary equivalent, so you don't see the difference.
00:40:09.410 - 00:40:22.030, Speaker B: Lion, do you have any document or write up to the proposal or as to do on you on your very long list?
00:40:25.370 - 00:40:27.350, Speaker A: No, I do not, but I can make one.
00:40:29.450 - 00:40:43.200, Speaker B: Thank you. Nishant, do you want to talk? You are muted.
00:40:43.940 - 00:41:04.370, Speaker D: Oh, yeah, sorry. Yeah. So I just had a tangential question. In the bandwidth estimates that we have done, I've noticed it mostly encompasses the consensus layer, but do we know how much bandwidth the execution layer would take up in terms of transactions propagated?
00:41:35.680 - 00:41:38.780, Speaker B: I guess that question is to Ryan.
00:41:42.960 - 00:42:08.750, Speaker D: Yeah, I guess for anyone who can, because like for gossips, we know the amplification is roughly like six to eight. But in the execution layer, let's say you have full dank charting and you're sending out like 26 blobs every slot. Do you know how much amplification comes from that?
00:42:29.180 - 00:42:47.480, Speaker A: There seems to be no real debts here, so just ask on this board. But I would imagine at that time, if we have so many data, then people that are bandwidth constraint would not be producing logs. So then you can just disable the mempho.
00:42:53.100 - 00:43:07.720, Speaker D: But, you know, wouldn't peer does be effective for them because now they're getting saturated by trying to basically propagate these blobs in the EO and all our works optimizing the CEO.
00:43:08.620 - 00:43:30.720, Speaker A: Right. Blobs work a bit different. They are not propagated by default. You have to. So it works like on gossip. The I have, I want, you have to actually tell your, your peer to actually send you the blood transactions. So you, if you want, you can opt out of the bandwidth for block transaction.
00:43:33.660 - 00:43:36.200, Speaker D: Is this implemented in El. So is this like.
00:43:38.930 - 00:43:40.710, Speaker A: This is implemented since then?
00:43:42.090 - 00:43:43.026, Speaker D: Oh, okay.
00:43:43.138 - 00:43:56.990, Speaker A: But I would imagine that now all clients pull the blob transactions, but as the spec is written today, you don't have to do that if you want to. So that could be like a flag on geth. I'm not sure if it ever exists.
00:43:59.970 - 00:44:18.500, Speaker D: Okay. Because I was monitoring gets mempool and I basically got every single blob transaction that was included in the network. So I was thinking if the average node runner doesn't know about this, they're going to be incurring a huge amount of bandwidth if we increase the platform.
00:44:45.930 - 00:45:09.440, Speaker B: Feedback. So banabas, want everyone to check if these parameters are correct. Please take a look in the chat.
00:45:11.940 - 00:45:15.320, Speaker C: Yeah, I was putting it on this part so we don't have it lost.
00:45:18.860 - 00:45:22.240, Speaker B: Yeah. And do we have any other topics to discuss today?
00:45:31.390 - 00:45:56.180, Speaker F: I want to ask what's the plan for the Devnet structure? Are we planning to run multiple super nodes per client? I think it may be useful to have a few more super nodes in last time so we don't just walk out because we don't, we can't get the columns. I think that's what happened to Devnet zero.
00:46:02.920 - 00:46:33.990, Speaker C: What is the proposed structure? So I'm thinking of launching three lighthouse, three prism, three decanodes, each with 100 validator. So that would be 900 validators plus one extra node from each of the non working clients so that they will be able to debug on the nodes. So would we want one super node per each working client? Would three total supernodes be sufficient?
00:46:39.770 - 00:46:58.510, Speaker F: I think at a minimal it would be ideally two per client. It's just in case. If we do an upgrade and don't start again, at least we'll still have one working. But yeah, I think one per client would be nice. I think we already have three. That probably better than just one or two.
00:47:00.010 - 00:47:24.530, Speaker C: Okay, so I think I know what the flag is for Lighthouse, but I'm not sure if I know the flag for the other client. Could you maybe drop it in discord? This is for Lighthouse. I know that, but I'm not sure about Prism and Taku.
00:47:29.190 - 00:47:30.890, Speaker G: I will send to just caught.
00:47:38.120 - 00:47:39.060, Speaker C: I'm sorry.
00:47:43.680 - 00:47:46.020, Speaker G: I will send to discord for telco.
00:47:46.920 - 00:47:48.180, Speaker C: Okay. Thank you.
00:47:54.880 - 00:48:31.020, Speaker E: I also have a question about withholding some comments for a blog proposal. Is it possible? Yes, for at least some nodes to enable the withhold flag in order to be sure that the recon function is working well for other nodes and for sure for supernodes. On Prism, we have an option. We have a flag. If you use it, it will prevent the property nodes to seed all the columns.
00:49:09.530 - 00:49:17.350, Speaker B: So it seems other guys doesn't have such back.
00:49:20.430 - 00:49:29.450, Speaker E: I think Lighthouse has this hack. Lighthouse prism as well. I don't know, for other clients.
00:49:41.030 - 00:49:42.780, Speaker F: Yeah, I think it'll be useful to test it out.
00:49:42.870 - 00:49:43.500, Speaker A: It.
00:50:10.810 - 00:50:40.310, Speaker B: Let's keep discussing it with discord. Any other topics for today? No. No. Okay. Thank you for joining the call. Thank you for making it. Talk soon.
00:50:40.310 - 00:50:41.450, Speaker B: Bye bye.
00:50:43.990 - 00:50:45.110, Speaker J: Thank you. Goodbye.
00:50:45.230 - 00:50:46.090, Speaker F: Thank you.
00:50:47.270 - 00:50:47.550, Speaker C: Bye.
