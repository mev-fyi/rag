00:02:20.470 - 00:04:35.706, Speaker A: It to HDE one seven six. So main thing today is the Denkun upgrade. So be good to get some updates on the devnets from the different client teams, and also to try and figure out what we want to do over the next couple of weeks and potentially over the holidays, as we have an ACDC scheduled next week, Acde two weeks from now. And then I don't know if we'll have ACDC over the holidays around sort of New Year's, so they're the last couple calls we have to sort of coordinate. And then after that, I just have a couple of small process things around meta east, which we discussed last time, and then starting the discussion around the next upgrade. But, yeah, to kick it off, I know we launched Devnet twelve. We've been running some tests on it.
00:04:35.706 - 00:04:50.000, Speaker A: So maybe we can start with just a status update on Devnet twelve and then go into the various client teams. And where they're at, I don't know, Perry, or if Barnabas, either of them, is on the call.
00:04:50.930 - 00:05:15.986, Speaker B: I can go. Yeah. So we've had Devnet twelve up since last week. It's been finalizing during regular operation. We did find a red park that's been patched. Now, on Friday, we launched Mario's blobber tool, along with a new action it can perform that triggers slashing. So that's the new equivocation condition.
00:05:15.986 - 00:05:57.810, Speaker B: And via the test, we found, I think, one or two lighthouse slasher related bugs. On Monday, those bugs were patched. And now we do have slashings on chain. So if you go to Dora, there's a section there where you can see some nodes are slashed. Once we did confirm that slashing works, we turned on MeV on more nodes, and we did find at least one Mav relay related bug that's also been patched now. And we've rolled out Mav to a lot more nodes. You can see that there's a healthy distribution of delivered payloads.
00:05:57.810 - 00:06:27.978, Speaker B: We're still seeing on off errors on the relay, so nothing's fundamentally broken, but there's still something broken that would probably be dug deeper by the Flashpots team. And I see there's some chatter on interrupt now with Jim posting a potential map bug with lighthouse. I think. I think that's pretty much everything we have on Devnetwork.
00:06:28.154 - 00:06:30.910, Speaker A: Can you just verify what's an on off error?
00:06:33.270 - 00:06:54.086, Speaker B: We're noticing a few error logs on the relay. Got it, but not necessarily sure why. There's something about slots in the past. I think Shana from flashbots mentioned it could happen when there are no transactions that the builder would be potentially building a block with, but that doesn't happen very often.
00:06:54.268 - 00:06:55.240, Speaker C: Got it.
00:06:56.650 - 00:07:06.540, Speaker A: Thank you. And then there's a comment from Danny about trying to test the fallback to the relays by shutting them off from time to time. Have we done that?
00:07:07.470 - 00:07:11.050, Speaker B: We haven't yet. We need to still test the socket breakup.
00:07:12.770 - 00:07:23.700, Speaker A: Awesome. Yeah. Any client team want to share some more granular detail from their end?
00:07:26.550 - 00:07:45.222, Speaker D: Still working on revamping some of our original design with regards to blobs catching verification layer, so I think we should be able to join twelve, maybe next week, if not in two weeks. So that's just an update from our end.
00:07:45.356 - 00:07:46.520, Speaker A: Got it. Thanks.
00:07:47.390 - 00:08:14.500, Speaker C: Hey, and I know, I think David was talking to Casey yesterday, but essentially a lot of the security researchers are super eager to help. Do know the testing team and DevOps team are super eager to integrate y'all. So if there's anything that people can do to help support y'all to get you to the finish line, just make sure to reach out.
00:08:15.190 - 00:08:15.970, Speaker A: Yeah. Thanks.
00:08:16.040 - 00:08:22.820, Speaker D: We have got a bunch of good feedback from Justin from the security team already, so thank you so much.
00:08:26.010 - 00:09:15.820, Speaker A: Awesome. Thanks Terrence. Any other team? So if there's no specific updates, I guess what the teams feel like the right next step is at this point once prism joins the Devnet, and obviously if we find any issues there, we fix them. Do teams feel comfortable moving to testnets or at least the first one like Gordy? Do we want to do potentially a shadow fork in the next couple of weeks as well to see if we get more data out of that? Yeah. How do people feel we should be moving forward at this point?
00:09:22.200 - 00:09:24.010, Speaker E: Base is ready. Full steam ahead.
00:09:27.320 - 00:09:28.950, Speaker D: Yeah, same other mind.
00:09:34.450 - 00:09:35.920, Speaker F: Hurry is ready.
00:09:39.650 - 00:09:41.460, Speaker G: Aloh. Star is also ready.
00:09:43.350 - 00:09:46.580, Speaker H: No concerns on techie side yet. Happy to move forward.
00:09:51.980 - 00:10:23.380, Speaker A: I guess. Is there any team aside from Prism who feels like there's something big or significant that they still need to work on? Obviously every team probably has some bugs and some small things to fix, but is there anything else that people are concerned with? And Perry has a comment about testing the slashers. Are there other clients which have slashers that's ready that we could test on the Devnet right now?
00:10:25.510 - 00:10:29.730, Speaker C: I'm pretty sure that's just prism and lighthouse future supported.
00:10:31.430 - 00:10:32.034, Speaker B: Okay.
00:10:32.152 - 00:11:59.774, Speaker A: So we can test prism as soon as we have it. So I think if prism needs a couple more weeks and potentially we find some issues and fix them and also not wanting to fork like a testnet over the holidays, I feel like we might be in a spot where we can aim to coordinate a fork on Gordy as soon as people get back from holidays. Again, assuming we don't find some other crazy issues or something that delays things by a few weeks. But generally, does that seem reasonable to people? That, I don't know when would be the first 2024 call, but using that as a time to set the fork date and then potentially having the fork two weeks outside from that. So if the first call is on the first or second week of January, this means we'd be forking Gordy sometime mid January, something like that. Does that seem reasonable to people at this point? And again, pending. Obviously, I know prism has some changes we need to test on the devnet, and that could still reveal an issue.
00:11:59.774 - 00:12:04.610, Speaker A: But assuming that after a couple of weeks it's integrated and it's also tested.
00:12:10.460 - 00:12:18.760, Speaker C: Yeah, I'm pretty pro, conditionally doing that. Assuming we can get Prism integrated in battle.
00:12:26.620 - 00:12:29.410, Speaker A: We, if we're aiming for that. Oh, sorry, Ben, please.
00:12:31.220 - 00:12:57.320, Speaker H: I just wanted to check status on something. So one of our concerns, moving from targeting two to three blobs per block, was around network propagation. Are we now satisfied that this is going to be fine on main net? Have we gathered enough data, or is that something we need to do on testnets? And where do we fit this into the program and the planning?
00:13:00.720 - 00:14:05.468, Speaker C: So from my understanding, both the main net big block experiments, as well as the organic big blocks on the order of even 1.5, sometimes near two megabytes that we've seen on main net were really solid. And testnets continue to be solid. And mainnet shadow forks would be telling, at least to some degree. Although all of those testnet related items are on the order of small, I would be supportive of between now and mid January trying to do some multiregion, pretty large node experiments, either with like potentially even on main net shadow fork level, just to really get a view on it. Let's see, what does Perry say? Yeah, so Perry, you said 300 nodes.
00:14:05.644 - 00:14:06.032, Speaker A: Yeah.
00:14:06.086 - 00:14:25.312, Speaker B: The plan is that once every client is ready for Devnet twelve, we're going to do one really big shadow fork. So something like 300 nodes, and we have the tooling mostly ready for that. Well, we'll find out once that shadow fork is up. Sorry, go ahead.
00:14:25.386 - 00:14:32.200, Speaker C: The cost differential on doing that same thing on main net is on the order of many multiples.
00:14:33.260 - 00:14:36.644, Speaker B: Surprisingly not that much. Curly is really big right now.
00:14:36.782 - 00:15:30.508, Speaker C: Okay, interesting. Yeah, I guess the main thing that we'd want, because in the gossip flow is not really execution against the main net state. But the big thing is the gossip kind of competing with main net level. Probably transaction gossip is the main other thing that would be kind of telling and probably switching from gorely to Mainnet. Although we could potentially make sure that a gorely main net shadow fork had a lot of transaction gossip. But anyway, I would be supportive of 300 or even ramping up to more if it's reasonable in terms of man hours and cost, even if it's like an hour long test or whatever.
00:15:30.674 - 00:16:00.580, Speaker B: Yeah, I think we'd have to keep the test really small for sure. But at least one thing we've noticed in the past is all the shadow fox, et cetera, that we've done are too pristine. We're only going to get any real data for a p to p related upgrade from natural testnet. So at some point we're going to have to bite the Bulletin to Gurley and I would make the case that we spend any extra money we're going to spend on better monitoring setup for Gurley and do it earlier.
00:16:01.400 - 00:16:05.030, Speaker C: Do we have an estimate of how many nodes are on Gorely proper?
00:16:08.220 - 00:16:11.930, Speaker B: I don't have that at hand, but I can find that out.
00:16:12.380 - 00:16:27.420, Speaker C: Yeah, it'd be really interesting to get just even order magnitude crawler data because if we're on the order of like 1000, I think that that would be very valuable data. If we're on the order of 300, then it's like not much different than what we're going to get.
00:16:27.490 - 00:16:46.020, Speaker A: The newest crawler has about three k on Gordy versus almost nine k on main net. So I don't know how reliable the numbers are, but if the ratio between them is good, then it's like still a third of main net, which is pretty decent.
00:16:46.600 - 00:16:58.600, Speaker C: That's really good. Especially with the way gossip should scale. Obviously you can have degenerate cases, but it should scale with the log of the size. So it's not actually terribly different in terms of hops.
00:17:06.970 - 00:17:40.530, Speaker A: Oh, actually. Sorry. Okay. No, yeah, it's about a three x ratio. Okay. So we can aim to do at least one Gordy shadow fork by the end of the month, then trying to fork Gordy next early in January. And then that'll be probably the time where we would change the Blob count, basically.
00:17:40.530 - 00:18:09.450, Speaker A: Like if we saw something going wrong on Gordy that was Sort of attributable to that. But aside from that there isn't know. We might see something on sepolia as well. But all of those networks have like a smaller, like if it works on Gordy. It should probably work on Sepolia and on Holski. But Gordy is probably the last significant test where we'd learn something that makes us change the Blob count before we go to mainnet.
00:18:11.310 - 00:18:15.120, Speaker C: Yeah, it makes our. We're sacrificing it.
00:18:15.650 - 00:18:16.974, Speaker A: Hopefully not, but yeah.
00:18:17.012 - 00:18:21.870, Speaker C: Anyway, it's okay if it is sacrificed.
00:18:27.190 - 00:18:30.340, Speaker A: Yeah. Ben, does that satisfy your questions?
00:18:30.950 - 00:18:45.480, Speaker H: Yeah, I think mean, to summarize, Gurley Network is likely where we'll get the best data on this. So as long as we've got the tooling and the kind of time to do that analysis at that point, I think that's fine.
00:18:47.450 - 00:19:00.620, Speaker C: And knowing that a Gorely shadow fork before we will be spamming and monitoring for this condition specifically so that if it fails there, we know it's probably going to almost certainly failing. Really.
00:19:11.330 - 00:19:34.950, Speaker A: Sweet. Okay. In terms of next steps. Yeah. Let's get some more testing on Devnet twelve. Get a Gordy shadow fork up and running. As soon as we're back next year, we can finalize the Gordy fork date potentially.
00:19:34.950 - 00:20:11.416, Speaker A: We can also have some proposals async on the discord and use the call only to finalize things. But we'll know a bit better over the next couple of weeks how things are looking. But the aim would be that in January Gordy gets forked. I see. Terrence, you have a pr in the chat about an execution API change. Do you want to bring that up now or is there anything else that people feel is more urgent with regards to the.
00:20:11.438 - 00:20:21.180, Speaker C: I have one more question on the gorely stuff. Is the node distribution similar to mainnet? Not necessarily the validator, but the node.
00:20:24.080 - 00:20:29.600, Speaker A: Yeah, I'm just looking at this Ethernet crawler.
00:20:34.260 - 00:20:35.772, Speaker C: If it were way out of whack.
00:20:35.836 - 00:20:36.949, Speaker A: It doesn't feel like not going to.
00:20:36.949 - 00:20:40.948, Speaker C: Necessarily perform like, but it doesn't feel crazy different.
00:20:41.034 - 00:20:46.710, Speaker A: But this is for El, not Cl, so I don't know what the Cl looks like.
00:20:47.640 - 00:20:49.316, Speaker C: Okay. And then if you can find some.
00:20:49.338 - 00:21:27.294, Speaker A: Data in terms of the countries, it also feels roughly similar. It feels like of the three testnets, by far the closest. Anything else on Gordy or Shadowforks? Okay, Terrence. Yeah. You want to talk about the execution APIs, right?
00:21:27.412 - 00:22:18.746, Speaker D: So little background in Cancun, when you try to get a payload from the Cl to the El, there is this optional parameter, it's called override, meaning that as an El client, you can trigger Cl client by telling there if there's some sort of censorship going on on the network. So censorship is a very subjective term here. This is El client may have different design implementation on how to observe this sort of censorship. Like for example, like a transaction that's been paying higher than normal priority fee, but it has been stuck in the manpool for a while and the builders are not including blah blah blah.
00:22:18.778 - 00:22:19.360, Speaker A: Right?
00:22:20.210 - 00:23:42.694, Speaker D: So since this is available in Cancun, I wonder if any eO client has looked into this and implementing this sort of override feature. And the reason I am asking this is that because we have seen over the last few weeks, right, there are staking pools. They are actively discussing delaying block propagation just to extract more mev. And this may become a problem once we have blob transaction because blob transaction also delay the block gossip, right? So imagine today we have blob transaction that is in the main pool, but it doesn't really make that much money because there are cheap block transaction they are borrowing. And then as a staking pool I will have the decision whether to include block transaction or whether to not include it instead of just wait a little bit longer and get more mvv. So there's some sort of this type of game that the staking pool can play and it's not ideal. So I think having some sort of censorship override with some sort of respect on block transaction could be fairly useful.
00:23:42.694 - 00:24:02.980, Speaker D: So I wonder if any client team has thinking about this or thinking about implementing like so like client has comment saying that Mario implemented it but then they don't think they will merge it in the near future. Yeah, I wonder if anyone has any more feedback comments on this?
00:24:08.580 - 00:24:11.584, Speaker B: We haven't implemented it yet in other.
00:24:11.622 - 00:24:12.400, Speaker A: Mine.
00:24:15.340 - 00:24:17.850, Speaker E: Base also has not implemented it yet.
00:24:22.180 - 00:24:23.330, Speaker B: Got it, thanks.
00:24:24.020 - 00:24:56.350, Speaker D: I can also add another comment on Discord later just to iterate what I said. Yeah, I still think it's very important to have this early on because 95% of the blocks today are built by builders and builder has a lot of power on just including blob transaction and choosing not to include it. And I suspect in the beginning blob transactions are going to be very cheap such that the builders may not even care. So there may be some weird information asymmetry there.
00:24:57.120 - 00:25:24.260, Speaker C: Yeah, and I guess to be fair, as Roman mentioned on the local setup, if a pool really wants to be playing these games, they can just override. But I guess also if we do see this, the equilibrium of what the priority fee in the normal case for l two s is probably going to shift to much higher. So hopefully you can find some sort of balance. But I agree, some of these dynamics are pretty concerning.
00:25:29.570 - 00:25:30.590, Speaker A: Boris.
00:25:31.970 - 00:26:23.390, Speaker F: So I was thinking along the lines of what Danny just said, that it just might be market this block transactions will have to pay enough so as to offset whatever the fee is required so that the proposer actually includes them. Because also it's something that risks their block to be reorganized more. But I think it's different than with usual transactions because this is now playing with these pools that are either publicly or privately delaying block production as much as possible. So now blocks not only need to compete with fees, but they also need to compete with that delay itself and all of the MeV that you can get out of delaying your block. This is a market that I think this wasn't prevented, this wasn't thought of when the fee mechanism for blops was designed.
00:26:31.140 - 00:26:41.540, Speaker D: So, yeah, I think this is true. But blobs can use the same chip as normal transactions to compensate for the increased offering risk.
00:26:55.050 - 00:27:21.010, Speaker A: Okay, I guess what's the best place to continue conversation on this? Obviously the pr is like closed, but is there? Okay, yeah, Terrence will bring it up on discord. Yeah. Which channel do you think makes sense? Or was there already a conversation on the discord in one of the channels? Okay, we'll use ACD.
00:27:22.470 - 00:27:38.280, Speaker C: There's also a good post recently by Mike and Casper on e three search. Kind of digging deeper into the time game stuff going on in general, if people want to take a look there to put some more brain power towards it.
00:27:43.370 - 00:28:37.600, Speaker A: Awesome. Yeah. Thanks for bringing this up, Terrence. Any final thoughts? Comments? Okay, anything else on Dencoon in general? Okay, if not, I have just three quick updates on the meta eep stuff that we talked about last week, but we have a meta eep draft for Denkun. I forget why, but there's some reason why it's not merged yet. I think it just needs an EIP editor to merge it. And then we didn't use meta Eips for a couple of years.
00:28:37.600 - 00:29:50.498, Speaker A: So instead of backfilling all of the meta Eips, I created one to reference all of the specs for the previous hard forks so that it doesn't look like the meta Eips were always there. But it's saying, like, for this chunk of forks where we didn't use meta EIps, here's where you can find the canonical specs. It includes both the El and Cl Forks. So if people want to leave comments on that, I'm hoping to get it merged soon as well. And then lastly, so I created this thread on eth magicians to start discussing the scope for the next network upgrade. I know there was also an issue on the Cl side on GitHub where people could post suggestions, but there started to be some conversation on it. And I think as we start wrapping up Dan Kuhn, then we can spend more and more time thinking about how to scope and prioritize things for the next upgrade.
00:29:50.498 - 00:30:54.090, Speaker A: And I think one thing that's probably worth it for client teams to think through now is like are there proposals that would cause the upgrade to be coupled or decoupled across the El and Cl? So the past couple forks were obviously coupled because we shipped stuff that sort of required it. But there's at least a possibility, I think for the next upgrade that this is not the case. And so it's worth thinking through that, like is this something we want to explore? And if there are things that couple, how important are they and do they risk delaying a bunch of other stuff? Yeah, I think that's coupling sort of happened by default the last three times, but we should, yeah. El triggerable exits is probably the only one that I'm aware that requires the fork to be coupled.
00:30:55.710 - 00:30:56.122, Speaker B: Okay.
00:30:56.176 - 00:31:01.100, Speaker A: It seems okay. There's more popping up in the chat. Okay.
00:31:01.870 - 00:31:20.660, Speaker C: Yeah, I'd really love to see wherever Verkel goes to not be coupled because simultaneously I think we'll probably work on some sort of data availability sampling upgrade. And those are very independent. But obviously then there's a lot of these small things that might be.
00:31:21.430 - 00:32:31.500, Speaker A: Yeah. And one thing maybe to think through is also how early or late in the process do we want to couple things like say that we did vertical and das and they're decoupled and then we choose to add el triggered exits. Can we couple stuff later in the process? What does that look like? Yeah, I don't know. Any other teams or people have thoughts generally about approaching the next fork or how should we think about that? I don't think we should start discussing specific eips on this call, but yeah, at a higher level. Any thoughts? And I guess maybe. Okay, so last question then. These past couple calls have ended a bit on the earlier side.
00:32:31.500 - 00:33:13.140, Speaker A: So would people want to start discussing the next fork in more details, like in the next couple of weeks before the holidays? Or do people prefer to table that until early next year and sort of stay focused on the. Oh, like clients wants to get into it now. We can do that too. Yeah. Okay. It seems only like client wants to discuss Prague now. I'm down.
00:33:13.140 - 00:33:34.404, Speaker A: Okay. So yeah, I think we should probably do most of, oh after the holidays I'm fine, giving people a couple of minutes now if they want to bring some stuff up. But POTUS, I do agree that we.
00:33:34.442 - 00:34:05.170, Speaker F: Should probably defer this to after the holidays, but I very much find it unlikely that this is going to be decoupled, or at least on the Cl side. There are some kind of urgent matters with the growing validator set. So I think the few prs or ideas that are being discussed on how to lower the validator slice or to reuse indices or I mean, to set up the stage so that we can actually lower the validator size, we should start working towards that.
00:34:06.980 - 00:34:29.764, Speaker C: Yeah, to be fair, on 7002 or not on the Max CV proposal, you could potentially introduce additional CL messages to counteract the ability for needing the EL, but there's not like 100% I'm happy.
00:34:29.802 - 00:34:39.176, Speaker F: With the CL only fork, it's just that I'm sure there's going to be El changes that are also important and to be careful.
00:34:39.208 - 00:34:58.880, Speaker A: I wasn't trying to push necessarily for a decoupled one, but I just want us to make that like a conscious decision, not just default decoupling, because we've gotten used to doing that the past couple of Guillaume.
00:35:01.300 - 00:35:24.200, Speaker I: Yeah, just regarding the coupling, actually Vercol is mostly an el thing, but since we're putting we're distributing witnesses, and so far the decision we okay, it's not decision, but the approach we have made was to pass these proofs and witnesses via the CL so it would actually be coupled.
00:35:30.680 - 00:35:33.990, Speaker C: Is that p to p only, or is that a consensus change as well?
00:35:39.160 - 00:36:01.070, Speaker I: No, it's an execution engine change and then a peer to peer. Actually, I don't know how Mac and Gajinder implemented that in the state, so I don't think it's consensus change. No, but I might be wrong. It depends on the implementation detail. I'm not.
00:36:10.720 - 00:36:12.460, Speaker A: It. Justin.
00:36:16.820 - 00:36:25.010, Speaker E: I posted this comment in the chat has the ship already sailed on using meta Eips for hard fork scheduling? Are we all firm on that?
00:36:28.200 - 00:36:31.110, Speaker A: What is the argument against it?
00:36:31.880 - 00:36:58.860, Speaker E: Well, I just wanted to again, I apologize for not being here last session, but it seemed a little counterintuitive since we just spent a lot of time deciding ERCs and EIPs should not be in the same repo because they have different audiences. And so I was wondering, why not just simply create a separate EIP for hard forks and scheduling? I'm sorry, not an EIP, a separate repository rather for hard forks and scheduling.
00:37:00.960 - 00:37:46.430, Speaker A: So maybe my practical answer to that will be because there's just one of those that happens every year or maybe two. And it's kind of weird to have this separate repo for like ten eips. That said, I'm not completely, I'm not super against it. I do think we want them to be closely coupled to eips. Where they were before was randomly throughout the specs repo, and that's kind of hard for people to find versus like meta Eips. You can just go to Eips ethereum.org and search for London and it pops up there.
00:37:46.430 - 00:38:05.360, Speaker A: The main argument I see in favor of metaips is just like, they're easier to reference whether or not they live in a separate GitHub. Repo is maybe not super coordinated with that, but yeah, I think that's the argument for, by default, having them in the same repo.
00:38:06.020 - 00:38:14.244, Speaker E: Okay, so traditionally when I think of meta blank, it's usually specs about specs. For instance, eip one.
00:38:14.282 - 00:38:14.484, Speaker C: Right.
00:38:14.522 - 00:38:24.872, Speaker E: This is where we define the EIP process and how future eips are to be written, et cetera, et cetera. So when we say meta Eips, that is more along the lines of what I think.
00:38:24.926 - 00:38:28.584, Speaker A: Yeah. So we've been talking about maybe renaming them, which I'm totally fine with.
00:38:28.622 - 00:38:42.188, Speaker C: But to be fair, it is kind of a meta spec because it says this specification for this upgrade is these five specifications or something like that. So by referencing an EIP, it is meta. But I don't care about the.
00:38:42.274 - 00:39:12.660, Speaker A: Yeah, yeah. I care more about the proximity than the name. And yeah, some EIP editors have made the same argument as you, Justin. So if we decide to rename meta Eips tok descriptor, totally, whatever name we come up with, I think is fine. But I think not having, first of all, having a list of what's in the hard fork, I think is really important. And we didn't quite have that. The list ended up being the blog ethereum.org
00:39:12.660 - 00:39:28.652, Speaker A: article, which is not great, 100%, and then having that list be close to the actual eiPs. But I agree, the name is potentially not great. We can bike shed this off the call.
00:39:28.786 - 00:39:36.064, Speaker E: Yeah, I'm going to drop it. It's not so much about the name, it's about the repo. That's all right.
00:39:36.102 - 00:39:36.690, Speaker A: Thanks.
00:39:40.000 - 00:40:00.610, Speaker G: Just to chime in with respect to virkal being consensus fork on cl side. Yes, it will be, because execution witness is now part of the execution header, which will basically require us to have consensus on.
00:40:07.280 - 00:40:58.030, Speaker C: Okay. And I guess to be clear and to be fair, some of these things are more like insert the new field into the data structure rather than dealing with a feature beyond that. And so you're right, in a lot of decoupled forks, there might still be some of that process upgrade that needs to happen without a ton of effort happen. So we should at least keep that in mind when we're talking about decoupling, because I could have still imagined brokele could, quote, be decoupled, meaning the consensus layer. Make sure to give them a branch with this extra field in it, which could take not very long for testing, and then kind of come in together and make sure everything's locked tight in the end.
00:40:59.760 - 00:41:20.160, Speaker G: That's correct, because state transition has not changed on the CL side because of execution witness being added. Also, there could be a different mechanism in which execution witness can be part of blobs. So basically it doesn't even need to have that kind of upgrade.
00:41:29.150 - 00:41:31.660, Speaker C: Yeah, and I guess just another point.
00:41:36.750 - 00:41:38.938, Speaker A: If we had a data availability dominated.
00:41:39.034 - 00:41:49.390, Speaker C: Consensus layer fork, there'd probably be an upgrade to the data gas limit, but on the execution layer. But I'd still largely call that decoupled.
00:42:05.570 - 00:43:42.930, Speaker A: Okay, based on this conversation, I think for Prague and Electra, what we should probably do is then have people, I guess, signal now to people that if they want something, consider it's like the right time to start raising it, and that over the next month I suspect people will bring up a bunch of proposals. If client teams can keep an eye on both the eth magicians and GitHub thread, then we can sort of kick off the calls. Next year's getting a feeling from the client teams about all the different proposals, their relative value, and kind of go from there. That seems probably more productive than trying to start discussing random eips now. And I think in parallel to that, one thing we discussed in the context of devconnect is trying to do some sessions on specific topics remotely after devconnect to get people up to speed on stuff. So if things like vertical or potentially like a das fork on the CL are being considered, and they're pretty significant changes, it might make sense to organize a session for at least the two of those. And I don't know if there's something else, because I suspect those will take up more time to go over and have a much bigger degree of discussion than sort of a random small EIP.
00:43:42.930 - 00:44:32.750, Speaker A: Does that seem reasonable to people? No objections. One thumbs up, I guess. Yeah, we can try to roll with that. Okay, anything else anyone wanted to discuss before we wrap up today? Okay. If not, well, yeah, we can close out here. Thank you, everyone. And we will see you on the testing call on Monday and then ACBC next Thursday.
00:44:35.250 - 00:44:36.000, Speaker E: Thanks.
00:44:36.770 - 00:44:37.630, Speaker C: Bye.
00:44:37.970 - 00:44:41.660, Speaker A: Thank you. Bye. Thank you.
