00:00:00.330 - 00:00:15.230, Speaker A: You. Great. Hello. Welcome, everyone, to roll call number three, which is issue number nine. Four. Four in the PMS repo link in the chat. Yeah.
00:00:15.230 - 00:01:11.760, Speaker A: Great. So today, I think the main thing I want to touch on is wait for four readiness. We had a breakout on this, and the consensus there seem to be that a lot of people still have a lot of work to be done here, but things are generally trending positive, and then it's come up vicente on ACD on the l one side with people just wanting to figure out where all of this is. So if any of the teams on here who have been testing, I'd love to hear sort of where you are in this process with deploying blobs on the various void for four test nets. Zach.
00:01:13.460 - 00:02:07.428, Speaker B: Hey, everyone. Zach from ZK sync. So as far as our, I guess, testing efforts right now, we're at the point where we've been running it locally with pretty much, I would say, no issues. One maybe minor inconvenience. Our plan is to roll out to Sepolia today, but other than that, it seems like we're pretty much ready to go, especially with the main net release being solidified. One thing that I was actually curious about, and if this isn't the right meeting, let me know on the execution layer. Clients, I saw that with Geth, it only allows for one.
00:02:07.428 - 00:02:55.220, Speaker B: If a blob transaction is in the mempool, that's the only transaction that can live in there. This kind of makes us not only does it deviate from the behavior of a standard transaction, but it makes us do some additional logic. As in the past, we used to fire off a bunch of transactions, namely for a given batch, we would send over the commit, prove, and execute around the same time. So I'm curious if this is other people are kind of experiencing the same issues as well as if this is a temporary solution until full dank sharding or the number of blobs increases, or if this is something that we should expect in the long term.
00:03:04.590 - 00:03:12.474, Speaker A: Thanks. I actually don't have an answer to that. Has anyone else encountered a similar issue with only being able to play one.
00:03:12.512 - 00:03:30.900, Speaker C: Blob a block on Tyco side? I don't have anything to report, mostly because it's a chinese new year holidays, and we'll start integrating EIP 4844 starting from next week.
00:03:37.100 - 00:03:55.788, Speaker A: Okay, does that mean that, what does that mean in terms of timeline for you guys? In terms of shipping, are you going to need a few weeks after 4844 goes live before you guys are ready to deploy globs just an expectation.
00:03:55.884 - 00:03:58.976, Speaker C: I'm talking about Testnet. We're not even in mainnet yet.
00:03:59.158 - 00:03:59.952, Speaker A: Yeah, sure.
00:04:00.006 - 00:04:06.988, Speaker C: We'll be eip four compatible as soon as we launch.
00:04:07.164 - 00:04:13.900, Speaker A: Okay, great. Any other teams been testing?
00:04:20.820 - 00:05:02.220, Speaker D: Hey, I'm Derek from off chain labs. I can give a quick update on our side for the arbitrage stack. So eip four four four. Testing continues. And the formal Dow proposal on chain was submitted yesterday. So that puts us on track for, I think just after midnight Eastern on March 13 to activate on midnight, assuming the Dow process goes smoothly. Nothing major to flag on the testing front, but good thing to know, maybe other teams have experienced this, is that we've been monitoring and in talks with a few RPC providers in terms of beacon chain RPC availability as well as support for archived blobs.
00:05:02.220 - 00:05:21.190, Speaker D: So this is for syncing of nodes. So we're just kind of monitoring and working with infrastructure providers there. As for arbitram sepolia, we will probably activate it sometime next week, but that date isn't confirmed yet. But everything's on track.
00:05:27.060 - 00:06:07.470, Speaker A: Great. That sounds pretty interesting. Yeah, I think the RPCs are one of the things that always take a little bit of a while to catch up from the tooling standpoint. But given the femoral nature of blobs, that's really important to get right. Anyone else? Anskar?
00:06:08.770 - 00:07:45.230, Speaker E: Yeah, so I just want to briefly mention, because Zach, touch, touch, touch touched on it, that I'm not super familiar with the current situation in bit in guest, but just to flag it as we kind of move in the future, hopefully, hopefully on a kind of attractive timeline, we move beyond four eight four into kind of higher scale sampling based kind of throughput for data availability. Basically it's a bit of an open question in general, how to basically handle the mempool side of things. One of the reasons why there's special logic for mempool, and again, I'm not super focused where this ended up, but at least why that was a conversation in the first place is that because of the big size of individual blob transactions, it's much more of a potential DOS vector. And so nodes in general just have to be much more careful with how they handle these transactions. And so as we kind of keep scaling, at some point, basically the peer to peer mem pool will become less and less of a good instrument for getting basically blob transactions to build it. And I think we're also going to look into kind of different alternative architectures for this. I think the ramp will always at the very least kind of be a good kind of fallback and kind of censorship resistance vehicle.
00:07:45.230 - 00:08:16.470, Speaker E: But basically what I'm trying to say is just that it might be a good idea for people in general to already kind of start thinking about in the long term as we will have more and more blobs. Like what are the best ways to reliably get them from people that want to send them to builders. I think that is going to be a more relevant question in the long term.
00:08:22.940 - 00:09:16.310, Speaker B: Yeah, so on that note, I totally get the restriction on the blob mem pool for potential dos reasons. And so having a single only blob transaction in there somewhat makes sense, but I guess is it a completely separate mempool and that's why the rules are not just restrictive to you can only have one blob transaction in the mempool, or is it not a different mempool and it's just being overly restrictive to normal transactions as well? Also, again, happy to redirect these questions to a more appropriate forum or meeting.
00:09:20.880 - 00:09:56.490, Speaker E: Yeah, ultimately I definitely am not the right person to answer this. I think kind of reaching out to client teams directly is probably the right way to go. But I do know that yes, basically, most times at least I know this from Geth, that basically four logic in the mempool is really mostly separate from the normal mempool. So it really is treated basically as two separate mempools as far as my understanding cost.
00:10:13.360 - 00:11:15.250, Speaker A: Okay, any other client want to touch on how their void four four testing is going before we jump onto the next topic? Going once, going twice. Okay, next topic. So as I'm sure many of you know, I was one of the main people running the KZG ceremony, and so I submitted it into the Ethereum specs repo sort of the output of the ceremony. And I had a bug in my code that produced the wrong ceremony output. So for all of about one day we had the correct version. Then I made some other changes and I put in the incorrect version, and I have restored the correct version. This was just after our previous roll call.
00:11:15.250 - 00:11:25.940, Speaker A: Roll call number two. Anyway, point being that please just check the version of the ceremony file that you have. The output of the ceremony file.
00:11:27.640 - 00:11:28.004, Speaker E: Is.
00:11:28.042 - 00:11:49.630, Speaker A: The latest one, as per the consensus specs. Whatever KZG library you're using. I believe all the libraries that have this hard coded already have the correct version in and have had for a very long time. I don't think they ever imported the incorrect version, but just flagging this to make sure that everyone is on.
00:11:51.440 - 00:11:51.804, Speaker E: This.
00:11:51.842 - 00:12:27.140, Speaker A: Version of the ceremony output. And to please that the instructions are in the link I dropped to verify it yourself, as this is sort of like one of the key security aspects of this whole change. Okay. Any questions on this topic? No? Okay, great. Let's see.
00:12:30.010 - 00:12:30.760, Speaker E: Next.
00:12:31.690 - 00:12:53.210, Speaker A: Okay, my agenda for some reason keeps seeming to change here. I would like to do a quick recap on the breakout calls that have been happening. So I'll hand that over to Anskar just to touch on sort of what happened on the last few breakout calls.
00:12:54.530 - 00:14:13.462, Speaker E: Okay, yeah, sure. So basically for those that maybe have not joined these record calls since last roll call, we've had three kind of record calls. Also on the Wednesdays, kind of on the off Wednesdays, physically between the last one, this one touching on different kind of topics. One of them was on, first of all, readiness, which I guess we kind of already discussed on this call as well. The second one on fee markets and the third one on kind of pre compiles. I think it's probably if you didn't listen to them and are curious, you can find the recordings online basically on fee markets. We talked about both kind of the potential for kind of general repricing, but also specifically about a kind of multidimensional, basically there one kind of pass through pricing and the idea about it for a new transaction type there.
00:14:13.462 - 00:15:10.870, Speaker E: So there could be specific follow up at a point in the future. And then on the pre compiled side, we basically mostly talked about the kind of challenges with kind of shipping a new ip and where we at there. So definitely recommend go listening to the recordings, if that's interesting to you and you didn't catch those. A question that I had now would be for the people here, maybe that were part of these calls, just basically a little bit of general feedback. Did people generally like these? Does it make sense in the future to continue them? What kind of frequency would you like to see them at? Yeah, maybe a little bit of feedback for whether this format makes sense to continue would be useful.
00:15:39.000 - 00:16:07.170, Speaker A: Yes. My personal feelings on them is that they've been sort of awful to touch on some of the topics that we haven't been able to touch on so much. But I feel like the frequency has been a bit high. Part of that I think is just the sort of starting out in testing. But the idea of not having this roll call be something where there are too many calls, although the breakouts are optional, but to have it a bit less frequent I think would be a helpful thing moving forward.
00:16:16.330 - 00:17:05.094, Speaker E: Yeah, that seems reasonable. I'll be happy. Maybe someone else kind of has feedback in the local channel. We can keep talking about it. And also if you have specific topics you would want to see a breakout, we can help with that as well. My understanding is we have Justin on the call who wanted to, after this, also kind of briefly talk about this topic of shed sequencing pre confirmations on this call. But my understanding is, and maybe Carl and or Justin, correct me, but that there was also an interest in having a breakout session two weeks from now to go into more detail.
00:17:05.094 - 00:17:06.440, Speaker E: Is that right?
00:17:07.850 - 00:17:08.774, Speaker F: This is my side.
00:17:08.812 - 00:17:10.620, Speaker A: Yes, that'd be great.
00:17:11.310 - 00:17:11.674, Speaker E: Okay.
00:17:11.712 - 00:17:11.866, Speaker A: Yeah.
00:17:11.888 - 00:18:03.126, Speaker E: So then basically we would already have one specific breakout to announce, basically. So two weeks from today, again, same time we will have a breakout onshore sequencing and pre confirmation. So if you like, or if you're interested in what you'll hear in a second from Justin here is like basically a teaser. Then two weeks from now will be the time to go really into detail. And basically that will be a breakout session where we kind of, with Justin, we have a guest kind of leading these. So this is in the future also a format that we want to basically experiment more with, to basically bring in experts on different topics where it makes sense and basically have them lead these breakout calls. So for now, that's going to be the only breakout in between this roll call and the next two nouns.
00:18:03.126 - 00:18:23.870, Speaker E: And then there might be one more breakout after Justin's, depending on whether we can find a good topic. But we will announce that by then there will not be a breakout next week. That would be all for me on the topic of breakout.
00:18:24.290 - 00:18:39.218, Speaker A: Yeah. So then on that topic, Justin wanted to talk a little bit about what he's going to be talking about. So I'll hand over to Justin to sort of touch on, give short overview of what he'd like to talk about in his breakout. Perfect.
00:18:39.304 - 00:19:16.254, Speaker F: Thank you, Carl and Anskar. So yeah, this is a teaser on what I'm calling the Ethereum shared sequencer. You can also think of it as the Ethereum sequencing and pre confirmation layer. A lot of us here know that Ethereum can be used as shared base availability. It can also be used as shared settlement for roll ups. But it turns out that it can also be used as shared sequencing with pre confirmations. Now, in terms of research, one of the exciting things that was discovered is that we can add this pre confirmation layer without a hard fork.
00:19:16.254 - 00:19:24.846, Speaker F: And there's, I guess, a design that can be presented in the roll call in a couple of weeks on February.
00:19:24.878 - 00:19:25.730, Speaker A: 20 eigth.
00:19:27.770 - 00:20:40.598, Speaker F: If you're especially interested in this topic, there's also a dedicated call that's happening on Friday, this Friday the 16th, at 02:00 p.m.. UTC. So there will be a presentation there with all sorts of MEV infrastructure people, researchers, roll up, people as well as investors. And the goal of that call on Friday is a coordination call to try and get everyone aligned in this multiparty dance that needs to happen for shared sequencing to be available. Now. In terms of why shared sequencing, I believe is valuable is because it unlocks at synchronous composability, which if users want, that can provide a lot of network effects to Ethereum and to the rollups in general. And I guess I'm a little opinionated in terms of what I think makes a good sequencer a good shared sequencer, and some of the properties that I value are security, credible neutrality, and network effects.
00:20:40.598 - 00:21:23.046, Speaker F: And I would argue that FM kind of maximizes all three of these things. The FM sequencer is maximally secure, maximally credibly neutral, so that all the rollups and their competitors can feel comfortable opting into it. And it also has the most network effects today in the sense that it has half a trillion dollars of assets, ERC 20s, NFTs, and ETH, which is a couple of orders of magnitude larger than the next largest sequencer. So yeah, that's basically the teaser. I think I might have 1 minute for a burning question, but otherwise that's.
00:21:23.078 - 00:21:23.980, Speaker A: All from me.
00:21:34.700 - 00:21:38.040, Speaker F: If you want to be part of the call on Friday.
00:21:45.340 - 00:23:05.170, Speaker A: Okay, thank you, Justin, definitely, yeah, looking forward to hearing out the full breakout. And yeah, we'll see you on the Friday's call, I guess. Moving on to the next topic, which is rip seven 6114, exposing the full call stack to contracts within the EBM. I believe the authors are here and would like to pitch their rip for five minutes, maybe field some questions. Thanks, Carl. Yeah, we're here, Janner, did you want to talk through it real quick? Yeah, if you're speaking, we can't hear you. You're muted.
00:23:05.170 - 00:23:43.122, Speaker A: I see you're aware I can't unmute you from your end, but I have submitted a thing to ask you to unmute. Maybe leave and rejoin a second?
00:23:43.176 - 00:23:43.780, Speaker E: Yeah.
00:24:15.120 - 00:25:13.220, Speaker A: I guess while we're waiting for this, we can touch on the next topic very quickly, which is that we have a new rip editor, blush, which I think most people here are pretty familiar with from his contributions with the r one code pre compile. And yeah, he'll be helping Onscar and myself out in terms of just getting some stuff cleaned up on the repo and generally just helping make sure that the rip side stays nice and clean. So yeah, welcome Olash. Thank you. Chandler. Let's see if this works. Are you able to unmute now? I can't unmute you, but I can only request you to be unmuted.
00:25:13.220 - 00:26:03.750, Speaker A: Seems like this is a fun one. Andy, are you able to speak to any of this while we're waiting for Janet to rejoin or anyone else from this team? Carl, let's see if Janet can do it. I'm not an engineer, so I won't be able to. Janner. There you go. Sounds like we good works now.
00:26:04.360 - 00:26:52.324, Speaker G: Thank God. Yeah. Hi everyone, this is Janner from open Zeppelin. So we've been working on this new pre compile that exposes the call stack. And the main reason why we wanted to develop something like this is because we want to mitigate a potential attack pattern that is possible by using delegate call and proxies and having this kind of pre compile invisibility also levels the attackers with the protocols by giving information about the rest of the addresses that are involved. So I posted the forum link to the chat if anyone wants to have a look at the proposal or the discussions. We're really curious about your thoughts about this.
00:26:52.324 - 00:27:50.820, Speaker G: So I want to say there are two components of this improvement. One is the cold stack that is widely and theoretically known, but has never been implemented. And the second is a pre compiled interface that allows exposing this to EVM. So we propose a call stack that is really simple, that keeps track of the opcodes, addresses and the function signatures. And this call stack is only updated whenever any of call code, delegate call and static call upcodes are executed. So it's only these upcodes which manipulate the call stack. They push into this call stack when the frame enters and they pop from it when the frame exits.
00:27:50.820 - 00:28:57.976, Speaker G: And the second component, which is a pre compile, is merely provided as an interface. And it has an encoder that allows the contracts to call so that the call stack data could be retrieved in any point of execution. So in the proposal we're reasoning about these two different parts and also the gas costs of these two different parts as well. And we think that this is something that is fundamental to strengthen the screen on chain screening solutions and making on chain screening more transparent in terms of rules. And we believe this is the first step to implementing. And we believe that there is a second part of this screening which is checking the addresses that is provided by this pre compile but we don't necessarily think that this is the problem we want to solve in this proposal. We were only trying to present a way to give more visibility to the contracts about the call chain.
00:28:57.976 - 00:29:02.380, Speaker G: So I'm curious if you have anything to add. Andy?
00:29:08.320 - 00:29:19.970, Speaker H: Nothing to add for me, Janner, but I think I see Ariel from Spherex, who was involved in this too. I didn't know if wanted to give Ariel a chance to maybe add any comments he had.
00:29:23.220 - 00:29:26.704, Speaker A: Sure. Yeah.
00:29:26.742 - 00:30:04.960, Speaker I: In Spherex we are working on runtime prevention for malicious transactions, and this pre compile interface will let us have access to more features that will be able to decide better if a certain transaction is malicious or not. We have seen hackers getting more sophisticated with how they trying to conceal their efforts in exploding certain protocols, and this new ability allows us to expose them and prevent their malicious transactions from being finalized.
00:30:15.510 - 00:30:16.260, Speaker G: Yeah.
00:30:21.690 - 00:30:59.700, Speaker H: Has a very thorough security analysis of this been done yet as to how effective it'll be? Is it useful or will it always be useful? I mean, as someone who's been in Java for years, these stack analysis tools are how a lot of the various sandbox breakouts were caused because they were relying on that to ensure their security. So I'm a bit concerned that it's not going to be bulletproof. It may be useful as an additional tool, but I also have some more technical comments that I'll put in the magicians thread. It should be extended to create, and there are some specific solidity things. Is the magicians thread a more appropriate form for that?
00:31:08.300 - 00:31:16.910, Speaker G: Sorry, do you mean if this improvement has any security implications on the client implementations? Is that what you mean?
00:31:18.400 - 00:31:23.260, Speaker H: Are you sure that it can't be circumvented by just being even more stealthy?
00:31:24.560 - 00:32:27.580, Speaker G: Oh yeah, that's a good question. That's something we're always thinking, and attackers will always get smarter. This is something that probably fuels the cat and mouse game, since we're trying to increase the visibility. But we think that there is this once the contracts are flagged on chain, and that's one way to prevent the attackers on chain, and that means the contracts, protocol contracts, who would want to implement such checks would want to check these oracles, whitelists or blacklists, whatever reputation and risk scores. And once those are checked, they will be checked against the message sender, the caller contract. But it's possible for the message sender to hide its logic behind a proxy by using a delegate call to an actual logic contract. So the actual logic contract could be flagged, but the actual caller wouldn't be the logic contract because of the delegate call made by the proxy.
00:32:27.580 - 00:34:03.920, Speaker G: So we want to mitigate this problem, and we think that this is something the hackers will adopt once on chain prevention solutions become more popular, and we want to preemptively make this not possible, harder to make. So the second thing we're thinking is once the addresses, there can be more visibility on the addresses. Can there be another way of changing the code in the same transaction without giving visibility or anything that allows you to execute arbitrary code? And while we don't see a problem with the latter yet, we have a problem with the former. And so we think that before the contracts and interact, there should possibly a reasonable space until off chain detection mechanisms have develop an idea about these contracts, and those will be flagged on chain by the off chain detection. So we think that these kind of checks will be complementary to the precompile we are kind of right now proposing. And that means if attacks can be detected before they are executed, and they can be detected with high precision, then the on chain updates can be made in reasonable time, then attacks can be prevented.
00:34:04.740 - 00:34:11.300, Speaker H: Is it being done by address or by the selector?
00:34:13.960 - 00:34:39.790, Speaker G: So we mainly think the first way to do that is by address, because in detection we think that there is some hard resource intensive work to do. It is resource intensive for EVM. So doing detection off chain is much more powerful than trying to do it on chain. We can do a lot of more things than off chain as well.
00:34:41.600 - 00:34:41.964, Speaker A: Which.
00:34:42.002 - 00:35:13.130, Speaker G: Is why we mainly think that the addresses could be flagged on chain, and this solution could help combine this with that and implement a screening kind of solution if protocols opt into. But of course, since we're exposing the opcode and the function signatures, it could be possible for anyone to implement an on chain anomaly detection solution as so, which makes the prevention solution, the prevention logic, more transparent. Andreas, please.
00:35:16.990 - 00:35:19.494, Speaker A: Yeah, so what if the proxy deploys.
00:35:19.542 - 00:35:21.386, Speaker B: The attacking contract and the attack is.
00:35:21.408 - 00:35:22.890, Speaker A: Done from the constructor?
00:35:24.930 - 00:36:18.938, Speaker G: That's a good question. That's the kind of question we have been considering since early days of developing this proposal. So it looks like we are able to say part of the call stack implementation. Note down the addresses that are deployed as part of the same transaction, and then provide them as a second list that have been created. Create and create two, and those ones that are appear in the call stack. So once they are provided, one could then check if this deployed address, this is bigger than larger than zero, and then depending on that call, could be reverted. Thinking that one of the callers was deployed in the same transaction and it hasn't been scanned off chain.
00:36:18.938 - 00:37:14.590, Speaker G: But we think that it's possible to circumvent this by deploying the contract in a first transaction in the same block and then executing the attack in the second transaction in the same block. That would give us no visibility, and again, it would also still be very difficult if it were in different blocks even, because then the off chain detection would need to race with compete with the attacker to be faster with the update on chain. So we think that this is a problem to solve altogether with lists provided by external sources and written on chain. So any contract who would be securely be able to act would need to have some age on the chain. We're believing.
00:37:19.030 - 00:38:51.200, Speaker C: I've already mentioned this in the frame magicians, but from the point of view of ZK rollup this sounds hard to prove efficiently. And also from the point of view of EVM design in general, the people at Epsilon usually wants to have predictable size and compute costs, but this is dynamic because it depends on everything that has been done before and not just on the input. So I think this will be quite problematic on that front. And the other thing is that even in c, for example, which is the lowest of the highest level language, you don't manipulate a stack because when you use the instructions called long jump and set jump, it breaks memory management. Well, a lot of assumptions. So I think this will need lots of iring out before it's implementable. I'd say at least in a ZK roll up.
00:38:51.200 - 00:39:05.460, Speaker C: And the non determinism makes me think there is another attack vector just to DOS smart contracts that use this by having quite a large call stack, for example.
00:39:09.590 - 00:40:23.580, Speaker G: Mami, thank you. I've seen your feedback under the forum post, so thank you very much. We believe that we need to have more ZK conversations on how this could be adopted, but I actually tried to reply to your concerns in the forum. I'm not sure you have seen them. And what we're proposing as part of this proposal is a brand new call stack that hasn't existed, so it's not related to the stack in each frame that is used by the opcodes that are pushed to and popped from, and so nothing in the compiled codes can none of the user written custom code can actually manipulate this call stack, it's merely a representation of the execution itself. So ideally one could try to reconstruct the state of the call stack by just looking at the call trace, which I believe is already used by some of the ZKE evms I know a little bit about how scroll does it. I believe Tyco might be doing something similar, but I'm not sure.
00:40:23.580 - 00:41:25.310, Speaker G: So we believe that it could be maybe somehow made part of the tables used in circuit generation and proofs. So since it's manipulated by only certain upcodes and it's not manipulated by user code, and we think that testing this is pretty straightforward and the type of inputs are kind of really simple. And in the reference implementation we try to mimic how the other stack is implemented. And we want to propose this with a pre compiled interface because it's easier to evolve. Given all of these simple situation, do you still think it would be difficult to implement this in Zkavm? Or do you still believe that it's still very complex to implement even if it has this little level of complexity?
00:41:25.890 - 00:41:52.200, Speaker C: I will have to look at ZKE, EVM secret, all the call data, et cetera that you static call, but I'm pretty sure those are some of the biggest circuits that we have and those where we concentrate all the audit resources and bugs as well.
00:41:53.310 - 00:42:02.650, Speaker A: Also, how would you get that frame? You would have to do that as a truth table.
00:42:03.310 - 00:42:04.700, Speaker B: That's where I'm like.
00:42:06.610 - 00:42:20.130, Speaker A: Could be very large. Even if you use it as a private input, it would be very expensive just from the number of constraints that would generate.
00:42:22.470 - 00:43:06.030, Speaker G: Yeah, I'm not sure how that would be exactly used to generate it as I'm not very savvy about the topic, but I would say the values at the time of calling the pre compile might be more useful. So ideally I think one could construct a table by looking at the pre compiled interface calls and then try to determine by looking at the trace which of the call code delegate call static calls would be in the call stack at that time and then put it into the table. But yeah, again, this is something I'm imagining and I've done exactly if it would work, and we definitely need more suggestions and research on how this could be implemented.
00:43:12.810 - 00:43:58.440, Speaker A: Okay, thank you Jenna. I think it's a sort of good overview of where we are and some of the complexities of this rip. I think let's take more discussion to the forums. So that was it for the main topics, but I see that we have someone from the GEF team who's now joined us light client, the ZK sync people earlier mentioned that they were having issues submitting more than one blob per block. So is it possible for whoever was that raised that issue to just explain a little bit more what they're having? And I can see?
00:44:00.270 - 00:45:14.160, Speaker B: Yeah, sure. So the scenario that we were encountering, it wasn't one blob per block because we were able to post more than one blob. When we submit our blob transaction, if it's not included immediately, we're not able to submit any other transactions as we get address already reserved in the RPC response. And so I understand due to potential DOS situations, there might be restrictions on having multiple blob transactions, not multiple blobs. But my question there is if that's a short term restriction, or that's going to be long term, even as the number of blobs increases, and then if there is any particular reason that it's not just isolated to blob transactions themselves, but if there's a blob transaction in the Mempool 1559, or legacy transactions can exist as well.
00:45:20.170 - 00:46:03.020, Speaker A: Client, do you have any insight into that? I think address already reserved is supposed to mean that you're trying to send a transaction. So we have the different types of transaction pools now we have the normal transactions and the blob transactions pool. I think address reserved usually means that you have a pending transaction in a different pool. So if you have a legacy transaction pending, and then you try to submit a blob transaction, you would see address already reserved or vice versa. So if you have a blob transaction pending and you try to send legacy, it's going to say address already reserved, right?
00:46:03.170 - 00:46:08.392, Speaker B: Yeah. And that's the scenario that we're running into. And I was just curious.
00:46:08.456 - 00:46:13.650, Speaker A: You said blob transactions though, so are you sending different types of transactions from the same address?
00:46:14.100 - 00:46:35.110, Speaker B: Yeah. So basically we have three transactions. When it comes to a batch, we commit the batch, proof the batch, and execute the batch. Only committing is a 4844 transaction because that's kind of where our pub data is sent to. L one.
00:46:37.660 - 00:46:51.196, Speaker A: Probably going to change at any point. So I would recommend looking into submitting the non blob transactions under a different address. Okay.
00:46:51.378 - 00:47:07.232, Speaker B: Yeah, we played around with that. The downsides again, our downsides for that are anyone running our sequencer then needs to keep track of two addresses and infrastructure around that.
00:47:07.286 - 00:47:07.890, Speaker A: But.
00:47:10.260 - 00:47:29.240, Speaker B: Good to know that we should design around that rather than it being changed. And then I'm not sure if you have any insight. Is this something that all execution layer clients are going to be implementing or is this specific to geth?
00:47:32.120 - 00:48:13.120, Speaker A: I think this one might be specific to geth right now. But the mechanics of actually propagating the blob transactions across different clients, that's something that's being debated right now. Okay. I mean, it's possible it changes in the future. It's just that's the kind of higher level design for the different transaction pools for guests. And that's mainly because I don't think we found a good way to separate the concerns between the two pools. There's a lot of problems with allowing legacy transactions to bump off blob transactions.
00:48:13.120 - 00:49:10.260, Speaker A: And the code is kind of isolated in two different places. So it's not something that would change in the near future, but it's possible longer term if we come to a good way of allowing that to happen. Okay, cool, thanks. That was very helpful to understand and to clarify. And then I think that brings us to the end of the agenda. Does anyone else have any points they would like to discuss, just as a more general point? Okay, I'm going to take those. And no, then.
00:49:10.260 - 00:49:37.920, Speaker A: Well, thanks, everyone, for attending. This has been a fun call. Thanks. Some good discussions that were had. The next call, we'll have the breakout on the 20 eigth of Feb. And then the next call will be on the day of launch for four, wait, for four. So that'll be fun to see.
00:49:37.920 - 00:49:53.842, Speaker A: So it'll be definitely an exciting one. See you all on the 13th. Thank you, Carl. Thanks, Likeland.
00:49:53.906 - 00:49:54.680, Speaker B: Thank you.
00:49:55.610 - 00:49:56.566, Speaker G: Thank you.
00:49:56.748 - 00:49:57.942, Speaker E: Thanks. Bye, everyone.
00:49:58.076 - 00:49:59.730, Speaker A: Thanks. Bye.
