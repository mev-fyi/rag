00:02:23.468 - 00:03:13.866, Speaker A: Welcome, everyone, to acde number one seven four. So we'll cover Dan Kuhn today. Some updates on the Gordy Fork client updates. There's also some RPC discussion that people wanted to bring up. Then aside from that, Guillaume has a vertical EIP to discuss. And then given it's devconnect next week and us thanksgiving after that, it makes sense to figure out which calls we want to have in the next couple of weeks and whether we want to cancel any of them, I guess, to start. So on the Gordy shadow fork, Perry, I believe you shared this analysis.
00:03:13.866 - 00:03:17.000, Speaker A: Do you want to give us a quick walkthrough of how things went?
00:03:18.330 - 00:03:19.078, Speaker B: Yeah.
00:03:19.244 - 00:03:54.378, Speaker C: So we had a girly shadow fork earlier this week. We basically synced up all the nodes to girly head, then started a new beacon chain and forked off a small section of the nodes. So roughly 21 nodes. And afterwards we set the Denkun fork, so we enabled Denkoon on girly. Since then, we've been running Zatu to collect all the metrics. So block blob, arrival times, et cetera, and the analysis is shared there. I think there's nothing insane that pops up.
00:03:54.378 - 00:04:27.260, Speaker C: There's a couple of regular spikes. We're still not 100% sure why, but besides that, block processing times and gossip times look really low. But that's also kind of expected, considering it's a really small network. The main takeaway, or the thing that we're yet to properly analyze is the amount of time it takes to build payloads, et cetera, doesn't look very high. And we're still getting organic transaction gossip from Gurley. So the question is just, is there anything else we should be trying on this network before we delete it?
00:04:32.050 - 00:04:35.790, Speaker A: Thanks. Anyone have anything they would like to see on the Shadowfork?
00:04:37.330 - 00:04:59.830, Speaker D: Yeah, I mean, my intuition is given we're doing the Cl networking rework and given that so far, things look stable and it's a good test that we probably have what we need and we're probably going to want to run another one, and potentially after we run another one, run another one at scale, but after we have the rework on the consensus layer.
00:05:00.570 - 00:05:05.014, Speaker A: Right. This is basically our baseline before that change comes in.
00:05:05.212 - 00:05:11.210, Speaker D: Yeah. So, I mean, as a baseline, it sounds like it works. I don't personally see more stuff to do right now.
00:05:11.280 - 00:05:28.990, Speaker A: So I guess for the next ones, do we see value in having it on Gordy again? Would we want to go straight to mainnet for the next shadow fork? Yeah, the teams have an opinion about that for DevOps.
00:05:35.950 - 00:06:01.126, Speaker C: I think mainly we prefer continuing girly shadowfork because most of the upgrade changes we're going to notice are peer to peer layer, and renting more nodes that can support the girly network is cheaper than renting more nodes for Mainnet. We're definitely going to do a mainnet shadow fork before we do the official main net fork, but the question is do we do that earlier or.
00:06:01.308 - 00:06:10.790, Speaker A: Okay, okay, in that case I think yeah, it probably makes sense to do Gordy again as a first one, and if there's no issues then move to Mainnet.
00:06:12.250 - 00:06:28.460, Speaker D: Yeah, I'd be an advocate for that. Doing a large gorely after doing a small gorely, then doing a small main net and doing a large main net. But again after we're past another couple of devnets with the fixed or at least one.
00:06:28.770 - 00:06:38.580, Speaker A: Yeah, I agree. I would do at least one devnet with the final spec changes before we do another shadow fork. Does anyone see the need to do another shadow fork before that?
00:06:43.080 - 00:06:45.380, Speaker D: Yeah, for DevOps.
00:06:47.320 - 00:06:49.316, Speaker A: Sorry, you broke up a bit there.
00:06:49.498 - 00:06:51.220, Speaker D: Actual underlying logic.
00:06:54.740 - 00:06:57.200, Speaker A: We heard about half of what you said, daddy.
00:07:00.020 - 00:07:01.072, Speaker D: How about now?
00:07:01.206 - 00:07:02.130, Speaker A: It's better.
00:07:04.980 - 00:07:11.190, Speaker D: Unless DevOps is just trying to strengthen DevOps tooling, I wouldn't continue to do them now.
00:07:14.820 - 00:07:17.170, Speaker C: We're used to testing in proud web man.
00:07:18.660 - 00:08:01.570, Speaker A: Nice. Okay, anything else on the shadow fork? Okay, if not. So the second thing on the agenda. So there was a pr to add the Balab gas price to east fee history. And then in the comments there, there was a proposal to also have like a single RPC method that returns all of the gas price related values. But maybe. Alexi, do you want to start walking us through your pr and then we can discuss sort of the alternative proposal that was posted in it?
00:08:03.380 - 00:08:22.132, Speaker D: Yeah, let me try to present. Sure. Just let me know if you see the screen.
00:08:22.266 - 00:08:25.640, Speaker A: Yeah, we see your editor and your terminal.
00:08:26.380 - 00:08:27.130, Speaker D: Okay.
00:08:27.820 - 00:08:29.960, Speaker A: And now we see your GitHub.
00:08:33.020 - 00:10:26.706, Speaker D: So let's start from description. What is blobgas and why do we need API for it? So blob gas is like a guess in addition to regular one, but which is used for blobs and is relevant for blob transactions for now. And to understand what price maximum price for pop gas is needed to be mentioned in a transaction, we need to provide some kind of estimation to include this data. After all transaction specifically the field max PPR blobbs. To do this we can query it through a new endpoint. There are two variations. Initially it's gas prices was provided and the form of this point it's a pretty simple method that returns several values.
00:10:26.706 - 00:12:24.750, Speaker D: Regular gas which can be used like a a regular gas cap for the transaction, blob gas and max priority which can be set as a tip for transaction which started working from eaP one five nine simpler version of it. If we just return blobgas price and it can be used when posting when sending blob transactions, I have no strong preference. I thought that the first approach could be more optimal because we return this data based on the same structures like box. And it could be one method because usually all these queries are queried together. But after all we can do just a batch call which can be done quite simple simply and may work too. Another update is for fixary in addition to regular gas ratio and base fees per such kind of gas. We could add the same for block gas and other fields do not need any change because their report are about priority fee.
00:12:24.750 - 00:12:33.090, Speaker D: That's the update I would like to propose and happy to hear feedback.
00:12:37.540 - 00:12:43.910, Speaker A: Thank you. Does anyone have thoughts preferences on the various options?
00:12:52.170 - 00:12:56.310, Speaker D: I would post the link with examples in the description.
00:13:11.120 - 00:13:19.790, Speaker A: Okay, so there's a question about whether this already exists in graphql. Does anyone know?
00:13:25.980 - 00:13:28.044, Speaker E: Pretty unlikely it's not.
00:13:28.162 - 00:13:28.830, Speaker A: Okay.
00:13:33.920 - 00:14:24.620, Speaker E: I mean, I shared some thoughts on the pr, but I generally would prefer to have ETh underscore blob gas price because that sort of follows the pattern that we currently have, unless there's like a strong reason to do the gas prices. So I was hoping someone might make an argument for that. But as it is, we don't really need to return gas price as far as I know, because that's kind of a legacy value. We really just need the max priority fee and the base fee and now the the blob gas price. So I would prefer to have it as a separate method and just expect people to batch the RPC request.
00:14:26.160 - 00:15:43.626, Speaker A: Got it. And I know. Yeah, so there were some comments on the agenda in the discord now for having a single call. Does anyone want to make the case for why batching them or why combining them in a single call is better? If not, it feels like we should at the very least add blob gas price. That seems kind of like a no brainer. It also seems like a no brainer to update eat fee history to also include the data there. How do people feel about merging those two and then the bundled method maybe doesn't have to be part of the spec, but if clients want to offer it, they obviously can just add extra JSON rpcs but at least on the JSON RPC spec, we'd have a single method for ETH blob gas price, we'd have the blob values as part of fee history, and then the rest is sort of an optional thing clients can do.
00:15:43.626 - 00:16:11.940, Speaker A: And there's an argument that clients should not do stuff that's not in the specs, which unfortunately we can't control what clients do. But I guess, yeah. Does anyone disagree with merging the single blob gas price method and the changes to fee history and not having the combined one? And worst case, we can also always do the combined one later if there's enough demand for it.
00:16:17.900 - 00:16:29.900, Speaker E: I'm not sure if there is any l two people or someone who was working with ethers who wanted to chime in, since they're the people who are using this method.
00:16:37.030 - 00:16:37.890, Speaker D: I guess not.
00:16:37.960 - 00:16:41.478, Speaker A: Yeah. Okay. So I guess, yeah, sorry, please.
00:16:41.644 - 00:16:51.402, Speaker E: I was going to say we can try and follow up with those folks asynchronously, but hopefully we'll move with the one method and updating fee history.
00:16:51.536 - 00:17:26.350, Speaker A: Yeah, okay. Anything else on that? Okay, if not, anything else on Denkun at all that wasn't included on the agenda. Okay then. Next up, guillaume, you wanted to discuss vertical proof verification precompile?
00:17:28.530 - 00:18:08.890, Speaker B: Yes, it's more like I was trying to bring people's attention to it. So it comes from a conversation I had with some people from open Zeppelin at first and then other people afterwards. Let me see if I can share my screen. Should work. So yeah, it's really about collecting input. The idea is simple. It's that because vertical entails a lot of complex math updating a lot of tools.
00:18:08.890 - 00:18:58.566, Speaker B: The idea that came up would be to use a proof verification pre compiled. So offer a pre compiled so that, for example, bridges or forgot the word for l two s. But yeah, l two bridges, let's call them. Or even application developer could verify vertical proofs so that they don't have to wait for all the libraries like open Zeplin like foundry to get up to speed to get up to speed and implement that in solidity. So it's just about adding a simple pre compile. So currently I suggested Xerox 21 as an address. But yeah, of course that can be changed.
00:18:58.566 - 00:19:57.994, Speaker B: And the idea is that it receives a chunk of memory that starts with a version byte and then just where the proof data can be found. Sorry, actually. So it first receives a version byte and then a location memory where the proof data can be found, along with its size and also the state root of the tree that the proof is proven against and what the pre compile does. So it's upgradable if we want to move on to a different proof system. But if version is zero, we use the standard polynomial commitment scheme, suggested polynomial commitment scheme based multi proof that was suggested by otherwise. So we return one if the proof verifies and zero otherwise. And there's also like the EIP also describes gas costs.
00:19:57.994 - 00:20:45.854, Speaker B: So I think the model is not really complete. For example, I realize it's missing some data to actually verifying the IP approved, but that will be updated. But the general idea is you have this constant point cost that corresponds to every time you need to deserialize a commitment. So it's paid for each commitment that gets deserialized, and then you need to evaluate the polynomial at some opening point. So you also pay that for each opening you have to do. Yeah, there's not much more to that. Clearly it's something that needs a lot more input, so that's why I want to bring your attention to it.
00:20:45.854 - 00:20:54.660, Speaker B: Of course, if there are questions, I'm happy to answer, but it's more like a call for core devs to pay attention to that.
00:21:00.720 - 00:21:04.060, Speaker A: There's a hand up by Defconnect Berlin.
00:21:06.560 - 00:21:07.630, Speaker D: Yeah, hello.
00:21:08.400 - 00:21:09.052, Speaker B: Hi.
00:21:09.186 - 00:21:13.836, Speaker D: This curve is over Bandersnatch, or, sorry, like this proving scheme would be over Bandersnatch.
00:21:13.868 - 00:21:16.348, Speaker B: The curve bender wagon.
00:21:16.524 - 00:21:25.830, Speaker D: Bender wagon. Okay, so I guess my question is how it relates to the BLS twelve curve, and if there's any interplay between those set of pre compiles and this one.
00:21:27.800 - 00:22:17.570, Speaker B: How does it relate? Yeah, I forgot which of the two fields is the one from DLS, but otherwise, my understanding is that it's a completely different curve. I'm not an EC elliptic curve expert, but my understanding is that they are different. And so you could potentially, I guess, use that BLS pre compile, but you would have to re implement all the proving yourself. All the proving yourself. That means that every application developer would have to reimplement that, and the goal of this contract is to save some time.
00:22:18.520 - 00:22:35.428, Speaker D: Right? Yeah. So we just have pre compiles for both then, because that's something I'd like to do for the next hard fork is the BLS twelve 381 curve. But yeah, it sounds like they're almost similar, but not quite exact. So they are completely different curve.
00:22:35.524 - 00:22:35.784, Speaker A: Yeah.
00:22:35.822 - 00:23:28.068, Speaker D: So no, the BLF precompile cannot verify the worker proofs. We could add a banditnatch pre compile. I think that would actually also make sense, probably. I think my preference is actually to have a state verification pre compiler. And the reason for that is that when we change the state in the future, we could make it backward compatible so that smart contracts can be built that verify a part of the state, and you can just upgrade the state commitment. And the smart contract could be completely the same. So it could be an immutable smart contract.
00:23:28.068 - 00:23:38.110, Speaker D: And I think that would be preferable, rather than having to have lots of smart contracts upgrade if we do need to change the state commitment in the future again.
00:23:50.020 - 00:23:53.590, Speaker A: Got it. Dano, you had your hand up. Yeah.
00:23:55.800 - 00:24:54.330, Speaker F: This might be a controversial take on this, but I think one of the great things about 4788 as a pre compile is that there is EVM code that says this is the pre compile. This is a canonical answer. So this kind of goes against kind of some of the idea that why wait for Openzeppelin to implement this in solidity? What if all pre compiles going forward that the canonical form is a form of solidity, or viper or fee or whatever, and that defines it in modulo gas? We're going to have to change a gas schedule on that. And that the clients could do a pre compile that would optimize that, but would truly be a pre compile at that point. There would be an uncompiled version that is the canonical, this is a reference implementation, this is the correct state. That would solve a lot of problems in devs who aren't phds in cryptography, trying to get the small little details of these curves correct when they don't necessarily have a large set of test cases to run against.
00:24:58.170 - 00:25:04.600, Speaker A: And so the implication here is you can implement any of them in solidity basically.
00:25:06.010 - 00:25:18.300, Speaker F: Right. And early on, when you don't have, when you're doing the next hard fork, the client, you just slide the EVM in, and then when you optimize it, you come check and you got some local code. It'll always be correct.
00:25:19.810 - 00:25:45.140, Speaker D: So this curve specifically will be used for worker trees anyway, right? So all clients will need, we have already created libraries for that. I think currently the general position is that client devs never touch the cryptography themselves. Well, unless they are really comfortable with it, and that we have ready made libraries. And this is already the case for Virkal in this.
00:25:46.310 - 00:25:54.710, Speaker F: Yeah, Verkel does kind of change this argument a bit, since it's going to have to be part of the client. But I think it's an idea I want to put out there for like cryptography.
00:25:55.130 - 00:25:59.370, Speaker D: The idea is, I was commenting on this particular EIP.
00:26:01.790 - 00:26:13.920, Speaker A: This is a bit off topic, but could we do that for BLS? Because I remember the whole debates around EVM Max were to allow us to build BLS on one.
00:26:15.250 - 00:26:24.750, Speaker F: I know the epsilon team at one hackathon did through EVM Max, the existing pre compilers. I don't know if they moved on to BLS twelve and using EVM Max.
00:26:24.910 - 00:26:47.850, Speaker A: Right, got it. Okay. And I guess back to just the original EIP. I assume there's an east magicians link. People can raise issues there, but were there any other questions about the EIP?
00:26:52.320 - 00:26:57.740, Speaker D: Are there specific implementations you can point to of the multiproof pcs?
00:26:59.360 - 00:27:29.530, Speaker B: The multiproof pcs itself? Yes. There mean one of them is go vertical, the other one is there's a Python implementation, reference implementation in fact, by Dancrad. And there's a rust one, I think. Oh yeah, there might be a nethermind one to a c sharp one by Nethermind. But yeah, there's no implementation of a pre compile itself, but it's going to be easy to couple together if that's what you need.
00:27:30.220 - 00:27:34.410, Speaker D: Yeah, just curious, maybe toss links to those repos and the ETH magician started.
00:27:36.700 - 00:28:33.506, Speaker A: Sure. Cool. Anything else on the pre compile? Okay, ipsen on. There's a comment around saying they've started implementing BLS and EVM Max, and after devconnect we'll know if it's possible. Anything else people wanted to chat about? I had the call scheduled, but if there's anything more substantive we can get to that first. If not, so next week there's dev connect. So next week we should have had a testing call on Monday and ACDC on Thursday.
00:28:33.506 - 00:28:50.650, Speaker A: Do people want to keep either of those? Yeah. Does anyone feel like there's value in having them, or should we wait until either the week after, just as a heads up? That'll be American Thanksgiving on the Thursday.
00:28:53.390 - 00:29:18.340, Speaker D: Yeah, I was thinking if we don't do the consensus layer call, I might ping on that Thursday the discord and just ask for a quick written update from the consensus layer teams on getting through the networking changes and surface any issues that maybe have arisen or unknowns at that point. And we could do a similar thing the week after on Wednesday if we don't do the Thanksgiving call us thanksgiving call.
00:29:20.630 - 00:29:27.030, Speaker A: We could also have the call on a Wednesday if we want to have one the week after and not skip two weeks in a row.
00:29:28.570 - 00:29:45.120, Speaker D: Well, and I guess one of the main things that will come up in that time period is if there are open issues or questions related to this update. So I'm also okay like doing the ping a couple of times and then scheduling an ad hoc call if we need it.
00:29:49.660 - 00:29:54.920, Speaker A: Okay, that works for me. Does anyone have Ben?
00:29:56.540 - 00:30:34.916, Speaker G: Yeah, just interested in signaling. So just delving back into history. When we were planning the capella and the Deneb Cancun upgrades, we committed. We broke the on pass about delivering both together by committing to doing 4844 as a fast follow on to capella with withdrawals. We're talking about June July time, which is amusing and ambitious in retrospect. But I'm concerned about the optics. These week here and week there add up to delay over time.
00:30:34.916 - 00:31:06.960, Speaker G: We've seen it time and time again and I know everyone is working super hard on delivering Denev Cancun because I'm inside the process, but from outside the process, canceling two core devs calls in a row doesn't. The optics don't look great to me and I'm starting to see things hotting up again on the need to deliver this thing. So that would be my case for keeping the calls. Obviously there's no point if nobody's going to be here, but I think generally that should be our default approach.
00:31:12.280 - 00:32:06.514, Speaker A: Yeah, I have less of a strong opinion on the dev connect one just because there will be a lot of people there, even though there's maybe a bit less than expected. I do like the idea of having your written update though from client teams so that we can still get a status update. And I'm happy to keep I'm not american so I'm happy to do the one on us thanksgiving the week after that, or to move it to Wednesday if that means more people show up. So either of those is fine for me. Yeah, and I don't know, maybe we can get teams to chime in on the discord if they have a preference. But yeah, I agree. Like skipping two might not be ideal.
00:32:06.514 - 00:32:51.710, Speaker A: So we can maybe do next week's async, get an update from teams, and then figure out the best day for the week after that. Do people think that makes sense? Okay, I'll post something in the discord, get some more feedback from client. Oh, now there's more and more support for having the calls. Ok, I'm fine either way. I don't know Danny, do you want to do a quick call next week?
00:32:53.600 - 00:33:10.496, Speaker D: I don't mind. I'm comfortable either way. I might not live stream it given my remote capabilities and set up. I might even take the call from a phone, but I think we can.
00:33:10.518 - 00:33:38.168, Speaker A: Probably get it going I think we can probably find someone to live stream it. Yeah. Okay, then let's keep both calls, I guess. Okay, let's default to this. Let's keep both calls. If like a whole team cannot make a call, then share a written update on the agenda and we can kind of point to that, at least on the call. But there will be a call and we'll try our best to live stream it.
00:33:38.168 - 00:33:45.980, Speaker A: But worst case we'll just upload the zoom recording later. And yeah, if your team can't make it, just share an update.
00:33:50.660 - 00:33:57.220, Speaker D: Yeah, when I toss up the agenda, I'll make a comment. If you can't attend to share an update in the agenda.
00:34:01.160 - 00:35:00.630, Speaker A: Sweet. Anything else? And I guess maybe the only one. I just wanted to check, do we want the testing call Monday as well, especially given these awkward devs have become shorter? I would lean towards canceling next Monday's testing call though, and just having ACDC next Thursday, Acde this Thursday after that, and then whichever next. I think the next testing call was scheduled for the Monday after that one, so November 27, but I think having just one sync call next week is probably sufficient. Does anyone think we should have the testing call as well on Monday? Okay, cool. So let's do that. We'll add a note in the agenda and we'll share something in the discord.
00:35:00.630 - 00:35:42.630, Speaker A: Thank you, Ben, for pushing us to stay on track. Anything else anyone wanted to discuss? Okay, well, thanks everyone. I will see you all at or most of you at DevConnect next week and then we'll see everyone on ACDC whether you're at DevConnect or not next Thursday. Talk to you all soon.
00:35:45.320 - 00:35:46.820, Speaker B: Thanks. Bye.
00:35:47.800 - 00:35:48.790, Speaker D: Thank you.
