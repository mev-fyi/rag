00:03:13.135 - 00:03:13.715, Speaker A: Hello.
00:03:17.575 - 00:03:18.315, Speaker B: Hello.
00:03:19.655 - 00:03:20.039, Speaker A: Hi.
00:03:20.087 - 00:03:20.715, Speaker C: Hi.
00:03:29.585 - 00:03:54.705, Speaker D: Everyone. Happy perfect square day. That's. That was the client update. Anyone wants to start? I see prison.
00:03:56.365 - 00:03:57.145, Speaker E: Yes.
00:03:57.565 - 00:05:12.245, Speaker F: Hi everybody. So right now about the Pure Dazevenet 2 prism is still offline. We had some issues with Grandin which published invalid data colon gossip and in this case and this invalid data colon gossip was also republished by I guess. I guess everybody but Lighthouse and in this case Prism was because message was invalid Prism was just disconnected disconnecting basically every peers because prism Pierce received 128 invalid gossip message from data codons. So we deactivated the pure disconnection because of that. And also right now, yes, Prism has very strange issue when grounding is in the mix. Discovery V5 is really really really slow when Prism tries to iterate over peers and it only happens when grounding is in the mix.
00:05:12.245 - 00:05:54.355, Speaker F: We have a workaround for that. It's not a real fix, but something which allow us to proceed to the devnet. And also Prism had some issues when syncing when the devnet is not finalizing, which is the case actually I have a fix for that. I tested it locally and locally I am able to to sync again. I tested it for full node. I want to test it for super node before sending every before. Yes, releasing a new version of Prism to the devnet.
00:05:54.355 - 00:05:59.715, Speaker F: Yep, pretty all for Prism.
00:06:03.175 - 00:06:10.615, Speaker D: Thank you. Manufacturing. Okay, then next one slow start.
00:06:15.275 - 00:08:02.975, Speaker G: Yeah. So as I mentioned before as well as in the channel so Lodestar was restarted because of some configuration issue of basically the persistent identity persist pair identity parameter was not provided which basically would load star nodes into syncing but then it could not none of the loads are not could sync. For the issues that I have already raised in the channel, I'm not sure whether other clients have gotten to see it, but I think few of the clients provided data column at some slot in which there was not a blob and then there was few client There were few clients who did not provide data columns which were supposed to be super nodes because I checked and as per the cat metadata they advertised themselves as super nodes but they were not replying with the data columns so none of the loads are node could sync it. I hope other clients have looked into the issue. I myself have debugged the lords are calls and lots of processing with lots of only nodes and it seems to be working fine. So since this is since these are the issues mostly from supernodes so I don't basically see any issue of node ID or data columns or ROM mismatch of data calculated Data columns.
00:08:10.995 - 00:08:22.615, Speaker F: Yeah, I brought something in the chat. I don't know if it is expected but the boot there is a boot node in the network and the boot node advertise itself as a super node with the CSC of 128.
00:08:28.955 - 00:08:35.615, Speaker A: I think yes it is our boot node is a Taku boot node but it's not synced to head.
00:08:41.955 - 00:08:53.345, Speaker G: So for each of the clients I was seeing issue I posted basically where lots are got the error and.
00:08:55.925 - 00:08:56.237, Speaker H: So.
00:08:56.261 - 00:09:20.675, Speaker G: If any of you guys have debugged it sort of figured that out because I know that Agnesh Nimbus has been debugging Nimbus response and yeah, and I also double checked on Lotsa using lots of only nodes. So as per my understanding lots of code seems to be working fine.
00:09:38.905 - 00:09:46.485, Speaker D: Thank you Gajinder. Next we have Eku here.
00:09:49.185 - 00:10:31.915, Speaker C: Yeah hi Dimitri. Hi. So we have updated Rust KZG and I have noticed that it's not parallel by default like cksg. You need to put into a constructor parameter of parallelization and we don't do this in tech yet. But I have added an issue and I see in benchmarks that if it's parallelized good enough like CKZG is on par with ckzg. I got the same timing in techo. Also we have greatly improved our sync.
00:10:31.915 - 00:11:41.055, Speaker C: We were sampling slot by slot before during sync and now we have dedicated sampler for syncing. We investigate into issue with Vantechovo distributed bad data column sidecars. I have checked everything, everything looks good and I'm not sure where it leaked but I have added get data columns sidecar rest API so we could check next time on the node what data column sidecar is in database does it exist? And I joined the question I think Mano asked in channel how to reach rest API of the node at least your own client node. And also we have added in our field and get pips we will ask it for it. That's. That's all.
00:11:53.645 - 00:12:35.049, Speaker E: Cool. I can go next if that's all protected On Lighthouse side we've identified a few issues on Devnet 2 I think Prism raise 1 which is the invalid columns not getting rejected by Lighthouse. I've confirmed that it's an issue and we'll fix it this week. And also there's been some observed stuck lookup issues on devnet. That's. That's why a few of the Lighthouse node got stuck with and getting out of sync and not able to recover. Lion's been looking to that.
00:12:35.049 - 00:13:19.035, Speaker E: I'm not sure if that's been fixed yet. I think another issue was the issues raised by Kinga One of the Lighthouse Super Node is not serving columns. I haven't had a chance to reproduce it, so I think I'm going to give it a try sometime this week as well. Yeah, so a few issues on dev node 2. Other than that, I think we're also going to update the Rust KZG library. Kev also told me that it's there's been some improvement in the computation time. It's been brought down from like an average of 200 milliseconds to 150 milliseconds.
00:13:19.035 - 00:13:41.495, Speaker E: So that's pretty exciting. I'm going to give it a try. And other than that, we've also released some testing metrics for peer DAs with 16 blobs. I've published a blog post ensure in the chat if anyone hasn't seen it yet, but that's all from Lighthouse.
00:13:48.595 - 00:14:22.545, Speaker D: Thank you Jimmy. Anyone from Nimbus here? Anyone from Grandin here. Okay, next, let's go to the update. So click. Client teams have shared some issues. Anyone wants to add something?
00:14:38.375 - 00:15:26.105, Speaker A: So regarding net 2 we are not looking very good. I think we haven't been stabilizing for over like five days and possibly that also causes some issues. I'm not sure if any of the Prism images will be able to sync up to head even when the chain is unfinished for so long. We could possibly give it some time till the Prism team has a new image ready and then we could try to come back and try to revive this chain. But otherwise we could do a relaunch of two spec and call Devon 3 and then possibly do the DevNet 3 spec that we have discussed before with the rebase on top of Vectra Net 4 spec for a future peer net.
00:15:30.205 - 00:16:02.065, Speaker F: Yeah, for present. So as I said earlier, I'm able to locally sync to the head Sync to the head for a full node for a supernode. It's currently in progress. I think it's interesting not to drop the testnet even after a long period of lack of finalization just to see if we are able to go back to a finalization mode.
00:16:05.925 - 00:16:26.765, Speaker A: Yeah, in that case I think we can keep the devnet around for a bit longer and see if we can revive it and then possibly wait for picture devnet4 for it to have a final spec and then we can do the rebase on top of that for the relaunch possibly two, three or four weeks from now.
00:16:41.155 - 00:16:41.755, Speaker B: Out.
00:16:41.875 - 00:17:33.395, Speaker D: I mean folders fix site. I'm looking at the definite Planning document this one. So I think we can include the two PRs to the next release in this week. So that. I mean so that could be a temperate temporary detnet target for whatever it is. And so I'm curious. So I mean cuttings have problems that interrupt definite but when it is in a solo client environment.
00:17:33.395 - 00:17:39.035, Speaker D: Did you still having this problems.
00:17:41.255 - 00:17:41.759, Speaker E: Like.
00:17:41.847 - 00:17:44.915, Speaker D: Not serving columns, things like that.
00:17:50.055 - 00:17:50.367, Speaker E: Or.
00:17:50.391 - 00:17:58.665, Speaker D: Basically I wonder if it's an interoperability or it's networking and more complicated problem.
00:18:02.645 - 00:18:07.425, Speaker E: So is the question related to Lighthouse not serving columns? So I missed the question?
00:18:10.885 - 00:18:35.195, Speaker D: No, just generally like all the. If it's a solo client environment. I mean solo client network is it. Does it work smoothly as we expected or we were also facing this similar problems.
00:18:39.415 - 00:19:04.485, Speaker E: We have actually seen that. I seen that issue on a Lighthouse only networks but I think we could do more testing. I'm not 100% sure if it's. If it's like a. Into a problem or a Lighthouse problem. I think it might actually be Lighthouse but I think I'll do more testing to confirm.
00:19:07.705 - 00:19:22.015, Speaker D: Thank you, Jimmy. Anything else regarding the definite planning?
00:19:23.995 - 00:20:05.125, Speaker E: Oh yeah, you mentioned. Sorry, shall I? Yeah, you mentioned the PR to decouple network subnets. Yeah, I've raised my thought. Just want to see what other teams think. I'm kind of interested to hear what people think of it. I mean decoupling is nice but I'm just not sure where is in terms of priority because we also have violated custody coming up which we want to implement and I assume that's probably higher priority but I'm kind of kicking to hear people's thoughts on it.
00:20:13.715 - 00:20:17.455, Speaker I: Would this change be difficult for Lighthouse?
00:20:28.555 - 00:20:45.027, Speaker E: I haven't really looked into what the change would be. I guess it would be probably the same as everyone else. I don't imagine it being too different. Yeah, I think.
00:20:45.051 - 00:20:47.815, Speaker F: I think the shank will not be like huge, right?
00:20:51.635 - 00:21:09.145, Speaker I: Yeah. At least for us. I don't think it'll be that big. Just looks like we would need to have a demo labeled abstraction for the network subnets. But other than that I don't think it's too bad.
00:21:23.205 - 00:21:25.745, Speaker E: Is there any other reason not to do it?
00:21:57.255 - 00:22:21.925, Speaker D: Maybe let's say do you want to delay the decision to maybe Thursday for the core dev score to finalize it or I don't want to wonder if people have deadline.
00:22:23.985 - 00:22:56.055, Speaker H: Yeah. I wanted to echo the concerns that Jimmy raised of being an unnecessary abstraction at the moment because it's. We are unlikely to raise the subnet count for potentially a long time. So yeah, I don't really see the point on having to deal with it now. I know it's not a big thing, not a super strong opinion that we are fine with what we have now. So trying to minimize complexity when it's not necessary.
00:23:08.165 - 00:23:46.605, Speaker D: Now the current one to come in. I guess that's also depends on the definite three timeline. Right. So do we have more time to, I mean refactor it or we want to spend more time on improving network?
00:23:50.785 - 00:24:04.195, Speaker A: We could also possibly just do the rebase on top of Spectra for DevNet 3 and then we could do DevNet 4 with new features because I think the Ruby itself is going to be a significant amount of work for some teams.
00:24:20.055 - 00:24:37.575, Speaker E: I'm curious to know what's the estimated timeline for teams to do rebase? I think Prism's already got it ready. Lighthouse also has it but we haven't tested it so there shouldn't be much work for us. What about Techo and Nimbus and other teams?
00:24:41.515 - 00:24:45.775, Speaker C: We need I think two weeks at least to rebase on Vectra.
00:25:00.965 - 00:25:02.145, Speaker G: For rebase?
00:25:02.965 - 00:25:03.785, Speaker A: Yeah.
00:25:05.005 - 00:25:08.145, Speaker G: Yeah, I guess one, two weeks would be fine with us.
00:25:16.725 - 00:25:49.025, Speaker A: Yeah. I think Vector DevNet 4 will launch possibly next week and then maybe a week after the Pactra DevNet 4 launches we can aim to do IRDAs DevNet 3 launch with the same spec as picture DevNet 4 on the consensus spec wise unless we need to make a new release for changes. I'm not sure if we're going to have any changes in Peer Desk that will require a higher consensus back.
00:26:02.405 - 00:26:37.555, Speaker D: So if definite, I mean Petra Definite photo is launching next week then I'd say we probably won't include Picture updates in this release so that we can release in today or tomorrow for picture and then next release we have, I mean it will be more Peer Desk focused. Does that make sense?
00:26:45.555 - 00:26:49.175, Speaker A: There's one more release scheduled for Petra, right?
00:26:50.195 - 00:27:23.709, Speaker D: I mean, yeah, this week. We plan to have a release for Petra this week. So I guess we can, I mean we can include this to peer desk PRs or not, but if it doesn't matter much for clients to work on Peer Desk then and we don't have to include it this I would actually.
00:27:23.757 - 00:27:33.605, Speaker A: Include the 3893 just so we have the rebase in there and then client teams can actually test against the same consensus spec as for Vector four.
00:27:35.145 - 00:27:43.525, Speaker D: Yep, we can do that. So just delay the decouple one.
00:27:44.105 - 00:27:55.445, Speaker A: So yeah, I would do that. I would focus on just the factory base and then we can do the decoupling in a future DevNet when everyone is on board.
00:27:56.915 - 00:28:57.545, Speaker D: Okay, sounds good to me. Yeah, because this, no this trend will. This release will include some picture updates and so peer desk definite has to follow picture status after this release. Just FYI. Be ready. Okay, I think we got some conclusion. Any objections before we change the subject? Any other big discussion or open discussion we want to talk today.
00:29:06.045 - 00:29:57.823, Speaker E: I'd like to follow up on the validated custody change. I don't see any progress on the. On the PR itself. I was wondering whether that's ready for implementation or are we still doing further research on that? Let me try to find the pr. Yeah, this one. Yeah. So it looks like the last approval was in was end of August by Alex.
00:29:57.823 - 00:30:09.645, Speaker E: I don't know if there's any further progress made to this feature so I was wondering when do we want to include this? All three want to wait for later devnet.
00:30:11.905 - 00:30:15.605, Speaker A: Is it hard to implement this?
00:30:21.465 - 00:30:50.005, Speaker E: It's some. Yeah, yeah I think there will be some work. I don't know exactly how much work, but there will be some work. It's not super involved but will require some work and some testing and. But I think this is. This is kind of like. I think what Francesco mentioned would be required for us to ship Pedas so be would be good to have it completed and tested early.
00:30:53.985 - 00:31:46.195, Speaker F: I have a question about validate Orchestrati and more specifically about validator custody accountability. What's the point to implement validator custody if we cannot guarantee that the node which are linked to validator clients or to validators are not penalized if they do not custody the amount of current they should? I mean currently with this spec I guess every valid every client could simply cheat and do not subscribe to the correct do not subscribe to more than the minimum amount of subnets even if they have a lot of validators attached to it. There is no penalty basically not to respect this rule, the validator custody rule.
00:31:52.865 - 00:32:41.915, Speaker H: So that's my take. I'm not sure if that's a complete answer but we only need sufficient amount of super nodes in the network to provide security. So even if some people cheat, that's fine. We just need enough nodes to not cheat. So yes, if that was a widespread problem it could become an issue, but I think we can deal with a minor amount and I would imagine from the point of view of a large operator, the bandwidth savings probably do not offset the development costs of maintaining a fork. Plus there is the I guess honesty assumption of operators just wanting to contribute to chain in a positive way. I Guess.
00:32:50.195 - 00:32:50.975, Speaker F: Okay.
00:32:57.195 - 00:33:17.945, Speaker H: I guess the same argument can be done of why isn't everyone playing timing games? Some people are, some people don't. It's. It's a matter of where every single operator lays on the trade off of say grid technical knowledge and capacity to execute customizations.
00:33:20.645 - 00:33:54.565, Speaker F: Yeah, but I think playing timing games is a little bit different because the worst case if you play timing games for you is to. If you miss the block and so you have a direct incentive not to play timing game too much about validator custody. If there is too many people who cheat, we may. I mean I guess the worst case would be to finalize a block, an available block.
00:33:58.625 - 00:34:10.315, Speaker H: No, I don't think that's the case. Like if. If we don't have super nodes, the network could stall if we are not able to source the columns. But I don't think we will ever finalize something that's not available.
00:34:41.184 - 00:35:44.105, Speaker D: Oh, I mean back to the pr maybe give people a week to comment and then we will revisit it. Thank you. Anything else you want to comment or we will move on. Any other topics to discuss today?
00:35:47.905 - 00:35:52.285, Speaker B: Hey everyone. Yeah, sorry. Go online.
00:35:53.985 - 00:35:55.845, Speaker H: No, please, please, you speak first.
00:35:56.705 - 00:36:23.495, Speaker B: Yeah, I just. I would like to discuss metric specs. If you had any chance to look through the pr. I will. I'd like to share the link for the dashboard. If you have an access to expand Upgrafana, you can check this draft dashboard. So you can see some of the metrics for Lighthouse.
00:36:24.235 - 00:36:28.335, Speaker D: Yeah. Katya, would you like to share screen or.
00:36:30.995 - 00:36:58.025, Speaker B: I can't see the. I can actually. But yeah, I can't see the button to share the screen. Do you need. Oh yeah, I see. Okay. Can you see my screen?
00:36:59.085 - 00:36:59.825, Speaker D: Yes.
00:37:00.605 - 00:37:56.117, Speaker B: Yeah. So we already have the draft dashboard and you can see some of the clients on one dashboard. So you can compare the metrics and you can filter by the client, for example, Lighthouse. But these are only a few metrics we have. They are the names of the metrics are from on this PR we're trying to discuss. So if other clients already have these metrics, but maybe the names of the metrics are different. So you can just rename them and you will see them on the dashboard and you can compare with the other clients for example, or check anything else you want.
00:37:56.117 - 00:38:44.895, Speaker B: So I would like to discuss the prefixes of this metrics. We already raised a discussion here in the pr. I will share the link. But is it a problem to rename metrics for example? Because in every client metrics are implemented differently. For some clients it's easy just to rename a metric and for other clients it can be, for example enumeration for prefix. So I would like to know what you think of it. And we have three types of prefixes, bacon, gossip sub and lib p2p rpc.
00:38:44.895 - 00:39:02.245, Speaker B: So this the latest one lib2p there was a discussion should we rename it to reg respond or not. So I'd like to know the client's view.
00:39:07.585 - 00:39:09.885, Speaker D: Thank you Katya, any comment?
00:39:14.505 - 00:39:44.625, Speaker H: So as for renaming, I think at least internally in Lighthouse we consider the remaining of metrics that are breaking change because it can break the setups of other people, both alerts and dashboards. So that's going to be complicated but that's one of the hurdles of any standardization effort. These are new metrics. Yeah. If they are not used in production then it should be fine.
00:39:45.005 - 00:39:51.145, Speaker B: Yeah, we definitely will discuss. Yeah we discuss peerless metrics only.
00:39:51.735 - 00:39:53.755, Speaker H: Okay, then it should be fine. Yes.
00:39:59.895 - 00:40:18.955, Speaker E: I think there are also some like gossip metrics and some of them could. I'm not sure but I think some of them potentially in the library for us. So I don't if it is then it will be a bit more difficult to rename.
00:40:27.545 - 00:40:35.085, Speaker B: But I think yeah, probably.
00:40:54.795 - 00:41:20.305, Speaker E: Yeah I think it would be useful to try to identify the key metrics that we want to standardize a few of them Things like gossip bandwidth would be. I think it would be interesting to see if all the clients have similar issues like having the metric collection in the library itself. But for peer that specific metrics I think it should be a bit easier.
00:41:23.245 - 00:41:32.905, Speaker A: Should peer specific metrics be prefixed with name? Maybe that way we can nicely identify peer to specific metrics.
00:41:35.375 - 00:41:40.395, Speaker H: I think that that will not scale well because we then we have to do that every fork.
00:41:50.535 - 00:41:58.575, Speaker A: I mean the general name of peerdes like it it would not be a breaking change or what do you mean it wouldn't scale.
00:41:58.615 - 00:42:48.925, Speaker H: Up But I think the convention normally is just prefix with the name of the binary or model of the thing. So in this case would be the proposals that we had either liquid 2P, RPC pickon RPC something like that. If we mix now specific feature once we move to Daz or to some other feature then we have to resist that nomenclature. Usually we do the model so leap it to PRPC and then the metric type with timing so data columns, bytes per topic, things like this. So I'm not I think adding. I'm not sure how we could put PR with respect thing the core naming convention.
00:43:04.035 - 00:43:12.095, Speaker B: So we can currently leave it as it is the naming of this metrics as it listed now in pr.
00:43:17.955 - 00:43:27.695, Speaker H: I think if you want to come to consensus on these metrics, it's best to do it on the PR because we don't have full representation of this call of everyone involved.
00:43:32.555 - 00:44:07.115, Speaker B: So we are leaving this PR open for now. Right. Because for example, currently I'm trying to implement some metrics for clients and I use these names I have in this pr, but as it is still not in the production, we can rename them later, for example, or if we agree on something else on other prefixes.
00:44:18.305 - 00:44:29.165, Speaker H: Yeah, I would prefer to not have to rename down the line. So it's best to do good effort now to come to consensus on what makes the most sense and just commit to it.
00:44:39.425 - 00:45:10.475, Speaker E: Yep. I think. Sorry. I think it would be nice if we could break down the PR just to include the peer das specific metric, which is the first section. It contains the dark column KG custody metrics. I think it would be nice to have that first because those are all new metrics and the remaining gossip sub and request response metrics are kind of existing ones. So some clients might already have them implemented so it will be hard to rename them.
00:45:10.475 - 00:45:23.555, Speaker E: Yeah, I think we should probably. I think those one will take probably longer to come to consensus. But the first section, the data columns, it might be easier to get them done.
00:45:26.415 - 00:46:00.165, Speaker H: Yeah, I agree. I would say as an advice for you, Katia, this is going to be a difficult effort because I think clients already have metrics and probably we all have conflicting naming schemes. So I would recommend you to do outreach with every single client and maybe draft in a doc the current situation with everyone and hopefully find the least the path of least resistance into something that's somewhat standardized if possible.
00:46:03.025 - 00:46:21.965, Speaker B: Got it. Okay. And what about other clients like Nimbus and Prism? Do you already have some metrics? Do you use them for peer DAs specifically?
00:46:23.745 - 00:46:59.895, Speaker I: Yeah, so I posted in the channel. But I think for some of these metrics we have it in a different name. So it's going to be difficult to standardize this because these are like generic prefixes for all P2Pmessages across all topics. So having it different just for peer das is kind of a weird case. But I think some of the much newer ones like reconstruction time and stuff like that that we can standardize on, but I don't think we can standardize on everything thing.
00:47:03.515 - 00:47:38.155, Speaker B: Yeah, we can start from pure dust metrics at least and look what we've got. I think it's a good idea to create to to split this PR in into small PRs and probably then you will raise issues for your clients. So yeah, we can, we can try to build a summary dashboard and compare the metrics. Thank you.
00:47:45.135 - 00:48:33.025, Speaker A: Katya, Could I ask all the client devs to make a comment on this PR or possibly in the new PR with a smaller changes and just let Katya know what is the corresponding metric name. Otherwise it's going to be a huge battle for Katya to find those out for every single different client. Especially because everything is written in different languages. So it would be just a huge help if everyone could comment, look through the PR and see which metrics you already have, which ones you don't have and the ones you have and you don't want to rename just so we can create a dashboard that would work for every single client even if it requires a different name. We can just make the dashboard a bit more complex, but this shouldn't be a big deal.
00:48:37.965 - 00:48:55.895, Speaker B: Yeah, and I'm also going to update the Cortosis dashboard for peerdos metrics. So in this case if you run cryptosis locally or you will see the dashboard at once for standardized peerdesk metrics, so it probably will be helpful.
00:48:59.395 - 00:49:16.215, Speaker A: Also Laion, you mentioned that you don't want to introduce or you do a rename for specific matrices, but would you be able to consider to add an additional metric for the same value under a different name?
00:49:21.195 - 00:49:41.725, Speaker H: I'm not sure our code base would support that. We would, I mean everything is possible but we would have to. Yeah, architect something custom look a bit ugly could be done but if we cannot do that, that would be best.
00:49:42.385 - 00:50:13.895, Speaker B: Yeah, I think we can start from peer DAS metrics for new metrics and we'll see the result, how it works together, all the metrics for clients and I mean at least before renaming the P2Pmetrics we can start from Tiradas metrics to standardize them.
00:50:21.075 - 00:50:47.135, Speaker E: I think with the existing metrics that teams can't rename, like the RPC bandwidth and the gossip numbers. I think potentially there might be some grafana features to allow you to map them into the same metrics. But I'm. I mean I'm not an expert, but I think there might be ways around it without having to add the metrics into the client code or even at Prometheus level.
00:50:50.635 - 00:51:27.515, Speaker B: Yeah, I can check it actually. That's a good point. New, fresh new battle for matrix specs.
00:51:31.015 - 00:52:35.205, Speaker H: Yeah, sorry, I wanted to bring up again this effort led by Michael and Jimmy. I guess you are all familiar with Engine Get Blobs v1 in the context of the current discussions of distributed block building and raising the block count, I think it's an incredibly important feature that we should prioritize and it has a lot of intersection with peer ras at least I think maybe Jimmy correct me if I'm wrong, we have already incorporated engine get blobs v1 into our peer RAS block production flow. So I wanted to know if other clients have even implemented this for then cool and if we should start incorporating or testing it for PF as it can influence how much can we push things without screwing up solo stickers.
00:52:38.745 - 00:53:04.005, Speaker I: Yeah so for Prism we've been discussing the general design on this is definitely a priority but one thing holding us back currently is I don't think all the execution clients have implemented this if I'm not get hasn't so it's a bit more difficult to test on our end. But yeah I would agree it's definitely something that we should have sooner rather than later.
00:53:08.265 - 00:53:43.775, Speaker E: Yeah I think it will actually be great to ship that before we get to peer das given that potentially there might be a lob increase in Petra. I'm not sure if I'm supposed to mention those words, but if we do increase the blob then we definitely want to have that just to be I mean just to keep the network stable even before peer DAs. So I wouldn't like kind of I think it would definitely help PEs a lot more but it would be nice to even consider doing that before backdraw.
00:53:46.235 - 00:54:48.295, Speaker H: Yeah I want to say I don't understand why this is not privatized more. It's a incredibly useful low hanging fruit that we can do to significantly help the network because consider two of the other options that has been discussed for solo stakers such as adding a flag to the execution clients where they say do not ever produce a block with more than two blobs or do not include blobs at all or only include blobs if they pay a specific fee if the execution client is smart enough and with this feature only includes blobs that have been properly propagated to the network to the home staker. It can include as much blobs as we want with zero bandwidth cost which is exactly what we want. Not have solo stakers sensor in the network but have a backbone that can include as much logs as necessary and I don't know to me that it's the perfect solution so I really hope that we can prioritize this as soon as possible.
00:54:53.395 - 00:54:58.265, Speaker I: Do you know why the PR has not been merged? It looks like it's been open for a while.
00:55:00.805 - 00:55:06.745, Speaker H: Well, I guess it's probably the same issue that Katya is facing. We have to come to consensus.
00:55:09.005 - 00:55:12.705, Speaker A: We just need someone that has merge permissions and then they can merge it.
00:55:22.215 - 00:55:41.195, Speaker E: It looks like it's already been implemented by three clients though I think on the Yale side and it looks like Ethereum JS has also got a PR open, so maybe just GIF that's left. Yeah, thanks. Thanks.
00:55:51.265 - 00:56:13.845, Speaker D: Thank you. Any comments on it? If not any other copies to discuss today?
00:56:20.115 - 00:56:34.575, Speaker A: Has anyone did 7742 EIP 7742 to be able to send different number of blobs, target and max to the el.
00:56:44.965 - 00:56:49.185, Speaker E: So is that in any of the Devnet targets yet?
00:56:51.085 - 00:58:08.595, Speaker A: Not yet, but we could possibly push for it. I heard that Marius is going to implement it in G and then we can start testing that. I'm not sure when it's going to be done from the side because it sounds like it's a bit more tricky to implement it on the. I'm just curious if any client teams have taken a look if there's any problems maybe with it we could also target for like Devnet 4. Regarding that, is the idea to include the new maximum target in the config by the way, or how are we going to change max? Because if we do that, then every time we want to bump it, we're going to need to like add a new prefix or something for each fork.
00:58:22.825 - 00:58:31.005, Speaker D: Right? Yeah, for the configuration. For this configuration spec the prefix is required.
00:58:34.305 - 00:58:53.775, Speaker A: Are we going to have like version one, version two, version three or what is the plan? Or we just call it by the name of the update and then we do max with that update like picture or whatever the next work is going to be called.
00:59:04.355 - 00:59:10.895, Speaker D: I guess it depends on you if picture will update it, right?
00:59:15.765 - 00:59:31.545, Speaker A: I mean in the end we still need a way to increase this down the line. So is it just going to be like the name of the update with max blob and target blob? Because right now we just have a single value without any prefix.
00:59:41.895 - 01:00:06.497, Speaker D: Right? So do you mean that we should add this configurable parameter in the peer desk right now just for. Yeah, would be compatible.
01:00:06.681 - 01:00:33.655, Speaker A: Yeah. So right now we have max blobs per block and we don't have any flag for target blobs per block. So it would be good to have something like fulu Max blobs per block Fulloo target blobs per block and that way those values can be read by the CL teams and can pass that assuming that 7742 will be included in F.
01:00:42.315 - 01:00:51.695, Speaker D: Now I can make a draft VR of it receive makes sense for Techn4.
01:01:01.325 - 01:01:13.265, Speaker A: Or is there any other way we could do this other than set it in the config? Because this this is going to add a bunch of different values. If we like want to bump this every single update.
01:01:25.775 - 01:04:09.765, Speaker D: But it it's supposed to change at the fog boundary then I I mean for consensus break I don't see this other option but client is might have other fancier configuration setting I don't know and anyone wants to share your client implementation? No? Okay. Okay I will make a draft Y then we will see how many lines we will affect at least from the six okay so we're a bit running out of time. Before we end this call I want to quickly share updates that Josh Rudolph and I recently looking looking at how to improve Peer Desk and future foldas project management also so we brainstorm some ideas. This is a link to describe some basic things we can do from scratch for one of this is to create a Gear Desk contributor list and also we can try to make you know collect the Gear Desk reading materials research in a more structured way. Hopefully you can make a like a website or notion page like verco.info. so this is the initiative. I asked some people to fill in the contributor list and I will share the form later after this call.
01:04:09.765 - 01:04:52.955, Speaker D: Yeah so if you have any comments please let me know or can share it right now but we are running out of time. Yeah any store time list. Okay thank you everyone think let's call it a day. See you soon. Bye.
01:04:54.815 - 01:04:55.895, Speaker A: Bye bye thank you.
01:04:55.935 - 01:04:56.559, Speaker H: Bye.
