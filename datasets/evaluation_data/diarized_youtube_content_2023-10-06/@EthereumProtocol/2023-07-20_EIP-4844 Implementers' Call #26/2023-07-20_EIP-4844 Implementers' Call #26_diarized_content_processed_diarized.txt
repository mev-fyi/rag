00:00:02.810 - 00:00:15.910, Speaker A: Here is agenda. Okay, cool. Anybody want to give us Devnet seven updates? I saw something akin to stability.
00:00:18.650 - 00:00:52.980, Speaker B: Yeah, so we have a fund as in Devnet we mainly have Lighthouse and tech who as majority the validators on the network with gets another mind sharing the baseload. We have a stable frame rated network so we can use it for all sorts of blob tooling updates. And one of them has been the blob explorer. It's been updated to the spec now and it works. Yeah. Besides that, any other blob related testing can happen on this test.
00:00:55.770 - 00:00:58.310, Speaker A: Tweet. Are we consistently spamming load.
00:01:00.170 - 00:01:13.660, Speaker B: Avoiding doing too much right now? Because we did trigger an issue with Bezu and want to give it some space to figure out that issue. But once that's done, we have everything required to consistently spam it.
00:01:14.110 - 00:01:17.100, Speaker A: Kind of ready to go, but we're sending some.
00:01:18.130 - 00:01:25.680, Speaker B: Yeah, we had a couple of periods where there were like 2000 blocks submitted and then whatever makes it, makes it.
00:01:28.290 - 00:01:29.040, Speaker A: Cool.
00:01:29.510 - 00:01:33.940, Speaker B: Just waiting for the explorer to load. But I can give you more accurate stats there.
00:01:36.150 - 00:01:52.214, Speaker A: Any questions for Perry on the main points here on the client split? Is that because the clients that are not taking the majority load are not as stable or because they're not online or what?
00:01:52.412 - 00:02:15.600, Speaker B: Because they're not as stable and some clients are just following the chain, they're not proposing or attesting. There's an issue tracker here so you can see who's attesting, who's proposing, who's following, and you can scroll down to see issues you've already triaged and a couple of unanswered ones.
00:02:17.650 - 00:02:23.060, Speaker A: Hey Perry, do you know or like what the next steps would be for dealing with that deposits issue?
00:02:24.630 - 00:02:53.740, Speaker B: Yeah, I think that's the big one. Also wanted to bring it up today. I'm also not really sure how we proceed with that one. To give some more context, Justin made a couple of deposits last week, I think it was 20, and last we checked on Friday they still hadn't made it on the provided data set and we're not really sure why. They seem to have been noticed according to the lighthouse e one voting stats. So they should have made it. But yeah.
00:02:56.490 - 00:03:07.580, Speaker A: Is the was there consensus on the eth one data voting and process deposits or. We don't know.
00:03:08.110 - 00:03:13.870, Speaker B: All of the tooling I have for debugging deposits are lighthouse based and as far as I can tell, lighthouse sees everything.
00:03:13.940 - 00:03:14.560, Speaker A: Okay.
00:03:15.170 - 00:03:49.920, Speaker B: So either it's an issue on techo side or it's something completely. So Proto's message about wrong fork digest. If that's the case, then lighthouse shouldn't have seen it. But Lighthouse does see it. So I guess the summary there is. Can someone from Techu help us figure out how to see the Ethan voting view from Teku's perspective? And that should already give us everything, because Teku plus Lighthouse is majority already.
00:03:51.650 - 00:04:20.426, Speaker A: I guess to follow home on Proto's point, Lighthouse would see it, the voting would happen. Those deposits would be added to the chain, but they'd be a no op because their proof of possession would be incorrect. So it could go through the whole flow. And then you don't get new validators because the. So a good theory to look into. Yes.
00:04:20.608 - 00:04:23.530, Speaker B: We'll investigate a bit more testing.
00:04:31.000 - 00:04:36.310, Speaker A: Cool. Other updates or discussion points for Devnet seven?
00:04:38.920 - 00:04:53.864, Speaker C: I can give a quick status on Besu. We have an issue with the data hash operation. We've got it expressed in our traces, and we think we have it covered under hive tests. So we should fix for that any hour now.
00:04:53.902 - 00:04:54.890, Speaker A: I would say.
00:04:57.260 - 00:05:02.156, Speaker C: We expect to continue proceeding on the chain once that's solved. This is a regression because this used.
00:05:02.178 - 00:05:14.000, Speaker A: To work on the previous devdem. Sorry. And this is something that is in Hive, or can be in Hive. Yes.
00:05:14.690 - 00:05:24.180, Speaker C: We are currently failing 19 hive tests. Got you in the Cancun suite. And they all seem to orbit this issue.
00:05:30.000 - 00:06:05.610, Speaker B: And I'm just posting a message about the current status of alt devnets. So something about Devnet seven? Devnet eight specs are out, release date unknown. So we'd stick to Devnet seven until everyone's comfortable moving on. And just a sort of long term deprecation warning, I guess. We want to stop doing ballotrics Genesis and move to capella Genesis. And we won't do that for Devnet eight, but we would like to do that for Devnet nine. We're also going to be testing it offline to make sure and create some issues, but we just wanted to bring it up so that there's a bit of a forcing function there.
00:06:16.920 - 00:06:49.390, Speaker A: Okay. Any other updates people want to share about Devon s seven? Cool. Devon eight. There's a few discussion points we can do. The easy one first. The should override builder flag, which is a new flag making its way into the engine API. That should be relatively straightforward to get in.
00:06:49.390 - 00:07:38.608, Speaker A: I believe there was some discussion on the discord about if this should be in Devnet eight. I believe that was the kind of general takeaway from the past two awkward dev calls, but it would be good to hear if anyone is opposed to that making into the Devnet eight spec going once. Great. Okay. I believe that is the consensus that will make it into the Devnet eight spec. Cool. There was an item put here by Tim and there's not a link to what the discussion is, but it's just APIs for l two s.
00:07:38.608 - 00:07:45.360, Speaker A: So if anybody has the relevant discussion points for that, by all means. Yeah.
00:07:45.430 - 00:08:46.900, Speaker D: So I guess two weeks ago that team has this call to action. They want feedback from layer two team on whether the current API or the current beacon API is good enough for layer two. As far as I know we haven't heard much feedback from layer two teams. Just like whether the current beacon API especially get blobs is good or is useful enough. But I went back and I surveyed around arbitram just to get their feedback. I guess the most useful feedback I get is that instead of retrieving blobs using block id, it is very nice, compelling for them to be able to retrieve blobs using version cache. So I created this beacon API issue essentially to basically enable blob retrieval using version hash.
00:08:46.900 - 00:09:19.010, Speaker D: Because their smart contract is transported instead of using block id, version hash will become much easier. I don't know if this is like a mandatory endpoint that all clients should implement or this is like optional, but I guess if clients do implement it, then that's very nice because there's some client diversity there and then because, yeah, I am sure that the air two team will be very happy for this.
00:09:19.860 - 00:09:33.956, Speaker A: Okay. And if this isn't implemented, then essentially the team has to identify what block their transaction made it into and then can do the retrieval from there. Right, right.
00:09:34.138 - 00:09:38.160, Speaker D: There is an additional layer of mapping involved.
00:09:38.320 - 00:10:28.212, Speaker A: Yeah, understood. It looks like there was at least a little bit discussion here. Is there anybody else on the cassette flair that wants to chime in on it? Looks like it would be an added endpoint because of the ambiguity of the 32 byte hash rather than using the same. Yeah, yeah. Anybody have discussion for this one? Proto or anyone on the optimism side have opinions on how they believe they're going to use this? Okay, I see Proto. I'll just read this. Introduce the API by block id in February.
00:10:28.212 - 00:10:41.532, Speaker A: Still works with optimism at l two. Ideally we can fetch it by something that can be identified by El, identification by slot and then consistency checks, result works. Additional endpoints would require indexing. Right.
00:10:41.586 - 00:10:46.188, Speaker D: So this does bring some Cl complexity.
00:10:46.284 - 00:10:46.544, Speaker A: Right?
00:10:46.582 - 00:11:02.340, Speaker D: Because now a Cl client needs just additional mapping for a block id to version hash. So there is some increased complexity there. So therefore I think like an optional endpoint makes sense.
00:11:02.490 - 00:11:34.960, Speaker A: But yeah, optional endpoint is going to end up fragmenting use, so I'd prefer not to go down that path and figure out what's needed here. I guess my question is when is an l two not aware of the blocks their transactions made it into? Because that would be when they have to do this extra mapping.
00:11:36.180 - 00:11:36.592, Speaker E: Right.
00:11:36.646 - 00:11:58.760, Speaker D: Because for arbitram, currently only version hash is stored on chain. They don't store any block id related things on chain. So to verify the blobs, they can just get a version hash from the onchain contract and do a quick lookup versus another layer of mapping to map block ID with the version hash.
00:12:12.360 - 00:12:48.890, Speaker A: Yeah, Proto. I'm not sure why you can't unmute. I'm trying to. I'll make you a co host, see if that works. Yeah. Okay. Probably zoom back further discussion on this.
00:12:48.890 - 00:12:56.376, Speaker A: Thoughts on direction? We can take it or at least take it back to this issue. Yeah.
00:12:56.478 - 00:13:00.430, Speaker D: Or yeah, if there's any additional comments, please comment on the issues.
00:13:07.450 - 00:13:26.112, Speaker A: All right, cool, thanks. Rodo, is arbitram on testnets? On our testnets or have a plan to join? And same with optimism.
00:13:26.256 - 00:13:42.330, Speaker D: Yeah, I can speak for arbitram. There's definitely plan to join as soon as possible. Just we're waiting for things to stabilize in a little bit because I know definite a is coming. So I'll probably wait for definite a when we have most of the things in there.
00:13:48.860 - 00:14:24.576, Speaker A: Okay, cool. Yeah. Okay, this is probably nine we'll bring up again, but in the context of as we get into the more and more serious testnets, we should be inviting more and more teams to be prototyping, getting feedback. Hey proto, I saw you rejoined. Do you have anything you want to toss in? Okay, I'm fine now. Sorry about so about the API thing. It's either indexing or from timestamp to slot.
00:14:24.576 - 00:14:47.630, Speaker A: Current API works. It just isn't ideal. Now for layer two participation. We are looking to rebase our prototype on the latest version of portray. It's just timing. We need to figure out when to execute on this and join a definite. Right.
00:14:47.630 - 00:16:04.010, Speaker A: Okay, cool. We'll keep this conversation going in the coming weeks either on this call or on all core devs. Anything else on the API's discussion? And Terrence, if you do want to surface that issue on the consensus layer call to push for more feedback, we can if we don't get sufficient input there. Okay, anything else on devnet eight? Okay, next up is a pr to the execution APIs. Clarify cancun payloads handling by early APIs reorder checks. I guess this is not even necessarily for specifics any of the context here. Hello.
00:16:04.010 - 00:18:02.404, Speaker A: So this API update helps to clarify some corner cases, how we handle v three payloads by v two. And it also reorders checks, I would say defines order of validations needed for a new payload get payload methods. So it now becomes more clear in terms of testing and implementation what error codes we should expect in different situations connected to timestamp and other fields passed to the APIs. So it is expected that v two and v three methods should not, should decline requests when timestamp is not of correct fork. And additionally, if any additional fields are passed, or fields aren't passed or contain null values, it should be declined with a different code of invalid params. And this request, well, the intention was to clarify these kernel cases. Got you.
00:18:02.404 - 00:18:50.768, Speaker A: So is this non substantive? It's all clarifications. Yeah, mostly clarifications. And it's very useful for hive tests when we have some undefined behavior. And it would be good to have my next. Okay, great. I saw Lukaj just approved and that Mikhail did it too. Why don't we have this up for another day or two for eyes and then get it in? It makes sense to me.
00:18:50.768 - 00:20:04.840, Speaker A: It might require a slight modification of checking the constraints. So this pr makes El to check that the strict set of fields is passed onto the method. So if more fields are passed should be rejected. This is to aid finding bugs on the earliest stage of development, like on devnets. If Cl mas is up with the structure or with the method or with the timestamp, meaning that it sends a payload of, say, cancun onto Shanghai method and vice versa. Got it. Okay, any questions here? Great, thanks Mario.
00:20:04.840 - 00:20:40.010, Speaker A: Okay, thanks Lexi. Okay, we did slate some time if there were any updates on some of the Cl gossip sub modifications that have been under discussion, both flood staggering, I do not want, and other things to that tune. Anybody that's working on these r and D avenues have any updates?
00:20:41.390 - 00:20:44.794, Speaker E: Yeah, hey, can I say a few words about great.
00:20:44.832 - 00:20:45.420, Speaker B: Please.
00:20:47.470 - 00:21:45.520, Speaker E: So yeah, actually I don't want PR, but I must say that I don't want. It's not like a silver bullet, it's just actually slight optimization for large messages. It mostly reduced the total load of network, so not a great latency gainer. We also going to add another pr to gossip. It's pink pong. It actually could be used for several things. Those are also like small, all of them actually small optimizations, which we probably need to do testing in a real world.
00:21:45.520 - 00:23:08.758, Speaker E: Actually my general impression is that the block decoupling, which is already done and like flat publish fix would probably be enough for eight four. Maybe all other optimization could be done like asynchronously from all client because they don't need art forks. About flat publish, actually I have a simple idea for resolving this issue. It's just disabling flat publish for blocks and blobs topics. It should probably work quite okay. So we have no security issues with this, I believe. Actually, I'm not sure at which point we enabled flat publish.
00:23:08.758 - 00:23:15.770, Speaker E: I think it's like for more efficient attestation publishing.
00:23:17.470 - 00:23:27.280, Speaker A: Soon before phase zero, it was enabled because it seemed like an obvious optimization and people didn't think about the parallelization issues too much.
00:23:28.290 - 00:23:33.230, Speaker E: Yeah, probably it makes sense just to disable it for blocks and blobs.
00:23:33.590 - 00:23:49.240, Speaker A: I have a question. When you disable it, do you still send the messages in parallel to the peers on your mesh? So you still might hit issues, but just not crazy ones.
00:23:49.930 - 00:24:15.920, Speaker E: Yeah, right. But it shouldn't be big deal for good bandwidth, but actually staggered standing would be good to have. But as I said, that probably another optimization that we could enable later.
00:24:17.410 - 00:24:28.530, Speaker A: Should we in the CLP to p spec actually make a note that flood publishing should be disabled on those topics?
00:24:30.790 - 00:24:52.010, Speaker E: Yeah, but actually currently flood publish is like a global option for gossip. Yeah, actually another approach is just to disable it for messages larger than some threshold.
00:24:56.030 - 00:25:02.170, Speaker A: But that would still need to go into the lib p to p. Yeah, kind of configure.
00:25:02.250 - 00:25:02.880, Speaker E: Right.
00:25:03.490 - 00:25:04.240, Speaker A: Okay.
00:25:08.370 - 00:25:23.090, Speaker F: I think we had a discussion that we don't have to disable the fat publish. Right. It's like we can still enable it, but we can just send the message to the non mesh peers later instead of immediately.
00:25:27.050 - 00:25:38.620, Speaker A: Yeah, but it's more of just, that's not like an easily configurable thing for lib PDP like that. Any of these require going into and making deeper modifications, right?
00:25:39.310 - 00:25:55.520, Speaker E: Yeah. Also the question later is kind of, I don't know how to say fuzzy because you never know when your message was.
00:25:56.370 - 00:26:03.566, Speaker A: Once you've seated the mesh, maybe you should just stop instead of sending it to your 50 other peers.
00:26:03.758 - 00:26:39.580, Speaker E: Yeah, actually for large messages I have message already when join the game because sending to 100 peers message like of size, half of a megabyte would most likely take more than one hert bit. So you most likely will be sending, I have message on this page.
00:26:41.070 - 00:27:27.680, Speaker A: Right. I just realized I was muted. Did you hear any of that? Do all Cl clients have an r and d path to do either the disabling or the staggering, or is that still an unknown for a few clients? I know age is working on this for Lighthouse, but I don't have too much in the way of specific updates.
00:27:28.820 - 00:27:31.490, Speaker D: I'm not sure. Yeah, I have to check.
00:27:32.580 - 00:28:19.960, Speaker A: I actually think age was planning to add, like, disabling flood publishing in the next lighthouse release, but don't quote me on that. Got you. Okay. I guess this is one of the single biggest things that could change the success of these large messages. Definitely prioritize getting it out. And I guess nice that if a client or two don't do it, then it'll just affect the client in question and the proposals coming from that client. But it'd be best to probably have this across the board.
00:28:19.960 - 00:28:45.670, Speaker A: Okay. Anton, you said on the. I don't want. You said it's not really a latency gain. It's more of to help reduce total bandwidth consumption. Total load. Do we have that quantified at all?
00:28:47.160 - 00:28:52.550, Speaker E: Sorry. Right. It's not like. Yeah, correct.
00:28:53.640 - 00:29:00.680, Speaker A: But do you have an idea of the potential reduction in total load latency aside?
00:29:01.900 - 00:29:24.720, Speaker E: Yeah, so, yeah, we have that list of potential improvements. It includes staggered. I think that probably the best gainer is if staggered sending could be implemented in a good fashion.
00:29:25.220 - 00:29:25.680, Speaker A: Probably.
00:29:25.750 - 00:30:02.620, Speaker E: Then we have, like, TCP. TCP slow start issue. It affects a lot. Yeah. We have a dirty workaround and it's like a frequent ping, so that would be not very good solution, but should work. For example, if we implement. If we can pin and gossip once per 200 milliseconds.
00:30:02.620 - 00:30:49.080, Speaker E: Yeah, it's actually dirty, but it works. I think that staggered sending and tcp slow start are the next things we could address. That gossip pin pong messages, I mentioned, they're helpful for staggered sending, TCP slow startup avoidance, also. Yeah, I don't want. Just like an easy way to reduce total load.
00:30:50.460 - 00:31:00.970, Speaker A: Right. On the reduction of load for I do not want or I don't want. Is that quantified? Is it a 50% reduction? What's the.
00:31:01.580 - 00:31:17.830, Speaker E: It's kind. Kind of 30% reduction? Yeah, it's my simulation results, but it's kind of three to five latency reduction, so not huge.
00:31:23.500 - 00:31:30.390, Speaker A: Got it. Other networking discussion points or questions for Anton?
00:31:37.690 - 00:31:48.860, Speaker F: Do you think we should fog the lip p two P spec level so that we can have our own spec and we can add anything we want to the spec?
00:31:56.160 - 00:33:48.790, Speaker A: Yeah, I mean, that's certainly a path. I think that we're better off if we can work with the larger lip PDB community and have shared tooling and thus don't have to fork the software as well. But if we do hit a hang up, that's definitely an option. Does anybody have other opinions here? Yeah, if we're having timing issues, we can fork and try to then merge it back upstream if we need to work around, but I would try to play in the same environment if we can. Other networking comments? Okay, any other discussion points for today? Yeah, I would like to ask other execution clients about replacing of block transactions. Do we have some consensus on how much? Must be a bump of max fee per gas, max priority per gas, and max fee per data gas. This is mempool replacement? Yes.
00:33:48.790 - 00:34:05.220, Speaker A: Maris, do you know the rules currently in place in the Gethman pool?
00:34:08.120 - 00:34:10.150, Speaker G: I'm just looking it up.
00:34:18.200 - 00:34:35.840, Speaker A: And then I think important only, are there rules around? I had a blob transaction and I switch it to a non blob transaction. I guess you want the replacement fee to be on that high end rather than the normal transaction.
00:34:36.180 - 00:34:53.780, Speaker G: Yeah, so I think right now we have 100% for everything. So basically you have to increase all of the fields by 100%. So double.
00:34:55.000 - 00:35:02.090, Speaker A: And that's to go from a blob transaction to a blob transaction. And to go from a blob transaction to another type rate.
00:35:03.980 - 00:35:12.190, Speaker G: That's to go from a blob transaction to a blob transaction. I'm not sure.
00:35:15.680 - 00:35:19.836, Speaker A: That would be a loophole. Right. If you could go cheap to the other.
00:35:20.018 - 00:36:02.120, Speaker G: No. So we have one thing that. One thing that we implemented, or that Peter implemented is that you cannot have two transactions in two different pools. So basically the block pool is its own transaction pool, and the other pool is the normal pool. And so what you cannot have is one normal transaction, one block transaction, and then another normal transaction or something like this. So that's forbidden in gas.
00:36:02.200 - 00:36:06.920, Speaker A: So once I have a nonce in a particular pool for an account, I can't swap pools.
00:36:07.080 - 00:36:22.080, Speaker G: Yes. I'm not sure about replacement, though. And once the transaction is mined, you can have a transaction in a different pool.
00:36:24.420 - 00:36:25.440, Speaker A: Roberta?
00:36:26.820 - 00:36:46.424, Speaker H: Yeah, we discussed this quite a while ago at this point, but I think one of the proposals that came up, and I don't know if anyone followed up on it, was to disallow any attempted replacement of a blob transaction with a transaction containing fewer blobs. Because in some sense, you've already kind of taken the hit once you've processed it.
00:36:46.462 - 00:36:46.808, Speaker A: Right.
00:36:46.894 - 00:36:52.990, Speaker H: And so it's just a potential denial of service factor. But that was just one idea that had been proposed before.
00:36:53.600 - 00:36:54.636, Speaker A: So, like, if you have a one.
00:36:54.658 - 00:37:01.070, Speaker H: Blob transaction, you can only replace it with another one blob transaction or something with more blobs, not something less.
00:37:08.580 - 00:37:31.192, Speaker A: I don't think anyone's implemented that, though. Eric, from what was discussing, it was even like that for block transactions only fields with fee can change like all others, must remain the same to replace it. But making it possible to add more blobs might have sense.
00:37:31.246 - 00:37:34.250, Speaker B: Like, just need to have consensus here.
00:37:57.210 - 00:38:51.610, Speaker G: Yeah, I don't know. I don't really know what the rules in geth are right now. Yeah, basically I think that Anska is right, that we don't expect many people to use those transactions, and the people that use those transactions are very capital heavy organizations that can provide or write their own transactionary layers and stuff to do. So regarding the rules, I think we should maybe discuss on when Peter is also part of the discussion.
00:38:53.470 - 00:39:46.690, Speaker A: Yeah, it'd also be great if I know Peter's done a lot of the thought and design upfront and then probably has iterated quite a bit on the design. If we can just document what the rules are in geth at some point. Might be useful for everyone. Very cool. Try to bring it up in AcBe with Peter. Anything else on mempool blob transactions, please. Okay, other discussion points for today.
00:39:46.690 - 00:40:14.822, Speaker A: Great. Good work on Devnet seven. Excited to see it continue to stabilize and get some more. Can client distributions over time. Talk to you all very soon. Thank you. Thank you.
00:40:14.822 - 00:40:17.830, Speaker A: Thank you. Bye, everyone. Bye.
