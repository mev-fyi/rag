00:00:03.800 - 00:00:46.590, Speaker A: Hi everyone, and welcome to the sequencing and preconfirmations. Call number eleven. Today we have folks from Coinbase who will be talking about keyspace. I think we have a presentation specifically by Niran, but before that I kind of want to talk about EFCC. There is a persistent testnet that is being spun up. So this is a public and persistent testnet called Helga. The idea is to basically continue on the success of Zubalin and go from a devnet to a testnet.
00:00:46.590 - 00:01:32.170, Speaker A: I think most of the focus is not on the Horlesky testnet. By having our own private testnet, we'll be able to move a little bit faster. The name Helga comes from Dutch. It means bright because the future is bright because of base roll ups. And as far as I can tell, there is a lot of interest from many different teams, 20 or so teams across the whole stack. So if you are interested in participating or getting involved in this persistent testnet, do get in touch. Without further ado, Niran, the floor is yours to talk about.
00:01:32.170 - 00:01:33.070, Speaker A: Key space.
00:01:35.010 - 00:02:07.842, Speaker B: All right, thanks, Justin. Thanks everybody for being here. I don't normally live in California, but I'm in California right now and it's very early, but hopefully that does not affect the quality, the presentation. So let me start my slides as I'm going through this. This is fairly informal, so feel free to chime in and if it gets too much I'll push back on it. But let's start by assuming that it's going to be totally fine. So my name is Niran Babalola.
00:02:07.842 - 00:02:53.570, Speaker B: I work on the base team at Coinbase. Our goal in general is we want as many people on chain as possible. So that means as many users, that means as many builders as possible, that means as much value as possible, and that means that we have to scale the network so that can all happen cheaply for all these users. So that's what we believe, that's what we're working on. Part of that vision is making sure that people can use smart wallets across all the chains that they use. So for us, this isn't just a base thing. We expect that for people to have the kind of user experience that we want to have, that they're going to need smart wallets to have paymasters, to have passkey signatures for their accounts, all the benefits of that.
00:02:53.570 - 00:03:36.838, Speaker B: But they need that to be a unified experience. No matter where they go. If they go to another chain and they have a bad experience, that means they're having a bad experience. On base two, we don't want that. So where key space fits in is that it's a cross chain key store for these smart wallets. So Vitalik has done a lot of writing about this vision of having a single source of truth for the signing keys of your wallet. Because once you move away from externally owned accounts to smart wallets, the smart wallet is storing the signing key that controls the wallet in the storage or the account.
00:03:36.838 - 00:04:17.634, Speaker B: If you're on another chain, you don't have that storage everything gets out of. So there's a lot in the key space docs, there's a lot of writing in the references that I've linked to, so feel free to, if you haven't seen all that writing, that Vitalik is done. There's a lot of links there, it's all good stuff. So we're trying to implement a version of that vision. Key space is implemented as a ZK rollup. So we settle to Ethereum, the state route stored there, and every time we're trying to update the route, there's a zero knowledge proof that's being verified to update the root each time. This is not a Zkevm, it's not a general purpose blockchain at all.
00:04:17.634 - 00:05:12.938, Speaker B: And in that sense, it feels weird to even think of it as an independent blockchain. When you think of an. When I think of an independent blockchain, I think of optimism, I think of arbitrary, I think of no such chain. I think of these things that have their own rich state that people are querying, that people are sending transactions with, et cetera. In my head, what keyspace is doing feels more like a fancy smart contract. It's one application with very limited operations that you can take on it like a smart contract would be, but it is its own independent chain. We limit the operations that can happen on key space, because at the end of the day, this chain is controlling access to your funds if there's some sort of bug, some sort of error, because we're trying to support all the use cases in the world that end up cascading to people's wallets, and people have a bad time.
00:05:12.938 - 00:05:43.810, Speaker B: So instead we want to minimize the attack surface of key space. So there's very few operations that can happen. We want it to be straightforward for people to audit, for people to trust, and for it to be reliable over time. So that's what key space is. It's a minimal keystore roll up, it's a ZK roll up, few defined operations, and that's what's going on here. So in the context of this call. I haven't actually been to any of these presentations, but my understanding is that it's a lot of roll up builders thinking heavily about preconfirmations and things like that.
00:05:43.810 - 00:06:24.032, Speaker B: So in that context, the limited statefulness and key space kind of informs like what we've been thinking that we need to care about. And we're very interested in feedback from you all who have done a lot of thinking about this. But to think about where key space fits in. In Ethereum, we have this rich account based state where the order of all transactions is affecting what happens on chain. It's rich statefulness. It's why Ethereum is this very useful platform for writing these applications. If you go a step down, you have bitcoins, Utxo based state, where the order of the transactions that are consuming the same Utxos is very important.
00:06:24.032 - 00:06:54.856, Speaker B: But other than that, if I'm using some Utxos and you're using some utxos, our state is independent, so the ordering of those is less important than it would be in Ethereum. And then taking a step even lower than that is something like certificate transparency. If you've heard of it, great. If you haven't, it's not that important. But it's basically the state doesn't depend on the ordering at all, it's just a log of the certificates that have been issued. So the ordering isn't super important. Key space is kind of in between bitcoin and certificate transparency.
00:06:54.856 - 00:07:32.180, Speaker B: On the spectrum, every wallet has its own record and each record has its own independent history. So you can manage your keys, I can be managing my keys. And the fact that our transactions might be included in a different order isn't super important. And it's for this reason that we haven't been super focused on things like pre confirmations, things like that. We don't think that we need to think about that. If you think differently, we want to know so we can correct that. But that's how we view statefulness in key space, in terms of how key space is sequenced.
00:07:32.180 - 00:08:28.850, Speaker B: Right now, the deployed testnet version is permissionlessly sequenced by anybody who wants to show up and advance the state of the roll up. There's no that you don't get a slot to prove or anything, it's just you show up and you prove this is not, not shippable in its current state. It's just the way things are right now. In particular, the current model introduces a lot of contention. If I start proving a block and you start proving a block, we're gonna have a bad time, because if, when your proof gets on chain, all the work that I've done is now invalida. And sometimes that work is very computationally heavy to get proving times of currently about five to ten minutes. With our current circuits, we are running a relatively beefy machine, cost money to do, and we don't want that sort of contention to be there in real life.
00:08:28.850 - 00:09:28.156, Speaker B: So the end game for sequencing in key space, we believe, is base sequencing. We want to inherit a lot of the benefits that you get from relying on the L1 sequencers to do the sequencing. So in terms of liveness right now, if our sequencer stopped, it's very hard for somebody else to step in. If we had a whitelist sort of approach, you could benefit from having a couple of other people, but we really want any L1 validator to be able to step in. We also want to benefit from the decentralization of the validator set having any, instead of trying to build our own validator set using ethereums, knowing that the allocation of proving the sequencing slots is already well distributed. And then we definitely want Ethereum alignment, because again, we're not trying to create our own ecosystem. What we want to end up with is something that the entire Ethereum ecosystem sees as core infrastructure.
00:09:28.156 - 00:09:36.330, Speaker B: We want many, many users wallets to be integrating with this system. Does somebody have a question?
00:09:39.470 - 00:09:42.490, Speaker A: George, maybe I'll just mute you if you don't have a question.
00:09:43.750 - 00:10:45.250, Speaker B: No worries. So the model of base sequencing that is kind of the latest thinking that we're familiar with is the vanilla base sequencing, where basically if the current L1 validator is opted into the system, then they're the validator for our chain. And if they're not opted in, then a random opted in validator would be selected. That's the setup that we expect to end up with. So, getting more into how key space works, there's basically a sequencer running that's accepting users key change requests through RPC calls. When you want to change your keys, you say hey, key space known. Here's my signed keychains transaction, can you include it for me? The sequencer then takes a batch of key change requests and creates a recursive batch proof for all those requests.
00:10:45.250 - 00:11:47.772, Speaker B: It submits those proofs on chain to advance the stateroot that currently costs 330,000 gas with our implementation. My understanding is that the lower bound for these types of proofs is 200,000 gas. So there's some wiggle room, but we can't get it down by like an order of magnitude. It's kind of just the cost of doing business with ZK roll. It's on Ethereum right now, definitely looking for input there. And then there's also a forced inclusion path to avoid censorship by the sequencer, you can submit your recovery directly on L1, and then the sequencer is forced to include your recovery in an upcoming batch. So the way the force inclusion mechanism works right now is that for every one off chain transaction that the sequencer wants to approve, basically something that they receive through an RPC call, they have to include one of the forced inclusion transactions alongside it.
00:11:47.772 - 00:12:34.272, Speaker B: So when you submit your forced inclusion transaction, you're not guaranteed any sort of ordering, you're just guaranteed ordering within the set of force inclusion transactions. The sequencer will be picking one of each and going from there to create its batches. So you'll be eventually included in the state of key space. So once we have the updated key space route on l one, where we really want this is l two. Most of the smart wall activity we expect to happen on l two. You can have a smartwall on l one, you can read the key space state route from L1, but where the action really is is l two. So we need to get these key store routes onto l two.
00:12:34.272 - 00:13:33.202, Speaker B: And the way that we do this currently differs depending on what kind of chain we are syncing to. For op stack chains, the l one block hash is available from one of the built in contracts, so we can do merkle proofs from that l one block hash to be able to sync the keystore state route to a contract on the l two that wallets can access. For other chains that don't have the l one block hash, we need to rely on an oracle for our current proof of concept. We're sinking to, I believe, seven chains, and then the 8th chain is ethereum mainnet. The way that we get to most of these chains is using wormhole as an oracle. At the end of the day, we don't want to necessarily rely on a single oracle. We're investigating things like Hashi that can produce consensus among oracles for the l one block hash and then proof from there.
00:13:33.202 - 00:14:45.290, Speaker B: But the current deployed system from the latest release from this week uses wormhole to sync the key space route directly, so there's no intermediary of the l one block hash that's being synced. Since we're using an oracle, we're just saying, just get me the key space state root and get it to the chain that I care about. So that's how we get to most of the chains right now. So there's those two trustless chains right now, which are based on optimism and of our supported chains that the op stack chains and then the rest are oracle based chains. Arbitrum is an l two, so it could theoretically have the l one block hash that contracts could access, but there's currently no way to actually access that l one block hash on arbitrum that we know about. So if we're wrong about that, we want to know, and if we can get that l one block hash, we definitely want to have it. The other way of generating these proofs would be instead of using the l one block hash to use the the EIP 4788 beacon root proofs, we've implemented this, but currently I don't think we're actually going to end up using this anywhere.
00:14:45.290 - 00:15:49.820, Speaker B: For the chains that are actually roll ups, they tend to either implement the l one black cache or nothing at all. Arbitrum, for instance, doesn't provide the l one block hash, but they also didn't implement EIP 4788. Their approach was to basically make the beacon root oracle, I think, revert if you try to call it, because we're like, we don't have a beacon chain, so this shouldn't return anything. Op stack does support beacon roots, but since we have the l one block hash already, we're just using that to prove that. So we expect that mechanism won't be used, but it's a an avenue. If for some reason people prefer implementing that to the l one block hash oracle. The other benefit of the beacon oracle is that the standard specifies the address that the oracle should be reading from, and the addresses kind of have cascading effects on addresses throughout the system.
00:15:49.820 - 00:16:41.730, Speaker B: If I have to refer to a particular address on one chain and a particular address on another chain, then my ability to use create to generate these contracts is limited. I'm going to have to use something else to get the consistent addresses. But with the beacon root oracle, we could actually deploy it using create two and that would be nice, but we don't think that's going to happen. Lastly, the deposit transactions are the most straightforward way to do this for rollups, but they're just very expensive right now. But I think ten times more expensive than doing Merkle proofs. So Merkel proofs it is. So once we have the key space route on DL two chains, the all L1s, everybody knows what the latest root of key space is.
00:16:41.730 - 00:17:47.759, Speaker B: The next thing is for wallets to actually integrate with key space. And the way this integration works is if you think about the way you have signing keys in your smart wallets today, you just store it directly in the contract storage. So when you're integrating key space, you basically remove that your signing keys no longer live in your smart contract, they live in key space. So if your signing keys aren't there locally, that means that with every transaction you send every single one, you need to prove that the signing key that you're signing with can actually access the wallet. So instead of just providing a signature of the transaction that you're trying to send, you provide a signature. You provide the data that's stored in key space for your record, which in this case is the public key, and then you provide a proof of your configuration in key space. This proof is a snark right now, but it's equivalent to a Merkel proof of the state in key space.
00:17:47.759 - 00:18:30.582, Speaker B: There's nothing fancy going on there. It's just cheaper to verify the snark than it is to verify a Merkel proof. And actually that may be incorrect. Let me back up there. By using snarks for the state proof when you're sending a transaction with your wallet, that gives us the ability to aggregate those proofs later on. Currently there's no aggregation happening, but if we built a ERC 4337 aggregator, we could aggregate those proofs, which would lower the overhead for these transactions. So once you're providing all this data, you do your normal signature verification.
00:18:30.582 - 00:19:35.972, Speaker B: But on top of that you also need to verify the snark that's saying that hey, this key that I'm using is actually in the latest state of key space for my wallet, and with those two things together, your transaction is authorized. So if that's happening on the smart contract side, on the client side, in your JavaScript library and your mobile app or whatever you're doing, in addition to just signing the transactions the way that you usually do, you also need to be fetching that state proof from a key space node when you're in the process of signing a transaction, making sure that you have the latest proof to be able to send with your transaction to authorize that signature. So that's how the wallets are integrated. There's also work that bundlers can do to make key space work better. So one is to support those ERC 4337 aggregators that I was talking about. Those state proofs that we're including within each transaction add at current prices an overhead of about $0.11 for each transaction.
00:19:35.972 - 00:20:13.632, Speaker B: That's much higher than our goal. We want transactions to happen in 1 second and for them to cost one cent. And if we have an eleven cent overhead for these transactions, that's not where we want to end up. We want to get it much lower than that. So aggregation is one path towards lowering that overhead. And then another thing that bundlers can do for us is to allow our state lookup across contracts. So when we provide that state proof with each transaction and we're generating and we're verifying the proof, we need to access the latest keystore root value.
00:20:13.632 - 00:21:03.812, Speaker B: That value lives in another contract. And the ERC 4337 standard does not allow that in the process of validating transactions, and it does this to protect the bundler from losing value. However, the bundlers that we've spoken to are very aware that that constraint is too strict. So basically making sure that there are possibilities to work around that by configuring your bundler instance to say hey, these specific state accesses we know about, just allow them. The standard doesn't allow them, but we've looked at them and just let it go. That's one path for allowing this. The other is to create kind of a standard singleton contract for these cross contract accesses.
00:21:03.812 - 00:22:21.562, Speaker B: And what would be special about the Singleton contract is that the bundlers issue is that they don't know when the state is going to change in this contract that you're doing this access to. So if you have a singleton contract where each storage slot has a kind of frequency that is enforced by the contract, that you try to update it and it just doesn't allow it because you updated it too recently, then the bundlers can say, hey, we trust accesses to this contract, you don't have to tell us about it. You don't have to get each one of us to configure this because we know this contract enforces the constraints that we need to be able to protect our resources, that's another path forward. So either per application configuration, we hope that people value key space enough to be able to do that. And just a general solution for all applications that want to be able to do cross contract accesses in their smart wallets is another path forward. Lastly, there's work that rollups can do to make it easier to integrate with key space. We're already deploying to rollups deploying all l one s, but the cost and the experience can be better if rollups were doing more to support our use case.
00:22:21.562 - 00:23:15.858, Speaker B: So one thing is just support reading data directly from l one that if we're doing that, then we wouldn't have to do all the syncing work to get the root from l one to all the other chains. And if you're a roll up, you have that l one node right there. Why would you let me read from it? It would be so convenient if you just let me read from the l one node that you have on your machine. So there's various proposals out there to make progress on this, but my understanding is that nothing really has legs yet. The scroll has their own implementation that they've already gone and rolled out. And that makes their implementation of a keystore much more convenient because I can just read all one. There's an op stack proposal that seems to be stale now, but they basically had a remote static call opcode that would be very useful, but we would like to read from l one.
00:23:15.858 - 00:23:53.294, Speaker B: That would be very nice. Another thing that they could do, and this is very much a wish list. I don't know how, how convincing I can be on this front, but if rollups, if all, if all the l two s, all the l one s, if they just decide that key space is so important that they want to make key space part of their consensus, that would be great. Then you could just read directly from key space. I don't have to do my state proof of like, oh, this is the key that controls my wallet right now. Just read from key space and say, okay, here's my opcode that lets me read from key space and says that my. That I'm allowed to do this.
00:23:53.294 - 00:24:11.884, Speaker B: That would be really great. I don't expect that to happen. But if y'all are excited about it, I want that. That would be good. So in terms of our roadmap, where we're going from here. So we have this testnet beta that's on the eight chains that I mentioned. I like.
00:24:11.884 - 00:24:48.490, Speaker B: It's, it's in a state where it works. And we just need to make it better in terms of speed, in terms of cost, in terms of operational efficiencies and in terms of security. So that's really where we're headed. The kind of the top line issue right now is optimizing our recursive circuits because that has cascading issues, cascading effects to the whole system. If we change the proving systems that we're using, that's basically rewriting all the circuits involved. And that's kind of a big impact. So we want to take on that cost earlier rather than later.
00:24:48.490 - 00:25:48.220, Speaker B: And there's various systems that have pitched that they have 30 x 100 x speed improvements in terms of proving time over what we're currently using, which is kind of the first generation of Planck proofs. So we could try using plancky two and see if that gets us the speed up that we want. We could try using Plank E three, which currently it seems like there's a lot of momentum behind. In particular, if we end up using planky three, then we can benefit from using something like the SP one ZKVM. So when people want to write their own custom contracts with their custom wallets, with their custom recovery logic, we're going to provide our own circuits with general recovery logic out of the gates. If you have like a m of n multi signature wallet, you're probably not going to have to write your own circuits. But if you have some sort of fancy wallet that you want to implement, you do currently have to write your own circuit to get that implemented.
00:25:48.220 - 00:26:41.830, Speaker B: It would be great if instead of saying hey, write some domain specific circuits to get your logic written, if it could say, write some rust or write some general purpose language, have it compiled to using a ZK VM and just plug in to our verification systems, that would be great. So that's one benefit that we see from that. We're not necessarily right now at the cutting edge of knowing what the latest going on in the Zk ecosystem is. So again, if planky three and SP one sounds like, oh, there's actually another approach that you should be taking looking for input there. But right now we're excited about trying out planky three and seeing where that gets us. Instead of having five to ten minute proving times. I hope it gets down to seconds and that'll be pretty cool.
00:26:41.830 - 00:27:38.312, Speaker B: Another thing we want to do is the ERC 4337 aggregation that I mentioned before. We also want to think about making key space in l three. So what that means is I was talking about how the lower bound of our batch proof is 200,000 gas and right now the actual cost is 330,000 gas. That ends up being, at current prices, eight dollars to twenty dollars for a batch proof to settle on Ethereum's main net. That's a problem for our application. Like if you're, let's say you're a Zke EVM, hopefully you have enough transactions happening in your system that you can kind of spread that cost out among all the users and it doesn't end up being that big of a deal. We are a minimal ZK roll up for changing keys.
00:27:38.312 - 00:28:41.548, Speaker B: People don't change their keys that often. So our low utilization ends up making that cost of proof validation a big deal. In the early days, it's very unlikely that we'll be able to batch any recoveries together. Like if a recovery is happening, the chance that somebody else is changing their key at the same time very low. Are we going to tell people that when you want to change your key, it's going to change once per day, and we're going to batch up the ten recoveries we have each day to get the cost down? That's probably not the greatest story. So instead what we expect to happen is to get that proof validation cost off of l one, and to have it on an l two instead, using some sort of a special purpose l two, that's for aggregating proofs. So when I say it's an l three, it's not that it's settling on base or settling on some other general purpose l two, it's really thinking more like something like polygons ag layer.
00:28:41.548 - 00:29:41.140, Speaker B: I think a veil has a proof aggregation system. I think in the stark wares ecosystem they have their own kind of proof aggregation system. That's the kind of l two that I'm talking about, where again we're seeking a minimal attack surface, some minimal functionality, but really we're just seeking something that aggregates the proofs so we can settle to l one and share with everybody else that happens to be trying to settle their proofs to l one at the same time, rather than taking that cost on ourselves. So that takes the cost down significantly. I think Vitalik was writing an estimate that's basically like the cost of a storage plus some bookkeeping operations. So something like 8000 or 13,000 gas. I forgot exactly what the estimate was, but it's a far cry from 330,000 guests, and that would make it much more feasible to have even potentially, users cover the costs of their own recoveries.
00:29:41.140 - 00:30:33.630, Speaker B: Right now in the testnet version, that's just we're running the thing. If we were launching key spaces in l two, we'd probably try to find some way to cover that cost, because it doesn't make sense for users to pay eight to $20 to change the keys. So trying to get to a path where we can, even if we launch with an l two, where we have a roadmap, to get to an l three, where those costs no longer need to be subsidized at that large of a scale. Speaking of subsidies, we also want to be able to collect user fees from recovery transactions. And when I say collect user fees, I'm not saying we want to be rich, we want to make money from users changing their keys. What I'm saying is that we want key space to be more resilient than it is today. Right now, the operation depends on the generosity of coinbase incorporated.
00:30:33.630 - 00:31:23.390, Speaker B: We run the thing the costs are covered by. We're running a sequencer. It's just the keys are going to get processed. But for key space to be the kind of ecosystem wide infrastructure that we want it to be, it cannot depend on generosity. It has to depend on economic guarantees. And instead, so the current thinking around making sure that there's a way for users to pay for recoveries or for their wallet provider to pay for their recoveries for them is to allow intents to be used to trigger recoveries, rather than just directly sending your recovery to the key space node. What that means is making sure that there's a way for the sequencer to prove that they've included a particular, included a particular user's recovery transaction.
00:31:23.390 - 00:32:21.466, Speaker B: So then they can go to whichever chain that the user initiated their intent on and claim the value that they want to be compensated with for processing that recovery. In that model, we could continue subsidizing it. Like if Coinbase is running the sequencer and we say, hey, we support these intents, but like we're just covering it for now. So we don't care if you actually funded an intent or nothing, we could keep doing that. But then on the day when Coinbase decides we're not so generous anymore, we're not subsidizing this anymore, anybody else could step in and say, okay, I'm going to run a sequencer, but instead of doing what they were doing and just accepting any recovery, I'm only going to accept recoveries with the funded intent, and then I'm going to collect that money. At the end of the day, that's the kind of workflow that we want to support so the system can be more resilient and not dependent on, on subsidies anymore, even if it is subsidized. Lastly, we want to be able to support time delayed recoveries.
00:32:21.466 - 00:33:01.990, Speaker B: So one very common pattern in terms of securing crypto systems is just time delays. If something goes wrong, if you have the time to take action to reject whatever was happening, that makes systems easier to trust, easier to reason about. And right now, there's no mechanism to have time delays within key space. So let's say you actually lost your key. It's not like, oh, I'm changing my key, I have this one pass key, I want to approve another one. But man, I lost my key. I don't have it anymore.
00:33:01.990 - 00:33:45.340, Speaker B: It would be nice to be able to have some sort of guardian feature where there's somebody else who can help you recover your key. But right now in the current implementation of key space there's no distinction between different recovery methods. If I'm adding a guardian, they have as much rights as I do to my wallet. They have control to all my funds and I probably don't want that. So time delayed recoveries allow us to add basically another type of recovery where when you initiate it there's some sort of configurable waiting period. And then after that waiting period has passed that's when the key change is processed. So that ends up being a significant change to the circuits.
00:33:45.340 - 00:34:42.910, Speaker B: Because right now when you're the account circuit, which is what's used to prove the key changes the inputs that it takes are just user provided inputs. There's no kind of environment provided inputs to the circuitous. And that's what we would need to be able to implement this kind of prepare and commit two stage process for time delayed recoveries. So the easiest thing to reason about is let's say you were initiating your time delayed recoveries on l one. Then we need some way to get the l one data into the circuit and improve there. I am assuming that that path is relatively straightforward that if we wanted to allow the recoveries to be initiated on l one doing the proof in circuit from the l one block hash might make. Might make that possible.
00:34:42.910 - 00:35:30.840, Speaker B: Where it gets harder is if we want to say, okay, it's expensive to initiate the recoveries on l one. So maybe we want the guardian to be able to initiate the recoveries on an l two. What does that look like? Well, if it's an optimistic l two the settlement times are fairly long. But maybe those settlement times are actually not an issue in our time delay. If our time delay is one week or two weeks then the fact that it takes a week to settle maybe we don't care and we just end up doing proofs against an l two in that way. But also want to investigate the ZKl two s and doing time delayed recovery proofs from those chains as well. So that's kind of an overview of where we're heading.
00:35:30.840 - 00:36:09.808, Speaker B: We're always looking for feedback in terms of like zero knowledge systems. We've learned so much from the people who've been doing cutting edge work there's always more to learn there. So looking for feedback on that roll up architecture. A lot of people have been thinking about how to make this happen. Well, we're looking for feedback there. And if you've just been thinking about wallets for a long time and you have a case that you want to throw away, we're always interested in talking that we want this to be a neutral piece of infrastructure for the whole ethereum ecosystem to rely on. And at the end of the day we want millions of users using smart wallets to send their transactions that cost $0.01
00:36:09.808 - 00:36:18.410, Speaker B: that take 1 second to process so we can build an economy on top of these decentralized systems that empower people. That's key space.
00:36:20.950 - 00:36:43.900, Speaker A: Amazing. Thank you so much, Niran, for this extremely thorough and comprehensive presentation. We have about 20 minutes for questions during the public part of the call. Anyone want to kick us off on the questions? Len, go ahead.
00:36:45.000 - 00:37:22.256, Speaker C: So yeah, thanks for the talk. I have a question about. So you mentioned that you probably or you don't need pre conformations, but you also mentioned that you're gonna use like the vanilla based sequencing, which is actually a pre confirmation solution because it. So where they require the l one proposer to explicitly opt in. And opting in is required because they need to be like subject to slashing condition. And the slashing condition is required because you need like slashing when they equivocate on the pre conformation. So I was wondering like why you need that kind of explicit opt in.
00:37:22.256 - 00:37:26.980, Speaker C: Like is it going to be used for something else and pre conformations?
00:37:28.800 - 00:38:44.242, Speaker B: That's a great question. So the answer is I don't know what. I don't know. So the thing that seems attractive about vanilla based sequencing is really kind of the smoother adoption curve. The fact that like the, we want to kind of leverage the distribution of proving slots that the l one logic already provides for us. So the opt in is really, and the thinking so far is just a way to like when the nice kind of the happy path for distributing these proven slots is literally for every l one sequencer to be running key space or for many LM sequencers to be running key space. Because then we know that many people are involved in proving for key space and the chances of liveness failures because there were like few people actually involved doing it and then they decide to stop is reduced.
00:38:44.242 - 00:39:24.870, Speaker B: So really that's what we're looking for in the approach. If there's a simpler way than vanilla based sequencing, if the opt in isn't actually necessary, that's definitely something we want to think about. But the way I think about the opt in right now it's just we're trying to know who to select from. In both the happy case where the current l one sequencer is saying hey, I want to participate in the system, or in the unhappy case where it's like the current l one sequencer knows nothing about key space, who do we pick now? Should be one of the people who've opted in. But if there's, if there's another way to do it, and I suspect that there might be, I'm very interested in that.
00:39:26.660 - 00:39:32.040, Speaker C: I see. So it's kind of like to get the liveness guarantee from having explicit opted in.
00:39:32.780 - 00:39:33.572, Speaker B: That's what we're looking for.
00:39:33.596 - 00:39:36.120, Speaker C: Yeah. Okay, I see. Yeah. Thanks.
00:39:38.340 - 00:40:09.910, Speaker A: And once you have user fees, then I guess the searchers will be interested in providing bundles to the builders. And you'll get this for free with PBS. But one of the things you want to make sure of is that you don't have this contention with different people building proofs that are inconsistent with each other. But if you can get the proving time to be under 12 seconds, which might actually be possible in your case because you have a very simple virtual machine, then this is the best, best case scenario where you don't need any opting in.
00:40:12.810 - 00:40:48.652, Speaker B: That would be great if the proofs were that fast. There's a question in the chat. Is proving and sequencing permissionless? It currently is. And what we've deployed right now, there are two paths towards a broader launch. One is just biting the bullet and doing base sequencing and having the first production release of key space using that method. And then it would continue to be permissionless and we would just be relying on the LN sequencer distribution to make that happen. There's another path towards release where we say, okay, that work is going to take too much time, but we want to get this out there.
00:40:48.652 - 00:41:12.392, Speaker B: Let's just launch the way that most roll ups launch and have a whitelist of allowed sequencers and launch them that way. So that's the scenario where it doesn't end up being permissionless, but that's not our end goal. Sorry, that was just my question. But thanks, that was a great talk. I really enjoyed that. And the way you explain yourself, how.
00:41:12.416 - 00:41:15.434, Speaker D: Would you weigh up the odds of.
00:41:15.592 - 00:42:20.650, Speaker B: Path one versus path two right now in your head? Like path one being going based and path to having this whitelist? And do you have an intuition for that? I think there's a decent chance that we end up actually implementing base sequencing. That depends on ideally we're not super early doing that, but especially if there are live implementations out there, I expect that we would follow quickly with our own implementation and launch in that way. But we just haven't done enough research to know how big of a commitment implementing that would end up being. If it ends up being very large, I can see us launching. It seems fairly typical to kind of kick the can on that particular issue with rollups and say we're just going to gradually decentralize it. So if I had to say which one is more likely, I would probably say gradually decentralizing it, but it would be better than that.
00:42:21.830 - 00:42:22.770, Speaker C: Thank you.
00:42:23.510 - 00:42:25.366, Speaker B: I have a question, I have a.
00:42:25.398 - 00:42:27.350, Speaker E: Question about the latency.
00:42:27.510 - 00:42:30.990, Speaker B: So for the various key store rob.
00:42:31.070 - 00:42:35.802, Speaker E: Operations, what are the latency requirements in your mind?
00:42:35.966 - 00:42:38.418, Speaker B: I think this is related to what.
00:42:38.434 - 00:42:43.794, Speaker E: Kind of sequencing model you end up choosing. And if you are using proof aggregation.
00:42:43.882 - 00:42:49.642, Speaker B: Then because we are working on proof aggregation, we want to understand what's requirement.
00:42:49.746 - 00:42:56.390, Speaker E: On the latency side when people are doing this ZK operations, if that makes sense.
00:42:57.130 - 00:43:45.380, Speaker B: Yeah, that makes sense for our use. Case changing keys it's typically not super time sensitive because of the requirements for changing the key. You have to have access to the old key, you're signing the new key, you want to be able to use the new key. It would be nice, but since the wallet has access to both keys at the same time, if it's slow to update, you just keep signing transactions with the old key until the update happens. If it took 2 hours for a key to update, I think would be fine. I think we have kind of like a constraint that we put in one of the design documents that might be less than that. But if your keys change in 30 minutes, I think we're good.
00:43:49.400 - 00:43:50.220, Speaker E: Thanks.
00:43:57.090 - 00:44:12.710, Speaker A: One question I have is around governance and how updates to the contract are done. Ideally we have a super simple virtual machine and it's fully immutable. But do you think it's more likely than not that there will be governance initially? And what would that look like?
00:44:14.290 - 00:44:57.470, Speaker B: So the current thinking is no governance at all. So basically deploying key space as an immutable system, and when an upgrade needs to happen, you deploy a new key space. The lever for upgrades in that scenario would be on the wallet side rather than on the key space side. When you want to update to the latest version of key space, you update your smart contract wallet. That wallet is pointing to a different key store and using different logic than your initial wallet was. So because the wallets themselves are typically upgradable, we believe that key space probably does not need to be and it's probably better if it's not. So then people can opt into whatever version they want to use.
00:45:01.170 - 00:45:10.230, Speaker A: Amazing. So a very similar model, I guess, to uniswap where you have v one, v two, v three, potentially exactly like that. Approve.
00:45:11.170 - 00:45:11.866, Speaker B: Hello.
00:45:12.018 - 00:45:58.240, Speaker E: So I have two questions. Actually, the first one was based on what you just recently answered. So regarding the part where he said, if you have access to both keys, and even if it takes like 30 minutes to update, it's fine. So my first question is like, obviously this, like, if you lose, I was thinking this scenario like, let's suppose you lose a key, right? For any XYz reason, maybe you realized you pushed it on GitHub by mistake, or you did something and like, similar to the web two world, you lost, you lose an API key so you can instantly go revoke it. Now this doesn't exist in the AOA world, but I do you think like smart contracts would like to provide this feature? Like by any chance if you leak your private key, you can instantly revoke it, because if that's true, then a 30 minutes or two hour upgrade might be like very late for something like that.
00:45:59.340 - 00:47:19.350, Speaker B: So you're absolutely correct there. In terms of thinking of the engineering behind this, it's kind of easy to take the lazy approach and be like, the current systems do not provide this. So do the new systems necessarily need to provide this, at least for the first iterations of this kind of system? It's much easier to approach it in a way that where latency is acceptable because the existing eoas just don't provide this at all. And the main scenarios of key changes, like the fact that smart wallets allow you to change your keys, I guess maybe some people are implementing it as kind of like a disaster recovery sort of thing. Oh, I exposed my key, let me change it. But the primary use case that I've seen for it is just letting you move to a different device or move to a different signing key rather than the you, rather than the fact that you've exposed it. Since exposing a key is a catastrophic thing today, expecting it to continue to be catastrophic is the way that we're currently thinking about it.
00:47:19.350 - 00:47:44.850, Speaker B: Don't expose your key. That's bad. The key space can, like just technically speaking can help people respond to that. I published my key on GitHub. Let me change my key as fast as I can. Like it's possible to design a system that does that. It's just in my head so far down the list on things that wallets need from us that it hasn't been a priority to design for them.
00:47:45.910 - 00:48:07.000, Speaker E: Makes sense. Thank you. And my second question was around, I think in the arbitram side, you mentioned that you have to use an oracle because you don't have the block hashes. So maybe I'm missing something, but is it possible that you deploy a contract on l one and you send the previous block hashes by, like, l one, l two messaging, or some other way?
00:48:08.100 - 00:48:52.670, Speaker B: Yes, so we could implement a trustless deployment of key space on arbitrum through deposit transactions. The only reason that we don't is because the deposit transactions are expensive. But I honestly hadn't thought of it deeply until you just mentioned it. So it really depends which factor ends up being more important to people. If people are like, hey, we want trustless deployments of key space in as many places as possible, then we would just fall back to deposit transactions on arbitrum and achieve trustlessness that way. If the costs are more important, then since we don't have a cheap, trustless way to do it on arbitrum, we would stick with the oracle. But that's a very good point to bring up.
00:48:52.670 - 00:48:54.630, Speaker B: We could just use deposit transactions.
00:48:56.730 - 00:48:58.150, Speaker E: Makes sense. Thank you.
00:49:07.450 - 00:49:14.270, Speaker A: One question I have is, what is the team size of the keyspace project?
00:49:15.530 - 00:49:55.566, Speaker B: So right now it's about three people. I think we have one more person who's about to join the team. But just so I'm not taking credit for somebody else's work, this work has been led by Michael the Hoog, who's done a lot of thinking about the keystore roll up. Really kind of wanted to be like a torch bearer for like, Vitalik did all this writing. It's like, hey, like, somebody needs to go and implement this and making sure that it gets implemented in a way that the ecosystem can rely on. So Michael's done a lot of thinking on that. I joined the base team in like late January, early February, so I haven't actually been around that long, although maybe I need to stop saying that because the months are passing pretty quickly.
00:49:55.566 - 00:50:31.020, Speaker B: But I've enjoyed, like, it's been a very fun project to work on. I've been in like, the crypto space for a while, and most of the projects that I work on, you know, having real people use them and get value from them, it's hard to get to that point. But working on something like key space where it can, like, be a core piece of infrastructure for, for the ecosystem, has been a relative delight in that fact, because I can, like the other wallet teams like interacting with them, getting to know like their concerns because they actually want real users to be impacted by these systems. It's been a fun ride.
00:50:38.200 - 00:50:52.960, Speaker A: And if there's no more questions, I'll keep asking. You mentioned that you wanted to potentially do a fast follow in terms of launching a base roll up. Do you have an ideal timeline in mind as to when you'd like to launch?
00:50:54.660 - 00:51:34.450, Speaker B: So we're currently on a cadence of roughly quarterly launches. So really getting as much out as we can each quarter is the idea for the next release. I expect that sequencing isn't actually going to be super high on the list because we're going to be doing all this work, trying to evaluate planky three, all this work on. There's another thing that was super, I forgot what our second priority was, but the, currently there's higher priorities than the sequencing, so I expect that that might get punted down a little bit. So if we were launching a base roll up, I would currently guess that that would be early next year.
00:51:38.350 - 00:51:57.460, Speaker A: Understood. Thank you. Any final questions before we wrap up the public part of the call? Or maybe Niran? If you have questions for this group, we could also entertain them.
00:51:58.240 - 00:52:29.258, Speaker B: Yeah. The main things that we're always looking for is feedback on base sequencing in general. We've been following the discussion from afar. When our assumptions sound like they might be off, like thinking of yourselves as the experts rather than us and giving us the feedback. Definitely interested there. And like just in terms of choosing like ZK stacks and things like that, the experiences that people have had, that's always welcome as well. Justin, thanks for all the information you've provided for us so far.
00:52:29.258 - 00:52:33.030, Speaker B: That's been very helpful and always, always interested in more feedback.
00:52:36.350 - 00:52:37.170, Speaker A: Len.
00:52:37.590 - 00:53:26.492, Speaker C: Okay, so I have a more nerdy question about the based reconfirmation sequencing side. So. And it might be more for George, but um, like, so in the vanilla based sequencing, so like you randomly elect for a non opted in slot, or when the proposal is not not opted in, you randomly select like uh, a validator who has opted in. But the problem here is that they don't have control over that slot. So they can't promise that they will actually include the transactions in their allocated slot. So it can like come delayed if like the, because they have to go through the mempool and the builder might not include them. And maybe the builder will include like the next slots, like opted invalidators transaction before, like the current random one like that.
00:53:26.492 - 00:53:59.700, Speaker C: So like another approach is like from Justin's, like original based roll up design where art sequencing, a pre pre conformation design where like you just let the next pre confront the look ahead handle like the preconfirmations for like the non opted in slots. So this way eventually that entity, that proposer will have their slot and they will have control over it so they can include it. It seems to give better guarantees. So I was wondering about that part of the design.
00:54:01.840 - 00:54:29.060, Speaker B: That's a great point that I hadn't thought too deeply about. So basically, just to restate what you're saying, if we select a random validator from the pool of validators that have opted in and say, hey, you're the one who's doing the next proving block for key space, they may not actually be able to get their transaction into a block. Yeah, I need to think more about that. That makes a lot of sense. So what's your suggestion? Go ahead.
00:54:29.960 - 00:55:16.930, Speaker D: Yeah, hi. Well, there is not too big a difference between what Justin is suggesting and what Manila based sequencing is suggesting because you certainly end up with a scenario where you might not have a single opted in based sequencer within the looking at Q. So you end up with the same things. The good part of what came up out of Zubrim is this l one prank affirmations gateway that actually enable you to, despite not being the basic answer yourself, you can get a lump breaker formation of the inclusion of this transaction, which is then again even furthermore increase. But yeah, this is the, the trade off within any base sequencing when the current l one proposal has not opted in.
00:55:17.910 - 00:55:18.690, Speaker C: Yeah.
00:55:19.030 - 00:55:23.850, Speaker D: And I hope there is no static noise around main.
00:55:24.470 - 00:55:35.166, Speaker C: Yeah. Like. Yes, in the case that there's no peak conference in the look ahead. Yeah, you do have to fall back to some random, but it doesn't have to be random at every slot. That seems.
00:55:35.198 - 00:56:01.506, Speaker D: No, no, I don't think you understand understood the correct thing. The elected sequencer. The elected sequencer through the fallback selection can use the l one preconformations infrastructure that is currently being built. I know, for example the header. That is header, right. Hilder. That is being built.
00:56:01.506 - 00:56:29.932, Speaker D: Sorry. Shout out to drew. Yeah. So helder is currently being built. So you can use that. And now that you have l one pre conformations, the cool part is that in this way you can have even more sequencers through vanilla based sequencing and you can have more decentralization because you're having rotation among actors. Because one of the worst things in my opinion, you can do is to give a monopoly over 1012 minutes over a single, to a single proposal.
00:56:29.932 - 00:56:44.402, Speaker D: So in this way we're actually shuffling monopoly every 12 seconds. I'd argue that this is better in terms of, I guess many decentralized points.
00:56:44.586 - 00:56:52.270, Speaker C: So is it that there's going to be a separate l one inclusion pre confer entity that can guarantee the inclusion?
00:56:54.770 - 00:57:22.480, Speaker D: That's what currently is being built in Helder. Again, the scope of l two and l one preconformations are differing and they are diverging at certain places. Currently there is l one preconformations gateway being built as a testnet that is going to be. I don't know whether I should be saying this but expect some exciting stuff on HCC. I guess.
00:57:23.740 - 00:57:33.770, Speaker C: I see. Even if you have the inclusion I'm still wonder about the order of these landing. But I guess you can handle that at the contract level.
00:57:34.230 - 00:57:34.630, Speaker D: Yes.
00:57:34.670 - 00:57:36.158, Speaker C: Yeah. Okay, I see.
00:57:36.294 - 00:58:18.306, Speaker D: Yes. One of the worst parts to be like clear on this one is that it's hard for one to prove who's the current proposer. But yeah, I have written extensively in the white paper on the various ways that you can mitigate this, this downside. I think that is a very simple solution to this. But as our great friend Connor says, we don't like designing systems that bent on several l one forks. So there are like workarounds that don't require l one forks. But l one fork is going to help us tremendously just by telling us who's the current proposer, their BLS public key or whatever it is there.
00:58:18.306 - 00:58:31.210, Speaker D: And then base sequencing can be like many, many points of vanilla. Base sequencing and base sequencing can be enforced through solidity and smart contracts, which is going to be. Yeah, very cool.
00:58:33.790 - 00:58:34.810, Speaker C: Yeah, thanks.
00:58:36.830 - 00:58:58.670, Speaker B: Just to make sure I understood what you're just saying, if you have l one pre conformations, then when in the case where you're choosing a random opted in validator, they know that their transaction is going to get into the block because they have pre confirmation of it. But what if they try to get a pre conformation and they don't get one?
00:59:00.530 - 00:59:24.824, Speaker D: This is the same as submitting this within a block along with the precomprofession, but then they're paying the inclusion tip. So yeah, I mean it's the risk of when you're not actually the proposer. There is definitely a risk in any case that is not primary selection. Regardless. There is, there is. There is this risk that is there.
00:59:24.992 - 00:59:27.220, Speaker C: Yeah, it can be the case with.
00:59:28.600 - 00:59:42.750, Speaker D: We can discuss more on this option in order for me to understand like what's better, better like what's going to be important for you there in order to be able to advise later on solution, I guess.
00:59:46.410 - 00:59:51.610, Speaker A: Okay, Niran, thank you so much again for this presentation. This concludes the public part of the call.
