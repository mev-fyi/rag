00:00:00.570 - 00:00:29.480, Speaker A: It seems like most of the client teams are represented. Unfortunately, I don't see anybody who's going to be able to live stream this call. So I'm going to turn the record button on and then we can get started. I will not turn the record button on because I do not have permission to turn it on. Let me mess someone and see if we can get it recorded. But we can get started. If not.
00:00:40.110 - 00:00:44.250, Speaker B: I think it's being recorded. Actually, it's us recording at least.
00:00:44.320 - 00:01:07.220, Speaker A: Yeah, you're right. Okay, I'll just message him later and we'll get the recording. Thanks for mentioning that. Let's go ahead and get started then. Okay, great. Welcome everyone to the Denkone. Interrupt testing call number 32.
00:01:07.220 - 00:01:20.680, Speaker A: Doesn't look like there is a whole lot of things on the agenda today. Maybe let's get started with an overview of what's been going on on Devnet nine since we just launched that last week. Does anybody want to share?
00:01:24.090 - 00:02:02.430, Speaker C: I can get started. So Devnet nine launched on Friday by mistake. I made a bit of an oopsie by launching it with 170 validators instead of the 1300 that I anticipated. So I made quickly some deposits and during the weekend all the deposits have reached the peak and chain and now we have the intended start. So we have a validator distribution of 70% in gas and. Nevermind and let me get the actual split.
00:02:04.610 - 00:02:05.360, Speaker A: I.
00:02:13.880 - 00:03:10.630, Speaker C: I don't have it ready at hand, but yeah, 70% is geth and remind and we have Ericon and Bezu in the low twenty s or eighteen percent or something like that. And Ethereum js and breath in the remaining 4% or so. And so far all the clients seem to be staying up to head. And just earlier today we had some prism issues, but they seem to have resolved themselves just by restart. And currently the only pair that is unable to stay sync is nimbles and Ethereum Js, which seems to be a problem on the Ethereum Js side. That's it for maybe someone from some client teams can comment on.
00:03:11.240 - 00:03:14.496, Speaker A: Yeah, that'd be great. Merrick.
00:03:14.688 - 00:03:15.188, Speaker D: Yeah.
00:03:15.274 - 00:03:15.940, Speaker E: Quick question.
00:03:16.010 - 00:03:27.930, Speaker D: Does anyone know something about bad blocks? Because I see free bad blocks being produced. Not sure if anyone investigated that.
00:03:31.230 - 00:03:50.210, Speaker F: You can give an update on that front. We are in fact producing bad blocks with bad version hashes. We're still investigating it. It's weird because we're fine accepting them, but we're producing them incorrectly. We haven't seen that before. It's some type of regression. So we're going to look to make sure that the trusted setup is being used correctly.
00:03:51.590 - 00:03:59.880, Speaker D: Okay, thank you. So basically all three blocks were produced by Bezu. Am I correct here?
00:04:00.410 - 00:04:08.410, Speaker F: I'm based off some analysis that Marius did and posted in the discord. I have not looked any further.
00:04:08.990 - 00:04:09.740, Speaker D: Okay.
00:04:13.550 - 00:04:34.000, Speaker A: Good to know. Gajender, did you have an update? If you're speaking, I can't hear you unmuted.
00:04:36.820 - 00:04:38.950, Speaker D: Now it is, yes.
00:04:39.320 - 00:05:18.430, Speaker G: Okay. Yeah. So on Ethereum Js side, most of the issues that are coming up is because of some sync related to our beacon sync. Basically we seem to be having some issue where we are resetting the chain to genesis. And so there are iterations that we are trying to roll out and see what the issue is and sort of rectified. But on nimbus Ethereum JS pair, I don't see that is the issue. And that does not seem to be Ethereum Js problem because it's not even.
00:05:18.430 - 00:05:42.410, Speaker G: I mean, basically the fetchers are not getting the data from the peers to basically complete the beacon sync. I think that is the issue. And I'll check what is happening over there. Maybe it doesn't have good set of peers on its end and I'll coordinate with Barnabas on that.
00:05:45.420 - 00:05:52.520, Speaker A: You're saying this is on the Lodestar side? It doesn't have good peers? Or were you talking for East JS?
00:05:53.200 - 00:05:57.580, Speaker G: So this is Nimbus Ethereum JS pair, and I'm talking for Ethereum Js.
00:06:02.000 - 00:06:06.160, Speaker A: What data were you trying to give over peer to peer for Ethereum JS?
00:06:07.540 - 00:06:17.280, Speaker G: So it's basically trying to beacon sync the blocks from the range about 3000 downwards to Genesis.
00:06:19.800 - 00:06:22.084, Speaker A: Okay, I see. Yeah.
00:06:22.122 - 00:06:22.372, Speaker D: Okay.
00:06:22.426 - 00:06:33.290, Speaker A: Makes sense. Justin, did you still have your hand up?
00:06:34.060 - 00:06:39.050, Speaker F: Yes, but it's regarding the hive test, so if that's going to be a later agenda item, I can hold till that.
00:06:40.780 - 00:06:48.830, Speaker A: Yeah, I mean, unless anybody has anything else on Devnet nine that they wanted to bring up, I think we can talk about hive a bit.
00:06:52.720 - 00:07:14.150, Speaker D: Yeah. Merrick, one thing. So we did on Devnet eight, spam testing with sending blob tool and it broke a few nodes. So maybe we will do it again soon and we will see if we see the same issues.
00:07:17.910 - 00:07:47.980, Speaker B: Yes. On Devnet eight, it was sent 16,000 transactions with six blobs each. And few nodes were struggling with it. On Devnet nine, Aspar, we did test with sending 16,000 but one blob transactions and there was totally no problem for the whole network. Like, no issues.
00:07:52.100 - 00:07:52.850, Speaker D: Cool.
00:07:54.180 - 00:08:03.480, Speaker A: Yeah, it will be good to run that on nine. Any last Devnet nine comments.
00:08:08.140 - 00:08:10.570, Speaker C: Is anyone here from Mev team?
00:08:16.630 - 00:08:17.940, Speaker A: I don't see anyone.
00:08:20.810 - 00:08:23.350, Speaker B: Yeah, this would be like someone from flashbots.
00:08:23.850 - 00:08:30.390, Speaker C: It would be great if we could start onboarding flashbots or MeV builders.
00:08:32.410 - 00:08:33.880, Speaker B: On Devnet nine.
00:08:34.570 - 00:08:35.320, Speaker C: Yes.
00:08:35.790 - 00:08:40.380, Speaker B: Okay. Do you know, is Perry here? I don't see him.
00:08:41.390 - 00:08:43.686, Speaker C: Perry is on Monday, today and tomorrow.
00:08:43.798 - 00:08:47.180, Speaker B: Okay. I can poke them, but.
00:08:49.250 - 00:08:52.960, Speaker C: I'm also in a group with them, so I could also reach out to them.
00:08:54.530 - 00:08:57.920, Speaker B: Probably. That's probably the better bet then. Yeah, if you say something.
00:09:10.000 - 00:09:17.390, Speaker A: Great. Let's move on. Justin, you wanted to say something about the hive test?
00:09:17.920 - 00:09:41.270, Speaker F: I sure do. So we have two issues right now that we're. We kind of discussed these in discord, so I'm imagining they're being handled. The first one was, I think, mentioned on Thursday or so. It was the fork choice. I'll just paste the name of the test here since that's kind of relevant. Fork Chase updated v two, then v three.
00:09:41.270 - 00:10:31.540, Speaker F: Valid payload building requests. That's the one where we are basically testing first block past fork transactions are coming in and we are disregarding them prior to the fork if they are blob transactions per spec. And so I was hoping that that test could be adjusted so that the transactions required for the first block are not sent till the fork has been passed. The second one is an issue called invalid payload attributes. Missing beacon route. The test is expecting an invalid response based on the payload attribute. However, the fork choice update is a syncing state and so that's being returned before an invalid.
00:10:31.540 - 00:10:35.850, Speaker F: I think that's an incorrect test, but happy to debate it.
00:10:37.740 - 00:11:00.896, Speaker D: Yeah. For the first one, definitely we are going to fix it. The issue is that, yeah, we are sending block transactions before the fork. And I think most of the clients have complained that they don't want to accept them for the fork. And I think we should adjust that on Hive. For the second one, I have not taken a look. I will just dig in to see if I can fix.
00:11:00.896 - 00:11:02.480, Speaker D: Yep. Thanks, Maria.
00:11:06.280 - 00:11:07.670, Speaker F: That's it for us.
00:11:08.760 - 00:11:20.890, Speaker D: Another question for you guys. Is this issue that you're seeing for the motherblocks, is there anything that we can do on Hive to detect it? Because it seems like we should be able to do it.
00:11:21.580 - 00:11:33.580, Speaker F: Great question. I'm very surprised that this was a regression as well, and I'm not quite sure where it's coming from yet. That's why I'm sus that it's a configuration issue because we don't see this on hive when I run locally.
00:11:36.970 - 00:11:58.000, Speaker E: So looking at the logs, at least the tech will reject those blocks because versioned hashes number of versioned hashes are different from the number of blob hashes. So I think you can have some tests on those.
00:11:59.410 - 00:12:00.394, Speaker F: That's helpful.
00:12:00.522 - 00:12:01.310, Speaker D: Thank you.
00:12:01.460 - 00:12:14.420, Speaker E: Yeah, you can post the exact error that we print out if makes sense for you to build some tests over it.
00:12:17.550 - 00:12:31.794, Speaker D: Yeah, because as far as I remember, so what we do is verification of the version hashes everywhere in Hive. So it's really weird that we are not catching this, but if you have some locks or anything that we can take a look at.
00:12:31.912 - 00:12:53.560, Speaker E: Yeah, it's just that the number of hashes version of the hashes doesn't match the blob hashes. So kind of that we receive. I haven't counted. So you can pick this log here because there are two sets logged. And I think like the version of the hash size is different by the blob hash size, probably.
00:12:55.070 - 00:13:41.560, Speaker D: Maria, do you have tests that verify if client will not produce blocks with more blobs than needed or something like that? Maybe Bezu added too many blobs to block. I don't know. Yeah, we definitely have tests where we verify that the client does not accept blocks with higher number of blobs. But I believe that we do check the number of version of hashes somewhere in the engine API. So this should be the place where we verify that. Let me double check and I will get back to you. If not, I'm going to add another verification for this.
00:13:50.550 - 00:14:30.660, Speaker A: I just wanted to say on this missing invalid payload attribute to missing beacon route, we're actually finding the non syncing version because we are first checking if the payload attributes are valid and then issuing the fourth choice updated. So it just seems like these two are a little bit at odds because in the syncing version it wants to update the fork choice and check if the params are valid and then I guess revert to the fork choice updated if the params are invalid. Yeah.
00:14:34.710 - 00:15:39.254, Speaker G: Another inconsistency, Mario, I want to bring in is again in the validations of the data itself. For example, in some cases what is happening is that in new payload itself, for example, the version has not correct and one expect invalid over there and in FCU then one expect syncing. That is justified because the block itself is considered not seen by the client, but in some of the cases it expects invalid. So there is this inconsistency which basically causes one set of the test to fail. If you try to do it this way that okay, if new payload, in new payload, the data is incorrect and not the block execution. And then you sort of treat it as if you have not seen the block. In some of the cases one expects invalid and I think that there should also be syncing over there.
00:15:39.254 - 00:15:46.890, Speaker G: So there is some inconsistency in, again, how this is being treated in different ways in few of the cases.
00:15:48.750 - 00:16:43.770, Speaker D: Yes, definitely. So the thing here is that we have a set of tests where we send something invalid in the block, and normally this invalid field is part of the block header. But in this special case this invalid property is just a property of the new payload request itself. So I think there is no way for you guys to cache somehow that you've seen this invalid property and that is fine. I think if everyone agrees, we can just simply remove the fork just updated that follows this check and not expect the client to keep this invalid version at some point or. Yeah, I don't know if I made myself clear, we can just simply remove the FCU in this specific check if everyone agrees.
00:16:44.670 - 00:17:08.322, Speaker G: I think the current set of checks is nice, but what I'm saying is that there should be one consistency. Either the folk choice should think that it will get a syncing response or it will get an invalid response. So what is happening? In some of the cases it is expected syncing and in some of the cases it's expecting invalid. So it has to be either way or completely removing it is also fine.
00:17:08.376 - 00:17:53.010, Speaker D: I mean, yeah, I think I recently just made a modification just to account for these kinds of fields where we don't have something in the header that represents the invalid thing that we're verifying. Maybe I made a mistake somewhere that we are not expecting syncing for that. What I mean is that we can make so that these kind of fields we don't check for invalid, we can just have syncing as a valid response. Yeah, let me double check which cases are doing this and I can get back to you with a proper response.
00:18:01.240 - 00:18:30.350, Speaker A: Cool. Yeah, let's send that information to Mario. Any other comments or questions on hive tests, either engine tests or PI spec tests? Oh yeah, go ahead.
00:18:31.360 - 00:18:49.430, Speaker G: Sorry. Something Matt, that you already mentioned on the thread that for the invalid transactions, since we tend to also verify them, maybe the invalid response should also be as part of the accepted responses in the test.
00:18:53.320 - 00:18:57.690, Speaker A: For the additional static verification you do on transactions, right?
00:19:00.300 - 00:19:00.856, Speaker D: Yes.
00:19:00.958 - 00:19:16.380, Speaker G: So basically anything that we can do without executing the block, basically on the assembly of the block. So I think transactions is one. And if there is some other thing, then I'll also sort of escalate.
00:19:20.960 - 00:19:45.072, Speaker A: Mean. That's I think that's a good idea. But I guess there's also a bit of a question. I'm curious what other El teams think. What, Mario, you think? Basically, right now we have a couple things that we're specifically checking to see if whenever you receive new payload, if these things are invalid. But there are more things that we could be checking. So right now we're making sure that the version hashes are matching.
00:19:45.072 - 00:20:42.040, Speaker A: And I know that there are some tests that make sure that the blob gas limit is correct, but we could do more things. We could make sure that, as Kagenger is saying, that the max nonce of the transaction is being respected. We could check to see if the gas limit of individual transactions are respecting the block gas limit. There are more things to do here, and I think it might be good to give clients a little bit of flexibility in deciding whether or not to return invalid. I don't know if anyone feels strongly against that position. So maybe concretely, there are these hive tests checking the blob gas limit. Right now it requires clients to return invalid.
00:20:42.040 - 00:20:49.500, Speaker A: What I'm asking is to allow invalid or syncing.
00:20:52.240 - 00:21:22.310, Speaker D: Yeah, I understand. So the reasoning behind that was that to check the blob gas use, I think you only have to check just a minimum part of the number of transactions. And that's it. So I guess it was an easy check for clients to do. That was my reasoning. And since Ethereum JS was doing it, I thought it was logical to do that as part of the check. But I'm not sure if other clients agree.
00:21:24.280 - 00:22:15.910, Speaker A: Yeah, it definitely makes sense. The bigger question is, I don't know when these types of things will arise on an actual chain and how clients will respond in the different scenarios. So if we get this and we're syncing, how long is it going to take before we get to that payload execute and tell this consensus client that it was wrong? And right now those things aren't necessarily specked out. And so we have gender is sort of finding this issue with the nonces. There are some things that aren't explicitly specked out that we don't necessarily have tests for. And so I think it would just be good to either explicitly outline every static check we want to do, or to have a little bit of flexibility on allowing clients to fail early or fail later.
00:22:16.600 - 00:22:16.964, Speaker D: Good.
00:22:17.002 - 00:22:17.700, Speaker A: Ginger?
00:22:18.620 - 00:22:49.810, Speaker G: Yeah, so currently in the invalid transaction, lot of invalid transaction cases. So hive is expecting to be syncing, but what we are doing is we are throwing invalids right at the point of new payloads. And that is sort of causing us to fail the test. So basically, hive is being flexible right now, but Ethereum JS wants to be a little bit more conservative. So, yeah, if that flexibility can be built in, that would be really nice.
00:22:52.260 - 00:23:37.456, Speaker D: I think the safest thing to do could be. So if there is a check that can be done beforehand, we're going to give you the flexibility to return either invalid or syncing. But if there is something that it's impossible to check and the client is returning invalid, that should be like a failure to me. So, for example, returning, I don't know, gas used if you don't have the parent or something. I'm not sure. If there's something that cannot be checked and the client is returning hive invalid, that should be a failure. But, yeah, if there's something that can be checked and the client is returning syncing, that shouldn't be a failure, it's okay to just proceed with the normal parts of the test.
00:23:37.456 - 00:23:38.530, Speaker D: Is that okay?
00:23:39.300 - 00:23:39.760, Speaker G: Yes.
00:23:39.830 - 00:23:40.930, Speaker D: That makes sense.
00:23:41.700 - 00:23:52.470, Speaker E: Yeah, I think makes sense to me as well, because if you return invalid, becomes unrecoverable for Tagwood, at least.
00:23:55.160 - 00:24:08.760, Speaker A: One thing on these tests. I think if it returns syncing, it might be good to try and build a payload on top of it, just to make sure that, that syncing doesn't resolve as becoming like, the head of the chain.
00:24:10.460 - 00:24:40.180, Speaker D: Definitely. So that's the normal testing workflow. So what happens when you return syncing? Is that okay? It tries to build on top of it to see if it's really syncing. Then it provides you with parent for you to be able to reconstruct the chain, and then it tries to proceed with the check, and then you should definitely return invalid. So we can do that if that's how the client is implemented right now for those specific fields.
00:24:47.040 - 00:24:52.160, Speaker A: Okay, cool. I think that resolves everybody's concerns on that topic.
00:24:53.780 - 00:24:57.970, Speaker F: Is there going to be an accompanying spec clarification to go with that?
00:25:01.540 - 00:25:10.368, Speaker A: Is that an offer to update the spec, Justin? I'm just kidding.
00:25:10.544 - 00:25:14.390, Speaker F: No, let's talk later maybe. Yeah, maybe.
00:25:15.880 - 00:25:19.370, Speaker A: Fair enough. I could look at trying to update the spec later too, also.
00:25:21.180 - 00:25:24.570, Speaker F: Yeah, I'm willing to put up or shut up.
00:25:26.060 - 00:25:43.040, Speaker A: Sorry to put you on the spot there. Let's talk about Mav so we don't have to think about that for a second. Yeah. Let's circle back to mev testing on devnet nine. I understand somebody from flashbots has joined Shanna.
00:25:44.180 - 00:25:44.930, Speaker H: Yeah.
00:25:46.900 - 00:25:55.540, Speaker A: Hey, do you want to give us an update on where you guys are at with respect to getting building infrastructure set up around the devnets.
00:25:57.560 - 00:26:30.210, Speaker H: Basically, we just have all the docker images ready for map boost, relay and the builder. And we just need to have like an e to e tesla. I think for the builder, we don't have everything ready to support kind of bundles containing blob transactions, but I think that's fine for devet nine. That's basically what we have right now. The implementation is where you just need to test it.
00:26:33.140 - 00:26:36.370, Speaker C: Do you guys plan to run a relayer for devnet nine?
00:26:38.500 - 00:26:46.880, Speaker H: I believe peritosh will help us with running that relay.
00:26:51.190 - 00:27:01.960, Speaker C: I can probably also get it up and running, but I was just curious if flashbots had any intention on trying to set up the infrastructure themselves too.
00:27:02.730 - 00:27:13.030, Speaker H: I don't think we're planning to set up our infrastructure for devnets. I think we're planning to do our infrastructure for testnets like ongaly.
00:27:15.230 - 00:27:17.114, Speaker C: And possibly for Holski also.
00:27:17.232 - 00:27:17.900, Speaker H: Yeah.
00:27:23.060 - 00:27:50.540, Speaker C: I personally think that's a bit too late, but maybe other plan teams disagree with me on this one. I think it would be very good if we would have mev workflow tested properly before we fork any of the testnets. Which means also flashbots running infrastructure themselves instead of us setting it up, so in case something breaks, you guys can iterate much quicker.
00:28:02.650 - 00:28:03.990, Speaker A: Is that a possibility?
00:28:08.010 - 00:28:10.570, Speaker H: Setting our own relay on devnets?
00:28:12.510 - 00:28:17.500, Speaker A: Yeah, having flashbots run some of this infrastructure and monitoring it.
00:28:23.010 - 00:28:34.480, Speaker H: I think I need to talk with the team to see we're kind of like, not at capacity to support this right now.
00:28:41.430 - 00:28:42.454, Speaker D: It is.
00:28:42.492 - 00:28:56.650, Speaker A: It mostly just like engineering capacity you guys are lacking to support this? Or are there other roadblocks that aren't allowing you guys to run the infrastructure? Test infrastructure on devnets?
00:28:57.790 - 00:29:00.490, Speaker H: Yeah, mainly in engineering capacity.
00:29:03.620 - 00:29:23.510, Speaker D: Probably DevOps capacity, right? Yeah, of course. Shama is programmer, and I'm sure flashbots has any. Okay, ignore me.
00:29:32.310 - 00:29:43.240, Speaker C: I'm happy to try to set this up for Devnet nine. I think it would be just better if Flashbot were to run it.
00:29:47.550 - 00:30:16.690, Speaker A: Yeah. Let's continue this conversation offline. I do think that this is a bit of a reoccurring theme, and the building infrastructure is extremely important for Ethereum, and it would be really good to test this stuff now during the devnets before we get to public testnets. So if we can find a way to help you guys get to where that's more of the common path, I think that's going to be a much more positive place for all sides of the equation.
00:30:19.270 - 00:30:20.020, Speaker D: Cool.
00:30:20.950 - 00:30:39.130, Speaker A: Any final comments on building infrastructure? Mvv for Devnet nine. It sounds like Barnabas is going to help spin up that infrastructure. And in the meantime, we'll chat offline to see how to accelerate future Devnet builders.
00:30:39.710 - 00:30:45.260, Speaker C: Could I get an update on which Cl team is ready for running these?
00:30:48.220 - 00:30:49.690, Speaker I: Lighthouse is ready.
00:30:51.040 - 00:30:52.590, Speaker G: Node style is ready.
00:30:53.120 - 00:30:54.830, Speaker E: Taku should be ready.
00:30:55.280 - 00:30:56.750, Speaker D: Presum is ready.
00:31:00.180 - 00:31:18.320, Speaker C: Anyone from Nimbus. Okay, so I guess the only questionable client is nimbus. Possibly they're also ready. Okay, I will try to set this up tomorrow and get back to you guys.
00:31:23.510 - 00:31:35.700, Speaker A: Thank you, Barnabas. Okay with that? That looks like everything that was on the agenda. Mario, you had some updates. Did you want to go over those?
00:31:36.710 - 00:32:33.622, Speaker D: Yeah, no, the first one was already covered, but the second one is that we have a new site for the interrupt are. Let me share the link first. So, this one contains, at first, will only contain Gotharm against all of the Cl clients, and it will run a set of interrupt tests. So just like a sanity check, just a testnet, a syncing suite, and a builder suite. And eventually I will add a reorg suite that will be running continuously so you can check the results here. I think most of the tests are passing for most of the clients, but I will keep updating this if you want to check the list of tests and the description of it. This is currently just a pr, but it will be merged to master in the following days.
00:32:33.622 - 00:33:05.590, Speaker D: But you can check the rhythm here. Please take a look. If you think there are some checks missing or something, please let me know. In general, what happens in each of these test cases is that we instantiate a couple of nodes with half of the validators each. Then we start either on capella or we start directly on the nav genesis. Then we go through the fork. If we started on capella, we start sending block transactions to the network.
00:33:05.590 - 00:33:41.806, Speaker D: We check on some of the test cases, we check for final session. And when we do that, we do check every single beacon block, and we cross check between the beacon block, the blobs, the number of blobs included, the version hash is included. And we do check that on the execution payloads to see that everything is matching what other checks we do. Basically. That's basically it. But yeah, these verifications are made on every single test suite that we run. The syncing, the builder, everything.
00:33:41.806 - 00:34:31.710, Speaker D: So, for example, in the builder tests, we run the entire workflow. And we also do the verification that the blobs, the bursting hashes included, the bitcoin chain match, what the builder did, the builder included, and also what is included in the execution chain. So yeah, a lot of cross checks in general, but if there's anything missing. Also we are open to suggestions or more test cases that we can do. Just please leave a comment either, or maybe just think about it and we can revisit this on the next meeting. But also please just leave comments on the pr that would be helpful. Also, this is the pr link.
00:34:31.710 - 00:34:33.710, Speaker D: And that's it.
00:34:35.460 - 00:34:52.950, Speaker A: Wow. Awesome work on that simulator. That's amazing. Cool. Any other comments, guys? For Dinkun testing devnet nine?
00:34:55.320 - 00:35:00.828, Speaker I: Yes, I had a request for testing scenarios.
00:35:00.864 - 00:35:01.530, Speaker A: I guess.
00:35:03.820 - 00:35:51.130, Speaker I: It'S related to this issue. TLDR is like the proposer can send multiple valid blobs for a given blob index. And so I think Lighthouse would have had an issue where we wouldn't have been able to import a second valid blob. So someone could have been able to split a few of the networks. So I was just hoping we could test this either on this devnet, on whatever feature. Testnet, whatever, because this would have been initial in Lighthouse. Not sure how other clients handle it exactly.
00:35:51.130 - 00:35:57.930, Speaker I: We're working on a fix. It should be merged soon, but just want to bring that up.
00:36:04.790 - 00:36:06.900, Speaker A: Have any other teams noticed this?
00:36:13.700 - 00:36:21.060, Speaker E: Could you repeat the underlying issue? I think I lost the beginning of the description.
00:36:22.360 - 00:37:13.380, Speaker I: Yeah. So since there's no slashing related to a blob message, a proposer can send multiple valid blobs related to the same block and at the same blob index without any penalty, apart from maybe reducing their ods of getting the block accepted. So what was happening in Lighthouse is I think we would just import the initial blob and then we'd ignore all other blobs over gossip. But for example, if the network accepted a different valid blob, we wouldn't have been able to import that valid blob. So we wouldn't have been able to continue to follow the chain.
00:37:14.360 - 00:37:21.050, Speaker E: I think, like you, we just accept the first that we saw.
00:37:21.580 - 00:37:24.228, Speaker I: Yes, that's the gossip rule.
00:37:24.324 - 00:37:25.752, Speaker D: Yeah, that's the gossip rule.
00:37:25.816 - 00:37:29.580, Speaker I: But you could potentially send different blobs to different part of the network.
00:37:31.520 - 00:37:47.280, Speaker E: Yeah, we haven't actually tested anything like that. But this should be work on Tegu because we should then look up by root once the block has been confirmed to be imported.
00:37:47.940 - 00:37:54.740, Speaker I: I suspect that, yeah, our issue was like our caching was stopping us from importing.
00:37:56.440 - 00:37:56.900, Speaker A: Right.
00:37:56.970 - 00:38:05.652, Speaker E: So you get a cache level in between that prevents you to do the actual lookup.
00:38:05.716 - 00:38:06.330, Speaker D: Okay.
00:38:06.780 - 00:38:09.450, Speaker E: We don't have that, so should be fine.
00:38:10.620 - 00:38:16.360, Speaker I: Yeah, I just think it'd be worth testing. This is like potentially a consensus learning type bug.
00:38:16.880 - 00:38:17.580, Speaker E: Yeah, sure.
00:38:17.650 - 00:38:30.000, Speaker I: Are we suggesting to change the gossip rule or no, I think the gossip rule is fine. I think it's like teams just have to make sure they're able to recover via RPC.
00:38:32.180 - 00:38:45.190, Speaker E: Yeah, I agree. Gossip is fine. We just need to be sure that we can look up everything once the chain diverted from the thing we think is correct.
00:38:53.130 - 00:38:54.310, Speaker A: Kuchinder?
00:38:54.890 - 00:39:31.010, Speaker G: Yeah, I also think that current gossip is fine because if the proposer is sort of spamming, then his block is not likely to get included, which is basically disincentive for the proposal itself. And obviously, if you see. So what we do in Lordstar is that if we, for example, see the gossip for the next block, either blob or block, then we pull by root the parent. So that basically makes us sync to the chain hat.
00:39:40.640 - 00:40:19.940, Speaker I: So just to confirm right here, to solve this issue, what we're doing is that, say today we receive blobs, we save them, and then we catch it, and then we receive some duplication of blobs that we ignore it, but the block actually points to the duplication. So in this case, do you just manually request the blobs over RPC? I think I missed the beginning of the question, but, yeah, I think we have to just recover via RPC.
00:40:20.920 - 00:40:21.380, Speaker D: Okay.
00:40:21.450 - 00:40:25.930, Speaker I: And then this is just RPC by root, right? Yeah. Right.
00:40:37.090 - 00:40:40.350, Speaker A: Did you get your question answered, Mario?
00:40:42.530 - 00:40:51.730, Speaker D: More or less. So you get like a ballot signature on a blob that is not present on the block.
00:40:54.630 - 00:41:21.740, Speaker I: Yeah, that could be a way that this would happen. So maybe two blobs that are, sorry, two blob sidecars that are identical, except they have different KZG commitments and different blobs. But the KZG is valid there. And if you get both of those before you see the block, you don't know which one is actually in the blocks, you don't know which to keep.
00:41:22.990 - 00:41:37.774, Speaker D: I see. So you know the proposal and you receive a valid signature from that proposer, but you don't know which blobs are actually included in become block. Right. And so you accept them all. Oh, I see.
00:41:37.812 - 00:41:49.410, Speaker I: Yeah. Well, so how it works now is you just accept the first one you see, but then it's possible the block comes later. You have to find the other blob sidecar.
00:41:51.210 - 00:42:45.480, Speaker D: I think the easiest way we can even do this on hive testing, if we have an RPC endpoint that we can use to mimic this behavior, is there anything that we can do? Because a hive at the moment for example does not have a way of doing a malicious CL client. So everything that we inject from Hive to the CL clients, we do it via RPC. So is there any way that we can inject. We have the validator keys, so we can just fake any signature, whatever. But the problem is delivering this, spamming blobs to the clients. Is this possible via RPC in some way?
00:42:48.090 - 00:42:53.690, Speaker I: No, I think we need to inject the messages via gossip.
00:42:55.630 - 00:42:56.380, Speaker E: Yeah.
00:43:05.730 - 00:43:13.534, Speaker D: What do you guys use to inject in your tests? Gossiping messages, random gossip.
00:43:13.662 - 00:43:37.820, Speaker E: Maybe running the same key into different clients without running any leisure. So I'm just thinking out loud. So you get multiple blocks and multiple blobs for that. So you don't know which one will win. Yeah, maybe it could work.
00:43:48.300 - 00:44:08.910, Speaker D: Yeah, but there's no way to, I mean, yeah, the easiest way just to be just on demand, being able to inject something to the peer to peer network, some message that hives. Thanks, for example. Yeah, I have to think about it.
00:44:17.470 - 00:44:36.094, Speaker A: Thanks for bringing that up, Sean. Sounds like, yeah, we can talk a bit more about that offline, try and figure out how to integrate a test into our testing infrastructure to look into that. But it would be good if we could execute the attack on Devnet nine after clients take a look at six.
00:44:36.132 - 00:44:36.910, Speaker D: Amp.
00:44:39.910 - 00:44:44.180, Speaker A: Okay, any last comments or questions?
00:44:47.670 - 00:45:30.450, Speaker B: I would like to discuss transaction replacements, like the case with replacing blob transaction by the transaction with less number of blobs. So for example, at Nettermind we are not allowing two blob transactions to be replaced by one blob transaction. And I'm curious, what is the approach of other els? It was discussed a few weeks ago, but we didn't have agreement. And I wonder if we should agree on some approach and cover it by hive test or just keep it as implementational detail of every client.
00:45:39.530 - 00:47:07.042, Speaker A: I would have to look to see what the guest behavior is. I'm actually not sure if we allow that or not. I would generally prefer to keep the hive tests around the TX pool pretty abstract and not put requirements like this on transaction gossip, unless there's a pretty strong reason. I don't know how other teams feel about that, but it seems like a reasonable requirement in the TX pool. Okay, no comments. Maybe we should post it on discord, see if we can get a response from visu. Okay, still eleven minutes.
00:47:07.042 - 00:47:41.690, Speaker A: Any other things that we need to chat about or are we coming to an end? I'll take the silence as we're coming to an end. So thanks a lot everyone. We'll see you again here in two weeks on the consensus layer. Call on Thursday and on discord. In the meantime, have a good rest of your day. Thanks.
00:47:41.760 - 00:47:42.374, Speaker B: Bye.
00:47:42.502 - 00:47:43.420, Speaker I: Thank you.
00:47:44.270 - 00:47:45.910, Speaker E: Bye. Goodbye.
