00:00:00.570 - 00:00:36.520, Speaker A: You this meeting is being recorded. Okay. I'm not sure it's recording too, but it's being recorded. Yeah, maybe just for the recording. We have been going through the outstanding questions for UF one, and we are discussing the validation. I guess we actually decided for this now, right?
00:00:46.200 - 00:01:20.796, Speaker B: Yeah. My concern about the deep validation, I think is not a concern because we validate, this is for the sake of the recording. We basically validate each byte, whether it's a header byte or a code validation byte or a data byte. And when you recursion a subcontainer, you skip over the whole bit of it, and then you revalidate the interior, which again falls in the category of evaluate it once for header byte, data byte, or code validation. So we're not really evaluating bytes more than once. So in any nesting dolls you get with the deploying factories. Deploying factories.
00:01:20.796 - 00:01:36.920, Speaker B: Deploying factories are costed for each expansion charge for each expansion. So the computational cost matches the expected charge cost. So there's no mismatch between the gas charge and the effort expended. So I'm not concerned anymore, and I'm okay with subcontainer validation.
00:01:46.050 - 00:02:26.320, Speaker A: Awesome. Yeah, I think that the important piece is that we have to write down any concerns we may have into the security considerations just to aid the review for others. Otherwise, if they're going to have the same concerns, it's going to take longer to get onto the same page. All right, shall we move to the next question, which I think is create four.
00:02:30.650 - 00:03:24.422, Speaker B: Right? So this is my proposed change. Right now, the create four spec is written. That takes a stack argument that references an index in a transaction. My concern is stuff like 4337, where you're taking what might be user operations that might contain these or might or might not, or future transaction formats where they might be bundled together and you might lose your index location within your transaction, especially for taking user operation and bundling them into a type two transaction would have that extra field. My proposal is instead of indexes, we use hashes and we refer to the contracts by their hash. So there's a bit of a cost overhead. When you bring in the transaction, you're going to have to hash it, but if it'ssd encoded, it's probably going to be hashed anyway, and you refer to it by hash.
00:03:24.422 - 00:04:09.122, Speaker B: Now, one concern is this might be more larger code. There might be a push 32 before the create four. But if you're embedding the hash of a contract and you're doing it for size, then you should be using create three, and embedding it in the of container. There might be some strange situations where you're needing to do diamond pattern and all that, and you want to lock in the version. But I think the more typical case is you're going to be passing in your contract hash via call data, and it's not going to be in the code, it's going to come exterior from the code. So what you get on the stack is going to get call data, or it's going to come from maybe long term memory. And that's where you're going to get the contract to deploy.
00:04:09.122 - 00:05:06.790, Speaker B: So the size growth isn't going to be too much larger. And looking beyond the current situation of the EVM on layer ones, I think this opens up more design space for L2 evms, where they might have especially ZKE evms, where they might have a small list of contracts they want to deploy, and it's provided by the L2 outside of the transactions, and say, well, you can do uniswaps and you can do pools and you can do minecraft things and that's it. And so you could refer those contracts by hashes when you deploy those there. So you could refer to stuff outside of it. So I view create three as you're deploying it from within the container, and create four is you're deploying from outside the EVM and using a hash to reference it. Where our first proposed implementation is that the hash comes from a transaction containing the data. So that's my high level pitch for why create four should shift to hashes.
00:05:06.790 - 00:05:16.320, Speaker B: And we could accommodate hashes and indexes by putting in rules like if there's 310 bytes, then treat it as an index. We could accommodate both in this as well if we need to.
00:05:19.160 - 00:06:52.980, Speaker A: I would be against having clever rules. I would either just go for a hash or index. I was leaning towards the hash, but initially only because that allows for block level duplication. But as you mentioned, I completely forgot about the user space account abstraction. I think that's the main reason why I would be for the hashes, because that simplifies the relayers and any kind of building on that level. I guess the question is, anyone strongly in favor of having indexes? Anyone somewhat in favor of having clever options of indexes and hashes? And I guess we're going to go with hashes for now and see if anybody from orcodefs comes up with some arguments against.
00:06:54.790 - 00:07:02.840, Speaker C: So the implication for the tooling would be that the creation transactions would have to include hash right in the call data.
00:07:04.570 - 00:07:24.720, Speaker B: Yeah, or it would be a push 32. I guess there are some contracts where you just do index one in order to deploy index one. So that is the downside. You'd have to pull it out of call data and put it on the stack instead of just pushing one. Like the creator contract. Exactly.
00:07:34.690 - 00:07:55.700, Speaker A: Yeah. I mean, we. We could have. Yeah, I would try to avoid having any kind of level rules, like empty hash, having some specific behavior or anything like that.
00:08:10.500 - 00:08:54.420, Speaker B: So we would just change the input from index. If we go back to that index would just become hash. Hash salt aux data up at the top comment line. So we would just change the ABI of the contract. I mean, it's already set up at 32 byte intervals, so it's not too much of a change. In fact, it's probably you just change the text of nicode index to nicode hash and we're done. There's no actual structural changes to this.
00:09:01.040 - 00:09:03.810, Speaker C: Yeah. The contract will stay the same, I guess.
00:09:31.310 - 00:10:20.350, Speaker A: Yeah. I would suggest we go with the hash for now just so that we actually try it out, because I guess nobody properly implemented it and tried it in different contexts. So, yeah, I would try it out with the hashes and come back if you find any issues with that. Right. Nobody against this proposal. Then I suggest we go to the next point. We're bidding unreachable sections.
00:10:20.350 - 00:10:24.880, Speaker A: Andre, do you want to explain that?
00:10:26.290 - 00:11:05.850, Speaker C: Yeah, this just seems logical. Since we disallow unreachable code, then sections that don't have any call f or jump f into them should be forbidden. I guess it was like, an oversight from our site originally, and we never got this into the spec, so seems to be a good idea to me. The only downside is some testing might be more complicated because you can't just create, like, 256 sections, like, to test the limits. Each one of them should be reachable.
00:11:07.070 - 00:11:29.270, Speaker A: And things like that's.
00:11:32.360 - 00:11:55.610, Speaker B: I think it's fine, because I think creating a test case to do max sections won't be too hard. Every section just becomes jump f, next section number, and then the last section becomes stop. I think it's a minor, and I think it goes along with the theme of no dead code, and I think that's an important thing to get.
00:11:58.160 - 00:12:09.500, Speaker D: Yeah, I don't think this is controversial. It just falls under no dead code. It's just a section which is not reachable instead of a basic block.
00:12:22.870 - 00:13:33.470, Speaker A: All right, let's do it. Unless we find some issues along the way. And then we have this section for changes which can be rolled out in a different stage. There's just one feedback, at least from within the epsilon team. I think where we have been mostly leaning towards is it would be nice to spec these out, but probably we wouldn't push very hard to have optional opcodes just for the reason of having too big of a scope. But specking out jump fi and maybe the create with external address could be useful for the point of realizing if there's any shortcoming or an issue elsewhere in the design. But I wouldn't push hard to actually have them in the first stage or first roll out because the scope is already quite significant.
00:13:36.370 - 00:13:53.018, Speaker D: By the way, I don't know if I should have mentioned at the beginning of the call, but I submitted poll requests one for return data load and one for this exchange opcode. So return data load is already specked out if you want to accept.
00:13:53.054 - 00:13:56.374, Speaker A: So Exchange is the swap mn, right?
00:13:56.572 - 00:14:13.930, Speaker D: Yes. And something interesting that Dano and me figured out is that the addressable range is actually larger by a factor of two. So like with one byte you can actually address 32 stack items instead of just 16.
00:14:17.390 - 00:14:47.960, Speaker B: Yeah, if you assume you're never swapping one because you use the existing swap set. So it's one plus for the first in any order in first and second, and you encode the second one as a delta from the first again offset by one, you can get up to 32 for your maximum swap. The downside is you can move it a maximum of 16 spaces on the stack, but sometimes that's what you would need. So I think that also makes sure that there's no duplicate swap instructions. Like swap 76 isn't the same as swap 67.
00:14:53.080 - 00:15:08.490, Speaker D: Yeah, basically there's a bunch of duplicate swap instructions. Swap one two is the same as swap two one. And also swap eleven is a no op. So you can basically, Danos encoding is better because it doesn't waste space.
00:15:28.370 - 00:15:30.430, Speaker A: What's the number for exchange?
00:15:33.090 - 00:15:39.330, Speaker D: I just made a pull request against the six six three ip. They're all in the discord chat.
00:15:39.750 - 00:15:54.630, Speaker B: Oh yeah, six six three. All right, that makes sense to just put it in there since that's the successor to swap n that's more functional and dupe n would remain.
00:16:00.730 - 00:16:03.190, Speaker D: Posted the pull request in the chat.
00:16:06.830 - 00:16:08.694, Speaker A: Let me link this into the discussion.
00:16:08.742 - 00:16:14.970, Speaker D: Actually, would it help for me to submit a pull request for jump fi?
00:16:25.160 - 00:16:26.870, Speaker B: Yes, I would say so.
00:16:29.000 - 00:16:32.730, Speaker D: I'll try to do that today then.
00:16:34.940 - 00:16:42.410, Speaker A: Yeah, I think that one we could actually try to get into the existing efe, right?
00:16:42.860 - 00:17:12.610, Speaker D: Yeah. Should we create separate one? Because jump f. I mean, I was thinking about it. And usually instruction sets don't have this conditional call instruction, but instruction sets also don't usually have jump f, which is like a tail recursive call instruction. So I think under the logic that these are specialized call up instructions, it makes sense.
00:17:15.790 - 00:18:00.030, Speaker A: Yeah. Jump f is six two six. Yeah. At some point we actually wanted to merge that back into the function CIP, but now it covers the non returning aspect, so maybe it's better to have it separate. Select jump. If I could go into six two six. Yeah, I think we're on the same page that specking out cannot hurt, because specking out would tell us if there's any other issue or shortcoming elsewhere.
00:18:00.030 - 00:19:08.140, Speaker A: But the end of the day, I probably would be willing to drop jumpify if that's needed to get the rest of the features in and probably the same. Yeah, I don't have a strong opinion on which swap or exchange version should we have, as long as we have at least one. Do we actually have anything to discuss with these? I link the ips. Yeah. Charles, do you want to give like a short overview of free time data load?
00:19:12.080 - 00:19:51.880, Speaker D: Sure. It's basically exactly what you expect. It's three gas if you pull up the pull request, I added a little bit of motivation for it. And there's the spec of it. It's three gas. It takes the offset from the stack, exceptional failure if there's an out of bounds, and otherwise it just puts the word that it reads back onto the stack.
00:19:59.780 - 00:20:18.630, Speaker A: And the main reason for this, because you do utilize in wiper the fact that calls can write some of the return data, right? And you write like, I don't know, how much do you write?
00:20:23.400 - 00:20:24.150, Speaker D: Sorry.
00:20:26.540 - 00:20:35.944, Speaker A: So I guess this return data load is for the fact that the old calls would write the return data as well, right.
00:20:36.062 - 00:22:07.080, Speaker D: Use the output buffer and then decode from the output buffer. And then to restore that functionality, we need to be able to treat return data kind of similarly to how we treat memory as a buffer. And so return data load kind of replaces the usage of m load here. And otherwise there's something a little bit weird when you need to. So normally you just, let's see, how do you put this? So normally when you use a call, you specify the output buffer, which is like offset and length, and then the EVM implementation will just truncate the return data if the callie returns more than the buffer size that's given, but without that you have to do it in the EVM. So in order to copy return data into the output buffer, you need some sequence of instructions to calculate the minimum of the return data size and the allocated buffer size. But Viper statically, so you can't like really, you have to bound it statically instead of at runtime or you want to bond.
00:22:14.090 - 00:23:09.740, Speaker A: Think from. I have two different comments. Think from solidity's perspective. If I remember correctly, Daniel said he would be super happy with return data load, just generally because it's nicer and a counterpart to call data load. But for your specific situation, and also solidity has been complaining about it. But I kind of wonder if, and probably we discussed this on the last call as well, but kind of wonder that the underlying problem is not the lack of return data load, but rather that return data copy is not zero padding. Because if it would, then you could just keep calling it without taking care of the return data size, right?
00:23:11.550 - 00:23:13.660, Speaker D: No, that's not really the issue.
00:23:18.430 - 00:23:20.700, Speaker A: So what is the main issue then?
00:23:21.710 - 00:24:47.196, Speaker D: The issue is what happens when you go to ABI decode the return data. So let's take a very simple case where the contract returns a UN 256. So right now what you would do is you just call, and then you provide an output buffer, which is like 32 bytes, and then the return value is just what you get when you m load from that buffer. And then in the new thing, you need to call return data copy, and then return data copy into some buffer, and then m load from there. And when you go to return data copy into that buffer, you need to make sure at EVM time that the return data size fits into the buffer. Basically what happens is that in order to ABI decode things, you basically need a buffer. So the first thing you go to do after calling some contract is copy stuff from return data into a memory buffer.
00:24:47.196 - 00:24:51.520, Speaker D: And if you have return data load, then you don't need to do that copy.
00:24:56.600 - 00:25:12.440, Speaker A: Yeah. Although this last argument right now, you do designate some memory area. Right, because you use the output buffer option of the calls.
00:25:13.500 - 00:25:14.250, Speaker D: Yes.
00:25:16.860 - 00:25:31.150, Speaker A: I think that part may not be like the strong part of the argument, but I do have one question. What happens when you try to return data load out of bounds? Is it returning zero?
00:25:31.680 - 00:25:40.610, Speaker D: No, it does the same thing as it's consistent with return data copy. So that's step three here.
00:25:47.850 - 00:25:52.790, Speaker A: Yeah, this part I would definitely discuss with solidity.
00:25:53.930 - 00:26:53.260, Speaker D: Yeah. And this is, yeah, this is kind of a weird thing. And we discussed this for a little while on the last call. But for now at least, it helps Abi decoding, because another thing you need to do when you ABI decode is validate, you need to do certain offset validations. And this kind of gives it to you for free, the out of bounds return data behavior. So it basically allows you to not do return data. It allows you to skip return data size checks actually, which I don't know if that's considered a trick or not.
00:26:53.260 - 00:26:58.600, Speaker D: And then it's also.
00:27:01.850 - 00:27:03.800, Speaker A: Sorry, say it again.
00:27:04.330 - 00:27:09.720, Speaker D: Of course it's also consistent with return data copy, which is maybe the most important thing.
00:27:11.230 - 00:27:18.860, Speaker A: Yeah, but also return data copy is the one tiny piece inconsistent with the rest of EVM. Right.
00:27:19.710 - 00:27:22.140, Speaker D: And we discussed that on open last call.
00:27:22.770 - 00:27:32.078, Speaker A: But do you have some actual numbers and what kind of regressions you get without this?
00:27:32.244 - 00:27:40.100, Speaker D: I posted the EVM snippet that's required. It's like a ten byte piece of code or something for every call.
00:27:57.570 - 00:28:20.840, Speaker A: I personally don't have strong arguments against having return data load, but I do have concerns regarding the semantics that it's really hard to actually find a good option. There are just so many different sites to it. What is the opcode count at this point?
00:28:22.010 - 00:29:32.090, Speaker D: Do you mean the exceptional failure semantics? Yeah, I think that it's actually best and most forward compatible future proof to make return data load and return data copy zero pad, which is maybe unintuitive and not the most popular opinion, but it makes it easier to pack return data. But let me think about this, because call data and return data are priced very differently. So maybe you want to do something like call data is packed, so it makes sense to zero pad it, but return data is never packed or something like this. So maybe it makes sense for return data to not be packed. And so we do charge the out of bounds thing. But I mean, that's a really weird thing that just comes from how call data and return data are priced differently to fill.
00:29:49.710 - 00:30:24.520, Speaker A: Just trying to fit in the entire conversation of 60 minutes. So we discussed, I think, return data load to some extent, the exchange to some extent, and there are two more points here regarding jump fi. And I think what we discussed there, just to recap, is that we are going to at least spec it out and see if we run into any issues.
00:30:29.230 - 00:30:44.560, Speaker D: By the way, I think Andre's less restricted stack validation might work. Require you have strict stack validation on backwards jumps, but not strict validation on forward jumps, right?
00:30:47.010 - 00:30:48.720, Speaker C: Yes, something like that.
00:30:51.830 - 00:31:02.850, Speaker D: Yeah, I think that's actually fine because it prevents this unbounded thing happening when you have loops, but it does kind of what you want them forward jumps.
00:31:06.380 - 00:31:14.360, Speaker C: Yeah, exactly. It kind of still disallows unbounded loops, but allows jumping into one forward location from different stack heights.
00:31:14.780 - 00:32:22.010, Speaker D: Yeah, I think it's a really good compromise actually, because it's easy to reason about from an implementer's perspective, I think I had a proposal before which removed the stack validation entirely, which produces some hard to reason about cases, but it also removes the relevant restrictions that compiler, that as a compiler writer I'm concerned with. So from the point of view of the compiler produces the desired effect, and then from the point of view of the implementation, it's not too complicated either. So what's. Sorry.
00:32:22.940 - 00:32:26.570, Speaker A: Yeah, I'm not familiar with the stack validation changes here.
00:32:29.420 - 00:33:51.540, Speaker D: Yeah, the reason jump fi was proposed is because the stack validation rules are in some sense too strict, so they require you to add all these branches in code that just used jump I before. So there were two proposals, right? One is jump fi, which rolls r jump I and jump f into one instruction. And then the other idea is to loosen the stack validation rules. And there's one approach which Andre implemented, and I think it's actually what WAsm does, which I remember reading somewhere, but I couldn't find it, which is they have strict validation on backwards jumps to prevent unbounded loop things, but less strict validation on forward jumps.
00:34:05.380 - 00:34:20.420, Speaker A: Yeah. Can one of you maybe explain the difference because it's only just less strict. What has been said so far? What is the restriction? Exactly? What has been lifted?
00:34:21.500 - 00:34:58.350, Speaker C: Restriction is the requirement to all jump to have equal stack height for every path that goes through this location. This is Viper's concern, and I think solidity had similar is that they want to jump into some helpers from different stack heights, not from constant stack height. And this is not allowed in current stack or relation. You have to balance before jumping somehow.
00:35:02.460 - 00:35:20.640, Speaker A: Yeah, that was also one of the reasons non returning functions were kind of introduced. Would this loosening allow us to remove the non returning functions?
00:35:33.860 - 00:35:34.610, Speaker C: Maybe.
00:35:52.570 - 00:36:44.310, Speaker A: I guess this particular question maybe needs a bit more focused effort. Probably it's not like a blocker for tomorrow that the less strict validation. Would that also allow us to remove the non returning function? Special case, because those were also introduced because of stack balancing reasons. But in terms of tomorrow, I would probably argue that this one particular question is something we can let slip. I don't think there's going to be like a blocker for tomorrow and specifying jump fi. Charles, do you think you're going to get it done by tomorrow?
00:36:45.930 - 00:36:50.780, Speaker D: Yeah, draft and then it would be helpful if people look over it.
00:36:52.350 - 00:37:03.070, Speaker A: Yeah, I mean, let's maybe try to see the implications, but I don't think we're going to make a decision on this today and I don't think it blocks the presentation of tomorrow.
00:37:04.530 - 00:37:15.860, Speaker D: Right. It seems like a bit of a detail, but I think in terms of choosing which direction to go, I think it's kind of important.
00:37:19.380 - 00:37:53.174, Speaker A: Yeah, I don't think we're in the position on this call to make the decision at least. I don't really have good enough context. Yeah, the minimum we should. This is also something we probably really want to get feedback from solidity. So probably the best course of action is describing this problem in detail and then the two different options we have. In detail. Yeah.
00:37:53.174 - 00:38:29.370, Speaker A: I would suggest that we try to craft the one pager explaining the problem in the two options and then get feedback from at least solidity. And potentially, I don't know. Pavel has been super involved with the stack validation but I'm not sure when he's going to be available. But at least solidity should get feedback. Is anyone volunteering to write it up?
00:38:30.380 - 00:38:32.170, Speaker C: I can try to write it up.
00:38:35.210 - 00:38:50.540, Speaker A: Awesome. Yeah. Maybe we can come back to it also on the next call just to finalize that. Okay. Should we briefly discuss this last point? Create free with additional address?
00:38:54.030 - 00:39:02.560, Speaker D: No, I don't think that's important. If we need to choose one thing to talk about. Can we talk about exchange a little bit?
00:39:05.940 - 00:39:26.090, Speaker A: Yeah, I do want to set aside at least 5 minutes, which would be last 5 minutes. I'm not sure if people have a bit more time, but to discuss like tomorrow's presentation, I'm not sure. Daniel Charles, do you have 510 more minutes?
00:39:26.700 - 00:39:28.570, Speaker B: I've got a lot of time.
00:39:29.500 - 00:39:30.680, Speaker D: Yeah, sometime.
00:39:33.520 - 00:39:37.790, Speaker A: Right? Yeah. Let's discuss exchange then.
00:39:44.250 - 00:40:19.310, Speaker D: So originally I actually drafted the pull request with two different exchange opcodes, but then Dan and may realize that this single exchange opcode gives access to 32 slots. And so I just dropped the second exchange but can add it if we want access to 512 slots.
00:40:30.270 - 00:40:35.610, Speaker B: How do you get 512 slots? 256. That's all we get with one immediate arc.
00:40:36.370 - 00:40:40.030, Speaker D: Two immediates. Two bytes of immediates.
00:40:42.770 - 00:40:48.354, Speaker B: Oh yeah, you're right. Okay. If we use the exchange with the two byte immediate. Got it.
00:40:48.472 - 00:40:49.460, Speaker D: Yeah, exactly.
00:41:24.680 - 00:41:36.850, Speaker A: Yes. This exchange is n plus m and the arguments are unsigned. Right?
00:41:37.460 - 00:42:09.096, Speaker B: Right. Because if you're ordering any two items on the stack, you can strictly order them. One comes first, one comes second. The first one has to be within the first 16 bytes so you have zero mapped to one. Since we have swap one n already set up. So you would add one to the first value and then you would have the zero index, you would add one because it's at least the one after and then adds to the remainder. So I think we actually get 34 bytes, maybe, or 35 bytes of depth.
00:42:09.096 - 00:42:37.590, Speaker B: But 32 is the easier one to sell. Logically, we could be precise later, but the point is, with the nibble approach, there's no wasted swaps. So if you swap six seven or seven six, those would swap the same things. So we do the encoding. So it is first one and then delta for second one. So we get more distance out of those duplicate combinations per se.
00:42:38.840 - 00:43:03.976, Speaker D: Yeah. So my original thing, which was swap nm, actually, more than half of the instructions are duplicates. So like, swap one two and swap two one are the same and then swap eleven is also a no up and swap two. Two is a no up and so on. So it's like half plus there's like some kind of matrix.
00:43:04.008 - 00:43:04.590, Speaker A: Right.
00:43:07.360 - 00:43:10.504, Speaker D: Anyway, a lot of them are duplicates. So you can get a better idea.
00:43:10.562 - 00:43:16.560, Speaker A: Why don't you offset by 16? So n starts at 16.
00:43:18.180 - 00:43:20.850, Speaker D: I think n could start at two.
00:43:24.630 - 00:43:37.106, Speaker B: I think we want to start at two instead of 16, because that gives us space if we need to get rid of the dupe n one through 16 to use for other things with a minimum of disruption.
00:43:37.298 - 00:43:43.260, Speaker D: Yeah. And if we're going to do that, then we should also make dupe and start with from 17 or whatever it is.
00:43:45.390 - 00:43:46.140, Speaker B: Well.
00:43:48.510 - 00:44:02.958, Speaker A: What I don't get, and I haven't tried it fully, this currently can only go to like 32 items. How does it goes, like deeper swap?
00:44:03.134 - 00:44:05.220, Speaker D: Yeah, I think that's a good thing.
00:44:10.180 - 00:44:11.730, Speaker A: Sorry, I didn't get that.
00:44:14.100 - 00:44:16.850, Speaker D: I think it's 33 if we start from two.
00:44:17.640 - 00:44:18.390, Speaker A: Yeah.
00:44:21.000 - 00:44:47.390, Speaker D: I mean, I started out by saying that with the one byte immediate, then we can address 33 items or 32 or 33 items. And then with two bytes, we can address 512 items. I originally drafted it with exchange and exchange two, but then 33 items felt like kind of enough, so I dropped exchange two.
00:44:49.760 - 00:45:08.050, Speaker A: Yeah, I think solidity had, like, different problems. They wanted the depth as opposed to swapping any two items. And for that, this won't be as useful. Right, because the swap end can go to 55.
00:45:09.300 - 00:45:16.310, Speaker D: I mean. Yeah, you can ask them.
00:45:17.560 - 00:46:03.022, Speaker A: Yeah, I think it's the same as with jump fi. This really has to be discussed with them as well. So maybe for the next call we can actually properly arrange to make sure that some of the film solidity comes on. But I think we should do this async. Yeah. In fact, I'm going to send it to Daniel to see if he can review this pr, because it's very different problems it's trying to solve. Right.
00:46:03.022 - 00:46:04.910, Speaker A: And it has different trade offs.
00:46:10.400 - 00:46:32.150, Speaker D: Yeah, I mean, there's kind of similar and related problems, but it is a trade off, especially with respect to code size versus number of opcode slots. Like if opcode slots weren't an issue, then we would just add exchange too.
00:46:56.120 - 00:47:08.920, Speaker A: All right. Yeah. I'm not sure we can make any more progress on this on exchange. Andre, do you have any opinion?
00:47:12.180 - 00:47:14.880, Speaker C: Not really. Yeah, we should ask solidity.
00:47:28.790 - 00:47:34.180, Speaker A: All right, should we discuss these last? Nice to haves on the bottom.
00:47:42.960 - 00:48:17.012, Speaker B: I like data section optional. The question is, if it's zero, do we require its absence or do we allow it to be set to zero? The issue with fiddly little rules. We could say that. Well, I mean, the data section has to be when you're using a subcontainer. If the rule is that your data section needs to be the actual size of the final contract, not much of the subcontainer, then it doesn't matter. Then we're not going to rewrite the data. We're expecting a fixed data size.
00:48:17.012 - 00:49:03.456, Speaker B: We're not going to rewrite it. So that might get rid of the whole rewriting issue. So if there is zero data, then I think if we're not going to change the data section size, I think we not only make it optional, but we prohibit data size zero in the header. This all of course hinges on whether or not we rewrite the data section. The data section sized, but I think.
00:49:03.638 - 00:49:30.680, Speaker A: This dynasy listed here, pending data section, maybe more than just appending them. That's the problem of the compiler. The compiler wouldn't, I mean, it could choose to do so, but it would likely just include an empty or short data section if it expects it to fill it up. Right. But it's still a choice made by the compiler.
00:49:31.660 - 00:49:34.250, Speaker B: Right, go ahead.
00:49:35.040 - 00:50:35.340, Speaker C: What I meant here is we are so with this pack where like the previous pack where data section declared in the header is equals to its size, and then we append and update the size value in the header in case it was empty, and then it becomes non empty at return contract point, then it will be not just depending data section, but also inserting the, inserting this header in the subcontinent header. Inserting data section header. But this is not a problem. If we go with allowing truncated data sections, then it's not a problem actually. So it gets simpler then appending is really just appending the data section.
00:50:38.800 - 00:51:20.620, Speaker A: Yeah, exactly. But what I meant is the problem of the compiler is that a compiler would know that it will want to append data, so then it would prepare the deployment code to have the header. This only really comes up as a problem. If you do have create reeve it external address for the init code, then you may need to extend the headers. Right. But that's like another problem. Generally when you take unknown in it code that you need to check whether it satisfies what you expected.
00:51:20.620 - 00:51:51.150, Speaker A: Of course the compiler can shoot shit itself in the foot with more optionality, but I don't think it's a downside. It's really up to the compiler.
00:51:55.080 - 00:52:05.770, Speaker C: I'm not sure, because what I meant is complexity of the implementation of appending of the data section at the return contract point.
00:52:07.580 - 00:52:09.272, Speaker A: Oh, you mean in the client.
00:52:09.416 - 00:52:10.110, Speaker C: Yeah.
00:52:15.320 - 00:52:32.280, Speaker A: Right. Yeah. I guess one option is that if you're pending and there's no data section, then that's a failure, right. And then there's no complexity.
00:52:34.460 - 00:52:48.220, Speaker C: I mean now we decided anyway to truncated data sections and then if you intend to append later, you will declare its size already and it will not be empty. It will not be an empty header.
00:53:02.870 - 00:53:09.090, Speaker D: Maybe there's a stupid question, but are data only contracts allowed in EOS? So no code sections?
00:53:10.950 - 00:53:12.340, Speaker C: No, not at all.
00:53:14.090 - 00:53:45.970, Speaker B: But you could trivialize it. Just have one code section that's just a stop byte, which would have the effect of, or have the invalid instruction. So you can't call it the explicit invalid or the revert. So you could get around it with minimum of header impact. I know that's something that the SDR two people are going to want, which is they either want ext data copy or they want a way to get at it so they intend to put some data in these contracts, like inscriptions.
00:53:48.070 - 00:53:51.860, Speaker D: I'm just wondering if some of the same considerations apply here.
00:53:56.840 - 00:54:12.250, Speaker B: What if they're putting links in that are longer than what's in the section? That might be it, or variable. I mean that is the downside. But I would expect they would deploy their data contracts via top level transaction, to be honest.
00:54:38.510 - 00:54:41.610, Speaker A: What is the motivation for making it optional?
00:54:43.310 - 00:54:50.060, Speaker B: Reduce the header size for contracts that don't use.
00:54:57.890 - 00:54:58.686, Speaker A: It.
00:54:58.868 - 00:55:19.630, Speaker C: Yeah, it was made mandatory just for the ease of parsing at some point last year. But then we added container sections which are optional, and now we have data section mandatory. But container section is optional and it's kind of inconsistent. And the argument for simplicity of person doesn't stand anymore.
00:55:19.710 - 00:56:06.930, Speaker A: Kind of. Yeah, I guess in it container stuff is a good point, but based on that logic, if we want to support data contracts first class. Then we also want to make code sections optional, right?
00:56:21.070 - 00:56:26.650, Speaker C: That's the definition of data contracts. Non code sections must be optional.
00:56:29.780 - 00:57:01.770, Speaker A: Yeah, I guess what I mean is you could argue for either of these options, really? Because you could say that the subcontainers case is the special case of creation and the others are regular contracts. But, yeah, I don't have a strong opinion. As I said, you could argue for any of these in various different ways.
00:57:03.420 - 00:57:10.040, Speaker C: Okay, I would opt for no change, then leave it like now, make it mandatory.
00:57:10.860 - 00:57:42.470, Speaker B: With the create hash, though, you could still put in a stub that's the same, and then have a variable amount of data that comes after it. My expectation is that's how the S core two people would want to do it. So they'd have like a stub that has their interface to read the data and then their data structure. The first bytes will be how long it is and segmenting and other info, and then it's a variable length after that. So I think a fixed length contract might be of less utility for them because create four still has an auxiliary data option to put in it too.
00:57:45.720 - 00:58:59.980, Speaker D: By the way, one issue with relying on the S store contract to return data is that you kind of have to trust it. So it opens up things like reentrancy or the data contract not doing what you expect it to. And this is actually some kind of issue that I was trying to solve with the create three variant, which is like, yes, you can call this dedicated factory contract, which is supposed to deploy a contract for you, but you have to transfer execution context, and then it makes tracing more complicated too. So, like with the S store to returning data contract. Now every time you want to do an X data copy, you introduce kind of a trace, a call into the trace, which I don't think it's necessarily considered like a showstopper, but I think it is kind of a consideration.
00:59:01.540 - 00:59:28.550, Speaker B: Yeah, I do think ext code copy is going to be a better solution for the SDR two community. Ext data copy. Sorry, not exe code copy. So their code might be nothing more than basically stopping payments and stopping any calls and throwing exceptions, saying, I'm a data contract, please try again later. Yeah, or please don't try again.
00:59:29.820 - 00:59:40.120, Speaker D: That should be considered together with the create three variant, because they're both kind of doing the same thing, except just one is a call context and one is a create context.
00:59:40.540 - 00:59:41.290, Speaker B: Yeah.
01:00:00.010 - 01:00:00.614, Speaker A: It.
01:00:00.732 - 01:00:08.150, Speaker D: But I mean, as a compiler writer, I have no opinion on whether the data section is optional or not. I think it's.
01:00:17.530 - 01:00:23.270, Speaker B: So when people are micro benchmarking it would be nice. But how many tiny contracts are deployed?
01:00:27.850 - 01:00:44.880, Speaker A: It's not only tiny contracts, but if you look at, I mean, at least for solidity, if you look at solidity contracts, they have immutables because that's like a cheapest way to store variable data.
01:00:45.410 - 01:01:02.660, Speaker B: Right, which is why we need august data. But also let's consider that any real contract is going to be putting in it's metadata, it's hash, it's the trailer Seabore stuff anyway. So not many contracts are going to have zero data.
01:01:03.590 - 01:01:14.760, Speaker A: I mean, that's for solidity. But the question is then for Viper Charis, do you think you're going to have like a large number of contracts not having data?
01:01:29.310 - 01:01:53.970, Speaker D: I don't think that the overhead of the data section is that significant if we're talking about code size. One thing I do think is significant is the size of the code section header, because if I recall correctly, it's like six bytes or something per subroutine, which is a little bit significant if you have a lot of internal methods.
01:01:58.720 - 01:02:02.240, Speaker B: But how could we encode the data so that we have it for stack validation?
01:02:04.340 - 01:02:11.970, Speaker D: I mean, you can just choose a more compressed encoding for all the offsets and lengths. I think.
01:02:15.620 - 01:02:16.400, Speaker A: It'S.
01:02:18.600 - 01:02:29.160, Speaker B: So let's just leave the data section required, that'll solve it. It's more effort than return to get consensus on it, so let's just keep it as is inspect.
01:02:32.780 - 01:02:42.110, Speaker D: Yeah, it's not a, but what is a significant overhead is the size of headers. Actually.
01:02:54.030 - 01:03:17.380, Speaker A: Speaking of size of headers, the last point is adding one more build. So should we maybe discuss it first? Andre? I'm not sure. Is it you who added yes.
01:03:17.830 - 01:03:20.930, Speaker B: Is that for code section or for the entire contract?
01:03:22.170 - 01:03:49.370, Speaker C: This is for code section. This is a minor change in validation. So we allow mug stack height in the type section to mean not exact equal max stack height, but like upper bound. So for tests it simplifies life because you can put there just marks.
01:03:51.870 - 01:03:52.234, Speaker D: And.
01:03:52.272 - 01:04:47.150, Speaker C: Then like any code is valid with that. Otherwise, like currently for testing, you'd have to calculate also the stack height of the code you wrote. For current validation is we do stack validation and calculate during that, calculate maximum stack height and check that calculated stack height equals the one declared in the header. And this proposed change would check that calculated stack height is less than or equal the one declared in the header.
01:05:03.640 - 01:05:07.684, Speaker B: So a lazy compiler could just say 100 and just be done with it.
01:05:07.802 - 01:05:15.240, Speaker C: Yeah, exactly. And it's useful to have lazy compilers and testing scenarios.
01:05:23.760 - 01:05:38.090, Speaker B: I think this is what net does. Is that what they do in theirs? I guess my question is, is there precedent for this in other vms? Because if there is, then I'm cool with it.
01:05:48.050 - 01:06:04.450, Speaker D: I don't actually understand what the difference is from the current behavior. Isn't Max stock height just the upper bound of the number of stack items in the code section?
01:06:06.730 - 01:06:13.240, Speaker C: Currently it's not an upper bound, but exact value of maximum stack height that code section may reach.
01:06:17.370 - 01:06:22.170, Speaker B: Right. It's whether you have to bingo it, or whether you just have to be less than the number when you run your calculations.
01:06:23.470 - 01:06:37.390, Speaker D: For compiler. From the perspective of the compilers, it's better to just be upper bound instead of match. Exactly. Because like Dan said, you can just kind of be lazy.
01:06:38.530 - 01:06:59.590, Speaker C: So it affects call f in a way that this value is used at call f validation or at runtime maybe. So if you declare like max upper bound for each, then you will have lower maximum call f stack depth.
01:07:01.690 - 01:07:06.390, Speaker B: It is a foot gun. But if we're talking about laser compilers are probably not concerned about max recursion.
01:07:08.170 - 01:07:18.220, Speaker D: I don't think that's the problem. I think the problem is more catching compiler bugs because it's a foot guide. Right?
01:07:18.670 - 01:07:57.240, Speaker B: Yeah, but the space this opens, I think, is some of the stuff that we were looking for to allow variable stack heights to come into terminal operations. I think that's why we were discussing this. So like you have that one function that is. Well, actually this would have been in the inputs and outputs variable. No, because that'd be inputs and outputs. Anyway, I thought, Charles, you were talking about times where you'd want to have everyone share a revert, and you don't want to have to care about what the stack height is when you're going to throw a revert, as long as the top five arguments have all the information you need, and then you would go in and you would revert it. I thought that was the value that this would add.
01:07:59.710 - 01:08:45.470, Speaker D: I don't think they're directly related, because this is just height that happens in the called subroutine. Right. It makes it like not a black box because you can call subroutine and then it can blow up the stack. Yeah, it doesn't have to do with its inputs and outputs. The lazy compiler approaches just make everything, make Max stack height, like always 1000 or like 1024 or something. Andre, can you explain how it simplifies testing again?
01:08:46.160 - 01:09:18.230, Speaker C: So just currently, when you write the test and you make a container we do it mostly by hand. You have to calculate also the code max stack height. Because validation requires it to be equal. The declared max stack height in the header must be equal of real Max stack height. And with this proposal for testing, we can be lazy and just put 1024 in each test and then it's fine.
01:09:23.160 - 01:09:24.080, Speaker D: Right. I think for the.
01:09:24.090 - 01:09:26.520, Speaker B: You mean 255 because you only have one byte.
01:09:27.420 - 01:09:30.200, Speaker C: It's two bytes, I think. Yeah, it's two bytes.
01:09:33.040 - 01:09:54.610, Speaker D: I think for the compiler it doesn't matter exactly, but the strict validation is a little safer because it's a little bit like a checksum bit. Right. It ensures that the thing that we're declaring in the header is actually matching what we did when we analyzed the code before we outputted it.
01:09:59.090 - 01:09:59.646, Speaker A: Yeah.
01:09:59.748 - 01:10:03.520, Speaker C: This might catch some bugs at compiler development time.
01:10:06.800 - 01:10:53.040, Speaker D: Yeah, exactly. So if there's like a bug in the compiler, just like the contract just won't deploy, it'll fail validation. Yeah. In terms of the testing, I think there's actually a way you can cheat which is just to do the same thing, which is like you calculate the Max stack height that happens in the subroutine and then paste that as the header. I don't know if that's an acceptable thing.
01:10:53.410 - 01:11:07.140, Speaker C: Yeah, that should work probably too. Like if we pull the stack validation algorithm into testing too.
01:11:10.070 - 01:11:12.994, Speaker D: Yeah. I don't know how circular that makes things, right.
01:11:13.032 - 01:11:13.620, Speaker B: But.
01:11:16.630 - 01:13:05.720, Speaker D: I don't think that that is any worse than putting 1024 in every header. But what if each call frame had its own stack? Then it would be more, the abstraction would be less leaky. It. I think the interesting thing is like inside of a subroutine, you can't access things that are outside of the call frame. Right. So there's no benefit, I think, to enforcing the global stack restriction, unless I'm misunderstanding that.
01:13:32.470 - 01:13:41.220, Speaker A: Yeah, I think we have been 90 minutes into the call, so maybe we're less fresh to think about this.
01:13:42.550 - 01:13:48.360, Speaker B: Yeah. Let's transition to the ACD strategy and we can pick this up next week.
01:13:49.930 - 01:14:36.760, Speaker A: Exactly. Yeah. Maybe just a small intro. We had a discussion in Epsilon regarding this and our view was that we just want to you to present the facts and basically show where we are and what is a reasonable time frame if we just go in there with uncertainty. Okay, question one, two, three. All of these are like still open. We're just trying to figure out.
01:14:36.760 - 01:15:58.370, Speaker A: Then it suggests that this is going to take forever. So I think we have to be more confident in the presentation. And we can only be confident if you're confident internally that we have a good enough specification. Yeah, I think that's basically the short summary thing. We wanted to probably spend like 1015 minutes tops with some slides on explaining the different pieces we have. Obviously, I don't think we need to do like a recap, like a 50th recap of what the header format is, of course, but rather what kind of components we have in the design system because we have so many different sets of instructions for different reasons. Yeah, I'm not sure any reflection to what I said.
01:16:00.580 - 01:16:35.900, Speaker B: I think those are good. But I think you also probably need to start the discussion off with some of the bigger objections. One of them we need to be clear we're not replacing legacy, that EOF is going to be parallel to legacy, and then that 90% of the code between legacy and EOF is reused between the two. So we get rid of the concern that we're bringing in an entirely separate system in here and focus on that EOF is just a container format. And then I think we should pitch the benefits and that we can break things that need to be broken and list of things we're going to break. We're going to break gas observability.
01:16:38.900 - 01:16:41.712, Speaker A: I'm just trying to keep a note. Can you say like the first piece.
01:16:41.766 - 01:16:49.600, Speaker B: Again, the first piece I would probably. About the legacy and the container.
01:16:52.840 - 01:16:56.020, Speaker A: Exactly in a single sentence.
01:16:59.480 - 01:17:38.770, Speaker B: EOF is a container format that will operate alongside legacy EVM instructions. It uses 90% of the same code to execute the evaluation. It just changes how we package the EVM instructions. So that way I think one of the big objections is if we're going to rewrite EVM top to bottom, why don't we just bring in my favorite alternative execution framework so we make it clear what we're really changing is the packaging format and that it's mostly the same. It's almost entirely the same.
01:17:41.940 - 01:17:47.376, Speaker A: Except it's not. Right? Except it's not.
01:17:47.558 - 01:17:48.396, Speaker B: Except it's not.
01:17:48.438 - 01:17:51.236, Speaker D: Well, you still have message frames, you.
01:17:51.258 - 01:18:33.840, Speaker B: Still have opcodes, you still have storage, you still have transient storage, you still have memory. The main things are all still there. So I think the next point that we would want to make explains why we even do this container format is that we're taking the opportunity in a container format to break rules that can't be broken without a structural change. We're getting rid of code observability, we're getting rid of gas observability. We're allowing new opcodes with immediate arguments. Those I think are the three big things that we're getting in there had a list of other things we are breaking. Oh, address space expansion.
01:18:33.840 - 01:19:09.470, Speaker B: Maybe put a star there that we're not committed to that. But if we're going to do it, this is where we should do it. And we can break fundamental issues in the container format that we just could not even address in the legacy format. So I think that sets the table for need, and then I think we can go into the overview of what are some of the features we're adding that this allows us to add, like, code validation containers, how we're dealing with code observability issues.
01:19:24.190 - 01:19:39.380, Speaker A: It. Yeah. When I said it changes everything. Yeah. I think it's a good framing, but exactly like. .2 I think it goes against the notion of one.
01:19:39.380 - 01:19:47.080, Speaker A: It's just a container format, but we have changed everything. But it's fine for me.
01:19:47.770 - 01:20:03.610, Speaker D: I think that one objection that might come up is like, why not wasm or some other vm? Because UF is significantly different enough from legacy EVM that people might be like, why not just use some?
01:20:03.680 - 01:20:17.840, Speaker A: Honestly. Honestly, I doubt that would come up, because WASm doesn't fix any of the problems we have. Gas weaving is probably observability on any of these.
01:20:23.590 - 01:20:29.246, Speaker B: Any language that doesn't have a gas construct is going to be a hard sell, because that's the halting problem solution.
01:20:29.438 - 01:20:37.480, Speaker D: Yeah. I mean, Stylus does this now, right? They have something which injects gas metering into WASM code.
01:20:38.490 - 01:20:59.760, Speaker A: Yeah. I mean, ewasm did the very same, but ewasm and stylus and the parity version of ewasm and all of these things, they basically bring wasm to where the EVM is and doesn't do anything more than.
01:21:02.210 - 01:21:07.200, Speaker D: Just it's worth it to be prepared for that line of question.
01:21:08.290 - 01:21:39.270, Speaker A: Yeah, I wouldn't be afraid of that, to be honest, because WASm has been killed off, like, several times on all cordevs. I think the answer is mean. Just the answer is simple. It doesn't actually solve the main problems you're trying to solve, which is any of, like, the code and gas observability, upgradability, et cetera. But, yeah. Daniel, you wanted to say.
01:21:40.920 - 01:22:07.440, Speaker B: I mean, there's so many things you have to change to wasm anyway, like getting rid of floating points. We need a standardized gas weaving that's been rehashed, and there's a reason it hasn't gotten large traction, because the performance isn't that much better. I think that was one of your presentations in Osaka, was, to that effect. Know, we tried to make it fast, but we couldn't without doing strange things like moving math in a pre compiled.
01:22:14.380 - 01:23:09.310, Speaker A: Yeah. For the use cases we have, it doesn't bring as many benefits, but it has basically zero overlap with what we do in UF. With the original version of EOF, the overlap would have been much bigger because that was much closer conceptually. But after code and gas observability changes, we are in a very different area. But yeah, Dennis, I was keeping notes. And you got to the overview.
01:23:10.210 - 01:23:10.862, Speaker B: Yeah.
01:23:10.996 - 01:23:12.000, Speaker A: Do you have any?
01:23:14.050 - 01:23:58.538, Speaker B: I think those are the big things to expand on the overview. I think this is the wrong crowd to pitch. The EVM is the nexus of all smart contracts in the future, because I think one thing we might want to bring up is, what if there's eof two and eof three? The first answer is we're going to try and break enough things that we don't have to do it for five to ten years, ideally never. But with the structures that we have with code validation, we can engage in transpolation. We can take, let's say, eof two. We need to get rid of all the dupes and swaps. It's a fairly mechanical process to take EOF code in that EOF one into this hypothetical EOf two, and mechanically translate the code because of the code validation.
01:23:58.538 - 01:24:51.486, Speaker B: So that's one advantage. That's probably at the end of the presentation, or maybe as a backup slide in case someone asks what happens with eof two? We could translate eof one to eof two as needed. In fact, I think it's possible to take all EOf one code, barring size and gas equivalents, and reverse compile it down to legacy code. I think everything we do here will be ugly, but it could be done so we could still transpile it into the equivalent bases. So going forward there would be legacy code, maybe that would be shut down to new deployments on main net and one version of Eof. And if we upgrade a version of EOF, we would have to have a way to describe how you go from EOF version one to two as part of your transition. But that's five to ten years down the road if we do it.
01:24:51.486 - 01:25:31.100, Speaker B: And the greater ecosystem pitch is what if there's L2s that only ever use EOF? This is great for zks because we do things like get rid of dynamic jumps and some of the other code validation makes it friendlier for them. We could cross compile it into wasm if we wanted to. I mean, that's nothing stopping you from cross compiling your EvM today into wasm. And I think maybe an overview of the high level features like containers and what each EIP provides and how it fits in.
01:25:37.050 - 01:25:52.860, Speaker D: I think to present that it would be helpful to have a working compiler implementation that proves that you can take Uf and compile it to something else, either Uf two or wasm or whatever it is.
01:25:55.470 - 01:26:06.126, Speaker B: Yeah, that would make a compelling pictures is why I think it belongs in a backup slide, because I'm not aware of any transpilers that ready for that yet.
01:26:06.308 - 01:26:14.190, Speaker D: I think as Greg Covid has been pointing out for a long time, you can't write a compiler for current legacy.
01:26:16.930 - 01:27:09.480, Speaker B: That's well constant time compiler, and that's where the code validation and stack validation comes in and the dynamic jumps. It's easier to write compiler bombs for dynamic jumps. The big thing is you could have the stack value for the jump come from call data and you could have it go into big old nest of arbitrary jump desks. So the amount of compiler contortions you have to do to that quickly escapes linear time. So I think that's where Greg's main complaint is and that's really fixed by code validation and static jumps I think is what his 615 was trying to address.
01:27:10.810 - 01:27:21.770, Speaker D: Yeah, I mean even if you can write a compiler, it's like significantly harder. Right. I think anybody who's worked on the EVM kind of knows.
01:27:22.270 - 01:28:05.346, Speaker B: Right. Well, because what Greg wanted to do was to do just in time compilation where everything was just in time compiled straight into your local opcode. But Java's known for a while that there's some compiler things. So Java's written a mixed mode where you do mixed interpreted or mixed compiled and they go the extra mile. It's like, well you can make some assumptions, like if you know this field never changes from zero, you can just compile it in as if it's a constant. And they do fancy things like if they detect the field changing, they'll go invalidate all that code and do interpret it. I mean this is some high level stuff that Java does, but the amount of risk to return, they're always finding ways to break the compiler.
01:28:05.346 - 01:28:10.680, Speaker B: In every release of every dot release someone's found some silly corner case that does strange things.
01:28:10.990 - 01:28:15.866, Speaker D: Yeah. Isn't the JVM stack validation spec like 160 pages or something?
01:28:16.048 - 01:28:17.100, Speaker B: Sure is.
01:28:18.670 - 01:28:24.350, Speaker D: It's crazy long good way to frame it is like it improves maintainability of the EVM.
01:28:25.890 - 01:29:16.210, Speaker B: Yeah, that's one good way to phrase it. Yeah. I think the reason that I have to look at the spec. Again, why some of those sections are like 160 is they go into detail for each and every opcode and they also need to worry about backwards compatibility clear back to Java version 1.1 or some really far out there requirement where they've changed the class file formats and they define the equality between each of the different formats. So that's why I think the strategy of one EOf would solve that because there'd be one transformation into it and then we just deal with one format and there's proofs in their spec.
01:29:26.520 - 01:30:28.200, Speaker A: I do like your proposed structure, but on the detailed overview, the .1 and two probably going to take like two to 4 minutes. That means we have like 1012 minutes if we want to keep within like a 15 minutes bound, which I think would be good. Otherwise people are going to get tired, right? So 10 minutes for the overview part, which I think means that we definitely should give a clear overview of what kind of eips we have and what kind of problems or categories of changes each of those focus on. But obviously we cannot really dive deep into the eips and that probably would be overkill anyway. People will get extremely tired. But do you have a good idea of how should we group the features?
01:30:33.880 - 01:31:19.370, Speaker B: Let me grab my here's a talk I gave at East Chicago. Um, you'll want to start looking at slide 2024. I grouped them just by EIP. So I talked about static relative jumps. Then I talked about code sections. So that was 42, 47, 50 code sections, functions 663, unlimited swap and dupe. We probably want to revise my slide there to consider exchange.
01:31:19.370 - 01:32:11.930, Speaker B: We don't have an EIP for data operations yet. Explain how this is to deal with. And I probably explained for each of these how these address one of the required breakage things. So data operations, we would explain that we need to do this to replace all of our code introspection things. So I list why we're banning opcodes and why this replaces those. I think the data operations is a big one that's going to need that pitch. Slide 28 is create three and create four and then call two, delegate call two how that's dealing with gas, that's 70, 69, 35, 40 EOF code validation.
01:32:11.930 - 01:33:03.770, Speaker B: And then there's a small list of other things. Actually maybe I should share this. We got to isolate this share screen. Yeah, so as far as, so start with the container layout and I just blew through this really quick. But 4200 static relative jumps just talked about why we need those three and what they do did deal with containers yet code sections and functions. Then talked about that. This is new functionality, it's not breaking things, but it helps code size and reusability.
01:33:03.770 - 01:33:39.480, Speaker B: Unlimited swap and dupe. Like I said, we would need to update this for exchange. I like putting this up here because it's got a low EIP number so we can really pitch that we're dealing with long standing problems. And the reason I would say that this couldn't be done until now is because of the immediate code. Because I started this presentation is to a relatively audience that was unaware of a lot of EVM. I told the story of how adding relative jumps just couldn't be done and what broke it. I think everyone on the call is probably aware of that, so we probably don't need to tell that story.
01:33:39.480 - 01:34:15.438, Speaker B: But like unlimited swap and dupes we couldn't do without more immediate arguments. And this solves a lot of long requested features and then the data operations. Again, we talk about this in context of if we're going to break things like code observability, here's what it takes to replace it and get the functionality we need. The same with create three and create four. That's the other half of code observability. We need to be able to create stuff without putting the bytecode into memory. And then call two and delegate call two if we're going to break gas observability.
01:34:15.438 - 01:34:56.874, Speaker B: Here's what it looks like and what we would need to fix it. And call code is just a bonus. Code validation. This is mostly, I don't know if it's necessary to break well, I mean we are breaking things. We're not letting just random garbage go out there. But the value that we get from this going forward is we can do more logical transitions from past versions of EOF to future versions, and then other EOF interactions, some other random things that we needed to deal with, some corner cases and stack validation. This allows for deeper compiler black magic.
01:34:56.874 - 01:35:25.830, Speaker B: And this is an AI generated thing. I said generate terrors of Hanoi. And computers don't get how to play terrors of Hanoi because this is like the wrongest thing you can do in terrors of Hanoi. So I think that's a great explanation of how we can do that. And then I'll list of the future bubbles. But there was one slide that I focused on compatible versus incompatible changes. So I think this is probably another point we need to make.
01:35:25.830 - 01:35:50.906, Speaker B: Are we going to add features to the Eof? Yes, we are. The stuff on the left column is the stuff of breaking space for that's easier to add. The stuff on the right column is basically impossible to add without changing the EOF version. We're doing these things here, we're doing it once. Ideally we're breaking everything we can. So we're just adding new opcodes, we're adding new byte sequences. But I think these are the two standards.
01:35:50.906 - 01:36:19.318, Speaker B: If we're making the invalid byte sequence valid then that's fine. And we're treating all prior contracts the same. And so these things don't require a contract version bump. We don't have to change the EOF version if we're just simply making new stuff. But if we're changing semantics like self destruct, typically that's incompatible. If we don't execute contracts the same as self destruct anymore, typically incompatible. And we're getting rid of features you can't do arbitrary jumps from call data code.
01:36:19.318 - 01:36:58.102, Speaker B: Again, we're also removing that. So these are the sort of things that we would try to avoid. And these are the big things we listed. So this might be a backup slide we could have up to say well how are you getting handle adding new features into EOF that are smaller features like the new opcodes we're looking at putting in. Like EVM Max is a great example of a compatible change. It's just a bunch of new opcodes, it's adding functionality and it's not breaking things. And it could still go into EOFV one just with a future reversion like EoFV Eleven, what we would call that.
01:36:58.102 - 01:37:13.050, Speaker B: Or like what we're doing today with all of our fork versions of EVM. And I couldn't resist the main. So here's my list of things we were breaking.
01:37:20.260 - 01:37:29.456, Speaker A: So would you suggest to. Are you volunteering to run maybe a shorter version of these slides?
01:37:29.648 - 01:37:39.130, Speaker B: Yes, I can trim these down. I can get revision ready in a few hours based on what I proposed. Take all the cutesy stuff out and get it down to 15 minutes.
01:37:44.700 - 01:37:57.310, Speaker A: Yeah, I think that would be nice. I would definitely try to focus on the structure you suggested, which may need some changes to this.
01:37:58.000 - 01:37:58.750, Speaker B: Right.
01:37:59.520 - 01:38:03.980, Speaker A: I feel like maybe some of the slides are maybe too detailed.
01:38:04.320 - 01:38:04.856, Speaker B: Agreed.
01:38:04.888 - 01:38:11.490, Speaker A: I mean yeah they can stay detailed, but maybe we don't need to talk about each of those points, just on a high level.
01:38:11.860 - 01:38:22.370, Speaker B: Yeah, I agree. Take out all the detail. Okay, yeah, go ahead.
01:38:22.900 - 01:38:25.116, Speaker A: I will be around late tonight.
01:38:25.148 - 01:38:25.536, Speaker B: I'll be around.
01:38:25.558 - 01:39:02.880, Speaker A: I'll be on the call to help with this. Yeah, but to review any different versions of the slides or any of mean. I will be on the call, of course. I think everybody else from Ipsila is going to be. I mean, I don't. Yeah, maybe there's some pieces where we probably should speak up as ipsilon. Just that it doesn't come off that ipsylon has stopped working on this, because we haven't.
01:39:02.880 - 01:39:38.940, Speaker A: But I don't think it has to be know presenting it, because you do have a good set of slides and a good story. So I think you should be doing it. I just want to avoid the notion that ipsylon is done working in the UF, because that would be, like, a bad signal. Right. So that's maybe the only thing we have to somehow avoid and maybe going to be there. I'm not sure if you have any suggestions how or is this even, like, a valid concern.
01:39:39.920 - 01:39:51.410, Speaker B: Okay. All right, so I'll start trimming these down. I should have. You guys are. Ipsilant's mostly Europe based, right?
01:39:52.820 - 01:39:53.570, Speaker A: Yeah.
01:39:54.020 - 01:40:03.190, Speaker B: Okay. So I'll probably just need to wake up earlier and check my slack well before. Sometimes I wake up right before all core devs, but I'll be a bit more responsible this time.
01:40:06.280 - 01:40:10.720, Speaker A: Yeah, maybe don't sleep through all core dance this time.
01:40:10.890 - 01:40:17.770, Speaker B: Haven't done that yet. Cool.
01:40:21.020 - 01:40:30.908, Speaker A: I'm probably going to take at least an hour break, if not too. But otherwise, they are going to be available late, which means for at least another 5 hours. Five, six.
01:40:30.994 - 01:40:36.770, Speaker B: Okay. Yeah. I'll try and get something ready within 2 hours. So take a two hour break.
01:40:40.220 - 01:40:58.380, Speaker A: That's good. I mean. Yeah. This was my feedback. I'm not sure. Radek, andre from epsilon. Any feedback from our perspective? No, I'm fine with this.
01:40:58.380 - 01:41:10.130, Speaker A: So thanks, Dana, for the volunteers and this presentation, it should be great. And I also will available tomorrow morning to review it.
01:41:10.900 - 01:41:19.380, Speaker B: Cool. So I'll get slides updated within 2 hours, and then plan on when I'll wake up tomorrow for full feedback.
01:41:20.680 - 01:41:22.292, Speaker A: Yeah. Great. Thank you.
01:41:22.426 - 01:41:23.430, Speaker B: Great. Thanks.
01:41:25.000 - 01:41:28.740, Speaker A: Awesome. Charles, do you have any feedback to the slides?
01:41:35.560 - 01:41:53.500, Speaker D: On the slides, exactly. But I think, Axe, you mentioned being confident about the proposed changes, and I think that from Viper's perspective, I would be most confident if there weren't any performance regressions being introduced in EOF. And I'm talking about code size regressions.
01:41:59.710 - 01:42:08.970, Speaker A: I mean, those are really. I mean, which one is the biggest one is the return data load?
01:42:10.830 - 01:42:16.190, Speaker D: I think the biggest one is actually the jump fi or the stack validation rules.
01:42:19.420 - 01:42:58.516, Speaker A: Yeah, I think on confidence with those, I guess we are tending towards, like, on the jump f question, at least we're tending towards choosing one or the other. Right. So it's a matter of which is the better solution. It's not a question whether something will change. So I think on that maybe we can be more confident that that question could be solved. Maybe there's less confidence on the return data load because it's yet another instruction. But I think that again solidity would be in favor of.
01:42:58.516 - 01:43:20.220, Speaker A: Because they raised it before. I think in terms of confidence whether those features would be liked by the compilers. There's no lack of confidence there. The only issue we have is this recurring issue of too many instructions and too many changes. That's the only downside.
01:43:26.360 - 01:43:27.110, Speaker D: Yeah.
01:43:30.620 - 01:43:44.170, Speaker A: Daniel, do you actually have any idea regarding. Because this question probably going to come up, right. It's just so many instructions. Do we have a good answer to that?
01:43:44.940 - 01:44:15.552, Speaker B: So yeah, I was going to put together a page on testing. I think one of the things we have going for us is I think EVM testing is more understood now than it was a few years ago. We've got a couple of great fuzzers. Guido V is writing an amazing fuzzer that's getting some amazing findings. And we don't need to have these large interrupt events because you can test the EVM in isolation. You don't need to test gets EVM interacting with basis EVM. There's no interaction, there's only one set of inputs.
01:44:15.552 - 01:44:58.044, Speaker B: So it's a lot easier to test these. There's a lot less DevOps setup. It's mostly setting up time, grinding on the fuzzers and we would just need to write a fuzzer against the container format. We need to write fuzzers that generate extra code sections to exercise that code. And of course the fuzzers that exercise. The current operations and some of the findings from Marius and Guido are that as far as the legacy evms, all the active evms that are live on the network right now are all giving the same answers. So we've got tools that will get us the signals we need, which I think is we didn't quite have these tools before the merge.
01:44:58.044 - 01:45:21.630, Speaker B: I don't think we really quite cracked the nut on how good fuzzing was going to be in finding all of these issues. We didn't have good enough fuzzing frameworks for this back when we were doing the Berlin and those types of changes. So I think we've really improved the testing. So I think I'll put a slide together on testing as well, saying that it's getting simpler and it's better understood and we have a lot of the framework ready in place.
01:45:31.930 - 01:46:13.954, Speaker A: So I guess you will have a slide. Regarding testing and what you said on fuzzing. Yeah, I think the two parts is testing one and two, which I think you cover on the slides, is these big changes like code and gas observability. It's just an implication. They depend on these new instructions. And in fact I think for most of them there were equal number of instructions. We don't really add many new, but we disable as many.
01:46:13.954 - 01:46:32.090, Speaker A: Maybe the only exception is for data observability. We are kind of equal or less for creation. We do have more because of the return contract and maybe jumps. It's a bit more instructions than what we remove.
01:46:32.750 - 01:46:44.000, Speaker B: So code sections is new, unlimited dupe and swap is new. There's no deletes there. Let me go back down to my list.
01:46:47.890 - 01:46:55.650, Speaker A: Do you think maybe we should have like a table of these instructions are added, these are removed and what's the balance?
01:46:56.950 - 01:46:59.170, Speaker B: That would be, I think, interesting and valuable.
01:47:04.230 - 01:47:05.074, Speaker A: It would be what?
01:47:05.112 - 01:47:05.700, Speaker D: Sorry.
01:47:06.550 - 01:47:09.170, Speaker B: I think that would be valuable to have that table.
01:47:10.490 - 01:47:45.750, Speaker A: Yeah. It's like a double edged sword, but on one side at least, depending what number we get, on one side at least it shows that. Okay. Yeah, it's a lot of instruction, but we also remove a lot. And if the difference is really maybe like five, six instructions, then I think we are kind of good because it's not significantly more complex instruction count wise. It's just a lot of them are replaced.
01:47:46.490 - 01:47:50.310, Speaker D: I feel like we can also just start killing off dupenswap instructions.
01:47:54.350 - 01:47:55.100, Speaker B: Yeah.
01:47:55.550 - 01:48:05.610, Speaker D: Because nobody really not that well liked.
01:48:13.470 - 01:48:28.810, Speaker B: And we're net one here. That's good. Yeah, I think we're like net one hand. It's the call f and the RETF, I think are where the real ads come in.
01:48:44.770 - 01:48:45.310, Speaker A: All right.
01:48:45.380 - 01:48:45.614, Speaker B: Yeah.
01:48:45.652 - 01:48:51.618, Speaker A: Let me know if I can help with the slides in any way. But at least we'll be able to review them.
01:48:51.784 - 01:48:57.780, Speaker B: Yeah, I'll get to a first pass. I'll start churning from here for a couple of hours and then share it on the EVM channel.
01:49:01.420 - 01:49:25.280, Speaker A: This was a monster marathon of Uf call, but it was worth. Think we got most of it sorted. Yeah. Charis, I mean, don't be concerned. I think the size regressions, it's going to be sorted, so. Yeah, I wouldn't be concerned with the size regressions.
01:49:30.340 - 01:49:30.704, Speaker B: Yeah.
01:49:30.742 - 01:49:39.030, Speaker D: Okay. And I also want to emphasize the importance of exchange. But we can all get on the call with solidity and discuss.
01:49:40.760 - 01:50:01.716, Speaker A: Yeah, exactly. I think it's better if we first get consensus between solidity and viper with these, like, two outstanding questions. And I wouldn't raise eyebrows on all cordefs before that. I mean, there's no point because we know there's going to be a solution to it. We just need to organize the discussion.
01:50:01.908 - 01:50:02.724, Speaker D: You wouldn't raise.
01:50:02.772 - 01:50:02.984, Speaker A: What?
01:50:03.022 - 01:50:03.610, Speaker D: Sorry.
01:50:04.380 - 01:50:09.150, Speaker A: I wouldn't raise any eyebrows on all cordefs that. Oh, there is one outstanding question here.
01:50:10.800 - 01:50:11.212, Speaker D: Yeah.
01:50:11.266 - 01:50:31.820, Speaker A: Okay. I wouldn't give extra gram to skeptical people. All right, awesome. I have nothing else to say from my perspective. Thank you all, and see you tomorrow.
01:50:32.200 - 01:50:34.100, Speaker D: All right, thanks, guys.
01:50:34.170 - 01:50:34.900, Speaker A: See you.
01:50:35.050 - 01:50:35.572, Speaker C: Thank you.
01:50:35.626 - 01:50:37.968, Speaker A: Bye. Thank you. Bye.
