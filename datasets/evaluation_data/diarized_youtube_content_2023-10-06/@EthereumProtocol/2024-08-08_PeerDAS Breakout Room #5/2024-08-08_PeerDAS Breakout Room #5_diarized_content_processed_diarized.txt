00:01:04.690 - 00:01:05.270, Speaker A: Hello.
00:02:35.290 - 00:02:36.110, Speaker B: Hello.
00:02:43.850 - 00:02:44.630, Speaker A: Hello.
00:02:46.810 - 00:04:53.330, Speaker B: Let's wait for one more minute. Hello everyone. That's the article. So we start with the client updates. Anyone want to share? I think not every client attending today but anyone wants to start. So maybe before the update just didn't confirm. So Deathnet one was shut down and then currently we don't have any active Devnet, is that correct?
00:04:54.310 - 00:04:55.770, Speaker C: That's correct.
00:04:58.790 - 00:05:05.290, Speaker B: Okay, now client update recent.
00:05:08.740 - 00:06:05.426, Speaker D: So during the last two weeks we work on a fix about our initial synchronization. Also on a fix about our sampling, about new feature we implemented the metadata v three with the custody subnet count is the metadata. So basically if the peer is able to respond with custody subnet counts in metadata entry, we use it, else we fall back on the node record. We implemented this pull request, this feature even if the pull request is not yet merged in the specification. Yes. And what we are doing now, we are working on the blobsidecars beacon API right now. There is a xv one beacon block.
00:06:05.426 - 00:06:28.064, Speaker D: Sidecar Beacon API is not able to respond with peer desk. And also we are working on a better pipeline for data column verification. But basically this is it for Prisme.
00:06:28.102 - 00:06:32.840, Speaker B: Thank you. And then Lighthouse.
00:06:35.780 - 00:07:22.020, Speaker C: Hello hello everyone for Lighthouse, we have been debugging our networking issues. Previously we had issues with our lighthouse node discontinuing other lighthouse nodes. So we've identified the issue and we've made a fix for it. So it's, there's an improvement in network stability and then we've also been doing a bit of debugging on the Rustkasg library and Kev has made a few fixes for us. So I think our Das branch is getting a bit more stable now. And currently we have focused on refactoring and progressively kind of merge our Das branch into our main branch. We've made some good progress.
00:07:22.020 - 00:08:15.700, Speaker C: So we're about halfway there. I think we'll continue to focus on finishing the merge into the main branch so we can kind of pick up the speed a bit quicker after it's finished merging. Another change that we've made is we've disabled sampling by default for now because I think sampling was causing us a bit of issue and then we're trying to isolate some networking issues. So it's currently disabled by default, but we can also enable it, I think. Although there's also a proposal from Francesco and Alex Stokes about not shipping sampling with PDA. So I think that could potentially be useful if we want to enable and disable sampling in the future devnet. So we've got a flag added to enable or disable pyad sampling.
00:08:15.700 - 00:08:48.849, Speaker C: I think we're probably going to start preparing for Devnet two soon. I don't think we have a spec soon, but I don't think we have a spec yet. But I think we've talked about some of the things that we want to implement like the metadata v three, so we might potentially look at that soon as well. At the moment we're trying to focus on getting our Das branch merged into our main branch and then we'll start looking at the Plas Devnet to specification changes. That's all for Lighthouse. Thanks.
00:08:52.269 - 00:08:58.729, Speaker B: Thank you. Any other clients? Lo star.
00:09:03.069 - 00:09:59.608, Speaker E: Hey guys. So there isn't much update on Lo star. I'm working on getting the metadata implemented and. But otherwise I think without sampling we should be good for any devnet that we can spin up. But I have a question with, regarding, regarding to, with regards to the data sampling. So are we going to keep data availability. Sorry, with regard to data availability, are we going to keep data availability for the current slot or it is going for parent slot? Because on the basis of that there might be some refactoring or some network that I might need to do for, for the fork choice.
00:09:59.608 - 00:10:12.270, Speaker E: And I think this is sort of an important point. What is the availability that we are envisioning when we go live? Because yeah, a lot of works depend upon that.
00:10:19.370 - 00:10:26.950, Speaker B: Thank you. Brentian.
00:10:28.800 - 00:11:13.240, Speaker D: Hi. Solis from Grenada team. So we had some fixes for the birdhouse. A few folks are working on this from the fellowship program. So essentially I think we are moving forward. However, I hear that there are discussions to remove sampling and during this call I would like to hear what is the status on this and if it's going to be accepted quite soon.
00:11:21.670 - 00:11:27.290, Speaker B: Thank you. And do we have any.
00:11:35.670 - 00:12:14.276, Speaker F: I have found and fixed several issues with our default sampler and also we have implemented lost assembler, but it's difficult to test it in local environment. So until Devnet launch, I'm not sure in it. Both samplers are not completely integrated in fork choice. So we just log when we sampled. We don't break fork choice. If we cannot sample, we want more testing before enabling it. And also we have added all required by spec validation and gossip.
00:12:14.276 - 00:12:32.830, Speaker F: And the graycresp, we didn't have it before and everything is not merged yet. Me and Dent are now on vacation, so I hope we are not launching Devnet two on this week. That's all for me.
00:12:39.810 - 00:12:44.820, Speaker B: Thank you. Do we have members here today?
00:12:45.840 - 00:12:46.456, Speaker G: Yeah.
00:12:46.568 - 00:12:47.980, Speaker B: Oh, hi. Yes.
00:12:48.560 - 00:12:49.340, Speaker H: Yeah.
00:12:49.720 - 00:14:00.190, Speaker G: So last week we actually focused more on having a lesser diff in terms of our Petra branch and the pure dust branch. So we started off with upstreaming the CKCG to the latest and we fixed all issues that were basically creating a diff in the pectoral branch first, like having that extra precompute parameter to accelerate I think load brushed it setup. Then last week we started implementing lossy sampling as well, but we sort of halted it till Thursday's call. Like last to last Thursday's call where I think we all agreed to drop sampling. So there is not much progress in that region right now. After that we refactored alpha three specs and also like all the related crypto stuff as well. And while passing the test vectors I actually came across that there was some inconsistency between the alpha three test vectors and the alpha three public functions that were exposed.
00:14:00.190 - 00:14:23.140, Speaker G: So I think that is fixed. I think that can get fixed in the next alpha four release. We are still wip on metadata v three for custody subnet count and we are still work in progress for the upcoming prs like validator custody and the fork choice. So yeah, that is pretty much from members.
00:14:26.680 - 00:15:02.920, Speaker B: Thank you. So on the agenda it was about definite one, but I think banabas is here now and then do we want to talk about the restart of definite one or talk about definite two instead? What's the code name of the next dev name exactly?
00:15:06.580 - 00:15:20.360, Speaker A: I haven't been testing any peer des branches lately on kurtosis. I would be happy to begin this again. I'm just curious about client readiness basically.
00:15:22.540 - 00:15:23.360, Speaker I: Yep.
00:15:24.790 - 00:15:38.730, Speaker H: Can we set a date for the next Devnet if other clients are feeling ready to and yeah, and also like fix up on the features that we want the next Devnet.
00:15:42.870 - 00:16:19.540, Speaker E: I think before we sort of set the date on next, I think, yeah, it's worth discussing what the spec is going to be. And my question that I already raised when I talked earlier was that, so how is the data sampling going to look like when this goes live? Because I think we would want the next version of Devnet to look as closely as how it should be in the life. So if somebody has an input on data sampling point, because I think, I'm very curious about it.
00:16:24.450 - 00:16:28.310, Speaker H: Could you like clarify what issues with sampling do you have?
00:16:30.770 - 00:16:54.690, Speaker E: Replace sampling by availability. Sorry, I meant availability. So data availability in the sense that are we going to do a data availability of the current slot itself or are we going to do data availability of parenthood or let's say of an athen tester? So what is the data availability going to look like when we go live?
00:16:56.110 - 00:17:10.370, Speaker H: It's, it's always for the current slot, correct? Currently with the nav, if you want to validate the block you got to validate the blobs for it in the current slot, not in the next slot and so on.
00:17:12.039 - 00:17:19.779, Speaker E: Yeah that is okay. But are we, is that what we are going live with? What we will go live with for peer Das?
00:17:21.199 - 00:17:35.419, Speaker H: Yeah, that's for us. That would be hard works right now. So if you get a block you have to validate its DA at its current slot.
00:17:36.770 - 00:17:44.390, Speaker E: Yeah, for, but in the spec, for example it was mentioned that the DA, one of the previous of the parents locked. Right. So.
00:17:46.450 - 00:17:58.110, Speaker H: Is this for sampling or. Because from what I understand for verifying any block for it's. You want to validate its DA, it's always done at its slot itself.
00:18:00.490 - 00:18:33.210, Speaker E: Yeah. This is not regarding sampling and I just want to sort of clarify with regard to availability. I mean sampling is easy to do, DF and A. Thanks Astro, because it does not affect the folk choice that you have. And one of that is sort of my major, major curiosity that that are we going to stick with the current slot or the availability is going to change? Because if it's going to change then the fork choice will change and hence there is some work I think that should be put in before we launch any other evidence.
00:18:38.990 - 00:19:13.586, Speaker C: So there's a recent proposal by Alex and I think Francesco made a post about it just today, yesterday for some time zones that how you look like without sampling. If we don't do sampling then we're just doing the, the custody column verification, which means we're doing it for the current slot. Is that correct? If we're just doing the custody columns and no sampling? Yep.
00:19:13.698 - 00:20:12.570, Speaker E: We are only worried about. So when we are importing the block and the availability check is only about the current slot of whatever custody that you want, I think then yes, then there is no major change that is required in the work choice. And sampling is basically to see whether the peer is relevant or not and it can happen at any point or not happen at all. And obviously if for example you are using availability of the current slot then you already can say that we have sampled for that particular custody that we are interested in. So I guess that is okay. But yeah, I just want to confirm that whether availability checks are going to be on the current slot or do, is there a possibility that availability of the parent or of some ancestor will be checked? Because that will radically change the folk choice.
00:20:16.390 - 00:20:29.990, Speaker H: I mean you can't import a block if you haven't verified its parents. Da. So I'm guessing there's no change from how we do it in dinner right now for blobs.
00:20:30.570 - 00:20:58.560, Speaker E: I think there was a version of spec where it said that availability checks will have, will happen for the parent. But of course if we are doing availability checks for the current slot while importing, obviously we would have no block in between the folk choice, which does not have. So the parent does not come into the question. Uh, so if it's going to stay that way, then I guess it's okay.
00:20:59.740 - 00:21:06.440, Speaker H: Yeah, it should stay that way. Um, I think you're referring to sampling then because that was when it was brought up.
00:21:08.060 - 00:21:27.550, Speaker E: Okay, I'll recheck because I think I'm confused. I was confused and I was thinking that even for availability, that will import a block if its parent is available. So parent data columns are available. So if that is not the case, then my confusion is sort of resolved.
00:21:28.290 - 00:21:33.870, Speaker H: Yeah, I think you're referring to sampling in that case because I remember that is when it was brought up.
00:21:34.850 - 00:21:36.150, Speaker E: Okay, cool. Thanks.
00:21:39.690 - 00:23:02.560, Speaker A: With regards to sampling. So I'm not an expert on this, but because someone asked whether, you know, sampling is going to be the final form or it's not going to be the final form, I think it's still under discussion. And I think, and by sampling I mean like sampling of, not the current thing of the, you know, after the fact. Basically I think this is still under discussion. And looking at Francesco's newest post, I do have the feeling that if people think that it greatly reduces implementation complexity, I think it's quite like making this clear will make things easier to decide, basically because it is mostly an implementation complexity reducing suggestion. So, you know, from this discussion I get the feeling that it really improves implementation complexity. So if that's true, I think it is worth mentioning, and it might, it will influence whether, you know, we end up doing something in the end or nothing.
00:23:10.810 - 00:24:10.190, Speaker E: Okay, I'm just reading from the consensus specs that has been sort of linked from the AIP and it says, I know, don't for choice work choice. It will be a replacement office data available. Call in data with column sampling instead of full download. Note that is data available will likely do a minus one follow distance so that you need to check the availability of slot and minus one first load n. So it's not about sampling, it's about availability. And if there is a change of availability, in the sense that in the current event we are doing the availability of the current slot or block that we are importing. But if, if it's going to parent then it will sort of change the way the fork choice is implemented and it needs to, that needs to be taken care of in the sense that why I mentioned it, why I mentioned this is that any devnet that we spin off should be as close to the devnets that we want to go live.
00:24:10.190 - 00:24:27.304, Speaker E: So if we want to do n minus one availability then we need to depth that code in and get it out and make sure that next devnets we are doing it on n one sampling instead of the current n sampling availability.
00:24:27.472 - 00:24:50.100, Speaker H: Sorry, so what are you referring to is trailing slot for choice? So in the interrupt we kind of decided that it is not a good idea to do this because you don't, you do not want to put sampling in the critical path for consensus which is in fork choice. So I don't think, you know, we have to like worry about checking for this.
00:24:50.440 - 00:24:59.030, Speaker E: In that case, can we sort of update the specs and sort of link the correct specs in the EIP because it can be confusing.
00:25:00.090 - 00:25:39.470, Speaker H: Yeah, I agree. We should probably update it. There have been a lot of changes. So on another note, I've gone through Francesco's post. Among all the other clients is the content us that we should go with subnet Das and leave peer sampling as something to do in the future. Would it make implementation easier for you guys?
00:25:46.660 - 00:26:11.360, Speaker E: Yeah, I think sampling is sort of what I see is there sort of an independent module that can be added and once we have a working Devnet we can sort of increase the complexity by adding the sampling so that the pairing is stronger and we can have a stable devnet. So as of now I think I'm good with adding sampling later on.
00:26:15.670 - 00:26:34.490, Speaker C: Yeah, I also agree as well. I think sampling, I think a few clients already have sampling implemented, but I think there are quite a lot of edge cases that we need to think about and also handle. So I think if we exclude sampling that's going to reduce the complexity quite a lot and make it be easier to make progress.
00:26:42.000 - 00:26:52.260, Speaker H: So like when you guys mention on moving sampling, do you just mean for the Devnet or do you mean in general for pure dos as it is?
00:27:00.640 - 00:27:09.380, Speaker E: In my opinion, I think we can decide about sampling and sort of later parts of the devnet where we think that things are stable enough.
00:27:15.600 - 00:27:16.740, Speaker H: That makes sense.
00:27:26.920 - 00:28:05.420, Speaker C: Sorry. I think Francesco's comments was also like saying that like if we can tweak the network parameters of something that does, then sampling doesn't really add like more security I think that was the argument that we could potentially exclude sampling. I think there's some trade off there so it might be worth having a look at the post. I haven't actually finished reading it so I can't really comment too much about it. But it seems like Projesska is pretty confident that we've done ship sampling. The security will be okay. So shall I?
00:28:07.680 - 00:29:43.950, Speaker B: No, I just want to wrap the conversation. So is that the rough convincer that the clients can keep working on the networking issues and the refactoring and at the same time that just call the spec fellows will try to refine the sampling logic that what our next to do in the next one or two weeks. Can you give me a sum up? Any objection? Thank you. And then any other means so we can talk about the sampling logic in the correlator. But just before that I want to mention that we will have spec release today or tomorrow. The release notes is here. So mainly the regular release that I don't think it would be the death name logic but this release will include some KDG updates and I just want to make sure that it won't influence that what you are working on lately.
00:29:43.950 - 00:30:09.830, Speaker B: Yep. And is there any like urgent prs that we should include it or. Okay, and I suppose the next. Or I mean the next release or next next release will. We will try to focus on appear desk definate.
00:30:17.850 - 00:30:25.470, Speaker G: I think. I think it would be helpful if metadata v three is like reviewed and merged.
00:30:30.250 - 00:31:02.142, Speaker B: You mean the diff between the release. Give me 1 second. Oh metaverse. Okay. This one. Okay. Included after this code.
00:31:02.142 - 00:31:15.846, Speaker B: Thank you. And any other updates? Sorry, any other request please let me.
00:31:15.878 - 00:31:44.960, Speaker A: Know in like this is Francesco's fork choice printhead which is not getting merged and should not get immediately merged. But it's likely going to get in the next release and it does change a few things but it's not for this release. But it seems like the next spec release is also going to be important because it's going to include the sampling stuff and the fork choice stuff.
00:31:52.190 - 00:31:53.610, Speaker B: Yep, agree.
00:32:05.230 - 00:32:17.410, Speaker G: I have a. I have a query. If we are planning to drop sampling in the consecutive devnets then in the next release why would we have sampling merged?
00:32:23.610 - 00:32:32.910, Speaker A: I don't think it has been fully decided whether we're going to drop something in the next Devnet. Has it been decided? I'm sorry if it has, I have missed it.
00:32:34.690 - 00:32:37.386, Speaker G: Okay okay okay okay I get it. Thanks.
00:32:37.498 - 00:32:51.540, Speaker A: But I do actually think that we should very soon decide whether the next Devnet should have sampling or nothing. I think that is something we should do very soon.
00:32:55.800 - 00:32:57.220, Speaker G: Yeah, sounds good.
00:33:00.320 - 00:33:20.860, Speaker E: I think we already decided that next Devnet will not have sampling. In fact, next few devnets will not have sampling. But later on when we have stable devnets, then we can decide to add on sampling. But that was sort of my import of the discussion that at least for the next few devnets we will not have sampling.
00:33:21.960 - 00:33:23.900, Speaker A: Okay, thanks. I had missed.
00:33:27.280 - 00:33:39.780, Speaker H: So I think clients, you know, who have already implemented sampling can continue doing so, but it shouldn't be a requirement for any of the client who's having issues to actually have sampling working.
00:33:42.110 - 00:33:50.406, Speaker E: So they can be the only difference that they should not disconnect up here if sampling fails, I guess because.
00:33:50.518 - 00:34:08.200, Speaker H: Yeah, so like for, like for us, what we do is that we try to sample something and we can't sample it, we just log it out. It doesn't really do anything, but it does test, you know, that our incremental das parts of actually work and stuff like that.
00:34:24.740 - 00:34:57.590, Speaker C: Just a question regarding the next devnet launch. Does it work? Is it work? Creating a hack MD pay for the devnet two spec? And I know once metadata v three implemented, I don't think we want to include any further changes, but I think it would be good to include things like making sure that every client serves the by route and by range request for launchdevneta.
00:35:05.320 - 00:35:27.550, Speaker B: Dropped one immediately. Yes. Um, so I think Alpha Four will include the meta metadata and some kg updates.
00:35:31.890 - 00:35:34.590, Speaker A: I think we should aim for all four then.
00:35:42.210 - 00:35:49.240, Speaker B: Can the client squeeze eight? That's okay.
00:35:50.700 - 00:36:08.600, Speaker G: I think if we are targeting alpha four, I guess we at least need some more time for the devnet. Like if, if we, if we agree that the devnet to is or the next devnet is compliant with alpha four.
00:36:12.860 - 00:36:13.212, Speaker H: Yeah.
00:36:13.236 - 00:36:16.350, Speaker G: Yeah. Okay. Okay then that's, that's okay.
00:36:19.250 - 00:36:26.830, Speaker A: Yeah. I'm not sure when op four could be merged in theory, but I don't expect it to be happening right away.
00:36:27.770 - 00:37:06.440, Speaker B: Mom. I plan to merge it in 24 hours, but of course without folk choice for this currency does and then the metadata APIs. Okay. Okay. Do you have any other open discussion?
00:37:15.850 - 00:37:41.714, Speaker C: Yeah, just wondering if there's any progress with the custody group chain proposed by Pop. I think it's called decouple, the networking subnet with Das core. I think that's going to be a bit of code change, so I kind of want to know if that's something we're going to go ahead or not. He's not on the call, so you are.
00:37:41.762 - 00:37:42.430, Speaker I: Sorry.
00:37:48.930 - 00:38:37.246, Speaker B: Pop. If you are talking. We can't hear you still no, it so pop the message in the chat that. More reviews?
00:38:37.358 - 00:38:54.908, Speaker I: Yes, I think. I think I want to go ahead but yeah, but I think I want more people to look at it first. I think there are not many people to look at it currently but I think I. Yeah, I want to go.
00:38:54.924 - 00:38:55.640, Speaker C: In it.
00:39:00.140 - 00:39:01.080, Speaker I: So good.
00:39:30.470 - 00:39:40.890, Speaker C: It feels like most people that comment on the PR generally agree to it but there's just not been like formal approval for actions yet.
00:39:49.640 - 00:40:03.220, Speaker H: Yeah, the PR looks good to me. It'll take us some time to get the changes done but if no clients against this, I think maybe we can target this for Alpha five.
00:40:14.290 - 00:40:54.210, Speaker B: I think we got good support today and so yeah, alpha five could include we can target on decouple PR and also the of choice and how to deal with simply it's the three main course for the next release. Okay. And then any other open discussion.
00:40:59.670 - 00:41:52.780, Speaker A: Francesco is gonna join the call in five minutes he says. And he also told me about the data availability checks should happen on the current block, not the parent. So if the spec is ambiguous in this regard, we should probably make it more clear. That said, I mainly joined the call to ask if anyone has questions or about the KSG libraries or if people have. I heard that the lighthouses has been using Kev's rust library, which is nice if people have questions or transition questions or anything.
00:42:06.810 - 00:42:16.750, Speaker C: Yeah, no questions from us at this stage. I've run into a few issues but Kev's fixed it pretty quickly and I think it's looking pretty stable for us now. Thanks.
00:42:31.900 - 00:42:39.400, Speaker B: Another open discussion while we're waiting for San Francisco.
00:43:13.830 - 00:43:20.330, Speaker A: I guess we don't need to wait for Francesco and we can sort of resolve this on discord or something.
00:43:22.210 - 00:43:22.950, Speaker B: Right?
00:43:24.570 - 00:44:00.560, Speaker C: Only two more minutes though. Yeah, I don't have any questions. Looking at Francesco's post, do we want to talk about the complexity about sampling? Does any client have any issues regarding having sampling implemented robustly?
00:44:19.310 - 00:44:31.810, Speaker A: I guess the opposite would also be interesting. Like having sampling implemented in not modular way such that disabling it is complicated. If we end up with disabling it.
00:45:01.120 - 00:46:06.940, Speaker B: Another open discussion. It. It was a long five minutes. Should we move this conversation to discord? Oh, okay. Well I have one open discussion. So on the agenda, I think someone mentioned that. For instance, I want to derive we should make this biweekly call to weekly.
00:46:06.940 - 00:46:28.670, Speaker B: That's the preference from you guys. Should we make it to weekly so that we could probably isolate the process. Any feedback, suggestions?
00:46:35.930 - 00:46:40.150, Speaker H: What's the benefit of doing this? Weekly for Francis.
00:46:42.170 - 00:46:55.000, Speaker B: I think mainly just more frequently updates, frequent Q and a frequent problem solved, I guess.
00:46:58.180 - 00:47:15.290, Speaker H: I think Francis issue is that he's based out of western us, so he can't attend these calls at all. Even if we do this weekly at this time, he would still face the same issues.
00:47:21.630 - 00:47:38.850, Speaker B: Right. So it's two questions. One is to make it more frequent and then second question is to change the US, move it to a more us friendly time.
00:47:44.150 - 00:47:57.690, Speaker H: From experience, moving into us friendly time would make it like terrible for people in APAC because it would be easily, I think, ten to twelve at night.
00:48:01.790 - 00:48:31.570, Speaker B: In 8th if this meant a little drum, a drum voice to learn. Francesco, hi.
00:48:32.190 - 00:48:32.970, Speaker I: Hello.
00:48:36.150 - 00:48:41.890, Speaker B: All right. Would you like to talk about your new document?
00:48:43.910 - 00:49:47.662, Speaker I: Yeah, sure. I mean, I guess George said there were some virtual questions which didn't seem super related to the document. So if people want to start with that, you can talk about that. Otherwise. Okay then. Yeah, yeah, basically, I mean the document is just sort of a bit expanding on what Alex mentioned at last, ACDC or whatever it was basically saying that there at least is an avenue to further simplify peer das to where we really just for the moment not worry about implementing peer sampling and just leave that as something to then do later. And this can mean just that it can end up just being like implementation and testing cycle kind of thing where maybe in a few months then we are in a place where we can put peer sampling on something that already exists and is working well.
00:49:47.662 - 00:51:27.496, Speaker I: Or it can mean that in a first version, in the first rollout really there is no peer sampling and then is just added later at some later point. And yeah, I mean, there's not that much more to that, I guess. More, I mean the document is just basically just trying to convince people why this is not a crazy idea and why it's actually kind of secure and, yeah, like is sort of okay to do. And I guess, yeah, some little point that I try to make both there and on discord is that this is not some last minute thing. I mean, at least some of us had been thinking about this from even at the end of last year as a potential option, even though it's not what kind of was pursued for the beginning because it seemed like maybe we can make the, let's say a fuller version of peer dots work and maybe that's still the case. But yeah, this is something that already, I mean the whole subnetas concept was basically this essentially like, essentially like an intermediate stepping stone where the sampling part is really just what is done through distribution, like what? Basically custody, like just getting columns through subnets rather than having this separate peer sampling mechanism. So it's really just about avoiding the complexities of peer sampling, which is all of the things around timing and figuring out.
00:51:27.496 - 00:52:14.990, Speaker I: When do you timeout? When do you send parallel queries? Do you send parallel queries? How many do you send? Just all of this DOS protection. All these things that really are tricky about sampling strategies. And I feel like there's probably room for that to be a bit more of a, you know, research question or at least like something between like research and implementation, as opposed to something that's just immediately shipping. So that's basically it. I guess I would, yeah. Love to hear from people if you actually think that this is helpful to you, like if you think this is actually is a meaningful simplification or if you don't think that it actually makes much of a difference. And getting sampling to work.
00:52:14.990 - 00:52:20.030, Speaker I: Yeah. Just doesn't seem to be like much of an impediment at this point.
00:52:31.650 - 00:53:01.070, Speaker H: Yeah, I think not having sampling would, uh, simplify it for, I think, pretty much all clients. But I'm worried about, uh, if we go into the fork, which is, you know, essentially subnet Das, adding in pure sampling later, you know, after subnet Das is live, will actually be a lot more harder rather than, you know, we're thinking that we can just tack it on. But from what I see, um, without. You can't take it on without a hard phone.
00:53:02.810 - 00:53:30.990, Speaker I: I'm curious why you think that. Because it kind of seems to me that as long as you have all clients at some point being able to respond to sampling queries, then actually enabling the sending of sampling queries can basically be rolled out in whatever way anyone can individually start doing that without too much of a problem. But yeah, I'm curious, what's your perspective there?
00:53:31.770 - 00:54:05.918, Speaker H: Yeah, so like, you know, we started something dust. You would have a certain amount of custody requirement, and if you want to gradually introduce peer sampling, you would drop that custody requirement. But your peers around you, they do not know that you have actually dropped it. There's no way to indicate that because from this is like set, you know, in the config parameter that every peer would custody. These are my subnets, and therefore, you know, you would have all these columns. Right. And you would participate in gossip for these subnets.
00:54:05.918 - 00:54:15.770, Speaker H: But if you, let's say, drop the custody requirement and then you sample for these other columns, the peers around you don't know that you've actually started doing that.
00:54:18.280 - 00:54:51.140, Speaker I: Well, like what we could do, for example, is just to introduce peer sampling at first but still keep the, you know, just as a. Basically where it doesn't actually do anything. It's just something you do kind of has as it is now in clients, as I understand, and then at some point you drop the custody requirement and. Yeah, I guess, right. That might need, yeah, that might need a coordination point, I guess. But I guess, is that too much of a problem?
00:54:53.640 - 00:54:58.720, Speaker H: I mean if it needs a coordination point, it need a fork of some sort.
00:54:58.760 - 00:54:59.008, Speaker B: Right.
00:54:59.064 - 00:54:59.660, Speaker H: So.
00:55:00.960 - 00:56:00.924, Speaker I: Right, that's fair. I mean, that also, I guess, right. That's a bit of a complication, but also doesn't seem the end of the world to me just because, I mean, at the end of the day, it's like, I think the way I think about at least most of the reason then to introduce peer sampling is just to possibly get a bit more scale. So it seems like the kind of thing that can just wait for a time where we can have such a coordination point. It's not something, I mean, at least the way I see it, it's not something where, you know, that we need to, like how can I say there's no like emergency where like this need to be added for the system to be complete and work. I mean, otherwise we wouldn't, you know, we wouldn't ship a thing that doesn't work by itself. So at least the way I see it, it would be okay then to wait for another fork orientation point or whatever.
00:56:00.924 - 00:56:05.650, Speaker I: But, yeah. Happy to hear other opinions.
00:56:11.230 - 00:56:32.170, Speaker H: Yeah, I mean, like, I guess from implementation point of view, it's, it makes our job a lot easier, but it is kind of like kick the can down the road. We got to think about the next fork. If we were, you know, to add peer sampling or nothing, or would we just be okay with the end state being subnet tasks?
00:56:34.590 - 00:57:25.310, Speaker I: I think the end state, no, but it's like, it can stay like that for, I think, a while. I don't think there's any problem with having to wait for another fork to then add peer sampling. I think basically, I guess Ansgar has been trying to beam into reality this thing of three x blob count increase every year. I don't know, that might be ambitious, but I think this thing would be in line with that. You can probably get two, three x whatever increase just by doing this initial version. And then adding peer sampling later is just a way to get another, uh, extra kind of throughput increase, basically.
00:57:28.330 - 00:57:40.150, Speaker H: Like do we have like numbers on like how much throughput increase. That would be like, you know, let's say if we drop the custody count by how much we're planning to increase right now with peer sampling.
00:57:42.450 - 00:58:48.278, Speaker I: So, like, for example, if we right now set the custody requirement, let's say to, I don't know, let's say twelve, just to split the middle between eight and 16, which are sort of, I guess, two of the reasonable numbers that people might want, then more or less you download. I mean, it's basically about like, it's like about a five x kind of efficiency increase compared to 444. And. Yeah, so that's basically the room you have to increase throughput, then we might not want to increase it by five x, but we do have in principle up to that much room. So, yeah, I think it's not a huge constraint. I don't think there's any world in which we ten x throughput or like 20 x throughput in a year or something like that. I think we all want to be cautious regardless of what these measures are like in theory.
00:58:48.278 - 00:59:16.570, Speaker I: So it feels to me that if we have a five x ability to in principle increase it by five x, maybe we can increase it by two or three x and then still have room to move forward in a later fork. And then, I mean, yeah, adding peer sampling, then it gives even a little bit, a little bit more room. I mean, I can tell the exact numbers, but it's, yeah, I guess, I don't know how much. It's super important, but.
00:59:19.150 - 00:59:21.570, Speaker H: No, that's fair enough. Makes sense.
00:59:44.960 - 00:59:48.020, Speaker B: Any other question, discussion on the.
00:59:49.560 - 01:00:52.910, Speaker I: Yeah, sorry. Just one, one last thing about this. I guess I think maybe one other way to look at it really is to just say like, we don't know at this point, you know, we, what will be, I mean, we don't know what the timelines look like. We don't know what we might be able to have done in however many months, unspecified number of months until the fork. So one option is that we just start with this, just take it as a first step of implementation and testing and really just try to get a that done and then only then focus on peer sampling. And I mean, it's possible that we end up feeling like, oh, it really doesn't take too much more work to do peer sampling and we can possibly like delay the rollout and just do the whole thing, or that we see that actually a peer sampling really doesn't need more work. Why not just go with the initial thing, which already gives us a throughput increase? Kind of my thinking is at least that.
01:00:52.910 - 01:01:23.460, Speaker I: Like it feels a lot less risky. Like it feels like that at least by proceeding in this order, like first trying to really get distribution and reconstruction done, like basically doing the subnet task part of the whole thing and then focusing on sampling, then it's much more likely that something will like they will be able to ship something and get some throughput increase and yeah, and possibly we can get the whole thing done but just we don't have to have that uncertainty essentially.
01:01:42.930 - 01:03:01.422, Speaker B: Thank you, Francesco. Any other open discussion? So back to the cold meeting time. Does anyone feel strong that we should change the time frequent the period or the time? If not then, then the next week will be in two weeks on the 20th centimeter. Okay. Any other closing notes before we say bye bye to each other? No. Okay. Thank you.
01:03:01.422 - 01:03:05.090, Speaker B: Thank you for attending. Have a nice day.
01:03:05.950 - 01:03:06.702, Speaker D: Thank you.
01:03:06.806 - 01:03:07.610, Speaker B: Bye bye.
01:03:07.910 - 01:03:08.790, Speaker I: Thank you.
01:03:08.950 - 01:03:09.850, Speaker G: Bye bye.
