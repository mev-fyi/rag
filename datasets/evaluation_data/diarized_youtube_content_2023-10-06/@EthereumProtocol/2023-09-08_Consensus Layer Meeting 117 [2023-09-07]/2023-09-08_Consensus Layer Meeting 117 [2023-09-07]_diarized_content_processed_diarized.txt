00:07:19.550 - 00:08:04.842, Speaker A: Welcome to the consensus layer, call number 117. This is issue eight five four in the PM repo focus today, obviously, I guess not obviously, but focus day will be on continued on Daneb. We'll talk about testnets. I do want to bring up the testing plan that was brought up a couple of weeks ago and make sure that we've put some eyes on it and that we know who's going to be working on the CLP to P specific stuff. Because I know it's kind of a high urgency, high concern problem for a lot of teams. Then we have promised to revisit the four eight four main net parameterization. So Codex is here.
00:08:04.842 - 00:08:59.622, Speaker A: They've been looking at some big main net blocks, so we can dig into that. And then if there's any other inputs or thoughts on the two four versus three six parameterization, we can discuss, I have attempt to form consensus on main net parameters. I actually don't know what the temperature gauge is going to be here, so I don't know if that's something we can do today or something we can reopen the conversation for in the future. And then Eton has a stable container proposal, which is, I think, an update to EIP 7495 that we can take a look at. Let's get started. Devnets, are there any progress or discussion points worth bringing up today on our public call? Everything is healthy.
00:08:59.686 - 00:09:20.610, Speaker B: We have 100% plot proposals, and all the nodes are up to the head. And I've just deposited some more validators for Eragon. So we're going to be able to check if Aragon is also able to make block proposals. And we have indeed all the clients.
00:09:22.870 - 00:09:23.890, Speaker A: Very exciting.
00:09:25.430 - 00:09:54.042, Speaker B: Wasn't there some clients that had some issues that kind of fixed themselves at some point? Yeah, but everything has been fixed as of last night or this morning. Do you know why they didn't want to sync in the first place? If anyone from prison here, maybe they can.
00:09:54.096 - 00:09:55.610, Speaker A: Yeah, Terrence has handed.
00:09:56.450 - 00:10:30.710, Speaker C: Yeah, I can share some lesson learned. So why we did not see this issue before? So we found a bunch of issues with regard to saving the blobside car per the retention window. So it's funny, I'm happy we got to test this for more than 18 days. So, basically, the saving of the blob sitecar gets really messed up in the index after the retention window. So the blobs are getting overwritten or deleted. So that's why we're having trouble syncing. So that's the first issue.
00:10:30.710 - 00:11:01.966, Speaker C: The second issue is that we were using the wrong blob subnetcon. So remember, we moved from four to six, but we did not update the parameter. That's the blob subnetcon. So we're subtracting the four subnets this whole time. So we also fixed that. And then the last issue is that we were finally incorrectly hashing the hashing root of the block that's inside the blob sitecar field. So we were hashing the block without the state root before putting the state root in the block.
00:11:01.966 - 00:11:13.160, Speaker C: So basically all our sidecar had to run block roots inside. So, yeah, hooray to testing. I'm glad we figured out most of those issues within 24 hours.
00:11:16.170 - 00:11:36.414, Speaker A: Nice. Any other updates or comments related to Devnet eight? Are we transaction spamming real quick and blob spamming? I assume so, yes.
00:11:36.532 - 00:11:38.926, Speaker B: Not fuzzing, just spamming for now.
00:11:39.028 - 00:11:40.980, Speaker A: Got you. Sorry, Justin. Go on.
00:11:41.430 - 00:11:56.454, Speaker C: Yeah, real quick. We're investigating a potential issue with basically proposing empty blocks, but only with lighthouse, which is weird. So lighthouse folks expect me or someone from my team to reach out.
00:11:56.492 - 00:11:57.080, Speaker A: Thanks.
00:11:58.250 - 00:12:17.870, Speaker C: So Michael's actually been looking into this on our team, and I think he's gotten to the bottom of it. We just haven't merged the fix for it yet. We were doing multiple fork choice updated like quickly in a row on proposal, and one of them had the incorrect parent beacon block route.
00:12:19.970 - 00:12:20.478, Speaker A: Awesome.
00:12:20.564 - 00:12:21.360, Speaker C: Thank you.
00:12:25.920 - 00:12:59.160, Speaker A: Cool. Other Devnet eight discussion point. Okay. I believe where we left it at the previous call was not having a timeline for Devnet nine. I got murmurs that that was changing. Does anybody want to discuss Devnet nine?
00:13:00.970 - 00:13:09.306, Speaker B: Yeah, we said, I think on the Monday call that Devnet nine should launch on Tuesday next week. That's the plan.
00:13:09.488 - 00:13:18.160, Speaker A: Okay, cool. Yeah. Any other things to share or discuss around that while we're here?
00:13:21.040 - 00:13:45.540, Speaker C: I would like some more time to update the hive tests for the execution clients. I'm not sure if we're going to manage to update everything for Tuesday. Is everybody ready? Are all clients ready for Tuesday?
00:13:53.110 - 00:13:57.460, Speaker D: I think the only change is how we're deploying the transaction, right?
00:14:01.470 - 00:14:18.670, Speaker C: Yeah, but we are also including more tests on the transit storage. We are also including more tests on the beacon root contract. And I'm including more tests on the Hive engine API simulator.
00:14:19.490 - 00:14:21.538, Speaker A: So it's all additional coverage, right?
00:14:21.624 - 00:14:29.250, Speaker C: Yeah, it's additional coverage, but it would be nice to have it before the launch.
00:14:39.030 - 00:14:48.840, Speaker B: I guess we can make a couple of extra days, but we're going to be quite busy. So if we don't do it on Tuesday, I think we will have to push it at least another whole week.
00:14:49.930 - 00:14:58.060, Speaker D: I think that's fine, considering the only change is really the transaction. And if we want to be more chaotic on Devnet eight, we can be next week.
00:14:58.670 - 00:15:15.140, Speaker C: Yeah, I agree. Okay, so it's only extra coverage from higher side. So if we find anything, even after the Devnet nine has launched, we can just report it. And I don't think we are going to find anything that breaks Devnet nine, so it's okay.
00:15:17.190 - 00:15:21.780, Speaker A: Perry, were you saying the opposite and saying we could wait a week and just spam eight more?
00:15:22.630 - 00:15:39.580, Speaker D: Yeah, I think I'd prefer just waiting a week and then having everything that Hive can catch get caught by hive and then just focus on causing more chaos on Devnet eight next week. Considering that Devnet nine is pretty much just Devnet eight, but with one change.
00:15:41.950 - 00:15:43.770, Speaker B: And builders hopefully.
00:15:45.470 - 00:15:54.560, Speaker D: Yeah, but they haven't necessarily been working on local testnets yet, so another week for builders doesn't seem too bad either.
00:15:57.810 - 00:16:06.420, Speaker A: That's the direction I would lean in. But I wasn't a part of the call on Monday that made the decision to keep it moving quick.
00:16:11.150 - 00:16:30.240, Speaker B: We still have one pr open, three nine eight that I've been checking on. Whether we could merge that the transaction and receipts. Is there any holdback for this or can we just get it merged and be done with it?
00:16:32.310 - 00:16:34.260, Speaker A: I mean, we need tests for it.
00:16:38.090 - 00:16:43.000, Speaker D: And are those a Blocker submerging the PR?
00:16:43.690 - 00:17:02.920, Speaker A: Yeah, this is in the Eth API, not the engine API, right? Yes. Okay.
00:17:08.770 - 00:17:16.260, Speaker D: Anyone PF is listening. This is an easy project with high immediate value.
00:17:18.950 - 00:17:36.200, Speaker A: Is anyone strongly opposed to Devnet nine being at plus one week from Tuesday rather than Tuesday? We can hopefully merge this pr and get the hype test up. We can do more chaos on Devnet eight and something else in there.
00:17:36.810 - 00:17:46.060, Speaker D: Just one more thing for Devnet nine. We'll be doing capella genesis. As far as I know, Pannabus has already tested and it works on all clients, but just still wanted to mention that.
00:17:47.630 - 00:17:53.120, Speaker B: Yeah, I haven't tested rest just yet, but I think it should work.
00:17:57.530 - 00:18:50.830, Speaker A: Okay, Devnet nine at plus one week from Tuesday. Sounds good. Cool. Anything else on devnets? Great. A couple of weeks ago, Perry and team shared this document. Just on testing overview. There are some things in here related to cl testing, so probably things around network partitions or long running forks and resolving those in the context of being able to get blobs or not if they're passed to print depth.
00:18:50.830 - 00:19:37.550, Speaker A: Delaying blobs, delaying blocks, and not blobs. Like that kind of stuff. I think a number and just maybe some think tests in the context of blobs based on conversations with synchronous layer teams. Those are some of these exceptional code paths related to the new PTP stuff around blobs are definitely some of the high priority concerns and items. I just wanted to bring up the testing plan again and make this. A couple of weeks ago we talked about this. Are our bases covered in terms of the tools and the people that are working on these things, or do we need to make a more concerted effort in the next few weeks to tackle some of those items I just discussed?
00:19:41.950 - 00:20:12.200, Speaker D: Just give some updates on this one. So, sync testing. It looks like the Nethermand team has been running some sync tests themselves. I'll let them expand on that if someone from Nethermand is here, but you might have seen some results of that on the interop channel. We still haven't gotten around to updating our own sync test suite, so we're relying on the Netherland team for that. Chaos testing is still a work in progress. We're hoping to get that done by the end of the month.
00:20:12.200 - 00:21:02.680, Speaker D: We're kind of a bit held up because this protocol bug soon and we're trying to get some things prepared before that and Mev related tests. There was an issue with MacBooks not being able to handle capella genesis. And as far as I know, since as of yesterday that's been fixed and they're actively getting ready to test on local nets. And I still wanted to ask if Cl side have had a chance to try it with mock mev mode on kertosis. Because if that's working as well, then Devnet nine, we can already test the entire mev workflow. But yeah, besides that, we're having a call with Mario after this kind of. If someone's interested, feel free to join that one.
00:21:04.410 - 00:21:20.540, Speaker A: Great. And if anybody on consensus layer teams did have a chance to look at this testing plan since it was first introduced, are our bases covered? Are there glaring omissions that we need to make sure to shore up over the next handful of weeks?
00:21:32.490 - 00:21:55.280, Speaker C: Okay, so I'm just like briefly looking at it now, but do we want to deal with scenarios where the finalized checkpoint is further in the past than blob expiration and how do we recover from that? That would require checkpoint sync from an unfinalized checkpoint, I think.
00:21:56.050 - 00:22:48.960, Speaker A: Yes. I guess there's two things there. One is having a fork that goes beyond that depth. I guess there's a number of things you can have a fork that goes beyond that depth and actually show that you're not going to automatically resolve the fork because you can't do DA and fork meaning a partition because you could have done DA on the other fork even if you weren't seeing it as your head. And then there's the resolving from those problems. I think that it's probably worth throwing what we can at it and these scenarios are pretty exceptional and worth testing. Do you think does like Lighthouse, et cetera, work from starting from a non finalized checkpoint from usageivity or is that just not something that's been coded up before?
00:22:50.770 - 00:23:00.180, Speaker C: I think it does, but it would be nice to test, especially if it's like it would be a requirement to recover from a scenario like this.
00:23:06.200 - 00:23:22.884, Speaker B: Devnotate is already 20 days old or 22 days old. So it should have some expired blobs and it only has a single archive node that should have all the blob data in there. So we should be able to do all these kind of tests on devotate.
00:23:22.932 - 00:23:36.844, Speaker A: I think. Yeah. One of the things though is to make the network not finalized for the depth of the pruning period. So in the order of weeks, unless we change the parameterization of a test.
00:23:36.882 - 00:23:47.890, Speaker D: Net, is there actually any objection to change that for depth at nine? Because we could do this test there and then keep the period as like one day or something.
00:23:52.830 - 00:24:20.820, Speaker A: Yeah, it's probably worthwhile having a much shorter prune depth because I think you can get a bit more chaos. It certainly changes the load requirement from a certain perspective because the amount of data that blobs that are going to be stored locally for each one becomes much, much shorter. But I do think that if not Devnet nine, some sort of lakeside CHS net seems really valuable to put it on a pretty short prune depth because I think things are going to fall out.
00:24:25.500 - 00:24:29.400, Speaker B: But do you mean the ejection balance should not be fixing?
00:24:29.820 - 00:24:50.810, Speaker A: No, the pruning period. So it's like three weeks parameterized, but you could make it half a day or a day. And you can do a lot more chaotic things around that threshold if you do. So does that make sense.
00:24:55.290 - 00:25:11.820, Speaker D: About once we're happy with some base level of stability, we can just start like a line of testnets with attack nets called attack nets. Leave devnets alone so people can do that tooling and stable testing and then we can have all the chaotic testing on an attack net.
00:25:12.670 - 00:25:53.940, Speaker A: Yeah, I think I agree, especially in the context of like if we want Devnet nine to look and feel like the thing that we're going to launch. It should probably look and feel like the thing that we're going to launch. Okay, cool. There's a testing call Mondays. I think it was pretty low attended on the consensus layer side this past Monday. I think it's every other Monday. And I think as we are moving towards ironing out the final kinks, it's definitely worthwhile having some more attendance there.
00:25:53.940 - 00:26:24.760, Speaker A: What is the printing variable? It's in the PDP spec on Denev. I can pull it up. Men epics for blob sidecar requests and I'll send the spec. But it's with him.
00:26:26.410 - 00:26:32.300, Speaker B: Right. So that's 4096 epochs. That's the default, which is 18 days.
00:26:32.750 - 00:26:48.240, Speaker A: Yeah. So 128 or 256. Half a day or a day. It probably gets weird if you make it really short, but might be worth playing with too.
00:26:49.330 - 00:26:50.080, Speaker B: Okay.
00:26:57.300 - 00:27:33.578, Speaker A: All right, cool. Anything else on testing conversation for today? Great. Thank you. So, as I said at the open, even if it proves tough to make decisions today, it's time to reopen the conversation around the main net parameterization for the blob data. Gas limit. I think maybe that name was changed recently, but blob gas limit, essentially the gas limit. So Codex is going to kick it off.
00:27:33.578 - 00:27:40.750, Speaker A: They've been taking a look at big mainnet blocks and the information we can parse out of that codex.
00:27:42.610 - 00:27:54.430, Speaker B: Thank you. Me try to share screen. Okay, so.
00:27:56.800 - 00:28:02.880, Speaker A: Yes, we can hear and see you still see the slides, see your screen. That's what I meant.
00:28:03.700 - 00:28:19.688, Speaker B: Do you still see the. Ok, cool. Yeah. So let me just. Quick background. So I'm Chaba Chabakilai from the Codex team. We are mainly working on decentralized storage, but we are also working with the Nimbus team on lift p to p.
00:28:19.688 - 00:28:55.280, Speaker B: And we are working with EF as a grant on data warehouse sampling. And what we show here is kind of a side product of that work. So as part of the data warranty sampling, we were looking into gossip sub performance, and especially to understand how it works now onto the network with big blocks and with small messages. We will not show that now. And then we also had some nice findings about big blocks now on the network. So let's just look into this. How big blocks work now on the mainnet.
00:28:55.280 - 00:29:30.510, Speaker B: So that was the big block experiment by the foundation. I think you're all aware of this. So there was a capacity testing of the mainnet in which large transactions were submitted, which then made it into large blocks, which then got finalized. So here you see one example. One of these blocks that was 1 mb in size and it was nicely finalized. So this would be a nice result for the experiments. Nice big blocks can be finalized, but the experiment was about much more.
00:29:30.510 - 00:30:00.436, Speaker B: Besides sending these big transactions which became into big blocks, DF was also setting up the sentinels. So I don't know how much you will. These sentinels were set up in three geographic locations. In each location, each CL client was running. Basically depending on the time period. A few of instances were running in these locations and then metrics were collected. Well, client implementation.
00:30:00.436 - 00:31:02.584, Speaker B: So here you see the dashboard which was shared previously, in which you see delays in seconds and some distributions of these. And here you see the below the line doted time instances when blocks were big. And we wanted to get a bit more data out of this. So we were looking into details and this is what we got. So if you look into the internal latency data and you look into the blocks, and then you categorize blocks according to block size, then you can basically plot how different block sizes are getting diffused in the network. Now this is not one block, one block diffused in the network, because for each block we have only a few measurements. But if we take a large number of blocks between 64 and 120 kb, for example, and we have measurements on all of these, then from these we can plot a CDF.
00:31:02.584 - 00:32:00.190, Speaker B: So what you see here as an orange curve, for example, is a large number of blocks vital with that size and latency measurements on these delay means the latency compared to the slot style. So when client implementation was saying, I have received this block, and what you see here is what we expect. So the bigger the block, the bigger the delay. The point that you see here that highlighted is 6428 kilobyte block. 80% of blocks arrives before that time, which is kind of 2 seconds, a little bit less than 2 seconds. So this is very nice, we've seen things are arriving, we've seen the increasing delay. We wanted also to quantify this delay, but we were running into a few problems and the promise was that looking at different client implementations, we were seeing different numbers.
00:32:00.190 - 00:32:36.884, Speaker B: So the figure that I was showing you was replaced. But then if we look into the different clients, and here the client means the client receiving the block, so not the client which was originally putting the block on the network. We don't know that this is the client which was receiving a block. And we see these kind of differences. So as you can see, there are differences. And we already know that these differences are partly due to semantics. So the meaning of the timestamp that we have as a latency is different in different clients.
00:32:36.884 - 00:33:09.104, Speaker B: We know that there are some clients, I think lism is one of these, which are exposing the timestamp when they receive the message on the gossip sub. Others are doing all the processing that they have to do and then they are exposing the timestamps. I don't know. This is just a guess. Others might have low priority reporting thread, so they might just have a timestamp, which is very different. So we don't know the things behind, but we have different bursts for different clients. We were also looking into attestation latencies.
00:33:09.104 - 00:33:25.850, Speaker B: I'm not showing that now the facilities that these are very small messages, but we have lots of them. So we have much more data to plot from. This is what you have seen with the big blocks. And then looking into this, we had also some nice things which. Lao, will.
00:33:29.420 - 00:33:30.410, Speaker E: You hear me?
00:33:30.860 - 00:33:31.664, Speaker B: Yep.
00:33:31.812 - 00:33:32.924, Speaker A: Okay. Yes.
00:33:32.962 - 00:34:31.240, Speaker E: So looking into this, at some point, we started looking into the data sets that we have with the group amigalabs, and we discovered that we observe also other big blocks present in the network. Can you pass to the next slide? And we noticed that these were out of the period where we were doing this experiment of injecting large transactions to make large blocks in purpose. And so we noticed that there were organic blocks that were really big in Mainnet. And so we did an study on this. So here we collected data over the last six months, so from March to the end of August, and we found that we have over 100,000 blocks that are larger than 200 kb. Knowing that the average block size in Mainnet is about 100 kb, so this is about 1.3 million slots.
00:34:31.240 - 00:34:54.628, Speaker E: And so 109,000 is about 8% of the blocks have these kind of big sizes. The biggest block was in these six months of period was sent about 15 days ago, on August 22. You have the slot on the screen and the size is about 2.3 megabytes, which is really big. I mean, it's surprising. At the beginning, we didn't know that.
00:34:54.634 - 00:34:57.220, Speaker A: We could have such big blocks.
00:34:57.640 - 00:35:42.768, Speaker E: Next slide we can see here. Just to give you an idea of the distribution of the v blocks, this is all for blocks that are over 250 kb here. We don't see really well the distribution on the right. So if we pass to the logarithmic scale, we can have a little bit more understanding of what we see on the site of large blocks. Next slide. What is interesting about these large blocks and the fact that we can find them organically in Mainnet, is that we can use them to compare. Anybody can basically use them to compare how block propagation happens for these kind of large blocks.
00:35:42.768 - 00:36:26.340, Speaker E: So in this figure, you see for example, that between blocks of 250 kb versus 2.5 megabytes, it takes about two more seconds, more or less in average. And so these kind of studies I think help a lot as we move toward dank sharding and very large blocks. And again, it would be nice to have some kind of homogeneity on how the different clients report this so that we see more clear distribution and we can compare results across clients. Yeah, just to finish, we have some resources here, so if you have questions.
00:36:26.410 - 00:36:28.500, Speaker A: We'D be happy to answer. Yeah, go ahead.
00:36:28.570 - 00:36:41.850, Speaker B: Just wanted to add that on this slide, the x axis is logarithmic scale. So that's why you'd see a different shape from the previous one. So you have the 1 second is the 1000, and then the 10 seconds is 10,000. And you see.
00:36:44.380 - 00:36:52.300, Speaker A: Don has a question. Is this the compressed or uncompressed size? Because the size shown on ether scanner, I believe is uncompressed.
00:36:52.640 - 00:37:11.840, Speaker B: Yeah, we have discussed this. This is the uncompressed size. So now we are collecting the complex sizes and we will plot it as a function of complex size, actually both metals. So complex size is metals in network, uncomplicated size, metals in the complexion and processing. So it's also interesting to see those differences.
00:37:13.320 - 00:37:14.070, Speaker A: Thanks.
00:37:16.880 - 00:37:29.010, Speaker B: Yeah, we were sharing these links in the issue description so you can go and check out. These are Jupyter notebooks, so play with them.
00:37:32.890 - 00:38:01.920, Speaker A: Great. This is awesome. Any other questions for the codecs, guys? Obviously I think one thing that we need to contextualize here is these are sent as single payloads versus the method of serving blobs via them. Being broken up into the different subnets is going to have a different impact on how they're propagated. But still, I think it's very valuable. Any questions.
00:38:14.170 - 00:38:39.470, Speaker B: In the meantime, just to from our side? The goal of this was partly to look into, also into how small messages will propagate. And for that we were looking into station latencies. We didn't put it in the slides, but that somehow we defined the two. We should also have an understanding of how breaking up things will work in future constructs.
00:38:39.890 - 00:38:56.390, Speaker E: There's a question about the description of the methodology used to collect items data. I think this is not described in the notebook links that they are in here in the slide, but we are writing right now a post and Brody is going to be ready in a couple of days describing all the methodology.
00:38:57.210 - 00:39:31.666, Speaker B: Yeah. In the first link, Brock says latency. At the beginning you have a link to the EF post about the nodes and the setup. So that answers part of the question. And then you have our methodology to derive these results. Maybe it's important to say that behind such a distribution curve heel, you actually have several random variables. At some point the block was put in the network, then the network was getting to all the nodes in which we are sampling in a few points.
00:39:31.666 - 00:39:48.520, Speaker B: And then this is an aggregate of many blocks of data collected from many blocks. For one specific block, this curve would be steeper. But when you are adding it up for several blocks, each one finding its path, then you are getting.
00:39:53.850 - 00:39:54.840, Speaker A: Got it.
00:39:55.290 - 00:40:09.340, Speaker B: Yeah. I think from our point of view, it would be really interesting to understand who is reporting how, and maybe agree on unifying this or just exposing more timestamps so that we have better data.
00:40:12.210 - 00:40:15.486, Speaker A: Right around the client disparities we're seeing here.
00:40:15.668 - 00:40:30.610, Speaker B: Yeah. Around these differences, which are kind of huge differences, but we already know from some of the teams that the differences, it's just we don't have the full picture of who's doing what in their reporting.
00:40:41.690 - 00:40:54.380, Speaker A: I guess. Since we're here, are clients immediately aware of the disparities here? Why? Maybe Lodestar reports a much longer number. Maybe it has to do with finishing up a database operation or something like that.
00:40:59.340 - 00:41:01.450, Speaker F: Yeah, we are aware. We'll look into it.
00:41:03.740 - 00:41:04.490, Speaker A: Cool.
00:41:05.260 - 00:41:21.740, Speaker B: Yeah, I'm not even sure the timestamp is coming from you or it's coming from the logging or the reporting. So that can be also a thing. For example, you are reporting on a reply, and then it's getting timestamped much later. So this would be good to notice.
00:41:35.530 - 00:41:46.620, Speaker A: Okay, thank you. Anything else on these experiments? I think the codex guys would be happy to take questions asynchronously as well.
00:41:47.310 - 00:41:48.940, Speaker B: Yes, absolutely.
00:41:50.990 - 00:42:22.020, Speaker A: Cool. Thank you. This is great. Do we have other inputs other than these large block experiments to take into consideration as we have the conversation about main net parameterization? Our devnets work. Our devnets are also toys compared to probably main net topology. Are there any comments about the devnets and 36 there?
00:42:27.740 - 00:42:42.830, Speaker C: So I guess it might be interesting on devnets, if we had blocks that took longer for the execution to happen, we could test how different execution times impact our block processing times.
00:42:47.010 - 00:42:59.060, Speaker A: Because this is going to impact propagation. Probably not because propagation happens before that, but it might impact local import times, which are relevant to attestations or what.
00:43:00.070 - 00:43:06.840, Speaker C: Yeah. Also block production, which would be like the first input to propagation, right?
00:43:07.930 - 00:44:20.740, Speaker A: Yeah, and I guess kind of to Eton's question, if you're receiving three, six blocks at 3 seconds and you have really long processing, you might actually not attest to them correctly. SEO does compound. Okay, I think there's probably not a lot of inputs that we have beyond our observing what we've just discussed, and maybe there's also not a lot of more updated thoughts on it. So conversation is a little late today. Then I guess what I'll say is we need to keep having this conversation over the next few weeks because the lack of conversation here doesn't make me feel like we can try to make a decision yet. Am I reading the room right?
00:44:24.220 - 00:45:12.580, Speaker G: Yeah, I could say that my feeling is just probably we haven't did already the tests that are needed, the cows that needs to be created to actually exercise those bad and difficult paths. The current has been used for bug fixing so far. This is my feeling. So I think the actual test on these things should start now that clients are kind of more reliable, and we can actually fire stable testnets and then start playing with these partitions and delays.
00:45:12.940 - 00:46:03.660, Speaker A: All right, so chaos and load from here can help inform the conversation. Okay, this is something, maybe we won't bring it up next week, but we bring it up in a couple of weeks to at least make sure that we're getting the data that we want. Anything else on poor parameterization? Anything else on Daneb? Okay, Tom, you can take it away with the 7495 updates. And looks like dapline threw on something last minute to talk about after.
00:46:04.750 - 00:47:16.640, Speaker H: Yeah, sure. So as part of this SSC transaction and receipts work, I started this a long time ago, maybe already half a year, I think. But the current approach, I tried to unify these unions and normalized approaches, and I created a construct called a stable container. Like it's an SSC container, serializes very similarly, but it can have optional fields and they get skipped during serialization, but they still consume space in the Merkel tree. So what this allows us to do is essentially to have something similar to a union where some objects have some of the optional field set and other objects have other field set. But the cool thing is, as part of the Merkel tree, the common fields that are used, they always merkleize at the same location. So if you have a Merkel proof verifier for some of the fields that just doesn't care about the rest.
00:47:18.550 - 00:47:18.914, Speaker F: That.
00:47:18.952 - 00:48:24.802, Speaker H: Verifier doesn't need to change all the time anymore. So I was wondering whether this table container could be used for other purposes as well. One idea is to for example, attach it to the execution payload header so that we don't have to worry about those bumps anymore, where every time we add new fields and it reaches another power of two, we break all the verifiers or like some other structures as well. And also in the vertical specs there is currently an older version like these optional fields are there as part of the transition logic, I think. And there we could also use this stable container just because it's much more compact in serialization. The optional fields there, they just don't consume any space and like a zero field or I think the optional also doesn't use space. So yeah, whatever.
00:48:24.802 - 00:48:41.370, Speaker H: I mean, question there is just. I'm looking for reviews for EIP 74, 95 and also maybe an estimate how complicated it would be to get it into the various SSC libraries.
00:48:43.070 - 00:48:43.866, Speaker A: Yeah.
00:48:44.048 - 00:48:57.600, Speaker H: Channel in discord if there are questions is the typed transactions channel or just in the EIP there is also a discussion link at the top. That's all for now.
00:49:00.150 - 00:49:17.080, Speaker A: Yeah. Any questions? All right, take a look at it and follow up asynchronously. Thanks Tom Daplion.
00:49:21.430 - 00:50:30.502, Speaker F: Hey, so I'm linking this PR that opened a while ago and I'm bringing it up now because it only makes sense if we want to do this for Denkun, which I know there is zero appetite on changing anything for Denkun at the moment, but I think the PR holds some merit and I think at least Dankart and some other people showed interest to do something like this. So at least I want to bring it to everyone's attention that this is something we could do. And it only makes sense if we do it for the encode. So if we want to do it for the encode, we have to do it basically either now or very soon. So the background here is, as you know, the network is growing at a very rapid rate and if we don't do anything about it, it will reach levels that could trigger some scenarios that we may be problematic. The way to address this on the long term is changing the rewards curve. But doing this would require a level of research that we cannot afford for Denco.
00:50:30.502 - 00:51:47.060, Speaker F: So it will take at least for the next fork, which would be being optimistic mid 2024, maybe later, which at what point the network may be at a level that again triggers these undesirable scenarios, which is basically too much. If stake and dangerous capture by lsds and also having state size to a level that clients cannot cope with it. None of this problem is truly catastrophic, but it could be. So the idea of this PR is let's limit the churn so that at least we buy some time. And whenever we come up with a definitive solution, in a year or half a year or two years, the network is still at a size that could be manageable. And specifically, if we go for some solution, such as changing the rewards curve to encourage stakers to leave, if we already have such a big validator set, I think it will be rather complicated to undo. So again, just take a look at the proposal, think about it, and if we want to do something about it, we should do it now.
00:51:52.780 - 00:51:56.120, Speaker A: Thanks, Amit.
00:52:04.360 - 00:52:10.630, Speaker B: Can you quickly explain for layman what this proposal is about?
00:52:13.020 - 00:52:13.528, Speaker A: Yes.
00:52:13.614 - 00:52:50.050, Speaker F: So this proposal has the intention to limit the growth of the validator set just by limiting the churn coefficient. So currently the growth is exponential with more validators. The amount of validators that we allow to enter the set is proportional to that, to the number of active. So what this proposal specifically introduces is a max, so that the maximum number of validators that are allowed per epoch remains constant and doesn't keep growing with the set size.
00:52:52.520 - 00:53:40.830, Speaker A: Just a note, the churn quotient grows with the set size because essentially, as a function of the set size, to keep the same security parameters with respect to the week subjectivity period, you can change the set size only so much per unit time as a function of the set size. And so that's why it does grow with it. Obviously, there are other considerations coming into play these days, so it bounds it to be linear as a band aid while people think more deeply about more sustainable solutions.
00:53:43.410 - 00:54:21.770, Speaker F: Yeah, this proposal is the minimum that we can do that could be considered somewhat fair. Like any other solution that tries to mitigate the problem becomes either really political or security sensitive and would require more analysis. I think this is probably the only one that we could accept without a long time looking into it. I mean, there are definitely some considerations that Danny raised about fairness for people that want to enter before and after this change, but I think it's not incredibly significant compared to what could happen downstream.
00:54:27.160 - 00:55:13.380, Speaker A: Yeah. To be clear, I don't think there's a perfect band Aid. And so if there's an appetite for Band Aid, this seems in a reasonable direction. But I'm not going to throw my hat in the ring too much on if this should modify or back at this point, recording stopped recording in progress. Are there other comments today?
00:55:17.850 - 00:55:20.390, Speaker B: Is this actually going to be considered for Duncan?
00:55:25.690 - 00:55:37.580, Speaker A: The answer is no. No one speaks up and says this is something that has to go in. But here we are talking about it.
00:55:40.090 - 00:55:43.660, Speaker F: Well, in this call you have to ask the negative question, right?
00:55:48.540 - 00:55:49.290, Speaker A: Yeah.
00:55:53.720 - 00:55:58.100, Speaker F: Let's say, is someone against adding this to the NEP?
00:56:03.830 - 00:56:50.880, Speaker C: I think I might be, I'd have to think about a little bit more. But my two cent generally is that this is sort of a patch fix and it is really small, but it's small enough that we could do it in its own fork in something equivalent to a difficulty bomb bullet or something. I think it'd be better to put more effort into a better fix. And also, I don't know, clients are continuing to improve how they handle larger validator set sizes, stuff like that. Yes, I don't think I'm in favor of it at this point, but.
00:57:10.680 - 00:57:58.050, Speaker A: Yeah, I mean, this is one of those things that happens late in the cycle. Urgent or potentially urgent things come up. I think at this point, given the state, the default is to do nothing, and that I'd want a pretty loud. We need to do this if we're going to derail that. Obviously, if I'm misreading the room, then be loud and tell me otherwise. But I think we would need people to be speaking up. If that means review the proposal and sit on it a week and we talk about this asynchronously, or even bring it up on the executionary call, that's okay.
00:57:58.050 - 00:58:56.416, Speaker A: But I do think that we're at the default stable spec, unless it's quite a verbal consensus here to switch. Okay. I do encourage you to look at and review this proposal. There are high urgency problems in this domain, and it's going to be probably a big part of the electra conversation if nothing's done now. So let's continue the conversation. And if this does bubble up as critical high urgency, after others have reviewed this and thought more deeply about it this week, speak up. Thank you.
00:58:56.416 - 00:58:57.200, Speaker A: Dapline.
00:59:01.330 - 00:59:47.600, Speaker G: What is really wording me, I would just add one thing is that sometimes things happens nonlinearly, so you reach some level of things and things go bad very quickly. It's something that definitely let us thinking for a while. And I'm personally not against this proposal, but we also have olsky testnet right about to be launched. Maybe this could give us more insights and prioritize it eventually in the future.
00:59:51.680 - 00:59:52.044, Speaker A: Right.
00:59:52.082 - 01:00:23.910, Speaker F: I want to mention, if it's not clear, maybe in the proposal there are two angles. So one is the state growth and what can that cost to clients and performance? And the other one is the economics of it. Specifically, if you don't believe that Lido having a hold of 50% of all the ETH in existence is healthy. There is a train of thought that the only way to prevent that is by limiting overall its stake because of the one winner takes all dynamics. So, just to consider.
01:00:24.840 - 01:00:26.552, Speaker B: Yeah, I was thinking only on the.
01:00:26.606 - 01:00:29.530, Speaker G: Technical side of it at the moment.
01:00:32.060 - 01:01:09.136, Speaker A: Um, it's not actually, I think, abundantly clear to me that by just changing the total amount of ETH that will be staked via different rate limiting mechanisms, that that would change lido dominance. Maybe tougher because you might have stickier short term equilibriums, but I don't know if that changes the convergence there or the incentives there. Yeah, I'm very concerned about it prevents.
01:01:09.168 - 01:01:17.000, Speaker F: Lido from basically having a higher market cap than ethereum in terms of liquidity.
01:01:23.940 - 01:01:30.690, Speaker C: This is just slowing the process of that happening, though. It doesn't actually change it from happening.
01:01:31.940 - 01:03:13.646, Speaker A: I think the argument be it slows what happens to then have a more sustainable solution before the equilibriums are totally out of whack. Donkra did say in the chat he's in, but. And if we don't do it, this probably becomes like a very high urgency thing right after the fork. How complex is the a? It's not quite a parameter change. It's like a parameter addition and conditional at the fork, it being constant rather than a function. But it's a couple of lines. Okay.
01:03:13.646 - 01:03:35.830, Speaker A: Is there anyone's suggestion on how to move forward? Do nothing, let it marinate for a week, make sure the spec is correct with tests, in case we do want to bring this up again next week. What's the path?
01:03:39.190 - 01:04:28.720, Speaker D: I'm not sure if this was discussed on ACDC before, but on the execution side, we typically try and not include something in the fork the first time it's brought on the call, just so folks have a couple of weeks to review it. Obviously, the farther we push this out, the closer we are to test nets and wanting to have specs finalized. But I don't know if, given the small size of this change and this sort of weak ish consensus that there seems to be around it, it might make sense to delay another couple of weeks before officially including it.
01:04:30.050 - 01:04:38.420, Speaker F: I want to point out that this is not the first time this is brought up. This was brought up in call one one three on July 11 for the first time.
01:05:08.460 - 01:05:44.290, Speaker A: Is this something that we can put as a special agenda item on the execution layer? Call in one week and people can go back to their teams and talk about it and either come with, like, a strong yo or strong. Yes. From various teams so that we can push ourselves out of the murky equilibrium we sit in today. And in the meantime, we can review the spec and make sure that we're good on tests in the event that this were to go in.
01:05:49.250 - 01:05:56.500, Speaker F: Or maybe here. If Dankar wants to help on writing a piece to help get the arguments through on why this is important.
01:05:59.820 - 01:06:35.172, Speaker A: I will leave that to Dankar if he wants to. Okay. If we're sitting in the super uncertain zone in a week, then this isn't going to happen. So talk with your teams. Please try to form a consensus on whether this is verbally precisely where you all stand, either as a team. That'd be great. Or if you all have differing opinions to express those.
01:06:35.172 - 01:06:56.010, Speaker A: But if you can express those in one week time, we can make the decision. Cool. Thank you. Dapline. Anything else for discussion today?
01:07:03.450 - 01:07:19.520, Speaker B: Barnabas Olivia launches in one week, as in eight days. Next week, Friday is the launch day. So hopefully client teams can make a release before the launch day and everyone is ready to go.
01:07:22.310 - 01:07:24.242, Speaker A: Very exciting. Thank you.
01:07:24.296 - 01:07:29.720, Speaker D: And if you're a genesis validator and don't know about the coordination group, then please reach out.
01:07:41.110 - 01:08:00.256, Speaker A: Okay. Anything else for today? Great. Thank you. And big ask. Talk to your teams. Sit on this proposal. Come with strong opinions on the execution layer.
01:08:00.256 - 01:08:07.100, Speaker A: Call in one week time. Thanks, everyone. Talk to you all soon. Thank you. Bye.
01:08:11.120 - 01:08:11.900, Speaker F: Bye.
01:08:13.040 - 01:08:14.030, Speaker H: Thank you.
