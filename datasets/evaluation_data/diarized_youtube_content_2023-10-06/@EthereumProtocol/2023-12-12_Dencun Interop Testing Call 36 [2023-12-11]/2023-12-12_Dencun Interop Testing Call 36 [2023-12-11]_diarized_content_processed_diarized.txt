00:01:15.410 - 00:01:16.054, Speaker A: Hello.
00:01:16.252 - 00:01:44.766, Speaker B: Welcome everyone, to our 36th Dencoon testing call. I guess the two main things I had on the agenda today were just any updates on Devnet twelve from any of the clients or testing teams, and then chatting about getting the Gordy shadow fork up. If there's anything there. And if there's anything else anyone wants to discuss, we probably have plenty of extra time to do.
00:01:44.788 - 00:01:45.360, Speaker A: So.
00:01:46.690 - 00:01:55.220, Speaker B: Yeah, I guess to start, anyone want to share some updates or thoughts about how Devnet twelve is going?
00:02:02.560 - 00:02:37.880, Speaker C: Yeah, I can begin. So Devnet twelve still doesn't have prism on, but other than that we've been quite stable. So we did some blob testing last weekend. Maybe Pari can discuss a bit more about that. And we tested some deposits and exits, and the previously mentioned Nimbus bug has been resolved. Last week we were talking about Nimbus exit bug that has been patched up, and we tested that. The fix did work.
00:02:37.950 - 00:02:39.610, Speaker A: Indeed. Nice.
00:02:41.180 - 00:03:21.670, Speaker D: Yeah. By blob thing, Barnabas, do you mean the blobber or what blob testing do you mean? Yeah, Mario's on the call and can go deeper, but yeah, we use his blobber tool to test equivocations. And we had slashings, as expected, on the network. I think there were about five slashed validators, and then we turned it off to wait until prisms online to slash some more. But the setup exists. Mev's been deployed on all nodes, every client pair that we've had on there. So basically heavy Cl has at least produced one mev block, which is perfect.
00:03:21.670 - 00:03:54.610, Speaker D: The only one that was missing was, I think, Lodestar. But Gajindra pointed out that we haven't added the builder flag on Lodestar's validator client. So once we do that, I presume it would be fine. We'll be doing that soon, but yeah, otherwise. We also ran the regular check that every single node has produced at least one block with a blob, and every single combo has done so. Every El as well as Cl has in fact done that. Yes, builder API with blobs as well.
00:03:54.610 - 00:04:32.444, Speaker D: In case it's hard to reproduce that command, I will try and just paste the result. It's not going to be very readable, but you can see we basically have Dora, which is the explorer, and it's just storing everything to a database anyway. So Philip just wrote a nice SQl query for us to get a nice graph as to which combination has proposed what. And it also connects to the relay API, so we can easily figure out which blocks are proposed via MeV is.
00:04:32.482 - 00:04:43.932, Speaker B: The blob rate simply the amount? I guess. Sorry, another way to phrase this question. For the blob rate, were we trying to get to one is the ideal?
00:04:44.076 - 00:04:44.480, Speaker D: No.
00:04:44.550 - 00:04:44.976, Speaker B: Okay.
00:04:45.078 - 00:04:56.608, Speaker D: I don't think so. I think it's just an indication of how many were produced. It could very well be that there was no blob. Possible blob two. Exactly. There was just nothing valid that could be included.
00:04:56.704 - 00:04:57.590, Speaker A: Got it.
00:04:57.960 - 00:05:13.530, Speaker D: The main reason we just have that number is in case there was one node with like, 0.1 and the rest of them are really high, then that's indicative of an issue, which in this case is true with lodestar ethereum GS, because the node is down like half the time.
00:05:21.380 - 00:05:26.908, Speaker A: Very cool. Yeah.
00:05:26.994 - 00:05:49.096, Speaker B: Any other observations, comments, thoughts on the devnet? Yeah, it's really cool to see that we have the entire pipeline working, I guess.
00:05:49.198 - 00:05:49.944, Speaker A: Yeah.
00:05:50.142 - 00:05:57.816, Speaker B: Next up is the shadow fork. Then, I don't know. Perry, Barnabas, do either of you want to give us an update on how.
00:05:57.838 - 00:05:59.070, Speaker A: That'S been set up?
00:05:59.920 - 00:06:10.136, Speaker C: Next up would be onboarding Prism first. And then if all the blobbing tests pass with Prism, then we can discuss the girlish shadow fork.
00:06:10.168 - 00:06:11.532, Speaker B: I think you'd want to wait.
00:06:11.586 - 00:06:23.552, Speaker C: It would be just. Yeah, you would want to wait with Prism. Otherwise we might need to repeat the shadow fork, which would be very.
00:06:23.606 - 00:06:34.980, Speaker D: Yeah, the idea was that we wanted to do a really big shadow fork, so it would make sense to wait for prism to do that. And in the meantime, just to make sure our tooling is okay, we've been doing virtual shadow forks.
00:06:35.880 - 00:06:38.260, Speaker B: You've been doing what? Sorry, what type of shadow forks?
00:06:38.420 - 00:06:48.380, Speaker D: Worker shadow forks, like on the vertical test nets. And, I mean, we've been finding worker bugs, but that's a different story. But the plus side for us is that our tooling is really nice.
00:06:48.530 - 00:06:49.470, Speaker A: Got it.
00:06:50.160 - 00:07:05.890, Speaker B: And would there be value in doing a smaller Gordy shadow fork while we wait for Prism? Or do you think that we're basically getting all the information we need, like a small shadow fork won't give us more than what we get on the devnets already.
00:07:07.700 - 00:07:26.150, Speaker C: The problem is that 40% of the network, if not more, is Prism. Prism is really a majority client. If it would be one of the minority clients with like a few percent, then I think we should be good to go.
00:07:27.000 - 00:07:36.876, Speaker B: So what does it do, actually? So if 40% of Gordy right now is Prism, when we shadow fork, we.
00:07:36.898 - 00:07:57.740, Speaker D: Start a new beacon chain. So we would just choose. We would use a new split. The main reason it won't, or I think that it won't teach us too much is that 4844 is mainly a networking related update. So having like a small. There's not much overhead in terms of producing a block.
00:07:57.820 - 00:07:58.610, Speaker A: Got it.
00:08:02.020 - 00:08:09.750, Speaker B: Okay. Is there someone from Prism on the call?
00:08:11.400 - 00:08:13.696, Speaker E: I'm here if you'd like some updates.
00:08:13.808 - 00:08:14.470, Speaker D: Yeah.
00:08:16.200 - 00:08:53.236, Speaker E: A big blocker for us is that we had kind of like a work in progress blob storage solution where we were putting our blobs in our database and we wanted to make sure that we were testing our final file system storage solution. So that's complete. We're still working on pruning, but that doesn't matter for this kind of timescale. And we've made some big changes to our verification code, which we've implemented in gossip but not in RPC. So that's kind of the last piece that's missing. And I'm also working with David and Justin on security. They're doing some extra code review.
00:08:53.418 - 00:08:54.390, Speaker A: Got it.
00:08:55.720 - 00:09:04.360, Speaker B: And I know I always feel shitty asking for timelines, but I guess Barnabas front ran me in the chat.
00:09:05.100 - 00:09:18.124, Speaker E: I think we're pretty close. I'm sure we'll be done. I shouldn't say I'm sure, but I think we'll be done by before the next testing call. I think we'll check in kind of towards the end of this week, but, yeah, we're making a lot of progress. I think we're close.
00:09:18.242 - 00:09:18.620, Speaker A: Yeah.
00:09:18.690 - 00:09:57.450, Speaker B: And I guess the reason I ask is basically next week is pretty much the last week before everybody will go on holiday. So if Prism is not ready to test on a shadow fork by next week, do people feel comfortable pushing back the shadow fork to early January? Or would we rather have a shadow fork even with a different network makeup, before the holidays, so that if we find bugs, then it might give us some of the client teams a couple more weeks to fix stuff.
00:09:59.900 - 00:10:00.360, Speaker A: Yeah.
00:10:00.430 - 00:10:01.690, Speaker B: Any thoughts on that?
00:10:04.300 - 00:10:06.910, Speaker C: I think it depends on prism at this point.
00:10:07.680 - 00:10:08.044, Speaker A: Yeah.
00:10:08.082 - 00:10:15.788, Speaker C: If we can get prism onboarded and working by, let's say, Thursday, then probably we can do a shadow fork next week.
00:10:15.954 - 00:10:16.670, Speaker A: Okay.
00:10:17.840 - 00:10:18.156, Speaker B: Yeah.
00:10:18.178 - 00:10:38.048, Speaker D: And girly nodes shouldn't take too long to sync. I think we still need to tune, like our Eric on sync parameters. Sometimes it takes a bit long, but if we can trigger something, maybe before Christmas weekend or something, Banbus and I should still be around afterwards to do it between Christmas and New Year's.
00:10:38.144 - 00:10:38.790, Speaker B: Okay.
00:10:43.880 - 00:10:59.310, Speaker E: Yeah, we can get DevOps a build sooner if you want to just start trying it. We're still testing and we know there's problems with RPC. But gossip should be working. So we can give you binary to try.
00:10:59.680 - 00:11:05.564, Speaker C: We just need a branch. If you give me a branch, I can build it off of that.
00:11:05.682 - 00:11:17.280, Speaker E: Okay. The gossip changes are merged to develop. I don't want you to waste time on something that could blow up because we haven't done a ton of testing yet on the gossip path.
00:11:17.620 - 00:11:27.700, Speaker D: Yeah, we won't probably immediately do it on Devnet twelve anyway. We just toss it through kertosis and likely hive and see what breaks.
00:11:36.350 - 00:11:38.940, Speaker E: Yeah, we'll coordinate in our discord channel.
00:11:39.390 - 00:11:40.140, Speaker B: Awesome.
00:11:49.070 - 00:11:49.530, Speaker A: Yeah.
00:11:49.600 - 00:11:51.950, Speaker B: Anything else on the Gordy Shadowfork?
00:12:01.040 - 00:12:12.210, Speaker D: If not, I guess have other clients already started merging stuff into master? Because it would be nice that we're at a place where we can cut releases from master in Jan. Yeah.
00:12:22.020 - 00:12:22.672, Speaker A: Okay.
00:12:22.806 - 00:12:24.370, Speaker B: Lighthouse getting there.
00:12:29.460 - 00:12:37.380, Speaker C: I think Taku just merged their changes also. Just get a message in interrupt.
00:12:42.800 - 00:12:53.090, Speaker D: Oh yeah, I just deployed the teku changes on Teku gets one. Enrico, if you want to have a look, logs look fine, but if you sanity check it, then I can deploy to everything.
00:12:53.620 - 00:13:21.390, Speaker B: Yeah, I'll do the check now. And I guess does anyone think they won't be in a spot to have their changes merged into master by January where they see at least another month before this happens?
00:13:33.260 - 00:13:34.010, Speaker A: Okay.
00:13:39.670 - 00:13:55.000, Speaker B: Any other testing topics? Just posted. Should we merge this?
00:14:06.830 - 00:14:26.610, Speaker F: I guess Michael is in here. Yeah, but I can ask him offline. I don't know if other clients have looked at this. In general, it looks like most of the people who've been involved with it have kind of approved and so would like to merge it today or tomorrow. But yeah, just if anyone on the call the second to take a look and send any check, I would be appreciated.
00:14:27.030 - 00:14:33.560, Speaker B: Yeah, and maybe it's sort of just pinion on all cordevs and if nobody objects in the next day or so, we just merge it.
00:14:35.130 - 00:14:37.478, Speaker F: Yeah, I'll try with Michael and do that.
00:14:37.644 - 00:14:38.550, Speaker A: Sweet.
00:14:39.290 - 00:14:42.280, Speaker C: This is mostly just a cosmetic change, right?
00:14:42.750 - 00:14:57.520, Speaker F: Yeah, it's really just clarifying the actual precedent of the errors. So now you have a better understanding of did the El accept that fortress updated or.
00:15:15.190 - 00:15:21.640, Speaker B: See Mario will review anything else people want to bring up.
00:15:24.410 - 00:16:07.860, Speaker G: There's a small change on execution spec test that we were working on. Mainly this pr. We're going to include the state test format on the execution spec test because right now we have only blockchain test format. Basically we'll be able to fill all the tests using state test from now on also as well as blockchain test. I just want to give everyone a heads up. If there is any client that doesn't support this state test format, I think most of them should. But if anyone does not, just let me know.
00:16:07.860 - 00:16:26.060, Speaker G: Probably we will make a release soon, in the next couple of weeks with all the state test format tests filled. So yeah, we will let you know and we will try to run with all the clients whenever possible when we are ready. Sorry.
00:16:29.230 - 00:16:29.980, Speaker A: Awesome.
00:16:31.150 - 00:16:42.400, Speaker B: It's a bit more of just like an execution spec test question. But aside from the state tests, which parts of ethereum tests does it not cover?
00:16:43.970 - 00:16:50.602, Speaker G: So we are missing state test format and also the transaction test format.
00:16:50.666 - 00:16:51.520, Speaker A: Got it.
00:16:51.890 - 00:16:59.986, Speaker G: Those two. But we're working on state test. I think it is the most important and pressing one because we probably are missing coverage there.
00:17:00.088 - 00:17:00.478, Speaker A: Got it.
00:17:00.504 - 00:17:05.320, Speaker G: So yeah, we need to get this ready as soon as possible.
00:17:06.890 - 00:17:07.750, Speaker A: Sweet.
00:17:08.570 - 00:17:24.940, Speaker G: And then after this, probably the next step should be just to start porting over tests from Turing tests to have day full coverage. But this is important mainly because this increases cancun coverage. So I think this is the most pressing issue right now.
00:17:34.230 - 00:18:15.860, Speaker B: Anything else anyone wants to bring up, if there's nothing else? The last quick thing I had. So the next of these call is supposed to be two weeks from now. Which would be exactly on Christmas day. I suspect most people will be out that day. One idea. So I guess we can either just skip it. But one other idea that was still tentative.
00:18:15.860 - 00:18:57.330, Speaker B: Let's talk with Danny about this. I don't know if it was fully on board, but basically the ACD during Christmas week on the 20 eigth. I don't think either me or Danny will be around to run it. So if people want to have a testing call on that week, it might be good to just merge it together. And have an optional call on the Thursday for anyone who's around and wants to catch up. Otherwise, if people would rather not have a call at all that week, we can just straight up cancel it. I think Danny and I will be able to handle acds on the week of the 18th and of the first.
00:18:58.500 - 00:18:59.164, Speaker A: Yeah.
00:18:59.302 - 00:19:21.390, Speaker B: Anyone have thoughts, preferences on that? I guess. Does anyone want to keep the call on the 20 eigth? Feel like there's value in having one. Otherwise we can just default to canceling for that. Barnabas wants to call. So Barnabas might be here running the call.
00:19:21.940 - 00:19:23.936, Speaker C: I'd be here alone, I guess.
00:19:24.038 - 00:19:29.440, Speaker B: Okay, so Barnadus will be here giving a poop to anybody who shows up on the 20 eigth.
00:19:32.020 - 00:19:34.524, Speaker C: I'm on 25, 25th.
00:19:34.572 - 00:19:36.260, Speaker B: I don't think we should do 25th.
00:19:36.840 - 00:19:37.590, Speaker C: Okay.
00:19:39.000 - 00:19:57.210, Speaker B: If we want to have something at all that week I would just do Thursday 14 utc and have that be a testing call and not like an all cordes so that anyone who wants to talk testing has a space to do it. We can delete it for now and decide it on the week of if we want it.
00:19:59.580 - 00:20:05.230, Speaker D: Yeah, maybe we just decide, close it to the date depending on what's there to test. Cool.
00:20:06.080 - 00:20:20.690, Speaker B: I'll put it on the agenda for all core devs, not this week, but next week and we can decide if we want to have one. But for sure I would not have it on the 25th. I think that's probably the worst possible time.
00:20:26.000 - 00:20:26.696, Speaker A: Sweet.
00:20:26.808 - 00:20:39.870, Speaker B: Anything else? It okay, well thanks everyone. Talk to you all this Thursday on ACD.
00:20:42.760 - 00:20:43.364, Speaker A: Cool.
00:20:43.482 - 00:20:44.260, Speaker B: Bye.
00:20:47.000 - 00:20:49.440, Speaker C: Goodbye. Bye.
