00:00:02.880 - 00:00:54.034, Speaker A: Okay, welcome everyone to the sequencing and pre conformation call number six. This call is going to be about real time proving and real time settlement. I have a presentation which I prepared which I gave at ZK Summit a couple days ago. I guess this is an opportunity to give the presentation to those who haven't seen it, but more importantly for folks to ask questions and to have hopefully a lively discussion around it. So do feel free to interrupt me as we go. Unfortunately, I don't have a great connection. I'm on mobile connection, so I decided to turn off my camera and I'll go ahead and turn on my slides.
00:00:54.034 - 00:02:07.504, Speaker A: Okay, so I'm going to be talking about real time proving, which to a large extent is all about ZK Asics, not proving asics. And the three parts of my presentation is first of all about giving motivations as to why real time proving is valuable, why we want it, and it actually goes way beyond synchronous composability. Part two is to try and give you a sense for the progress on snark asics. And I'll be able to show with you when I turn on my camera the first ever snark ASIC. And then in the third part, which I guess we could go through very, very quickly because I think it's slightly less relevant, is, you know, my opinions on, you know, what is the best, one of the best designs for a snark ASIC. Do let me know if you want to ask questions, just interrupt me. No need to raise your hand.
00:02:07.504 - 00:03:05.088, Speaker A: Okay, so motivation I guess before the motivation, I want to give a quick definition. In my mind, real time proving is when the proof latency, meaning the amount of time it takes for a proof to arrive relative to native execution, is less, ideally significantly less than the slot time. And so because we have a slot time of 12 seconds, the ten second kind of ballpark is the minimum that you need to qualify as real time proving. But ideally we'll get much, much lower. 1 second 100 millisecond proof latency. And as we all know, real time proving is one of these things that allows for synchronous composability. And the slide that I want to show you here is this one where I've introduced a new term which I call the synchrony window.
00:03:05.088 - 00:03:58.314, Speaker A: So if we have a proof latency, for example, of 1 second relative to a slot duration of 12 seconds, then our synchrony window is 11 seconds. And the reason is that it takes 1 second to generate a proof. And I need to include a proof in my block as a sequencer, then I need to start the proving process 11 seconds in. And so my opportunity for synchrony is limited to the synchrony window of only 11 seconds. So what I think will happen is that initially we'll have real time proving on the order of 10 seconds. And that will mean that the synchrony window will only be the very 1st 2 seconds, which is probably the most valuable part of the block because it's the top of block. But eventually we'll be able to squeeze out more and more synchrony out of our blocks.
00:03:58.314 - 00:05:25.204, Speaker A: Now, one thing I do want to highlight is that we can use snarks here for this real time passing of messages, of trustless messages. I'm going to be muting a couple of people. But we can also use another technique, which is a slightly less trustless and more collateral inefficient, which is to use a liquidity provider. So instead of actually passing assets immediately and trustlessly through a shared contract or through some sort of deposit contract or through the r1, we can also use liquidity providers that kind of simulate this. So the liquidity provider here in the middle can immediately send tokens and assets on the desired roll up to simulate synchronous composability. Even though the dashed lines here, the green dashed line and the dashed yellow line can take a very long time, for example, seven days. So this is kind of interesting, for example for optimistic roll ups or even ZK rollups today that have, that don't have real time proving.
00:05:25.204 - 00:06:08.834, Speaker A: And so what I expect will happen is that we're going to see these liquidity providers fill in the gap, hopefully in a few months. And then whenever the hardware is ready with real time proving, and everyone has opted in, then we won't need those liquidity providers anymore. Yeah, of course there's downside to liquidity providers. There's fees and intimidation and liquidity constraints. And it also doesn't work with things like nfts very well. Now, the second big opportunity for real time proving is this idea of light validation. So we have a notion of a big node with builders, relays and provers, and the notion of a light node, users on their mobile phone or on their smartwatch.
00:06:08.834 - 00:07:04.488, Speaker A: And right now we have this awkward kind of sitting type of node, which is the validator nodes and the medium nodes in the sense that they can't run on the phone, but they don't need that much hardware, a laptop is sufficient. And what I think will be happening is that one by one the clients will become ZK clients. So geth might become ZK Geth and Besu might become ZK Besu. And in order to become a ZK client that is useful for validators, you need to have real time proving. And the reason is that when a new block comes in, you need to know within a few seconds whether or not it's a valid block so that you can sync up with the tip of the chain. And so yeah, this is a key step in individual clients becoming ZK clients. And one of the nice things here is that it's purely off chain.
00:07:04.488 - 00:08:25.748, Speaker A: You have different client teams or even teams outside of the client team, snarkifying the client teams. And actually we have a couple of projects, I think RIsC Zero and the succinct team that have been able to run ref on top of their platform. So they've already been able to snarkify a whole client like ref, and the provers are able to, to keep up with the chain from a bandwidth standpoint. And now we're down to latency optimizations. Now, once we have a ZK client for every client which has real time proving, we're going to be in a position where we can introduce this really cool precompile, which I call the EVM in EVM pre compile. And it basically allows for EVM execution within the EVM. And what this pre compile takes as input is a pre state, a post state, and a state diff, and some proofs, and it returns true or false, depending on whether or not the state transition between the pre state route and the post state route is a correct state transition corresponding to the state diff.
00:08:25.748 - 00:09:22.794, Speaker A: And the proofs are indeed valid cryptographic proofs for this statement. And one of the interesting things here is that it's a very strange pre compile because one of the inputs is an implicit input that is submitted off chain, and it's done on a per client basis. So Geth, for example, or ZkGEF, might have a specific proof system that it has adopted a specific circuit that describes the EVM, and it may have a certain performance characteristic, it might have bugs and whatnot. And if I'm running Zkgeh, I will have to verify the ZK Geth proof. If I'm running another client, I'll be verifying my own client. So the advantage of keeping these proofs off chain is that we can have client diversity, we don't have to enshrine any specific proof system. It also means that the clients can just continuously update their proofs without having to do hard forks.
00:09:22.794 - 00:10:02.444, Speaker A: It's just a very, very clean design, which is pretty much the same that we have right now without the ZK part. And if you're interested in what's placed in the Vitalik's roadmap diagram, it's under the Verge. There's a little square here called Explore EVM verification precompile. Now, if you are a rollout, quick question. Yeah. Is there any particular reason you chose to write a state div there? Yes, great question. So the state diff here is a data availability constraint.
00:10:02.444 - 00:11:07.966, Speaker A: So the data corresponding to the state transition needs to be available. And this is a bit of a technical reason, which is that we want anyone to be able to produce proofs for all the clients, or for any of the clients, and if the data is not available, then they can't produce the proof. And so unfortunately, this EVM and EVM precompile doesn't work very well for validiums, because it requires the data to be actually on Ethereum. So yeah, it's really a precompile that's designed specifically for rollups, because the state def needs to be, needs to be available. One more follow up question. So, for instance, if we are executing the set of transactions, the state div here includes the effect of the transactions, or the transactions themselves, like the signatures and so on, are included here or somewhere. It could be anything.
00:11:07.966 - 00:11:31.072, Speaker A: I mean, technically, it just needs to be sufficiently, it needs to be data from which you can calculate the state dif. So it doesn't have to be the state dif itself. It could, as you said, it could also be the input transactions. This is not like well defined at this point. Yeah. The only strong requirement is that it's sufficiently enough data to be able to recompute this data. Makes sense.
00:11:31.072 - 00:12:05.994, Speaker A: Thank you. Perfect. Now, a roll up that uses this precompiler is called a native roll up. And there's a few advantages of native roll ups. One is that they're super easy to program. Literally, it's like one line of code, which is in stark comparison to what it takes to deploy an EVM roll up today. You need many engineers, and you need to invest a lot of time and effort to build circuits and for proofs and whatnot.
00:12:05.994 - 00:13:07.098, Speaker A: The second big advantage is that if you want to have an EVM equivalent rollup, then this roll up is correct. By construction, you're literally reusing the logic of the clients that secure the l one. Whatever secures the l one is the exact same logic that also secures these native rollups in some sense, there's no way in which there could be a bug. And then a third advantage is this notion of governance synchrony. So the l one has its own governance in the sense that we can do hard forks and we can change the EVM and add opcodes. And this is a real hassle for l two s that want to stay EVM equivalent, because now they also need to do an equivalent change. And in order for them to do that, because they don't have access to the social layer, they have to use some sort of governance mechanism.
00:13:07.098 - 00:13:51.470, Speaker A: It could be a multisig, it could be a governance token. And this governance is in and of itself an attack vector. And it might also not be perfect. It might have latency and whatnot. And so one of the nice things of native roll ups is that as soon as the EVM at layer one updates, all the native roll ups immediately update in the exact same slot as the hard fork happens. And then there's this fourth really cool possibility, which is now you can have native roll ups without any gas limit whatsoever. And the reason is that the gas limit is there to prevent primarily a denial of service attack on the validators that need to re execute the block.
00:13:51.470 - 00:14:55.406, Speaker A: And so if re execution is constant time, because you have to verify snarks on the order of one millisecond, then now you can remove the gas limit. And basically the only bottleneck is just how fast your computer is running to do the native execution and also to do the proving that corresponds to it. And then once we have these native roll ups in some sense, we've reintroduced the notion of execution charting, which used to be called phase two a few years ago, but it's actually a much cooler version of execution charting. So previously we had like a fixed number of execution shards, 1024 or 64 execution shards. And now here you can have as many execution shards as you want. And it's permissionless, it's programmable, and you can add whatever logic you want around your shards or your roll ups. You can add governance, you can add to economics, you can add public goods funding, you can do whatever you want.
00:14:55.406 - 00:15:45.594, Speaker A: So it's a really cool kind of endgame and a much better version of execution charting that we had compared to a few years ago. Okay, so this is the summary slide of the motivation. What does real time proving unlock? It unlocks real time settlement for the roll ups, which gives us this universal synchronous composability. It unlocks ZK clients or snarks clients, which gives us light validators. This will happen incrementally, one client at a time off chain. And then once all the clients have been snockified, then we can think about introducing this Precompile. And this precompile gives us native roll ups and it gives us this native execution sharding, which is super cool.
00:15:45.594 - 00:16:57.994, Speaker A: And one of the things I want to highlight in this last slide is that there's actually incentives for, in this case, the block builders to invest in real time proving and specifically to lower the latency of their real time proving. And the reason is that the more, the faster you can do the proving, the lower the proof latency will be, and therefore the greater the synchrony window will be. And so therefore you can have more transactions that are synchronous in your blocks. And those should be denser in the sense that they should be higher paying transactions. Another thing is that the faster you can prove, and by fast I mean the lowest latency, the more time you'll have to build your blocks as a builder. And the reason is that the block needs to come with the proof. And so if you have, let's say, 1 second of overhead to building the proof, well, that's 1 second that could otherwise have been used for block building, but instead is used for proving.
00:16:57.994 - 00:18:23.384, Speaker A: And then finally, once we have these native roll offs, especially native roll offs that don't have a gas limit, then as I said previously, you're just going to be bottlenecked by physics and computation. And so if you invest in these really fast executors and provers, then you'll be in a position where you can literally stuff more transactions in your blocks. And all of these things lead to more mev, which is an incentive to go invest in real time proving. Okay, any question in this first section? Okay, so now let me talk about ZK ASIC progress, because a lot of people, you know, their initial reaction is, okay, these Asics all sound great, but like it sounds, it sounds also a little crazy and far fetched. So it turns out that there's a lot of effort today that is focused on these not GASic. So there's three companies, which I guess I'm calling the flagship companies in this context, which are axial, Flysig and fabric. And I call them flagship because that's the only thing they do.
00:18:23.384 - 00:19:06.774, Speaker A: Literally. These companies were created to build one single product, which is a snark proving ASIC. And they've already in some cases shipped an ASIC. So in the case of axial, there is already a chip and there's a ton of progress that's done by these other companies, and they also very well funded, and they have a lot of talent in house. And then there's another crop of companies which are also building ZK asics, but for totally different reasons. It's for alleo mining. So Alleo is this blockchain where the proof of work mechanism is effectively snarkifying some sort of statement, some sort of puzzle.
00:19:06.774 - 00:19:56.764, Speaker A: And so through the backdoor, in some sense, these companies are interested in snarks through mining. And then there's other companies, and I list five here that are interested in building these chips, but most of them haven't started yet. So, for example, Ponos is one of them, in Gonyama is one of them. Auradyne told me that they have started the process of an ASIC for a ZK protocol, but they won't tell me which ZK protocol. I don't know exactly where they fit. Maybe they actually fit in this green category. But all in all, there's a lot of companies working on this.
00:19:56.764 - 00:20:29.476, Speaker A: Now, I mentioned that we already have an Asic, and it's from Sesic. And let me turn on my video so that you can see what it looks like. But yeah, this is basically the bottom. You can see all the pins. And then this is the top of the chip. And this shiny area here, I believe, is the actual die where there's silicon. And it's about 100 square millimeter chip.
00:20:29.476 - 00:21:12.314, Speaker A: So it's a reasonably sized chip, it's not excessively large, and it's built using a twelve nanometer process from TSMC. So this is an eight year old process. So it's not bleeding edge, but it's also like not too shabby. And so what I expect will happen is that we'll start seeing chips on even better processes. In some sense, this is a test chip. Now, what does this chip do? The way I think about it is that it's a partial accelerator for snarks. So when you're proving snarks, you have these big workloads which are called msms or entities.
00:21:12.314 - 00:22:12.762, Speaker A: And this chip will basically do these heavy duty workloads. It also does finite field vector addition and multiplication, and there's some amount of programmability, so it supports pretty much any proof system or like most of them. And the reason is that you can configure the prime in the context of your, your FFT or your NTT. So if you have the stocks, for example, from stockware, or from Zksync, or from polygon, or whatever it is, then you can configure your prime. And then in the context of MSM, is you can also configure your curve, your elliptic curve, to work with BN 254, which is very standard, but also other curves like BIS 381. And this is what the axial board looks like. They've only kind of got it working a few days ago.
00:22:12.762 - 00:22:48.394, Speaker A: So they have these. These four chips on the board. It's a PCIe board, and they have 64gb of memory. So I believe these are the memory chips here, which are connected next to the chips. And this whole board here is a board that you can go plug into to a PCIe connection and get acceleration. And this is like the performance that they measured specifically for MSM. And so you can see it's like, compared to a 3090 gpu.
00:22:48.394 - 00:23:10.020, Speaker A: So that's an Nvidia GPU. They actually got. They got better performance. So lower number here is actually better. And it costs significantly less. So it costs about five times less than this GPU. And it also costs roughly six times less from a power consumption standpoint.
00:23:10.020 - 00:23:50.304, Speaker A: So this board consumes 6.3 times less than this GPU. So all in all, it's a win everywhere. It's a better performance, it's better cost and better power consumption. And also, once they're able to compress this chip and make it, because this is more of a test board, I also think that the size of the board will be much smaller than an actual gpu that's axial. They've come out with the v zero chip, I guess, and they have plans for. For new chips already.
00:23:50.304 - 00:24:28.974, Speaker A: But there's these two other companies. There's CYC, which is taking a slightly more ambitious approach for the first version of their ASIC, which is a full end to end snark accelerator. And the reason this is interesting is because even though the MSMs and the entities are the major workloads, once you completely accelerate them, let's imagine that they take zero time. You're still left with other stuff. And so the other stuff starts becoming the bottleneck. And let's imagine that the MSM entities take, let's say, 80% of the time. You still have 20% of the time, which is other stuff.
00:24:28.974 - 00:25:33.424, Speaker A: And so in the best case, if you were to accelerate the 80%, you only have a five x speedup, which is good, but it's not the 100 x speedup that ASICs really promised. And so seismic is kind of shooting for this more ambitious kind of 100 x. And then there's a company called fabric, that is basically building a cryptography GPU. You can think of it as a GPU. But where the individual circuits, for example, the multiplier circuits, instead of being floating point multipliers or integer multipliers that are used in the context of AI, those would be finite field multipliers. So in some sense, even more ambitious than what seismic is doing, but also much more complicated. So now they need to come up with their own tooling, their own drivers, their own programming language and whatnot.
00:25:33.424 - 00:26:12.160, Speaker A: And so there's a higher chance of failure, but also potentially huge rewards. Because it's an accelerator that could work for any kind of heavy duty cryptography, including fhe, not just Snox. Okay, so this is the Seisig board. So this is the company in the middle that's building a full enter and snark accelerator. What they've done for their R and D process is that they've worked with FPGA's, which are basically programmable. You can think of them as programmable Asics. And they basically have three FPGA's per board.
00:26:12.160 - 00:26:42.610, Speaker A: So there's two boards here. And they connect the boards up into some sort of mesh of FPGA's. And their specific setup has 54 FPGA's. So they have three FPGA's per board and 18 boards. And this is the performance that they've measured on their setup. They can do an MSM of size two circumflex 30. So that's about a billion base points and coefficients in less than 200 milliseconds.
00:26:42.610 - 00:27:41.344, Speaker A: So in less than one fifth of a second, which this is an absolutely kind of mind boggling performance and similar mind boggling performance on entities. And so now that they have this FPGA proof of concept, it turns out that the natural next step is to shrink the whole board, basically into one single ASIC. Actually, they're trying to do something even more ambitious, which is to take three boards with nine chips and compress all of that into a single 100 millimeter ASIC. And the reason is that on these FPGA's, they're only using very specific parts of it. And so they can compress all of that into one really super high performance ASIC. Okay, now, the second crop of asics is the Aleo miners. And so allo should come to mainnet fairly soon.
00:27:41.344 - 00:28:36.270, Speaker A: I had a call with Howard, and he seemed fairly optimistic. And the way that I think about it is this continuous Z price. So for those that don't know, Z price is an annual prize that actually Aleo was involved, I believe, in the founding of the prize, and it's millions of dollars. It's something like five to $10 million every year in prizes to accelerate the state of the art of the snark proving. And here we potentially have a z prize, which is ten times the size or even more. So instead of $10 million a year, it could be $100 million a year, or even hundreds of millions of dollars per year. And what I expect will happen is that these companies that I listed at the beginning, the four of them, as soon as the Aleo puzzles have been fixed in Mainnet, then they will engage in this six month race.
00:28:36.270 - 00:29:30.234, Speaker A: Because what Aleo has done is that they've used certain puzzles for their testnets, but they've explicitly said that they're going to use a different puzzle for Mainnet, which they haven't yet disclosed. And so, you know, there's no point to build a asics for the testnet puzzles. So everyone's waiting eagerly for the mainnet puzzles. And what I expect will happen is that instead of having just three companies that are dedicated to snark Asics, we're going to have roughly six companies. And so that's going to be a fantastic outcome from an incentivization standpoint. Okay, any questions on, on pot? One or two so far? Okay.
00:29:30.354 - 00:30:01.286, Speaker B: Okay. Sorry to interrupt. Okay. I'm from Axio, so thanks for having me. Just want to clarify one thing about, uh, our, our chip under the rule map of seismic. Yeah, we also consider. Yeah, because in this acceleration of snark is also within our consideration because, because we included the vector addition and multiplication.
00:30:01.286 - 00:30:51.134, Speaker B: Although also you can, you can operate three operations at the same time. So you can add one variable and the second and multiply with the third variable. So actually, we can also support the end to end acceleration. Yeah, because you can send the vector into the DDR and to the MSM or NTC, and then you can let this stay in the DDR and then to the MSM later. Oh, it's also. Yeah, yeah. So, so you don't have to transmit the data in and back from the host every time you call the MSM.
00:30:52.914 - 00:31:36.642, Speaker A: Okay. That's very good to hear. I didn't actually know that. So, yeah. One of the downsides, for example, of doing acceleration on a GPU of msns and entities only, is that you, you send the data over to the accelerator and then you get the result back, and then all this back and forth between the so called host, which is a general purpose computer with a cpu at the accelerator, that causes a lot of overhead. But what you're saying instead is that the host can send all the data in one chunk and then it will get stored in these DDR. So presumably the 64gb of memory which has super fast interface to the chips and then whatever operation you want, it could be additions, multiplications, entities and assembly.
00:31:36.642 - 00:31:40.534, Speaker A: You can do it all without communicating back to the host.
00:31:42.354 - 00:32:58.670, Speaker B: Yeah, not necessarily all the data, I mean, because sometimes when the size got very large, it's impossible to put all the witness in the DDR. So for small size third quarter we can pull all the witness in the DDR and do everything, I mean the whole snark proving inside the chip and then send a proof back. But for larger ones we can use the data concurrency as we can send the data inside and do the computation inside at the same time. So we can, I mean we can save a lot of time to reduce the communication cost. Yeah, so anyway, anyway we any, I mean, anyway we have, we have, we are developed software, customized the software that we can accept the project and the proof system with very fast proof generation. Yeah.
00:32:58.822 - 00:33:27.694, Speaker A: Okay, understood. Thank you so much. This is super helpful. So I guess what you're saying is that even if you're bored with 64gb of memory, well obviously if your data chunks are bigger than 64gb, then it won't all fit in memory. But I guess one nice thing here is that we can just design a new board with maybe 128gb of memory because that's separate from the process of building the chip, which is the hard part. And then another thing which.
00:33:31.794 - 00:34:33.082, Speaker B: Yeah, that's one way to do it, but it's not the optimal way actually for next generation chip. We are considering another way because we allow the chip to chip communication. So we can have two chips that you should support, 64gb memory and then we can let the communication, let them communicate with each other and then we can do very large entities together and then we can also start store the data, I mean the first half in one chip and second half in another chip. Yeah, but that's for next year, for this generation we can use the data concurrency, that's, we can, when doing the communication inside the chip, we, we send the data in and out so that we can reduce the, I mean we can hide the communication time. I see, yeah, yeah.
00:34:33.138 - 00:34:33.610, Speaker A: Okay.
00:34:33.682 - 00:34:55.004, Speaker B: Yeah, yeah, there are no, this is great. Two is two. Yeah, there are two is two. To accelerate to any proof generation because the most important thing is to reduce the communication time. Yeah, because the communication time is very kind of small.
00:34:58.224 - 00:35:30.284, Speaker A: Okay, gotcha. So I guess Hussein is saying that, generally speaking, when you want to prove these very large knocks, you're bottlenecked by communication, which is to say, you know, input output or bandwidth between the host and the chip, the accelerator, or even within the accelerator, you're not bottlenecked by doing the actual computation. And there's also sort of tricks like sending the data while you're doing computation on some other stuff. But there's also the next generation, which will allow even better communication from chip to chip.
00:35:32.304 - 00:35:33.764, Speaker B: Yes, yes, exactly.
00:35:34.584 - 00:36:01.084, Speaker A: Nice. But I guess one of the things that you said is if we have small amounts of data to start with, then we're in a very luxurious position and hardware acceleration becomes much faster. And this is one of the things that I want to talk about in my opinions about how we should be thinking about real time proofing and acceleration. And in some sense, this is a co design between the hardware part and the cryptographic and software part.
00:36:03.024 - 00:36:50.644, Speaker C: Just before we go on the opinions part. So I'm Mami from Taiko, and yesterday the hello to team at VFRAM Foundation PSE privacy scaling explorations merged something called the ZK acceleration layer. So we started with MSM and entity will be next. And the idea was to provide a standard in rust rest traits so that people working on hardware just have to implement this trait and they would support well for starter hello two. But hopefully this can be adapted to hard works. There is not much to like. Implementing the traits is really easy.
00:36:50.644 - 00:37:29.286, Speaker C: Arcworks can have their own version and there is just a small adapter and similarly hello two people using hello two. So you have, in terms of product, you have scroll, you also have ZK processors like Axiom, you have Tico, of course. And you even have companies like, I think Chroma with Pyken that are doing hello compatible in c. But anyway, this is a standardization for ZK acceleration. The next step will be MSM Ntd. Sorry. And it actually also supports caching inputs.
00:37:29.286 - 00:38:02.434, Speaker C: So either the base point or the coefficients or both, because, well, that was a common feedback. That communication was critical. I will send a link to the pr and also to the telegram chat if you want to discuss it. I think most of the hardware providers like superscalar fabric and seismic are already in.
00:38:03.094 - 00:38:38.664, Speaker A: Yeah. Okay. Thank you so much for this note, Mami. Yeah, once we have standardization, this will basically make the lives of the accelerators so much easier because we can build drivers and tooling to specifically match this API. And we'll be able to go to move forward faster. Okay. ZK ASIC opinions, which actually have some implications on what kind of proof systems and statements we want to be proving.
00:38:38.664 - 00:39:11.464, Speaker A: So just as a quick 101 on snocks, there's two parts to a snock. There's the front end and the back end. And the way that they're connected is through the witnesses. So you have the front end which generates the witness, and we call it witness generation. And then you have the backend which consumes the witness and produces the proof. And this is where the msns and the ffts and the other stuff happens. And just as a rule of thumb, these witnesses are enormous.
00:39:11.464 - 00:40:00.362, Speaker A: So from a data standpoint, they're extremely large. Hence the big fat red arrow. And so if you want to minimize the cost of just communicating and moving around this witness, you want to be doing it on chip. And so my first opinion is that really we want to have one chip which does everything, both the witness generation and the proof generation. Today, the way it's done is that the witness generation is on the host, external to the accelerator. And then, you know, as was mentioned by Hussein, we're in a position where this big fat red arrow is slowing us down. So what comes into the chip is the statement, which is a fairly small statement with a thin green arrow.
00:40:00.362 - 00:40:37.770, Speaker A: And then what comes out is also very small. It's a small proof. And all the, the high throughput stuff stays within the chip. Okay, opinion number two is that really we want to be dealing with bite size chunks. And this goes back to this concept of memory. If we have a very big statement that comes in, then that's going to lead to very high memory requirements. And so what I suggest here is that instead of sending the statements, the native statements, to a specific application, you send them bytecode.
00:40:37.770 - 00:41:12.624, Speaker A: And bytecode basically implicitly means that there is a virtual machine here that works a little bit like a cpu that has clock cycles and ingests this bytecode. And every clock is a tiny chunk. This is what many teams are doing. They're working with, for example, a RISC five virtual machine. Or for example, Stockware has the Chiro virtual machine, which is pretty well known. And because you're working with these small chunks, it's memory friendly. And so it makes the construction of these chips much more feasible.
00:41:12.624 - 00:41:47.644, Speaker A: And then in order to have lots of small chunks, you need a way to stitch together the chunks. And this is where recursion comes in. And there's like the fancy name for recursion is PCD proof, carrying data. And there's also this other term called continuation. So you do a bit of computation, and then you continue it with another piece of computation, and you continue it again and again and again. And we basically need proof systems that are friendly to continuations. So that's opinion number two.
00:41:47.644 - 00:42:40.224, Speaker A: Opinion number three is that we really want to be doing as much programmability as possible out of the ASIC. So programming chips is very, very complicated. Having chips that are programmable is very complicated. But doing programmability in software is like ten times simpler, if not 100 times simpler. The nice thing is that if you have a virtual machine here, which is general purpose like Turing complete, then you can send bytecode here that you can really represent any statement. And so if you have some sort of high level application which is written in rust even or some other language, and you have some sort of action that generates a statement, well, that's fine. The ASIC can handle anything, because here you basically have a general purpose virtual machine which is consuming the bytecode.
00:42:40.224 - 00:43:49.956, Speaker A: And then another kind of concern is, what about the programmability of the proof system? So oftentimes you have hardware manufacturers like Axial that are worried about the community not adopting their chip because it might not be programmable enough. And so what axial did is that they allowed to configure the curves and the primes and all of these things. But my opinion actually is that all of this configuration should happen in software. And the reason is that there's a notion of a rapid snock. So you have some sort of internal snark that you want consumed in a small contract, but the smart contract has constraints. For example, if you want really low gas costs, you need to be sending in a so called bn 254 proof or graph 16 proof. And so many, many different projects nowadays have this design where basically they take an internal proof and then they wrap it to generate a wrapper proof, which I call a wrap for short.
00:43:49.956 - 00:44:35.850, Speaker A: And then they send that in the smart contract. And the nice thing about the wrapper proof is that it can translate between proof systems. So you can go from stocks to graph 16 or whatever to graph 16. And so what I think should happen is that the hardware manufacturers should be very opinionated on the proof system here, one that is friendly to continuation, friendly to memory, friendly to hardware, friendly to all these things. And then whatever they use, it doesn't matter because you can abstract it away with your wrapper snark at the end. And so in some sense, that what the hardware manufacturers can do is that they can provide some sort of drivers or software to do the wrapper, the wrapping at the end, so that you can consume your proof on chain. And if you look at the top roll ups today, they're optimistic rollups.
00:44:35.850 - 00:45:15.518, Speaker A: And the teams don't have any knowledge about snarks. And so they don't really care what curve or what proof system. The only thing they care about is that you can give them something that works and it's cheap to verify. And so I think it's perfectly fine for the manufacturers here to be opinionated on the internal proof systems that is used. And then the fourth opinion that I have is you want to be embracing some of the standards of the industry. So one of them here is graph 16. This is not super relevant because it's part of the software stack.
00:45:15.518 - 00:45:46.814, Speaker A: So worst case you can just have a different wrap. But maybe the most important one here is the RISC five byte code. So if you want to be sending in RISC five byte code, then you need to have a RISC five virtual machine in here that needs to be baked into the ASIC. That can't change. And as far as I can tell, RISC five seems to be the winning virtual machine. So there's succinct with SP one. There's Nexus that's doing it.
00:45:46.814 - 00:45:59.744, Speaker A: There's jolt from the A 16 Z team that was announced very recently. There's RISC zero, of course. And so I just tend to really like the RISC V virtual machine.
00:45:59.784 - 00:46:00.404, Speaker C: It's.
00:46:00.984 - 00:46:01.884, Speaker A: Go ahead.
00:46:02.544 - 00:46:13.204, Speaker C: I could see some, how do you say, pros to also wasm, if only for browser based proving and web GPU as well.
00:46:18.944 - 00:46:28.004, Speaker A: Yeah, absolutely. And there's also merits for ARm as well. I think there's like basically a trade off space depending on the complexity of the virtual machine.
00:46:30.424 - 00:47:05.564, Speaker C: I think arm is a bit too complex. So it's very nice for programming, but the fact that it has a status flag, for example, addition with carry makes it, I think. I'm not a hardware expert, but more painful to implement in FPGA or ASIC, but if we can have arm, we can make it much faster because ad with carry is really painful when you program for editors arithmetic, but it's not a problem for stocks.
00:47:09.544 - 00:47:57.554, Speaker A: Yeah, so I mean, my guess is that in the short term we're going to see like this super simple instruction sets like first five that I believe only have like 40 opcodes. And then we're going to go up the complexity stack, maybe with WaSm and maybe with Arm. I'm told that ARM has now 1000 opcodes. It's no longer a simple instruction set at all. But one of the interesting things I guess thinking fundamentally is that the proof system, the proving, should not be the bottleneck eventually. And the reason is that you can massively parallelize it and so you're going to be bottlenecked by the native execution. And then the question becomes, okay, what is the fastest native execution that you have? What is the fastest cpu's? And it might be the m three cpu from Apple and that turns out to be an arm cpu.
00:47:57.554 - 00:48:49.476, Speaker A: So maybe in ten years time I would change the slide to arm. But in the meantime it seems that in the short and medium term risk five or wasn't might be a good answer here. Okay, opinion number five, which in some sense is a continuation of opinion number three. So if we have the programmatic that's off chip, then on chip we can just fix everything. And this is why there's the frozen emoji, because you're kind of freezing the logic that's internal. And when you freeze, you can pick a specific prime, a specific curve, and you can really squeeze as much juice as possible. So for example, in the case of axial, they have these 384 bit multipliers.
00:48:49.476 - 00:49:29.884, Speaker A: And so if your prime is 256 bits, well now you've kind of wasted a bunch of die area and you're consuming a needless amount of power just to support these larger primes. But even with 256 bit, if you were to fix a very specific prime, then you could get another kind of maybe two x. So the kind of potentially a four x difference between having programmability of the prime versus not having it. And then also just building a chip which is not programmable is just so much simpler than building a programmable.
00:49:34.524 - 00:49:52.184, Speaker B: Sorry to interrupt. Okay, please. I have two questions about this architecture. So first one that do we do the, I mean the bottom level proof and the recursion in the chip?
00:49:55.924 - 00:50:36.384, Speaker A: Yes. So the recursion actually comes into two places. One is that it's internal to the chip, so you have these cycles that come in, you have these little chunks and you have to glue everything together. So there's some amount of recursion in the chip. But the other cool thing is that this can be recursion, that is cross chip, and that allows you to scale horizontally. So if each chip can only run the RISC V CPU at let's say 10 MHz, you really want to, want to run a system at 1 GHz. So you can basically have 100 chips that are vertically, so horizontally scaled and they all stitch together with these continuations that they produce.
00:50:37.644 - 00:50:38.780, Speaker B: Okay, I see, I see.
00:50:38.852 - 00:50:39.516, Speaker A: Great.
00:50:39.700 - 00:51:10.796, Speaker D: Yeah, so like, yeah, I can, I can elaborate a bit more on this question. On this question. This is Leo Feng Francesc. Yeah, so like actually like we started the ZKVM paired hardware designs sometime last year. Yeah. So we found it like very hardware friendly, like in several aspects. The first is due to this continuation, which means you can basically slide execution into several segments and each segment, they are independent of each other.
00:51:10.796 - 00:51:26.774, Speaker D: And second, the I O bandwidth for each segment is very sweet. Yeah, so it's like around 15 MHz in, 250 kb out.
00:51:26.854 - 00:51:27.110, Speaker A: Yeah.
00:51:27.142 - 00:52:03.274, Speaker D: So like minimum bandwidth. And the third thing, which also like very, very good, is like the memory requirement. Like for each booth generation, like on each accelerator is controlled, fully controlled by you. Like you can tweak, you can take the size of the segment to make sure you just have enough memory to support the proof generation. There this three things. We have our own ZKVM design which we borrow a lot of ideas from. Succinct risk zero.
00:52:03.274 - 00:52:09.254, Speaker D: And we have developed the corresponding hardware component to accelerate it.
00:52:14.814 - 00:52:24.554, Speaker A: Amazing. Thank you, Leor, for your, for your point of view. And thank you so much for also for being one of those companies that's focusing on solving this problem.
00:52:25.854 - 00:52:54.364, Speaker B: Okay, I have a second question as well. Okay, so about, so when we accelerate the proof generation and the recursion fast enough, will the gross 16 wrap be the bottleneck? Because as far as we know that in order to get a small proof size on Chun, they tend to make gross 16 very large in size.
00:52:56.704 - 00:53:27.850, Speaker D: Yeah, so like I can probably also talk a little bit more on that part. Yeah, so like, so after like each proof generation, you get a proof, like the proof of your choice. You can have fried proof or other proof. Then you have this two to one recursion. So you recurse or not, you basically build a merkle tree of proofs. And after that, probably the last step is group 16. You just wrap it up and then put it on chain.
00:53:27.850 - 00:53:43.194, Speaker D: So the last step, it depends on your design or depends on the financial that need from your customers. So there are a lot of factors like you can take with.
00:53:44.254 - 00:54:33.404, Speaker B: Oh, but my question that the two part in this architecture, the first proof generation and the final proof generation, sax goes 16. They are totally different because they have. The first is, I mean wide and low in size, but the second is very, very tall and it's small. I mean, I mean it's a narrow tall thing and. But the first wide smaller thing, I mean, it's counted, it's totally different, especially for the entity, right?
00:54:35.584 - 00:55:15.904, Speaker A: Yeah, so you're right, Hughes. And from a proof system standpoint, these are completely different proof systems. And what the rapper does in some sense is a translation between the proof systems. Now, one thing that Leo said, you only have to do the rapid snark basically when you want to settle your snark. So that only happens once a slot or once a minute or once an hour, whenever, how often you want to do the settlement. And in some sense, the internal recursion will happen millions of times, relative. It might be a million to one ratio in terms of workload between the internal one and the final one.
00:55:15.904 - 00:55:51.288, Speaker A: Now, one thing that you're right, we should be concerned about, especially in the context of low latency proofing, is we also want this wrapping to be reasonably low latency. Now the good news is that these continuations, they're themselves snark proofs, they are themselves small. So as an example, if you look at risk zero or sp one from succinct, like the continuation is itself a stock. So it's. You could actually put it on chain.
00:55:51.336 - 00:55:56.312, Speaker B: Yes, I know it's small. Yes, it's small in size. Yes, I see.
00:55:56.408 - 00:55:56.880, Speaker A: Yes.
00:55:56.992 - 00:56:13.420, Speaker B: Yeah, I know that the final rep is happening very, I mean, very small is the number of times. Yeah. So maybe you can. We don't need to design another chip for it. Yeah, okay.
00:56:13.492 - 00:56:22.724, Speaker A: Okay, I got it. You could use the general purpose chip, which you've already built, the v zero chip, and that will be good enough. Or you could use GPU's. Exactly.
00:56:22.884 - 00:56:25.664, Speaker B: Yeah, yeah, I see, I see. Okay, thank you.
00:56:32.324 - 00:57:12.542, Speaker A: Okay, so I guess there's two common, like pushbacks on this approach that I get in terms of freezing. One is that people will tell you that the proof systems are continuously improving. So every year you have a proof system which is three times faster than the previous year, and that's totally valid. And because it takes time to design a chip, let's say one year, well, by the time your chip is out, then the proof system is already outdated. There's something that three times better. But my response to this is that if we go down this route, you get 100 x. And so, you know, sure, you've lost on the three x, but you're still 100 x faster.
00:57:12.542 - 00:57:16.554, Speaker A: So you have still a net benefit of 33 x.
00:57:16.934 - 00:57:29.074, Speaker C: But when you look at, for example, the announcement from starware or from the folding folks, it's a 50 x improvement, not just three x.
00:57:31.274 - 00:58:01.354, Speaker A: Absolutely. This is a valid point. And so in some sense, you're making a bet here that we're starting to get diminishing returns on the proof systems. And if you look at, for example, folding schemes like Nova style folding, the constants are optimal. You have a constant of one. And so it feels like we are starting to at least soon start hitting a wall. I might be completely wrong, but I guess this is.
00:58:01.354 - 00:59:14.682, Speaker A: You're right, Mami, that if there's another 50 x coming next year, we should just wait and not build any stuff. And then the second kind of pushback that I get is if we're fixing the RISC zero circuit here, what if there's a bug in the circuit? Now you have to trash your Asic. It's garbage because it's fixed. And this is where formal verification, I think, can come in. And it's also another reason to have a fairly simple virtual machine here, is that you can actually do formal verification on these simpler virtual machines. One of the things that we're thinking about within different foundations is actually to dramatically boost the formal verification effort across the whole industry, because we're dealing with circuits and implementations everywhere. We have the asics, potentially ZK a six, but we also have ZK rollups that are going to have hundreds of billions of dollars, and we want to remove the training wheels, but we also want to make sure that we don't have these enormous bugs that could potentially be a systemic risk to FM itself.
00:59:14.682 - 01:00:02.414, Speaker A: And then we have the clients, the EVM clients, that could potentially be systemic because it could lead to consensus failures. And so it's still early days, but where I and Vitalik and others are trying to push for this large scale competition to push forward for modification. And so what I think is a good approach is basically to pick a specific circuit for risk five and make sure it's formally verified before you go tape out the ASIC. And actually, this is typical in asics. They will generally try and prove formally verified properties of the binary circuit, of the transistors and the wires, because even a single bug could mean that the whole chip is gone.
01:00:03.714 - 01:00:20.664, Speaker C: Justin, you mentioned ZK rollup in terms of source of bugs. But if they use ZKVM, like RiSC zero, SP one jolt Nexus, the bug would be in gaff or reef, not in the roller itself.
01:00:22.284 - 01:01:09.172, Speaker A: Yeah, that's correct. So that's one of the great advantages of the ZKVM abstraction, is that you can reuse the existing tooling and compilers and clients. And we are comfortable with the quality of geth and breadth, and we are comfortable with the, the go compiler and the rust compiler. These are kind of known quantities. The thing that is an unknown quantity is basically anything that has to do with circuits and below. So circuits and proof systems. And so this is what we want to minimize the surface area and what we want to have the highest diligence of on which is potentially using formal verification.
01:01:09.172 - 01:01:21.894, Speaker A: And so really the form verification in the context of a ZK client that uses SP one for example would primarily be focused on the risk of it.
01:01:23.594 - 01:01:43.624, Speaker C: Another approach is that there are compiler that use SMT AST. So satisfiability model, load theory like C and Vols can generate both formal verification formal proofs and compile to ZK circuit.
01:01:46.444 - 01:01:46.780, Speaker A: Like.
01:01:46.812 - 01:01:55.664, Speaker C: I think it's Osdemir something, a turkish guy which is doing a PhD at Stanford that's spearheading this effort.
01:01:58.764 - 01:02:19.014, Speaker A: I mean that sounds great. If you could message me on Telegram then I'm. It turns out that there's many different teams that I have already started for more verification efforts within like internally. I think scroll has looked into it, Matlab has looked into it, sounds like Taiko has also looked into it. So I think it'll be good to join forces.
01:02:19.054 - 01:02:44.204, Speaker C: So some audit company there is veridice, I think their CTO has a PhD in formal verification and offers circuit format verification for secret constant, published some papers and also nevermind as a format verification team. And if I were to formally verify some ZK, I would go to one of those three.
01:02:48.104 - 01:03:20.264, Speaker A: Amazing. I mean yeah, let's touch, let's touch base off chain. Okay, this is, I believe my last slide. So just trying to put some numbers on the ZKVM approaches. So the ZKVM approach, one of the big downsides is that you have very high proof overhead. So if you have like one cycle of risk five, you're gonna need roughly 1 million cycles of risk five to do the proofing. Relative to the native execution.
01:03:20.264 - 01:04:16.782, Speaker A: That's maybe a couple of orders of magnitude relative to handwriting your statements. But as we talked about, the handwritten stuff is not necessarily friendly to hardware, but also not to developers or formal verification. So I do think it is fine, a good gambit to start off with this very large number in terms of overhead, which is being optimized quite heavily. So I believe like later this year we're going to be in a position where the overhead is only 100,000 x. Now when you move to ASIC land, what you can expect, this rough back of the envelope is two orders of magnitude improvement. So the 100,000 x goes to 1000 x. And so one thing that is kind of, in my opinion, possibly like totally doable, is that we have this 100 square millimeter chip, which is what I showed you on the camera.
01:04:16.782 - 01:05:09.324, Speaker A: Just turn on the camera again. Basically this image here on the slide is pretty much the same thing as this. And if you look at a RISC V chip that are kind of manufactured, there's many of them. The die area is on the order of zero point 1 mm². So it's an absolutely tiny core, and that's roughly 1000 times less than the die area that we have. And so what I'm hoping will happen is that we can have a full end to end prover that looks like this. You have the native execution that's done on chip, and then it kind of communicates to all the provers around it that almost on a cycle by cycle basis, or with extremely low latency will produce a corresponding proof.
01:05:09.324 - 01:06:17.164, Speaker A: And it's possible that this chip is going to be bottlenecked by power. Because the way that modern chips work is that if you turn on every single transistor all the time, it will just melt. This red area might be limited, let's say to 10 MHz or 100 MHz. Let's say 10 MHz if you want to run at 1 GHz. Well you need to scale horizontally with these continuations that I talked about with ten different chips, each running at, sorry, 100 different chips, each running at 10. That's the end of the presentation and we're at the hour, so I guess I'll open it up to one last question and then we'll have the post call discussion. You okay? Looks like we have no more questions in the public part of the call, so I'll close it here and open the discussion for the post call.
