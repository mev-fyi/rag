00:03:35.210 - 00:04:15.240, Speaker A: And we are live. Welcome everyone to ACD 170. So today we'll cover obviously the updates on the Devnets. Then there's actually two final additions that we have to discuss. So the Max epoch churn limit and then the blob based v op code. And then we had some questions around Devnet nine, and if we have time after this. The RET team has never sort of formally introduced their clients on all core dev, so we can give them some time to do this.
00:04:15.240 - 00:04:59.770, Speaker A: Yeah, so I guess to get started, does anyone want to give an update on Devnet eight and how things have been going in the past week? Yeah, I can try. Barnabas is still on his way to the office, so I think he has a better idea of what happened last week. But we had a few updates, I think, both to Prism and to Bezu. The Nethermine team was trying out a couple of new things. I think they can speak a bit more about that later on. I think that's largely it we should be finalizing. I do see some nodes are off, so I guess we'll be looking into why they're off afterwards.
00:04:59.770 - 00:05:35.302, Speaker A: I think that's all on the Devnet eight side, but otherwise we've been testing a lot of mev workflows. We have the mock builder that's more or less up and running, and we've been trying out lighthouse as well as Lodestar on kertosis. And if any other cls are ready then please just dm me which image or branch and I can start trying out all the mev workflows there. Nice. Thanks. Yeah. Any other client team want to add more color to that? Yeah.
00:05:35.302 - 00:06:58.400, Speaker A: So on Nethermind nodes we are running the branch with blob pool implementation, and as far everything looks fine. So we are testing it on one node for few days, and since few hours it's running on all nethermine nodes and everything looks fine as far. Nice. Any other client team want to share updates? I guess rather than updates? I have some quite interesting data point to briefly mention. This is fairly early, but we haven't done much investigation, but we're seeing Blob cycle that's arrived approximately 500 milliseconds later than the block and lighthouse also reported they're seeing similar behavior. So I'm not sure whether we can tie this behavior particularly to double one single client implementation, or this is just a general p to p behavior, but something definitely worth investigating moving forward because we likely will see a fairly high reorb rate if today there is additional 500 millisecond delay, given a four second cut off. But, yeah, that's just something to look into.
00:06:58.400 - 00:07:57.330, Speaker A: Sorry. Are you seeing the aggregate of both of these things? Like, once I get both a 500 millisecond delay, or are you saying you're getting a block normally and then blobs are delayed on average? Yes, they are kind of the same, because in our implementation, we wait until we have both before we process for choice, and we're seeing blobs, on average, 500 millisecond later. So block usually arrive within the first, like, 50 millisecond, and then blocks are arriving 500 milliseconds around there. Yeah, I guess I ask about the difference because you would expect a race condition between the two. Normally, they're roughly the same size. Right? Like 100 something. And so the fact that it's taking longer for this other message to propagate is very interesting.
00:07:57.330 - 00:08:31.914, Speaker A: And maybe that's because of the additional verification and the hop times or something like that, but I would suspect that it might be something more akin to a certain client isn't forwarding until they get the block or something like that, which is causing latency. So very interesting. I'd love to follow along. Right. I think my predemit analysis is that the gate payload takes longer to return if there's a blops bundle involved. So the gate payload takes about three times as long. This is my preliminary investigation where this is leading.
00:08:31.914 - 00:09:06.184, Speaker A: But, yeah, I can give more updates later. Yeah, cool. Great that we're seeing stuff. Any other client teams have updates they want to share? One more thing I wanted to add. Marius has started 4788 related fuzzing on the Devnet eight as well. Got it. And I guess on the El side.
00:09:06.184 - 00:10:20.590, Speaker A: So never mind. Mentioned you're rolling out the blob transaction pool across all the nodes on the Devnet now from the other client teams. I'm curious about the status of implementation on the transaction pool, because I know that was probably the last big open thing a few weeks ago. Yes, I'm curious, does every team have something to deal with the blob transactions? And how confident are you that it's production ready for everyone? It's still work in progress, so we've checked in one pr, but there will be maybe one or more prs to improve our global transaction pool. Okay, so we are hoping to test that on Devnet nine. Okay, forget the blob pool itself has been ready for quite a while, and we're quite confident that it works. Okay.
00:10:20.590 - 00:11:16.478, Speaker A: The only thing that is missing in Gatscode is the specialized fetcher for blob transactions. So currently blob transactions are announced the same way as every other transaction, and they are retrieved without throttling or without any other protective mechanism. So that's the thing we still need to add. Got it, thanks. On the bezu side, yes, for beso, in the next release, there is the new layer transaction pool enabled by default. This new implementation doesn't have yet any specific feature for blob transaction. The overall concept is that instead of being limited by number of transaction is limited by size.
00:11:16.478 - 00:12:18.610, Speaker A: So in this way, even having more big transaction means less transaction in the pool, but should not create problem with the memory. The idea is to continue to tune it in order to have specific configuration for specific transaction type for blobs. There will be less blob for senders and also strict limit for blobs. But at the moment, bro transaction are treated the same as other. And yes, we still need to see how to fine tune it. Got it, thanks. So there's a question in the chat about els accepting blob transactions before the fork in the transaction pool.
00:12:18.610 - 00:13:17.800, Speaker A: I'm curious, does anyone see a strong reason to do that? By default, Bezel doesn't accept this before the fork. Why does the hive test demand it? There's just a hive test testing the interplay of the four choice v two four choice v three. And it's trying to make sure there are blobs in the first block after. It just seems most els activate their transaction type, the new transaction types whenever the fork is activated. So you can't propagate transactions of that new type until the fork is active. I wasn't sure if anybody was wanting to start supporting the transaction type before the fork. If there was any use case outside of just this test.
00:13:17.800 - 00:13:58.052, Speaker A: It doesn't seem of immense value if there's additional complexity to do so. Yeah, it's not really much more complex. It depends exactly what we do. But it could be a little weird if as soon as we push out the main net release candidates and people upgrade for blob transactions, just sit in the pool for a while, days or weeks. That would be weird. Yeah, it seems easier to just change the hive test to have an empty block after the fork or something. Yeah, we'll talk about it offline.
00:13:58.052 - 00:14:47.390, Speaker A: Thanks. Yeah, we can make sure that it doesn't happen. I mean, it's really easy to simply modify the hype test. Nice. Sweet. Anything else on Devnet eight or recent client updates that people want to share? Okay, so next up then there's two potential spec changes you have to discuss. Yeah, I just wanted to cover Devnet nine after we discuss these spec changes, so that we can then debate if any of those spec changes make it in, should they be part of Devnet nine or not.
00:14:47.390 - 00:15:34.140, Speaker A: So the first of the spec changes is Cl change. So there's now an EIP for it, EIP 7514, which Dapline presented on last week's call about adding Max epoch churn limit to the validator queue, which would lower the rate at which validators can join and sort of limit or push back the time when we'd have 50% and then more than that of ETh being staked. We discussed this on last week's call. We're hoping to make a decision today, given it's already quite late in the fork planning. So. Yeah. Curious to hear from Cl folks.
00:15:34.140 - 00:16:12.760, Speaker A: Yeah. What do you think? A quick note. I believe that the current favored proposal is asymmetric, meaning this is only to the activation side of the queue and doesn't change how exits happen. And there's an open pr. It's had review. I believe shawway has been adding some tests and that in the event that people want to do it, it's very simple. From a testing and release decred.
00:16:12.760 - 00:17:13.550, Speaker A: Yeah, I mean, I'm in favor of doing this. I think there is a chance that the queue will stop being full at some point, end of this year, start of next year or something. But I think it's a big gamble to bet on this. We're not sure what's going to happen, and I have a fear that we might get into a situation where we could have to make very drastic moves on staking. And so if we can agree to do this churn limit change, then it would be much more comfortable for us to make more measured transitions during the course of the next year or the year after, rather than having to rush things. Got it. Thanks.
00:17:13.550 - 00:18:45.660, Speaker A: Yeah. How do Cl client teams feel about this from the lighthouse side? I think we all are keen to see this, but the asymmetric version? Yeah, so I was like a little resistant on the last call, but after looking into it further, I think, one, the asymmetric version is nicer, and two, the actual impact on wait times I don't think will be as extreme as I was maybe initially thinking, because the rate of new deposits is currently lower than churn. So I think that the huge spike that we had after withdrawals isn't actually the norm. But yeah, I still agree that it'd be good to have this upper epoch chain limit to limit the worst case scenario. Got it. Thanks Enrico. Yeah, I want to report we did an internal survey and generally teku team is in favor of adding this limit and yeah, also we were looking at the last version and looks simple to implement.
00:18:45.660 - 00:19:46.060, Speaker A: There was just a comment from Ben giving maybe there might be other alternatives to include in Dennib, maybe having another consensus only for to include it since maybe we are not super in rush, but it's just a comment and generally speaking we are in favor of having this even in. Got it. Thanks. Any other Cl teams? I suspect we can count Lodestar as four. Okay, yeah, I see a plus one from Lodestar. Yeah. I don't know if anyone from prism has thoughts.
00:19:46.060 - 00:20:28.820, Speaker A: I don't have much thoughts. I mean I trust all the Cl teams on this but my slide input is just like with Dnab, with blob. The gossip made gear wars. So it seems like to me it makes sense to account for that extra factor which we don't have today. Got it. And anyone from nimbus on doesn't seem like so, okay, so it seems like there's pretty broad support to have this. Pretty broad support to have it in Deneb.
00:20:28.820 - 00:21:12.156, Speaker A: And then there's a couple comments in the chat about the actual limit, which I don't think we've chosen. So the EIP proposes a couple different limits to be considered. Yes. That's probably assuming we're going forward with this, then that's probably the thing we want to flesh out now. Anzgar, do you have your hand up? Yeah, I wanted to just say on that point, I personally would really like to see a limit, more like eight. I think kind of the default one that was proposed was twelve, but there was different ones kind of in comparison. And to me eight looked like the best compromise.
00:21:12.156 - 00:21:56.710, Speaker A: It doesn't limit the throughput of the queue too much for users, but it also still ends up giving us a significant kind of extra breathering room to just have more time to figure out all of these questions. Whereas with twelve, basically if the queue remains full, we still have a relatively rapid size growth. So I don't know unless people think eight is too aggressive, I think that would be really nice value. Thanks Danny. Yeah, I echo onscar sentiment. Eight. Okay, I guess does anyone want to make the case for another number than eight? It is a power of two as well has that going for it.
00:21:56.710 - 00:22:45.174, Speaker A: Okay, so in that case I guess we're adding EIP 70 514 to the NEB. We'll set the constant to eight, and we can update the EIP and remove sort of the extra tables to make it clear what the impact is. Yeah. Does that make sense to everyone? Any objections or pushbacks? And just a note? I would suspect we can do a release on Monday. I don't think we can confidently get it out tomorrow with the testing that we want. Got it. Yeah.
00:22:45.174 - 00:23:41.080, Speaker A: And then let's do the blob base view, and then we'll discuss how all of this fits with the Devnet nine as well. Okay. Yeah, let's consider this. Let's have this be included for Daneb and. Okay, so we had another proposal, this time on the el, which was to add a base fee to retrieve the blob gas price. So Arbitram mentioned this, and then Carl put together a quick EIP to formalize it. But the idea here would be that similarly to how we can query the execution layer, base fee, or, I don't know, gas price, base fee, we could also allow querying of the blob base fee, and this makes it easier for l two s to generate transactions and whatnot that are dependent on this.
00:23:41.080 - 00:25:07.140, Speaker A: Okay, sorry. Before we go that, Mikhail has a final question on the churn EIP on the validator side. So do we consider the asymmetric version of the EIP or the symmetric version, which seems, I think we all said asymmetric, but I just want to make sure the discussion of the outset was asymmetric. That's the pr that we've been under review and we're working on testing on in terms of the security from an accountability standpoint with respect to finality, there's only an improvement in that and the improvement in weak productivity period in either of these proposals. And we don't see any downside from a security standpoint in having it asymmetric. Obviously, if there are dissenting voices on that, we'd love to hear them immediately, but right now, that's our understanding of the security argument, and that's our understanding of kind of doesn't create any sort of degenerative incentives either. Okay, anyone want to push back against asymmetric? Okay, so we'll go that route, and, yeah, we'll make sure to update the EIP to make that clear as well.
00:25:07.140 - 00:25:37.950, Speaker A: Okay, back to the base fee. So, yeah, we have the CIP Carl put together. It exposes the blob base fee in the EVM. So it's very similar to EIP 3198, which exposes just the base VLP code. And, yes, this would make it easier for l two s to interact. Oh, Lee, you're the one that proposed it, correct? Yeah. So I didn't write the EIP, to be clear.
00:25:37.950 - 00:26:33.680, Speaker A: Thanks, Carl, for that. But I initially suggested the gas price or blob based fee op code. Do you want to maybe just talk a minute for why? Is this useful for arbitram and other l two s? Yeah, absolutely. So l two s need to charge users for the price of posting the user's transactions to l one, or specifically roll ups do, I suppose. Because how l two s generally work is there's a sequencer which accepts users transactions. Or if there isn't a sequencer, there's some sort of aggregator model. There's some entity that accepts users transactions over the RPC, posts them to layer one, and it has to charge the users a fee to pay for the cost of posting those transactions to layer one.
00:26:33.680 - 00:27:30.180, Speaker A: And in arbitrum, this is a trustless model where the arbitram system automatically sets the price of this l one data. And it does that by looking at how much batch posting cost and what the current base fee was at the time of batch posting. That's in the system today. But in EIP 48 44, without the blob based fee opcode, it's no longer visible to the EVM what the cost of hosting the blob was or what the current cost of blobs is. So this EIP aims to address this by simply exposing that information it's already needed in that the el already needs to know this value to charge for the blobs. So exposing it to the EVM shouldn't be much additional complexity. There.
00:27:30.180 - 00:27:49.400, Speaker A: Got it. Thanks. Yeah. So we have as. Yeah. Perlo from optimism. I'd be curious yet to hear, is this also helpful for optimism? And, yeah, I don't know if you have a mic.
00:27:49.400 - 00:28:24.594, Speaker A: Okay, so there's a comment. The blob based view feature is not required. Do you want to just. Yeah, go for it. Yes, sorry, Matthew, Alfred. Well, so it's not, strictly speaking, like, not strictly necessary for layer two, where you can do the fee accounting in the layer two, and the layer two should be able to access any state, any data from the layer one already. Now, it isn't nice to have.
00:28:24.594 - 00:29:14.978, Speaker A: I think it's a good improvement. And since we already have the base fee information, I don't see why we should not have the blob base fee, to be clear. You mean accessing through Merkel proofs into layer one headers? Yeah, exactly. So for example, Romdao information that the layer two exposes from the layer one is also already basically passed along from layer one into layer two. And layer twos don't use the difficulty upcode for this. This is entirely through just a function of some data that the layer two node sees and then converts into a layer two block. Got it? Yeah.
00:29:14.978 - 00:30:29.114, Speaker A: Historically, arbitrarim has not parsed layer one block headers. I think definitely applying that as a workaround would make sense, and I think we figured out how that could be done for the blob base fee in particular. It's particularly helpful for this workaround that the previous blocks header contains all of the information needed to figure out the blob base b of the next block, which would because the parent blocks hash is exposed to the EVM. But I think that this would be still a very useful opcode to have both in terms of I'm not sure on other l two s architectures, and I'm not sure how many of them have looked at this particular part of EIP 48 44 or how applicable this workaround would be to them. And also, if you want to make an l two immutable, you can't be parsing the l one block header with no way to upgrade it because there's a risk of the l one block header format changing in the future. Right. That's a good point, yeah.
00:30:29.114 - 00:31:15.646, Speaker A: There's a comment by Terrence about if there's any Zk teams that have feedback on this. If there's anyone from a ZK roll up team on the call, now's your chance to speak. So clearly, this is like, oh, I mean, obviously not from a ZK team. I just wanted to briefly mention that while the EP as is kind of a proposed to just have the opcode that just pushes that one value onto the stack. I think part of the reason, maybe, if I remember correctly, why the poet for four did not initially include an opcode like this was just that we weren't. Or like that people weren't quite sure how best to expose this. Just because.
00:31:15.646 - 00:32:04.562, Speaker A: I think instead of just always having individual bits and pieces of header information or EVM context information, at some point we would want to have some sort of more structured way where you can basically go through some sort of pre compile so you can completely stop always having to go through the header. So we are no longer restricted in updating the header structure and all these kind of things. I mean, just as arbitram just raised basically, ideally you don't want to go through the header if this is enough of nice to have that. We want to have it in Denkun. Of course that's not feasible because we need a simple way to do it. In that case I would personally still prefer at least to have the opcode be basically forward compatible with multiple fee dimensions. So we just basically take one value from the stack and then that's just the fee dimension basically.
00:32:04.562 - 00:32:28.318, Speaker A: So at zero it returns the normal base fee, at one it returns the database fee. Now we have two opcodes that both return the normal base fee. But I guess that doesn't really matter. So then at least it's forward compatible. I think that might be a nice compromise. It's a trivial change of to the Eap, but yeah, if we don't add it to Denkun I think we should instead start like a conversation soon for the next folk after. Or maybe we should still do that.
00:32:28.318 - 00:33:09.126, Speaker A: How to expose kind of header information in a more structured way inside the EVM. Right. Thanks Carl. Yeah, I mean I think it's very nice to be able to expose more of these things from the header. But I also don't think it's quite as simple as anskars make it out to be. Just trivially looking up header items is relatively constant. But if we want to start adding more fee dimensions, particularly if we're starting to look into l two things like proof of costs or whatever, then I think it starts becoming very complicated as to what the gas schedule looks like for all of this.
00:33:09.126 - 00:33:55.666, Speaker A: So I think if we do want to do that, having this perfectly future proof thing, I think that looks like a very complicated research problem that will take much longer and would potentially push out this solution for a while. Got it. Lee. And then Andrew? Yeah, I wanted to just briefly mention that while I definitely think something like an opcode that exposes different parts of the EVM context could be very useful, I don't think it could be quite as general as reading different parts of the header. There are some parts of the header which should not be exposed to the EVM because that would break the mining process. And also the blob gas price isn't a field in the header. It's computed from two different fields in the header.
00:33:55.666 - 00:34:36.520, Speaker A: So I think exposing the EVM context would definitely make sense. I'm not sure about just exposing generalized header fields. Also I think it would make more sense for this to be an opcode instead of a pre compiled if we're talking about generally exposing different parts of the EVM. Because for instance base fee, block number, those things. I don't think you want to pay the overhead of calling a pre compiled for blob base fee. I'm not concerned about overhead for. But if this is just a general EVM context getting thing, then keeping it as knob code would probably make the most.
00:34:36.520 - 00:35:02.990, Speaker A: Got it. Thanks. Andrew, you're on mute. Andrew. Oh, sorry. Yeah, I think we should do it because it's basically consistent with base feed. It simplifies life abitram and probably other l tools.
00:35:02.990 - 00:35:49.134, Speaker A: It's an easy change so it doesn't introduce any inconsistencies. If we decide later to do something clever about exposing the header, we'll have to do it for base c and blob base c and so on. So like, adding blob base c does not complicate that future refactoring. Got it. Thanks. Yeah, and I guess this is probably a good transition to hear from some of the El teams. Like do any El teams think this is not worth included or are opposed to this? Or I guess have any other comments? Yeah, so Geth already implemented it.
00:35:49.134 - 00:36:17.554, Speaker A: I think it took Marius ten whole minutes to implement it as we were on a call together. Never mind. Seems in support. Carl, do you want to add? Yeah, just on the last comments thing I see, I had a collision with the opcode number. Whatever. So it will change to exactly what Geth has right now. So get's basically done it.
00:36:17.554 - 00:36:36.846, Speaker A: Okay. And there's one minor change. So what's the opcode number that we would use? I had zero x 49 just because next to blob base fee I was using EVM codes. And that didn't account for blob hash, which already has zero x 49. So then it'll be zero x four a. Okay, so zero x four a would be the right one. Okay.
00:36:36.846 - 00:37:09.520, Speaker A: And we already have a pr for testing. Great. Yeah. Okay, so last call. Any objections to including this? Okay, so let's add this one on the execution layer side. So we'll update the Eip to reflect the new opcode address. And I believe that's the only change we need to do to this EIP.
00:37:09.520 - 00:38:19.926, Speaker A: Okay, so with all of that out of the way, Devnet nine. So we had a plan to launch Devnet nine and no, we should not add more things. Ef Berlin. Yeah, we had a plan to launch Devnet nine in I believe, five days. Okay, so Barnaves is saying we're still on track for that, which is great. I assume we should include those two changes in Devnet nine. Does anyone disagree with that? Okay, so we include both eips do any client teams think it's not possible to get that done within a week? I would say as with the previous Devnet cycles, we're going to first have to have the client releases, then make sure the Hive PR is merged, have it in hive, do the local testing, and get on the Devnet.
00:38:19.926 - 00:39:17.840, Speaker A: So I don't think Devnet nine Tuesday, which is essentially four days from now, is realistic. Probably ship move it by another week. Got it. And I guess, is it realistic for teams to say a week from now, like on next week's Cl call, having implemented those things and passing tests, given they're both quite small? And then assuming we get there, then on the Cl call we can make sure to iron out the final timelines for Devnet nine. We also have the testing call next Monday. So if people run into issues in the next couple of days, or there's something that's more complicated than it seems, we can discuss it there. But let's maybe aim for implementations that are passing tests next Thursday and then on the CL call, sanity check the launch date for Devnet nine.
00:39:17.840 - 00:39:46.358, Speaker A: And hopefully those are not all plus ones on the vacation, but plus ones on getting that done by next week. Okay. For Devnet nine. One more thing. We've been doing these 4788 audits in the past few weeks. We've got three audits. I believe two out of the three are complete.
00:39:46.358 - 00:40:35.370, Speaker A: The third one is finishing this week. So they've revealed some small tweaks to the 4788 contract. We were planning to go over all the audits reports on the next AcDe. So two weeks from now. So there was a question as to should we have those post audit changes part of Devnet nine, or do we want to wait potentially another two weeks until all the audits are finished and we've had time to review, to review the changes and the reports. Yeah. How do people feel about that? I don't know if there's that much that will be necessary to review.
00:40:35.370 - 00:41:38.460, Speaker A: So maybe in the optimistic case, we just plan on moving forward. Okay, I guess I'd be fine sharing the audits and a summary of changes in ACD and if it is an issue to deal with it, rather than assuming we need to get on a call to share. Yeah. So I guess given we have at least two of them done, and I think the other one is finishing tomorrow, if I got that right. Can we discuss that on the CL call next week? And if there's any more change that comes from this week's audit, we'll have them included there. And even though we may not have all the final reports and everything, we can at least walk through the pr, which people can already start reviewing, and then on the next CL call, sort of agree to merge that pr and to the changes. Yeah, that's fine.
00:41:38.460 - 00:42:14.680, Speaker A: Our calls are converging, but yes, we're getting close to shipping this thing, so try to save weeks where we can. I can also mention proposed. Yeah. Do you want to go over the output of the first two and how that's changed? I'll post the pr in the chat here. Yeah. I'm hopeful that there are no other changes beyond what was found in the first two, because generally the audits have been pretty good. There hasn't really been anything specifically found.
00:42:14.680 - 00:43:08.872, Speaker A: There were three things that were sort of nice to have improvements. The first main thing, this was the thing that we wanted to fix the most, was that there was an edge case on the get method of the system contract, where if I passed in the timestamp of zero, just all zeros, it would actually return zero as the beacon root. And it's not really useful for anyone. I don't think that there's really anything that could have happened if this was live, because you can't create then a proof against a beacon root of all zeros. But it was kind of a weird edge case on the correctness of the pre compile. It was designed, intended that the contract would revert in case it asked for a timestamp that didn't have an associated beacon route. So that was the main thing that we really wanted to address.
00:43:08.872 - 00:43:57.016, Speaker A: And so now, in addition to the other checks that exist in the EIP, to make sure that the timestamp passed in is actually associated with the result, we make sure that the timestamp that's passed in is nonzero. So that was the big thing. The two other changes were very optional, honestly, just like, slight improvements. But Anskar had taken a look at this a little while ago, and he thought that the history buffer length, the ring buffer length, was not the optimal size if the slot time were to change. And so he proposed that we modify the buffer length to be a prime number, roughly around the number of beacon roots that we wanted to store. So we wanted to store 81 92. Turns out that 81 91 is a prime number.
00:43:57.016 - 00:44:43.584, Speaker A: And what that gives us is that no matter what slot time we change it to, the buffer will always be 100% full, so we won't incur any additional storage overhead of kind of dormant storage slots. If the slot time changes. So this was kind of a nice small change that was beneficial. And then the last one was really just more for aesthetic preferences. The original contract was written to use the call data copy opcode or, sorry, the call data load opcode two or three times, I think. And we just modified the logic slightly to use call data load only once, just in case in the future if call data happened to change in price. It was just aesthetics.
00:44:43.584 - 00:45:19.750, Speaker A: We really didn't need to load it from call data multiple times. Once it was on the stack, we kind of just duplicated and then swap as necessary. So those are the three things that we sort of found that we want to change based on the audits. Nothing else has really been found. And yeah, optimistic that after this next audit, nothing else we. I mean, honestly, the main reason that we haven't merged that PR to the EIP itself is that we're trying to just finish mining an address to deploy the contract to. So yeah, once, once we find that, we'll update it and merge it.
00:45:19.750 - 00:46:10.280, Speaker A: Yeah. Thanks for sharing. Dano, can we get the explanation of the prime modulus into the EIP? I think that's really useful, but I think we will lose track of it if we only have it in this call. Also, there might be issues that it might be unreliable when we're transitioning between block change times because it's going to fill the buffer in a different order. So that might be worth calling out as a security consideration, not worth stopping, but I think it should be called out. Thank you. Any other comments? Thoughts? I think there was a comment from Mario.
00:46:10.280 - 00:46:34.960, Speaker A: The buffer size or something was wrong in the PR, something like that. Oh, sorry. From Mario Vega. I think he was looking at five tests for that PR and download discord. There was some comment. Yeah, on the bytecode. I think the modulus value was incorrect.
00:46:34.960 - 00:47:02.350, Speaker A: I don't know if that's already addressed. Yeah, it's updated actually. Maybe it's not updated on the eap. It's updated on the PR to the bytecode. But yeah, I don't know if I've pushed that onto the EIP. Yeah. And I guess the other option is we can also push to merge all these changes now.
00:47:02.350 - 00:48:13.890, Speaker A: And so it's kind of finalizing the EIP even though we haven't had all the full audit reports. But it might make it easier to set up tests and whatnot. Would people rather do that or are we fine waiting until next week and having the reports out? And I guess confidence that nothing came out in this last day of audit or maybe another way to phrase this. Is anyone blocked if we don't merge this before next Thursday? Yeah, I mean, these are generally transparent changes to the actual clients. Like the interfaces don't change, so I don't see how it's going to cause complexity one way or the other. Okay. And Maris, when you say you would like to re audit it, do you mean having new audits run on this Leila's version, or.
00:48:13.890 - 00:49:08.560, Speaker A: Yeah, sorry, it's a bit loud here. I would just like to run the audit that myself did on it again on this new version, but I held off on that because I wasn't sure if the improvements would make it through or if the changes would make it through. Does anyone dissent against the changes as they currently stand and that any other changes would be like additive or in relation to further fountain bugs? Because I think given the state of where we're at, you could confidently audit that. Also, the audit report is coming out tomorrow, so you can confidently audit the pr, especially after that audit report comes out. The reports won't come out tomorrow. Sorry. The audit will be done.
00:49:08.560 - 00:50:22.950, Speaker A: They'll probably send us an informal message to say looks good, or here are some issues, but the report probably won't. I see, yeah, but yeah. Does anyone oppose merging as this changes as is? Okay, so let's merge these in. We'll discuss it again next week. Just at the very least, the sanity check that nothing else changed. And yeah, it'll make something a bit cleaner for people to review in the coming week. Anything else on 4788? Yeah, well, I wanted to bring another thing.
00:50:22.950 - 00:50:52.430, Speaker A: There is this subtle issue. If you start with an empty system address, then currently I clock. Oh, we lost. Did you just drop off the call? Yeah. I can't hear it. Sorry. Is it better now? Yeah, you're back.
00:50:52.430 - 00:52:13.436, Speaker A: Okay. Basically you would expect the transaction to touch the system address and then delete it at the end if it's empty. But it doesn't happen in get, nor does it happen in everyone because there is the subtlety that subbalance does not touch the address if the transferred value is zero. So effectively what it means is that the system address is not touched and left empty after a system transaction, which is if we just agree that. Okay, there is this quirk, somewhat unexpected. I just wanted us to agree whether we keep things as they are currently implemented in get or change anything about it. And there are test cases in the execution spec tests that actually are rendered with this behavior that the system address.
00:52:13.436 - 00:53:13.470, Speaker A: NMT system address is not cleared at the end of the system transaction. It could be fine if we agree that's this weird quirky behavior, why not? Or we change it. Okay, I see there's a couple of raised hands already. So merek. Okay, my opinion is that we should remove this account from the state just by reading all eips. However, it's important to add that it's not mainnet issue and we analyze here only the edge case that is possible in high test. But if we agree that we should remove this account from the state, it would mean that some clients would stop passing the test only because of this empty edge case.
00:53:13.470 - 00:54:31.880, Speaker A: However, the client won't pass the test if they implement 4788 as a direct right to the state without system transaction. So they do not toch system account. So what else? We have an EIP 4747 that said that we removed all empty accounts on Mainet and clients are free to remove empty account handling edge cases from their code base. So the thing is that we end up in the funny situation where there are two valid results of these tests. Unfortunately, this edge case is bundled together with other cases that all clients really want to pass and test. So my proposition is to move this edge case scenario outside of other four, seven, eight and capcom testing and just keep it as a separate test. And not sure, maybe it sounds weird, but clients will be free to pass it or not, depending if they want to rely on this system account assumptions in Mainnet.
00:54:31.880 - 00:57:13.750, Speaker A: Okay, anyone else have thoughts or makes sense to me? Okay, so just to make sure we're all clear, do you mind just summarizing quickly what the path forward is? So I guess we should remove this empty account handling from high test and maybe keep it as a separate test and it will be up to the client if it is weird, but it will be up to the client to pass or not this step. So they can rely on assumption that there is no empty accounts on mainnet and they don't need to touch and remove this account. Does it make sense for Andrew, for you, for your like client as well? Well, I would like us to agree on the same behavior because it's kind of one of those corner cases that better if we fall clients behave the same because then we'll have worry less about protocol failures. Yeah, I would prefer to formalize it and agree on a single approach. So what version you would like to see removal or keeping in the state? Well, probably the easiest version for me is to keep things as is to keep this weird that exists in guest and in Aragon and not remove empty system address. Yeah, but it would mean that it's in theory, by reading all the IPS like 158 and 161, we should remove this. Maybe we separate this test.
00:57:13.750 - 00:58:59.086, Speaker A: It will be, I don't know, a bit easier to exactly when an address is touched, it transfer, it was never specified. The touching of addresses was never formally specified in the yellow paper. So I guess we could follow what, follow guest and treat guest behavior as the standard or maybe, I don't know. Does eels specify whether an address is touched or not? The execution spec, does it specify exactly when address is attached? Anyone from eels on the so I've been treating the reference test as normative, whether or not it's in the yellow paper. So when I do EVM stuff in Beisu, that's the approach I've been taking is that the tests that have been in place for years are normative. As far as system accounts, the rules, not deleting system accounts is about specifically not deleting the right BMD address. That's what's written into the yellow paper.
00:58:59.086 - 01:00:01.588, Speaker A: So that's the one exception that resulted from the Shanghai attacks. So given there's some uncertainty here, is it worth maybe taking you offline and bringing you up on Monday's testing call after people have had like a couple of days to look into it more? Okay, perfect. Let's do that. So I'll make a note and I ping the El teams as well. Yeah, let's discuss this on Monday. Okay, so I think, yeah, this was the last Devnet nine open question, so we'll sort that one out on Monday. But aside from that, basically we've agreed.
01:00:01.588 - 01:00:58.362, Speaker A: We'll add the Max epoch churn, we'll add the block base fee. We'll also add the changes that have currently been proposed by the audits for the 4788 contract. Is there anything else people feel should go in the dev nine specs? Assuming we'd want Devnet nine to be the last one before we move to actual testnets. So is there anything else we should be testing there? Yeah, Mario. So yeah, probably the trusted setup update would be a good idea to have it in the main net trusted setup using this. So loading the main net file and using that. Yeah, but this also comes with the update of almost all tests, I think.
01:00:58.362 - 01:01:45.510, Speaker A: So it's a pretty high modification to all of the tests. It's easy to do, but I was just wondering if it's ready for us to use. So long and short of that is going to need another, say a week and a half to get that ready. Making some changes as to file formats and that kind of thing. Okay, so then I would not block Devnet nine on it. Agreed. I guess we could have like a super short lived Devnet if we just want to run through this before going on like Holski or Gordy.
01:01:45.510 - 01:02:37.460, Speaker A: But yeah, that would be the last thing. I guess that's out of scope for Devnet nine. Okay. Anything else? Okay, so yeah, let's hopefully sort out this issue with the touch addresses on Monday, and then by Thursday, hopefully teams all have everything implemented for Devnet nine. And on the Cl call we can discuss when we actually want to have this thing go live. Does that make sense to everyone? Okay, sweet. So that was, I think, everything from the spec side.
01:02:37.460 - 01:03:06.090, Speaker A: Last thing we had today is the RET team wanted to take time to actually introduce their clients. So they've been working on it for a while and slowly becoming part of the stack. So yeah, Giorgios, take it away. Hello. Good morning. Let me share my screen. This should be visible.
01:03:06.090 - 01:03:39.320, Speaker A: Yeah. All right, sweet. So thank you for coming to my TED talk, and thank you, Tim, for the introduction. We're going to talk about rest, which is a new execution layer that we've been building at paradigm for the last, maybe almost a year now. Year and a month or a few weeks. The repository is here, the docs are here, we have a book, a chat room. You can find me here and I will get into a few details about the client today.
01:03:39.320 - 01:04:26.870, Speaker A: And feel free to interrupt me. So what have we done? Firstly, we have an execution layer that's Apache MIT license in rust. The reason for the license was to make it maximally permissive for third parties to be able to consume it without having to think about the license. We view the node as the first use case, but what I'm really excited about is the usage as an SDK in infra. So we pay a lot of attention to our abstractions, to our tests or documentation such that people can use components of the node without having to have the full thing. And we've already seen nice results for that, which I can go on in a bit. It has very good performance.
01:04:26.870 - 01:05:00.542, Speaker A: We test on beefy hardware, so caveatting that this might not always work for everyone, but on the hardware we tested on, on latitude and third party discovery reduced it. We sync in 50 hours, which is really impressive. And that is in archive mode, full node syncs slightly faster due to less data written. But overall the rough zip ball is like around that. I have two ips here that people should feel free to play with. They're running alpha eight. It's been up for two weeks.
01:05:00.542 - 01:06:13.958, Speaker A: Alpha eight is running on the same machine and it's keeping up with the tip without any issues. So we want people to hit this box and ideally identify new resource leakages or issues that we haven't identified so far. We take an open source first approach, a lot of culture around mentoring, onboarding people and so on. So far we have over 150 contributors, with only eight of us being paradigm funded, me included, and we have some encouraging numbers also on the library usage, just to give you a quick architectural sketch. And I acknowledge that this should be a lot deeper, but just to take this from a high level, on the syncing mode we employ Aragon's technique of the pipeline or stage sync, which means that first you download all the headers, then all the bodies, then you recover all the senders, then you execute, then you generate the state route, blah blah blah. And this has a nice thing because it allows you to separate workloads from each other. And by separating the workload from each other you can heavily optimize because each workload is very specific.
01:06:13.958 - 01:07:19.146, Speaker A: So some stages are going to be cpu bound, others are going to be I o bound, and you can very carefully optimize each one without stepping on other people's toes. We employ a new mechanism that we call the blockchain tree for staying up to sync. So the pipeline or the stage sync is used only for the historical sync. You should almost think of it as like a database batch read a batch historical backfill where the benefits of loading over big ranges of blocks are valuable. Whereas for the live sync we employ an in memory data structure which basically tracks the last end blocks configurable. It creates a tree of all the possible states that you can be in, depending on the messages received from the engine API, and then anytime we receive a fork choice updated, we can nonicalize a branch of that tree, flush it to disk, and everything else gets discarded in the database. We employ the flat database design, also an ergon innovation, using the fast incremental staterout algorithm that they also came up with.
01:07:19.146 - 01:08:06.138, Speaker A: And we use MDBX via rust because we like consistency, we like having multiple readers on the database, and we have historically really liked its performance, which is also driven somewhat by the usage of Mema. I haven't run personal benchmarks on this, but the node also performs well on zfs. Currently we employ ZStD compression in some of our tables, but some people have run ref nodes with ZFS and ZSTD or LZ four I forget enabled and they had an archive node as low as 1.2 terabytes. Big asterisk on this number. I haven't replicated it, but I've heard it from more than one person. Rev is the engine at the core of ref.
01:08:06.138 - 01:08:45.602, Speaker A: It's a VM developed by Dragon, who some people here might know. We use that EVM in foundry as well. It's I think the fastest. I think EVM one might be faster in some benchmarks or most benchmarks, but it's general very easy to consume as a library. It's very easy to hook on in various steps. So it allowed us to build a lot of nice things on top of it for JSON RPC. Notably, we support both guests debug trace, including the JavaScript tracer, by embedding a JavaScript runtime whenever a user provides a tracer.
01:08:45.602 - 01:09:55.950, Speaker A: And we also support parity's original trace module, which is obviously very useful for MeV, searching for data analytics and so on. One thing that we've done, which thing was very useful, is that we run the engine API and the blockchain tree in a separate thread from the main system, which means that if somebody hits the RPC very hard, it does not cause resource contention with the consensus critical functionality, which is nice. We take a very defense in depth approach to our releases docs and testing, or I guess in our testing, and I'm not going to go through all of this right now, but we have a lot of docs, we have a whole book for node operators, we have rust docs inside the code base. In fact we have links in the code which do not let you merge publicly, expose the code without documentation. And we have a good map of the repository here for anybody that wants to follow. We run hive on a nightly basis, it runs on CI, we get reports about it, we fix them. And yeah, we're overall excited to also be included soon in the upstream hive testing suite.
01:09:55.950 - 01:10:47.280, Speaker A: Another thing that is very interesting is that we developed an RPC load testing tool which we've been using to evaluate RPC providers, third party RPC providers, but we also use it for equality checking as a differential fuzzer, effectively between two rpcs. So while we were testing our tracing implementations, for example, we were running flood with the equality mode on for an aragon, a geth and a retnode, and we could see where was there a diff and we could slowly find the bugs. So very feedback loop driven approach with automated tooling for testing and so on. So far so good in terms of performance benchmarks. This is the box that I was mentioning. So it's a big beefy box. It costs order of $350 a month, which is quite a lot.
01:10:47.280 - 01:11:33.770, Speaker A: The sync. What we found out is that the sync really depends on the disk. And I know that most people in this room are experts, like, we didn't know about this. And so we were very surprised to see a very big variance in syncing time depending on the disk. And matter of fact, there is a great gist on GitHub, or gist with a bunch of disks that you can use for syncing nodes, not just rest nodes, any kind of node. And overall, pretty exciting to see more people kind of publishing their numbers and overall getting a better understanding of what is the optimal disk setup for a good ethereum node. Again, something that people know here is that the AVM is single threaded, so less cores, more clock speed is good for performance.
01:11:33.770 - 01:12:12.380, Speaker A: However, we're starting to iterate on some parallel EVM work, and we're excited to see if that's going to change in the future ram requirements. It's eight gigs, but we hope to go lower because it's all configurable. And basically it's a bunch of in memory caches that we can tune down. It already has sync on Ethereum, on arm, after coordinating with the founder of the project. So there is promise also in lower end devices. This is like a meme I like to use from Raul, who we might have here. Basically, we're not production ready.
01:12:12.380 - 01:12:39.698, Speaker A: We are eight alphas in. We've been running nodes for weeks without issues. But I cannot in good faith recommend somebody to put $100,000 or whatever it is today in software that is a few months old without an audit. If there is interest in helping with an audit, please reach out. Or if you want to run a node, also, please reach out. I know people are running it in production. We don't provide any liability or warranty for it.
01:12:39.698 - 01:12:58.710, Speaker A: I don't know that we will ever will. But we hope to be production ready by the end of the year. Cancun, which is the big question for here. Basically, you can see the status here for it for four. We basically have it all done in the EVM. We're merging the PR today. Mem copy is done, tstore is done.
01:12:58.710 - 01:13:33.346, Speaker A: Self destruct. We just had an open pr. The blob base fee. We just had the PR for it, actually, like a few minutes ago for seven, eight, eight. It was very nice to actually see that it was not required to make EVM changes for it, and I think it's almost done. And for four eight four, the main thing that we're missing is an on disk implementation of the blob pool. But we have an in memory implementation of it, which means that we can move on, hopefully to the Devnet testing high level cultural point from our team.
01:13:33.346 - 01:14:00.794, Speaker A: We try to be good vibes and collaborative, and I think this is kind of like the cornerstone of the vibe. We have been attending all core devs. I haven't been because it's 07:00 a.m. On a Thursday. But sometimes I think most of the team is present because they're in Europe. We are happy to be part of any calls discords like sending us stuff to prototype. We want to hope that we are able to prototype quickly on things.
01:14:00.794 - 01:14:37.800, Speaker A: That was part of the core motivation of building the client. We're happy to give feedback on future ips or to be asked for feedback and just to say it. We don't have any political agenda here to push for any crazy changes. If there is anything that is very numbers driven and we can provide a good objective case for it, we're happy to do the research or to push for it if it makes sense. But I don't think you would expect me to say, oh, there's a paradigm private funded company and we would push this. That's not the vibe here. So just wanted to commit to that.
01:14:37.800 - 01:14:57.962, Speaker A: And somebody has asked us about if we would do SCL. We're not planning to. I am open to creating a single binary for combining the nodes together as an experiment. But I don't know that this is like a great practice. Just because you could doesn't mean you should. So TBD there. But again, open to experimentation.
01:14:57.962 - 01:15:15.838, Speaker A: We're like clean slate. This is a big slide. So next up, Devnet nine. We hope to be ready for it. Holsky as well. Very excited for that. To kind of get out of our shell into the next steps.
01:15:15.838 - 01:15:55.322, Speaker A: For roadmap items, we have snapshots. Snapshots are kind of ergon style flat files that are used for accessing historical data. Unfortunately, due to the way that we set up the sync, which is also similar to Ergon, we're not compatible to the get Nethermind snapsync. We want to be part of some kind of snapshot thing that lets us be collaborative and also see data at the network, not just be bittorrent leeches. It's not clear how we get there. So if there is questions or discussion we can have on that, please reach out. And the third one is production readiness.
01:15:55.322 - 01:16:35.062, Speaker A: So the hope is basically we're feature complete with cancun and snapshots, and end of the year, two three months of more polish audits, refactors upping our coverage, ensuring we're 100% on Hive, all of that. And we hope to have main net production testers soon. And ideally we can enter 2024 and be like, ref is 1.0, use it. Let's go. There's a lot of research that we want to do, in particular, more performance things. I'm a big fan of IO innovations in databases, whether it is IO uring in the kernel or direct I o.
01:16:35.062 - 01:17:10.570, Speaker A: If you have experience with that, please help us. On parallel VM, we have designs, we have read all the block STM papers, we have read the aptos, the move, the polygon implementations. We know how to do it, I think. But I think there's like low level issues that prevent you from going too fast. So we're also happy to discuss about these. And yeah, please talk to us about any future Ethereum changes. We're happy to experiment, discuss, provide critical feedback, say this is a great idea, say this is a bad idea, but always good vibe and collaborative.
01:17:10.570 - 01:18:07.046, Speaker A: I think that's it. Happy to take any questions. Thank you. There was one question earlier around Snap, but it got answered both in the chat and through your presentation. There's a question by Justin what do you use for the underlying key value store? We use libmDbx, which is the library that was used in Selgworm, Eregon, and Akula, and this is a C library, but we have a file to it over rust, and the bindings were written by Artem, who was the previous rust node developer. We have abstracted the database to be behind an interface, and we already have been experimenting with alternative databases. The main thinking here is that we want to not be kind of like logged in to a certain MDB.
01:18:07.046 - 01:18:49.800, Speaker A: For example, we don't love the usage of memap in MDBx because even though it gives performance, in some cases the data sets are big, and when the data set is bigger than the memory, it ends up hurting performance, actually. So in general, we wanted to be able to experiment. So MDBX is the first database I know somebody has been experimenting with a postgres backend without it necessarily being a good idea. But we have a clean interface for implementing new ones. So we started with that one because that was the easiest one to use, the best for performance. But we're open to new ones if you could give me a Cassandra DB like integration that gives us good performance. I would be also open to it.
01:18:49.800 - 01:19:38.054, Speaker A: What's the use for the underlying KV store? I'm not sure I understand the question. That was the question you just answered. Okay. Yeah, there's like two details also on that which are interesting, I think, which is that MDBX or in general, like these types of databases, they have transactions, which means that to write many things to the database you don't need to multithread and just spawn a bunch of go threads, which I think is how it was done. In level DB you're able to open transaction roll up in memory a bunch of database changes and just flush them. And we have architected in a way that only happens across blocks. So by leveraging the acid of the database, you're ensuring that your DB is always in a consistent state.
01:19:38.054 - 01:20:11.092, Speaker A: And this is very powerful because you never control c and end up in a corrupted state which is a lot of node DevOps issues. Yeah. Are there any more questions? Comments? Either raise your hands or put it in the chat. Okay, yeah, well yeah, thanks for sharing this. Great work on Rhett. Yeah, of course. Thank you.
01:20:11.092 - 01:20:31.450, Speaker A: Snapsync is very top of mind. I'm seeing a couple of questions on this. I don't know. Honestly, I don't know. I thought about adding pre images in the things being gossiped. I heard that there is pre images being gossiped in the vertical trees. Yeah, I'm not sure.
01:20:31.450 - 01:21:11.932, Speaker A: Ipfs back and when I o and bandwidth stop being an issue, we had a first look at vertical trees. I have a doc, honestly I haven't read it in weeks so I don't have a good answer there. I know one person. Our team is responsible for doing explorations and yeah, we have a thread. The dual mertle tree component seems like very scary and a lot of maintenance overhead. So no strong opinion, but just saying it feels meaty. Sweet.
01:21:11.932 - 01:21:46.930, Speaker A: Any final questions? Okay, well yeah, thanks Georgios. And I think we also have a rest tag in the R D discord if people want to tag you there for any questions. Sweet. That's everything we had on the agenda. Anything else people wanted to cover before we wrap up? Okay. If not, well, thanks everyone. Talk to you all on the Monday testing call.
01:21:46.930 - 01:21:50.530, Speaker A: Thanks, Tim. Thank you. Thank you.
