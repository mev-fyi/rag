00:00:04.090 - 00:00:39.830, Speaker A: Okay. Hi, everyone. Welcome to the Ethereum sequencing and preconf call number two. This is our third call. The previous calls focused on the basics of the design of Ethereum sequencing with pre confirmations. And we also did a round of intros to kind of gauge interest in base sequencing in general. And the focus of today's call is on potential downside, potential risks and concerns that people have with base sequencing.
00:00:39.830 - 00:01:12.770, Speaker A: And we have seven presenters who will each focus on one separate risk or concern. We're going to try and keep it quick because we don't have that much time. The goal really here is to try and understand the concern, not so much to solve the concern. So if you have any questions, please keep them around clarification of what the concern is as opposed to trying to solve it. The first presenter is Ellie Davidson from espresso. Eddie, the floor is yours.
00:01:14.390 - 00:01:51.550, Speaker B: Okay, perfect. Let me just click share my screen. Okay, so there should be some slides up now. So today what I want to talk about is this fundamental trade off we have between l one, l two interoperability, and l two pre confirmation finality. So by l one, l two interoperability. I mean, like, we want to be able to do atomic swaps between the l one and the l two. We want to be able to react to l one state very quickly on the l two.
00:01:51.550 - 00:03:01.540, Speaker B: And then by pre confirmation finality, I mean, as a user, how confident am I that my pre confirmation is going to actually mean that my transaction becomes final on ethereum? And so we kind of have this spectrum and this trade off where on one end we have slow interop, but we get really strong precomp guarantees, and on the other end we get weaker precomp guarantees, but really good interop with the l one. So the first open question that I want to bring is what exactly are precomps promising? Are they promising l one finality or l one inclusion? So I think that they should promise l one finality because this is what the user cares about. And this is actually what l two s today are essentially promising, is l one finality, but I think that we should be mindful about what we're actually promising. So from there, I just want to introduce this idea of conditional finality, which I think has been brought up in previous meetings. But it's this finality that an l two block is only going to become finalized if a particular l one block becomes finalized. And I think that this is a useful way to look at any l two. So any l two has conditional finality, even non based l two s.
00:03:01.540 - 00:04:11.462, Speaker B: But the difference is that non based l two s can choose an already finalized ethereum block or some ethereum blocks that's sufficiently deep in the chain that it has a very low risk of being reorked. So this is kind of represented by these basketballs on this spectrum. A non based sequencer or non based l two can choose anywhere on this spectrum, but most of them tend to choose towards the left side. But the issue is that with based sequencing, we kind of are forcing roll ups to be on this right side. Because the l two data is being proposed in the l one block, they naturally have to be conditioned on the pre confers block and not on some future or some previous block. And this means that there is a higher chance of reorgan. So as a quick caveat to this, you can say, well, in base sequencing, sure, my user pre confirmation is conditioned on the pre confers block, but what about includers? Includers could include the transaction in their block before the pre confers block.
00:04:11.462 - 00:05:24.660, Speaker B: And to this I say, yeah, that is true, includers could do this. But I think it's important that we understand that includers are not actually part of the pre confirmation promise. We can incentivize includers to include transactions earlier than prior to the pre confers block, but we can't force them, and it's not part of the promise. So I think that this is something we can optimistically hope for, but we can't guarantee. And so with that, I want to say, well then, what is the risk that I'm talking about to base sequencing? And specifically, the risk is that because of this conditional finality, and because of the fact that based sequencers, as we've been discussing them today, because they are forced to have weaker precomp finality, they have a worse experience than current l two s, and they actually, in a way, have a worse experience than the current l one. So current l two s, like I said, a lot of them are conditionally final on a finalized Ethereum block or some very deep Ethereum block that's almost never going to reorg. And so because of this, as a user, once I get a pre confirmation from a centralized sequencer or a decentralized sequencer, I can be pretty much 100% sure that my transaction will become final.
00:05:24.660 - 00:06:07.006, Speaker B: So this is a very big deviation from the current state. And it's also worse than the l one state, because like I said, in the l one state, if I submit my transaction to the public Ethereum mempool, and it shows up in the current block, then. Okay, I need to wait like 30 seconds, 60 seconds, to see that it's sufficiently deep in the Ethereum chain. But with base sequencing, it's not on the current block, it's on the pre confers block. And so the pre confer might not be the proposer for like five or ten blocks. So I can't actually start waiting to see if my pre confirmation is going to become final until the pre confer actually proposes their block. Maybe an includer includes my transaction before that, but there's no guarantee that that happens.
00:06:07.006 - 00:06:46.010, Speaker B: So I wanted to keep this super quick so we can discuss. But basically, here's kind of what I would say the takeaways and the open questions are. So, one, I would say currently based roll ups are forced to have weaker precomp finality. It's great that they get really good l one, l two interoperability. But this might actually not matter to all users, and a lot of users are really going to value this strong finality that l two s offer today. Second takeaway is, I think that we should be very explicit about what we're promising with a pre confirmation. And I think what matters to a user is l one finality, and not l one inclusion.
00:06:46.010 - 00:07:27.730, Speaker B: And then finally, includers are not part of the pre confirmation. So I think that they're very valuable to helping the base sequencing experience. But we need to be mindful about the incentives so that they're properly incentivized to actually include pre confirmed transactions. And we also need to be mindful that they are not part of the pre confirmation promise. And so we cannot rely on that as a guarantee that they will include transactions. And then really quickly, I just wanted to go over some possible solutions and some thoughts. So, one is execution tickets, you could say, well, with execution tickets, we should have fewer missed blocks, maybe fewer reorgs, because we're going to have less timing games, et cetera.
00:07:27.730 - 00:08:09.382, Speaker B: And so maybe if the chance of reorgging is a lot less, then maybe we're less worried about this. Maybe instead of having to wait for five ethereum blocks deep, I, as a user, feel comfortable waiting for one block or something like that. Another option is insurance, or slashing for reorgs. So this is based on the idea that a pre confirmation should be promising l one finality. So even if the pre confer is promised, we could optionally slash them if their block gets reorbed and return this to a user in the form of insurance. So this definitely helps the user experience. They get rewarded if their transaction does not become final.
00:08:09.382 - 00:08:49.262, Speaker B: But obviously in this case, we would be slashing preconfers for something that is not their fault. And then finally, I just want to mention a quick solution that espresso has been working on. And this is, well, perhaps we can actually allow more customizable conditional finality. So perhaps we can say like based rollups or non based roll ups today that can choose to be anywhere on the spectrum between strong finality and weak finality. We can say based roll ups can choose that as well. And we could even go a step further and say, why don't we let users choose that on an individual transaction level? So that's also something that we've been working on in espresso, but I'll end there. Open it up for discussion.
00:08:49.262 - 00:09:01.720, Speaker B: But yeah, key takeaways is that you have this fundamental trade off between finality and l one, l two. Interoperability and base roll ups do not currently have a lot of flexibility to choose that trade off.
00:09:03.370 - 00:09:07.500, Speaker A: Perfect. Thank you, Ellie, we have a couple of minutes. George, you have your hand up.
00:09:09.230 - 00:09:10.342, Speaker C: Hey, thank you, Ellie.
00:09:10.406 - 00:09:27.920, Speaker D: I think what you are plugging here is the risk is absolutely wonderful and you're absolutely spot on. And I agree with you that having the gaps between pre confirmers actually something that's going to cause probably worsening of the user experience.
00:09:28.450 - 00:09:29.200, Speaker C: And.
00:09:31.010 - 00:10:26.100, Speaker D: I don't know about the rest of you, but for me, the original concept by Justin on what based roll up is, was like kind of a guiding light rather than the set in stone design and architecture. And I love the suggestions that you have there, especially the ones around execution tickets. And I think that a solution that essentially draws from the validator set, but does not necessarily have gaps between the validators, so that you don't need to wait to find a validator that's willing to be a preconformer, but rather have a steady queue of preconformers is going to be a possible solution to what you are flagging there. Yeah, I'd love to see something like this. I'm personally also working on something similar that I would love to share probably sometime over the next weeks.
00:10:27.750 - 00:10:31.890, Speaker A: Thanks, George. Looking forward to your write up, Noah.
00:10:33.990 - 00:10:59.870, Speaker E: About additional finality. So let's say that I'm trading on uniswap e two. I have a transaction, I put in a trade, I'm swapping e three, USDC. Let's say I'm on rule of one, and then let's say Justin's trading right after me. And I say that I'm doing conditional finality. So I'm going to only wait for a stronger finality guarantee. Is Justin trading off my state or a different state? Because that seems a little bit od.
00:11:03.810 - 00:11:51.914, Speaker B: Yeah, so this is a really good question. There is some interdependencies there. So if you're using some sort of a shared contract like Uniswap, so you actually could condition it off of different finalities. So if Justin conditions his transaction off of one block and you condition your transaction afterwards off of a deeper Ethereum block, then it's possible that your transaction will be included and Justin's will not, even though yours came later. But that might not be maybe what users want. And so this is an open design question. How we would do this is that we might want to say, hey, if you're interacting with something that interacts with the l one, then any future transaction of that is going to have to be conditioned on the latest Ethereum block.
00:11:51.914 - 00:12:12.040, Speaker B: That is conditioned on because obviously a lot of users know as a user, you probably don't want your transaction to be included if Justin isn't. You want it to be included if Justin's is. So yeah, this is an open design problem about how we kind of represent this intent of what users are going to want to be conditioned on. But it's a good question.
00:12:13.050 - 00:12:14.470, Speaker E: Thanks for the clarity.
00:12:15.610 - 00:12:30.890, Speaker A: Sorry Rohan, we have to move to the next topic, but we will have a post call discussion to discuss further. The next presenter is Cooper from Aztec talking about layer one builder censorship.
00:12:33.150 - 00:12:51.230, Speaker E: Hey everyone, hopefully you can see this. Okay, cool. We're talking about the Ethereum shared sequencer and pre confirmations. Obviously I work at Aztec Labs. L one builders are frequently censoring people. I don't need to explain this to everyone on this call. Shout out to Tony for his lovely websites and diagrams.
00:12:51.230 - 00:13:28.650, Speaker E: To give you some context, I'm thinking specifically through the lens of Aztecs designs and how we would pivot to potentially use the Ethereum shared sequencer. So option one, just the TLDR is like you have a proposal. Randomly elected sequencer publishes a transaction ordering title one, you'd have a separate phase where that proposer chooses a specific proving address. They put up a bond. At some later point that prover submits those proofs to l one, super high level hand wavy, click the link and I'll share it in the channel afterwards if you want to read more. And so mapping these kind of phase analogs to using the Ethereum trade sequencer. Right, phase one, super obvious.
00:13:28.650 - 00:14:08.970, Speaker E: Aztecs proposer becomes generally the Ethereum proposer. Seems likely that we would need some customized builders or dedicated actors for the Aztecs ecosystem to kind of bootstrap know, resulting in this kind of like, gap period before what I think is called the super builders come in. So at some point, Ethereum shared sequencer, the people using the shared sequencer need confidence that they actually have the underlying block data. So in Aztec, because we're a privacy protocol, these are clients that generated zero knowledge proofs. You actually need most of those zero knowledge proofs in order to know that this block is going to become valid. And so I think that this is a largely unsolved problem. Likely all zero knowledge roll ups are probably talking about this right now in one way or another as part of evaluating this design.
00:14:08.970 - 00:14:49.894, Speaker E: I would say from our perspective, the Ethereum chart sequence seems way easier to integrate into an optimistic roll up. But maybe we're wrong about that. And then phase three, someone submits the stuff to that one. So it's like pretty similar, right, in mental models, it's not a huge divergence of what we were previously thinking. And so there's two specific concerns that I think are somewhat well articulated. So you have weak censorship, which is delaying transactions from phases one, two, and three from the previous slides or delaying blobs from one, two, and three from the previous slides. Blobs would probably be only uploaded in phase three or phase two, depending on how strong da guarantees you need to provide to whatever proving network your zero knowledge roll up ultimately decides on.
00:14:49.894 - 00:15:35.218, Speaker E: And then there's strong censorship. And that's like builder just flat out to refuse to touch aztec transactions because it's too messy or complicated. And depending on the severity of these concerns, we might want to redesign this block production scheme in these phases to publish to one eventually, rather than trying to provide more real time finality. So if we're trying to get a bunch of blobs into blocks every 3 minutes or every 7 minutes, that could have much larger censorship implications than if we're happy just getting in every 24 hours. And so we would need to potentially redesign to accommodate that. That doesn't prevent strong censorship, and we need to land a bunch of data on chain at some point. And if that doesn't happen, and you're a privacy first roll up, and you need to get included from the builders, and they won't, I don't know what happens.
00:15:35.218 - 00:16:09.962, Speaker E: Do we pivot to becoming an l one or something like that? I actually don't know what happens in that case. And so this is just like a high level list of things that we think what it would take to use the Ethereum shared sequencer. I don't think all these are required in an mvp or like a first iteration, but these are the things that we see as important. So like inclusion lists. Inclusion lists with blobs. Right now we have a godly 32 transactions due to client side proofs. We have some other options which is like invent a new fancy proof compression scheme, become a validium, which is controversial as all are aware of for various reasons.
00:16:09.962 - 00:16:52.154, Speaker E: We have some more information on why this is the case. We would have to have a pretty clear understanding of builders compliance needs and concerns. We don't have an enshrined or canonical bridge or portal, so we do think that compliance challenges should be implemented at the actual bridge level rather than the message in boxes for our given roll up. We would need to have much better understanding of the peer to peer network and other networking considerations. Privacy roll ups have a shitload of data that honestly no one else realistically has to deal with, and so that's probably why a lot of the other two aren't really talking about it. And then yes, back to phase two from the previous slide. We would need some mechanism for delegating proof rights we're currently in the frenet designs have something that we're calling prover boost, which is just an auction for the rights to prove a block.
00:16:52.154 - 00:17:40.990, Speaker E: But I think all ZK roll ups need kind of clarity on what the ZK integration with the Ethereum shared sequencer ultimately looks like. And they'll all take different design trade offs, right? Aztec has different concerns than potential other l two s. And then I would say outside of the ethereum shared sequencer, we would love and are very excited about blob and block futures on inclusion. Specifically, I would say from our design challenges we care more about inclusion than actual state results. One of those seems much easier to engineer. And just knowing that, let's say, asset can land seven blobs within every 4 minutes is like a super nice guarantee to know, or even just to know, what the relative cost of that inclusion might be in the future. So these are things that would help us gain confidence and trying to articulate that even if we don't go with the Ethereum shared sequencer, we see value in these inclusions and these pre confirmations.
00:17:40.990 - 00:18:06.200, Speaker E: If you want to help us out, contact me on telegram or email. If you could give us a specific sense of your compliance requirements, that would be incredible. All of our research specifically on block production is public on our forum. You can advocate for the prioritization of inclusion lists. And I think every roll up is going to be saying this forever, but increase the DA limits and. Yeah, so that's my take on builder censorship. Thank you all for letting me present.
00:18:08.010 - 00:18:13.110, Speaker A: Perfect. Thank you cooper any clarifying questions from the audience?
00:18:14.590 - 00:18:25.130, Speaker D: Yes cooper what was it that you said about if people could clarify regarding the censorship or compliance requirements? Do you mean from the perspective of the validators?
00:18:28.930 - 00:18:37.870, Speaker E: Yes. In terms of relays or across the board, just getting a better sense of your compliance requirements would be super helpful.
00:18:38.450 - 00:18:43.006, Speaker D: Okay. We can chat about it offline, but we have a lot of insight on it.
00:18:43.188 - 00:18:45.150, Speaker E: I would love your help there, Yuri.
00:18:48.370 - 00:19:07.530, Speaker F: Cooper, it might be helpful for you to elaborate a bit on one of the concerns you highlighted about needing the proof data to be available and how this is different from, I guess, general data availability concerns, like the unique aspects of this problem for aztec.
00:19:07.950 - 00:20:04.570, Speaker E: Yeah. So in our case right now, if you propose a block, and that block is composed of client side generated zero knowledge proofs at the time that you're proposing it, you don't necessarily yet know if all those proofs are valid. You haven't proven them yet. And so you're subject to potentially, like proof withholding tax or invalid proof attacks where they're putting up block proposals that contain proofs that could never be proven. And so we either have to rearchitect our entire proving stack and our execution environment to support some type of reverting zero knowledge proofs in some way, or we have to make sure that at the time that they're actually being committed to, there is sufficient data and sufficient confidence that they will eventually get proven. We're working on other ways where you can kind of optimistically verify them, but the current challenge fundamentally comes from you can't include a zero knowledge proof in a block that can't get proven.
00:20:05.550 - 00:20:12.430, Speaker F: In other words, you want to make sure that something doesn't get pre confirmed before you know that it will be able to be proven.
00:20:13.490 - 00:20:29.140, Speaker E: In our case, we don't want it to be proposed before it can get proven, or we need to redesign a way that we can basically filter those unprovable transactions out in an elegant way before they get finalized. On the.
00:20:32.190 - 00:20:33.198, Speaker D: Mean, is it fair to.
00:20:33.204 - 00:20:35.680, Speaker F: Say that a block cannot be finalized without.
00:20:36.770 - 00:20:37.520, Speaker E: Yes.
00:20:40.850 - 00:20:43.838, Speaker A: Okay, one last question. Yuri, go ahead.
00:20:44.004 - 00:21:06.534, Speaker D: Yeah, I just want, Cooper, could you explain that last piece between you and Ben? Basically, you want to pre con approve, but you don't want to commit to it yet because maybe it's wrong or it goes on forever, et cetera. So you kind of want to buy the pre con, but the ability to back out of it if you don't want it to. I'm not sure I'm clear on what.
00:21:06.572 - 00:21:07.480, Speaker G: Is it that.
00:21:11.130 - 00:21:31.790, Speaker E: I would bifurcate our interest in pre comps from needing a proposal phase that says that this block proposal can eventually get proven, or we can filter out the proofs in that block that will not make its way into the final proof. Does that make sense? So I would think about it separately from the pre confirmation.
00:21:35.770 - 00:21:46.970, Speaker A: Perfect. Thank you, Cooper. Again, we can discuss more after the call. Jonas is next. Talking about pre confer delegation and centralization concerns.
00:21:53.270 - 00:22:49.314, Speaker G: Can everyone hear me and see my screen? Yep. Okay, awesome. So, yeah, I'm going to be exploring the pre confirmation delegator model, and specifically, like these questions. Which existing party is most suited to take on the Preconfer role today, and what are kind of the risks associated with each party? First, some quick definitions to make sure we're clear. So the sequencer is an l one proposer that is opted in to be a sequencer for a base roll up, and for the purposes of this presentation, is also delegated to a preconfer. A preconfer is a sophisticated actor that can handle the requirements of giving pre confirmations, and he's also given full sequencing rights by the sequencer. Aliveness fault is a violated preconfer commitment due to a mis block, which can be either the fault of the preconfer or the l one proposer.
00:22:49.314 - 00:23:30.082, Speaker G: And a safety fault is a violated preconfer commitment due to just a different block being proposed. And that is fully attributable to the preconfer. So a delegation model schematically looks like this. You have a sequencer that delegates all of the pre confirmation rights and sequencing rights to an operator, which we call the preconfer in this scenario. And the users interact directly with the preconfer and not with the sequencer scope. So focus is going to be based on base proposer pre confirmations. So non l one pre confirmations and the role of block building strategies is kind of treated as a black box.
00:23:30.082 - 00:24:26.440, Speaker G: Or we can just assume that the preconfer runs a first come, first serve strategy. So in terms of candidates, the first, most obvious candidate is kind of the relay. And that is because the relay is trusted by the validators already or by most of them. So there is kind of a small leap in terms of trust for precomp delegation. They're already a highly sophisticated entity, so they will be able to handle the requirements, and they have this visibility into the execution payload and given some small changes to mev boosts, also a way to communicate constraints to builders. But there are also a couple of reasons why relays are not maybe the ideal party. So there is kind of an increased dependency on a trusted third party that the roadmap is kind of moving away from, with like PBS and execution tickets, at least as it is today.
00:24:26.440 - 00:26:05.110, Speaker G: It's also pretty complex, because sequencers need to be restaked for liveness faults, and there needs to be some sort of mutual slashing mechanism, or at least a way to determine, in case of aliveness fault, who was actually at fault. And then I would also say in terms of centralization pressures, if there is a relay that's like a first mover, and that garners delegations from most of the sequencers, there's effectively like only one or two preconfers that are sequencing l two blocks, which becomes like a single point of failure. And then finally, we do have a dependency on restaking, again because of the liveness faults, and because sequencers need to be able to be held accountable. But aside from that, I think that for solo stakers, the relay as a preconfer is actually a good idea. But can we do better for the largest part of our economic security, namely node operators and staking pools? And additionally, can we move faster to a proof of concept, without needing restaking and other dependencies? So, given these points, I think that node operators are also in a very good position to run preconfers, mainly because the sequencer and the preconfer in this case is the same entity. So there is no delegation of trust, which then leads to more capital efficiency. Like there's no additional stake that needs to be put up from the sequencer for aliveness faults, because safety and aliveness faults can both result in precomfort slashing.
00:26:05.110 - 00:26:37.326, Speaker G: It's also less complex. So we don't need this mutual slashing and restaking in case of aliveness fault. And there is no revenue sharing. The sequencer doesn't need to share revenue with a third party preconfer, because it's all internal. And also I would argue that it's more decentralized. So there are more node operators at this point than there are relays. So there will be this kind of forced rotation between precompers based on who the next sequencer is.
00:26:37.326 - 00:27:09.166, Speaker G: So I think like, implementation wise, it could resemble something like Solana's turbine. And so, yeah, this is something that I wanted to bring up. I think that in the short term, I think node operators are probably a good way to get to a proof of concept. And also might reduce some of the centralized pressures of having like one or two relays become the de facto pre conf provider, if that makes sense. So, yeah, that was it. And we're chainbound. We're looking into building high performance pre conf infra.
00:27:09.166 - 00:27:11.500, Speaker G: If you're interested, please reach out.
00:27:14.380 - 00:27:17.450, Speaker A: Perfect. Thank you. Yuki, you have your hand up.
00:27:17.900 - 00:28:17.070, Speaker D: I have a quick question. So you mentioned on the first one that relay could be potential preconference. And then you also mentioned that the rethinking could be one way to bring accountability to the liveness port situation, which is when there could be some sort of missed thoughts. But don't you also think that try to bring accountability in the relays as a preconference? In the case that when we don't know exactly the cause of the missed thoughts could be a bit, I guess, mistakenly splashing the relays. In that case, for example, if the missed thoughts were potentially due to just like, the proposers just simply not proposing a block or something, they gave up on calling the get payload, for example, then there was simply no block. That would not be a problem of relays in that case.
00:28:17.680 - 00:28:18.236, Speaker H: No.
00:28:18.338 - 00:28:34.500, Speaker G: Exactly. And that's why the l one proposer, or the sequencer in this scenario also needs to be state. And there needs to be some mechanism of determining who's at fault here. Is it the relay, is it the preconfer? Or is it the l one proposer that just didn't propose a block?
00:28:35.160 - 00:28:45.350, Speaker D: So you're saying both the relays and the proposal would be double stake, or in the case it's both restake? Is that what you're saying?
00:28:45.800 - 00:29:08.860, Speaker G: Well, the relay as a preconfer, would put up a large amount of stake, enough to become a preconfer. And then the sequencer or the l one proposer could kind of restake a part of their stake to take on penalties if they fail to propose a block. So they would both need to be staked somehow.
00:29:10.740 - 00:29:11.600, Speaker A: Ben.
00:29:14.260 - 00:29:14.672, Speaker E: Yeah.
00:29:14.726 - 00:29:36.440, Speaker F: So what do you mean though, when you say they need to stake enough to become a preconfer? But what's the mechanism for electing them? What's the mechanism through which they become elected? You can have parties that are staking, then you have to choose one to become a preconfer. I didn't understand the proposal.
00:29:37.340 - 00:30:15.680, Speaker G: Yeah, I see. So a preconfer could stake in some sort of contract registry. And then the sequencers that want to delegate to that preconfert, they would somehow signify by signing a message that they want to be delegated to that specific preconfer. And then as a client, as a user, you would somehow read the on chain registry, see which is the next sequencer in line and who is he delegated to, and then kind of requests or send your preconfer requests to that specific preconfer.
00:30:17.060 - 00:30:17.536, Speaker A: I see.
00:30:17.558 - 00:30:29.560, Speaker F: So you're saying to allow the l one proposer to just delegate preconfer rights to someone and just. There's an additional requirement that the only people they can delegate to are people who have staked in some contract.
00:30:30.380 - 00:30:31.770, Speaker G: Exactly, yeah.
00:30:34.620 - 00:30:55.330, Speaker F: The last slide you had. Could you go to the last slide? So I didn't understand the point on this slide. Can you re explain it? Who are the node operators you're talking about here? And what do you mean by no revenue sharing also, right.
00:30:56.820 - 00:31:25.092, Speaker G: Yeah. Probably in a case of like a delegated preconfer, if the relationship between the sequencer and the pre conference kind of untrusted, the preconference is taking on these additional duties. Sorry. It's taking on additional duties that it performs on behalf of the sequencer.
00:31:25.156 - 00:31:25.528, Speaker C: Right.
00:31:25.614 - 00:31:51.170, Speaker G: So it needs to be paid somehow. But if the validator set, or the node operator, and with node operator, I mean like chorus one and the bigger node operators, if they can internalize all of that, then they don't need to share revenue with someone outside of their organization, because they are technically competent enough to run their own preconfer, if that makes sense.
00:31:51.860 - 00:31:55.140, Speaker F: So the node operators are also the l one proposers.
00:31:56.200 - 00:31:56.950, Speaker G: Yes.
00:31:57.480 - 00:32:03.220, Speaker F: So this is saying, let's let the l one proposers be preconfers.
00:32:03.880 - 00:32:12.440, Speaker G: Yes. Given that they can put up enough stake and choose like one delegator to accept all of the pre comps.
00:32:15.050 - 00:32:28.330, Speaker A: Perfect. Thank you, Jonas. Next up we have Ben, who will talk about Mev misalignment between the roll ups and the proposers.
00:32:28.750 - 00:33:17.174, Speaker F: Thank you, Justin. Yeah, I don't have slides, but it's very short and we can leave most of it for discussion. So in the vanilla design of based sequencing, we just give the l one proposer the right to propose the next blocks for all the roll ups that are involved. And what this means is that it gives this l one proposer entire ability to capture any form of sequencing revenue that's not being charged within the roll ups itself. So everything beyond the gas that's being charged within the roll up execution environments themselves. And in particular, that includes all forms of mev. So all arbitrage opportunities.
00:33:17.174 - 00:34:28.106, Speaker F: And you can even think of pre confirmations and the tips that are being paid. For pre confirmations as also a form of mev. And all of this can be captured by the L one proposer. And importantly, the protocol is not even aware of how much is being captured, right, because the L one proposers are not selected in any smart way. They're not selected through execution tickets, they're just selected randomly. And so this creates an incentive misalignment problem for roll ups, because, great, you're joining an ethereum shared sequencer, but on the other hand, you're as opposed to running your own sequencer now, potentially giving up on a large portion of your revenue. And so the question is, how could this be designed in a way that does give shared proposing rights to the L one proposer, so that we can increase the utility of the system for everyone, but that does not get all the value to flow to this shared proposer.
00:34:28.106 - 00:35:51.562, Speaker F: There is a way for the protocol to capture this value and also redistribute it potentially to roll ups. So I wanted to spend mostly just highlighting the concern I've talked about before in these meetings, how there is at least one alternative design, the one that we've been describing at Espresso, which is to run, even if we can't make these changes to the l one today, you can still run an auction or a lottery mechanism, basically execution tickets, to select these preconfers. So not to make the preconfers, just these parties that can be delegated to, but actually to elect them through a smart mechanism, and then later give the L one proposer the ability to decide to buy those proposing rights from the preconfers. So by default, the preconfers actually have the rights to produce the next blocks. And instead of saying that the right goes to the Ellen proposer and then it delegates, rather, the Ellen proposer always has a right of first refusal. It can always buy the proposing rights from the auction selected pre confers. And now we know inside the protocol how much the revenue is and where it's coming from.
00:35:51.562 - 00:36:03.840, Speaker F: And we can, by designing the auction appropriately, also figure out how to redistribute the marginal contribution of each roll up to the overall revenue. I think Uri has a question.
00:36:05.650 - 00:36:06.640, Speaker D: As always.
00:36:08.530 - 00:36:10.080, Speaker C: What are you saying now?
00:36:11.750 - 00:36:39.594, Speaker D: So basically you're saying that, listen, if we just give the rights to do that, then all the MEV leaks to the l one, just like it just goes there, and therefore none of the value actually stays with the roll ups. And how could we find a better solution for that? And what you're saying is, if the shared sequencer, which is not the l one proposer, like some mechanism, connect in between. Those have the right, and the shared sequencer have.
00:36:39.712 - 00:36:40.186, Speaker G: Sorry.
00:36:40.288 - 00:37:09.906, Speaker D: And the l one proposers buy, whether execution tickets, whether some other mechanism, they buy this right to do it, then the value won't leak to the l one proposers, and instead it would share, will go to that mechanism. And then you make like a leap, and from there it could go back and be redistributed, et cetera. And I want to put a question mark on that leak. It makes sense. Yeah, don't let it leak all the way. You stop it here. Okay, now it's at the shared sequencer.
00:37:09.906 - 00:37:24.620, Speaker D: So all the MEV is at the shared sequencer there. How is it easier to push it back to the roll ups from them, compared to getting it from the l one proposers? Or like, do you have something?
00:37:27.150 - 00:37:28.150, Speaker F: It's concreted?
00:37:28.230 - 00:37:28.426, Speaker G: Yeah.
00:37:28.448 - 00:38:03.922, Speaker F: It's also explained in a Hakim deposit. I'll drop a link to again. But it's not that. This is an implementation of the mechanism through which preconfers are elected. So the preconfers are elected through an auction, rather than just being assigned to the l one proposal. The auction is run in a way where roll ups are effectively selling or auctioning off their block proposal rights to preconfers.
00:38:04.066 - 00:38:06.070, Speaker G: And the result, it's kind of like.
00:38:06.140 - 00:38:08.214, Speaker D: Selling order flow, really, right?
00:38:08.252 - 00:38:10.474, Speaker C: It's like from that level, very similar.
00:38:10.512 - 00:38:15.020, Speaker F: To selling order flow. And in fact, a roll up could set a reserve price that says.
00:38:16.990 - 00:38:17.354, Speaker G: The.
00:38:17.392 - 00:39:10.700, Speaker F: Shared proposer is only going to be able to buy the right to propose my next block if it pays over this price. Otherwise, I propose my own block. So this also allows for, it doesn't force a situation where there's always one proposer for all roll ups. We could have situations where in certain slots, there are multiple proposers who sort of partition the roll up space, as long as the shared proposer is creating an economic surplus. And if the most economically efficient allocation is to have one shared preconfer that's going to propose the next block for everyone, then the auction will find this solution. But by basically allowing roll ups to sell or auction off their block space in this mechanism, you allow them to capture value.
00:39:11.650 - 00:39:12.400, Speaker G: Interesting.
00:39:14.050 - 00:39:25.630, Speaker A: Kartek. Sorry, we're out of time. Thanks, Ben, for the presentation. Next up we have Yir from scroll talking about scaling bottlenecks with shared sequences.
00:39:26.930 - 00:39:44.146, Speaker H: Okay, sure. Thanks. Sorry. Also because the writers are very late, so I also don't have time to prepare slides. So really sorry for that, but I want to bring a different view for this shared sequencer. Because people talk about pre confirmation, economic guarantee a lot. I want to talk from a very different view around performance.
00:39:44.146 - 00:40:31.590, Speaker H: Because the way I think about layer two and layer one, the relationship between two layers is that layer one actually solves this kind of designation like consensus, all those problems. So all the things that, because I think about layer two or any blockchain as a database and also processor. So basically this processor received transaction and can read and write into this kind of database. And then because you want decentralized, so you need consensus among those kind of database. So you have layer one, but it's not efficient enough. So the way I look about layer two is that, okay, so right now let's forget about decentralization, just for example performance first. So it's like, basically like a layer two is building a high performance DB system with a processor and processing our transaction, maybe in a centralized way.
00:40:31.590 - 00:41:45.102, Speaker H: And then it can read and write into this database and process transaction really fast. And then all the security, decentralization is guaranteed by posting a proof to layer one. And now if you want to have shared sequencer, especially if you have this kind of atomic compatibility between two kind of different row ups. So what you are assuming is that it's a distributed database system where you have two different database, you have two processor, and then normally they only read and write into their own state. And now you are basically allowing this kind of atomicity between two different such system, which is like this processor sometimes can write to the database of the other rob and which actually cause a big problem for if you look at that from a view of just a distributed system or distribute high performance distributed data DB system, because there are two different guarantees for consistency between, because sometimes you will have read and write conflicts because the other ro up when they are maintaining their own state, their own database, they can write to some state and the other rob can sometimes also write to the same state. So you kind of need some locking mechanism. So if you have to take some term from the field of distributed system, there is two ways.
00:41:45.102 - 00:42:32.860, Speaker H: One is a weak consistency, one is a strong consistency guarantee across multiple different replicas. And in blockchain we definitely need strong consistency. The requirement for strong consistency is that all the kind of updates must be immediately propagated to all the copies. So it has a very strong requirement for some global synchronization mechanism. And it's like in the literature of distributed system, it's very unfortunate that such mechanism are very hard or even impossible to implement in a very scalable way, because it's just fundamentally very bad to enable such kind of read and write into different system, especially with the possibility of revert. So it will massively. So imagine you are building one single system.
00:42:32.860 - 00:43:35.706, Speaker H: You only allow that to happen within your own system. You can make that super, super performant like Solana or some really high performance chain. But now once you enable the possibility for cross read and write and enable this possibility for having conflicts, then the performance of each system will be massively decreased, especially if that based, because that need to be considered into part of your system design. So that would cause the performance of both sides become seriously massively influenced. So that's a problem when I'm talking about this problem with Justin. One immediate solution which from Justin and also we have researcher Mohammed working on this, is that okay, so can you have, now if you have this kind of enabled cross chain transaction and read and write into our own state, that's a problem. Now can you kind of, maybe if one is operating both rob node, can you have design some mechanism where every ten block there is a different block that only contain cross chain transaction, super transaction.
00:43:35.706 - 00:44:02.534, Speaker H: And as far as it's economically variable, because some people want to pay more. And then you can make this kind of only happen within a certain period of time. So that in other time you only need to handle transaction within your os system. So you can still design that in a very high performing way. But this design also requires the same block. Time also has a very centralized assumption that one entity have to operate on both sides. And also it's actually a trade off between liveness and economics.
00:44:02.534 - 00:44:59.754, Speaker H: Because imagine you have one half of your time just producing, processing such block. So which means one row up's liveness can also influence the other row up liveness. Because one row up, once the neon cross chain transaction can lock the state in the other row up, which will influence at least your performance will be like half decreased. So there is a trade off between how much economically you want to kind of make this happen versus in reality how much it influences the performance. Eventually we don't want to end the game where some people can send an atomic transaction to lock the state on both sides. So basically influence the liveness on both sides with a very cheap, just one transaction and which will make this kind of actually less scalable. And essentially we are thinking about a very scalable future, where the ethereum and plus is oil layer two are really processing like 10 million transactions per second.
00:44:59.754 - 00:45:39.980, Speaker H: We really need to think from not just economics, not just this kind of blockchain. Perspective, but actually from a way for how we build a performant enough system to kind of handle this. Imagine if you allow this compatibility to happen between 100 different chains, each chain maybe maximum throughput might be like 1000 and then you need way more such kind of system to kind of talk with each other within some block time which will massively influence even like you add up all those kind of 100 chance, what the performance will look like, what the user experience will look like. And is this kind of really designed for massive adoption? So it's a very tricky question.
00:45:43.550 - 00:45:44.058, Speaker G: Especially.
00:45:44.144 - 00:46:24.914, Speaker H: Because this can only happen with Ziki proof and real time Ziki proof. So there will be also some dependencies when you have this atomic compatibility. Because basically imagine I want to swap on the other wall, I need to send a transaction initiator on one chain, swap on the other chain and send back to my own chain. And then the ZK proof also requires state that is dependent for each transaction. So you need real time proving for each transaction it's not just block level if you want to ignore all the automatic assumptions. So there will be a lot of dependency issues on actually implementing such a system. But again, we are still working on research on this direction.
00:46:24.914 - 00:47:06.770, Speaker H: But I just want to bring a totally different view on not just economics, but actually from massive adoption, how you build a really high performance system and what the future will really look like. If you want the whole blockchain system need to handle like 10 million transactions per second. And is that like several relatively high performance system and you really limit what the interoperability look like? Maybe just async compatibility, which is more practical or you want to enable this which will influence the design for each row up. But yeah, those are some of my concerns. Just bring a different view on how we are thinking about that. But we are also working on research to kind of how we can solve this problem. I think Mohammed might have some write up, but yeah, those are some of the concerns.
00:47:09.130 - 00:47:27.090, Speaker A: Perfect. Thank you. Ye. I guess ye's talk was the first talk that was not specific to base sequencing, but applies to any shared sequencer. Do you have any clarifying questions from the audience, Ben?
00:47:27.670 - 00:48:09.678, Speaker F: I feel like I'm speaking a lot, so if somebody else has a question, you should go first. Okay, so thanks for the presentation. So I guess two related questions. One, you sort of presented as we should either decide to limit interoperability for everyone and have better performance or not. But couldn't you imagine that there would be a design space for roll ups and some roll ups would choose to interoperate at the cost maybe of throughput and others, maybe they're only servicing some very high throughput games that don't need interoperability and then they just choose not to.
00:48:09.844 - 00:48:46.938, Speaker H: Yeah, that's a good question. So just clarify. So I think they are definitely like design space. And to allow this happen, if some row up opt into such system and allow economically they can interpret each other. Because if you really think about long term endgame for ro up, how many row up will be there? There might be very general purpose and why people are using that. It's because they are general purpose, they have enough liquidity and then they can process enough transaction. So I guess most of the well adopted row up in a very longer term will be high performance enough so that they actually need more like a synchronous compatibility that influence their performance less, influence their liveness less.
00:48:46.938 - 00:49:14.338, Speaker H: And then there will be maybe some smaller routes and then they can kind of allow this to happen, but with totally different assumptions. So I think that's totally fine. It's totally possible to have some routes like denying that. But if you really think about long term, like what the kind of 10 million transaction per second the whole system will look like, how many general purpose will look like, what's the requirement for each row up and should we allow this to happen? So it's more from a very long term endgame for large general purpose roll up.
00:49:14.424 - 00:50:21.180, Speaker F: Well, also it seems to be it's a decision that roll ups can make to allow cross chain message processing, even if they are part of the Ethereum shared sequencer or not. In other words, some pre confirmations don't have to do with cross chain messaging, they just have to do with making promises over atomic guarantees. I want my transaction to execute on two different roll ups. I don't want to actually pass messages. That's something especially coming from a ZK roll up. Do you see like you talk about these concurrent systems and parallel processing as being very important for efficiency of processing tens of millions of transactions, but to what degree is this concurrency really getting around the bottlenecks, especially for, I guess, ZK roll ups where you're doing so much computation, I mean so much computation, to produce a proof relative to this database processing that I'm wondering how you see when we have tens of millions in transactions and the ability to produce proofs for all of them that.
00:50:23.230 - 00:50:23.594, Speaker E: The.
00:50:23.632 - 00:50:30.938, Speaker F: Interdependencies that will be caused by cross dependencies between these databases will create a bottleneck.
00:50:31.114 - 00:51:35.460, Speaker H: Yeah, that's a good question. It's definitely like for prover, you require way more computation, but it's very easy to kind of also offsource to another decentralized proverb marketplace and then making that highly parallel, and then finalize later using optimistic, like, firstly, you commit data and then you later handle this. But this will actually definitely become like state growth and all those stuff, how you process your transaction. Those things will definitely become a bottleneck when you grow, really, let's say go over like 10,000 transactions per second or something like that. So it's definitely become bottleneck at some point where we reach the level of massive adoption, because everyone talk about welcome 1 billion people into ecosystem, into Ethereum, but if you really think about 1 billion people, everyone send one transaction per day, it's already required over 10,000 throughput just for payment only. So it's like really, if you imagine blockchain is really getting massive adoption, instead of just playing like a toy where you have ten or 100, that will become a very.
00:51:36.550 - 00:52:08.394, Speaker F: Maybe. Just to clarify, if 99% of your work is in the proof, then it seems like proving is what needs to be distributed. And if 1% of your work is in constructing the state changes that need to be proved, then it doesn't seem necessarily hard to construct. If there's a single proposer who's proposing for multiple roll ups, it could calculate for each of these roll ups what the state changes. That's 1% of the work, and then 99% of the work needs to be distributed. I guess that was the point.
00:52:08.432 - 00:52:36.658, Speaker H: Yeah, I don't understand that. But even that, basically the problem, even you allow very complicated machine to run that, allowing this kind of synchronic compatibility in a distributed system. It's a bad design from how you design high performance system. And that's basically a conclusion from the literature of another space, and which we are working on some benchmark and looking to some practical evidence. But yeah, we can discuss offline.
00:52:36.834 - 00:52:43.960, Speaker A: Perfect. Thank you. Yeah. Next up we have Alex from Zksync, but I'm not sure he's on the call. Alex, are you here?
00:52:44.410 - 00:52:46.570, Speaker F: I think Alex is on a flight.
00:52:47.390 - 00:52:58.990, Speaker A: Oh, okay, that explains, great. So we have time for the last talk as well. Alex watts on pre confirmation based market manipulation.
00:53:00.290 - 00:54:01.060, Speaker C: Hey, everybody, my name is Alex. I am the founder of Fastlane Labs. We're an mev company focused on validator, aligned mev on polygon and app aligned mev on pretty much everywhere else. I wanted to go over one of the potential attack vectors, and I find this attack vector to be particularly nefarious because it's not one of the most intuitive ones. The way the attack vector comes about is when we start looking at pre confirmations as including information on the result of the execution of a transaction. And so if we look at splitting the hold on, Zoom is disrupting people. So if we look at splitting the value of a pre confirmation into two components, you're looking at the inclusion being one and the information being another.
00:54:01.060 - 00:55:21.910, Speaker C: There is a lot of value in that second component. Efficient users, power users, think stat, arp teams, et cetera. What they will try to do if they want to be as efficient as possible is cram as much information requests into the return data of their pre confirmation, such that when they are getting back this low latency, like very high speed pre confirmation result, they are going to have a lot of visibility on the new state net of a series of other pre confirmations that would have been happening before, which would also be very latency sensitive. So what this does is it opens the door for a few different attack vectors. The main one that I'm going to focus on today is the fact that people's actions, especially for bots and other automated traders, are often a result of other prior actions that happened before. For example, if a bot wanted to purchase a thousand JD coins on an Amm, then they may or may not then sell a thousand JD coins on a centralized exchange based on the result of that pre confirmation. If an oracle updated successfully, then because they know that the oracle updated successfully, then they won't do another update.
00:55:21.910 - 00:56:21.978, Speaker C: If a swap reverted based on the pre confirmation, then they might initiate another swap on a different amm. If a balance increased. If a balance threshold is met, then maybe an off chain auction ofa mechanism might match an intent to this user whose balance increased, because now they have enough of a balance to actually fulfill an intent. And so what I want to focus on is the fact that preconfirmers can actually attack users without violating any sort of inclusion or sequencing guarantees solely based off of giving them incorrect information about what happened to their transaction. As an example, let's take a user who wants to do a swap. Let's say this is a statar team of some kind. So they send in a transaction to do an arbitrage swap, and as part of the return data they want to get the rates of the pools after the swap.
00:56:21.978 - 00:57:22.334, Speaker C: What was the price impact? What was the success, what's the current rate now? And maybe they want to do another swap afterwards. And so the pre confirmation or the pre confirmer could take this swap, execute it exactly when they said they would like sequence it correctly, but then lie about the return data, they could tell the bot, hey, your swap reverted. Sorry. But by the way, these are the current rates based on the state request that you put into your transaction, knowing that the bot would then see the current rates and think, oh my gosh, there's an arbitrage opportunity. Still, I should submit another transaction. But in fact, the bot's transaction did go through, so now they're being artificially tricked into doing a second transaction, a second swap in the exact same pool that the pre confirmer is telling the bot is a good pool to swap in, which the pre confirmer now is able to sandwich. It essentially knows how the bot will react.
00:57:22.334 - 00:58:07.194, Speaker C: Because it's a bot, it's predictable. That's how bots work. And so the pre confirmer can actually, like very high confidence sandwich the bot based just off of the return data that it gives to the bot. And that is a very tricky thing, because it can do this without violating any sequencing guarantees whatsoever. It can stick to the entirety of the sequence, no problem. So there's a solution. You could have the pre confirmer or preconfer sign the return data, but then you start running into some issues with latency races, such as how do you know that the return data is fake? One of the ways is you could actually have a PTP gossip network.
00:58:07.194 - 00:59:13.358, Speaker C: You could have return data get gossiped among a set of peers. But keep in mind that you have to prove a positive rather than the absence of something. And so in order to detect some sort of equivocation or fake return data, even if the pre confer is sending out the real data, which they would presumably do, so they don't get slashed, they can even try to trick that. They would send out real pre confirmation data to all of the other nodes, and they could send out the fake pre confirmation data to the user, which, by the way, in the mev world it's fairly straightforward to identify the IP addresses of all the major players. So I think we should assume that they can be detectable, unless we're looking at the encrypted PTP layer, which I know a lot of you guys are working on, which is pretty cool. But anyway, I say all this to say that it does become a latency race, and it would be trivial for the preconfer to delay propagation of the real pre confirmation result until after it knows that the target, the victim, sees the fake data and has time to respond to it with another real transaction. There's a lot of different potential attacks.
00:59:13.358 - 01:00:14.054, Speaker C: It's not just the sandwich. Some of them are quite nefarious. And importantly, if the preconfirmer knows that it's going to be slashed, it can just stack these up. The slower the nodes in this p two p gossip network, the more vulnerable they're going to be. And the majority of the solutions to this problem have significant impact on scalability or throughput has been, and Ellie has been talking about a lot. So I'm going to skip that part of the slide deck because you guys hit pretty much every point that I was looking at. The other huge issue, though, is that if you did want to have a user validate it in real time without having to rely on a gossip network, and you do have these pre confirmers signing off on the hash of the return data, as well as the hash of the previous hash of return data, then you would need each user to have full visibility on all of the transactions that came before it.
01:00:14.054 - 01:01:09.062, Speaker C: That also has some scalability issues. That has huge issues and impacts for crosschain and interrupt. And it kind of also really incentivizes vertical integration because it's quite difficult to price this kind of thing otherwise. The biggest issue is that you can calculate whether or not something like this would be profitable before you even do the attack. And so when you're running into insurance and insurability for something like this, you generally want to make sure that you don't have a hugely adverse selection component where the attacker would only do the thing you're trying to insure if they know that they are profiting more than the insurance would cover. So it does make pricing insurance quite difficult. Collateral dilution could also get reused excessively.
01:01:09.062 - 01:01:29.200, Speaker C: But yeah, that's the general idea. I know that there are some sequencer specific mitigations that could be used. I'm super excited to learn more about those. But I think in general, this is something that we should definitely keep an eye on as we dive deeper into this, the validity of not just the sequencing, but also the result.
01:01:30.210 - 01:01:50.790, Speaker A: Perfect. Thank you, Alex. I guess Alex's talk was the first talk that was even more specific, I guess, or I should say even less specific because it applies to all sequences with these types of pre confirmations, not just base sequences or shared sequences. We have a couple hands up and then we'll close the call. Kartik and then Noah.
01:01:51.610 - 01:03:12.450, Speaker I: Yeah, thanks, Dogard, for presenting this. I didn't really think about this as an attack vector, but it definitely is. That being said, a few things to push back on one given your construction, like your initial solution of just having them assign and then imposing slashability conditions on if they sent the wrong data, which is easily traceable after the fact. I don't think that the premise of them only taking these opportunities when it is profitable necessarily holds because the actual pre conference themselves is kind of doing like a stat arb in and of itself, right where they're sending incorrect data, but they don't have confirmation that the actual MeV bot is going to act in this data in a way that's going to be profitable for them. So I think the dominant strategy for the bot would be, okay, let's opt into both the preconfer signing whatever the post a route is, or like whatever transaction data I want after my swap executes, and then also wait to hear from x other nodes who have gotten this gossip to them if there's any disagreement that I can slash him, if there's no disagreement that I can continue with my swap accordingly, and then that's just negative ev for the actual pre conference in this setup.
01:03:12.870 - 01:03:44.380, Speaker C: So I would push back on the first part. I do think it would be very high conviction for the attacker to know ahead of time what the profit would be. The way that you can calculate volume for stat ar like this is looking at the liquidity of the pool as well as the spread, like the rate delta between Binance and uniswap. And so there's not that much drawing outside the lines of all the major market makers. If we're being honest.
01:03:45.070 - 01:04:07.650, Speaker I: I don't think it's on the actual pricing of it. I think it's the uncertainties coming from is this player going to execute how I think he's going to if he's running a naive bot that immediately sees this and trusts it, or is he going to wait a little bit, get confirmation from another node that, okay, this is actually correct. If it's not correct, then he's going to slash. If it is correct, then he'll execute.
01:04:07.990 - 01:04:12.100, Speaker C: But if we're looking at like a first come first serve thing, then anyone who waits will never win.
01:04:13.190 - 01:04:22.818, Speaker I: Well, I think this will just be the dominant strategy in that thing. And then you're only waiting like one network hop, which is minimal latency, maybe like ten milliseconds.
01:04:22.994 - 01:04:30.540, Speaker C: Right. But there's going to be someone that doesn't want to wait and actually wants to get the money, and you wouldn't be able to know ahead of time.
01:04:31.390 - 01:04:35.820, Speaker I: Sure, I guess that'll just get priced in, maybe.
01:04:38.030 - 01:04:49.990, Speaker A: Alex, thank you so much for your presentation. Noah, if that's okay, we'll move your question to the post call discussion. Thanks, everyone. We're closing the call here and chat. Bye.
