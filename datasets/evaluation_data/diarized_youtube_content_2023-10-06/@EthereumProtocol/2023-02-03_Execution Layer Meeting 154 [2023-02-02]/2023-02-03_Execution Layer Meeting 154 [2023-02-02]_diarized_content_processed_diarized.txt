00:02:49.800 - 00:03:55.850, Speaker A: So hi everyone, welcome to Alcor Devs one, five, four couple things to cover today. So some Shanghai updates. We had a new testnet launch earlier this week and then it's worth talking about how we go from here to the existing public testnets and whether clients are ready to start looking at that. Then on the Cancun side there was a lot of discussion around 4844 in the past week or two, so there's a few things that make sense to go over today. I thought I had added this to the agenda. So there's three things there around the transactions with zero blobs, transaction pool spec, blob block coupling and then there's something else. We've had some benchmarks done for the pre compiles so we can go over those as well.
00:03:55.850 - 00:04:49.720, Speaker A: Then Matt had an execution API PR that's been open for a few weeks. He wanted feedback on and to wrap up. We've also been discussing SSZ, sorry, quite a lot these past few weeks and there's a couple proposals there. So maybe just to start, anyone from the DevOps team want to give us an update on the new Chapella testnet? Sure. Hi everyone so we launched the young testnet yesterday at 03:00 p.m. UTC time and we have 61,000 preceded validator. 58,000 of them is run by the EF currently and vouch and attestant is running 2000 more and we have another thousand keys to allocate to different projects.
00:04:49.720 - 00:05:48.492, Speaker A: We still have about 400 keys to give out. If any client team is interested just let me know. And we have fully working faucets and beacon chain and transaction explorers. So block scout and beacon chain both have a working explorer up and running and we have a launchpad instance and all of this information can be found on the link I posted below. Currently we are not able to process any new deposits because we have some trouble on loading new deposits into the beacon chain, but I'm sure it's going to be sort out in a few hours or days and we will have capella on Tuesday at 03:00 p.m. UTC time. Got it.
00:05:48.492 - 00:06:24.090, Speaker A: And then one question so you mentioned that you have about 400 validator keys. Is that the only way if somebody has a validator and they want to run through the transition? No. So we will have the launch pad and we have faucets also. So you will be able to get some 33 e at each faucet request. And to have that you can go through the launch pad exactly the same way as you would for the main net or for any other testnet. Awesome. That's great.
00:06:24.090 - 00:08:08.920, Speaker A: Anyone have questions comments on that? Can you quickly explain what the issue is? I'm not really familiar with how new voting is taking place on the consensus layer, but maybe one of the client members can explain because we've been having some discussion in the interrupt channel about this. Basically, we've been waiting for a long time to have some new deposits included, but they just seem to be voting the wrong head except lighthouse. So generally there's a mechanism that some follow this since they're following the El and vote on the deposit route and process deposit setting. This has definitely proven to be brittle, especially when we're configuring new networks from time to time. So I would suspect it's likely configuration given that all these clients handle this fine on main net, but I don't have details beyond that. Okay, anything else on the new testnet? Not really. We had one short discussion about whether BLS changes should be allowed or not by different tools, but it's not related to all core devs.
00:08:08.920 - 00:09:16.816, Speaker A: Got it. Okay, then I guess I'd be curious to hear from some of the client teams how they feel about the general readiness of their Chappella code, and when it makes sense to potentially consider forking some of the existing public testnets. So whether Gordy or Sepolia, and if people have preference around the ordering of those two. Not that we have to make a final decision today, but I think it'd just be good to put this in context where the different teams are at and how they'd like to approach the existing public testnets quickly. On an ordering standpoint, I think it makes sense to do Sepolia before Gorely, given that Gorely is used by much more validators outside of a limited set, and so that gives a bit more time for documentation and tooling to be more robust. So just from an ordering standpoint, that's my preference. Yeah, that would be mine as well.
00:09:16.816 - 00:10:04.460, Speaker A: Does anyone disagree with that? Think that we should do Gordy first? Okay, so assuming we go stepolia to Gordy. Yeah, I'd be curious to hear from client is are you close to a spot where you could put out a release to fork Sepolia? Not quite. Any other concern about public testnets. So from netherminite side, we don't see any major issues. We have a few things to polish, but I don't see any blockers now. Okay, speaking of Teku, we're good to go. No blockers.
00:10:04.460 - 00:10:31.616, Speaker A: Awesome. Same with Besu here and agreement on Sepolia first cool load spot is also good to go. Okay. As well as ethereum js for testnet. Sorry, you said also ethereum js, is that right? Ethereum js for testnet? Yeah. Okay. We are not on minus.
00:10:31.616 - 00:11:18.130, Speaker A: Yeah, on the prism side, we're also good to go. Okay, I can see. Please. Yes. Geth is like, I'm pretty confident in our code. I don't know, having a testnet breaking this close to fucking the test nets is something that we should really look into. And I hope that from my point of view, it looks like a consensus layer problem, but would be really good to look into that.
00:11:18.130 - 00:11:45.336, Speaker A: Otherwise our client should be okay. So to be clear, that network has not gone through the Chappella upgrade, is that correct? That's correct. Okay. This is a configuration only. Chappella is going to happen Tuesday next week. If we can get everything up and running by capella, I think we should be still very good to go. Yeah.
00:11:45.336 - 00:12:36.290, Speaker A: I'm going to lean towards calling this a configuration issue and not a Chappella issue until proven otherwise, but agreed that we need to figure it out. So I would like to ask all client teams to try to give me a hand and try to figure out what kind of configuration this might be. Because at this point I'm not really sure. Right. And obviously I think it would make sense to see Chappella work well on this testnet before we put out releases and set a specific date for Sepolia. But yeah, assuming it's just a config. Yeah, that's obviously not the same as an issue with the actual fork transition.
00:12:36.290 - 00:13:09.520, Speaker A: I think next week, Thursday, we will have a much better oversight on how things are going. That makes sense. I don't know if anyone from Aragon wants to share their view. I'm sorry. So what's the question? How ready are you guys for potentially four k in Sepolia? Yeah, I think we are ready. That's Shanghai. Yeah.
00:13:09.520 - 00:13:39.080, Speaker A: Yes, I think for Shanghai we have everything implemented. Okay, let me look at my list here. So I think that's all the el folks and I think we're missing lighthouse. I don't know if anyone's on the call. Yeah, we're good as well. Okay, I'm missing one client. Anyone didn't go? Sorry.
00:13:39.080 - 00:13:49.580, Speaker A: Maybe Bezu. No, we got Bezu. Nimbus. Oh, Nimbus. Yes. Thank you. From what I've heard, everything is ready.
00:13:49.580 - 00:14:40.714, Speaker A: Like feature wise. We are still doing some optimizations on BLS to execution changes. And I think then that's it. Okay, so I guess assuming that this fork on the new testnet goes smoothly, and we'll know that Tuesday, then I think it would make sense on Thursday to agree to a fork epoch for Sepolia. And then if we could announce it midweek, the week after, it gives people sort of three, four days to put out a release for usually, you know, we can probably fork Sipolia like a week after that just to give folks some time to upgrade. But it's a pretty small set of validators on the network. So I guess just.
00:14:40.714 - 00:15:19.720, Speaker A: Yeah, from client teams. If you can be ready to put out a release shortly after next week's consensus layer call, I think that would be great. Again, assuming we don't find anything going terribly wrong with this fork on the new testnet, does that make sense to people? Cool. And then, yeah, I guess if you're a staker, node operator, infrastructure provider, listening please. This is a time where you need to start paying attention. So we'll have a fork on Sipolia. You can join this new testnet if you want to try things before.
00:15:19.720 - 00:16:21.692, Speaker A: And I think once Sipolia forks we can start talking about Gordy and how we want to handle that one. Anything else on testnets or Chappella generally? Okay, so cancun, lots of work has happened on four, four, four in the past couple of weeks. There's a couple of things that sort of made sense to bubble up here. And maybe first, I don't know if Radek is on to talk about the benchmarks. Yeah, Radek. Hi Tim, that's actually me. Okay, sorry.
00:16:21.692 - 00:16:59.198, Speaker A: I will take for the benchmarks. Let me share the screen with you guys first. Here we go. Can you see it? Yes. Okay, so, yes, some time ago we got this lovely little leppo. The point of this leppo was to track some benchmark data across different implementations and across different versions. So what was started off here? Let me just show you here.
00:16:59.198 - 00:18:13.430, Speaker A: Well, I'm not sure if you guys have seen it or not, but let me just go quickly through it. So, different pre compiles and tracking the times of execution on different machines here. The point of executing precompiles was actually to see if the gas cost of it is accurate. How do we know if it's accurate? Well, we took, or maybe let me show you the latest Shanghai version of it. No, not this one. Stantinapol maybe? Yeah, I just wanted to show that all the results are normalized to one of the pre compiles, which is EC recovery. EC recovery being the most standard one.
00:18:13.430 - 00:19:20.010, Speaker A: And if we look here on the last column, this actually tells us what would be the cost of that pre compile execution if we normalize it to eclic over the time. Right. Maybe this is going to be more clear if we look at this work that we actually have. So, the problem with the current state of this lipo is that we only have go ephedium as a client, and we didn't track all the versions and all the precompiles, and there's no way to compare them. And that's exactly what we did here. So that's the selection of the pre compiles that we decided to track, executed them on some reference machine. Being my laptop, it doesn't really matter what machine it is.
00:19:20.010 - 00:20:18.814, Speaker A: All we want release to compare timings and the ECD cover precompile is the basis, and all the other precompiles are tested against the EClic. So the first benchmark we've done is actually duplicating the work that was already done. And that's something we called a direct execution. So we take an appropriate method in goafilium, execute it. In case of goafilium, this is a method called run pre compile contract. We execute it, measured the time using some benchmarking library. We got this and we did it for most of the pre compiles, calculate the times or normalized them to eclipo and.
00:20:18.814 - 00:20:49.000, Speaker A: Okay, let's take some examples here. The bn two, five, six pailing time that the nominal cost. So the current cost is 113,000 and the calculated cost is 158. So close enough, that's not too bad. The same for more. So the nominal cost is 6000, calculated 6200. That's actually very good.
00:20:49.000 - 00:21:52.842, Speaker A: The main point of the exercise was actually to try to estimate the cost of the new point evaluation pre compile. So for the point evaluation, we have two failed tests and one valid test. So this implementation, all the pre compiles for the point evaluation are valued at 50,000 at the moment. The fail test, actually in this implementation, they complete quite quickly, and the calculated cost is 9000. The valid test is 76,000. Right? So now it's decision whether we need to change that nominal gas cost to adjust it to something more like 76 or leave it as it is. But that was the first step with go ethereum.
00:21:52.842 - 00:22:50.826, Speaker A: The second step was to say, okay, these executions are done in more direct way. What we want to do is to run them in more real life scenario. So for all the precompiles, maybe I show you here for all the precompiles. We've created simple contracts, which all they do is they push some parameters, known parameters, and then call the pre compile. And then we execute that bytecode and we benchmark the bytecode execution, which is something more deal life. So again, go ephedium case. Let's take maybe point evaluation.
00:22:50.826 - 00:23:22.970, Speaker A: So the direct. That's bad choice. Let me start with this one, the padding one. The direct execution was maybe that's, again better choice because we have had multiple paddings. You see the COVID That's the good one. The direct execution was 48 milliseconds on my machine. The bytecode execution was 70 milliseconds on my machine.
00:23:22.970 - 00:24:20.570, Speaker A: The point evaluation is something similar. I said it's a bad choice because for some reason, the bytecode execution was slightly faster on my machine than the Diodex execution. It's weird, because in the bytecode execution, we expect all the EVM machine overhead to be also calculated. But, you know, benchmarking is not exact science, just measuring some timings. But anyway, having the bytecode execution now, we said, okay, the same bytecode, we can execute against all other clients and measure the time. And that's exactly what we did. So we measured the same bytecodes against Nethermind, Erigon and Bessu.
00:24:20.570 - 00:25:18.510, Speaker A: And then at the bottom of the page, there's a small comparison between clients. For the GAF, we also got a different implementation of point evaluation. Pre compile also measured it well, we can compare it. Of course, it's not a kind of a competition between clients. It's more like to know that the gas cost estimation is what we actually pay in terms of the computer resources. In here you have. Also in here, the specification tells you exactly how it was executed and which version of clients we used.
00:25:18.510 - 00:25:54.646, Speaker A: And that's it. Yeah, thanks. And I guess, do you maybe want to quickly go over the cross client comparison for the point evaluation? Pre compile directly, and then crab. Let's go here. So the top line is the actual timing. The bottom line is the calculated gas cost for that specific client. Got it.
00:25:54.646 - 00:26:20.290, Speaker A: Thanks, Denkrad. I see you've had your hand up for a while. Sure. Yeah. I just wanted to comment that I don't know why we're doing those benchmarks with Gokz when we want to use the snark KCG. Sorry, the snark. Yeah, we can definitely rerun the benchmarks with snark.
00:26:20.290 - 00:26:48.954, Speaker A: Yeah, that would be good. Probably like, wasting time discussing these results. Yeah, that's definitely something that's doable. And this is why we had the libraries using the specs. But now that we have the infrastructure, it's, I assume, pretty easy to just rerun it. Oh, it's already there actually, with guest for the gnark we have for the. Yeah.
00:26:48.954 - 00:27:27.132, Speaker A: Nice. Wow. That is. Any other questions, thoughts? And I guess that the original kind of motivation to do this was to figure out whether the 50,000 gas price for the point evaluation pre compile was reasonable. It does seem that's correct, yeah. I don't know. Looking at this as a non expert, it seems like none of the clients have an execution time.
00:27:27.132 - 00:28:05.680, Speaker A: That would imply a gas price of higher than fifty k. Is that correct? That's correct. Yeah. I don't know. My takeaway would be that we've probably priced this correctly, but does anyone disagree when looking at this? Okay, well, yeah, thank you for walking through this. The repo is obviously available, so I've linked it on the call agenda and shared it in the chat here. If any client teams have small tweaks they like to see or whatnot.
00:28:05.680 - 00:28:55.808, Speaker A: Yeah, let me know and we can look into getting those done. Okay, cheers, guys. Yeah, thank you. Okay, next topic. Also, four four four stuff. So the zero blob transactions. So the idea here is, should four four four transaction that does not contain blob data be valid? Basically, if it's not, where should the sort of blocker be? Do we want this as a consensus rule? Do we just want clients to disallow those in the mem pool? I know Peter had originally kind of came up with a transaction pool spec that had some implications there.
00:28:55.808 - 00:29:31.464, Speaker A: Then. Yeah. Ensgar Denkrad, you both have some comments on the agenda. So Ensgar let you go. So, yeah, I think basically the concerns were that on the mempool side, it is pretty hard to have logic for zero block transactions because a couple of assumptions that otherwise you can make for block transactions are basically no longer valid. The question is just, would we simply want to exclude them from mempools, but still have them be valid? In that case, they could still be included by a relayer bud like system or not. I think there was some disagreement.
00:29:31.464 - 00:30:13.882, Speaker A: My personal preference would be to still allow them on the consensus rule side and only ban them from mempools. But at the same time, I think just given that, I think our priority is just to make compromises and get four out of the door as soon as possible, I think there wouldn't be any big harm in restricting them on the consensus rules level as well. For now, we could always lift that ban later on, but, yeah, I'd prefer to only do it on the mental side, if possible. Yeah, I pretty much agree with ant scoundus. Okay, Lucas, you have your hand. Oh, sorry. Then grab, please.
00:30:13.882 - 00:30:54.354, Speaker A: Yeah. No. So, from my side, I don't. Okay. But the potential problem is that we would have to have specialized logic if we want to include them in case of reorgs, which we now have. There's a question. Do we want to keep that logic in general, that we reorg the transactions back to Mempool, and then we have become to the same problem of having specialized cases in mempool, in my opinion.
00:30:54.354 - 00:31:47.350, Speaker A: There is also a question mark on the general UX user experience and usability. Because if we mark those transactions as blob transactions, they have, in my opinion, special domain, meaning, and they are easier to reason about in all of the cases, if they went to block, if they are only in mempool, et cetera, et cetera, that they have blobs and they have special treatment because of this. And if we allow zero blob transaction, this reasoning is broken. And then you have to also inspect how many blobs they have in terms of the rules that was used for this transaction. So, in my opinion, it just makes the clear boundaries what is what. And that's why I'm for banning those kind of transactions. Thanks, Vitalik.
00:31:47.350 - 00:32:57.840, Speaker A: Yeah, one thing just worth mentioning is that this does kind of intersect a bit with the discussion that we're going to have later in the call on changing to SSZ. Because, like, my proposal, for example, does end up banning zero blob transactions as a side effect, but introducing a new transaction type for those. Got it. Denny, just a quick comment on the UX that we necessarily want to provide with the mempool. I think providing, if someone's blob transaction was in the mempool, providing them with the faculty to have it be back in the mempool upon a reorg is probably a reasonable UX. Whereas I think if they bypass the mempool and are using special faculties and have zero blobs, or blobs are never seen in the mempool. Reorganizing and thus putting those back in the mempool kind of requires a number of abstraction breaking changes to the engine API, and also seems like an extension of what we actually need to provide.
00:32:57.840 - 00:33:49.454, Speaker A: These people were bypassing the mempool in the first place, so I don't think we should design our systems to put them into the mempool in the case that there's a reorg. Nonetheless, I know that's not the direct comment, the direct point of what we're talking about here, but it was brought up. Thanks, Lucas. So, but then you have special cases, right? Some of things you now put to mempool, some, you know, don't. So it's fine, right? But it's like why? So if I can reverse the question and why zero block transactions are helpful and useful? Because that's unclear for me, why it would be good to have zero blob transactions decad. So first, I mean, you'll have special cases anyway, because you also don't want transaction with several blobs in an info. At least I think we should have them.
00:33:49.454 - 00:34:49.042, Speaker A: And why do you need them? I don't think there's any strict case for needing them, but I also don't think there's a good reason to exclude them. And it might simplify someone's logic. Say if like a roll up builds a pipeline that handles one type of transactions for everything, and maybe there are some cases where they just want to push a new state route and no blobs or something like that. So I can't see a good reason not to have them, because everything just works. It's simply a case of executing everything with zero blobs. So my opinion is fine, but say, just in your example of the roll up infrastructure, I understand they might want to use the same address, right? Like the same signing key, basically for those transactions. But why couldn't they sign like a type two transaction instead of a type three transaction to do the statement right now, that would mean they would also need infrastructure for LP transactions and all that.
00:34:49.042 - 00:35:46.802, Speaker A: So it is a difference. I don't think this is a very strong argument, but I also just don't personally see any argument for not allowing zero. Got it. Vitalik? Yeah, I just wanted to also point out that in the case where we do not end up accepting either my proposals or some of the other more detailed proposals that include a new SSD transaction type. If we don't include those, and we also mandate that blob transactions have to have at least one blob, then we lose an opportunity for regular users to kind of get acquainted and set up their infrastructure and start using SSD transactions more quickly. Right? Yeah. My understanding is that we will go with having another type of SSD transactions.
00:35:46.802 - 00:36:26.240, Speaker A: So that's why when it simplifies for the users, right. They just specify different type and it works mostly the same. Right. Okay. I guess I feel like it probably makes sense to have the SSD discussion and then come back to this. Does anyone feel like we need to make a decision right now about this? Okay, so make sure to come back to this. After f Sazi, there were just some more four, four things before I think it makes sense to talk about.
00:36:26.240 - 00:37:32.610, Speaker A: So something that came up in the 4844 call this week is that with these new blob transactions, obviously you need to do some different logic in the transaction pool. And it was unclear what's the best way to test that, or to have clients know that their implementation works in that. We obviously say that zero blob transactions are blocked at the consensus level. That's fine, we can test for that. But there will be degrees of freedoms that clients can have to test their transaction pool or to implement their transaction pool. And is there like something like a test suite or something like that, that we'd want to have to make sure that even though all the El clients might have slightly different transaction pool implementations, they know that it meets whatever sort of requirements they should have. And this feels like a bit different than all the other testing infrastructure we've had before, which is more focused on consensus.
00:37:32.610 - 00:38:28.040, Speaker A: So, yeah, I don't. Does anyone feel like there's value in having a cross client thing for this? And if so, what do you think that would look like? This kind of thing is hard to test in a cross client manner because it's very nondeterministic as to like, it's very stateful for each client and what they're going to do. I don't know. There might be some kind of testnet scenario stuff that we can do in a given amount of time. Under certain scenarios, transactions are included. Thus the mempool must be working. But other than that, I don't think it's an easy thing to test in a terministic way.
00:38:28.040 - 00:39:19.190, Speaker A: Right. Maybe it's more a case of just having a setup where we can run spam the transaction pools many times over and almost more like fuzzing the transaction pools could. We probably have a hive test on that, because we can use engine API and just a network of El clients that are exchanged with these kind of transactions. And then after some time that we expect these transactions should be included in the payload. We just call the process to build the payload and check whether they are included or not. And probably this kind of approach could work. Yeah, but we need some strict rules that we can enforce in this test, which may come out of the specification of this tool.
00:39:19.190 - 00:40:09.860, Speaker A: Right. Another option is also to say, obviously clients will have unit tests and will test their specific implementations, but maybe it's fine if there's discrepancies in behaviors. Yeah, it just feels like it is kind of something out of protocol, which we're adding some sort of design constraints on. And it would be good if there was a way to know that the implementations roughly worked. Maybe shadow forks are sufficient for that. Yeah. It's also unclear to me how much of this sign document makes it into a must on specification.
00:40:09.860 - 00:40:50.710, Speaker A: Some of this is just like the quality of service you want to provide to users under various scenarios that traditionally hasn't really made it into specifications. So I'm hesitant to put it into joint testing. Okay, fair enough. Does anyone disagree with some of that? Like the DOS considerations and other types of things clearly are put into specs, but like how to handle reorders and stuff? I don't know if there's precedent for having that unspecification. Yeah, this approach definitely has its own restrictions. It only could be tested up to some limited extent. Agree with that.
00:40:50.710 - 00:42:24.222, Speaker A: I mean, Marius, to your comment about not accepting blobs, I guess you could do this today with not accepting 1559 transactions as a client, right? You just provide like a worse product. Yes, but if we were to not put it in the spec, then this would be something that clients could do. And I think there might actually be value in not having this, not accepting block transactions. Obviously they still have to execute, they still have to process those transactions as part of the chain, but you're saying there would be value in them not having to handle the mempool part of it, and you just can't submit your transaction through that client, is that right? Yes. Okay. Yeah, I guess it doesn't seem like anyone is strongly in favor of having some cross testing for that, and that it's fine if there is some differences in behavior. So I think we can just leave it at that.
00:42:24.222 - 00:43:47.220, Speaker A: Oh, sorry, Lucas. So I would potentially be in favor, and I think if those transaction type, and this is very special transaction type is in spec, mempool should generally handle them. At least they should be able to if they are not, maybe explicitly configured not to. But I have no idea how to design this kind of test because of the mempool, implementations are so different and it's like no idea how to do that. So we have a few kind of a big suit of transaction pool unit tests, but they look very deeply into the transaction pool. So it's like we cannot really spin them out into something that's available for our clients, unfortunately, I think we could, but this will take a lot of work. Well, and it would probably take a lot of modifications on other clients to be able to have similar structures or similar formats or be able to ingest whatever you output.
00:43:47.220 - 00:44:28.532, Speaker A: Yes, exactly. Well, ingesting isn't really the problem, it's just eth send raw transaction. The problem is more making sure that the rules are followed and verifying this state from the client and getting the state back out of the client to see. In which I think the test could even use dev P two p. Right. And the state to verify would be the produced block that it has those transactions. Right.
00:44:28.532 - 00:45:18.070, Speaker A: That we expect them more or less to have. We could do that, but that would be even more complicated and more brittle. Yeah, like I said, it's quite hard to do that. Okay, so I feel like, yeah, this is here not like the most urgent thing. We can discuss it async, if there's test suites we want to come up with. But yeah, I would assume that beyond just some basic test cases, a lot of the functionality will just be tested by the clients themselves. For now.
00:45:18.070 - 00:46:07.772, Speaker A: Anything else on this? Okay, the other big four for four thing was blob and block coupling or decoupling in terms of sync. There was a lot of conversations about that as well. A couple of comments on the agenda. Anzgar, is this what you wanted to bring up? Yeah, sure. But if you have anything to give. Yes, I was going to ask you if you want to give it just a quick background and then go into your comments. Sure.
00:46:07.772 - 00:47:12.256, Speaker A: So basically, I think the idea is just that clients felt that having the block, including the blob section, be one contiguous p to pn message, had some issues, some inefficiencies, and wanted to split that up. And as part of that split up, there are several different potential designs. And in some of these versions, we would also have to slightly the adjust cryptography. And just given that the cryptography libraries are pretty close to kind of audit, so there's urgency with any changes there, I think. And given some people are skeptical whether we should do this change at all, this is something we should make a decision on soon. I just wanted to briefly say from my perspective, just to basically classify the different types of splits we could do. So basically there's one version where we only split bop section as one continuous chunk from the rest of the consensus block.
00:47:12.256 - 00:47:41.880, Speaker A: And in that case we would not have to adjust the cryptography. But also we only get some of the benefits because that would still be a pretty big chunk. One other alternative, that's the one that is proposed right now, would be to split it on a per blop basis. So we hand each blob individually, and there, I think there would be a pretty strong desire to adjust the cryptography. Otherwise it's a bit silly. You have subnet where you can download individual blobs, but then they come without individual fruits, so you can't really validate them. So that would indeed need cryptography adjustments.
00:47:41.880 - 00:48:33.950, Speaker A: And then I think there's a third option that was also discussed that I personally am in favor of. That would be to basically just treat the blobs section as an opaque chunk of data and just split by some fixed cut off. And in that case, we don't have this individual access to the blobs, but you basically still get the p to pm benefits, and you could potentially extend that later on to the normal concentric payload as well. And it would not require changes to the cryptography. So basically, I think for today's call we should just talk about do we need this change at all? Again, this is mostly something cl teams felt strongly about, and I think other people are typical. And then if we wanted this change, which version do we want to go with? But mindful that some of these do require cryptography changes, and there, the timelines are pretty short. Got it.
00:48:33.950 - 00:49:55.480, Speaker A: I guess there were a couple comments on the chat from you and about not necessarily separating them. Does anyone want to make the case for why we should split them out and we can go from there? I'm pro splitting, but I can make the anti splitting argument. Sorry, let me put this. I was going to say, do you want to start with the pro splitting? Okay, pro splitting. So essentially, by using separate subnets and separate meshes for these individual sliced messages, we can probably leverage more efficiency across the network and essentially more parallelization in propagation of these messages. There's a lot of intuition around this that this is likely the case, especially given our understanding of how attestations and other things work on real network. But whether this is a 20% gain, 80% gain in terms of propagation times, whether this helps in the normal mode or only in attack modes, there's not as clear exactly the benefits that we're buying, other than a notion of this is a more resilient design.
00:49:55.480 - 00:50:44.036, Speaker A: So there's a bit of work being done by Anton on TXRX to put this into a simulation framework, as are a couple of the people looking into simulations on load analysis. So hopefully we get some experimental analysis. But I would say that after Yossik did propose, this seemed like the networking experts, the gossip sub experts on the various teams were convinced by the arguments. Okay, I can add to that. Hey, there's actually a bandwidth reduction as well. So there's what Danny mentioned, the parallel sending. There's some parallel processing to do inside the client that is attractive with this proposal.
00:50:44.036 - 00:52:19.108, Speaker A: And there's also bandwidth reduction due to a technique I tend to call lazy sending, in which you simply don't have to send data to some clients thanks to a gossip sub trick. And that trick becomes a lot more viable when the pieces of data are smaller, which is what we're doing here. We're basically taking the block and blobs and lining up the blobs, each on their own subnets, making them amenable to this kind of bandwidth reduction. And what we've seen on attestations when we implemented this in nimbus is like maybe 30% bandwidth reduction. So it's really attractive from that point of view as well. And another argument here is that although at the current proposed amount of blobs and blob sizes, sending fully might be sufficient for our propagation time and bandwidth needs that if over the next 18 months we want to turn up that data gas limit, that we're likely, we want to have to employ more sophisticated means like this maybe episode and other things, both to help propagation times, but also to help with that bandwidth load. So to get these benefits, how important is it that the isolated blocks that fly on the network are verifiable in isolation? Because that's what causes the cryptography layer changes.
00:52:19.108 - 00:53:18.900, Speaker A: The fact that now we are asked to be able to verify individual blobs, whereas in the previous design, the status quo, we're only verifying all the blobs together with an aggregate proof. So generally the proposer signature, the proposer being a bound unit of one actor, is sufficient for most of our baseline antidos measures. Anything on top of that that you verify, that you can verify cheaply on each hop is good. It's nice to have for blocks, we don't execute everything, but we do verify the proposal signature, and then we do pretty much all the constant time checks. I'd say all the constant time checks are nice to have. It's really that proposal signature that's the core of our antidos. So again, that crypto check, if you can do it cheap, it is a nice to have, but I wouldn't say it's critical.
00:53:18.900 - 00:53:56.470, Speaker A: The signature is critical. I see. Okay. Because one of the arguments during the presentation in the interrupt was that you're able to verify the blobs as they come, so you basically parallelize the band with the computation. There's a potential argument there, but I don't think it's the most critical. Okay. I mean, in that case, I'm closing towards not changing the cryptography and just using the signatures for authenticity and then doing the block verification up on receivable of everything.
00:53:56.470 - 00:54:57.064, Speaker A: I don't have a strong opinion either way. There is some minor performance benefits to having the blobs verifiable individually in terms of pipelining in the client, because now that you're receiving blobs at random times, it's good to get them verified. As much work done on each blob individually as possible before you aggregate them. But it's not a strong opinion. The other one was that it was slightly more easy to specify what the message should look like when they're individually proven, because we don't really have any natural place to put the aggregate signature anymore. It could go in the block, but then it has to stay in the block forever. Or it could go in like blob zero, or you could send it with all.
00:54:57.064 - 00:55:19.170, Speaker A: Or it could go in blob zero, which is kind of ugly. Interesting. You mean the aggregate proof? Yeah, that's a good point. Yeah, definitely don't put in the block, please. Yeah, so I'm weekly in favor of separate proofs. Weekly, but I'm still in favor of them. Vitalik, you've had your head up a bit.
00:55:19.170 - 00:56:34.250, Speaker A: I also just wanted to make the brief point that splitting is probably more future compatible with more radical ways to verify blobs more efficiently down the line, such as erasure coding in the medium term and actual data availability sampling in the long term. It just makes it so that any portion of the network would be able to do it passively on its own without interfering with the rest of the protocol. What's the erasure coding thing that dankard? Also, this is the thing where if, let's say, a particular block has seven blobs, then you erase your extend them and you have 14 blobs, and you broadcast the 14 blobs, and then whenever you receive the first seven of them, you can immediately regenerate the other 14 and broadcast all of them. It actually is similar to a technique that's been used by peer to peer networks in non blockchain contexts, and it can increase speed of propagation significantly and probably also increase vendor efficiency because you have less redundancy or less overhead for the same redundancy in terms of safety. I see. So it's not a four eight four thing, it's a four eight 4.5 thing.
00:56:34.250 - 00:57:16.010, Speaker A: Right, exactly. The point is that this style makes it more future compatible with doing that, because that could be done without changing consensus, or micro league, probably, possibly even part of a network could do it without the other part. Just want to get back to a comment in the chat as well. So, Terrence, you asked whether we had a timeline for the simulation result. I don't know, Mikhail, if you have some thoughts on that. Okay, maybe next week's. Yeah, probably we'll have clear understanding when.
00:57:16.010 - 00:57:52.790, Speaker A: Not that clear, but some progress towards the goal. Right. Okay. And I guess one of the reasons why it was somewhat urgent to move this conversation forward is the audits of the crypto libraries. And I believe they're scheduled for about a month from now. And so if we need to change how we do verification of the blobs, that's quite a short notice and we should do it now. But I guess from this conversation, it seems like even if we didn't do individual verification of the blobs, we might still get some benefits out of this.
00:57:52.790 - 00:58:55.080, Speaker A: So it probably makes sense to wait at least a week until we have the simulations results. But from the perspective of the KCG libraries, not make any major changes. Now, does that make sense to people? It does, and we asked the auditing people about the potential to push the date forward and they said that it should be okay, but we should give them some heads up before. So, yeah, I think next week is totally nice to have results in this. Okay, Mikhail, see you. Question. Why is the audit of crypto libraries? What depends on this decision, because if we change blob verification to be for individual blobs rather than an aggregate, I believe that requires changes.
00:58:55.080 - 00:59:34.980, Speaker A: Yes, exactly, Tim. So there is kind of going to be a different verification function that will do it in this case, yeah. Right now we are verifying all the blobs in aggregate to get some speed boost, because we get them all in one point. But with this new design, we would be getting blobs incrementally. So there is benefits to be able to verify the blobs as they come. Okay, I see. Probably we can chat about this applying.
00:59:34.980 - 01:00:34.596, Speaker A: Okay, yeah, let's do that. And let's bring this up on Sel call next week once we have also some more data on the simulations. Anything else on this? Okay, next up, light client, you've had this on the agenda for a couple of calls now, so wanted to make sure we got to it. But it was adding a new Eth accounts endpoints. Do you want to quickly walk us through? Yeah, sure. Just this is pretty quick. But Martin actually wrote a request into the execution APIs repository in November to add this new method eth get account, and the reason is we've had a few users requesting this type of functionality.
01:00:34.596 - 01:02:07.986, Speaker A: Basically what it does is it takes in some address and it returns the trileaf definition of the account, which is the balance, the non storage route, and the code hash if the account exists. I just sort of want to get a feel for if clients are okay to add something like this. We kind of want to make sure we're moving the JSON RPC forward together in unison with all the clients. So unless there's any strong feelings against it, then we can work on finalizing the spec and merging it soon. Okay, any comments? Thoughts? Does it make sense to give people just today and tomorrow to review this and we can merge it Monday if there's no new objections? I saw Justin had left like a review yesterday, so maybe giving people a couple days from here, but yeah, merging it on Monday if there's no. Yeah, I was just going to say bass is pretty happy with it, honestly. Okay, is anyone from Aragon here? I feel like you guys tend to have different ideas about maybe what the RPC would look like, or maybe this could affect how you retrieve information.
01:02:07.986 - 01:02:53.270, Speaker A: Does it seem okay to you? No, I think this is a simple method. That should be fine. Okay, sounds good. I'll give people until early next week and then we'll get it merged and cut a release for the APIs repo. Cool. Oh yeah. Does this cause issues with vertical tries? Is Guillaume on the call? What was the exact proposal again? It takes a request in which is the user's address, and it returns whatever the trileaf definition is, which right now is balanced non storage root code hash.
01:02:53.270 - 01:03:38.026, Speaker A: I don't know exactly how that would be changed. Wait, so what's the purpose? It just allows people to request the account information. Right, I see. Like request the header information. Vertical trees have a concept of a header, which is not super different, or rather they don't necessarily have the same hash structure, but they do have the concept that there is a field for the nonce, there's a field for the balance, there's a field for the code hash, and so on. Okay. And it's forward compatible.
01:03:38.026 - 01:04:06.698, Speaker A: This doesn't have any hashes. This doesn't have any proofs or anything. So it should be Lucas and then Andrew. So other existing methods like that have this block parameter, parameter that allows you to get value not at the head. Is that considered here? Yeah, sorry. That is here. It's the same format as for the other state retrieval method.
01:04:06.698 - 01:04:47.458, Speaker A: So it takes block tag number or hash. Okay, I need to dig deeper because it wasn't just on the PR conversation. It's not obvious. Maybe it's in actual files. Andrew? Yeah. I think that in Aragon it might be difficult or impossible to return storage route for historical snapshots. So we only have merco, Patricia, tria routes for the current state.
01:04:47.458 - 01:05:36.734, Speaker A: We don't have historical. I mean, we also don't have all of the historical ones either. So I think they would just return an error like missing try note or something. Yeah, we can return balance. Nonsense. Well, code hash doesn't change ever, but unless it's recreated with create two. But we can simply omit storage root if it's not the current state, it could be marked as optional if it's not in the node.
01:05:36.734 - 01:06:02.520, Speaker A: Right? Yeah, it's possible. I think we should just take this conversation to the PR, but I think it's. Thanks. Okay. Yeah. So yeah, let's discuss it in the next couple of days. Ideally we can merge it if there's no strong objections already next week.
01:06:02.520 - 01:07:15.114, Speaker A: Okay, next. And last big topic for today is Shz. Vitalik, you had a proposal and then Ethan, you had like a EIP that was in draft about this, so maybe it makes sense. Vitalik, do you want to start? And then Ethan, you can kind of walk through your EIP draft. Sure. So what motivated this whole research direction is basically the realization that the kind of current EIP 4844 design puts the EIP 4844 transactions in this weird halfway house where they have SSD serialization, they're SSC based, but then their signing route and their hash tree route are still based on a serialization. And this seems like obviously not what we want in the far future, right? Because there is a goal of eventually moving things over to SSE in some backwards compatible way and unifying things under one framework.
01:07:15.114 - 01:09:22.322, Speaker A: And so the proposal that I had is basically we're starting to think through how we can kind of tweak the current EIP in order to achieve that. But then a lot of people at our discussions and the research retreat last week were of the opinion that it might actually be possible and even better, to kind of do a more radical reform more quickly. Basically because this would reduce the total amount of work that would need to be done. And the radical reform, what it does is it basically does like maybe three quarters of the work of moving the transaction part of ethereum over to SSD, right, and over to a pure version of SSD. So where the txid of a transaction actually would be the hash tree root of a transaction, which is compatible with what the consensus layer does, and then the signing route of a transaction would be the hash tree root of the message part, which is also what the consensus layer does for blocks and attestations and all the other structures, it introduces two new SSD transaction types. One, which is basically the existing four, eight, four blob transaction, except the TxID is a hash tree root and the signing hash as a hash tree root of the message part. And it introduces a second new transaction, which is basically the same thing, except it's allowed to have zero blobs, right? So one of the things that people seem generally more in favor of where instead of having one new SSD transaction type and allowing it to either have zero blobs or one or two blobs, you have one type that is specifically for blobs, and it must have at least one blob, and then another, which does not contain blobs, which allows you to basically do everything purely with SSD and rely purely on SSD libraries and hash tree encoding and all of those things to create and sign transactions.
01:09:22.322 - 01:11:45.670, Speaker A: Now for the three existing transaction types, so this is legacy, which based on the v value, covers both, kind of like legacy, legacy and chain id based EIP one five five. It covers EIP 29 30, which added the access list, and it covers EIP 1559, which added the new gas pricing mechanics. For those three transaction types. It basically stores them in the block in an SSZ way, but at the same time it still essentially uses a pure function to convert the transaction from SSD to RLP in order to compute both the signing hash and the TxID, right? So if I send an old style transaction, whichever one of those three types it is, then I could still use old RLP based libraries, I don't have to upgrade. And I would send my transaction, then when the transaction gets into the mempool, it would get converted into the SSD format, it would get included in a block in the SSD format, and then when it gets that confirmation as the TXId, I would still see the same kind of RLP constructed TXID that I'm used to. And so even as a DAP user would not need to upgrade anything on chain or off chain in order to continue to be functional. So that's basically it right, so defining five transaction types, three of which are basically legacy backwards compatibility wrappers and two of which are the 4844 type with blobs and the 4844 type without blobs, storing them in the block in SSD and converting the transactions list in the execution payload away from being an opaque bytes object and into being an actual SSZ list of SSZ objects, and creates these backwards compatibility functions for older transaction types so people who use them could still continue using them the same way without changes or comments on this.
01:11:45.670 - 01:15:01.946, Speaker A: I'm very proud this proposal. I'm not entirely sure if I'm not missing something there, but about this conversion, how it will work, but otherwise, if it does work as intended, it's like silver bullet to have both backwards compatibility as well as go forwards very quickly. The conversion function in both directions is a pure function, and I wrote a Python implementation that even has a test case to confirm that it works. It's in the doc that's looked in the EIP, for example, thinking on the post of practicality of storaging, retrieving those transactions, indexing them internally, because now we have both SSD and ketchup, right? To identify them in some ways and is there a problem there? I guess many indexing strategies are possible, right? Including the very lazy one of just adding a conversion to RLP layer before you do the index step, though it would require the new SSD based logic for the two purely ssd based transaction types. Also, this conversion is fairly intensive in terms of both, especially like cpu memory a bit, right? So for example, I would say it's not okay. Yeah, I think in total it's like less than 100, just like bit twittering operations if you include all of the different things that are happening, which is way less than even a single cryptographic thing. Okay, and does it mean that we have the same transaction route between co and do I believe it does well, okay, there is one kind of nuance here, which is that if we want to support backwards compatibility even for applications, which do Merkel proofs that go into the historical state where you actually have Merkel verification in RLP, then I proposed one modification which basically keeps an MPT transaction route that contains only the old style transactions, and that's something that could be deprecated an extra one or two versions in the future.
01:15:01.946 - 01:16:31.734, Speaker A: But aside from that, I think there's no reason to have different routes because they do both become a pure SSD from an encoding perspective. Because I think if we introduce this transitionary period with two roots, then it will be very difficult to get rid of the second route, and my preference would be not to introduce like a special transitionary period and just switch to the CR route. Got it. That's understood. And I personally have no opposition to this, but that requires the kind of standard, I think backwards compatibility audit procedure of making an effort to go through which important applications do use those kinds of mechanisms, and whether or not they can be told, we can make sure that they upgrade fairly quickly. I think the one that might be the highest value. I'm not sure if anyone from optimism is on the call, but I believe it's the case that bedrock, which is their newer version, it reduces gas costs for submitting a roll up, for submitting a batch by allowing batches to be regular transactions, and then in fraud proofs, allowing you to make a merkel Patricia proof of the transaction.
01:16:31.734 - 01:17:28.010, Speaker A: But even that, I guess number one, optimism is a highly capable team that's very involved in discussions including vias. And number two, I think their fraud proofs technically haven't even turned on, and they declared an intention to switch the blobs as soon as blobs become possible. Anyway, that's the main case with significant amounts of value that I know about. But otherwise, I think we have been signaling for a long time that the data structures in Ethereum blocks are going to change and people have had a lot of warning. Yeah, proto after optimism is here. So you want to give some context on that? Sure. So like any roll up, you do have to be able to prove data is included in a transaction or received to really verify the availability.
01:17:28.010 - 01:18:04.754, Speaker A: We do so with raw transaction data, so we prove that the transaction is indeed committed to as part of this try referenced in the blockadder. But we do not strictly rely on the transaction hash. Merco petition try overlap thing that happens in Ethereum. We can just do the proof. We're not married. Right. But I think the issue is what happens when a future and possibly near future version of Ethereum doesn't even have an MPT route.
01:18:04.754 - 01:18:50.070, Speaker A: So it just has an SSD route. And that would require you to probably better, to be honest, it's easier, less error prone code. Right? Okay, so less error prone code. And I guess you guys technically have not taken off training wheels at all yet, so it's very possible for you to upgrade like code that handles dealing with blobs or with SSD transactions for roll ups. In general, I think we should consider the upgrades, but at the same time, a L2 does not upgrade at the exact same time as layer one for its L2 functionalities. It's only really the block transition. It's fine to change the behavior.
01:18:50.070 - 01:19:47.424, Speaker A: Got it. Even if you have default proof or secret proof ready, you should be able to prepare the newer version. And then based on block number, you just deploy both. Okay. Any other? I guess, questions? Comments? I know Ethan had his proposal, but that kind of gets into the weeds of how things are serialized, which I'm not sure whether or not we want to necessarily get into into the next 12 minutes, because it is a bit of a rabbit hole topic. Not really. Okay, the main question is, when we process 4844 transactions as part of a block, like, not the blob transactions that we have from the network that have the blobs.
01:19:47.424 - 01:21:21.756, Speaker A: So the ones in the block, they don't have the blobs anymore. And I was wondering, are we processing them in any different way still? Like, do we still care when processing blocks if there were originally some blobs attached to a transaction or not? And if they are processed the same, then we can actually make one normalized transaction type that we store as part of the block. And this essentially guarantees that anyone who wants to do proofs on transactions will be able to use stable generalized indices for any field within a transaction. So for example, the transaction's value, it would always be at the same index, regardless if it's a block transaction or anything else. Laxine? Yeah, I just wanted to say, I assume a lot of clients do this, where they end up creating some sort of normalized transaction once they actually get into the transaction processing. But I do want to push back a little bit on using that as the rationale for creating the overall transaction object, as in, like a normalized transaction. The reason being is that there's still a lot of processing that happens that's transaction specific, whether it's on the network level, in the mem pool, or just setting up the state processor.
01:21:21.756 - 01:22:00.584, Speaker A: These things do need to exist. Like, we need to know how to hash different types of transactions. The transactions still exist. And so I think that it's just much clearer to reason and think about these things whenever they're actually separated. And we can define all the methods in a more generic way, rather than having to do it. Switching on types constantly, you would still have to switch on the type, even if it's side by side. Right? It's just that instead of having the type on the entire transaction, the type would just be something that's used for hashing, because that's the only part where it matters.
01:22:00.584 - 01:23:37.570, Speaker A: Right? So there would be a number that tells you, hey, if it's three, then it was originally hashed like that. So for the purpose of signature verification and transaction id, I think that's the only part that cares about mean. It's the only change really that I had on top of Vitalik's document is combine the different transaction types so that the proofs always have the same shape. There is also one concern I had with the hash tree root being the thing that's being signed, because they can conflict with each other. For example, if we have this legacy transaction container in Vitalix document, and then we add another field to it that can have a zero value that means something different than it being absent, it will hash to the same root. So maybe we will still need for the signature purpose to add some version identifier that is not necessarily part of the SSC tree. To be clear, we can have proofs that have the same generalized index for each transaction element if we use a different mercurialization scheme, like this onion format.
01:23:37.570 - 01:25:23.880, Speaker A: If the onion format, different fields, and the same field ends up at a different index. Okay, maybe I misunderstand, but I don't think we should go into too much depth on that here. Yeah, I guess what is the best next step for just getting a general roadmap for SSD? I know we've been discussing this in the type transaction discord. Does it make sense to just keep the conversation there? Is there anything else we should be doing beyond that in the next couple of weeks? I think there needs to be some discussion about these changes to SSV, because I think that even the proposal that Ethan came up with, there is a new type of container where you specify the maximum size of the container, and so this would be a change. Okay, does it make sense to have a separate call just for this SSD stuff? I'm a bit hesitant, given we have a lot of parallel calls going on right now, but maybe that is the best way to make progress on it. Probably, yes. I mean, there is also other discussion, like whether we can do the entire block in one go instead of this gradual process, so that only the state tree would be the final Merkel Patricia tree.
01:25:23.880 - 01:27:00.890, Speaker A: But yeah, separate call is probably best. Okay, so let's use the type transaction channel to find a time to do this. And then yeah, we can sort of share updates on this or the CL call as we have a better spec. Anything else on that topic? Okay, if not the last thing. Marius, you wanted to talk about block benchmarking. Yes, so in Austria, Marek from the Nethermind team had a really good idea, and the idea was that we create blocks and some kind of benchmarking infrastructure on the clients so we can feed them these blocks, and they will execute them, verify them, and spit out the time that it took for them to execute. We kind of started a bit with the EF security and testing team, and these blocks will most likely not be publicly available, but I will provide them to all of the client teams directly so that we don't leak any weird information.
01:27:00.890 - 01:27:33.456, Speaker A: Yeah. Just as a heads up to everyone that you might receive a bunch of funny blocks in the next couple of weeks. That's it. Sounds good. Funny blocks by Marius Lucas. I have a few questions. So will there be some base state? These blocks will be executed on how it will be from this? Or is it like Genesis blocks or first block after? Yes.
01:27:33.456 - 01:28:05.576, Speaker A: Right. Now, the idea is to base them on Genesis just because it's easier. And we will probably create a bunch of blocks so that we can set up state as we want to. So what do we want to measure? Right. Because going through the opcodes and executing the block most of the time is accessing state. Right. At least for Nethermind.
01:28:05.576 - 01:28:34.244, Speaker A: And this also is dependent on the storage model of the state that different clients are using. And I think we are working on optimizing our one. But it. So that's one of the things. Yeah. State access is only, like, one part of the equation, I think. Yeah, but, for example, providing Nethermind state access is, like, at least 70% of block processing time.
01:28:34.244 - 01:29:05.728, Speaker A: And this scales with how the big state is. Right. The bigger the state, the longer the access. Yes. So if we have, like, a small genesis, what we can really measure correctly is only the actual opcodes time state access being irrelevant, right? Kind of, yes. But, yeah. There might be a future version where we start from a specific state.
01:29:05.728 - 01:29:40.520, Speaker A: But, like, setting this up is not as easy as just giving the clients a bunch of blocks. Yeah. State is big, right. So it's not easily. Okay, yeah, cool. It will be cool to have this one. Will this be like, really mixed blocks, or, like, each blocks will focus on some opcode, and we will just stress test the clients based on this opcode.
01:29:40.520 - 01:30:13.910, Speaker A: I think it would start out with specific tests for specific things that we think might be. Might be very heavy, problematic. Yes. Okay, thank you. Thanks. Cool. Anything else as we wrap up? Okay, well, yeah, thanks, everyone.
01:30:13.910 - 01:30:42.030, Speaker A: See you all on the Cl call next week. Oh, actually, like, client. Is there an eof breakout next week? Are these still going on? Yeah, it should be one Wednesday, I think it is. Okay. So, yeah, Wednesday, 15 utc, there's an eof breakout, and then there's a four, four, four, call on Tuesday, 1530 utc. Cl, call next Thursday at 14 utc and we'll see about. All right.
01:30:42.030 - 01:31:00.950, Speaker A: Did we say we want to do an SSD call, too? Yes. And I think the question is when we'd want to do it. So we can talk about that on the discord. But, yeah, looking at next week, it's already pretty packed. Yeah. All right. We'll talk on the discord then.
01:31:00.950 - 01:31:10.368, Speaker A: Cool. Thanks a lot, everyone. Thanks, Tim. Thank you. Thank you. Thank you. Bye.
01:31:10.368 - 01:37:31.070, Speaker A: Thanks. Sa, you.
