00:00:03.080 - 00:00:47.945, Speaker A: Alrighty. Welcome to virco implementers call 24 issue 1149 in the PM repo. On the agenda today we have six items, team updates, number two, def net readiness check three binary and verkal discussion, then discussing deletions in verkal. Number five, pectra impact. And last, if we have time, discussion on vertical sync and potential spec change to add a hash of the proof to the EL block header. Cool. That's the agenda we can start off with Anyone who has updates, client teams or testing.
00:00:57.775 - 00:01:59.365, Speaker B: I can go for besu. So we've been working on a STEM based flat database and we have a working implementation of that. It looks like it's going to be a simplifying factor for sync for us and also there's potentially some performance difference in terms of having to generate the stem in order to do S load and so forth. But Karim is hopeful that it's going to be not only simplified but there won't be a performance impact. Kind of will simplify our flat database implementation. Luis has been working on the GAS cost and the witness changes in 4762 in the update and I think he's making good headway with that. I believe he found what he thought was an issue and it sounds like it's an issue where we're charging access witness for a code access witness cost on a simple GAS transfer.
00:01:59.365 - 00:02:19.965, Speaker B: So it seems like we might have at least maybe a spec problem or maybe that's an intentional thing. Probably wanted to discuss that. I think he's trying to get on the call now actually. But yeah, that's the status update for basically right now we have some vertical transition work that's been stalled for about a week or so, but we're going to be getting back on that shortly.
00:02:26.105 - 00:03:10.375, Speaker C: Yeah, I'd be quite interested in the performance actually, because this is. Yeah. So I'll give you an update. Last week I was with the guest team trying to merge some more stuff into guests. So there's been some progress in that and we discussed the sync as well and it seems that, yeah, we would like to figure out how performant it is to compute all the leaves as they are needed. So reading S loads but also just building a snapshot that is based on those leaves. So yeah, I'd be interested in your performance, like if you have benchmarks, because we have to do the same thing.
00:03:10.375 - 00:04:03.475, Speaker C: Yeah, Otherwise Ignacio has done a lot of work on the testing framework. We found a few bugs and yeah, probably Tanishki is going to talk about this, but we also Found an issue in the spec so there's a spec update that we need to discuss during this call. Hopefully it's only a five minute thing and otherwise yeah, basically implementing costs like Niota cost, they seem to be more or less ready as far as we're concerned. So we would be pretty much ready to launch for a new testnet. We still have maybe two or three issues but they should be able to be fixed this week and yeah, I think that's it. Ignacio, you got something else to add?
00:04:05.375 - 00:04:06.995, Speaker D: Oh, that's a fair summary.
00:04:13.495 - 00:04:50.605, Speaker E: I can go for Ethereum JS we have made some further work to prepare for the eventual Kastinen update. We are going to work on running the actual tests this week. In parallel to this we've also made some upgrade to Rust Cryptography WASM while the JavaScript wrappers around that so that we can begin creating proofs ourselves. So this paves the way for a stateful vertical state manager, whereas before we've only run blocks statelessly.
00:04:56.755 - 00:05:32.375, Speaker F: I can give an update for another mine so we've been mostly working on like getting the Hive test running in Nethermine. We found a few bugs and I think they're fixed now. And except for that as G mentioned there is one change in the spec on like how we charge gas work on and we continue working on the transition. Like the heat refactor is almost done so once that's merged we'll start implementing the transition part. Apart from that, yeah, we're working on some cryptography improvements as well.
00:05:36.435 - 00:06:11.625, Speaker D: I just wanted to clarify something about the tests mostly for Gabriel that mentioned that he's planning to run them. So in the next I would say like one or two days I will release a new version of the Fixtures because I'm now that witness assertions are working I'm finding that some tests require some fixes. So yeah, maybe like you want to hold a bit until I publish this new version so you don't waste time by debugging tests that might be wrong.
00:06:12.765 - 00:06:13.585, Speaker A: Yep.
00:06:19.735 - 00:07:26.785, Speaker G: I can go for Aragon next. I've been working on integrating the gas cost changes and the Witness calculation changes. So writing dirty code I have been successfully able to complete it. Notice some issues with the state management in Aragon with regards to Verkle Tree. I don't know, maybe something got messed up but I'm hopeful I'll be able to resolve it in this week. So far it seems like everyone might be able to sync to the tip for Devnet 6 so I'd have to make some little bit of changes for the next devnet and then I might start working on passing all the tests. We in Aragon 3 have already started fully committing to migrating everything to Aragon 3 which is our next version with a different model of state and everything.
00:07:26.785 - 00:08:34.274, Speaker G: My current implementation is based off on the Aragon 2 which is an older version of it, although all of my changes are on top of Cancun, not Chappella. Oh sorry, not Shanghai or something which means that I'm trying to focus on writing write the whole thing in a way that I don't have to rebase again. So that has been working out for me. But moving forward I think I would abandon this line of code and then start working on the Aragon 3 version of the whole thing. But before that I would try finishing off the rest of the work and try joining the next DevNet DevNet 7 hopefully soon after it launches or before it launches trying to get it ready.
00:08:36.614 - 00:08:42.954, Speaker C: Quick question, did you solve the issue with the fork id? Because I realized I dropped the ball on this.
00:08:43.254 - 00:09:32.435, Speaker G: No I didn't. I hard coded it so it seems like why the issue might be may have something to do with some other clients difference in how the genesis file is treated. I didn't want to spend a lot of time on that because folk ID calculation logic is rather simple and straightforward at best. It might be a misunderstanding on my end so I've just kept it open for now. Surprisingly enough when I tried to use get, it did give me the wrong folk ID the same as Aragon's fork ID for some reason, but I abandoned the investigation there because I thought I'd be able to resolve it later.
00:09:34.775 - 00:09:36.515, Speaker C: Yeah, that makes sense. Cool.
00:09:44.135 - 00:10:08.745, Speaker H: I can go for Lordstar which is not really testnet related, but I have been trying to refresh the branch rebased on the latest code that is there in Lordstar which should provide a more stable base to run the test net on because of the improved performance in the latest Lodestar code base.
00:10:16.005 - 00:10:46.045, Speaker C: Right. Which makes me think if we are to relaunch the testnet we need Lighthouse. I mean Loadstar supports the new Pro format, Lighthouse most probably doesn't and I think we have Tiku also. Is anybody from Tiku here? I guess not. So we'll have to ask to find out who's working on this for Tiku because there used to be Gabriel Fukushima but I think he moved on to somewhere else.
00:10:50.345 - 00:11:55.055, Speaker I: I just wanted to bring something up from Bessu much if Gary already talked about the gas costs so I'm working on the gas costs and I've found out that we were still using the old key design of having separate balance nonce and whatever. And so I did some refactoring on the vertical try library to now compound all of these into the basic data leaf. So that kind of tracked me down a little bit on the on the gas cost side. But I finished that refactoring and fixing or setting the gas cost right. I think it will be more straightforward from now on. I did found something on the spec about simple transactions. I've discussed that in the element in the matrix chat.
00:11:55.055 - 00:12:29.055, Speaker I: Then Ignacio replied there about simple transactions where we also touch the code hash key on the on the origin account. That was looked a bit odd to me, but Ignacio clarified that it's mostly for code simplification. So that's it. Sorry for joining late. So completely miscarries update.
00:12:32.795 - 00:12:36.015, Speaker B: Thanks for adding that bit of color. I knew there was something but I wasn't sure.
00:12:54.365 - 00:12:54.725, Speaker D: Cool.
00:12:54.765 - 00:12:58.625, Speaker A: I think we got everybody. Did I miss anyone or did we miss anyone?
00:13:01.805 - 00:13:42.955, Speaker J: I can give a quick couple minute overview from the testing site. So as a Ignacio mentioned, we have the witness assertions now within the framework when filling tests. So essentially how the framework works. Currently we have post state checks. So essentially you write a test and you assert that a balance is what you expect or some storage value is what you expect in the test. And it's more of a sanity check to make sure that the test that you have written is doing what you expect. So it's essentially that but for witness specific values.
00:13:42.955 - 00:14:46.155, Speaker J: So yeah, Ignacio had kind of wrote an API for this and now we have this within the framework now and with the witness checks we have essentially, essentially a runtime issue, I guess with the transition tool it's not really a huge deal and it's very resolvable. So with filling the tests in east we have we're using Geth or a lot of the vertical calculations that we can't do in Python natively, which means calling Geth a bunch of times, at least the way the framework is written right now. But I guess the next step on our side is to optimize how we fill the test. So instead of calling Geth maybe on average 10 times, we can just do a single call to Geth and it will essentially improve the transition tool runtime. So on our side we'll just be optimizing that.
00:14:58.785 - 00:15:14.769, Speaker A: Cool. Okay, moving on then. Agenda item number two, DevNet readiness. I think we already understand where everybody is here. Maybe we don't need to spend much time. Does anyone have anything they want to add here. Guillaume, you flag that you might want to touch on this.
00:15:14.817 - 00:15:52.105, Speaker C: But yeah, there's actually a couple specs that are still open, so updates to the spec that are still open. Let me try to find maybe just quickly go over them and we don't have to spend like a long time just figuring out what they are. But yeah, like should we support them for the test. Net or not? It's three EIPs. The first one is an unconditional account. Right during contract in it, in fact. Yeah, this one is the one that was reported.
00:15:52.105 - 00:16:30.615, Speaker C: It was an error found by Tanishq. And the idea is that we. Yeah, we. Instead of when we initialize the contract, we write everything. And this is because there can be a case where you run out of gas before deploying the code, like while deploying the code. But you like, you know, so the contract should not be created, but the account itself is created before. So what we do is we simply, instead of making it dependent on the.
00:16:30.615 - 00:17:12.777, Speaker C: On the. Yeah. On many events that are a bit too complicated, we just say every time you create a contract, like when a contract creation is initialized, you at any time you create the basic data leaf key and you also create the code hash leaf key. It's covered by the 20. I keep forgetting the amount of guests that represents, but the 24,000 guests. So I think it's only fair to. This is only fair that it should be covered by this upfront cost that you have for the contract creation.
00:17:12.777 - 00:17:54.795, Speaker C: But on top of that, it's also simplifying because you don't want to find yourself straddling across a lot of use cases where the data leaf is created but the code hash is not. So just to be completely fair and completely. To simplify the code, I think we should just touch everything at the same time and be done with it. This is what this update to the EIP is. If you're okay with that, we can merge it. I mean, I don't expect everybody to give me an answer on the spot, but please have a look. It's eip.
00:17:54.915 - 00:17:55.275, Speaker A: Yep.
00:17:55.315 - 00:18:23.085, Speaker C: There's a hand being raised. Okay. If someone raised their hands, I didn't see it, but yeah, so that would be just a simplification. So just tell me what you think and then we merge it and we have this for the test. Net because I think it's a big simplification. The next one is by Gginder. I'm trying to.
00:18:23.085 - 00:18:47.089, Speaker C: Yeah, it's just a clarification for the system contract update. I don't Think it's a big one. I think we addressed everything. So also have a look at this one. Like yeah, I'd like to get everything merged this week. So please have a look and tell us if you don't want us to merge it because we would like to merge it this week and the last one. Yes.
00:18:47.089 - 00:19:38.335, Speaker C: So the last one is simply. Well it's not really necessary for this test actually. Yes it is. It's all about saying like we we had this Discussion in Kenya 29:35 so state expiry sorry not state history. Contract is creating is favoring EVM execution and we're saying in this EIP that's true up until the Verkol fork and when we get there we no longer favor system contract execution but instead we favor direct updates to the state. Now if you still want to do contract execution, that's totally up to you but you have to filter the leaves because we don't. Sorry the code chunks because we don't want the code chunks to end up in the state, sorry in the witness.
00:19:38.335 - 00:20:06.295, Speaker C: So yeah, this one should also be merged ASAP so I can copy them in just a minute. But that would be the three things we would like to merge before we relaunch the testnet on the spec front so that everybody can agree like there's no outstanding changes that some people implemented and others didn't. Yeah, that's pretty much it for this topic.
00:20:10.195 - 00:20:40.813, Speaker A: Sweet. Thank you. If no comments we can keep going. Next up we have number three, binary and Verkal. So I have a few thoughts that I wanted to start off with and curious if what I'm going to say, I don't know, just overall makes sense to people. If anyone has any thoughts, comments, questions. Obviously it's sort of a.
00:20:40.813 - 00:21:49.607, Speaker A: Maybe not obvious, but yeah, I think it's becoming more and more of a sort of a hot topic which is this question of binary versus vertical and the viability of I guess ZK based stateless clients and something like the next three to five years let's say. And so yeah, I just want to share my, I guess my own personal viewpoint on it and see if it resonates with people or people might disagree with how I'm sort of framing it in my own head. And so I guess, yeah, the way that I'd summarize it is clearly today Verkal is the most viable path to solving state growth and enabling stateless clients. But know that's today and of course we need to keep an eye on the future. It's you Know also quite clear that ZK tech is advancing rapidly, though how rapidly it's advancing and like what the trajectory is over the next three to five years will depend on who you ask. And reality is it's probably just pretty tough to predict the future. But yeah, I cannot argue that there's a lot of investment in that area, of course, and it wouldn't be a huge shock.
00:21:49.607 - 00:22:47.349, Speaker A: I don't think it would be a big surprise to anyone here if we continue to see massive advancements. So, yeah, I guess the obvious question is what do we do given this dynamic, this sort of dynamic behind Verkal is most ready today, but isn't shipping today. And by the time we are ready to ship Verkal on Mainnet, it's at least possible that ZK stuff will be ready or close to ready. And our goal of course is to find the solution that's the best solution for the sort of medium or long term health of the protocol. Obviously we all have invested a lot of time and energy into Verkal and so we have some biases there, but I think all of us would also agree that we want to find the best solution. So, yeah, so I guess given all that, my perspective at least is that it makes sense for us to basically do two things. One is invest some amount of bandwidth into exploring binary trees.
00:22:47.349 - 00:23:48.601, Speaker A: And of course this is something like for Guillaume, going back to the future, he's done this quite a bit and perhaps others here as well. So yeah, what does that mean exactly? Investing more bandwidth into exploring binary trees. I guess that's where I'd love to get feedback. More thoughts, but at the very least making sure we understand, like what are the current benchmarks of ZK proving times how many hashes a second, what are the metrics we want to be looking at? What are those benchmarks on what hardware? Lots of questions, it's not necessarily super easy to answer. And then keeping an eye on this progress over the next six months to determine the viability so that we get a more accurate sense where we are today and where we are going and checking back on this. But yeah, like beyond that, I guess beyond understanding benchmarks, understanding sort of hardware, hardware requirements, this kind of stuff. I guess it's a bit tbd, like what does this mean to.
00:23:48.601 - 00:24:44.575, Speaker A: For us to invest some amount of bandwidth into exploring binary. And then so that's number one. And then number two, I think, I guess just as important, to the extent possible, we try to plan some of our development in a way that is most tree agnostic And I've discussed this some with Guillaume and Ignacio and you know, like, I guess I don't yet know exactly what that looks like or to the extent it's feasible, but just as an example, spending more time on the conversion, the actual state migration process and presumably as far as I understand, it would look similar whether we are migrating to Binary or Verkal. Yeah. But would love to know what people think of all of this. Overall, I think, and I mentioned this to Guillaume, is like what we want to do is to keep our eyes open. We don't want to.
00:24:44.575 - 00:25:25.285, Speaker A: I think that's how we get to the best solution and that's how we actually, at the end of the day, if we arrive at the conclusion that Verkal is the best solution, we want to have arrived there through like, of course, like a transparent and like honest process. And like that will allow us to have our endorsement or our recommendation be the most. What's the word I'm looking for? Like most. Yeah, valid. If we. We got there through like an actual evaluation process. So yeah, sorry if that was kind of a lot, but just wanted to like, I guess in my own head, like there's just been lots of conversations around this and I would love if we don't.
00:25:25.285 - 00:25:54.455, Speaker A: I think it's less productive if we spend the time debating this. Like binary is better, no verticals better and just kind of going back and forth and more. So like, let's try to look at this. What are the metrics we need to. What are the metrics that we're looking at? And you know what, like we're going to move forward in a more tree agnostic way, at least over the next six months, let's say, so we don't have to get bogged down in this debate every month. Anyways, so that's my thoughts and would be curious if anyone has anything.
00:25:57.955 - 00:26:31.455, Speaker C: Just to add something quickly, we would like to update Verkolder info. We're going to add a page regarding this so that people can track where we at in our reflection and of course contribute to it if they have something to say, which, you know, we're no ZK expert. There's plenty of room for other people to give their input, so we'll be sharing it hopefully by the end of this week. There's document that was in the pipe that needs to be refreshed, but yeah, I hope we can do that by the end of the week.
00:26:33.875 - 00:26:36.455, Speaker A: Yep, that sounds good.
00:26:37.395 - 00:27:38.205, Speaker B: Just to add one bit, I would say that designing in a tree agnostic way And I think in addition to that, like leaving room for a fast follow is probably wise because even if we determine that there is a by the time we're ready to deliver vertical, there's. There's a path forward that is defensible and future proof it's going to be add another X number of months or worse to the delivery. So what I would say is in design consideration and inspect consideration, you should keep in mind a fast follow, like things like keeping around a pre image store should be an expectation for Verkal. Like if we intend to go Verkal. But we know that there's something that might want to follow that would be more future proof. Just kind of setting the stage that we might be doing this transition one more time. So we don't want to have the pain of doing, you know, pre image storage distribution and so forth again.
00:27:38.205 - 00:27:45.805, Speaker B: So I can just setting that expectation I think would help with, you know, a subsequent fast follow.
00:27:47.465 - 00:28:21.281, Speaker A: Yeah, that makes, I think that's a good point. It's hard to predict I guess if we will end up doing a fast follow or not. Like let's say we do ship Verkel. What are the odds that we do have fast follow? But yet to your point, we can avoid a lot of potential pain if we just plan as we assume as if we are doing a fast follow. If I understood correctly and that makes sense, that that makes sense to me. Yeah, I guess one thought is that if it becomes obvious that a fast follow or likely that we'll have a fast follow, then I think the basically we'll get a lot of pushback. People say nobody wants to do that.
00:28:21.281 - 00:28:29.671, Speaker A: Nobody wants two migrations. Like let's just go right to ZK or sorry to binary. But yeah, that will be something that will be. Yeah.
00:28:29.823 - 00:28:45.115, Speaker B: And fast is a relative term too, right? I mean a fast fork follow might be, you know, 18 months, two years. Who knows what that would be. But I think just setting the expectation that we need to leave the door open for being able to do this transition less painlessly on a subsequent iteration.
00:28:46.255 - 00:28:47.595, Speaker A: Yep, makes sense.
00:28:48.455 - 00:29:12.175, Speaker C: And just to add to this the initial roadmap, I mean this is something I've said multiple times but just to make sure it's known. We like from the initial version of the roadmap was always like we would do vertical for a while and then binary tree. So there was always. We always kept in mind that the next, the next process should always be faster and at least simpler.
00:29:16.235 - 00:29:21.735, Speaker A: Yep. I think, yeah, it's a good reminder and we should continue to articulate that.
00:29:27.205 - 00:30:37.755, Speaker D: Yeah, and I just want to sorry touch a bit more on the point of doing state tree agnostic progress and all that. I think that we have naturally reached a point where that is true because I think mostly everybody already has a Virgo tree implementation and most of the work that we were planning to do now was around state conversion and 95% or even 100% of that EIP is agnostic to the target tree. And also all this pending discussion about pre image distribution is basically the same problem for Burkle or for binary trees because preimages are for the Merkle Patricia tree. So all that thing is mostly the same. So I think that most of the pending work is actually on stuff that is triagnostic. So that's pretty useful too. Maybe the part that is less agnostic is Witness Generations or cryptography optimizations for Burkle.
00:30:37.755 - 00:30:55.745, Speaker D: So maybe we can hold a bit there. But yeah, 4762 is mostly the same, state conversion is mostly the same and pretty much distribution is basically the same.
00:31:01.445 - 00:31:14.925, Speaker A: Yep, makes sense. Any other thoughts? Do people generally agree with I don't know the what we've said so far? My framing of things anything I said, disagree or not make sense.
00:31:20.745 - 00:31:39.765, Speaker C: One thing to add is we're talking about renaming this the stateless implementers call maybe instead of the Verko implementers call to signal that we're looking into stuff that Verko is still the main avenue. But yeah, we're not closing the. We're not shutting the door.
00:31:42.025 - 00:32:28.015, Speaker A: Any objections to the rename from Vic to sick? If no, then I guess we'll. We'll move forward with the rebrand. So yeah, maybe we can relight some excitement. I think we've maybe. Maybe we lost the ref people over the past few weeks not to pick on them but maybe we can get them back. Okay, moving on then. Number four discuss deletions in Verkal.
00:32:31.765 - 00:33:13.151, Speaker C: Oh yeah, so this is not something I'm pushing for. It's something that Peter and Gary especially in the guest team are pushing for. Their fear is that the fact we have no deletions is going to bloat the state. So I'm currently working on optimizing the storage cost. Sorry, the storage like the serialization of the Verkal tree in the guest database to prove. Well, to make a point which can go either way. Either they're right and never deleting anything is going to bloat the state.
00:33:13.151 - 00:33:33.743, Speaker C: In which case we should definitely consider deletion. Otherwise if it shows that it's. That it's. Well, that it's not a big problem. Then I guess that's enough for me to push back against this desire. So at the core there. Yeah, there are two reasons.
00:33:33.743 - 00:34:21.584, Speaker C: One of them I think is not. I mean in my view should not be the problem of the tree design. It's the fact that guess really doesn't make any difference between something that's like something that never existed or when a zero was written. I think it could be solved at the database layer problem, which is what I'm working on to see if I can make it work. But the other, the initial problem and the initial complaint, especially by Peter, is that the state is going to be very bloated if we do this. I'm not saying we're changing the spec right now. I'm just saying like this is something that.
00:34:21.584 - 00:35:07.745, Speaker C: This is some strong pushback that I received last week. We might have to argument either way and yeah, maybe if people have questions about why we need this, I'd be happy to answer them. But otherwise. Yeah. Are people for deletion like introducing deletions? Are people strongly against it? There's a very good reason to be against it. It's to hand handle the conversion because then you need to somehow flag that something was deleted from the Verkle tree and you should not go into the Merkle tree. So that's a complexity that needs to be worked on.
00:35:07.745 - 00:35:18.425, Speaker C: So that's my reason not to do it. I'm just curious if people have an opinion or if they would really like to do it if they don't want to do it, etc. Etc.
00:35:38.045 - 00:35:39.745, Speaker A: Looks like no strong opinions.
00:35:40.045 - 00:35:45.545, Speaker B: Is there room, is there room for there. For there to be a transition specific deletion behavior?
00:35:46.395 - 00:36:12.535, Speaker C: Yes. So that's exactly what they were suggesting and I have to implement now. You would have. So at least in the guest code base we have three trees. We have like the Merkle tree, we have the Verkle tree and then we have this adapter tree which is not really a tree. It's just doing the. Yeah, like dispatching all the requests so the writes and the reads.
00:36:12.535 - 00:36:47.637, Speaker C: This layer would be the one saving to the database where something got deleted so that when you read it again it will skip the NPT altogether. There's a way to do this. It hasn't been implemented yet. I don't know how complex that approach is. I fear that it might be a bit complex. But if it works then I guess that's another good reason to have Deletions? Yeah. No, like there's an idea.
00:36:47.637 - 00:36:50.105, Speaker C: No concrete code yet.
00:37:05.525 - 00:37:13.715, Speaker A: Okay. If nothing else on this topic, we can keep going. Agenda.
00:37:14.295 - 00:37:31.215, Speaker C: Sorry, I saw a message. Didn't see who that was, but to talk to about state expiry. Oh, yeah. Get gender. Yeah. So that's the other thing about. Because the other argument is that not deleting helps with state expiry.
00:37:31.215 - 00:37:58.745, Speaker C: Like Han created this EIP to. Or at least came up with this idea to. To have state expiry based on this. This whole stateless model, which is really good in my opinion. But then if you introduce deletion, you could not do. Yeah, you could not do that there. Like the response by Peter and Gary, which is also completely fair, is that it's only a problem once you activate state expiry.
00:37:58.745 - 00:38:07.565, Speaker C: So the idea would be to enable deletions up until the point we have state expiry and then disable deletions and have state expiry.
00:38:15.145 - 00:38:19.605, Speaker H: But we then don't need deletions if we have state expiry.
00:38:20.785 - 00:38:46.625, Speaker C: Exactly. So that's when you disable them. Like, up until you get to that point, the state is. The state growth is a problem. So deletions exist, but once you have state expiry, you have a better or at least simpler approach to handle your state growth. So you disable deletion and you enable state expiry. And in fact, you need to disable deletions because otherwise state expiry, like this model is broken.
00:39:00.615 - 00:39:03.315, Speaker H: Maybe we go live with straight expiry.
00:39:07.535 - 00:39:37.805, Speaker C: Yeah, we could do this. But that means it's going to imply more coding. So I expect ACD to push back on this. Oh, yeah, I see a message by Ignacio. Yes, it will increase. It will blow to the code size. Sorry, the witness size a little bit, but we.
00:39:37.805 - 00:39:56.705, Speaker C: Yeah, we would be. I think if deletions don't happen that often, it's just about creating a list of locations that get deleted. So there will be a bit of bloat, but we believe we can make an efficient enough way of updating the witness format to handle that.
00:40:16.045 - 00:40:16.709, Speaker D: Cool.
00:40:16.837 - 00:40:42.165, Speaker A: I guess we should keep going. Stay on time. Yeah. So then, if that's all good. Next up is number five, evaluating the tetra impact of a few things. In particular, those being 7702 and EOF and a few others. But Ignacio, or does anyone else want to start here?
00:40:44.385 - 00:41:58.795, Speaker D: Yeah, I can mention that in the last days I've been reading in More detail the EOF EIPs to understand how, like the new container and instructions might impact like the ip. So I basically went through all that and created a draft PR with the changes that I think are required. So I will share that in the chat. So I mean there's no rush for people to review this, but if anyone has UF eips in their mind, I want to kind of check if this makes sense. That's welcome. Nothing about these changes should feel entirely surprising. Feels like natural changes overall considering all the new instructions that exist and some extra touching that we need to do in the container header and other sections in some instructions.
00:41:58.795 - 00:42:12.255, Speaker D: But if you find anything surprising or anything missing, just feel free to create a comment there or pygmy. So yeah, that's basically it.
00:42:23.155 - 00:42:55.205, Speaker C: Right. And I did have some notes because Matt from the guest team explained to me what 7702 entailed. So there's. Yeah, there's an issue on the Go Ethereum like in my repo Go Ethereum issues. That explains that what 7702 does. There's a couple like. I mean compared to EOF, at least 7702 looks pretty simple.
00:42:55.205 - 00:43:32.093, Speaker C: It's just about making sure like I'm giving you the do5 second summary. You can have contracts that. Or sorry, accounts that delegate their. Yeah, their. I don't know how to say that, but basically when you do an operation on an account, it's a delegation that goes to another account. It's encoded by some field which is 0x EF01. So EOF is Xerox EF00 and 0EF01 is followed by an address that this contract delegates to.
00:43:32.093 - 00:44:05.515, Speaker C: So that should be a contract with a code. So it's just about simply making sure that when you add something to the witness, you add the delegate the contract that the operation is delegated to instead of the. Of the account itself. Otherwise the witness will be empty. So it does bloat the witness a little bit, but yeah, once again it's not. I mean you're paying for this, so 7702 is going to be a tad more expensive than it was. But that's also the case with EOF anyway.
00:44:05.515 - 00:44:31.537, Speaker C: So yeah, that's pretty much it. If there are questions, I'm happy to answer them. But that was more like a heads up. Cool.
00:44:31.601 - 00:44:40.165, Speaker A: If nothing else. Shall we move on? Last up in the final. Oh, sorry, someone say something?
00:44:40.745 - 00:44:43.105, Speaker C: No, I just answered your question. Yes, we can move on.
00:44:43.185 - 00:44:55.335, Speaker A: Yeah, cool, thanks. Last up, the Verkal Sync and potentially related or included in this bucket is a spec Change to add a hash of the proof to the EO block.
00:45:00.795 - 00:45:08.975, Speaker C: Yeah, sorry, I'm confused. What is. What is the spec change? I didn't. I didn't expect that.
00:45:10.275 - 00:45:12.375, Speaker A: Yeah, I guess we can already touch.
00:45:12.865 - 00:45:39.009, Speaker C: Sorry. Yeah, now I remember. Yes, of course. So, yeah, once again, we discussed with the guest team last week how to make Goethe be able to snap sync or vertical sync. So there's already a model proposed by Tanish, which is pretty much what we agreed on. A couple things were suggested by Peter. One of them is, currently we don't add anything related to the proof to the.
00:45:39.009 - 00:46:22.549, Speaker C: To the block header. Peter wanted to be a bit more strict about witness validation. So it would be that every. Even full nodes will have to validate witnesses and make sure that no extra data is being passed. Like, as a way to prevent people from bloating the witness with things that are indeed correct but are not necessary. So what Peter would like to do is basically flag a block that has too many leaves as invalid, even though it's correct in every other respect. And the other thing would be to.
00:46:22.549 - 00:47:10.435, Speaker C: Because, of course, if we do it like, currently, the witness is a sidecar. Currently, I mean, the witness and proof. If we start introducing this rule, then we have to make the witness itself part of the block. Because if I'm a dishonest host, sorry, a dishonest node, I would receive the honest block and then replace the proof with a bloated one to get the block declared as invalid, when in fact the producer was honest. So that means that we would have to produce the proof in the block building cycle and we would hash this proof and add it to the header. So that was one request. The other thing that Peter wanted.
00:47:10.435 - 00:47:46.245, Speaker C: Yeah, so have a snapshot that is per stem and not by the hash like the traditional hash like it currently is. So basically I have a vertical specific snapshot, but I think that's already what. What tanishq is doing. There was another thing I'm forgetting. I mean, okay, these are the two main. The two main changes, I think. Yeah, there was another detail, but it escapes me now.
00:47:46.245 - 00:48:22.467, Speaker C: So, yeah, at some point we were talking about. Oh, sorry, yes, there's another detail. Currently we passed the pre state and the post state. The post values. That was meant as a way to make it easier for very, very like nodes who don't want to run the block to somehow trust that the data they've been given is correct. It takes roughly like, it roughly doubles the size of the witness. So, yeah, Peter was especially.
00:48:22.467 - 00:48:41.365, Speaker C: But other members gave some Pushback. I never liked this idea to begin with. I. Yeah. So my question was more like those who have thought about the sink. I already asked Tanish this, but maybe there are others. Others who were counting on those post values.
00:48:41.365 - 00:49:15.055, Speaker C: We would be removing them if everybody is okay. So I still have to. Yeah. To ask Dan Crud if he. If he's okay with that. But I think there's a good agreement that it's taking space and not being really useful, at least. I mean, like Martin was saying, there's very light.
00:49:15.055 - 00:49:37.305, Speaker C: Very light, like VMs today. So you could totally execute a block even in a light client. That's not a problem. Another thing that escapes me again, that I was just thinking. Oh, yeah. Is the duration, how long you need to save the witness on your disk. Right? So that's the other important thing.
00:49:37.305 - 00:50:02.685, Speaker C: Because you hash. Okay. So there are two things about this hash. You need to. Like when you build a block, you need to rebuild a witness to be able to hash it. Or Peter was saying after a while, like, if the block is old enough, has enough confirmations, you could say you only verified the block until, you know, like a month ago. And before that, so you verify the witness.
00:50:02.685 - 00:50:26.955, Speaker C: Otherwise you just trust the hash. Because, yeah, if it was a month ago, presumably you're on the right chain. And the other thing he was saying is, yeah, we need to keep the witness, however, because he wants to be able to. So his use case is that he says when you sync, you have to trust the network. So. But. And then it's a manual verification.
00:50:26.955 - 00:50:45.871, Speaker C: So it's up to you to verify that you have synced the right. The right chain. So that's a manual process, like he says. Sync is a manual process. But there's the use case where you go on holiday for like two weeks. Your node dies day one, as it tends to do. And then.
00:50:45.871 - 00:51:10.577, Speaker C: Excuse me. And then you need to be able to replay every single block to catch up by replaying every block. And for this, you need the witnesses. Like, if you're a stateless client, you need to be able to download the witnesses. So what Peter wants to do is keep the witnesses on disk for a certain period of time to. Yeah. To be able to.
00:51:10.577 - 00:51:36.915, Speaker C: What he calls a headless. To do a performer. Headless sync, like. Or automated in a way that unlike the initial sync where you. You are here and you verify that you're on the right chain. If you have a power cut or an Internet cut and you come back online, you have to make sure that. Sorry, you don't have to make sure the node has to be able to recover by itself without you.
00:51:37.935 - 00:51:38.335, Speaker B: Yeah.
00:51:38.375 - 00:52:25.533, Speaker C: Watching it or manually doing that verification. So that's the argument that means we're not saving as much space as we could. But yeah, we would keep the witnesses for one month, which I calculated if you keep the post value would add an extra 60 gigabytes, which is not light. But yeah, that's basically the ideas that we discussed last week. So I don't know if there's feedback questions or if everybody seems to be fine with that. But I mean, yeah, so keeping the witnesses, that would be a guest, a guest thing. Everybody's welcome to do the same thing, of course, but yeah, that's what guests would do.
00:52:25.533 - 00:52:29.225, Speaker C: And yeah, if there are questions or pushback or feedback, I'm interested.
00:52:42.195 - 00:52:52.615, Speaker B: Wouldn't keeping block witnesses back just to the latest finalized block be sufficient? Do we really need to keep a rolling month? Is there a reason to keep them beyond last finalized?
00:52:53.475 - 00:53:38.645, Speaker C: Yes, I mean there's a reason. I mean, I'm not convinced, but Peter is really insisting on this. The reason is that yes, if your node crashes, for example, and you're gone on a two week, three week holiday, you come back, you definitely passed finalization and you need to be able to rejoin the network automatically. So without that, the cutoff of one month is that typically everybody's holiday is less than a month. So that, that's why, like I said, you're, you're welcome not to do this if you don't want to. You, you can keep it two weeks, you can keep it all the way to finalization. But that's the reasoning behind it, why we want to keep so much.
00:53:38.645 - 00:54:12.745, Speaker C: So portal network is definitely a plan for that. And in fact, Peter said, yeah, that would be a perfect use case for a portal network. But my understanding is that, and Milos is here, my understanding was portal people themselves were thinking that was not a good use case. I'm not sure. I think it might be a use case to keep witness there.
00:54:15.365 - 00:54:16.085, Speaker G: Together with.
00:54:16.125 - 00:54:32.075, Speaker C: Many other stuff that are on the portal network. But it was never kind of discussed in details. But I would say it would, it makes sense. Yeah. So that would be a super great use case in that. Yeah, that would be a great option.
00:55:01.225 - 00:55:16.015, Speaker A: All right, well, we are just about at time. There's nothing else in the last minute or so. We can end there. Cool. All right, thanks everybody. Talk to you guys.
00:55:16.835 - 00:55:17.523, Speaker C: Bye.
00:55:17.659 - 00:55:18.395, Speaker A: See you. One. Bye.
