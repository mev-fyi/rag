00:00:00.250 - 00:00:36.534, Speaker A: You this meeting is being recorded. Okay, so welcome everyone. Eip four four, call number 13. Like I was just saying before, recorded. Basically we had an interop meeting last week where all the client teams, some researchers and other folks were there in person. And pretty sure most people on this call were there, at least the vast majority. And we made a lot of good progress on 4844.
00:00:36.534 - 00:01:21.720, Speaker A: And actually maybe it makes sense to start to start with that. And then we also identified a lot of open issues or things we need to work on in the coming months. And I think we should spend of the call today, just like making sure there's people who are sort of on top of those different issues and then we can wrap up. If client teams, testing folks or researchers have updates that we don't cover, we can close with those. But yeah, maybe to start. So we had this week, last week we had most of the client teams in person working on Devnet four. I believe we got all the client teams except perhaps Nimbus to join a Devnet and interop together.
00:01:21.720 - 00:01:25.990, Speaker A: Yeah. Does anyone want to share a bit more? Cut around that perhaps?
00:01:30.630 - 00:01:33.730, Speaker B: Yeah. Am I audible?
00:01:34.710 - 00:01:36.580, Speaker A: Yes. Cool.
00:01:37.270 - 00:02:36.050, Speaker B: Yeah, so we had the Devnet four had full client participation, except like you mentioned, Nimbus. We got client interrupt with all sorts of combinations like Geth Prism, Geth Lighthouse, even Ethereum, JS and Lodestar, VMJs, Teku, et cetera. And that was all working fine. We ran into a couple issues during syncing, which eventually client devs are able to get that working. There's also a couple standing issues, but so far it looks like all clients except in this are able to sync through the Devnet. And yeah, everything in consensus. One note, this Devnet was like, excuse me, hopefully it's not Edwise Flu.
00:02:36.050 - 00:03:07.070, Speaker B: This devnet was like pretty like the goal of it was just to get clients up to spec and ensure that they're able to sync to tip. So we didn't test things like big transaction spam, like create a lot of blobs in the chain or do any of the bad blocks testing that was done for withdrawals. Just keep in mind those goals. We probably would need to iterate on more stressful environment in future test nets. But for this one it was pretty good to see full participation.
00:03:08.130 - 00:03:32.280, Speaker A: I just wanted to add on top of that, I did try to spam it, but not intensively. I have this generator too with the blob utility. Blob util of utility. And then I tried spamming it for like 2 hours. It seems fine. But of course this is nothing, but this is just like an appetizer thing. But yeah, I did try play around with it.
00:03:33.450 - 00:03:54.880, Speaker B: Yeah, that was really great. That actually helped us highlight a bug in several client implementations, several execution clients assumed that there was like a maximum of two blobs per transaction, whereas that was only applies to the block validity. So that was actually great, the spamming that Terrence did.
00:03:57.250 - 00:04:04.050, Speaker A: Nice. Anyone else have anything they want to add on the devnet?
00:04:06.870 - 00:04:21.320, Speaker C: So we had a couple sync related bugs in lighthouse and we made some fixes for those. I was wondering if we could push some docker images with the latest from the four four branch so we can test those out.
00:04:25.950 - 00:04:32.010, Speaker B: Yeah, we can definitely do that. We can sync offline the details.
00:04:33.150 - 00:04:33.900, Speaker C: Cool.
00:04:37.250 - 00:05:08.120, Speaker A: Fabio. Yes. Bezo here. So we are connected externally to the devnet four. We are progressing. We have some problem with some blocks that we are addressing right now with the hope to be to fully join the devnet in the next days. Nice.
00:05:14.670 - 00:05:17.020, Speaker D: An update from Teku too.
00:05:17.950 - 00:06:10.170, Speaker A: Actually, we managed to join the devnet four in the very last day on Friday. Actually we were following the chain correctly, but we spotted a bug and we were actually deleting the blobs that are being finalized. So the bug is now fixed and we'll deploy a fix very soon. Sweet. Any other updates? Nevermind. Still wants to synchronize with lighthouse. It's our nearest aim and that's it.
00:06:10.170 - 00:07:14.250, Speaker A: Thank you. Anyone else? Okay, in that case then, yeah, so we got pretty much everyone on Devnet four, obviously still some bugs. We can keep working on those. There were a couple of other things, actually quite a list of other things that we discussed in person. So I'll just sort of go through them. The first one was the KZG ceremony output and finding kind of an optimal format to feed this into clients so that they can use it as input. I know Carl sort of led a discussion on this, but I don't know if anyone wants to give a quick recap there of sort of what we agreed to with regards to just the output of the ceremony and how clients would use it.
00:07:14.250 - 00:07:57.738, Speaker A: Okay, yeah, we can come back to that. I think the other thing as well, I can give a quick update. My recollection is that the output is going to be just like hex encoded with new lines. Right. I think that's where we landed. We talked about doing some pre processed or the pre pre processing data, but I think that's where we landed was to. I forget exactly what it was.
00:07:57.738 - 00:08:02.420, Speaker A: I think it's like hex encoded with new lines. Right?
00:08:03.110 - 00:08:10.580, Speaker B: I think that's the same format that CKCG four four already uses. Are we going to be following that?
00:08:13.030 - 00:08:45.360, Speaker A: Okay. Yeah, I think so. Okay, sweet. Yeah. And then obviously capcoin teams work with the KCG folks to make sure this is good. And then in terms of the libraries themselves, I think we did a fair amount of bug finding during interop and of hardening of the libraries. George, do you want to maybe give a quick update there?
00:08:48.070 - 00:09:29.920, Speaker D: Yes, I guess I can give one. You caught me a bit off guard. I don't fully have. Okay, so let's see, since Alexey, I see you're here, but I think you are not at the interope. Or if you were, we didn't get to meet. Anyhow, so one of the big updates from the things we did at the interope is that we switched the entire public API interface of the KCG library to accept bytes for everything, including field elements, including groups, everything. So clients just pass in bytes to the KCG library, and the KZG library takes care of everything.
00:09:29.920 - 00:10:28.080, Speaker D: This seemed to be the most foolproof approach. At the same time, another decision was to switch GokZG to use the GnarC library for the backend. It seemed a more prudent choice over Killitz or whatever, given that Gnarc has been audited. So I think Kev, who, I'm not sure if he's here, but anyhow, I think he's working on changing globate g to use gnark. This is a big departure from the previous situation, and it also has to do with the fact that we're getting an audit in like one month and a half. So things should be stabilizing at some point. So this is like a bit of a thing that's happening right now.
00:10:28.080 - 00:10:58.540, Speaker D: At the same time, we did lots of work at improving the code base of CKCg. We added a unit test framework. We started doing more tests, we found some bugs, all that stuff. So yeah, I guess that's it from the library side. I mean, I have some more cryptography things in terms of the decoupling blobs, potential change, but that has nothing to do with the library. So I will let that for later.
00:11:00.350 - 00:11:37.340, Speaker A: Sounds good. Yeah. Thanks for the overview. Anyone else have an update on the libraries? Okay, another big thing that we talked about interop was the transaction pool for the blobs. So Peter from the get team spent a bunch of time, sorry, I posted the wrong link. This is the right link in GitHub. So Peter from the get team spent a bunch of time looking into this and coming up with a design that he thinks would be optimal.
00:11:37.340 - 00:12:16.134, Speaker A: And there was a lot of conversations about this. Denkrad, you had some comments on the proposal there as well. Yeah, my feeling is like all the el client teams are sort of looking into an implementation for this. I don't know if anyone wants to give a more detailed update. Okay. Yeah, in that case, I do recommend anyone who's interested in this, just have a look at the doc or the gist that I posted there and can go, oh, lucas.
00:12:16.182 - 00:12:56.234, Speaker E: Yeah, I remember having some conversations around it. And yeah, there are a few question marks how it should work. Those can be considered implementation details. So different clients can work here in different ways as long as they have all the security concerns covered. And by security concerns, I mean like being able to not blow up the storage, for example, if they store on disk or in memory. Right. So things like that.
00:12:56.234 - 00:14:16.420, Speaker E: But yeah, there are those question marks about should we handle replayability right after York, and as well as how should we treat multiple nonsense between transactions that are blob transactions, not blob transactions, and how we should. So for example, if you have transactions that are blobs and not blobs with the same nons, who wins and why? And can you replace one with another, or is it restricted? And also, should we potentially have, how should we track multiple transactions from multiple blob transactions from same address? There are like multiple questions on the details of implementation, and I'm not sure how much of that should be as like a spec, and how much should be just implementation detail and what the exact rules then should be on the spec side. So there are multiple questions here that it would be good to iron out and have some tests that would be kind of like attack vectors on this, just to be sure.
00:14:18.950 - 00:15:10.450, Speaker A: Right. That's actually a good point. I don't know that we have specs for any transaction pool things for any previous eips, but then it's like, yeah, maybe the better approach here is to have a test suite that spams the transaction pools, or at the very least on the devnets, we can spam the networks with a bunch of transactions and see how clients handle that. And we can potentially trigger reorgs. Yeah. So I, I feel like for the transaction pools implementations themselves, we can keep tracking them here, and I suspect it'll also be discussed on core devs. But maybe one thing we should look into in addition to that is what's the best way to test it? Whether it's a devnet or like a static test suite.
00:15:12.570 - 00:15:24.620, Speaker E: So I'm just pointing out this is quite complex problem and there can be like multiple solutions. And the question mark is like, where is the protocol level? Where is the implementation level?
00:15:25.070 - 00:15:25.866, Speaker A: Right.
00:15:26.048 - 00:15:41.710, Speaker E: And only one thing that we, I think, agreed on was that definitely we have a separate. That blob transactions cannot have like zero blobs. It's a separate type. And that's really helpful for even reasoning about.
00:15:41.860 - 00:15:51.300, Speaker A: Yeah, and I think there's a pr about that already in the EIP. Right. I think Matt put a pr for this. Let me check.
00:15:54.950 - 00:16:07.030, Speaker F: I don't think we agreed on. I know that some people, but I think we agreed that the transaction pool would not contain zero block transactions.
00:16:09.930 - 00:16:15.020, Speaker A: Right. So this is like a client validation thing, not a consensus rule, is that right?
00:16:17.390 - 00:16:23.420, Speaker F: Well, I mean, there were some people who were pushing for it to be consensus rule, but I don't think there is consensus on that.
00:16:25.570 - 00:16:26.078, Speaker A: Okay.
00:16:26.164 - 00:16:43.410, Speaker E: I got a feeling that from the session when we discussed how to ssd all the transactions having SSZ, we kind of established that we will introduced another type that would be non blob. But I'm not sure that was the consensus.
00:16:48.880 - 00:16:55.680, Speaker F: Right, and where does that say that in consensus, you can't have zero blob transactions.
00:16:58.580 - 00:16:59.040, Speaker A: Okay.
00:16:59.110 - 00:17:03.760, Speaker E: But then it makes it awkward. Right. Because of those rules.
00:17:10.850 - 00:17:13.840, Speaker F: Transaction is not a special case at all. It works.
00:17:18.870 - 00:17:27.410, Speaker E: I love to explain to users why their two different blob transactions behave differently.
00:17:30.310 - 00:18:21.480, Speaker A: So does it make sense to maybe bring this specific issue up of the zero blob transactions on our core devs, so we get all the client teams there to chime in. Okay, yeah. So let's do that. And I think with regards to testing, even in the world where regardless of whether zero blob transactions are valid or not, it probably makes sense to have some way to test all of the boundary conditions. And even if it's not like part of a spec. Right. Like the clients might handle reorgs differently or whatnot, but they should not crash over a reorg.
00:18:21.480 - 00:18:58.300, Speaker A: Yeah. So I think just generally thinking about how we test even the non consensus parts of four. Four. Four is also something we should, we should discuss on awkward devs. Yeah. Anything else on the transaction pool, testinia? Okay, other big topic that came up was actually one more question on the transaction pool stuff.
00:19:02.270 - 00:19:03.674, Speaker C: Do we have a clear next step.
00:19:03.712 - 00:20:03.054, Speaker A: Besides the zero transaction thing, on writing this into some kind of specification, do clients need anything else to make progress on this? I'm nervous that it's a little ambiguous. Right. Now. So historically we haven't had specs for clients are free to implement the transaction pool how they want. Given that you don't come to consensus on it, it makes sense to have probably some reference docs or some high level design things. But it is also fine if Aragon does this differently than guest, than Besu, as long as ideally none of their different implementation has issues. I think this is the thing I'd want to discuss on awkward devs is like, what's the right way to test this in a way? Like what is the thing where we want to ensure all clients behave the same way? And that doesn't mean there's like a full spec.
00:20:03.054 - 00:20:14.270, Speaker A: Right. But for example, should they reject this type of transaction, or should we expect them to include this type of transaction if there's a reorg and whatnot?
00:20:15.410 - 00:20:28.738, Speaker G: Yeah, I think that's the most challenging issue that I'm saying that needs to be resolved. Is replaying blob transactions required, because that's where you have to introduce some significant new complexity and code pass into most transaction pool implementations.
00:20:28.914 - 00:20:55.646, Speaker A: Right. Okay. And then there's a comment, I guess just before we. Does that make sense to you, Jesse? Then I'll go to the comment in the then. Okay, there's another comment in the chat. Adding data gas consumed to the transaction receipt format. Yeah.
00:20:55.646 - 00:20:59.066, Speaker A: Do you mind giving some background on came up.
00:20:59.108 - 00:21:54.194, Speaker G: This is Andrew. This came up during testing in Devnet for when we were trying to triage a bug in our Ethereum js code. And I know Robert, you and I talked about this on R and D on the 44 four testing channel a little bit, but just adding that field for the total data gas consumers, we're talking about transactions to the, once a transaction is mined, putting that field in the transaction receipt, that's returned over JSON RPC, forget transaction receipt. I wasn't sure if that needed to go in the EIP or if we were going to agree to that. Where would we document that? It wasn't obvious to me if that's something that goes in the EIP itself or we just update the RPC. Any concerns about that? Because it feels like a useful data point since that's not reflected in the transaction receipt. So you won't be able to easily derive the full kind of state transition for an account balance between blocks for the sending account without that.
00:21:54.194 - 00:22:20.970, Speaker G: Yeah, and a little more background. I mean, there was some debate in the discord regarding, well, it can be computed, so is it really needed there? But I think putting the burden on all clients to be able to recompute that appropriately given the computation might change in the future, to me seems a little onerous. So my feeling is it should be returned by the receipt. It doesn't need to be part of the consensus hash, but it should be returned.
00:22:22.030 - 00:22:36.126, Speaker A: Right. So if we're not changing basically the consensus objects, then the way to propose this would be to open a pr on the JSON RPC spec and maybe just put it on the outcoord as agenda to get people's thoughts there.
00:22:36.308 - 00:22:36.942, Speaker G: Okay.
00:22:37.076 - 00:22:38.354, Speaker A: Yeah, cool.
00:22:38.392 - 00:22:38.626, Speaker D: All right.
00:22:38.648 - 00:22:46.770, Speaker G: Yeah, I just want to make sure because I think it would be useful for downstream, just el related stuff. Thanks.
00:22:46.920 - 00:22:59.254, Speaker A: Cool. And yeah, just in case people are not aware, the JSON RPC specs is now in the execution APIs. It used to be in eips. They changed a while back. So that would be the repo where you open a pr.
00:22:59.452 - 00:23:00.840, Speaker G: Okay, cool, thanks.
00:23:10.550 - 00:23:42.834, Speaker A: Okay. And then the other big thing we discussed was decoupling blobs and blocks for sync. I know this is something we've gone kind of back and forth on, but it seems like we've landed on a decoupled approach. Does anyone want to give a quick recap of what changed and why we're going this way? Yeah, go ahead. Yeah.
00:23:42.872 - 00:24:31.998, Speaker C: So we decided to split up blocks and blobs in gossip sub. And what are the main reasons for this? Well, generally it's for just propagating the blobs as efficiently as we can. One of the reasons was we were seeing some evidence in the big blobs experiments or big blocks experiments on Goreli. That message size correlates with the network propagation. So by splitting up the blob message sizes, we can operate more efficiently in gossip. And there's room for some optimizations that could also reduce bandwidth. Generally it just gives us more optionality.
00:24:31.998 - 00:25:19.380, Speaker C: I think that's the main advantage. So Yasuk's been working on the spec for it, so I think it'd be good if consensus client teams gave it a look. This is like notes to the spec I just sent in the chat. I think he's going to raise a pr for it relatively soon. And yeah, the implications of decoupling blobs and blocks on gossip are that we probably want to do the same in the byroots requests and our by range sync requests are already decoupled, so I don't think we'll have to touch that logic. Yeah, it's about it.
00:25:21.510 - 00:25:58.800, Speaker A: Thanks for the summary. So on the prison side, I have began implementing the changes. I know it's still very early that there's a lot of work to do, but I'm trying to make the implementation follow very closely to the design doc so that I can probably give better feedback just from the implementation's point of view. I also left the first round of review on the design dog. I don't think Yasik has made any progress, I think, since last week, but that's okay. I think he'll probably come back to it very soon. So yeah, that's the update from my end.
00:25:58.800 - 00:27:10.500, Speaker A: Nice. Any other updates on this? It okay, and we sort of touched on this a little bit, but basically another big part of what I would discuss is everything around bandwidth and networking requirements and how the blobs sort of affect us on the network. So splitting out blobs and blobs can help us make some optimizations there a bit. I thought there was also a lot of updates on the big block chat. I don't know if anyone wants to give an update on that. Okay, so we can follow up on that later. But just from skimming the chat, it does seem like we're still setting some big blocks on Gordy and looking into how they affect propagation of attestations and other things on the network.
00:27:10.500 - 00:27:15.530, Speaker A: Another topic that came up.
00:27:16.940 - 00:28:17.710, Speaker D: Sorry Tim, just to go a bit on the decoupling topic again before we depart too far away from it. Maybe that was clear, but it was not clear to me. I don't fully understand how certain we are that we're going with this, because it affects quite deeply the cryptography layer, especially depending of how it's implemented. If we want the individual decoupled blocks to be verifiable on the networking layer, it will affect the cryptography layer quite a bit. So I'm not fully sure if we have decided we're going for it, because if we are, we need to start doing changes, have them ready before the audit and all that stuff. Can someone give a bit more insight on how final this decision is?
00:28:22.000 - 00:28:30.160, Speaker C: I think we reached consensus at interop, but does anyone here have opposing views?
00:28:43.890 - 00:28:51.070, Speaker A: Okay, if it does have these ripple effects on. Oh please. Dengue.
00:28:51.990 - 00:29:17.240, Speaker F: Yeah, I personally still believe that the benefits are probably overstated and there's probably nice to have, but the benefits are probably more like 20% rather than two x. So I was always against it, but I have started working on it and I'm willing to implement it if that's what everyone wants.
00:29:21.380 - 00:29:30.150, Speaker G: Yeah, I think I kind of feel the same way. It feels a little unnecessary at this stage, but I haven't been involved in the big block tests, so I may be missing some of the data.
00:29:32.040 - 00:29:35.288, Speaker D: Right. Sorry Roberta, I was just going to.
00:29:35.294 - 00:29:37.610, Speaker G: Say I don't feel hugely strongly about it.
00:29:38.380 - 00:29:45.020, Speaker F: And just to be clear, we also don't have any data. This is currently all based on people's intuitions.
00:29:54.170 - 00:30:51.298, Speaker D: So I'm a bit prejudiced on this. I also don't have precise data on how good this engine is in terms of networking performance. But apart from that, I guess it will make the networking layer a bit more complicated. But that's not my domain. In terms of cryptography, we will need to first of all switch the specs for the new model of cryptography, which know probably we want to do one KCG proof per blob, so that the KCG proofs travel with the blobs. Now, since the blobs will be sent isolated and then. So we need to get that peer merged, change the interface of the KZG library, change the KZG libraries inside, change the test for the like.
00:30:51.298 - 00:31:18.980, Speaker D: There is a bit of logistics and bureaucracy that needs to happen with this change. And we're also one month and a half or even less before the audit. I'm just saying, let's just make sure that this is a worthwhile change and not just based on intuition, or if it's based on intuition, it should be a strong one.
00:31:19.990 - 00:32:07.774, Speaker A: Yeah, so I guess when we have this session, we had this session last week in person, and we didn't really consider the effect of KCG library. I think everyone just look at it from the client implementation point of view. And we did come up to like okay with this change, the delay will be one month and people seem to be okay with it. But now with the KCG library changes that we just brought up, so the delay may seem more think like, I don't think we have the right party in this call. For example, Yasik is not here, Danny is not here. So maybe the best thing we can do is just to talk about it on Thursday's call when we have all the relevant parties. I don't think we can make a decision.
00:32:07.774 - 00:32:25.080, Speaker A: Right. We can definitely talk about it on the call Thursday, but we should make sure that folks like YAsc and cldevs know to show up, because it's not a call. But yeah.
00:32:27.610 - 00:32:31.718, Speaker F: So I just wanted to add. Hello, can you hear me?
00:32:31.884 - 00:32:33.770, Speaker A: Yeah, Dancrad and Alex.
00:32:34.590 - 00:32:52.560, Speaker F: Yeah, I wanted to say so I'm pretty confident we can make this change. It is like late in the development, but I'm confident that we can get the libraries ready. But, yeah, we just want to be sure, and if so, we won't really want to do it within at most one or two weeks and be certain about it.
00:32:56.790 - 00:32:57.582, Speaker A: Alex.
00:32:57.726 - 00:33:01.620, Speaker F: And I don't think it will introduce any further delays, just to be clear.
00:33:04.230 - 00:33:19.170, Speaker G: Yeah, I was just going to bring up, because I believe when we were talking about this, there was an intermediate solution where we just have the proposer sign over the fragments of the bobs, and then I don't see how that would require changing the PZG APIs.
00:33:21.310 - 00:34:21.260, Speaker F: It doesn't. So, yes, we could theoretically use the exact same cryptography as we are doing now. It just would feel a bit out of place, because basically, if we changed it now and make this blob change, it will give us much more flexibility in the future. It basically allows clients to process blobs in any order that they want and verify that their commitments are correct. It allows the proposer to copy the proofs directly from transactions, rather than having to compute their own. And it would allow future additions like erasure, coding the blobs, so that you could, say, have double the number of blobs, but only need half of them. So there are some strong reasons that if we do this, change that we also change the cryptography, so that we have all these nice to haves in the future.
00:34:21.260 - 00:34:26.560, Speaker F: But yes, theoretically, we could also just not change the cryptography at all.
00:34:29.330 - 00:35:10.620, Speaker D: Right, so there is kind of like three different options. Like, either we don't do the decoupling at all, or we do the decoupling and we change the crypto, or we do the decoupling and we don't change the crypto. And just to kind of make this more concrete, even if we change the crypto, the crypto is not going to get, from what I understand, more complicated. The complexity maybe even decreases in total. So, yeah, I agree with Dankra that it's probably not going to delay us, because crypto and the rest of the work is kind of parallelizable. But yeah, we should start kind of soon to do stuff.
00:35:13.470 - 00:35:15.850, Speaker F: Yeah, the main constraint here is the audit.
00:35:16.910 - 00:36:00.310, Speaker A: Great. Antigar. Yeah, I'm not sure if you can hear me. Audio quality is good, but basically, I think my would just be that kind of on the straight off space. I think it would be important to basically get the choice right, because if we say we would not touch the cryptography, then that would mean that the individual blocks cannot be handled individually anyway, like nowhere on the network. So then, in that case, I think it would be a mistake to have special case logic for splitting right at the blob sizes. Then I think we should just keep treat the entire blob section as one opaque stream of data and just have some size based chunking.
00:36:00.310 - 00:36:48.712, Speaker A: And because size based Chunking would be much easier to later on extend to the full block. So basically the point is just that I feel like we are a little bit at risk here right now to just go basically just make very quick choices because we have time pressure, but kind of not get this right in terms of kind of the principled approach. Yeah. Okay. So we can definitely discuss it more on the call Thursday. What's the best place? I guess we can use the, should we use the KZG Discord channel to discuss this until then? Or doesn't feel or maybe, I don't know, sharded data might be more. Yeah, that works.
00:36:48.712 - 00:37:06.590, Speaker A: Okay, so I've put it on the call already for Thursday and I'll make sure to reemphasize it. Let's use the sharded data channel to discuss until then. And hopefully we have just more context when we make the decision then.
00:37:08.080 - 00:37:33.190, Speaker D: Sorry, just since the audit is suddenly becoming a huge deadline, I'm not sure perhaps it's even considered that we could push it forward. I don't know. I agree with Hansgar. We should take the right decision and let's not just put deadlines that we could perhaps push if we could. I don't know.
00:37:36.220 - 00:37:39.930, Speaker A: Does anyone have a feel for how flexible the audit dates are?
00:37:44.460 - 00:37:45.710, Speaker D: I'm not sure.
00:37:47.840 - 00:37:56.588, Speaker F: We would have to ask Sigma prime, but there may be an overlap in this call. I have no idea if it's you.
00:37:56.674 - 00:38:04.000, Speaker A: Can you reach out to them dencrad before Thursday so we at least know by then if that would be a possibility.
00:38:06.260 - 00:38:06.768, Speaker F: Sure.
00:38:06.854 - 00:38:55.460, Speaker A: Yes. Okay. Anything else? Just on blob block coupling, KZG libraries. Okay. The other thing that we started to bring up at the workshop was multi client sync. So I guess as I understand it now, clients can sort of sync from similar peers, but we don't necessarily have full all cross client coverage. Does anyone have more context there?
00:39:01.270 - 00:39:32.880, Speaker C: I can talk about Lighthouse. Yeah, so we were trying to sync with Teku, but because they had the issue where they were pruning blobs past finalization, we would ban them if they didn't have blobs within the data availability period, then with prism. We're still trying to figure that out. It seems like we're rate limiting them, but yeah, I'm still trying to dig through that.
00:39:36.610 - 00:40:32.848, Speaker A: Any other client has some updates to share there. Okay, then another topic that we started to discuss was how do we hash the blob transactions? What is the right interplay between SSD and RLP there? And I know that this is bubbled and also like a much broader discussion around SSD on the El. Does anyone want to give a quick recap of if not Roberto, did you just go off? I mean, I don't know if I.
00:40:32.854 - 00:40:44.544, Speaker G: Have the full context, but there is broader discussion going on around moving the El over to SSZ entirely. And I think that's pretty coupled with this decision.
00:40:44.672 - 00:40:45.348, Speaker A: Right.
00:40:45.514 - 00:41:39.460, Speaker G: But yeah, there's some discussions and some proposals that have popped up very recently on the discord that it's probably worth everyone involved in 4844 taking a look. Yeah, I think the proposals are around moving all the transactions within a block, whether they're legacy or blob transactions, to an SSC encoding, which of course means you'll have to then translate the legacy ones back and forth between RLP and SSC in order to have backwards compatibility of the hashes. But once you do have everything in SSC, then it makes sense to stick with a hash tree route to be consistent with a consensus layer. And so it makes things kind of nicer in that regard. But it is scope creep. It is enlarging the amount of work that we need to do for capella. So I think that's the big downside.
00:41:39.460 - 00:41:42.790, Speaker G: I mean, sorry for.
00:41:44.760 - 00:42:02.270, Speaker A: Got it. Yeah. And so I've added basically the SSE agenda items, the larger ones on awkward as this Thursday. Is there anything before this decision that on the FOIA four front we should figure out?
00:42:05.600 - 00:42:16.210, Speaker G: Yeah, I don't know. I think we just need to wait and see what folks want to do there. Because I think, like I said, if they decide to go full SSE on the El, that decides the hashing issue for us.
00:42:16.740 - 00:42:17.488, Speaker A: Okay.
00:42:17.654 - 00:42:19.730, Speaker G: The decision is obvious. In that case.
00:42:24.690 - 00:42:43.650, Speaker A: It'S quite interesting, that question about unions in SSD. It is about the entire format of SSD and not about especially will we use it or not? Probably it should be decided.
00:42:43.990 - 00:43:01.850, Speaker G: Yeah. There's another proposal that doesn't use unions but introduces like an optional, like I said, there's a lot of kind of ongoing discussion. I can link to one of the more recent ones. Have it here somewhere. I'll paste it in the chat.
00:43:04.430 - 00:43:52.200, Speaker A: Okay. Yeah. Okay. So both of those I've already put on the agenda for awkwardev. So yeah, if folks want to review that before the call, and then next week on this call, we can figure out how to go based on the decision. Okay, next up, we sort of covered this a little bit already, but basically spamming the devnet. I think Marius wanted to update his TX fuzz repo to better support blob transactions.
00:43:52.200 - 00:44:57.164, Speaker A: Obviously this is a good way we can test any transaction pool implementations. Yeah. Anything else just on Devnet spamming that anyone wanted to bring up? Okay. And then last thing was basically breaking spec changes and devnet five. So from the interop, my feeling was the conclusion was over the next two weeks, so call it until mid February. We want to make sure to get any breaking spec changes into 4844 so that then we can hopefully launch a public devnet five in the couple of weeks after that and then get that live by the end of March or, sorry, end of February, beginning of March. Yeah.
00:44:57.164 - 00:45:50.276, Speaker A: So obviously we covered a couple of these things today on the call already. But if that generally makes sense to people, say we have a call exactly two weeks from today. We try and get all of the breaking spec changes in by then, use the spec as it is on the 14th as a base for Devnet five. And then yeah, we'll have a devnet with hopefully a stable spec two weeks after that. Cool. Anything else? So that was everything we had on the list. We did cover sort of client updates throughout, but if there's any other updates either from clients testing, research that people want to bring up.
00:45:50.276 - 00:46:22.806, Speaker A: Yeah, now's the chance. Okay, well, thanks a lot everyone. Yeah, I appreciate all the work in the past couple of weeks to get there, interrupt and everything we did there. Yeah, see you all on this call next week. Thank you everyone. Bye.
