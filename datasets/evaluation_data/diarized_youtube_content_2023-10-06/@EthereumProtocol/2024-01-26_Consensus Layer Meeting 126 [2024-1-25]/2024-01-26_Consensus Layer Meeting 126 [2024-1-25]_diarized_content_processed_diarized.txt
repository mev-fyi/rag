00:03:44.480 - 00:04:24.776, Speaker A: All right, we should be live. Welcome to consensus layer. Call awkward devs 126. This is issue nine three eight in the PM repo. First off, we will have any discussions related to NEB testing, testnets, et cetera. Then we, we will begin our discussions around electra next planned upgrade, hard fork on the consensus layer, and then a quick discussion or note around some research around stake participation targeting. Cool.
00:04:24.776 - 00:05:36.070, Speaker A: So we go ahead and get started. There was a blog post yesterday containing releases for, I believe, all main net clients for the next Daneb Dankun testnets. Any notes on that other than the existence? All right, great. If you are running validators, please upgrade. If you run nodes on these testnets, please upgrade. If you plan on using blobs, it is time to test them. Any other discussion points around testing testnets or anything related to Daneb for today? Okay, great.
00:05:36.070 - 00:05:57.900, Speaker A: Next up, Electra. So there is an issue in the specs repo. This is 3449 in the specs repo. Specs repo. Here's the link. Okay, thank you, Tom. This is going to be, we're going to move through this list.
00:05:57.900 - 00:07:22.550, Speaker A: We may or may not get to everything today, and the general format will be. One of the authors should give a quick, high level description of the EIP and then just some notes on relative complexity in general, maybe some comparisons to prior work, the components it touches, and if it's cross layer and then open for general discussion. I do appreciate the teams that did have some brief posts on what they are eager to support and not support for the fork, but please also do elevate that here, either as individuals or as a team, just so that everyone gets on the same page. Cool. Any questions or thoughts before we get started here? Okay, this is the beginning of this process. This doesn't necessarily mean that all decisions will be made today and especially as kind of specifications become refined and packaged and people do initial prototyping. Obviously this conversation will continue as it has in the past.
00:07:22.550 - 00:08:09.940, Speaker A: Okay, cool. So looking at that list, the first EIP on there is EIP 61 ten. Is Mikhail here? Yeah, I'm here. Yeah. Cool. Okay, so yeah, this EIP basically deprecates ETh one bridge that we still have today to bring new deposits into the beacon chain. With deprecating ETH one bridge, we deprecate also eth one data voting and the things related to it, like dependency of Cl on the JSON RPC API.
00:08:09.940 - 00:10:14.632, Speaker A: Because currently, today deposits are being fetched via JSON RPC API from execution layer clients, which is kind of maybe stressful for execution layer clients sometimes, and also it creates some problems with configurations from time to time for node operators and developers have to solve those problems. Also it's ever growing data complexity because all those deposits are required to reconstruct the deposit tree to process new deposits. And this is done by CL clients today when they are syncing with the chain. Yet it will be fair to say that there is an alternative proposal to cut this complexity, which is basically deposit contract depository snapshots, which is there is an EAP for that, but this is kind of our ultimate solution for getting rid of this Ethan bridge. Also related thing is the security of the protocol. There is a design flaw that we know for several years already, and for mainnet this is not an issue, but for general the protocol, practically it's not an issue because it requires a lot of adversarial and it requires Al work as well. It requires Yale change, but on the l side it is very well scoped and the change is really minimal, pretty kind of isolated and in terms of testing and in terms of implementation, and there is already a prototype in lighthouse and Bezu and I know that other teams are working on prototyping this EIP.
00:10:14.632 - 00:11:19.580, Speaker A: We ran several devnets during the prototyping phase to see how it works, how it plays out. I would say that specifications are pretty mature and we need some fixes after prototyping phase. Also I would like to say that on the activation, like when this EAP is activated in a network, when the change is activated, it does not require any further coordination on the transition period from ETH one data bridge to this new mechanism. So it's pretty deterministic and will happen automatically. And ETH one data breach will be able to be deprecated thereafter after this transition period finish, which is like several hours. And yeah, it can be done in uncoordinated fashion. So that's mainly.
00:11:19.580 - 00:13:16.170, Speaker A: Sorry for being hectic on it, I did not prepare a presentation, but that's the kind of summary on this EIP. No, that's perfect. I think most people for most of these eips have pretty deep context, so just a brief overview is really great. I think almost everyone's call understands the security of what you could call this ETH one bridge or the deposit bridge has this honesty assumption on proposers over a certain time horizon, rather than just a straight validation of these two systems that are now connected and related to each other. And so it's certainly like a major security improvement, although an unlikely in normal circumstances place for an attack to happen. Are there any questions around this EIP? Both security feature sets, complexity, anything here? Okay, I believe that there has been general signal for support on the execution layer. Was that Tim, would you call that like unanimous meaning in the event that the consensus layer wants to do this and no further issues or having to print things due to complexity, that this would be moving forward as of now? Yeah, I think 6110 and 7002 are the two where if we have to start working on a Devnet tomorrow, along with another one that doesn't impact the CL, it seems like all El teams would start working on.
00:13:16.170 - 00:14:38.430, Speaker A: Is there any, are there any consensus layer teams that are not supportive of this in electra as of their understanding of the fork today? I say not because I've seen a lot of signals for support in both some of these blog posts as well as in the chat. Okay. Generally very positive signal on this on both layers. I don't know if we have terminology, official terminology to use here, but as of now, 61 ten can move forward in electra. Next. EIP 60 914 reusing validator indices dapline do you want to take this one? Sure. So I can quickly say that DZP is useful in the long term, but it's not the time to roll it out.
00:14:38.430 - 00:15:31.710, Speaker A: So very briefly, I would say we should skip it for the fork. I'm not sure if any team wants to include it for the fork, but I'm not sure if that is wise. I did write a hack MD. I can link it on the notes, but that would be all for now. Okay, does anyone want further detail on what this eap entails, or does anybody want to make the case for inclusion? Okay, and I'm seeing from a couple of different teams kind of a general agreement that no need to prioritize this in the upcoming work. Cool. Thank you EIP 7002.
00:15:31.710 - 00:16:00.120, Speaker A: I can give a quick on this one. This is execution layer, triggerable exits. Essentially, just for some context, there are two keys. There's the active key and the withdrawal credentials. The active key manages the stake. The withdrawal credentials ultimately own the funds. Since phase zero, there's arguably kind of a bug in this relationship in that only the active credentials can trigger exits.
00:16:00.120 - 00:17:49.892, Speaker A: And so if the active key is lost, or if there's some sort of more dynamic relationship between who owns the active key and who owns the withdrawal credentials, you can have pretty degenerate cases and degenerate outcomes. Additionally, with execution layer withdrawal credentials, one introduced a couple of forks ago or a fork ago. The ownership of the stake can be controlled by smart contract which is really exciting for trustless pools and all sorts of other more dynamic relationships in staking, but these cannot trigger exits. And so again, you have this pretty degenerate relationship existing kind of across the board in this exciting design landscape that makes the barrier to entry in these kind of trustless pools designs much higher because you then need some sort of committee construction to be able to hold pre signed exits and use them and things like that. Whereas if you're a big entrenched player, that might be a sufficient solution, at least for the time being. Whereas if you're a new entrant, that's like one of those semi major things you have to accomplish. So anyway, 7002 adds what is right now written as a stateful pre compile, although after the way we've done the beacon route in Daneb Dankun, it is going to be changed to just deployed EVM bytecode, which lite client does have some assembly written for this that is under review and will make its way into zip.
00:17:49.892 - 00:18:47.420, Speaker A: That's just a note. This is cross layer. This is relatively straightforward on the consensus layer in that it is a new operation type, but that triggers very well known kind of exit code paths. It's relatively straightforward as well on the execution layer. A lot of the complexity being able to be masked in that EVM code and kind of on a per block basis doing some stuff in relation to the state that is in that EVM code and hoisting some messages into the block as a new validity condition. Yeah, so nothing crazy here. Being cross layer is really probably the biggest complexity component.
00:18:47.420 - 00:20:25.440, Speaker A: And I will say that in relation to things that might happen with maxib now or even in the future, some of the logic on the message being passed around might be future proofed for potential partial withdrawals. Not that that really without changes to the validator, doesn't really make sense today, but that's a pending potential change in there. Yeah, I think this is very important in relation to just like fixing the ownership story of staking and very importantly I think enabling much more trustless designs in on chain pools. Any questions? Anybody want to signal support or against? I guess as Tim mentioned, this is generally conditionally included on the execution layer side. And so in terms of cross layer support we have that. Okay, seeing general positive support. I think we saw general positive support in the blog post that teams released and even notes of top priority from prism.
00:20:25.440 - 00:21:28.126, Speaker A: Is anyone against the inclusion of this in Electra? Great. We will as of now include it in Electra. Please keep your eyes on this assembly that light client's working on. That's the kind of thing that it's very nice that we get to kind of capture a lot of complexity in one place rather than having to implement it across many client teams, but it's something we definitely need to get right. So EVM wizards want to do some review in the coming month or two, please. Anything else on this one? Great. Moving on to EIP 7251.
00:21:28.126 - 00:22:01.930, Speaker A: Mike. Hey, Danny. Hey, everyone. Yeah, so we'll do a quick run through on 7251 and 7547, starting with 7251. I think most people in the call are probably familiar, but to give a quick high level overview, this proposal is to increase the max effective balance. Currently, this is set at 32 e and represents both the minimum balance to become a validator and the maximum balance of a validator. So anything above that balance gets swept off.
00:22:01.930 - 00:22:42.822, Speaker A: So this proposal has a number of features that kind of make sense of it because it seems simple, but there's kind of a decent amount that goes into it. The current list of features I'll link in the chat just to run through them quickly. First is increasing the max effective balance to 2048. 2nd is custom ceilings, which lets validators specify at what level they want the sweep to kick in. The third is a consolidation mechanism by which validators can consolidate multiple validators into a single validator. Fourth would be execution layer triggered partial withdrawals. So this is kind of piggybacking on 7002, which is what Danny was talking about.
00:22:42.822 - 00:23:36.150, Speaker A: And the last one is the initial slashing penalty, making that zero. So I guess the high level reason for doing the CIP is to allow some consolidation of the total number of validators we have. We're currently at around like 900,000, but many of those are representing single entities because many entities have much more than 32 each day. So quantity, for example, runs around 140,000 validators right now. And each of those validators has to sign an attestation that has to be aggregated and then the signature verified for each epoch. So yeah, just trying to cut down on both the network graphic and also improve some of the UX around solo staking. So I have a short super high level list of pros and cons that I'll link here in terms of complexity.
00:23:36.150 - 00:24:38.906, Speaker A: The reason this EIP, I think is a little more controversial, or has kind of been a little more uncertain is because there's kind of a lot that goes into making it effective. One of the most important parts here is this in protocol consolidation, and we just wrote a doc on this recently I'll link that. And the high level takeaway here is that without an in protocol consolidation mechanism, the only way big staking be able to merge validators would be to fully exit that stake and redeploy it as larger validators, which would be a big hit in terms of their rewards. So that's kind of the incentive for the in protocol consolidation mechanism. Yeah, I think in general there's lots that has been written on this. I'll link the kind of general related work doc, in terms of reducing the complexity to get the scope down. I think if there's appetite for that, would be happy to discuss.
00:24:38.906 - 00:25:20.200, Speaker A: Generally have been kind of working under the assumption that in order for it to be worth doing, it makes sense to kind of do it right, just because if we did it and it didn't gain any adoption, then it wouldn't be as valuable. But this has led to some extra features, I guess so, yeah. Happy to keep discussing. I think that's the high level summary I wanted to give there. Yeah. Any questions for Mike? There is certainly some support in the chat, but also some kind of questions and pushback. Sean.
00:25:20.200 - 00:27:08.620, Speaker A: Hey, so I generally support this. I think it's really important to increase the networking load for consensus messages because then we can increase the networking load for things like increasing Bob count. But with regards to the question on how big of a scope do we want in order to how attractive of a feature or how can we get people to actually use this feature, could we also potentially do this in stages where we do the minimum secure amount to increase Max EB while also keeping the other consensus properties secure unchanged, and then in later forks, potentially add things like in protocol consolidation or other of the extended features to sort of not have this feature be too complex if we have too many other changes in this fork, and also enable some of the nicer features like the compounding to happen for new deposits, potentially allow people to deposit aggregated validators. New deposits, yeah. So to respond to your first question, I don't think any of the features that we've described change the security properties at all, actually, especially in terms of the consolidation. That's the main design constraint. It's like we don't want to change either the weak subjectivity or the amount of churn that the protocol can handle.
00:27:08.620 - 00:28:29.520, Speaker A: But I definitely do agree that maybe it does make sense to consider kind of like a mini max vector balance change. It's funny because minimax is like an algorithm. So yeah, happy to kind of like brainstorm on what exactly that minimum feature set would be, I think it would probably not include thin protocol consolidation and it probably would not include, well, I don't know, we'd have to decide about execution layer, partial withdrawals, but yeah, thanks for bringing that up, Sean. Definitely happy to maybe chat more offline and come back next week or in a couple of weeks with a counterproposal. Sean, do you mean just protocol security or do you mean just kind of security in relation to complexity of rolling out a fork? Sorry, what was the question? You mentioned security. Did you mean security as in how to make sure this protocol is secure or security in relation to just managing a complex fork and reducing complexity? My understanding was there were some changes related to how we have to, I think, wait, exit chain, for example, and changes to slashing to keep the security properties the same. And that sort of meant like there is work involved in the consensus changes related to that.
00:28:29.520 - 00:29:27.946, Speaker A: So a minimum change would have to include those along with raising Max Eb. And I'm not sure exactly what else, but something like that. And just to be clear right now, my stance generally, I think is to do a fully fleshed out version of this if possible. But if there's too much in the fork, then maybe there's still a way to do a minimum version of this. Yeah. Lord sir is of the view that we would not want to divide it across two folks, and if we want to implement it, we should basically just do it in the single folk because the context division is more complex. And I think if we can sort of finalize the spec in terms of how we are going to maintain the security.
00:29:27.946 - 00:30:33.840, Speaker A: Basically right now, as per spec, it says that the validator exit churn will be respected, but then there is a certain period in which the operators will not earn revenue. So if we have done the temperature check with the operators that they are okay with that, and then the consolidation can actually happen so that we can see the benefit on the beacon state, then I would say that we should do it in the single fork. Just to clarify one point on the revenue thing. So yeah, in the consolidation doc, we talk about this. The only time that the consolidating validators won't earn the revenue for what we're calling the source validator, which is the one that's merging into the target, is during the time between the exit epoch for the source validator and the withdrawable epoch for the source validator. So it's not based on the exit queue, it's just that 256 epoch delay. So in that regard, yes, they'll lose a little bit of revenue, but it's kind of always bounded and always constant, which is good.
00:30:33.840 - 00:31:41.282, Speaker A: But yeah, thanks again for bringing all that up. Very good context. So the thing with that regard is that are the operators with whom we have done the temperature check with, are they okay with this little loss of revenue on there? Because if the consolidation doesn't happen, then we don't see any benefit. Yeah, we've been working super closely with the large operators and generally have kind of tailored the feature set based on not only what we think will be good for solo stakers, but also what they would actually make use of. So I guess both Lido and Coinbase have been really involved in this process and talked to figment last week, and they're also kind of keen to continue learning more as we finalize the design. But yeah, I guess all the temperature checks so far, especially once we kind of settled on the initial slashing penalty reduction, which was most of their concern, all the temperature checks have been quite good. Yeah.
00:31:41.282 - 00:32:53.172, Speaker A: So as you guys have seen from the blog post, prison signal, no support. And we all agree that we need this EIP at some point, right? But the fundamental reason is that I haven't heard much voice is that do we want like a smaller fork in 2024, or do we want to wait until 2025 to do a bigger fork? And we do think that a small fork is very beneficial. So therefore we just think this EIP has too much complexity to fit in a small fork, assuming it's coming in October and November. Thanks, Terrence. Yeah, and circling back to Sean's point, maybe if there's appetite for that small fork thing, then thinking through what a mini version of the EIP could look like might be extra useful in that regard. So happy to talk to you offline about that too. Terrence Potos yeah, just with regard to this pr, I just wanted to mention that there is no mini version in terms of complexity for us.
00:32:53.172 - 00:33:34.930, Speaker A: Just changing the max effective balance is not just changing a constant. And the fact that we can fork into something that changes the max effective balance, even if we didn't do anything else, even if we didn't change the validator structure, to have this slashing being of bitmask would be very large complexity, at least on Prism's code base. It'd be very hard to test. We do support the CIP, certainly we do support it. We actually want it. We prioritize cpds, and we think this is necessary for that. But I don't see how this can be scoped in any form for 2024.
00:33:34.930 - 00:35:14.666, Speaker A: Does there exist a prototype in any client? I'm working on one for present because I'm working on epds that includes this, and this is the last change that I'll add because this touches everything. So I don't have a product, but I started thinking how I would implement it in prison and my head blows up immediately. Got you. Does anybody else want to echo the discussion of complexity or pushback or provide any more color here? I think the consideration of this might also be taken in context with the other feature, heavy EIP, which is peer Das. So depending upon the peer Das or maxib, we would want the inclusion of at least one of these things. Got it. So there's a bunch of discussion just happening in the chat around small versus big fork and what that means and what the intended timelines are and how that couples with the execution layer intentions.
00:35:14.666 - 00:36:21.518, Speaker A: I do believe that the execution layer quote is angling towards small fork with a 2024 target, knowing that timelines are hard to predict until you get into the meat. This also, given the feature set that we're discussing, looks almost certainly like a cross layer fork. And so the intention, unless we kind of open up a broader conversation, would probably be try to couple complexity in a way that we're kind of hitting a similar intention in terms of when to ship, unless something in here emerges as worth the complexity and worth the time in the discussion. So that's just to kind of contextually frame this. But that doesn't mean that we can't debate that. Potos. Yeah, so we didn't start this meeting trying to get an agreement on how to scope this fork.
00:36:21.518 - 00:37:33.578, Speaker A: I don't really care about when it actually happens, but what is the scope that we're going to have and what are the dates that we have in mind? So I am under the assumption that in the previous meeting there was some sort of loose agreement that we should scope it for the end of this year. If this changes, and this is not a reality, then prison would have a completely different fork in mind. If we had an agreement that we want to scope the fork for 2025, then we would be pushing for not only Max EB, but also inclusion list and epbs in it. We would want to have a larger port, but we're very conservative because we know that we haven't port ever twice in a year, not in the CL. And I think it's not realistic to try to put this many ips if we're scoping for this year. Got it. And we can challenge that notion of scoping for the year, but I think that is the intention as of now, unless it becomes challenged and we open up that conversation again.
00:37:33.578 - 00:39:07.224, Speaker A: So I do think that's the correct framing as we move through this. Okay, so no one is against a Max EV type feature to make it onto main net at some point. People generally agree on the value here and the necessity here. There is disagreement as to the relative complexity, especially contextualized, in how this next forp is being framed. And I will also say that it's also a matter of balancing the complexity with the expected outcome, because there has been also some debates about how this feature will be used at the end by big node operators. So we got a very high complexity and not certain outcome on the benefits that has been designed here. So there is also an unknown behind the scene, right? And as Mike said, there's been a lot of attempt to kind of proactively mitigate that and ensure that something's being built that will be used.
00:39:07.224 - 00:40:09.194, Speaker A: Obviously when push comes to shove, how people choose to run their businesses is not up to this group, and the timeline in which this kind of thing might be adopted is not up to this group. So there is uncertainty, although it's been attempted to be mitigated. Would anyone like to continue the conversation on this, or is the preference to look at the next handful of eips to wrap our heads a bit more around the fork, the complexity, the intention of the rest, and to come back to this in further discussion. Okay, that's what we're going to do. Next up, EIP 7547 inclusion list. Is this also you, Mike? Yeah. Cool.
00:40:09.194 - 00:41:18.674, Speaker A: This may be a shorter kind of list of things, but yeah, I guess starting with the kind of motivation here, Tony made this awesome dashboard called censorship Pix. It shows the relative censorship for different operators in the block production pipeline. I guess the one that has changed recently that is kind of very relevant is the amount of builders that are censoring. So currently 67% of builder blocks are censored and about 95% of Ethereum blocks, 90% to 95% of Ethereum blocks are metaboost blocks generally. So yeah, I guess a large chunk of Ethereum's blocks are censored. So motivating the need for inclusion lists, I think is just getting some kind of version of a forced transaction inclusion mechanism on chain to increase the censorship resistance and kind of make it more robust to these types of weak censorship attacks. So, yeah, I guess in terms of complexity, I think this EIP is much less complex than 7251.
00:41:18.674 - 00:41:56.666, Speaker A: There is one kind of major design decision that I want to bring up, which is this kind of unconditional versus conditional enforcement of the inclusion list. This is a doc that is in draft. We're kind of just working out the details right now. There's been a lot of conversations over the past few days, but yeah, so kind of making sure we have the design of exactly what it means to enforce the inclusion list is super important. But I guess generally the design looks something like this. No free lunch version. And yeah, I guess the inclusionless stuff has been discussed for a long time.
00:41:56.666 - 00:43:04.290, Speaker A: Like Vitalik and Francesco have been talking about it since, I guess, 2021, 2022. So kind of just taking a lot of this research and trying to get it closer to fully ready to go on chain, I think is the general pushaway. Terence and Potez have all kind of poked around at different parts of the inclusionless spec. I'll just send a link to the current pull request that was opened 6 hours ago. Yeah, I guess in terms of complexity on the l side, there's not as much because most of the validation happens when you check the enforcement of the inclusion list as a precondition of block validity. And also there's some amount of work that needs to be done on the peer to peer to make sure that when a block is gossiped, there's a corresponding inclusionless gossip with it before you consider the block for the head candidate. So, yeah, there's a little less written on this, but I'm also compiling a related work document in this gist here.
00:43:04.290 - 00:43:51.492, Speaker A: So happy to share that. And definitely, I guess this is a relatively newer EIP, but I think the ideas are very well established and important for kind of one of the core properties of Ethereum, which is that censorship resistance. So would be super excited to kind of keep jamming on this and keep prioritizing this in the near term, if that's what there's appetite for. Yeah, Terrence. Yeah, I can give a quick brief background on this. So this was essentially derived as part of the EPBS project with the EPF. Sorry, EPBS project with the EPF last year.
00:43:51.492 - 00:44:53.592, Speaker A: And as you guys have heard that polis and I have been working on EPBS spec for the last few months, and this inclusion list is basically part of it. We think this is too small to be included in 2024. Right. And then if it comes down to 2025, I think from our end we will push heavily for epbs because this is part of epbs already and which we will have more to share in the next coming month. Yeah, just wanted to point out one tiny thing is that you don't need epbs to do inclusionless, just to make sure everyone's on the same page at some point. I think we did think that was the case, but now we do feel confident that the design would work with or without epbs. But yeah, I do think the relationship there and the work you guys did with the EPF was super valuable.
00:44:53.592 - 00:45:33.344, Speaker A: So thanks for that. Other questions or comments? I have much less of a read on how client teams are thinking about this right now. So some verbal discussion around support or questions if you don't have a good understanding of this would be great, Sean. So yeah, this seems cool. I'd want to do something like this. I would support something like this. This one does strike me as pretty complicated though.
00:45:33.344 - 00:46:36.320, Speaker A: From when I was going through the EIP, it seems like there are changes to sync, for example, and gossip, and this would probably require changes in the engine API and the beacon API. So it's like similarly touching a lot of parts of the client to what it was like making the NEB changes. On the other hand though, we now have some experience with passing around data like blob data. So maybe it won't be as complicated, but yeah, generally it struck me as like relatively heavy complexity. Photos yeah. Since I have this design very fresh in my mind and I just want to confirm with Sean still, this is much smaller scope, at least in Prism's code base, than maxib. So it's much easier to implement for Prism than maxib.
00:46:36.320 - 00:47:32.550, Speaker A: But it's true, you require to have new sidecars for the inclusion lists exactly like the net blobs. You require to have new endpoints on the engine API so that you get an inclusion list from the EL and you send the inclusion list to validate to the EL. There are already prs, some of the EPF fellows or some of the EP fellows that work with us already worked out the list validation and the list production on Gaeth. So there's some code already to be seen for El. On the El side. On the CL side, the heaviest core changes are around gossiping because you need to gossip these new objects, and on four choice because you need to validate these sidecars to even say that these blocks are valid to even put them in for choice. Cool, thank you.
00:47:32.550 - 00:49:34.830, Speaker A: Yeah. Any other team want to kind of discuss how they're feeling in relation to CIP right now? Yeah, speaking for mean, we support the CIP, but we don't strongly feel about it because we feel that not all options have been exhausted. There is should override builder flag in the EL and using that lot of censorship can be dealt with if this particular flag is actually implemented in the El code base and is shipped and it does not require any consensus hard fork. So for loadstart that path is a bit preferable because again, if we want to keep the scope of the folk small, then we would basically vote in favor of some feature EIP than basically this one is the general understanding of the should override builder as kind of a band aid that will help for a while, or is there an understanding that that might be like a long term viable solution? Pothos I don't share loadstar reading of this. An inclusion list is something that the Yale is suggesting the proposer to force include. It's kind of too strong to guess that the transactions, I mean we are seeing 90% of blocks already censored. So if say particular contracts have transactions that are outstanding, then we're going to be not taking any builders block with this flag instead of trying to choose builders that are non censoring.
00:49:34.830 - 00:50:52.460, Speaker A: I think overblowing this flag is just too much of a leap. But we haven't even seen this flag in action, so we don't even know that how useful this flag is. Definitely, and we need to see what are the heuristics that are going to be implemented on the else. Hopefully this is going to be implemented soon because there are some simple heuristics and hopefully it's going to be against strong censorships, but not something that would just disrupt the network by trying to override every single block. Because we currently have statistically 90% of blocks produced by many blocks produced by censoring builders. But if this flag is actually implemented, then it would achieve the same purpose that inclusion list would do in the sense, no, it can't because the proposer sends it before you get the block from the builder. So this flag does not allow you to see the block from the builder with this flag.
00:50:52.460 - 00:51:24.520, Speaker A: The EL is going to signal there is already censorship on the network. You should not even ask for the builder's block in the next slot. You should not even consider the builder's block on the next slot. This is because the way we have met boost you don't get to choose after you see the block. So this is very different than inclusion list. Where in inclusion list you're going to say to the builder, give me a block that has these transactions, and then you're going to filter out every censoring builder with this flag. You filter out every builder.
00:51:24.520 - 00:52:55.630, Speaker A: I understand that inclusion list is a stronger way to prevent censorship, but yeah, also note builder could be a first step that can be taken to see whether this flag can have effect and prevent censorship. And POTUS, I guess the convinced layer also gets to add their own heuristic in relation to that flag. So 100% return value of it doesn't necessarily mean 100% usage of it. I know that's not necessarily the point. Here are any consensus layer teams making the case for inclusion of 7547 in Electra. Given the knowns, the unknowns, and the kind of general intention of the scoping of the fork that was asking anyone making the case, any positive anybody wants to say yes, this should be included, this should be prioritized for this work. All right, we're going to set aside for now.
00:52:55.630 - 00:53:22.280, Speaker A: Again, the conversation will continue. So it's not necessarily death sentence, but this is not currently being prioritized or included. Potas, just a quick note. Again, the same note as before. If this fork happens to change the scope for next year, this would be a very top priority for prison. We believe this is strongly needed and we can do it in the next few months. We can have a proof of concept at least of this.
00:53:22.280 - 00:54:30.168, Speaker A: Thank you. Okay. EIP 7549 move committee index outside fistation DAP land hey, so DziP clears technical depth from the previous roadmap where we were going to execution charts. There is no reason now to sign over this bit of data. Signing over it means that we cannot aggregate equal votes in terms of FFG lndgost. This has very significant implications not to our own clients, but to any other application that tries to prove finality outside of the clients. For example, breaches that want to develop zero knowledge proverb, they would immensely benefit from verifying, much less pairings.
00:54:30.168 - 00:55:43.230, Speaker A: With this optimization, it can significantly accelerate the delivery of trustless solutions for basically bridging from consensus networks to other networks. So it's not as important to ourselves to improve the efficiency as our clients, but to other consumers that want to prove consensus in some restrained execution environment. Is there a marginal difference? Beacon block verification at layer one or is? Yeah, we would benefit, but I think clients are already kind of optimized. That would be not like a significant difference. Maybe cpu would go from 25% to 20%, but it's not a game changer. It is a game changer for these scenarios that I'm talking about, like ZK breaches and these sort of applications. And from a complexity standpoint, this is a very minimal of the way you specified it kind of leveraging a helper in phase zero.
00:55:43.230 - 00:56:47.662, Speaker A: This ends up being a very minimal change, really changing just a couple of data structures and ensuring that we kind of get the tests ripple through the test that way. Yeah, so there are different ways that we can deprecate the existing committee index, the most basic one where we just keep everything as is and just set it to zero. It should be a really trivial change. We can go more crazy about it and scope creep the hell out of this, but if we keep it tiny, it should be really painless. Danka. Yeah, I mean, I think because the current draft mentions all three options, I think for a small fog only setting it to zero is realistic. I think realistically, given all the infrastructure that has been built around signing things like protection and distribute validators and so on, I think it's a major change to change the format of attestations.
00:56:47.662 - 00:57:44.656, Speaker A: But if we set it to zero, then it can be a very small one. Yeah, I echo that. All right. I believe we've seen very positive sentiment from all client teams and no dissent. Does anybody want to bring up any reason not to include this in Electra? And it probably goes without saying, this is not cross layer. Okay, as of now we will include 75490. That's funny.
00:57:44.656 - 00:59:45.710, Speaker A: The next EIP 7594 is peer dust. I can give a quick overview on that. So data availability sampling in some form is the method to go from port four, four levels of scale where everyone's downloading all blobs to extended scale. Beyond that, call it full dank sharding by not having enabling nodes to be able to ensure that data is available without downloading all data. This has been a very high r and D task for quite a while now, and almost all of the complexity, although a very elegant mathematical construction, ends up being in the networking layer and kind of the practical instantiation of doing this in a distributed context. Peer Das peer data availability sampling is an attempt to utilize known, very well tried and true known networking components, and to get a kind of basic data availability sampling out the gate, utilizing discovering peers in a similar way that you discover peers for at nets, utilizing gossip and subnets, and utilizing a diversity of peers for sampling. It also does attempt to leverage a notion of data availability sampling providers by allowing peers to custody more than the honesty requirement and let that known to peers such that if there are funded or altruistic nodes or whatever that want to serve the network that can kind of naturally slot into this.
00:59:45.710 - 01:01:12.856, Speaker A: This is something that has a specification, this is something that multiple people are working on prototypes of. But it's also something that compared to 7549, has quite a bit of complexity and quite a bit of things to work through. That's not to say that it isn't everything that it utilizes is very well known, but putting the pieces together certainly on these networking things can be a challenge. Cool. Terrence, I mostly have questions, so my first question is are we increasing the target and meds count? If yes, then I feel like maybe we probably should wait until how EIP 4844 is to see if there's sufficient demand and if today we are not increasing the target and math count. Is this backwards compatible changes in a way that if we just apply like a data sampling network on top of the current four account, do we even need like a hardware? I understand we probably need like an epoch such that the peers can pay attention to the subnet. But besides that, are there consensus changes as it is specified right now? There are no consensus changes.
01:01:12.856 - 01:02:26.840, Speaker A: Yes, you're right in that if you're going to leverage these new gossip topics and things, you would want to premise it upon an epoch. That is not to say that peer DOS should not be coupled with a data gas limit increase. I just personally think that that would likely look like a separate, very self contained EIP that just looks like essentially a gas limit increase EIP, which I guess in the event that people want to move forward with 7594, I would argue that it's an open question as to whether and to what value you would do that at an initial fork. Dockard, you are. Yeah. So in terms of what terrence said about the demand, I think what is important to realize is that I don't think there will be a question about demand for 4844. Right now we see roll ups are already using about net, but this has grown by a factor of ten over the last year.
01:02:26.840 - 01:03:53.718, Speaker A: The data amount right. So the amount right now is on the order of one block per block, but this has grown by a factor of ten over the last year. I'm going to share the graph in the chat, but I think it's very important to realize that this will very quickly become an urgent thing. I think because we will run quickly into the regime where roll ups will also question why are we using 4844 at all if it's not cheaper than call data? So yeah, I think the demand is the smallest worry I have about this. I think that will be very obvious very quickly after 4844. Comments, questions how people are feeling about this DAP plan? Yeah, I think every other EIP has made it, but scaling remains the best investment in terms of time and output. There is a very clear return on investment investing in this field.
01:03:53.718 - 01:04:52.440, Speaker A: So it seems very unwise to not have this as the top priority, basically constant until we deliver full and sharding, provided that we do all the other good things on the back as it allows. Yeah, I was curious about the crypto side of things for this, mainly like the efficiency of verification of large amount of the greater amount of blobs here. So what is the state of the cryptography libraries basically for peer dress? Is Justin Traglia or George here? Or maybe donkey? If you have. I will. I don't have the numbers in my head. So what you're asking about would not be the blobs, because we wouldn't be verifying full blobs anymore. Instead we would be verifying samples.
01:04:52.440 - 01:05:52.090, Speaker A: And I do not have the numbers in my head. I am pretty confident that it's not going to be a problem, but I will quickly look, I think Justin compiled something and I will find those numbers. And Justin has no mic, but can maybe provide a resource or some quick data for us if possible. And there's also the question of not only efficiency, but just the state of these libraries. And I know that there's being work done, but I can't make a claim as to what that works looks like in relation to production. Yeah, I think the complexity is not enormous, so I'm fairly confident that we will be able to finish all the work on the libraries well in time. We already have a c implementation and I think we have everything we need to do that very quickly.
01:05:52.090 - 01:06:35.460, Speaker A: Would you call it iterative or fundamental in relation to what exists for four? I mean, there are some fundamentally new things. Yeah. But most of the. It's mostly very similar to what exists already. The one big piece that changes will be the computations of the samples, which you don't have to do unless you produce blocks. So that will be on a slightly different. Yeah, that will have much higher complexity than the other things we've done.
01:06:35.460 - 01:07:32.694, Speaker A: But the verification all stays very simple and the additions are quite small, I would say potas. Yeah, I'm going to sound like a broken record, but I'm not worried about the cryptography nor all of these changes that are on software, that are isolated changes. I mean, they can be worked in parallel from everything else, and they can be self tested, they can be unit tested and they can be self audited. We've learned something from Daneb is that testing, networking changes, gossiping new objects. Testing changes to gossip is complicated and it takes time. And we were under the assumption that we were going to prioritize. This is what led all of our decisions, that we were going to prioritize vertical on 2025 by scoping this in 2024.
01:07:32.694 - 01:08:12.760, Speaker A: I don't see how can we test this and vertical in parallel and ship something like this by this year. That's the reason why we were not supporting this for this small fork. If we scope it for this year. Let me clarify something you just said. You say you don't see how you can test this in Verkel in parallel in the event that this were not included in what is attempted to be scoped this year. It seems like a very obvious thing to work on in parallel to vertical because it is very independent. Yeah, definitely.
01:08:12.760 - 01:08:55.310, Speaker A: But then we'll have like a year. I understand, but then you will have the double the time to test this instead of just test this and vertical in parallel and ship this by this year. So we're going to have to have this ready and be tested in a period of the next five months, six months probably, Max. Sean, my perspective is sort of that Daneb actually helped us get a lot better at testing these types of changes. So I would be more confident that we could test pure Das more quickly. I feel like the tooling's got better. DevOps has been putting in tons of work.
01:08:55.310 - 01:10:08.160, Speaker A: Yeah, I'll also say the premising fork choice, inclusion and things based upon alternative network messages was probably like a major hurdle in working on portfolio four, whereas this is leveraging the same logic, obviously can be very different in many ways, but maybe that also reduces the complexity. I would hope, and I do agree, Sean, we have gotten better and even from here hope to get much better. Perry? Yeah, I want to mention two things. I think the first one is that like a lot of people have said, we didn't have a lot of the testing tools we needed for effectively testing network upgrades. And I feel like now the tool gotten to a stage where we actually can do that quite reliably. And we've kind of been spending time testing viral in parallel already. And one thing we've learned is that most of the worker changes would benefit greatly from Shadowfox.
01:10:08.160 - 01:12:18.480, Speaker A: So we have a way to do Shadowfox, but in an automated manner such that you don't actually need to call someone from DevOps to do them, you can just run them and like a CI test. And at least our working assumption is that if we can deliver that over the next month too, then most of the EL teams can, in a siloed manner, test transition at least, and they only need us at the point when we're talking about doing the transition with the network complications, which would most likely be later in the year, at which point we've gotten farther ahead with peer dust. So we're trying to parallelize at least our time such that we're not blocking either party. Yeah, even if it were not included, intended to be included for Electra, I would still make the case for spending quite a bit of cycles on it in parallel, assuming that Electra was kind of a bread and butter fork with respect to very well scoped and understood consensus changes, I think that hopefully would keep some resources available to work pretty hard on this in parallel. And if that is the case, you could make an argument for sensitive inclusion and Electra as like kind of the parallel r and D track, but that to not hold up what would otherwise be a bread and butter fork in the event that we hit unknowns, maybe that's a reasonable compromise. I don't know if we need to or we'll have the time to fully make this call today. We have 20 minutes left.
01:12:18.480 - 01:14:40.910, Speaker A: We have the ssification set of eips, and we also will save six minutes at the end for quick research update do we want to table this and think about where it may sit and if there are r and d compromises here, or do we want to continue this conversation right now? Does anyone else have anything to add additionally to this conversation right now? Okay, I think that likely in plus two weeks we'll kind of pick up the conversation around some of these more complex things and where we sit in relation to them, in relation to electra or potential future ports, this being one of them, maxi B being the other obvious one. Okay, etan ssdification hey, so it's a couple different pips, but they all have a shared motivation, namely that right now every application that builds on top of Ethereum, they use this JSON RPC API, and normally they have to use a trusted server for that because there are many responses in that API where there is no efficient way to verify that the response is correct. For state balances, NFT ownerships, we have eth get proof and Verkel will make those proofs more compact. So there, it's solved. But there are three other trees, namely transactions, withdrawals and receipts, where additional changes are necessary to make it viable to use those, to create proofs that we can put into those JSON RPC responses. Why is it important that there are proofs there. Right now, the big providers of JSON RPC, such as Infura, they have privacy policies where they lock your user data.
01:14:40.910 - 01:15:32.570, Speaker A: They tie together your ip with all the wallets that you are accessing. So there is like a privacy concern there. And there is also a security concern because you are trusting a server for providing the correct response. So with those proofs, it becomes possible to verify that the response is correct. They can still censor you, but you can just ask a decentralized network of servers, like they cannot send incorrect data anymore. So one key for transactions that is necessary to introduce is a commitment to the transaction id. Right now, transactions have an id, but it doesn't exist anywhere in the Merkel Patricia tree.
01:15:32.570 - 01:16:39.160, Speaker A: So in the blockchain, the transaction id doesn't exist. So any request by transaction id, such as obtaining its receipt or its transaction data, it takes an id and you cannot even verify that transaction actually exists. On chain, you have to download all the transactions and all the receipts to verify that a transaction actually is part of a block. Right now, it could be optimized a little more with a Merkel Patricia triproof, but it's not efficient. That's also one thing that Protolamda brought up just a couple, like maybe an hour ago in this EIP for receipts that it's important for breaches, because a receipt can be multiple megabytes. So it can even be that like a fraud proof doesn't even fit into a transaction. So having a standardized way on how to mercalize a receipt and a transaction would help with layer twos as well.
01:16:39.160 - 01:17:31.990, Speaker A: So what is being added is this transaction route concept. So we can have Merkel proof that a particular transaction route is included on chain. That would also help simplify the inclusion list, because it is no longer necessary to include the full transaction in every block multiple times. So we can just include the transaction route there. And it also includes the from address of who is signing the transaction right now. That is also not on chain data. You actually need the full transaction right now and then hash it to obtain the signer from the signature and the transaction hash, which is quite an expensive operation for embedded devices.
01:17:31.990 - 01:18:38.780, Speaker A: And it is also opening up, for example EIP 8102, which proposes BLS signature scheme, but without from address commitment, it becomes impossible to efficiently deploy such an EIP. So it fixes that. It adds the from address and the transaction group. For receipts, we are adding the contract address. If someone is deploying a new contract right now that is also not part of on chain data. So adding that there is important, and also there is a slight change to the gas used right now in the receipt, there is a field called cumulative gas used. So if you want to know how much gas was used by a particular transaction, to compute the correct gas usage, you need to download multiple receipts and then compute the difference to see how much an individual transaction actually used.
01:18:38.780 - 01:19:49.090, Speaker A: So changing this to actual gas used by just a single transaction simplifies that well, so you can download a single receipt, you know how much that transaction used. By not including cumulative gas used anymore, it also opens up future parallelization in execution, because you can now process multiple transactions at the same time, as long as they do not touch the same storage slots and accounts. This cumulative gas use is the final part that right now prevents that, I think. Yeah, so those are the three different routes. There is also this signature scheme proposal, the three other eips, they could be implemented by simply converting all the existing transactions and receipts from new blocks onward. So this only applies to new blocks anyway. And we could, while building the block, convert the transactions from RLP to SSE to gain those advantages.
01:19:49.090 - 01:20:34.514, Speaker A: In 6493, there is also a scheme to create transactions directly like this. So metamask could directly sign an SSC transaction to avoid the conversion step. And SSC is also about 30% smaller than RLP when stored on disk because it uses a compressed format. It's called snappy. It's the same one as is used on consensus because there are a lot of zero bytes in receipts. Especially. It saves a lot of data as well, pushing out the four four a little bit more like giving it more time.
01:20:34.514 - 01:21:34.050, Speaker A: It's still an important EIP. And yeah, the final EIP, this 74 95, it essentially defines how we want to represent transactions, receipts, withdrawals. It is a format that is forward compatible to avoid a situation like we have right now in consensus layer. The problem there is, for example, rocket pool. They query the beacon state to see whether a validator is slashed. And right now, every time that the number of fields reaches a new power of two in the beacon state or in the execution payload, the proof format changes. So they have to update all of their proof allocation logic to consume the new formats.
01:21:34.050 - 01:22:26.760, Speaker A: With 74 95, this is no longer necessary, because any proof will continue to work even if future additions or removals change the actual data. So if you just care about slashing, as long as slashings are a concept, you can write a verifier right now. I mean, after 74 95 is deployed, and it will continue to work until slashings are no longer a thing conceptually, regardless of other changes. Yeah, but it's different. Like we can choose to use a different approach. Of course, if we say that it's fine for clients to have to update the code continuously. Yes.
01:22:26.760 - 01:23:30.726, Speaker A: Does shifting to a stable container change the mercurialization? It changes once, yeah. So any container that is touched changes it once. I have proposed to just do it for where it's necessary for transactions and receipts, but it could also be done for other structures. Of course, one thing to keep in mind is that 74 95 also introduces a way to represent optional values. So right now in the virtual consensus specs there is an optional value. I'm not sure if the final design will still include it, but if we are going with a vertical design that uses optionals, we should decide how we want to represent optionals. So at least the 74 95 must be decided before vertical, and the transactions encoding needs to be decided before inclusion lists.
01:23:30.726 - 01:24:35.278, Speaker A: Those are the two dependencies. I have also created a demo where you can explore how a transaction looks after this EIP. For example here I just picked a random one and put it in the chat. The first link is the transaction SSC, the second link is the receipt SSE. And if you want to look into RLP, you just delete the SSE part from the link if someone wants to explore it works in the web anyway. For almost all of this, the high complexity component ends up being on the execution layer, whereas it's more of kind of bookkeeping on the consensus layer, except potentially using stable container. If you stable container the whole beacon state or something, that might be a reasonably higher complexity change.
01:24:35.278 - 01:25:22.922, Speaker A: Yes, and of course the transaction representation in Cl and El, it would be aligned to be the same. So right now the Cl and El use a different transaction container. And before we move on to questions, was this brought up on the execution layer call? And is there any signaling over there that we should know about before we begin this discussion? I didn't bring it up for a while now. I just joined this one because I saw Electra being discussed. But the SSE proposals are there for a while now. Gotcha. We are going to shift gears in about a minute.
01:25:22.922 - 01:25:45.382, Speaker A: Guillaume, let's do the question and then the SSD vacation. We can continue in subsequent call. Guillaume. Yeah, not really a question, just a remark. So yeah, the vertical spec could change. That's not a problem, especially if we don't guarantee that proofs will be available right at the first vertical fork. They might be activated later.
01:25:45.382 - 01:26:44.450, Speaker A: So yeah, there's no dependency there. Just what I wanted to specify. Got it. Given that a lot of the complexity of this does lie on the execution layer, I do think that we should hunt a bit to getting some more signal from them. And depending on how that goes, we can put this on the agenda for the next time on the consensus layer call. And also, even if that doesn't go well and people have questions and further comments from the consensus layer side, we can also discuss in plus two weeks. In two weeks we are going to bring up discussion of Maxv and peer Das again in terms of complexity, in terms of priorities, in terms of fork or not, in terms of even if not fork, how to think about them as R D items to help accomplish our goals over time.
01:26:44.450 - 01:27:30.076, Speaker A: In plus two weeks and for the last five minutes, we do have some time for kind of an R D update discussion, or at least the beginnings of such anscar. And Anders. Yeah, everyone, I think there's a powerless guy. And also please excuse the beeping in the. So basically I just wanted to briefly select a topic that could potentially contribute like one more candidate, electra eip. And that's kind of the staking reward mechanism. The way to think about it is that the original kind of staking reward curve was chosen pragmatically to kind of get the beacon chain off the ground.
01:27:30.076 - 01:28:52.764, Speaker A: And then we haven't really looked at it in much detail since. And then of course, by now we have a lot of experience looking at the real world kind of usage for the beacon chain and for staking, and a lot of interesting mechanisms like lsts and now of course restaking popping up, which are great to see, but of course kind of were hard to predict. So over the last few months, kind of on the research side, we have started to look a bit more into that mechanism again. Of course, the idea is to kind of make sure we have a situation in place where Ethereum is just long term stable and needs some further changes. I don't think it's certain necessarily to us that we do need changes from currently, but basically the idea is to project forward. What if we basically don't touch the staking mechanism? Where will we be in twelve months, 18 months, 24 months? Right. Like three, four, five years from now? So basically, what path are we on? What properties does that have? Do we like that? What alternatives could there be this kind of exploration, basically in terms of communication there of our thoughts, we'll have some kind of articles and write ups out on that over the next couple of weeks, and then a decision that we might want to make is around.
01:28:52.764 - 01:29:51.152, Speaker A: Are we comfortable with the path we are on? If we are at least kind of not sure we're comfortable with that, there could be a small eip we might want to get into lecture to basically just tempt down the rewards a little bit, to basically slow down any potential kind of future staking ratio kind of creep up that could happen. That would also, by the way, alleviate a little bit of the constant pressure on total validator count. But again, this is all kind of, I mean, now we are at the end of the call, so the idea is not to have a good discussion now it's just to flag the topic and say that by the next call we'll have some kind of material out and then we can kind of start to have that conversation. Yeah, and that's all for now. I think Anders is basically the first one that already has a write up out that you published yesterday. Anders, do you maybe very briefly want to say what you've looked at? Yeah. So you say that.
01:29:51.152 - 01:31:03.992, Speaker A: Yeah, I posted yesterday evening on it research and I just write a link here. Basically what I'm looking at is sort of what can be done to the issuance level and what happens to various important features such as consensus incentives and reward variability for sold status if we are to change the issuance level. The conclusion is essentially that if we wish to temper issuance, we would like to do it with a rather moderate approach because if you do it too aggressively, we would sort of unbalance the different economic forces we have, Mel and issuance. And this could then render the micro incentives of the consensus mechanism a little bit ineffective. Because if we push down, if we push down rewards so much that the stakers only receive MEP, then there's a lot of sort of bad things that happens under the equinox. And so then I just review sort of what can be done and what cannot be done in terms of different reward curves that could be explored. And I showed some Vitalik does quite a nice summary also of this in a comment on that post.
01:31:03.992 - 01:31:50.360, Speaker A: So you could read that if you want some sort of quick overview. That's it. Great. So do check out the post. I think the intention is to, there's some subsequent posts and research to come out and the conversation will be furthered at plus two weeks or plus four weeks time. But this is just kind of setting the stage that there is intended to be a conversation around some of these forces. Any single one question we have time for? Yeah, okay.
01:31:50.360 - 01:32:18.770, Speaker A: Take a look at the post. Hopefully be some other stuff between now and a couple of weeks from now and towards the latter component of the next call, we can dig a bit deeper into some of the research concepts and potential things to do. Thank you. That was a really productive call. We got through a lot more than I expected. Talk to you all on the execution layer. Call in a week and then we'll pick up this conversation in two weeks.
01:32:18.770 - 01:32:26.270, Speaker A: Thanks. Bye. Thanks, everyone. Thanks. Bye. Thanks.
