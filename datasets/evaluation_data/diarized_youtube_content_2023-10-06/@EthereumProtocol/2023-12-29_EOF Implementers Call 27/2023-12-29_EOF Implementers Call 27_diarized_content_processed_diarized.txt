00:00:00.410 - 00:00:00.960, Speaker A: You.
00:00:02.930 - 00:00:29.960, Speaker B: Hey everybody, welcome to the EOF implementers call number 27. I think we talked about Elf on all court devs a couple of weeks ago for the first time in a while and had a pretty good conversation about it there. And yeah, I'm curious what people have been up to in the last couple of weeks on the client side. Does anybody have an update they wanted to share?
00:00:41.230 - 00:00:41.882, Speaker A: Not everybody.
00:00:41.936 - 00:00:46.010, Speaker B: All at once please. Hyman.
00:00:48.290 - 00:00:50.560, Speaker C: Hello, can you hear me by the way?
00:00:51.010 - 00:00:51.760, Speaker B: Yes.
00:00:52.850 - 00:01:00.814, Speaker C: Okay, in Netherlands we are re implementing UF because our branch has been stale.
00:01:00.862 - 00:01:03.362, Speaker D: For a while right now and trying.
00:01:03.416 - 00:01:06.270, Speaker C: To catch up quickly with the mega.
00:01:06.350 - 00:01:10.840, Speaker E: Spec and that's what we're doing right now.
00:01:14.890 - 00:01:15.640, Speaker A: Cool.
00:01:17.050 - 00:01:18.840, Speaker B: Anything on EvM one?
00:01:28.120 - 00:02:03.660, Speaker D: I don't think we have much. Many people are off these days. We still have several implementations in progress of the changes to the spec that were discussed in the last calls. So like disallowing unreachable sections is one and new design for create three and four is also in progress and also making create four address unicodes by hashes instead of indexes. So these are the changes we are working on, but still in progress.
00:02:05.140 - 00:02:10.960, Speaker B: Sounds good Dano, anything on basis side for updates?
00:02:11.620 - 00:02:38.370, Speaker E: No, I've most been writing presentations and doing video advocacy to try and get EOf to happen. So one question I was looking into possibly doing jump fi and call fi. Just had some questions like what are the opcode numbers going to be for these proposed operations so that can be discussed later on. It's my update.
00:02:41.110 - 00:03:00.890, Speaker B: Thanks Dana. I guess that's probably all the client updates. Let's go through anything on the compiler side and then we can jump into some spec discussion. I see we've got Daniel here. Any updates from solidity on EOF?
00:03:02.030 - 00:03:34.950, Speaker C: Not really. Yeah, I mean we've had the implementation of the previous version last year, but since then, since I'm a bit skeptical about the guarantees that all cortex can make for it ever happening, was a bit hesitant to spend time re implementing the new version. We may do that, but yeah, still catching up with the changes we've discussed internally with ipsilon. A lot of the changes, but from our side we haven't yet updated our implementation to the latest version.
00:03:37.370 - 00:03:43.480, Speaker B: Thanks for that update, Daniel. Charles, anything to share on Viper side?
00:03:44.490 - 00:03:45.880, Speaker A: No, about the same.
00:03:48.430 - 00:04:04.430, Speaker B: Okay, all the updates are done then. Shall we jump into spec discussion? I don't think I saw anything specific on the agenda. What do you guys have on your dockets?
00:04:06.850 - 00:05:13.774, Speaker F: Yeah, I think we discussed it in the channel, but maybe it never made it into the agenda point I think there was basically two points we really wanted to discuss, especially that we have both Daniel and Charles on the call. So the two questions regarding language implementations, and one of them is the various versions of the swap dupe. So eip six six three. The other point was the stack validation or air jump if, and I think lastly it's just really the call to size or versus gas regressions. I think these are the three topics of which we really need to make a decision on. Probably I would suggest to start with the stack validation, because Andre wrote up a lengthy explainer of that topic, and we did share with Daniel, I believe, Charles, you also read it, maybe that's like a good one to start with. I'm not sure.
00:05:13.774 - 00:05:17.990, Speaker F: Andre, do you want to lead the discussion since you wrote the document?
00:05:22.280 - 00:06:44.400, Speaker D: Sure, I can give an intro. It was written by Paolo Marshall. It will actually be introduction. So in two words, there is a problem with the current stack validation, which makes things difficult for compilers, or leads to code size regressions where they want to jump into helpers from different stack heights, and current stack height validation restricts this to any instruction in the code to be reachable only from the same stack height. And so we've thought about one algorithm that loosens this restriction, but makes it the algorithm itself becomes a bit more difficult or complicated. And there's also an extra requirement for how the code blocks are ordered in the code section. Specifically, any block is reachable via a forward jump or just a flow of instructions.
00:06:44.400 - 00:08:23.500, Speaker D: So it means that no blocks are reachable only via a backward jump. And then this way we can go over all instructions in one pass, linear pass instead of using a work list, and then keep track of the range of stack heights for each instruction, and in the end allow a range instead of one specific stack height for each instruction. And this seems to solve the problem, but makes it more complicated. And another solution which is not really a solution to the problem, but rather approach that compilers would take instead of using functions or code sections for these helpers, but just use rather. No, this first approach was actually when they don't use code sections for helpers, but use just regular relative jumps. And if they put the helpers into the functions, then jumpify might be the solution that leads to a better code size in the end and makes it acceptable to put these helpers into separate functions. So to me it seems that on the one side this more complicated algorithm is more complexity.
00:08:23.500 - 00:08:55.370, Speaker D: But then the question that we have also if it's possible then to drop non returning functions from the spec and jump f, maybe they are not needed, and then everything will be using just these jumps and helpers will be just like blocks inside the same code section. Then maybe in the end the entire spec will be simpler even. That would be nice, I think. Yeah.
00:08:57.180 - 00:09:31.590, Speaker A: One difference is that jumpify has some other good properties which it allows better code sharing. So you can always factor out some shared code into a subroutine. Whereas if two subroutines have the same code, merely relaxing the stack validation rules won't help you outline that code.
00:09:35.080 - 00:10:22.900, Speaker C: I mean, it's two separate things really. It's code detailation within functions and across functions. The stack validation relaxation allows code deduplication within one function, and the jump fi approach makes it easier to do a cross function code deduplication. The disadvantage is that you need to have header sections for the functions that you outline. That's the main thing where the stack validation has the advantage. If you don't want to deduplicate across functions, you save the overhead of the function headers. Yeah, ideally we'd have both, right?
00:10:23.050 - 00:10:49.180, Speaker A: Ideally we'd have both. But I'm saying that the deduplication across functions is like more general, because you can always deduplicate across functions, even at the cost of a function header and call. But if you only have the stack validation relaxation, then you might be in some situations where you can deduplicate code within a subroutine, but not across subroutines.
00:10:54.710 - 00:11:04.630, Speaker C: The actual code side trade off would be easier to evaluate if we had complete implementations of all the options, of course, but it's significant effort.
00:11:10.670 - 00:11:50.660, Speaker A: The code size issue has also helped a little bit. It's helped a little bit if there's r jump and I. So a lot of instruction sets have jump if zero and jump if not zero. So if EVM had both of those instructions, then we could save a bunch of is zeros, which doesn't entirely fix the problem, but it makes it more palatable and it's also really easy.
00:12:01.210 - 00:12:04.200, Speaker C: That's getting a lot of jump up codes then.
00:12:13.590 - 00:12:18.726, Speaker A: Um, right, I mean, in general, in our.
00:12:18.828 - 00:12:19.926, Speaker C: Sorry, go ahead.
00:12:20.028 - 00:12:32.140, Speaker A: Yeah, so we could choose between them or. Yeah, like you said, we would need a complete implementation to really compare all of them.
00:12:34.510 - 00:13:17.240, Speaker C: The last implementation that we had of the previous version, I mean, all of this applies to the previous version as well, basically indicated that actually the dropping of jump tests already decreased code size costs a lot. The code size increase I was initially more worried about than after I had this initial implementation. Although that implementation was written against an incomplete EVM, one implementation that allowed jumping to blocks from different stack sizes and had code deep publication in it, even though only within functions. So that had actually as a code size improvement in the end.
00:13:17.550 - 00:13:23.530, Speaker A: But yeah, for viper actually, even with the jump test removal, we got a code size regression.
00:13:25.390 - 00:14:07.340, Speaker C: Interesting. The main question is, and I mean from the compiler side, I think it's rather clear that we ideally want to have all of this. The question is, what's the maximal option out of this that we get or can realistically expect? So what kind of choices can we have between combinations of what we just discussed? That's basically what we need to do. I would say.
00:14:25.170 - 00:15:34.080, Speaker D: Yeah, we're trying to minimize the overall complexity, and I would say we should have either one of them. And I should note also that both of them are forwards compatible, so we could theoretically introduce another one as a later upgrade. And I don't know, given that if jump f and non returning functions are useful by themselves, and we can't get rid of them with relaxed stack validation, then maybe jumpify is a simple one to add. And this is just so the options I would say to having in mind the goal of minimizing the complexity is less restricted stack validation but non returning functions and no jump f or restricted stack validation and jump f and non return functions and jump fi. That's, I think, how I see it.
00:15:36.610 - 00:16:01.640, Speaker A: I think I'm okay with jumpify. I think it's like a general solution, and it seems to me that the implementation is simplest for the clients. I have a concern that it's a little bit more expensive than our jumpy, and that might add up.
00:16:03.630 - 00:16:24.330, Speaker C: The problem is it makes it more complicated for us to determine when sharing the code is actually cheaper, because you always have to account for the additional overhead. The code to be quotient using r jumpy within a single function is always beneficial. So that's a no brainer, but it's not like that's a block.
00:16:24.410 - 00:16:30.340, Speaker A: Yeah, it produces a code size versus gas kind of trade off which you have to take.
00:16:34.950 - 00:17:47.200, Speaker C: I would say. I'd really like to have the ability to deduplicate within functions, but I would tentatively define with jump fi as well, even though it's from our side. I think the more complicated solution, I mean, one more thing to consider maybe is one can simulate jump fi with r jump I and jump f. Of course that makes it a bit more expensive then, and is reliant on having the stack height relaxation, because these r jumps would jump from different stack heights, but that not sure whether there's a good measurement of how much an improvement jump fi would actually be on top of that. If one has code application within function without having code replication within functions, that sucks. But I mean with that, not entirely sure.
00:17:50.210 - 00:18:23.340, Speaker A: Yeah. So one use case I realized for jump fi is that you can actually perform stack unwinding because you can have like a help, you can kind of emulate ret fi, which you can also do with r jumps r jump I. But it's kind of interesting because you can have like a shared stack unwinding routine. So it gives you different ways of kind of terminating execution of the subroutine.
00:18:25.680 - 00:18:28.140, Speaker C: Sharing that between multiple subroutines.
00:18:29.040 - 00:18:29.790, Speaker A: Yeah.
00:18:34.160 - 00:19:05.530, Speaker C: But why does it need to be conditional? I mean, you can still do that with an archump I and a jump f. I mean, it's just minor optimization there of that becoming a bit cheaper. That only works if we have the stack height relaxation, because all these cases where you share stuff like that, I mean, stuck on wedding, maybe not, but a lot of cases are from different stack heights. I'm a bit worried that the strict stack validation will bite us in the back in the head because.
00:19:09.760 - 00:19:28.960, Speaker A: Yeah, it's restrictive and I see the want from the bytecode analysis side, but it does make the compiler's job a lot harder.
00:19:29.460 - 00:19:33.330, Speaker F: Maybe there's like one reason for.
00:19:35.080 - 00:19:35.830, Speaker C: It.
00:19:36.280 - 00:19:52.840, Speaker F: Is that if we are unsure what is the good version, if you do start with a much more restricted version, then it can always be relaxed. But going the other way around is going to be way harder, if not impossible.
00:19:56.610 - 00:20:23.590, Speaker A: But I don't think that there's really any reason to go from the relaxed validation rules to more stricter validation rules. I don't think there's any benefit to analysis. The only con is it's a more complicated algorithm, but there's no semantics reason that I see that we would ever want to get more restrictive on the stack validation.
00:20:26.240 - 00:20:39.170, Speaker C: It's true. Could even imagine that it's eventually maybe with a different algorithm even possible to get around the ordering requirement. Haven't planned to think through that, but never got around to.
00:20:42.680 - 00:20:59.210, Speaker A: Do you mean the forwards versus backwards requirement? Yeah, I agree. I suspect that it can be completely relaxed, but I haven't gotten around to dealing through it.
00:20:59.820 - 00:21:08.200, Speaker C: But yeah, I mean, the restriction is something we can deal with in the compiler. That's not too much of a hassle. I think.
00:21:11.050 - 00:21:21.900, Speaker A: You mean the ordering restriction. Yeah, it's fine. The ordering restriction basically says that you can't have these unbounded loops, which from a compiler perspective is totally fine.
00:21:34.850 - 00:21:38.960, Speaker C: But yeah, that kind of didn't bring us further now.
00:21:42.130 - 00:23:10.910, Speaker A: Well, it did, because like you said in this document, where we go through jump fi at the very end of the document, there's no should I share my screen? Hello? Yeah, so this is with current eof, this is with jump fi, but there's no example of what it would be like with relaxed stack validation. And what that would look like is, I don't know if I can leave a comment, or it would basically look like push condition the end of the subroutine at end of code.
00:23:16.860 - 00:23:34.610, Speaker C: And there you have the choice whether it occurs in multiple functions and to jump f in the helper code, or if it only occurs in one function to just inline it there. I think that's actually nice, and I'm not sure whether the jump fi really improves that much across that.
00:23:35.300 - 00:23:36.050, Speaker A: Right.
00:23:54.000 - 00:24:08.816, Speaker C: Go ahead. Okay. I mean, the additional code size involved in that pattern is probably, maybe even alleviated by being able to inline some of the helpers. They're not used across functions.
00:24:09.008 - 00:24:24.010, Speaker A: Yes, you can take the choice whether to inline the helper or jump f out, but it's obvious here that the gas performance is better is best.
00:24:27.500 - 00:24:30.460, Speaker C: What do you mean that jump fi is cheaper?
00:24:31.600 - 00:24:35.180, Speaker A: Sorry, it's best in the happy path.
00:24:39.080 - 00:24:39.830, Speaker C: Yeah.
00:24:50.480 - 00:24:57.090, Speaker A: And usually these kinds of things are used with something that's very likely or unlikely, so you can kind of optimize around that.
00:24:59.790 - 00:25:29.980, Speaker C: Yeah, I mean, this discussion now so far leaves me to conclude that actually the stackhyde validation relaxation helps equally or even more than jump fi. I'm completely fine with what you have in the comment there as a pattern. Yeah, I think only works with the stack validation relaxation. Otherwise it can only happen in very few cases in comparison.
00:25:37.240 - 00:26:00.780, Speaker A: My initial take was to do the relaxation on the validation, and then at some point jump. If I was suggested, I think, like we've been saying, it kind of solves the problem, but the stack validation is kind of, I think it's more elegant. From the compiler perspective.
00:26:05.920 - 00:26:11.570, Speaker C: That'S a workable conclusion. For the rest.
00:26:24.270 - 00:26:51.090, Speaker D: Yeah, I would agree. From my perspective, it's also more elegant solution jumpify feels like a hack, like it's something that solves one specific use case for compilers. But I understand that you ideally want both stack height relaxation and to keep jump f, because it's also useful for deduplicating it across functions.
00:26:51.910 - 00:26:54.680, Speaker C: Yeah, jump f is very important in that sense.
00:27:01.930 - 00:27:06.854, Speaker A: Hang on, if we have the stack validation relaxation, do we still need jump f?
00:27:07.052 - 00:27:10.586, Speaker D: Yeah, that's one of the questions I.
00:27:10.608 - 00:27:17.626, Speaker C: Mean, you just had it in the pattern replacing jump fi. So I would say yes, but you.
00:27:17.648 - 00:27:19.180, Speaker A: Could use call f there.
00:27:22.610 - 00:27:37.620, Speaker C: What is the latest restrictions on stack heights and stuff for callf and all that? Does that work in all cases? And then you need call f redf potentially. I mean, jump f is nice in any case.
00:27:39.270 - 00:27:39.586, Speaker E: Yeah.
00:27:39.608 - 00:27:44.850, Speaker D: You would need call f and redf to make it not fall off the code section.
00:27:48.460 - 00:27:51.320, Speaker A: Without jump f. Oh, because you can't terminate.
00:27:51.900 - 00:27:52.650, Speaker D: Yeah.
00:27:59.720 - 00:28:19.630, Speaker C: I was under the impression that jump f and stack height validation relaxation together is an option, and I would choose that option. You're saying that if we go for stack height validation relaxation, jump f would be too much on top of that?
00:28:21.680 - 00:28:43.884, Speaker D: Yeah. It's just my view that I don't like jump f complications by itself. It makes stack validation tricky around it with the non returning functions, it's a bit messy. The spec of jump f, I would say, and ideally if we could get rid of it, that would be really nice. But I guess if it's really useful.
00:28:43.932 - 00:28:44.530, Speaker C: Then.
00:28:46.920 - 00:29:02.090, Speaker D: I don't know, we can keep it. I guess it's hard to evaluate the entirety of complexity of the spec at this point. It's just enormous. In any case, to me it seems.
00:29:06.490 - 00:29:06.902, Speaker A: Yeah.
00:29:06.956 - 00:30:04.560, Speaker C: I'm also not entirely sure out of my head now whether the stack height validation without jump f would still be fine. I remember that I had a lot of good reasons for asking for jump f in the last version, but yeah, I need to revisit that. But in any case, it seems like the relaxed stack height validation is generally agreed upon as a nice solution. So that's something to target anyways. And then jump f and jump fi potentially. Wait, they can still be introduced, I guess.
00:30:15.870 - 00:30:40.460, Speaker A: Yeah. I'm not sure if jump f is still needed. Actually. I'm looking at the stack validation rules and it's like, I think it takes some time to think through because there's the no returning case versus the non returning case. The returning versus non returning cases.
00:30:42.660 - 00:31:01.620, Speaker C: I'd also need to think it through. But we can say that with the relaxed stackhead validation, jump f I is unnecessary. Definitely. And jump f may be unnecessary, which in general seems to point towards going for the relaxed stack height validation.
00:31:02.760 - 00:31:03.990, Speaker A: Yeah, I agree.
00:31:11.350 - 00:31:14.760, Speaker C: I think that's as far as we'd get in this call about that.
00:31:24.690 - 00:31:35.490, Speaker D: Yeah, I think I'm fine with this conclusion. So we include both relaxed stack validation and jump f, but not jump fy. That's the tentative direction.
00:31:36.390 - 00:31:40.740, Speaker C: Yeah. And jump f. We can still evaluate whether we can actually drop or not.
00:31:47.310 - 00:31:48.730, Speaker A: Yeah, agreed.
00:32:03.770 - 00:32:07.720, Speaker C: Maybe let's try to move to the swap end stuff.
00:32:11.580 - 00:32:13.210, Speaker A: Yeah, that's fine by me.
00:32:16.140 - 00:32:26.636, Speaker C: Yeah, we're not sure how to do that. Do you see my point that it's very. Two orthogonal things we are addressing there? And I mean, I see that for you it's more important than one thing.
00:32:26.658 - 00:33:00.730, Speaker A: And for us I was saying. Oh yeah, I think I said on the last call that it's probably easiest just to have both instructions. There's actually obviously quite a lot of variants of stack manipulation instructions which are useful, but I think for our use cases, I think that just having both swap in and exchange is the easiest thing to do.
00:33:02.300 - 00:33:18.296, Speaker C: If that's fine in sense of opcode space, that's definitely the best solution. I mean, we really need swap n. If we get swap n, I mean, exchange is nice to have, we don't need that, but I would also be happy to have it. I guess for you it's rather the opposite.
00:33:18.488 - 00:33:53.290, Speaker A: Yeah, it's the opposite. I mean, kind of, kind of. For us it's easier to spill to memory and then stack scheduling is a little bit like intractable, unless you have this exchange opcode, because in the general case, because you can never guarantee that things are going to line up, are going to end up on the stack in the order that you want.
00:33:55.420 - 00:34:19.088, Speaker C: Intractable is the question. You can still emulate exchange with free upcodes. I mean that's annoying, but it works. Whereas swap n you can't emulate. But yeah, exchange is a nice thing. And I mean, any mathematical treatment of permutations assumes an exchange operation. It's much easier to build algorithms around it.
00:34:19.088 - 00:34:38.680, Speaker C: It's not as efficient since you always. For the case that you produce something on stack top, which is a common case because you always produce everything on stack top. But yeah, I would say for that the best conclusion would be just to keep eip six six three as is and create a new one for exchange.
00:34:45.380 - 00:34:51.590, Speaker A: I think the best outcome is to add it to eip six six three, otherwise we have too many eips floating around.
00:34:54.920 - 00:35:22.830, Speaker C: Fine, either way, as I said, I really need swap n around. That simplifies our compiler complexity by extreme amounts. Spilling from memory for us is a nightmare if you have dynamic types in memory. But yeah. Question is whether there is any objections from clients and whatnot to introduce three opcodes in that EIP instead of two.
00:35:26.000 - 00:35:40.720, Speaker A: Yeah, the feedback I got and the reason I replaced the opcode in the pull request is because the feedback I got was like opcode space, extremely, extremely limited. And so we need to choose exactly one opcode.
00:35:42.100 - 00:35:46.790, Speaker C: Maybe just save jump fi. So that's one good maybe.
00:35:49.560 - 00:35:51.430, Speaker A: Right. And maybe jump f.
00:36:11.100 - 00:36:19.290, Speaker C: Yeah, I'm not sure what more to discuss. I mean, for us, any solution that preserves swap n is fine. That's about it.
00:36:21.420 - 00:36:25.020, Speaker A: So I think the question is, is it okay to add an opcode slot.
00:36:29.600 - 00:36:37.570, Speaker E: For one of the exchange or two of the exchanges? The single byte or the two byte immediate, or both?
00:36:38.580 - 00:36:40.720, Speaker A: At least the single byte immediate.
00:36:43.540 - 00:36:47.570, Speaker E: That should be fine. I mean, we're going to have to probably spill out of the e block anyway.
00:36:54.010 - 00:37:13.874, Speaker A: I mean, if we have, like a budget for swap opcodes. I can prioritize which one can start adding. Until we fill up the budgeted space. Because, like I said, there's always more stack manipulation instructions, which are useful. Yeah.
00:37:13.912 - 00:37:24.310, Speaker C: I mean, a single byte exchange version sounds reasonable. I mean, the two byte one could be emulated by a lot of swap n instructions then. Which is not nice, but workable.
00:37:25.290 - 00:37:31.430, Speaker A: Yeah. Usually you don't end up with that kind of stuff too deep in the stack.
00:37:32.810 - 00:37:37.674, Speaker C: Yeah, well, sure. I agree to that, but it's fine.
00:37:37.792 - 00:37:46.358, Speaker A: Yeah, and like you said, you can do a three instruction thing. Things are more deep in the stack.
00:37:46.534 - 00:37:57.230, Speaker E: What if we did the two byte instruction first? Because we can always add a one byte instruction later. And adding the two byte instruction gives us the coverage. And the one byte would just be size reduction. That we could quantify.
00:38:06.400 - 00:38:10.204, Speaker A: Let me think about that. Yeah.
00:38:10.242 - 00:38:26.720, Speaker C: I mean, I'd leave that up to Charles. But I mean, the disadvantages, of course, that code size increases if you want to use exchange a lot. That's a significant effect, I would say, if a lot of that is small stack depth. And you still need two bytes.
00:38:28.820 - 00:38:41.210, Speaker E: So it would save one byte. Do a swap five and a swap three. That gets you back. To get an effective five and three swap. You would do a swap, five, three. That would save one byte from the two. If you're doing the swaps a lot.
00:38:41.900 - 00:38:52.250, Speaker A: Yeah. And this is an extremely common operation. So it's like two versus three bytes. It's a big difference.
00:38:54.720 - 00:39:04.750, Speaker C: Yeah. Okay. True that. Actually, in that sense, even already saves. I mean, for us, it's not that common, actually. But, yeah, either way, fine with me.
00:39:11.610 - 00:39:29.440, Speaker E: Actually be a three byte swap. Because you'd have to swap five to one, one to three. And then do a one to five swap back to put one back where it was. So the exchange one or two byte version saves no matter how we would consider it.
00:39:30.450 - 00:39:44.210, Speaker A: Yeah, it saves two instructions, but not necessarily on code size. So there's instructions, instructions.
00:39:46.890 - 00:39:48.434, Speaker E: It would save three or four bytes.
00:39:48.482 - 00:40:12.090, Speaker A: Per, um, not necessarily, because like, swap five, swap three, swap five is three bytes and nine gas, whereas exchange with one byte immediate is three gas and two bytes. But exchange with two byte immediate is three bytes and three gas.
00:40:12.670 - 00:40:17.070, Speaker E: But you would need the third swap to restore the number one position on the stack.
00:40:19.810 - 00:40:21.470, Speaker C: Yeah, that's still three bytes.
00:40:23.970 - 00:40:26.930, Speaker E: It's three swaps versus one exchange.
00:40:28.230 - 00:40:35.010, Speaker C: The exchange is two or three bytes, depending on how many immediates.
00:40:35.750 - 00:40:47.650, Speaker A: Yeah, it's one instruction and two or three bytes depending on the number of immediates. So we're talking both about bytes and number of instructions.
00:40:53.750 - 00:41:16.746, Speaker C: In any case, I'd myself find an exchange with two immediates that then covers the entire space. Nicer for us because we're more likely into running into deep stack. But I'm also fine. If Viper says they don't run into deep stack and want to save even more, even another byte in it, that would be also fine.
00:41:16.848 - 00:42:03.560, Speaker A: The issue is, usually you want to do exchange at the worst case, you are kind of trying to do exchange at the last step before executing an instruction. And by definition those things aren't going to be, I mean, the largest instruction is probably like call, which I think has seven arguments on the stack. So you can always kind of preemptively schedule something really deep in the stack. But I think you kind of get. Yeah, maybe I can think about this more, but I think that mostly you're trying to reorder things in the stack that are kind of close to the top.
00:42:04.810 - 00:42:26.460, Speaker C: For us, things work differently and we schedule within blocks two optimal stack locations. I said before the actual instruction, we don't need to shuffle anything. Though for us, shuffling only, it takes place in inter block scheduling, across jumps, and there it's the entire space. But yeah, that's different implementations in the end.
00:42:28.450 - 00:42:55.110, Speaker E: So even if it's a wash with three swap instructions, one byte and a two byte exchange, that's one opcode, and it saves six gas. And in the EVM loop it's two less trips through the loop, which I think the trip through the loop is probably a bit higher cost than parsing the immediates. So implementation wise it is faster, even if it is a wash on byte size, and it's a gas savings to end users.
00:42:55.530 - 00:42:56.280, Speaker C: Yes.
00:43:05.650 - 00:43:11.310, Speaker A: I think I'm leaning towards the one byte immediate.
00:43:15.840 - 00:43:20.024, Speaker E: So one byte exchange and a one byte swap that goes deep.
00:43:20.152 - 00:43:23.150, Speaker C: Yes, exactly, and live with that.
00:43:52.470 - 00:44:36.210, Speaker E: So I wonder if we should make exchange a separate Eip. Then the argument is there's too many eips, but when you look at the unified spec, I think we can demonstrate that's as much of a red herring as we're doing these separations to be able to package it up in different finely grained specifications of the very specific thing. So I don't think too many eips is necessarily. I think it's something that can be defended against and argued against. People try and bring there's too many eips, but we can point them to a unified mega spec that is actually shorter than the merge spec and shorter than the Berkele tree spec. So I think I'm not as concerned about that. That is a legitimate concern, but I think that's one that can be mitigated.
00:44:37.510 - 00:44:51.160, Speaker A: One concern I have about multiple eips is that they won't go in together, so it is more likely that one will be slashed from the final spec.
00:44:52.890 - 00:45:02.060, Speaker E: And unfortunately, I think that's the horse trading we're going to be in in a couple of weeks. If we can't trade horses, it just might not happen.
00:45:08.890 - 00:45:23.002, Speaker C: I mean, I would say conceptually, it's really two quite different things that solve quite different problems. So conceptually, I would say it would make sense to make it separate eips, but it doesn't matter to me at all. Fine.
00:45:23.056 - 00:45:51.570, Speaker A: Either way, I think they actually make sense to go together because they're both like new stack, they're all just stack manipulation instructions with immediates. Because the new swap and dupe instructions aren't even unlimited. They only address 256 slots. So they're really just new ways of manipulating the stack.
00:45:56.220 - 00:45:59.370, Speaker C: In that sense, that's common ground. Yeah.
00:46:01.760 - 00:46:23.430, Speaker F: Initially they were unlimited, but we reduced them to a single byte immediate. Yeah, I'm fine having it the semi IP or multiple eips. But I guess the agreement is that we do want to keep the swap n as it was, and we introduce a new exchange. Is that correct?
00:46:24.440 - 00:46:26.790, Speaker A: Yeah, I think the agreement is that we want to have.
00:46:31.930 - 00:46:34.870, Speaker C: Wiper wants to have exchange and we want to have swap.
00:46:41.870 - 00:47:06.340, Speaker F: Mean. Since you do have the PR open to change the existing mean, if you want to change that, to add the new instruction, I guess it would be simple to merge. That being said, what Dano mentioned that we maybe need to have some cards to trade. I mean, even if it's the semi IP, we can make that statement, right? That exchange could be moved out.
00:47:08.950 - 00:48:09.290, Speaker E: So on the subject of cards to trade, one of the breaking lines I'm thinking of is we put all of the restrictions in the first release and we add the crazier replacement stuff like getting rid of ext code copy and replacing it with code visibility and replacing the call stuff. Do some higher level stuff in the second one so the first one doesn't change too much other than the packaging and the things that are needed to make it useful for. I think I mentioned some of this in the ETh magicians thread and to put the features that the compilers need in the first revision and then do the higher meta changes in the second revision, which would be the code visibility and the gas visibility changes. So as far as horse trading, that's what my initial offer would be if they say no to Mega. And of course my first position is if we can get Mega, let's do mega. I'm all in for the big change at once, but it's not my decision.
00:48:11.870 - 00:48:24.494, Speaker C: Isn't the problem with that approach though, that that's basically what we were at last year that didn't work. I mean, that had already most of the compiler relevant changes and all that.
00:48:24.692 - 00:49:08.950, Speaker E: Yeah, I totally agree. But now, I mean, the problem is we're getting contradictory requests from opposite sides of ACD and we just need to get them in the room together and say look, this coordination failure is going to kill this and it's going to drain the moat around the EVM chain if we can't update the evms. So I think we just need to get them together and have them say one of you has to relent on your position that it either needs to go in only once or that needs to be in smaller is EVM is ossified and we're losing a keynote. Assuming people show up to Istanbul to get in the same room.
00:49:12.420 - 00:49:55.660, Speaker F: Yeah, don't jinx that. If we only have like 5 minutes left. And there was one last topic which would be nice to discuss, the call two changes and the just open it up. Yeah, I think with call two that the return data is removed. Charles, you brought up some concerns that this may cause some regressions in terms of gas usage or code size in wiper. Do you want to maybe very quickly expand on that? And I would be curious to hear Daniel's view on call too.
00:49:57.650 - 00:50:42.710, Speaker A: Yeah, basically when you already have a buffer that is bounded that you want to have the call insert the return data into right now you just add the output buffer as the call arguments, but with call two as it currently stands, you would have to do some calculation to calculate which is smaller, the allocated size of your return buffer and return data size. So you need to add extra instructions around every call too, and return data load solves this, because then you can just abid decode directly out of the return data buffer.
00:50:51.680 - 00:51:21.390, Speaker C: I honestly didn't have the time to review this stuff, but return data load is nice in general, if that's the question, the return buffer is fine on my side. Not to have and to have. Sorry, I'm not up to speed on the latest versions of that stuff.
00:51:22.740 - 00:51:59.160, Speaker F: Yeah, so maybe there are like two simple questions to answer here. So what call two is doing, it removes the gas argument, and it also removes the output data area argument so it won't copy anything to the output. And it also returns three different states, success, failure, or revert. And question number one is, are we okay with removing the output data arguments, or would that on its own cause some issues for solidity?
00:52:04.270 - 00:52:40.070, Speaker C: No unsolvable issues. The trade off and cost would need as difficult to anticipate, but I wouldn't expect it to be bad either way. Fine in that regard. I mean, we also do use the arguments for static return sizes, which is probably the more common case in viper even I would imagine. But I think the cost of always using return data copy is not bad for how we implemented things currently. Not too bad?
00:52:47.820 - 00:53:19.920, Speaker F: Okay, yeah, I think for the output area the base argument is, unless there's going to be a strong argument for keeping it, then we probably want to just remove it in order to simplify dot codes. So this is something to probably follow for the next few weeks. And the second question is the return data load instruction, which I guess answer is yes, that both compilers would love to have this instruction.
00:53:22.180 - 00:53:32.550, Speaker C: The thing to think about there is that of course compilers want to have all sorts of instructions all the time, because it makes everything easier, but return data load is very nice. That's definitely the case.
00:53:34.040 - 00:53:55.320, Speaker A: Yeah, I think the issue here kind of is like if called two is going to force us to use the return data buffer, then the feature set should be on par with memory, because previously we could use memory and now we can't. So we need return data load.
00:53:55.820 - 00:54:19.920, Speaker F: Here comes like a difficult question regarding return data load on the semantics. So return data copy currently aborts if you go out of bounds, but m load and call data load, just return zeros. So what would be the semantics of return data load? Would it be padding with zeros, or would it abort?
00:54:20.820 - 00:54:33.860, Speaker C: Can we please change the semantics of return data copy to not revert in the first place? That would be ideal, and then also do that for return data load, just make it exactly behave like call data copy and coded a load.
00:54:38.200 - 00:55:20.740, Speaker A: Yeah, I'm okay with either. The only issue with return data load reverting if it gets too many bytes is that if somebody ever wants to return packed data, like one byte of data, then you have no way of accessing that with return data load, which is less of a problem actually, than call data load. So like call data load, there's more incentive to pack it. But return data load is priced so that there's no incentive really to not pack with zeros, I mean, from the returner side. So there could just be a convention that return data is always at least 32 bytes.
00:55:30.320 - 00:55:50.820, Speaker C: General, the divergence between behavior in call data copy and return data copy is quite horrible. In general, I would still argue. I would love to see that unified. And if I had a choice, unified and not reverting and zero padding.
00:55:52.280 - 00:56:09.080, Speaker A: One nice thing about return data reverting is that it simplifies ABI decoding checks. So when you're doing ABI decoding, you can just rely on the return data reverting on out of bounds, and you don't have to do them manually.
00:56:13.700 - 00:56:29.700, Speaker E: So if we got rid of the revert on return data copy, would solidity and Viper need to write two separate subroutines to handle the new revert free logic? Or would just be, if we're in the reverting logic, here are some extra steps. Otherwise, skip these steps.
00:56:34.260 - 00:56:46.720, Speaker C: I'm not quite sure I understood, but I mean, we never rely on return data copy reverting. We just have to avoid running into it. For us, it's a cure simplification to not hope we have it revert.
00:56:47.140 - 00:56:59.850, Speaker E: Right? So the code that you would write, that would make sure it doesn't revert and do the verification, you would just turn that code off when you're generating EOF, you wouldn't have to write different code to replace it, you would just not generate it.
00:57:01.260 - 00:57:03.592, Speaker C: Basically, yeah.
00:57:03.646 - 00:57:20.076, Speaker A: For Viper that's the case. So, like, whenever you do API decoding, there's some checks that it's a certain size and so on, and you would just skip those checks if the return data is reverting. If it's not reverting, then you just add those checks the same as you do for call data.
00:57:20.258 - 00:57:36.100, Speaker E: Okay, that makes me more comfortable, because if we were going to have to write two separate sections of code, whether it was EOF or a legacy, then it brings a security complexity risk. But if we're literally just turning a code section off when we're in Eof that's the same surface mostly.
00:57:43.360 - 00:57:52.656, Speaker A: We might maintain two different code paths depending on the output target, and that's.
00:57:52.688 - 00:57:57.240, Speaker E: The concern as we be increasing the surface. But if the returns are big enough that it might be worth the risk.
00:58:00.060 - 00:58:24.490, Speaker A: Yeah, let me think about that. I actually haven't considered what would happen if return data is not reverting, so it might be we might be able to get it to be like basically the same code. In either case, I'm not sure.
00:58:32.330 - 00:58:43.640, Speaker E: I think the ideal would be that the difference between legacy and UF code is here's some code that checks for revert and it's just absent, but I'll let your compiler guys make the best decision there.
00:58:47.690 - 00:59:00.700, Speaker C: For us, it's not a problem. We could live with the change either way and have unified misremembering that wrap extremely at the moment, I think it's basically avoid the situation from happening.
00:59:03.950 - 00:59:10.480, Speaker A: Daniel, if you're asking do we depend on return data reverting right now, the answer is no, right?
00:59:10.930 - 00:59:11.806, Speaker E: So that's fine.
00:59:11.908 - 00:59:21.070, Speaker A: We don't depend on it now and we might be able to depend on it in the future with call two, but otherwise we would maintain the same code path.
00:59:21.650 - 00:59:30.740, Speaker E: Yeah, the big upside is whenever there's a solidity tricks and troubles API conference talk, they always mention this. So this would be nice to get this off their list.
00:59:39.680 - 01:00:30.840, Speaker F: Yeah, I wanted to mention one last thing. We are like 6 minutes over the lot of timeframe. I'm not sure how much time people have, but in two weeks when we would have the next UF call, we have devconnect. And I think most of us are going to be in Istanbul. And I think in fact two weeks exactly from now is going to be the EVM summit day. So I wonder if you could maybe allocate some time to meet up there and maybe discuss some of these points more in depth? Or should we consider getting some in person meeting if most of us are going to be there or otherwise? Just probably set up the next UF call to be the week after Devconnect.
01:00:37.050 - 01:00:45.100, Speaker E: I'll be there Sunday through Thursday, Monday through Thursday at least, debating whether to trim my travels or keep it. But those core days I'll be there.
01:00:54.190 - 01:01:00.700, Speaker A: I'm still deciding whether to go, but it would be Friday through Friday and I could meet for the UF stuff.
01:01:04.470 - 01:01:08.870, Speaker C: I'll be there. Let's be at the EVM summit.
01:01:17.510 - 01:01:23.394, Speaker B: Okay, so probably skip the next one and pick back up one month from.
01:01:23.432 - 01:01:24.020, Speaker F: Now.
01:01:28.230 - 01:01:31.890, Speaker B: Skip the next call, but meet in Devconnect sometime.
01:01:32.230 - 01:01:42.520, Speaker A: If there's a meeting being planned at DevConnect, it might be good to have it be like dual attendance so people can attend remotely if they want.
01:01:44.090 - 01:01:52.780, Speaker B: Yeah, sounds good. Let us know if you're not going to make it, then we can accommodate you. Or if there was someone else that said so.
01:01:57.500 - 01:02:47.050, Speaker F: Right? Yeah, I think we made a really good progress today in closing out these last questions. Hopefully we can close them out during that meeting. But Daniel, you mentioned that we may want to think about how to actually deliver this and how to split it up if there's more pushback on some aspects. So what do you think regarding? I mean, we propose this for prag. So what kind of timelines are we looking forward to in terms of ACD discussions, do you think? Assuming we make most of the decisions during Devconnect, then probably we can implement those in the specs like in a week or two after. When are we going to bring this up on all core devs again?
01:02:47.900 - 01:03:38.708, Speaker E: So I was hoping that we would be able to have more all core devs meeting together in Istanbul, but looks like some people may not be there. I think we should try and have these talks in parallel in assemble, if we can pull them off, if we can get the right people in the same room otherwise, because I think we need to get a clear signal from the people who are raising the objections as to whether a two step solution would work or if they just object to EOF in principle. And I'm secretly concerned that there are people who just object to EOF in principle and want to see the current EVM frozen. And that's the biggest concern. But if they're just concerned about the size. And another concern that was brought up in Bogota was we don't want multiple versions of EOF live. But I'm thinking this goes into my thread that I was posting on Ethereum magicians.
01:03:38.708 - 01:04:41.390, Speaker E: If there are still two versions, but they only differ in opcode set available, if all of the validation is exactly the same, it's just a function of which opcodes we push into the opcode validation loop, then I think it's easier to say that to have three different instruction sets available, legacy EOF one and EOf two, if that's what's going to take and have uft be the main recommended one. But the EOF one just doesn't have the visibility, the observability stuff removed and we just change which job goes we put in, we put all of the restrictions in EOfB one and v two and we use the same validation core logic to verify them. That's what I think the two most viable approaches are. And of course, my favorite one is just to just do everything at one time and see what the concerns are and see if we can address their complexity concerns. If the issue is just amount of testing, if the issue is about utility, whatever their issues are, make sure that we can get them and make sure that the objections just are. I don't like this and don't want to see it. Know if there's technical issues, if we can address them.
01:04:41.390 - 01:05:22.920, Speaker E: If the issue is it's too. That's sometimes that's intractable, but that's what I would want to see is if we can gather this together while we're in Istanbul. But I don't think we should go back to all core devs before the second call in January with this, unless we have a concrete plan that everyone's on board with. I think we need probably after they split the established the Gorely fork so we could do it in between the gorely and whatever the next fork would be. But I'm open to other alternative ideas. It's not that plan or nothing, but know an initial plan to go.
01:05:34.580 - 01:05:49.190, Speaker F: I mean. I mean, let's see what we end up doing at Devconnect. I think the only question I had left whether we should tentatively schedule the online call the week after DevConnect or as Matt said, two weeks.
01:05:52.440 - 01:05:52.852, Speaker A: Yeah.
01:05:52.906 - 01:06:11.148, Speaker F: And I think I'm tentatively leaning towards the week after dev connect just so we can get. Oh, but that would be out of cadence. Yeah, I guess we can do it, like, one month from now and then. Let's hope that week cadence would be.
01:06:11.154 - 01:06:20.480, Speaker E: The 29th dev connect would be the 15th week, the fifth Wednesday in November. We got five Wednesdays.
01:06:25.530 - 01:06:33.130, Speaker F: I mean, it's two weeks from now, and we have it every two weeks, so, like, four weeks from. Yeah. Would be the ideal.
01:06:34.830 - 01:06:37.770, Speaker E: Let's. I like that idea. Keep it in cadence.
01:06:46.800 - 01:06:59.730, Speaker F: Right? Awesome. We went slightly over, but I think this was super useful. I'm not sure. Matt, you're the moderator. Do you have any closing instructions to say?
01:07:01.140 - 01:07:12.568, Speaker B: No. Great call. Thanks a lot for the discussion. Appreciate you being on here. Also, Daniel, it's pretty helpful sometimes to just sit down synchronously and discuss these things.
01:07:12.614 - 01:07:13.210, Speaker F: Things.
01:07:14.060 - 01:07:21.210, Speaker B: Hopefully, I'll see a lot of you guys at DevConnect. Safe travels, and we'll pick this call back up in the near future.
01:07:24.060 - 01:07:25.450, Speaker A: Thanks, you guys.
01:07:25.900 - 01:07:26.564, Speaker F: Bye.
01:07:26.692 - 01:07:27.428, Speaker E: Thanks. Bye.
