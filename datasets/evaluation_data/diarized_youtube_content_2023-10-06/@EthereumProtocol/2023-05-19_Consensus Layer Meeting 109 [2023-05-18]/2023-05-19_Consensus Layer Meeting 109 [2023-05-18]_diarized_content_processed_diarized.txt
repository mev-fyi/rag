00:03:54.850 - 00:04:27.908, Speaker A: We should be live on YouTube. Let us know if you're there. This is consensus layer, call number one nine. This is issue seven eight three in the pm repo. If you're following along, great. I know that there was a main net incident. Fortunately, chain stayed alive and kept going and resolved itself.
00:04:27.908 - 00:05:06.002, Speaker A: I know that there have been a couple of client releases and there is also a post mortem out by Prism. Is there anything that we want to discuss about that today? I know there's ongoing testing and analysis. Okay, if you're listening in, check out Prism's quest warm. It's good. I think there will be continued work on this for a while. Thanks. All right, moving on, we're going to look at Deneb.
00:05:06.002 - 00:06:48.540, Speaker A: We have a number of issues. I think a lot of these are updates and things that we're trying to make final decisions on and we will begin to go through them. So there's this open pr three three eight, which is to make it so that the KZG commitments, the blob commitments, can be much larger than the actual gas limit. It seems like the consensus now is to have it at a pretty large number on the order of thousands, but to keep there to be a restrictive constant that's in the realm of the gas limit, such that the smaller number can be increased without changing the shape of the Merkel tree, which is a property of SSD. Given the max list size, is there any update on this? It seems like we're on the order, on the verge of consensus and that we just need to do a final cleanup and review. Okay, is anybody against the two value strategy where we have a large value that helps us shape the merkle tree and a small value which is in the order of what we can actually handle? Okay, I believe this PR was by Guchinder. Is there anything that we need to do to get this cleaned up or are we in a good spot to just do a final review and merge?
00:06:49.680 - 00:07:23.464, Speaker B: I think we should be good for the final review because there is one comment and regarding the sitecar validation, so I can remove the extra validation command that I added while validating the sitecar that the index should be less than max blobs per limit. So since the subnets anyway are going to be open than max blobs per limit, so it's sort of a redundant validation that I added, I can remove that.
00:07:23.662 - 00:08:15.370, Speaker A: Okay, great. Other than that, we'll put some finalize on it. And get this in just for context. This is probably one of about four things that we're trying to get finalized for a release for new testnets. So this is on the list and we'll get that done. Okay, the next is there are a couple of prs about this ongoing conversation, I believe decision made to pass the version hashes into the engine API to do the validation against the transactions so that we don't do the transaction peaking on the consensus layer. There is the engine API and there is the consensus spec change.
00:08:15.370 - 00:08:24.140, Speaker A: Mikhail, are there any comments here other than just needing to review and get these merged?
00:08:25.840 - 00:09:23.372, Speaker C: Basically, yeah. I would just like to say that the PR is kind of ready to get merged into the engine API. And one, not a big question, but just would like to attract client developers to this thing. Is that how to pass these version hashes? Currently PR suggests to do this by introducing a second parameter to a new payload call, which obviously breaks some abstractions that we had before on new payload. But the other option would be to introduce an envelope to pass payload and block hashes as one object. But that as well can break some abstractions. So it's kind of like what's done currently is just try to avoid using new data structures to engine API.
00:09:23.372 - 00:10:30.550, Speaker C: But probably it is easier for some clients to implement it as the past inside of an envelope. So that's basically one of the things that I'd like to people to take a look at. If that's fine as is, then we're ready to go with this PR. Yeah, to address as Gar's comment. Yeah, so, and yeah, so that's basically this PR just what it does, it just defers this check to El. It does not change this trust assumption in any way and pass in a list of hashes. Then we need to decide which.
00:10:30.550 - 00:10:37.590, Speaker C: Yeah, we could do a hash probably. Does it bias that much?
00:10:42.300 - 00:11:04.630, Speaker A: You it? Yeah, if the data transfer was a problem, then we would consider that, but by compressing it we just reduce the ability to communicate error messages and other things. So unless there's a strong reason I wouldn't do it.
00:11:07.740 - 00:11:27.948, Speaker C: Right. But yeah, follow up to the chat. So yes, it's not about the availability. So I don't understand what's broken by just deferring this check to El, because.
00:11:28.034 - 00:12:43.140, Speaker A: The check only happens when you're synchronously communicating over the engine API. So in the event that the execution layer is syncing, it can't ever verify these Merkel commitments for the past components because it doesn't have them. Right. And so it only happens when you're doing this synchronous check. There is a precedent for a value that's done in the synchronous check, and that's the block hash, which is also passed in from the, there's a block hash embedded in the execution payload in the consensus layer, and that's passed in, and that's only checked the consistency between what's shown up in the execution layer and whether that hash is correct when you're synchronously communicating. So, Anskar, there is a precedent for this, and the assumption is that this type of synchronous check, even though it's only happening when it's synchronous, you can only really potentially fool a syncing node. And so yes, that exists.
00:12:43.140 - 00:12:57.128, Speaker A: It already exists with the block hash. You can fool syncing nodes about this validation, but it's very difficult to fool the network. Does that make sense on scar, right?
00:12:57.214 - 00:13:39.930, Speaker C: I mean, I'm not super familiar with the block cash situation. I would then consider this an issue as well, but I don't think it's a good idea to, once we recognize there's an issue, to basically say, well, this property is gone forever now, so now we might as well just stop caring about it. My question then would be, if we stop care about this, why do we even still store the El header other than of course, the values that we want to make accessible via the block hashes from inside the EVM? I mean, that's a reason. But for integrity checks, then we might as well just completely never store anything and just treat everything as ephemeral information.
00:13:41.740 - 00:13:56.860, Speaker A: I don't follow that, nor do I necessarily agree. But the block hash or marius or somebody familiar with the engine API, even if you're syncing, is block hash checked?
00:13:59.040 - 00:14:01.784, Speaker D: You mean the execution layer block hash?
00:14:01.912 - 00:14:18.660, Speaker A: No, the block hash. So there's a block hash that's explicitly in the. Okay, block hash is always checked by the engine API. So these could also always be checked by the engine API. But add overhead during sync.
00:14:21.880 - 00:14:33.940, Speaker D: You mean during the consensus layer sync, not the execution layers? Yeah, during the consensus layers. During the execution layer sync, we always check the block history because we have to construct chain.
00:14:34.100 - 00:14:44.980, Speaker A: No, sorry. We're talking about two different things. The engine API has an additional field where the consensus layer says block hash.
00:14:45.140 - 00:14:48.236, Speaker D: Yes, we check that on every, no matter what.
00:14:48.418 - 00:14:53.448, Speaker A: Yeah. Even when you're in like a sync status and doing other stuff, if somebody gives you a new payload, you check.
00:14:53.554 - 00:14:54.160, Speaker C: Yes.
00:14:54.310 - 00:15:17.812, Speaker A: Okay, so to not change the assumption, the sync assumption here, this list of KZD commitments would also have to be checked at the same time. Like block hash is checked. No, matter what, even if execution layer is syncing, is that something that we're willing to do otherwise, we do change the assumption.
00:15:17.996 - 00:15:31.208, Speaker C: It is specified in the way that we are discussing now to do it disregarding the sync status and do it instantly, similar to a block hash check.
00:15:31.374 - 00:15:38.750, Speaker A: Okay, so then as long as clients can and are willing to do that, this does not change the security assumption at all.
00:15:40.080 - 00:15:48.930, Speaker C: Yeah, and I thought that by default it is possible, as we have discussed it. So if it's not, please rise your concern in the PR.
00:15:50.020 - 00:16:40.610, Speaker A: So, Anscar, just to be explicit, in the execution payload, which is a consensus layer data structure, there's an extra field called the block hash, which is the execution layer block hash of the rest of the payload contents. This is passed into the engine API. And before the execution layer does anything, it checks to make sure that that hash is consistent with the rest of the payload fields. And so it does this kind of like synchronous check, and it does that independent of syncing. And so these KZG commitments being correct against the transactions would also be done kind of in that preprocessing before returning anything related to syncing or otherwise. And so there is a precedent for doing this.
00:16:41.220 - 00:16:48.470, Speaker D: How costly is that check? Because the blockage check is very cheap for us.
00:16:53.240 - 00:16:58.250, Speaker A: So you have to deserialize transactions and pull out a field and check the field against it.
00:16:58.860 - 00:16:59.610, Speaker C: Today.
00:17:01.020 - 00:17:01.624, Speaker D: That's fine.
00:17:01.662 - 00:17:05.560, Speaker C: We already have to deserialize them and remerkilize them to get the buff.
00:17:07.820 - 00:17:57.530, Speaker A: Right. So this is cheap. Anskar, I can explain it again, or we can talk about it outside, but it's exactly the same thing. The block hashes are not guaranteed to be self consistent here because this is an extra field in the consensus layer. It's kind of like how the KZB commitments are an extra field in the consensus layer, and we have to ask the execution layer now to make sure that they're equivalent to what's in the transaction. Similarly, there's an extra field called block hash in the consensus layer that we ask to make sure it's consistent with the rest of the fields in the execution layer. But yeah, I'm confident that this is very similar and we can talk about it outside the call.
00:17:57.530 - 00:18:40.320, Speaker A: When do Cl start sending new payloads while the EL is syncing? The Cl does, yes. Is it from the weak subjectivity checkpoint? It's from wherever you're syncing from. And that point that you're syncing from is the weak subjectivity checkpoint that you've chosen. It certainly could be. It also could be since a day ago when you turn your note off. It also could be that you use a weak subjectivity hash and eugenesis sync and check that those hashes are correct when you get to the head. So it just depends on your model.
00:18:40.320 - 00:19:39.124, Speaker A: Okay. So there's a few things here. One, I'm pretty confident this is the same type of thing as block hash. And if we're doing the validity check up front, independent of syncing, then we're in an okay, validity check mode that looks exactly like we've done something in the past. We do need to validate that this is not put induced unprecedented load in the preprocessing check, which we're pretty confident is the case because you have to do the deserialization to do the mercury anyway. And so we should sanity check both of those things. And if anybody has issue with the way this is done, they should jump into the engine API PR very soon.
00:19:39.124 - 00:20:17.712, Speaker A: Like this is something that we've pretty much decided and we need to make sure Sandy check that this is correct and okay. Because we're looking to do a release this coming week to get this onto testnets. Okay. And Anskar, you and I can sync on the validity of this and convince each other that this is either okay or not. Anything else on this one? Yeah.
00:20:17.766 - 00:20:50.670, Speaker C: So one thing to note is I don't have a good answer to this, but I want whoever is working on the builder, the relayer side to be aware of. Is that now before, given that the Cl can verify SSZ block transaction, you can actually verify that before. But now that Cl can no longer verify that, we have to rely the EL to verify that. So I'm not quite sure how the relayer will be able to verify that. This is kind of like an open question. So I encourage people to think about that. I don't have a good solution to this.
00:20:59.290 - 00:21:05.754, Speaker A: I don't know if I follow, because the relayer can also ask their execution client, correct?
00:21:05.952 - 00:21:08.502, Speaker C: Yeah, basically relayer needs to code a new payload.
00:21:08.566 - 00:21:08.794, Speaker A: Right.
00:21:08.832 - 00:21:12.102, Speaker C: But me might not want to do that because it's slow.
00:21:12.166 - 00:21:12.682, Speaker A: Right.
00:21:12.816 - 00:21:32.078, Speaker C: It's basically they want to reduce latency, but they just want to verify the alignment between the blob transaction and russian hash with the KCG commitment. But then instead of verifying the whole thing, they just want to verify that. But then you cannot do that anymore.
00:21:32.254 - 00:22:34.720, Speaker A: Yeah, I suppose if a relay is being lazy or quote, optimistic and they're not running all the transactions, then they're already taking an assumption that their builder is giving them correct values. So I don't know if not doing this consistency check is really changing their model too much. Yeah, fair, that's a fair point. But I mean, agreed, something to think about if that's how your relay is constructed. Okay, great. This is on the list of things that's going into release early this week for next testnets. Please take a look at this if you have concern or if you want to kind of weigh in on this strategy.
00:22:34.720 - 00:23:14.370, Speaker A: Okay, I believe we have a relatively non contentious pr that is adding a variable to kind of make the number of blob sidecar subnets look and how those are specified look like how attestation subnets are specified, which is with its own variable so that it can be tuned up and down independent of the actual number of blobs. Pop, do you want to give us a quick on this one?
00:23:15.620 - 00:23:41.956, Speaker E: Yes. Currently in the current spec, the number of psychosis subtopics is equal to max bob per block. But it's not good because if you want to change the constant max box, the number of subnet is changed.
00:23:41.988 - 00:23:42.570, Speaker D: Right.
00:23:46.140 - 00:24:18.420, Speaker E: So I create a new constant called the Cyclone subnet called, so if we have the max box per box greater than the subnet cult, there will be some subnet that will be used for toolbox. So it's like in this pr we can change the max bopper box independently without changing the number of subnets.
00:24:21.080 - 00:24:47.550, Speaker A: Right. And this parallels how application subnets are done, independent to max committees. I do think it's a no brainer to have these as two configurable values rather than just one. I think we had some build issues due to some CI cache problems, but this is likely to be merged later today unless anybody has an issue.
00:24:48.560 - 00:25:23.624, Speaker B: I have a confusion over this in the sense that in the first pr that we discussed, we introduced a variable max commitments per block, which was set to be 40 96. Now we have max blobs per block, which is currently set to four, and then we have another for subnet. So I think the new variable for subnet and max blobs per block, they are doing the same thing as of now because we rejected max blobs per block to be independent from what CL can handle in the subnet, right?
00:25:23.662 - 00:26:30.160, Speaker A: So they're not doing the same exact thing. So if you had max blobs per block as four and you changed the sub to eight and you changed the subnet count to you kept the subnet count at four, you would then have two blobs being distributed per subnet. And this allows you to tune essentially the gas limit independent to how it's delivered on the PDP mesh essentially allows for reuse of subnets for additional blobs instead of always making new subnets. All right, cool. Okay, next up, 3354. This is update the indianness of Hollywood commitments. This has been an ongoing discussion.
00:26:30.160 - 00:26:56.650, Speaker A: I believe that there's general agreement to swap over to big indian in discussions. It was maybe unclear if that's to do for everything, or rather just one interface. Can anybody give an update on where this stands in relation to what's going to happen in the libraries and everything? And if this is ready to go in.
00:27:06.230 - 00:27:59.940, Speaker B: Maybe can say something on this that on this el side, now that we have RLP transactions, basically all the data will anyway be encoded in big indian. And if the pre compiles are also big indian, so that basically solves the problem of El. But that means that library will have to take inputs also in the big indian form. So either the library fully shifts to big indian or particular pre compile verification function which verifies blob version hashes. Sorry. Which verifies commitment version hashes on proofs. Basically it can just take big indian as input and library can do whatever it wants to do inside.
00:28:02.150 - 00:28:25.880, Speaker D: Yeah, so it's really not an issue for us. Before we call the library, we can just turn the inputs library. The library should do what they want and as long as there's a shim anyway between the pre compiler and the library so we can turn the bytes around.
00:28:27.450 - 00:28:32.506, Speaker A: Zonkrad. I believe there was a desire to change the limited interfaces library, right?
00:28:32.688 - 00:28:53.890, Speaker D: Yeah, we definitely want to change the libraries because it doesn't make any sense for them to be little indian. Now the only reason was because we expected the consumers to be little indian, but now it seems all the consumers will be big indian. So there's no point in keeping the library little indian because all the backend BLS libraries will also be big indian.
00:28:55.430 - 00:29:18.880, Speaker A: Got it. With respect to the pr that's in the consensus layer, is this ready to go for final review or are there additional changes that need to be made? Does anybody know?
00:29:23.140 - 00:29:27.330, Speaker F: The tests are fixed. So to me it's ready.
00:29:30.510 - 00:30:06.890, Speaker A: Okay, so this is on the list of the handful of things that are going into the next release, and this is likely to be merged in the next few days. If you have any additional desire to review or comment, please do so immediately. Okay, great. Alex, I believe you wanted to give us some 4788 updates.
00:30:08.270 - 00:30:47.750, Speaker G: Yeah, I can do that. Let's see. So 4788 is this EIp to send over, let's say the beacon block route to the El so it can be exposed in the EVM in a trust minimized fashion. This has lots of great applications for staking pools, restaking, MEv stuff, all sorts of things. So this might be at this point more of an execution layer conversation to have. I think it's pretty minimal on the Cl, so there might not be too much to discuss here. But that being said, on the last acde we decided.
00:30:47.750 - 00:31:33.300, Speaker G: So the way it was written before was that the El would use timestamps to figure out which slots correspond to which block routes. So essentially the block route be sent over. And the question now is which slots does it pertain to? And if we send over some, well, if the El just has some slot data, like for example, knowing how to map timestamps into slots, then it can just do the computation itself. We decided to not sort of violate the abstraction barrier between the two layers and then send over the slots along with the route. And so that's what the latest changes are. I think there might have been even some pushback though, about this approach. So I don't know if anyone's had a chance to look and if they have any thoughts or feelings about the direction this is going.
00:31:35.290 - 00:32:01.100, Speaker A: Yeah, I was the one that put the comment. I didn't realize that that was the outcome of the last acve. I understand the abstraction layer. It does seem excessive to use the execution payload header as a messaging bus for the slot. But at the end of the day I don't have a very strong opinion here.
00:32:02.910 - 00:32:06.110, Speaker G: Yeah, I mean, I kind of agree. Go ahead, Marius.
00:32:06.690 - 00:33:03.280, Speaker D: Yeah, I also don't like putting the slot in the header. I don't fully get why we need to store it by slot and not store it by timestamp and let the smart contract figure out the slot or the other way around store the beacon. Right now we are storing a mapping of slot to beacon route. If we would turn it around and store a mapping of beacon route to timestamp, the contract would need to provide the beacon route. But that's kind of fine, I'd say, and then get the timestamp for it because I don't know if I'm wrong, but the only really use case I would envision is a slot trying to verify that a certain beacon route was there at some point.
00:33:05.810 - 00:33:09.200, Speaker A: Yeah, I mean it's generally to make proofs against, right?
00:33:11.410 - 00:33:41.726, Speaker D: Yeah, but you can, like if you provide all the data yourself, you can also make proof against it. But you need to make sure that this was actually at one point in the chain. Right. And so if that's the only thing we need this for, then why do we need to return the, why do we need to return the beacon route? If we can just provide the beacon route and say, okay, this was at one point or at this slot, this.
00:33:41.748 - 00:33:42.990, Speaker C: Was the beacon route.
00:33:43.330 - 00:34:12.440, Speaker A: Sure, I see how the lookup can work. Either way, it does complicate the data structure in using a mapping which would probably be more of unbounded in growth, rather than a ring buffer, which could be more simply bounded in growth. I think the UX of the mapping in that direction makes intuitively more sense to me, but I agree that given the use case, both would work.
00:34:17.130 - 00:35:13.990, Speaker D: Yeah. So anyway, I've implemented the current spec without the slot, with only the timestamp, and it's kind of not hard to do. It works. Right now what I don't like is that it's an opcode that reads a certain address and a certain storage slot. I would rather make it a pre compile stateful pre compile stateful pre compile. Yeah, it's kind of a new concept, but any way we implement it, it's going to be a new concept. If we implement it as something that is not in the state, it would be like a new concept because we cannot look it up like we look up the block hashes.
00:35:13.990 - 00:36:24.238, Speaker D: So we would have to have another part of the storage. And also I think Alex doesn't want this because it would mean that the state transition function is even less pure than it already. The putting it in the state makes more sense to me. And then if we put it in the state at a specific address, then it kind of makes sense to make this a pre compile, because then we kind of know what happens. And if we have this opcode that reads it, we have to think about all of these edge cases that we don't have if we know it's a pre compiler. So for example, what if someone calls this address the storage lives? Or what if someone self destructs to it? There's a bunch of edge case that are figured out with pre compiles that we don't have with this yet.
00:36:24.404 - 00:36:44.040, Speaker A: Yeah, I personally think stable compute compiles are actually very useful, especially in stateless, where you're trying to not have as many kind of like assumed hidden inputs to various things. I think that it's a design pattern worth making a thing, even though it's not a thing yet.
00:36:45.210 - 00:36:57.020, Speaker D: Yeah, and that's kind of my only problem with it, it's not yet a thing, and in order to make it a thing, we kind of have to think about it.
00:36:58.190 - 00:37:22.114, Speaker A: Yeah, that's one of the things is engineers seeing if stateful pre compiles break some deep assumptions in their software. Because from a theoretical standpoint they seem fine and clean. I think from an engineering standpoint it's unclear until you all say so. Yeah.
00:37:22.152 - 00:37:55.150, Speaker D: So in general, I think this is not really ready yet and we shouldn't push too hard on it before these things are ironed out. That's kind of my take. It's a good eep and we can definitely do it, but pushing it into what's the next one cancun would be kind of hard in my opinion.
00:37:58.120 - 00:38:06.660, Speaker A: Hard because you think it should be a stateful pre compile, but that there's going to be a lot of uncertainty when we go down that path.
00:38:07.900 - 00:38:16.330, Speaker D: Yeah, we should make sure we have this concept of a state pre compile down if we make this change.
00:38:17.820 - 00:38:31.310, Speaker A: Okay. I think it probably makes sense to do a draft pr of a stateful preconfiled version and to talk about this on the execution there call in one week.
00:38:32.020 - 00:38:44.930, Speaker G: Yeah, I agree. Marius, do you feel really strongly about somehow also trying to look at storing things by timestamp rather than slot and just ignore the slot at the.
00:38:45.480 - 00:38:50.900, Speaker D: Yes, yes. I feel very strongly about not having the slot in the header.
00:38:51.720 - 00:38:54.372, Speaker A: I feel strongly about. Sure.
00:38:54.426 - 00:39:08.090, Speaker G: But we could instead just have the El compute this some other way. Like the El just needs a few constants to be able to do the timestamp to slot conversion. That much of an ask.
00:39:09.120 - 00:39:11.500, Speaker D: That is kind of leaking the abstraction.
00:39:14.400 - 00:39:16.300, Speaker G: That's also the point of the CIP.
00:39:16.960 - 00:39:31.360, Speaker D: Yeah, but it means that for every testnet, for every network that we set up, we have to kind of do this whole dance of creating something that works on both sides.
00:39:32.180 - 00:39:44.630, Speaker G: Right. Okay, I'm going to think about timestamps and see if we can make that look nice. And yeah, I can make this look like a staple. Pre compile definitely in the next couple of days and then we can discuss next week.
00:39:46.220 - 00:39:46.970, Speaker D: Perfect.
00:39:55.220 - 00:41:40.630, Speaker A: Okay, the last thing is for Dankoon is 3360, which is we agreed on a previous call that this is something that we want to see in Dingkoon. This is the expanding the attestation slot range from, instead of slots per epoch to be either from epoch n can be included either in Epoch N or n plus one, where n is the epoch that the attestation was created in. This is to improve or to allow our security proofs to work properly for LMD Ghost, as well as opening up the usage of a confirmation rule which is in progress, which Aditya will talk about soon. This is two lines of change in the state transition spec as well as the modification of PDB gossip conditions. I guess the final to do is to get some review here and I'll probably open up an EIP in relation to it to give it kind of a number to track it. But I would like folks that from various teams to thumbs up it so we can keep it moving because it's important for security has generally been agreed on. Are there any questions on this pr 3360? Okay, we're going to keep that moving.
00:41:40.630 - 00:42:14.410, Speaker A: I'm not certain. I don't think it's going to make it into the release like Tuesday, but it would be in a subsequent release. Okay, great. Anything else on kun? Okay, xiaoi, you're up next. Removing merge conditionals and capella.
00:42:15.550 - 00:43:18.910, Speaker F: Yes. So in the bella tricks, we introduced some conditions checks to detect if the merge has been triggered or not. And I think this tests are so these conditions are still in the capella and dynamic specs, but I think we have again, very few benefits to defend in the spec status quo since they are just meaningless for the main net and for the future updates. So I wonder if we can just remove it from Capella so it will be a consensus change, but it should be compatible with mannet. Any comments?
00:43:21.940 - 00:44:42.812, Speaker A: Yeah, I think I've been the only one to speak out slightly against it in that it's nice in theory to be able to perform merges on other testnets at arbitrary forks post capella. I think the actual use of that is probably minimal to zero. And there's been general agreement from client teams that keeping that complexity fork to fork and managing that, especially in pads, that we're really not testing much at all anymore, means it's much preferable to just remove it from the spec, which would then remove it from various tests. So yeah, I think I was the only one that was slightly against and no one else agrees. So we'll move this forward unless others have. Want to chime in? Is Mario on the call? Mario Vega, do you know if we're doing hive tests on capella and after on merge transition conditions?
00:44:42.956 - 00:44:54.310, Speaker C: Yeah, I don't see how this could break any of this, but I really need to double check. Yeah, I'm not sure, but we also want to remove the premesh tests anyway.
00:44:55.640 - 00:45:27.296, Speaker A: Okay. Yeah. Okay, so this is in two prs 3232 and 3350. If you want to chime in before this moves forward, do so very soon. Great. Aditya, you're up next with the confirmation rule pr everyone.
00:45:27.478 - 00:46:12.392, Speaker H: This PR introduces a fast confirmation rule in the spec. I'm pretty excited for this one because I think it's a milestone addition for Ux. So at least from our current analysis, it looks like we should be able to confirm blocks within three to four slots. So under a minute on Mainnet, the way this rule works is you input some safety thresholds so your assumptions about how many validators are byzantine, how many of them are willing to get slashed, stuff like that. And this rule outputs whether a certain block is confirmed or not. And I guess there's two main places we would use this in spec. One of them is the folk choice update to the execution layer, specifically the safe block hash.
00:46:12.392 - 00:46:38.650, Speaker H: Earlier we used just the justified block root, but we can do much better now. It's going to be much closer to the head of the chain. And the second place is the beacon API. This is stuff we haven't specified yet. Still thinking about the best way to expose this to cl users, so we can have a discussion about this now, but also we can wait until people have read this and continue the discussion next time.
00:46:40.380 - 00:46:53.400, Speaker A: Do we have like on get block APIs? On the beacon APIs, do we have some special keywords like head and finalized and justified?
00:46:53.820 - 00:47:21.110, Speaker H: Yes, that's one place we could insert confirmed. You also probably need to set up your security parameters for. So I think another place we could do this in the beacon API is the folk choice debug endpoint. So it should output the confirmation score for all the unfinalized blocks. So that way if you're running some kind of visualizer, you have the data readily available.
00:47:22.840 - 00:47:28.452, Speaker A: And what's the synchrony assumption here, in addition to the safety threshold values?
00:47:28.596 - 00:47:28.904, Speaker C: Right.
00:47:28.942 - 00:47:41.150, Speaker H: The synchrony assumption is that attestations are delivered in the same slot, which means everything that happened in a particular slot gets delivered to all the honest people by the end of it.
00:47:48.270 - 00:48:05.920, Speaker A: Great. And before I take a deep look at this, what's your kind of estimation of complexity? Is this primarily just a lot of simple math, compute on values? Or is there additional things that need to be stored beyond what we already have?
00:48:06.530 - 00:48:27.670, Speaker H: I think it might take some effort to do this efficiently. For the most part, it's using all the folk choice machinery, so get weight is being used mostly. I can't see what a blocker for this would be, but it's mostly effort in performance engineering.
00:48:34.880 - 00:49:01.670, Speaker A: Okay, yeah, thanks Adicha and others that have been working on this. This is really exciting. Any questions for Adicha? Okay, great. And lion, we have one more issue, 3288 or pr. I haven't opened it up yet. Can you give us quick on this?
00:49:04.390 - 00:49:56.210, Speaker I: Yeah, this is a very simple change which we do for the net because backwards compatible, it will allow participants to submit a valid or exit which is valid indefinitely on the chain. Currently, if you submit an exit, well, if you produce an exit before of time, that exit is only valid for two forks. And that's pretty value x for certain use cases where you may want to present exits. So what this would do is it will lock the epoch that you use for verifying the signature to capella, allowing exits to be valid indefinitely. Again, it's a very simple change that has pretty nice UX benefits. So I think this should be able to be included for the NEP.
00:50:07.550 - 00:50:09.580, Speaker A: Does anyone else want to weigh in here?
00:50:14.120 - 00:50:25.370, Speaker F: I think it's very close to the Bos address change operation, so it makes sense to fix it as well.
00:50:37.440 - 00:50:40.144, Speaker A: Sorry, the BLS change operation should be.
00:50:40.182 - 00:50:49.200, Speaker F: Changed as be because the Bos address operation, the domain was also fixed.
00:50:50.100 - 00:50:51.090, Speaker A: I see.
00:50:51.800 - 00:51:04.390, Speaker F: So I think these two are. I mean the exist is also a one time operation, so it's fine to just. I mean, I support SPI change.
00:51:07.750 - 00:51:08.114, Speaker C: Is.
00:51:08.152 - 00:51:35.690, Speaker A: The primary counterargument that this was put in place to ensure that if there was some sort of sustained forking, your messages across forks would not be replayable, but that this is kind of a one time operation and doesn't really affect your security too much. It's just an exit that it doesn't really matter if there's replay.
00:51:45.200 - 00:51:52.240, Speaker I: If you're asking me, I don't know. I don't have the historic balance here. If I had to wait, I would say this was just overlooked.
00:51:56.390 - 00:52:00.900, Speaker C: So even if such exit happens, you can deposit again on that?
00:52:08.180 - 00:52:31.160, Speaker A: Yeah, I don't know if it was overlooked, but I don't think the UX was considered too much about offline signing and having to bring additional information to offline signing. Okay, there's no test on this pr. Someone was going to say something.
00:52:31.310 - 00:53:33.660, Speaker F: Sorry, I just want to chaming that we do have open pr on the deposit COI that we were discussing if we can add the voluntary exit operation feature. But right now it's sort of stuck at the domain. The signing domain like to support for offline signing, but also in the future upgrades we have to upgrade the current version. Sorry, the current folk version in every protocol upgrade. So it does worsen the UX and the maintenance for the deposit tool, the signing tool to me and for the test. I can fix it after the call. Maybe tomorrow.
00:53:38.100 - 00:54:17.760, Speaker A: Okay, let's get tests in there. And our evolving process says this should also have an accompanying EIP so we can track changes. So if we're going to do this, we should open up an EIP with quick motivation justification that links to this and get tests done and get some thumbs ups from various teams. After we do that. Tam, you want to write the EIP?
00:54:19.880 - 00:54:21.910, Speaker C: If that's the blocker, I will.
00:54:23.960 - 00:55:13.630, Speaker A: You can circle back with lion. I'm sure assistance would help. Keep this moving. Okay. Anything else on Deneb? On spec, on research? Anything else at all today? Okay, we have a lot of little things to do on all these prs that are going to be merged very soon. If you have any final comments or review on them, please do so now. We're aiming to have a release early mid this coming week so that we can keep the test nets moving.
00:55:13.630 - 00:55:18.110, Speaker A: Thank you, everyone. Talk to you all soon.
00:55:19.360 - 00:55:20.270, Speaker G: Thank you.
00:55:20.960 - 00:55:23.688, Speaker A: Thank you. Bye.
