00:00:00.250 - 00:02:01.780, Speaker A: Ooh, for you. Welcome everyone. We're live for the 33rd testing call now, so we wanted to talk about the El bandwidth issues around blob gossip, but it might be worth giving you a couple more minutes to see if more folks join. So perhaps it makes sense to chat about if there's any Devnet nine updates and then specifically Devnet ten. I saw that we're sort of getting ready for that, so I don't know if anyone from the DevOps team wants to give a quick update on where we're at with Devnets, and then we'll hop into the El bandwidth conversation and then discuss blob testing after.
00:02:05.110 - 00:02:20.490, Speaker B: Yeah, sure. So Devnet nine is going forward. There hasn't been much change since our last call on Thursday, PK 910. Might have something to share regarding blobs.
00:02:24.510 - 00:03:06.920, Speaker C: Yeah, we've been spamming the network over the weekend with blobs, and I've just grabbed some stats about how blobs are included in actual blocks by clients, and we expected that all client combinations would include blobs at the same rate, basically. But we have seen that especially Aragon and Ethereum Js clients are producing less blocks with blobs inside. So they are producing blocks without blobs. So that should be something we should look into at some point.
00:03:14.730 - 00:03:21.400, Speaker A: Sorry. So does this mean they're not able to produce a block with blobs? Is that correct?
00:03:22.570 - 00:03:27.926, Speaker C: They are producing blocks with blobs, but not as many as other client pairs.
00:03:27.958 - 00:03:28.314, Speaker A: I see.
00:03:28.352 - 00:03:42.778, Speaker C: Okay, so the average has been around 69%. And we see Ethereum Js and arrogant client pairs at about 33%, so much lower than the average.
00:03:42.954 - 00:03:44.000, Speaker A: Got it.
00:03:46.370 - 00:04:18.710, Speaker D: And what possible modification? Sorry, so one possible modification to the script would also be to just query how full the pool was and then correlate that to how many blobs got included, because indicode mentioned that it could also just be a pool related thing. And yeah, pools are pretty full from the perspective of, yeah, we're spamming it, but I don't think we've manually checked if Aragon's pools actually accepted all the transactions or any other node.
00:04:21.640 - 00:04:39.710, Speaker E: Speaking for Ethereum Js, I think it's mostly related to the resource consumption because Ethereum Js just runs on one thread, and all the pools, as well as its execution also runs on one thread. But yeah, we have plans to make it more robust over time.
00:04:42.320 - 00:05:20.694, Speaker A: Got it. And aside from that. Sorry. Yeah, Mario, we can't hear you. Mario, not sure if you're trying to speak. You don't seem muted on Zoom, but we can't hear anything. Now.
00:05:20.694 - 00:05:34.730, Speaker A: You're muted, but we can't hear you. Okay. I think someone else was about to say something with PK.
00:05:36.930 - 00:05:38.350, Speaker C: Nothing from Azad.
00:05:40.930 - 00:06:04.520, Speaker D: Maybe I can start off with one other thing. And then hopefully Mario's mic is fine at the end, because then he would fill more information there. Mario's built this tool called blobber. It kind of sits in between the beacon node as well as the validator. I'm just finding a link. And I'm guessing that's what you wanted to talk about.
00:06:05.770 - 00:06:07.480, Speaker A: I have the link right here.
00:06:08.010 - 00:06:08.422, Speaker D: Yeah.
00:06:08.476 - 00:06:09.910, Speaker A: Perfect. So that one.
00:06:10.060 - 00:06:35.070, Speaker D: It's been deployed on the testnet properly today, like an hour ago. And I don't think we've seen any proposals with it yet, but that would perform a couple of withholding attacks and. Yeah, I think we just redeployed. So Mario's worked on hypedesk for it as well. And I think he just redeployed the hive interop instance to include the block.
00:06:39.810 - 00:07:17.800, Speaker A: Got it. I saw we're getting ready for Devnet ten. It seems like we're just collecting the various releases or branches from the different client teams. Do you know when we expect it to have Genesis, or do we still want to do stuff on Devnet nine and see that stable before we get Devnet ten launched?
00:07:20.650 - 00:07:39.680, Speaker B: I think last week we mentioned that we might do Genesis on Thursday. I think it would depend on the different client teams if they can make a release. Because now, the minute KCG file. I think it's in a pr at this point. I'm not sure if it's merged in.
00:07:40.850 - 00:07:46.800, Speaker A: In the Cl specs. Yeah, a good question. Okay.
00:07:47.570 - 00:08:04.200, Speaker D: It's still open as a pr. I can link it. I'm not sure what else needs to go into it before it's merged, but that's the one we should be using, and it would be nice. Maybe it's not a release, but if it's in a branch and you can point us to which branch it is, that's already enough.
00:08:05.450 - 00:08:18.346, Speaker A: Okay. And, yeah, there was some reviews around testing just an hour ago or something. So, yeah, I suspect that it's still being worked on a bit. Yeah.
00:08:18.368 - 00:08:28.090, Speaker B: So I think we should definitely wait and then schedule any kind of timeline.
00:08:30.370 - 00:08:32.240, Speaker A: Okay. Yeah, that sounds good.
00:08:39.220 - 00:09:16.910, Speaker D: And one more thing is we've deployed Satu with the. It should be collecting blob and block information with lighthouse prism as well as Tekuna. So have a look at the Grafana dashboard. I think I linked it last week. I can fetch another link. And that should like, I think the bigger issue right now is what are we actually looking for? Because we have all of this blob information and all of this block information. So if there's clear questions we want answered going forward to testnets and more stable releases, then kind of reaching out with those questions, now is a good time.
00:09:25.840 - 00:09:36.990, Speaker A: Sweet. Anything else on the devnets before we go into the discussion? Yeah. Mario, are you back?
00:09:39.280 - 00:09:40.540, Speaker E: Yes. Can you guys hear?
00:09:40.610 - 00:09:42.224, Speaker A: Yes. Yes, we can. Okay. Yeah.
00:09:42.262 - 00:09:42.992, Speaker B: You want to go through?
00:09:43.046 - 00:10:38.770, Speaker E: Okay. So the comment I was about to say before was that the problem that I've seen, at least in hive, is that most of the clients, if something is some of the transaction, the blob transactions are reor back into the mempool. On normal transactions, they are reor back into the mempool. But that is not the case for block transactions. So what might be happening also is that we are so the transactions are missed because we are not rescinding them from testing site. And that is really hard to do with the testing transaction tools that we have right now. So that might be one of the reasons, I think, the only client that I've seen that reorgs the transaction back into the mempool is co ethereum and not the rest of them.
00:10:38.770 - 00:10:56.970, Speaker E: So this is something that we have to probably take into account. At least in hype, I had to take it into account. And the problem is that it's not really easy to detect that the transaction is missing from the testing side. Yeah, that was all.
00:11:02.820 - 00:11:12.420, Speaker A: And so is the idea that if there's a reorg, we want El clients to re include transactions from blobs.
00:11:15.000 - 00:11:25.316, Speaker E: It's not required anywhere, but I think we need to take this into account when we are sending spam transactions.
00:11:25.428 - 00:11:27.048, Speaker A: Right. Okay.
00:11:27.134 - 00:11:30.676, Speaker E: Yeah, because we have to roll back somehow.
00:11:30.868 - 00:11:57.650, Speaker A: Right? Yeah. Because I know we discussed this on and off, and I don't think we ever quite got a unanimous consensus on what the right behavior is. I guess we shouldn't enshrine it in the hive test somehow. But I understand that the hive test sort of need to deal with that edge case.
00:11:58.500 - 00:12:26.990, Speaker E: Yeah. And what we did, at least the consensus on the hive test, was that we shouldn't require this. And I understand because you have to do some fancy things on the mempool to cache the blobs. Just so you are ready, when it's reorganed out, you can put it back on the mempool. So it's not so straightforward to do. So the hype tests are not expecting this at the moment.
00:12:27.760 - 00:12:28.236, Speaker A: Got it.
00:12:28.258 - 00:12:41.920, Speaker E: But yeah, it's something that we have to take into account where when we are spamming things, it might be the case when we are missing some transaction and then we cannot proceed with the rest of the transactions because of the nonsense, whatnot.
00:12:42.980 - 00:13:15.420, Speaker A: That makes sense. Okay, I think we have folks from the El side now, light clients. Do you want to give some background on the blob bandwidth conversation we had on Cordev's last week and then can hear from other client teams?
00:13:17.680 - 00:14:22.160, Speaker F: Yeah, unfortunately I don't really have specific bandwidth numbers or know exactly what Peter is hoping to get the numbers down to. But my understanding is that whenever he started running up on Devnet nine and was propagating the blob transactions, the way that our transaction gossiping, fetching logic works is that we sort of just fully saturate our bandwidth. So if there's enough blobs out there in the pool, then we're going to just naively try and retrieve them as quickly as we can. And there's like no throttling mechanism there for clients coming online or lots of blob transactions coming into the pool. And I think what he would like to make happen is have some way that clients throttle themselves some maximum amount of their bandwidth so that we're able to continue fetching the blobs if there's a lot of them, to fetch and not inhibit other operations, other client operations or operations for that user's local Internet.
00:14:24.040 - 00:14:41.000, Speaker A: Got it? Yeah. Any other El team want to chime in about their thinking around this? Any numbers they've seen?
00:14:42.090 - 00:15:28.260, Speaker G: So in Nethermind, the feature for having into DB process block transactions and then re adding to the pool. We don't have it on Devnet nine. It is already implemented on the branch, but it wasn't touched for some time and will be refactored. But we generally have it. Just need to polish it and include into the branch from which we are using image on Devnet nine and about throttling, we will have it, but we don't have yet.
00:15:34.530 - 00:16:31.040, Speaker A: Got it. Any other team have thoughts? It. Okay, I guess. Is this a question around you? Guess, yeah. For the teams that don't have super strong thoughts, do you see this as a blocker to keep moving forward or are you comfortable with the way your gossip is set up?
00:16:36.980 - 00:16:42.684, Speaker E: Sorry, this is about blob transactions on Reorg or about.
00:16:42.742 - 00:17:27.250, Speaker A: No, just in general. The fact that the blob transactions increased the gossip utilization of the node. And I think guests wanted to implement a more like a fetcher, like something that basically control the amount of bandwidth that it's using. But I'm curious to see if there's anyone who, if other teams feel like this is also a blocker for them basically, or if they're fine with the bandwidth consumption as it is, or if they don't know what the current bandwidth numbers look like. Yeah.
00:17:32.570 - 00:17:37.320, Speaker E: I don't have an answer off the top of my head. We have to look into it. I don't know yet.
00:17:38.810 - 00:17:39.800, Speaker A: Got it.
00:17:40.570 - 00:17:44.300, Speaker F: I also need to ask the team. I don't have an answer either.
00:17:45.630 - 00:18:36.730, Speaker A: Yeah, got it. And so if we feel like the bandwidth usage is too high and then we need to implement something like a specific fetcher or like a throttling system, do people think we would then need an additional devnet to test that? Or is that something we'd be comfortable testing on testnets directly? Yeah, because I guess if we're thinking of potentially holding devnet ten based on this, then we should make that clear now and also figure out how we figure out exactly what we want to build. But if it's more of a nice to have and something that we're comfortable testing on testnets, then there's not as much of a rush to make a decision or agree on next steps.
00:18:42.250 - 00:19:01.554, Speaker D: I would make the case that it's probably okay to test it on actual public testnets because technically the way for us to test it on devnets would just be to spin up a new node and then I guess sync to head and then monitor how it looks. So it'll probably also show up in sync tests when we do public testnets.
00:19:01.702 - 00:19:31.720, Speaker A: I think one thing that's nice with public testnets as well is that you get to see versus historical data, right? So that'll give us a good comparison. Whereas Devnets, there's just not a ton of historical data. And then I guess potentially the risk though with testnets is just nobody actually uses blobs on it. So we should make sure though to actually spam blobs on the test nets once we fork them.
00:19:35.000 - 00:19:37.520, Speaker D: 99% sure we're going to be spamming blobs.
00:19:37.600 - 00:20:42.680, Speaker A: Yeah, I'll send anyone some gordy ether if they need to do that on Gordy, but. Okay, I think that makes sense. Obviously teams haven't had a ton of time to look into, like we should start looking at it and then I think testing it, quote unquote for real on Gore, seeing what bandwidth looks like there and if we want to add any sort of transaction fetching capabilities. Yeah, let's do that on the actual testnet. And even for that, though, you can imagine forking Gordy, realizing we need to add a fetcher and just shipping a different release and running that on Gordy and seeing how it works. Like we maybe don't even need to wait for another testnet or for all nodes to adopt some sort of fetcher or throttling system because we'd just be able to see it on the new nodes. And then by the time we're at main net, whatever we come up with would obviously be like a default.
00:20:46.040 - 00:20:49.990, Speaker B: It could also be just a flag, possibly, right?
00:20:50.600 - 00:20:51.300, Speaker A: Yeah.
00:20:51.450 - 00:20:57.116, Speaker B: Some clients or some operators might have beefier machines, right?
00:20:57.138 - 00:20:57.564, Speaker A: Yeah.
00:20:57.682 - 00:21:05.660, Speaker B: And then they can allow different metrics. So maybe it shouldn't be like an on off switch, but rather like configurable.
00:21:06.240 - 00:21:57.366, Speaker A: Yeah, that makes sense. Okay. Yeah, I think that approach is reasonable. Is there anything else that people feel is a blocker to Devnet ten? Except the stuff we've already discussed on all core devs last week? Or is the list of what we wanted to see before going live still accurate? Where obviously just hive tests being stable, the MEV pipeline being more tested, the KZG setup being in, and then we had something around getting better, tooling around, analyzing blobs. I'm not sure if there's been any progress on that in the last couple of days. Oh yes. Yeah.
00:21:57.388 - 00:21:59.286, Speaker D: That's the Zappo stuff that's been set.
00:21:59.468 - 00:22:02.200, Speaker A: Yeah, yeah, I see the Grafana leak. Awesome.
00:22:04.970 - 00:22:20.080, Speaker D: I mean, the base tooling is done and we have the data, the raw data now and then. It's just a question of what do we actually want to see. We've set up some template dashboards, but that's just our idea of what we want to see. I'm sure client devs have a better idea.
00:22:21.010 - 00:22:26.480, Speaker A: Okay, that's cool. That's where the metrics and we can have an initial look.
00:22:28.950 - 00:22:41.634, Speaker B: One more thing. That Mev blob inclusion is still not something that's working. So it's still something that needs to be working before. I think we cannot this possibly. It's not a blocker.
00:22:41.762 - 00:22:52.920, Speaker A: Do we know where the blobs are getting stuck? Is it the builder? Yes, exactly.
00:22:53.630 - 00:23:13.722, Speaker B: Because every client pair was able to submit at least one blob, according to Dora. We just haven't got any metrics on how many blobs were included with MeV. But maybe that's also something we can take a look somehow.
00:23:13.786 - 00:23:27.570, Speaker A: So do we just not know, or do we know that the builder is not including blocks? Is not building blocks with blobs.
00:23:28.710 - 00:23:35.522, Speaker B: It's Schrodinger's blob. We haven't found a block that had a blob that was built with Amivi.
00:23:35.666 - 00:23:42.410, Speaker A: Is there a way we can. Can we just turn off local block production?
00:23:47.070 - 00:23:49.510, Speaker D: What do you mean by turn off local block production?
00:23:49.590 - 00:23:52.666, Speaker B: Or for which one it would fall back.
00:23:52.848 - 00:23:53.354, Speaker A: All right.
00:23:53.392 - 00:23:54.714, Speaker B: Miv is not available.
00:23:54.912 - 00:23:57.706, Speaker A: It always falls back local loadstar.
00:23:57.738 - 00:24:09.540, Speaker E: It's possible using a flag, so I can help you with that. We have such flag in Nethermind, so we can turn off local logs as well.
00:24:11.030 - 00:24:26.658, Speaker B: Okay, then what we could also just do is load star Nethermind, just disable local block production and see if we can fill up its blob pool and just see if can actually submit any valid blocks with blobs.
00:24:26.754 - 00:24:29.320, Speaker A: Yeah, Teku and Lighthouse have it as well.
00:24:29.930 - 00:24:42.860, Speaker D: Wouldn't it be easier to go the other way around? The Mav relay has a website of all the blobs that it has produced. So you can just query them and see which of them have blobs. If the answer is nothing, then we have a problem.
00:24:45.650 - 00:24:47.680, Speaker A: Right, the flashbots relay website.
00:24:48.370 - 00:24:51.822, Speaker D: Exactly. Because that'll give us a definitive answer.
00:24:51.876 - 00:24:52.094, Speaker A: Right.
00:24:52.132 - 00:24:59.786, Speaker D: It's extremely unlikely that a relay has been running for one week and there has been zero blobs that have been appropriate for inclusion.
00:24:59.898 - 00:25:08.194, Speaker A: So I think, yeah, it seems easier to query that first, assuming the answer to that is zero, then we can.
00:25:08.232 - 00:25:09.602, Speaker D: Maybe run this one.
00:25:09.656 - 00:25:19.400, Speaker A: Yeah. So Teku, Lighthouse, and Lodestar all have it, and then at least, never mind on the El, so we can run those and try to force blobs through.
00:25:23.290 - 00:25:41.818, Speaker D: I have. Or at least I think our working theory was just that the builder strategy doesn't take into account blocks. That would be the extremely naive idea. And we have Mavflud that kind of submits extremely lucrative blocks.
00:25:41.914 - 00:25:42.222, Speaker H: Sorry.
00:25:42.276 - 00:25:47.490, Speaker D: Lucrative transactions. We think the builder is just kind of grabbing them and ignoring the blobs.
00:25:48.870 - 00:26:13.290, Speaker A: Right. Is there a way for the blob transactions to just submit, like, a really high? Would the builder be, like, ignoring the transaction type? Because wouldn't it just look at the total gas paid for the transaction.
00:26:14.670 - 00:26:25.610, Speaker D: I honestly don't know what the builder strategy is. We're just running it on strategy three D, and I think Mev flood submits really juicy transactions so that the builder always grabs them.
00:26:25.760 - 00:26:26.780, Speaker A: Got it.
00:26:27.630 - 00:26:32.800, Speaker I: Do we know if this mevblood tool is making blobs, though? Because I think that was an issue that they had.
00:26:35.730 - 00:26:40.110, Speaker D: Mev tool making blobs? I don't think we have something like that yet.
00:26:40.260 - 00:26:54.440, Speaker I: Well, no, but if Mevflud does, if that's what we're using. But also, I think the flushbots builder should just use the mempool. So if we just send a massive, or I'll say, very valuable, blob transaction, it should become clear.
00:26:57.900 - 00:27:04.764, Speaker D: We can try that out, too. I guess we can just make some blobs that are extremely expensive and then submit them and see if they.
00:27:04.962 - 00:27:15.484, Speaker B: I think we don't have Mav flood running anymore because we ran out of eat funds right now. We spent, like, over a million eat.
00:27:15.602 - 00:27:21.376, Speaker A: Already on the Devnet. Yeah.
00:27:21.478 - 00:27:23.490, Speaker D: To clarify, on a Devnet. Yeah.
00:27:30.760 - 00:27:44.448, Speaker A: Okay, so I guess we can at least look at the blobs or blocks built on the relay. And does anyone know if there's an API for the relay where we can.
00:27:44.554 - 00:27:48.228, Speaker I: Yeah, so there's the specs I just dropped. And there's, like, API specs.
00:27:48.324 - 00:27:49.256, Speaker A: Okay, nice.
00:27:49.438 - 00:27:57.260, Speaker I: Although I will say I've even been trying to build stuff off of these and found some things that were incorrect. So proceed carefully.
00:27:58.080 - 00:28:20.452, Speaker A: I guess there's only 800 payloads, so worst case, looking at those eight. Sorry, no, not manually. I was going to say, yeah, I guess you need a node to pick up the transactions from or the blocks from.
00:28:20.506 - 00:28:25.412, Speaker D: Yeah, I mean, we just need to see the slot numbers, and then we can query the rest ourselves, I guess.
00:28:25.466 - 00:28:34.424, Speaker A: Yeah, exactly. Data API. Yeah. Okay. Yeah, I think also.
00:28:34.462 - 00:28:41.480, Speaker I: Yeah, so there's, like, a small view of it on this web page. The boost relay D nine. Yeah. What Barnabas just linked.
00:28:44.300 - 00:28:48.910, Speaker A: Yeah. Okay, so I think using that, we can probably.
00:28:50.160 - 00:28:59.890, Speaker D: I have an even dumber approach. You can just call the web page and it has the data in it, and then you can just use said in Linux magic to get the best.
00:29:02.420 - 00:29:22.730, Speaker A: Okay, so with five calls there, we should be able to get all the historical data, see if any of them have blobs, and then if not, figure out whether it's an issue on the builder side or whether just running the clients with a different setting will get them through.
00:29:24.860 - 00:29:27.770, Speaker I: And you're running the builder from this Net branch, right?
00:29:29.820 - 00:29:30.232, Speaker A: Yes.
00:29:30.286 - 00:29:39.912, Speaker B: So the builder is off of, like, client's latest Devnet nine commit, as in.
00:29:39.966 - 00:29:46.190, Speaker D: The Shana is updated to, like, client's latest commit, but, yeah, it's the flashpots branch, right?
00:29:46.820 - 00:29:47.570, Speaker A: Yes.
00:29:52.490 - 00:29:55.800, Speaker B: But it would be good to have another builder also.
00:30:07.960 - 00:30:24.360, Speaker A: Okay, so, yeah, I think there's a couple threads we can follow up on offline around all this. Anything else people want to bring up that we should be looking at in the next couple of days before we try and launch Devnet.
00:30:29.790 - 00:30:40.990, Speaker I: Sorry. One more thing, just to close the loop here. It looks like this docker image is from what Matt merged. But if you go look@their.net branch, they have stuff on top of that that looks very relevant.
00:30:43.960 - 00:30:44.710, Speaker A: Cool.
00:30:46.920 - 00:31:02.280, Speaker I: So the docker image, from what I can tell, it looks like it's off of commit the 66. And if you just look at the history, it looks like all of the important Deneb stuff is on top of that. Like, blob bundle fixes.
00:31:04.240 - 00:31:07.080, Speaker A: On which branch is that, Alex?
00:31:07.240 - 00:31:09.224, Speaker I: Flashbots builder Daneb.
00:31:09.352 - 00:31:10.812, Speaker A: Okay, got it.
00:31:10.866 - 00:31:18.352, Speaker I: So, yeah, if you just look at the link I just pasted, you can see there's, like, a few more commits on top. I checked the.
00:31:18.486 - 00:31:19.840, Speaker A: Yeah, nice.
00:31:19.910 - 00:31:30.950, Speaker D: Yeah, we can rebuild an image and then try, if that helps. And it looks like Philip's already done an initial analysis. The last 200 blocks have zero blobs. So there's definitely an issue.
00:31:32.200 - 00:31:32.950, Speaker A: Okay.
00:31:42.520 - 00:31:50.920, Speaker B: You should look at the hash from Matt commit. Oh, yeah, I see what you mean. You have four more commits.
00:31:57.230 - 00:32:33.170, Speaker A: Okay. And it seems like at least the last 200 blocks, there's zero blobs. So clearly there's probably something going on there that's not the clients. Okay, so Vim versus nano is something we can leave to the DevOps standup blop censorship. Alex, what can we do?
00:32:33.860 - 00:32:35.804, Speaker I: Oh, no, sorry. That was a joke.
00:32:35.932 - 00:32:36.368, Speaker A: Yeah.
00:32:36.454 - 00:32:38.340, Speaker I: The builders are censoring the blobs.
00:32:38.840 - 00:32:43.190, Speaker A: Okay, anything else?
00:32:44.200 - 00:32:57.130, Speaker B: I guess that mev related will be censoring blob transactions. If they cannot extract any mev from them.
00:32:58.540 - 00:33:12.510, Speaker I: Okay, this is a joke. I was just saying, because if you look at the relay, there are no blobs. It's like they're censoring them. The relays have. They're indifferent to blobs. They would love blobs if they pay them.
00:33:18.420 - 00:33:24.240, Speaker D: So it could be that if there's some fancy NFT drop. That there's a couple of blocks without any blocks.
00:33:26.180 - 00:33:35.140, Speaker I: Yeah. I mean, the whole idea is, right, that if all of the economic activity moves to l two s, then in, say, a year, that's going to be the juicy transactions.
00:33:36.440 - 00:33:39.830, Speaker D: Yeah, makes sense. Just pay more if you want to include it.
00:33:51.010 - 00:34:12.640, Speaker A: Okay. Anything else before we wrap up? Sweet. Well, no, thanks, everyone. Yeah. Let's figure out these last couple of issues in the next few days, and we'll talk. Oh, sorry, Mario.
00:34:14.340 - 00:34:20.020, Speaker E: Yeah, sorry. I thought about the topic, but I have another topic that I want to talk about. That's okay.
00:34:20.090 - 00:34:20.756, Speaker A: Yes, please.
00:34:20.858 - 00:34:41.018, Speaker E: Yeah, I think it might be a little bit better if I just share my screen. Sure, yeah, let me try. Okay. Can you see my screen?
00:34:41.104 - 00:34:41.740, Speaker A: Yeah.
00:34:42.110 - 00:35:32.010, Speaker E: Okay, so regarding the testing of the blobs gossiping on the consensus layer. So we have developed this handy tool that helps us make gossip testing basically on the hive and also on the devnets. Basically what this thing is just like a proxy. It sits between the beacon client and the validator client, and it basically listens to all the rest API calls. And it basically just lets everything go through, everything that is not a proposal. So when a proposal comes by, it basically intercepts the proposal and it can sign the blops and the block also. But it can also do some extra actions.
00:35:32.010 - 00:36:04.930, Speaker E: And these extra actions is what we are currently testing on Hive. There are many of them. I'd like to share basically this link. You guys can come over and see what it can do. Most of the things that it can do is just send extra blobs. They are all listed here, extra blobs, conflicting blobs, incorrect blobs, and all that stuff that we can test. And we have a set of tests on hive that we can just execute.
00:36:04.930 - 00:37:04.360, Speaker E: I will just share the link after this call on the R D so you guys can take a look at the list of things that we're doing. And just if you can, comment on more things that we should test also. Welcome. The main thing that we are having trouble right now is that basically this thing is when we try to intercept a proposal and inject more blobs, we need to create a dummy client that connects via that peer to peer to the consensus clients. And the problem here is that we are finding that most of the clients are simply just banning us because we do not implement the entire consensus peer to peer, the entire consensus peer to peer set of specifications. So we just partially implement that. And because of that we are downscored very quickly.
00:37:04.360 - 00:38:10.700, Speaker E: So one of the things that would make these tests a little bit more consistent is that we need somehow to add this blobber client into the trusted peers list. And this is very easy to do. I only tried it with Lighthouse and they have this parameter that is basically just trusted peer to peers. And we give it a peer id, which is the blobbers peer id because we know it beforehand before launching the blobber. And Lighthouse is basically passing all tests. But the rest of the clients are, well, the rest of the clients that I've tested are just disconnecting us quickly and we don't have a consistent test result at the moment. So even though we have like six tests right now over here, most of them are not consistent and we don't have a consistent set of results to begin submitting bugs and all that.
00:38:10.700 - 00:39:17.540, Speaker E: That's basically it. So we have this tool and I will share this page so you can take a look. Every test case that we have written for Hive at least has the description what it does and what it sends to the clients and what it should expect from the clients. We are also planning on deploying this tool on well, partish is currently working on deploying this tool on the testnets, so we will see how the clients fare there because there are a lot of more clients in there and they're possibly going to be more issues than in hive. But I think this hive test suite will help us at least with the advantage here is that we have only one type of client per test run and we can single out if one client is failing with one specific test type. So yeah, that's basically what we wanted to share. I will share this link and the hive test suite so you guys can take a look and some feedback is also welcome.
00:39:17.540 - 00:39:23.140, Speaker E: I don't know if there are any questions.
00:39:26.070 - 00:39:33.106, Speaker D: It's also deployed now on the Devnet, but just on one note. So lighthouse Rex is doing weird shit along with tickle.
00:39:33.138 - 00:39:44.000, Speaker A: Get nice. Any questions, comments?
00:39:50.480 - 00:40:18.260, Speaker E: Also, if the rest of the clients already have something like trusted peer ids parameters that you can share with me, that would be really helpful. At the moment I only have a prism already. Let me know in the r and D discord what's their equivalent parameter for this. But yeah, please, if the rest of the clients already have this, please just let me know. I can just try to include it in Hive.
00:40:19.400 - 00:40:27.400, Speaker H: Yeah, I can send it on the R D discord after the chat since we use those flags quite frequently.
00:40:28.940 - 00:40:39.180, Speaker D: Just one question, Frederick. Are you guys using the flags on the main branches of the clients or do you guys maintain a fork with the flag?
00:40:40.160 - 00:40:57.570, Speaker H: It's on the main one. I think some clients have it kind of as a secret flag, so to speak, but we don't maintain any specific clients or forks for it. It's the client teams that have it.
00:40:58.340 - 00:40:59.090, Speaker A: Thanks.
00:41:01.060 - 00:41:24.680, Speaker E: Yeah, thanks. And also just clarify, this is just gossiping the blobs. At least in Hive. There is no set of tests yet regarding the peer to peer requests for blobs, so that's still work in progress. I guess this is just gossiping.
00:41:40.180 - 00:41:42.080, Speaker A: Were you going to say something, Frederick?
00:41:44.260 - 00:41:58.840, Speaker H: No, just that I will send it afterwards. I was going to say that if you want, I'm on my phone now, so I can't send it. Otherwise I will do it just right in the chat. I will do it in a few minutes. But I was going to say that you can also look at ETB.
00:42:00.940 - 00:42:01.256, Speaker A: If.
00:42:01.278 - 00:42:13.070, Speaker H: You know where that repo is. And it will have the flags when launching the. Yeah, I think it has the flags built in there.
00:42:16.520 - 00:42:37.130, Speaker A: Awesome. Thanks. Anything else? Okay, well, thanks a lot everyone, and talk to you all on Thursday's call.
00:42:41.600 - 00:42:42.620, Speaker I: Thanks everyone.
00:42:42.770 - 00:42:43.180, Speaker A: Thanks.
00:42:43.250 - 00:42:43.976, Speaker B: Bye.
00:42:44.168 - 00:42:45.144, Speaker A: Thanks. Bye.
00:42:45.272 - 00:42:46.540, Speaker E: Thank you. Bye.
