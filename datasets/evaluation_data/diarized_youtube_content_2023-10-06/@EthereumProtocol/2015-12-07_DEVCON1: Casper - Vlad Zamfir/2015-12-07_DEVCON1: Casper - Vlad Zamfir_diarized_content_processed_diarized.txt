00:00:18.090 - 00:01:13.860, Speaker A: So this talk is basically designed to come after my first talk, which hopefully, I mean, there were more of you listening to that talk in this talk, so I guess we won't have a big problem. And basically the first talk was kind of discussing the problem space. What are the kind of the design requirements that we have when we are trying to build a public consensus protocol. And Casper is my kind of, and in collaboration with Vitalik and a few other people, attempted solution to this public consensus problem. So this talk should give you kind of an overview of the process that led to the development of Casper. Casper's design philosophy, the different parts of Casper as a protocol, and then the failure modes, like what can go wrong given how many faulty nodes. So let's get started.
00:01:13.860 - 00:02:21.850, Speaker A: So the first design principle is that it should be suited for public consensus, which is basically to say suited for the kind of analysis that I shared in my talk earlier this morning. Basically, Casper should work in a context where a majority of nodes are rational and colluding to undermine protocol. They're willing to collude in large majorities in order to undermine protocol guarantees, as long as that's profitable. So basically what we want to have is coverage of byzantine faults with disincentives, and specifically byzantine faults that undermine protocol guarantees. And the reason we want to cover them with disincentives is because we want an attacker who has a budget to spend, to have to budget a lot of money in order to undermine the properties of the consensus protocol. So this is like the first, most important design goal of Casper. We want to make it expensive for an adversary to bribe a rational majority of nodes to undermine the protocol.
00:02:21.850 - 00:02:59.618, Speaker A: So we want to cover the space of byzantine faults with disincentives. That's important, but not in both. So another key design goal is that it should be light, client friendly. And then the last design goal is that it should be good for users and application developers, basically. And there's two main ways what that means to me. One of them is that Casper tells you the state of the finality of transactions. So you won't be surprised if a transaction is reverted and you will know when it can't be reverted.
00:02:59.618 - 00:03:49.958, Speaker A: And then Casper is also low latency, which I think is really important for user experience. I mean, as everyone probably agrees, when you're at a website, you want it to respond right away. A distributed application should be the same, and we're going to cover some background knowledge before we really get into the heart of what's new about Casper. So the first thing is Casper is a variation on security deposit based proof of stake. So it's hard to stress enough how important security deposits are to securing public protocols, especially proof of stake consensus protocols. And I'll explain why. But first I'll mention that this concept was discovered independently by many people, including Jae Kwan, Nick Williamson of credits.
00:03:49.958 - 00:04:44.038, Speaker A: Jaequan is working on this project called Tendermint. Dominic Williams, who you saw on the panel earlier, and Vitalik and I, we all thought of this protocol, this idea that we could use security deposits to secure a consensus protocol. And the idea is that people would have to place a security deposit in order to produce blocks, and if they do something bad, then they would lose their deposit. So proof stake in general uses digital assets, defined in the consensus as anti civil mechanism, whereas proof of work uses computational resources, defined outside of the consensus as an anti civil mechanism. And there are other variations on this kind of proof of work thing theme of using outside resources, like you have proof of stores, proof of bandwidth, and all these things could be used conceivably as an anti civil mechanism. And there have been some people who look at designs of cryptocurrencies based on those. But basically, there's traditionally said to be two fundamental problems with proof of stake, the nothing at stake problem and the long range tag problem.
00:04:44.038 - 00:05:51.258, Speaker A: So the nothing at stake problem basically is in traditional proof stake protocols, like pure coin, et cetera. Next, nodes don't have any disincentive from being byzantine. Signatures are easy to produce and they don't lose tokens for being byzantine. And so they have an incentive to sign on both chains, and they basically have no expenses. And so any profit they can make from underlying undermining the contest protocol's guarantees is something that they will do in a worst case analysis, because they don't have any disincentives, and they'll basically do it for free. Security deposits solve the nothing at stake problem by requiring that validators put something at stake and by removing that security deposit when they're demonstrably byzantine in some way. So it basically requires that this proof of them being byzantine is discovered and published to the blockchain in order to affect their balances in their deposits, because everything that affects the state of the blockchain has to be published into the blockchain, and so that the state transaction function can update the state of their bonds or the state of something in the ledger.
00:05:51.258 - 00:06:41.950, Speaker A: So basically, as long as these evidence transactions aren't censored, faults can be punished. And now this is kind of a broad, almost philosophical point where in proof of work, the consensus producing changes. The consensus is always expensive in an effort to make it prohibitively expensive for an attacker to produce a longer, more heavier fork. So it's more expensive always for the network than an attacker should be willing to spend at any time. In security deposit proof stake, consensus should be cheap for everyone except for attackers during an attack. And this is kind of a big shift in approach, where instead of everyone always incurring more of an expense than attacker might ever incur. People don't incur an expense, but an attacker will incur a very large expense.
00:06:41.950 - 00:07:39.630, Speaker A: So this approach promises to be much more economically efficient, because instead of everyone having to always incur an expense, only attackers incurred the expense during an attack. So this is also very an important perspective change in how to secure consensus. So, in proof of work, the disincentive is basically, again, the disincentive is not receiving an incentive which compensates you for your computational work. In security deposit based proof of stake, the disincentives is basically just a forfeiture of their security deposit. So then we have the long range attack problem. Long range attack problem is basically that an adversary who controls old keys, keys that no longer have coins in them, perhaps? Well, the reason we assume they don't no longer have keys in them, because then it's cheap for the attacker to buy these keys. Attacker could use old keys to produce chains that have a higher score.
00:07:39.630 - 00:08:35.940, Speaker A: And this is kind of fundamental of traditional proof of stake, and it's fundamental all proof of stake, so long as the thing that people use to authenticate the current state is the genesis block, because the genesis block is something else, by our current assumption, produced a long time ago. And the people who have something at stake then no longer have something at stake now. And so if you rely, you can't rely on their signatures. Their signatures effectively mean nothing because they no longer have something at stake, and they can't be punished for equivocating, for making bad signatures. So what we need to do to get around the long range attack problem is to change the authentication model. Instead of authenticating everything based on your knowledge of the Genesis block, you authenticate based on your knowledge of who currently has something at stake, and therefore, who currently has signatures that you can trust. So, basically, this means that clients have to know who currently has a deposit in order to synchronize with the state.
00:08:35.940 - 00:09:28.666, Speaker A: And if they're able to stay online often enough, then they'll be able to maintain this list. So the validators rotate over time. The set of validators today is not going to be the same as the set of validators in two years. So if you come online in two years with only knowing the list of the validators today, you're not going be to able to synchronize to the consensus. But if you come online regularly enough to watch the validators rotate, then you can use the fact that you know who's bonded today to learn who's bonded in the future at every step using the economic assurance that if these people were lying to you, they would lose their security deposits. So this kind of, is this thing called weak subjectivity. Basically, it's kind of like a single public key for the whole blockchain, which you need to authenticate out of band before you can synchronize and which kind of changes over time.
00:09:28.666 - 00:10:35.960, Speaker A: But you can figure out, you can watch the change happening if you have the current public key. And so you can kind of be man in the middle attacked if you have the wrong key there. And you'll show up in some kind of nonexistent economy. And so there's basically some trade offs here, right? In kind of proof of work, you just need to know the Genesis block and you can authenticate the current state, whereas in this kind of proof of stake model, you need to be online sufficiently often to have a current picture of the set of bonded validators. So there is some cost there in terms of what we can provide as a service to nodes who are offline very often. But there is one main benefit to this approach, which is that it's much, much more like client friendly than proof of work or any genesis block authentication model could ever be, because you don't have to download header chains, you don't have to compare scores of forks that come from the beginning of time. You have a piece of currently economically meaningful information which you can use to immediately sync up with the current state.
00:10:35.960 - 00:11:36.070, Speaker A: And so the level of client friendliness that's possible in this kind of weekly subjective model is much, much greater than what's possible in proof of work or kind of genesis block authentication model. So those are the trade offs for that design. Again, this idea of that you need to know that people who currently have a deposit is fundamental to using security deposit based proof stake in a way that is economically secure, because people who no longer have deposits are willing to sell their keys for free, almost free for epsilon, and epsilon is greater than zero. So some more background knowledge. There's the ghost protocol for proof of work. This is a protocol called Greedy Heaviest observed subtree produced by Sombalinsky and Zohar quite some time ago. And basically it can reduce the block latency without reducing security for a proof of work chain.
00:11:36.070 - 00:12:19.354, Speaker A: In proof of work, a miner is only rewarded for a block if it is eventually part of the chain. So there are these things called orphan blocks, which a miner finds but doesn't get included in the chain. And these blocks don't contribute to the security of bitcoin, for example. But in Ethereum, we have this kind of implementation of ghost that makes it so that we include orphan blocks, and they contribute both to the score of the fork, and we compensate the miners for the work. So the miners aren't screwed by having their block be orphaned and the fork is secured because we don't lose the work. The work still is part of the comparison of the forks. Basically, ghost made proof of work.
00:12:19.354 - 00:13:10.570, Speaker A: Lower latency without compromising on, basically. But in Ethereum, we have an implementation of ghost, but it's not the full ghost protocol as given by Zohar and Sam Polanski. And basically we include the headers of orphan blocks into blockchains, and that will contribute to their fork. But what we don't do is actually have a block tree based rule. And the reason we do this is because it's kind of simpler and we get most of the benefits. But if we did have the full implementation, we could have even lower latency blocks. We'd be able to kind of execute transactions for morphin blocks, and selfish mining and mining pools would be even less attractive.
00:13:10.570 - 00:14:40.790, Speaker A: So Casper is an adaptation of ghost to proof stake, and we have this similar kind of thing where we can have a more complex version with trees, or a simpler version that still has some of the ghost like properties. And this is one of the things that we're still debating between me and Vitalik about what will be appropriate for the release of. So certainly incentives will be a function of orphaned information. So that much we agree on. Now, what we don't agree on is whether or not we will allow orphan blocks to have the bets that they're including contribute to the bet history, or whether to execute transactions for market blocks. If we allow these things, then it'll be like a little bit more complex from the protocol point of view, but we'll be able to have slightly lower latency. And basically with more complex versions of Casper that fully implement this tree choice rule, we can go well below the end to end network latency in terms of producing blocks, which is good for user experience, but it increases the implementation complexity because we actually have to do tree choice as opposed to just blockchain choice, because if all the blocks are chained in a single chain, then you kind of have to wait for at least half a network latency, say, well, you will have to wait for the next validator to produce a block on the previous one's block.
00:14:40.790 - 00:15:06.640, Speaker A: So we kind of were arguing about whether the complexity of bringing the block time below the network latency is worth it. Additionally, this is kind of a neat thing. I came up with a way to make this tweet choice rule light client friendly. So you could imagine that as a light client watching, like, subnetwork latency blocks, you might have a kind of a nightmare. Really? I only have three minutes left. You're kidding. Oh my God.
00:15:06.640 - 00:15:47.254, Speaker A: Do I actually have three minutes left? Oh my God. Okay, so you calculated the two choice rule in the state transition function, and you trust the cap theorem from our previous talk? Castro chooses availability. We're going to do consensus by having iterative betting on whether blocks win at a particular height. And basically the betting games at different heights are independent, and the betting games can finalize blocks. If you have a whole bunch of finalized blocks in a row, then you have finalized state. So non finalized blocks, sorry. Finalized blocks aren't reverted for kind of two reasons.
00:15:47.254 - 00:16:57.774, Speaker A: One of them is that reverting a finalized block would create a very large amount of evidence of malfeasance, which would lead to a very large amount of loss of security deposits. So we have an economic assurance that two finalized blocks won't be created, because it creates a lot of this evidence which can be used to punish a lot of them, which means that an adversary would have to spend a lot of money to revert to finalized block. And the other reason is that clients with their fork choice rule will refuse to adopt a fork that doesn't include a finalized block. And so this version of finality, via clients strict unwillingness to revert blocks can lead to consensus failure in the case where there's two finalized blocks shown to two different clients before they see each other's finalized blocks. But it provides an ironclad guarantee of finality in the sense that we know that this block will not be reverted because the client just refuses to do it based on their four choice rule. So Casper designed to be censorship resistant. Basically, the way we do this is if some nodes are offline, we're going to disincentivize.
00:16:57.774 - 00:17:44.560, Speaker A: We're going to assume that they were censored and punish everyone. So if you were to censor some minority, then the remaining everyone would lose money. So that if a large coalition were to cooperate to censor a minority, then they would have to do that at their expense. So, unlike proof of work and traditional proof of stake, you don't get a raise for censoring. You lose money, your salary goes down, and now we basically have to go through our failure modes really quickly. So history reversion is something that can happen for non finalized blocks, in which case clients will know that the block wasn't finalized because Casper tells you the state of the finality of the. That's there is some significant mitigation there.
00:17:44.560 - 00:18:43.650, Speaker A: Preventing finality is kind of a more serious failure mode, where a coalition could stop finality from occurring, which prevents them from introducing a new set of validators. This is kind of important. You need to finalize a new set of validators before the first set of validators can rotate, because you must know for sure who the validators are when you come online as a client. Censorship is like the most severe failure mode. If censorship is successful, then evidence transactions can't get in, and people aren't going to be disincentivized from committing byzantine behavior because they won't lose their security deposits. So while we do have an economic assurance that their censorship is expensive, if you get sufficiently close to 100% of nodes to get together to censor, that disincentive shrinks because the protocol can't tell if there's any censorship. If all nodes are censoring, if some nodes are missing, it could kind of infer that maybe the blocks in those nodes aren't being included.
00:18:43.650 - 00:19:23.498, Speaker A: So, moreover, the cost of censorship only exists for a finite amount of time, because we can't punish people forever if some nodes are offline. So there is like a cost that an adversary can pay to censor the network. But there are some ways you can mitigate it. One of them is through cryptographic means, through time lock cryptography, and, sorry, I'm really close to done or obfuscating transactions, basically to make it hard to censor some transactions without censoring all of them. There's also economic sanctions. Clients can detect censorship under certain network assumptions. And then they could treat that fork as having a low score and therefore choose another fork, which might have a minority of nodes, but who aren't censoring.
00:19:23.498 - 00:20:02.670, Speaker A: Or we could just create a hard fork that removes the censoring validators from the set of validators. And if there's enough community backing behind it, then that's like a very strong economic disincentive to attack. If the community is able really to govern the code so well that they can hard fork in response to censorship. So while some of these failure modes are pretty bad, actually Casper has more complete coverage of faults, this kind of economic setting, than any other consensus protocol out there right now. But the devil isn't the details. There's lots of parts of the way that these rules for incentivizing consensus and disincentivizing things have to be specified. So the complete specification isn't available yet.
00:20:02.670 - 00:20:20.830, Speaker A: As you may have noticed, there's still details of the protocol that need to be finalized. However, we are starting to do formal implementation and verification of protocol. Specifically, we're looking at some of the economics, but mostly the convergence properties. And that concludes my talk. Sorry for the rush.
