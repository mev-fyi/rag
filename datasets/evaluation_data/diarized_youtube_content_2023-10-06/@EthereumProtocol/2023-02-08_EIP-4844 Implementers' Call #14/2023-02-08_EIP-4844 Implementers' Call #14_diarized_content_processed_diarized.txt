00:00:00.570 - 00:00:38.762, Speaker A: You okay? Hi, everyone. Call 14 now for four, four, four. Couple spec items on the agenda today and then probably makes sense to chat about Devnet four. And, yeah, how we, how we see things starting to shape for Devnet five. I know there's also been some updates on some more blob or call data. Sorry, large blob spam tests. So it probably makes sense to quickly cover those as well.
00:00:38.762 - 00:01:36.796, Speaker A: And then we can discuss anything else people want to bring up. But maybe to start Mophie, you had a pr you wanted to go over, which is basically the one which introduces Dineb in the CL specs and kind of adds four four four as part of that. We can't hear you, Mophie. If you're talking, you're unmuted. But does anyone else on the call have a. Oh, sorry, and I posted the wrong one in the chat. Is anyone else on the.
00:01:36.796 - 00:01:39.870, Speaker A: Okay, no, sorry. Actually that is the right one.
00:01:42.560 - 00:01:44.984, Speaker B: So this is like the beacon API.
00:01:45.032 - 00:01:45.292, Speaker A: Sorry.
00:01:45.346 - 00:02:23.690, Speaker B: Yeah, I think clients should already have this implemented because you would need to in order to participate on the devnet. For example. I think this is more about just getting the spec updated so that we can work on top of it with some other beacon API changes that are going to be necessary for block and blob decoupling. And so that would be the blob signing, individual blob signing, which I think generally we have to decide what that looks like.
00:02:28.370 - 00:02:29.440, Speaker C: Carl it.
00:02:33.010 - 00:02:33.760, Speaker A: Anyone?
00:02:37.250 - 00:02:43.346, Speaker B: I would say that the links pr requires more of a rubber stamp than anything.
00:02:43.448 - 00:02:44.466, Speaker A: Yeah. Okay.
00:02:44.648 - 00:03:20.350, Speaker B: But we don't yet have a pr to add block and blob signing. I think that would be the next to do thing. And that's something I can work on today. A general sort of structural question I would have with that is do we want sending like submitting signed blobs to the beacon node? Do we want that to be one blob at a time? I'm assuming. Yes, since we can gossip them. Like, we'll gossip them as soon as we get a single sign blob.
00:03:23.170 - 00:03:42.582, Speaker D: So I'm looking at the pr, I assume this is jflobside card. Does this have to do with validated client or. This is mostly for end users to retrieve the blobs icar? Sorry, I'm a little lost here.
00:03:42.716 - 00:04:09.614, Speaker B: Yeah, I think the existing pr to add a get blobs endpoint, that's the one that's more like for outside the validator flow blob. So like how we have two separate get block endpoints. So I don't think anything related to the validator flow is in a pr from what I've seen yet.
00:04:09.732 - 00:04:15.150, Speaker D: Okay, but we should add that to a PR, basically, because now they're decoupled.
00:04:16.290 - 00:04:25.700, Speaker B: Yeah, I can work on adding validator specific logic to that spec today over the next couple of days.
00:04:27.030 - 00:05:11.460, Speaker A: Yeah. Nice. Okay, so no objections to this pr, and then we can add a follow up one, both for the signatures and then for the validator behavior. Does that make sense? Cool. Okay, and then Mophie, you had a second one that you just closed, which is a duplicate of Roberto's very old pr last year. Sorry, Jimmy's pr from last year. Do you want to maybe give some background on that? Do you have a mic?
00:05:12.970 - 00:05:26.150, Speaker B: This is the one that. Terrence. Just one? Yeah, this is more the user facing one outside of flow.
00:05:26.310 - 00:05:27.034, Speaker A: Got it.
00:05:27.152 - 00:05:36.380, Speaker B: It'd be nice to come to some sort of consensus around this one, but it's not a blocker for us interoperating or anything.
00:05:37.810 - 00:06:11.560, Speaker D: I think prism has a version implemented. It may not be the version specified, but it's the version that blobs scanner is using. I think it was like a packet on project from Devcon. And then we actually launched the blob scanner back in Devnet four so people can see what the blobs look like. So yeah, it's definitely using some sort of API, but I'm not sure if this one. So yeah, we should probably finalize it so the clients can implement it.
00:06:15.610 - 00:06:25.770, Speaker A: Okay. Does it make sense to just keep iterating on this pr to finalize it? Or do we want something like to reopen a new one? I know it's.
00:06:29.410 - 00:06:36.830, Speaker D: Yeah, maybe I can pin it. I'm going to leave a comment right now just to follow up and see how to proceed for it.
00:06:36.980 - 00:07:04.018, Speaker A: Okay, sounds great. Okay. And then last one, free the blobs. So this is blob and block decoupling PR by just that he just put up. Yeah. Terrence, you had added this on the agenda, so want to give a bit of context here. Sure.
00:07:04.124 - 00:07:40.754, Speaker D: So back in the interrupt week, there's rock consensus to decouple, block and blog. So for optimization reasons, I think they have been brought up a few times last week already. So we have been iterating on the design doc. So now we're moving from the design doc to the PR itself. So this is more like the official way to get this merged, basically. So highly recommend everyone that's interested as implementing to review the PR. The sooner we can come to consensus, we can merge the PR.
00:07:40.754 - 00:07:59.980, Speaker D: The client can begin implementing it, then we can move to Devnet five. So this is probably the biggest blocker before. Ideally I think a client can probably two to three weeks to implement this. I think the sooner we can merge this the better. So yeah, please review.
00:08:01.550 - 00:08:05.180, Speaker A: Okay. And then yeah, Danny says he'll review this today.
00:08:07.630 - 00:08:09.118, Speaker E: Can I ask you a question about this?
00:08:09.204 - 00:08:09.742, Speaker A: Yes, please.
00:08:09.796 - 00:08:34.550, Speaker E: I just want to do a final gut check that this is something that must go in this first release of 4844. And I guess the question I'm thinking through is what is the incremental cost of us saying this is something we're going to land in the next hard fork when we increase the number of bobs and we have kind of more justification for needing these efficiencies.
00:08:42.620 - 00:09:59.810, Speaker B: Is the question like does this solution, I guess, do the short term costs lead us to a quicker, I guess step two as far as blog decoupling? And I think the answer to that is yes. I think it would be a lot easier to do this decoupling stuff now before we have production implementations of all this stuff than it would be to fully productionize the couple design and then move to something like this when we want to increase the number of blobs we're handling. Generally, my two cent is like the decoupled design is more flexible too, which helps us with the fact that it's really difficult to gather information about what this type of stuff looks like on Mainet. I think that's one of the things we're having trouble with, is it's really tough to model exactly what such a dynamic and diverse network looks like when we're making radical changes to it.
00:10:01.940 - 00:10:44.208, Speaker D: And Jesse. So I think the answer is at the current blob targets, the sizes we could probably ship without this. But given the complexities of once you ship something, changing it isn't just changing it, it's like very carefully manipulating and modifying it through a fork boundary that if that's where we're going anyway, there's probably a major complexity. Saving in doing it right now I think is the core of the argument. But we don't have incredibly good network data at this point. So the must or should here is not even that clear.
00:10:44.374 - 00:11:25.100, Speaker E: Yeah, I think that makes sense. All that makes sense to me. I think the thing that I'm just wary of is between this and the SSV migration, it seems like we're adding in a lot of kind of like now is a good time to do a significant engineering investment in the future scope, which I think is totally reasonable. And I just want to be given our other goals around kind of like urgency and getting this out to scale. I want to just be like a pressure test on do we need to do it all right now, are there ways we can chunk it up into multiple releases that can make sure we're balancing kind of the speed with the long term kind of foundation investments?
00:11:27.920 - 00:11:47.460, Speaker B: One thing I'll say about that is the blob decoupling is a lot of consensus layer work and the SSC migration is a lot of execution layer work. So they're not like directly stacking on each other. It generally seems like it'd be better to make these changes sooner.
00:11:48.040 - 00:12:06.200, Speaker D: Yeah, I think it's also worth mentioning that El plants are also working on the blobs TS menpool, so they will take a little bit time there. So we can also use this time for decoupling.
00:12:14.480 - 00:12:22.080, Speaker A: And I guess, I don't know. Does anyone on a client team feel strongly that we shouldn't decouple?
00:12:27.670 - 00:12:52.220, Speaker F: Yeah, I mean, I think this is mainly a consensus client team issue. Consensus client teams are comfortable with it. It sounds like we can land it, but I think some of the SSC related changes will percolate up to the consensus layer a little bit. It does sort of change the execution payload format, for example. But admittedly it shouldn't be a giant burden on the consensus layer. But it is looking pretty messy on the execution layer, I have to say.
00:12:56.190 - 00:13:20.066, Speaker A: Yeah. Dan crab. Yeah, I still want to highlight that. I still think the decoupling is. I don't know, we're discussing this. It is a complication and so far we don't even have any data that says it actually improves things on the networking layer. It's just like some people think it is.
00:13:20.066 - 00:14:25.990, Speaker A: And from what I hear, the first simulation actually don't agree with this intuition. Not that I trust any of them just yet, but yeah, I know. Talking about the simulations, we said we would kind of discuss them on the CL call this week and kind of use that as a way to inform this decision in terms of the next two days. Beyond that, does it make sense to just review the decoupling spec, make sure that assuming this is the way we would want to go, people are happy with the pr, and then we can discuss it on Thursday's call, hopefully with a bit more data and see from there if we think it's worth going there. It does seem like most of the client teams want to go that way, but yeah, I don't think we need to make this decision today, but it would be good for people to come to Thursday's call at least like fully understanding the spec that's being proposed.
00:14:28.490 - 00:14:31.160, Speaker D: Yes, that's how I'm thinking about it.
00:14:32.410 - 00:15:07.080, Speaker G: Okay, can I give one more question? So I see in the comments that there was discussion on this aggregate proof, I mean in HackmD, but I don't see that there was decision what to do with aggregate, because in this new proposal there is no more place to put segregate. Was there a discussion about that?
00:15:10.410 - 00:15:21.500, Speaker A: Sorry. Is this how we verify the validity of each blob? And if we verify blobs individually versus like an aggregate, is that what you're asking about?
00:15:22.510 - 00:16:32.750, Speaker G: Well, let me rephrase. I'm talking about this pull request that is called free the blobs 3244. And it suggests to remove blobs sidecar container that previously contained KZG aggregated proof. And in the new proposal, there is no more this container means there is no more the aggregate. And left the comment they suggested to basically drop this KZG aggregated proof entirely and just stick to individual proving of each blob. And in hackmd notes, I see that there was actual discussion about that, but seems there is no decision. One question is, is there particular solution for that or was discussed? And another thing that it seems that display request not.
00:16:32.750 - 00:16:39.010, Speaker G: I mean, it will take a bit of time to fully complete it. There are some missing parts.
00:16:40.870 - 00:17:47.990, Speaker C: Okay, so we had a bit of a discussion on this. I think the current not finalized plan is to switch to individual proofs in general and also provide a function that does an aggregated verification, but without using an aggregate proof. So we don't do aggregate proofs anymore, but we do provide a method that gives faster verification if you have a bunch of blobs at hand. And the reason it's not final yet is because we're kind of sort of like benchmarking this technique. There is also pr opened by Duncrad on the consensus spec. So we want to benchmark it, see if the verification costs are reasonable. And if they are, then I think that's indeed the plan to move forward with individual proofs for each blob so that they travel nicely and isolated.
00:17:50.030 - 00:18:02.362, Speaker G: Okay, I'll need to check this. There is a separate pull request in the consensus spec, correct for this?
00:18:02.416 - 00:18:03.018, Speaker C: Yes.
00:18:03.184 - 00:18:03.900, Speaker G: Okay.
00:18:05.250 - 00:18:24.050, Speaker C: It kind of removes the aggregated KzG proof thing, and it changes the verification function to work with individual blobs. And it also exposes a multiverify function that amortizes the verification of these individual proofs.
00:18:25.530 - 00:18:41.020, Speaker G: Okay, I'll check it, because I was thinking about pedalizing this aggregated proof function, and it wasn't very friendly for that. But let's see. I'll check this. Another request. Thanks.
00:18:42.990 - 00:18:53.280, Speaker C: It's still slightly rough, like it's not yet ready for merge. It's just a proof of concept, I would call it, but it can give you an idea of how the verification works.
00:18:54.050 - 00:18:55.920, Speaker G: Okay, thanks.
00:19:02.200 - 00:19:16.244, Speaker A: There's a question in the chat, George, about does this mean any changes to the El side proof? Is that about the transaction proof?
00:19:16.292 - 00:19:16.792, Speaker C: Yeah.
00:19:16.926 - 00:19:30.510, Speaker A: Yes, it does. So the transaction would also change to individual proofs in that case. So instead of having one proof for all the blobs, you would have one per blob. But there's a function to verify all these proofs together.
00:19:33.040 - 00:19:54.340, Speaker C: Murphy, you would think that we don't need to do this, and you're right that we don't need. But I guess the idea is that we don't want to have two different approaches. One is on the El we do aggregated proof, and on the Cl, we do individual proof. So for the sake of keeping just one code path for everything, I think it makes sense to unify both approaches.
00:20:07.900 - 00:20:14.570, Speaker A: Okay. Anything else on this front?
00:20:16.060 - 00:20:33.170, Speaker C: I think from our side, we are going to do some benchmarking this week. And I think by next week we should have more data on the verification costs and whether we can go forward with this. I think we can, but we just want to make sure.
00:20:35.060 - 00:20:54.300, Speaker A: Sounds good. Okay. Any others? Oh, sorry. Mophie has a comment. Mophie, do you have a mic or still? No.
00:20:54.910 - 00:21:26.040, Speaker C: I see what you mean. Murphy, you're saying that we were using the aggregated proof on the El site to do express verification on the mempool. And with this new approach, we will not have this avenue. Yeah, I guess that's again part of the. We need to benchmark it and see if we can have it fast enough to warrant this change. That's a good point.
00:21:31.570 - 00:21:48.770, Speaker A: Actually. I thought we only want single block transactions on the mempool. And that's one comment. And second comment is that I think the difference between the two is very minor. That will be like hundreds of microseconds.
00:21:54.830 - 00:22:03.178, Speaker D: But we still need to pass blobs from yield site to seal side and verify it somehow before.
00:22:03.344 - 00:22:10.294, Speaker A: Right, sorry, proofs from El site to Cl site.
00:22:10.352 - 00:22:20.994, Speaker D: Why blobs? I mean, pass blobs from yield side to Cl site, right, when we're constructing blocks or.
00:22:21.192 - 00:22:25.620, Speaker A: When would you ever do that? Only if you're building a block. Right?
00:22:26.310 - 00:22:51.900, Speaker D: Yeah. Share site requires it. And the request with get blobs bundle, right. And this bundle should not contain any incorrect blobs. And we verify them on the yield site using commitments and proofs, right?
00:22:52.670 - 00:22:53.420, Speaker A: Yes.
00:22:55.790 - 00:23:07.022, Speaker D: What's your question? It seems like we are changing schema not to have any proof like that. Am I correct?
00:23:07.076 - 00:23:14.370, Speaker A: No, we have proofs. It's just one proof per blob, rather than one proof for all the blobs, but they can still be verified efficiently.
00:23:18.710 - 00:23:19.460, Speaker D: Okay.
00:23:22.700 - 00:23:37.230, Speaker C: But what is this one blob per transaction thing? I didn't know about this change. I thought we have multiple blobs per transaction. Blobs per transaction in the mempool. In the mempool, yes.
00:23:40.260 - 00:24:19.020, Speaker A: Well, I mean, my suggestion is to make the mempool as simple as possible, and to only support exactly one blob per transaction in the mempool. And all other things have to go through, get into blobs in some blocks, into some other way. Yeah. What is the argument to have multiple blobs, especially on this release, to have multiple blobs per transaction, because there's only two max per block anyways. Well, in the blocks, we support them just for the mem pool, I think. Right. We want to minimize the DoS vectors and everything.
00:24:19.020 - 00:24:57.370, Speaker A: And one way to do that is to not allow multiple blobs, for example. Otherwise, I can do things like cancel one transaction with four blobs, with a transaction that only has one blob and only pay for the one blob, stuff like that. Yeah, I agree with you. I guess I'm trying to understand what's the reason why you would want multi blob transactions in the mempool, given that the most you could ever get in one block is like two? Obviously, there might be some small gap. I am hoping that we'll have more than two, but the Max is not two. Right. Anyway.
00:24:57.370 - 00:25:03.930, Speaker A: Okay, got it. And also, we want to change that. Right. We probably want to increase that.
00:25:04.780 - 00:25:20.690, Speaker C: But I don't understand what you mean. You mean that there can be transactions on the blockchain with more than one blobs, but on the mempool they are decoupled. Or you're saying that we also decrease the Max blobs per transaction to one?
00:25:21.460 - 00:25:28.610, Speaker A: No, only on the mempool. You cannot post transactions with more than one. That's all. No consensus change.
00:25:30.420 - 00:25:39.350, Speaker C: This thing is on the Al specs. Like, that's how the AL is implemented right now. Or this is a change that comes with the free. The blobs thing.
00:25:43.080 - 00:25:46.824, Speaker A: Has nothing to do with a free the blobs no, this.
00:25:46.862 - 00:25:56.750, Speaker D: Has to do with this transaction pool design and whether that is a reasonable UX restriction to achieve antidos in a simple way.
00:25:58.560 - 00:26:05.710, Speaker F: If I understand you right, you're saying builders can still build blocks with more than one blob, but for the mempool purpose, we don't allow it.
00:26:13.120 - 00:26:19.240, Speaker C: Okay, let me ask the stupid question. How does this transaction reach the block if they don't go through the mempool?
00:26:19.420 - 00:26:24.150, Speaker A: Through a private mempool, right? Like something like mev boost, basically.
00:26:26.120 - 00:26:26.900, Speaker C: I see.
00:26:26.970 - 00:26:31.860, Speaker A: Okay, Lucas, I have a question.
00:26:32.010 - 00:26:42.680, Speaker H: Because if we are having different roles, consensus roles and different mempool roles, aren't we discouraging people from using mempool like in the first place? Is that a.
00:26:42.830 - 00:27:01.650, Speaker A: Well, yeah, we already do this, right? Like we already have restrictions on the max transaction size. Will gossip. So this is basically the same thing, right? So the short answer is yes, but it's something we've already done.
00:27:02.900 - 00:27:18.790, Speaker D: There are implications here, though. I mean, if you do have that as a restriction on the mem pool, then at a certain point, given certain demand and usage of blobs, local blocks will never be built because they're just missing an entire dimensionality of the sea market.
00:27:20.360 - 00:27:52.450, Speaker H: Exactly. So you're like pushing people more and more towards mev and builders and stuff, which I would argue that there's some healthiness in the network. If building local blocks is at least comparable to that and can compete in some way, it will lose most of the time, but at least it can compete. And here it can compete even less and less with time.
00:27:53.300 - 00:28:21.610, Speaker A: And I guess then the question is, how big of a use case is it to have multiple blobs per single transactions? Right? Like, if that's the default use case, for whatever reason, then most blob transactions don't make it through the mempool. If the default use case is one blob, one transaction, then most of the blob transactions would go through the public mempool. So I don't know if anyone has an intuition for like.
00:28:24.570 - 00:28:41.530, Speaker D: Yeah, that's very unclear if that's going to be the equilibrium. It just depends on demand across different roll ups, how they choose to use it, how things are being bundled. If one roll up dominates the activity, then you would not see one blob transaction.
00:28:42.850 - 00:28:52.610, Speaker F: If it dos is the worry, can't we also just introduce a replacement rule that suggests you need to pay at least as much data gas as you did for the replaced transaction?
00:28:57.270 - 00:29:10.566, Speaker H: So that was Peter idea to allow replacement of blob transactions with 100% gas increase. Right. Fees increase instead of standard like 10%.
00:29:10.748 - 00:29:20.330, Speaker F: Yeah, it's related. But I think we also need to capture the case where you replace, for example, a four blob transaction with a one blob transaction or a zero blob transaction.
00:29:22.750 - 00:29:27.900, Speaker H: So I think Pampal wouldn't allow zero blob transactions. That was the point.
00:29:29.630 - 00:29:44.900, Speaker A: But you could basically replace your blob transaction with another transaction type from the same account with the same nons. It's not a zero blob transaction, but it is.
00:29:46.310 - 00:29:51.060, Speaker H: Well, what Peter was suggesting is like, do not allow.
00:29:53.590 - 00:30:05.830, Speaker F: I guess. I guess that's basically what I'm saying. Let's just not allow replacement with a transaction that dramatically kind of reduces. It seems like this restriction of only allowing one blob is a little overkill.
00:30:07.630 - 00:30:23.134, Speaker A: Yeah. Peter was proposing to allow such replacement only if transaction is exactly the same, only the price is different. So it wouldn't be possible to change anything inside this transaction except of price.
00:30:23.332 - 00:30:51.640, Speaker H: So my point would be because I'm not sure if there is anyone outside of nevermind. Okay, I see people from Bezu, but also people from guests should be involved in this conversation. I'm not sure if anyone is here working on transaction pool. So yeah, it's kind of a bit pointless to. To have decided something without their.
00:30:52.010 - 00:31:17.760, Speaker A: Yeah. And yeah. Trying to bubble back. Guess the reason we sort of ended up in this rabbit hole is Mophie's question about the aggregated proofs. And if the performance of always assuming there's Max, one blob per transaction is sufficient. Is that right?
00:31:22.550 - 00:31:45.720, Speaker C: We originally introduced the aggregated proof technique just for the mempool to be faster. So if we're switching to one blob per transaction on the mempool, which I don't know if it's a good idea or bad, it makes even more sense to switch to individual KZG proofs because we don't need the aggregated one for the mempool anyway.
00:31:47.210 - 00:32:07.730, Speaker A: Okay. Yes. Okay, so I'll follow up with guess about this offline and see if we can either discuss this on next week's call here, or either all core devs next week.
00:32:09.000 - 00:32:19.210, Speaker C: Sorry, just to understand this whole discussion about the one blob per transaction mempool thing, is there a pr or something? Or is this something that I don't know.
00:32:21.500 - 00:32:54.280, Speaker A: There's Peter's transaction pool doc from Austria. But there is no spec for the transaction pool. Right? It's not something that's under consensus. So there's not actually a spec of it anywhere. And different clients, obviously this is the other thing is different clients can choose to do different things. Obviously for something like this it'd be good to have overall design, but yeah, we don't have a spec for the transaction pool.
00:32:54.460 - 00:33:04.500, Speaker H: Lucas, if clients will have vastly incompatible transaction pool rules, it won't make sense because the transactions won't be propagated properly.
00:33:09.100 - 00:33:52.820, Speaker A: I guess the EIP itself is something. We could add some stuff there, but we sort of had this problem as well. I guess 1559 was a bit different. It was about better sorting of transactions to build a block, and not as much about gossip. But that was another case where we sort of introduced another degree or like dimension in the transaction pool and sort of proposed a way forward. But we can't force clients to do yeah, it's not part of the consensus rules whether or not clients follow that. But yes, it does impact then how things get propagated on the network.
00:33:52.820 - 00:34:17.780, Speaker A: Okay, I'll try and follow up with geth on that. And I think just generally the transaction pool design is something we should kind of keep chatting about in the coming weeks. Anything else on that point.
00:34:23.970 - 00:35:50.380, Speaker G: If not just final? Meaning most of these suggestions, including the Duncrat pull request that he just mentioned, the one thing that I just wanted to notice that in many cases, especially in this case where we have now blobs per block that is maximum at four, and this number is actually smaller than or equal than most of the modern cpus, the number of the cores in the modern cpus. So in many cases it would be just easier to spin individual verification checks of individual blobs on different cores. And that will be faster than any other trick that involves various combinations with challenges and so on. The general idea seems that it's never to do parallel thing at the spec level, but I'm not sure. Maybe it would be better to consider that maybe somebody has comment on that's. Or I didn't express myself clearly enough.
00:35:52.030 - 00:36:05.520, Speaker A: I'm not sure personally how much those sort of optimizations are part of the spec or not.
00:36:08.230 - 00:36:16.210, Speaker G: Okay, then let's leave. I'll put the comment in the pr and maybe somebody will have something to comment.
00:36:17.110 - 00:36:49.040, Speaker C: I think it's like multithreading, and doing the verification on many cores is certainly a huge speed up that libraries should do, and we do see the speed up occurring. So I don't think it should be part of the spec, but I do think that if the libraries can multithread something, they should now how this multithreading should be arranged on the single proof thing. That's something a bit more complicated. I agree.
00:36:49.890 - 00:37:37.230, Speaker G: No, I mean, the note here is that the majority of these suggestions that involves multiple things, like multiple blobs for instance, they do not consider that it's possible to do things fast in parallel. And the algorithms proposed are not parallelizable by design or something like that because they are optimized to be fast for single thread performance. I mean, if somebody is going to optimize it, then they will do completely different algorithm comparing to the suggested one. Because the suggested one is optimized for single thread.
00:37:41.340 - 00:37:52.204, Speaker C: Maybe. I mean, I don't know. Msms can also be multithreaded. I don't know. I would say we take this offline, how we can speed it up.
00:37:52.242 - 00:37:53.870, Speaker G: Okay, thanks.
00:37:58.560 - 00:38:26.926, Speaker A: Okay, anything else on the specs? Okay, I guess it probably makes sense to go over devnet four and how clients are tracking. Yeah. Anyone have an update or anything they wanted to bring up there can give.
00:38:26.948 - 00:39:08.350, Speaker I: An update for Bezu? So we are now following Devnet fall chain correctly locally. We are also able to build block without any blob transaction because we still have an issue validating blob transaction that we are working on. And this should be the last issue before we could join Devnet as a standard client supported client.
00:39:11.490 - 00:39:12.560, Speaker A: Thank you.
00:39:13.970 - 00:40:12.526, Speaker B: So for Lighthouse, we just merged a fix for serving finalized blocks that have blob transactions. So previously you couldn't sync from a lighthouse node by range. So if we're able to deploy that as well as there is a geth change needed. 1 second. If we're able to deploy the latest lighthouse EIP 4844 branch plus geth with that pr I just linked merged. Then other nodes should hopefully be able to sync from Lighthouse on the devnet. And then other than that, diva's been working on an outbound rate limiter that's configurable in Lighthouse.
00:40:12.526 - 00:40:17.460, Speaker B: So I'm hoping that will help us with any rate limiting issues we've been running into.
00:40:23.820 - 00:40:24.616, Speaker A: Thank you.
00:40:24.718 - 00:41:02.230, Speaker B: Update from Techu yeah, we're working this week to move from the 4844 branch and everything back to master as we used to do. And we also have some sync issues that we discovered, but not yet in the 4844 branch. We'll have those fixed soon on Master, and then we will then ask for a refresh of our nodes building from master this time.
00:41:05.960 - 00:41:25.160, Speaker D: Update from prison. So we actually haven't been paying so much attention to devnet four. We're actually just full speed ahead of Devnet five. So we are working on decoupling block and blob. We're reviewing the spec and beginning our initial implementation.
00:41:30.640 - 00:41:32.270, Speaker A: Sweet. Anyone else?
00:41:34.080 - 00:41:54.470, Speaker D: Never mind. That's quite complete. Devnet for implementation and just synchronized with Lighthouse, which was the last sealed site client on the list we were trying to synchronize. Is that.
00:42:04.640 - 00:42:57.242, Speaker A: Anything else on Devnet four? Okay. And yeah, obviously I think for Devnet five we'll want to just wrap up these spec conversations we had earlier. Last thing I had on the agenda. So I know there's been a bunch of large blocks sent over Gordy. I don't think Dan is on the call. I don't know if anyone else who's looked at the numbers is definitely been more movement there and we can try and provide an update on the next one of these calls. Yeah.
00:42:57.242 - 00:43:01.280, Speaker A: Anything else that anyone wanted to bring up?
00:43:04.280 - 00:43:12.512, Speaker F: We could discuss the SSE execution layer stuff. Eton is here. He's been working on proposal.
00:43:12.656 - 00:43:15.080, Speaker A: Yeah. Any updates?
00:43:16.380 - 00:44:10.330, Speaker I: Yeah, hello. So it's not entirely 4844 specific in general, but I'm still working on this EIP 6404 to transition the transaction route receives route and withdrawals route in the El block header to SSC. So one issue like for 4844 is that on the network it requires these special casings about how the mempool works, about how the transaction propagates. And question came up whether it is possible to somehow support zero blob type five transactions, so that going forward.
00:44:12.540 - 00:44:12.904, Speaker A: We.
00:44:12.942 - 00:45:05.880, Speaker I: Don'T have to maintain both the with blob and no blob transaction every time that a new transaction feature is added. So, for example, if we right now want a SSC transaction without blobs, we would have to define a transaction type four, for example. And then four would be no blobs and five would be with blobs. So let's say we add a new magic feature to the transactions and we want it to be available in both kinds. Then we need type six for no blob with magic feature and type seven for with blob with magic feature and essentially doubling all the transaction types going forward. So yeah, I was wondering whether it's possible to somehow support CRO blobs without introducing transaction types.
00:45:17.710 - 00:45:33.082, Speaker A: Does anyone have thoughts on this? It. Okay, if not, I guess we can continue this conversation. I think it's in types transactions. Right on the discord.
00:45:33.226 - 00:45:36.250, Speaker I: Yeah, it's in type transactions in execution.
00:45:36.330 - 00:45:38.894, Speaker D: R and D. Okay, yeah.
00:45:38.932 - 00:45:41.840, Speaker A: So let's continue chatting there about it then.
00:45:42.450 - 00:45:52.760, Speaker F: Yeah, the reason I wanted to discuss is I think it does kind of impact four four if we're going to be introducing a new transaction type. It seems like the time to get this correct is right now.
00:45:54.730 - 00:45:57.960, Speaker A: Before like the next Devnet, right? Is that what you say?
00:45:58.570 - 00:46:01.430, Speaker F: Ideally, but certainly before the next fork, right?
00:46:01.580 - 00:46:02.280, Speaker A: Yes.
00:46:07.550 - 00:46:13.850, Speaker F: It's a little messier than I had hoped, I guess. So having more eyes on this now I think would be helpful.
00:46:14.830 - 00:46:35.860, Speaker I: Yeah. Overall, Vitalik also mentioned that he would like to discuss the union versus normalized transaction encoding for SSE. So I would really appreciate if someone could schedule a call for just SSC transactions where all of these topics could be.
00:46:37.430 - 00:47:08.320, Speaker A: Yeah, let me try and do that. So there's an EOF call tomorrow at 15 UTC. So maybe next week at that time because it's every two weeks, maybe that's a good time to schedule it. So it's like basically a week from now or a bit more. So it gives people time to kind of read up on it and it'll be before the next alcohol dev. So whatever comes out of that call, we can kind of bring up on all core devs as well. Does that make sense?
00:47:09.010 - 00:47:15.280, Speaker I: Makes sense for me. I mean, for me, anytime works. This is priority for me right now.
00:47:15.730 - 00:47:26.530, Speaker A: Okay. And yes, I'll share this with the get folks. Okay. So I'll organize it for next Wednesday, 15 UTC.
00:47:27.110 - 00:47:28.820, Speaker I: Okay, thanks a lot.
00:47:29.430 - 00:47:37.800, Speaker A: Cool. Anything else people want to discuss.
00:47:42.650 - 00:47:43.494, Speaker H: Anyway?
00:47:43.692 - 00:48:07.134, Speaker D: If we will have two or one SSD transactions types, new types. Right. We would most likely try to use SSD root hashes for them, probably. And we could try to start this transition before the net five, making it.
00:48:07.252 - 00:48:09.090, Speaker G: A part of its scope.
00:48:10.470 - 00:48:11.940, Speaker D: What do you think, guys?
00:48:27.860 - 00:48:33.430, Speaker A: Um, I'm sorry, I'm not sure I quite understood the question.
00:48:38.040 - 00:49:03.740, Speaker D: Let me rephrase. The idea is to use SSZ root hash for transaction hashing instead of the current approach because most likely we will do it in the future and we have time to do it now. Bringing SSD mercurial edition dependency to our projects.
00:49:09.830 - 00:49:17.080, Speaker A: Right now we're using the RLP route for hashing and you're saying we should switch to the SSZ one because we're going to switch anyways, right?
00:49:17.770 - 00:49:18.520, Speaker D: Yeah.
00:49:23.210 - 00:49:32.860, Speaker A: I personally would like us to have the broader SSZ conversation next week before we make a decision on that. Is there a reason we have to make the decision quicker than that?
00:49:39.500 - 00:49:40.664, Speaker D: No, probably.
00:49:40.862 - 00:50:01.170, Speaker A: Okay. I think it probably makes sense to then just, okay, let's have the SSD call next Wednesday and get back to that. Yeah, I wouldn't want us to make a decision and then have people work on it and realize we want to go the other way because of something that came out on the SSD call.
00:50:05.380 - 00:50:07.952, Speaker H: Generally there is a Vitalik API about this.
00:50:08.006 - 00:50:08.610, Speaker A: Right.
00:50:11.400 - 00:50:18.580, Speaker F: Proposals. There's one from Vitalik and the one that Eton is working on, which is a unified transaction version.
00:50:19.480 - 00:50:49.730, Speaker I: Yeah, I can put the link. It's in six four four. If you go to the Ethereum magicians link, there is a link to Vitalik's post who made a proposal based on unions. And in six four four, I tried the same with the normalized transactions. So we have both strategies and we still need to decide which one of those we will go forward.
00:50:57.380 - 00:51:38.540, Speaker A: Yeah. Okay. So, yeah, let's discuss all of that next week. I'll also check with Vitalik to see if he can make it at that time so that we basically have everyone who has a strong opinion, ideally on the call. Anything else? Okay, well, thanks everyone. Talk to you all Thursday on the Cl call. Let's look at the blob block coupling thing before then, and I'll post an update on the SSD call as soon as.
00:51:38.540 - 00:51:46.020, Speaker A: I just want to confirm the time with a couple of people. Yeah, talk to you soon.
00:51:48.310 - 00:51:49.380, Speaker B: Thanks, everyone.
00:51:49.910 - 00:51:51.250, Speaker H: Thank you. Bye.
00:51:51.670 - 00:51:52.160, Speaker A: Thank you.
