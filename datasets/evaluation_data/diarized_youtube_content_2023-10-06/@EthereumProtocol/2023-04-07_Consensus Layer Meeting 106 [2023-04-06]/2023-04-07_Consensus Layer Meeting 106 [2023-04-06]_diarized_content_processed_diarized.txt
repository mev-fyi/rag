00:03:14.990 - 00:03:48.180, Speaker A: Is issue seven five two on PM repo since their call number one six. Relatively light scheduled agenda. Obviously if you have points to bring up in different categories, by all means. We'll get started off with propella, which I believe is just a little bit over one week from now. What's the precise date? April twelveth, less than a week. 22 27 UTC. I forget what the epoch number is.
00:03:48.180 - 00:04:02.600, Speaker A: Cool. If you are listening and you run infrastructure or nodes or validators, now's the time upgrade. Perry, we had main net shadow fork three this morning, right? Anything to report here?
00:04:04.010 - 00:04:40.420, Speaker B: Yeah, it's been looking really great so far. We had two builders on the network this time. I haven't heard back from one, but the other one is producing blocks. We are noticing a couple of unable to verify signatures, but still looking into why. Besides that, healthy block production. All client combinations made it through and we use mainet releases for everything. We also tried a couple of the standard procedures like deposits were made, a couple of withdrawals were done with zero x zero credentials, a couple with zero x one.
00:04:40.420 - 00:04:46.440, Speaker B: And. Yeah, just playing around with that sequence and everything looks good so far.
00:04:47.450 - 00:04:50.550, Speaker A: The signatures that you can't verify, those are blocks.
00:04:52.810 - 00:04:57.180, Speaker B: Those are signatures I think.
00:04:57.790 - 00:05:39.640, Speaker A: Okay, interesting. Cool. Anything else on this? Any questions for Perry? Okay, thank you. And anything else on capella before next week? Great, good work everyone. Excited to see it go through. Dineb. There are no scheduled discussion points for Dineb this week.
00:05:43.450 - 00:06:19.490, Speaker C: I have something that I wanted to put on the agenda but forgot. I would like to modify the blob transaction. Transaction type from zero x five to zero x three. I think it was chosen in that way because there were some different transaction types on some layer twos or something. I don't think we should create the expectation that Mainet will care about transaction types on other networks.
00:06:19.990 - 00:06:20.740, Speaker A: And.
00:06:24.410 - 00:06:28.040, Speaker C: I would like to modify the transaction type.
00:06:31.050 - 00:06:47.770, Speaker A: Yeah, on this I think it would be good to have maybe a high bit set for non mainnet so that there was just a different space that people could use as a standard. But. Yeah, I see the point, Osgar.
00:06:48.670 - 00:07:02.242, Speaker D: Yeah, I mean, I was basically going to echo a similar sentiment. I feel like this is something where we should probably sooner than later start some sort of standardization procedure. I mean, ideally there should also be like non conflicting standards between layer twos, but maybe we don't care so much.
00:07:02.296 - 00:07:05.138, Speaker A: But yeah, it does seem like a.
00:07:05.144 - 00:07:15.460, Speaker D: Bad precedent to set to just skip versions on main net because we can't really keep doing that. But still, we should look into standardizing this.
00:07:16.950 - 00:07:47.440, Speaker A: Yeah, there is a little bit of precedent here, at least on the consensus layer. I just shared a link to domain types from phase zero. There is domain application mask which essentially just says if you're going to use the application layer and you're going to utilize the same signing scheme, set this bit and you won't run into main net. Does that surface as an EAP erC? I'm not exactly sure, but I think it'd be nice.
00:07:52.210 - 00:07:59.474, Speaker D: Just a sanity check. Do we have specific feedback for any either layer twos or wallets on whether this would cause massive problems or anything?
00:07:59.672 - 00:08:01.220, Speaker A: In this specific case.
00:08:05.690 - 00:08:19.980, Speaker C: I have no feedback, but I also don't know why this was chosen this way. Exactly why this was chosen this way. I think there was some code in Aragon, so maybe the Aragon team can commit on it.
00:08:23.870 - 00:09:23.850, Speaker A: Yeah, just to call out to any chain similar to the chain ids that we are pretty much that are pretty much standardized, we should do a better effort at standardizing transaction types and domains and whatnot. I think optimism uses one transaction type soon and so does arbitrum. Or at least I think they use up to seven transaction types. I'll have to check. And yes, also pre compulse, right? Is this something Marius does have a EIP PR as well?
00:09:26.300 - 00:09:31.400, Speaker C: Yes, it has a spec PR and an EPR.
00:09:33.420 - 00:10:10.330, Speaker A: Yeah, I mean I agree with not trying to deal with conflicts because we're not going to be able to resolve it. And that setting the precedent here that we just do sequential and main net makes sense. Maybe it's merge these prs, shout into the void and see if it's going to cause a deeper issue than we expect. Rather than. I think merging the PR sends a stronger signal than trying to just knock on some doors and see what's up. Especially considering we still have some lead time on this.
00:10:15.910 - 00:10:31.500, Speaker C: Yeah, I think the only thing about merging these chairs is the question like what's the status of the test nets? Is it fine to just merge it right now or should we wait for another round of, I don't know, changes?
00:10:34.270 - 00:11:08.680, Speaker A: Yeah, I mean on the consensus layer side we'd have to do a release and pretty much the testnet would target either the current release which has zero X five, or the next release which could have zero X three. As for the test nets on the Eip el side, I think it's more targeting a commit or targeting head so we just have to send the signal. But it seems easy enough to change unless except for maybe Aragon or others that have other types of transactions that would conflict and we don't know the complexity there.
00:11:16.430 - 00:11:32.990, Speaker C: So the way I understood it from Roberto is that even Aragon removed this code, the removed the transaction type three that they had in their code base. So maybe it's not even an issue anymore.
00:11:52.870 - 00:12:44.284, Speaker A: Okay, so there's potentially a little bit of coordination effort to circle back with the Testnet schedule and decide if these could go into next or the following. But seems like general agreement to plant a flag here and merge these. Anything else on this one? Thank you Maris. Other dynam related discussions. I apologize, I was traveling on Monday. I am curious if. I'm curious.
00:12:44.284 - 00:12:59.316, Speaker A: Just like general state of blob decoupling networking work, if that's becoming relatively feature complete or if that's still like a very active R D for teams, I.
00:12:59.338 - 00:13:00.656, Speaker E: Can give a quick update.
00:13:00.768 - 00:13:01.204, Speaker B: Go ahead.
00:13:01.242 - 00:13:10.714, Speaker A: Sorry Terrence. Okay, sorry.
00:13:10.912 - 00:13:41.942, Speaker E: And yeah, I can give a quick update on the PSM side of things. So we were able to implement all the gossip features. We have been working on blobs by range and blobs by root and we have successfully tested that yesterday. So that seems to be working. The only thing that we don't have is backfilling, but I don't think that's like a testnet blocker. That's going to take us a few weeks, if not months to implement backfilling. So I think prism can probably pair with any client right now to start testing that.
00:13:41.942 - 00:13:43.480, Speaker E: So that's where we are.
00:13:47.050 - 00:14:43.290, Speaker A: In Lighthouse right now we're working on the single blob lookups and integrating that with our preprocessing functionality and generally in block processing. I'd say that's the big remaining feature for us. In parallel, we're trying to work on just a testable version of Lighthouse that could run on a local testnet. So we'd start actually testing some stuff that's generally where we're at. Cool. Was the complexity generally as expected, or were there unknowns and continued worries here? I would say as expected so far or it's like a bit of a pain, but it's nothing unsolvable. Cool.
00:14:44.940 - 00:15:23.030, Speaker F: Loadstar is sort of ready to interrupt with any other client and ethereum js on the Yale side is also sort of ready. And what I'm currently changing in Lodestar is the signing communication between beacon and the validator to change into block contents from the earlier independent blob and block signing. So that should anyway be ready by end of this week. But anyway, it's not really a blocker to participate in Devnet since it's internal between Lodestar and validator as of now.
00:15:27.920 - 00:16:20.510, Speaker G: Update from Tegu we are still in developing phase. We are concentrating mostly on the API side for signing plus the logic for caching and looking up Misad gossip to reconstruct bundles of blob, sitecar and blocks while we are following the head. So all the complexity of handling caching and pools of objects and then complete the block import. Yeah, we have still work to do so we are not ready for any interrupt at the moment.
00:16:28.060 - 00:18:35.280, Speaker A: Thanks. Regarding Dune, any other items to discuss? Easy enough actually, there's something on my mind on capella with the historic roots revamp we essentially have historic roots list that starts at capella. We don't have it full and we were talking about once that becomes fixed. Essentially once you finalize the chapella update, you could in a future fork have a fixed list that you can insert and essentially utilize the new historic roots mechanism fully from Genesis that was mentioned as a potential future item. That does seem like a pretty low hanging fruit. Is there an appetite for specifying that and getting that into Daneb? It's a bit of a question for Yasik as well, because I think Q is eager to do so, but is there any gauge of sentiment on that? Essentially like left is a to do from the last upgrade and wondering we should tackle that to do here. Okay, I'll circle back with Yasik and maybe just put it up as a feature spec for discussion and then we can, once it's complete, decide where to insert it.
00:18:35.280 - 00:19:59.900, Speaker A: Anything else on Deneb? As we are with some to do lists for Daneb, I am recalling something that about slash validators and gad speaking proposal and probably some other miscalanos, things that at some point in time we have decided probably to put into dinner. I'm just wondering, are we still going to do that, at least for this get beacon proposer thing was getbeacon proposer essentially filters out validators in the selection process so that you end up with more full selection of blocks. Yeah, I think there's probably room for a couple of small non cross layer items, but they would probably need to be bubbled up very quickly next time. Active discussion decisions soon.
00:20:00.830 - 00:20:01.580, Speaker H: Yeah.
00:20:03.790 - 00:20:52.550, Speaker A: Probably. It's not super critical thing, right? So we can do it after the fork, after the node. So I'm just wondering if it makes sense to work concurrently or just make more sense to defer it to the next one. That's my kind of question. Yeah, I mean, my general answer is probably along with a major feature if there are cleanups and minor items, especially when they're not cross layer. There's probably room for a couple and that we can't always push them off or we're never going to do them. So we should refine those specs and have the conversation, and if not, make a stronger commitment to them being in the next work.
00:20:52.550 - 00:22:56.650, Speaker A: Okay, other discussion points one to nab on research spec, et cetera. I did toss up a PR from Daplion, this general design that he and I were discussing on validator index reuse. I brought it up not as a necessary point to decide where it goes into a fork or not, but to gauge general sentiment here. I've heard two arguments. One is it is really nice to reuse indices and not have big gaps or not have gaps form from an engineering perspective, but also that the counterargument is you can probably handle this with some engineering complexity and not necessarily put it into the spec. I just wanted to get some eyes on this and get a general feeling on if this, which is a relatively minor feature, is worthwhile from the perspective of client engineers. And if you don't have comments today, I just want to bubble it up to actually leave comments in the pr.
00:22:56.650 - 00:23:38.590, Speaker A: Nonetheless, this will probably be merged soon. It's under like underscore features, so it's not slated for a fork, but it's at least we're good at feature complete for discussion. No comments. Okay, we're kind of getting to the end of the list of our agenda. Does anyone have anything else to talk about?
00:23:46.410 - 00:23:54.440, Speaker H: I can give a quick update about what happened if the map boost relays earlier this week.
00:23:54.810 - 00:23:56.120, Speaker A: Yeah, that'd be great.
00:23:57.130 - 00:25:20.340, Speaker H: All right, so let me post the postmortem here that we posted two days ago. On Monday, a proposer attacker tried to steal a block from a relay to unbundle and then change the transactions and propose that to make a ton of money. It took slashing into account, obviously, and they managed to do this by sending an invalid beacon block with a zero state route. And at this point, the clients they used to broadcast before validate. So they really sent this block to the beacon node, but the beacon node found it to be invalid so the proposer could receive the response from the relay and didn't have to raise a second block because that was invalid and for some reason not propagated quickly enough because actually it should have been. In the end, they managed to steal a ton of money from sandwiching bots, and then they turned off their validators and that was it. As a response of that we changed two things on the release in coordination with the other release as well.
00:25:20.340 - 00:26:17.094, Speaker H: There is a get payload cut of time that is around like 4 seconds now that if proposers try to request late into the slot, the payload won't be returned. And more importantly now the CL clients that we use, they use to validate before broadcasting because standard CL behavior is broadcast and then validate. But for the relays, we really need the CL client to validate the payload before broadcasting. But this introduces additional latency, of course, about 300 to 700 milliseconds for the validation before it gets broadcast. And this pushes the broadcast time of a block further out. On top of this, are you doing.
00:26:17.132 - 00:26:19.750, Speaker A: Full validation, including the execution layer?
00:26:20.970 - 00:26:42.814, Speaker H: I think the CL clients do that. Our really at least already validates the execution payload when we receive the pillow submission. I think CL clients also do that. I'm not 100% sure, but this all together. Okay, Terrence poses. Maybe you can chime in here.
00:26:43.012 - 00:26:43.470, Speaker C: Yeah.
00:26:43.540 - 00:27:23.690, Speaker E: So since then we have provided a patch such that we don't have to do El validation. In particular, the method that we are using to validate before broadcast is rather slow because it touches four choice. It saves the state and the block to the DB. We don't need to do that. So since then we have provided a patch that just does CL consensus rule validation only. And that is going to be much faster. I think this is going to be tested right now and getting stoked before the relayer is willing to just use that validation.
00:27:25.710 - 00:27:34.640, Speaker A: Okay. Yeah, that's what I was going to get at, is that there's probably a lot you can carve out there if you need to do some validation only.
00:27:35.090 - 00:28:33.870, Speaker H: Yeah, totally. And we definitely want to have code that is as performant as it can get here to reduce the latency before broadcasting. But all things together that the proposer can request the payload a bit into the slot like this is like 1 second, one and a half seconds sometimes even, and then the validation delay before broadcasting, it pushes the broadcast timing kind of into the slot further. And we're seeing now additional amount of unusual amounts of orphan blocks. Right now it's about, I think four an hour up from like, I think ten a day or so. Because multiple relays have the issue that with the validation latency and other latencies and more aggressive orphaning behavior by ICL clients too, that the blocks, they're just getting out so late that we're seeing a lot more orphaning right now.
00:28:34.020 - 00:28:42.878, Speaker A: Yeah. Question are these because people are beginning.
00:28:42.974 - 00:28:45.080, Speaker H: To make their 1 second I'm back in a moment.
00:28:45.770 - 00:29:11.650, Speaker A: Sorry, Chris, did you say you'd be back in one moment? I apologize.
00:29:24.930 - 00:29:30.000, Speaker H: Yeah, I'm very sorry. My kids just had an accident and started screaming next to me.
00:29:35.430 - 00:29:39.380, Speaker A: Got it. Are you back and available or are you taking a moment?
00:29:41.370 - 00:29:43.400, Speaker H: Yeah, I would say I'm back.
00:29:44.810 - 00:30:51.610, Speaker A: Okay. Are these due to proposers beginning the mev boost dance late into the slot? Are these orphans happening in relation to people who are actually beginning the mev boost dance at the start of the slot? My worry is that people who have deviated from honest behavior, sending a block at the start of the slot into more rational behavior, waiting to try to get more muv, are the ones that are being orphaned here, and that such actors who are rational and modify their behavior will modify their behavior and make the calls earlier in the slot. So I'm just wondering if this is like an issue for honest validators or for rational deviant validators, because if it's the latter, I wouldn't worry too much and they're going to change their behavior.
00:31:02.770 - 00:31:04.880, Speaker H: Can I repeat the question real quick?
00:31:06.770 - 00:31:42.890, Speaker A: Yes. Is this a problem for honest validators that are making the calls to Mavboost at the start of the slot, or is this a call for rational deviant validators that are calling Mavoost at 2 seconds into the slot to try to get more mev? If it's the latter, I wouldn't necessarily worry too much because they will tune their behavior to call earlier. But if it's the former I would worry because now essentially that's showing that there is too much latency in the comms.
00:31:43.950 - 00:32:22.840, Speaker H: Yeah, we are still monitoring and investigating. We will follow up with more data. I think even for like all clients are configured to start the flow at t equals zero. But there can be a number of reasons for additional latencies here. Just bad network connection can make the get header request slow and then already start late, and then by randomness the validation is a bit slower than usual and suddenly things get pushed back really far.
00:32:34.050 - 00:32:34.414, Speaker A: Right?
00:32:34.452 - 00:33:34.350, Speaker I: So perhaps you can say a little bit about the thing with delays. Even before this incident we were tracking reorgs because not only lighthouse, but now prism is also trying to reorg late blocks. We've been mostly successful. We've seen some problems on slot one. So where when the second slot of the epoch tries to reorg the first block on the epoch, because the first block was late, then prison has an issue in that particular situation, which does happen a little bit often, because the first plot of the epoch oftentimes is late after the relayer started posting these patches, we are seeing much more reorbs. I mean, it's clear now we have more missing slots. And I do suspect that unless this has changed, the situation is going to get worse and worse.
00:33:34.350 - 00:34:03.130, Speaker I: The delays they're talking about is over a second to 2 seconds. So this is essentially like a coin flop on each block the relayer is sending, whether or not it's going to be reordered or not. I strongly advise to look in this batch that Terrence said that they're soaking, because the difference seems to be between one to 2 seconds currently on the validation, to just 100 milliseconds afterwards, and with 100 milliseconds, I think we're absolutely safe.
00:34:10.160 - 00:34:32.740, Speaker A: Got it. And Chris, you're claiming four to 600 milliseconds. Do you have very highly resourced machines? When we say one to 2 seconds, are we talking about two different types of machines here?
00:34:33.370 - 00:34:51.660, Speaker H: I think it depends on what type of validation. But in general, like our, we're seeing like 400 to 600 milliseconds validation delay on the beacon node and ultrasound, we have maybe a bit less even, but if you can get that down to 100, that would be amazing.
00:34:52.910 - 00:35:20.450, Speaker I: You also need to recall that the time that makes the cut for us to reorg is not the time that we receive the block, it's the time when we put it in port choice, which happens after our own validation. And that's the validation on the computer of the validators might not be a large computer. So even if you send a block at 3 seconds, that takes over a second to execute, that's a coin flop, a coin toss.
00:35:34.000 - 00:36:16.830, Speaker A: I do think the providing more visibility into honest late reorgs would be valuable, because I do think that a number of entities might have begun trying to modify behavior to get and broadcast late, and that this, combined with modified relay behavior, is maybe amplifying the problem. And so these actors will probably figure out themselves. But just making clear that essentially there's a modification that is utilized by the majority of the network, I think would help.
00:36:26.320 - 00:37:35.672, Speaker H: Yeah, I totally agree. This is certainly something that is pretty interesting and important to monitor and to keep an eye out, and to think about possible ways to mitigate and that have also as little as possible negative impact on solar stickers by increasing just regular mist. Lots, for instance. We are with the other relays, I know ultrasound relay is also very active on monitoring their numbers and their performance, and the mid slots and everything. And we are working together with the other relays to improve the parameters in the situation over time to collect more data and keep an eye on it. There will also be more data coming out from this and we are working on unshot, hardening the mask boost flow overall and analyzing potential attack vectors that could be possible. By the way, one more thing here is that it would be really interesting to get more security research on MF boost.
00:37:35.672 - 00:37:54.964, Speaker H: Really code base there was an audit and there were a bunch of people looking at it. But yeah, there is maybe interest in having a more coordinated, wider bug bounty program that is not solely funded by Flashpot, but more of an industry wide thing.
00:37:55.002 - 00:38:33.980, Speaker A: Maybe on specification of the proposal boost reords that's landed in a pr in limbo for quite a while, but now something like 70% of the network will be utilizing it. Any opposition to shifting that into spec, even if there is a note that it's optional behavior?
00:38:42.370 - 00:39:51.510, Speaker C: Marius yeah, so I'm not opposed to shifting math boost into the spec, but I would like to make clear that this bug wasn't an issue with MavBoost, but it was with the relayer. And so it kind of feels like, I don't know, flashbots or the relay service providers have failed their users, the builders, and are kind of trying to pawn this on everyone else. So I would like to make sure I don't oppose doing putting the math boost software into into the general bug bounty program, but I very much oppose painting this as something that was like an Ethereum fault or something. This was definitely an issue of these service providers and not of the general Ethereum ecosystem.
00:40:00.770 - 00:40:37.290, Speaker A: Yeah, thank you. Just to clarify, I was asking, there's a specification point in the fork choice that's hanging out in a pr that has not been merged for quite a while, even though most now the majority of the network is utilizing it. So I guess there's two discussion points there. One is how to provide map boost with a better bug bounty, and then also secondary questions of relay bounties and other things like that. And the other is the fork choice proposer boost reorgs into specification.
00:41:01.640 - 00:41:16.330, Speaker C: Yeah, again, same argument. I think this part is like map boost is part of the core and it should be specified and standardized and specked out.
00:41:24.580 - 00:41:33.590, Speaker H: Here. I would say map boost is somewhat specked with the builder specs already. What kind of different spec would you be thinking about?
00:41:37.790 - 00:42:36.612, Speaker A: Yeah, just to make it clear, Maurice, their builder specs are under the ethereum.org and are well specified and well standardized. And those are the maybe poorly named as I looked at it, but those are the communications with how to deal with talk to. So I think that is well specified at this point. Okay. On pull request 30 34, which is the late reorg specified by Michael Sprowl. It again seems strange that is not part of the spec at this point, if that's the behavior, even if it is optional behavior.
00:42:36.612 - 00:43:33.760, Speaker A: So let's do a pass on that and make consideration over the next week or two on if and how to finally put that into the spec. Okay. Yeah, I think there's a number of active conversations around web boost, around safe relay behavior, around equivocations and other things. So I don't think this is necessarily done, but be good to get the optimization and that helps with latencies and to continue to monitor. Is there any other discussion points related to this incident or webboost in general protest?
00:43:34.420 - 00:44:25.280, Speaker I: Yeah, so there was a question that perhaps it's good to gauge here. One of the options was for clients to provide an endpoint that you would submit a blinded block and it would just tell you if it's valid or not, just by checking the consensus validity of the block. And this was our first reaction of something that we would want to have, because having the relayers forking cl clients and having a prison fork and a lighthouse fork is not a good status quo. And I myself am divided about this. I'm not really sure if this is something that we want to have, if it has to be specified or not. I initially disagreed on having this specified. Paul Hahner I don't know if Paul is here, but Paul seemed to agree that this should be something that we would want to have.
00:44:25.280 - 00:44:27.360, Speaker I: And I just wanted to have this discussion.
00:44:33.350 - 00:45:25.574, Speaker A: I mean, my intuition is that if we don't, then there will be a subset of clients that are used by relays instead of a diversity of clients that support the behavior. And the behavior will not necessarily be standardized or will be de facto standardized, whereas the RPC endpoint would certainly standardize it. It doesn't seem, it seems like one of those things that if we don't do, we end up in a worse spot, and if we do, then we're kind of like adding this very specific behavior for this use case, but knowing, which is a bit strange, but knowing that it prevents the actualization of a bunch of strange and modified behavior. Right.
00:45:25.612 - 00:46:00.260, Speaker I: So my worry is that by adding things that make the situation to continue on, sort of like diverting us from realizing that we need PBS and protocol and facilitating the continuation of the status quo is not something that I'm happy about. But I also agree that having relayers forking prison and trying on production, testing on production. The prison patches or lighthouse, not good either.
00:46:05.900 - 00:47:16.104, Speaker A: Yeah, I guess the counterargument to that is Memvis exists and it's not going to go away tomorrow. So there are leaving the issues to just stand there leads the system in a more fragile place because that is the practical nature of the system today. And if this were a matter of like, okay, we're going to do enshrined PBS in three months because we're not going to shore up the system then. Sure, maybe, but I don't think anyone's discussing timelines like that. So the band aids seem necessary. Wouldn't we need something like this blinded block consensus verification endpoint? If we had PBS, yeah, you wouldn't necessarily have it as an.
00:47:16.142 - 00:47:29.096, Speaker I: This would be probably something needed for PBS. So for PBS, probably we will need some sort of validation of the block, which is your consensus anyways. So this endpoint would be needed.
00:47:29.278 - 00:48:09.802, Speaker A: You need the workflow. You wouldn't necessarily be using the endpoint, right, because you'd get that message from gossip and you'd be doing consensus things on it to decide if you need to do an attestation to that subcomponent of the block. So yes, you'd need the workflow. No, I don't think you'd need the RPC. But even then that's a reasonable additional argument. Yeah, I don't know. I kind of like the idea of having the separate endpoint, but it's also kind of for a pretty specialized purpose then for everyone to have to support.
00:48:09.856 - 00:48:10.460, Speaker H: It.
00:48:16.530 - 00:48:38.040, Speaker G: Even if all the clients implement it. I don't know if there will be on any case some super specialized logic to super optimize that endpoint, because seems that every millisecond matters here. So does this prevent in any case having specialized fork for super optimized that one?
00:48:50.370 - 00:50:33.390, Speaker A: Yeah, hard to say, but it at least allows the default relay behavior to pick up a client and just do it rather than having to fork something. Obviously. Hard to say what these highly incentivized actors will ultimately do. Is anybody willing to specify the endpoint in the PR for further discussion? I can in the next couple of weeks probably, but right now I feel like I got a lot of network on my plate. It almost seems like this should be a part of the builder specs, even though at that point it sort of becomes like the builder specs are half execution, half consensus. Why is this part of the builder specs? Because this is a functionality presumably a relay would be calling. Yeah, like a relay implements the builder specs to be like.
00:50:33.390 - 00:51:21.550, Speaker A: Yeah, I guess it's more of like an internal API for the relay. Yeah, I think the consensus APIs is the place because it's a functionality that a particular user needs, particular use case needs. You could obviously in the builder specs have some notes about the things a relay could or should be doing in between places in the communications, but I don't think that's where you'd actually specify the RPC.
00:51:28.220 - 00:51:34.556, Speaker H: I think we should maybe consider this conversation in next week again or so, because there may be additional checks that.
00:51:34.578 - 00:51:42.380, Speaker A: We wanted as part of this flow too. Yeah, that sounds good.
00:51:49.760 - 00:52:14.820, Speaker H: I just wanted to say thanks for everybody involved in chiming in, into the analysis, into mitigation, into helping chart the path forward and keeping an eye on the data. It was a huge team effort. And yeah, also thanks for giving this room here, and I apologize that I just had a little small children incident in between and got probably unfocused.
00:52:24.500 - 00:53:14.500, Speaker A: Okay, thank you very much. We will bubble up at least that discussion point for next week and can discuss any other updates that have come since then. Nothing else on the agenda today. Are there any other items people would like to discuss before we close? Okay, I believe there are at least a couple watch parties for Chappella. Everyone upgrade your nodes and happy fork. Thank you.
