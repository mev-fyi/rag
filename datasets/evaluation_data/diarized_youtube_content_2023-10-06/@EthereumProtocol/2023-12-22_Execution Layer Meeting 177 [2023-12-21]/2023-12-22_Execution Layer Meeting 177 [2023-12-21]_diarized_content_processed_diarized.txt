00:05:16.100 - 00:05:30.600, Speaker A: This should be fix, but just went over what was on the agenda. Okay, so to start, Denkoon, the Gordy shadow fork. Harry or Barnabas, you want to give an update?
00:05:33.020 - 00:06:06.416, Speaker B: Yeah, I can go. So we had the curly shadow work this week. The network consisting roughly of 300 nodes, split across New York, Frankfurt, Bangalore and Sydney. We're running 100 validators per node. And then we initially synced the curly network and then forked it into our own shadow fork started with capella and Dankoon was at Epoch 220. We didn't notice any issues with the shadow fork itself. Almost all the clients joined the shadow fork.
00:06:06.416 - 00:06:39.970, Speaker B: We weren't able to sync Eragon in time, unfortunately, but we're going to shift to a snapshot approach next time, so that doesn't happen. But all the other clients joined the shadow Fox successfully and they all forked into Cancun without any issue. We were observing roughly 99% to 100% performance at a station as well as block production. And then we started blob spamming. So there's a link to the more detailed analysis that I've put up today.
00:06:40.340 - 00:06:43.664, Speaker C: There were zero blob transactions before that point.
00:06:43.862 - 00:07:15.016, Speaker B: Exactly. So for roughly a period of 1 hour, it was just Cancun and nothing else. And then you can see the massive spike in the overall network use graph. And that's exactly when the blob spamming began. So you're seeing nodes use roughly 700 kbps. So that's bytes per second in the Cancun flash capella paradigm. And once blob spamming starts properly, you're seeing it at roughly the 900 kbps paradigm.
00:07:15.016 - 00:07:57.064, Speaker B: So we can assume that there's a roughly 200 kbps increase with healthy blob usage. One caveat I will note is that the network is extremely flat. There are 300 nodes, and most of them have between 71 hundred peers, which means you're going to be peered with someone from almost every region. So it's as perfect networking as you can possibly imagine. There's no clustering or anything that's happening, but also not particularly sure how to simulate the clustering or what would be useful there. And then we went on to the blob analysis itself. So we have a few blob spamming tools that we use.
00:07:57.064 - 00:08:37.620, Speaker B: So we have Marius's TX fuz that submits blobs, and we have PK 910 gumi blob spammer. We were targeting at roughly the four blob range. And if you look at the distribution, I think we've successfully hit like three with six being the least and zero being the least as well. So I'd assume this is pretty much a scenario we'd also see on main net, because the target is three. So most of the time we would likely see three blobs per block. The next one is the average blob propagation time, most of the time across 95% of the nodes. So this includes the ones in Australia.
00:08:37.620 - 00:09:22.550, Speaker B: They are receiving all the blobs in under 2 seconds. If we don't include the P 95 value and just take the average, it's under 500 milliseconds. And we have a couple more graphs as to how the effect of more blobs what effect that has on network latency with respect to blob propagation as well as block propagation. The caveat is that for block propagation, the way different CLS emit their events is varied. So some might emit the first time they see a block without verifying it. Others might emit it after they've verified all the blobs. So just keep that in mind when you're looking at that graph at least.
00:09:22.550 - 00:10:10.512, Speaker B: And in terms of block latency, we see an increase of roughly 300 milliseconds. So the presence of blobs increases the latency by about 300 milliseconds. Attestations don't seem to be affected. They still happen at roughly the four second mark. So we're able to achieve everything we need to buy every node is able to have all the data it needs by the four second mark within the last roughly two days. We haven't seen any reorgs on this network, but I did post an alternate message on interop yesterday as to periodic spikes we see, and the periodic spikes seem to be correlating with epoch transitions. So I think it's a known fact that epoch transitions are heavier.
00:10:10.512 - 00:10:18.170, Speaker B: So we do see that the block or the slot at the epoch transition would take longer to propagate, as opposed to the rest.
00:10:21.890 - 00:10:48.920, Speaker C: On the overall bandwidth consumption increase that we're seeing. You said 700 to 900, if I remember correctly. Do you have any insight, if that is, from the consensus layer or the execution layer? Obviously, we're propagating blobs on the gossip, but we're also sending on the consensus layer. We're also sending in the mem pool. I presume you're not sending these just through builder networks. These are hitting the public.
00:10:49.610 - 00:11:11.710, Speaker B: Exactly. They're hitting the public mem pool. So we're not able to get the metrics per cl and El. So what we've done is instead target the public interface. So this is just the combat. Each node has one El and one Cl. And this is all the traffic that has come out and gone in from that public interface.
00:11:11.710 - 00:11:24.660, Speaker B: There are some client specific metrics that we can track, but the issue is that they're not necessarily standardized. So you might mean different things based on which client you talk to.
00:11:25.030 - 00:11:47.910, Speaker C: One way to get transparency on the ratio there would be to spam blobs, but only through assuming we had nearly 100% builder network or something, only send them to builders through this and then you'd see only the increase from the consensus layer because you wouldn't have any blobs in the public mem pool.
00:11:48.350 - 00:11:49.338, Speaker B: That's true.
00:11:49.504 - 00:11:55.194, Speaker C: That might be interesting. Obviously that's work, but could be interesting.
00:11:55.392 - 00:12:41.450, Speaker B: Yeah, we can give that a shot. I'd propose we try that on Devnet twelve though, because the other point we wanted to make is that we'd like to turn off Gully shadow fork today evening. So if that's some last minute test, we can do it, but that's about it. And I've added the blobber a couple of hours ago, so it adds some latency of 1.5 seconds to random blobs. And currently I don't see any effect of that. So I guess the analogy on Mainet is if you have a random dispersion of, I don't know, ten nodes out of 300 that are really slow in propagating things, there's no network wide perceived impact.
00:12:45.420 - 00:12:47.528, Speaker C: They're not the originators of the message.
00:12:47.614 - 00:13:19.670, Speaker B: Though in some cases they are in. Sometimes they are round robining which node we submit the Blobs to. Yeah, I think that's all we had from the curly shadow fork. There was one thing that the rest team wanted to look into with rate limiting, because we noticed these massive spikes once in a while, but I think they already have a pr ready for it.
00:13:29.030 - 00:13:36.690, Speaker A: Yeah, thank you. This is really valuable analysis. Any other questions? Comments?
00:13:41.040 - 00:14:15.320, Speaker B: One last point I wanted to make was we've listed a link to every graph that I've referenced. The analysis is based on all the clients together, but there's a filter on top, so you can filter just for your el or cl that you care about and look at per client behaviors. I think there's just too many clients to do individual analysis, but it would be awesome if you guys can have a look. The metrics are persisted for the next couple of months, so there's no need to do it before the network is torn down. Just adjust your time frame accordingly.
00:14:20.120 - 00:14:21.850, Speaker C: This is awesome. Guys, thanks.
00:14:27.190 - 00:15:01.320, Speaker A: Any other questions, comments on this? Okay. If not, yeah, I guess the next thing I was hoping to discuss was what to do in terms of next steps. A couple of people reached out. Oh, sorry. Okay, before we do that, anyone opposed to tearing down the Gordy shadow fork by the end of today? Please voice your objection now.
00:15:03.930 - 00:15:11.980, Speaker D: Yeah, we want to look into the epoch transition thing. And so basically we are debugging it, and we might need some time for that.
00:15:13.070 - 00:15:16.506, Speaker A: Okay, so at least load star, would.
00:15:16.528 - 00:15:18.940, Speaker E: You not be able to do that on Devnet twelve, though?
00:15:23.530 - 00:15:24.518, Speaker D: On what?
00:15:24.684 - 00:15:25.880, Speaker A: Devnet twelve.
00:15:28.250 - 00:15:31.240, Speaker D: So you are seeing the same behavior on Devnet twelve as well.
00:15:32.010 - 00:15:36.280, Speaker B: I would be quite sure that it's present on Devnet twelve as well.
00:15:39.100 - 00:15:48.200, Speaker D: Okay, we'll basically look over there as well and see if we find we can replicate it there, then we can get rid of the goalie. Shut up, fork.
00:15:51.180 - 00:17:16.238, Speaker A: Okay, any other team who wants to look at something on the shadow fork? Okay, so let's coordinate offline, but at least check with loadstar before we shut it down. Okay, so, yeah, next up, basically discussing next steps from here. So I know in the couple of calls ago, we talked about potentially forking Gordy, assuming the shadow forked went well and that client teams were ready to do that, I'm pretty sure we had prism run through the shadow fork and live on Devnet twelve now. So I'm curious to hear from teams, like, is anyone uncomfortable with moving to Gordy as a next step? And if no one's uncomfortable, when would people want to see that fork happen? So, yeah, maybe. First question, anyone not ready to move to Gordy? Sometime in January. Okay, promising. And so if we were to fork.
00:17:16.254 - 00:17:24.040, Speaker C: Gordy, do we have consensus layer teams represented? I'm on mobile, so I'm kind of scanning through right now.
00:17:25.050 - 00:17:31.240, Speaker A: Let me see. At the very least, there's someone from prism here.
00:17:33.150 - 00:17:34.570, Speaker C: That's what I was worried.
00:17:36.270 - 00:17:36.778, Speaker F: Here.
00:17:36.864 - 00:18:00.660, Speaker A: Oh, cool. Okay, so, James, just to make sure, from the prism side, does January for Gordy feel reasonable based on what our team talked about? I think so. There's a couple of issues that we're still trying to.
00:18:02.550 - 00:18:06.370, Speaker C: Work out, but it looks promising.
00:18:09.500 - 00:19:55.670, Speaker A: Got it. If we want to fork this in January, I think we probably want to have the client releases out something like ten days before the fork actually happens. So based on that, basically, if teams are ready to put out a release as soon as they come back from the holidays, then we could target something like mid January. And then if teams feel like they might need a couple of weeks coming back from the holidays to polish things up and put out a release for Gordy, then maybe it makes more sense to target end of January. And the question is one, do people have a preference there? Then two, is it helpful to agree on like an epoch or at least a tentative epoch today so that when teams are starting to put out release they can use that number? I guess if there's no objections or pushback, I would personally go first. I okay, there's January eigth release around prism. I was going to say I would personally go for either the 17th or the 25, which are two dates where we get like a midweek accumulator boundary slot.
00:19:55.670 - 00:20:49.860, Speaker A: Does anyone have a strong preference between the 17th versus 25? Okay, lots of people like eleven and 17. Some people like 1725. So yeah, I think 17 is a decent compromise. So from the row I had in the table on the agenda, this would be the epoch number. So it would be epoch number 231680 on the network start slot is 741-3760 which happens at 06:32 a.m. UTC on the 17th. And then there's other local time zones on that.
00:20:49.860 - 00:21:38.400, Speaker A: But yeah, let's do that. Obviously if we find a major issue or something crazy before then, we can always cancel. But this would mean that ideally we're putting out the blog post for the fork sometime during the week of the 7th or, sorry, the week of the eigth so that people have at least a week to update. So if client teams can target to have a release at the latest on the eigth or the 9 January, then we'll put out the blog post with all of those. And yeah, that should give people time to update before the fork happens. Any comments? Questions? Thoughts?
00:21:44.670 - 00:21:45.900, Speaker G: What's the.
00:21:49.630 - 00:22:17.490, Speaker A: Know? I don't know if Adrian added it to his. Oh he does. Let me just check this real quick. Gordy, the slot. I'll post a timestamp. I'll verify it offline. But I think this is the timestamp what I just posted in the chat.
00:22:17.490 - 00:23:25.520, Speaker A: Yeah, there's a comment by Angar around talking about a best case timeline between different testnets. I think historically two weeks is the closest we've done. Just it's like the pace at which client teams can put out a release and have the Testnet fork, depending on how confident we are. One thing we can also do is bundle have a single client release for something like Sepolia and Holski. If we assume that if Gordy goes really well, and we assume Holski is going to be trivial to fork once Sepolia forks, we can just have a client release and a single announcement telling people to update their nodes once and that might save us a week or something. Aside from that, assuming we actually have like a client release per testnet, it's hard to do less than two weeks per testnet. And then between the last Testnet and Mainnet we might want a bit more time for people to upgrade their nodes.
00:23:25.520 - 00:24:01.410, Speaker A: Yeah. Okay. So yeah, let's aim for releases around the eigth or 9th. We'll have the fork on the 17th, and then from there we can also figure out next steps for the other testnets and if we want to bundle those releases. Anything else on I guess Denkun as a whole before we move on to other things.
00:24:03.220 - 00:24:31.050, Speaker E: Shouldn'T we try to tentatively assign like plus two weeks from Girdley to the next testnet and then in case something goes wrong, then possibly modify it instead of just waiting for Girdley to happen and then discuss when we could fork the next one? Because then we're going to be losing another two weeks at least in just discussing potential future fork times.
00:24:33.900 - 00:25:30.606, Speaker A: I'm not against it. I don't know how client teams feel about that. I would want to look at the numbers not on the fly right now on the call. But if people want to agree to rough dates now, we can definitely propose tentative like slots and then maybe what we could do as well. Actually, yeah, what we can do is propose tentative slots that go in like a two week motion if you put our okay with that and then reconfirm them on the call two weeks from now so that teams won't have their releases out yet. And then if they want to already include that in a release, it still gives them a couple of days. But yeah, I guess.
00:25:30.606 - 00:25:33.246, Speaker A: Would anyone be a sensitive 31st and.
00:25:33.268 - 00:25:38.490, Speaker C: Then 14th and continually reconfirm after each of those forks?
00:25:38.570 - 00:25:58.582, Speaker A: And I guess my question between 31st and 14th is assuming we do Sepolia first and Holski, do we feel like we need two weeks between those two given if Gordy and Sepolia have gone? Well, Holski should be trivial. So I could see the value.
00:25:58.636 - 00:25:59.714, Speaker C: 31St and 7th.
00:25:59.762 - 00:26:45.214, Speaker A: Yeah, 31st and 7th. I definitely would not want to bake in a main net date yet. I'd want to sort of roll through that and at least get Gordy and sepolia smoothly upgraded. But yeah. Does anyone think that like 31st and 7th is too aggressive or we should do something different? Okay, let's do that. So I'll look at some epochs and slot numbers right after this call. But let's do Gordy on the 17 January.
00:26:45.214 - 00:27:19.950, Speaker A: Client teams have a release on the eigth or 9th that we can announce. Then we'll do Sepolia around the 31 January and Holski around February 7. Again, assuming nothing goes wrong. If at any point we see something, we can always change those dates. But yeah, teams can start planning around that. And the implication as well being that we'd only have a single client release for the Sipolia and Holski forks. So yeah, we'd want both of those coded in network.
00:27:19.950 - 00:27:39.140, Speaker A: And yeah, Barnabas, you have the question. Did we agree to do Sipolia before Holski? I guess sipolia feels higher risk than Holski. So if we are combining the releases, it makes sense to do it before. But I don't know if anyone has a argument to that.
00:27:45.680 - 00:27:47.204, Speaker C: Define higher risk.
00:27:47.352 - 00:28:02.356, Speaker A: So there's just like stuff on it, right? There's like a bigger state, there's more people using it, whereas Hosky is basically completely new. Although Hosky has like a larger validator set. I guess it would be earlier to.
00:28:02.378 - 00:28:10.820, Speaker E: Catch some network related bugs on Hoskid and sepolia. So that's why I would personally prefer to fork Hoskid before Sepolia.
00:28:12.600 - 00:28:33.910, Speaker A: I think higher risk is also more informative too, right? There's more people using the sepolia. We might want to consider learning from it earlier. But yeah, I guess it is a good point. If there are more validators on Holski, and the main thing we're concerned around is like networking and blob propagation, I think it's fine to do Holski first if people want to derisk that.
00:28:37.520 - 00:28:40.750, Speaker C: Yeah, I could make pretty compelling arguments either way.
00:28:45.060 - 00:29:08.960, Speaker A: I guess. Does anyone, again, aside from Barnabas, have a strong preference on ordering? The argument for Sepolia first is just there's more usage, there's a bigger state, but there are less validator nodes.
00:29:10.660 - 00:29:26.570, Speaker C: So usage is also important for actual users testing things. So if that gives l two s an extra week to test where they're actually currently testing their environments, then that's a good argument for me.
00:29:36.380 - 00:29:43.850, Speaker A: Yeah, I don't know if any l two people are on the call. I don't think so, but. Oh, Carl. Yeah.
00:29:45.660 - 00:30:07.840, Speaker F: I think that would be helpful. Most of the testing has been done in private so far, and I think it also just act as a signal that, hey, this is really happening in the short term and might help have more robust deployments ready sooner around main net launch. Because I think that might not necessarily be the case for some of the l two s right now. So the sooner the better, in my opinion.
00:30:08.580 - 00:30:34.830, Speaker A: Okay, so I guess that would bias me towards Sepolia first. I don't know how much is on Sepolia already, but I've definitely heard l two s wanting to move stuff to Sepolia if they haven't already. So, yeah, giving them an extra week to do that seems reasonable. Yeah. Okay. So I think I'd push towards sticking with Sepolia first. Barnabas, are you okay with that?
00:30:38.990 - 00:30:39.980, Speaker E: Yeah, sure.
00:30:40.430 - 00:30:42.170, Speaker A: Okay, so we have the schedule.
00:30:42.770 - 00:31:07.640, Speaker C: Gordy, just to state it out loud, we picked the oldest testnet because if it borks, who cares? We picked the second oldest testnet because that will help users more than the newest test net in terms of timing and then the newest testnet I just stated out loud, it doesn't have to be a general rule, but that is reasonable logic, and maybe we will reuse it.
00:31:08.090 - 00:31:26.460, Speaker A: Yeah, and I guess the argument to have done Holski first is if we cared less about user behavior, like on chain behavior, and wanted to optimize more for validator behavior, because it's just more validators on the host key.
00:31:29.870 - 00:31:42.240, Speaker E: It's not just really about the validators, but it's the number of nodes on the network. Right. The polio only has like 100 nodes or something like that. And Olski has significantly more.
00:31:44.850 - 00:31:54.306, Speaker B: I think Holski is kind of like in the order of magnitude of the shadow fog we just did. At least based on some crawling, it's like 500, 600.
00:31:54.488 - 00:31:56.280, Speaker C: And what do you think sepoli is.
00:31:56.810 - 00:31:59.190, Speaker B: A couple hundred, like 200 at max.
00:32:00.250 - 00:32:19.216, Speaker C: We could beef it up if we felt like right before. Obviously that doesn't beef up the validator nodes, but you'd still get deeper, like block propagation path. I don't know. I probably wouldn't advocate for that, but.
00:32:19.238 - 00:32:39.350, Speaker A: It is a possibility. Both of these are also like an order of magnitude under main net, right? Definitely. It's not like Holsky is like 75% of main net and Sepolio is like 10%.
00:32:40.620 - 00:32:58.776, Speaker B: Yeah, I think the most we're going to learn is from Curly, because the validator set is small enough. There are a lot of esoteric setups and so on. Host key just has a very large validator set size, which means by default, most people are just running really beefy machines with a lot of validator keys on one host.
00:32:58.968 - 00:33:17.940, Speaker A: Got it. Okay. Yeah. So I think let's stick with the Gordy sepolia Holsky ordering. If we see something going wrong on Gordy. We can always readjust that. And if we think that the next test being on Holski is better, we can just swap them around.
00:33:17.940 - 00:34:15.390, Speaker A: But for now, assuming things go well, let's just follow that order. I'll come up with some slot numbers. I'll come up with some slot numbers after the, yeah, and then, yeah, clients should have the first release for Gordy and then the second release after Gordy for the two other test notes. That makes sense to everyone. Cool. Anything else on Vancouver? Okay, if not, next up, Ansgar, you wanted to talk about precompile address ranges and how l two pre compiles may affect this?
00:34:19.860 - 00:34:22.532, Speaker H: Yeah, I mean, Carl has his hand up. I'm not sure.
00:34:22.666 - 00:34:24.516, Speaker A: I think Carl's just filled up from.
00:34:24.538 - 00:34:25.270, Speaker B: The last.
00:34:29.400 - 00:35:00.480, Speaker H: So basically. But funny enough, that's also Carl and me. So just to recap, we have recently started this IP process, roll up improvement proposal process, to standardize EVM and EVM related changes across L2 chains. Of course, purely opt in. So it's not a governance call or anything. It's really just a standardization forum. And now as we're starting to get the first kind of proposals through this process, we're starting to just kind of run into several kind of questions.
00:35:00.480 - 00:36:00.352, Speaker H: And one of those specifically referred to pre compiles. So you might have already, I think, I think we already discussed on Alcodas as well, the potential future r one curve pre compile SEcp 256 r one. And that now has an IP number and is scheduled to go live on several roll ups basically as soon as possible. They're all just waiting for it to be finalized in this process. And so the last remaining question we have is now for a pre compile like that that starts out as initially a L2 targeting EVM change. Should that basically take the next sequential pre compile number that we have open on Mainnet or should that go into a separate range? And so the one kind of nuance that makes this not a trivial question, because I think initially a lot of people had the intuition that kind of a separate range for L2s might be better. But the problem, of course, is that a lot of EVM changes that will start on L2s first will at some point later potentially come to Mainet, especially in the future.
00:36:00.352 - 00:36:48.592, Speaker H: We would expect that most eips actually start on L2s because they just chip things faster and then later on potentially come down. And so, of course, we wouldn't want to have them be on a separate address on mainet from L2. So then that would mean that if we give L2s their own pre compile ranges, that would mean that now on layer one we would at some point also start shipping from that new range, which is a bit weird. So alternatively, of course we could just keep one continuous range. And then that would mean that at some point in the future on Mainnet, once we ship future pre compiles, we might start to have gaps where there's just no pre compiles for a few addresses because they are only on L2s and then some that we shipped on layer one. So that's kind of the trade off here. And given that again like a few L2s, really want to ship this r one pre compiled soon.
00:36:48.592 - 00:37:47.270, Speaker H: Basically the hope was that we could kind of just make at least a decision, maybe one off decision, ideally of course a more systematic decision here, how we want to treat this. Yeah, and the one ones maybe because Holger also asked on the call scheduling GitHub issue, there will be some sort of registry as well, like a meta rip with a list of all the precompiles and the different L2s that they are shipped to. We haven't created that yet, but we are in the process of also setting that up. And then the question is, if we decide to have one continuous range on layer one and L2s, is that enough to basically just have this kind of list of pre compiles in the IPS repo? Or would cordafs then also prefer to have some sort of meta eip in the eips repo as well? That kind of clearly lists which pre compile addresses are basically blocked. So yeah, those are basically two questions that I would have for people.
00:37:48.600 - 00:37:50.580, Speaker A: Thank you, Danny.
00:37:53.900 - 00:39:27.620, Speaker C: Given we don't know how much I'm pro adding a range for l two s, and if there is significant adoption of an EIP before L1 adopts it to use disjoint sequencing and use what was selected from that range for L1. Because it's very unclear at this point what's going to happen with rips in terms of like, is there going to be one eip for the r one curve? Yes, probably, but you could imagine maybe there's two because there's splintering in terms of what one l two wants to do in relation to another l two. Then we also have to think of what is that Eip being adopted on l two s. What is an l two? What makes it in that range. There's all sorts of these questions which could make that space quite utilized could make that space quite fragmented. Obviously, I know that's the goal is to avoid a lot of those things, but as we don't know how that's going to happen, I don't think it makes much sense to give it the main net allocation range at this time, and to instead pick from it if we want to use it. Obviously having disjoint ranges is also kind of annoying, so I understand the argument the other way, but I think this gives us just a better optionality as we kind of watch the l two the rip process unfold.
00:39:30.540 - 00:40:34.116, Speaker G: I think we will be having disjoint ranges anyway, because no matter how we do it, in the end there will be eips or rips that are finalized that will not make it to Mainet, so there will be unassigned pre compiled addresses. I would also prefer the rip process to have their own disjointed range. Like it doesn't really need to be disjointed, but we could say something like, okay, from, I don't know, address 256 upwards is the l two range, and the address zero or one upward is until 256 is the normal range.
00:40:34.228 - 00:40:50.400, Speaker C: And Marius, if for example, they selected one from the range on the above 256, and then say it was the r one curve, and then l one was going to ship that same exact functionality, would we then utilize their number? Is that what you're suggesting?
00:40:51.620 - 00:41:19.880, Speaker G: That's the other question. I'm not 100% sold on just using what they propose. I think there needs to be a debate about that, and it kind of feels like Anska is just like proposing this, as it's a given that we adopt the same address, but I don't think so.
00:41:19.950 - 00:41:23.550, Speaker A: What's the rationale for not. Yeah.
00:41:26.000 - 00:41:45.468, Speaker G: Well, that's what I'm saying. There's a debate to be had around this, and of course it would be preferable for users for smart contract devs to have the same address, but it's.
00:41:45.484 - 00:41:51.924, Speaker C: Really a matter of how this manifests in compilers and things most of the time, right?
00:41:51.962 - 00:42:12.490, Speaker A: Yeah. And I guess my question is, is there an advantage to having the same functionality at two different addresses? It feels like it might only just create additional risks or confusion. So I'm trying to understand what's the argument for having the r one curve at address a on an l two, but address b on an l one.
00:42:13.580 - 00:42:41.360, Speaker C: Encumbering l one to l two? For example, if all the l two s were like, oh, actually we're going to modify the deployed eip and functionality at that address then they might modify, extend, et cetera. And now l one is using something different than l two, these things can move independently. So that's my main argument.
00:42:41.520 - 00:42:56.650, Speaker F: Okay, so to be clear on that, an rip process will enforce that. If you do want to do that, once we agree that something is finalized, you need to deploy a new rip and you'll have to use a different address because that's horrendous ux on the l two side as well.
00:42:58.540 - 00:43:30.150, Speaker C: I don't know, that's kind of debatable in my opinion. As to when you're adding new functionality, whether if it's an extension of functionality or just a minor breaking change of functionality, then you do change things like self destruct. We changed for good reason. And so things that are calling a previous place still get a different functionality. So that's certainly precedent for changing functionality at an address space.
00:43:33.480 - 00:44:06.460, Speaker F: I think if that's a concern, then we should deploy rips as eips to the same address as the rip, and then the main net thing just follows whatever I said in the EIP. But I don't know, it just seems silly to potentially lose out on all these synergies just because there's a potential for change in the future. Like we just default into a worst case scenario unnecessarily.
00:44:11.520 - 00:44:23.820, Speaker C: Yeah, I disagree just because the practicalities of how I've seen things upgrade in the past, but obviously it's optimal from a certain perspective not to change things once they've been deployed.
00:44:28.900 - 00:45:08.300, Speaker A: One concern about this is that if the addresses are not the same, then there will be different versions of compilers, and compiler developers will have to deal with different versions of the compiler for each network. But they would have to do that anyways, because on l one, even with a separate address range, on l one that address is nothing, and on l two it might be a pre compile. Right? So they're going to need ideally two versions but not n versions.
00:45:12.420 - 00:45:17.744, Speaker C: There is certainly network specific metadata depending on compile target as well.
00:45:17.782 - 00:45:24.460, Speaker A: Yeah, and guard, I don't know if you raised your hand again.
00:45:24.830 - 00:45:58.386, Speaker H: Yeah, I just wanted to briefly say, because I didn't want to make it sound like, by the way, that I assumed that was a given. Of course that was just kind of an opinion. I wasn't aware that there might be kind of opinions against ever kind of deploying l two precomputes at the same address. I think it's fine, we can discuss this and if this is something that basically we don't feel comfortable at least committing to yet which it sounds like we are not, then I think that would mean that we also already have an answer for my other question around the precompile ranges. Because then we should definitely not use the same range.
00:45:58.418 - 00:45:58.566, Speaker A: Right?
00:45:58.588 - 00:46:38.820, Speaker H: Because if we of course use the same range, then we're kind of committed to shipping at the same address. So then I would say kind of as a takeaway from today, we should just give l two pre compiles their own separate range, maybe. Question, are people generally okay with 256? That seems like a very sane default, and basically zero x whatever, 256 and upwards. I mean, of course not zero x whatever the hex that is. And then we can basically discuss this once we move closer to actually shipping one of those pre compiles to layer one, which I could imagine might be the r one pre compile relatively soon. But then we don't have to make that decision today.
00:46:42.020 - 00:47:30.220, Speaker A: Yeah, that seems reasonable. Where we can always choose to take zero x 256 if we want to go that route, or zero x ten if we prefer. Yeah. Does anyone disagree with the zero x 256 range? If not, yeah. And I guess we should probably put this in some informational EIP or Rip, or at least somewhere other than the transcript of this call to highlight that we reserve zero X. I guess we would reserve 256 onwards for L two s. Like maybe we want to bound that range, at least for now, to some.
00:47:30.220 - 00:47:36.152, Speaker A: I don't know if it's like 100 or 1000 pre compiles we already have.
00:47:36.226 - 00:47:43.980, Speaker G: Bound and that is zero. Like basically we have zero to 1024 is the bound of the pre compiles.
00:47:44.060 - 00:47:48.144, Speaker A: Okay, so we're saying that then should.
00:47:48.182 - 00:47:53.830, Speaker C: We just flip the upper bit and call it half the range rather than starting at 256?
00:48:02.610 - 00:48:07.860, Speaker A: I feel like we'll probably have more than a one to one ratio, though.
00:48:09.430 - 00:48:13.154, Speaker G: So you're proposing 512, right?
00:48:13.352 - 00:48:31.304, Speaker C: Yeah, it's essentially like the bit flip signals which one you're on. That feels natural to me. If there's compelling arguments about the ratio of the amount that we'll see.
00:48:31.342 - 00:48:51.120, Speaker A: Yeah, I feel like three to one feels more reasonable. Like if you look in the history of Ethereum, we've shipped ten pre compiles in like seven years. Some of the eips have a bunch of pre compiles they want to add, but feels unlikely. We hit 256, but I can see l two.
00:48:51.190 - 00:48:59.920, Speaker C: Yeah, there's a lot more degrees of freedom on who's doing innovation and on what. So it's a compelling argument.
00:49:02.020 - 00:49:14.440, Speaker I: Could it be possible to have a range that's actually starting from zero xff? Like all the way at the end of the space and having it decreasing so that you can extend it as much as you want, it will never clash.
00:49:21.220 - 00:49:24.210, Speaker G: Well, you cannot extend it as much as you want.
00:49:26.180 - 00:49:28.304, Speaker C: But you don't have to decide where they're going to meet.
00:49:28.422 - 00:49:31.308, Speaker A: You don't.
00:49:31.324 - 00:49:44.660, Speaker I: Yeah, exactly. You don't have to worry where the wrench starts because you know where it starts. It starts at the, well, not fff all the way because it's been reserved for other chains, but fffe, for example, and just have it decrease and decrease and decrease.
00:49:44.820 - 00:49:52.250, Speaker A: Are there some weird gas cost arguments to that? I don't think so, but the fact that it's zero.
00:49:53.020 - 00:49:54.436, Speaker G: There definitely are.
00:49:54.558 - 00:50:02.124, Speaker A: Yeah. If you're loading, sorry, go ahead. Yeah.
00:50:02.162 - 00:50:15.330, Speaker G: If you call into a pre compile, that is like, if you're loading the address of pre compile, you can just add two, push two instead of push 32.
00:50:16.100 - 00:50:46.238, Speaker A: So how about we do 512 minus one? How about we do 512 and then if we ever exceed the 512 on the l two s, we can give the sort of second 256 range back to the l two s, which will be kind of ugly, but something we can deal with five years from now. Does anyone disagree with that?
00:50:46.244 - 00:50:53.422, Speaker F: I mean, we could also just add another 1024 on top of it if we ever needed to. And give them that twelve as well.
00:50:53.476 - 00:50:53.886, Speaker A: Right.
00:50:53.988 - 00:50:56.690, Speaker F: Give us that three to one ratio, whatever, it doesn't matter.
00:50:56.760 - 00:51:11.190, Speaker A: Yeah, but I think, yeah, 512 probably gives us room for several years and we can revisit this, or other people can revisit this in 510 years and think we made a really bad decision.
00:51:14.110 - 00:51:17.414, Speaker C: Or we can revisit ourselves and think we made a bad decision.
00:51:17.542 - 00:51:57.310, Speaker A: Yeah. Okay, so let's do that. I think an informational EIP probably makes sense because we want l one to be aware of this, and then we can have rips that reference the range or something. I think if it's just an rip, then someone just looking at l one stuff might not know that we have reserved this range. So, yeah, I would add an eip for this. Okay.
00:51:57.760 - 00:52:00.030, Speaker F: We can just kind of like. And throw together.
00:52:01.040 - 00:52:02.688, Speaker A: Sorry, go ahead.
00:52:02.854 - 00:52:25.960, Speaker H: No, I just want to say, just to point it out though, because we have had the similar conversation in the past, that people would, are generally uncomfortable actually having L2 specific information in an eap. So EAp would literally just say that basically the upper half of that range is blocked for Mainnet. And then to actually see which of these addresses correspond to which L2 pre compiles, you'd have to go to the IP repo. Does it sound like a good yeah, exactly.
00:52:26.030 - 00:53:52.030, Speaker A: But knowing that on l one, we shouldn't deploy anything in that range, I think is what we want in the Eip. Sweet. Anything else on this? Okay, next up real quick, we discussed this a bit last week on ACDC, but in the next couple of weeks with regards to awkward devs, we said we're going to skip a formal awkward devs next week, potentially have a testing call at 14 utc on Thursday. If people want to show up and organize something the week after that, January 4. Given we're going to want a client release shortly after, it feels like we should have an awkward devs call, but I just want to check does anyone feel strongly that we should not? Okay, so we'll keep the call. I will be out, but light client will be running the call, so he'll see you all there in two weeks. And then, last thing on the agenda, we have this thread for the Prague and Electra EIP proposals.
00:53:52.030 - 00:54:44.190, Speaker A: I went over it a couple of days ago to compile sort of everything that's been proposed so far. There's been two more proposals in the last day, so 7547 and 7212, but aside from that we have a decent list already there. I was curious to hear if anyone has thoughts about the overall set of proposals, anything they'd want to bring up now to discuss or bring to people's attention. I think at the very least, people should be reviewing this over the next couple of weeks, and as we move towards the testnets for Denkoon, we should start triaging through all of this sort of individually, but yeah. Any initial thoughts, comments, questions from any of the teams?
00:54:54.390 - 00:55:01.266, Speaker I: Yeah, just one question. Are we still targeting a very short fork, a very small fork for Prague?
00:55:01.458 - 00:55:03.800, Speaker A: I don't think we've agreed to anything yet.
00:55:06.250 - 00:56:22.110, Speaker H: Yeah, Anzgar, yeah, I basically had a comment on that. I just wanted to say, because we are also actively working on EIP, one or several eips regarding adjustments to the kind of staking reward mechanism. I think related to similar how in Dinkun we ship basically the throttling of the queues just to make sure that we kind of never go out of bounds in terms of how many validators we have on Ethereum. And so while those are not quite finalized yet, I think in general it would be nice from that point of view if we did have enough of a fine grained forking schedule that we don't have to, worst case, wait for nine plus months on a change. Once that becomes urgent, with potential kind of developments in the total amount of staked eth and whatnot. So in principle, there would be a preference towards having more of like a smallish fork that we can still ship by the end of next year at least, instead of one that really bundles everything that might be delayed until, I don't know, mid 2025. And then we basically are not very reactive in case we need to make small adjustments.
00:56:23.730 - 00:56:44.470, Speaker C: One additional thing to note is a number of the consensus layer proposals and do not have to be cross layer. So there is an ability to make an independent decision here on a lot of those eips if the execution layer is going to be digging deep into a big picture.
00:56:51.330 - 00:57:49.540, Speaker A: Right. And I think on the el side, the big fork is basically, do we choose to do Verco now and then prioritize it? And we might include other eips alongside Vercol, but effectively we'd only ship the fork when vertical is ready. And just generally any large feature has taken us over a year to ship, whether it's the merge, obviously, whether it's Ford for Ford that we're working on now. So I think if we do go down the road of vertical, at least on the El side, we shouldn't assume it's going to go live in less than a year. Right. And I mean, we can hope and work hard and whatnot for that, but if there's something that actually urgently needs to go live before that bundling, would Verquel seems unlikely to have it go live in less than a year.
00:57:51.030 - 00:58:05.160, Speaker C: Yeah, my read is there is demand to get some of these things out on a consensus layer in 2024. And so we can just keep that in mind as we're making the decision on that side.
00:58:09.370 - 00:58:41.954, Speaker I: Just to be clear, nothing should go at the same time as Roko. It's such a complex thing that there's too much risk. I mean, if it's activating, even if it's activating some pre compiles, I would prefer seeing smallish fork that takes three months to release. I know it's never been done, but if it's just for a pre compile, it might be doable. I would rather have that than anything happening at the same time as Roko. Because once again, the complexity is bonkers, I guess.
00:58:41.992 - 00:58:44.306, Speaker C: Yeah, on that you're really selling it.
00:58:44.408 - 00:59:30.260, Speaker A: Yeah, on that. Then I think, again with the three month thing, it's hard to realistically commit to that. And I think the reason is there's never just one pre compile we want to ship. Like, if we want to have a fork, there's a bunch of stuff that's going to want to be included, and there's like high fixed cost to shipping a fork. So we should probably bundle a bunch of things together. By the time we're bundling three to five eips, it's basically more like a six to nine month thing. So I think it's fine to do that, but we should not tell ourselves like, oh, we're going to choose the things and fork in April after we've chosen in January, because it's only one thing.
00:59:30.260 - 01:01:20.158, Speaker A: That said, I think what we did do this time around is we sort of pre committed 4844 to the fork after while we were working on the withdrawals fork. So if we wanted to ship a bunch of smaller things on the El side, whether coupled or not with the Cl, and then slowly start working on vertical in parallel and sort of pre commit the next fork to that, that is something we can do as well. And this way we can get all the smaller things out earlier, but also sort of reserve the next work for vertical in advance and potentially have parts of the teams working on it in parallel. And I guess maybe this is probably the main thing that would be good to agree on in the first couple of weeks of next year, regardless of what the small eips are. I think on this thread everybody can get a feel for the type of stuff that's being proposed. But do client teams want to do at least on the El side, something before vertical or not? And then if we agree to that early in January, we can then figure out, okay, what are the things we want to do before vertical? And this will give us a good idea of how long this fork is going to take. And maybe on vertical specifically, like Guillaume was just saying, it's a pretty huge change.
01:01:20.158 - 01:01:52.890, Speaker A: I know there's been the Verco implementers calls and there's the website now and Josh has been doing the summaries on Twitter. But do people feel like it would be valuable to maybe dedicate part of the ACDe call to go deep in vertical and have people sort of understand exactly where things are at and potentially ask some questions? Yeah. Would it be worth having a call like that, like early next year so that we can zoom in to the details?
01:01:57.020 - 01:01:58.090, Speaker G: Good idea.
01:02:03.360 - 01:03:25.340, Speaker A: Okay, so yeah, let's maybe do that. I don't want to schedule an exact date. We can maybe play it a bit by ear, but I think if teams want to start looking at everything around vertical, start looking at all the other smaller eips and get a feeling for? Do we want to do those eips before vertical? And if so, do we have preference on which ones? We can discuss that in January and then find a time, even if it's not like a full awkward devs. Maybe have half of an awkward devs focused on just going deep into vertical so that we sort of know what we're getting into, or at least what the current state of things is. Any other questions? Comments? Okay, anything else anyone wanted to discuss before we wrap up? Okay, if not, we can leave it at that. Thanks, everyone. Yeah, let's get Gordy out for Denkoon and hope everybody has happy holidays.
01:03:25.340 - 01:03:33.240, Speaker A: Thank you. See you next year, y'all.
01:03:33.400 - 01:03:34.430, Speaker C: Thanks all.
01:03:36.960 - 01:03:40.710, Speaker A: Happy holidays. Sa.
