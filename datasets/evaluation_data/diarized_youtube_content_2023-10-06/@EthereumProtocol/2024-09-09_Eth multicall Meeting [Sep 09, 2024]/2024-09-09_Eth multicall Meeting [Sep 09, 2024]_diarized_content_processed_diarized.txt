00:00:01.000 - 00:00:09.365, Speaker A: Hello everybody. So Lukasz will probably not come today. And so let's begin.
00:00:10.025 - 00:00:10.885, Speaker B: Yeah.
00:00:12.185 - 00:00:54.575, Speaker A: So on our side, with help of Clari and Lukesh, I'm investigating the problem with self destruct logs seen also helped. Potentially it will solve this. Maybe we'll also solve the issue with the hashes potentially, but we'll see. And we are currently working on it, still fixing it. So hopefully we'll fix it this week I'll create the first patch but it is currently failing on rlp. So we'll fix it. And yeah, there definitely was an issue.
00:00:54.575 - 00:01:09.485, Speaker A: So that's it with us. How are you guys? I remember there were some talks during the week on the chat board. So if you would be able to recite that for audience that is watching On Demand, that would be perfect. Thank you.
00:01:12.945 - 00:01:44.675, Speaker C: Hey, I can go next. Yeah. As for me, I. We had a bug in one of our tests that made it fail every once in a while, which was super weird because I don't really have any concurrency in my multicolored code. So it was very strange and Killari helped me track it down. So it turned out to be a bug just in the test itself, not in the code. And it was really.
00:01:44.675 - 00:02:12.515, Speaker C: Yeah, strange because I was generating an address randomly and every once in a while it was having zeros in the beginning which caused a different gas price. Anyway. Yeah. And apart from that, we had the team meetup last week and during this meetup my team kind of looked at multicult PR as well and they merged it. So that was a big milestone for me.
00:02:13.175 - 00:02:13.807, Speaker D: Yay.
00:02:13.911 - 00:02:15.095, Speaker A: Congratulations.
00:02:15.255 - 00:02:32.945, Speaker C: Yeah, thanks. Right. There was also some chats in the Discord channel, but maybe we can talk about that after other updates.
00:02:37.605 - 00:03:08.885, Speaker B: I have been doing some testing with the timestamp stuff and then nothing else really. And there's currently differences between the timestamps. The Nethermine is not returning the correct timestamp in all of the cases. I think that's still the biggest issue that is with those hashes. And I wouldn't think that the self destructing would be fixing some hashes. Of course, fixed hashes in the cases where the self destruct is used, but it's not used into most of the testes.
00:03:11.045 - 00:03:33.065, Speaker A: Yeah, but while we were fixing that, we found out that maybe we were updating state on the same block and not on the previous block that Cina does. So maybe this would be one of the big differences. But yeah, definitely timestamps would be the main culprit of any potential hash changes that can appear.
00:03:33.685 - 00:03:44.565, Speaker B: Oh yeah, that would probably explain it. I think it's somewhat confusing that when you are specifying the block, then you are actually specifying the parent block and not the block that you are creating. Then you're just creating one block.
00:03:45.545 - 00:04:15.575, Speaker A: Maybe there's a block current logic of ours is basically we, when we are given the state overrides, we apply them in the current version that is online and public. We are applying it to the same block onto which we then apply the transactions. And in my understanding Sina does this on the previous block. So now we'll update it to be also on the previous block and this will also potentially change the hashing.
00:04:17.515 - 00:04:25.175, Speaker C: I find this a bit confusing. I don't really understand what you mean by apply the state overrides on the current block.
00:04:26.235 - 00:04:44.075, Speaker A: So basically we can commit a tree on this current block and start working from there. At least it was allowed in nethermind architecture, so we were using that. But now we will go to the and apply it to previous block.
00:04:46.615 - 00:04:46.927, Speaker D: In.
00:04:46.951 - 00:05:03.085, Speaker B: The IT call and it simulates. When you are specifying that block number, then you always specify the parent block number, not the current block number. And then when you are creating blocks, then you are specifying the current block that is being created. Is that what is what you mean by previous block?
00:05:03.505 - 00:05:04.713, Speaker A: Yeah, kind of.
00:05:04.889 - 00:05:32.941, Speaker C: Okay, so the reason why I ask is because the end result should be mostly the same. I feel like there might be some corner cases that I'm missing, but the state should be computed like the final state should be the same. Is it because there is the commit makes some difference. So like.
00:05:33.013 - 00:05:41.149, Speaker A: Yeah, at least it is possible hash wise to which block that commit was allocated. But we'll see.
00:05:41.197 - 00:05:47.745, Speaker C: Oh, oh yeah, I see, I see, I see. Okay. Okay. Yeah. Ah, okay. That's a interesting question.
00:05:48.965 - 00:05:50.213, Speaker A: Only way up.
00:05:50.269 - 00:06:13.103, Speaker C: Oh, then wait. Okay, if that's what you mean, then those changes will be applied as part of the current block. So on that we agree. I kind of misunderstood your question at first. So.
00:06:13.239 - 00:06:18.875, Speaker A: All right, so you are applying it on the beginning of the current block.
00:06:21.465 - 00:06:22.205, Speaker C: Yes.
00:06:23.145 - 00:06:24.225, Speaker A: Yeah. All right.
00:06:24.265 - 00:06:25.805, Speaker C: Yes, yes.
00:06:26.305 - 00:06:29.645, Speaker A: Then timestamps are the like the biggest.
00:06:32.625 - 00:06:37.685, Speaker C: Yeah. If we have different in state hash, then it's probably something else.
00:06:37.985 - 00:06:51.875, Speaker A: Yep. All right, so you've discussed on the chart something about Simulate V2. Can you talk a little bit about that?
00:06:53.015 - 00:07:41.065, Speaker C: Yes, that's an idea to add basically in v2 return also the whole Merkle proof that is needed for a client to do the simulation themselves. And I think this is important for improving the wallet ecosystem so that basically when the user wants to check their token balance. They also get a proof from the provider that they can check themselves and not only trust that their balance is 42.
00:07:44.615 - 00:07:55.795, Speaker B: Have you tried making a version out of it? I'm just wondering that. The proof is probably going to be really huge and I don't know if it's going to be so huge that it's going to be useful or not.
00:07:57.855 - 00:08:02.835, Speaker C: I think well, the proof will depend on the complexity of the simulation.
00:08:03.495 - 00:08:03.855, Speaker A: Yeah.
00:08:03.895 - 00:08:37.854, Speaker C: So we probably will have to limit like I mean the gas limit that we have kind of prevents it from going out of place or. Yeah, by setting a low gas limit it should be possible to contain the size. But I think yes, it would be interesting. I can work on a branch and then we do some tests with common use cases like just looking at balances and things that are most common.
00:08:38.634 - 00:08:50.814, Speaker B: Do you have ideas what would be the worst case scenario? What could the user do to generate as big proof as possible? I'm unfamiliar how the proof works so I don't know what's the worst case? What do you need to do to query?
00:08:51.274 - 00:09:51.511, Speaker D: Worst case would be querying lots of different pieces of state that are all over the merkle tree and merkle tree is generally fairly well distributed so that's actually pretty easy to do. Basically do a bunch of state reads and you'll get a bunch of like getting balances. Yeah, like every state read particularly so every state read you do is going to be problematic especially if you do them from different accounts because you won't even be able to get like any sort of reuse in your and your proof. I mean this, this is, this is an idea that people have been wanting to do for a very long time. I think there's among the core devs, I think there's a lot of desire for it and the problem has always historically been the witness sizes for merkle trees are just horrendously huge. Like many megabytes for a good witness. And this is I think basically the number one reason people want to move to merkle trees is to address this issue so we can have witnesses that are not crazy big.
00:09:51.511 - 00:10:28.523, Speaker D: I can get witness size down to kilobytes instead of megabytes. Now for a single RPC call, I don't know if it's going to be in the megabytes range. Witnesses are usually talked about in the context of blocks and so for a full block I believe a merkle witness is megabytes. A virkow witness is like hundreds of kilobytes or something. I'm guessing for an ETH call or an ETH simulate or whatever, you're going to look at some amount smaller than that, but it's still probably going to be huge. That being said, I'm also a big fan of proofs. This makes light clients possible.
00:10:28.523 - 00:10:45.009, Speaker D: Look into. You might want to look into Helios. Helios was built by somebody, I forget who. And it's a light client that basically depends on this kind of proof generation. And it was a proof of concept and I think you can actually use it. But they all surrender the same problem. These.
00:10:45.009 - 00:10:57.805, Speaker D: The proofs are just. Or the witnesses are just too massive to be pragmatic. But I'm. I'm saying all this just to warn you ahead of time that this is what you're running to. But I'm a big fan of it.
00:10:59.225 - 00:11:31.687, Speaker C: Yeah. And especially in the case of Helios, I think a problem that they are facing is that to verify its call, basically they need a lot of round trips, so they start executing and whenever they face a state that they're missing, they have to query the provider and so on. So that's why if we provide the proof upfront, that that's going to save them a lot of pain. And. Yes. So to your point. Yes.
00:11:31.687 - 00:11:53.315, Speaker C: So it's for the blocks. It is, I think around like 4 megabytes or so. But yeah, given that we were working on V1 for almost a year, I think by the time we ship V2, there's a good chance the Verkal will be around the corner.
00:11:54.935 - 00:12:04.795, Speaker A: And what happens if somebody simulates block that is prevale or even pre posts?
00:12:06.895 - 00:12:33.095, Speaker D: Oh, yeah. I mean, that's. That is a question. If you implement this after verkle trees, do you allow this function to work pre Verkle trees and just give me witnesses or not? I'm guessing my. My recommendation would be to have a client setting that says like maximum proof size or maximum witness size. And then people running the RPC nodes can just configure it and say, hey, not allowed to request things bigger than like 200 kilobytes or whatever.
00:12:34.955 - 00:12:44.135, Speaker C: Yep. And I think if this is in place, then we can already ship it with Merkle and it will already be good for many simple use cases which are very common.
00:12:45.755 - 00:13:00.505, Speaker A: Yes, Lukas, please. Lucas, you are a bit cut out.
00:13:03.805 - 00:13:05.985, Speaker E: I think I cannot make it better.
00:13:20.125 - 00:13:39.035, Speaker A: Still. INAUDIBLE all right, let's continue this one probably, I think, and we'll be able to clear it up tomorrow or the next week.
00:13:39.775 - 00:13:56.535, Speaker D: I mean, I think I Doubt we'll clear it up next week. I think it's gonna be an ongoing thing. I would love to see you do it like a prototype implementation scene and see how painful the witnesses are for simple things like balance lookups and token transfers and swaps and things like that. That'd be very useful information.
00:13:57.875 - 00:13:58.363, Speaker C: Yep.
00:13:58.419 - 00:14:07.735, Speaker A: I had a question. So as we are currently more or less released on two clients, when does this will jump aboard?
00:14:09.715 - 00:14:25.693, Speaker D: That is an excellent question. So we should probably start talking about what our strategy is for merging, getting this merged into this PR merged. And so are the. Are the clients both passing all the tests right now or are we still in a state where we're not.
00:14:25.829 - 00:14:31.997, Speaker A: We have has differences that make basically every test not equal, but as far.
00:14:32.021 - 00:14:44.255, Speaker D: As behavior goes, they're mostly the same for the most. Right. Like this is just a bug hunt, not a disagreement on how we should do things. Is that correct?
00:14:44.875 - 00:14:58.775, Speaker B: There's the hash differences and timestamp differences and then there's that structure on how it looks as. And I think that's the. Those are all the difference at the moment.
00:14:59.515 - 00:15:02.435, Speaker D: Okay, let's see. Maybe we should.
00:15:02.515 - 00:15:20.625, Speaker B: And then the difference is on some error code. So. But I haven't been bothered too much about them in a way that. Just mostly about if the other client errors are not error, then I think it's a bigger bug. But if the error codes are written differently, then I haven't been changing those too much.
00:15:22.965 - 00:15:44.549, Speaker D: Okay. So I think we need to figure out what our plan is for getting this merged. It does feel like we should get the two clients passing all the tests before we merge, but that sounds like we're pretty close to that. Maybe a week or two. Optimistically. Let's say we hit the optimistic. It doesn't really matter.
00:15:44.549 - 00:16:10.515, Speaker D: But if we get the two clients passing all or the majority of tests and then do we need to do anything else before we merge, do you think? Or should we go do another approach to all core devs? Or do we just go ahead and merge it? Or talk to the people maintaining that repo and get them to merge it and then go to all four devs and say hey, new spec everybody please implement.
00:16:12.255 - 00:16:26.715, Speaker B: They have been also discussing about if this would be IP or not. And I'm at least bit confused on like since I have been thinking that this is not shouldn't be ip but then some people feel that it should be. If so, then I don't really understand how it should be.
00:16:27.505 - 00:16:52.561, Speaker D: So the people that are in charge of EAPs have decided that it's not NEEP. So by definition that means it's not NEEP. There are a number of people who dislike the APIs not being part of the EIP process. And so that's what the discussion in all core devs discord is about, is people disagreeing on whether or not execution APIs should be moved back into the process or not. In the meantime, until that's resolved, they are not.
00:16:52.593 - 00:17:00.005, Speaker A: E was ETH call and its derivatives in EIP process.
00:17:00.345 - 00:17:08.845, Speaker D: It was not. It was in fact very little of the JSON RPC spec went through the EIP process. I think there's like one or two endpoints.
00:17:09.785 - 00:17:15.205, Speaker B: There's some yet there in there. I think more than one or two, like 10 or something.
00:17:17.545 - 00:17:20.353, Speaker D: Maybe it's possible there is as many as 10 and.
00:17:20.369 - 00:17:26.765, Speaker B: Then there's those wallet methods that's under ETH as well, like ETH get accounts.
00:17:27.145 - 00:18:02.771, Speaker D: Yeah, so there was a period of time. So basically we started out, if you go back in time to the beginning, there was geth and parity basically and I guess geth first you go back far enough and that had an API adjacent RPC API and then the other clients copied that. And then at some point after that people started wanting to make changes to those APIs. And there's a question, okay, where should we discuss this? And the EIP process was used and then that was done for a while and then that was kicked out of the IP process to the execution APIs repository instead. And so that's where we're at now.
00:18:02.963 - 00:18:23.665, Speaker A: Is my understanding correct that if we will like go the EIP road, then the issimulate method would be enabled only on certain like spec versions and it will be like all right for it to be not enabled in like pre Verkel for example, or pre post.
00:18:24.845 - 00:18:55.885, Speaker D: So this is, this is exactly what, or this is one of the reasons why this was moved out of the EIP process into execution specs. Because from a core devs perspective, the core EIPS are all consensus things and execution APIs is not a consensus issue like you, you don't need to. The clients can disagree and nothing will break. The blockchain won't break, it'll just some downstream apps will break. And so it was moved out of the IP process for. For that reason because they don't. They also because they don't lock to versions at all.
00:18:55.885 - 00:19:05.265, Speaker D: And so even when this was part of the IP process, it wasn't version locked to any hard fork because it's not part of consensus only Consensus changes get locked to hard fork versions.
00:19:06.335 - 00:19:07.195, Speaker A: Thank you.
00:19:11.415 - 00:20:29.925, Speaker C: So there exists such category within the EAPs called the interfaces UAPs and historically there has been some things there as was mentioned, for example like the whole GraphQL endpoint that was specced out as an EAP as part of the interfaces AP I think if there's ever a method that needs an EAP it would be assimilate. Up till now, because we've had to like just having the spec wasn't really enough for us. We had to write down this whole notes document that is kind of very weirdly placed lost and hidden in some a docs folder that nobody checks because there's nothing else in there. So I think there can be a case to be made for this. But I know that yes, the maintainers like the AIP editors probably are not on board. But I'm not sure how to solve this either because I think how it is now the spec itself is kind of lacking for our use case.
00:20:31.825 - 00:21:03.775, Speaker A: I would say that esimulate can be a case for an EAP as its validate on mode will sync up the understanding of client implementations on reactions on calls, on basically every call. So you would basically get guarantee that every transaction and every block can be executed in the same manner no matter the client. So this can be went through this route like synchronization between clients.
00:21:05.235 - 00:21:40.565, Speaker D: So all of these arguments are arguments that other people have made and continue to make in favor of putting execution specs into the IP process. The it is certainly not unreasonable to join that battle, but just be aware that people have been fighting this battle for years now. And so if we wanted to go that route, it means this is very unlikely to get merged anytime soon. So if we're okay with just sitting kind of this in this weird intermediate state for an undefined amount of time while we wait for that battle to be fought and this ultimately decided again.
00:21:40.995 - 00:22:06.055, Speaker A: And this just be separated like you merge the Ethereum spec like say right now, no matter when and then in like in some time we create an EIP and discover there. So basically you get your like unstandardized version of it and then you synchronize it up through an eap.
00:22:07.765 - 00:22:24.945, Speaker D: You certainly can. You're going if you want to fight the battle of to put this stuff into eips or not, I think merging it into the execution specs is going to weaken your argument significantly. The county argument you're going to get is like see the execution specs repo worked fine. You guys got your thing in everybody's happy.
00:22:27.925 - 00:23:25.815, Speaker C: So yeah, just to point out. So to get this PEC as it is merged we would need a thumbs up from the other clients because. So the de facto is that when a method is added to the ETH namespace of the execution API, then specs report, then it means that every client must implement it. And right now we don't have really an approval or veto from any client from any other client apart from Geth and nethermind. So the least we should do is go to the all core devs again and say like hey guys, we really need like a reaction whether you're okay with implementing this in future or not. We would need a decision from them.
00:23:28.195 - 00:24:02.775, Speaker D: Yeah, I would be totally fine with that. Just present. We can present it one more time, just tell people, remind people what it is and say is already in Geth. Another mind is it is Basu and Reth and who's the other one I'm missing? Aren't there 5 clients these days or is there only 4 now? Anyways, get thumbs up from all the clients. Aragon thank you. If we can just get a thumbs up from the client saying yes, we are not opposed to this method existing. We do not have it implemented yet, but at some point in the future we will, then we can merge the spec.
00:24:02.775 - 00:24:07.495, Speaker D: But I agree that we should get at least a thumbs up that it's a good idea.
00:24:11.755 - 00:24:14.295, Speaker B: Do you know people from tos if you can reach them out?
00:24:15.995 - 00:24:38.925, Speaker D: Not personally, at least not the people that would be making this sort of decision. Allport does. So we have reached out to them previously and no one said this is a bad idea. They all said that's interesting. It's low on our road, low on our priority list. So we're probably not going to do it now, but we're fine with it. So I got kind of a tentative thumbs up but nothing official.
00:24:38.925 - 00:24:42.165, Speaker D: This is like back channels discussion.
00:24:42.785 - 00:25:16.495, Speaker A: Yeah. I have a question for Sina. When the like RPC is merged into potential RPCs that are under ETH, does it mean like that everybody would see its hive tests as part of Eth5 tests and thus would need to make sure that like first you implement it and only then you pass the hive test. So is it this potential risk. Yeah. Get approved anytime soon?
00:25:17.155 - 00:25:30.091, Speaker C: Yeah. Basically if this is merged and we merge the test, then all the other clients will get an extra 100 something failing tests because they haven't implemented ESIM.
00:25:30.163 - 00:25:33.455, Speaker A: It will get high on their priorities, definitely.
00:25:35.115 - 00:25:41.415, Speaker D: I'm assuming there's some way for them to just Say we haven't implemented this and ignore the tests or is that not possible?
00:25:41.715 - 00:25:57.615, Speaker C: It's not possible as of now, and I think I'm hesitant about that. It opens doors to like segregated east namespace.
00:25:59.035 - 00:26:17.075, Speaker D: Yeah, I guess. I guess you wouldn't want to ignore them, but maybe like red, green versus yellow, where yellow is. Red means failing, green means passing, yellow means we haven't implemented this yet. And so you still get the visibility that there's unimplemented code, but it's easy to differentiate between your clients. Broken.
00:26:17.375 - 00:26:47.205, Speaker A: Oh, this actually means that either they. So it is a good argument for inclusion of this into EAP proposal mindset because it would definitely allow them to plan ahead. Like by this time of this release, we definitely shall include the ecap. And until then we can like not think about it or it's test. But on this like EAP merger, we definitely shall be included.
00:26:51.465 - 00:27:11.521, Speaker C: Yeah, I doubt that. Even if it becomes an eap, then there would be like an activation block where everybody starts turning under. It simulates. I think there's. I don't see why that should be necessary. Like we have it already implemented and we want people to be able to.
00:27:11.553 - 00:27:20.045, Speaker A: Use it for Merkle Tree would be like in megabytes range. I think some people may be inclined to like await for Verkal.
00:27:22.345 - 00:27:28.975, Speaker C: Oh, if you're talking about V2, then maybe I misunderstood. Yeah, that's a discussion to be had.
00:27:29.315 - 00:27:37.655, Speaker A: Yeah. But if three clients haven't even yet started implementing V1, then maybe they'll be implementing V2 straight away, so.
00:27:40.675 - 00:27:49.175, Speaker B: Well, there's a lot of things that I wanted in the V2, so I think it's still a kind of research thing or something that we need to discuss what we want to be included there.
00:27:55.925 - 00:28:12.285, Speaker D: Yeah. So the hive test situation does make this challenging. You're right. I didn't realize that was going to. You don't have a good solution to that. I think the next step. Okay, so how about this? Let's get the two clients passing all the tests we have.
00:28:12.285 - 00:28:47.069, Speaker D: Then let's approach all core devs and say we have this, we have hive tests for it even. But we're hesitant to enable the hive tests in the main hive test suite until most or all of the clients have implemented it. And then maybe that maybe we can get some feedback from them on ideas for how we can deal with this. So we can get the hive test updated without them breaking like failing all the tests or get some commitment from other clients to actually get this implemented. But I feel like get the two clients passing then go to ACD is probably the next two steps. Does anyone disagree with that?
00:28:47.237 - 00:28:50.745, Speaker A: Is most three or four out of five.
00:28:53.285 - 00:28:54.505, Speaker D: Is most what?
00:28:55.365 - 00:28:58.869, Speaker A: What is most clients three or four out of five.
00:28:58.957 - 00:29:30.875, Speaker D: Oh yeah. I mean that's. That would be part of the question in all core devs. Like at what point is it okay to implement to add hive tests for JSON RPC endpoints? Does it need all five clients or is four enough? Is three enough? Is two enough? Is one enough? Like where. Where's the threshold? Is it just as soon as you get a spec merge, do you go and add the hive test and if your client is now red permanently, that's your problem. Go input the spec. I think that's a quick question for ACB and they can make a call.
00:29:33.975 - 00:29:35.235, Speaker A: Yep, sells.
00:29:37.615 - 00:29:41.435, Speaker D: Okay, so step one, let's get the test passing.
00:29:43.615 - 00:29:47.715, Speaker A: Yep, we'll do all right. Do we have anything else for today?
00:29:48.495 - 00:29:53.195, Speaker B: I don't know if Lucas can now speak. I don't know what Lucas had in mind.
00:29:54.095 - 00:29:55.235, Speaker E: Can you hear me?
00:29:55.655 - 00:29:56.095, Speaker D: Yep.
00:29:56.135 - 00:29:57.511, Speaker B: Yeah, yeah.
00:29:57.543 - 00:30:31.091, Speaker E: I was just talking that weakness size is proportional to gas, right? So it even simply each call with 100 gas, 100 million gas can have a huge witnesses and proofs. So that's not. There's no connection to that. It's a block or it's a single call or whatever. And there was something I wanted to say but I don't remember now to be honest. Ah, that Virgo might not happen. So there is a big question mark there currently on Verkle trees.
00:30:31.091 - 00:30:39.055, Speaker E: Well there probably something will happen with a bigger tree fanout but it doesn't have to be vertical trees or it might be something else.
00:30:40.235 - 00:30:46.611, Speaker D: I see. So they still want to solve the general problem of giant witnesses. It just might be something besides Verkals.
00:30:46.803 - 00:30:47.931, Speaker E: Yeah, yeah, yeah.
00:30:48.043 - 00:30:48.775, Speaker D: Yes.
00:30:49.675 - 00:30:56.835, Speaker B: Is there a way of doing ETH call and get the witness of eth call that then we can at least know how big it can get?
00:30:57.175 - 00:31:03.235, Speaker D: Not currently. That's basically what Sina was proposing to add. Just do it in assembly instead of ethical.
00:31:03.855 - 00:31:08.767, Speaker B: Yeah, but I wonder if we have ethgol already and then we want to do it in similar.
00:31:08.871 - 00:31:11.315, Speaker D: No, as far as I know no one's implemented this yet.
00:31:14.375 - 00:31:19.155, Speaker B: Like with 100 million gas. Like what is the worst case size you can get?
00:31:20.095 - 00:31:23.747, Speaker D: I mean it's going to be horrible. Yeah, I agree.
00:31:23.931 - 00:31:29.655, Speaker B: Gigabytes. Or is it gigabytes or 10 megabytes or what is.
00:31:30.315 - 00:32:08.885, Speaker D: You can do some back of the napkin calculations. Basically if you have a well distributed merkle tree which we do then each storage read, the worst case scenario has a certain amount of nodes in the tree that you're going to need and you can assume that you're going to need that every time because you have no overlap in your tree. And so you divide 10 million gas by the 5,000 gas per read. That gives you a number of reads. You multiply that out by basically the merkle size per state. You may be -32 or 64 for the first two layers in the tree because those one will duplicate. But by and large you're going to end up with around that order of magnitude.
00:32:08.885 - 00:32:19.115, Speaker D: Like I said, I think if seen as interest in implementing I think the easiest solution here is just scene that implements it and we get a definitive answer.
00:32:20.095 - 00:32:31.155, Speaker B: No, I guess that's not very practical then unless there's useful calls that you can make in the but in most cases you don't read that big weakness.
00:32:33.055 - 00:33:23.745, Speaker C: Yeah, and that's my. That's where I'm pretty sure that for really simple simulations it is feasible it will be around 100 kilobytes. Well the contract code is a question mark. I'm assuming that especially if clients like wallet can hard code or cash contract codes and we don't need to emit them in the witness, that's going to give us a huge saving amount. And I'm pretty sure most of the usual calls will be easily, easily done, easily possible. But yeah, it's. Yeah, I would like to prototype this and then we can test and have a more informed conversation.
00:33:25.085 - 00:34:07.155, Speaker D: Yeah, if you wanted to leave out contract code, you probably want to have some way to make it so the caller can say I already have the code for this and I expect it to be used. Please don't give it to me again. Like you said, I already have the code at address X so don't include that in the witness. That way the caller can say here's maybe an array of things I already have or here's some things I suspect this call is going to include in the witness and I already have that data so please don't include it. And that can definitely get your witness size down, but it's going to increase complexity in the API. And so I think that's just a trade off we'll have to make and I suspect if we go to merkle trees we're almost certainly going to have to make that trade off. And even with vertical trees just code size can get big though it is limited.
00:34:07.155 - 00:34:10.475, Speaker D: 27:25 Kilobyte is the biggest code.
00:34:11.215 - 00:34:23.671, Speaker C: Well, the point with virtual trees is that the code will be chunked so that you don't need to send the whole bytecode and only the. Yeah, yeah.
00:34:23.703 - 00:34:31.025, Speaker B: I think it would be reasonable to assume that the client knows the contract codes because they can be curious separately on them cached.
00:34:33.405 - 00:35:01.375, Speaker D: Well, so the problem is the wallet is what you would want to do. The proof verification, not the app, most likely. And the app has context of what's being called, but the wallet does not. And so you need a way for the app to tell the wallet, hey, here's the code at X. It's probably going to be in part of this call and that needs to then tell the client the same thing.
00:35:02.235 - 00:35:21.255, Speaker B: Yeah, but you could make the witness so that it says that this code in this address was used and then you first run it and then you get all those addresses and then you run a get code for all of those and then you run it again or you don't need to run it again. But then you can kind of have all the data after running the get codes and then you can cache those for later calls.
00:35:21.595 - 00:35:52.405, Speaker D: Yeah, I mean that's kind of what Helios is doing mentioned. Helios is running into problems with that though. They're doing that not only for code, they're doing it for everything. So basically the client says, here's what we're executing or what we're executing. The Helios client basically just makes calls to get proof for every single individual lookup. We can definitely do better and limit that to just calls to look up code. And maybe that's a good trade off.
00:35:52.405 - 00:36:19.779, Speaker D: I'm a little hesitant. Like an ideal witness is one where the witnesses everything you need all at once. You're not making a bunch of round trips. But you're right, the code can likely be cached very heavily by its side, unlike storage data. Yeah, these are all good questions, I think.
00:36:19.827 - 00:36:43.465, Speaker B: Yeah, caching all that stuff sounds a bit more difficult, but I guess still useful to do that as well. But then you need somehow need to indicate them what you have already. Can it be made so that the each simulate will just give all those places that are touched and that they can be cured separately so that you need to make multiple calls instead of one.
00:36:45.885 - 00:36:46.905, Speaker D: What do you mean?
00:36:47.885 - 00:37:14.655, Speaker B: I mean that instead of providing the whole witness, it will just say which data need to be needed and then it kind of gives an instruction what you need to query to get all witness or get all the data to have the witness how much you can shrink down down in the witness, but not kind of sending anything that can be curated separately.
00:37:17.235 - 00:37:40.705, Speaker D: It's certainly possible you'd save some round trips, but you might end up with the client having to run. It might work. It would be better than the current situation with Helios. I guess there's. These are all things we could do as stepwise improvements with full witnesses on the one extreme and then Helios on the other extreme.
00:37:41.165 - 00:37:43.145, Speaker B: Does the Helios work at the moment?
00:37:45.325 - 00:37:49.225, Speaker D: It does what it is programmed to do. It's not really usable.
00:37:50.125 - 00:38:21.175, Speaker C: Yeah, I think for a lot of stuff it is just not escall and estimate gas. Because like even if we do estimulate with proofs, then estimate gas is another question. That one I have no idea how we want to approach, but certainly interesting question. Anyhow, guys, I. I have to drop off now. I'm happy to discuss this further. Async or next week?
00:38:22.115 - 00:38:25.265, Speaker A: Yeah, let's just continue discussion next week.
00:38:25.435 - 00:38:26.265, Speaker B: Yep.
00:38:27.125 - 00:38:41.645, Speaker C: But from. Yeah, what it sounds like is that there is some interest in this group of like potentially seeing if it's worth researching. Yeah, I think so. Yeah.
00:38:41.685 - 00:38:42.077, Speaker A: Yep.
00:38:42.141 - 00:38:47.105, Speaker C: All right. Yeah, then thank you and talk to you later.
00:38:50.855 - 00:38:52.195, Speaker D: Do we have anything else?
00:38:52.735 - 00:38:54.755, Speaker A: I think not. Really?
00:38:55.255 - 00:38:56.395, Speaker B: I don't think so.
00:38:56.935 - 00:39:00.815, Speaker A: Let's continue I think over next week. All right, see you.
