00:03:03.435 - 00:03:28.421, Speaker A: Okay, cool. I believe we are live. Let me grab the agenda. Let's see. There we go everyone, welcome to Consensus. Oh wait, that's the wrong link. That is the first item on the agenda.
00:03:28.421 - 00:04:17.105, Speaker A: Okay, cool, there's the right link. So this is consensus layer call 139. I put the agenda in the chat here. It's PM issue 1116 and yeah, looks like we have quite a number of things to get through today, so let's get started. First up, Electra and I think Xiaowei is a few minutes behind, but she wanted to call out that there's a PR for a new specs release, Alpha 4. Let's see, that was the link that I put in the chat just above there. So yeah, generally just an FYI, there's a few things from the last release, nothing too major, but yeah, that should be released soon.
00:04:17.105 - 00:04:34.945, Speaker A: So next up we can look at the Devnets. So we have Devnet 2. Now let's see, maybe Pariobris, if you could give us an overview of DevNet 2. It looks like it's been going pretty well.
00:04:37.475 - 00:05:24.175, Speaker B: Yeah, sure. So in the past few days we've been working on trying to get the chain to finalize. Yesterday we managed to get over 85% participation and voting correctly. Currently we still have a few execution client bugs, but I think most of the Consensus layer clients are able to propose properly. We just did a quick non finality test and we found a smaller issue on Prism when we tried to come back, but I think they can give an update on that. Everything else seems pretty okay and all the issues have been reported for the appropriate client teams.
00:05:24.955 - 00:05:32.185, Speaker A: Okay, awesome. Are there particular like EL eips that were causing issues or is it kind of just a mix of things?
00:05:36.605 - 00:05:38.105, Speaker B: Could you please repeat that?
00:05:39.085 - 00:05:47.905, Speaker A: Yeah, you said there were some EL issues with the DevNet. I was just wondering if there were like was it a particular EIP or was it more just a mix of different things across the clients?
00:05:49.285 - 00:06:03.645, Speaker B: So Ethereum JF is just having a general part time keeping up to the chain and Aragon had some block building issues that they're working on. I'm not sure if anyone from Ethereum JS or Aragon are here to give us an update.
00:06:07.505 - 00:07:23.375, Speaker C: Yeah, on Ethereum js, basically whenever the blocks will be full it will start lagging. But yeah, apart from that, as I inspected Lighthouse, Ethereum JS and it sort of was synced to the head so I didn't inspect other nodes with branding. I think there is an issue of optimistic sync because when Ethereum JS is trying to pull Optimistic Sync blocks. Basically Grandin is forcing Ethereum JS to optimistic Sync even when starting from Genesis I think it jumps the block so somehow forces its optimistic sync and when Ethereum JS is pulling the blocks from other EL clients using enodes so it is throwing an error that the block bodies they don't have request bytes which is basically something the other clients might need to rectify or they might have forgotten adding block bytes request bytes to the block body. So I guess this is something that EL clients might need to fix.
00:07:34.555 - 00:07:39.535, Speaker A: Yeah Radek yeah just want to briefly.
00:07:39.915 - 00:07:54.547, Speaker B: Say what happened on Prism since we were mentioned so it was just that we didn't we forgot to update validation on aggregates for Electra.
00:07:54.611 - 00:07:54.891, Speaker D: It was.
00:07:54.923 - 00:08:15.917, Speaker B: It is related to the attestation EIP so we were losing peers on the minimal configuration because President thought that the committee index is incorrect and the validator does not belong to the to the committee and so it down rated peers and we started losing them at Electra.
00:08:15.981 - 00:08:18.317, Speaker D: So but the fix is already in.
00:08:18.341 - 00:08:21.545, Speaker B: A PR and it will be merged today.
00:08:23.445 - 00:08:24.225, Speaker E: Cool.
00:08:24.525 - 00:08:25.465, Speaker A: Parvat.
00:08:27.765 - 00:08:29.605, Speaker B: Yeah so just to add, there's.
00:08:29.725 - 00:08:32.637, Speaker C: Another issue that I've posted in the.
00:08:32.661 - 00:08:55.163, Speaker B: Lighthouse node thread on Discord so I tried a kurtosis devnet with just Lighthouse and Teku and when I submitted a consolidation Lighthouse seems to complain that there's a missing field of source pub key from Bezu and then it fails to propose this fixes itself once another client has proposed a block.
00:08:55.259 - 00:09:01.259, Speaker C: So it feels like Bezu or yeah, I think one of them is not communicating properly.
00:09:01.427 - 00:09:04.431, Speaker B: Likely Bezuki yeah, it would be great.
00:09:04.463 - 00:09:05.955, Speaker C: If someone could have a look at that.
00:09:13.455 - 00:09:13.951, Speaker A: Cool.
00:09:14.023 - 00:09:27.383, Speaker B: And we can also slowly talk about DevNet 3 because I also noticed Casey's message and I agree that we should probably coordinate a bit better on these calls as to when we do Devnet relaunches because I think we have everything.
00:09:27.439 - 00:09:36.581, Speaker C: We kind of want from Devnet 2. The main thing is if 7702 feels ready then we can start talking about Devnet 3, right?
00:09:36.613 - 00:10:38.965, Speaker A: Yeah, that was my next question. So it sounds like there's some debugging on DevNet 2. Generally we feel like the non finality experiment went well. It sounds like other than maybe some minor issues with Prism. But yeah, so assuming that all comes together then I think DEVNET2 I think will be about as much as we're going to get from there then with Devnet 3. Yeah, I think 7702 is the main thing. Maybe just from my own knowledge Is the spec there together? Is it ready or not yet? Does anyone know? Okay, yeah, we got DevNet 3 was 2 and then the 7702 last PR.
00:10:38.965 - 00:11:07.463, Speaker A: Okay, I mean I'll assume that basically means it's underway. So. Right, so then. Okay, so that sounds like Devnet 2. We still have some things to do. Devnet 3 needs a little more time, but hopefully that's sounding like in the next few weeks even sooner would be better. But yeah, next few weeks then we'll be in a good place for DevNet 3.
00:11:07.463 - 00:11:15.075, Speaker A: Anything else on DevNet 2 that we should discuss now?
00:11:23.065 - 00:11:25.845, Speaker C: Pari, have you discussed the consolidation issue?
00:11:28.425 - 00:11:29.081, Speaker B: Not yet.
00:11:29.153 - 00:11:30.445, Speaker C: Do you want to bring it up?
00:11:32.105 - 00:11:32.617, Speaker B: Yeah.
00:11:32.681 - 00:12:48.805, Speaker C: So basically Pari did an experimentation on Devnet 2 regarding consolidation. So two times we tried to consolidate one validator into another and what really happened was that instead of consolidating the balance, so basically consolidation as such went through in the sense that one validator was sort of exited and other validators withdrawal credentials changed to compounding but the balance was not consolidated. What really happened was that the source validators balance was sort of withdrawn and the reason for that was that there was a one off work for which Mikael raised a pr. So in process pending consolidations as well as in process deposits. So there, there was a one off in the, in the epoch calculation that is used because if you use get current epoch of the state it will, it will not come to the. It will not. It will be one less than the epoch transition for the epoch for which you are running.
00:12:48.805 - 00:13:07.375, Speaker C: So that sort of fixes. So comparing it to the next epoch sort of fixes the issue and the PR is merged and I think in the next Devnet we will try consolidations again where the behavior should be correct.
00:13:12.475 - 00:13:44.675, Speaker A: Right, yeah, nice find on that one. I don't know, I'll check with Xiao. I don't know if that will go into Alpha 4 but that would then suggest it will go into an Alpha 5 and I think it makes a lot of sense for Devnet3 to have that included. So yeah, we'll be sure to get that taken care of. So there's some chat here around Devnet coordination, testing, timing. Let's touch on this. So.
00:13:44.675 - 00:14:02.455, Speaker A: Right. I mean maybe a good place to start is just Barnabas comment here around a testing call. I know we all have plenty of calls already. Do we feel like a breakout for this sort of thing would be helpful or do we want to try to coordinate more async?
00:14:13.835 - 00:14:18.539, Speaker B: I think in the past the breakout had Been helpful. We can also keep it really short.
00:14:18.627 - 00:14:22.755, Speaker C: Instead of a one hour break off breakout, we can just keep it 15.
00:14:22.835 - 00:14:24.895, Speaker B: Minutes or half an hour if required.
00:14:26.915 - 00:14:32.755, Speaker A: Right. And maybe we could just do them one off just for Devnet launches rather than standing call.
00:14:34.855 - 00:14:50.835, Speaker B: I think standing call is better because every week we will have something different, especially when we have continuous devnets and we're going to have peer desk devnets, we're going to have EOF devnets, we're going to have vector devnets. So there will be a bunch of topics to discuss.
00:14:52.015 - 00:14:59.425, Speaker A: Yeah, makes sense. Uh, let's see. Should we just use the same time from the previous testing call.
00:15:02.245 - 00:15:06.013, Speaker B: That was Monday, 4pm I think.
00:15:06.149 - 00:15:40.155, Speaker A: Yeah, something like that. Okay. I mean it sounds like it would be helpful, especially as we get closer to shipping factor. So I think that's probably a good idea. POTUS asks why not use the first of these calls? I mean, yeah, but. So yeah, POTUS is suggesting like just using now basically. But it seems like even in addition to this there's space for more fine grained discussion.
00:15:40.155 - 00:16:37.957, Speaker A: So I think it makes sense. Again, the call is optional, but it'll be a dedicated time for people to talk about exactly this and hopefully it will smooth the whole Devnet process. Cool. Any other comments on devnet2 or devnets? Next up we have essentially some other things that will go into some Devnet but just around processing things. Any closing comments? Okay, so let's see if I have all my numbers. Right. I think the idea was for DevNet 3 we would include this change engine.
00:16:37.957 - 00:17:09.184, Speaker A: Get Blobs V1. Let me grab a link. So yeah, basically the idea here is we could save on a lot of bandwidth with BLOB propagation if a node has a way to essentially check their mempool first. And I think this makes a lot of sense. Right. Presumably blobs are in the mempool and you kind of already have this propagation process just via the mempool. And yeah, this essentially provides a way to avoid needing to fetch data twice.
00:17:09.184 - 00:17:15.660, Speaker A: Luke, did you have a comment on this? Yeah, I put the comment in the.
00:17:15.692 - 00:17:19.224, Speaker B: PR today, but the comment is that.
00:17:19.804 - 00:17:23.324, Speaker E: ELS as far as I know at least nevermind.
00:17:23.364 - 00:17:25.664, Speaker B: And I think we looked into ref implementation.
00:17:25.964 - 00:17:30.824, Speaker E: Keep the data by transaction hashes so.
00:17:31.204 - 00:17:34.024, Speaker A: Not by BLOB version hashes.
00:17:34.365 - 00:17:36.965, Speaker E: And the question is, does CL have.
00:17:37.005 - 00:17:45.157, Speaker B: This data and can request this by transaction hashes or at least add transaction hashes to the request? It would be very straightforward for us.
00:17:45.181 - 00:17:49.949, Speaker E: To implement it if it will request by BLOB version hashes we either need.
00:17:49.997 - 00:17:52.413, Speaker B: To iterate through all transaction pool or.
00:17:52.469 - 00:17:54.997, Speaker E: Need to build a separate index to.
00:17:55.021 - 00:18:00.205, Speaker A: Keep the data in this way, which is adding a bit of complexity.
00:18:00.285 - 00:18:05.385, Speaker B: So the question is, does CL have transaction hashes too that can request by it?
00:18:16.445 - 00:18:36.745, Speaker A: Right, Let me go check. Yeah. So PTO says transactions are raw bytes, so there might be a way to recompute them. I'd have to go check the PR to see what they proposed, but doing it by commitment is what makes the most sense. So I see the issue here.
00:18:37.685 - 00:18:42.373, Speaker B: So if not, it's like it's still doable. It's not like it's a deal breaker.
00:18:42.429 - 00:18:45.381, Speaker C: It's just a difference between a bit.
00:18:45.413 - 00:18:47.665, Speaker E: More work and something really.
00:18:50.765 - 00:18:51.893, Speaker B: Not suboptimal.
00:18:51.949 - 00:18:54.105, Speaker E: Performance, which still might be good enough.
00:18:54.725 - 00:19:00.665, Speaker B: But if the transaction hashes were available, it would be extremely straightforward for us to implement.
00:19:04.355 - 00:19:07.535, Speaker A: Right. Do any clients care to comment?
00:19:11.075 - 00:19:43.385, Speaker B: So it's going to be hard to break that invariant that we don't need to interpret anything inside of the payload. Particularly transactions are just raw bytes for us. So one way out of getting the EL to do this is for us to request the transaction hashes for a given payload and then the EL giving this back to us. But I think this probably is harder. I mean, doing this in two communications is probably harder than just asking the EL to keep this map.
00:19:46.085 - 00:19:54.075, Speaker A: Yeah, that'd be my intuition as well that essentially just having this index on the EL is the simplest way forward, even though it is a little bit more there.
00:19:58.095 - 00:20:04.195, Speaker B: Okay, just wanted to, you know, ask if this is possible and if not, that's fine too.
00:20:06.455 - 00:20:37.075, Speaker A: Cool, thanks. So, okay, that was good. And related to this was a clarification to the P2P spec. I also dropped this PR in the chat here. It's 3864 and generally this makes sense along with the other change. Yeah, take a look. There's been some review and yeah, I think the next step would be to get both of these merged and then they can go into DevNet 3.
00:20:37.075 - 00:21:20.365, Speaker A: Let's see. I don't think Enrico is here, but yeah, has anyone had a chance to look anything they would like to discuss about these two PRs at the moment? Okay, cool. So yeah, just be aware the intent would be to get these into net 3, so take a look and we'll get them merged shortly. Let's see. I lost the agenda. Here we go. Cool.
00:21:20.365 - 00:21:52.725, Speaker A: So next up we had a request to discuss Implex. So Lighthouse has been working for quite some time on deprecating nplex. Generally I don't think. Well, I think there's a number of issues with it, but yeah, so Age was asking for essentially timelines in the past. We've generally agreed to go ahead with deprecation, but it's the sort of thing where clients need to pump some alternative. The alternatives aren't quite baked. So yeah, he wanted to discuss general status there.
00:21:52.725 - 00:22:15.025, Speaker A: As far as I've seen so far, Techie was saying that they essentially have progress on this alternative, Yamux, but it's not quite ready. Any other clients have an update on NPLEX deprecation? Do you feel like you could. Are you ready already? Do you feel like you need more time or any thoughts on timeline?
00:22:18.125 - 00:22:20.509, Speaker B: So I can speak for Lodestar here.
00:22:20.677 - 00:22:27.545, Speaker E: We have started testing YAMUX on our side. We noticed that we have a lot of.
00:22:29.485 - 00:22:42.655, Speaker B: Performance gains with yamux, but that's only with Yamux on its own. And when we enable that with mplex, the combined overhead gives us issues. So, you know, we're kind of ready.
00:22:42.695 - 00:22:46.991, Speaker E: To switch over to Yamux anytime, but we prefer to just, you know, switch.
00:22:47.023 - 00:22:56.555, Speaker B: Off NPLEX and go right to yamux. But we also don't want to cut off our other peers. So that's kind of our status at Lodestar.
00:23:00.495 - 00:23:17.035, Speaker A: Gotcha. Yeah. I mean that kind of implies then that we. Well, yeah, that one's tricky. Cause it almost implies we just want to switch to hard fork if there's going to be issues like this for everyone around performance.
00:23:18.215 - 00:23:25.555, Speaker B: Yeah, I think PL means that there's going to be some time to switch off, not that it needs to be coordinated at a fork.
00:23:27.895 - 00:23:37.457, Speaker E: Yeah, this is just in our case specifically. It's just when we have both MPLEX and Yamux enabled, we get a lot of churn and are not a very.
00:23:37.481 - 00:23:39.657, Speaker B: Good peer with both of them enabled.
00:23:39.721 - 00:23:42.025, Speaker E: So our preference is just whenever people.
00:23:42.065 - 00:23:44.645, Speaker B: Are ready to just kind of go over to yamuks.
00:23:51.385 - 00:24:10.335, Speaker C: Yeah, I'm not sure about the state of Nimbus. I know Prism's fine. So if Teku can support both at once, then it should be fine. Just swap over, I imagine. I mean, unless we do it at a hard fork, there's always going to be old nodes that haven't updated though.
00:24:13.275 - 00:24:28.655, Speaker B: Yeah, sorry, can't load star just add a flag and you just pick one or the other one. And then your load star nodes are going to be only peered with some subset of the network. At least until we deprecate nplex.
00:24:33.235 - 00:24:40.775, Speaker A: I mean, one issue There is like if everyone switches to yamx and then Lodestar is still on nplex and they just kind of prune themselves off the network.
00:24:50.155 - 00:24:51.495, Speaker B: That could be an option.
00:24:55.495 - 00:24:57.915, Speaker C: Does anyone know where Nimbus is with this?
00:24:59.415 - 00:25:13.675, Speaker E: We have it implemented. I'm not sure how well it is tested compared to Amplex, but if Dustin is on the call, I'm not sure. I don't see him today, but I could ask him.
00:25:18.265 - 00:25:22.045, Speaker C: Okay, awesome. It just means I just chase up tecu and then. And then we're all good.
00:25:27.465 - 00:26:05.185, Speaker A: Okay, great. So yeah, follow up with Teku and then yeah, maybe in another month or two we can revisit this on cl. Okay. Any closing Implex discussion, otherwise we will go to the SSE EIPS. Okay. Right. So Etan wanted to discuss inclusion of VIP 7688, which is the stable container EIP.
00:26:05.185 - 00:26:07.029, Speaker A: Let's see.
00:26:07.077 - 00:26:07.373, Speaker D: I don't know.
00:26:07.389 - 00:26:15.245, Speaker A: Etan, do you have any quick update you'd like to give around this before we move on to discussing inclusion?
00:26:16.665 - 00:27:05.541, Speaker E: Sure. Last time it was mostly this DEVNET 2 instabilities that we wanted to resolve before discussing extending the scope, which makes sense now. From today it seems that those issues have been fixed and that devnet2 is stable. So the remaining question now is if we could include the 7,688 stable container without risking peer Das getting out of hand. At least in Nimbus it's not the same people who work on those features. So from our side it would be fine. But I would like to also ask the others for opinion and that would be for devnet3 like that they won in a couple weeks.
00:27:05.541 - 00:27:54.305, Speaker E: Not like something that needs to be rushed out. On the implementation side, Guillaume has started on Zig SSE implementation. I also saw that Casey asked something about testing in the chat, so I guess that he's continuing progress as well on the go. Implementation rust is also still incomplete as I know. But yeah, there's a devnet on the stability now that box, if you scroll down to the left, there is a cortosis config. Yeah, that's essentially the update. And yeah, would like to ask about DevNet 3 inclusion of this feature.
00:27:56.575 - 00:28:05.223, Speaker A: Right. And just to clarify, I wouldn't. I mean it is. It could go to some DevNet, but I think the bigger question is do we put it into Pectra? Exactly.
00:28:05.279 - 00:28:05.943, Speaker E: Yeah.
00:28:06.119 - 00:28:21.275, Speaker A: Right. And I mean again, just to make the point again and again, Pectra is already massive. But yeah, do any clients here have a. Have a sense on this. How are we thinking about it?
00:28:26.225 - 00:28:51.165, Speaker F: I'll talk for Prism real quick that I have Merkelization passing tests with our new code generator, so there's a lot of steps for us to switch over to that new code generator. So whether it's able to land in a devnet in two weeks is a little, a little iffy just on the timeline, but I think we're positive on inclusion overall.
00:28:51.865 - 00:28:52.645, Speaker B: Yeah.
00:28:53.105 - 00:29:19.735, Speaker A: Right. So again, before this kind of gets away from us, I'll echo what Perry's saying in the chat. It would be Devnet 5 or later before I think we'd even see this on a devnet. So the question is not so much like, oh, do we rush this feature now? It's more just like, does this stack up along with everything else? And again, I would strongly urge you to consider the size of ping.
00:29:27.195 - 00:29:41.375, Speaker F: Yeah. For Lighthouse we have a solid implementation of this and something I guess unique to the SSC changes versus other changes is that as we add the.
00:29:43.415 - 00:29:43.727, Speaker B: As.
00:29:43.751 - 00:29:45.335, Speaker A: We upgrade this into the spec, we'll.
00:29:45.375 - 00:29:50.791, Speaker F: Get all the SSE tests updated so we'll be able to like regression test.
00:29:50.823 - 00:29:51.795, Speaker A: A lot of stuff.
00:29:53.535 - 00:30:07.195, Speaker F: So it could be relatively well tested as far as like a late added change. Yeah, it sounded like people from Lighthouse generally supported it.
00:30:09.585 - 00:30:29.565, Speaker A: Right. But there's like support now versus like in the next hard fork, you know, like, there's like. Yes, I think we all agree that the features are beneficial, but then it's very much a question of like scheduling and then like the testing load across DevOps and the different testing teams, the testing that each of the client teams do individually and all this stuff.
00:30:31.265 - 00:30:36.155, Speaker F: Yeah, so there's a timing component thing that's.
00:30:36.975 - 00:30:39.799, Speaker B: Yeah, I, I think on some level.
00:30:39.847 - 00:30:45.815, Speaker F: That'S who needs to use this feature and how big of a deal is it for them to use this feature.
00:30:45.975 - 00:30:49.715, Speaker A: By Petra or like a year plus later? So.
00:30:50.455 - 00:30:54.735, Speaker F: Yeah, and I, I feel like I haven't really heard much feedback about that.
00:30:54.855 - 00:30:55.515, Speaker B: But.
00:30:58.335 - 00:31:23.135, Speaker C: Yeah, so I have been speaking to some people. I saw that rocketpool had a thread and, and that's been posted. I also spoke to Eigen Layer. Their opinion is that with some of these hard forks we're actually changing core things in the protocol so they have to update their proofs anyway. So stable containers isn't really a, like a massive thing for them to have. They have to change the proofs anyway. Essentially.
00:31:29.195 - 00:31:29.547, Speaker E: Yeah.
00:31:29.571 - 00:32:17.465, Speaker B: I want to say two things, but one of them is what Adrian just said. One is that there's been two arguments that I've seen One from Sean from Lighthouse about having a prescribed overhead to ports, which is true. We have a finite amount of time that it's added to every fork in testing and getting it and shipping. But on the other hand, there's. This is offset by the exponential amount of time that testing grows with adding new features, because you need to test every single combination of them. And this is exponential. So I would urge next time we already screwed up this board by adding indiscriminately many, many features that we shouldn't have had to keep this exponential growth in mind.
00:32:17.465 - 00:33:02.145, Speaker B: As for this particular one, I mean, again, I'm definitely biased in that I'm pushing for a change like EPBs, but any such change will not be contemplated by stable containers. We will change all of the G indices as soon as we move a payload outside. And it's hard to believe that we're not going to be making changes in the protocol that are not going to be contemplated by stable containers. So I sincerely doubt of the impact on contracts for stable containers. I can certainly see that some contracts might be saved from changing, but I can also see how Rocket Pool might need to change their contracts anyways.
00:33:12.885 - 00:33:36.465, Speaker A: Right. And yeah, I mean, to add to this, I would just probably echo what Tim saying in the chat here that essentially let's focus on getting the current pector set working well a devnet that's going super smooth from Genesis before really opening the scope discussion further. So can we agree to that?
00:33:42.245 - 00:34:02.197, Speaker E: From my point, it makes sense. Yeah. Like to delay it until the rest is more stable with the caveat that we should have a time at least where we definitely agree on, like we cannot indefinitely delay the decision.
00:34:02.301 - 00:34:02.945, Speaker F: Right.
00:34:05.205 - 00:34:17.064, Speaker A: Well, we shouldn't. I mean, it could be a thing where, you know, it might take, you know, a hard fork or two, but that's like. Yeah, different. Different conversation. Yeah.
00:34:17.104 - 00:34:26.944, Speaker E: Like what says that we should have another. Should bring up. Bring it up one more time before DevNet 5. Because that's the earliest one where it could go in.
00:34:26.984 - 00:34:27.564, Speaker B: Right?
00:34:28.784 - 00:35:01.015, Speaker A: Sure. Yeah. And I mean, again, like not. Yeah, that sounds good. Okay, cool. Anything else there? Otherwise we'll move to pure dos. Okay, thanks.
00:35:01.015 - 00:35:35.135, Speaker A: So, pure dos. First, I am curious if there are any development updates. Last time we touched on this, I think people were more in sort of engineering mode and just doing some more groundwork for the changes Peridos brings. Generally. There was also a bit of a complication with targeting Deneb versus Electra and then integrating the Pirados work into Electra, given Electra is still kind of A moving target. So yeah, I expect there hasn't been too much here, but does anyone have anything they'd like to share?
00:35:40.515 - 00:36:57.165, Speaker D: I can quickly share the updates for Prism during the last two weeks, yeah we implemented the metadata victory. So basically if you want to know what are the columns your peer custody, you need to read custodic subnet count in the Ethereum node record. But unfortunately this node record is always only available for your outbound peers and so it's an issue. And so lion did a specification change where as a custod sublet counts is also in the metadata, which is called metadata v3 and so we implemented it. Yeah, we also work about some tricks on data column sampling and on initial syncs and we are right now working on the Blob sidecar Beacon API. Yeah, basically for blobs before peer das is quite simple to reply to this API request because you just have to read the blob in your database in your store. But with data columns you have to convert data columns to blobs and you may have to also to reconstruct all the data columns if you don't custodies them all.
00:36:57.165 - 00:37:11.805, Speaker D: And yeah, also we work on our data column verification pipeline. This is for our updates during the last two weeks on Prism.
00:37:13.025 - 00:37:14.045, Speaker A: Cool, thanks.
00:37:16.745 - 00:37:29.595, Speaker C: So unless you are of full note custodying everything, you won't anyway be able to reply to blob sidecar requests. So I think blob sidecar request should be deprecated anyway.
00:37:30.735 - 00:38:16.215, Speaker D: Yeah, actually it's not really because. So of course if you are a superar node you are able to respond to this request. But if you are not a super node but if you custody at least the half of the colons so you are able to reconstruct all the columns from the half. It is a principle of construction. And then you can transform columns to blobs and I don't think we should deprecate it at all because a lot of application and among other L2 really needs this API endpoint for an external application. I guess it's the only way this API endpoint is the only way to to retrieve the blobs actually.
00:38:19.355 - 00:38:36.975, Speaker C: But if CLN doesn't have any full blobs or does not have 50% of the columns, so it can't reconstruct any, how will it sort of respond to the request? And then there is question of peers always penalizing you if you are not properly responding.
00:38:38.235 - 00:39:50.863, Speaker D: So here I'm talking about the Beacon API so there is no really question of penalizing a peer because As a peer you expose the number of. As a peer you expose yes Columns you should custody. And so here the question is not here, yes, you should always custody the currents you advertise. But for your question, what about if a node is not able to. Is not custody enough codons? And in this case, what happens if this API, this Beacon API call is done? Just as far as for Prism, what we do you can force the node to custody all the colons by enabling the flags the flag. By enabling the all subnet flag. You have a flag which is called yes, something like get all the subnets.
00:39:50.863 - 00:40:12.615, Speaker D: And if you enable the flag so you will have all the columns. And if a user uses Beacon API call and is not able to get all the because the node doesn't have all the currents in the error, we advertise the user that he should enable this flag on Beacon node start.
00:40:16.875 - 00:40:23.895, Speaker C: All right, so we can still deprecate the request response endpoints, right?
00:40:29.445 - 00:40:38.905, Speaker D: I don't think we should, but because if we deprecate this Blob sidecar endpoint we will break a lot of application.
00:40:43.085 - 00:40:45.613, Speaker C: So I think the applications like the.
00:40:45.629 - 00:40:50.741, Speaker A: Gossip RPC separate from the Beacon API here.
00:40:50.773 - 00:40:54.315, Speaker D: I'm talking about the Beacon API. Yes, unless a Beacon API.
00:40:56.535 - 00:41:06.155, Speaker C: Yeah, so that. That is fine. And I. As Alex mentioned, I was talking more about the request response and.
00:41:14.215 - 00:41:35.075, Speaker A: Right. So yeah, I. I think you're correct, Ginger. That would make sense. It sounds like this is a topic for peer DOS breakout separately. Also echoing what's in the chat, we definitely need some way to get blobs if a node has them. So the Beacon API should stay.
00:41:35.075 - 00:41:40.955, Speaker A: I guess there is a question then. Like how does a node respond if they don't have all the Blob data?
00:41:47.945 - 00:42:25.095, Speaker D: Yeah, so if you're not doesn't have all the Blob data. So basically all the columns and as I said, either you can reconstruct. So in this case you reconstruct and then you respond to the API to the Beacon API call. And if you cannot reconstruct because you have not enough data in your store, because basically you don't study enough data, you just return 4,04 from just return an error. And in Prism we advertise the user that he could get all the data if he enables the subscribeto net flag on the Beacon node start.
00:42:27.115 - 00:42:33.695, Speaker A: Right? Yeah, no, that makes sense. Is that in the beacon APIs or is it planned to be soon?
00:42:39.045 - 00:43:00.785, Speaker D: So the advertisement is just in the response. When we respond with an error in the error message, we just add A please enable the subscribe to our subnets flag. That's all we do. But maybe we should add this as a requirement in the Beacon API as well.
00:43:01.735 - 00:43:18.515, Speaker A: Yeah, no, that makes sense. Okay, cool. Yeah, Casey had a suggestion to use resource unavailable here. Yeah, that makes sense.
00:43:20.375 - 00:43:21.115, Speaker B: But.
00:43:24.815 - 00:44:13.215, Speaker A: Okay, so next, we had discussed last time about dropping sampling essentially to ship peer dos more quickly. It would cut a lot of complexity and Francesco was kind enough to make a PR that would demo this. So, yeah, zooming out a bit with peer DOS in general, there will at least be another devnet, I would assume, if not several, before we even think about merging this into Petra. And this is definitely an option here if we think it will simplify development of these devnets in parallel to drop sampling. Have clients had a chance to think about this more, maybe even look at the PR and do they have any sense of which direction we should go in?
00:44:20.435 - 00:45:14.605, Speaker C: So what we discussed in the Peer Dash breakout call was. Yes, so for currently, we will focus towards not doing sampling. And the clients which are doing sampling, they can still do sampling, but they will not penalize the peers if the sampling fails and they will just log it. So it will be just for the information purposes. And we decided that we will think about inclusion of sampling at later devnets, because the priority right now would be to get a functional devnet that hasn't happened till now, in that sense that it has stayed up and has not forked and the new nodes could sync. So that manner, the priority is to get a stable devnet.
00:45:16.945 - 00:45:43.015, Speaker A: Yeah, that makes sense. Right? And it seems like it's simple enough just to essentially ignore sampling. Like, I don't think there'll be a ton of code thrash to make its decision now and then just enable it later. So that generally makes sense. I'm not able to make the peer dos breakout calls, but yeah, I'm not sure if people have been talking about devnet timing there. Does anyone know who's been able to make those calls?
00:45:49.755 - 00:46:41.395, Speaker C: I think, yeah, Banos could update us. I think we want the next DevNet to happen soon, without sampling and with the metadata PR changes. But there is also another PR which sorts of. Sorts of separates out the custody groups then from subnets. And I'm not sure whether we want to include that PR as well in the next devnet, but I think we should, because we should try to make sure that we do devnets as close as to the architecture that we intend to go live with. So maybe Barnabas can give us more information on this.
00:46:43.175 - 00:46:53.925, Speaker B: I think the general Idea was to try to launch another devnet as soon as possible with Alpha 4 spec. So whatever is going to make kit into Alpha 4, that's what we were going to launch with.
00:46:53.965 - 00:46:54.545, Speaker A: But.
00:46:56.885 - 00:47:02.225, Speaker B: I think we can discuss that in the next call. What exactly we want to include.
00:47:04.885 - 00:47:45.003, Speaker A: Okay. And I think the two things were this metadata change, which I think is an alpha for already. The other thing would be this PR from Francesco. So that would probably take a little bit more time, but yeah, so that sounds good. Get to either specs release 4 or 5 with a peer DOS target and then that will be the next Parados devnet and it will happen when it's ready. Okay. Anything else on pure dos? Otherwise we'll move to some more open research.
00:47:45.003 - 00:47:47.347, Speaker A: Oh yes, Barnabas had this comment.
00:47:47.411 - 00:47:51.739, Speaker B: Yeah, I had one more question regarding.
00:47:51.787 - 00:47:55.451, Speaker E: The how can we uncouple the blob.
00:47:55.483 - 00:47:57.243, Speaker B: Limit from here and.
00:47:57.299 - 00:48:31.235, Speaker A: Yeah, right. Yeah, so I had this PR to do this and yeah, I think there were some. A few things that needed to be addressed which I haven't had time to do yet. Right. I mean I think the way I kind of see this shaking up is this would come later, but was also. Yeah, I guess I want to see a little more progress on peer DOS before feeling a lot of urgency here. I don't know if anyone else feels more strongly.
00:48:33.065 - 00:48:45.405, Speaker B: I feel like this would need to be included regardless whether peer does mix it in or not. Because we gonna need to have a way to update the number of blobs whether we have Peer Desk or not.
00:48:46.425 - 00:49:11.153, Speaker A: Right. And this PR I think just makes it simpler and easier to think about. We could still in Pectra with no other changes, increase the blob count just in the way we did essentially for Deneb. So. And that's kind of what I'm saying. Like this is nice to have, but I don't feel a ton of urgency at the moment. Wait, but I mean like this isn't.
00:49:11.169 - 00:49:13.385, Speaker B: The whole point that in case TS.
00:49:13.425 - 00:49:15.537, Speaker C: Doesn'T make it that we will be.
00:49:15.561 - 00:49:32.045, Speaker A: Able to like upgrade it separately? Well, like ideally. But I'm just saying, like worst case we just have the constant set in both layers and we just update them regularly like this.
00:49:35.185 - 00:49:37.177, Speaker B: Well, in practice as well, because like.
00:49:37.201 - 00:49:39.545, Speaker C: If we say oh, Peerless is ready.
00:49:39.585 - 00:49:46.137, Speaker A: Two months after text draft, it's very unlikely that we'll be in a whole other el hard work for it.
00:49:46.241 - 00:49:47.417, Speaker C: Whereas if you can do it as.
00:49:47.441 - 00:50:55.435, Speaker A: A CL only after. Right, okay, well in any case, yeah, I will circle back to this and get it ready and yeah, hopefully by the next Yale call we can be in a place to discuss in depth around inclusion. And yeah, there's generally support in chat, so. Great. Anything else on Peer dos? Okay, so next up, we had a question from ETAN around the interaction between consolidations and the sync committee etan. Would you like to give us an overview of the question?
00:50:56.575 - 00:52:01.545, Speaker E: Yeah, from how I understand the consolidations, the way how it works is that it transfers the balance from one validator to another. But it could be that there are still like the source validator could still have open sync committee duties that lasts all the way into the next period, like up to two days in the worst case, like 27 hours plus one epoch. Less than 27 hours I guess is the worst one. I wonder if that is something that needs to be addressed. I think at the very least it should be explicitly thought about whether such a consolidation should be delayed until the source validator no longer has active duties. The duties themselves at this time are not slashable like the sync committee duties. But yeah, I don't know.
00:52:01.545 - 00:52:40.087, Speaker E: We cannot just ignore the consolidation because it comes from the EL as well. So it is not like if it doesn't work, just retry it a day after because there is an EL transaction that needs to be made and when it succeeds, I guess the consolidation is expected to eventually go through. And I think exits have a similar issue. But yeah, they do. Yeah, but for exits at least the balance is no longer in play. Maybe there are also, I don't know.
00:52:40.151 - 00:53:26.755, Speaker A: Like, oh yeah, it's still slashable. So yeah, I mean, I agree we should think about this. My gut would say just to leave it as is. Like there is this similar quirk with exits where you can still be in the sync committee even after you've exited. So for this I would say to keep it the same just because it's going to complicate things to have pending consolidations in this sense and do all the accounting. I don't know if anyone else sees an immediate issue here with that approach. So I guess to make it more concrete, leave it as is and probably just have a note somewhere like, hey, by the way, this could happen just so implementers are aware.
00:53:29.455 - 00:53:38.315, Speaker E: Yeah, I think that would make sense as well. Just to have it explicit and not be an oversight because it's an interesting interaction.
00:53:41.295 - 00:54:17.495, Speaker A: Right? Yeah, I agree. Nice find. Okay, cool. I can work on a PR just to call it out more explicitly and that's probably a good place to continue the thread here. Let's see. So next up, we had this PR to add Quick to the enr.
00:54:18.395 - 00:54:19.195, Speaker B: Let's see.
00:54:19.275 - 00:54:25.123, Speaker A: So Pop raised the point. I don't know if he's on the call. Yeah. Oh, hey. Yes, I am.
00:54:25.259 - 00:54:43.175, Speaker B: Yeah. The PI is about adding a quick entry into the enr. What? I think I wish to have Qwik as an optional transport protocol.
00:54:43.255 - 00:54:43.639, Speaker D: Like.
00:54:43.727 - 00:54:53.943, Speaker B: Yeah, clearly we try to use Yamath. Right. But if you support qwik, I think.
00:54:53.999 - 00:54:55.835, Speaker D: It'S better to use Qwik instead.
00:54:56.135 - 00:54:59.175, Speaker B: Yeah. So this PI is just about adding.
00:54:59.215 - 00:55:01.631, Speaker D: The INR so that the In.
00:55:01.783 - 00:55:06.755, Speaker B: So that you know that. Okay, this node is this node support Quick. Yeah.
00:55:08.745 - 00:55:28.085, Speaker A: Right. So, yeah, in general, I think this is something we want eventually. I think at the moment, only Lighthouse supports qwik. Please correct me if I'm wrong. Okay, nice. Right. So, yeah, I guess this is the thing where it's optional in the sense like you can add it if you want.
00:55:28.085 - 00:55:41.275, Speaker A: Right. So then I guess Pop's point was just, I guess merging in the pr just to make it more formal. Anyone feel we should not do that?
00:55:43.295 - 00:56:11.795, Speaker D: Yeah, just to reply to Barnabas who said Latest already had QWIK in the. That's right. But yes, it's not in the specification. And that's why I wrote the specification just to be sure that every new client which want to add this Quick support use the same. Actually Lighthouse use this exact Quick Node record entry. And I just copied what Lighthouse used to do.
00:56:14.295 - 00:56:34.285, Speaker A: Yeah, makes sense. And yeah, I mean, I think there's a question of the actual identifier. I think what we have the QIC works. Cayman has a question. Do we need a separate entry for the V4 and V6 quick ports?
00:56:36.785 - 00:56:56.288, Speaker C: Yeah, we have quicks, we have quick six, but I was just looking at the specs. Like in the specs, we only have UDP and TCP, but I think there's TCP 6 and UDP 6. So maybe there's a separate PR for IPv6 support. But Lighthouse uses Qwik and Qwik 6 for the.
00:56:56.372 - 00:56:57.547, Speaker B: In our fields.
00:56:57.631 - 00:57:03.423, Speaker A: Is that in this PR? Do you know the Quick 6? No, it's not.
00:57:03.507 - 00:57:11.398, Speaker C: It's not in this PR. But we probably need to also add TCP 6 and you do P6 if.
00:57:11.482 - 00:57:13.916, Speaker F: We'Re going to do IP6 support.
00:57:14.000 - 00:57:15.595, Speaker A: But there is only.
00:57:15.679 - 00:57:19.372, Speaker D: There is only an IP6 field in the ENR.
00:57:19.456 - 00:57:28.145, Speaker B: So why do we need like TCP 6 and Quick 6? Like, because you just look at the IP6 NT.
00:57:28.965 - 00:57:39.225, Speaker C: Yeah, but if you have dual stack where you have IPv4 and IPP6 you can have different ports for different sockets. So then there's ambiguity there.
00:57:42.365 - 00:57:43.265, Speaker B: Got it.
00:57:48.535 - 00:57:52.555, Speaker A: Do we feel like we want to add the V6 identifiers?
00:57:57.655 - 00:58:00.035, Speaker D: I think we should do it in a new pr.
00:58:00.335 - 00:58:01.115, Speaker B: Yeah.
00:58:03.775 - 00:58:15.925, Speaker D: I can add it in this pull request. It's okay. Why do we need to do it in a new pull request?
00:58:19.785 - 00:58:51.615, Speaker A: One argument is it'd be simpler just to merge in the quick thing. I think we've been working on this for a little while and then otherwise, yeah, it might just add latency if there's back and forth on the V6 part. Okay. I think it also works in the same pr. So yeah, I think whoever is writing this can decide.
00:58:56.355 - 00:58:56.755, Speaker B: Okay.
00:58:56.795 - 00:59:05.435, Speaker D: So it's okay for me to add a quick six as well in this pull request. And I'm the author of this pull request. Just to be clear.
00:59:08.255 - 00:59:34.305, Speaker A: We have a thumbs up. Cool, thank you. And okay, last on the agenda, we had a request from. Probably they'd like to share some of the work they've been doing around disk v5 and some network analysis. Let's see. Is someone.
00:59:34.685 - 00:59:35.365, Speaker B: Let's see.
00:59:35.445 - 00:59:35.717, Speaker F: Yep.
00:59:35.741 - 00:59:36.061, Speaker A: Yeah.
00:59:36.133 - 00:59:36.461, Speaker B: Hey.
00:59:36.533 - 01:00:56.981, Speaker F: Hi, this is Gib from problab. So, yeah, I'd like to present what we've been working on with respect to monitoring the consensus layer network, especially the DCV5 DHT. We've also done some work on the gossip sub part, but Mikael will be presenting this work probably during the next call or the one after. So what we've been doing is we've been running a crawler that's basically getting a snapshot of the DCP5 DHT every 30 minutes. So just enumerating all the peers and trying to connect to them. So try to open a Liberty connection, but Also sending so dc5 pure UDP request to learn about the full routing table of every single peers. And this way we can just enumerate the whole network, get the enr, get all the information, we put everything in a big database and thanks to the LIPITP connection, we can also get some extra information from the identified protocols, such as the supported protocols, the multi addresses, so all the multi addresses that are supported and so on.
01:00:56.981 - 01:01:52.065, Speaker F: From all of these data that we gather, we can do some statistics then on cloud providers on where the nodes are located, on the protocol that are supported, and so on. So let me quickly share my screen and also send the link here. All right, so that's the product that we have. In the end, we build weekly reports, so we create one report for every week and we show a lot of data. So I will not stop on every single plot there. But for for instance we show the client diversity. Just one thing to be clear, that's counting the number of Beacon nodes and not the number of validators.
01:01:52.065 - 01:03:07.655, Speaker F: So you might be seeing different numbers from different sources. And we're showing the number of nodes, not the number of validators. So we can see distribution over time. We can also see the version distribution for the user agent, so can be helpful to see how your client is getting adopted. And so using this plot you can see for instance here the last version of Lighthouse has like another adoption rate among the lighthouse nodes of 29% and by the end of the week, okay, didn't grow much. But say if we take another one here, followed Star we can see that the latest version has quite some adoption during the last week. And yeah, then so we have other numbers and something weird so we don't need to address it now, but we have some numbers that we would like to flag and then we can take the discussion offline.
01:03:07.655 - 01:03:52.125, Speaker F: Is for instance here we see the supported protocols for all of the nodes. We can see that for instance 99.9% support the mesh sub 1.1 protocol which is kind of required. And then we can see the breakdown for each of the user agent and with all of the supported protocol. And we can see for instance for Taequ that Taequi is only advertising these protocols, at least through identify. So I'm not sure whether the other protocols are also supported but just not advertised or if they're missing.
01:03:52.125 - 01:05:00.285, Speaker F: So yeah, and then maybe one last plot. So yes, some things we can also do is checking here, so we can see in the distribution per country that here there's a small drop from the number of nodes that are hosted in the US and an increase in the nodes that are hosted elsewhere. And if we look. So I think yeah, it was again Teku nodes. So it means that they have been changed from a US location to elsewhere. And when looking at the cloud providers, because we know it's techu nodes, we can see that they've been taken from a data center in the US and that they are now hosted elsewhere, not inside the data center outside of the US in another country. So that's how we can use for instance the stats we're doing.
01:05:00.285 - 01:06:02.435, Speaker F: And the last thing that I wanted to touch upon is on the number of stale record. The way the disk V5DHT works is each node is going to keep connection to other nodes so they're going to keep their ENRs. And then upon request, they're going to share it to each other. And it turns out that around a quarter of the records they hold are pointing to peers that aren't responsive, so to peers that may be dead or offline. And that's not healthy in the network. So it means that we didn't do any analysis on which clients aren't pruning the records or if it's all of them. But it's generally not really healthy because when you use these, you want to discover new peers, you don't want to learn about some peers that are not there anymore.
01:06:02.435 - 01:07:05.151, Speaker F: And so that's a bit concerning, but it's not like a major red flag. So yeah, if you have questions, so we'd love to have feedback. And so we plan on adding a plot on qwik, showing so which clients support QWIK and so on in the future. And if there's anything, any other plot that you would want to see or if there's something useful you wanted to add, feel free to reach out and. Yeah, or if you think something is wrong, it's possible. So we've discovered recently that for instance, we weren't able to actually count the number of Lighthouse nodes that were supporting quic, so we retracted the plot. But so it's also possible that for instance, for the take note, we missed something and.
01:07:05.151 - 01:07:20.275, Speaker F: Yeah, so I think that's it. I'm gonna write in the chat the email that you can reach us at and if you have any question.
01:07:24.625 - 01:07:47.525, Speaker B: Yeah, the question that I asked in the chat, I found it strange that Lighthouse is the only one with this. With these curves reverted where there's more data center nodes than the non data center. Yeah. So why is this? I mean, is there any. It's striking.
01:07:51.005 - 01:07:51.333, Speaker C: Yeah.
01:07:51.349 - 01:08:19.195, Speaker F: So we don't really know why. So we basically our methodology here is to gather the IP addresses and then check whether the IP addresses are associated with any known data center or cloud providers. And then we classify it this way, whether it's data center or non data center. So I guess it depends on.
01:08:21.055 - 01:08:27.715, Speaker B: I can't even think of an explanation because also this is according to your numbers, this is the client that you have most data points.
01:08:28.415 - 01:08:43.375, Speaker F: Yeah, that's right. But so it means that overall around half of the clients are running from a data center and half of them not from the data center.
01:08:43.565 - 01:09:06.975, Speaker B: Yeah, no, so the total numbers is not something that strikes me, but it does strike me that if you look at Nimbus, then essentially every node is on a non data center. I mean it's a Huge difference. Prism is also higher, not that much higher, but it's higher for the non data center ones and Lighthouse is the only one that has that is connected in data center. So that's why I'm surprised.
01:09:08.445 - 01:09:32.865, Speaker F: Yeah, yeah. I mean I guess the question would be to ask to the operators of the actual nodes or I mean unless you know them and you're sure that most of the nodes are running from outside the data center and then we can always double check so the sources that we have to map IP addresses to data centers.
01:09:53.295 - 01:09:54.875, Speaker A: Some other questions in the chat.
01:09:55.615 - 01:10:22.145, Speaker F: Yeah, I see a question from Adrian. What classifies as unreachable in the dht? So we consider node as unreachable if we cannot open a lib2p connection nor send the UDP DC5 yeah packet connection. So in this case we consider a node as unreachable if we cannot do either of these.
01:10:25.085 - 01:10:27.305, Speaker C: If you can do one of them, is it reachable?
01:10:30.015 - 01:11:04.475, Speaker F: Yeah, so the condition if we can reach out the node only p2p it is reachable and if we cannot but we get but we can reach the node on UDP and then the error that we get from the lipid P connection. So there is an active error, not a timeout and the error is everything but prid mismatch, we count it as online. So yeah, essentially a UDP connection is enough to be consider as online.
01:11:06.255 - 01:11:07.395, Speaker C: Okay, thanks.
01:11:12.175 - 01:11:33.415, Speaker F: And then is this filtered by mainnet? Yes. So it's mainnet for all of the plots but the stale records one so we can see. So that's the one filtered by mainnet and this one is global for all of these five and for all of the other plots it's filtered just for mainnet.
01:11:48.165 - 01:12:07.585, Speaker A: Cool. Yeah, generally super nice work. The charges look really nice. I like the version thing because we have this question with hard forks. We tell everyone to update but then don't necessarily know if people have updated in time. So I could see this being useful for that. Just watching the data update week by week or maybe even day by day.
01:12:08.485 - 01:12:36.015, Speaker F: Yeah. Also concerning hard fork, we wrote the blog post here. So showing exactly the same data where we can see so nodes leaving the Capella network and then joining the NAB and some of the nodes that are still remaining in Capella after the hard fork and so on. So yeah, if you want to have a read, just check it in the chat.
01:12:40.645 - 01:12:48.705, Speaker A: Cool, thanks. And yeah, if there's any feedback, is this email the right place? TeamReblab IO.
01:12:50.605 - 01:12:51.985, Speaker F: Cool, thanks a lot.
01:12:53.045 - 01:13:06.385, Speaker A: Yeah, thanks. Okay, we have a Few minutes remaining. Any other closing comments? Otherwise we can wrap up a little bit early today.
01:13:11.145 - 01:13:59.725, Speaker E: I have one more thing for the requests, like the validator operation request and there is this disconnect still between the EL which has this requests list that contains all of deposits, withdrawals and consolidation requests, and on the CL where we have these three separate lists in the execution payload. So I wonder whether we should put those separate lists into a sub container just so that we have like when we later extend it with additional request types that they still conceptually are grouped together. I have linked it in the chat as well.
01:14:01.865 - 01:14:09.885, Speaker A: Yeah, thanks. Yeah, I think both these came up on a past call and yeah, I'm not sure people have had time to review.
01:14:11.545 - 01:14:20.849, Speaker E: The last call was for the Engine API and this one is something similar for the CL data structure.
01:14:21.017 - 01:14:26.865, Speaker B: No, no last call. We discussed them both and it's important on the CL data structure itself.
01:14:32.045 - 01:14:35.533, Speaker A: Okay, right. So I mean, actually my take on.
01:14:35.549 - 01:14:59.915, Speaker B: This last time we were, we were supposed to open that issue. We opened it and the idea was that we were going to discuss it there. My feeling about this that, well, Misha doesn't like this, but everyone else seems to like it. But there isn't enough feedback in that issue. So at least we should have at least someone from each client signaling something in that issue so that we can actually go ahead or not.
01:15:07.135 - 01:15:16.855, Speaker A: Okay, so can CL teams take a look? I'll try to follow up independently with everyone before the next click and we can make some progress then.
01:15:17.595 - 01:15:40.615, Speaker B: So there's something to note. This change is a very simple change, perhaps in our code base, but it's a pain in, well, to write spec pr. I'm definitely not writing that spec pr and it's really going to be painful to write that spec PR and changing all the Python tests. So it should be done quickly if we're going to move. If we're going to move to such a thing.
01:15:47.865 - 01:16:34.045, Speaker A: Got it. Okay, well, I'll take a look at these and I will try to follow up independently with the client teams and yeah, let's just try to make some progress here. I don't think there's anything new to add otherwise. Unless you see something else. Eton or potus. Okay, yeah, thanks for bringing these up. Well, let's not drop the thread there.
01:16:34.045 - 01:17:03.955, Speaker A: And otherwise. Yeah, Anything else? Let me double check the agenda. People like to add comments as we go. Okay, yeah, I think we've covered everything and yeah, unless there's anything else. Going once, going twice, going three times. Okay, then I think we'll go ahead and wrap up. Thanks, everyone.
01:17:03.955 - 01:17:05.095, Speaker A: See you next time.
01:17:06.195 - 01:17:07.215, Speaker B: Bye, everyone.
01:17:07.555 - 01:17:07.795, Speaker C: Thanks.
