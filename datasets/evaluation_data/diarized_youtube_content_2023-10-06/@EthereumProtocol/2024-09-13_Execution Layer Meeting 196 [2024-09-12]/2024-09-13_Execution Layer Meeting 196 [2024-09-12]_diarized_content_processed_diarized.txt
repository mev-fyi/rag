00:00:00.160 - 00:04:34.420, Speaker A: Sadeena. Sadeena. And we're live. Welcome everyone to Acde number 196. Today we'll talk about Petra as we have been lately. Devnet three got this week, so we can chat about that, then we can continue the conversation. We started on the CL call last week around all of the encoding changes for the different eips.
00:04:34.420 - 00:05:16.420, Speaker A: There were two other picture eIP issues that people put on the agenda. So one around 7702 signature checks and then one around the pricing of some of the BLS precompiles. And then probably the biggest thing, trying to wrap up the conversation around potential additions to Pectra. So we discussed those two weeks ago and then some teams have already shared their views. And then lastly, there's just a small heads up around the way the network configs are being documented in the different repos for mainnet and testnets. So we can just. Yeah, quickly covered up.
00:05:16.420 - 00:05:22.340, Speaker A: But to kick us off, Perry, do you want to talk through the launch of Devnet three?
00:05:23.600 - 00:06:01.420, Speaker B: Yes. So we finally have Devnet three up and running. The main change from Devnet two is the fix to the consolidations issue, as well as adding 7702 since yesterday. We've also tested that consolidations work as expected now, and they do. And we have had some 7702 calls and they also work as expected. We're currently seeing some issues with Nethermind as well as Ethereum J's, and I think both the client teams are aware and looking into it in case someone from the client team wants to mention where there are. Maybe now's the time.
00:06:03.760 - 00:06:06.728, Speaker A: Yeah, I think our issue was fixed.
00:06:06.904 - 00:06:43.176, Speaker B: So I see that in fork monitor our nodes follow the chain of nice after the update. So that's good to know. And besides that, there was one minor thing I wanted to bring up. Are ER teams under the assumption that we're running the spec version 1.5 or 1.4 purely for updating the spec document? It seems like there isn't any material difference, but yeah, just so that we have it on record properly.
00:06:43.368 - 00:06:45.260, Speaker A: You mean CL teams, right.
00:06:45.720 - 00:06:48.312, Speaker B: El teams. There's also the execution spectrum.
00:06:48.456 - 00:07:02.420, Speaker A: Okay. Also executed test. Okay. Yeah. Aragon was under the assumption of running 1.5 for Demnet three.
00:07:05.450 - 00:07:06.346, Speaker C: Okay.
00:07:06.538 - 00:07:14.634, Speaker B: I think according to Mario's message it was also that they're mostly the same except for a few extra tests and disallowing. Yeah.
00:07:14.682 - 00:07:15.418, Speaker A: Okay.
00:07:15.594 - 00:07:24.910, Speaker B: So I just update the spec version so that it's also clear for people looking at it. I think that's it from the Devnet side of things for Petra.
00:07:26.850 - 00:07:31.590, Speaker A: Thank you. Anyone else have comments, thoughts, questions about.
00:07:37.240 - 00:07:41.980, Speaker B: I guess if you have a wallet developer you like, please ask them to test 7702 now.
00:07:42.560 - 00:07:43.024, Speaker A: Okay.
00:07:43.072 - 00:07:50.312, Speaker B: Faucet is running so you can get funds and you can deploy contracts and you can, I guess, make it awesome.
00:07:50.336 - 00:08:45.154, Speaker A: Yeah, that's great. So we'll give a shout out to the call, but yeah. And yeah. Perry, do you mind sharing the link for just the Devnet config page in the chat here? I believe that also has a link to the RPC on it, as well as all the other things like the spec or adding it to metamask. And thanks, PJ. Anything else on Devnet three? Okay, if not, then moving on. Next, we had all these proposals to change the encoding of the data that's been passed from the El to the CL we discussed on the CL call last week.
00:08:45.154 - 00:09:16.510, Speaker A: And then Felix said he'd open up some prs for them, which he did. And effectively, I believe all of the prs do sort of the same change just in the context of the different eips. I know there's been a bunch of conversation about this on the discord in the past day or so. I don't know if Felix is on. Yeah, Felix, you maybe want to start by giving a bit of context on the prs and then we can get into the discussion that we were having on the awkwardevs channel.
00:09:17.130 - 00:10:09.344, Speaker C: Yeah, so, I mean, there are two ties to this, and the discussion we've been having is mostly in on the like CL side. So this being Acde, I think the changes that I'm proposing are like, I think very uncontroversial for the, for the El. So basically this is going back to some stuff that we've been proposing kind of before. So in the. It all kind of started when we, when we were seriously reviewing the implementation of the requests mechanism in geth. And we kind of noticed that there are two big problems with there. So one problem is we used to be able to turn a block into executable data and back with basically no knowledge of the fork configuration.
00:10:09.344 - 00:10:43.938, Speaker C: And this is something we would like to preserve. So at the moment, when you create a block, you can sort of pack things into it and without having a lot of context. So it's kind of a freestanding object for us. And this would change a little bit with the introduction of the requests if they were to be done as separate lists. So we, like Matt originally just proposed turning requests into a single list. And on the Cl side, this was met with some criticism. And I mean, when I started reviewing his implementation of it.
00:10:43.938 - 00:11:48.730, Speaker C: I also kind of felt it just like the El just has way too much knowledge of these requests, because in the end, the whole, the point of the requests construction is that it's some data, which is technically, I mean, it is generated by the El, but the El shouldn't really know too much about what's inside of these requests, because they are supposed to, just supposed to be for the CL to process, and they are meaningless for the els. So in the changes that I'm proposing now, we're basically removing responsibility for the EL to ever decode these requests. What we want to achieve is that the EL should call the system contracts and then whatever is returned by the contract. The only thing it has to do there is basically just, it has to turn the buffer returned by the contract. It has to turn that into a list, and for that, it has to know the size of each output request. So that's like one data item we still have to track. But other than that, we don't really have a knowledge of the structure of the returned requests.
00:11:48.730 - 00:12:09.390, Speaker C: We turn them into a list of just byte arrays. And then basically we are supposed to return them to the CL's for processing, and they will be put into the EL block as well. Reporters, I can quickly finish my description, or do we have very urgent question?
00:12:10.050 - 00:12:12.938, Speaker A: Yeah, you can. I'm just in line. Yeah.
00:12:12.994 - 00:12:13.402, Speaker C: Okay.
00:12:13.466 - 00:12:14.426, Speaker A: Finish, Alexa.
00:12:14.538 - 00:13:02.824, Speaker C: Yeah, so let me just finish the tangent. So basically, the whole point of the, of the prs to change the eapsheen is to just make it so that the output format is exactly equivalent to what is returned by the contracts. So we don't have to parse them at all. We basically just pass them through. And then there is one PR to actually update the request eIP itself, because it turns out that it was defined to use this tree hashing function that we also use for receipts. But actually, we realized that it's totally unnecessary because nobody is supposed to make a proof against this root hash. The requests root hash in the El block is meaningless because the requests that are in there have not been validated for correctness by the Cl.
00:13:02.824 - 00:13:58.534, Speaker C: So, for example, let me remind you, for example, for the withdrawal request, any withdrawal request can be submitted on chain, even for a validator that doesn't exist or has already withdrawn or whatever. Just this request alone is meaningless. And just creating a proof of that would actually be kind of dangerous, because then you'd be proving that, like, this request exists in the El chain, but you don't know if at that block, the request is actually valid or the validator can cover it or whatever. The thing that people should make proofs about is when it's processed and validated in the Cl. And for this reason, the validated request, they will be put into the CL block and as an SSE list. So there can be an SSE merkle proof about these objects. And so for this reason I am proposing that we change the hash, the request hash that we put into the El block header.
00:13:58.534 - 00:14:20.586, Speaker C: We change that into a flash. And this is similar to how we treated the, like the omers they're called. Basically it's just a hash of the. Listen, instead of having this like Merkel tree construction there. Anyway, so this is my overview of eIps, and now I'm ready to answer questions.
00:14:20.738 - 00:15:21.500, Speaker A: Thanks, Felix POTUS. Yeah, so I had a quick question. So you're calling the system contracts, and I assume that you're calling them in the order that you're getting these requests in the block. You're calling the system contract, getting whatever is returned. You treat it as a flat slice of bytes, and then you want to store this somewhere so that you send it to the Cl. What I don't understand is if these system contracts are different system contracts and you know which ones they are, because we are prefixing the return with whatever, a number that just indicates which system contract was. Why would it be difficult for the EL to just collect many different lists and just put them in the order that you're getting? So instead of using the index as a prefix, use the index as the end, the index in the outer list, to put the complete the inner list with a flat array.
00:15:22.160 - 00:15:55.008, Speaker C: Yeah, I mean, this is something that if you guys absolutely want it, we can have it as a separate list. But the way I see it is that we can make this list of list construction. But I would. So from our point of view, yes, there is a point in the code where we will be calling the individual system contracts. And we know there we have the fork configuration, so we know which ones we should call. But we also have, obviously we have the knowledge of like which contract is it and so on. So there is some, some knowledge there.
00:15:55.008 - 00:16:31.402, Speaker C: But what we want to avoid is like having any more knowledge of the system outside of that. So there's a lot of other places in the client code basis where we have to deal with these requests. And we just want in these places to have as little complexity as possible. It has not. Yes, there will be like when we are actually making these system calls, we have all the knowledge that's required, but it's just more of like a convenience for us. That's why we proposed this like flat list. If you must have it as three separate lists, it can be arranged, but we would be happier to just have it as a single list, to be honest.
00:16:31.586 - 00:17:15.390, Speaker A: So if I could just make this quick comment. Is that so someone will do this? Like either you do it or we will do it, but we will definitely grab this list which is prefixed and then make this list of lists. Anyways, the one advantage that I see of having already the list of lists on this API is that if we ever move to SS, that's exactly what we will use in the SS object, that will be a list of lists. So that's perhaps why it's more future proof to get it on the API as list of lists. But as I said before, Prism will not disagree with implementing this. If this really makes the El life.
00:17:15.690 - 00:17:53.210, Speaker C: Easier, it kind of does make it a lot easier because we have this, we, we basically don't have to know anything about the request further down the pipeline. We only have to know about the requests in this one place where we actually perform them. And yeah, from that point onwards we just don't think about it. And this also goes, for example for decoding blocks and things like that. Like whenever we, we have to deal with a block we assume that yes, there's this list of opaque objects there. Sorry Tim, what was the question?
00:17:53.910 - 00:18:20.400, Speaker A: Sorry, Rhett said in the comment they already do this, but I'm wondering if by this they mean compiling the three different lists. Yeah, yeah, we. So because we need to deserialize the RLP to get a JSON representation of each of the requests. These are different types. So we end up doing three different lists and then doing a wrapper type and then collating it into one list.
00:18:20.520 - 00:18:50.884, Speaker C: Yeah, this is exactly what I meant. So all of this complexity is totally unnecessary for the El. And this is why I'm proposing to remove. And yes, on the Cl side it will mean that you have to pick apart this list and figure out by the prefix. But I mean, to be honest, it is, this information is like kind of predated. So there won't like, in the normal case there won't be any like wrong. I mean the requests won't really come out of order.
00:18:50.884 - 00:19:22.356, Speaker C: So for example, because they are just generated in the right order by the ELS. I mean we can argue if it, if it somehow improves. So if there is a specific proposal that would make it way better for the CEO's then I'm happy to integrate it as well. But as it stands, the way we see it, it's the best for us and that's why I propose it. So, yeah, like we need to. I don't really know how we should put this to a vote. I do think that we should kind of decide this today because.
00:19:22.356 - 00:20:11.670, Speaker C: Yeah, I mean it's, we're sort of like in the, kind of late in the, in the whole like fork changing game even though the fork probably not going to happen this year. But I still think that like if we gonna have the next Devnet or something, we should probably have it in by then. So. Yeah, and I have created a separate pr to change the withdrawal system contract to return the output in a way that's more friendly to the, to the CL specifically. There was this one field in there that is big Enyan and I've changed it to little because it makes it easier for the CL's to decode it. And this should probably then also be integrated into the EAP and stuff. So I would have to update my pr to include the new system contract code into the EAP.
00:20:13.930 - 00:20:28.800, Speaker A: There's a comment by Destin about the ordering as well, and that if we do move forward with this, we want it, we want to have like effectively the ordering guaranteed and adding this in the, basically the engine API spec.
00:20:29.540 - 00:20:29.972, Speaker C: Yeah.
00:20:30.036 - 00:20:33.120, Speaker A: Does anyone have any issues or concerns with that?
00:20:35.620 - 00:21:08.740, Speaker C: I mean, it's simple for us because we can literally, I mean, I'm totally happy to just put it into the spec. Like right now, the sentence I would add is something like, the requests have to be ordered by type and within the type, they have to be ordered in the same way that they were returned by the contract. But honestly, this is just what's going to happen automatically because during the processing, we create them in this order. So the ordering is, I feel, fully defined by the semantics of creating the request.
00:21:10.160 - 00:21:33.670, Speaker A: Right, okay, so if we make this explicit in the spec, as I understand it, it's like, yeah, clients already sort of do this, but then we should make it explicit to make sure that all the clients follow this. So yeah, I think if we can add a pr to the engine API that just adds this.
00:21:36.210 - 00:21:48.660, Speaker C: Yeah, I mean, I'm fine with adding that into the existing pr. I mean, this is definitely, I absolutely not against adding as much text as you guys want about this in the specification.
00:21:49.640 - 00:22:13.510, Speaker A: Okay. Okay, perfect. So let's, I guess this like concern aside, which seems like it can be resolved. Does anyone have like a strong objection to moving forward? With this. And I know, like, yeah, the issue is more like a cl one around the decoding. And I don't think Tursek is on the call. He's the person who left the big comment.
00:22:14.930 - 00:22:16.410, Speaker D: Yeah. Oh, hey.
00:22:16.530 - 00:22:18.738, Speaker A: Hi. Yes.
00:22:18.834 - 00:22:19.510, Speaker D: Yeah.
00:22:19.850 - 00:22:39.150, Speaker A: Okay. Yes, yes. Sorry, I didn't, yeah, I had a brain freeze between you, the GitHub handle and zoom name. So. Okay, so aside from the specification being more formalized, are you okay if we move forward with the el not compiling the individual lists by default?
00:22:42.690 - 00:23:26.570, Speaker D: Well, okay. It would be an acceptable compromise, I guess. But that is, I really, I mean, I see this, all this really really dismissive stuff about, oh, the executioner edge API just provides what's in the, what's in the, what the EL is doing. We can, I think I need to repeat this. We absolutely must not and cannot rely on geth does this right or ref does. This is, this is not, this is an absolute non starter. I really need to emphasize this.
00:23:27.430 - 00:23:35.370, Speaker E: This is not how it's specified. It says very specifically in each EIP how the request should be ordered within the list of requests.
00:23:36.630 - 00:23:44.110, Speaker D: It doesn't address the, I assume you read my GitHub comments that it does not address the interleaving case. It doesn't.
00:23:44.150 - 00:23:57.650, Speaker E: I specifically, it does, it does, it does explicitly say the interleaving case. It's exactly like Felix just said. You order by the type and then intra type is defined by the EAPS 6110 7002 Max EB.
00:23:58.510 - 00:24:01.826, Speaker D: Yes. Within, that's, within the El block. That's.
00:24:01.898 - 00:24:09.190, Speaker E: And the engine API is defined to send the data from the El block over to the CL in the same way that's ordered in the El block.
00:24:10.330 - 00:24:12.470, Speaker D: Not precisely enough.
00:24:13.010 - 00:24:51.634, Speaker C: Well, I mean, we can add this, like, look, Dustin, it's fine for me to add the sentence. The intent of the change is to relay the request exactly as it is in the El block. Because this is what creates a simplification for us. Like this is why I'm proposing this, because then we will be able to just put into the engine API what we have in our block object and we don't have to think about it like anymore. I understand your concern that these are different specifications and they each need to be complete. And I will add the sentence. So given that this sentence will be added.
00:24:51.634 - 00:24:54.970, Speaker C: Do you have any other comments about this.
00:24:57.870 - 00:25:37.786, Speaker D: Beyond that? I think, I mean, some I've said before and that POTUS has, I mean, I have, I have other concerns, but, and I do, I do want to echo what POTUS was saying, for example, about the essentially this is not SSC friendly, I know you've already, or, well, it has been responded to in other ways in other ACD calls, but it would borderline work.
00:25:37.978 - 00:25:38.710, Speaker A: Sure.
00:25:39.250 - 00:26:40.656, Speaker D: It's not difficult to parse exactly the other thing that needs to be defined here, and this is not just an ordering issue, I'll say is, and I actually did not see this defined in the El semantic or the engine, the L semantics either, is the invalid type thing. So what now, what happens, for example, if you got a type, this is both meant as a general question, but also, I mean operationally, if you want to answer, I guess. But that right now we have three request types currently, and what if somebody that, what if the Cl sees something with a request type numbered three, like a fourth request type coming out of the El, what is it? I mean, is it defined what the Cl is supposed to do? Because this becomes, as I mentioned there, this is a consensus issue.
00:26:40.688 - 00:26:41.168, Speaker A: Issue.
00:26:41.304 - 00:27:46.230, Speaker D: Now one dismiss this as a oh, that's buggy El. I mean, assuming it's premature, we don't have one yet, but that's another example of on the execution API. This is maybe not a concern for the El, I understand. For the El, as you have said, they're supposed to be black boxes, they're supposed to be opaque, and the EL doesn't actually care just conveying some bytes. But obviously the Cl does care about the semantics of this and the CL does define the consensus. And so there are questions that come up on top of this as well that I think should be resolved, and should be resolved hopefully sooner than later, for honestly, broadly the same reasons as I think you have been pushing to get this in this sooner than later, which is to say consensus issues of this sort are things we want to discover early in testing, not, not late.
00:27:46.850 - 00:28:44.000, Speaker C: Yeah, so I think that what we can add a bit more text on the pull request onto engine API as well. So I see it is that obviously the correct answer is for the CL to reject the block if it contains requests that it cannot sort into any of the available request lists into the CL box. So the algorithm that the CL has to perform is basically when it block information from the EL, it has to go through the request and it has to sort of put these, I mean it has to validate the request and then it has to put the valid ones into the CL block enclosure. And if it finds a request that isn't supported under the current fork, then obviously it's an invalid block. So it has to be treated as invalid in terms of the fork choice. But I think this is somewhat similar to if, for example, the El had returned that the block is invalid. So it has to be treated in the same way.
00:28:44.000 - 00:28:50.280, Speaker C: Whereas like, yeah, it's basically the same as having an invalid execution result or something.
00:28:52.180 - 00:29:13.664, Speaker D: Okay. I mean, that's very reasonable. That's my inclination personally as well as to say if any, any single invalid thing makes the whole thing invalid. That said, there's precedent. I'm not, you know, so that the invalid deposits don't make an entire block invalid in the Cl, for example. And I'm not saying that here, but.
00:29:13.712 - 00:29:39.472, Speaker C: I'm saying that's an interesting point. Yeah. Actually that I don't really know how it works for these. I mean, if you make, how does it actually work? Maybe you can quickly give a comment. I'm sorry for stretching this out so long, but this is honestly like what happens if the, like if a withdrawal request is submitted, for example, that isn't covered by the validator, then it's just ignored. Or, I mean, it doesn't obviously mean that doesn't lead to the block being invalid, but it's another kind of invalid. Right.
00:29:39.472 - 00:29:41.460, Speaker C: So you basically just discard this.
00:29:42.440 - 00:30:58.692, Speaker D: Yeah, basically for the request, invalid requests are skipped. Yeah. But whatever we do, I guess my point is, and I take your point, but not wanting to stretch this, but I do regard this, I'm bringing that specifically in this context because I regard this as part of this at some level. I mean, PR that you're trying to get in, which is to say if there are new error conditions that are not possible now that are possible after your pr, then those should be discussed as part of getting your pr in. And one of them is, for example, you know, this, let's say this, this interweaved case, you can, I mean, does maybe you say maybe that's just an automatic reject, but then that should be again specified. I mean, I know we already, but specifically a specific aspect of that to say that is an invalid thing somehow that should be specified because otherwise the risk is we get people trying to follow postal law, be liberal and what you accept thing. And this is dangerous in a way.
00:30:58.876 - 00:31:00.760, Speaker C: I absolutely agree with this. Yeah.
00:31:02.300 - 00:31:38.030, Speaker D: So if we want to prohibit helpful quote unquote Cl's from saying, oh well, we see request type one, then two, then zero, then one. Let me just sort that for you. Like we, because the first one who sorts it, they'll be more compatible with, with other, with random bug. And this is, and now that's, now everyone has to do that now we get HTML five, you know, in adjacent RPC. And so like, this is something that has to be prevented upfront from, you know, day zero.
00:31:39.810 - 00:32:00.842, Speaker C: Yeah, I understand this. And this, these concerns I will deal with by just adding additional text. But you are right that these things exist. And maybe like some of that wouldn't exist as much if we had put it differently in terms of the encoding. So it adds like these cases to the consideration. I agree with this. Okay.
00:32:00.906 - 00:32:02.550, Speaker D: Okay. Yeah.
00:32:04.090 - 00:32:04.946, Speaker F: Can I.
00:32:05.058 - 00:32:05.750, Speaker A: Yes.
00:32:06.530 - 00:32:50.450, Speaker F: Yeah. So the position from Eels perspective on this is that when you execute the block, you deterministically generate a set of requests that are put in the block, and there is no choice here or no discretion at all in the execution. You run the execution, you get a list of requests. And if the list of requests in the block is different than the one you generated during execution, including if it contains extraneous requests, or if it contains the request in a different order, it's just invalid. So there is only one valid form of the requests. So the El, unless it is broken, guarantees that what order the requests come in. And there's only one deterministically valid order.
00:32:50.450 - 00:33:58.284, Speaker F: And the second thing, so from our point of view, if it's an ordering issue, it's broken. And if I, the Cl, received something from the EL and it concludes that the only possible way this could happen is that the El is broken, it should reject the request and complain. The other thing that this was compared to is there are other situations where a request can be invalid, but in a way that the EL can't tell. The EL does not check deposit signatures, it does not check whether withdrawal requests are valid. So it just passes them on to the CL, and the Cl processes them if they're valid and ignores them if they are invalid. So yeah, I don't think this should be an issue. It should just be clear that there is only one acceptable order ring that the El can produce and that the Cl can rely on this if it wants to, and if it detects these, this has not happened.
00:33:58.284 - 00:34:01.360, Speaker F: It is welcome to just project the request, as in that.
00:34:05.020 - 00:34:41.840, Speaker D: Okay. I mean, I think as long as that's codified, that that mostly resolves the specific concerns I listed. I mean, it still strikes me as a kind of pointless, pointlessly flexible, like is this a false flexibility as described? But sure, if one wants to insist on having that falsely flexible list and then, you know, ad hoc constraining it, or rather post hoc constraining it, I guess that that works.
00:34:44.100 - 00:35:44.906, Speaker A: Okay. Yeah, in that case, I think we should. Yeah, wrap up this discussion this point. So as I posted in the chat, I think the next steps here are making the changes to the spec, specifically just having some stronger wording around, ordering guarantees and how we deal with invalid requests, making sure that's reflected not just in the eips but also on the execution APIs. And then hopefully we can get these merged async in the next few days. But let's try to get this done at the latest before next ACDC so that we have like the finalized set of specs for all these eips and the engine API as it relates to encoding these requests by next Thursday. Does that sound reasonable to people? Any concerns, objections? Thoughts? Okay, thanks everyone.
00:35:44.906 - 00:36:04.910, Speaker A: And yeah, thanks Felix and Dustin for going to the details around the implementation specifics. Okay, next up, 7702. I forget who put this on the agenda. Oh, Andrew, are you on?
00:36:06.610 - 00:36:07.546, Speaker C: Yes, I am.
00:36:07.658 - 00:36:08.350, Speaker A: Yeah.
00:36:10.050 - 00:37:08.170, Speaker C: Yeah. So basically at the moment how 7702 is specified, it restricts one part of the signature, namely s. It should be, it says that s must be less or equal than this sec. Two hundred fifty six k one n divided by two as specified by eap two. But the problem is that eap two is actually a refinement of a pre homestead check that actually puts some constraints on r as well, as I mentioned in the PR. So we should either make it consistent and check that the signature, all signature values valid. And yeah, there is for like, you can look at the method in gas.
00:37:08.170 - 00:38:19.870, Speaker C: It's kind of basically it has two flavors, pre homestead and post homestead. And we should use post Homestead. Also, there is an alternative option by lite client to actually, actually not check authorization signatures for validity only. Check that. Like that, say o signature values, namely y parity, r and s are fit into 256 bits. And basically I think like the say in 7702, that in case of invalid signature values outside of the EAP two range, the transaction is not invalid itself, but that authorization will be ignored when executing the transaction. I don't care much which option.
00:38:22.050 - 00:38:22.386, Speaker A: Is.
00:38:22.418 - 00:38:28.150, Speaker C: Adopted, but I think like how 7702 is specified now is inconsistent.
00:38:29.090 - 00:38:34.590, Speaker A: Thanks. Yeah, lightclient, do you want to just give a bit more context on your pr?
00:38:36.210 - 00:40:08.046, Speaker E: Yeah, so I think originally there was a pr that I merged by Dragon that added some, basically some instructions about what the serialization should look like and some reasonable bounds for the values in the 7702 transactions. And when I merged, I don't think I quite understood the implications it would have on the validity of the transaction. And I've generally had this idea that the transaction, the validity of the transaction should really just be based on whether the sender of the 7702 transaction can pay for the authorization that exists in the transaction, and it has a correct nonce. MYPR is basically trying to bring us back to that world where in the pr that I had merged from jargon that said, some specific sizes of the different types of the transaction, it also included this signature check that exists for the authorization list. In my perspective, I don't think that we need to have these validity checks because it really just adds complication to the protocol that it doesn't really feel necessary. Because if the sender of the transaction can pay for an invalid authorization, then we should let them pay for the invalid authorization. They're incentivized to not send junk data to the chain, but the data should be priced in a way where if they do do it, then it's cost correctly.
00:40:08.046 - 00:40:54.720, Speaker E: It's just the same as sending no op transactions to the chain. I think by having the validity checks we have to iterate through all the authorization lists before we accept the transaction. And it just adds a lot, it adds a lot more testing that we need to do to make sure that all of the different validity checks are correct. So my preference is to remove as many of the validity checks as we can. I think in my pr I've just made the validity depend on the data, the width of the different data types. So I think most things are just U 256. That's what I would prefer to go through rather than fixing what exists here, which was what Andrew's pr was doing.
00:40:56.420 - 00:41:00.160, Speaker A: Thanks. Does anyone else have strong opinions?
00:41:01.660 - 00:42:07.030, Speaker F: I'm very keen on the approach, but like Tran suggested, it's probably worth pointing out. What the issue actually is here is that there has to be two stages of investigating these authorizations. Firstly, we have to basically pass them and make sure that they are actually authorizations and that they've been paid for. We can't allow people to stuff arbitrary data of arbitrary size in there, for obvious reasons, but we can't actually validate the authorization because that requires doing some curve math that's too expensive, and we don't want to require people in the mempool to go and do large amounts of curve math in the mempool. We just want to do one check, make sure the thing is paid for, and then include it. LiteclientPR takes a very sensible approach that basically we just pass the request, we take the request, we pass it if it doesn't pass because one of the data types is too big or the structure is wrong, then the entire transaction is invalid because the RLP structure of it's invalid. But we stop strictly there.
00:42:07.030 - 00:42:24.930, Speaker F: We do not investigate at all anything other than is stuff the correct size. And I think that the advantage of that distinction is that it's a very clear distinction. It's is this check, does the transaction pass? Rather than are these authorizations valid? So I would support lifeline's approach.
00:42:27.070 - 00:42:43.586, Speaker A: Thanks. Does anyone disagree or thinks another approach would be better? I think I generally agree with the approach. I'm just wondering why the v component.
00:42:43.618 - 00:42:47.002, Speaker E: Of the signature needs to be 256 bits.
00:42:47.186 - 00:42:51.110, Speaker A: Why would it need to be constrained to that? Is it not in practice a lot smaller?
00:42:53.220 - 00:43:25.430, Speaker E: The main reason is just this has been the signature width that we've accepted in the past. And I think it's much better to reuse all of the exact same signature tooling rather than have multiple signatures in the protocol where we have different widths that we're accepting. So if we wanted to change the signature with I would rather that be separate EIP to do it across many different signature types.
00:43:27.050 - 00:43:36.430, Speaker A: So you're saying it's because we have an eap where we already do this constraint and it's 256 bits in those eaps.
00:43:39.010 - 00:44:29.540, Speaker E: I think all transaction types V is allowed to be up to U 256. Like in practice, I don't think it's really ever valid because the only way you would get that high is if you had a chain id that high. So there is kind of this implicit, there is an implicit idea that it is the size. But yeah, I could be convinced. I could be convinced to reduce the size of van. I in general don't think it's really that necessary because people will pay for the data, people will overpay for that data, and it seems like an extra check that's not really doing that much. The signature won't be valid.
00:44:29.540 - 00:44:43.880, Speaker E: Ultimately, it just feels like we're adding an extra way for a transaction to become invalid, that it doesn't seem super necessary. I guess that's generally my take.
00:44:44.340 - 00:45:09.830, Speaker A: I think let's discuss async and then I'm fine with merging this. If it's like, if this is already the standard, then I won't push too much back on it. But I think in practice, obviously it's not 256 bits and it does have some implications for performance. But I don't think it's so major that we should luck on it for now.
00:45:10.770 - 00:45:14.070, Speaker E: Yeah, let's look into it a little bit. Async. Thanks.
00:45:14.450 - 00:45:26.950, Speaker A: And I guess if we're arguing about potentially reducing it. We could also bias towards merging this and then adding the more constrained version after.
00:45:28.530 - 00:45:44.642, Speaker E: Yeah, that's my preference. I think in general people support this direction. So let's merge this after the call and expect that we'll have these changes for Devnet four. And in the next week or two we can look async about constraining the size of v. Yeah.
00:45:44.826 - 00:46:32.446, Speaker A: Does that make sense to you, Oliver? Yeah. Okay. Anyone disagree with merging light clients pr, which would supersede Andrew's and then using that as basically a spec for Devnet four, assuming we don't make further changes. Okay, awesome. Then let's move forward with that and continue the discussion about the different bounds on the v value. Yeah, thanks Andrew and my client for bringing this up. Then last issue about an existing picture EIP, the pricing for the, for the BLS precompiles.
00:46:32.446 - 00:46:37.370, Speaker A: I don't know, Jared put this on the agenda. I'm not sure if Jared is on the call.
00:46:38.950 - 00:46:40.050, Speaker E: Yeah, I'm here.
00:46:40.590 - 00:46:41.630, Speaker A: Do you want to walk through this?
00:46:41.670 - 00:48:21.570, Speaker E: So, yeah, totally. So, yeah, well, benchmarking the performance of the various precompiles and the geth implementation, I discovered that the MSM precompiles use multithreaded execution and that reducing them to single threaded results in performance that makes the, that makes them heavily underpriced relative to both the other BLS precompiles and the EC recover precompile, and especially for the case of the g one. And so, yeah, I mean, this is just our implementation that I benchmarked so far. But yeah, but basically I want to broach the idea that we should reprice these against a single, the performance of a single threaded implementation, because introducing concurrent execution to the EVM kind of breaks a previous precedent that we've set. And not only that, but it essentially takes resources that could otherwise be used by the client and does not pay for them. So. Yeah, so as far as like how to reprice them, the easiest idea that I came up with is just to across the board.
00:48:21.570 - 00:49:27.540, Speaker E: So just to recap, in the pricing model for the MSM precompiles, there is this table of values, a discount table. And that is basically if we scale that by a factor of two, that would be the easiest way to bring the price up to the level of the EC recover precompile in the worst case. And then obviously if we wanted to get more complex, we could change each entry of the discount table and price it to, and set the entries to target the performance to some target. Like, I don't know, for example, the EC recover pre compile. So. Yeah, yeah, that's kind of what I had to say.
00:49:29.920 - 00:49:44.678, Speaker A: And just one, one quick, yeah, one quick comment. When you say, you know, increase the discounts by a factor of two, I assume you may have make like reduce the discount by 50% for those curves, make them twice as expensive.
00:49:44.734 - 00:49:53.126, Speaker E: So the naming of the table is a bit of a misnomer because the price scales inversely with it, but. Yeah, exactly. Make them twice as expensive across the board.
00:49:53.198 - 00:49:56.302, Speaker A: Got it? Yeah, just wanted to make sure we're here there. Thanks.
00:49:56.486 - 00:49:57.250, Speaker E: Yeah.
00:49:59.350 - 00:50:21.620, Speaker A: Anyone have comments, questions, thoughts about this? Yeah, there's a comment about checking other implementations and then plus one by nethermind, I guess. How easy is it for clients to, how easy is it for clients to replicate these benchmarks?
00:50:23.920 - 00:50:39.018, Speaker B: So we have solution. We created a tool that can benchmark all clients some time ago, but we haven't written BLS test here.
00:50:39.114 - 00:50:41.770, Speaker C: So if we do that work, then.
00:50:41.850 - 00:50:44.790, Speaker B: We will have benchmarks across all the clients.
00:50:46.330 - 00:50:47.190, Speaker A: Okay.
00:50:49.290 - 00:51:11.920, Speaker E: If I can just jump in also, so the benchmarks I've implemented, I've also translated them to execute as state tests. So if clients have the ability to benchmark state tests, then they would be able to replicate the inputs that I've used without much difficulty.
00:51:12.700 - 00:51:18.092, Speaker B: We use engine API request, but we can talk about it async how to.
00:51:18.116 - 00:51:22.360, Speaker A: Translate your benchmarks to our.
00:51:24.190 - 00:51:52.330, Speaker C: Just one comment here. This tool was built because benchmarking is hard, especially for some managed languages like C sharp and Java, because there's multi tiered compilation going on. So you need to properly warm up everything, et cetera, et cetera. So it's generally complicated. So that's why this tool was built, to have a proper benchmarks. And I would advise to use something more complicated than just a single run.
00:51:56.620 - 00:53:02.422, Speaker A: And I guess if different teams potentially have different approaches here, is it reasonable for like Ret Besu, Aragon and Nethervine to just look into benchmarking these on their clients individually in the next couple of weeks and then kind of report back as we have those benchmarks, and hopefully by the next acde we're able to have agreement about changing the prices. And I think it's probably good to use Jared's suggestion. Simple idea of using a factor of two X on the discount table and then having other teams look at whether this is reasonable. And for whatever team or whatever other issue it's not, we can discuss it. Two weeks, I assume, would be sufficient for people to look into this. Okay, so I'll take the silence as a yes. Yeah.
00:53:02.422 - 00:53:44.418, Speaker A: So I guess in terms of next steps, let's have each team look into it, run the benchmarks, we can report back. Is there like an issue or like a channel? I guess we can just use the execution layer channel in the R and D discord to discuss this. Like the execution dev channel, which Jared was already posting benchmarks in. And then. Oh, Besu is a maybe, Justin, is that on the timeline or on whether it's possible for Besu? Basically, yes. Now. Okay, so yeah, yeah, let's follow up.
00:53:44.418 - 00:54:36.426, Speaker A: Let's follow up on this, on the execution dev channel. But then hopefully at the latest, two weeks from now, we have the benchmarks from all the teams. And then as soon as we have that, if they're all roughly similar, we can just two x the costs. And if there's like big discrepancies between different client teams benchmarks, we can look into that further. Does that make sense? Okay, well, yeah, thanks, Jared, for sharing this. And yeah, we can now move on to last, I guess, big thing on the agenda. So on the last acde, there were a bunch of potential eips that were brought up to consider for inclusion in pectrade.
00:54:36.426 - 00:55:10.116, Speaker A: We already have a ton of eips in Pectra. It is by far already the biggest fork in terms of number of eips, but there's still some other stuff. So we had four eips that were CFI'd. So the r1 precompile inclusion list was, I think we've agreed we're not going to do the call data cost increase. And then the decoupling of the blob count between the El and the Cl. And in addition to this, there were a couple more eips. So we had these sseips we've been talking about for a couple months.
00:55:10.116 - 00:56:04.958, Speaker A: There was this EOF related EIP has code which would allow effectively to check whether EOF account is a smart contract account or an EOA. And then lastly there was Max's eip to increase the minimum base fee per blob guests. And so I asked the teams to sort of share what their perspectives were on these, on these different, these different eips. We heard back from ref, from Ethereum, J's and from Nethermind, each of them, you can see on the agenda their opinions. And it seems like across at least these three teams, the decoupling of the blobs is like the least contentious. So it probably makes sense to start there. So, yeah, for EIP 7742, I don't know.
00:56:04.958 - 00:56:43.470, Speaker A: Yeah, Aragon, Besu Geth, how you all felt about this one? Does anyone have concerns? But it seems like the one thing that had the broadest support and that's also pretty straightforward. Yeah, this is Matt from basically, I think the same, same normal scope group considerations. But yeah, we're in favor of specifically the decoupling and loosely in favor of those other two as well, just to get that out there. But we can discuss those more as we go. Thanks anyone from Aragon or guests.
00:56:53.140 - 00:56:56.680, Speaker C: So I think we are slightly in favor. No strong opinions.
00:56:58.740 - 00:56:59.220, Speaker D: Okay.
00:56:59.260 - 00:57:54.220, Speaker A: And then there's some comments in the chat that, yes, the fork is big. And you know, should we consider removing some eips? Yeah, like I'll also echo this where we have a lot of different things in this fork. Yeah, I don't know if anyone has thoughts there they want to share. Alex has a comment saying instead of removing, we could consider splitting into two forks. Alex, do you want to expand on that? Sure. Yeah. I mean, I think everyone agrees that it's a really big fork as schedule.
00:57:54.220 - 00:58:40.780, Speaker A: So a natural thing to do is just to break it into two. Generally smaller forks are less risky. In particular with Petra right now, there are a bunch of cross layer eips which really raise the testing and security review loads. That's not great. And yeah, I mean, I'm curious how people think about this. Yeah, like client has a comment about Perry proposing this. I remember there was like a DevOps doc that had like four or five different options and Perry is saying plus one to a split.
00:58:40.780 - 00:58:46.860, Speaker A: I don't know, Perry, do you want to share more context there?
00:58:48.840 - 00:59:25.660, Speaker B: Yeah, I think the main reasoning is that currently we have a lot of eips and we're tending to touch many, many, many layers of the stack. And the more we add, even at the current load, it's hard for any one person to have an overview of all the changes. At least the approach we're taking right now to testing is one level. The testing team looks at more the El Eips and we're looking more at the CL Eips. But I do think it's a lot. But, yeah, just want to put it up out there as well.
00:59:28.680 - 00:59:51.440, Speaker A: Yeah, maybe to be specific, like one option is just looking at sort of the core picture that we have right now, say on Devnet three. And there's things that we're discussing say like eof or pure dos. I think those could very naturally go into a second fork just given development timelines. And again, accounting for how big Hector would ultimately be if we do it all at once.
00:59:55.020 - 01:00:24.028, Speaker B: Anskar yeah, I just wanted to basically very briefly say in support of Piras, because I do think kind of a split might make sense, and I'm actually conflicted on pirates as well. But I wanted to at least give voice to the point that to me, over the next year, this is kind of the part of the upcoming upgrades that is the most core to kind of the strategic roadmap of Ethereum. And so at the very least it.
01:00:24.044 - 01:00:25.428, Speaker A: Might be unavoidable that it would have.
01:00:25.444 - 01:00:34.084, Speaker B: To go into the second part of the split. But I would at least say that we should at least try to basically give it, give it a try evaluate.
01:00:34.132 - 01:00:36.364, Speaker C: Like, is there any chance we could have it be in the first part?
01:00:36.412 - 01:00:46.040, Speaker B: And if not, then, yeah, of course it has to go into the second part. But just want to mention to me, Piedas really stands out in terms of its strategic importance to Ethereum.
01:00:51.260 - 01:00:52.040, Speaker C: Thanks.
01:00:57.420 - 01:01:48.050, Speaker A: There's a question by Oliver, like, how we would split. I think one thing we should consider there is that when we start bundling things in devnets and testing releases and whatnot, there's then a cost to unbundle them. And so, not that what we already have in Devnet should exactly be Petra, but I think that should basically be our default of stuff that's already bundled. And being testing together is going to be quicker to ship together than if we both add more stuff and also remove more stuff. And in the past we've seen like mainnet consensus issues happen when we remove eips from forks as well. So we shouldn't. Yeah, we shouldn't treat removing something from the current scope as effectively, like free.
01:01:48.050 - 01:02:50.470, Speaker A: We should assume that also takes some testing and engineering work. And so like. Yeah, so there's a comment about Devnet three, like, I'm trying to find a spec real quick, but basically Devnet three right now comparing it to Pektra. So. 253-729-3561 110 70 two MAXCB 75 49 76, 85, 77, 02 are all in. So I think this means that the two eips included in Petra today that are not in Devnet three are peer Das and EOF, which is a whole set of eips. And then none of the additional eips that we've considered in the past couple of weeks that we were just talking about are included yet either.
01:02:50.470 - 01:03:15.400, Speaker A: So that's kind of the way to think about it, where Devnet three has everything except EOF and pure Das and then, which are also being prototyped in Devnet right now, and then all these other Eips, like 7623, 7742 and whatnot, don't have any prototypes or like, implementations, as far as I know.
01:03:29.580 - 01:03:51.030, Speaker B: So I guess, yeah, please bring up, and already kind of playing the devil's advocate. If we do split eips today, what's the guarantee that we're not going to add more eips to the next walk and kind of be exactly where we are right now, just six months down the line? And how do we prevent that? We don't do that, I guess.
01:03:52.210 - 01:05:11.960, Speaker A: I guess maybe to touch on that. And like, Kev has a comment saying, like, if we split, when would the second fork be? I don't think that we can. I don't think it's useful to like, think about when the second fork would be, because we're just going to optimistically say something and then probably be wrong. The way I would think about it is assume everything that's already in Petra is the set of things we want to ship as quickly as possible. Does bundling it all in one release make the whole thing ship quicker? Or does isolating some of these changes together make the whole thing ship quicker? And my intuition is like, if we just look at what's already in Petra today, it's potentially quicker to literally ship Devnet three, separate from pure Das and EOF, and that the date when all of this is live on mainnet is earlier than if it was all combined. There was all these weird interactions across it. But yeah, the other question there is if six months from now, if six months from now, we keep adding a bunch of stuff to whatever Petra two is, then we're in the same problem.
01:05:11.960 - 01:05:27.050, Speaker A: So in terms of scope, yeah, I think we'd want to basically limit things as much as we can potentially start planning for, like the fork after that. But yeah. Daniel.
01:05:28.830 - 01:05:57.086, Speaker G: So I think the model we should probably consider is basically what you've been talking about. Is it in the Devnet? Then it's set to go for the next fork. And we could just close the door on Devnet three right now, polish it up, fix it bug, fix it only had absolutely essential changes. And start Devnet zero on Busaka. So we shift Devnet four becomes Devnet, of Hector becomes Devnet zero fusaka. We put a UF in there. We put all the other things that are ready to go in.
01:05:57.086 - 01:06:19.770, Speaker G: We start testing that in parallel. But we also polish up Devnet three ship that just as soon as we get it polished and ready to go. That's something that I can get behind. If we start putting a stake in the ground that this is already what's in the next Devnet, the next hard fork with the Devnet zero, that hard fork, that would have solves a lot of concerns to say, well, when is it? Well, look and see which devnet it's in. And that's your answer.
01:06:20.710 - 01:07:30.442, Speaker A: Actually, yeah, I like this idea. I think maybe the one that I'd say is like maybe there's like a Devnet four that's like Petra one and then Devnet five is like Petra two or something like that. Given that there were maybe things we wanted to add that are simple. Like I don't know if people still feel that strongly about decoupling the blob count or like increasing call data costs, but those eips are relatively simple. I'd be curious to hear from people if we went with something like this. Do we think Devnet three is the thing to ship? Do we think there's maybe still a couple things extra to ship as part of Petra and then Petra two becomes Devnet five? Um, I think that's a good way forward. And maybe just to add another plus one here, like I think the scope has been decided because we kind of have just scheduled two forks with Petra already.
01:07:30.442 - 01:08:15.460, Speaker A: And so this is just recognizing that like timing wise, risk wise, you know, production wise, it makes a lot more sense to have two here rather than one hard fork. And yes, to be clear, kev, I think Petra too. I don't want to use like Fusaka because like we already have vertical schedule for that. But like, yes, I would think of Petra two whether it's eventually called Fusaka and you renamed the Virko fork, but like Petra two is say, you know, EOf and peer Das. But, but yeah, like it doesn't also include verkle. And we can, like we can call it fusaka or whatever. Like we can sort this out outside of this call.
01:08:15.460 - 01:08:23.440, Speaker A: But just want to be clear that like this is a separate thing than like the fork that has vertical. Yeah, dano.
01:08:24.860 - 01:08:51.935, Speaker G: So my controversial take on Verkle is that it gets a third test line. That is the current fork, whether it's cancun or Prague or whatever comes after Prague. And it is that baseline. And when it passes its quality metrics, that is when we ship it independent of any other feature. Because it's so big it wants to be its own thing. So we let it be its own thing, let it grow. When it's ready to ship and passes the quality metrics, that's when we pull the trigger and start scheduling it so it's not tied to the regular release train.
01:08:51.935 - 01:09:07.890, Speaker G: Something that big is causing scheduling problems because already talking about inventing new fork names and I moving stuff around because we conceptually tie this sway name before. No, it ships when it's ready and we give it its space so cook can make sure it's ready when it passes quality metrics, out the door goes.
01:09:09.310 - 01:09:38.020, Speaker C: This is a good point, but you have to agree that it will only really start passing the serious quality metrics when we put it as the next fork. Like, this is traditionally how things have been going. I mean, maybe for virgos different because there have been devnets and, and all this kind of stuff, even though it's not scheduled right now. But I mean, for sure, the finish line is sort of when it becomes visible by something being scheduled, that's when we really start putting all the effort in.
01:09:42.800 - 01:09:48.240, Speaker A: And more importantly, this vertical was more.
01:09:48.280 - 01:09:53.521, Speaker C: Ready than many things that were scheduled. And we pushed it because it was biggest.
01:09:53.655 - 01:09:55.677, Speaker A: But okay, sorry, I also have to.
01:09:55.693 - 01:10:04.209, Speaker C: Do babysitting at the same time. But yeah, the forum was already ready a long time ago at least.
01:10:04.669 - 01:10:07.369, Speaker A: Like, the quality metrics were already pretty advanced.
01:10:09.589 - 01:10:15.409, Speaker C: We can't really push logistics because otherwise you're sending a signal that is very.
01:10:16.389 - 01:10:18.165, Speaker A: I mean, no one will ever invest.
01:10:18.197 - 01:10:23.070, Speaker C: In it, but no one will ever invest in, in larger forks. And sorry about the noise.
01:10:24.130 - 01:10:40.630, Speaker G: Well, the quality metric I had in mind was two or three successful main net shadow forks. So you move towards the full dress rehearsal. When the dress rehearsal is ready, you just swap in and say, that's the one that's going next. I don't think you had a successful main net fork yet. You've had some testnet forks. No, no, it's pretty full size.
01:10:41.410 - 01:10:46.114, Speaker A: This is, this, this is unprecedented. Like, this is the amount of quality.
01:10:46.282 - 01:10:48.186, Speaker C: EOf was never held up.
01:10:48.338 - 01:10:50.710, Speaker A: And this is completely unacceptable.
01:10:51.330 - 01:10:53.070, Speaker C: I think that's.
01:10:53.370 - 01:12:04.364, Speaker A: Yeah, I think another way to look at this is like when we scheduled Verkle for the fork after Osaka or, sorry, Pectra, we then started adding a bunch of stuff that we thought we should do before vorkel. And that list sort of grew longer and longer and longer. And there's a sense in which, like we, like, we effectively prioritize this ahead of Verkle one way or another. And whether it's like one fork or two forks, I guess that's the main thing that at this point feels like the decision to be made. But, yeah, it seems like, I know teams wanted to work on EOf before workhold we made the decision, we're already quite far ahead in the testing. And again, we can always revisit these decisions, but there's also a cost to that. I think a way to look at it is assuming this is the set of things we want to do now, and that we've already committed to doing before.
01:12:04.364 - 01:13:18.952, Speaker A: Verkle, what's the quickest way to ship all those things? And, yeah, I think my sense and like, what a bunch of other people have echoed is like splitting this set of things we wanted to for work into two separate forks is probably quicker than keeping it bundled together because there's like effectively nonlinear cost in testing and engineering complexity of having a massive fork rather than two small ones. And then I think the big risk though, if we do this, is that we split into forks and then we look at the second one and we're like, wow, this only has ten ips. Now we can bump it up all the way to 20, and then we sort of end up in this infinite loop of splitting forks over and over. My hope is we're slightly better than that and we can commit to, like, keeping the scope relatively tight. Yeah, I guess. Yeah, maybe. Yeah.
01:13:18.952 - 01:14:21.810, Speaker A: Another question is like, is there anyone or any team that feels like a single unified fork, including like, regardless of the additions we're talking about, but like, just including the stuff that's already in Petra would be quicker than like, yeah, two separate ones. Yeah, Ben. I mean, it really comes down to the devnet process, I think. How much splitting into two? Because now we have to do two sets of devnets versus doing m one. I don't know the answer to that one. Well, I guess so. We already effective, we have today three sets of devnets, right? There's tetra Devnets, there's the EOf devnets, there's the peer das devnet.
01:14:21.810 - 01:15:02.380, Speaker A: As I understand Dano's proposal, it's basically we say we use Devnet three as the basis for Pektra one. We do that, we keep working on it, we ship it, and then the work that was currently happening in the EOF and peer Das Devnet tracks gets bundled into and that we were planning to include in Devnet four. Instead of Devnet four being just like a continuation of changes to the Petra spec, Devnet four just gets rebased on Petra and effectively becomes Devnet zero of Petra two or something like that.
01:15:14.440 - 01:15:42.860, Speaker B: Also, just want to make a point that peer dust hasn't been rebased on Pektra either. So it was kind of a commitment we had to make anyway, so we can just continue to keep the pure Das line of Devnet separate and just pin like a Petra branch. We would consider canonical for peer Das and I guess we would do the exact same thing for EOF. But I guess we can discuss those details more. The breakout rooms that we would eventually have.
01:15:47.180 - 01:16:46.552, Speaker A: And I guess so we have Devnet three that's launched today or, sorry, yesterday. I assume teams still have some work on this, but at the same time this is kind of a huge decision to consider. So we probably should give people time to think about it. But what's the timeline by which teams, you know, like feel we need to make a call here where like, yeah, if we, if we don't have a decision on the specs, then we effectively won't know what to implement in the next Devnet. Like do we need to make this call today? Do we want to give people like a week or two to think about this? Yeah, my sense is like a week would be reasonable, especially if we're considering different ways to speak. Split it, but yeah, sorry, Alex, I think he came off mute. I was going to say if we don't decide today.
01:16:46.552 - 01:17:56.436, Speaker A: Yeah, then I would say either. I mean, next week would be great. Or definitely by next ACD. Yeah, yeah, I think, yeah, I think Acde feels like almost a bit. Yeah, too far. But my suggestion would be by the next week, looking at first the binary decision, do we split or not? Second, if we split, are we happy with the scope of Devnet three as spectra, one or other things that we feel strongly that we should include? And then also if we do make some changes to the actual EIP is already in Devnet three, like we discussed in the earlier half of the call, how to approach those and how do we want to get to a spec freeze? Yeah, but I think that's kind of reasonable. And there's a comment in the chat as well around timelines.
01:17:56.436 - 01:19:15.140, Speaker A: So I think there's also broad agreement that if we split the ideas that we want to ship to Petra one quicker than or as quick as possible, and you know, early next year should be like our target. So if we think about like stuff we would consider adding to Petra one, you know, it should be stuff that doesn't really change the timeline under which we'd like expect the ship to fork. Yeah, so maybe. Yeah, so I guess, yeah. A week from now on ACDC, we can continue this discussion, see if like any team strongly oppose splitting and if there's consensus towards splitting, then what's the set of things that we can optimistically ship, you know, sometime between the end of this year, early next year. Do we need to bring anything else in that feels urgent and take it from there? Any other thoughts, comments? Okay, well, yeah, thanks everyone. I'll try to write up some summary of this as well after people who weren't on the call.
01:19:15.140 - 01:19:59.140, Speaker A: But let's continue this discussion on next week's call. Okay, and then the last thing that we had on the agenda today was some updates around network configs for Mainnet. Sapolyahoski PK, you posted about this. Do you want to give a quick overview? Yep, can give. Yeah. There is a set of prs to align the format of the three public network Genesis repositories. There isn't a big change, just aligning the format of all and to have a consistent format of all these repositories.
01:19:59.140 - 01:20:57.094, Speaker A: So in particular, we added the execution layer, genesis for Mainnet and sepulcher, and we started tracking the execution layer boot nodes for these networks. So it would be great if client teams could have a look at it and add potential missing boot nodes that are operated by them. So later on we can ping these boot nodes and have some monitoring and blame the offline ones. That's basically it. Thanks. Any questions, comments? Okay, then, that's everything we had on the agenda. Anything else people wanted to cover? Okay, well, thanks a lot, everyone.
01:20:57.094 - 01:21:28.740, Speaker A: Yeah, we covered a lot today. And again, reminder to think through the options for the fork before next ACDC. And in the meantime, we'll also finalize the specs around the encoding stuff. And if teams can start looking at the benchmarks for the BLSP compiles, that also be great so we can get the pricing done. And. Yeah, I think that's it for today. Thanks, everyone for joining and talk to you all on next week's calls.
01:21:30.240 - 01:21:31.300, Speaker E: Thanks, Tom.
01:21:31.800 - 01:21:33.240, Speaker A: Thank you. Cheers, everyone.
01:21:33.360 - 01:21:34.300, Speaker C: Thank you, Tim.
01:21:34.760 - 01:21:35.540, Speaker A: Thanks.
01:21:36.610 - 01:21:37.590, Speaker B: Thanks, everyone.
01:22:01.460 - 01:28:24.462, Speaker A: Radhe, Sadeena. Sadeena. Sadeena. Radhe, Sadeena. Sadeena. Radhe namdeh. Sadeena.
01:28:24.462 - 01:29:15.480, Speaker A: Sadeena. Sadeena.
