00:00:01.240 - 00:00:54.900, Speaker A: But yeah, welcome everyone to roll call number six. That is issue 1071 in the PM repo link. In the chat, first thing to touch on is the blob base fee spike that we saw on June 20. I think. I mean, first of all, this was largely something that was due to organic demand. Like this was just genuinely people using the chain as intended, and l one responded as expected, which is good to see from an l one perspective, but obviously sucks if as an l two, you ended up way overpaying for posting blobs in a certain amount of time. If that wasn't what was intended to do.
00:00:54.900 - 00:01:47.590, Speaker A: I think as many people are aware here we have the das upgrade coming up in Petron zero one. That's EIP 7594, which should hopefully help alleviate some of these things. And there was a breakout call yesterday on peer Das, and one of the topics discussed there was potentially not having the target be half of the max, which, depending on how we set parameters there, could also help. I'm not aware I wasn't on this call. I don't know if anyone here was. Does anyone know what the current idea, what the current thinking is around target and Max?
00:01:49.400 - 00:02:39.386, Speaker B: I'd like to just point out that the I was looking into the blob incident, and I don't think the l one was working as expected. I found an issue with how. Well, I guess it depends where you say the issue is, but the way the gap transaction pool works is it rejects transaction blob transactions with gapped nonces. In the way op stack works anyway is it only sends batcher transactions from a. It only accepts them from a single address. So when the batcher gets behind, it will submit multiple transactions with consecutive nonces. And ordinarily that would be fine, but it also turns out that Geth has a non deterministic order in which it announces and requests blob transactions.
00:02:39.386 - 00:03:29.418, Speaker B: So if it were to request some out of order, one of them would be immediately rejected and then not further propagated. And so I believe that was leading to propagation issues, because what we were seeing is we were continuously increasing the priority fees of our transactions and they would just kind of disappear. And I believe this is the cause of it. I posted a pr for Geth there. I don't know if that was affecting non op stack chains, but I'm pretty sure it was a problem affecting our ability to land transactions anyway, especially when it started getting behind. We had our system configured to submit up to five transactions at a time. And of course, the more parallelism you have, the worse this issue tends to come back to you.
00:03:29.418 - 00:04:05.250, Speaker B: So that's just a recent finding I wanted to share. I don't, I don't know if it affected other chains, but it was certainly a big issue in base we had. You know, we were perhaps, I think, 3000 blocks behind at one point trying to catch up. And that is why we ended up overpaying also because at the point we started posting the batch of transactions, we were including l two transactions that had been submitted much, much earlier and were only charged for a much lower blob base fee. So all these kind of things conspired to just kind of fed on into each other.
00:04:10.000 - 00:04:10.520, Speaker C: Okay.
00:04:10.560 - 00:04:14.608, Speaker A: Yeah, that's definitely some interesting things. Yeah.
00:04:14.624 - 00:04:26.260, Speaker D: Andres is there, is there is that is the non determinism issue only? Is that specific to Geth, or is this also a case for, like, Bezo?
00:04:27.680 - 00:04:56.578, Speaker B: I've pinged the Nethermind folks. I've pinged the ref folks. I haven't asked more broadly, it may well be an issue. The other issue with Geth is it also was not requesting blob transactions right away. So this exacerbated the problem. It would basically wait half a second after the announcement before fetching the blob transactions, which is kind of how it treats non blob transactions because it's waiting for them to be propagated automatically. But blob transactions aren't pre broadcast.
00:04:56.578 - 00:05:21.736, Speaker B: So that was another factor as well. So basically it would wait till, you know, it got several announcements, and then by that time, the non determinism was much more likely to happen. So that problem, I think, is specific to Geth. At least it wasn't in Nethermind and it wasn't in reth. But the non determinism part, I'm still trying to understand that. And I've reached out to at least those client teams. But anyway, that's why I wanted to socialize it here.
00:05:21.736 - 00:05:22.580, Speaker B: A little more.
00:05:25.610 - 00:05:38.522, Speaker A: Light client, as our resident geth expert and comment in the chat, do you have any thoughts on this? I haven't looked into this, but it.
00:05:38.546 - 00:05:42.990, Speaker C: Makes sense to me. The PR and I think that we'll merge this soon.
00:05:46.050 - 00:06:03.770, Speaker A: Okay, cool. Thank you. So it sounds like this is the issue here due to the non determinism. Am I understanding correctly? Like, if, if they are the not, the non aligned non scaps is generally okay. If we could at least have determinism.
00:06:03.810 - 00:06:16.034, Speaker B: And how they were fetched, I mean, that'll certainly help. I mean, because it's networking, you're never guaranteed anything. But yes, the. That. In that PR. Exactly. The fix is just that, to just try to always request an order.
00:06:16.034 - 00:06:26.790, Speaker B: And I think that's probably good enough, but getting rid of the nuns gap rejection would be another fix as well. But obviously that opens you up to some denial of service issues.
00:06:29.570 - 00:06:52.960, Speaker A: Yeah, and I think that's the bit of the spikiness around the mempool and how the replacements work and all of that, for DoS reasons, I think has also produced a lot of the pain that happened during this event. Things like needing to double the fees to replace is. Yeah, not great, but needed on the one side.
00:06:53.780 - 00:07:02.920, Speaker D: Just a quick question. My stupidity. Why are the block transactions not announced? Just like regular transactions?
00:07:05.420 - 00:07:07.280, Speaker B: Why are they not broadcast, you mean?
00:07:07.660 - 00:07:10.230, Speaker A: Yeah, immediately it just.
00:07:10.270 - 00:07:26.730, Speaker B: Because they're much, much larger than ordinary transactions. And so we didn't. The decision was not to eagerly broadcast them, which results in some amount of redundancy. So by not broadcasting them, you're guaranteed to only fetch it once, minimizing your.
00:07:28.270 - 00:07:50.260, Speaker D: Would it make sense to have a. To have a. To have a. To have a digest broadcast? So then you fetch it, but at least you know that it's there so you're not running into. Into that. Into that issue that you're all of a sudden like, oh wait, like ten transactions?
00:07:52.680 - 00:08:14.480, Speaker B: Yeah, that's not a bad idea. There is this metadata that does come with announcements right now. It contains the transaction type as well as the transaction size, but it does not include, for example, the sender and nonce. But, you know, not sure that's going to be necessary. Perhaps we'll see after this fix whether we encounter similar issues.
00:08:21.540 - 00:09:20.700, Speaker A: So one of the other proposals that's been floating around, or one of the other things that we mentioned a bunch, is the fact that because you can include multiple blobs per transaction becomes very hard to pack blocks from like an appsack standpoint. And additionally, I think that's a large cause of the conservatism with which the l one s are treating the blob mempool. Obviously it's not a, like, longer term fix, but like, would people, or is they any interest in potentially having some agreement amongst l two s to not post multiple blobs at once per transaction? Would that be something that people would be interested in? Obviously it means that you can't amortize the base transaction fee across as many blobs.
00:09:22.120 - 00:09:52.300, Speaker B: I mean, I think that's something we would be open to. In fact, we did try to turn the number of blobs per transaction down during the incident, thinking it would help. But unfortunately, because of the other issue I just described, it made that one even worse, because we had to submit more parallel transactions at a time. But once that issue is fixed, I think that is a reasonable thing to implement. Right now, I think base is submitting five blobs at a time. It usually means we're paired up with Tyco, which is usually sending one blob at a time pretty consistently. But this is a parameter.
00:09:52.300 - 00:09:57.596, Speaker B: No issues with changing it. It still works.
00:09:57.788 - 00:11:13.100, Speaker A: Yeah, I think it's largely a case of like, it might not necessarily be like, it's not necessarily helpful if only one or two people change, but if we can get most people to change, and it makes it much easier to make these decisions, because I'm sure the knapsack is one part of it, but if we can just be more conservative in these things, then maybe we can change the mempool rules around how these things get. Like what's required as a replacement fee as well. That is, if you only have one blob, then you could have a low replacement fee, or, as also has been discussed, having another endpoint, which doesn't replace the entire transaction, but sort of just the header with the priority fee. But I think that would be a bit more invasive. Okay. One of the other interesting things I found out, we're doing a bit of digging here, is, I think some figures have been pointed around at flashbots as builder for not accepting blobs as a compounder on this, but doing a little bit of digging. It looks like flashbots is only at 2% of blocks.
00:11:13.100 - 00:11:30.910, Speaker A: So I don't think that has a major impact on the total amount of chain throughput, although obviously not good to have. And they have subsequently started accepting blobs again, which is interesting to see. Okay.
00:11:31.410 - 00:12:00.790, Speaker B: Claimed it was a bug, and that it's apparently been fixed, but what I've been sort of tracking the number of locks with zero blobs, and it's gotten better. It was 40% during that incident, and now it's 36 ish percent. So it is dropping. Actually, I just checked it. Right now it's down to 34%. So it does seem like some builders are being a little more eager with blobs, which is good news.
00:12:03.690 - 00:12:07.570, Speaker A: Do you have data into that, per builder?
00:12:08.950 - 00:12:20.050, Speaker B: No, I haven't done any recent analysis. Breaking it down. We did it for the. During the incident, but since then, all I've checked is to make sure that flashbots have started to accept them.
00:12:21.510 - 00:12:32.660, Speaker A: They definitely have. I've seen data the way they have yep. Yeah, I'm just curious to see what's the remaining 34% or whatever you mentioned.
00:12:33.560 - 00:12:35.264, Speaker B: Yeah, I don't have the breakdown, unfortunately.
00:12:35.352 - 00:12:54.540, Speaker A: Okay. All right. Any further items that people would like to discuss on this topic? No. Okay, moving on. There's.
00:12:56.280 - 00:12:56.980, Speaker B: A.
00:12:59.470 - 00:13:11.090, Speaker A: Elias wanted to erase his rip on l one s load, but I don't believe he's on the call. Anyone else who wishes to speak about that?
00:13:12.030 - 00:13:15.646, Speaker C: This was proposed by Ice Malon, but I speak on his behalf.
00:13:15.718 - 00:13:19.130, Speaker A: Oh, okay, cool. Yeah. Do you want to speak about that for a little bit?
00:13:20.470 - 00:13:25.520, Speaker C: Yeah, I have a short presentation, if that's okay. I will share my screen and go through that.
00:13:25.860 - 00:13:27.240, Speaker A: Sure, that's fine.
00:13:34.220 - 00:13:35.724, Speaker C: Seems like I don't have permission yet.
00:13:35.772 - 00:13:38.120, Speaker A: Sure you should. Now.
00:13:38.860 - 00:13:39.720, Speaker E: Okay.
00:13:43.940 - 00:13:45.680, Speaker C: Can you see my screen now?
00:13:47.740 - 00:13:48.348, Speaker B: Yep.
00:13:48.404 - 00:13:49.160, Speaker A: Zendered.
00:13:49.910 - 00:14:23.264, Speaker C: Okay, thanks. I also share this presentation in the chat for your reference, so. Hey, everyone, I'm Peter from Scroll. We recently proposed rip 7728, called l one s load precompile. So I want to take a few minutes to walk you through this proposal. The idea of this precompile is to make it easier for l two devs to use any I want state without having to care about MPT proofs. We do this by introducing a new precompile that we call r one s load.
00:14:23.264 - 00:15:07.910, Speaker C: And basically, this is just a simple interface over it, getstoryjet. And this is a service that the sequencer kind of provides to Dapps on any l two network. The proof verification is not something that Dapps should care about, but it's left to the sequencer and to the proverse. And we believe that this is a simple building block that makes it easier to build any cross chain depth between ethereum and the l two. There have been some similar proposals before Taiko proposed I won call and I think optimism. A similar one called remote static call, which are in a way more powerful because you could have, like, a full evian call context in those proposals. But at least in our experience, it's called.
00:15:07.910 - 00:15:18.870, Speaker C: This is not really feasible to implant at this point because we would need to have, like, ZKVM embedded with.
00:15:21.570 - 00:15:21.930, Speaker A: Less.
00:15:21.970 - 00:16:01.650, Speaker C: Powerful, but less fancy, but something more feasible in our view. The spec is quite simple. So this will be a pre compile that takes the contract address of an island contract and up to five storage slots. So this five is kind of the arbitrary number and would love to hear our thoughts on this. And then it issues a batched RPC call to an r one node and then returns those red story slots to the l two color. So here's the details, but it's fairly straightforward. Just the encoding of the inputs and outputs in terms of gas.
00:16:01.650 - 00:17:05.468, Speaker C: So we have a fixed gas cost per load gas cost, which is based on this k one to five constant. These are not the final values, but our current thinking is that the fixed gas cost is around 2000, and the per load gas cost is to be the same as s load gas cost for cold slots. The fixed guest cost is quite interesting because this time the main cost, I would say is latency, the RPC cold latency, which is not a resource that we have experienced pricing as opposed to storage or compute. So this is really not final number here. Yeah. To answer Richard's question, currently is only called I guess we could consider, you know, within the same transaction, if you read same slot twice, then the second time you can just read it from local cache. But I think that's not really something that Dapps would use quite often.
00:17:05.468 - 00:17:46.862, Speaker C: So I'm not sure if that's useful, but we can incorporate that into the RP. From a dev perspective, this would be quite simple to use. You can just static call, provide like a list of keys and the address, and then use abi decode to decode the results and then use that. So quite developer friendly I would say. In terms of implementation, we have an example implementation, I wouldn't call it reference implementation because the drip gas cost and some other details are not finalized yet. There's two things I want to highlight here. One is that we assume that the l two node connects to an l one archive node.
00:17:46.862 - 00:18:32.936, Speaker C: I think it's a reasonable assumption that the l two node connects to an l one node, but requiring it to be an archive node might be a stronger requirement. That is kind of an issue with this rip. So the sequencer would need to execute a recent l one state. So full node would be enough here, but any l two full or node who wants to replay historical transactions would need to connect at least temporarily to an l one archive node, and then they could switch over to another one if they don't want to maintain that long term. The other prerequisite is that the outsuit protocol has a notion of the latest seen l one block number. We at scroll have a spec for this already, not live on mainnet yet. Optimism also has a system contract that has this.
00:18:32.936 - 00:19:35.944, Speaker C: I'm not sure about other roll ups, but I think this is generally something that's useful in many contexts for rollups. So it seems to be a reasonable prerequisite to have in terms of implementation. I have it linked here we have a get fork for our sequencer, and we have this example, implantation in that geth fork. And then the verification part is basically we would do sequencer, or the roll up would do the verification of the proofs on behalf of the user. So that would mean that approving time. So sequencing time, you can just call the Getstoryjet RPC, but at proving time, you would need to get the corresponding state proof, and you would need to verify it in a ZK ro app, inside a ZK proof or inside your fraud proof mechanism to make sure that all these reads were performed correctly. One of the concerns that we heard is the latency.
00:19:35.944 - 00:20:13.724, Speaker C: Like how fresh is this state that is available to l two? Dapps I think Vitalik mentioned these different stages. So the most conservative one is if the l two roll up only imports finalized blocks, which is safe, you only need to care about reorgs. But the latency is not great. If you wait for ten confirmations, that would be stage one. That's much better. Ux, and the risk is not huge, but you need to be able to add the reorgs in the worst case, and then the best would be just almost no latency, or like one or two slots. But then Reorg is something that you would need to deal quite often, deal with quite often.
00:20:13.724 - 00:21:15.570, Speaker C: So how we relay this l one block information to l two is not really in scope for this RFP, but this is an important consideration nevertheless. And finally, there are a few open questions. The first one I already mentioned, is it, is it okay to require ltnos to connect to an l one archive node? As I mentioned, for the sequencer, this doesn't seem to be necessary. But as someone mentioned in the telegram group, currently it's not even necessarily required for l two floor nodes to connect to an l one node at all, you could just get the same information from the sequencer. Of course, you would need to trust the sequencer on this, but it's not a strong requirement now, whereas for this rip, it's definitely necessary to at least connect to an I one full node to be able to follow the chain. Dealing with RPC errors is also an interesting question. Since these calls to this precompiler part of the state transition, you cannot.
00:21:15.570 - 00:21:58.142, Speaker C: You need to make sure that you deal with these errors in a deterministic way. So our proposal now is, in the sequencer, you can retry or reject the transaction, but once it's included in the ledger, the only thing that nodes can do is retry. So if your r one node has a connection issue or whatever, then you need to retry until this call succeeds. Otherwise you cannot reproduce the state. There's the question of RPC latency. So one aspect of this is how do we price this in the in guess, and also whether this will become a bottleneck for the sequencer. We think this is probably not an issue because the sequencer would definitely use the local node, so latency could be optimized.
00:21:58.142 - 00:22:51.460, Speaker C: It wouldn't call like infuria or some remote service, but for follow nodes this might be significant latency if they use some third party service. The fourth one again, this is what I mentioned on the previous slide. The more recent state we can expose to l two depths, the more useful it is. And indeed some devs might want some guarantees that the state that they can read is recent and not from two months ago. As for the guarantees, we don't have a great answer here, although I mentioned this is not really in scope for this RP, but this is definitely something that affects its usefulness, and we'd love your love to hear your thoughts on this. The proving overhead we can only speak for ourselves in scroll. For us, the MPT verification for Iowa proofs seems to be acceptable.
00:22:51.460 - 00:23:48.810, Speaker C: It's something that our provers can manage, and this is basically the main selling point of this LNA slot compared to the previous l one call precompile that I mentioned that this is feasible in the Zkhdem context, and that one is less feasible. And finally, a previous version of this precompile allowed reading state from an arbitrary album block height, or at least the last 200 blocks or something like that. We removed this for simplicity, but some people suggested that if we allow tabs to specify the block height, then this might enable some use cases that are not possible otherwise. So this is also somewhat of an open question whether we should add this to the to the input list or not if you want to try it out. We currently maintain an open devnet, so feel free to take a look to interact with it. Call this precompile. And finally we have the rip and the discussions linked here.
00:23:48.810 - 00:23:53.890, Speaker C: So that's, that concludes the presentation. Is there are there any questions?
00:24:03.880 - 00:24:24.450, Speaker A: Thank you. Peter. Have you had any like when you talk about the dealing with different latencies and whether you use only finalized blocks or more recent things, and you talk about just having to deal with it, who's dealing with it? There is this. You imagine this being on the dap side or on the l two side.
00:24:26.630 - 00:24:30.734, Speaker C: You mean the latency for relaying the blocks from l and l two.
00:24:30.862 - 00:24:31.570, Speaker A: Yes.
00:24:34.670 - 00:24:52.610, Speaker C: So this is something that the roll up would also provide for the users. And as I mentioned, optimism blocks contract is an example for this already live, I think, on their mainnet. But this is not part of this rip. Yeah, I think this is more like a prerequisite to diabon s tag.
00:24:53.690 - 00:25:11.550, Speaker A: Yes, but my question is, you said that people have to deal with it, but I'm asking who you're envisioning dealing with it. Is this something that you expect l two s to have? Sorry? The contracts deployed on l two s to have to handle reorgs if they happen.
00:25:12.450 - 00:25:21.990, Speaker C: I don't know. So this would be handled by the sequencer. So if you rely on any I one state, then that was reorgan ledger would need to reorg accordingly.
00:25:24.010 - 00:25:24.910, Speaker A: I see.
00:25:25.410 - 00:25:37.550, Speaker C: If you relate, relay that information, that could be deposit messages. But l one block hashes also belong to the same category. If you relate that, and it's not true anymore, then you will need to rework accordingly.
00:25:38.610 - 00:25:39.470, Speaker A: Okay.
00:25:44.940 - 00:25:45.724, Speaker C: Thank you.
00:25:45.852 - 00:25:46.560, Speaker A: Yeah.
00:25:47.420 - 00:26:18.910, Speaker F: Adding one comment here is that we think like to make the l one slow, the more useful like, to the developers. It should be like the using at least like at a stage one. So feels like otherwise reading a very long latency while making like the developers in an application with a very stale state. So might like the limited use cases for this precompile.
00:26:40.580 - 00:26:56.600, Speaker A: Outside of sort of paste road lapse. Is this reorging the l two based on l one reorgs as this infrastructure that already exists in some l two s?
00:27:03.870 - 00:27:23.038, Speaker C: So I believe optimism has this state derivation function. Based on that, they can definitely kind of deal with reorgs at scroll. We're working on this functionality. Currently, we're still stage zero. We just wait for finalization. But there's. I don't think this is anything inherent to Bayes roll ups.
00:27:23.038 - 00:27:27.768, Speaker C: Non Bayes roll ups can also have this functionality. Totally.
00:27:27.784 - 00:27:30.180, Speaker A: It's just a requirement on the base, which is why I was asking.
00:27:31.160 - 00:27:31.940, Speaker C: Yeah.
00:27:49.720 - 00:28:17.130, Speaker A: Any further questions on this topic? Nope. Okay, well, thank you very much, Peter. And then next topic, I believe is Richard wanted to talk about rip 7740, pre installing deterministic deployment factories.
00:28:20.040 - 00:29:23.566, Speaker E: Yes, it comes out from a pretty old thread over a year ago or two years ago, pre rip time, basically, where it was talked about, should there be precompiled for create two, so that it can be used by UA's across chains. And at some point then they basically get to the point that some of the roll ups, namely optimism, started pre installing some of the workarounds for this non existence of the precompiler for create two, there were a couple, I think I listed some in the RP. There are a couple of factories also used by safe. We used multiple ones so that we can utilize them for deployments. Right. And what we realized is that it's still a little bit of a hassle and actually also one of the bottlenecks, if we deploy them, we have to make sure that is such a deployment. Is our deployment factory available? Because only this allows us to deploy to the same address with the same security assumptions in all places.
00:29:23.566 - 00:30:41.168, Speaker E: And it's actually not only a problem for us, but for a lot of other teams. For example, Coinbase is also utilizing our factory, a couple other factories being written, and it's not always possible to use the one that is trustlessly deployed due to certain gas trade offs, gas pricing and gas limit trade offs. That is one part and the other part which extends actually on this problem is that currently. So up to now there was no unified way across all roll ups, including the ones that are not type one, type two. So for optimism, arbitrary polygon and all the ones based on these stacks there, it's possible that you just redeploy the contracts, the factories that you have in one way or another. But when you think about Cksync, for example, there's currently no way to actually get this done because create two obviously depends on the bytecode, and bytecode is just different. These two problems would be nice to push them forward in a way that we can say, hey, this would be a major benefit if you deploy frost chain protocols, if they can be at the same address, if you can, especially for smart accounts, enable the same address or deployment to the same address.
00:30:41.168 - 00:31:44.752, Speaker E: These are just nice, at least security properties, but can also be used for UX properties. And this is basically where, what I wanted to figure out, is it even realistic to get alignment from the roll ups on this one, or do they say no? We basically would have to try to get each of the roll ups individually to go commit to certain pre installs for this one. And then also not sure if anybody actually from the ZK sync stack is here, or any of the non type one, type two stacks. Not sure which other teams there. Also, is there a willingness to look into figuring out how we could bring or build a solution that goes across next box? So these are the two questions for me before I would like to push this further forward. I want to avoid that. It's basically better windmills or whatever you want to say, however you say it in English, so not sure who is the best to respond.
00:31:44.752 - 00:32:08.350, Speaker E: I mean, let's first the first question is there a general willingness to define such a list of pre installs that I mean, they are to some extent already present on most of the roll ups? It would be more formalizing this for new roll ups spinning up based on existing stacks. That would be my first question. Basically, is this realistic or is it something that is very unlikely to get through?
00:32:10.170 - 00:32:19.430, Speaker C: I'm curious, by pre install, do you specifically mean genesis pre deploy or you don't care about the deployment? You just want to make sure that these are available on nitro.
00:32:24.370 - 00:32:57.470, Speaker E: Simplified speaking, I don't care about the means. They should be available on each roll up at the same address, basically with same logic. I think Optimus. The one I referenced was Optimus, and that's why I use preinstalls where it was basically from the get go. But in the end, if there are other more or easier ways or more feasible ways, I would be fine with this too. I have to admit that I'm not that deep into the roll up space that I would say this is the best approach and would be happy for feedback.
00:33:00.380 - 00:33:29.420, Speaker B: So interestingly, with base I think it was the create two deployer. The person tried to deploy it, not realizing that bridging funds over from mainnet incremented the nunce and that completely broke it. So we actually had to do a hard fork where we redeployed the createtodeployer at the expected address. So this would have prevented that situation. So I just wanted to point out that data point. I do see the value in it.
00:33:36.400 - 00:33:49.532, Speaker C: I also think it's a good idea to have a standard list so they can track who's compliant and who's nothing.
00:33:49.546 - 00:34:33.150, Speaker E: That's already good to hear. That would give me at least some motivation to keep this pushing then. Do we have anybody here from the casync? Sorry to be ignorant on the who is working on projects, else I would have to reach out to them on a different channel. Okay then maybe I will not be able to take part in the next roll call. It's during my holidays, but I would take it on to either have somebody else represented or joined the one afterwards. Even so, it's a long time ago, like a long time in the future. But then I would try to align with CK sync and then I would try to present a more finer version where I would say this is really too much since it's currently a draft.
00:34:36.130 - 00:35:08.960, Speaker A: That sounds great, thank you. I think there's a perfect thing to try. Standardize here and definitely see the use. But also just to point out that we don't like part of this process is not necessarily having consensus, and only one way that is compulsory to do it. Obviously it's nice if everyone agrees, but if we end up with outliers, that's too hard to include them. That's also okay. If you ship a standard that is mostly used or somewhat used by subset, that's still an improvement over the baseline.
00:35:08.960 - 00:35:37.270, Speaker A: Great. Thank you, Richard. Okay, I believe those are all the agenda items we had. Does anyone have any other items I'd like to discuss? Going once, going twice.
00:35:37.730 - 00:35:40.070, Speaker D: I had, I had suggested.
00:35:42.220 - 00:35:42.788, Speaker A: To discuss.
00:35:42.844 - 00:35:47.520, Speaker D: The breakout session about the alto transaction fees back.
00:35:53.180 - 00:35:56.960, Speaker A: Yeah, sure. Should we do that async?
00:35:58.660 - 00:36:16.730, Speaker D: Well, I mean, it's like, it's like, I mean, the specs there, right. It's, we would like to get some feedback from client teams, so I think it would be good to actually schedule that. But yes, we can do it async, but as long as it's getting scheduled.
00:36:18.030 - 00:36:30.010, Speaker A: Sure, we can make sure that happens. Okay, cool. All right. Any other topics?
00:36:31.550 - 00:36:45.540, Speaker D: Oh, just one last thing. There was a discussion thread around l, two transaction statuses and the fact that they're not standardized in any way, shape or form. So I.
00:36:49.440 - 00:36:53.816, Speaker A: Where was this happening? And B. What? Exactly. Okay.
00:36:53.888 - 00:37:26.966, Speaker D: Yeah, the roll call telegram group, because there is, there's no, there's no common definition. What, what? Different statuses, even if they're different, mean. Right. I mean, you can, you can maybe call it different things, but it should, it should, it should, you know, the meaning should be the same, if not the words, even though the word word should also be, be the same, so that we have, you know, equivalent to HTTP codes. That is clear what they mean. Right.
00:37:27.078 - 00:37:27.730, Speaker B: So.
00:37:30.300 - 00:38:22.930, Speaker D: The L two standards working group over on the open, Ethereum open projects community took, made a proposal, made a proposal, put that on ETH magicians yesterday and also on open issue in the rip repo, just to get the discussion going, because I think it would be beneficial to the community if there's a common, common glossary of these terms that everybody sticks to. I think it's something relatively simple that could be achieved fairly quickly.
00:38:30.070 - 00:39:00.420, Speaker A: Okay, thank you. Yep. We can see if there's people interested in adopting something like that. Any further topics? Okay, fantastic. Thank you, everyone, for attending and we'll chat to you next time. A great day.
00:39:01.040 - 00:39:01.820, Speaker B: Thanks.
00:39:04.280 - 00:39:05.900, Speaker A: All right, thank you.
00:39:06.320 - 00:39:06.760, Speaker C: Thank you.
