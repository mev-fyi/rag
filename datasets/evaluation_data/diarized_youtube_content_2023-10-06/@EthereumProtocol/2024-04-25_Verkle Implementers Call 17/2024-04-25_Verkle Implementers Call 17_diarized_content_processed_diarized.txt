00:00:04.040 - 00:00:22.114, Speaker A: All right, let's start things off. Welcome to vertical implementers call 17. This is issue 1024 in the pm repo. Thanks everybody for joining. Starting things off as usual with any client team updates. Would anyone like to start things off?
00:00:25.174 - 00:01:09.164, Speaker B: Yeah, I can start. So yeah, some weeks ago cost tenant six was launched. So some hours after it was launched I was deploying some contracts and mostly to check this witness Explorer thing that I showed in the previous call was working correctly for a particular transaction. I saw something really really weird. At first I thought it was related to the Windows Explorer because it was pretty new, but then I realized that was a get back that we have. So yeah, I fixed that and we relaunched the testnet. The.
00:01:09.164 - 00:02:30.908, Speaker B: Yeah, the testnet and I think since then things are working correctly. After it was relaunched I redeployed all these contracts and tested that the fix was working and seems to be the case. Then Guillaume was sinking a new death node until to have a more recent state. So we can do replay benchmarks and see again like performance bottlenecks, maybe get some insights regarding witness sizes and things like that because our previous replay state was pretty old so we will probably work on that soon. I think we still have to rebase some branches, things like that. But yeah we are going in a good direction. And also we had some time to talk with many people involved in the EOF implementation to share ideas regarding how a world where Merkel, trees and EOF would exist and how they will work together.
00:02:30.908 - 00:02:42.104, Speaker B: And later in this call I will do some small presentation to share those ideas and to maybe get more feedback from people. That's it for me.
00:02:55.784 - 00:02:56.764, Speaker A: Any other?
00:02:57.464 - 00:04:39.190, Speaker C: Okay I can go from ethereum just side. So we basically tried to state less saying the constraint and six network after it was relaunched and so basically we were able to sync most of the box but in few of the blocks we found some issue like 6128 4075 for 4487. And so basically block hash access was missing on this and I confirmed that. So our block hash ring buffer was of the correct length of 8192 and then in 8915 block ext code hash was missing data. Basically we do isempty blockcheck over there and which basically also looks into the balance and once and I think that data is not bundled as of now. And this is something that we have talked about previously and maybe could be addressed in could be decided and addressed in future testnet how it could be handled and in block 6989 basically for self destruct to beneficiary address ETH is doing is doing the beneficiary balance excess, which is not, which was not present in the witness. And this was also basically discussed with Guillem and will be decided how to handle it in the future.
00:04:39.190 - 00:04:45.274, Speaker C: So these are sort of updates from Ethereum J's Gabriel, anything you want to add?
00:04:48.654 - 00:05:20.894, Speaker A: No, not much to add here, except that we've fully migrated to the vertical cryptography wasm. We used to use a RoSAM, but we've migrated to our own report that we maintain that uses Kev Andre's library as underlyings. So this paves the way for an actual vertical draw implementation which will be able to provide a stateful sink as well. So I'll probably begin working on that as well. So yeah, that's it for my additional.
00:05:25.674 - 00:06:02.434, Speaker D: I think I can go next regarding the cost and testnet. So nethermine failed on block around like 30,000. I think I know what the issue is. I'll debug it probably this week, except for that we are working on transition, so I ran some tests locally and some unit tests and everything seems to be working fine. Next we'll be testing the transition on kurtosis, because as far as I remember, I'll just mentioned that you cannot test transitional cortosis. So that's the next step. And once I fix the bug for the question and testnet, then I resume testing the vocal sync as well.
00:06:17.254 - 00:06:19.034, Speaker A: Awesome. Anyone else?
00:06:21.474 - 00:06:58.294, Speaker E: I can vote for Nimas. So Advaita and Agniesz are not present. I will represent them. So they made progress with integrating or implementing local in our execution layer clients. So we have a problem in our client that there's some technical dev there around the database area. So that's what's been sort of slowing us down during the last few months. So we decided to split the effort into two.
00:06:58.294 - 00:08:05.064, Speaker E: Basically the first is to kind of sort of circumvent the current database and just come up with something very basic for constant integration. It won't be used in Mainnet, it will only be used in Constantine, but at least we can proceed with the integration going forward. We are thinking of brand new. Modern thinking started implementing a brand new database that will support both vocal and mercurial ground up with a nice feature that it will be very storage efficient. So that's an ongoing effort. It will take a while, but after that hopefully we'll be able to really support locally. So next up for us is continuing with the basic effort to integrate with consonant, and then we'll also change the gas costs.
00:08:05.064 - 00:08:47.614, Speaker E: We've also been looking at the ET two layer at the consensus layer to integrate as well into consternin. And there we encounter some other issue that, like, apparently occurs currently based on the Capella fog, which is a bit problematic for us. It's a bit buggy. So, like, I'm asking that on behalf of Andre and Ganesh, whether there's any plans to switch to the. In the consensus layer.
00:08:49.514 - 00:08:50.814, Speaker F: Not immediately.
00:08:53.474 - 00:09:03.054, Speaker E: Okay. Because that would have made our lives easier and we could have integrated it quicker on top of the net.
00:09:07.314 - 00:09:49.234, Speaker F: Yeah, it's understood. But unfortunately, guess is is going to have a hard time rebasing. I mean, by the way, this is something that I would like to add to the, to the go update. We have started merging some prs into master, but everybody is aware of how long it takes to get stuff merged into guests. So until this happens, we would have to go through yet another, if not like two really difficult rebases and time is better spent somewhere else. So I understand the discomfort, but. And we would like to fix that, but it's going to take at least a couple of months.
00:09:50.334 - 00:09:51.194, Speaker D: Okay.
00:09:52.454 - 00:09:55.074, Speaker E: All right, well, that's it from our part.
00:10:04.994 - 00:10:05.490, Speaker G: Cool.
00:10:05.562 - 00:10:06.614, Speaker A: Any other updates?
00:10:08.914 - 00:10:12.266, Speaker H: I can. I don't know if you already shared something, Joshua.
00:10:12.410 - 00:10:13.474, Speaker A: No, I have not. Thank you.
00:10:13.514 - 00:10:54.534, Speaker H: Okay, so just on basic side, we started to join the Devnet. So we have a gas cost issue in the block 645 in a specific transaction. So I asked if someone can share me the trace of the transaction to understand what is the difference between. Because I tried to look at the modification in the spec and I don't see any problem. So I think I'm missing something. So yeah, if someone can share a trace, it will help me. We have also Thomas that is working on some optimization in Bezel.
00:10:54.534 - 00:11:14.094, Speaker H: We have more people joining the team. So we have Gary that will help me to work on gas cost and he will also start to look at the proof implementation. So yeah, pause moment. Mainly trying to change the testnet. Yes, Guillaume.
00:11:15.834 - 00:11:24.978, Speaker F: The trace you need. Like, did I understand correctly that you, you just need the gas cost? Because if you do, you can use Ignacio's block explorer.
00:11:25.106 - 00:11:25.394, Speaker B: No?
00:11:25.434 - 00:11:34.894, Speaker H: Yes. The gas cost of each opcode. Because I look at the explorer of Ignacio and I don't see any difference. So I think I'm missing something else.
00:11:36.024 - 00:11:47.284, Speaker F: Okay, weird. Yeah. Okay. I can try to generate you this trace, but yeah, I need to go home first and it's gonna be like Wednesday.
00:11:47.904 - 00:11:55.044, Speaker H: Okay, no problem. I will continue to look, maybe I will find something before, but yeah, thanks.
00:11:58.464 - 00:12:12.874, Speaker B: I should also say that that block number that you are referring to is exactly the block number where I deploy, deployed and executed this more complex contract. So it's, it is probably something a bit more weird.
00:12:15.374 - 00:12:16.794, Speaker H: Yeah, I think.
00:12:19.534 - 00:12:25.194, Speaker D: Yes, I was just saying that I think I can generate the trace and send it on the channel by tomorrow probably.
00:12:26.374 - 00:12:27.794, Speaker H: Okay, thanks.
00:12:35.954 - 00:12:36.786, Speaker G: Cool.
00:12:36.970 - 00:12:54.974, Speaker A: If no other updates, we can move on. I don't think there's anyone from the testing team here this week. I shared their update, their abbreviated update in the chat. If anyone has anything else they'd like to add as far as the testing side of things go, otherwise we can.
00:12:55.814 - 00:13:19.166, Speaker F: That's me. I don't know how different it is from the update you, you shared. Let me just quickly have a look. Yeah, basically they want to. Yeah, they start using the new, the new input that we give them seems to work, but they've been delayed. They will start. They're trying to get something ready by the, by the interrupt.
00:13:19.166 - 00:13:20.914, Speaker F: That's, that's what you brought me.
00:13:22.754 - 00:13:36.174, Speaker A: Perfect. Cool. Okay, well then I guess next up is the calcinent updates v six status and the seven wish lists.
00:13:41.674 - 00:14:38.054, Speaker F: Yeah, I think, but I've been disconnected from things that v six doesn't really have any problems, so. Okay. Galindo reported some, some issues that we can discuss if need be. Otherwise we can discuss them offline. For the seven, my wish list would be to implement. Well, okay, first confirm that there are some issues on the Devnet six and also, and also fix them. But I know Avi and Uniswap were talking about running their scripts to deploy on the testnet, so I would like to do that on Devnet six if we could, because I think it would be quite useful to find some, like more bugs and try to keep Devnet seven as the most, as the most stable testnet we've had since number three, I think.
00:14:38.054 - 00:15:01.074, Speaker F: And yeah, another thing that I would like to have on the Devnet seven would be try to implement the fill cost, which so far was not, was not implemented. I think it's time to tackle this before the interrupt so that we can have some proper estimate of what the gas cost really look like.
00:15:11.414 - 00:15:12.194, Speaker A: Cool.
00:15:12.974 - 00:15:13.478, Speaker F: Okay.
00:15:13.526 - 00:15:15.234, Speaker A: If nothing else on that topic.
00:15:17.234 - 00:15:17.594, Speaker G: Next.
00:15:17.634 - 00:15:21.834, Speaker A: Up, a presentation from the portal network team.
00:15:21.914 - 00:15:25.434, Speaker G: Milos, hi, can you hear me?
00:15:25.594 - 00:15:26.334, Speaker A: Yes.
00:15:26.794 - 00:15:31.294, Speaker G: Okay, I'm sharing the screen. Can you see the slides?
00:15:34.554 - 00:15:36.094, Speaker A: Yep. Perfect. We can.
00:15:36.674 - 00:16:11.100, Speaker G: Okay, cool. So I already shared the link. Maybe somebody can also add it to the chat. I share it on a discord channel as well. Basically I'm doing a research on how the worklet state can be stored in a portal network, meaning the archive state, entire archive state. I will give a bit of background on what portal network is for the one that are not familiar. Basically the portal network is decentralized API for accessing the, uh, archive node kind of API.
00:16:11.100 - 00:17:02.316, Speaker G: For the Ethereum state, it's actually three networks in it, or sub networks, beacon history and state. And part of this research is focused on the state network. Currently what is being in development is the, is the state mercle current, basically current state of the Ethereum network. And how it works is that every node in the network has a node id, and every content has a content id. And basically every node stores the content that is close to its own id. That way you know where to look in the network specific content, like you know which other nodes is very likely to have it. And for the mercury tree and the current Ethereum state, there are three types of content.
00:17:02.316 - 00:17:59.194, Speaker G: The accounts tree node, which is basically the tree node of the main tree, contract storage tree node, which is the tree node that belongs to contract storage, and the contract by code. So very simple structure, nothing too complicated there. I'm going to skip the slides for the worker tree, because I assume everybody here is familiar with that. So the naive approach for the worklet case would be to basically do the same store every tree node, but that has a problem. The problem is also present with mercury tree, but is less noticeable or less impactful. And the problem is that if you have a node, the tree node that is updating only one element from one block to another, the content id will be completely different. So it will store it on some completely different space in the network.
00:17:59.194 - 00:18:59.808, Speaker G: But you are duplicating how much storage the whole portal network actually needs. And if the branching factor is 256, as a case of the worklet tree, you basically have a lot of waste in memory that way. For the mercle three, the branching factory is 16. So that's still present, but not as noticeable, let's say. And that's one of the main problems. And the solution that I came up with was to split each node basically into two layer mini kind of tree. So every node, instead of having 256 children, there will be two layer nodes where basically they would accumulate the weakness thanks to the Patterson hash, and where basically on the lower level, each node will just be a 16 consecutive values, and the one level above will just combine and add them.
00:18:59.808 - 00:20:17.504, Speaker G: We can see that the others are basically just pedestrian hash with 16 elements and everything else being set to zero, and the last commitment is just the sum of them. So there is no pedestal hash involved. There is needs to be a bit of, let's say careful engineering here, because the commitments that are being passed through on the lower levels are actually the elliptic points, not the scalar field values. But it should work, it should be possible to do it that way. This process can again be split into three or four layer trees with four elements, but that also increases the lookup value on the portal network, because you would have more layers to go through to find the value that you're looking for. But it's something that might be explored of how long that would take versus how much saving in memory would actually bring, et cetera. Another approach that was being considered, which is more similar, how the execution layer clients stored the state, and that is that the same node on the same path would be stored within the same portal network node or client.
00:20:17.504 - 00:21:52.126, Speaker G: And so basically the same node will be responsible for the whole versions of the same three node. That one breaks one of the main assumptions, and that is that the hotspots that it creates hotspots basically some nodes would have way more activity for particularly popular like states that are updated very frequently. And basically that breaks like a symmetry and linear growth between the radius that portal network node wants to store and the storage size it actually will need. There was also an idea that we basically discussed, even for the Mercle case, is to together look at the epoch where not the epoch, not in a sense of ethereum layer network, but portal network kind of epoch, where the epoch would be longer period, and just the nodes would store the three nodes within that epoch and the local changes. So basically something in the middle. This has a problem, because every time there is a change from one epoch to another, you have to duplicate the entire three states across the network, which is very bad. This is probably some approach that we might consider more when we go into storing only the header of the state, which will be probably stored in a different way and optimize for that specific use case.
00:21:52.126 - 00:22:20.264, Speaker G: And this is probably the path that we will go for that. But for storing the archive state, this is probably not good enough. And that's relatively short presentation from my side on what we are doing and how we plan to go forward. Basically I just wanted to give an update and maybe ask if there is anything that anybody sees, let's say, wrong with this approach, or any suggestions or anything like that.
00:22:23.844 - 00:22:24.784, Speaker A: Looks great.
00:22:27.854 - 00:22:53.354, Speaker F: Yeah, I have a couple of questions. So the whole structure was basically made so that it would make things easier for the portal network. So precisely because you requested this, this is why we end with this tree structure. So I'm just a bit confused why you need to come up with this two layered tree structure. And also, isn't that going to be extremely expensive to compute proofs?
00:22:55.794 - 00:23:38.180, Speaker G: Well, it depends. Like, the thing is, I'm not sure what we will need proofs for. That's the first thing. The second thing is, if you have a tree node, and most likely from one block to another, only one value will change in that three node except the root and maybe a first layer or first two layers. So then you will have to duplicate the entire 256 values and store it somewhere else in the network. And that's a very high duplication factor of how much total storage the core portal network would need to keep the entire archive state. So I guess that's why, that's the benefits of.
00:23:38.180 - 00:23:45.584, Speaker G: That's the idea of splitting it into multiple layers. And what was the other question?
00:23:47.764 - 00:24:09.564, Speaker F: Yeah, just why? I mean, the. Okay, the proof, right? You answered the proof. I mean, my question was like, are you, are you no longer interested in the structure? You, you lobbied for us. Okay, maybe lobbied is a big word, but you asked us to, to adopt, basically.
00:24:11.744 - 00:24:14.924, Speaker G: Can you clarify which one?
00:24:15.964 - 00:25:03.764, Speaker F: Right, so this whole single tree structure with 256 values and everything merged into a single tree, that was a request by Piper. Right. The reason why we have this, we no longer use the per account. I mean, there are. Okay, now we found that we would, we could adopt this, the structure, for our own purpose and make a lot of optimizations based on it. But until Piper made this request, the whole point was that the structure of the tree used to be an account tree and storage trees below them. So we changed the whole structure to adapt it to the portal network.
00:25:03.764 - 00:25:17.044, Speaker F: I'm just wondering, are you suggesting we change the whole structure of the tree ourselves, or is it just an internal portal network representation? That wasn't clear to me. This 16.
00:25:19.184 - 00:25:58.744, Speaker G: Right, right. No, this would be just the portal network internal representation. So how the structure would be stored within the portal network. And this approach, I can probably go back there with this approach. This would basically represent one node of the worklet tree, how it would be represented within the portal network, and the end up commitment would be the same as it currently is, because it's just linear algebra combined together. So there will be no change, or this doesn't require any change of the worklet tree structure. This will just basically be how it will be stored within the portal network.
00:25:58.744 - 00:26:51.420, Speaker G: I now understand what you mean. Like embedding the state, the smart contract storage within the same tree I actually wasn't there. I wasn't part of the portal network at that point, so I'm not really sure all the reasons, I mean, it definitely helps us from having more unified storage across different areas and stuff. So it's still useful. This proposed change doesn't actually affect anything regarding the worklet tree design of the like for the execution plants or anything like that. This is just how we would actually store it within the portal network. And regarding the proof, it would basically be that we in portal network would most likely not use this kind of proofs.
00:26:51.420 - 00:27:57.344, Speaker G: We have a system where currently, with mercury, whenever the data is gossiped within the portal network, the data that is gossiped has to be proven. That is valid. And the way it currently works with the mercury tree, you pick one leaf that you want to store, that was updated, and then you gossip basically that leaf and all of its parents at once. And how it works is that the node responsible for that leaf would receive the leaf, verify the proof, and then it will just remove the leaf and gossip the parent of that leaf to the rest of the network. And that would sort of like recursively propagate all the way to the root. So basically when we gossip, we don't actually, we actually want to have all of the parents already provided there so that we can recursively gossip them. And for the worklet tree, I guess we would probably preserve the same logic, which means we don't actually benefit much from the, from the ziki proofs that we can actually get from the worklet trees.
00:27:57.344 - 00:28:35.304, Speaker G: We didn't do much of the work in that area of whether we can optimize something there. But that's the initial thinking. We would still just propagate the leaves and all of the parents with the full content, such that we also have to, because we also have to propagate them through the network as well. Anyway. So that's kind of like the thinking that we might actually not. But I agree, this kind of proving scheme would probably not work if you want to concise proof in a later stages. And you will still have to iterate through the tree to find the node that you're looking for.
00:28:37.924 - 00:29:09.080, Speaker F: Right. Okay, thanks for clarifying another question I had, but you already kind of touched to it just now. Uh, you know, like the, the top of the tree. Like your problem is the churn at the top of the tree. But we already have this kind of information attached to the block, right? So if you see your information inside the block, that means it has been touched. But if you don't see it there. It's, you need to find where it was last touched in the block.
00:29:09.080 - 00:29:43.344, Speaker F: So yeah, you could, you could, like, if you want to write an archive node or, okay, not a node, an archive network, you would be able to get those notes directly from the proof. So you could get it by doing some archive of those proofs themselves instead of reinventing your own, your own method. I mean, it's just food for thought for you. You know best what you need. But maybe there's some potential saving for you either in design or storage by looking at what's in the blocks already.
00:29:44.644 - 00:30:55.100, Speaker G: Yeah, I briefly was thinking about that. The problem is that the portal network is designed such that the nodes in the network can join and leave kind of randomly. And they were not going to go and fetch all historical blocks and keep updating themselves. So there has to be a mechanism where you can just join and then the content that is, that is close to your node id should be given to your city, to you through some means, and you can keep following what is happening currently with the head of the network. But if you disconnect, like there is no strong requirements or expectations that every node will observe every block header all the time and have everything updated. And that's why I didn't actually find a way to actually fully use that information in a, in this kind of distributed structure. But definitely something that I was looking into and still thinking about, like, because we will have, we call them bridges that will actually going to seed the data into the network and they are probably going to use this information very heavily.
00:30:55.100 - 00:31:03.864, Speaker G: But from individual nodes point of view, it's a bit harder because they don't have the same expectations, I'd say, from them.
00:31:05.724 - 00:31:27.624, Speaker F: Right? Yeah. One last question. In this regard, the churn that you expect, what is the depth that you expect it to be a problem? Is it like depth one, depth five? Well, five is going to be hard to reach, but for a vertical tree, but yeah. Do you have any idea of where the churn starts being a big problem?
00:31:29.404 - 00:32:47.610, Speaker G: I mean, nothing seems to be a big problem per se, because like, I'm not sure what you mean by a problem because we will just store it the way we kind of designed the storage layer to work. The problem is the idea is that if you want to find a balance on some archive states, like million blocks away in the past, you would know the state root and you would have to traverse the tree, and you would do that by issuing the queries through the portal network for fetching each tree node on the way to find the leaf that you're actually interested in. And if we do this splitting, then the deeper the tree is, the more query you need to make on the portal network, meaning it will take longer time to find information you're looking for. So that's a trade off. Technically we can go all the way to the binary tree, but then that would just take way too long. And on the other hand, if you have a big branching factor, then you increase the demand from the whole portal network. Storage, like combined storage of the entire portal network, you increase the requirements of how much it should be to support everything.
00:32:47.610 - 00:33:02.534, Speaker G: So there is a trade off on latency versus the total global storage. There is no like a strong requirement that it has to be smaller than this or bigger than this. It just like depends on like a preferences and trade offs.
00:33:05.474 - 00:33:23.934, Speaker F: Okay. Yeah. I'm not trying to redesign your entire thing right now while you're putting you on the spot. One last question I would have, but you know, you were also talking about potentially meeting in Prague. Would you by any chance be in Berlin before or after?
00:33:26.794 - 00:33:37.734, Speaker G: I'm not, I don't think anybody else from the team would be there. I know the entire, almost entire portal network will be in Prague, but I'm not sure if anybody would be in Berlin.
00:33:38.434 - 00:33:39.294, Speaker F: Okay.
00:33:39.794 - 00:33:40.106, Speaker D: Yeah.
00:33:40.130 - 00:34:16.483, Speaker F: Okay then let's see. But yeah, I think there's some iterations like we could try to iterate on that if you're interested. One question, last question I had because I don't want to hog the entire call. Would it be possible for you guys? Do you really need to have every single block or could you just have the history for, let's say every thousand blocks and then ask people to resync the node based on the proof from the blockchain? Would that be a model that would be realistic for you guys?
00:34:17.943 - 00:35:34.474, Speaker G: I mean, we would have to think of the use cases of like, it depends, like how exactly would that be stored? I guess we currently weren't thinking in that direction. That's one of the ideas that something similar, let's say, is this idea of having epoch and path, let's say every 10,000 blocks or something, and then every node would store the state of a tree node at the beginning of that epoch and all the consecutive updates. The issue with that is that we encounter is that we would have to reduplicate at the beginning of every epoch. We would have to reduplicate the entire, basically state where basically we would have too much snapshots that we will store across like the entire portal network like we can. We will still have to do some calculations to figure out how much exact because as you keep adjusting the epoch like making it bigger, that problem becomes smaller. But then the problem of the hotspots and having some notes having to do way more work than the others becomes more exaggerated. So it, there is also a trade off in that design.
00:35:34.474 - 00:36:10.974, Speaker G: So it depends on the use cases. What exactly would that be? It's something that we kind of want to explore. But this approach of using the path for the content id, not just like a commitment, is something that we are most likely going to pursue for storing the head of the block. That way you can achieve maybe off one request for the head state and that's something that we are. That's going to be a completely separate discussion and research in that area. It's a bit lower priority for us at the moment.
00:36:15.074 - 00:36:16.458, Speaker A: Cool. Awesome.
00:36:16.626 - 00:36:17.682, Speaker G: Thanks a lot.
00:36:17.858 - 00:36:40.974, Speaker A: Thank you Milos. Okay, since we only have 22 minutes left, we can try to get to the rest of this quickly. Next up on the agenda, there was a topic around the 32 byte chunking ideas for Verkle, I think Ignacio maybe. Was this you on the agenda or Gill?
00:36:41.914 - 00:37:09.094, Speaker F: I think it was me. Okay, interrupt me ignatio if. If you had any. Yeah, if you thought that was you. Yeah, basically. Not much to say what I accept that in order because of all this EOF circle like considerations we had, I was. It's something that had been talked about with Pavel a long time, a long time ago.
00:37:09.094 - 00:38:18.968, Speaker F: I wanted to finally take some time to try to implement it to see if we could get more information as to the benefits of packing everything, both to evaluate the impact that UF would have and also to see if it wouldn't be an optimization that is interesting for legacy code, legacy bytecode as well. So I want to do this. The question I had for the community would be do we want to put. So in this model what we would do would be to put the bytes, instead of putting the bytes at the beginning of each chunk, like the push data bytes at the beginning of each chunk, we would put that in a buffer somewhere. And the question is where should this be? Either we put it at the beginning of a 128 block. So yeah, like the first 128 leaves. The problem is we have to somehow store the actual start offset.
00:38:18.968 - 00:38:56.360, Speaker F: Or I guess we could compute it from the size. That would be possible, but then we probably. That has a gas cost impact because we would need to assess the size every time we need to touch the code, which is not currently the case. Sorry actually it is in the current EIP, but will not necessarily be the case in the future version of this EIP. Or we could put it in the reserved. Well, some of it, because that's the problem. We could put some of it in the reserved leaves of the header group.
00:38:56.360 - 00:39:25.334, Speaker F: But in case, even if the code is currently 24 kb, that would not fit in the available space. So we would need to find a way to overflow. Or we add yet another offset. And then that means that every time we read the code, we have to go get it somewhere else. So that's an extra cost. I honestly am not sure which one is the best approach. They all have their drawbacks.
00:39:25.334 - 00:39:34.254, Speaker F: I wanted to know if anybody had any opinion as to where this section, let's call it section, would have to go.
00:39:41.074 - 00:40:06.344, Speaker B: I mean, I agree that it's very hard to predict. Probably we will need to test the two ideas of like putting this information in the header and not put in there. Because. Yeah, it kind of depends on the contract, I guess, and the length of the contract. Right. Because the amount of bytes that you need is, depends on how many chunks the contract has. So.
00:40:06.344 - 00:40:14.264, Speaker B: Yeah, sounds like it will depend on each contract, really?
00:40:16.484 - 00:40:16.932, Speaker F: Yeah.
00:40:16.988 - 00:41:16.904, Speaker B: And also like, another thing, another thing is that if we pack together each metabyte bytes, my feeling is that whenever you read one of them, you will be indirectly, like loading the metabolite of tanks next to each other. And I don't think that's entirely useful because this metabolic byte is only needed whenever you do a jump to really check that this jump is correct or valid. And the chance that you, whenever you execute a chunk, you will jump to a chunk that is really next to that chunk. I know how probably probable that is, how much you will leverage these other meta data bytes that you have read already. I know if that makes sense, it was a bit of a tongue twister, but.
00:41:17.764 - 00:41:53.804, Speaker F: No, no, it's only, it's totally understandable. Yeah, that's the biggest problem. And in fact that's the problem with the whole section reading in EOF. I mean, this is what we, I'm sure we will discuss that in just a few minutes, but I think it's still worth being tried so that we have concrete data. So I don't know where to start, but I'm afraid the answer is we have to start all two or all three, depending on how you look at it. I'm just not sure where to start, really. I think, yeah, I think I will first try to do it.
00:41:53.804 - 00:42:30.716, Speaker F: Put some of it in the header and then somehow use the same technique for the overflow. Yes, it's definitely going to be extremely expensive to if you have to read an extra section just to every time you read a new chunk. Sorry, a new chunk every time you read another chunk. And in fact, this is exactly what Vitalik said in the EfV. Zero Pr. But yeah. Okay, let's say unless there's another anybody else disagreeing, I will start with that.
00:42:30.716 - 00:42:38.034, Speaker F: I will put it in the account header and at least it will serve as a base point and we can try to optimize afterwards.
00:42:44.734 - 00:42:51.834, Speaker A: Cool. Okay, last up on the agenda, a presentation on virtual and EOf from Ignacio.
00:42:54.174 - 00:43:01.754, Speaker B: Yes, I will share my screen. Can you see my screen?
00:43:01.934 - 00:43:03.174, Speaker A: Yep. Perfect.
00:43:04.114 - 00:44:27.844, Speaker B: Okay. Yeah, so these last days we have been chatting with some people from UF, trying to exchange a bit of our thoughts regarding how a world where Verkle and Uf we live together will look like, which are like some concerns and things like that. So the idea of this presentation is mainly like sharing all these ideas for others to be aware. So a bit of context that probably everybody knows already, but UF is being considered for Prague, vertical is being considered for Osaka, Prague comes before Osaka. So the idea is to try to see how these both AIP interact, interact with each other and try to see the impact of them on ethereum users on the vertical three spec implementation and also see if this might put some create maybe some problems for UF adoption. So it's mostly like an exploration of all these topics to give like a really, really quick DLDR on Berkeley trees only like focused on the discussion of this presentation. The main motivation of oracle trees is to make stateless clients viable.
00:44:27.844 - 00:45:44.464, Speaker B: And for that to happen, vertical trees needs to introduce this concept of an execution witness, which is basically all the data that you need for replaying a block in order to verify it. Because stateless client do not have any ethereum state, so you need to provide them all the needed data to execute the block. The important part for this topic is that Berkeley trees now include the smart contract code as part of the state in the state tree, which is something that it isn't true today. This smart contract code is usually like in the client's database, so it's not really in the tree. So with that point in mind, there are like two really obvious conclusions. The first one is that the bigger your contract is, the more gas you will have to pay, at least compared to the gas that you pay today, which is 200 per byte because you had to write all this code into the tree. Which is more expensive.
00:45:44.464 - 00:46:49.006, Speaker B: So that's the first one. And the other like insight in quotes, is that whenever you execute a transaction, the more byte codes that your transaction execution needs, the more gas you will have to pay, because we have to include all these bytecode in the execution witness. This is a cost that is new for users today. They don't have to pay for all this, of course. This is something that contract developers and compiler devs will have to think about, because it's a new optimization dimension that they should account for. But the, the bottom line for broker trees perspective is bigger contracts and needing more bytecodes will mean paying more gas. Okay, so that was like a DL tldr for broker trees.
00:46:49.006 - 00:48:13.876, Speaker B: If we do the same for UF, which I'm not an expert on, so have that in mind. The idea of EOF is to add more structure to the smart contract code. So it introduced some new sections like the header and type sections. It also includes a lot of new instructions and ideas to make the code execution more efficient, and also allows the client to avoid doing some random checks, just like the jump desk analysis, because like I think that every jump should be valid. So that's like a recent amount of work that clients shouldn't do at runtime. And all this structure allows also to make like the EVM like spec more like evolve better because you can add remove features or instructions without breaking existing code. So all these benefits have also, or might also have some effects, such as like maybe bigger code because we have to include like new sections, and also maybe having to read more byte codes while executing transactions, since we might have to read totally or partially these new sections.
00:48:13.876 - 00:49:31.724, Speaker B: Okay, this is like open questions. So if we merge together these two insights of UF and vertical regarding gas concerns, we could say that EOF might increase the call size and might need more by coast to be read during contract execution. And these two things are exactly the things that burko trees make more expensive because we need to add the contract to the tree and also include the vehicles in the witness. There are a lot of caveats here, because EOF, as I already said, bring new efficiencies. So for example, the sham desk instruction can be avoided. So the contract size might be smaller from that angle, and that might be, that might offset these new headers that will be included. Also, there's like a lot of work that compilers can do to really optimize for the code layout or how these UF sections are placed in the tree, such that you have to read the fewer number of chunks possible.
00:49:31.724 - 00:50:58.934, Speaker B: Now, how much all these efficiency gains or potential optimizations can offset the new costs. Like the thing that we are trying to figure out. A potential idea to explore this a bit is to try to come up with a client that have both at least a minimal implementation of EOF and Berkeley trees, launch a testnet with that client, or maybe just run a simulated chain locally and try to deploy a set of reasonable contracts, both compiled with legacy code and with EOF, and just like execute transactions on these contracts and compare the gas costs. So ideally we will like to have compilers that are already accounting for both EOF and vertical trees optimizations. Because even if you have compilers optimizations only for vertical trees, there will be missing considerations that will make sense for UF. And if you have only UF compilers that have optimizations only for UF, that will be missing all the cold chunking optimizations. So asking for this is very hard because this is a lot of work and compiler teams have really probably limited bandwidth.
00:50:58.934 - 00:52:15.874, Speaker B: But yeah, at least even if we don't have these optimized compilers, at least with this kind of test, we can have like an upper bound of the gas difference. And if we try to do this kind of testing, maybe we can have some feedback loop to improve the eof and broken specs so they can place better with each other. There's another angle apart from gas cost risk, which might be EOF adoption risks, which this entirely depends of if EOF versus legacy has a big gas difference. So kind of depends. But if there's a gas difference that is reasonable. Maybe users will simply try, whenever they deploy a new contract to deploy with legacy code and with EOF, and simply do the deployment with whatever setup it uses less gas for them. And that might be a bit annoying because this might make harder to stop accepting legacy code in the future, which I guess is like the idea, because if that isn't the case, we will have to accept that we will have two modes of contract execution forever.
00:52:15.874 - 00:53:38.964, Speaker B: I think that will be, I mean, not entirely nice for complexity reasons for clients, I guess, not only for full clients, but also for stateless clients. We should also be aware of these, like two modes of contract execution. Regarding work of tree complexity risks, one open question, which is a bit related to what Guillaume was talking about before, is we have to figure out how code chunky will work for UF, which will probably be different than for legacy code. So there's like a chance that we will have like let's say double spec for legacy and EOF executions in the vertical tree spec, because depending on the contract type, we have to explain how chunking works differently and things like that. Also, UF introduces a bunch of new instructions, and depending on these instructions, we will have to figure out probably new gas costs for them, if there are any that indirectly risk the tree state. For example, today instructions like balance and block hash indirectly require state from the chain. So these instructions also require some extra gas cost considerations because they increase the witness size.
00:53:38.964 - 00:55:01.722, Speaker B: And finally, Duchodee complexity of EOF, there's like a reasonable chance that this can delay osaka fork. And while this happens, the state growth is still of course happening. And maybe the market, the state transition between the merkle potential tree and the burka tree will take a bit longer. Yeah, we will, we will prefer this to take the short amount of time as possible, because there's a lot of logic happening and there's like a risk here. So I think that is fair to say that it might seem that EOF and Berkeley implementations are non overlapping because they are seemingly touching different parts of clients. But the point here is that their effects are really composing with each other, mostly like the gas cost effects. I think that the original UF assumption was that doing all this code structure changing wouldn't affect much gas cost because the code wasn't really part of the tree.
00:55:01.722 - 00:56:17.228, Speaker B: But since vertical trees need to include call in the tree for stateless clients, then we have the situation in which these seemingly different eips interact with each other at the gas layer. Like an open question is maybe more minimal version of UF can help here? I don't know, kind of depends what a minimal version of UFO even means and if it really like helps with any of this problem. Just to clarify, again, this presentation isn't about saying that there is a problem or a big problem. The only thing that we are trying to share here is that maybe we don't know, and a lot of these questions might have like it depends answer. And it maybe would be helpful to really find out if any of these things are things that will backfire in the future. And yeah, trying to have more like real data to support or simply discard these concerns. So that's it.
00:56:17.228 - 00:56:26.474, Speaker B: And also like the idea to share all this with other core devs and get their feedback and ideas about this, these, these things.
00:56:38.814 - 00:56:40.874, Speaker A: Any questions or feedback?
00:56:41.654 - 00:57:39.274, Speaker I: Well, I have a question. Well, you mentioned that this is the first time and I got aware of the OauF and you mentioned that. And the goal of EOF is to no longer keeping the codes in the tree, in the Virgo tree, I mean, no longer kept in the storage. So how could a blockbuilder to know which code or which transcript to execute or to verify? And because then, because even if the block, the blockbuilder owns the whole story, the entire storage, and there's no code included in the tree. So how. So how did the block builder know which code will be triggered in the transaction builder block?
00:57:41.734 - 00:58:01.486, Speaker B: So I'm not sure that question is related to this topic. Like blockbusters will have all the states of the chain, so if the contract is an UF contract or a legacy contract, they will know anyway. And I'm not sure this is related to UF or verkle, really not.
00:58:01.550 - 00:58:25.454, Speaker I: Not for vertical. Just to make sure that if there's no code, no code stored in the tree, installing the storage. So in which form was a code replacement to the blockbuster or something else?
00:58:25.834 - 00:58:45.934, Speaker A: Maybe just since we're at time, we can continue discussion async unless anyone wants to. I'm just. I'm curious, I guess, to stay on the topic of EOF. If anyone else here not to. Yeah, cut the conversation off, but I know Dano is here and I don't want to put you on the spot, Dano, but curious if you have any quick thoughts.
00:58:46.274 - 00:59:23.024, Speaker D: Yeah, are the slides available? Because there are some things I can clarify. I think the biggest thing to mention about EOF is that it's core, it's just a container format. We don't change fundamentally how the EVM works. So, you know, one of the big concerns about how are we going to keep two versions of EVM, it's going to be the same operation execution loop, and you're going to be asking the same questions, like, instead of asking, well, am I Shanghai? Am I by Zantium? When you operate an opcode, you might have an extra check to say, is this frame EOF? And if so, execute. Otherwise don't execute. So a lot of the core logic in there is going to be absolutely the same. And that was one of the goals of EOF.
00:59:23.024 - 01:00:30.344, Speaker D: The big goal of EOF was to build this container format for the EVM so we can do things like add operations that use immediate arguments, which is impossible to do safely with the current legacy system. There have been high level requests that include other large goals of the EVM future into it, which include things like removing the ability to introspect code and removing the ability to introspect gas while you're running, which will open up a lot of things like the ability to willy nilly, change the gas schedule without fear of breaking contracts that are deployed. If you can't read the gas, then the solution is just to always provide more gas. So those were some of the things. That's why it looks like there's a lot of opcodes, but what we're doing is we're just replacing opcodes that in a way that can handle this container format. And they operate exactly the same, just they might use immediates, they might not have gas introspection, they might need to use a data section to get their embedded data. So that's where a lot of these things come from, is taking existing behavior patterns and containerizing it.
01:00:30.344 - 01:01:24.034, Speaker D: There was a prototype mega EOF compiler written by solidity before mega EOF, before big EOf got pulled from Shanghai. And in some tests I've done there I've seen between one to 3% gas, not gas opcode size reductions of bytes, but I don't know how that's going to translate to operational. There may be less bytes, but they're maybe more operated more effectively. So the biggest open question we won't really get answered until we start getting quality implementations from solidity and from Viper against the current EOF spec. I'm hopeful it's going to be less, but it might be large, but it won't be anything outside of the one to 3% I'm expecting. Either way, it's going to be statistical noise for a lot of these. So I'm not expecting any major large size or operational gas cost gains.
01:01:24.034 - 01:01:44.314, Speaker D: Anything that is costed in the header tends to be offset elsewhere, but because of contract size restrictions you really can't, you know, multiply those gains within a contract. So it's, it's going to be mostly a wash, is my expectation. I think those are a lot of the high level concerns out of there that what my expectations are going on right now.
01:01:46.894 - 01:02:07.238, Speaker F: I know we. Please share the last slide again, because there were a couple of questions that I'd like to go over. The fact that it doesn't change, I mean, the fact doesn't change anything about the execution is demonstrably wrong, but. Okay, how is it wrong?
01:02:07.326 - 01:02:08.754, Speaker D: What are we fundamentally changing?
01:02:10.334 - 01:02:18.204, Speaker F: We just showed you that the gas cost was going to change, it was going to be affected. So yes, it's demonstrably so that just demonstrated to be wrong.
01:02:18.324 - 01:02:29.624, Speaker D: That's detail. The fundamental model is the same. What I'm saying is the model is the same. The details might change, but the model is going to be the same. We're going to burn gas. We're going to have message frames. We're going to call it contracts.
01:02:30.964 - 01:03:05.264, Speaker F: Right. So the devil is in the details. But okay, let's save this one for the ACD. My question was, like, the one I'm interested in is, you know, I know you've been asked in the past to do EOF, like a minimal version, and then someone swooped in at the last minute and asked for the big version of EOF, but I'm still. Let's say the context has changed. That was before Cancun. Now Cancun is there and verkle is next.
01:03:05.264 - 01:03:29.514, Speaker F: What could we get as a minimal version of Uf? Like, let's say EOF comes in three versions, like three forks or two forks. What could we get that would still be useful and at least be small enough a scope that we could study it and not go back and forth on the question forever.
01:03:30.214 - 01:04:07.282, Speaker D: So first thing you do in three forks, you're going to have to maintain three more growingly different versions of the VM. And if we have to break the contract format multiple times, then we'll have to support those multiple contract forms forever. One of the requests is if we're going to break these things, then we break it once. And there's only one difference between an EOF contract and legacy contract. We don't have a legacy contract. And then a container contract that has gas introspection and then a container contract that does not have gas introspection still has code introspection and then one that has everything that we need. So if we have those three different container things, that's going to be a huge technical debt.
01:04:07.282 - 01:04:45.226, Speaker D: So the first requirement that is a lot of credence is if you're going to break every break things, break as much things that one time is possible. So we need to get banning code introspection and banning gas introspection and preparing for address based expansion in the first go. Otherwise forever, we're going to be supporting two or three different contract formats. I mean, we're already supporting two different versions of the access list transaction because they shipped like just a few months apart. So we got access list, we got 1559. We're trying to avoid this by doing it in one big go. Otherwise everyone everywhere is going to support all three versions forever.
01:04:45.226 - 01:05:26.134, Speaker D: So to split it up into smaller parts actually increases the surface area of what downstream consumers are going to have to support. It also increases the surface area for testing in the surface area for attacks. There might be some design issue in v two that's not there in v three, and then we have to worry about all the details on that. So while it might make it easier to deploy and get out to do it in three different steps, it significantly increases the risk and surface area of where things are going to need a change. So that's my concern with splitting it up. I mean, I think that was the initial pitch. When you start looking at the security and maintenance issues, it creates more of a problem than it solves.
01:05:27.834 - 01:06:06.394, Speaker F: That's a fair enough assessment, although I would counter that, yes, we want to break as little, well, we want to break stuff as few times as possible, but if the breakage is orders of magnitude bigger than each individual breakage, I'd rather have several smaller breaks than one big break, but. Okay, so just, just to be clear, I understand your, I understand your, your point that it's completely fair, but just to have my question answered, the minimal, minimum version of EOF, what would that be?
01:06:07.254 - 01:06:42.004, Speaker D: A minimum version of EOF would be one the compilers would actually write compilers for. And I think that includes we could probably get rid of the TX create and opcode, but I think everything else there, we've tried to pare it down. So we only put the minimal features in there. And if we cut much more out of it, compilers won't pay attention to it and they're our primary customer. If compilers will compile the EOF, it's a dead feature. That's. We back in Berlin, we had subroutine opcode proposed to go in and it got pulled very late because solidity came in and said, hey, we won't use this.
01:06:42.004 - 01:06:51.064, Speaker D: So the real litmus test on how much can we cut is how much can we cut before solidity? And Viper will say, we're not going to use this.
01:06:52.764 - 01:06:53.244, Speaker B: Okay.
01:06:53.284 - 01:07:21.264, Speaker F: And I think, what's there, okay, so in your view, like, at most we could, we would still have to ship 18 instructions. And on top of that, okay, just something. I remember that story, there was, that was not eof at the time. There was eIp, I'm trying to remember 617 or maybe that's the. Anyway. Yeah. Why 615? Thank you.
01:07:21.264 - 01:07:29.904, Speaker F: Why do they feel this, do you know, like, why do they feel that static jumps are not good enough for them?
01:07:32.364 - 01:07:37.372, Speaker D: What do you mean static jumps? Dynamic jumps. Why are dynamic jumps. Problem, right?
01:07:37.428 - 01:07:37.988, Speaker F: Yes, sir.
01:07:38.076 - 01:08:04.704, Speaker D: Dynamic jumps, it's impossible to analyze in static time. You can run code in there that has order n squared order, exponential complexity, jumps in analysis to prove it safe. Because if you jump from something on the stack, you can introduce data from outside of the code to determine where that's going to jump. And you can write these horrendous compiler bombs that absolutely frees up your block production.
01:08:06.444 - 01:08:23.004, Speaker F: Right. So sorry, that was actually not my question. But thanks for the refresher. That was useful nonetheless. My question was why? Why? Do you know why? Solidity. And I think it was Christian at the time who said 615 is not going to make in because we're not going to do that. Why would they not do this?
01:08:24.024 - 01:09:03.073, Speaker D: So one of the issues had to do with immediate arguments. When I do my EOF presentations and conferences always start out with showing how if we were to add an immediate for static jumps today, how it would change the interpretation of other contracts that may already exist on the blockchain. And if this is known ahead of time, trolls will come in and create problems on a chain. We've seen this before in some other spaces we've tried to preserve, such as preserving the EOF zero zero header. We wanted to have it just be Ef, but someone came in and put in EOf emojis because they thought it would be funny. So people are going to come and control the chain. And then the Shanghai attacks were absolutely there just to slow down the chain.
01:09:03.073 - 01:09:33.904, Speaker D: So we can't just add in static, you just can't add an immediate. We also can't add in code separation. Code and data separation because there's no effective way to show what the difference is between data and code. You could jump into code at the end and there are a lot of contracts out there that interleave their code with their, with their static stuff. And we can't just shut that down. So we need to figure out, you know, it's some of the existing patterns that are in use. We can't just invalidate because they're already out there live on the chain.
01:09:33.904 - 01:09:36.274, Speaker D: Yeah.
01:09:37.854 - 01:09:54.678, Speaker F: Okay. Yeah, that clarifies. Thanks. I will try to talk to Daniel because, you know, last time I talked to Daniel about this stuff, he seemed pretty psyched about it. So. Yeah, I mean, okay, Daniel is not Christian. It's Daniel's problem.
01:09:54.678 - 01:10:26.146, Speaker F: It's no longer Christian's problem. So opinions might change. I guess I will. It's understood that the compiler, you know, whether or not they want to go over the threshold and implement them will require some like an effort. That is, it's got to be worth the effort. I will talk to Daniel in Berlin when I'm back to Berlin and see what would be the cutoff for them. And then we can continue that, but.
01:10:26.146 - 01:10:30.064, Speaker F: Okay. Thank you. I understand context now.
01:10:31.404 - 01:10:32.184, Speaker D: Thanks.
01:10:34.044 - 01:10:47.476, Speaker A: Thanks, Dana. Dana, for, for joining the call. You're always welcome to come back to future Vix if you'd like. Cool. And perhaps we can do another call if folks want to, on this topic of EOF and verkle.
01:10:47.580 - 01:10:49.012, Speaker D: It'll definitely do one.
01:10:49.148 - 01:10:57.142, Speaker A: Yeah. Cool. It'll definitely get brought up on ACD as well. But, yeah. Thanks again, dano, for coming and just helping to share your thoughts here.
01:10:57.278 - 01:10:57.598, Speaker G: Cool.
01:10:57.646 - 01:10:57.950, Speaker D: Okay.
01:10:57.982 - 01:11:04.974, Speaker A: Well, yeah, sorry we went a bit over. Thanks, everybody, for joining and see you again in a couple of weeks. Thanks.
