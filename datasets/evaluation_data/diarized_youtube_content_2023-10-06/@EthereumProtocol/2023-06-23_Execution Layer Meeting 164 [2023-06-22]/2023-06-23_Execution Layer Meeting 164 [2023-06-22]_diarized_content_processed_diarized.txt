00:01:45.620 - 00:02:33.356, Speaker A: Okay, and we are live. Welcome everyone to ACD one six four. We have the folks from the execution specs repo that wanted to give a quick presentation on the new testing framework today. They have to hop off. So we'll start with that, try to time box it at around ten minutes, and then we can do some question and conversations about it at the end if we need more time, and then after that we'll discuss a bunch of the Denkoon stuff. And finally the ErC's eips repo split at the end. But yes, Dan, you're the one leading this, right?
00:02:33.538 - 00:02:36.832, Speaker B: Yeah. Hi everyone. Can you see my screen already?
00:02:36.966 - 00:02:39.936, Speaker A: Yep, we can. So, yeah, go for it.
00:02:40.118 - 00:03:39.936, Speaker B: Ok, perfect. Hello everyone. Yeah, thanks a lot for the opportunity for us to show the new execution spec test repo. So I'm Dan, I joined the EF testing team just in April, a couple of months ago, working alongside Spencer and Mario and Dramitri, who are going to help me with this presentation. So the testing team, just before we jump to the mean, just to broaden the scope a little bit, our aims are really to ensure that client implementations for features for upcoming protocol know they meet spec, and sometimes the spec can be a little bit difficult to interpret, or maybe not so clear when you actually come to implement it. So we also try and provide really a source of truth for client implementations for teams to meet. And just more generally though, we really like to help the entire ecosystem improve the development cycle.
00:03:39.936 - 00:04:29.140, Speaker B: So we really want to help all the client teams have a faster cycle for their work. So I just started in April and Spencer started in January. So we have a bit more capacity and we'd love for you to get in touch and give us ideas, feedback, let us know how we can help you, and if it's in the scope that we can scale it in our relatively small team, then we're happy to do it. Okay, moving on to the repo. So just a quick background. It's a framework and a library that enable the implementation of test cases for execution clients in Python as source code. And liclient started the initial development at the end of 2021 just to explore a different approach to writing test cases for clients.
00:04:29.140 - 00:05:54.700, Speaker B: And then it didn't receive initial huge adoption, but after discussions in Devcon, people liked the idea a lot and test case development started in earnest, like late last year, and the focus being on new tests for Shanghai. So for the upcoming release, and now we think it's a good time to show you the stuff because there's been a lot for it for test coverage added to this framework or to this repo. And there's been a lot of framework improvements and some new online doc generated, which makes it helpful and good to use. All right, so most people here will be familiar with Ethereum tests. So what's different about this approach with this repository? So ethereum tests, the tests are implemented as YAML, and here the test cases are implemented as Python code. The other difference is the repo itself contains the tooling or the libraries and the framework to generate fixtures using transition tools. And this helps avoid the two step approach that's used in Ethereum tests, where some YAML spec gets checked in, which is perhaps generated from some source code, which is often not available, and then from the YAML spec, JSON fixtures are generated generally from I think it's retest.
00:05:54.700 - 00:06:32.036, Speaker B: And why do we like this approach? And I mean basically the main reason is we have test cases as code. From this we have readability due to the structuring of the repository. We have a single step to generate JSON fixtures to test clients against as code. The test cases are easily parameterizable with a few lines of code. The test cases and framework are versioned together, which means you'll never have any clash between different versions. It should just work. The code is easy to modify.
00:06:32.036 - 00:07:11.656, Speaker B: The documentation is in line with the source code, so you can generate beautiful documentation from the test cases. And all this makes it a lot of fun to implement tests with. You can drop into a debugger, check the state of your test and so on. We're focused on the big long box in the middle there execution spec test. So on the right you have a command line using the framework fill and refill tests. And the test cases are here on the left in the test directory. And these use external tooling, the EVM binary from Geth currently.
00:07:11.656 - 00:07:58.256, Speaker B: So a quick comment about that is that the transition tool from EVM one should soon be integrated, and we'd really love all client teams to have a transition tool available, and we'd be very happy to integrate it in this framework if it is. Yeah, so this, using this stuff, we also use Sol C just to compile some yule. And based on this we generate output which are fixtures JSON files, which then can be consumed by the test cases consumed by clients in Hive or via client executables directly. So now we're going to have a quick look at more detail to the test cases themselves. So I'll hand over to Spencer for an explanation of what kind of tests.
00:07:58.288 - 00:08:59.480, Speaker C: We dan so yeah, so those familiar with Ethereum tests will know that we have two test types, state tests and blockchain tests. To summarize them quickly, state tests are designed to verify the behavior of a single transaction or operation within a block, whereas blockchain tests focus on the effects that occur from the block to block interaction. In the ethereum state, it's worth highlighting that thanks to Mario, we now also have fork transition tests. And these are a branch of blockchain tests where, for example, block one and block two could be in Shanghai, block three in Cancun, highlighting the transition there. So all of the test cases so far have auto generated documentation. Many thanks to Dan, which you're showing off here. This example here is probably one of the most basic state test cases that exists, which simply verifies one plus two, but it helps us paint the picture a little clearer.
00:08:59.480 - 00:09:48.760, Speaker C: So at the top of the test case, we specify from what fork the test is valid from. So here we've specified homestead. This enables the test to be filled multiple times for each fork. From and including homestead, we have the pre state where the first account holds some yule compiled bytecode for a simple s store of ad operation, a transaction from the test address to the first account, which executes the yule, and then the expected post state with the result of the ad operation in storage. So yes, one plus two equals free. Now at the top page we have this command line. So the python code here can be filled using this command at the top into the client consumable JSON file.
00:09:48.760 - 00:10:54.492, Speaker C: Okay, so this is a simple single test case. What if we wanted to expand on this and for example, easily specify different parameters for the add operation or even change the opcode we're using? Well, with a new framework we can use pytest parameterization. So this test here is for point evaluation, pre compiles specifically for invalid blocks, and we have a bunch of cases here. So this test case is marked valid from Cancun. And as before, if we scroll down to the bottom a little bit of the test, we have the pre and the post state and an individual transaction. But instead of these being defined inside the test function, they're defined outside the test function as a global PI test fixture. Now these allow us to use them within other test functions where the values input into the test functions are entirely dependent on the parameterization of the test.
00:10:54.492 - 00:11:36.700, Speaker C: So here we can see we're parameterizing by the specific point evaluation vectors. And the main idea I want to get across here is how easy it is to add another test vector. So for this case, if anyone here can think of other examples for an invalid precompile call, all you essentially have to do is add to the list of parameters and specify a descriptive test id that describes the new test case added. So yeah, super simple. And if anyone is interested, please feel free to contribute. If you want to know more about the specifics of each test from the docs, you can look into the git source from the link at the top of each doc page. So thanks and yeah, I'll pass you over to Mario.
00:11:38.080 - 00:11:38.830, Speaker A: Thanks.
00:11:41.200 - 00:11:42.976, Speaker C: Over the course of the last couple.
00:11:42.998 - 00:13:10.990, Speaker A: Of months we've been using this repo extensively to test the 48 four EIP and basically the EVM changes to the 48 four. So that comprises the block types of transactions, which is the minimal version, not the network version, the blob hash opcode, the new block headers, which are the exodus and data use fields, and also the pre compiled point evaluation that was already highlighted in the example before. So I really want to highlight a great practical example of the efficiency of parameterization in DVM tests. And this is a recent spec change in 40 f of four, which is the value of its maximum blocks per block count. So originally in the 40 f four spec, the maximum number of blocks was four and was just recently it was updated to six, and with test parameterization we were able to simply update one single line of test code and the test cases for all the new combinations were automatically generated for us. Granted, we did need to prepare extra parameterization code beforehand to account for change like this, and also this type of change at this moment, this manual. Since we currently from the Python test code, we do not parse directly from the sped document, but yet this was definitely made the process much quicker at the time of the change.
00:13:10.990 - 00:14:15.250, Speaker A: And it's also worth mentioning that we can perform this change over and over again. We can just change it back to four, or change it up to eight or whatever the spec defines. And here on your left you can see the original number of test cases with maximum of four blocks, and on the right you can see the new test cases. So a great thing about the new documentation is that you can go inside the doc and see exactly what we have implemented and exactly all the number of test cases. So in this case we have the block transaction test cases. You can see every single test case that was developed and you can go and just look at each individual test case, if there is any coverage missing, or if there is a particular test idea that you feel that is not tested by test. Definitely we are open to feedback and we can obviously add more test cases as needed.
00:14:15.250 - 00:15:55.194, Speaker A: And just to finalize here we have both the links to the repo and the link to the documentation preview, so please feel free to access these links. And as I already mentioned, all test cases that are currently implemented, you can see them in this doc review. It's very cool and if there's any coverage missing just please feel free to reach out and we will make sure that we include more test any I think that's it from us. Are there any questions from anyone? Ruben yeah, I just wanted to ask, will there be any pre generated JSON files hosted? Because I noticed there is a Shanghai JSON files are now in Ethereum test repo, right? Yes, definitely. The current release that we were following, it's up to change, but basically every release we package all the generated JSON fixtures and we package them in ATAR files and we upload that. Next to the release we have two kinds of releases, which is the currently mainnet version. So we have all the tests for what's in mainnet now and we have the upcoming fork release.
00:15:55.194 - 00:16:58.860, Speaker A: So right now we only have 44 four, but we plan to include all the aps for Cancun. So we will have a Cancun release which will contain all these files. As for when the release is coming, we are planning to release very soon. We are finishing up the change to the framework. So when this is done we will release the new framework and we also will release all the pre generated fixtures for you. Thank you. Any other questions? Hello? Yeah, hi, this is Dimitri, the former retested developer and it's really cool to see what you guys are doing with the pytests and think it's really cool, especially the prematureization part.
00:16:58.860 - 00:18:11.388, Speaker A: But what I also want to say I would like to deliver the message to client developers to implement the transition tool protocol that we are using to generate the test. This is transition tools used both in Python and currently in the tested which is still maintained and generate all the tests in ethereum test report. And as many clients implement transition tool, we would be able to run the test and generate the test using python for instance, and see all this new implementation and new aips because of transition tool implementation from a client dev. So I really encourage Clandev to do that. It's not so difficult and I would like to see more teams joining the club. Comment thanks. Any final questions? Comments? Yes, I have one here, just a comment to what dimitri said.
00:18:11.388 - 00:18:50.330, Speaker A: So at least in the case of geth, the t eight n implementation does not correspond to geth execution. T eight n is written to be correct, and it's written to be used as a back end tool for generating tests, but it's not as reliable as a test executor and test runner as the execution of state tests. So I don't think it should be used to test like get compliance. You should not use ta dan for that. You should use the get state test runner. That's it. It might be the same for other clients, I don't know.
00:18:50.330 - 00:18:53.470, Speaker A: Got it?
00:18:54.480 - 00:19:39.370, Speaker D: Yes, I agree. But in general it is very nice if everyone implemented this because then we could start implementing new tests, new eips in other clients. Right now it's basically only really possible in gas and maybe biso to implement a new feature and fill these fixtures. So if other clients also implement the t eight n tool would be really nice. Like researchers can prototype in their favorite language and don't always need to rely on cath to do it.
00:19:49.770 - 00:20:05.910, Speaker A: Okay, any other comments, questions about execution tests? My phone crashed at one point, so maybe I missed it. But is there the ability to fill with eels?
00:20:10.020 - 00:20:12.610, Speaker D: They're working on it, as far as I know.
00:20:14.420 - 00:21:19.780, Speaker A: Got you. Yeah, I think there's gate and support and as long as they have that, we can definitely use them to fill. Yeah, as most of you know, on the consensus layer we write kind of the python specs and simultaneously get to do testing and filling out of that. There are certainly drawbacks, but it allows for pretty rapid generation of test vectors along with the spec building. Definitely. I think a deeper integration is definitely possible and it's something that we should definitely discuss. For example, right now we depend on manual updates, as I mentioned for 48 four, because once the spec is updated, we have to manually go inside and change maybe some, one or two parameters for the test, be up the spec.
00:21:19.780 - 00:21:57.406, Speaker A: But yeah, ideally it should be automatic if we update the yield suspec, we should also somehow consume this update in the indexation fact test and also automatically update the test. Definitely. The other thing is, I'd love to talk to you all about the consensus spec test. I think they could use some love. They could use some documentation. I'm sure there's a lot of things we clean up, so maybe worth chatting soon. Definitely.
00:21:57.406 - 00:22:42.696, Speaker A: Dan is the mastermind behind the documentation right now. He was the one that put up all the python scripting that was necessary to automatically generate from the doc strings of each test case. We basically just grabbed that and then just automatically generating the documentation. Yeah, very cool. I'll still write the test, I promise. I just want some help to make them pretty cool. Anything else? Okay, well, yeah, thanks.
00:22:42.696 - 00:23:42.152, Speaker A: Thanks Al for the presentation guys. And people can reach you on the RND discord to provide feedback or ask questions. Okay, next up we have a bunch of Denkoon updates. So first just a small change to the four four four precompile address. Alex opened the pr. I don't know if there's any context you want to add, but seems pretty straightforward. Yeah, I was just looking at pre compiles for 4788 and I just noticed that basically 4844 was not tightly packed in the sense of just having the one right after the current last pre compile we've added on Mainet and this EIP or, sorry, this pr just suggests putting it there, I guess.
00:23:42.152 - 00:23:48.810, Speaker A: Does anyone disagree with that? And if not we can have this for the next Devnet seven.
00:23:51.040 - 00:24:09.888, Speaker D: Just as a background. It was like the gap is because of the BLS precompiled, but I agree we should tightly pack them and the BLS precompileds are not going to be on Mainet this fox so looks good to me.
00:24:10.054 - 00:24:46.190, Speaker A: So we probably should simultaneously adjust the BLS pre compiled EIP just so that we don't have conflict if there's testing on it. Yes, Alex, I guess who's going to do it other than me. And I'm not sure that 25 37 is exactly what will even go to main net eventually. So I'm not super inclined to go. You could even add it at like plus eight or something. Just getting out of the conflicting zone if you don't mind. Okay, sure.
00:24:46.190 - 00:25:42.530, Speaker A: Okay so perfect. Let's make the change on four four four and then reflect it in the BLS IP as well, to not have a conflict there. And then Alex, your second pr on 4788, do you want to give some context on this one? Yeah, this one is much more involved than the one we just discussed. So we had discussed 4788 as a very quick brief reminder. This is taking the beacon block route, which is a cryptographic accumulator so you can make proofs about Kintensa state. Taking this route and putting it into the VM, it unlocks a lot of cool applications. You can go to the EIP to see a little bit more about that.
00:25:42.530 - 00:26:36.892, Speaker A: And we've had a little back and forth on exactly how to implement this. What the API looks like. Ultimately there's a pre compile that will expose these routes. And the one thing was essentially like, yeah, what does the API of this pre compile look like? Just going off of the data the El has, they have a timestamp and this root really handy. So the question becomes, okay, do we key the essentially API to the pre compile by the timestamp by the root? How does this work? And there's some different options here. One option is just to write by timestamp, but there's not a nice way to basically then bound how much storage this thing uses, which generally is nice. We don't want this unbounded thing growing forever.
00:26:36.892 - 00:27:37.028, Speaker A: So like 510 years from now, you look at this pre compiled storage and it's gigabytes, something like that. So anyway, point being I made a pass on changing how these things are stored so that the storage is bounded. There's only a certain number that will ever be in the pre compiled storage. And yeah, the only thing that's a little funny is basically there's two ring buffers, simply because if we only had one ring buffer, so if we have a ring buffer, then we can bound the storage, because basically for every timestamp there's like one sort of slot. It goes into the issue there is that if we have a missed slot on the CL or just in the chain broadly, then what's going to happen is you basically have a prior write that would then basically correspond to a later timestamp. And this could theoretically be used to attack some dap, let's say that's using this. So that's no good.
00:27:37.028 - 00:28:23.196, Speaker A: And the way you get around this is just basically record both the timestamp and the root. And one more caveat from there is just because the storage size of these things is only 32 bytes and the roots are 32 bytes, I can tightly pack them into one storage slot in the state. And so now there's like two ring buffers. So if you look at this, there's quite some change, but it's mainly just this complexity around duggling these two ring buffers into the EVM storage, but otherwise it should be pretty clear. And yeah, I think at this point I'm happy to answer any questions if something wasn't clear. But yeah, please take a look. I think this is essentially the best option just based on all the constraints that people have voiced.
00:28:23.196 - 00:29:50.830, Speaker A: And yeah, this looks good, we'll merge it and this EIP will pretty much be settled for Cancun. It's kind of a funny approach when you first look at it, but I do agree that given the constraints that it is actually a really solid solution. I guess the other design option could be like instead of two ring buffers have ring buffer where it's in and plus you stagger them rather than having them as total separate. But this is totally fine, I like it. Any other comments, questions? Okay, so yeah, if folks want to review that and then assuming there's no issues, we can get it merged the next couple of days. Okay, next up, Powell just wanted to share, I don't think he's on the call, I don't think he could make it. But there's the M copy tests that have been merged, so I'll put the pr here, but if clients want to test against that, and I see already Baseu and Ethereum js say that they passed the tests there.
00:29:50.830 - 00:30:37.540, Speaker A: I don't know if anyone else had thoughts, comments on that. Okay. And then if not, Mikhail, you had pr for the Cancun engine API spec. Do you want to give some quick context on. Yeah, sure. So yeah, there is a pr opened with the Cancun specification and the aim is to merge to get it merged like next Monday, this kind of last call. What's in this specification? Basically the main thing is blob extension specification that everyone is familiar with.
00:30:37.540 - 00:31:16.870, Speaker A: The other one is like exchange position configuration deprecation notice. And the most recent thing that was added is the payload attributes version three and folk choice updated version three. This is to accommodate the parent beacon block root and make it into the payload build process. Because we need parent beacon block root because of the IP. That's basically it. And more eyes on it would be great. So if you have a look before Monday, that'd be great.
00:31:16.870 - 00:31:41.230, Speaker A: Also really appreciate more approvals on that. Any questions related to the Cancun stack of the engine API? Did the 4788 changes make it in there or they still think. Yeah, they are there. All right, cool.
00:31:47.220 - 00:31:59.312, Speaker D: Might be a stupid question, but how do we get this timestamp for the parent beacon block? Can we just take the parent execution?
00:31:59.376 - 00:32:10.730, Speaker A: Is this a 4788 question? Yes. Well, so you don't like, you just have the header, the header has the timestamp and there's a root and that's all you need to know.
00:32:11.580 - 00:32:16.232, Speaker D: So we use the parent, the current.
00:32:16.286 - 00:32:21.230, Speaker A: Timestamp is mapping to the pre and the caller needs to map those things.
00:32:21.760 - 00:32:22.076, Speaker D: See?
00:32:22.098 - 00:32:22.620, Speaker A: I see.
00:32:22.690 - 00:32:23.340, Speaker D: Okay, perfect.
00:32:23.410 - 00:33:11.254, Speaker A: Thank you. I don't know if it should be Denkun, by the way, because we had Paris and Shanghai before. Yeah, but open to rename it to Denkun if people are okay with that. Yeah, I don't know if we want to retroactively rename the other ones, but it seems like given both sides use, it probably makes more sense. But if there's anything that depends on it, it's also not a big deal. If we will keep having these kind of combined names for fork, then it probably makes sense. Yeah.
00:33:11.254 - 00:34:11.310, Speaker A: And the El is the server. Yes, that's why it is named as it is by the fork that is on El five. Okay. And did you want to talk about the deprecation a bit more? I know you had your doc that you shared. Yeah, sure. To deprecate, exchange transition configuration gracefully. We should have a procedure of doing that because if we just drop the support of these methods in uncoordinated fashion, then users might see regret messages in their logs that this method either not being called for some period of time or it's not exist on the Yale side when Cl calls.
00:34:11.310 - 00:35:24.922, Speaker A: Yeah, I've made this document and the procedure looks like really a simple thing. So the first step would be El clients should stop logging an error message if this method hasn't been called for some period of time, for any period of time. And then all El clients should cut a release on that with this change in the release. After that, CL clients removing this method or just stop calling it or entirely removing it from the code base and make a release as well. And this release should be either de Nurb or earlier. And after Demkun happens, El clients are free to remove it as well. So why this so complicated at the first glance? It's not complicated if you take a deeper look into that, is it? Because just yeah, we should first break the dependency between Cl and DL on making these calls and then we can remove this method.
00:35:24.922 - 00:36:31.450, Speaker A: So this document has the procedure description and also has tables with the status of development. So this is how it can be done. Alternatively, we can do it in a highly coordinated fashion and say that El client remove this method, support and Cl client do the same. But these changes all coming only into Denob release, so strictly into the last release before the fork, which I don't see as a comfortable solution for everyone, but maybe people have different opinions on that and whatever we choose, I think that would be great to discuss it here with the alkaline developers and have their agreement on doing this deprecation and doing it in the exact way as we come to. So any questions or suggestions?
00:36:32.990 - 00:36:39.340, Speaker D: Just a quick thing from us, we already removed this like a long time ago and no one really.
00:36:43.170 - 00:36:45.326, Speaker A: So the message you mean, right?
00:36:45.508 - 00:37:02.786, Speaker D: Yes, we only give a message if we don't receive any updates from the beacon client so like if the beacon client doesn't call any method, and we.
00:37:02.808 - 00:37:58.830, Speaker A: Are working exactly in the same way, if we don't get any fork choice, any new payloads, then we print warning. And to clarify, Maris, you have removed the message, right? But the method handler exists and if Cl calls it. Same for basic. Oh, that's great. Basically if there is no position. So I would say that this is the way we do it, like remove it from Yale first. Can you guys mark just make a green checkbox on the clients that have already removed the error message? I mean like in this document would be really useful.
00:37:58.830 - 00:38:50.790, Speaker A: So any other Yale client developers want to chime in on that? In Aragon we've never printed an error message, so it's already implemented. So great. So we have Bezu, Aragon, Gap, Nethermind and Ethereum. GS is one that left, right? Yeah. So basically we can just move this to the cl side at this point. Yeah, that's great. Okay, so let's discuss it on the Cl call next week then.
00:38:50.790 - 00:39:22.930, Speaker A: Cool, thank you. Of course. Sweet. Okay, so next up I guess I want to do some space if there's any Devnet updates that people wanted to talk about on this. Yeah. Anyone want to give us an overview of where things are? Yeah, I can give a current status. So we had Devnet six relaunched on Friday and where we went through a couple of issues.
00:39:22.930 - 00:39:50.490, Speaker A: It's finalizing again, but just with a minor subset of clients. So I think the approach we're going to take is wait for clients to have more stable releases and relaunch it cleanly with Devnet Seven. I can also incorporate the pr that Alex brought up earlier today and then make a spec list and share it on the interrupt channel. And maybe one of the client teams can talk about the issues we saw.
00:39:51.980 - 00:40:51.550, Speaker D: So I can talk about the gas issues. Basically there were a bunch of them. I kind of completely rewrote a lot of the code. Basically the Defnet is kind of not really indicative of our current code anymore, but the current code should be way better. One thing that kind of led to problems was that we didn't really distinguish between the network or correctly distinguish between the network form and the normal transaction format. And so there were blocks mined with the transaction in the network format in it. So basically like the block contained the blobs, which is wrong.
00:40:54.400 - 00:40:58.510, Speaker A: Even though guest created did Geth reject such blocks or accept them.
00:41:01.200 - 00:41:14.530, Speaker D: It was kind of all over the place. I'm pretty sure some clients accepted them. So there was a lot of forking going on at some point.
00:41:15.700 - 00:41:30.650, Speaker A: I see. All right. When I hear this kind of stuff, I'm like, can we catch it earlier in the pipeline? Obviously block production is kind of a weird one, but the acceptance of those should certainly be in our rejection of those should certainly be in test.
00:41:31.900 - 00:41:56.248, Speaker D: The problem was that a lot of clients didn't really work 100% and so there was a lot of things going on and it wasn't like, okay, only the geth nodes fucked off now, but it was more like the whole network is kind of destroyed now.
00:41:56.354 - 00:42:07.220, Speaker A: And we were kind of relying on Ethereum js to be like the stable boot nodes. But when we started the transaction fuzzer, we then had issues with Ethereum js as well. So there were kind of like no stable nodes on the network.
00:42:08.360 - 00:42:35.470, Speaker D: Yeah. So now it's very clear that this cannot happen anymore on so. But yeah, it's something that I would really like to test out on another Devnet and create tests for that. That's it from my side.
00:42:41.830 - 00:43:18.480, Speaker A: Any other question in the chat? It seems very clear. Devnet seven should be four for four only. Yeah. Does any other client want to share their experience? Yeah, just briefly. The basic challenges mostly revolved around RPC interaction and some formatting challenges that we had between the CLS and us. There was just a couple of ambiguities. I don't remember anything specific, but they were fairly quickly deduced and straightened out.
00:43:18.480 - 00:43:31.810, Speaker A: That's all. Then hive test continue please.
00:43:32.260 - 00:43:36.580, Speaker D: Yes, this can be tested by have tests.
00:43:37.320 - 00:45:23.560, Speaker A: You mean the block with the network type three transaction, right? Yes, definitely. I think we can add that index and we can consume that in Hive and we should be having a test for that before the next step just by load, but get loop by number like that with some plans synchronized and probably it could be a source for such working from transaction and it's for potential startup sale to make it not in a regular way, but started partially to find local network form transactions. Type your network because I still think it's not on storage. We are not sure that this produced by some certain client or it is fixed at all. And it may appear on the note seven as the same issue. Again, I. Your mic wasn't great, but I think I made out most of that.
00:45:23.560 - 00:46:00.236, Speaker A: Any other. Sorry. Yeah, do you want to add something that I say? Oh, your mic is better now quickly. So the idea is to confirm that issue is disappeared actually, because now we're not sure. Will not break next. Sorry. Yeah, your mic, sorry, broke up.
00:46:00.236 - 00:46:29.590, Speaker A: You're not sure about what exactly that this broken form of transaction network form will not propagate in Devnet seven. Again, we are not still sure that what is the source of these popping transactions, which client sent them. Okay.
00:46:35.810 - 00:47:16.380, Speaker D: Yeah, I'm quite sure that it was geth, but yeah, we just saw too much weirdness going on, I think for only guests to be the issue. So either other clients accepted those transactions or it was the other way around that they accepted transactions without blobs. Like blob transactions without blobs or something. I'm pretty sure we will break another definite seven, but that's fine.
00:47:21.110 - 00:48:05.380, Speaker A: Okay, but let it be another way because I'm exhausted by this issue. Okay, so yeah, I guess Devnet seven, we want to restart as a 4844 only Devnet and potentially just add the small change for the pre compile address. Or do we even not want to do that and simply focus on restarting things as they are and dealing with the address change after?
00:48:05.910 - 00:48:11.538, Speaker D: I think we should deal with the address change when we implement all of the cancun aps.
00:48:11.634 - 00:48:18.710, Speaker A: Okay, so basically keep the same specs as for Devnet six, just try to restart it and have it run cleanly.
00:48:22.750 - 00:49:12.280, Speaker D: Yeah, so what's really cool is Mario created hive tests for void for four. So one thing that would be really nice is every client could make sure that they are passing those hive tests. We are missing one right now. That's the transaction propagation one. So transaction propagation is still broken a bit, but yeah, so I would give the Devnet another week or something for everyone to double check with Hive that they are passing everything. We don't see these issues on the Devnet for the first time.
00:49:18.590 - 00:50:10.048, Speaker A: I added the link to the PR for these hive tests. If you need help and anyone who needs help running, just let me know. Gijinder yeah, I can update a little bit about Ethereum Js as well as loadstock. So Ethereum JS had an issue where we were not calculating the data gas used correctly. So that was however resolved early on. But post that there were no issues in Ethereum Js apart from the fact that Ethereum js cannot really handle the transaction load. So when the team started fuzzing and put load on it, Ethereum JS was not able to cope up.
00:50:10.048 - 00:51:26.920, Speaker A: So that was primarily the issue with Ethereum JS. So I think next time when we basically fill the network with heavy blocks, we should make sure that the other EL clients can basically support the network before doing this. And the second thing about Lodestar, also today I discovered a bug in Lodestar while syncing the blobs, which sort of have. Thanks to Teku guys, I have sort of figured out the fix for it. But as such, Lodestar nodes went down because we basically wait for a gossip block to see in 32 slots, which we didn't see from my debugging at particular points. And then the nodes went down and they never came back up again because then there were some issues that were fixed by lighthouse guys and their nodes were still up. So lord, someone was not able to sync from Lighthouse.
00:51:26.920 - 00:52:38.850, Speaker A: So these are some of the issues. But I think in the next run of Devnet seven with the same spec, we should be more stable and hopefully we can have Devnet eight with the extended dynam features. Got it. Enrico? Yeah. I just wanted to add from Cl side that most of the sync problems, as far as I know, was mostly caused by a rate limiting situation. So once we start removing rate limiting on Lighthouse and then also on the teco side, things was better for the other clients to sync up. So I think it will be a Cl discussion maybe next week to actually start putting some numbers and agree on some agreement about the rate limit levels that we want to put.
00:52:38.850 - 00:53:38.030, Speaker A: Got it. Okay, so anything else on the Devnets? If not, then I guess, yeah, let's get Devnet seven up with the same spec and we can discuss this more as well on the four four four call next Monday. Do we have an expected ETA for seven being up? I think the ETA wants to hype desk green. Good. All right, thanks. Yeah. Okay.
00:53:38.030 - 00:54:32.896, Speaker A: Anything else? Okay, Danny, you wanted to give a quick note on the CL spec? Hey, yeah. The Devnet six stuff might complicate this, but the intention right now is to release a CL spec tomorrow or Monday with the full feature build. So that's four for four, but also 4788-7044 yes, Tim, it is nice that I can just say those numbers instead of trying to say the features. So you're right. So that's the intention. So we'll have full test vectors for all of those built, but those will not be the target for Devnet seven. So you would use the previous release for that.
00:54:32.896 - 00:55:32.404, Speaker A: But hopefully it'll unblock some development on getting the full feature stuff developed. If we need to add more 4844 test or if something is broken and with a change, make a breaking change, which I don't expect at this point. We might have to do some sort of like feature branch and special test vector build for it for four. That said, I think we're willing to take that risk and potential complexity so that we can get the full Daneb feature release out, because I know some people want to get development done and we need to unblock that. Obviously this is related to consensus layer, folks. Generally, I know there's some of you all here, so if you do have an issue with that, please let us know and we will do the release. And then if there are issues on the call on Thursday, we can discuss them.
00:55:32.404 - 00:56:26.500, Speaker A: But I think that this is the right path given the things we're juggling. Any comment or question? Okay, we'll put in the announcement channel. We have a release and test vectors for the fully featured Deneb. Awesome. Okay. And on a similar note, so last topic for today is the proposal of splitting out ERcs from Eips in the repo. We've discussed this a fair bit for a long, long time, but light client has finally put up a pr which actually proposes or shows what it would look like technically.
00:56:26.500 - 00:57:14.484, Speaker A: Yeah. And I guess just for background, the main rationale here is ERCs mostly focus on application layer standards, EIps on protocol layer standards, and there might be value in having both processes be able to evolve separately. We have a bit more EIP editors now, so I think we have a bit more bandwidth to actually handle doing this split. And at least some of the current EIP editors would also stay on the ERC side as well. But this would allow for having two separate processes. So I'll pause here. I don't know my time.
00:57:14.484 - 00:58:31.616, Speaker A: Do you want to give some context on this specific pr? If not, if anyone has comments? Yeah, I guess I could just say quickly, I think the big question is how will we deal with numbering for these two things? And this is still open for debate a little bit, I guess. But ultimately I think that the best plan is to sort of, whenever this migration happens, to have some sort of external document that EIP editors look at to see what the latest EIP number available is, and just assign that to the ERC and the eips. I know that we talked about maybe making core eips like even numbers and ercs od numbers, but I think that given the number of historical eips we already have that don't abide by this, it's going to add some confusion. So that's currently what the proposal and the pr is to do with the number in the rest of the information is pretty straightforward if you want to look at the proposal in the pr. Got it. So I guess, yeah, I'd be curious to first hear from the client team. Does anyone oppose this and I know there's some folks on the EIP editor side who have concerns.
00:58:31.616 - 00:58:39.850, Speaker A: So maybe starting with client teams, and then we can go to EIP editors who are here.
00:58:43.960 - 00:59:26.000, Speaker D: I think I voiced my opinion on this a lot already. I think the EIP process should serve the core developers, and it's not right now. And one big issue with it is that we have an influx of so many ercs that it's not really possible for someone to stay informed of all the eips that are coming up and have meaningful reviews on them. So I'm very strongly in favor of splitting the two repositories.
00:59:30.700 - 00:59:49.324, Speaker A: Got it. And me. What else? This is Justin here. Yeah, same thing. Huge agreement. Just point of clarity, I'm assuming we're not intending on reusing any EIP numbers being recovered, is that correct? That's correct. Fantastic.
00:59:49.324 - 01:00:45.490, Speaker A: Thank you. Okay, so that was a comment by Nethermind as well in the chat. Yeah, daddy. We could argue next year about saving the numbers and I guess, I don't know from the CL side. Does anyone have thoughts now that you've all started using this repo? I'd agree with what Maria said, but just know there's just so much going on with having both eips and ercs. If we can reduce that at all, it's really helpful for people working on eips. So, yeah, I definitely support this change and think it's really important to do.
01:00:45.490 - 01:01:31.200, Speaker A: Cool. Okay. So there are definitely some opposition on the EIP editor side. I don't know if there's anyone here who wants to share their thoughts or their view as a Cl and El Dev, I'm in favor of the change, but as an EIP data, I want to sort of figure out what is the harm that would come to the ERC process because of this, and would they be at any disadvantage. So those are the points that I think I would consider. Got it.
01:01:33.570 - 01:02:42.870, Speaker E: Yeah, I've Greg Covid here. I've been opposed to splitting the repos for years now, so I don't want to go into it right now. It's a premature decision. I'm really glad that light client put this up, but I think it was yesterday, so there'll be a lot more discussion. I'm very sensitive to the process having become too difficult for people to use. We do need to fix that, but just going, we're going to split the repo doesn't necessarily fix anything. And I think what we really need is to step back and get a lot more information and consensus from the people using it as to what are the pain points? What are the problems? If it's just, well, it's hard to follow a big repo, then the answer could be, well, the repo is really just a file cabinet that we're trying to keep organized.
01:02:42.870 - 01:03:12.510, Speaker E: It's not the best place to follow things. A better place is the pages that the foundation maintains organized. Everything's there by category. If you want to just check there, there's an RSS feed. We need to get a mailing list feed. Any other feeds that we can create, that you can follow. Other pain points are that getting a draft in is a pain.
01:03:12.510 - 01:03:57.840, Speaker E: We're very picky on link policies. Everything has to be spelled right. Everything. I lose track. Are these rules too tight? What do we need to do to make it easier to get a draft into the system in the first place? And I'm sort of the only OG editor left, and from my point of view, the process has just gotten too rigid. It's no longer strong suggestions that the editors can exercise some judgment on. It's become a set of fixed rules that apparently are chafing people.
01:03:57.840 - 01:04:46.640, Speaker E: So I just want to start top up, top down, understand really what the problems are, and then we can get to whether this particular technical change actually answers the problems and helps answer the problems. We don't start from, well, let's split the repos, create an ERC ghetto, and then things will be better. We don't know that that's the case. And we've been having this argument for so very long, but at this point, I'm still blocking consensus. So that was a long diatribe, and I could go on and on, but I already have in many places for a long.
01:04:55.990 - 01:06:02.040, Speaker A: Yeah, this is victor. Yeah, I very much agree with Greg. Maybe I'm one of the so called EIP editors that are referring to, but I really resonate more as a, identify more as an author. How many of you in this group like the current EIP process, raise your hand if you like it? And my question, the next question would be, how many of you are supporting the split of ERC and EIP, anticipating that the process to submit core EIP would be easier? Raise your hand if you are supporting because you want to make it easier. There's not many, but that's out of my expectation. But how many? Are you supporting it because you want to make it trickier? I don't know how possible it will be to get right hand responses through this. Right.
01:06:02.040 - 01:07:02.762, Speaker A: So my argument is that it seems to me that the proposal. We're saying that once we split the ripple, this problem will be solved with a core dev. And if you really look into the discussion between the editors and the proposer actually intended to do the split and keep the EIP submissions still hard while relaxing the process for ERC, which I'm in favor for ERC side. But I still think the core problems that we shouldn't make it so hard for people to contribute to EIP. I heard two pain from discussion with Marius. One is the noise from two sides. That's going to happen now that we have more different type of core EIP as well.
01:07:02.762 - 01:08:01.946, Speaker A: So why not split the core consensus, things like that. So we're trying to solve the problem instead of starting with splitting. And my last comments, if we really need to split, I think the impact on the ERC side would be that links to ercs are all going to be broken. So if we are starting a new repo for the core, I think I will be more in favor rather than removing ERC from this existing repo. So I'm curious, how many would you would be in favor if the direction is to move core eips outside of risk repo? That would be my question. I am pretty opposed to that. I think there's a pretty strong understanding in the community that eips tend to refer to core protocol changes.
01:08:01.946 - 01:08:58.650, Speaker A: If you look at people talk about like EIP 4844, EIP 1559, and obviously it's not like a perfect thing. But generally when we say EIP, people mean people understand consensus changes to the Ethereum protocol, whereas an ERCS has a similar, I think, connotation to the broader community of being like an application layer change. Like people know ERC 20 or ERC 1155 or whatnot. So regardless of what happens on the GitHub repo level, the repo is already called eips. So that's why it probably makes sense to keep that for the core consensus stuff. But I feel pretty strongly that whatever is called EIP should refer to consensus changes to the protocol. Whatever is called ercs is application level standards.
01:08:58.650 - 01:09:13.140, Speaker A: And then there's a couple of categories like interface and whatnot that you sort of need to figure out. But at a high level, I think this is how the community understands it. And you want to maintain that, to not confuse everybody about it.
01:09:16.230 - 01:11:00.710, Speaker D: Yeah, with the split, we are going to break some URLs, but I think maintaining the core URLs way more important than maintaining the. Anyway, I wanted to respond to Greg, because I think one thing that you touched upon was that the EIP process has become too rigid. And I kind of agree, and I think that's really the big pain point for a lot of core developers, is that it's rigid. There are a lot of rules, and a lot of those rules, I feel like, were introduced by Micah at some point. And the intention behind it was that it wasn't possible for him or for the EIP editors back then. I think it was way smaller set than today to really go through the eips, go through the incoming prs, and have a really go into depth and understand the changes and verify the changes on technical merit. And so they made up a lot of rules so that everything that it gets rid of a bunch of spam.
01:11:00.710 - 01:12:31.060, Speaker D: And I think that is one of the core problems, is that you bunch, in this very spammy side, the ERC site, where basically everyone that creates an application kind of thinks, okay, we have to do an ERC now and opens a pr for the ERC. And there's these core eips where you have people we want to discuss. We want to have the prs open as a discussion point so that we can maintain this open discussion, more like a request for comments instead of like a set standard. And, yeah, it's just two different things, and bunching them together was a bad decision. And so in order to make it easier for everyone, we should undo this. And also, I think it seems like almost everyone is in favor of doing that. I agree that this pr that Matt opened might not be like the solution, but I think there's very strong agreement that this change needs to happen.
01:12:31.060 - 01:13:01.900, Speaker D: And in my personal opinion, if one of the EAP editors or some of the EAP editors against this change, then it's fine, and we have discussions about it. But at some point, and these discussions have been going on for a really long time. And so I think at some point, we just need to make the decision to do this change, even if people object.
01:13:06.370 - 01:14:03.060, Speaker A: Justin? I think, yeah, just real quick, I'm in total agreement with everything that Marius asked, and I would like to kind of call in to question. I see a lot of conflation, know, discussion of the editorial process and policy itself, which I think we can all pretty much agree is orthogonal to this decision. I don't think we should interpret advocating for a split as endorsement of a current perfect world of editorial process and eip authorship process in general. So I think keeping those two separate in our mind leaves us many more options for solving this problem in a way that everyone's happy with. Yeah, and there's a comment by Victor around asking the ERC community member. I think there's definitely been some feedback in the. I also.
01:14:03.060 - 01:15:53.250, Speaker A: There's also part of me that feels know for years we've tried to find people from the ERC community to step up and steward this and be maintainers or own this process, and it sort of hasn't happened. And I think that spilling the eips out so that the people who are focused on eips can just focus on that and the community can organize around ercs as well. It's probably a signal that if they want to shape their process in a way that's better, someone has to step up and do it. And it feels a bit weird that because they're bundled together, then the people who want to do eips actually end up have to doing all the ERC lifting and it sort of doesn't lead to anyone showing up and making things better on the ERC side. So I think it's like if everyone, and it seems like all the client teams and most of the people who write eips are in favor of splitting this, I don't think that we necessarily need full buy in from everyone who's written an like, I think it's fine to say we're splitting this out. You can use this and you can add editors to the ERC side, but the folks who do most of the eips want a separate process and we can move forward with a. Okay, there's a couple ERC authors that are plus wanting to change in the chat.
01:15:53.250 - 01:15:57.320, Speaker A: Sorry, I just cut someone off. Puja, you?
01:15:57.850 - 01:16:49.238, Speaker F: Yeah, I'm sorry. I know I'm not an ERC or EIP editor, but I'm someone who has been involved with the EIP process and helping out with improving the process, trying to identify the pain point. And as part of ethereum catheters, we are working in this direction for past three years. So I would just like to make a few points here from the conversation so far. The two biggest problem that we can hear out in support of the pr is the inflex of pr that comes to the repository and that kind of create noise for which many people who are interested in core eips cannot be able to follow that. I don't know if I have a perfect solution, but I was just wondering if RSS feed can help with. Right, we have RSS feeds for status change and everything.
01:16:49.238 - 01:17:54.506, Speaker F: Can we have an RSS feed for core eaps only? I'm suggesting this just to maybe help out ERC proposals. What I have noticed in this period is like ERC proposals were in really bad shape about a couple of years ago, thanks to Magginet, who has actively supported the education of the process and how to document it best. He had also contributed and did the EIP editors apprenticeship meeting for very long. I guess over a dozen of meetings we have organized and it has supported a lot. We do have a dedicated ERC editor as of now, but we do not have many. Out of the six or seven EIP editors those are listed on EIP one, we do have only one ERC editor that is fully dedicated for ERC purposes. I still feel that there is a shortage of ERC editor and I feel like that if we just move ERC out of that, probably that would be harmful for the team, for the ERC people.
01:17:54.506 - 01:19:00.420, Speaker F: And the other biggest problem that I have identified during this conversation is about the external link, which I believe Samuelson has proposed this proposal 5757 and one by one we are trying to add permission to the external link. I do agree to Neriasa's point that there were a lot of strictness imposed by Micah and other editors at the time to make the process streamlined, to make the process even better, and we are reaching in that direction. I'm really requesting people to consider ERC as a part of Ethereum ecosystem and we can, if we can extend our support to these people, we should let them be there as well. Had there been proposal coming out of ERC team saying that we don't want to be a part of core EIP or the EIP system at all because we feel we are different, I think I would have been more in favor of going towards people request. Having said that.
01:19:02.870 - 01:19:39.790, Speaker D: Just to that point, why should the ERC community have the say? There are a bunch of people now discussing this point that if the ERC community wants to split out then it would be fine. But basically now, the way I see it, the EIP community is trying to split the so like no, I think we should have the same say in this decision.
01:19:41.330 - 01:20:32.798, Speaker F: That is right. I do agree. I feel like they are intertwined and they should be considered together. I have a very good example for 4337 that is an ERC as of now, but we are working towards some proposal that we can eventually in future may be able to have it supported by the EVM and by the main protocol. So I find it hard to actually separate both of them and I feel like going side by side if they are not creating much trouble to the core eIP process. I do agree that there are certain roadblockers for new authors who are trying to document process, and I feel like working in solving their issues after identifying what are the main problems. Just like external link policy was one of those, I think we can come to a better place where we can live together.
01:20:32.964 - 01:21:48.626, Speaker A: So I guess I have my thought on this is, it's almost like a tooling question, right? Like you mentioned, and Greg sort of mentioned this as well. Like, okay, if too much stuff in one GitHub is overwhelming, there's a bunch of other things we can do. Like yes, we can do RSS, we can do the pages on EEP ethereum at large. But then the alternative is like, if GitHub notifications is the tool that all the core devs use, then that's the thing we should use, right? The single complaint everyone has is there's too much friction in this. And if all the client devs are saying I want to watch a repo that has less issues in it, and that the issues in it are more relevant, then we should optimize for that. I also think at the end of the day, this is just like a repo and URL split. Like the idea that because they're in two different repos, I don't know, someone working on account abstraction can't pay attention to Ethereum, Eeps and Ethereum ercs seems a bit weird, given that today to contribute to Ethereum, we literally had a presentation earlier about a new testing repo.
01:21:48.626 - 01:22:26.594, Speaker A: We have the engine API repo. We have the El specs, the CL specs, the eips. People in practice already follow nearly ten repos if they want to follow everything. I don't know why splitting the repo and splitting the URLs is such a big. It doesn't seem like a big change in terms of process. It doesn't seem like it adds a ton of friction in terms of like if you want to write an ERC and write an EIP, it's not that different than if you want to write an EIP today and have your spec live in the CL specs repo. And people do that, and it seems to work.
01:22:26.594 - 01:23:24.390, Speaker A: So I'm much more inclined to just that splitting the repos doesn't sever the process and mean that no one can ever contribute from one to the other. It just gives more clarity. The same way that we have a repo for testing and a repo for the specs. Yeah, it just keeps things more encapsulated. But it doesn't seem like a huge deal to have two repos rather than one. Yeah, Justin. So to sharpen that point a little bit, the repo is actually, I think, the correct thing to collect this in and make that distinguishing on, because the editorial process itself actually manifests itself in a lot of automated ways, and there's a lot of tooling and hooks and things like that that contribute to the processes that we want to refine.
01:23:24.390 - 01:24:27.290, Speaker A: So I actually think that's actually saying that there's more of an advantage in going further with this and actually separating them out to allow people to automate the process however they see fit. I was kind of on the page like, well, Monorepo, multiple repos, who really cares? At the end of the day, there's folders that you look at, and I don't think that's quite correct. I think you're leaving a lot of automation potential on the table by not embracing the separate repo. Yeah, I would agree with that. If we do customize things more than having separate repos, obviously makes it much easier. And I think, yeah, I also agree with the point around ercs generally, it doesn't seem reasonable to halt protocol development based on application layer concerns. As a general principle, we haven't really done this in the past.
01:24:27.290 - 01:25:12.564, Speaker A: Ethereum protocol changes are already extremely slow relative to software development in general. So any external source of friction tends to be just a bad idea. So I don't know. It seems to me like all of the client teams are in favor of this. Some of the folks here who have been Erc authors as well, are in favor of this. There's a couple people on the EIP editor sides who are not in favor. But it seems to me like the balance is like we should go ahead and do this change.
01:25:12.564 - 01:25:46.720, Speaker A: We've been talking about it for years. It's also something that we can undo that's not incredibly hard to undo if we realize in a year that it was the wrong thing. But yeah, I think if all of the client teams are in favor, if parts of the EIP editors are in favor, yeah, we should go for it. Greg, we can't hear you. Greg, you're still on mute.
01:25:49.620 - 01:26:23.320, Speaker E: I nonetheless still block consensus. I very much hear you. There's a problem. I'm not convinced that simply splitting the repos helps. And we really are one community trying to improve Ethereum. This line is not so strong, and the problems do not seem to me to be with the tooling. I disagree that it is the core devs repo.
01:26:23.320 - 01:27:06.600, Speaker E: I really do. When the editors formed as a group we were a distinct group. And really, if the core devs found what we were doing useful, great. If not, we're sorry, but I'm not rejecting you. I'm just saying the editors have their own integrity, and the repo, in the end, is our file cabinet. It needs to serve our work and not necessarily the core dev's ability to follow the work jet hub. Repos are not really good for that.
01:27:06.600 - 01:27:22.320, Speaker E: And we need, and can provide other ways to notify. Our assessees are old fashioned. I don't know. Technically, can we feed notifications into discord, for instance?
01:27:24.340 - 01:27:32.850, Speaker A: That's a question, I guess, on your point around blocking this. There's a comment in the chat. I don't think a single person can block this.
01:27:34.180 - 01:27:42.084, Speaker E: Yes, I can. I'm an editor, and I can block consensus with the editors. They can decide what to do about it.
01:27:42.122 - 01:27:57.768, Speaker A: Well, sure, but another editor can merge to pr, right? So I don't know that we need unanimous consent. I don't think we've required unanimous consent for any change ever.
01:27:57.934 - 01:28:49.768, Speaker E: I'm saying it's a consensus change of the editors. I'm blocking consensus. We editors can decide what to do. I might be convinced, but I'd much rather see, and I've said a complete plan from the top down, that we've really looked at what the problem is, what are the pain points, and know that we're going to solve it. And then if splitting the repos as just saying, oh, two file cabinets will work better than one, and we've worked out the problems and how to fix them, numbering is more of a problem that it might seem. For instance, then if splitting the repos looks like the way to go, then that's the way to go. But we could split the repos and solve none of the problems.
01:28:49.854 - 01:29:10.850, Speaker A: But I think we've agreed to this, and I think all of the client teams still want to go ahead with, you know, Daniel has a comment. Like, consensus doesn't imply unanimity. And I think know at a point where we have extremely strong consensus on most of the people who write Corey ips and engage with that process.
01:29:11.700 - 01:29:22.710, Speaker E: Okay, what can I say? The editors are an independent group. We'll discuss this as a group.
01:29:24.680 - 01:30:16.170, Speaker A: So we have discussed this for years, though, and this is kind of the thing, and I guess back to Jamie's comment is I've personally discussed this since probably 2019, on and off through this. And so I think with the addition of having all the CL folks using this process, having had years of discussion that hasn't done much in terms of moving the conversation forward, I don't know that we learn much more by discussing things for another six months. I think we will learn a lot by actually splitting and running the experiment. If it's a bad idea, then worst case we just revert it back and it's not the end of the world. But.
01:30:19.980 - 01:30:48.560, Speaker E: I'm still not. The only problem that splitting the repos actually solves is that if somebody is using GitHub notifications to follow things, or browsing the repo for following things, it will help the core devs doing that. I don't know how many core devs try to follow it that way. There's other ways to follow it, but they do.
01:30:48.710 - 01:31:50.372, Speaker A: We literally have everything else on GitHub. All the conversations happen there. We have core devs following a bunch of different repos from research to testing to specs. And so it's like, sure, you can say use an RSS feed, but it doesn't make sense or whatever else, but we can just use the same tool. And again, this is just like a GitHub repo and a URL. The processes can evolve differently, but if it's true that the processes are optimal as they are today, and they don't change in the future, then the worst we've done is we've created an extra repo for people to follow and duplicated a bunch of automation, which is pretty minor in the grand scheme of things, and then everything else is the same. But in practice, if we do this change, and then things do start to change from one repo to the other, then it's a sign it was a good idea to do it.
01:31:50.372 - 01:32:51.690, Speaker A: And I don't think it's like discussing the issue for another six months on the EIP IP calls that'll give us more information. At this point, it's like actually trying it, seeing are there things we can do and going from there. I feel like I've had this conversation with many people over the past few years, and it always sort of ends up at the same spot where the vast majority of people want this to happen. They're a bit disengaged from the current process because they feel burnt out by it. And then you have a couple of people who are very engaged with the process, who sort of want more information when there's just like this general, I don't know, discontent that's shared. And I think, yeah, like a bigger step forward of actually spilling it out and trying the separate repos is probably the best path forward at this point.
01:32:52.540 - 01:33:02.570, Speaker E: I believe the best thing is the earlier thing, which is we make it very easy for drafts to get in.
01:33:03.280 - 01:33:07.084, Speaker A: But we can do that. Sure, we can do that in both repos then. That's not the end of the world.
01:33:07.122 - 01:33:09.692, Speaker E: We can do it now, but it hasn't happened.
01:33:09.746 - 01:33:29.232, Speaker A: That's the thing. We could have done it in 2020. We've been talking about this exactly. We've been talking about all of these things to change about the EIP process, but it just doesn't happen. And eventually we just have to do something because the process isn't serving any of the people it's meant to serve. It doesn't serve ERC authors, doesn't serve core developers. It doesn't serve EIP editors.
01:33:29.232 - 01:34:19.270, Speaker A: This is something that I think is going to alleviate the deadlock that we have in making decisions and changing the process and let the people who are really involved with their specific parts of the process develop it in a way that they succeed. I think, look, we're already over time. I think what I would suggest is, it's clear to me at least that from the core EIP side, there's strong consensus to split this out. And there's an EIPIP call next Wednesday, I believe. So what I would suggest is that we spend that call talking about the how rather than the know. Matt's pr has know issues with it. There's a couple issues that came up today, but I think having a conversation about how do we want to go about this is probably the right next step in.
01:34:19.270 - 01:35:06.792, Speaker A: Yeah. Given the strong support here. And I guess also if people feel very strong, if the ERC community feels very strongly that this is a terrible idea and we shouldn't do it, then they should probably voice that in the next couple of days before that EIPIP call and we can discuss it then. But yeah, otherwise it seems like from the core side, we have pretty strong consensus to split things. All right. Yeah. The EIPIP call is Wednesday, 14 utc.
01:35:06.792 - 01:35:46.690, Speaker A: The agenda, I don't know if someone can post a link to the agenda in the chat here, but it's in the cat herders repo. Okay, I know we're already a bit over time. Was there anything else people wanted to discuss? Okay, well, thanks, everyone. Talk to you soon. Thanks, Tim. Thanks, Tim. Bye bye.
01:35:48.390 - 01:36:06.730, Speaker F: Thank you. Sa.
