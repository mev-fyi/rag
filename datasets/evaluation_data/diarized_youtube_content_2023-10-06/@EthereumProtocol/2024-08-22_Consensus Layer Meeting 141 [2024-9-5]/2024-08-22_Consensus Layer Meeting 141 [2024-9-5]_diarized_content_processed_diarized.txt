00:00:28.940 - 00:00:58.150, Speaker A: Sadeena.
00:00:58.840 - 00:01:20.140, Speaker B: Okay, we should be live and let's get started. Great. So this is consensus layer. Call 141. Let me grab the agenda. I'll drop it in the zoom chat here. And yeah, there's quite a number of things on the agenda today, so let's just dive in.
00:01:20.140 - 00:01:33.070, Speaker B: First up is Pektra, and we have Devnet two and three. I think mainly we're focusing on three at the moment. Is there anything we want to quickly address with Devnet two?
00:01:35.530 - 00:01:47.950, Speaker A: Yeah, I think Prism found the bug that they were tracing down last week with nonfinality. I don't see Nishant is just joining in, but I don't know if they want to talk about it quickly.
00:01:49.620 - 00:01:51.700, Speaker C: So the bug was fixed over the.
00:01:51.740 - 00:01:53.548, Speaker B: Weekend, the pr was merged.
00:01:53.644 - 00:02:00.828, Speaker C: The TLDR is that we had. Basically we forgot to copy something in.
00:02:00.844 - 00:02:02.524, Speaker D: This example, the validator balance.
00:02:02.652 - 00:02:05.268, Speaker C: So the stay replay was run, but.
00:02:05.284 - 00:02:06.772, Speaker E: Yeah, it's fixed and then.
00:02:06.876 - 00:02:07.252, Speaker B: Yeah.
00:02:07.316 - 00:02:09.880, Speaker C: So I haven't seen any issue since then.
00:02:13.700 - 00:02:36.238, Speaker A: Yep. So that's about it with Devnet two. I think at some point the explorer couldn't handle the non finality, so instead. So considering no one else is really debugging on it, I'd push for deleting the network today. Devnet three work is ongoing. There's a kurtosis config that I've pinned in the interop channel. I tested it.
00:02:36.238 - 00:03:21.562, Speaker A: The nodes are able to stay with each other and special shout out. And thanks to the testing team, because they were able to convert their tests into a format that I can run them against live death mats and Mario's already created, as well as addressed the failing test with RET and Nethermind. And there is one more failing test with Aragon that I've created a thread for. Everything's an interrupt, so please have a look there. And just a side note, I also tested consolidations and the spec issue from the last Devnet has been fixed. So we have the first successful consolidation on a Devnet with an effective balance over 32, which is really, really nice to see. Yeah, and I guess the next question is adding more CL's to the mix.
00:03:21.562 - 00:03:46.850, Speaker A: I've only gotten the CL branch and version to use from Nimbus as well as Lighthouse, so it would be great if other client teams can just drop a message on interop as to which branch we should use, and we will expand our testing accordingly once once we figured out the spec issues. I think with three clients we're kind of okay to already launch Devnet three unless there's some reason to hold off and launch Devnet three with all clients.
00:03:51.430 - 00:04:14.696, Speaker B: Great, thanks. Yeah. Any other clients want to chime in on Devnet three readiness? It sounds like we could go ahead and kick it off. Even if not everyone's ready. Lotsa should be ready. So maybe unstable branch on loads of.
00:04:14.728 - 00:04:16.300, Speaker F: Or Devnet three can be used.
00:04:20.800 - 00:04:30.540, Speaker A: Awesome. And also if client teams aren't ready at the launch, that's still fine. We can make deposits and add you guys later. So not that anyone feels left out.
00:04:35.890 - 00:05:27.520, Speaker B: Cool. Anything else with Devnet three? Sounds like it's moving along pretty well. Yeah, I know that on the el side there are some things with 7702. I don't know if it's worth touching on that now, otherwise we have a number of Petra prs to discuss. Okay, let's move to that then. So first up, we looked at this one or two calls ago, but it's essentially handing it handling an edge case and the correlated penalty calculation was slashing. There is essentially an overflow with Max EB at high amounts of stake and this PR fixes it.
00:05:27.520 - 00:06:13.690, Speaker B: So this pr has been up for a little while now. It's 3882 on the consensus specs repo, and yeah, I think it's pretty much ready to merge. We got a thumbs up from a few different people. So yeah, I guess this is the final call. Anyone opposed to merging the sun? Okay, if you're a client team and you have a few minutes, please take a look. Otherwise sometime later today or tomorrow, I'll go ahead and merge that in. Next we have another pr.
00:06:13.690 - 00:06:52.910, Speaker B: We also looked at this one in the past as well. So this was refactoring how the requests are structured in the beacon block. This is PR 3875. And yeah, a number of people here have been working on this pr. I believe it's ready to go. So again, last call. Anyone think we should not merge this on? Okay, Mikael says there's a small thing to be fixed.
00:06:52.910 - 00:07:23.230, Speaker B: Okay, I can take a look. I'm not sure. Okay, yeah, there's a comment here. I see. Okay, I can take a look at this and yeah, we can get that fixed. Otherwise, anyone opposed to like, this general direction? There's been pretty good support so far. So looks like we just fixed that thing and we are ready to go.
00:07:23.230 - 00:08:16.200, Speaker B: Okay, next up then we have another printhead to the execution APIs. So this one, it's another pass at structuring the request, passing them from El to Cl. And essentially this is a slight different approach. I forget the number, but there was a pr we discussed before to essentially have the engine API structured in a way that the els can just sort of copy paste the way they represent requests on their side as they then are ferried across to the Cl. This is a slightly different approach. And let's see. Felix, is that you?
00:08:16.740 - 00:08:18.236, Speaker F: Yeah, that's mine. Okay.
00:08:18.308 - 00:08:20.040, Speaker B: Yeah. Would you like to give us a little overview?
00:08:21.060 - 00:09:24.648, Speaker F: Yeah, so basically this the first one, the first pr, which you mentioned also, it was number five, six, five. So there, Matt proposed this approach where we would just, instead of returning the request as JSON, as we do right now, and also. No, sorry. Instead of returning the request as separate lists to the CL, so like one list per request type would just return the unified list of all requests and leave it to the Cl to actually figure like split them up into the types for inclusion into the CL blog. Because as far as I understand, inside of the CL block structure, each request type has its own list. However, this was not universally liked, because, I don't know, it is. I think it has some problems on its own, like picking apart the request type from the JSON and so on.
00:09:24.648 - 00:10:00.928, Speaker F: Is maybe not is where I come in with my printhead. So I personally think that the purpose of the whole requests mechanism in EIP 7685 is just that. It should be generic. Like these requests originate within the chain and their destination is the consensus layer. So ideally the execution layer would not really know anything about these requests. I mean, it has to extract them from the chain, but it doesn't even really have to know what's contained, like the format of these requests. Unfortunately, right now it does have to know the format, because we have to convert the format within the EL three times.
00:10:00.928 - 00:11:23.012, Speaker F: So actually we have three different formats for these requests implemented within the EL, and they are specific to each request type. So it means we have to have a lot of code within the EL to handle these different types, and I feel like it's unnecessary. So an important observation we made is that for the newer eips, for consultation requests and withdrawal requests, the data which is returned by the contract is basically valid SSE, and it's not 100% valid SSE. I'm working on a patch right now to the contracts to actually make it that, but basically what they return could just be passed to the CL directing. Probably be more convenient for the CL to parse it, like more convenient than parsing adjacent anyways, and for the deposit contract, obviously it doesn't return valid SSE because we kind of gathered the data from the logs and it's ABI encoded. But the transformation going from the log data to valid SSE is a very simple one and it doesn't really require any kind of SZ handling. It just means we have to shift the bytes to a different position basically, and it's something that we can define and we have to do that anyways because we have to parse the API encoded data already.
00:11:23.012 - 00:11:37.040, Speaker F: So I don't think we're, it's not going to add code right now. So in total this change is, if we go through with this, it's going to remove a significant amount of code from the ELS, and I also believe it will remove some complexity in cels.
00:11:41.140 - 00:11:51.230, Speaker B: Cool, thanks. Yeah, I've taken a look at these and generally agree with everything you're saying. This does simplify, I think, both things on the El and Cl, which is really nice.
00:11:52.530 - 00:11:52.954, Speaker A: Yeah.
00:11:53.002 - 00:11:53.274, Speaker B: Thoughts?
00:11:53.322 - 00:12:20.450, Speaker F: I have to note that it's all a bit contingent on a bunch of changes to eips which have not been made yet. So this is more me just proposing this general mechanism. But it has to be said that next week on Acde I will also be presenting the changes to all of the eips. So that, yeah, I mean it requires cooperation from the El developer group as well.
00:12:21.630 - 00:12:51.880, Speaker B: Right. And to your point there are changes, but they're not anything too massive. So this seems like it simplifies things quite a bit with a pretty low lift. So yeah, I mean we will need El buy in as well. So I guess for now any Cl's, have you had a chance to take a look or have any thoughts about this approach? Any feedback? I like it, but it just a little unclear on what the CEO will be receiving.
00:12:52.500 - 00:12:54.996, Speaker D: I think it's a bit under specified at the moment.
00:12:55.188 - 00:13:37.922, Speaker F: Yeah, so there's no example in the spec. So the pull request has a long description, but it also has the content of the pr. And in the content you can kind of see that literally. Basically at the moment in the current spec for prog, for the engine API Prague, we return three separate lists which called deposit requests, withdrawal request and consolidation requests. And these are arrays of JSON objects. So there's basically one array for every request type. And our change here is to return a single array called requests.
00:13:37.922 - 00:14:13.322, Speaker F: And it will be an JSON array of hex strings. And these hex strings will then contain the encoded requests. And the encoded requests have a single request type byte in the beginning. So it's like zero, one or two, and as the first byte and then the remaining bytes will just be the SSD structure of the request. And this structure is defined in the Cl specification. So it's identical to what is already defined for these request types within the consensus layer specs. Yeah, I agree.
00:14:13.322 - 00:14:15.830, Speaker F: We could specify more.
00:14:17.890 - 00:14:20.098, Speaker B: Well, I don't know that the byte.
00:14:20.274 - 00:14:22.442, Speaker D: The distinguishing byte is specified.
00:14:22.506 - 00:14:22.850, Speaker B: Right.
00:14:22.930 - 00:14:24.266, Speaker D: That's new, right.
00:14:24.378 - 00:14:38.550, Speaker F: Or, I mean. Yeah, it's kind of, I mean it is. If you look into the, into the pr, you kind of see that. Yeah, I mean it has a, it has this array of. Yeah, I mean, I guess I should.
00:14:39.050 - 00:15:04.120, Speaker B: No, I mean, I'm just clarifying so that this byte comes from the different eips. And again, I forget the number, but lite clan also had a pr that basically gives sort of a general structure for this request type and request data. So Felix is saying is the request data is essentially SSD. You would just cut off the first byte, deserialize the remainder, and you're good to go.
00:15:04.660 - 00:15:52.296, Speaker F: Yeah, I mean, I don't want to say the word, but I mean, this is like a union, basically. I mean, so the main thing is that it's not a. We were debating if it makes. But I mean, since the union was being removed, we didn't want to call it the union, but I mean, honestly, we gotta put it, put this information somewhere for the, for the CEO to distinguish the request and. Yeah, so basically that's why it's, it has this byte, we, it. I want to note that we have, we have this byte already in the El level encoding. So at the moment, in the current specs for the El, we do use this type of encoding with the initial byte for the request type, and it's in all the eips.
00:15:52.296 - 00:16:01.460, Speaker F: It's just that with this change, the CL will have to handle that as well. Anyway, I'm happy to answer a question from Porto's right now.
00:16:03.560 - 00:16:50.584, Speaker E: Yeah, it's just a comment. It's the same comment that I told Matt. So it seems that someone would have to do this work, which is we have this list of requests, and each request we need to put it in a bucket. And what you're suggesting now is to send this list with this extra data that we don't care about, or we only care about to deciding which bucket we're going to put it. And then the Cl needs to arrange these things in different buckets and then get the corresponding SS object, which is the list of requests that we care about. On the other hand, the same work could be done by DL. You could in principle send a list of lists.
00:16:50.584 - 00:17:29.510, Speaker E: So a list of. List of hex strings, which is ordered. And then you put all the request type one first, then all the request type two then. And it's exactly the same work that you will be doing, or we will be doing without having this byte that it's non existent today in the CL. So again, I don't really oppose, I don't have any strong opinion as to who should be doing this work. It's just that it's clear that this work has to be done by either the Cl. If you give it as one list, we will make this nested list of lists.
00:17:29.510 - 00:17:45.670, Speaker E: Or if you give us a nested list of lists, then we can already consume it, the same amount of work on one or the other one. So I don't really see why the push to have this, the separation of the first byte on the CL side when you.
00:17:48.170 - 00:18:27.800, Speaker F: Just. It is way easier for us though, because then it just means that we don't have to care about basically in the, er, implementation. It means when we are processing block, we will just go through the system calls, append it all to a list, and then just basically send it off to the, to the, to the CR. We don't have to think. And I think this is quite important for me because this mechanism is like defined for the, for the benefit of the, of the consensus layer. Like the execution layer does not care about these requests at all, and it should not really have to think about it too hard. So from my point of view, it makes more sense for us like this.
00:18:27.800 - 00:19:05.800, Speaker F: It kind of sidesteps a bunch of these issues as well that were mentioned in the earlier pr. This is kind of where we're. I don't want to get too deep into this discussion because there were a bunch of things like, yeah, can't you do it like this? Or whatever? This is not, I mean, we have come up with this approach because we think it will be something that is the easiest to implement for everyone, like mostly for the Elsa, and splitting it into these separate lists for the purpose of putting it in the beacon block. I think it's okay to have the Cl do it, in my view.
00:19:08.140 - 00:19:44.830, Speaker E: I also think it's not that much of a work for us. I think the biggest complaint, concern that Dustin from Nimbus had was that eventually, if we move to having SS ed, then this list that you're proposing will have to be encoded in a way. And it would be very different if you had a list of lists. If you had a list of lists. It's something that we can handle it today with our SS libraries but if you only have one list, we would have to come up with this new type, which either is an SS union or whatever new type is.
00:19:47.800 - 00:20:00.440, Speaker F: I mean it's just going to be a list of like byte vectors or something. I think honestly that's basically what it's going to be. I mean it is kind of a list of byte vectors right now in adjacent here. So I don't think it's going to be inherently a problem to.
00:20:00.480 - 00:20:14.664, Speaker E: Oh, so you're suggesting that we take it as a list of byte slices. Yeah, that's, that's, that actually would work. If, if the SS object that you would be sending is not going to be the SS object that we expect, that should be fine for us.
00:20:14.792 - 00:20:24.024, Speaker F: Yeah. So basically the whole proposal is just sending like an array of like bytes objects. We encode them as hex. Because it's a JSON perceiver then basically.
00:20:24.072 - 00:20:24.296, Speaker G: Yeah.
00:20:24.328 - 00:21:06.260, Speaker F: So I mean if we ever convert this stuff into s, I don't know, this will be handled in some way. I mean we handle this. It's the same for transactions right now. I mean for transactions we send these like opaque hex objects and they have the first byte containing the transaction type and then the remainder is this data which is totally the opaque and specific to the type. And I mean it's, it, you could say it's a kind of union, but honestly it's just because, you know, that's what you got to know which object it is and I don't know. Yeah, this like list of list thing, it has some disadvantages as well. It's, it's, yeah, I mean, let's see.
00:21:06.260 - 00:21:26.700, Speaker F: Okay, how about this? We won't really make the final decision today anyways. Next week is Acdezhe. I will be presenting this change also there from the perspective of the ELS, I guess you will also be there assuming, and then you can listen in again and think again. And I mean, yeah, it doesn't have to be decided today. I really just wanted to bring it up.
00:21:31.760 - 00:22:06.090, Speaker G: It's interesting for sure. I would note one difference from the transaction case is the Cl's mostly don't care about the details of transactions. I mean I will, I can't speak for every obviously case here, but generally speaking transactions are a black box. So whatever, you know, union like structure they might have internally is typically not that relevant for this purpose. I mean beyond whatever the ELS want to do with that. And, but the Cl. But it has no bearing.
00:22:06.090 - 00:22:13.710, Speaker G: The engine API aspect has no bearing on this one or the other. But in any case, that said, this is definitely an interesting proposal.
00:22:16.370 - 00:22:36.746, Speaker B: Yeah, to Felix's point, the requests are kind of black box for the El. So it's just kind of the inverse here. So. Yeah, yeah, thanks, Felix. Everyone please take a look. And it sounds like we will touch on it next. Acdeh, cool.
00:22:36.746 - 00:23:09.366, Speaker B: So the next thing we had on the agenda was PR 3818. So it's the wrong spot in the chat. There we go. There's a link in the chat. And so this PR is kind of big, but it's important. It's a follow up to 6110. I think the core issue really, as far as I see, is that there's no sort of rate limiting in the deposit contract just because of how it's written.
00:23:09.366 - 00:23:35.820, Speaker B: And so what that means is that there could be unnecessary load on the CL or like unacceptable load on the CL if someone went and made a bunch of deposits, say in one block with the request structure under six 1110. And this PR essentially adds some queuing to mitigate any issues with too much load. And yeah, Mikhail, I think you're here. Would you like to say more?
00:23:37.680 - 00:24:33.734, Speaker C: Yeah, just quick overview. This PR was kind of, while there opened, we were working on tests, so now it's kind of ready to get merged from the testing perspective as a good test coverage. Yeah, the overview of changes is, as Alex said, trade limits the deposit processing per epoch to sets it to 16. Actually the purpose of that is to rate limit signature verification operations. Also for the transition from Ethan Bridge. For the Ethan bridge deprecation. This queue is also important because it allows to when the front run attack where the withdrawal credentials can be front ran by the new deposit requests.
00:24:33.734 - 00:25:51.734, Speaker C: So it actually sets the street order of deposits being processed. So the first yes, the new deposit requests will wait for until the ETH one bridge deposits are fully processed that establish this strict order and prevents the front run attack. And yeah, there are some other things like finalizing deposit request position before applying the deposit. We have been discussing that this is not super important, but just nice to have, since we introduced the queue, it would be nice to have this functionality as well. And one thing I would like to pay attention to is the switch to compounding validator call, which is introduced by a max effective balance increase. And yeah, there is the new feature where one can top up its validator and switch to compounding credentials. This operation requires signature verification, so the signature should be valid in order to do the switch.
00:25:51.734 - 00:26:01.090, Speaker C: And this PR actually moves this switch to compiled into process deposit requests, which actually means that every deposit request.
00:26:04.350 - 00:26:04.710, Speaker B: That.
00:26:04.750 - 00:26:57.810, Speaker C: Has this operation is not going to be, is going to be like on top of the limit of signature verification that we have for EPUB process processing. So it's kind of like not rate limited. I don't know if it's, if we think about it, probably it's not that bad. The purpose why it's done this way is just because somebody will have to wait for the entire deposit queue if it's large enough to switch to compelling credentials, which is kind of like not that good ux, but probably it's nothing. Yeah. If we highly want to also make the signature verification, put it under the limit that we have for input processing or this operation, then we can move it to the queue as well. I mean the switch compounding operation.
00:26:57.810 - 00:27:06.110, Speaker C: So that's probably one thing that is questionable with respect to the design proposed by this printhead.
00:27:10.520 - 00:28:01.930, Speaker B: Thanks. So this came out of a lot of discussion at interop, although we were already getting some plus ones in the chat. Anyone have any questions around this printhead? Okay. I mean it does seem like we need the rate limiting for the deposits for the request. So yeah, I looked at the sum and I think this is pretty much the cleanest solution. Yeah, I guess it'd be nice cl teams if you could approve the pr just to give it a thumbs up. But I guess we can take these in the chat as a signal and.
00:28:01.930 - 00:28:20.170, Speaker B: Yeah, otherwise we'll look to merge this pretty soon, I guess. One question. I know we had discussed having more sophisticated queue management downstream to this pr. Are we still thinking that that's useful, desirable?
00:28:25.920 - 00:28:34.500, Speaker C: Yeah, the main reason for that was the performance. So probably it's still important.
00:28:39.160 - 00:29:21.920, Speaker B: Yeah. I wonder if this is a thing where we can just actually get numbers on the next Devnet and see how it shapes up. That was definitely everyone's intuition that there would be a bottleneck there, but that'll be inside. If we do want to do that, it'd be nice to do it sooner rather than later, just so we can get the core pector stuff stable. Okay, cool. So yeah, take a look at that and then we'll move on to the next agenda item. So let's see that one.
00:29:21.920 - 00:29:57.898, Speaker B: Okay. I did want to get a temperature check. So again there were, there's another stream of prs to further work with the attestation refinement that we have. Let me just grab links here. So there was this 1st 13787 and this next one. And they essentially just rework the attestation types in various places with the claim being that it's just simpler and easier for clients to manage things, whether that's attestations from the network, attestations on chain. They've kind of been sitting for a little while.
00:29:57.898 - 00:30:14.150, Speaker B: Has anyone had a chance here to look at them? And yeah, again, I'm kind of, I'd like to get to a place where, again, these like core factory ips are pretty final, sooner rather than later. So I'd like to push these along. If we're serious about these prs.
00:30:24.570 - 00:31:22.930, Speaker H: I mean, I can just say two words about them. The first one, no, they were also discussed at, at interrupt. That's where they're coming from. And basically the EIP that they reference, they kind of introduced refactoring of the attestation type and then piggybacked that refactoring onto the network types that could actually have remained the same. So what one of them does is retain the old format for aggregates on the network and then this gets translated into a new type for the on chain thing, for the on chain attestation. And we have to do this work anyway. So like the format there doesn't really matter.
00:31:22.930 - 00:32:35.110, Speaker H: But for the network it's easy. It's like more secure to use the previous type because it's more tightly defined to match what's going on on the network with the committee size. Then the second PR is actually even more interesting from a security perspective because it allows validating the signature of the attestation before computing a shuffling. And this is actually one of the bigger selling points of this PR. What happens right now is that you can just connect a client and maintain good reputation with your peers, and then when you want to introduce some instability in your neighbors, you just send them an attestation for which they don't have a shuffling. And there is no signature check on single vote attestations that is possible without performing, without computing the shuffling. And as any client Dev knows, like computing shuffling is very, very expensive.
00:32:35.110 - 00:33:49.340, Speaker H: So clients have a few options and they're all bad. One is dropping the attestation, which is kind of dumb because this typically happens when the network is not very stable. That's the best time to exploit this thing. And dropping at the stations at that point, just like delays getting back to nice stable finality, they can kind of process them. And this eats up resources for computing the shuffling and then for holding lots of shufflings in memory, which tends to do, they can delay processing it, which, which is also bad because it introduces these cascading delays in the propagation. So the PR kind of narrows doesn't eliminate this possibility that you'll have to computer shuffling, but it narrows it down so that only validators can cause others to have to propose to compute shufflings. And this is definitely an improvement versus just any random network being able to do this.
00:33:49.340 - 00:34:05.820, Speaker H: I like them and they're certainly simplifying the Nimbus codebase when we implemented them or when I implemented them. I'm kind of hoping that this story will be the same for the other Cl's.
00:34:10.050 - 00:34:13.950, Speaker B: Cool, thanks. Has any other Cl team had a chance to look at these?
00:34:25.130 - 00:34:46.520, Speaker E: Yeah, just repeating what is in the chat. We've looked into this. It seems to me that it's trivial to implement. But last time that we dealt with it attestations and we said the same thing. Well, we know we are where we are. This one does seem to be much simpler because it's only a little change on the subscriber and on the p two p side. I like the change.
00:34:46.520 - 00:34:48.640, Speaker E: I'm in favor of that.
00:34:52.980 - 00:35:12.638, Speaker B: Okay, cool. And we have another plus one from Enrico. So cool. Let's see. I mean, timing is hard, but like if we could get to a place where we feel like these are ready to merge by the next yl call, that'd be really nice. Otherwise. Yeah.
00:35:12.638 - 00:35:53.282, Speaker B: Client teams, please take a look and chime in on the prs and yeah, I'll do what I can to move these along. And yeah, we'll get that sorted. Okay, so that was it for Pektra before we moved to pure dos. Are there any other pectora things we want to touch on? Okay, appear to us then. I could start by asking for any implementation updates. I think we're looking at. Let's see.
00:35:53.282 - 00:35:59.390, Speaker B: Pure dust. I've met two, but I don't think it's live yet. Anything anyone would like to share on that front?
00:36:01.330 - 00:36:27.470, Speaker A: Yeah, just a brief local testing upgrade update. There's a pinned kertosis config if someone wants to try it out. There are three super nodes as well as three normal nodes in the config. That seems to be going well. I think Lodestar said that they would try it out locally and get back to works. And Nimbus will get back to us on Monday with potentially something that works.
00:36:31.370 - 00:36:37.430, Speaker B: Okay, awesome. So things are moving along there. Great.
00:36:37.810 - 00:36:52.610, Speaker A: So I guess one quick question I had on the topic. Is there some way to actually stress this thing? Because, I mean, right now it's just, it's running, but it's not doing much else, I guess, besides blobs.
00:36:55.070 - 00:36:58.450, Speaker B: Right? And it's still on DNA, right?
00:36:59.470 - 00:37:07.450, Speaker A: Yeah, the rebase on top of Petra is for the next four. For the next ten actually.
00:37:07.790 - 00:37:19.740, Speaker B: Right, right, yeah. I mean, one option would be to like run our Deneb stress testing on the peer dos.net just to see what happens. Otherwise, I mean, we could try bumping up the blob count.
00:37:22.600 - 00:37:24.060, Speaker A: Yep, sounds good.
00:37:24.640 - 00:37:38.260, Speaker B: Yeah. Which actually does call it a mod. Go ahead.
00:37:39.730 - 00:37:44.670, Speaker A: Yeah, I think we kind of having the same thought. How do we increase the block count?
00:37:45.850 - 00:38:28.020, Speaker B: Yeah, well, and I was going to ask about 7742. The prs to the various specs are all still open. I wasn't sure, it might have been discussed on the breakout this week, but I guess if clients have that, it does help simplify raising the blob limits. Okay. Is coordination for this mainly in the pure dos testing channel on the EtH R and D discord? Perry?
00:38:30.120 - 00:38:31.374, Speaker A: Yeah, exactly.
00:38:31.552 - 00:38:59.284, Speaker B: Okay. So yeah, we can just take it there and we'll push this along async. Cool. So thanks everyone for that. Next up. So this question came up from the breakout, and essentially, I think even already we've seen some problems around computing the pure dos proofs. Especially if you have like a sort of under resource node or a low resource node.
00:38:59.284 - 00:39:39.672, Speaker B: I think you can take something like up to a second and maybe even a little bit more to compute all the proofs that we need and especially as we move to a higher blob count. So for example, I think there's even discussion of say 1632 as a target and max limit for peer dos. That's great, because then we have more data throughput. But if it's going to take so much longer to make these proofs on the cl, then we have an issue. Because let's say you build up until pretty much a proposal deadline and then it takes you another second plus of blocking work to actually get the block out. So this came up and. Yeah, let's see.
00:39:39.672 - 00:40:10.180, Speaker B: I have a link here to the pure dust notes with a little more context, but there was a question around how to handle this. There are a few different options. I don't have the link, but ASn actually had a nice summary somewhere on the discord. And some of this will be dependent on what the els want to do. Let's see if I can remember the options here. I mean, the first one is just keep it as is. You could imagine.
00:40:10.180 - 00:41:04.970, Speaker B: Maybe the validator then knows in the Cl to like start block building or like when it goes to fetch the block that's been built, it does that a little bit earlier to offer time. Another idea would be to move this proof generation to the eldest, just sort of like to pipeline it, and then we gain some, you know, time there. This is kind of a violation. Well, it's definitely a violation of separation of concerns between the Al and Cl, which is not nice, even if it is say like a pretty straightforward option. So that then teed us up to a third proposal, which is essentially to have like richer communication between Cl and El. So that, you know, one way to think about this is like the El would essentially post to the Clint through the engine API for every blob. The Cl can then start working on these proofs in parallel to the build on process, and then we go about it that way.
00:41:04.970 - 00:41:28.780, Speaker B: So that was this sort of high level overview of the problem and some of the solutions that have been proposed. Does anyone here have any context I'd like to add, or do any of these feel like promising fast forward for peer to us right now? Sean? Yes, I talked to Jimmy about this yesterday.
00:41:29.240 - 00:41:30.740, Speaker F: He said this.
00:41:33.360 - 00:41:34.984, Speaker B: Change that Michaels proposed.
00:41:35.072 - 00:41:37.224, Speaker F: To allow the seal to get blobs.
00:41:37.352 - 00:41:40.820, Speaker B: From the Els mempool could help to some degree.
00:41:41.640 - 00:41:44.424, Speaker F: This is because you don't have to.
00:41:44.432 - 00:42:29.504, Speaker B: Wait on blobs over gossip necessarily if you already have them in your mempool. So it's an optimization there. Another thing we talked about was, and I don't know the exact details of this, but another, more, I guess, radical option is to have a decentralized proof generation. So have nodes across the network generate proofs with some sort of responsibility division. But yeah, again, I don't know too much the details there, but that's just another option we talked about. I think that's it. Cool.
00:42:29.504 - 00:42:57.030, Speaker B: Yeah, I mean, the thing is, I think you'd still need to make the proofs in any case, because like one, you won't know for some remote peer what they have in their mempool. So you'd always need to do this. And then the issue is, say you're the home sticker with like only a little bit of hardware, this problem, then you're kind of host. So yeah, I think we need to do something. The question then is what?
00:43:06.060 - 00:43:22.200, Speaker D: I mean, I think we should definitely push harder on the distributed building. I think that's the most sustainable solution anyway. And if you have such low resources, it's also likely that your bandwidth is low, so you struggle sending out those proofs anyway.
00:43:22.500 - 00:43:23.240, Speaker B: So.
00:43:25.300 - 00:44:42.212, Speaker D: In almost all cases, your best strategy will be to send out your block first and bet on other people adding those proofs, which we can definitely do like if they're in a mempool that will accelerate things. And I think otherwise, I really don't want proof generation to leak into the execution layer, because we're currently trying to decouple data availability more from the execution layer. That's why, for example, we also want to have the block limit only in the CL, rather having it set in both the CL and the EL. And I think in that context, it only makes sense if we make sure that all the proof generation logic is also in the CL, because further upgrades to data availability will require us changing that again. We will want more samples, want a different type of samples, for example, 2d samples and so on. And, and we want to make those upgrades without having to change the EL as well. So, yeah, I mean, I do think like we should find a reasonable way of like getting the blobs into the CL so that it can generate the proofs.
00:44:42.212 - 00:44:55.930, Speaker D: But I think like the actual more promising avenue for home stakers will be getting other nodes to generate the proofs for them. And I think like, yeah, there are many things that can be done about that.
00:44:59.630 - 00:45:30.020, Speaker B: Right? Yeah, that makes sense. And I think this is a good point about not having things leak into the yale, because as you say down the road, it'll only get more complicated. So, cool. George, put this link in the discord. So, yeah, take a look there. And I guess for now, just consider this open question we should all be thinking about. And yeah, otherwise I think we'll just keep, keep thinking about this.
00:45:30.020 - 00:45:39.420, Speaker B: Okay, anything else for pure dos?
00:45:43.200 - 00:46:11.540, Speaker G: So I do have a question about this. I guess I'm reading the notes and it, I mean it from linked here, for example, it breakout room notes is that it can take up to 1 second kind of under what circumstances or what kind of hardware kind of concretely is. We're talking about running a raspberry PI, or I mean an average laptop or. Yeah.
00:46:15.200 - 00:47:19.476, Speaker D: One x 86 cpu that takes like two to 250 milliseconds to generate one proof per, like on one core. So for example, if you had 32 proofs and four cores, then it would take 2 seconds, each core would generate eight. So yeah, it depends on like both the numbers of blobs we include and how many cores of what type you have. And raspberry PI is going to be more of a problem because that there even like it takes 1 second to generate all the proofs on one core. Like arm is just a bit slower at the moment and I think, but I don't feel like Raspberry PI is currently really like a common choice for stakers. Like I think like that was like our target five years ago and like I think we've probably moved on from that. And we have like slightly more powerful typical devices now.
00:47:19.476 - 00:47:23.040, Speaker D: Like, it feels even hard to get less than four cores nowadays.
00:47:26.700 - 00:47:51.208, Speaker G: Sure. No, I mean, yeah, I don't mean to put particularly seriously the idea of like it must run on Raspberry PI. But just as I get it, getting a sense of what the sort of quantitatively what the numbers here were, whether we're talking about it like relatively beefy home servers, or whether. And had to be, yeah.
00:47:51.344 - 00:47:58.260, Speaker D: On a beefy home server with like 16 cores, say like, you could easily get it done in a very short time.
00:47:59.040 - 00:48:25.446, Speaker G: Okay, so this is, so this, this is more of a case or as, as, as Justin posted the, like the rock five, that, because that is, that's, that's actually a better benchmark here because the, from a nimbus perspective, it's not clear that the Raspberry PI is a super viable today as a reliable node, although people might try, people to try, but the rock five is. That's definitely a thing.
00:48:25.518 - 00:48:37.920, Speaker D: I agree. I'm, I agree and I think that is what we should, I feel like that is what we should be looking at at the lower end of the nodes at the moment. I think raspberry PI is starting to look unrealistic.
00:48:38.860 - 00:48:40.560, Speaker G: No, I make sense.
00:48:44.020 - 00:49:02.850, Speaker I: So I just want to add that outside of benchmarks in Mainnet, the node will be doing a lot of other things rather than just computing cell proofs. So whatever benchmark you do get, you know, the actual number if you're running this on maintenance, would be higher.
00:49:14.310 - 00:50:19.450, Speaker J: Yeah, I just wanted to say that I said in chat just now that like one not super obvious thing is that the failure modes are quite different depending on how we approach this. So if we basically do the dumb thing and just have the CL compute the proofs after the block is requested from the ER, then that adds delay to the critical path. And that means that basically if there's any kind of issue or it takes too long, then that means the block just won't make it in time and will basically be anchored, which is quite unfortunate and pretty bad case. Whereas if you were to just do it in parallel with block production. So if the El had some way to initiate the kind of proof generation while it's assembling a block and it ignores block transactions until the proofs are finished for them, then basically the failure mode is much more graceful because then it would just mean that as a weak node, then maybe you just don't have all the proofs in time, and you have to ignore some of the blob transactions. And so you can't have. You may be missing out on some super long tail of the fees, but otherwise, your block is completely unaffected, and the timing is completely unaffected.
00:50:19.450 - 00:50:27.030, Speaker J: So I personally would very much recommend we don't do this. Something of generating the proofs on the critical path.
00:50:32.690 - 00:50:54.332, Speaker B: Yeah, makes sense. A question I have with this, sort of. I mean, I think first we just have to spec out what we mean with, like, decentralized proof. Jin, a little bit more. One thing is, let's say you have, you know, your rock five sitting there. You push out your block with, you know, commitments to the blobs. You rely on someone else to make the proofs for you.
00:50:54.332 - 00:51:12.490, Speaker B: Is there any dos concern there? I feel like that would be, like, the first problem we'd run into, where if I just get a block, I don't exactly know the rest of the context of the block. Maybe it's fine, but I'm not sure if anyone's thought about this.
00:51:14.390 - 00:51:36.782, Speaker I: Well, there's no dos concern. It's just repeated work across all the super nodes in the network. So if you have the commitments from the block, you can look at your local el mem pool. And if you have the blobs, you just create the cell proofs from that. The issue doing it this way is that, yeah, it's in the critical path, so.
00:51:36.846 - 00:51:38.610, Speaker B: But what if you don't have the blobs, though?
00:51:39.990 - 00:51:42.650, Speaker I: Yeah. Then you can't do anything. You're stuck.
00:51:44.630 - 00:51:52.622, Speaker D: Yeah. You do assume that some super node has all of. Otherwise it doesn't work, because currently you can't create any proofs if you don't.
00:51:52.646 - 00:51:53.650, Speaker G: Have all of them.
00:51:55.270 - 00:51:57.300, Speaker B: Right. Okay.
00:52:02.200 - 00:53:28.280, Speaker D: I mean, one of the ideas here would be to give us better channels to notify the supernotes that they would do some work. Like, I don't think, like, many people running these would find it a problem because we are talking about donating a few seconds of cpu time and, like, there's no extra load from distributing these because you would be doing that as part of p two p. Anyway, so, like, if we have, like, a nice mechanism. So, like, one of the ideas would be, what if we could broadcast, like, block headers before we broadcast the block? Like, that could be much faster, and they would get a heads up, and then, like, they could just do this, and possibly they would, like, even come fast enough that it would still be faster. Than actually sending everything. And I guess a more advanced version would be if homestake is announced in the slot before that, they won't be able to build the proofs and basically asking the supernotes, hey, could you precompute it for the ones that you see in the mempool? Now, it wouldn't be impossible to do this, but I mean, I guess it would require more messaging because now you need a signed message from the validator saying, hey, I want these proofs built.
00:53:31.380 - 00:53:44.588, Speaker B: Right. And we do advertise. Like you can just walk the DHT to find super nodes, right? Because that just means you have like all of your bit set for your custody channels. That would be the way to identify them, right?
00:53:44.644 - 00:54:05.740, Speaker D: I mean, I was assuming we just do it through like a gossip subtopic, but yeah, I mean, I guess you could, because like, I mean, this is very low bandwidth, right. It would only be like one tiny message per slot. But yeah, I guess you could also do it by directly contacting them.
00:54:05.900 - 00:54:13.880, Speaker B: That's what I was thinking. Yeah, I guess you could possibly do both. Cool.
00:54:13.960 - 00:54:35.670, Speaker D: I mean, another way of doing this would be asking the relays to do that. It's a bit of a weird thing because you're not actually using relays when you're a self builder, but it could still be a service that they provide where they simply have an endpoint. Like send me your block header and I will create your proofs and send them out on peer to peer.
00:54:39.370 - 00:54:40.114, Speaker B: Right.
00:54:40.282 - 00:54:49.470, Speaker D: Like, it would still be trivial for them to do that. And I think, like if you added that to the software, they would do it, but it's slightly weird that, right?
00:54:51.530 - 00:54:59.354, Speaker B: Yeah. Do you think there'd be any incentivization there? Like, would the proposer pay the relay or, you know, assume there's no, I.
00:54:59.362 - 00:55:23.690, Speaker D: Mean, I feel like, I feel like it's one of those things where the value would be so tiny that it's really not even worth paying for it. The relay does most of the work by simply the implementation. Is most of the work actually doing it is so trivial. How do you even pay for that? It's worth a millisecond or something.
00:55:25.470 - 00:55:30.184, Speaker B: Right. Okay.
00:55:30.272 - 00:55:31.980, Speaker G: Does this require that kind of.
00:55:37.880 - 00:55:38.192, Speaker D: Does.
00:55:38.216 - 00:56:06.526, Speaker G: This require that kind of infrastructure? In Devnet's testnets, and because we don't, the VLA infrastructure has historically taken sometimes a little while to come online. Just, I mean, once the network is running and mainnet forks, it's unproblematic. But even the first testnet sometimes doesn't have it. Have everything ready there, so.
00:56:06.678 - 00:56:38.710, Speaker D: Right, right. I guess that is advantage. If we solve it internally as part of the node software, then we don't have this external dependency. Are people keen to build this distributed thing and find some way of notifying the supernotes? Then we could maybe create a group and talk about aspect for this and do it. It doesn't feel to me like it's super complicated. We just need to agree on something and do it.
00:56:47.700 - 00:57:08.920, Speaker I: I mean an easiest solution would be just to have the El communicate which exact blob transactions it has in its current payload that it's building. And the Cl could just build them in parallel. We don't need to have a separate channel for all this stuff.
00:57:09.700 - 00:57:10.108, Speaker C: Right.
00:57:10.164 - 00:57:40.170, Speaker D: You're saying like you're. I guess the risk is that the latest like, like eR, block building takes much less time than the proof computation, or like probably less time in most cases at least. So like you basically doing it like this will kind of throw a little bit away of the time advantage you get by building it as soon as you receive the block.
00:57:42.670 - 00:58:00.050, Speaker I: Yeah, so like at least right now we send to the El one slot before to start building the block. And as part of client software we could just like regularly pull them and ask them which blob transactions are in the payload and.
00:58:00.560 - 00:58:31.640, Speaker D: Yeah, yeah. The problem is that this theoretically could change completely, right. You could like basically add 9 seconds into the slot, it's blobs one, two, three. And then suddenly it changes completely because of different fee market structure or something. And then it's 456 and all the work you've done is useless. And you now like the El sends you a block for which you haven't prepared any proofs.
00:58:33.900 - 00:58:41.040, Speaker I: Yeah, that'd be the worst case. Although I think in the average case it shouldn't change too much.
00:58:41.860 - 00:58:59.318, Speaker D: I agree. So sure. Like I mean if you are like if you're happy with it being a little bit dossable, which this is, then I guess that's fine. I agree that it works in the average case. I don't think there is a lot.
00:58:59.334 - 00:58:59.890, Speaker G: Of.
00:59:01.590 - 00:59:05.410, Speaker D: Competition like this on blob markets where people outbid each other.
00:59:07.430 - 00:59:19.970, Speaker I: Yeah, and you know, even if it becomes the worst case, you could just fall back on this distributed blob building and have all these super nodes do it if it becomes that much of an issue.
00:59:23.300 - 01:00:00.580, Speaker D: Right? Yeah, I mean I guess like if we say like we focus on distributed blob building, then we can probably say okay, like we get away with investing less in like optimizing the local proof computation because it's not really on the critical path anymore. Like I think most of the time with good distributed block building, you're better off sending out your execution block first because that's the only thing that really has to come from your node and everything else can come from somewhere else. So you're using your bandwidth more efficiently if you do it that way.
01:00:01.840 - 01:00:12.776, Speaker I: But there would always be added latency with this because you need the block, a block header to reach the super node and the supernode needs it's time to reconstruct.
01:00:12.808 - 01:01:06.170, Speaker D: Well, it doesn't necessarily add latency because like if you view it as a whole system, right, when you're sending out your block and samples at the same time, you might be banned with constraint on that. So it might actually take longer for other nodes to receive those. Whereas if you start, say, by sending out your block header first and then your execution block, then those get sent as soon as possible and your bandwidth is not clocked up with the samples in addition. So it's not necessarily true that sending everything in parallel is the optimal strategy. If you have others who can do that work for you, that might actually be a better strategy. Even if you have all the proofs, it can be a better strategy not to send them. If other nodes can send.
01:01:09.850 - 01:01:14.910, Speaker I: Yeah, if you're bandwidth constraint that would be a strategy that makes sense.
01:01:16.290 - 01:01:35.950, Speaker D: I mean, I would assume that most home stakers are bandwidth constraint because that's the hardest thing to get. Right like, I mean, getting like. Yeah, a high upload band with Internet connection is in most places not cheap or easy.
01:01:39.770 - 01:02:20.372, Speaker B: Right. I do want to surface Onsgar's comment here that, you know, there are several options here. It sounds like there is support for what we're calling option three in the post that George linked. So this is essentially having the clint poll or having the Al post which blobs are being considered locally. So it just has time to compute these proofs locally. That sounds like a good way to move forward, in which case, yeah, we should work on a spec just to start to get a sense of what that actually looks like in parallel. If we want to think about these more distributed proof generation tactics.
01:02:20.372 - 01:02:31.330, Speaker B: That also sounds super promising. Yeah. So Dankra, if you want to make a group, or we can use one of the discord channels, but yeah, I think we should move forward in that way.
01:02:33.070 - 01:02:34.330, Speaker D: Yep, sounds good.
01:02:35.230 - 01:02:36.010, Speaker H: Cool.
01:02:40.790 - 01:03:16.770, Speaker B: Okay, let's move on to the next thing on the agenda. And I think it was this one, right? So there's a pr to remove SSE unions from the spec. We were kind of joking earlier in the call about unions being a dirty word, and this PR is addressing that. So it's 3906 in the consensus specs repo. And I believe this PR just actually deletes the text itself. The rationale here is that. So, yeah, today we do have unions in the spec.
01:03:16.770 - 01:03:57.160, Speaker B: They're, I would say, a pretty experimental feature. I don't even think the production implementations of the z across the board have a unit implementation. They're not currently used in the consensus specs and so, you know, there's no reason to implement them. They, yeah, and because of this, it's like this kind of weird thing where it's like, okay, maybe we could use a union for this, say like the request types. And then it just kind of leads to a bunch of conversations that can sometimes be bike shedding. So to address all this, this PR basically just proposes to remove them entirely. I think this makes sense enough.
01:03:57.160 - 01:04:25.650, Speaker B: I would maybe suggest leaving a note somewhere just like, hey, by the way, this was here in this commit and it was removed there. You could imagine making features like we do for the specs themselves, like the beacon chain specs. You could have like a features folder for SSD. There are a couple options here, but yeah, I just wanted to have some time to discuss, first off, if we want to go ahead and do this, and then the best way.
01:04:33.870 - 01:05:29.496, Speaker G: So I guess as the author of this particular PR, I'll just say that part of, I guess my perspective is I regard them essentially as a fiction, is maybe the most blunt way of putting it. And this is my, when, as you say, come, it creates these spike shedding concerns is people treat it, I regard it as real, as inventing a new SSE feature. So when somebody proposes using unions, SSE unions for something, it's like, it's as if they, to me, as if they invented a new, a new thing. And they're also proposing that at the same time as whatever other feature they're proposing. And that's why, at least from my perspective, the bike shedding me internally might happen. I can't obviously speak for anyone else here. I would certainly be happy to put a link in that pr to have it.
01:05:29.496 - 01:06:03.760, Speaker G: I mean, the pure deletion thing is kind of the platonic form of this PR, I think, but I'd be happy to have it internally linked somehow to, here's the, maybe the last, either the commit which removed it, or I mean, it's kind of intertwined in the files, but somehow making it easy to. So you didn't have to manually sift through and find and get history where it is. We can use git history. That's what git is for, but to make it easy to find and easy to, if people want to see it for sure.
01:06:04.460 - 01:07:24.170, Speaker B: Yeah, that was my only point is just, you know, in previous iterations of the consensus specs, we have used them, say for like early sharding designs or things like this. So it'd be nice just to like be able to easily restore them in the event we need them in the future. I don't know if any portal people are here, but they did make a comment that they are using them, which is maybe a bit of a complication. But yeah, we might need to follow up with them just to see what makes sense with them. But otherwise, does anyone oppose taking unions out or like managing them some way? Okay, I'll ask the portal folks what they think, but yeah, then we'll figure something out if they want to keep it somehow. I would propose going with this feature route that I was talking about, but otherwise, yeah, it does seem like they kind of unions themselves in the spec complicate things, like Dustin was saying more than they really should be. It would make sense to address it somehow.
01:07:27.750 - 01:08:25.366, Speaker G: Okay. And as far as, I mean, propose a couple of, I can, the two most. There's a feature, so factoring it out to one of these feature branches is one, one option, another is to use git history, another and link to like a whole file, or there's a link basically to somehow self referentially to a removal pr that can, I guess, be added later. A two stage thing there, various approaches. Is this, I guess my only slight concern. I think I'd be broadly okay with the features thing, especially since they're, I mean, so with the portal thing, it's, that portal is strictly not Ethereum, and I would want to limit that. I mean, this is actually a point a little bit beyond just this particular.
01:08:25.366 - 01:09:06.920, Speaker G: Obviously I care about this peak web pr now, but the fact that somebody may have taken an encoding, say that Ethereum has a consensus spec encoding of something, and that if the consensus specs don't use it, or on the Yale side would say something very similar, but somebody else decides, oh, this random feature of RLP is great, and it turns out that Ethereum didn't care. I'm not sure I would regard that as Ethereum's responsibility as such. I don't know, kind of maybe organizationally or on a spec level, how that was best managed.
01:09:09.220 - 01:10:10.786, Speaker F: So I got to say that portal is very much a part of Ethereum in the, it may not be a part of the critical path right now, but it's definitely a protocol which is being developed for the benefit of the Ethereum protocol. And it's very much, I know, I mean, it's funded by EF a whole lot. So I wouldn't really put it like as a way external project with no connection. And I do think that on that second point, just defining an encoding is something that is kind of independent of what the encoding is used for. And SSE is a very general purpose encoding. It is very useful, it has properties that are very useful to Ethereum, but it is a generic encoding. And so changing encoding is okay and removing features is also okay.
01:10:10.786 - 01:10:46.620, Speaker F: But I mean, we do have to think a bit about like how this affects things that are using this encoding, or if, I mean, if we all consider units to be a mistake, then yeah, I mean, there got to be some, some other way to do this. But I don't think it's, it's a good thing to say that just because Ethereum is not using a certain feature of this encoding, it's like it should be deleted from the Ethereum version of the encoding. I think there's no such thing as an ethereum specific version of SC. It's just an encoding that we invented.
01:10:51.720 - 01:11:30.240, Speaker B: Yeah. And I think a nice way to square these is to just move it to some sort of feature spec and then it's there. Okay, I will follow up with the proto people and just see what their take is on this and we will figure out how to move forward. Okay, I think that was everything on the agenda. Let me just double check. Yeah, it looks like we got everything for today. Any closing comments? Otherwise we can wrap up a little bit early.
01:11:30.240 - 01:11:49.120, Speaker B: Okay, thanks everyone. We'll go ahead and close out the call and I'll see you on the next one.
01:11:51.230 - 01:11:51.566, Speaker H: Bye.
