00:00:02.840 - 00:00:18.090, Speaker A: Hello, welcome to vertical implementers call 19. This is issue 1064 in the PM repo. Okay, starting off with client team updates. Who would like to start things off?
00:00:21.550 - 00:00:41.710, Speaker B: I can do Ethereum J's. So we are working on a vertical try implementation while waiting for the next testnet to test on. We are we should be ready for casting in seven in terms of gas cost updates and like the basic leaf data and the way these things are going to be now architected.
00:00:59.180 - 00:02:13.960, Speaker C: So I can, I can continue with Bezu. So the Java client on our side I think the, in terms of the gas costs it was decided to wait for a little bit more stable spike. I don't know how like so I missed a couple of the of the last implementers call so I'm sorry if I missed some things but I think the team decided to wait for test framework for the gas cost. We meanwhile are thinking about the migration, the sink and how in particular we can retain the cloud DB while doing the migration. So there's an issue that we have that since we are storing with Verkle try the flat. Like sort of the flat DB is by stem. It's like we store the values by stem and we index them by stem.
00:02:13.960 - 00:03:16.830, Speaker C: That comes from a hash from some sort of a hash from the account and storage key. But before we had check of the address and get check of the code or something like this, we cannot retrieve that information from the stem. So we are wondering if we'll be doing two reads to make sure that we like trying to read in the vertical or if it's not there then trying to read the old flat DB during the migration. We're trying to avoid that and we are not exactly sure how to proceed at this point. But yeah, so that's one of the questions. And I don't know if like Gary is there, you could comment more on the gas costs. But we also have meanwhile also another thing is the we're trying to optimize the storage between FlatDB and the nodes, what we're going to store.
00:03:16.830 - 00:03:18.950, Speaker C: So that's where we are now.
00:03:25.330 - 00:03:40.936, Speaker D: Okay. Yeah, we've been working on the test framework and adding tests, so both I and in SEO have worked on different aspects of it with the testing team of course. And otherwise.
00:03:41.008 - 00:03:41.232, Speaker E: Yeah.
00:03:41.256 - 00:04:17.462, Speaker D: We've been collecting data to study a series of answer some questions that have been asked over the years or over the last few months actually. For example the extra gas consumption with or without EOf or the size of the witnesses, things like this. Also the size of the time it's going to take to make the transition. We don't have. So we collected the data. We don't have a complete analysis yet. It will be published.
00:04:17.462 - 00:04:53.472, Speaker D: Yeah. And hopefully the next two weeks or. Yeah, before the, before the end of the summer for sure. And we were also working at the spec update changes, I think. Yeah, I mean, we kind of had an agreement. So addressing what Thomas has said, the spec is pretty much set in stone, it just needs to be updated. So this is what I'm doing.
00:04:53.472 - 00:05:06.910, Speaker D: I mean, it's never set in stone until of course, it's been released, but we have a pretty clear vision of what we want to achieve now. Yeah, I don't know Ignace, you've got something else to add?
00:05:09.730 - 00:06:49.200, Speaker F: No, just to expand a bit on my side on that is for 4762 6800. And the other one that I don't remember the number. I already finished the test vectors. Everything is a bit on the air because there are some library changes that we need to do for them to be filled. But I already assumed the 4762 gas changes that we have been discussing. So probably after the testing library pending changes are done and all that, I can probably fill all these tests and as soon as they have the fixtures generated, I can share that with other clients for them to run. And regarding these other analysis stuff, I've been collecting around 1 million mainnet transactions on the tip of the chain the last few days using the guest live tracer, and my idea is to basically collect all the program counters that all the transactions touch when running, and using this to simulate how much gas the 31 byte chunker and 32 byte chunker will use.
00:06:49.200 - 00:07:37.930, Speaker F: First you compare both options because we have like some theoretical understanding on pros and cons of them. But I mean, the only real way to really know is by kind of simulating them on real transactions. Apart from that comparison, just knowing how much of gas overhead will call accessing other two transactions, which is pretty important to know from data, from real data. So yeah, probably in the next big call I will have some report with all these and I can share the results and we can go from there.
00:08:02.200 - 00:08:02.776, Speaker B: Cool.
00:08:02.888 - 00:08:04.380, Speaker A: Any other updates?
00:08:06.110 - 00:08:31.700, Speaker G: Yeah, so no major updates from Nethermind. We're still working on improving the cryptography performance and getting the sync to work. I probably have some more updates in the next week. I just had one comment. I think the transaction further is not working on the six test net, so if you can fix that, it'll be better to test the sync.
00:08:51.440 - 00:08:51.888, Speaker B: Cool.
00:08:51.944 - 00:09:00.390, Speaker A: Anything else? Any other updates? Anything on the testing side.
00:09:03.250 - 00:10:08.534, Speaker H: Hey guys. Yeah, so I believe last time, last time we spoke, we had the transition tests working, filling and executing on Geth with some failures, but that ended up being somewhat incorrect. So we had some false positives of, I guess, the test that we failed and this was on our side, but now we've resolved the bugs for that for the most part. And we're now filling and executing the transition tests on Geth with. Yeah, very, very small number of failures. So there are believe twelve failures right now, all specific to withdrawals. And I'm fairly confident it's a small bug on our side that I just need to, still need to fix.
00:10:08.534 - 00:10:46.620, Speaker H: But the moment we have all of these transition tests passing on Gethenne and we hope to make a release of these transition tests for every client to utilize. And during this process, there was some back and forth with Guillaume regarding the transition tests and I guess it led to some sort of overkill way of how we can really throttle like overly test the transition, especially on the transition block. I can maybe share something real quick if that's okay.
00:10:49.600 - 00:10:53.780, Speaker A: Yeah, sounds good. Are you able to share? Give me 1 second.
00:10:56.200 - 00:10:57.820, Speaker H: You guys see my screen? Okay?
00:11:01.680 - 00:11:03.260, Speaker A: Yes, I can.
00:11:04.040 - 00:11:42.210, Speaker H: Yeah. So I guess the, with regards to transition tests, the. I guess the free types of tests that. I guess the conclusion we came to is pre fork transition tests, the, the mid, mid fork transition test and the post fork. So with the pre fork, and ideally we want to. So I guess before I get into this, and these are tests that we already have. Tests that test many things on all the forks previously.
00:11:42.210 - 00:12:47.870, Speaker H: So these are filling tests that we already have that we want to utilize for testing verkle. So, yeah, so the pre fork transition tests, what we want to do with these is we want to execute all the test blocks before the vertical transition block so that the MPT is filled. And then afterwards we add some dummy blocks after the vertical transition blocks so that we can verify that everything's executed properly. Then we also have these mid fork transition tests. So this is where we execute the test blocks on the vertical transition block. So the basic approach is just for existing tests, the first block, the first test block for an existing test. And that block could be executed on the vertical transition block.
00:12:47.870 - 00:14:19.354, Speaker H: But ideally we want to update our framework so we can. And for example, let's say we have a test with four blocks, we want to execute each of those four blocks on the transition block. So for one test with four blocks, that would essentially output four specific to the test block executing on the fork transition block, then we also have these post fork transition tests where ideally we start with some dummy initial NPT, we perform the vertical transition, and then once the transition starts and there's some stride enabled, we'll then execute these test blocks. And we can get somewhat crazy with this, at least with the way we filled the tests. So I guess the most basic form would be with zero stride, which is what we're doing right now. And for the pre fork transition tests and the mid fork transition tests with no initial additional MPT. But ideally once we have these tests ready and clients are passing them, we will then I guess add some initial NPT.
00:14:19.354 - 00:15:18.630, Speaker H: So we have a really large MPT and then also stride as well. And we can kind of fill these tests accordingly. My hope, so this is specific to our framework, but my hope is that we can fill the tests with these kind of fields. True. So if we want to fill the pre fill tests for pre fork transition, mid fork transition, coursework transition, the amount of MPT accounts, additional NPT accounts that we want to add and also enabling stride. So ideally we can just write the fill command with these parameters and very easily generate the tests that we want regarding the transition specifically. And yeah, my hope as well is that we can kind of make, make simple releases first and then build on top of them and make them more complex.
00:15:18.630 - 00:16:20.478, Speaker H: But yeah, currently the tests we're focusing on for the transition tests are these mid fork transition tests. The basic approach were just the first. The first test block that we have is executed on the vertical transition block. And yeah, that's kind of it with regards to the transition test. It's something which I personally overlooked when we were generating the test before, but it's a step in the right direction and yeah, so just to summarize that, once we get finished with the basic transition tests and we'll create this release and then we'll focus our attention on all the test vectors that Ignacio has been writing for the vertical specific eips. So then we can fill the genesis test. So starting at Verkle, if anyone has any questions regarding this, please speak up.
00:16:20.478 - 00:16:26.110, Speaker H: You think this is a good approach or if we can approach this better?
00:16:31.490 - 00:17:23.420, Speaker I: I just have one question, if I may say. You're using Shanghai for the pre transition, so most likely will be Dengkun for pre transition. And you know, even if we do pass this one, you know, do you have to rewrite those tests for then kun? Or maybe we could just focus on, because we're writing the test anyways, might focus on then kun to local transition directly because I think it's not going to be much time after maybe, you know, three or four months that we sort of, you know, hack, you know, continue doing this hacky way. I mean, nothing against it, but you know, just something to think about.
00:17:24.160 - 00:17:52.090, Speaker H: No, for sure. That's a great question. So I guess the good thing about our framework is that it's written in such a way where you can fill from any fork. So it will on our side literally be changing the fork name and it should just work straight away with filling the test from Dengkun. So hopefully no worries.
00:18:02.710 - 00:18:03.566, Speaker B: Cool.
00:18:03.758 - 00:18:22.910, Speaker A: Awesome. Thanks for sharing this. Any other questions or comments on this stuff? Up next on the agenda, code size in Verkl and whether it might make sense to raise that to something as high as 2gb. Dano.
00:18:25.970 - 00:19:05.934, Speaker E: So based on the comments and the threads, I'm not sure what the current state is, but the basic concern I have right now, code sizes are limited to 24. There's a lot of pressure from the solidity community to raise that. Number one of the issues, while we don't raise it now, at least prior to Shanghai, was concerns about jump test analysis and having to do that jump test analysis every time we load the code from memory because of the way vertical. Well, two things happen. Number one, we charge for init code. Now when you deploy a contract. So if you're doing the jump test analysis, when you're deploying the contract, we charge you gas for that.
00:19:05.934 - 00:19:55.718, Speaker E: So the size of the analysis really shouldn't impact the system because it scales linearly with the gas cost. So creation at so the analysis at create time shouldn't be a problem. However, what Verkle adds is an interesting twist, is they kind of store and cache some of this analysis in with the code, whether they're going to do the overhang method, which is the current plan of record, or whether we go with ipsalons, only store the places that matters method that analysis is cached. So when you load it from the vertical tree, you don't have to do the analysis. And in fact, because you're loading it chunked, you can't do the analysis except for the stuff that you do it with. So based on that, it would be reasonable to increase or uncap it. There are contracts that there's a diamond pattern which contracts are currently using to deal with Livis's limitation by switching between the various contracts.
00:19:55.718 - 00:20:35.200, Speaker E: And some of those are easily larger than sixty four k. I don't know if they're going to get up much larger than they're probably not going to get two gigs in the next five years. But the short answer is that using two bytes to store the code size, I think is something that's going to become problematic in the near future. Not the far future as in within months of the vertical fork, the next fork, you might want to increase it, and we don't want to be up against the limit when we decide to increase it. So my pitch is that whatever we do for the code size in storage is, it is, it's more than two bytes. Three bytes would be better. That would give us 16 megabytes.
00:20:35.200 - 00:21:07.230, Speaker E: I don't think in, you know, three to five years there might be, you know, reasonable contracts that aren't explicitly trying to exploit the memory size that would target that. But, you know, as far as design processes, casting four bytes to a native data size in a programming language is a whole lot easier than casting three bytes to a native data size. It's a lot quicker. So if we were going to increase the size, four would be the next logical byte link to do it with. But generally speaking, two is too short for the solidity community.
00:21:15.500 - 00:21:31.120, Speaker D: Yeah, sorry, I wanted to raise my hand but I couldn't find this thing. Oh, it's right here. Okay, I have a couple questions. So, yeah, 16 megabyte, that was actually going to be a suggestion. I was going to make 16 megabytes because. Right. This is what I wrote in the issue.
00:21:31.120 - 00:21:47.020, Speaker D: Currently I don't see, you know, 4gb worth of code being ever practical. Right. Unless you have a different insight into this, no one will ever deploy even 2gb. Right.
00:21:48.640 - 00:21:57.260, Speaker E: You know, 640k is gotta be enough for anyone. For a PC it's hard to predict the future beyond five years, but I don't see it happening within five years.
00:21:57.560 - 00:22:43.946, Speaker D: Okay. Right. So what I was going to suggest is to, so currently indeed in the spec, this is, I remember having the conversation that you mentioned during the, like in the, in the issue, I remember having this conversation and yes, we were discussing 64 as a limit, saying it would be. I don't even exactly remember what the full context was, but I remember what we sort of agreed on, which was 64 right now, and then in the future we could expand. Okay, so now you seem to think, or you think that we are going to need a higher value pretty quickly, which, which is reasonable.
00:22:44.138 - 00:22:45.138, Speaker E: Within five years.
00:22:45.194 - 00:23:12.234, Speaker D: Yes, within five years. Okay, fair enough. Which, you know, I mean, depending on how long Petra takes, might be. Okay. I'm not going to say Verkle happens in five years, but looking at it, Verkle is not happening before end of 2025 at best, in the really very best of cases. So five years is like tomorrow. So that's a good reason to expand.
00:23:12.234 - 00:23:51.280, Speaker D: What I was saying, my semi counter was, I don't want to allocate bytes that are going to be sitting there unused when they could be used for something else. There's other people suggesting. So if we make the code size two bytes, that leaves us five reserve bytes in that basic data field. There's a lot of people eyeing those five bytes. At least. There's a few use cases, maybe not a lot, but a few. There's resurrection counters for state expiry, for example.
00:23:51.280 - 00:24:31.958, Speaker D: There's definitely state, sorry, there's definitely code size. There could be other extensions, like if we want to do account abstraction, things like holding, for example, a bit for whether or not, whether or not the contract has storage. Yeah, there's a lot of people contending for this, but three bytes, I would be quite happy with that. My question is, what did you say three bytes? Like four byte is better than three? Because you could just apply your mask and that's pretty, pretty easy to do, right? Unless I'm missing something, it is easy to do.
00:24:32.014 - 00:24:46.730, Speaker E: It's more bytes, it's more hassle, it's more ways to make mistakes. In the absence of resource constraints, that would be preferred. But if resource constraints are legitimately strong, then three bytes would be better than two.
00:24:48.390 - 00:25:36.600, Speaker D: So what I would suggest is that we reserve three bytes, but we kind of. So we put like the way we organize those 32 bytes for the basic data is that we put the zeros in such a way that if we ever need to expand three bytes into four, it will have priority, basically. So I would say we put the version number, then those four reserved values, then those three bytes, then whatever is left in the basic data account. So this way the code size can spill over into the reserve size, if need be, fairly easily. What would you think of that?
00:25:37.340 - 00:25:59.682, Speaker E: Would be fine. We would need tests, however, to fill in those reserved bytes with values and make sure that things still parse. If we reserve those, we need a test that those don't interfere, especially with the design spillover. That is one test particularly. We want to make sure that people don't just take those four bytes and rely on that reserved byte being zero. So we would need testing to cover that case, right?
00:25:59.866 - 00:26:04.790, Speaker D: Yeah, fair enough. I can add. Yeah, inaccio.
00:26:09.330 - 00:26:39.510, Speaker F: I just was going to say that three wise counsel to me and regarding like, where to place them, I think it's not really that relevant, because if we want to expand this, we have to set up a new version. So the whole interpretation is different. So this is not like placing them in another place kind of facilitates anything, because if we want to use those bytes, we have to change the version, and that's a completely different interpretation of the slots.
00:26:40.420 - 00:26:53.480, Speaker E: So if it was like a minor version where it's a forward compatible change, where those have no meaning, and the 1.0 for the 1.1 have meaning, I think clients could handle that. But if you're changing the order of things, then yes, you absolutely need a new version.
00:26:55.020 - 00:27:30.860, Speaker D: Yeah, that's what I was going to add. Indeed, you don't really need a new version, you just need a new fork to say, well, this byte used to be to always be zero, but now we removed the 16 megabyte limit. So beware, that fourth byte can no longer be zero all the time. And this way you would not have to create a new version. That was because honestly, I would rather reserve those four bytes right off the bat instead of forcing a new version just for a code size, like a change in the size of the code size.
00:27:34.000 - 00:27:50.860, Speaker E: But my anticipation is that a transition from needing less than 16 megabytes to more than 16 megabytes is going to be at least five years off and is probably going to involve a new version. Would be beneficial anyway. So it's not the worst case scenario if spilling over requires a new version.
00:27:51.280 - 00:28:17.750, Speaker D: Okay, okay, fair enough. I'm still going to modify, because I'm currently editing that EIP. I'm still going to edit it the way I. Yeah, but at the same time I'm just thinking those reserve bytes would probably be like, I mean, correspond to a new version anyway, so might as well. Maybe I'm just being overly cautious here.
00:28:18.490 - 00:28:21.290, Speaker E: It preserves design space. We don't have to use it.
00:28:21.450 - 00:28:39.960, Speaker D: Yeah, yeah. Okay, let's do that then. I'm going to say three bytes. Put them really at the beginning, right after the reserve space, so that we have indeed an open design space in the future. And I will share the updated EIP so that everybody can give feedback and disagree.
00:28:40.620 - 00:28:47.320, Speaker E: Thank you, appreciate it. It solves a lot of problems now that we won't have in the future because we have more options for space.
00:28:52.870 - 00:28:53.730, Speaker A: Sweet.
00:28:54.950 - 00:28:55.334, Speaker B: Cool.
00:28:55.382 - 00:29:11.902, Speaker A: If nothing else on that topic, we can move on. Next up, we had any discussion on the Testnet relaunch. Just because we don't have a lot of time left on this call, maybe we can try to keep this relatively short. I don't know, Guillaume, did you have any specific things you wanted to ask with this testnet? Relaunch Guillaume.
00:29:11.926 - 00:29:56.370, Speaker D: Yeah, this is a question I want to keep asking until we seriously look at restarting the testnet. The goal is still to restart the testnet around midsummer, by which I mean like end of July. Right. This new testnet would have a few of the bug fixes that were seen on the current Kelstein and Testnet. Just add the new guest model, the one we agreed on at the interop and in the meeting last week, and also finally implement the fill costs. Yeah, the question is very simple. Does anybody else want any other feature for this? Otherwise, I mean, I will keep asking at every Vik in case people change their minds.
00:29:56.370 - 00:30:14.310, Speaker D: We still have about a month to the launch, so teams should be working on the new testnet, which I think every team does except us. But yeah, basically the question is, any other wishlist for this testnet?
00:30:14.950 - 00:30:22.970, Speaker B: Are we going to have the pre state route included in the execution witness? Sorry, I might have missed some of the updates that are going to come in the next testnet.
00:30:23.870 - 00:30:33.970, Speaker D: You're right, the answer is yes, but I forgot to write that down. So yes, we would put the new test route in this new version.
00:30:34.550 - 00:30:35.970, Speaker B: All right, cool, thanks.
00:30:47.160 - 00:30:49.780, Speaker A: Sounds like no other wishlist items for now.
00:30:54.200 - 00:30:54.792, Speaker B: Cool.
00:30:54.896 - 00:31:15.514, Speaker A: Then we can move to the last agenda item. We have a discussion around pre image distribution. We had some of this discussion at the interop. There's been some more recent guess discussion and ideas tossed around. So just wanted to, I think a few people wanted to open that back up. Anyone that wants to start things off. I know.
00:31:15.514 - 00:31:19.510, Speaker A: Gabrielle, I think you had some thoughts you might want to start with there.
00:31:21.330 - 00:31:58.432, Speaker B: Yeah, sure. So pretty much distribution, we discussed that at the interrupt. One of the ideas that came is, well, during the transition period, well, as we all know, the issue is that, well, some of the state is accessed from the frozen MPT and without we're not able to convert to the Virgo trial unless we have the pre images. Now to most of the if someone does not have the privileges, they would be not able to sync the network.
00:31:58.496 - 00:31:58.720, Speaker A: Right.
00:31:58.760 - 00:33:04.660, Speaker B: Because they wouldn't be able to access the state that is necessary to execute the blocks. Now we have some space during the transition period because the execution witness is not actually useful until the full transition to verkle. So what came to mind is why not use this space to provide the actual cream images that have been converted. So a block could contain all the lists of the addresses with corresponding like storage slots when storage has been accessed, leading to the leaves that have to be converted in order to, you know, perform the conversion from the MPT to the verbal try within that block. So we could have a field inside of the execution witness called converted preimages. And then someone who's validating the chain of would be able to access their own local frozen mpt. Know which leaves to convert from these pre images, convert them to the vertical, try and also execute the block.
00:33:04.660 - 00:34:04.736, Speaker B: The main issue is that in addition to the space that it uses, which in my view is not really a concern, because we can definitely like do without the execution witness anyways during that transition period. And the size that we're talking about is not that large. The main issue is that you're still not able to produce blocks. You're only able to validate blocks because we still require someone to have the preimages locally accessible in order to build a block and provide these privileges in the first place. It reduces the scope of the problem because you can always delegate block building. You can always, if you want to have the burden of building blocks yourself, download them through your own means. But at least someone who didn't go through that process is still able to perform all of the duty as a validator, except the block building part.
00:34:04.736 - 00:34:20.660, Speaker B: So I think it doesn't fully solve the how are we going to distribute the privileges? But at least we have a. A very interesting plan b for users who don't want to go through the burden of storing that themselves.
00:34:23.360 - 00:35:07.338, Speaker D: Yeah, I was just thinking, is it true though that we can, like those validators that do not have the free images, can delegate to the builders? I'm not so sure. Because in the current model, what we want to do is start the sweep of the iterator in the previous twelve minutes. Sorry, in the previous. The last 4 seconds of the previous slot. And that means that if they don't do this at this time, they will have to do it when executing the block that they are being given. And so there's this first, the first 4 seconds of the block when they would have to do this. So I'm not sure you can actually fold this one off.
00:35:07.434 - 00:35:13.710, Speaker B: How interesting. Unless you could provide the future pre images to convert.
00:35:14.090 - 00:35:29.070, Speaker D: Yeah, unless exactly the preimages you pass are actually the pre images for the next block. Yeah, that could work. I don't know if it works, but I mean, it would solve this specific problem.
00:35:29.890 - 00:35:30.314, Speaker H: Yeah.
00:35:30.362 - 00:35:32.840, Speaker B: Well, if it depends, like do we.
00:35:34.540 - 00:35:34.828, Speaker E: Yeah.
00:35:34.844 - 00:35:51.920, Speaker B: I hadn't thought about, you know, the kind of processing when we want that processing to happen. If we expect people to begin converting the pre images because before they hear of that block, then we need to kind of front run it by blocking. That makes sense. Yeah. Tanishki.
00:35:53.860 - 00:36:30.410, Speaker G: Even if we add images, like for first, like getting the pre images for the next block will be a problem. Like we cannot, will not be able to add the pre images that we'll need to process the next block in the current block. Because again, it's the same problem that we do the pre mesh processing in the next 4 seconds. So like, it will take, end up taking much more time for the value for the block builder to like kind of process two set of sweeping operations to generate premise for the next block. And another problem with this is the first block. First transition block will also be a problem.
00:36:32.190 - 00:36:44.810, Speaker D: Well, you can always make at least the first transition block. You can always make this sweep start just before, like the block before. So you would initialize it this way.
00:36:46.390 - 00:37:36.250, Speaker B: Yeah, the sweep one block after and the first block is just, you just indicate which wants to sweep in the next and then begin sweeping. But this, this whole, like, if we had, for example, an exhaustive list of all the leaves that have to be converted, like with, with the pre images that's frozen at the transition time. Right. So the, let's say we just decide based on ordering that we're going to convert ten k leaves per block. This is deterministic from the transition time, right. So someone could in theory just have all of that like ready way before. And of course you have to actually update the vertical pride the right time so that the state matches.
00:37:36.250 - 00:37:44.600, Speaker B: But I don't see why it wouldn't work to just announce it one block before so that people have that time to begin the processing.
00:37:51.180 - 00:38:23.080, Speaker G: Yeah, I think then we'll have to like go back to the model of premise pre images where like everything is sorted in sequence. And that, I get that the problem with that was that the size becomes quite big because there are a lot of repeated pre images. But I think if it's kind of a trade off, like if we want to do this, then we'll have to go back to that image model where everything is sorted into in a way that it will be sweep. Yeah, I think we can do that.
00:38:38.830 - 00:39:44.940, Speaker B: Yeah. There's been in, in pre image distribution discussions, there's been mentions of like if we want to exhaustively distribute the pre images to all clients and have in protocol way of like signaling that that's also can also be, you know, challenging to do for, you know, clients or nodes to be able to announce that that also requires some additional thought and coding. If we at least have that. And I still see this as just a backup plan, I'm not really happy with people, like, delegating block building. I'd rather have everyone have the privileges. I'm just saying that it seems like such a, like, engineering challenge to get, like, everyone on board that I'd rather have a somewhat satisfactory backup plan to validate blocks and then have a smaller scope distribution problem. So, yeah, we kind of can do away with, like, just allowing a lot of time for people to download it, but not having.
00:39:44.940 - 00:39:53.690, Speaker B: Requiring people to signal it in protocol and then begin the transition at that point, which is something that was mentioned as a proposal. Yes, on that.
00:39:55.350 - 00:40:56.350, Speaker I: So I think the pre images should be there in the previous block. Associated to that is the question. How would you know what pre images are to be considered for the next block? Now, at the point of frozen Merkel three, there would have to be some sort of an index that, you know, from n to n plus ten k you're going to convert to Verkho, but say there is a bug. I 10,000 leaves have a few discrepancies with another nodes, 10,000 leaves, which might or might not be acting maliciously by putting it onto the block. We might have another angle of debugging as well. So I'm not saying like, do away with redistribution, but I'm saying have both of them.
00:40:58.090 - 00:41:39.902, Speaker B: Yeah, I think even if so, I'm still assuming that clients would, like. There's no stateless execution at that point. Right. So everyone has the local state, including the frozen NPT. And I don't recall exactly what the conversion spec is, but my understanding is it's defined based on the key of the leaves. So even if blockbuilder proposes ten k primages, you're still able to validate that these pre images correspond to the actual leaf keys that you have locally. Let's say you sorted them somehow, that you know that the next ten k are the ones that have to be converted.
00:41:39.902 - 00:41:57.050, Speaker B: And if there's one missing or there's one extra, or like, there's a gap where it starts at the wrong place, the wrong index, then you're able to refuse that block. So you're still able to perform all of your conversion duties except providing the actual privileges to be converted.
00:41:59.670 - 00:42:25.500, Speaker I: Yes, but there has to be a confirmation in protocol that the frozen MPT is exactly the same for everybody. Of course there is the root. I'm not so sure if the indexing, I don't know so we're just assuming that everybody's going to use the same index at that point. So it's a fair assumption here.
00:42:25.960 - 00:42:40.466, Speaker B: Yeah. Well, maybe, Guillaume, you can describe, I'm not sure myself, like, how were we planning on having consensus on which leaves have to be converted at which block, right?
00:42:40.498 - 00:44:09.010, Speaker D: So the way it happens is simply a lexicographic enumeration of the tree, right? So everybody, I mean, maybe not has the exact same index in terms of database indexes, but everybody has the same view of the tree, right, because it's the last block that the last MPT block. So everybody has the same content, so everybody can iterate, and everybody should provide, should see the same iteration of accounts. And then for each account, if they're state, you also iterate it in, you iterate the leaves of the storage in lexicographic hash order. So what this means is that, yes, you can verify, like, if you're given a block with the pre images for the next block, it's relatively easy. It's a bit IO intensive, but it's relatively easy to check that every single one of those pre images correspond to your next 10,000 leaves. And if those 10,000 leaves don't match the pre images that you are being given in the specific order that you have been given it, then you should reject the previous block, because that means there's a disagreement on the state of the frozen NPT, and in that case, you are on a different fork. That happened.
00:44:09.010 - 00:44:18.570, Speaker D: That happened before the tree was frozen, basically. Perry said something.
00:44:20.120 - 00:44:39.128, Speaker J: Yeah, I can also just read it out. How would this work? If the last MPT block comes in really late or if it's reopened out, would you then just restart the transition with that being taken into account, thereby just delaying the first transition block, or. Okay, to use that is already finalized.
00:44:39.184 - 00:44:39.856, Speaker D: Okay.
00:44:40.008 - 00:44:44.660, Speaker B: I think we want to go with the finalized. Oh, I just saw the message from.
00:44:48.620 - 00:44:49.680, Speaker J: Yeah, thanks.
00:44:53.220 - 00:45:17.210, Speaker D: Right. And I thought you were actually asking something else. If a reorg happens during the conversion process, so the MPT is finalized, but you might have to replay the iteration of the tree itself. This is what you have to do in case of a reorg anyway. So it's not connected to the images. I mean, that's not the question you asked. It's just a precision I'm giving for better understanding.
00:45:34.030 - 00:45:41.420, Speaker B: Yeah. Bigger is to hear the people generally feel like this would be a valuable thing, or.
00:45:49.240 - 00:46:15.686, Speaker D: I, for one, think it's a good idea, but it doesn't solve the core problem. But like you said, it makes it more easy. It makes it easier to handle. So I'm generally favorable to it. It's just my opinion, of course, and I might change it after I implement things. But. Yeah, I think the core of the problem is still there, unfortunately.
00:46:15.686 - 00:46:19.450, Speaker D: But it's a nice trick and it definitely is helpful.
00:46:20.910 - 00:46:22.330, Speaker B: That's my feeling. Also.
00:46:24.310 - 00:46:33.320, Speaker A: Do we have a sense of implementation or complexity add of doing this? My assumption was not a lot.
00:46:34.140 - 00:46:36.120, Speaker B: I don't think it's very much.
00:46:36.540 - 00:46:37.760, Speaker A: Yeah, gotcha.
00:46:38.060 - 00:46:40.660, Speaker D: I mean, that's me again, when I implemented it, but. Yes.
00:46:40.780 - 00:46:43.988, Speaker B: Yeah. Gottfried has raised this.
00:46:44.044 - 00:46:45.236, Speaker D: Yeah, yeah.
00:46:45.308 - 00:46:46.480, Speaker E: Just a quick question.
00:46:46.940 - 00:46:52.684, Speaker D: We are assuming here that for a missed slot, we do not do any sweeping.
00:46:52.732 - 00:46:52.964, Speaker B: Right.
00:46:53.012 - 00:46:59.556, Speaker D: Because otherwise, this, due to this delay of one block, this doesn't work. Just to be clear that this doesn't.
00:46:59.588 - 00:47:02.516, Speaker B: Allow us to do more sweeping or whatever.
00:47:02.708 - 00:47:24.932, Speaker D: More transition. If some thoughts were missed. Yeah, we would not be doing any sleeping. Sorry. Sweeping and probably no sleeping either during this transition period. But we. Yes, I think with this approach, it's even worse to.
00:47:24.932 - 00:47:37.820, Speaker D: It would be even worse to actually have to process, because if you have a reorg, that means your block that has all the pre images would be really. Become really large.
00:47:40.440 - 00:48:30.610, Speaker B: Yes. But isn't that also a benefit of providing the exact premises to be converted or that have been converted in the block itself, as you can? Actually, I'm wondering, like, that brings me to the thought, like, since we've sort of just said that it would be helpful to have the pre images not that have been converted in this block, but the pre images to convert in the next block. Then what happens if that next block is, you know, empty? I mean, you might have begun the conversion process yourself, but then you notice that this is an empty block, and then you can. You have to roll that back. Right?
00:48:31.510 - 00:48:34.062, Speaker D: You mean empty like missing slot or.
00:48:34.166 - 00:48:38.610, Speaker B: Yeah, just a missed slot. Like no block has been proposed.
00:48:39.030 - 00:48:50.102, Speaker D: Then the sweep happens in the block after that, the first non missed slot after that. So it's going be useful anyway. Yeah, yeah, yeah, yeah, yeah.
00:48:50.126 - 00:48:53.570, Speaker B: I guess that works. Yeah, that's right.
00:49:06.670 - 00:49:07.070, Speaker E: Cool.
00:49:07.110 - 00:49:17.140, Speaker A: Well, I'm sure, though, this won't be the last discussion on this question of pre image distribution, and maybe we can come back to it the next vic, or continue the discussion asynchronously.
00:49:19.240 - 00:49:31.940, Speaker B: Which EIP is the conversion process? And like, if I want to add this as a, like, addition to an EIP, where should I add that? Is that 6800?
00:49:32.320 - 00:50:03.390, Speaker D: No, no, there's another. So the conversion has been described in two eips, but one of them hasn't been created yet. I'm still looking for the right wording for it. So the first one is 6070 612. We just described the overlay method, but doesn't describe the conversion. And the reason why it's split in two is simply because I expect people to say, oh, the conversion is a bit too much for this fork. So let's just go with the two trees and then do the conversion at a later time.
00:50:03.390 - 00:50:31.190, Speaker D: It's basically to offer some flexibility so far. People think they should come together and I agree, but I'm expecting. Yeah, I see it as some failure mode, basically. So there's another. There's another eap that should come, but I haven't had the time to do it yet, to complete it yet. I can try to finish it in the next two weeks and ping you then. And then you can add your stuff to it.
00:50:31.580 - 00:50:33.320, Speaker B: Cool. Yeah, perfect.
00:50:42.860 - 00:50:54.316, Speaker D: We don't have that much time left, but did Perry also want to present something or. Or did they dream that Perry is here?
00:50:54.348 - 00:50:55.480, Speaker A: But I don't think.
00:50:56.020 - 00:51:27.970, Speaker J: Yeah, I don't think there's much to say, but I can just bring it up as a general topic. Um, there is like increased discussion around Eip for falls and a thought around whatever methodology we use to distribute. Um, the historic state. Uh, the history in four four could potentially be reused in vocal as well. Uh, but, yeah, that of course has the downside that four four s is not defined yet and it's not smart to layer research topics on top of each other. Uh, but, yeah, just a thought I wanted to put out there.
00:51:29.670 - 00:51:40.370, Speaker D: Right, I'm sorry. Yeah, my question was, you also wanted to prototype this for four four. Has work started on this?
00:51:41.150 - 00:51:41.526, Speaker B: Yeah.
00:51:41.558 - 00:52:40.400, Speaker J: So there's one proposal I made last week on ether search that's using torrents for four four. The main advantage of tolerance being that they're an extremely mature piece of technology that has existed for 20 something years at this point, and it gives us a lot of the feature sets that we already want for four four s. Initial reactions on client team seem quite good. I think that at least going to be playing around with it to see if it's feasible. I can link. Yeah, Josh just linked the post, but yeah, theoretically you could reuse the same methodology for pre image distribution as well. Cool.
00:52:40.520 - 00:52:42.700, Speaker A: Anything else in the final minutes?
00:52:43.760 - 00:53:12.380, Speaker D: Yeah, maybe just add some information about this. I have been working on the conversion a bit in the last two weeks, or actually the last month. It takes about a day and a half to produce. At least that's what it takes Geth. I'm actually not quite sure how long it takes Erigan, for example, to produce this. But Geth would take about a day and a half. To produce such a file.
00:53:19.650 - 00:53:21.230, Speaker I: Aragon takes 2 hours.
00:53:22.690 - 00:53:23.670, Speaker D: That's nice.
00:53:31.370 - 00:53:31.754, Speaker B: Cool.
00:53:31.802 - 00:53:41.640, Speaker A: All right, well, I guess we can leave things there for this week if nothing else. Thanks all, for joining. See you guys next time.
00:53:42.420 - 00:53:43.720, Speaker D: Yes, thank you.
00:53:44.060 - 00:53:45.444, Speaker B: Thank you. Bye bye.
00:53:45.492 - 00:53:46.236, Speaker A: See you.
00:53:46.388 - 00:53:46.676, Speaker C: Bye.
