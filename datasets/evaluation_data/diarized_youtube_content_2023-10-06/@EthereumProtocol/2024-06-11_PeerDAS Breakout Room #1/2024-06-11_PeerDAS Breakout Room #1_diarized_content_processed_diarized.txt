00:02:14.075 - 00:02:19.175, Speaker A: Hello, I can change the name.
00:07:30.955 - 00:08:48.585, Speaker B: Hello everyone. Welcome to the first here DES breakout room. So thank you for joining at this challenging time. Yeah, where is our king de lion? Okay, yeah, so we have the break on the GitHub and just a few seconds ago I just created meeting notes and how people can help to collaborate on the document. You should have access. So I listed the basic agenda of like client updates and spec discussion and next steps. But if you have some more items you want to discuss, feel free to bring it up here or on the agenda.
00:08:48.585 - 00:09:26.385, Speaker B: So I thought I here to moderate because of the spec. I want to make sure that the spake can reflect what is the current implementation status. But I don't necessarily have to be the moderate here and I think you guys did great job in the interrupt and have more information and knowledge of what is the current status. So does anyone want to share the client updates after the interruption?
00:09:32.435 - 00:09:41.655, Speaker C: Hello. Hi, this is Jimmy. I can share some updates on the Lighthouse side. Can you guys all hear me?
00:09:43.395 - 00:09:44.175, Speaker B: Yes.
00:09:45.075 - 00:10:45.425, Speaker C: Great. Yeah. So after interrupt we've got a few things that we've changed including making the peer Desk network parameters configurable now so we can configure the desk parameters like number of samples and custody requirement. Those kind of variable can be configurable at runtime so we no longer need to rebuild it. The one thing that we haven't made it configurable is the blob count. But I think that's probably something we can work on once we decide what we're going to do with increasing blob count in either Electra or the Peer Desk fork. And then on that we've deadlines made a couple of good custody lookup improvements in Lighthouse including batching the request and load balancing request to peers and also prioritize peers based on the success rate.
00:10:45.425 - 00:11:16.725, Speaker C: So there's a few improvements in lookup and then also we've worked on storage efficiency improvements, but I think mostly we've just been working on making more resist making more resilient based on the feedback that we got from Interop. But now that we've most recently been focused on our current release, we're looking to get back to peer DAS again soon. That's it from the Lighthouse side.
00:11:24.105 - 00:12:08.605, Speaker D: Yeah, I can go next. So for the Prism side we've been slowly starting to productionize our peer DAS code. For the Interop we took a lot of shortcuts so that we could get something working. One of the shortcuts was that we reused a lot of our BLOB methods for data columns. So we're slowly trying to decouple them. We're adding custom column specific feeds from a database that you can differentiate between blobs and columns. Some of the things that we are looking at are basically on how to reconstruct data effectively.
00:12:08.605 - 00:12:51.005, Speaker D: We've managed to get that working. So Manu has done a lot of work there. Prism now can reconstruct data that is as long as we have 50% of data we will reconstruct it and broadcast it out to the network. We're using a time delta approach where instead of reconstructing immediately we do reconstruct it but we wait to broadcast it. Turn the event in the column does exist in network. We don't honestly increased amount of duplicates out there. Another thing that we've been working on is to look at our basically how valid custody would affect how you generate node id.
00:12:51.005 - 00:12:55.865, Speaker D: Maybe Manu can add a comment to that because he's been doing some work on it.
00:12:58.125 - 00:12:58.985, Speaker E: Yep.
00:13:00.845 - 00:14:21.725, Speaker F: Let me find the link. I wrote a little document. So basically on Prism, at each start we generate a new peer to peer private key which then we derive into public key then into node id. It's not totally doable right now because if you continue to do that with pure nas, possibly the new private key will generate a totally new node ID and so a totally new set of subnets. And so at each reboot we will have to backfill all the columns. So we started to develop a strategy just to try to brute force private key in order to reach in order to obtain the same subnet IDs we had on the previous restart. And we have something working but which is totally not compatible with validator custody because with validator custody possibly we don't know in advance what are the subnets we will have to custody because it can change at runtime.
00:14:21.725 - 00:14:58.015, Speaker F: And so I wrote a little document I just link in the chat explaining why we could have some issues about validator custody if we why validator custody would be not compatible with the fact that we generate a new private case at each beacon boot. I don't know if other client team also tries to generate a new private key at each boot or if Prism is the only one to do it.
00:15:06.665 - 00:15:12.885, Speaker G: I may say that TECO is reusing previous private key always.
00:15:16.865 - 00:15:20.465, Speaker F: TECU does generate a new one or not. Sorry, I didn't understand.
00:15:20.585 - 00:15:23.071, Speaker G: No, no, it doesn't generate new one.
00:15:23.193 - 00:15:23.855, Speaker A: Okay.
00:15:26.275 - 00:15:28.615, Speaker F: Yeah, I guess Prism is the only one to do it.
00:15:30.035 - 00:16:34.391, Speaker G: Yeah, lots regenerates the new node ID on each restart. But I think we can sort of fix it by saving and rereading the previous ENR that was generated. And only D wipes should basically generate new ENRs. But I think the reason why we did that was so that, for example, if any nodes are banning a particular ENR for whatever reason, so a restart basically helps you get over it. So those kind of issues might come up where nodes have banned some ENRs and they stop finding peers. Because it can happen that, for example, you started out, started something, it had some bug and then you fixed it or your elbows down and basically so for whatever reasons you fixed it, you are back up, but now you can't find peers. So that would be a bit of a problem.
00:16:34.391 - 00:16:35.155, Speaker G: I think.
00:16:42.055 - 00:17:31.025, Speaker E: I think we have discussed in one of the PRs a little bit. This is the generation and I remember my point there was that if you are generating a new one, but then you are trying to be in the same bucket, basically observing the same columns, you are in a quite unique position. So it doesn't help your anonymity in, in any way. So if that's the goal, then you are breaking it just by searching for a very similar one. Because you will be kind of the only node which has selected the same set of eight columns out of 128. That's just too many.
00:17:31.965 - 00:17:44.675, Speaker F: Yeah, it was exactly the purpose of it. It was to generate a new total new private key, but which match the same data column subnets. Yes, it was the goal.
00:17:45.535 - 00:18:34.095, Speaker E: Yeah, the thing is that we don't have as many nodes as combinations, I think. So if you are going for the same, for the exact same combination of columns, then you will be the only one having that. And then it's quite clear that is still U. Even if you, your IP address has changed, you are going back into the same scenario and same subset of nodes in which it is a very small subset because there's only you in it and you after that. So it's at the surface you are having a new identifier, but then you are linking it in a way which is easy to correlate. Okay, that's my feeling.
00:18:36.195 - 00:19:01.141, Speaker F: Interesting. Yes, I see the question. It was for anonymity. Totally. Barry and Francesco. Yes, and so we were quite okay to lose a bit of anonymity by brute forcing and. But yes, it's not really possible anymore.
00:19:01.141 - 00:19:14.385, Speaker F: And as Anton said, yes, we may be in the position where we are the only one to select to have a different private key and to select the exactly same subnets as well.
00:19:24.895 - 00:20:13.145, Speaker H: One Compromise here could be that you. I don't know how easy it is in practice to do this, but like as a full node where you, you only have to custody four columns, maybe you can still do this if you don't run any validators and if you run validators then it seems like, it seems to me like you already don't really have any chance of being in any way anonymous like you have. You know, there is a clear fingerprint that comes from just you running those same validators. So in that case I'm not sure that it really matters to or like what it adds to to do this. So I don't know. Yeah, maybe you can. It might still make sense to keep regenerating it if you don't run any validators, but if you do, then I think yeah, it would seems like it kind of clashes with validator custody to try to do this.
00:20:15.395 - 00:20:18.495, Speaker F: Yeah, right. Yes.
00:20:23.395 - 00:20:43.465, Speaker G: Even if you are not running any validators, you will need to have the consistency because you won't be able to serve peer request and then you will get banned. So you won't find any peers. So doesn't matter whether you run validators or not, you will have to maintain the custody consistency for whatever node ID you are publish you are advertising.
00:20:46.445 - 00:20:55.629, Speaker H: Then you can brute force. Like if you're running just a full node with the custody requirement being 4, then it's pretty feasible to brute forces I think is what you were saying.
00:20:55.757 - 00:21:47.835, Speaker F: Yeah, it is quite easy. I linked in my document also computation. So for 32 subnets, yes, there is only of course 32 combination possible with one subnet custody and with, and with 100. So with 128 subnets and with 4 million custodies, there is about 10 million combinations, which is a lot, but which is still doable as brute force. So yes, for a full node without any validator it's perfectly doable. But with validator is more complicated because you don't know you have to to match more than four subnets. And yes, and as Barry said, if you have a validator, I mean the networks already knows you because you have a validator.
00:21:47.835 - 00:21:59.515, Speaker F: So maybe it's not so important to try to get an anonymized peer to peer. Peer to peer become node private key.
00:22:07.985 - 00:22:14.233, Speaker E: By the way, you don't have to do this calculation at startup, right? You can, you can pre compute the next one.
00:22:14.409 - 00:22:32.355, Speaker F: Yes, yeah, or even pre compute, let's say a few ones, maybe 10 ones or 16 ones in case of rapid restart for any reason.
00:22:37.175 - 00:23:05.515, Speaker G: Basically each beacon node Will for example now need to support another parameter that says that what are the data columns that you will custody or subnets that you will custody so that you can generate a node ID that is basically consistent with that custody or you generate something in the beginning and then you'll have to sort of persist and reread it each time if you want to change your prid.
00:23:09.055 - 00:23:47.785, Speaker F: Yeah, the goal was to generate each at every boot a new private key and so a new node ID which is consistent with basically at the first start of the node we just generate random random private key and so random node ID we compute all the columns we should custody we store them in a file and at the subsequent boot we just load this file, look at the previously custody columns subnets and then we try to regenerate a new private key corresponding to those subnets. It's quite simple.
00:24:00.695 - 00:24:32.905, Speaker E: So fallout of 128 is still 100 million combination? No, it's 10 million combination. So you would be still like which has the same combination. I don't know if it's a problem. It's. So my question is why are you changing your ID as it has?
00:24:36.645 - 00:24:40.461, Speaker F: Why do we want to change the ID in the first place? That is a question.
00:24:40.613 - 00:24:42.207, Speaker E: Just what are the reasons for that?
00:24:42.301 - 00:24:53.455, Speaker F: Yeah, it was for anonymity. Yeah Maybe Dshan can have more answer about that.
00:24:53.955 - 00:25:30.165, Speaker E: It's okay. I mean you are giving up on that in the sense that it can be uncovered but it takes some effort to figure out the relation between the two not in the id. So I need more information on the ID because I need also to know what one level more just if I know what you are subscribed to I know that you are the same because it's well, one out of 10 million. We will not have 10 million for some time. Every single node will have a unique convention.
00:25:51.835 - 00:26:04.505, Speaker B: Sorry to just recap it so only prison and London ID at each restart or other client. Also the sensing.
00:26:09.525 - 00:26:17.825, Speaker C: Sorry, Lighthouse. Lighthouse doesn't regenerate the ID so we. Yeah, so we maintain the same node id.
00:26:22.765 - 00:26:28.385, Speaker H: Yeah, Nimbus actually does regenerate node id. Yeah.
00:26:34.895 - 00:26:37.435, Speaker F: And did you achieve to do it with.
00:26:39.775 - 00:26:41.047, Speaker H: Nope, not yet. Not yet.
00:26:41.111 - 00:26:50.155, Speaker F: Okay okay. I can link my. If you're interested in. I can link my pull request about.
00:26:51.615 - 00:26:54.269, Speaker H: Yeah, that would be nice.
00:26:54.447 - 00:26:55.001, Speaker B: Thanks.
00:26:55.113 - 00:27:06.685, Speaker F: It's still in progress. There is not as much touch because maybe we will. We won't merge it maybe because of the dark study but yes, this is my pro request.
00:27:08.105 - 00:27:09.565, Speaker H: Yep taking a look.
00:27:21.835 - 00:27:24.335, Speaker F: One question is.
00:27:24.755 - 00:27:41.203, Speaker G: How critical is to Keep the same custody over a long period of time. If nodes are restarted in production just every couple of months because of, I don't know, software upgrade or something, is.
00:27:41.219 - 00:27:43.651, Speaker E: It critical to keep the same custody.
00:27:43.723 - 00:27:47.109, Speaker G: Set at that moment or is it.
00:27:47.157 - 00:27:50.093, Speaker E: Feasible to actually change the custody set.
00:27:50.189 - 00:27:53.105, Speaker G: And maintain the previous one for some period of time?
00:27:53.565 - 00:28:29.745, Speaker H: I mean it's fine if like one, you know, a few people like a certain percentage of the network changes even all at once. But if, let's say I don't know, whenever we have a hard fork and everyone upgrades or something like that, everyone changes their custody at the same time, then that's really bad. Like there should always be some like stable kind of part of the network that stays on the same subnets and can like respond to historical queries and just generally, you know, like the subnets shouldn't like fall apart all at once.
00:28:29.785 - 00:28:30.485, Speaker A: Let's say.
00:28:42.475 - 00:28:55.095, Speaker G: I think if you are changing your custody then you will have to sort of download the data columns for mini box to data column or some, whatever that period is and sort of make sure you can custody.
00:28:59.075 - 00:28:59.411, Speaker A: Right.
00:28:59.443 - 00:29:13.695, Speaker H: But even for that you need to make sure that other people are not trying to do the same all at the same time. Like if everyone or like 90% of the network was trying to do this all at once, then it really would be chaos.
00:29:30.405 - 00:29:41.985, Speaker G: Basically nodes can adopt this strategy on restart whenever their PR is changing. They can basically do this sync of this small window if they want to.
00:30:00.095 - 00:30:48.025, Speaker E: What maybe work. Although there would be huge chaos. So if nodes would know that this is the situation, then they could. The gossip sub networks would have to be built and then nodes could receive things even if it's not their own column. But it would be very chaotic and I don't know what would take over. Yeah, and in general just there would be a small percentage of nodes which would happen to have some common column in the old set and new set. But otherwise all the others would just hit on data that is not on the comb set.
00:30:48.025 - 00:31:19.315, Speaker E: I'm kind of thinking whether the mechanism that we have for repair and cross seeding is slowly solving such a thing as a last resort backup. But it's. It's definitely not ideal.
00:31:36.375 - 00:32:01.285, Speaker B: I feel that way. It's hard to get the conclusion in this call for this topic. Do people want to keep investig. I mean in talk discussing about it or we move on to. I mean leave it to the offline discussion?
00:32:05.395 - 00:32:38.105, Speaker F: I guess for me my takeaway is we will discrete it internally. But yes, for full node without any validator we can Try to maintain this shuffling of a private key at start and for full nodes with validator. I think we will keep always the same private key, but we will discuss it internally with the team.
00:33:02.175 - 00:33:13.555, Speaker B: Okay, let's keep continue in the K updates. Any updates from tu?
00:33:18.575 - 00:33:35.815, Speaker G: To be honest, on my best knowledge, no major things changed since Interop. Yeah, probably we'll have draft PR for sampling now and probably that's it.
00:33:39.035 - 00:33:43.615, Speaker B: Thank you. And any update for Nimbus?
00:33:45.795 - 00:34:46.609, Speaker H: So we kind of started working on peerdos at Interrupt. Since then we have implemented all of the like whatever finalized spec is there till now for in terms of consensus spec tests we are passing like networking and SSE consensus objects, Merkle root, all of that stuff. We are also passing all of the KZG spec tests from the NIM side. We have to do some more work in the request response protocol, get OR data column, sidecar by route, all of that stuff. And yeah, mostly just keeping up with the PR and the spec changes. Yeah, pretty much that. We also have support now for configurable network params, but I think it would be nice if we get like a finalized, finalized document or anything from Eats Panda Ops or.
00:34:46.609 - 00:34:55.073, Speaker H: Yeah, whatever they want to keep configurable. So yeah, that's pretty much up from Nimbus.
00:34:55.129 - 00:34:55.725, Speaker B: Yeah.
00:35:01.985 - 00:35:16.415, Speaker I: I think the general idea is that we keep everything that's in consensus pack as a spec configurable and if you don't want to be configurable, then it would be a preset. Yeah.
00:35:16.455 - 00:35:17.075, Speaker H: Okay.
00:35:20.535 - 00:35:36.715, Speaker I: Yeah, we probably need to go through by the way, each spec value whether we actually want that to be changeable in the future or not. I think there was some discussion about this in the interop event as well.
00:35:43.025 - 00:36:00.285, Speaker B: Yes. So I think it was there like some. I think we've moved some preset into configuration and is there any other configuration that we should move?
00:36:05.035 - 00:36:16.455, Speaker A: I think there's EIP activation. No fork epoch right now and then if we are changing that to peer DAS activation epoch, that would also be another change that we can.
00:36:20.275 - 00:36:36.185, Speaker B: I see. Yeah. Okay, we will have time to talk about definite one configuration. Thank you. Is there anyone from Low Star here?
00:36:37.645 - 00:36:42.545, Speaker A: One sec. I think Jimmy mentioned something in chat. You want to bring it up? Jimmy?
00:36:43.845 - 00:37:04.995, Speaker C: Yeah. So the max blobs per block is currently a preset. So in Lighthouse we can't. We can't configure this at runtime at the moment. So I was wondering if we were to bump this value, maybe we could think about putting into the config or there's Any thoughts on this?
00:37:09.735 - 00:37:25.835, Speaker A: I think the main issue with moving it to a config is that it also has to be changed on the EL side. I'm not sure if we have any other value that's in the config and arbitrarily changed by the CL that the EL doesn't know about.
00:37:27.775 - 00:37:33.115, Speaker I: What is the actual reason? The EL need to know how many blocks there are.
00:37:34.975 - 00:37:44.355, Speaker A: Gas estimation, otherwise you can't sync. Also it needs to know for building the block.
00:37:46.655 - 00:37:58.845, Speaker D: But like couldn't we tell the EL this is the current blob count that has been voted by all the validators same way we do for gas limit today and then it just uses that value.
00:37:59.505 - 00:38:01.285, Speaker I: Your MIC is very bad Nishan.
00:38:03.545 - 00:38:05.205, Speaker D: Did you hear it? No, no.
00:38:07.265 - 00:38:08.445, Speaker B: Still shaking.
00:38:12.785 - 00:38:15.165, Speaker A: I didn't hear you really well.
00:38:17.195 - 00:38:41.015, Speaker D: Okay, let me try again. So right now for transactions what happens is that you know we tell the EL that this is the gas limit that has been voted by the validator and you know if there's agreement we would bump up the gas limit. So could we do something like that for the blob count?
00:38:45.565 - 00:38:54.185, Speaker A: Does the CL actually tell the EL what the gas limit is today? I don't think. I mean you.
00:38:57.485 - 00:39:02.465, Speaker I: Okay, there are any EL devs here maybe they could try.
00:39:03.445 - 00:39:42.865, Speaker G: So we don't basically CL does not send any blob gas parameters in the fcu. But I think this is a change that has been proposed as well that the CL should send it so that EL doesn't need to hard fork each time when CL wants to raise the limit. So it makes probably make sense and maybe we should include it in the Electra itself that CL will send like how max gas limit is sent. Max blob gas limit will be sent and on the basis of that basically EL should be able to produce the block.
00:39:44.245 - 00:40:17.355, Speaker A: One of. So Dankrad and Anskar were thinking about this and I think they wanted to propose it and they probably are way ahead in their thought process at this point. But when we spoke about it, the issue was that it breaks sync on the EL side because the EL would then be syncing an old block and not know how many blobs it was supposed to have. So it wouldn't be able to validate if it estimated gas properly. So you would then have to somehow add it in the header or add it in the block body and that would be more changes that need to happen.
00:40:17.895 - 00:40:30.635, Speaker G: It is adding the header. I mean it couldn't just be part of the max limit, right? So EL should just check Max limit of the gas or I mean we don't actually need to check.
00:40:33.415 - 00:40:39.135, Speaker A: It still needs to know. Right. If the CL decides what the max limit is and the EL doesn't know.
00:40:39.295 - 00:40:43.435, Speaker G: Max limit is our max commitment limit, which is 4, 096.
00:40:45.895 - 00:40:57.115, Speaker A: Yeah. Then you change the paradigm to some sort of optimistic method where it just checks that it is something below 4096 but it doesn't know the exact value.
00:40:57.455 - 00:41:12.527, Speaker G: How does it. How, how. How is it different from checking against let's say six for now. Right. It doesn't actually matter. Once they have the state on which they can execute the block they will come to know. Or basically it.
00:41:12.527 - 00:41:22.235, Speaker G: Since it's basically a optimistic sync anyway, the parent. So parent is sort of validated in terms of hash. Right. Sorry.
00:41:24.455 - 00:41:53.685, Speaker A: Yeah, I mean I had an initial convo with Marik about it and I, I think the problem is that if you just knew it had to be some value below 4096, then I could get a block and I have to sort of verify it and all I can verify that the gas is right, assuming it's below 1496, which is different from how it works today. Because today I know that the max is six, so if I get a block with seven blobs in it, I know it's wrong.
00:41:53.765 - 00:42:08.755, Speaker G: But you verify against the transactions version hashes. Right. So it's not just the number of blocks or blob gas it used says in the header. And we have that in the header. Right, Blob gas used. So that is already there.
00:42:09.855 - 00:42:11.355, Speaker A: Yeah, fair enough.
00:42:19.255 - 00:42:26.245, Speaker I: Why can the CL just let the ear know in an engine API call what the target and the.
00:42:28.505 - 00:42:34.165, Speaker A: And that's the fact that would break sync or you would have to think about sync on the el.
00:42:38.825 - 00:43:07.835, Speaker G: I mean the problem is what PARI is mentioning is on the optimistic sync, when you basically get NFCU and then you sync back the chain and then you want to validate the data that you are getting in that chain for which you have not received any fcu. And anyway, the F. And anyway, the parameter that CL will be sending to EL will be in the block production fcu, not the normal FCU that it would send on that block import.
00:43:08.735 - 00:43:16.973, Speaker A: Yeah, and then the FCU triggers SnapSync. And I think the issue then is that SnapSync needs to take this into account.
00:43:17.052 - 00:43:17.365, Speaker E: Right.
00:43:17.444 - 00:43:21.455, Speaker G: SnapSync doesn't sync any blobs. Right. SnapSync only straight.
00:43:21.835 - 00:43:29.899, Speaker A: Yeah, but I would need to import the block to validate if it was valid or not. And if you don't validate the blob at all.
00:43:29.947 - 00:43:53.495, Speaker G: When for example you get a block and in a normal scenario when you get a block and you send it to over to the el, you're not sending in them the blobs, you are sending them to transactions which has version hashes, which is basically used to validate blob cast. So actually blobs are not synced by the EL once they are produced.
00:43:55.635 - 00:44:07.483, Speaker A: Yeah, that's fair. I'll try and get the argument we had and I can send it to you. But yeah, there was definitely something on this.
00:44:07.539 - 00:44:11.701, Speaker G: Yeah, you can start a thread on all code ads and we can talk about it.
00:44:11.853 - 00:44:23.745, Speaker A: Yeah, because I think Marek's preferred approach was to for example treat it like a difficulty bomb change if you want to simplify it or add it to the headers like one of the two approaches versus preferred option.
00:44:24.405 - 00:44:52.075, Speaker G: I mean it's not really required to make it more complex. I don't see the reason why we can't just have this upper limit and trust the data that is in the headers. Because if you for example get an FCU from the CL and you have to sync back the optimistic sync through optimistic sync, all the entire chain that you are downloading is valid because it is the parent child relationship that you are downloading and not downloading anything else. So if you are for example missing.
00:44:52.195 - 00:44:52.855, Speaker H: Your.
00:44:54.675 - 00:45:03.695, Speaker G: Canonicality on the basis of the block hash that you got in fcu, then all of the ancestral chain is sort of validated.
00:45:06.165 - 00:45:15.225, Speaker A: Yeah, fair enough. I'll. I think Dankra is also here, so I can just ask him if he wants to post stuff or just in general bring it up.
00:45:28.605 - 00:45:34.955, Speaker I: Do we actually want to change this for, by the way, 3 and 6?
00:45:36.615 - 00:45:44.915, Speaker G: So I would support basically cl fcuing the max blob gas for the block production.
00:45:49.415 - 00:45:54.599, Speaker I: And then max blocks per block would become a config variable instead of a preset.
00:45:54.767 - 00:47:25.105, Speaker G: So max blobs per block for the EL would then shift to 4096 gas. So they have max gas map max blob gas parameter which would basically now correspond to 4096 Times Blob Gas parameter. So they would configure that parameter to the new one and then the CL configuration can happen on the basis of whatever we decide whether we want it to be basically a time based rollout or increase of the gas or of the max blobs or through a hard fork or software kind of scenario. I mean that all that is fine. Just to echo the discussion that is going on in the Chat Yes, I think we should do it in Petra and not bump up the blob count from the CL side as well.
00:47:32.045 - 00:47:58.965, Speaker I: Regarding the what was the consensus on EIP 7594 fork version for CAPOC naming scheme? So how do we want to trigger peer desk? Actually I saw some pushback for actually using it as a fork and some pushback against doing it as a fork.
00:48:09.115 - 00:48:20.295, Speaker B: I am pro helping it with hard fork because easier and more clear. Just my opinion.
00:48:22.635 - 00:49:01.535, Speaker G: I also favorite it as having the hard folk because then the typing of the data that is referred in the code everywhere is sort of more clear and more easy and less messy. And I think the boilerplate code that would be required to add a new hard fork if it's not minimal it is it must actually be fixed across all the clients. So this is my opinion that it is cut and dry and should be just be a hard fork. But if teams choose it to be awesome peer activation epoch, obviously we'll make that happen as well.
00:49:03.325 - 00:49:11.905, Speaker A: Just a quick clarification, when you guys mean hard fork, do you mean a PR specific fork or do you want to just activate it on Spectra.
00:49:15.165 - 00:49:27.905, Speaker G: Fork on top? So it could so as a hard fork as a sort of which has basically its own for question and for keepoc but scheduled after Petra.
00:49:28.925 - 00:50:06.905, Speaker A: I think after Petra is going to be confusing. Maybe it makes sense to do denEB plus peer. I'm mainly making the argument because there's three streams and we haven't still decided what PETRA is. So EOF is also having a similar discussion right now on how they should activate it. If you decide that peer DAS needs to be activated on PETRA as well as Electra. Sorry, EOF needs to be activated on Petra and we don't know what PETRA is. I'm sorry but that's going to be an insanely huge cluster of mess that no one's going to be able to untangle.
00:50:06.905 - 00:50:25.985, Speaker A: It would be easier. All three start with Deneb as the base and then we do essentially three forks and then we can have three different lines and whenever we decide what's going into Petra finally we can merge stuff through. And that was kind of the advantage of having the PETRA for activation epoch approach.
00:50:27.595 - 00:50:28.735, Speaker I: But activation.
00:50:30.475 - 00:50:35.435, Speaker G: You mean for the devnet. So you mean when it will be activated in mainnet?
00:50:35.555 - 00:50:43.735, Speaker A: Right now I'm meaning for devnets but yeah, I have no particular thought on how it would be for later.
00:50:45.315 - 00:51:06.565, Speaker G: I think I'm okay with that devnet approach and mainnet will anyway come to decide when we'll see but is ready at what times? Timeline. But yeah, so if, if it's as a. As a fork happening, then basically we'll obviously decide whether Pector comes before or where a peer does come before. But I'm fine either way.
00:51:12.385 - 00:52:27.705, Speaker B: You mix everything all together all the time. But I think the top part is to figure out if we need to bump the blob count and the peer Desk upgrade altogether. In that case we need the hard work for the. I mean also to clean the EL part as well in the send upgrades. And including into picture is another topic here. I think it depends on the timeline and that's fine with forever for the definite, but just for the many half work, I think it makes more sense to combine them together. And sorry to try to stop here because we have still have many things on the agenda so I want to speed up a bit and so what's the action here? We want to use the activation epoch for the defnet.
00:52:31.885 - 00:52:34.465, Speaker I: I think different teams have different opinions on this.
00:52:35.655 - 00:52:43.595, Speaker A: Yeah, I think that's part of the problem. The argument keeps coming up and then we're always just moving between the two slides but not making a decision.
00:52:44.855 - 00:52:46.555, Speaker B: Yes. Oh.
00:52:51.535 - 00:52:53.727, Speaker I: The plan is now for the.
00:52:53.751 - 00:53:04.389, Speaker G: Devnets, can we just continue doing what we did for Devnet zero? I mean just have it as a fork and. Or has Lighthouse already moved to the.
00:53:04.477 - 00:53:19.985, Speaker A: New mechanism for DevNet Zero? We kept the same fork version as Deneb and then we had like. Yeah, we had a fork epoch which activated Petra, which in effect is Petra activation epoch.
00:53:20.765 - 00:53:35.885, Speaker I: No, no, he's talking about Peer desk and Peer Desk activation was the EIP 7594 for version and for epoch that we activated at like epoch one or two or whatever it was and we didn't activate picture at all.
00:53:37.945 - 00:53:40.765, Speaker A: Yeah, yeah, sorry, I was talking about peer desk, not P.
00:53:43.625 - 00:53:56.105, Speaker B: So is the question do we want to have Peer Desk definite combined with or not or separate? Definitely.
00:54:00.805 - 00:54:14.265, Speaker A: I would keep it separate for now because Spectra and Peer does just have completely different teams working on it and if we combine everything right now, we'll be blocked by other teams who haven't finished their stuff.
00:54:18.415 - 00:54:45.755, Speaker I: It's good, but then the idea is to keep everything the same. So we would have still EIP 7594 fork version and fork epoch. Keep it as a hard fork and let's leave everything as Devnet zero and just try to do Devnet one. And once Devnet one or Devnet whatever is stable, then we can come back to this and decide how we actually want to activate it.
00:54:49.975 - 00:55:27.735, Speaker B: Sounds good to me. At least we can keep moving and test. All good. Okay, I see some labs in the chat. Okay, thank you. So somehow we only have 10 minutes left for all this spec discussions. So I think it would give some time for the PR also to give a quick recap of all the PRs that we listed staff on Chopper.
00:55:32.315 - 00:56:22.913, Speaker E: Okay, so quickly the PR which is the second in the list is clarification of samplings and everything around sampling. So it's splitting description into simple selection and simple query and then the descriptions for both. And it contains a bunch of clarifications of how it is recommended to randomize and also function to have the to how you can get more samples and allow for some losses. So what we call OCDAs. I have most. I had a bunch of comments, most resolved. I still have a few from Pops Pop which are not resolved.
00:56:22.913 - 00:57:06.605, Speaker E: One is just function naming and whether the constants are going to the parameter or just used internally. I'm just waiting for POP on that to be happy with the solution. I hope and others. And the other one is more than a theoretical point. I was writing recommending sample selection without replacement because it's just easily implemented and just gives better performances. So if you just go and implement the spec, you get the better behavior. Um, but we had a small discussion on that and yeah, I'm just waiting for POP to agree or disagree with my last point.
00:57:07.985 - 00:57:18.125, Speaker J: Yeah, I think, I think the. I mean the function signature like the parameter of the. Of the function I think is good already. Yeah, I agree with that. Yeah.
00:57:21.595 - 00:58:04.695, Speaker E: Yeah. On the other one, you might need time so others want to chip in. It's really just so I'm recommending when I'm describing what's the random selection which is good to describe something in the specification so that if you implement the specification without syncing too much, you implement the light sync. I'm recommending uniform random selection without replacement as a recommendation. So if you have reasons to implement something else, you can implement something else. Yeah, I think it's not a big deal but Bob was saying maybe I should not recommend the without the placement part.
00:58:05.875 - 00:58:28.637, Speaker J: Yeah, I also think it's not a big deal. Like. Yeah, I think it's not a big deal. Like I just like my first thought is like I don't. I just don't understand why seduction with placement is like more preferable or like more recommended. Like I, I don't see much difference and that's all. But I don't have storm preference.
00:58:28.637 - 00:58:35.425, Speaker J: Like, yeah, it's okay it's okay for me to put the recommendation in the subject.
00:58:36.405 - 00:58:51.645, Speaker E: Yeah, it's just. So you have to choose something, either this or that, and slightly better the results. Replacement. So if you just implement the spec, you implement that and then you get slightly better performance. That's the only reasoning there.
00:58:54.745 - 00:59:14.999, Speaker J: I think. To be honest. I think selection with replacement, I think is. In my opinion, it's a lot easier to implement. You just select the column like from the same set like every time. So if you do. If you do select the column without replacement.
00:59:14.999 - 00:59:27.119, Speaker J: Right. You have to remember what you have chosen. And I think it's like it's harder to remain. But I think the one with replacement is simple in my opinion.
00:59:27.287 - 00:59:57.945, Speaker E: Yeah, I mean it's slightly simple, but the difference is a small. It's typically a primitive in the libraries. Or you just remember and close it out while you do a permutation. Yeah. So you can implement it easily. Anyway, I was putting it as a recommendation. So it's, you know, it just means that you can just forget it and do the other thing.
00:59:57.945 - 01:00:24.905, Speaker E: I just wanted to specify, it does give better probability sometimes slightly. Sometimes slightly more. It actually starts to matter when you are doing the incremental stuff. But that's not something I want to describe in the specification because that's more like a client strategy. Yeah, but I don't want to steal all the 10 minutes so we can continue it on the PR.
01:00:28.295 - 01:00:36.915, Speaker B: Yeah. Thank you. J. Next, Francesco would like to talk about the BlockChoice PR.
01:00:42.335 - 01:01:16.663, Speaker H: Hi. Yeah, so there's this PR that I just linked which does a few things. I mean, most of it or like some. Some of it is just the parameter changes that we sort of agreed on in the interop. I know, like it wasn't, you know, exactly agreed that we would do 128, but that's, I mean, up for discussion whether we want to do that instead of 64. I guess some people had the feeling that it makes sense to maybe try that out and see. I don't know if there is any reason not to do it because if we can do it, then we do get some kind of.
01:01:16.663 - 01:02:05.759, Speaker H: Well, we do get basically like a 2x better kind of efficiency. And the idea. The other thing is validator custody. There was some discussion with Justin around how exactly that should work. And I think this is still. I don't know, I think there's more to say about this. There's questions around like, I think particularly Ansgar, even though he didn't comment on the pr, but we've had some discussions if there's ways to introduce this assignment basically really being like in protocol, kind of like a public assignment so that there's some degree of accountability and whether or not it's possible to do this in a way that doesn't really conflict with the anonymity questions.
01:02:05.759 - 01:02:49.275, Speaker H: And so yeah, I think there's more to say about that. But I guess, yeah, the general. We still wanted to introduce the. The general concept of validator custody because we do think that, or at least I think that it's. It really de risks the whole transition to peer dos and just like really, you know, lets us use the fact that we don't actually have just a million validators but like some concentration of stake. And then there's the fourth choice part, which it's a few things. Like one is basically changing the way that on block that the basically that the availability checks work in on block in the spec, which is the block importing part.
01:02:49.275 - 01:03:41.901, Speaker H: Basically right now the way that that part works is that you just don't import something that is unavailable. Whereas here the only check that you would do there is after justification. So you only wouldn't import things that would justify an unavailable block and their unavailability is basically, you know, determining whether or not something is unavailable. There would just be through peer sampling. Otherwise the kind of normal kind of, you know, the live, let's say availability check is just a custody check. So you wouldn't do peer sampling. And yeah, again this is another thing that I think and we talked about this at the interrupt like it makes everything kind of simpler and more robust and lets us like delay by a lot the usage of peer sampling.
01:03:41.901 - 01:04:36.235, Speaker H: And I mean another small thing is like in the kind of in the fortress filtering part where you, you normally would just use custody, you also would use peer sampling, but only if something is from more than an epoch ago. And so again it's like quite loose. Like you should have plenty of time to be able to do peer sampling. And I mean if something, you know, if you really cannot satisfy peer sampling after a whole epoch of trying, then it seems like there's good reason probably not to follow that branch. I guess. Yeah. If anyone has questions about there's linked a couple of documents that one is like kind of summarizing we talked about at the interop and why certain things may make sense to do which is a lot of what it's in the pr and then yeah, the other document is more specifically about the four choice part.
01:04:36.235 - 01:05:40.175, Speaker H: Yeah, if anyone has questions we can talk about that and, but before that, one last thing was basically there was at some point the idea of trying to move to a fourth choice which is like block slot or some other thing which essentially would make it so that when people don't vote for a block, then the block is kind of discarded from the fourth choice like that you can essentially have the empty slot kind of win. And this makes certain attacks a bit basically. Yeah, it deals with certain reorg attacks a bit more easily. But I think that it's probably not worth the complexity in practice to do. To do this right now. I think basically these attacks are not that much different from a proposer dos kind of situation that we already have today. Like in theory, you know, this could happen and we kind of rely on well, two things.
01:05:40.175 - 01:06:09.233, Speaker H: Like one is just, you know, proposers like at least like bigger proposers protecting themselves. And then also in principle, if it really became a problem, we could have measures in place. And yeah, here we. It would even be simpler than, you know, what we would do with proposer dos. Like we don't need the whole SSLE machinery. We could do this for choice change which is much smaller. And so yeah, I think basically I would leave.
01:06:09.233 - 01:06:58.035, Speaker H: I would tend to leave things as they are from that perspective and kind of don't embark on this complexity of trying to change the for choice in that way and try to have a backup scheme and things like that. But yeah, open to. I mean I, I'm not sure if this is like something that people have opinions about because it is a bit deep in the, you know, the whole realm of fortress attacks, but it's not that complicated. So if you do have time, I think it would be nice for other people to. To try to like familiarize themselves a bit with this specific attack vector and like, you know, try to have an opinion on whether or not you think it's okay to just do nothing because yeah, I think that would be. That's my preference now. But like it would be good to have more confirmation on like what we think about this.
01:06:58.035 - 01:07:08.835, Speaker H: And again this is discussed in the PR and in the docs. Yeah. Now if you have any questions or whatever, let's. Yeah. Open up the floor.
01:07:11.175 - 01:07:29.155, Speaker B: Thank you Francesco. On the Nexus menu and the ENR record yard, just quick note, I think that's ready and just want to see if any objection. If not then I will go merge it.
01:07:31.775 - 01:08:41.533, Speaker F: Yep. So on peer das we introduce a new field in the Ethereum node record. This field is named Custodian Subnet counts and this is a key and the value is SSD uint64 As you may know there is a 300 bytes maximum for the Ethereum non record. And so what I propose to do is just to reduce the key from custodian subnetscount to CSC which saves 19 bytes and to move from for the value to move from the SSD UN 64 to just the big Indian integer which can save if we use strictly less than 128subnets count 7 bytes. And yes, I think we will use exactly 128subnets, so we will save 6 bytes. So in total we could save 25 bytes. And about the big integer is just using the same things than we have today with the UDP port of the TCP port for example.
01:08:41.533 - 01:09:04.255, Speaker F: Because yes, right now there is not at all SSD encoded stuff in the ER and by the way, ENR is already RLP encoded so there is really no reason to both encode with RLP and with ssd. That's pretty it. And so in total we save about 8% of the maximum size of the node record.
01:09:06.595 - 01:09:07.575, Speaker E: That's all.
01:09:09.755 - 01:09:22.494, Speaker B: Thank you. And next is pub and the passive sampling. Would you like to give an update?
01:09:24.434 - 01:10:15.945, Speaker J: Yeah, yeah, I noticed that when you do sampling right you. I think we don't usually know like when to samples like we usually know like because if we sample like too early we will not get a sample right. If we sample like too late, oh, we lose some time, like waste some time. So I think I have an idea of something called passive sampling. Instead of just using the request list font to get the sample, you just join the subnet just a moment before the slot and then you just passively receive the sample. So by doing this you get the sample as soon as possible. Yeah, it's like if.
01:10:15.945 - 01:10:19.465, Speaker J: Yeah, that's it, that's called passive something.
01:10:27.285 - 01:10:34.145, Speaker D: Is this the same as introducing the new control message observe in gossip sub?
01:10:36.045 - 01:11:19.845, Speaker J: Yeah, actually, yeah, I think actually you can do passive sampling without the public observation that is an update to the sub that I just proposed. It's just a different story. But with the topic of observation you can do passive sampling even better in terms of the bandwidth consumption. What you can do is just like instead of joining, joining the the subnet like directly, you just observe the topic. Because, because when you observe the topic you, you get only the message ID of. Of the message instead of the actual message. So you, you.
01:11:19.845 - 01:11:31.585, Speaker J: You will get no notified when, when there is a new message but you don't actually have to download the actual message. So. So yeah, yeah, that is benefit of topic observation.
01:11:34.445 - 01:12:30.373, Speaker E: So yeah, from my point of view, I think that there's three different ways of interaction possible here and then you can implement them as try to implement them as part of gossip sub or as part of the request response. So the one is, the first one is I ask for the column and you give it. If you have it, you tell me you don't have it if you don't have it. The second one is I ask for the column, I tell you that I'm interested in the column and then after some time when you get it, you send it. And the third one is what you say here, I think which is I tell you that I'm interested when you have it, you tell me that you have it and then I tell you, okay, now give it to me and then you give it. That is three different ways of getting a column at the end of the day. And we are a bit jumping back and forth between these three ways of doing it.
01:12:30.373 - 01:12:56.965, Speaker E: And then we are also discussing a little bit what kind of networking primitives to use that to use a new lib p2p gossip sub primitive which is the observe or to use a request response interpreted as Once you have it, send it to me. Honestly, I don't know which one is the better and I think it also depends on, on the sizes of the columns, which one is better or worse.
01:12:58.945 - 01:13:24.445, Speaker J: I think, I think like the second option, send the column to me when you have it I think is actually joining the subnet, right? It's like how is it different from joining the subnet? Like because when you join the subnet you actually tell your peers that okay, when you have the message, you just send them send me the message, right? It's like I think that is actually the subnet subscription.
01:13:24.945 - 01:14:14.215, Speaker E: Yeah, the joining the subnets comes with a bunch of limitations like the match degree and so on and so forth. So that's why it's kind of similar from the message point of view, but then you have lots of side effects there. So that's why one way is to use the request response and I send you the request and the semantics of that is send it when you have it and then freeing yourself from the gossip sub considerations. But I agree it can be mapped to the gossip sub if you modify the gossip sub interpretation. It's just, you know, I don't know which one networking PPTV is the best and I don't know which one interaction is the best. Just trying to about the options.
01:14:18.435 - 01:14:38.725, Speaker D: If you do use it as part of gossip Sub there are a few issues because from what I understood in the PR you'd be using a different mesh size than what we have now. So there is no way to you know like differentiate a normal peer that's going to, you know, forward the message was this special pair that's just trying to observe the message.
01:14:40.305 - 01:14:46.977, Speaker E: Yeah, so you wouldn't even need to modify Gossip sub to put this differentiation into it basically. I think.
01:14:47.081 - 01:14:50.405, Speaker D: Yeah, that'd be pretty non trivial from what I've seen.
01:14:56.985 - 01:16:38.975, Speaker B: Okay guys, we see of the meeting time. Before we wrap up this call could we go through what are the to do actions or like any very important thing that we should discuss today instead of the third stakehold by Thursday? I mean the acdc. No. Okay, so I think we could definitely talk about the blob max configuration and setting in and how it's in the el, the third state code. That could be a good topic to be there. And, and the second action this here is to go through the program preset and configuration files to check if anything that should be in the configuration or another side and anything else. And I make a small poll in the, in the chat to see if people want to make it a weekly call or bi weekly and so far we get more votes on the weekly.
01:16:38.975 - 01:17:01.335, Speaker B: I think it's because it's the first goal and we have too much, too many things to discuss and running out of the time and so. Okay, mobiles are coming. Mobiles are coming. Okay, interesting. So Jimmy please.
01:17:01.955 - 01:17:55.215, Speaker C: Yeah, I actually want to talk about like maybe you can talk about this Async but on Discord we've started having a conversation on what, what we can do for the proposers that are doing local block building because like if we move to peer DAs and we increase the blob count then it's going to significantly increase the upload when the proposer propose a block and if they use local block building then that means they will need to have like 32 to 64 megabytes of upload in one go. So I think we talked about approaches to how we can maintain home stakers feasibility. So maybe we can just continue this discussion. Async or unless anyone want to talk about this now.
01:18:05.155 - 01:18:09.055, Speaker D: Is the argument that it's unfeasible right now or in the future?
01:18:11.285 - 01:19:02.555, Speaker C: It's unfeasible if we increase the blob count to. I mean at the moment I think with the current blob size it's okay, but if we try to increase it to something like 32 megabytes I don't think it has to be 32, but once you increase the blob count it will basically make it like hard for home stakers to publish the block and blobs timely. And I think there's proposals on allowing a CL flag for the validator to provide a max blob count on what they can, what they can handle. But I feel like this might actually limit our scaling, so would be good if we can find alternative approaches.
01:19:04.255 - 01:20:05.135, Speaker E: So from the networking point of view, we had several lighting on the discussion in our simulators since quite a long time. We are using a modification to gossip sub in which we are sending out the data in a way that we are sending starting by sending out one copy of everything first and that reduces. So that means that you don't have to necessarily count with the gossip submultiplier it can still be quite some data to send out a full block or an extended block, but at least you don't have to count with an eight times multiplier on the bandwidth requirement. And I was promising to have a light upon that, which I don't have yet, but it's something we have used in our simulators since last year.
01:20:24.125 - 01:20:29.105, Speaker B: Kev, do you want to talk about the idea you mentioned in the chat?
01:20:32.925 - 01:20:34.845, Speaker F: Yeah, it was just that we could.
01:20:34.965 - 01:20:36.837, Speaker I: Send it to relayers who are probably.
01:20:36.901 - 01:20:40.505, Speaker F: Already doing it already for block builders.
01:20:42.255 - 01:20:45.703, Speaker I: Or altruistic nodes, like maybe the super.
01:20:45.759 - 01:20:47.111, Speaker E: Nodes that we have in the network.
01:20:47.223 - 01:20:50.315, Speaker I: Who will just propagate the blobs.
01:20:54.895 - 01:21:01.955, Speaker D: But would you need to statically pair with those nodes? Because how would you know, right, that these guys would send the whole block out?
01:21:03.175 - 01:21:04.615, Speaker F: Yeah, they might have to have like.
01:21:04.655 - 01:21:09.155, Speaker I: A flag that says, you know, I have a lot of bandwidth or something like this.
01:21:10.415 - 01:21:11.995, Speaker H: I don't know how we're going to.
01:21:12.575 - 01:21:14.983, Speaker I: Identify super nodes on the network either way.
01:21:15.039 - 01:21:17.435, Speaker F: I think they just look like regular nodes.
01:21:21.255 - 01:21:28.475, Speaker D: We could because if we know their custody count we can figure out that they'd be hosting a lot of data.
01:21:32.495 - 01:21:33.235, Speaker E: Great.
01:21:33.975 - 01:21:34.327, Speaker H: Yeah.
01:21:34.351 - 01:21:37.915, Speaker I: So that's sort of one a possible mitigation for this.
01:21:40.775 - 01:21:52.855, Speaker C: But that means if we have 32 blobs we still have to send out like how many like 8 megabytes of blobs to that super nodes.
01:21:52.895 - 01:21:53.475, Speaker G: Right?
01:21:54.335 - 01:21:55.115, Speaker F: Yeah.
01:21:57.975 - 01:22:53.371, Speaker E: Yeah. So that was in the depth proposal. There was one suppose not to need to send out which is building the block also in other places kind of distributed building. But otherwise you have to send out at least one copy of everything and the we can try to identify places which are kind of superseded the network so that you just send one copy to them and then they are threading it all around. I'm not sure that that's the most reliable solution. So having all these kind of network optimizations in also come with dead risks. And I'm just not sure that's the right choice.
01:22:53.371 - 01:23:43.545, Speaker E: But it can be done like send a first copy to someone like that and then if you have to send out a second copy, then send it to send it randomly. You can have these kind of mechanisms. So you're mostly looking at the 2D case and the 2D case the eligible code is very good in you are just sending out one piece of random copies. And even if things are lost, things are also recovered very well with the on the fly reconstruction that we have in the, and the 1D case. So in here in Peeldes, if sinks are not going out because you only have the bandwidth to send out one copy and then they are stalling, then sinks will be stalling until there is a node which can reconstruct and then seed other topics.
01:23:48.935 - 01:23:52.975, Speaker F: And the issue is the timing, right? Like if I, if local block builders.
01:23:53.015 - 01:23:57.735, Speaker I: Let'S say, had like 10 seconds to send out all of the blobs, that.
01:23:57.895 - 01:23:58.959, Speaker H: That would sort of.
01:23:59.087 - 01:24:03.311, Speaker I: It would no longer be an issue. Or is the issue just the total.
01:24:03.383 - 01:24:07.247, Speaker F: Number of bikes that need to be sent out regardless?
01:24:07.431 - 01:25:18.285, Speaker E: So the, the issue is. So with normal gossips of behavior, you typically start sending out and then you are sending it to eight copies of the first column and then eight copies of the second column. So you end up doing an uneven sending out if you don't have enough bandwidth, if. So the thing starts from if you don't have enough bandwidth to send it out in time, then you will have this ordering in what you are sending out. Because you are sending out to to eight nodes one piece and you are sending out another piece to no nodes. So the columns are not treated equal in that sense and balancing that so that you are putting nodes in the network in the position of doing the recovery faster in the timeline, basically. So that's what you achieve by sending out one copy of everything first of each column you are sending out one copy.
01:25:18.285 - 01:26:21.675, Speaker E: So the amount of data that you are sending out is basically twice the block size. Then depending on your bandwidth, that means something. Depending on how many megabytes and how many megabytes per second you have uplink, you took a few seconds to send that out, but at least you are not taking eight times more to arrive to a position where nodes can start reconstruction. If Problems happen. The problematic way would be I send out the first column, eight copies of that and I'm queuing up the second column to others and I'm queuing up the third column and it gives and ordering to the columns. So that's why either you are spreading value send and you are making sure the first copy is going out of everything relatively early so that the network can do its job. I mean the peer to peer distribution can do its job on every single column.
01:26:53.415 - 01:27:14.591, Speaker B: Finish and J and do we have any final point before we close today? Thank you so much. Go ahead.
01:27:14.743 - 01:27:45.725, Speaker E: But I, I started a PL on the, on the reconstruction and seeding so it's related to I think what Nishant is already implementing and what had been discussed in the interop. I was trying to write down spec for that but I'm not sure it's already major. I think it's a first version for the next core.
01:27:46.755 - 01:29:24.465, Speaker B: Thanks. Right, so I think we would for the peer data asset it's in a highly iteration stage so I think we would merge PRs more faster than the PETRA contents. So I hope the client teams can also help Signal like which PRs that you think we should include and which shouldn't on the GitHub otherwise I know like Java open the PR for a long time and they but we will still don't have that competency to merge it as soon as possible. So just hope to see more you know, thumb ups approvals, thumb down on the PRs and any other things for the call today. So I went back to see the boat so it turns out we still get more, we'll get more votes of the bi weekly code. So the next code will be likely will likely be scheduled on the 25th. June 25th.
01:29:24.465 - 01:29:41.615, Speaker B: Okay. Okay, so let's talk about it on Thursday and two weeks later. Thank you. The video will be published on this call. Thank you.
01:29:44.435 - 01:29:48.195, Speaker D: Thank you, thank you, thank you.
