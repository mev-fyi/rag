00:00:00.410 - 00:00:36.646, Speaker A: Wait. Okay. Welcome to four four, call number 17. So we didn't have one last week for Denver, so it probably makes sense to spend most of this just sharing any updates, any open questions that people are looking at. There wasn't anything specific added on the agenda. And why do we start? The calls at half past gives me an extra half hour of sleep in the morning. You know, Marius, it's nice I have.
00:00:36.668 - 00:00:43.990, Speaker B: A call that ends right when this starts. So some people have weird time induced calls.
00:00:44.150 - 00:01:07.650, Speaker A: Yeah. Okay. On the spec side, I know there's been a lot of talks. It's kind of specs and implementation now about handling decoupled blobs. So that seems like something that's worth going over. And then beyond that, general client updates. But if there's anything else people want to cover, can post on the agenda or in the chat.
00:01:07.650 - 00:01:35.020, Speaker A: But maybe to kick off, does anyone want to give a quick update on decoupled blobs? And I think. Gajinder, are you on the call? Yes. You had the pr up last Friday. Oh sir. Okay. That's just updated the EIP basically to match free to blobs, right?
00:01:35.550 - 00:02:48.180, Speaker C: Yes. So I have the pr up and for Loadstar I have patched up the entire flow and I'm trying to make sure that I can run it locally so I'm able to produce the blob, gossip it, sign it. So one confusion that I have right now is what is the exact signing flow that should be there? Because there is a discussion going on, on the signing flow pr on the beacon APIs that whether the flow should be sticky or not in terms of for a particular proposal when a validator is proposing. So in the current flow we have signing of the blinded blobsight car which sort of binds the validator to a particular beacon ordin for a given proposal. And so the confusion is whether we are going with that particular flow or we are thinking of signing a full blob sitecar which basically will enable validator to not to be binded to a particular beacon object.
00:02:55.290 - 00:02:58.870, Speaker A: Does anyone have thoughts on the signing flow?
00:03:03.930 - 00:04:53.530, Speaker D: Yes, we introduced the blinded flow for supporting builder flow actually. And then we realized that there is a nice side effect that we can reduce traffic between VC and VN by not sending an entire execution payload. And now this applies also to blobs. But the side effect of that is that when you don't send everything to the VC, then the VC needs to hit the same VN that has produced everything which then in big deployments where you want to have maybe failover capabilities you have that prevented in a full builder flow where the block has been fully prepared by an external relayer doesn't really applies because then multiple BN will then end up interacting with the same relayer. But there are some weird situation where the BN decided to fall back on the local execution layer built block and then you continuing the blinded flow and you are forced in any case to end up with the same beacon node because this is the only one that has the local produce blocks. So TLDR is that do we want to have the opportunity to remove stickiness between VC and Vn or we simply give up there, we don't care. And in the block production flow we always assume that VC talks every time with the same VN.
00:05:12.230 - 00:05:27.640, Speaker A: I guess. Does anyone have any other thoughts on this now? Or does it make sense to just continue that conversation on the PR directly? It seems like it was recently active there as well.
00:05:31.050 - 00:06:28.378, Speaker E: I mean, the only thing I would say is like Enrico sort of mentioned this, but his problem doesn't really exist if you are outsourcing the builder, it only exists in a fallback scenario there. And since most people are using an outsourced builder and long term, if we plan to have a separation of block building versus production, then maybe it makes sense. Maybe it's less of an issue to go with the blinded by default. But that's not to say I feel super strongly about that because I do like that. Validators, some of them currently don't outsource block building. So I don't know, I wouldn't want to punish that.
00:06:28.544 - 00:06:56.580, Speaker B: Well, this doesn't preclude not outsourcing block building. This precludes if you don't outsource block building and you want to do load balancing failover on other beacon nodes, which that's one, a small subset. And two, it's not like a crazy requirement to have a beacon node that is producing the block be the one to broadcast the signatures. But I get that we're cutting off that design if we do this.
00:06:59.930 - 00:07:14.060, Speaker E: Yeah, I mean, I would be interested to hear about any existing load balancing things that might have issues with this. Would this be an issue with a testing or whatever? Vouch, stuff like that?
00:07:16.510 - 00:07:21.820, Speaker B: Yeah, I think vouch is probably the primary design here that is using this.
00:07:31.370 - 00:07:35.960, Speaker A: Okay. And I guess, can we just get feedback from vouch on this?
00:07:42.460 - 00:07:49.550, Speaker B: Yeah, we should at least make Jim aware of it because I know he didn't want to have the non blinded blocks as well in the past.
00:07:50.160 - 00:08:01.200, Speaker C: Okay, so for all intents and purposes, I'm assuming that we are going with the blinded sidecar signing flow.
00:08:08.500 - 00:08:10.592, Speaker D: Yeah, I think this makes sense to me.
00:08:10.726 - 00:08:36.410, Speaker E: Yeah. Another question I had on that pr was whether we want to like when we publish these signed blobs to the beacon node, do we want to publish them individually or as like a batch? I currently had it as individual productions just because you can broadcast right away, but I'm not sure if anyone has opinions on this.
00:08:39.100 - 00:08:39.620, Speaker C: Currently.
00:08:39.710 - 00:08:40.990, Speaker B: The counter would be.
00:08:43.200 - 00:08:49.390, Speaker C: Sorry. Currently I'm implementing as individual, so I think individual is just fine.
00:08:55.690 - 00:09:33.800, Speaker B: Individual is probably fine. I think the only thing that you gain by keeping them coupled here in this API is avoiding having to think about any sort of race conditions between the various things. For example, if somebody wants to code that they're not broadcasting x unless they have y already, you don't have that issue. Similarly, you might want your vc, if there are some sort of reputation issues you're worried about, to only sign blobs that you have the block for. And so kind of like having these things coupled might make sense, but I don't feel very strongly here.
00:09:34.250 - 00:10:34.310, Speaker D: Yeah, another thing that I see here is that we do post four blobs. We post the block, and when we post the block, the API return code 200 actually means that the beacon node has successfully imported the block, which also means that we should do everything in parallel. The block publishing to respect that, needs to wait all the other blobs to be sent and then the block import actually makes through and can verify everything with the blobs and then import the block. It's kind of adding some extra interaction with these three five potentially post to the bigger node.
00:10:36.570 - 00:11:03.490, Speaker E: Yeah, publishing them individually means we sort of have to change the meaning of like the 202 versus 204 error code or sorry response codes for import. Because 202 means like publish to gossip without importing to the beacon chain. And now we're essentially doing that for everything until we get the fifth message. So that's like a quirk with publishing them individually.
00:11:07.070 - 00:11:36.370, Speaker C: So the way Lordstar is implementing it is that the validator basically makes sure that it has the block and the blobs before signing, but once it publishes it in parallel, beaconode really does not wait to make sure that data availability checks passes while gossiping, but it sort of makes sure that all the block and blobs are available before importing it locally.
00:11:46.040 - 00:12:09.800, Speaker F: I guess one thing to clarify is that are we just requesting lined blob sidecar to sign? If we are doing that, then we probably want to also change or update the consensus back to specify that. Right, because the consensus spec for the honest validator still says that we're signing the full blob sidecar.
00:12:12.480 - 00:12:53.930, Speaker B: I don't think I would actually elevate that into the consensus spec. I mean, validator doesn't know anything about outsourced building. The validator spec doesn't know anything about the separation of beacon node and validator client. And by signing a blinded object, you're also signing the outer object. And anytime a validator is signing a blinded object, you need to make sure that there's adequate data there that it can do its safety checks. Like for example the slot for any flashball message. But it seems like a leakage of the API and practical engineering considerations into the validator spec if we go that way.
00:12:54.460 - 00:12:56.170, Speaker F: Yeah, that makes sense.
00:13:13.750 - 00:13:22.660, Speaker B: Do we have enough to try to hammer out this issue, this PR before Thursday or do we need to chat about it more here?
00:13:26.250 - 00:13:40.060, Speaker E: It sounds like we can keep it how it is. I guess. The one thing that I think is still weird is the response codes, but I think we can figure that out on the PR probably.
00:13:41.150 - 00:14:16.454, Speaker B: Okay, what would you say? So just real quick, the gain for doing split is that you can immediately start broadcasting when you get, and then when you get the blobs in the beginning node. That's the main reason that we want to do this assigned sitecar. But if we're leaning into the blinded, we're not actually transferring that much data between the two. So I'm not sure if that's like an appreciable gain, but nonetheless I'm on.
00:14:16.492 - 00:14:20.886, Speaker E: Board with combining them for the sake of simplicity. If other people are also cool with.
00:14:20.908 - 00:14:21.480, Speaker A: That.
00:14:24.570 - 00:14:26.680, Speaker C: I think combining is a good idea.
00:14:29.710 - 00:14:40.620, Speaker B: Yeah, I guess I'm having trouble seeing what the complexity we're buying. Like what we're buying with the complexity of separation, especially if they're blinded, but I'm not deep in it.
00:14:41.810 - 00:14:51.630, Speaker D: Yeah, I think as well that having them combined, if we go to the blinded, definitely simplify the handling of the API.
00:14:55.380 - 00:14:58.390, Speaker E: Okay, I can update that as a pr then.
00:15:05.080 - 00:15:24.740, Speaker B: Okay, I guess otherwise, let's take it to your pr after you do the changes and try to get this done for either final discussion on Thursday or just merge by Thursday. Thanks for keeping that moving, Sean.
00:15:27.710 - 00:15:37.230, Speaker A: Okay, next up, Roberto, you had a comment about SSZ, so there's been a couple breakout calls before we move to SSZ.
00:15:38.130 - 00:16:31.710, Speaker B: I'm just wondering if there's any other kind of complexity like this around the decoupling. That's worth discussing. I know it seems like not but I'm just seeing if there's any other design stuff on decoupling that we're kind of lagging on. I think there's one thing that I've seen discussed in some chats around local heuristics on when you start requesting missing information. So say I got the block, which you might get the block before you get any blobs. Do you immediately request? Do you wait some portions of the slot before you request because you expect in the normal case to just get them on gossip and not need to do direct requests. So I think there's this design consideration of which heuristics you use, under which conditions and at which points into the slot time to begin doing requests.
00:16:31.710 - 00:16:53.720, Speaker B: I got a baby yelling behind me and so I don't think there's a right answer here, but it's just something for people to be considering. Maybe there's some other thoughts here, but that's the main other point of design that brought up.
00:16:54.570 - 00:17:35.540, Speaker F: But I also echo that that's also not like a blocker for the next Devnet. That's something that it will probably take us a while to figure out what's the perfect balance in terms of when to request versus when to wait. So, yeah, I think in the initial version we're just going to wait patiently from the gossip hoping we get it. And then if we don't get it, maybe just request once at the two second mark and just be done with it. But, yeah, definitely for the final product, it will be a lot more helpful than that.
00:17:37.590 - 00:17:46.790, Speaker B: Yeah. And this isn't something that would make it into the spec. These are like local design decisions. So I agree with you, Terrence. This isn't a blocker.
00:17:56.810 - 00:17:59.266, Speaker A: Cool. Does that make sense, Jesse?
00:17:59.458 - 00:18:26.500, Speaker B: Yeah, I was just going to say. So it sounds like there's like guidance of waiting patiently and then getting requesting at the two second mark. Do we want to communicate that out to anyone now, at least for this initial phase? I think everyone kind of has this as a known design consideration and that they're going to tune it over time. But I don't know where else we might want to write that down.
00:18:33.810 - 00:18:45.620, Speaker A: And there is like the Telegram group on this as well. Feels like conversation is happening about it.
00:18:49.540 - 00:18:50.610, Speaker B: Sounds good.
00:18:51.220 - 00:18:56.740, Speaker A: Okay. Anything else before SSD?
00:18:59.480 - 00:19:19.710, Speaker C: Yeah, on the yield side for the transaction network container. So this is the pr that sorts of updates the aggregate proof to proofs if someone wants to give it a review.
00:19:23.680 - 00:19:27.470, Speaker A: Okay, so that's PR 6610.
00:19:28.640 - 00:19:29.390, Speaker C: Yes.
00:19:34.090 - 00:19:41.290, Speaker A: Cool. We have some thumbs up. Anything else on the blobs.
00:19:46.070 - 00:19:49.460, Speaker G: On the crypto library. Maybe that is of interest.
00:19:49.850 - 00:19:51.320, Speaker A: Sure. Yes please.
00:19:52.010 - 00:20:43.910, Speaker G: So I think right now most bindings, I think all of the bindings support the new KCG verification interface. We are on the process of polishing most bindings. I think there is very few remaining that haven't been, that are not fully there. I think node needs a bit more tweaks. An interesting case is that the NIM bindings used to live outside of the KCG library for some reason, but now we're pulling them inside the library as well. So both to get equal treatment in terms of polishing, but also so that they are integrated into the audit scope.
00:20:45.850 - 00:20:46.600, Speaker A: And.
00:20:48.890 - 00:21:56.654, Speaker G: Right now there is another pr that slightly changes the cryptography interface on the consensus. Very, it's kind of minor, it just adds a few more helpings into the computation of KCG proof, so it makes it easier basically for consumers of the API. And we want to update the KZG library for this new interface. And after that we hope that we're going to start to ossify the code base and just do polishing improvements and adding test vectors. We have an audit in a month, so we hope that over the next two weeks we can kind of finish all the major changes and then for the last two weeks kind of freeze the whole code base. So that's what's going on. There is still some work to be done on the binding, so if you work for a client team, there might be some more work to be done.
00:21:56.654 - 00:22:00.080, Speaker G: But things are moving smoothly, I think.
00:22:01.010 - 00:22:05.550, Speaker A: Nice. Any other thoughts on the library?
00:22:09.300 - 00:22:15.350, Speaker F: I'm curious, are the goal bindings ready yet? Is anyone working on that?
00:22:19.400 - 00:23:06.740, Speaker G: So Kev is working? Okay, when you say go bindings, there is two ways to interpret this. Either you mean the go library that is written based on GNarC, or you mean the CKZG go bindings. I think both go client, both prism and Geth are going to use the GNarC based library that Kev is writing. And I think it has been updated to the new API as of a few days ago. But I think Kev is not here to confirm this. But I'm pretty sure that it supports the new API. But maybe you want to ask on the KZG channel just to be sure about this.
00:23:06.890 - 00:23:08.804, Speaker A: Got it, thanks. Hello?
00:23:09.002 - 00:23:09.860, Speaker H: Yeah, I'm here.
00:23:09.930 - 00:23:11.524, Speaker A: I don't know if you can hear me.
00:23:11.722 - 00:23:12.820, Speaker G: Hello Kev?
00:23:13.640 - 00:23:33.390, Speaker H: Hey, yeah, so it supports the new API, not the one that's just about to be merged? Yeah, sorry, I'm on my iPhone, not the one that's just about to be merged, which returns multiple things from compute. KCG proof. But that's sort of a trivial change that we can push through.
00:23:47.990 - 00:23:51.490, Speaker A: There's a question by Alexi about SPDX headers.
00:23:56.420 - 00:24:01.520, Speaker G: I don't know what SPDX is, if that's a question for the library.
00:24:02.580 - 00:24:50.596, Speaker A: No, no, it's just a general question about CKZG. It's like common practice to include a line or two about license for the library. SPDX license is likely a shortcut for such licenses. I'm not sure I understand the question. Sorry. Does anyone? Yeah, let me rephrase. I mean, CKZG library code does not include license information in source codes.
00:24:50.596 - 00:25:07.590, Speaker A: Right. Okay. And so an SPDX header adds the license information. Is that. Yeah, it's two line comment which includes this information about what is.
00:25:12.900 - 00:25:35.250, Speaker G: I mean, if CKCG doesn't have a software license, I don't remember it now. We should probably add something. I'm not sure if there's a license in the repo. Okay, so SPDX is some sort of metadata to expose that somewhere or something.
00:25:36.660 - 00:26:46.198, Speaker A: Yeah, I can post an example for later. Okay, so there's no objections to adding this, it seems. Okay. Anything else on the library? Okay, so next up, Roberto, you had a comment around SSZ, so there's been a couple of calls already. We haven't quite landed on a final, I believe design for SSZ and whether or not we want to do more than just the 4844 changes as part of Cancun. It seems like there's some appetite by clientevs to do that. But, yeah, I don't know if.
00:26:46.198 - 00:26:51.100, Speaker A: Is there anyone here with a strong opinion about SSD on the El side?
00:26:54.750 - 00:27:03.046, Speaker I: Yeah, I don't know how many people have been following along. So we did a temperature check, I think, early on on whether we wanted to couple this with four.
00:27:03.088 - 00:27:03.614, Speaker A: Four. Four.
00:27:03.652 - 00:27:17.780, Speaker I: And there was kind of, I guess, some agreement that, yeah, we probably should because we're introducing a new transaction type and if we don't couple it now, we're just going to introduce more technical debt. Hold on a second. My alarm is going off for some reason.
00:27:19.190 - 00:27:20.900, Speaker A: I was wondering what that was.
00:27:21.830 - 00:27:53.280, Speaker I: Yeah, but I think it's been about a month since that last temperature check and we've been working out the details of what that change might look like. I personally have been growing a bit worried because the complexity seems to keep exploding. And I don't know, the eips are pretty large and pretty messy and I'm not sure they're done yet. So I don't know if anyone else has been watching them. But yeah, I just wanted to sort of get people's feeling for that.
00:28:05.250 - 00:29:11.970, Speaker A: So I guess my feeling is I wouldn't change in terms of looking at the next devnets and implementations. I wouldn't want to make significant changes to the transaction type unless we agreed to a decision for SSD before. Right. So it probably makes sense to treat 4844 as like moving forward independently and then if we do agree to make a larger SSD overhaul in Cancun, that might affect port four four. But I assume it should be like a relatively contained change if we're just changing sort of the structure of the transaction. But I think trying to keep both efforts in sync is probably the highest overhead and least valuable thing. Even if we decide to move to SSD and the four four four implementations are a bit behind, we can then just take time to bring this up to speed.
00:29:11.970 - 00:29:28.950, Speaker A: Whereas it's like if the SSD sort of proposal changes two or three times and we have to implement it three times, that seems like the worst outcome. And I don't know that we have consensus today on what we want to do more broadly with regards to SSD.
00:29:31.230 - 00:29:45.280, Speaker I: Yeah, I think that's fair. It's definitely true there's not consensus over the solution. Right. We have the kind of single transaction version and then the multiple transaction version. I think it's still open over which one we'd go with.
00:29:49.170 - 00:29:57.490, Speaker A: I don't know if anyone disagrees with that, but yeah, I think we probably need a couple more weeks to settle the SSD discussion.
00:29:58.470 - 00:29:59.218, Speaker I: Yeah.
00:29:59.384 - 00:29:59.714, Speaker A: Okay.
00:29:59.752 - 00:30:16.246, Speaker I: I think that's fair. So just keep things decoupled for now, and if we decide to couple them, we'll pair it at that point. But anyway, if folks haven't taken a look at that, I think it's worth forming an opinion on.
00:30:16.348 - 00:30:43.280, Speaker A: Yeah, so the type transaction channel is where it's happening in the discord. And Ethan has a couple sort of documents that he shared throughout that, that show the different structure. And I don't know that he has run the. Oh yes, so he has run some benchmarks on the various designs. I just linked them here.
00:30:45.810 - 00:30:52.754, Speaker I: Yeah, he does a wonderful job. The docs he's created are top notch, so they're ready to be looked and.
00:30:52.872 - 00:31:04.870, Speaker A: Okay, I'll reach out to Ethan after this call and I think this is probably sufficient to make a decision on the next Alcor devs, but I'll check with him if there's anything that might be missing.
00:31:05.870 - 00:31:06.860, Speaker I: Great, thanks.
00:31:13.890 - 00:31:33.810, Speaker A: Any other spec related topics that people wanted to bring out. Okay, if not any client updates.
00:31:34.790 - 00:31:44.870, Speaker B: I had one more thing. We haven't talked about the transaction pool at all today. I'm just curious if anyone has an update on that or thoughts status?
00:31:51.140 - 00:32:06.070, Speaker I: The ideas we discussed last time, I put them in the eth magicians thread. I don't think there was any follow up discussion, so I'm going to assume that means they're not controversial and we can go ahead and add those. But yeah, I don't know if there's been any other talk.
00:32:11.080 - 00:32:12.676, Speaker A: Arius, I know the guest team is.
00:32:12.698 - 00:32:14.244, Speaker B: Not allowed up to transaction pool.
00:32:14.292 - 00:32:17.800, Speaker A: Anything on your side, Marius?
00:32:18.380 - 00:32:34.930, Speaker H: No, not really. We've been very busy doing some refactors to the overall gaff code base and so I don't think anyone has really looked into upcoming things yet.
00:32:46.370 - 00:32:47.120, Speaker A: Okay.
00:32:48.850 - 00:33:33.230, Speaker H: We're also working on together with Casey from Prism, we're working on a SSC code generation tool and library for generating. Yeah, this is kind of a focus for us to get those tools working as soon as possible so that we can focus on other things. It's definitely like a priority for us, but as I said, we have a lot of other stuff. It's only for go. Alexey.
00:33:35.410 - 00:33:45.730, Speaker A: Is it very bound to go or it may be like become cross platform? Cross language?
00:33:48.630 - 00:33:53.060, Speaker H: In theory it could be cross platform, but we're not planning to do that.
00:33:57.350 - 00:34:20.560, Speaker A: It seems sense. Okay, I guess this kind of moves us to client teams, but any other teams have updates they wanted to share in other minds? We didn't start works on transaction flow.
00:34:29.670 - 00:34:33.350, Speaker H: Sorry, is Eton here today?
00:34:33.420 - 00:34:34.440, Speaker A: No, he's not.
00:34:35.210 - 00:35:19.330, Speaker H: That's unfortunate. So I've been looking at the six four test cases. One thing I noticed is that. So I'm kind of against the normalized type in general, but this is like a different discussion. What I would like to see is more specialized transaction types in the future. So for example, not all transaction types need to have the access list, not all transaction types need to be able to deploy contracts. And so having some test cases that kind of reflect that would be great.
00:35:19.330 - 00:35:50.300, Speaker H: And this will, my opinion, show that the SSC union is kind of the preferred SSC encoding if we want to go that route. I'm not sure other people agree with going that route, but I think it would be very nice to have more specialized transactions and not require every transaction type to support everything.
00:35:56.050 - 00:36:06.740, Speaker B: Just so we can close the loop on that. What specifically are you saying folks might not agree with? I'm just trying to figure out what the delta is between what Roberto posted and what you're saying here.
00:36:11.270 - 00:36:58.290, Speaker H: I don't know if people agree with the assumption that transaction types should be getting more specialized. Basically, we just added fields, and whenever we created a new transaction types, we included all of the previous fields. I'm not 100% sure what you're doing with the void for four transactions, but the way I see it, it would be great to drop the create mechanism from void for four transactions. Drop the access list from void for four transactions. Basically, drop the feature set that void for four transactions can utilize.
00:37:04.190 - 00:37:04.746, Speaker B: But you.
00:37:04.768 - 00:37:14.400, Speaker A: Still can run an execute contract call. Right? And you need access list probably to make it cheaper. Am I wrong?
00:37:15.090 - 00:38:21.442, Speaker H: Well, no, I don't think we need that. So basically, these block transactions are for specific contracts, right? And access lists don't make anything cheaper. They just let you prepay for gas. So that it was introduced as a feature to unstuck contracts that we broke with gas cost increases. These roll up contracts for the block transactions, they aren't deployed yet, so we cannot have, like, we didn't break them before. So I think it doesn't make sense to introduce this feature for these transactions. It would make the void for four transactions way cheaper, way smaller.
00:38:21.442 - 00:38:47.630, Speaker H: I think it's just like it's a feature that no one will ever use. No one will ever send a four transaction with an access list, so why should we have it? The same argument for no one will ever send a void for four transactions and deploy a contract in the same transaction.
00:38:48.050 - 00:39:07.490, Speaker A: And you're saying that it's easier for clients to deal with more restrictions at the transaction type than to try and have the same sort of template across many transaction types. Right, where it's not a ton of overhead to add restrictions to 4844 transactions.
00:39:09.830 - 00:39:10.580, Speaker H: Yes.
00:39:11.050 - 00:39:44.910, Speaker A: Okay. And then Mophie is saying optimism might deploy some contracts in four four transactions. But I guess the difference is like, is it a must have versus something that could be interesting to do if it was there, but I don't know. Mophie, would this be like a blocker for optimism if the flexibility of four or four transactions was reduced?
00:39:57.800 - 00:40:21.960, Speaker H: Yeah, that's exactly what I'm saying. There might not be agreement about this. This is just my thesis that I kind of like these restrictions, and I kind of like not porting over features that have been kind of hacky in other transaction types.
00:40:24.460 - 00:40:49.110, Speaker A: So say that we decided that the four eight four transactions don't have access list, but they can still deploy contracts. Is that still like a net benefit in your perspective, or is it more like we need to really narrow down the scope as much as possible to get the benefit because otherwise you still can't discriminate too much based on it.
00:40:50.920 - 00:41:59.610, Speaker H: It kind of depends which route you want to approach this problem from. I would rather say we're not doing anything and every feature has to be well argued for including it. Or we can say we're doing everything and we have to argue each feature to remove it if we want it removed. The thing is, we're going to save a couple of bytes, maybe 20 bytes at most if we do it this way, or 30 bytes at most. But if you think about 1 billion transactions or I think we have like one and a half or maybe 2 billion transactions on Mainet right now, this kind of adds up.
00:42:08.450 - 00:43:37.550, Speaker A: Okay. And I guess, yeah, I think we should try and bring this up on all core devs like next week so that we can hopefully get to a decision because it feels like, yeah, there's this higher level design decision we need to take and then everything else sort of flows from there. So if people can review that and I'll post about it in the discord as well, but if people can review that before, not the Acdec this week, but the week after that. Sal cordes I think it would be good if we could get on the same page by then. Um, anything else people wanted to discuss? And if not, I guess one question I would have is, so I understand we're about to ship Shanghai and teams are very focused on just getting that out right now. But I guess I'm curious what makes sense in the next month or so in terms of milestones or things we want to do for four. Four considering that getting Shanghai out on main net is the thing we're going to have to do in weeks or so.
00:43:41.120 - 00:44:13.736, Speaker F: So Shanghai is pariety. So most of our teams are focusing on that. Besides that, I am just implementing free the blobs, decouple the blobs pr. So after that's done, I will try to launch a prism local devnet to see how that goes. And once that's done, I will let people know and hopefully by then we can maybe try the multi client testnet. I guess the more client readiness by then the better. And once more clients ready, we can probably do Devnet five.
00:44:13.838 - 00:44:16.090, Speaker B: So yeah, that's my take.
00:44:21.380 - 00:44:23.040, Speaker A: Anyone else have thoughts?
00:44:29.150 - 00:44:54.450, Speaker E: Yeah, go ahead lighthouse. We have a few people working on for it for four, even while we're doing the Capella stuff. So we still have a fair amount of work to do on the implementation before we have something working. But I think over the next month our goal is to get something working and also work on a local devnet and if that works, interop.
00:45:05.930 - 00:45:15.626, Speaker A: I was just going to chime up for Nimbus, similar to what I'm continuing to work on 4844 and I've been.
00:45:15.648 - 00:45:20.266, Speaker E: Making progress on the decoupled blobs implementation, so I don't want to put a.
00:45:20.288 - 00:45:32.074, Speaker A: Date, but it's coming along and it all leads to a much nicer implementation as a side benefit. So I'm hopeful that sometime this month I'll be able to pair up, hopefully.
00:45:32.122 - 00:46:21.140, Speaker D: With webels is ready. The same for Teco. We already started the coupling pr. There have been also other type of work that we had to do internally, so we are not currently on full speed there, but we are planning to get back on the decoupling and make sense to us to have something by the end of the month to start playing with other clients. Sounds reasonable for us too.
00:46:27.400 - 00:47:57.846, Speaker A: So I guess if that's roughly the spot people are in, and as we're shifting more of our focus away from Shanghai as it's going to be done, does it make sense to move these calls to maybe like bi weekly and we can check in like two weeks from now where the implementations are? If we feel like we're ready to put a multi client devnet together, then we can do that in two weeks. If not, then I think if we were to put it together more know, a month from now, then we can also make sure that it's built off the Chappella releases. So we can kind of basically go from this being a four four four devnet to maybe like the first Denkun multi client devnet or something like that. Yeah. Does that generally make sense to people? It seems like over time this is basically going to become the main focus of the all core devs calls. So I want to make sure we're not just taking up people's time for no reason. Okay then yeah, I guess in that case I'll move the meetings to biweekly, make sure that we cover the SSD topic on the next acde, which is in like ten days.
00:47:57.846 - 00:48:41.840, Speaker A: So that should hopefully give people enough time to review things. And then two weeks from today we can meet together, check where the implementations are at, how Shanghai is going, and kind of decide on whether we want to move to a multi client testnet or wait another two weeks to do that. And I think there's a decent chance like this Devnet five might just be the first, like Denkun Devnet. And obviously we still have the discord. And we don't have to block progress on these calls, but I think it's still valuable to have them. Maybe just a bit more spaced out. Sweet.
00:48:41.840 - 00:49:13.560, Speaker A: Anything else anyone wanted to cover before we wrap up? Okay. And it seems like we might have a lodestar Netherwind devnet being stood up in the next week or so. Cool. Well, yeah. Thanks, everyone. Talk to you soon. And all of the calendar invites to make this by weekly.
00:49:14.140 - 00:49:15.112, Speaker B: Thanks, everyone.
00:49:15.246 - 00:49:17.192, Speaker A: Thanks. Bye. Thanks.
00:49:17.246 - 00:49:17.524, Speaker G: Bye.
00:49:17.572 - 00:49:20.020, Speaker A: Bye, everyone. Bye.
