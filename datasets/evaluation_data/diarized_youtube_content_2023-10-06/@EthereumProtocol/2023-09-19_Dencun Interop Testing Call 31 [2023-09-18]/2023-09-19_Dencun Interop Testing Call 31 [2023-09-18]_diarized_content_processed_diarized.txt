00:02:18.780 - 00:02:55.350, Speaker A: Welcome, everyone, to our 31st testing call. Yeah, so I guess we can start with any updates on Devnet eight then. Mario, you had some updates from the testing team. And after that, the main issue we had for Devnet nine to discuss was the empty address handling. And I think, Peter, you've got some thoughts on that, I guess. Anything people wanted to share on Devnet eight from the past few days?
00:03:00.270 - 00:03:00.730, Speaker B: Yeah.
00:03:00.800 - 00:03:40.040, Speaker C: Hi, guys. So in Devnet eight, we seem to have had some issues with the validator keys being run on different machines. This was not intended. So it was probably a mistake that I've done. We going to be looking into this? Another thing is we have had good finalization in the past week. I think most clients were able to keep up. There was some issues with Bezel and prism, but they both have been rolled since and they were able to stay at the head.
00:03:40.040 - 00:04:06.114, Speaker C: There is another issue with Eregon where Ergon was returning some wrong invalid payload index for a specific call and even for payloads that were not invalid. So I think there's a fix that needs to come in from that. And that's it pretty much.
00:04:06.312 - 00:04:12.900, Speaker A: Thank you. Anyone else have anything to add on that?
00:04:17.620 - 00:04:58.620, Speaker D: Yeah, I would like to comment about the issue in Aragon. Yes, there was a problem in the code that there was a transient error, something akin to out of memory. I'm not exactly sure why it happened, but the problem is that if we have things like out of memory, we of course should not return invalid for that block. It's a transient error. So now we've fixed the call so that we don't treat such transient errors as invalid block.
00:05:01.920 - 00:05:26.660, Speaker A: Got it. Justin? I don't know. We can't hear you, Justin. In the meantime. Okay, Justin's fixing his mic. Andrew, is the fix merge somewhere?
00:05:28.840 - 00:05:40.810, Speaker D: Yes, it is, but I think in our devil there are some changes for Devnet nine. So I'll point you to the pre Devnet nine comment. Yes, do that.
00:05:42.540 - 00:05:43.176, Speaker A: Awesome thing.
00:05:43.198 - 00:05:44.588, Speaker B: Mike check. You guys hear me now?
00:05:44.674 - 00:05:45.310, Speaker A: Yes.
00:05:46.560 - 00:05:46.924, Speaker B: Okay.
00:05:46.962 - 00:05:52.636, Speaker D: Yeah, so real quick, we're doing okay on Devnet eight. One of the things that's curious is.
00:05:52.658 - 00:05:55.376, Speaker B: That we're keeping up. The Nimbus baseup pair is keeping up.
00:05:55.398 - 00:05:57.468, Speaker D: Just fine, but it's slow.
00:05:57.644 - 00:05:59.132, Speaker B: Like it's kind of struggling.
00:05:59.196 - 00:06:00.704, Speaker D: So that's going to be investigated, but.
00:06:00.742 - 00:06:01.890, Speaker B: Nothing else to report.
00:06:04.020 - 00:06:04.770, Speaker A: Got.
00:06:10.680 - 00:06:11.430, Speaker E: It.
00:06:12.840 - 00:07:11.048, Speaker A: Anything else on Devnet eight, Martin? Yeah. So in nethermind after last, all core devs, we added filter to have only max 16 transactions per one sender. 16 block transactions right now since Friday, it's running on Devnet eight, I was spamming the network a bit and it looks like we are behaving well, building blocks, not missing spots, and generally it looks fine. Next step is like we have it already implemented, but it's not on Devnet yet. It's a functionality with storing blobs, like processed blobs until finalization. So it needs a bit polishing and could be added to Devnet eight in few days and Devnet nine since very beginning. Awesome.
00:07:11.048 - 00:07:24.000, Speaker A: Thanks. Anyone else?
00:07:27.330 - 00:08:33.300, Speaker F: So for Lighthouse, we've been looking into some racist we have with single block or blob lookups and trying to reduce the number of lookups we're making because we're right now over querying. And in looking into this, we realized we're handling some of our caching incorrectly specifically for this scenario where you get a blob that's valid on its own. So it's KZG commitments are valid, but then you later see the block and the KZG commitment. Commitment in the block doesn't match the blob. So like evicting this correctly and also making sure that we're able to import the block when we actually get the correct matching pair. So we're still in the process of fixing this stuff. It hasn't been stuff we've observed on the Devnet, but I think these types of scenarios would be good to test maybe on Devnet nine.
00:08:33.300 - 00:08:35.540, Speaker F: That's pretty much it.
00:08:41.390 - 00:08:42.460, Speaker A: Got it.
00:08:44.210 - 00:09:02.600, Speaker E: One more thing I wanted to mention was that we cleared and updated the prism Bezu as well as the prism Ethereum Js modes, I think. And at least the prism Bezu has still been syncing since the morning. Kind of slow for such a small network. Could someone have a look?
00:09:04.010 - 00:09:06.360, Speaker B: Yeah, I can take a look after this.
00:09:22.250 - 00:09:58.860, Speaker A: Cool. Anything else on Devnet eight? Okay then in that case, next up, Mario Vega. You had a couple updates you wanted to share on the testing front. Shared a lot of those on the agenda, but do you want to quickly recap?
00:10:00.480 - 00:10:39.796, Speaker G: Yeah, of course, definitely. So yeah, just like a brief update of how we are standing on the devnet nine updates for the execution spec test and Hive. So yeah, the first thing is that we have the 1153 test already merged for Devin nine. So that's covered already, which we didn't have for Devnet eight. So that's good. And we have the update for the 47 80, the newest address that is on the current draft PR for the EIP. So we have the VPR ready, but we are missing the transaction details.
00:10:39.796 - 00:11:23.240, Speaker G: But I already pinged like client about this. So once that's merged in the pr, I'm sorry, in the eips, we can update the pr for the execution spectrum and merge also. And we also have a pr for 70 516, which is the blob base fee opcode. We already have tests for that and it's ready to be merged. We will be merging soon, maybe today or tomorrow. And once all of those prs are merged, we are going to make another release which will be the Devnet nine release, which contains all of the changes for Devnet nine. And we also will be updating Hive to contain all of these tests, plus any other Devnet nine related Hive updates.
00:11:23.240 - 00:12:30.316, Speaker G: We are looking to do this in the following couple of days so we can start running Devon nine ready tests for the clients. That's on the PI spec side of things on Hive yesterday we created the test case for the issue that we were seeing on Geth on the main net. So basically the issue and why we didn't catch this in Hive is because all of the pre merge blocks are marked as zero for Hive, just to make everything easily configurable. And the issue was that in Maine that is not true. We have the London block that is more than zero, obviously. So what we did in this pr is to try to make at least one of the premersh forks number greater than zero. So we have one single test that hard codes the London block number to one just to be able to catch this issue.
00:12:30.316 - 00:13:28.544, Speaker G: And it was able to catch the get issue before and now get is correctly passing this test after the batch. So we also ran this same test on all clients basically, and we already notified if we found anything on your client. Another update is that the mock builder is the nav ready? We already started testing with a bunch of the cls and basically most of them are passing. And we found a couple of issues on one of the CL clients. And we have this hive simulator which is the PR at the bottom of the comment that we have on the issue. So if you go in this pr, you can see how you can run the builder API tests on the CL on whatever Cl that you want to run. This is a very simple test.
00:13:28.544 - 00:14:11.790, Speaker G: Basically it's just making sure that the network starts. So you have the El and the CL both configured on the nav at the first epoch, I think, and then it starts sending blobs. And then you have your builder configured, which is the mock builder. So this mock builder, what it does is basically just requests from the El, a payload and most of the time it contains blobs. So what it does is just passes along that built payload to the CL in the form of the builder API. So we can test that back and forth interaction with the CL and the builder API. So yeah, that's basically it.
00:14:11.790 - 00:14:53.640, Speaker G: I posted the link also to the mock builder. If you guys want to take a look. We have some options to make this mock builder configurable so it can induce errors. For example, just inserting the incorrect bitcoin root in the payload or whatever, it's like a server. So you can basically just instruct the server to just introduce errors on demand, basically. So if you guys go into the repo, please take a look. Just any ideas on what other types of issues we can inject with this mock builder will be greatly appreciated.
00:14:53.640 - 00:14:57.150, Speaker G: So, yeah, that's basically it.
00:14:59.600 - 00:15:40.080, Speaker A: Thank you. Okay, then I guess moving on to Devnet nine. So the main issue we had on awkward devs last week is this discussion around touching accounts and how do we deal with the system address. Peter, you wrote a doc in Hackendi that you shared in the chat. Do you maybe want to walk us through that? I just posted the link in the chat.
00:15:40.900 - 00:16:20.780, Speaker E: Yeah. Hello, can everyone hear? So basically, the situation with empty accounts is that they've only existed on a few really old networks. Ethereum, Mainnet, Ethereum classic, and a bunch of testnets. All of our testnets that have ever had empty accounts are gone. Robstein had them. I don't know if anyone else had them. So basically at this point, empty accounts are effectively a testing artifact in the sense that people keep writing tests for them, despite the fact that they cover situations that can never happen in production.
00:16:20.780 - 00:17:18.336, Speaker E: So what I think we should do is rather than continue going through this situation where we're writing these tests and people have to argue about, well, what should happen in this weird edge case that we've come up with is we should just assert as a matter of principle that you can't have a merge network that has an FD account on it. This is quite straightforward because either we'll have to manually check like that at the merge. There were no OBD accounts on Mainnet. I'm pretty sure it's true. I did check it with a defunct client called a cooler at one point, but we'll have to do that again. It's not massively difficult. Then similarly for anything else, if the EIP one five eight block is zero, which is on everything that is in the main net, and you didn't put any accounts in the genesis, it's not possible for the account to be created on that network ever.
00:17:18.336 - 00:17:45.670, Speaker E: And so there can't be any. If we accept that principle, we can then just assert that empty accounts on merge networks shouldn't happen. We can declare that any test case that puts empty accounts in the pre state and relates to merge networks is just invalid. Yeah, that's the basic summary. Do people support this as an.
00:17:55.040 - 00:17:57.228, Speaker A: Oh yeah, dano. Sorry, I couldn't find mute.
00:17:57.324 - 00:18:04.820, Speaker H: So to be clear, the existing rule that if an account becomes empty and it's time to commit it to disk, nothing gets written that still exists.
00:18:06.120 - 00:18:45.264, Speaker E: Yes. So the current rule is that it is not possible for an account to be created empty. I think the EOP is slightly vague about whether during the transaction, like an empty account can exist and it then gets deleted at the end of a transaction. But that's not an observable behavior. If you decide that as a matter of principle, you're writing a client where the data model doesn't even acknowledge the possibility of an empty count existing, that's completely compatible in a world where they've all been deleted. They can't be created. The only question is whether ones that existed prior to spirit dragon are still.
00:18:45.302 - 00:18:45.890, Speaker B: Around.
00:18:52.430 - 00:18:56.240, Speaker E: To respond to light client's point.
00:18:57.250 - 00:18:57.806, Speaker A: Yes.
00:18:57.908 - 00:20:00.850, Speaker E: So there's basically two places you have to check that empty accounts haven't been created. Firstly, if you're creating a network, you have to not put any empty accounts in the Genesis alloc and you have to make sure that you started late in the spirit dragon. The second one is that you have to go and check that if you have a test, that the test doesn't put an empty account in the Genesis alloc. This might be a slightly non trivial amount of work for the legacy tests because there are various empty accounts in places. There are tests that aren't even about empty accounts where someone just decided to make the empty account. If you look at the doc I linked, there's a draft eep at the bottom which says that the TTD block is invalid if it contains any. The merge block, sorry, is invalid if the TTD block contains any empty accounts.
00:20:00.850 - 00:20:33.690, Speaker E: But yeah, clients literally just have to check that there are no empty accounts in Genesis. I was proposing to actually make clients do this check because it is fairly trivial. I. You either check that eIp one five eight fork block is zero and the genesis file, the genesis Alec has no of the accounts. Or you check that on main net, you check that the merge block is the mainnet merge block. Andrew?
00:20:35.650 - 00:20:52.240, Speaker D: Yeah. To me this approach is fine. But I would change the execution spec tests not to include empty accounts in Genesis, because there are a few test cases that do that.
00:20:52.850 - 00:21:31.650, Speaker E: Yeah, I mean, there kind of needs to be, in practice, there needs to be a sort of two pronged approach here. Firstly, where there are tests that contain empty accounts, they need to be rectified either by deleting them or in some cases altering them so the account is no longer empty. And secondly, we should make the tooling just refuse to accept that there can be empty accounts. It should just be, if you write a test, if you try to import a test and the test got an empty account, the tool should just complain, this test is invalid, contains an empty account on a post merge network.
00:21:34.150 - 00:22:09.246, Speaker G: Mario yeah, that's definitely doable. I think it's super easy just for us to include an assertion the if there's an empty account, just reject it. Because in this last example, I think it was one of us that wrote the test. I think it was me, probably, and we just didn't realize that there was an empty account. So it sparked this discussion, but it was completely non, we didn't mean to do this. So yeah, I think doing just a simple assertion in the execs, just so this doesn't happen again, it's just fairly simple.
00:22:09.428 - 00:22:20.900, Speaker E: Yeah. And there's a weird edge case involving what happens if Coinbase is an empty account and that is tested completely by accident. There's a test that's actually about.
00:22:23.750 - 00:22:24.066, Speaker A: The.
00:22:24.088 - 00:23:00.960, Speaker E: Pairing pre compile that just happened to set Coinbase to an empty account. And if you want to pass that test, you're required to delete the empty account as well as do what the Coinbase recompilers. So there might be a little bit of tidying up work just to deal with the fact that there are test cases floating around that are weird because they happen to contain an empty account by accident. Most of the ones that are actually testing empty accounts, you just need to put a condition that says this test shouldn't be run if you're testing post merge and just say it's a pre London and earlier only test.
00:23:02.290 - 00:23:09.938, Speaker G: Yeah. I think the moment that we insert the assertion, it's going to be flagging a lot of test cases automatically and.
00:23:09.944 - 00:23:11.874, Speaker E: Then someone just has to go through it.
00:23:12.072 - 00:23:12.820, Speaker G: Yeah.
00:23:14.230 - 00:24:08.434, Speaker E: Cool. So I've written like a draft deep that says that empty accounts are invalid from merge networks. If I just make that an EIP, we could then try and get it through the process. There is one mild complication, which is that we have to decide what process of checking needs to be done to ensure that my claim that all the empty accounts on main net were deleted before the merge is actually true. Because obviously the consequences if I'm wrong could be quite severe. I think conceptually it's quite easy. There's a command called GEF dump that will print a JSON of every single account in the state to standard output.
00:24:08.482 - 00:24:12.082, Speaker A: Right. But you need like an archive node at the merge block.
00:24:12.146 - 00:24:38.720, Speaker E: Basically you don't need. Well, you need. Yeah, and currently it's one of my tasks to investigate getting historical states for these sorts of things available. But at the moment I think the easiest thing is just to get someone to sync a GEF node to somewhere between the block where the empty accounts got deleted and the merge and just iterate over the state.
00:24:40.610 - 00:24:47.440, Speaker A: How quickly does Geth take the sync in archive? Couldn't you use Aragon? Or.
00:24:55.510 - 00:25:36.494, Speaker E: I could look into. So the thing is, there isn't an API endpoint in that. I don't think Ref has the capacity to iterate over every account in the state at all, although that could be implemented. Eragon does implement a debug endpoint that allows you to iterate over the state. It is very slow to the point that. Although then again if GEF is going to take 13 days and also you don't actually have to get up to the merge, you have to get up to block 14 million and something which.
00:25:36.532 - 00:25:39.230, Speaker A: Was under London Dragam.
00:25:40.870 - 00:25:42.340, Speaker I: What exactly is needed?
00:25:43.110 - 00:25:45.940, Speaker E: What we need gone.
00:25:46.390 - 00:25:47.362, Speaker I: Yeah, continue.
00:25:47.416 - 00:26:18.900, Speaker E: Sorry, what's needed? We need a reliable process that iterates over all of the accounts in a state after this 40 million something block and before the merge that will reliably check that none of them are empty. And the main significance is that we need to do it in a way that people are prepared to write their clients on the assumption that this is correct.
00:26:19.510 - 00:26:45.926, Speaker I: Because people will be 40 million. Yeah. Basically we are saving one table inside the database. That's like change sets. Change sets are basically the previous value that got changed. So iterating on those changes should be the way to verify there is no empty accounts before. After a 40 million block.
00:26:46.038 - 00:27:10.642, Speaker E: Actually, to be fair, if you did that against. I could do that against a refr. Yeah, I hadn't thought about doing it that way. That would also be quite easy if you just iterated over the entire chain set and basically kept a tally of all the current empty accounts. Which. Yeah, the main issue is not prove. Is not actually proving that is not actually like running the code.
00:27:10.642 - 00:27:37.900, Speaker E: It's like persuading people that the code is correct because technically it might end up being that I will write an implementation that iterates over a ref archive node which I have available and someone else will sync geth over a week. Just because if we do it twice, the chances that anyone makes a mistake and there's some empty account somewhere is lower.
00:27:39.070 - 00:28:08.130, Speaker I: But I would even need for the history state. Shouldn't we just take the plain state check if there is no empty account? Because we are talking about the future, in the future that we don't have any empty accounts, not just in the future, but presently, should we just iterate over the plane state check if there is no empty account? There should be good enough assumption for the future aips or even future tests.
00:28:08.470 - 00:28:41.630, Speaker E: To be fair, that is actually a point that you do to some extent. Most of the risk is covered if you check the current state. Obviously you can't then eliminate the principle. There's like some outside possibility that there was an empty account that existed at the merge and then got deleted. But yeah, I agree that you could basically cover all the risk just by taking a current GEF node, running GEF dump on it, and iterating the five line python script that checks that none of them are empty.
00:28:44.610 - 00:28:52.500, Speaker I: But others can say about this. For me it should be fine just to verify that there is no empty account.
00:28:53.670 - 00:29:16.220, Speaker E: Yes, the significance is that the EIP says that you can write your post merge courage on the assumption that there are no empty accounts. And so if something happened with an empty account between the merge and now, in principle that would break your client. Although I agree it's not likely to be a security risk in the way that if there are still empty accounts in the state, it would be.
00:29:24.680 - 00:30:00.880, Speaker I: To be honest, we probably shouldn't be bothered by that. Even if this breaking, if there is the case that something is found like that, we can change the AIP to introduce that new information, but even that can be like special case for some account. I think we are talking a lot and invest a lot of time is something that can potentially happen, but there isn't any EVM that's just after the merge.
00:30:02.980 - 00:30:43.770, Speaker E: Yeah, I think that's reasonable. The main thing is that the thing is all of the EVM, and also if you break the EVM in a way that it doesn't work on some historical block, people are going to notice. People test their clients by running them on historical by syncing main net, so they're going to notice. But yeah, I basically agree. This seems like an incredibly achievable goal. We just need to do it in some process that people are comfortable that the state doesn't contain empty accounts.
00:30:49.430 - 00:31:03.050, Speaker B: Isn't it the little orthogonal if the state contains empty accounts though, to what we're talking about? 4788 because it's not. Does the state contain an empty account? Is, is this specific address the system address empty?
00:31:03.470 - 00:31:44.230, Speaker E: So the present issue, the issue that triggered on Devnet nine, can actually be solved by inquiring about the one account which is not empty. The more like long term issue is that if we don't implement this, that says that there are no empty accounts anywhere. Every time someone changes the way the EVM works, there is the potential that they will implement a new edge case for state clearing, which we will then have to have a discussion about. And the goal is that by drawing a line under empty accounts, we can save having to have this sort of conversation which will otherwise continue in perpetuity.
00:31:48.340 - 00:31:54.000, Speaker B: Sure, I just think it's something to look into, but not on this call specifically.
00:31:55.620 - 00:32:36.560, Speaker E: Yeah, my proposal is that I'm going to go away and actually produce a formal proposal. It seems like people are up for doing all this work and it will probably be worth it in the long term. But I agree that from the point of view of testing, we just need to check that the account, that all FCE is empty on mainnet and then not empty on Mainnet, and then that resolves the issue here. We can immediately delete the test case that has caused this problem and be safe, and that will give us time for people to spend a few weeks dealing with the rest of us. Dano.
00:32:37.380 - 00:33:15.496, Speaker H: So my quick read of this is it's not the test case of the problem. The problem is we're using accounts in a new way, having an account be a sender without incrementing the notes or having any other required state in it, which there's no other way to do. And this is just the natural fallout of doing new things. So I'm not as concerned that we're hitting edge cases, because this is expected, because we're doing brand new stuff. And if we don't go through the process of figuring out what doing new stuff means in edge cases, then I think we're not being diligent in applying new innovative things to do the EVM, or we should stop doing new innovative.
00:33:15.528 - 00:33:16.536, Speaker E: Things with the EVM.
00:33:16.648 - 00:33:20.990, Speaker H: That's the other option. We could increment the notes every time we call the system account.
00:33:22.180 - 00:34:23.700, Speaker E: So I think the issue here is that the issue here, which is agreed very mildly, somewhat orthogonal to the present issue, is that empty accounts are a particular source of really annoying and subtle edge cases. There's a lot of technical subtlety. In what conditions does the empty account clearing trigger? And since there are no empty accounts, the answer to when does empty account clearing trigger is entirely boot. And it would save people dev time. Because we have a choice here. We either eragon have to modify their code just to support a hypothetical edge case that will only occur in a test suite, or we have to assert that this edge case can't occur. And it's quite common for these situations, just not involved.
00:34:23.700 - 00:34:53.980, Speaker E: And it's quite common for people to find new ways of potentially clearing empty accounts. They keep coming up, they keep getting tested, and there's special code in Eels just to handle one really niche edge case involving a hash collision where one of the things is an empty account. Despite the fact that in production practice, hash collisions and empty accounts do not occur.
00:34:54.800 - 00:35:02.776, Speaker H: Or when you call a system account, we make it behave like every other account. When it's the start of the transaction, we increase its notes, and it would never be empty.
00:35:02.808 - 00:35:03.390, Speaker B: Then.
00:35:12.290 - 00:35:16.766, Speaker E: You'Re going to end up with, like, that's the new space we're creating.
00:35:16.798 - 00:35:31.930, Speaker H: Is that system account. You can do these root level account calls without doing any change to the account, which no other eoa supports. That we're creating a new class of transaction system account, and we're not following the rules that eoas follow. So this is the fallout.
00:35:33.230 - 00:35:48.080, Speaker D: Well, I'd rather not do it because we already use system transactions for gnosis chain, and we don't increase system address. There's an answer over the system address there.
00:35:50.530 - 00:35:56.160, Speaker E: But the problem on gnosis chain doesn't matter because there can't be empty accounts on gnosis chain. So this doesn't come up.
00:35:57.270 - 00:36:39.950, Speaker D: Well, I think for gnosis chain, actually, this problem probably matters because there was this problem with empty system account on gnosis chain, and I had to introduce a hack into Aragon for gnosis chain. We could discuss it, but I think on this call, it doesn't matter too much, but I would rather keep things similar between how system transactions are implemented for ethereum and gnosis chain. Just makes my life easier.
00:36:41.570 - 00:36:42.590, Speaker A: Jagan.
00:36:44.450 - 00:36:59.480, Speaker I: I would argue a little bit against non increment because it feels like additional unnecessary state change. We can go it without it.
00:37:01.690 - 00:37:02.006, Speaker B: For.
00:37:02.028 - 00:37:11.510, Speaker H: Or against whatever thing we do. We're doing a different way to start the transaction. That we're running into empty changes is not surprising.
00:37:13.390 - 00:38:08.380, Speaker B: I mean, technically, it's not really a transaction. I don't know how other clients implement this, but I think we implement in this way where we process a transaction first and then normalize the transaction into an EVM message call. And it's sort of the same structure that we make internal calls within the normal transaction call trace if the user calls the call opcode. And I assume other clients are doing something similar to this where they process different types of transactions, put them into this call format. So we're kind of skipping the transaction portion of it where we're updating the nons because the nons needs to be updated because you have this signature and you have to deduct this value from this account. Instead we're like skipping directly into instantiating a call frame. So yeah, I don't know if in the increment the nonsense is really the right thing to do here.
00:38:08.380 - 00:38:21.230, Speaker B: The AAP also says.
00:38:23.040 - 00:38:24.696, Speaker E: Clients could implement.
00:38:24.808 - 00:38:31.150, Speaker B: The state changes directly, which would also be like, I don't see why.
00:38:33.460 - 00:38:33.824, Speaker E: That.
00:38:33.862 - 00:38:40.530, Speaker B: Approach to implementing it would have to increment the nons or touch the account, whatever.
00:38:41.460 - 00:38:49.060, Speaker H: Right, but we're changing the account state through a new means. Yes, as we're learning today, a little under specified.
00:38:50.360 - 00:39:31.532, Speaker B: Yeah. I think outcome should be to make a decision on how to do this kind of indifferent. I mean, it is specified on how to deal with this. I think the problem is that guest did not implement the specification. I mean, one six one says an account is touched if it is the source or destination of a call. So technically the source here is the system address and it should have been touched. It's just that we don't implement this specific aspect of one six one because it always occurs due to the fact that if the transaction is coming from an EOA, then the nonsense updated, which then creates the touching.
00:39:31.532 - 00:40:01.260, Speaker B: If it's called within the EVM context, then it has to be both at least the source and the destination of a call. And so as destination of a call, something is getting touched there. So really the reason that we're even having this debate is because we didn't implement the one six one spec to the t. Obviously there's no issue with the main net implementation because of this non thing, but I just wanted to point out that it was specified.
00:40:04.480 - 00:41:26.330, Speaker E: This is, then you get into the fact that one six one is a bit vague and very confusing because what one six one actually says, it says that if an account is subject to a potentially state changing operation, then it gets marked for deletion, and then if it's still empty at the end of the transaction, it means it gets deleted. The question then becomes is, does sending a message from the account doing this special transaction thing, does being the source of that count as a potentially state changing operation? And then confusing the EIP one six one then lists a bunch of what it thinks are potentially state changing operations which cause a whole bunch of confusion. And I think we would be better off just eliminating the possibility of people having to deal with empty accounts rather than trying to work out what constitutes a potentially state changing operation. I think it would just be better for like, be more useful use of people's time if they didn't have to continually argue about the semantics of one six one.
00:41:28.540 - 00:41:41.310, Speaker B: I agree. I think we should resolve that at a later point. I think for now we should just say we're not going to allow empty accounts in Genesis, and that's going to really resolve all our problems for 4788.
00:41:44.020 - 00:42:07.690, Speaker E: Yeah, let's say we're not going to allow empty accounts in Genesis. We are not going to allow people to make many fe be empty in a test suite or anywhere. And going forward, this is the same thing. And then going forward, I'm going to push this EIP, which means that we can finally not have to think about the accounts at all.
00:42:08.860 - 00:42:10.490, Speaker B: That sounds good to me.
00:42:12.300 - 00:42:15.930, Speaker A: Where should we write this down?
00:42:18.240 - 00:42:21.660, Speaker B: Where is the spec for allocation?
00:42:24.240 - 00:42:25.020, Speaker A: Um.
00:42:27.920 - 00:42:29.744, Speaker B: I don't think there is a spec for that.
00:42:29.862 - 00:42:30.530, Speaker A: Yeah.
00:42:35.060 - 00:43:10.780, Speaker B: Well, we just define has their own. They do have their own. I mean, if all the clients agree just to now go and write some pr that makes a check as you're loading the genesis allocation, that you're not adding account without a balance code or nonce, then we can just follow up on Thursday or next Thursday or something, make sure you did it or you'll fail the hive test. I guess we could have a hive test for checking to see if clients accept an empty account at Genesis.
00:43:14.320 - 00:43:20.290, Speaker A: Yeah, so I guess a test. Is there something we want to add to 4788 as well?
00:43:22.580 - 00:43:23.330, Speaker B: Sorry?
00:43:23.780 - 00:43:26.690, Speaker A: Is there something we want to add in 4788?
00:43:27.780 - 00:43:45.080, Speaker B: I don't think so. I mean, this is not really related to 4788 specifically, especially if we're going down this path. If we don't want to think about one six one in the context of 4788, then I definitely don't think that we should mention anything. We should just simply not have the scenario occur.
00:43:51.360 - 00:44:28.724, Speaker A: Okay. Does that make sense to everyone? Okay, perfect. Plenty of thumbs up. I think that was the last thing we had on the agenda. But I guess one thing I wanted to check in on is so we added these two eips on awkward devs last week. The churn limit and then the blob gas. No.
00:44:28.724 - 00:45:33.024, Speaker A: So Devnet nine is not still happening tomorrow. So what we decided on awkward devs last week was that because we added the two new eips, we would try to get everything implemented in all the clients by Thursday. And then Thursday we can figure out when we want to launch the devnet. So I wanted to see, does that still seem realistic to teams? Are there people who feel like we can't reach that for some reason? Okay, so we should be good. There was another conversation in awkward devs about renaming the opcode. And if people want to have a look at that, I don't think this changes any single thing in the implementations except the name. So, yeah, we can probably wait on that and then.
00:45:33.024 - 00:45:54.180, Speaker A: Okay, Terrence, you're saying you're waiting on the specs release? I think Danny said on the call that it would come Monday or Tuesday. Yeah, my client. I'm not sure I understand your questions. Your question?
00:45:55.510 - 00:46:59.490, Speaker B: Yeah, I was talking with Mark last week a little bit about something like this, and I am under the impression that during optimistic sync, lighthouse actually determines what withdrawals are supposed to be going into the execution payload header, and then computes the Merkel Patricia tree route. So they get the withdrawals hash, which is in the execution layer header, and then they hash the execution layer header to make sure it matches what's in the execution payload. I was wondering if other clients do this. Okay, because I think that there's a similar situation with 4844, because with withdrawals, we sort of have this situation where the CL tells the El what should be in the withdrawals, but then the El puts the withdrawals in its header. And as long as that data is always coming from the CL, it's okay. But during optimistic sync, the EL goes to the network to actually retrieve the blocks. And so it never receives new payload that says these are the withdrawals that are supposed to be in it.
00:46:59.490 - 00:47:56.310, Speaker B: And so it doesn't really know if the withdrawals are right. It can make sure the state transition is correct, but it doesn't know if the withdrawals themselves match what the CL expected. And so I think that's why Lighthouse is doing this check during optimistic sync. But the same thing is true for 4844, where on the beacon block you have the KCG commitments, which are basically the version hashes that should be in the block. So you're telling the El via a new payload, hey, here's the version hashes that should be in the block verify that the transaction has those version hashes and the EL does that verification. But if you kick them into optimistic sync and they go to the network, they don't know what version hashes were supposed to be in the block. So you again have this issue where the CL should probably compute the transaction tri route after doing the verification of what version hashes were actually in the block.
00:48:02.530 - 00:48:06.770, Speaker A: Okay, prism doesn't do this any other CL teams.
00:48:13.760 - 00:48:26.000, Speaker F: So yeah, I think Lighthouse during optimistic sync doesn't do the same for versioned hashes. But I also was talking to mark a little bit about it and I think it's something we need to implement.
00:48:38.270 - 00:49:22.570, Speaker G: I remember that when I was testing the builder API, most of the clients, if not all of them, rejected a block with an invalid withdrawals route on the blinded header. So it would seem to me that most of them are doing the check on the withdrawals, but I'm not sure. I just remember that most of them did the Cl. Yes, the Cl. So I built a one of the tests was just corrupt the withdrawals route, send the blinded header so without the withdrawals and see what DDCL did. And most of them, if I remember correctly, were rejecting my blinded block because the hash didn't match what they expected.
00:49:27.710 - 00:49:29.580, Speaker B: That's interesting. That kind of makes sense.
00:49:36.370 - 00:49:38.320, Speaker A: Okay, anything else?
00:49:42.650 - 00:51:02.950, Speaker B: I guess like one other quick question. Originally when we talked about versioning the engine API, it seemed like we were trying to not closely associate the fork timestamp with the method specifically. And now notice that most of the methods have gating both on are the parameters that you expect to be there given the fork there? And is the timestamp stamp for the fork active? And I was wondering if there was a real specific reason for both of these checks, or if it would be maybe okay to relax the timestamp check and just verify that the parameters that are necessary for that fork are not null. So like a concrete example is for fork trace updated v three. If you call it with attributes that don't include the beacon route, you would just say that's not a valid fork choice updated v three call rather than then also checking is cancun active or not?
00:51:13.390 - 00:51:16.330, Speaker G: Is this in relation to ahive tests?
00:51:18.350 - 00:52:02.310, Speaker B: This is in relation to a suite of hive tests. But I mean more generically, it's not really about Hive, it's more about implementing in clients. And yeah, I'm just noticing that it's getting a little unwieldy with how much conformance checks we have to do for these methods. And we can make it a little bit better, but I don't really see the benefit of checking both the fork time versus the time of whatever object is being passed in versus doing semantically checking. To me, they do the same thing. They're making sure. Is it correct to call this method right now? And, yeah, it's a bit problematic.
00:52:02.310 - 00:52:05.130, Speaker B: That's not problematic. It just seems like duplicate checks.
00:52:27.990 - 00:53:06.000, Speaker A: Other comments? Okay, I guess we can wrap up anything else before we finish up. Okay, well, thanks, everyone, and talk to you all soon. Thank you. Bye.
00:53:06.340 - 00:53:06.704, Speaker E: Thanks.
00:53:06.742 - 00:53:07.440, Speaker F: Bye.
01:04:30.470 - 01:04:55.820, Speaker E: Jesus, lo.
