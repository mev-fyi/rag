00:03:03.600 - 00:03:49.712, Speaker A: We are now live. Morning everyone, and welcome to Acde one six one. So yesterday, if people have any more thoughts or discussions we want to have around Cancun scope, we can do that first. Then it probably makes sense to spend most of the call talking about implementation for the current Cancun stuff. And specifically for 4844, there's a bunch of open issues that I've listed on the agenda. And at the end we have Zach on who wants to talk about kind of a funky eip because it would only apply to l two. And whether this should even be part of awkward devs is an open question.
00:03:49.712 - 00:04:35.570, Speaker A: So we can go through that towards the end. Yeah, I guess to kick us off on the Cancun stuff. So on the last call we sort of agreed to include a bunch of eips. So at 1153, four four was obviously included. This SSD optional EIP, and then the self destruct removal, we also CFI the couple more. So the BLS pre compile, the beacon block route, and the SSZ transaction signature scheme, I guess. Did any of the clients feel like I have any updated views on that, like anything that they think we should be considering or.
00:04:35.570 - 00:05:14.292, Speaker A: Yeah, I'll pause here. Otherwise it's fine to keep it as is and keep going with that. Okay, sweet. So we'll leave this as is. And I assume based on the SSD conversation we might make some changes. If not today, then maybe in the next few calls, or at least changing the actual specs. Okay, so next up, actually, sorry, just before we wrap up here, I guess I'll assume that by default we sort of keep this scope for Cancun.
00:05:14.292 - 00:05:47.940, Speaker A: And if anybody wants to change it going forward, just put something in the agenda or on the east magicians thread and I'll pick it up from there. But this way we don't have to go over this every call. Okay, yeah. And so next up, 4844. So there were a couple open issues or concerns. We can just go through them first. Andrew, you mentioned the difference between the input and input in the pre compile being little indian versus big indian.
00:05:47.940 - 00:05:50.970, Speaker A: Do you want to give some context on that?
00:05:52.780 - 00:06:30.640, Speaker B: Yeah, sure. I think the little indian encoding was inherited from Cl or was just taken from Cl. And on the Cl side things are little indian. But now there is an inconsistency. And on the l side in the evM, most things are big indian, there are kind of some inconsistency. I think Blake too pre compile might have some inconsistency, but most things are big indian. And now we have this weird situation where Z and y are like two inputs.
00:06:30.640 - 00:06:56.510, Speaker B: I'm not sure about the rest of the inputs, but at least two inputs are little indian, while the outputs are big indian of the point evaluation pre compiler. And I'm just thinking we can be okay with it, but maybe we should just explicitly clarify it in the eips that we do have this inconsistency. I don't know what are the views we might want to reconsider this?
00:07:03.990 - 00:07:12.200, Speaker A: Does anyone either have strong opinion on this or like a rationale for why we should keep it this way?
00:07:22.040 - 00:07:45.900, Speaker B: I think the output is kind of mostly for informational purposes, the output of the pre compile, so it's static. Maybe it's not too big a deal if we leave it as is. I think we should just add an extra clarification to the that we just highlighted the inconsistency.
00:07:48.400 - 00:08:10.900, Speaker A: Okay, and there's some comments in the chat from, I think, all the other CL clients saying they don't like little indian on the El, so I think we should have a really good reason to have the inputs little indian. Otherwise we should just make it big indian.
00:08:17.300 - 00:08:28.870, Speaker B: I think if there's any border or transition between little ending and big end, it should be at a high level as possible. And what we're talking about is way too low level to make that.
00:08:32.760 - 00:08:55.820, Speaker A: Yeah, there's like plus ones from all the El teams on Maris's comments. Should we just make this change to the spec, then put everything to big indian? And that means everything on the El stays low. Endian oh yeah, ethan has a good question. Does this affect some of the underlying crypto libraries? If we switched everything to big indian.
00:08:58.880 - 00:09:35.720, Speaker B: I think it can read inputs in both big engine or little engine. You just have to specify which. Does it convert twice? Because usually the libraries have like a native order that they prefer to operate on. Even if it converts, it'll read the inputs in its native representation, so there might be some conversion, but it's a very fast operation. It's negligible compared to the main runtime of the pre compile.
00:09:40.230 - 00:10:42.470, Speaker A: Yeah, I also think we shouldn't take the time for the conversion into account when making this decision. This is more about if for some reason all the inputs that are going to be into this pre compile are already little engine, then it would kind still not really much for me. It wouldn't make that much sense to have it little engine, but I would see the point. I don't think execution speed should be a consideration here. The big engine to little engine conversion is so fast, that makes no sense. But if there's a really important issue why we need this, then I would accept it. But from my point of view, I have no idea what the authors thought about.
00:10:42.470 - 00:11:57.468, Speaker A: So I would just go and say let's make it pick nu, and as everything else it. Okay, does anyone disagree with that? No disagreement per se, but we should check with whoever created the spec. I mean, there must be a reason why. Just because we can't say why it's little indian doesn't mean there's no mean. Denkrad, I believe, is on the call. I don't think Prolo is here, but Denkrad, do you have any thoughts on that? Sorry, just joined, I didn't get the context just now. Is there a reason why the pre compile in four? Four? Four uses little indian? And would there be a problem if we converted everything to big indian to just be consistent with the entire rest of the El? Oh, I see.
00:11:57.468 - 00:12:24.784, Speaker A: All right, that's a very good question. Yeah, I mean, this is like a difficult one because we are starting to switch the Cl. Everything is big indian. Yeah, I don't know, honestly, I don't know how much we thought about this. Okay. Yeah. So there's no reason within 4844 why it's much better to have the inputs be little indian.
00:12:24.784 - 00:12:56.800, Speaker A: It's kind of a default design on the Cl that sort of. Yes, I guess there are two things. Like the CL is big indian, but also crypto libraries tend to be big. Oh, they tend to be big. So it's actually the opposite. Yeah, it might actually make more sense than to make it big. And also because the El might do some computations in using those integers.
00:12:56.800 - 00:13:24.600, Speaker A: Okay. And I don't know if Proto's on the call. No. So I guess what we can do is at least we can open a pr on the EIP to make it big indian, leave it up for a few days, know if Prolo is the other major author that's not here, or if anyone else has objections, they can obviously raise it on the pr, but assuming that there's no objections, we just move it to big indian. Mikhail?
00:13:25.740 - 00:13:40.830, Speaker B: Yeah, as far as I can see, the spec for spec refers to a verify KCG proof function that is defined on Cl, which is little NDN, I believe. So that's probably the reason for.
00:13:41.780 - 00:13:44.930, Speaker A: Sorry, which function did you say?
00:13:46.100 - 00:14:16.170, Speaker B: Verify KzG proof. So the pre compile, as it is specified, calls to this function, and this function is specified in CL spec. And CL spec is little ending, I believe. So, so that's why it is specified this way. I mean this y input should be little ending to line to be aligned with CL.
00:14:20.570 - 00:14:46.160, Speaker A: And I guess otherwise we would have to do this conversion in the engine API or something like that. But what's the output on the consensus layer? Is it also big engine? I think this is insertion, right? It just asserts that the verification is correct.
00:14:47.510 - 00:14:58.420, Speaker B: Yeah, the output is fixed and it is specified in the currently that the output is big engine. So that's why it's inconsistent at the moment.
00:14:59.930 - 00:16:27.610, Speaker A: So I don't think the CL directly calls verify KCG proof, right? It does the blob verification, but never verify KCG proof as far as I know. So it's fine to change that. I mean, it is going to be a bit weird because now the KCG library will have both big indian and little indian interfaces, right? And I guess the question is where should those like if we're using big indian somewhere, little Indian somewhere else, we're going to need to have both interfaces somewhere. And the question is where's the least awkward part for that? And I guess the argument from this El folks was that it is really weird to have this as part of a single pre compile input and output on the El. I guess, yeah. If we can at least put up the PR to propose the change and move the discussion there in the next couple of days, that would be good. There's a 4844 call on Monday if we want to talk about this in more detail, or if there's any sort of objections that come up in the next couple of days.
00:16:27.610 - 00:17:15.580, Speaker A: What seems reasonable to at least draft this spec and take it from there? And I guess if there's no strong objections, just merge it in the EIP. Does anyone want to volunteer to write that PR? So just to understand this one, if the PR gets married, then we definitely need to change the KZG libraries. But does anything else change? What else changes? To do this thing.
00:17:18.190 - 00:17:20.650, Speaker B: Why would you need to change the libraries?
00:17:25.240 - 00:17:36.810, Speaker A: Because my understanding is that we're talking about the input output of verify KZG proof, which is what the library does. Or am I off topic? What is it?
00:17:41.500 - 00:18:14.100, Speaker B: I think that it can respect in a way that, okay, so we do a conversion from begin to little endeavor, but that will be a bit of an overhead. But it can be specked out in this way to emphasize that verify KCG function is littleendian. I mean like accepts little indian inputs. So I know that's one of the ways not necessarily change the libraries, but just change this back. It depends on how do we want to handle.
00:18:24.930 - 00:18:27.410, Speaker A: Okay. Andrew?
00:18:28.710 - 00:19:18.210, Speaker B: Yeah, I was just thinking, so we can add a parameter to that function that passes the inputs, whatever, verify. So basically parameterize the function, whether it's big engine or little engine, and then explicitly set it on the yell side. And I was thinking about the blast library. So blast library can do both, but maybe on top of blast the libraries might have to be, we might need to change the intermediate libraries besides blast KKZG libraries.
00:19:31.350 - 00:19:37.400, Speaker A: And I guess, yeah. So even if we need to modify the libraries a bit, do we still think it's a worthwhile change?
00:19:40.410 - 00:20:34.470, Speaker B: Well, I think it just adds clarity because if you have that little ending parameter as an extra explicit parameter to the function, then there is no ambiguity. You see it? Okay, in my definition of the pre compile, everything is big indian. And it's explicit because right now, as it specified, it's not immediately obvious that there is an inconsistency because that function is defined on the CL side. And without that extra knowledge that everything is Cl is little engine. And just reading the EIP, you don't see it. You don't see the inconsistency with the explicit indian argument. Everything is just more explicit.
00:20:42.750 - 00:22:10.950, Speaker A: Okay, any other thoughts? I think the important part is actually like how are contracts going to use this? And if the smart contracts are going to use big engine in everything else, and they now need to do the big engine to little engine conversion in their contract to call the pre compiler that does this conversion in reverse, then that's kind of bad, I think. So we should make sure that whatever we choose, we choose it into accordance to the people that call the pre compiler. I do agree with that. I think it hasn't been given that much thought. One thing that is going to be interesting here is that obviously the group elements are also, in theory they're large integers. I guess they are kind of compressed according to the BLS standard. But it's also potentially possible that in the future someone would want to do computations with these, especially once we have BLS pre compiles.
00:22:10.950 - 00:23:34.800, Speaker A: But yeah, I agree that for the field elements, it does seem to make much more sense to have them be big in the end. Okay, yeah, I think it's worth at least making the suggestions in the EIP. So Gejender said he can do that and kind of moving the discussion offline and getting more feedback on it before we merge it in. Yeah, does that make sense, everyone? Okay, next up then. So the big thing we discussed over the past two weeks was what to do with SSZ on the EL as well. There was a bunch of issues trying to get the libraries working and some quirks of SSZ on the EL. And then Matt put up a pr to show what it would look like if we just use RLP instead of SSZ for four four four.
00:23:34.800 - 00:24:52.808, Speaker A: And Shawei put up a pr on the CL side as well to kind of show the impact this would have on the CL specs. And there was some recent conversation on that second pr. So I guess I'd be curious to hear from teams what do people feel is the best path forward at this point? Should we move the EL transactions to use RLP? If so, what can we do on the CL specs to minimize the pain of bringing it in? Yeah, and I know like client Potis Danny, you were all chatting about that on the issue in the past few days. If any of you want to share your thoughts. Yeah, that'd be good. Yeah, so there's definitely a desire not to have RLP as an explicit dependency on the expenses layer for more than one reason. I think if we did go this path of RLP is the blob transaction encoding, we would want to do something like Matt's proposal.
00:24:52.808 - 00:25:41.470, Speaker A: But I think Matt's proposal mentioned that to put these version hashes in the blob sidecars instead. I think that what we think makes sense is that they would be some sort of adjunct list inside of the actual block body. We want to have the requirement that the block can be executed on the consensus layer and the execution layer without having the blob sidecars on hand and so we would shove them into the block body. I think that's kind of in line with the proposal. It is a bit more of a breaking change, but this is a breaking change. So I don't think that's a showstopper. Does that make sense? Yes.
00:25:41.470 - 00:25:42.780, Speaker A: Andrew?
00:25:44.640 - 00:26:00.610, Speaker B: Yeah, I was just thinking again, and to my mind the current situation is not too bad because the main concern with the current spec is that.
00:26:02.660 - 00:26:03.036, Speaker A: We.
00:26:03.078 - 00:26:52.908, Speaker B: Are going to have a different SSD format for transactions. So we will have to change the block transactions into something else. But on the other hand we are going to do the same. If now we revert back to using rop for block transactions, then I think Marios told that with that, block transactions will be temporary. And when we figure out how to do SSD properly, we kind of introduce a new type for block transactions, which is proper SSD. But then again, if we are doing that, then having this temporary SSD transaction type of block transactions. Again, it's only an intermediate solution.
00:26:52.908 - 00:27:04.790, Speaker B: And on the upside, we don't have to introduce RLP into ACL and so on. So I kind of currently, my thinking is that the current solution is not too bad. The current spec.
00:27:07.560 - 00:28:15.390, Speaker A: Yeah, I hear you on that. The main question is, okay, if we're going to have to change it, do we want to introduce a very limited additional SSD dependency on the execution layer that might ripple outward into potentially how l two wallets and other things utilize this? So it's the question of if we're not going to do it right, should we limit the dependency addition through this other mechanism? The nice thing about the method on the consensus layer that was proposed by light client is that it actually makes whatever the encoding is of these transactions, whether they change in the future. It's kind of transparent to the consensus layer. Like the consensus layer right now can't really read transactions. And we have this exceptional way to try to read a component of these blob transactions. And by going down this path, it makes it so that the consensus layer doesn't have to read these transactions and that the validations that have to happen occur on the execution layer regardless of the encoding. So it's a nice generic way to do it regardless as well.
00:28:17.760 - 00:28:56.330, Speaker C: I also just want to be very wary of introducing changes that we think are intermediary. We don't really know when we'll be able to make the full change. We don't know what the full change is going to end up looking like. And I think we've seen in past forks where we try to do something that's forward compatible and it ends up not being as forward compatible as we want them to be. So that's kind of why I feel like if we're not going to go full SSD and find the right solution now, it's probably best to just stick with what we know, which is these rop type transactions, and continue thinking about how to do SSZ later.
00:28:57.580 - 00:29:14.210, Speaker A: There's a question in the chat around why don't we think Ethan's, Leila's EIP is doing SSV? Well, which one? Yeah, I don't know. Ethan or Roberto. Do either of you want to give context on that?
00:29:15.300 - 00:29:24.960, Speaker B: I guess he's talking about 64 93 for the signature scheme, or 64 four overall, I'm not sure which one.
00:29:25.110 - 00:29:53.550, Speaker C: Yeah, and I think 64 four is definitely approaching what we want to do with SSE. But it doesn't really seem like there's appetite from the other developers to try and pursue this for Cancun. I generally think that that's what we want to be looking at doing for SSE, but what is in the proposal right now is not there. And specifically, I think taking the hash of a serialized SSZ value is not the idiomatic way of hashing SSZ objects, and it just doesn't make any sense.
00:29:54.640 - 00:29:59.550, Speaker B: So I was speaking 64 93, which I think is padable with 64 four.
00:30:01.920 - 00:30:27.000, Speaker C: Yeah, but it's just for the signature scheme, and so it really provides no value to users. The only value it provides is maybe not having to change the signature scheme in the future, which helps us avoid potentially breaking or making hardware wallets change how they compute the signature hash of the transaction. So it doesn't give us any of the benefits that we are looking for in SSD.
00:30:34.130 - 00:31:22.974, Speaker B: One benefit that it gives you is right now, if you use this flat hash function that just takes the serialization and then hashes that one and you sign that, you don't get any benefit from SSC for this transaction type. But if you use this ideomatic approach of the hash tree root, what you gain from it is that the hardware wallet can, for example, display certain fields. Like you can send just a part of the transaction to the hardware wallet, and it can verify that those parts are actually included in the hash that it is signing. That is not possible with the flat hash. For that, you need the tree structure. Right.
00:31:23.092 - 00:31:26.490, Speaker C: Why would you not send the full transaction to the hardware wallet?
00:31:26.650 - 00:31:33.306, Speaker B: If you are sending the full transaction, then why use SSC for the transaction?
00:31:33.498 - 00:31:42.930, Speaker C: That's what I'm saying. I don't think it really matters that much for the signature hash, because for you to verify the signature hash is like what you expect. You need all the elements anyways.
00:31:43.510 - 00:31:47.810, Speaker B: Can you send the full transaction? Isn't it like prep?
00:31:48.890 - 00:31:55.586, Speaker C: I think that's how all hardware wallets are implemented. Like, if you just send the hash, then you don't actually know what is behind the hash.
00:31:55.778 - 00:32:15.774, Speaker B: No, I mean like the hash plus Merkel proof of individual fields in the transaction so that you can, for example, check the destination, but you don't have to send all the blob contents. I'm not sure if that's useful or not, but it's possible. With SSC, the blob is not hashed, though.
00:32:15.812 - 00:32:24.350, Speaker A: The blob is a KZG commitment. So likely the transaction going into a hardware well would just be the fields plus the KZG commitment, not the blob.
00:32:24.710 - 00:33:06.030, Speaker B: Okay, yeah. Now then, it makes sense to just send the full thing. And I agree that it depends on the overall thing of this union thing or the normalized transaction, this bigger thing. Right. But right now the 6404 has this conversion step where it just takes the mempool transactions in whatever format they are on and then it converts them to a normalized representation when it is building a new block. Right. So for that vip, it doesn't matter how it looks before we could even add like CK commitments to it as part of the conversion.
00:33:06.030 - 00:33:25.790, Speaker B: But yeah, also it was argued before that hardware wallets won't actually be doing SSD blob transactions because this is mostly we're going to use by l two sequencers.
00:33:27.410 - 00:33:34.414, Speaker C: I still think l two sequencers probably would want some sort of hardware solution for signed transactions, even if it doesn't exist yet.
00:33:34.612 - 00:33:35.790, Speaker A: Yeah, agreed.
00:33:37.190 - 00:33:55.080, Speaker C: And I think the code ends up being pretty similar, whether it requires manual input like a ledger, or if it's some sort of other hardware like key mechanism where the key is just stored there and you're sending requests to it online.
00:34:04.250 - 00:34:49.240, Speaker A: Yeah, I still want to make the case that going down the path of what light clients just does make this much more future compatible, such that if and when the transaction type changes on the execution layer, the consensus layer doesn't really have to know about that, that just changes the validation of these version hashes that come in through the engine API. And if we're going to go that path, which I think is more generically future compatible, doing the least invasive thing on the execution layer would be my suggestion, which I think you can make a strong case that RLP is that, but potentially you can make the other case, given that at least SSD exists today in the test.
00:34:56.160 - 00:35:05.340, Speaker B: And could share the link to this proposal on the CL side. Is it in the agenda?
00:35:05.500 - 00:35:34.920, Speaker A: Yes. It's like basically a comment though. Let me. Yeah, it's like the first comment on Xiaoi's pr, basically. And then Danny. And basically, essentially we have to pull out the version hashes from the transactions, which are otherwise opaque to the consensus layer. And so we were doing that by using offsets with s and z.
00:35:34.920 - 00:36:37.490, Speaker A: If it's RLP, we would have to integrate an RLP library to do the recursive lookup. And then we have to make sure that those version hashes are actually the hashes and the commitments that we expect. Instead, we can make those version hashes an explicit component of the block. And when we pass them into the engine API such that the execution layer does the check that those version hashes are actually the ones that are inside of the transactions, and then we can do the higher level check on the consensus layer and not have to worry about the encoding of the otherwise opaque transactions. It sounds convoluted. It's moderately straightforward and prevents having to do any sort of peering into the user transactions on the consensus layer. Andrew, is your hand newly up?
00:36:37.640 - 00:37:02.730, Speaker B: Yeah, I'm just thinking aloud here. Would it be too much to him to just do six four in Cancun? Because it seems to me that it's the direction we want to go. So maybe we should just bite the bullet and implement proper SSD with Vob transactions.
00:37:06.510 - 00:37:09.900, Speaker A: Across all the transaction types, right?
00:37:11.890 - 00:37:12.640, Speaker B: Maybe.
00:37:14.370 - 00:37:24.930, Speaker C: Six four four is changing all the transactions to SSC and removing the Merkel Patricia tree route for the transactions and making an SSC route.
00:37:25.750 - 00:37:34.194, Speaker B: What about legacy transactions? Because plenty of hardware wallets that will never change, right?
00:37:34.312 - 00:37:49.598, Speaker C: It just changes the formatting that goes into the Merkel Patricia try goes into the SSZ route and the signature scheme, the signature hash of the legacy transaction stays the same. So the hardware wallets still sign the legacy signature hash.
00:37:49.794 - 00:37:55.580, Speaker B: Right, I see. Oh, okay. So it seems to me it's the direction we want to go, right?
00:37:55.950 - 00:37:57.100, Speaker C: I think so.
00:38:00.270 - 00:38:57.150, Speaker A: Alexei. It's kind of new idea, but probably it is possible to separate old transactions and new transactions. So we do not need to invent something new for old transactions to become a part of Merkel Tree. Instead we could have them separated and marked as deprecated. So we will have clean SSD solution and LP part which is marked as deprecated, and then we will not allow these legacy transactions in the future. But after some time and we will not mix these parts somehow.
00:38:57.650 - 00:39:17.730, Speaker B: Well, a bad solution because hardware wallets, we cannot make them obsolete. But I think the new EP, like 64, it takes it into account. It just introduces a backwards compatibility layer.
00:39:17.810 - 00:39:23.800, Speaker A: Which is nice, but we will have RLP all the time after that.
00:39:25.450 - 00:39:36.060, Speaker B: That's not a problem. It's always the problem that we have different transaction route between Co and DL. I don't think that having RLP on the l side is a problem.
00:39:39.850 - 00:40:08.400, Speaker C: Also, having two lists of transactions seems to complicate block production a fair bit. You start to get to a point where if you're a MEV builder, you're trying to extract value from transactions that are at the end of the legacy transaction list. And maybe you want to put your transaction at the beginning of the non legacy list. It just doesn't really seem ideal to have two ordering mechanisms now.
00:40:11.810 - 00:40:44.486, Speaker A: Well, you may have. Okay, we have ordered transaction listing block. Right. We just can put one of them into another route and it will be like two local orders. But we will still have the global order, aren't we? Yeah.
00:40:44.508 - 00:40:59.120, Speaker C: I'm not mean maybe we can have a global ordering still. But we talked about this a bit in Austria and it didn't seem like people wanted to have two transaction routes. Maybe I'm forgetting other reasons why.
00:41:08.690 - 00:41:16.514, Speaker D: Matt is the question. To have two transaction routes. One for the block transactions and the other one for the legacy transactions, I think.
00:41:16.552 - 00:41:23.206, Speaker C: Yeah, basically. But not just for blob transactions, for SSZ transactions which happen to only be blob right now.
00:41:23.308 - 00:41:23.670, Speaker A: Right.
00:41:23.740 - 00:41:51.360, Speaker D: So I was in this for a different reason, which I would love. If there's a discussion at some point, it's sort of transition to what is here. But the point is that it would allow to have separate PBS separations so that you can still propose a block built locally and use the builder for block production. And I think this is something that we may want to have.
00:41:59.180 - 00:42:22.610, Speaker A: We can. We can at least find some. Okay. Hardware wallets. Well, we cannot deprecate. Right. It doesn't seem okay that we can have this legacy all the time.
00:42:22.610 - 00:42:32.848, Speaker A: Maybe we could at some point get rid of all transactions.
00:42:33.024 - 00:42:54.990, Speaker C: Yeah, I mean, maybe have a pre compile that accepts a legacy transaction rop and signature instantiate hit transaction there. That's kind of been what has been thought about for totally getting rid of these legacy transactions. But then you start introducing a concept of sort of initiating a transaction with an EVM execution. And there's just like things that haven't really been thought fully through yet.
00:42:59.410 - 00:43:29.830, Speaker A: And I guess if we were to do like a full SSD transition in Cancun, these are all the things we need to figure out before. Correct. Especially if we're saying that the value we get from doing the full SSD transaction now is like, we don't have to change it again. Whereas right now with four four four, we just have a single transaction which we might end up changing.
00:43:33.370 - 00:44:22.398, Speaker C: Yeah, that's kind of what I would hope for in a quote unquote full SSE transition. It's not to say that we have to solve this transaction to bottle situation. I think that having the legacy signatures is generally okay. That's something that I don't think really comes into play as an issue until we start doing zking of all of these transactions in the block. And you might not want to have any RLP, but for today, as long as they're represented as SSD and the transaction route, that is the main thing we're looking for. But I guess there's still questions about should we have separate transaction tree for the blob transaction so that you can produce things locally. I don't think that this necessarily relies on SSD.
00:44:22.398 - 00:44:26.700, Speaker C: It's just like another arc to go down.
00:44:30.510 - 00:45:45.060, Speaker A: And I'm curious, did people feel like it is reasonable to potentially figure all that out for Cancun without basically massively delaying four everything else? I feel like we had this conversation and we keep saying it would massively delay and there are a lot of uncertainties, like a can of worms that will likely take months. Potentially that is less than months now because people have been thinking about it for a while. But I don't know. I do think that we keep asking the same question even though we presumably made a decision on it. Right. And I guess the point we're at is now it seems like the alternative is we move to RLP on 4844. That's also a nontrivial amount of work, but at least it seems like work that we have a really good understanding of.
00:45:49.850 - 00:46:13.230, Speaker C: Yeah, I feel like we're kind of at the point where we need to say we're going to go with RLP or we're going to say, okay, let's just sit down and be a little bit more serious and comb through 6404 and figure out what needs to be changed and what the exact world of SSE transactions is going to look like. Because I don't think a lot of El teams have gone through it deeply.
00:46:16.350 - 00:46:45.970, Speaker A: And yeah, I guess I'm curious. I'd love to hear from other El teams. Do you feel like it is something you have the bandwidth to go through deeply in the next, I don't know, two weeks to a month? And I think if not, then it probably makes sense to either keep what we have now, but that seems like the worst of all worlds, or use the RLP scheme so that at least all the transactions are consistent.
00:46:52.650 - 00:46:59.160, Speaker E: We have capacity to go through it. Okay.
00:46:59.790 - 00:47:01.370, Speaker A: Anyone from Besu?
00:47:08.550 - 00:47:13.490, Speaker B: Yeah, it doesn't seem like too much of a challenge for us, frankly.
00:47:17.210 - 00:47:17.960, Speaker A: Okay.
00:47:22.650 - 00:47:23.960, Speaker B: I'm sorry, say again?
00:47:24.410 - 00:47:28.642, Speaker E: Which part isn't a particular challenge? Converting everything to SSD.
00:47:28.786 - 00:47:29.754, Speaker A: I'm sorry?
00:47:29.952 - 00:47:31.462, Speaker B: No, the RLP adoption.
00:47:31.606 - 00:47:32.300, Speaker E: Okay.
00:47:34.670 - 00:47:45.406, Speaker B: Let me rephrase. Taking the existing SSD implementation as it exists today in 4844 and reverting it to use RLP would not be terribly difficult for us.
00:47:45.508 - 00:48:09.350, Speaker A: Right. Yeah. To be clear, the question though was do you have the bandwidth in the next month or so to think about a wholesale move to SSD. Like if we were to go down that road, I feel like we need to hash out the design in the next month. And so is that realistic?
00:48:10.890 - 00:48:17.830, Speaker B: I think a month is a little less realistic, but six weeks to eight weeks is more realistic.
00:48:21.620 - 00:48:54.920, Speaker A: Yeah, Danny. And just to contextualize, if it's six to eight weeks, then it becomes implementation, which then probably surfaces additional details that weren't previously discovered and has at least a few test nets or at least a couple to kind of go through those iterations. It's okay if that's a decision being made. I just want to contextualize. That doesn't mean and then it's done. Yeah, right.
00:48:54.990 - 00:49:48.274, Speaker B: So I'm actually considering two things going on here. One, the actual research and thinking through that lightkline is suggesting, but also our implementation of SSZ right now is very thin and very specific to a lot of the things that the CL has already adopted. And there are a number of gaps there that need to be revisited and implemented. Things like new data types like optionals, the way that collections and lists are nested, et cetera. So there will be some upstream work that needs to happen on the libraries that we use for SSD. I thought we already had this discussion around whether we tried to do a more thorough SSC conventional and dismissed it as pretty impractical. It seems od we'revisiting.
00:49:48.274 - 00:49:51.254, Speaker B: That seems a little crazy to me, to be honest.
00:49:51.452 - 00:50:22.530, Speaker A: Yeah. But I guess my point is welcome to if we, if we are not willing to revisit that, then to me that's like a pretty strong signal. We should just do RLP and properly take the time to revisit SSD. It seems like the current spec is like the worst of both worlds, and I'm fine with that. But it seemed from the discussion 510 minutes ago that others weren't.
00:50:23.110 - 00:50:39.830, Speaker B: I wouldn't call the current spec the worst of most rules. I think the issue was say we did a 64 93. I think the concern is that we miss something and we just have to introduce a new format anyway, and so all the work we put in SSE might not be as valuable.
00:50:40.650 - 00:50:58.590, Speaker C: I think the biggest concern is introducing a little bit of SSE and then never being able to introduce the rest of it. And then forever on the EL we have this one strange transaction format with strange signature scheme that's totally different than everything else. That to me is like the biggest concern.
00:50:59.410 - 00:51:07.600, Speaker B: I guess I don't see why introducing one SSD type now would make it harder to introduce the SSD in future.
00:51:08.130 - 00:51:19.800, Speaker C: It would make it harder. I'm just saying that there's always a large risk of doing any kind of change to the protocol, and I don't want to try and implement something with the expectation of changing it in the future.
00:51:21.130 - 00:52:02.980, Speaker E: Right. In order to introduce SSD, we need to introduce SSD signature scheme for the block transactions. For example, we need to figure out optionals and a few other stuff if we miss something or some of this work will not be fully compatible with the work needed to basically switch everything out. And all of a sudden we might need to swap out the SSD signature of block transactions to once again. And then it's going to be super messy. I mean yeah, everything's doable, but.
00:52:07.690 - 00:52:08.630, Speaker A: Andrew?
00:52:09.690 - 00:53:16.620, Speaker B: Yeah, I think the current spec, it's okay because the signature, what we sign in blob transactions like how SSZ works, that the first field is this chain id, which will always be one in the little endeavor form, so zero one and all zero bytes so it doesn't clash with RLP. It's okay because of the peculiarity, I don't think that we need to even going forward, I don't see a reason to introduce kind of any complications to signing transactions because presumably all SSZ transactions will start with a chain id, so there will be no clash. But even if we do change that signature, it will be only for blob transactions. Guess no hardware wallets. Not a big deal. So I kind of think it's okay.
00:53:30.240 - 00:54:26.880, Speaker A: It, I don't know, anyone have I guess. Yeah. Does anyone still think we should do the full SSD investigation now? Let's start there. What is full SSD? So basically consider 6404. Consider 6404 for Cancun, which will probably be like a relatively big exploration and likely will delay things a bit. So yeah, I guess I'm curious if anyone still thinks that's like the best path forward. It seems to me like no, but it's hard to gauge.
00:54:26.880 - 00:55:50.510, Speaker A: And then if not, is it better to keep the spec as is, even though it's a bit hacky in a way, or should we just do the work to get it to RLP? So it's a bit more upfront work, but at least we have something that's kind of compatible or aligned with everything else on the EL right now. And I guess it's also fine if we don't make that decision today. But it does feel like we're kind of bottlenecked on this for a lot of 4844 progress. So if we are going to decide to switch everything to RLP, I think it would be slightly better if we made that call today and people get an extra two weeks to work on it. But another option is as well, like sort of fully specking out what light client has in the comments with the changes proposed by Danny. And maybe we can make a call on that on the CL call next week. And POTUS has a comment about this might be received differently on the CL call next week as well.
00:55:54.310 - 00:56:01.080, Speaker C: Sorry, I don't think that was what was proposed though. Right. Like we were talking about the comment that Danny and I mentioned.
00:56:03.850 - 00:56:42.370, Speaker A: Just to be clear, if we went down this path, almost certainly the consensus layer would want to do this path where there's the additional data in the block that's passed into the execution layer payload as an additional validation rather than peeking into the opaque transactions. And I think that's what you were saying, tim. I just want to make that clear. The consensus layer does. So what you're saying is if we go to with the rip row, there will be no RP in the consensus? Yes, it's what he was saying. Yes. Poultis.
00:56:43.590 - 00:57:34.660, Speaker D: Yeah, but Danny just went to talk to the doctor, but I had concerns about that comment too, and I left it in the EIp. The latency of getting the blob, even though I'm not following the testament and it hasn't been tested, I guess at the level of maintenance, the latency of getting the blobs is much larger than the blob. And if we're going to have to wait for the full sidecar to validate. That comment says that we need to validate the KCG commitment against these extra fields in the sidecar before using the engine to send it to the El. Then that means that we're going to be processing blocks much later. We currently can just send to the El the block as soon as it arrives, even if we don't have the block and we're going to lose that ability, it seems to me.
00:57:35.190 - 00:57:51.030, Speaker C: Yeah, I think that Danny said in the comment, I think Danny said in the comment that it would probably be in the block, peer to peer object. And that's what I missed whenever I said sidecar. Not realizing that you have this timing issue.
00:57:51.180 - 00:57:59.980, Speaker D: If you add it to the block, then my whole complaint is not a problem. So if you just add this extra field to the block, I think I'm fine with that.
00:58:00.370 - 00:58:07.770, Speaker A: Okay, so we definitely want the design invariant that a block can be executed without blob sidecars.
00:58:07.930 - 00:58:24.130, Speaker D: Right? So then we're going to go back to this thing of being optimistic later on when the sidecar appears, then we could just. So if the El says that it's all right and we process this thing, we still need to wait for this full sidecar before putting it in portraits.
00:58:26.410 - 00:58:29.222, Speaker A: For any design like that. That's a very important design.
00:58:29.356 - 00:58:30.600, Speaker D: Yeah, that's fine.
00:58:31.930 - 00:59:54.160, Speaker A: And sorry for the misunderstanding when we were talking on the issue. Okay, so assuming that there's not massive opposition on the Cl side to this proposal, if we modify it to put it in the blot, I guess, does it make sense to fully spec that out and then in the next week decide between either doing that and so using RLP on the El SSE, on the CL and having the hash as part of the block that gets passed around, or we keep the spec as is. But there's no world where we do sort of the full SSD transition right now and we give people basically until the CL call to just review that so that a week from now we have the decision, or do people feel strongly enough that we should make the decision on this now? It.
00:59:54.250 - 00:59:56.072, Speaker B: The sooner we can get a decision, the better.
00:59:56.206 - 00:59:56.936, Speaker A: Okay.
00:59:57.118 - 01:00:11.740, Speaker B: I don't feel strongly either way. I'm lightly in favor of what we have with SSd, perhaps extended with the signing wrapper from 6943, but I think it's more important just to commit.
01:00:16.400 - 01:00:19.730, Speaker A: Any. Yeah. Any of the client teams have a strong opinion on.
01:00:22.500 - 01:00:25.344, Speaker B: I think we've heard strong opinions in favor of RLP as well.
01:00:25.382 - 01:00:27.250, Speaker A: Yeah, I don't want to dismiss that.
01:00:29.400 - 01:00:40.020, Speaker B: I think sooner rather than later is. I'm also in agreement, I think, with Roberto, I feel 44 four Devnet six looming.
01:00:43.150 - 01:01:27.560, Speaker A: Okay. And Marius is saying showing strong support for RLP in the chat, I guess. Yeah. Considering this discussion sort of bubbled out of people not being too happy with the current spec. Does anyone oppose if we make the decision to move to RLP now and we can discuss the specifics of how the hash is sent on the CL call, but that we could start working on the EL clients with RLP support basically this week rather than waiting an extra week and potentially get Devnet six a bit quicker with this in.
01:01:32.110 - 01:01:32.860, Speaker E: Yeah.
01:01:35.070 - 01:01:36.860, Speaker B: I would be in favor of that.
01:01:37.390 - 01:02:07.160, Speaker A: You would what? I would be in favor of that. Anyone not in favor? If we're going to go this path, we're going to add the field to the block to avoid the dependency creeping up in the consensus layer. And so I'm okay with committing to that, but committing that with the additional field. Okay. Anyone opposed?
01:02:08.220 - 01:02:14.270, Speaker B: Were there open concerns around adding it to the block? Just wanted to not try fully understand the implications of that.
01:02:17.680 - 01:03:16.644, Speaker A: I don't think so. I mean, it adds a list to the block that the execution layer makes sure that that list is in accordance with the type three transactions, which is an additional validation, but a simple validation. Okay, last chance for objections. Okay, so, yeah, let's do it. Then move forward. To use RLP, add the hash tree root to the blocks, not blobs on the Cl. And use this for Devnet six.
01:03:16.644 - 01:03:53.232, Speaker A: We can get some PR stood up for all of. Sorry, version blob hash to the block, not the hash root. My bad. So add that to the block, move the EL to RLP, not introducing RLP on the CL, and use all of this for Devnet six. Yeah. Light client. You'd already started some of these PR, so if you can kind of make your draft PR into a proper pr, that would be great.
01:03:53.232 - 01:04:01.490, Speaker A: And if someone on the Cl side can extend Shaway's pr, that'd be good as well.
01:04:02.260 - 01:04:11.556, Speaker C: So I can't quite make it a full pr until we get more people to review it, because once I make it a pr, because I'm an author, it'll auto merge. So I want to make sure that.
01:04:11.658 - 01:04:12.564, Speaker A: Okay, fair enough.
01:04:12.682 - 01:04:16.040, Speaker C: Is a draft right now, but it's ready for people to review.
01:04:16.190 - 01:04:17.850, Speaker A: Okay, good to know. Cool.
01:04:18.300 - 01:04:20.952, Speaker C: And whenever there's enough reviews, we'll merge it.
01:04:21.006 - 01:05:04.420, Speaker A: Yeah. And I guess if possible, if people could review it in the next couple of days and we can discuss it on Monday's four four four call, that would be great. Even if there's no objections by then, maybe we merge it. But if there are objections, at least we can discuss those on the call Monday. Um, and Terrence asked a question. Is anyone working on the Cl side? So Xiaoi is the one who opened the pr that client commented on. But I don't know how final or production quality that pr was.
01:05:04.420 - 01:05:30.670, Speaker A: That PR is no longer valid because that PR imports an RLP library to do the decoding rather than the version that we're talking about. It's very relatively simple. Probably myself or Shawi can handle it. I'll ping her and we can chat about it. Okay, cool. Awesome. So, yeah, we're doing RLP for four four four.
01:05:30.670 - 01:06:20.720, Speaker A: And save ourselves another two plus week of discussions on this. Nice. Anything else on this? Okay, next up, Mario, you wanted to chat about an issue that you opened in the EIP. I did not have time to read it before the call, unfortunately. Do you want to give some context on it? Yeah, it's a very simple issue. It's related to the version of hashes on the 44 four. Basically, there's a discrepancy between the interpretation of the implementations.
01:06:20.720 - 01:07:43.564, Speaker A: So in the EIP it mentions that the only reason to consider a block invalid block execution is that the blob version hashes. I mean, the amount of blobs is zero. But some implementations are also checking the blob commitment version of the blob transactions. So basically what I would like to do here is just to add a clarification in the IP, because my understanding is that the execution can proceed and is valid even when we have an incorrect commitment version in any of the block transactions. But most of the implementations right now are rejecting these blocks. Basically, I would like a clarification of this in the AIP for us to get the correct version, I mean, to get the correct idea and tests. Basically, to add a little bit more context, I'm implementing the go.
01:07:43.564 - 01:08:44.110, Speaker A: Ethereum current implementation does not check the version, the byte of the commitment version. Nevermind does check this, and ethereum Vas does check this too, if I understand correctly. And the tests right now do not care about this byte on the execution client, on the execution payloads. Basically that's about it. I don't know if anybody has the time to review the issue and we can just come to an agreement. Whether this is recorded or not, it will be great. Anyone have thoughts or comments? Actually, Gijinder and Alexa have a plus one on always requiring the version byte in the chat.
01:08:44.110 - 01:09:52.820, Speaker A: One thing that I like about requiring the version byte is that no one can update the version without updating the client. And that means that updating the version byte would be a hard fork, which it should be in my opinion. So I'm also requiring. Cool. Anyone against requiring it? Okay, so I guess we can do that. I assume this requires a change to the spec, basically to four four four. And then we can write some tests for it.
01:09:52.820 - 01:10:31.000, Speaker A: Yes, I can open up your just to make a small change, if that's okay. Awesome. Yeah, also. Okay, thanks. Sweet. Anything else on this? This might actually issue, like one of the issues that we saw on definite. So my father most likely sent transactions that did not have the version that it correctly set.
01:10:31.000 - 01:11:30.920, Speaker A: That's probably some of the breakage that we've seen. Got it. Anything else? Okay, if not. Okay, so this next one. Peter, I saw you posting about the KZG libraries a lot on the discord in the past week or two. I just wanted to check in on that and see is there anything on the library side that we should be doing or working on to make sure that they're in a good spot for us to use in production. If there's nothing new or anything you want to bring up, we can go over it, but I just wanted to make sure we have the chance to check it on it.
01:11:32.490 - 01:12:31.222, Speaker E: Yeah, so essentially with the KCG libraries, there are certain quirks that are kind of unexpected. The CKZG library, specifically the BLST sublibrary of it, has an interesting aspect. It has some optimizations baked in that require fairly modern cpus, and it will pretty much crash on an older cpu. Older cpu, basically any CI server will crash. By older cpu I don't mean ancient cpu, I mean just not the newest and greatest. And the problem with them is that they have a built flag through which you can control whether to enable this optimization or disable it. But the author refuses to implement runtime detection.
01:12:31.222 - 01:13:43.034, Speaker E: I mean, it would be probably trivial to do it, but for whatever philosophical reason, the author just doesn't want to do it. And this makes it particularly hard to integrate into other clients. I mean, pretty much every consensus client had already issues with it, as far as I know, and they had to. I mean, even if you look at Lighthouse, they have a modern build and a portable build, which is basically boils down to the CBLSt library. So screwy details. Now the other annoying problem is that since you require an environment variable set during build time, it doesn't really play nice with using it as a dependency. And for example, because in Go build, the Go build system is very simple, you just type go build and all of a sudden if you want to control the C library and make it portable, then you cannot do that.
01:13:43.034 - 01:14:28.880, Speaker E: With the default Go command you have to set an environment variable, which means that anybody building gap all of a sudden has to know about this lame environment variable, and if they don't know about it, then there's a fairly high chance that their build will crash at some point. So this is one of the really annoying issues with the CKDG library, with the GokzG library, again, not gokzg itself, rather the upstream, I don't know, one of the consensus crypto libraries. It makes some strange optimizations. It does a lot of stuff concurrently, and this concurrency cannot really be controlled, which is a bit of a strange decision for a crypto library because.
01:14:31.090 - 01:14:31.358, Speaker A: For.
01:14:31.364 - 01:14:40.050, Speaker E: Example, it can verify a blob concurrently on many threads, and on default it will verify it on all your threads.
01:14:42.230 - 01:14:42.706, Speaker A: For sure.
01:14:42.728 - 01:15:29.170, Speaker E: It's three times as faster verifying it on 20 threads. But then the question is that feels a bit of a wonky trade off to be three times faster on 20 threads that load. And there's really no way to control this threading. And I would much rather have a crypto library be single, threaded and dumb. And let me do the concurrency by verifying multiple blobs on different threads rather than the crypto library internally deciding on what to do. I did look into it, and it's a fairly ugly internal design decision. So I guess again, with some flags you could maybe, and with some modifications completely nuke it out somehow.
01:15:29.170 - 01:16:31.234, Speaker E: It's a bit wonky. Anywho, so these were my two main concerns with the two crypto libraries, on top of which the KCG stuff is built. Just for the record, with gas, what we did is we integrated both libraries, both GokZG and CKCG. We default to GokZG. And with regard to CKCG, what happens is that if you build gas via make or the docker images, or our own internal build scripts, or pretty much almost anything except the classical Go build command, then get will add the flag to enable CKZG and also add a flag to make it portable. So that's the default. All of our official builds will support both KZG GokZG and CKDJ.
01:16:31.234 - 01:17:41.600, Speaker E: You can switch between them with a flag, but anyone building get by a simple go build or go install command will not have CKCG available. So that was kind of our best idea on how to solve it. And for us it was. In all honesty, we have a fairly low level of trust in both KCG libraries and the underlying crypto libraries, because it's kind of new and we have an equally low trust in ourselves to fix any issues if they happen. And that's why we went with this approach of including both of them and making it switchable by a flag, because for sure we prefer the Go version. But if there is, let's say, an issue, a consensus issue between them, then we want to be able to have users switch immediately without having to figure out, okay, how do we swap out the library, how do we enable it, how do we integrate it, et cetera. So yeah, that's kind of the TLDR of the KZG saga that we had on the gas side.
01:17:43.890 - 01:18:34.990, Speaker A: Thanks for sharing. I noticed there was a couple of comments in the chats during this, but anyone have thoughts or anything we'd like to add there? Yeah, we can solve it by runtime selection of the library, but it would be better if the library could support runtime flag instead inside it. So we need to know whether we need to invest in this additional build and run time check. Or we could wait for solution in BLST.
01:18:47.720 - 01:19:37.680, Speaker D: Yeah, so as one of the ones that complained about random selection on BLST, the problem is the way this is coded. It's not that you have one routine that uses ADX and Mulex and another routine that doesn't. There are just bits and pieces that are added or not by pre processor instructions. So I think the only feasible way of changing this without big changes within the library is what the people of Nimbus were going to do, which is just disassembly with the flag. Disassembly without the flag, have the full binary ones and just choose at the beginning. So have a much larger library with two copies of the full routines with one flag and the other flag, and then choose on runtime. That would be their fork.
01:19:37.680 - 01:20:33.860, Speaker D: And I don't see any other easy way of doing making this change in BLST. So I don't expect the author to actually accept a change towards this anytime soon. And this is really annoying. This is something that really annoys me because it's something that the Ethereum foundation paid for this library, and every single client complained before the launch of the beacon chain, and this wasn't solved. And now that the execution clients are also complaining that it's getting some traction again. Yeah, so this is something that is incredibly annoying on the cl side, at the point that prism is shipping this binary by default, which is portable, because we did see some crashes. A lot of validators are being run on AWS currently, and they do crash because they run some old z and cpus.
01:20:33.860 - 01:21:39.530, Speaker D: The other comment I wanted to make, which is something that makes me uncomfortable with the status of the crypto libraries, is that they are still changing quite a bit, and in things that are kind of non trivial. On CKCG I saw Marius that was complaining about the way of loading the trusted setup. And on the GOKCG side it's something similar, like the version that we were trying did not accept the trusted setup that is posted already in the consensus player spec repo and the change that fixed this is not long ago, which means that if you update the crypto library to use that change, then you need to update Geth and get hasn't updated to this. It's becoming very problematic with all of the relations of the different clients with the crypto libraries to even start testing this. So I think we're still at a prototyping stage. I would say that we're very far away from developing production code in that area.
01:21:43.140 - 01:21:46.630, Speaker A: Thanks. Silvius, was your hand up about this as.
01:21:48.440 - 01:23:07.550, Speaker F: Yes. So a few notes on this extension thing where you need to compile the thing basically with either for new cpus, either for old cpus. I think we also discussed internally interesting approach, but it's probably similar with an imbus one where we just in the runtime, essentially we compile both versions of the BSD library with enable it extension and we disable it extension. And then on the runtime we check what cpu is running and if it's a new cpu we use one version and bit enabled extension. And if it's running with old cpu then we are taking the old library. I think this is ugly approach, but at least you would not need to have a separate build of your find. But this is probably similar with what Nimbus was thinking, isn't it? The approach that POTUS was talking before.
01:23:07.550 - 01:25:17.748, Speaker F: This is one way question on the parallelization. So for Skzg we have even much more, let's say not much more, but more parallelized version where we make verification faster. But the problem actually is not about using a lot, of course, because if it's available then why not to use it? But the problem is that if it start to compete with something else that client is doing, something else important that client is doing. So my question is for original person who raised this question, how do you deal with this elsewhere? You should have some hot computation that you already paralyzed and how do you prioritize it? If you could tell us some examples of that, that would help to make a solution that hopefully some built in solution in the crypto library that helps essentially that solves this competing problem. One way of we are thinking about is just to lower the thread priority, essentially. So the OS operating system scheduler will just make it less competing with other things that client is doing. If you could tell, how do you solve that currently? Because you talked about that you don't have control in this approach with crypto library.
01:25:17.748 - 01:25:20.250, Speaker F: But if you had control, how do you do that?
01:25:22.060 - 01:26:00.790, Speaker E: I think one of the problems is that this is not solvable at the crypto library level specifically because for example, if I'm doing a block processing, then I don't want it to be lower priorities. And for example, if a new block arrived, I want it to actually have the highest priority during processing. Now if on the other hand, the blob is being verified in the transaction pool, then for example, I would like to have it a lower priority. And this is something that from the library, from within, you cannot really guess which blob is important now and which blob is not important.
01:26:03.420 - 01:27:04.650, Speaker F: Yeah, I think a pretty easy approach would be, I mean, not ideal, but something that would be easy to achieve is just to set the thread priority whenever you call the verification function. So if you need that as fast as possible, then you say that the crypto library should spin threads for this particular verification with normal or high priority. And otherwise, if that's not important, you just say for crypto library that this is not important verification, and it just spins threads with lower priority so it doesn't compete with things with everything else that runs on the default priority at your client. Does this make sense?
01:27:10.320 - 01:27:12.110, Speaker E: Yeah, sure it does make sense.
01:27:16.960 - 01:27:31.750, Speaker F: I mean, there is no easy solution for that, but there are some solutions that probably should work most of the time. And the one I just described, it could be the solution for this problem.
01:27:40.420 - 01:28:03.210, Speaker A: Sorry to interrupt. Just to make sure I understood. You want to have a specific scheduling threat scheduling policy for every use case, and it will have, because it's dependent on what os you're running on. You will have to have an os specific library for every platform you want to support. Basically.
01:28:06.940 - 01:28:39.276, Speaker F: We checked on the three main platforms, which is like Linux, Windows and Ostex. And usually there is a way for these three operating systems, there is a way to change the priority of the thread. This pretty much is enough to implement this idea. Maybe for other platforms which we were not interested in, maybe it's even not possible to change the priority. So then yeah, this doesn't work on those platforms.
01:28:39.468 - 01:28:55.880, Speaker E: So I don't think that will work. For example in Go, because go doesn't have real threads. Go uses very few real threads and multiplexes, all the go routines on top. And I don't think you will have a way to lower the priority of a thread, of a go routine.
01:28:59.820 - 01:29:25.344, Speaker F: I don't know actually how it works. If that's only go environment, then yes, but if that's something external, something like if you call CKGGD or rust KZG from go, then it definitely should be able to use system threads because it's like c extension or something outside of.
01:29:25.382 - 01:30:09.840, Speaker E: The go thing that would be super expensive because spinning up threads is at least on Linux it's a bit faster spinning up threads, but for example on Windows it is extremely slow on the order of milliseconds. So if your crypto library will spend ten milliseconds or 50 milliseconds spinning up threads to do the half millisecond work, then that doesn't really fly. So essentially this also highlights the issue that once the crypto library is a very low level stuff, and it tries to do threading, which is a very high level stuff, and then you hit all these kind of strange issues with how to solve.
01:30:12.740 - 01:30:13.056, Speaker A: It.
01:30:13.078 - 01:30:16.640, Speaker E: Just feels that should not touch threading.
01:30:17.220 - 01:30:17.970, Speaker A: Yeah.
01:30:20.840 - 01:30:33.190, Speaker F: Just last note, I think we just really need to think, do we need like a top performance or if we can go on a single thread and everyone is happy with that.
01:30:34.680 - 01:31:26.010, Speaker A: So, I mean, I just want to chime in here. I've been trying to get in for a while, but I think the only thing where threading even makes sense is for blob construction. And I think we're getting way ahead ourselves by trying to optimize this now. I think it's perfectly fine right now to just completely ignore this, because the kind of roll ups who are going to do blob construction, first, it's not a lot of time, it's still talking tens of milliseconds here. And second, they can start optimizing that. They don't even have to use a standard build for that. So I feel like trying to optimize this now is just like, I would just say like we don't need threading at the moment and can just ignore it and maybe can have it somewhere in the code and they can have a special build or something.
01:31:29.820 - 01:31:48.010, Speaker F: Is there a decision pretty close to finality? How much of blobs there will be in block on the main net? I mean, is it like four or 16?
01:31:48.170 - 01:31:50.640, Speaker A: What does that have to do with it?
01:31:51.190 - 01:31:54.130, Speaker F: Well, on cl side, for verification.
01:31:54.950 - 01:32:03.620, Speaker A: For verification. Why do we need parallelism? We're talking about a few milliseconds, but.
01:32:07.050 - 01:32:43.986, Speaker F: There are a few things. There are slow computers, like slow nodes, where every millisecond just multiplies by, by the slowness of the computer. This is one thing. And there are some use cases where block verification is important, like every millisecond is important, like in MaV or whatever else domain. So I actually don't agree that you can just ignore the fact that those milliseconds adds up together. And if you have 16, I mean.
01:32:44.008 - 01:32:51.220, Speaker A: This is on CLR, right? It can be done in parallel to execution layer verification anyway. So how much does it actually add?
01:32:53.670 - 01:32:55.250, Speaker F: Sorry, could you repeat?
01:32:55.830 - 01:33:09.580, Speaker A: This is in parallel to execution layer. Like this is a Cl thing. So it would be in parallel to what the execution layer does anyway. So does it actually do block verification any faster if you parallelize it in Cl?
01:33:13.150 - 01:33:16.490, Speaker F: Sure. And why it shouldn't be faster?
01:33:16.830 - 01:33:25.066, Speaker A: Because I would assume that El is the bottleneck anyway. So if you add a few milliseconds.
01:33:25.178 - 01:33:40.786, Speaker F: On the, you're saying, okay, so your point is that Cl will always be significantly faster than El, so it doesn't matter that if it's lower on the Cl site?
01:33:40.888 - 01:33:50.870, Speaker A: Yeah, I mean, this would be my guess right now. Again, I also feel we're getting ahead of ourselves optimizing this for someone who's doing mev extraction.
01:33:54.970 - 01:34:21.200, Speaker F: Okay, I'm still with my opinion that we should do as fast as possible. But yeah, I get your point that if El is much slower than Cl, then technically we shouldn't have the problem here because Cl will be able to complete even on a single thread verification, because El is taking much more.
01:34:23.430 - 01:34:59.550, Speaker A: I mean, the Cl can parallelize other tasks in parallel to. So like, I would assume that signature verification is much more work, but you can easily, like, the Cl can simply thread this, the blob verification, the signature verification. Again, this feels like absolutely optimizing the blob verification to like 16 threads just seems crazy to me. I don't know, that seems like an insane, premature optimization that just makes things more complicated.
01:35:00.130 - 01:35:20.190, Speaker F: Well, I think that the key thing on Cl is that the blobs will come asynchronously, not at once. Naturally, the proper implementation is just to do verification whenever you get the block immediately.
01:35:20.270 - 01:35:23.540, Speaker A: So that would be an even stronger reason not to parallelize it.
01:35:24.070 - 01:35:27.278, Speaker F: No, I think if you have a lot of.
01:35:27.304 - 01:35:29.000, Speaker A: So why are we discussing this?
01:35:34.170 - 01:35:43.900, Speaker F: That was a bit of topic to the original questionnaire, original person.
01:35:44.270 - 01:36:27.670, Speaker A: So we're already like 5 minutes over know. I know there's a telegram channel discussing the libraries. Is that the best place to just. Actually, the telegram channel is like for CKZG. But is there somewhere that we should continue this conversation? I mean, we can use the, like, we have a bunch of 4844 channels, so we can use those. Yeah. I don't know if there's any final comments before we move this offline.
01:36:27.670 - 01:37:04.882, Speaker A: Okay, let's continue. Alexa, do you mean Ssz chat or KZG chat? No, it's, by the way, now, always wanted to have discord. We have a transaction type channel, which I think we can probably reuse for that. Yeah, we have a type transaction channel which we used for that in the past. So we can just keep using. Yeah. Okay.
01:37:04.882 - 01:38:05.960, Speaker A: So we're already a bit over time. I know we had Zach and Charles who wanted to give a bit of context on their eips. We can take two ish minutes on each of them if folks can stick around. If either of you, Zach or Charles, prefer to do this on the other call or on the next call with a bit more time, we can do that as well. But if not, Zach, do you want to go first and give some context? Hey, yeah. Hey, Zach Cole coming at you live from left field. Just coming to discuss Eip 69 69, and I guess out of respect for time, happy to pump this to the next call, but is this the best forum to discuss it, assuming that this EIP targets specifically l two implementation? With that in mind, there's a caveat that it does involve additional state transitions and may have implications on supply as well, since it's a modification of 1559.
01:38:05.960 - 01:38:36.050, Speaker A: So, yeah, Dano has some comment about this on the chat, but l two s can do things without Alcordev's permission or away. If people have technical feedback on the EIP, it probably makes sense to share it. And I know there's already a bit on eth magicians. Yeah. Anyone else have thoughts on either the EIp or whether we should even be discussing it? Daniel.
01:38:37.270 - 01:39:03.958, Speaker B: So to add on to that, about the jurisdiction, I mean, if we say something and the lt's ignore us, there is like no side effect versus if something for Mainnet, something decided on this call and say, get decides not to do it, or nether nonmine decides not to do it. Well, they're out of sync with Mainnet, so there's a bit of jurisdiction because we define what standard. But l two s can do what they want. So that's my concern, is that there's no stick, just carrot.
01:39:04.054 - 01:39:28.340, Speaker A: Yeah. I think maybe one difference is, and this is maybe not the best eip for this, but you could imagine l two s wanting to do something before main net and wanting some sort of sanity check of if this goes well on an l two, does l one want to do this? So that might be a case where I can make sense to get the feedback earlier rather than later.
01:39:29.510 - 01:39:40.710, Speaker B: And I do see that this ip might have that along with other ones, like 4337 and maybe in the future evm changes. So that is a question we need to resolve beyond just this Eip.
01:39:44.650 - 01:39:46.760, Speaker A: William, you have your hand up as well.
01:39:47.930 - 01:40:47.430, Speaker G: Yeah, I wanted to add a point somewhere to the one you just made, Tim. So, I mean, full disclosure, I work at Polygon. A lot of times there's a certain level of coordination that I feel ACD should be open to. Though, I mean, there's an opinion definitely open to debate. I understand your point, Dano, and I think that there needs to be some kind of resolution to that. But looking at an EIP like 30 74 or other ones that involve actual EPM state changes, I think that there's a lot of value in a certain level of coordination effort going through ACD, even if it's not necessarily going to land on Ethereum's EVM right now, it is the best rallying point around EVM engineering. And like Tim mentioned, also just being able to get some level of sanity check that things potentially make sense, or also to be able to send signal I think is valuable.
01:40:47.430 - 01:41:15.620, Speaker G: I think there's also a value to that for Ethereum itself, which is that l two s have much more of a potential or Alt L1s for experimenting with things that Ethereum probably should not be experimenting with. But if are successful on l two s can kind of trickle back up to Ethereum. So I think also kind of in that cycle there's at least some value there, but that's just my side of things.
01:41:16.710 - 01:41:22.470, Speaker A: Thanks for sharing and scar last comment on this and then we'll move on to Charles.
01:41:24.410 - 01:41:29.146, Speaker E: Yeah, I just wanted to bear briefly say, because I've talked to some people about similar ideas in the.
01:41:29.168 - 01:41:31.354, Speaker A: Past, and while I do think it.
01:41:31.392 - 01:41:33.834, Speaker E: Makes sense to have some sort of.
01:41:33.872 - 01:41:38.806, Speaker A: Coordination between all EVM chains, including layer.
01:41:38.838 - 01:41:52.800, Speaker E: Twos, I'm not so sure that at least in the long run, ACD would be a good forum because we can never actually promise to bring things on mainet right. It could always be that even if we think we might do it, then we later decide not to do it.
01:41:53.890 - 01:41:55.058, Speaker A: I think in the long run it.
01:41:55.064 - 01:42:03.326, Speaker E: Makes sense to have these kind of processes be separate, although I wouldn't be necessarily opposed to use ACD in some form for bootstrapping a process like that.
01:42:03.368 - 01:42:45.090, Speaker A: Once there's sufficient interest. But yeah, should continue this discussion from now. Yeah, and so I guess for the specific EIP we can use eth magicians, and for this meta discussion I agree, this is probably something we need to figure out. I'm not quite sure what's the best way to have that conversation, but it's been on my mind as well. I would definitely want to avoid a new permanent call if we can, for as long as possible. Yeah, but let's see. Okay, yeah, please act I'm sorry, just one more thing.
01:42:45.090 - 01:43:12.860, Speaker A: Is there any way that we could just get this EIP merged so we can alleviate any bottlenecks from the active working group. Sorry if that's not the best place to ask. So I know there was some issue with the numbering. I think it's probably easier to resolve those on GitHub or discord and not open that can of worms here. Yeah, sounds fair. Thank you. Yeah, and sorry for the delay in sort of jamming this right at the end.
01:43:12.860 - 01:43:18.010, Speaker A: Okay, no problem. Charles.
01:43:21.250 - 01:43:50.710, Speaker F: Hey, I know we're really over time now. I wanted to bring up again M copy and secondary pay, which were brought up last time. And I think at the time Tim said that to give everybody a bit of time to review it. And I was wondering, I'm happy to punt next week, but I want to keep asking if we can include at least M copy and also pay in Cancun.
01:43:53.530 - 01:44:46.294, Speaker A: So I guess we definitely shouldn't be making this decision 15 minutes over time. So there was some support for M copy last time. Seems like there is in the chat still. And then payoff code is a bit more controversial, it seems. Maybe what we can do is put M copy on the agenda for. And we could put pay as well put it on the agenda to make a decision about on the next call. So like two weeks from now we have an idea and generally as well try to figure out if there's other smaller eips that people want to see in because I know like 4788 or the BLS precompiles were also kind of wanted.
01:44:46.294 - 01:44:57.610, Speaker A: So if client teams can have an idea of which small eips are important and would want to be included in Cancun by the next call, we can have that conversation.
01:44:59.790 - 01:45:33.400, Speaker F: Okay, great. I do want to point out that EVM one did merge support for M copy. I think they're also working on state tests and there was also an open pull request on gas for M copy, which I think Gumbo and light have already reviewed. But if one of the maintainers wants to at least not necessarily do a full review, but at least take a look at it so they can get an idea of how nice m copy is and how useful it is. And it's not too hard. So I think that might help a little bit.
01:45:35.630 - 01:45:36.090, Speaker A: Awesome.
01:45:36.160 - 01:45:40.140, Speaker F: But anyway, yeah, we can continue next call.
01:45:41.470 - 01:46:06.100, Speaker A: Cool. I think this is a good spot to end. We're already 15 minutes over. Anything final anyone wants to bring up before we wrap up? Okay, well, thanks everyone. Talk to you all on the Cl call next week. Bye. Sa.
