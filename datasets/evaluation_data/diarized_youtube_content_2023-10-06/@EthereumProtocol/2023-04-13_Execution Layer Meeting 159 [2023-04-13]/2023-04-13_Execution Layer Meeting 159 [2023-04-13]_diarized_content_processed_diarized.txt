00:02:24.460 - 00:02:58.892, Speaker A: We are live. Acde one five nine posted the agenda in the chat, obviously to chat about is Shanghai Chappella, then some cancun topics, and then payload id conversation. That's already kind of happened quite a lot in the agenda itself. Yeah. And anything else that comes up. But I guess to start. Yeah, great work everyone on Chappella, it worked.
00:02:58.892 - 00:03:25.720, Speaker A: The network transitioned successfully. There were a couple issues, like minor issues that we saw. We can talk about them, but I think overall, yeah, this was very smooth and yeah, great work. Great work everyone. Does anyone want to give a recap? Danny, I see you have three comments in a row in the chat, so do you want to share your perspective?
00:03:26.300 - 00:04:02.884, Speaker B: Those were troll comments. Yeah, I mean it went really well. I think one of the most interesting things that I observed is a number of validators were attesting but not proposing. And that's always kind of an interesting thing because it means their nodes are up and they're following the chain and they're kind of voting on things. But then this other component, the block proposal, is borked. Sometimes that can be an issue with your local El, but given that we have the Mav boost dependency, that's kind of also one of the potential targets there. And the prism guys can talk a bit more about that.
00:04:02.884 - 00:05:04.020, Speaker B: But it was a Mav boost issue, one that I think we would hope to see covered in client tests as well as in hive tests. So we're kind of circling back to figure out that additionally lighthouse had some cpu high cpu usage and that was resolved with a hot fix, I believe. Not yet on their staple branch. Other than that, I believe, and maybe somebody can tell me if this is exactly what happened. We had a higher number of missed slots right at the fork. We saw something like 40,000 bls change operations broadcast because those weren't broadcast prior to the fork boundary and saw high cpu loads. So my current interpretation of the highest miss slot boundary right there is just essentially the gossip with honest messages dossing some of the nodes around that boundary.
00:05:04.020 - 00:05:11.290, Speaker B: So those are the main points I saw while I was paying attention last night. Anyone else?
00:05:13.020 - 00:06:07.224, Speaker C: I can add some color to the prism issue. So this was prism block that we missed to include the bos to exact changes when we submit a blind block to the relayer, meaning that the relayer will fail the signature verification because of this issue. This issue happened because of we missed a unit test case, unfortunately. Also we probably did test this in the production. By production I mean testnet. But given that there was so few prism validator that's running with builder, I think probably 1%. We completely missed that.
00:06:07.224 - 00:06:49.484, Speaker C: So we're working on postmortem right now, so that is to be shared by the end of the week. And what went well is that the circuit breaker actually ended up saving us because of prism configured to be matched missing five slots per epoch. So if we see more than five slots per epoch that's missed, we will default to local building that ended up saving us. I believe it wasn't for that. We'll probably see a lot more missed block over epoch. And yeah, thank you for Chris and other relay for coming together and helping last minute. So they ended up banning prism validators by forcing them to use local to use local builders.
00:06:49.484 - 00:06:59.020, Speaker C: It wasn't for that. We probably would also see more missed block along the way. So yeah, we're working on postmortem and then we will reduce that further.
00:07:00.800 - 00:07:14.512, Speaker A: Just one question just to make sure I understand. So when you say they banned prism validators, are you saying the flashbots relay basically refused to accept Prism validators connecting to it? Is that right?
00:07:14.646 - 00:07:15.330, Speaker D: Right.
00:07:16.180 - 00:07:35.590, Speaker C: So they're able to see through the user agent string, like who is requesting the header to sign. And if they see there was a prison validator, the relayer will reject that validator's request. And I believe all the relayers ended up doing this except for one of the relay area.
00:07:36.520 - 00:07:40.170, Speaker A: And Chris, as you have your hands up, I don't know if you want to add more color on this.
00:07:40.860 - 00:08:31.944, Speaker E: Yeah, a little bit more color. We have been watching the fork live with the ultrasound guys and with people from prison and from lighthouse, and we started investigating the issues together, collecting the data. And once it became clear that it's a signature error by prison, we could implement a very simple check on the user agent that there really does not return any bits for that user agent anymore. And luckily we were able to identify prism specifically by the user agent because they do send a particular user agent already. And we rolled out this patch analysis to the other relays that were basically on standby as well. Also shout out here to Mike from ultrasound. We have been in this together and basically all the relays rolled out this patch around 01:00 a.m..
00:08:31.944 - 00:08:52.272, Speaker E: UTC. Let me send the patch here. By the way, it's a very simple patch. Notably we couldn't get hold of any of the agnostic gnosis relay people. It was impossible. We tried like a ton of ways, including calling them. They only disabled these at 07:00 a.m..
00:08:52.272 - 00:09:23.412, Speaker E: UTC. So there were additional missed slots afterwards by proposal connected to gnosis here. In total I have some numbers. The flashboards really saw this invalid signature 43 times. So that would be 43 missed slots through the flashboards relay through diagnosis relay 57 times the signature and missed lots. And the ultrasound relay counted this 21 times. So this is altogether 121 fruity.
00:09:23.412 - 00:09:43.760, Speaker E: Three big relays. All the other relays the variance then by and I applied the patch immediately. So I think overall there were like a two hour window, roughly two and a half hour window where the prism request would fail on the get pillow call and make the proposals miss the slot.
00:09:46.500 - 00:10:06.090, Speaker A: Got it. Thanks for sharing. I don't know who would be the right person to write the post mortem on that side. Is that going to be part of the prism one or like Mevb postmortem? Just I think it'd be interesting to capture this information somewhere more permanent than like the call.
00:10:09.220 - 00:10:28.420, Speaker C: Yeah, we are working on postmortem right now. I think that should cover everything. It's also worth mentioning we issue a release yesterday that's 4.2. So if you want to run mat boost with your validator then you should be running that release.
00:10:29.560 - 00:10:30.630, Speaker B: Got it.
00:10:32.360 - 00:10:58.560, Speaker E: Also, to round this out, either we can contribute our data and timestamps to the prison postmortem, or we do another postmortem and drop this somewhere else. Both seems good. Just finally, any proposal that uses the new prism release, this will use a new user agent and this will automatically work again with the builder network.
00:11:02.260 - 00:11:24.696, Speaker A: Awesome. Thanks. Yeah, we can chat online about where to add the info, but sure they'll end up somewhere. Anyone else have just thoughts or comments about the prism medboost situation specifically? Yeah, I can add for the hive test here.
00:11:24.798 - 00:11:40.872, Speaker F: So basically the problem with Hive was that it's using a custom relayer. So this customer layer was the only purpose is just to generate invalid payloads and check that they are well caught by the consensus client.
00:11:41.016 - 00:11:43.180, Speaker D: But the problem was that for the.
00:11:43.330 - 00:12:00.630, Speaker F: Happy path it was not verifying the signatures, so that's why it was missed. I think this can be easily fixed in an upcoming new test for Hive and we can catch this issue in the future. Definitely.
00:12:02.680 - 00:12:03.830, Speaker B: Nice. Thanks.
00:12:04.200 - 00:12:34.830, Speaker G: Just a little more color. We also probably should have caught it on a shadow fork or one of the other testnets, but at least with the last shadow fork because we had the official builders and relays on there. But we were facing a different issue with signature verification and by the time that issue was patched, the queue was empty so we wouldn't have triggered this issue anymore. So yeah, I guess in the future we also have to try a few more scenarios with the relays on that.
00:12:35.360 - 00:12:47.170, Speaker A: Right. It's like if we can have the relays in earlier shadow forks, then maybe we have more shadow forks where they're there and we end up hitting this bug on one of them.
00:12:49.000 - 00:12:50.070, Speaker G: Yeah, exactly.
00:12:51.080 - 00:13:10.680, Speaker E: Just one note here, we did have the shadow forks and this particular circumstance was just not occurring, I think, on the main edge shadow fork. So maybe there is additional steps or protocols we can add to increase the test coverage on shadowforks to catch these types of edge cases.
00:13:12.300 - 00:13:42.150, Speaker A: Yeah, and I think they're probably custom to each upgrade, like the withdrawal or like the BLS change queue will probably never be as big as it was on the first block of this fork. But then thinking about what will be unique and kind of stressing the network when we have the next fork and trying to run through that specific scenario over and over is really important.
00:13:46.600 - 00:13:59.530, Speaker B: Real quick, Potez, you said in the chat that you think that the high missed proposals, even on that fork boundary was just the prism issue and not the doS. Do you have evidence of that?
00:14:00.300 - 00:14:02.944, Speaker H: Oh no, but it's just a matter of statistics.
00:14:03.012 - 00:14:03.630, Speaker A: Right.
00:14:05.040 - 00:14:33.312, Speaker H: Most of the network, about like 80% are sending their blocks to map boost. And apparently according to block print, 40% of those are prism. And every prison node would be failing at the fork if they send to map boost. And that more or less matches with the number of missed blocks per epoch that we had after the fork. So I think it's safe to say that most of the missed blocks were due to this bug.
00:14:33.456 - 00:14:46.250, Speaker B: Yeah. And I might be fooled by randomness. Just when I was looking at the heat map right at the boundary, it was like a very high density and then was less dense after.
00:14:46.940 - 00:14:51.512, Speaker H: Oh yeah, that's because it triggered the fail save.
00:14:51.566 - 00:14:51.928, Speaker A: Right.
00:14:52.014 - 00:15:08.320, Speaker H: So it started failing quickly, and then after that, proposals would start falling back to local execution. Also, the relayers reacted very fast. They banned us very quickly. I think it took less than an hour to be already banned.
00:15:12.340 - 00:15:13.330, Speaker B: Got it.
00:15:16.340 - 00:16:01.500, Speaker H: But I think one lesson that we all should be learning is that at least I myself, when I'm coding, I oftentimes think on the happy path, the happy path, which is a local execution. And even our unit tests are tested against local execution. And we only have sort of like end to end tests where we test with a builder and we deploy to testnets or even production with the builder. But I think we should set up a way in that all of our coding is oriented towards testing the builder because most of our blocks are going through the builder. This is a ridiculous bug that should not have happened at all. There's many places where this should have been tested and all of them failed at the same time. This is just impressive.
00:16:01.500 - 00:16:29.320, Speaker H: On Gurley, for example, we went back and checked at the day of the fork and we only had like three missed proposals out of all of those that were due to this bug, and they were just lost in random noise, which this is something that hive should have tested and we should have tested on a unit test and we failed to do it. I guess it's just because we're set up to thinking not on the builder, but thinking on the happy local execution path.
00:16:31.020 - 00:16:45.328, Speaker C: We also don't run enough validator due map boost for our testnet. We only run like 10%, which means that it's about 1% of the total share of the network, so it was.
00:16:45.414 - 00:16:46.930, Speaker B: Hard to catch this.
00:16:47.860 - 00:17:13.560, Speaker H: Yeah, we should definitely have a network testnet which is more or less stable so that we can actually test our production in which at least most blocks go through the builder so that we see these kind of things. We could not have seen this on Gurley where we only had like 1% of the network instead of the 40% and everyone sending through Metpus as in mainnet.
00:17:23.590 - 00:18:02.780, Speaker A: Thanks. Any other comments or thoughts on the prism bug specifically? Okay, I saw. I believe Lighthouse also put out a new release. I don't know if you want to quickly walk through. Does anyone from Lighthouse actually who can walk through what happened there? If not? Okay, so Polis says this was also on prisms. On prisms.
00:18:04.560 - 00:18:28.600, Speaker H: Okay, it's a chicken and egg problem. So Lighthouse has some cpu issues in the way they process exits, and they failed to cache correctly the exits and process them correctly when there were missed slots. So they had this bug, but it wouldn't have been apparent if we didn't have the missing blocks that were because of present mission.
00:18:30.460 - 00:18:41.590, Speaker A: Got it. Any other clients then have issues or interesting things they saw during the work they want to share?
00:18:42.280 - 00:19:45.640, Speaker I: Just a quick one on Teku, if I may. We are seeing occasional spells of slow block import for Teku specifically, so normally it takes about 100 milliseconds and most of the time it's still taking 100 milliseconds and it's fine, but for several minutes at a time this seems to shoot up by factor ten or so to over a second, which can sometimes delay block processing in total if it's a late received block beyond 4 seconds, making attestations inaccurate. We are investigating haven't found the root cause yet and it seems to be getting a lot better over time. It was seemed worse in the few hours post the upgrade. One hypothesis is it might be due to patches where there are long withdrawal sweeps where most validators have zero x credentials. But we haven't held it down for sure yet. But for anyone running techu, just be aware of that and we are investigating.
00:19:47.820 - 00:20:36.650, Speaker A: Awesome, thank you. Any other clients have anything to share? Okay, I guess anything else on Chappella? That last chance to bring it up as we move on. Okay, well once again, great work everyone. Like I think, you know, overall this went really well. The network is stable and withdrawals are happening. So yeah, very cool to see this finally live and I guess onto the next thing. Cancun.
00:20:36.650 - 00:20:57.570, Speaker A: Yeah Alex, you had posted an update to EIP 4788. Do you want to chat about basically the changes you've made? And then you also mentioned you wanted to have this considered for CFI, so maybe we can chat about that and just CFI and stuff in general afterwards?
00:20:59.990 - 00:21:38.394, Speaker D: Yeah, that works for me. So this EIP was written a little while ago actually for withdrawals, and we went down a different path that didn't really need it. And that being said, I've gone back and looked at this again. I made a number of updates, I'll just run through them kind of at a high level. The biggest thing is moving well, even just taking a step back. So 4788, the idea is to get some sort of cryptographic accumulator from this consensus layer into the execution layer. So for example, this could be like the state route and that's how the EIP was originally written.
00:21:38.394 - 00:22:22.154, Speaker D: So committing to the state of the beacon chain. And then the reason this is cool is because then there's going to be a VML code or something and you can access to this route and then you can make proofs or you can verify proofs against this route. So that's the drill idea, is to have some way to access the Katza state in a trust minimized fashion. And the way this EIP originally worked was using the state route from the beacon chain. I changed it to use each beacon block route instead of the state route. The reason why is because the way the beacon chain works is every state has a unique state route, but not, well, there can be the same block route if there's a missed slot. Let me rephrase that.
00:22:22.154 - 00:23:09.338, Speaker D: So basically, if there is a missed slot, and that's really the concern here, if there is a miss slot, then the block roots don't change because that's why it's missed. There's no block, but the state does change. And so what it means is if there was a row of miss slots, then there would essentially with state roots be this linear amount of work, whereas now it's like constant time or constant amount of work. So we moved to block roots. The original eap keyed it by block number, like el block number, and I've now changed that to key it by slot. I think there's a comment from the original EIP PR Vitalik went into reasonings of why we'd want to use slots, which I think I agree with at this point. So in short, it's better ux we key by also.
00:23:09.338 - 00:23:42.470, Speaker D: So I changed this to use a ring buffer rather than just like writing them all to state forever, just so there's a constant amount of storage that we use. This also mirrors the construction on the Cl. So that's pretty cool, just to have the uniformity there. And the way it's written now is that this block route is sent over kind of just like withdrawals are today, and you essentially just append it to the block header. So what that means is that it's up to the El to communicate which slots these are for if we keep by slot. And. Yeah.
00:23:42.470 - 00:23:47.930, Speaker D: Does anyone have any questions at a high level right now? I do. Okay, Danny has his hand raised.
00:23:48.350 - 00:24:20.450, Speaker B: I just had one clarifying comment, which is, I think one of the important things to consider here. So by moving to the block route, we don't have the linear potential load on the header if there's missed slots, but there still is a minor linear component which you do an S store per miss slot. So that's just when reading the eip and thinking about security implications that there is a linear piece right there. Everything else is constant.
00:24:22.390 - 00:24:33.720, Speaker A: I think this is similar to my question in the chat. And the reason for this is because during a missed slot you're still going to store a block root, you're just going to store the previous one like the latest one you have, right?
00:24:35.130 - 00:25:18.838, Speaker B: Correct. So you essentially play catch up for the missed slots. There is an alternative scheme here. You could actually move the complexity to read. For example, if you tried to read slot n and maybe it was a flag value like zero, you could walk back to n minus one, n minus two, et cetera, and have a higher complexity on read. Once you hit a non zero value return that and have a higher potential gas cost there. Given what the overhead of the block processing and the right here and the potential of missed slots and that kind of stuff, it's, I think, much preferable to put the complexity in the.
00:25:18.838 - 00:25:27.560, Speaker B: Right. But that's an optional, right?
00:25:31.870 - 00:25:50.874, Speaker D: Well, yeah, I guess so. The thing is, there's one issue. I see. Yeah. Maybe just quickly if anyone has any other questions so far. Okay, so I'll take that as a no and then as written. So yeah, everything Danny said is correct.
00:25:50.874 - 00:26:24.760, Speaker D: That's an important distinction. And also. Right, so the one thing is that as written, this is assuming that the els can essentially use the timestamps from the headers to drive slots, which I don't think they have that functionality now. I don't think it's being added, say from four, four four. Or like anything else that might go into Cancun. So yeah, that's a question for everyone here. Are we comfortable adding this logic or would we rather not sort of leak that abstraction barrier between El and Cl?
00:26:28.050 - 00:26:31.022, Speaker B: And what is the alternative to read by?
00:26:31.076 - 00:26:51.880, Speaker D: Well, you could just send the slot over, so you're already trusting the Cl for the root, you basically also trust them for the slot. So that just means there's more in the header. And you could imagine having the start slot to end slot. You could write the start slot to the state so that you only need the one slot number. You could send them both. There's a couple of different options.
00:26:52.330 - 00:27:12.110, Speaker B: Right. Or you could try to have this by timestamp and have bad timestamps be failure cases on read and other weird stuff, but that would bring a lot of moderate amount of complexity into the construction rate out of degraded ux.
00:27:13.010 - 00:27:37.640, Speaker D: Right. So for the El to compute these slot numbers, which is also something apps will probably want at some point you need two pieces of configuration from the Cl, so it's not too much, but it is something. And I think the bigger thing is that it violates again the subtraction we have, or like the barrier between each layer. Mikhail, your hands up.
00:27:38.970 - 00:28:36.280, Speaker F: Yeah, I just wanted to add that we still have non field of el lock header unused, which sounds like good place for a slot, so it doesn't add any data complexity. And potentially we can expose slot on the EVM level in some future if we think that it will be useful. And avoiding the deriving slots from timestamp on the l side is a good thing in my opinion, because if we at some point in time change it, I mean like the slot duration, so it will have to become a configurable framework, so more configs, more time between the two layers. If it can be avoided. That would be.
00:28:51.380 - 00:28:53.750, Speaker A: Sorry. Alex, are you going to say something?
00:28:54.760 - 00:29:16.250, Speaker D: Well, I was just going to call it El Devs, because I see many of them here and I feel like they'll have some opinion. What are you going to say, Tim?
00:29:17.260 - 00:29:32.590, Speaker A: I was going to say, so if I understand Mikhail correctly, it's like you remove the need for this pre compile that you're writing to you. Instead have the slot number as part of the block number for free as the non value.
00:29:34.880 - 00:30:06.472, Speaker D: Right. Well, there's kind of two things. So one of them was just exposing the slot to the EVM as well. And that's like a separate downstream question. But then upstream is like, yeah, how do you even get this data to the El in the first place? And one option is rather than try to look at the timestamps and the headers and compute the slot, you would just pass it along with this block route as well. And the question then is where does it go? There's apparently this knots which. Yeah, I believe is U 64.
00:30:06.472 - 00:30:12.648, Speaker D: That's the right size. It's just sitting there unused, basically. So we could put it there.
00:30:12.814 - 00:30:19.448, Speaker A: Yeah. And I assume there's no non opcode, right. I don't think this is previously exposed.
00:30:19.544 - 00:30:22.936, Speaker D: Don't think so, yeah, no, I assume.
00:30:22.968 - 00:30:25.950, Speaker A: We would have dealt with it during the merge if that was the case.
00:30:27.040 - 00:30:34.968, Speaker B: Hopefully it.
00:30:35.074 - 00:30:35.344, Speaker D: Yeah.
00:30:35.382 - 00:30:42.390, Speaker A: I'm curious if any El devs have opinions, thoughts, questions on this.
00:30:44.920 - 00:30:57.610, Speaker B: And we can give some room to talk about it in one week on ACDC, obviously from a contemporary perspective, but if Els come there, we can also hammer it from that angle as well, if you have some time to look at.
00:31:02.060 - 00:31:43.930, Speaker H: Yeah, so I'm a bit confused about this difference. So are we talking about exposing the slot number on the EVM on every block, instead of just making the computation from Genesis time plus this number on this timestamp just because we might change the slot duration. It seems to me that the complexity of changing the slot duration is going to be just adding a constant after a particular fork, which shouldn't happen in many forks, that we're going to change that slot time, just sort of like adding data on the EVM on every single blot to prevent this extra computation. Seems crazy to me.
00:31:47.100 - 00:32:09.824, Speaker F: I'm not sure what you mean by on the EVM. It's just passing a slot into El header and we have a place there which remains unused since the merge. So there is a field of Uni 64 data type which is filled with zeros currently, but I think we could.
00:32:09.862 - 00:32:17.600, Speaker H: Use something useful to send instead of the data that is completely equivalent to the timestamp that we have already sending on that header.
00:32:18.520 - 00:32:31.290, Speaker F: Oh yeah, that's fair, but we will have to keep to have this complexity of maintaining plot duration on the outside. Yeah, I got it.
00:32:33.900 - 00:33:27.320, Speaker B: I have a quick question. So often when I'm thinking about the state transition function, usually I want it to be a function of the prestate the block and then it outputs the post state. And I just realized that I believe this actually makes this a function of not only the prestate the current block, but also the previous block, because it's doing a read from that rather than reading the timestamp from the previous block from state. Is that a concern? That's usually kind of a little bit of a red flag to me when we're abusing and extending the inputs of the state transition function. I mean, we already need the last 256 blocks for the block hash opcode. Right. But that's maybe something we don't want and would want to fix.
00:33:27.320 - 00:33:30.840, Speaker B: So I wouldn't want to add another dependency of previous block.
00:33:31.420 - 00:33:48.932, Speaker D: Yeah, the reason that the CIP uses the state in the way that it does like client, is for that reason there was desire in the past to move away from having the state. Yeah, the prestate block, post state. But also there's almost this hidden input of this history element.
00:33:49.096 - 00:33:53.228, Speaker B: Yeah, but there's another hidden input right here, which is the previous block header.
00:33:53.404 - 00:33:57.148, Speaker D: All right, so that can be changed to write, I mean, yeah, it depends.
00:33:57.164 - 00:33:57.676, Speaker B: On how we want.
00:33:57.718 - 00:33:59.476, Speaker D: We need the previous block header for.
00:33:59.498 - 00:34:25.070, Speaker B: The 1559 verification, right? You tell me. Yes, we do. Yeah. Okay, so the elsta transition function is pre block header, pre state block currently and likely won't change. Okay then that's fine. I just don't want to extend the dependency. I agree.
00:34:25.070 - 00:34:31.710, Speaker B: Without consideration the 256. I think everyone thinks that's degenerate. Okay, thanks.
00:34:33.920 - 00:34:59.510, Speaker A: Yeah, there's some comments in the chat about exposing it to the EVM as well. So I think there's like two separate things, right? One is like where do we store this data, whether it's the state nons, something like that. But then obviously how do we expose it in the EVM? Alex, do you maybe want to take a minute or two to talk about why we should expose this in the EVM? And what's the value that you get out of this?
00:35:00.380 - 00:35:04.760, Speaker D: Well, are we talking about the slot numbers themselves or this route.
00:35:08.580 - 00:35:10.560, Speaker A: Either? Well, I assume.
00:35:11.460 - 00:35:13.970, Speaker D: Well, one of them, I think, is not that.
00:35:17.000 - 00:35:17.460, Speaker A: Correct?
00:35:17.530 - 00:35:50.332, Speaker D: Sure. Right. I mean, there's all sorts of things around bridges and clients and staking pools and all sorts of things. I mean like immediate use cases would definitely be around more trustless staking pool designs where right now a lot of them just have some oracle that there's like a multi stick updating or something like this. And then instead you could move that to, again, just like proving some bit of the state, anyone can do this and it reduces a lot of governance risk for all these staking pools. Lido rocket, know more.
00:35:50.386 - 00:35:59.090, Speaker B: Yeah, I think importantly it also reduces the barrier to entry to create something, because getting a good oracle is hard.
00:36:01.060 - 00:36:02.610, Speaker D: Yeah, that's a good point.
00:36:04.260 - 00:36:07.228, Speaker B: Was that the argument for putting it in the state or this is the.
00:36:07.254 - 00:36:09.444, Speaker F: Argument for the opcode in the first place?
00:36:09.642 - 00:36:33.940, Speaker D: Well, right, so we're just talking about, well, there's a number of decisions here because it could also not be an opcode, but basically this is just the idea of having the root be exposed in the EVM. So that's what the CIP sets out to do. That's the important bit. I don't think we should get tied up. And is the slot like, what does that look like? All of that stuff and.
00:36:34.030 - 00:36:43.490, Speaker A: Sorry, just to make sure, like what's the value that you get from having this slot number if it's not exposed in the EVM itself?
00:36:45.780 - 00:36:50.956, Speaker D: Well, so it would be written in this contract and anyone could read it. So in that sense it's be, it's.
00:36:50.988 - 00:37:02.932, Speaker A: In the, like Mikhail's idea of putting it in the nons. If we do that, then it's not written in a, so.
00:37:02.986 - 00:37:17.000, Speaker D: Oh no, it would. So that's just essentially to commit to it. Well, let's see. I mean maybe we could think about something weird, but it'll end up written to the state according to the current EIP.
00:37:17.740 - 00:37:19.130, Speaker A: Okay. Correct. Yeah.
00:37:19.900 - 00:37:22.776, Speaker D: Because then later as a User, I go and say, hey, what was the.
00:37:22.798 - 00:37:23.616, Speaker B: Root of the slot?
00:37:23.668 - 00:37:24.860, Speaker D: And I give it the slot number.
00:37:24.930 - 00:37:49.170, Speaker A: Yeah, okay. And then I can imagine also potentially if we are writing it to the state as well, and we also write it to the block header, then off chain applications that rely on block headers can get that information without having to read the state. Right. I assume the extra benefit that you get from having it there.
00:37:51.060 - 00:37:53.244, Speaker D: Having the roots or some slot.
00:37:53.292 - 00:37:57.880, Speaker A: No, having the slot number, sorry, having the slot number in the block or instead of the nonce.
00:37:59.100 - 00:38:06.600, Speaker D: Well, okay, those are the same thing. To me, that's just a question of if it's appended or if it replaces the nods.
00:38:11.530 - 00:38:18.060, Speaker A: I'm not sure I understand. So you're saying that. Sorry. The root is the same thing as the slot number because you can.
00:38:21.790 - 00:38:52.818, Speaker D: Send over the root and the slot number and the reason you want to put it in the header. Okay. So you want the root in there. Okay. You have to have both of them in the header because for sync actually. So I was trying to think if there's some way you could not have the slot in there because if that's breaking abstractions and all this, but either way, if I'm off like syncing a fork for some reason, then I need it in there because the co won't be able to tell me. So whatever needs to go into the state will end up in the header.
00:38:52.818 - 00:38:56.920, Speaker D: And where we're at now is having the root and then one slot number.
00:38:59.050 - 00:39:00.406, Speaker A: Okay, thanks.
00:39:00.588 - 00:39:01.800, Speaker B: That makes sense.
00:39:03.770 - 00:39:07.260, Speaker A: Garcia, you like briefly had your hand up. I don't know if there was something you wanted to add.
00:39:09.710 - 00:39:18.270, Speaker F: I was just maybe also trying to clarify basically between the root and the slot number and everything. But I think Alex did a good job clarifying.
00:39:21.090 - 00:39:26.000, Speaker A: Okay, does anyone else have questions or thoughts on this?
00:39:30.710 - 00:39:52.040, Speaker B: I think the only thought I have is I would like to consider more adding this as a piece of the state versus just having it as ephemeral thing in clients to fill the op code. Got it. Is that not what.
00:39:52.970 - 00:39:55.594, Speaker D: Yeah. Do you mean getting rid of it or keeping it?
00:39:55.792 - 00:40:02.860, Speaker B: Getting rid of it out of the state. I'm trying to convince myself that it makes sense as a piece of the state.
00:40:07.790 - 00:40:08.202, Speaker C: Yeah.
00:40:08.256 - 00:40:11.020, Speaker B: I mean, the main thing is.
00:40:14.850 - 00:40:15.118, Speaker F: If.
00:40:15.124 - 00:40:58.326, Speaker B: You don't have it in state, you can only read the previous header. Right. Because the previous beacon route, because it's the only thing that's going to be available because it's going to be in the header. And at that point you make the UX of sending transactions really tough. Your transaction has to be for precisely the previous block has to get into the next block because otherwise you're going to be making proofs against a route that's broken with respect to your proof. So the state and then being able to bound your proofs against a particular slot, make it so that you don't have like weird failed transactions and bad ux around using the top code. Is that what you were considering? My understanding is that you have access to 81 92 routes.
00:40:58.326 - 00:40:59.900, Speaker B: Is that wrong?
00:41:01.070 - 00:41:28.466, Speaker D: No. I think with what Danny was saying you could do that still with what like client is trying to do with having this extra history parameter. So this point is just like, do we want to have this implicit history or know. Vitalik and others in the past have definitely wanted to move away from that. I think it's cleaner. Danny put some comments in the chat around how we handle this in the Cl. And again, I do think it's cleaner.
00:41:28.578 - 00:42:11.762, Speaker B: Oh, life plan. Are you just claiming that you should just have them ephemerally hanging out in a. Yep. Oh, that's really bad for statelessness, right? I mean, why? We already have it for block hash and you have to have this data whether it's part of the state try or not. It doesn't seem right. But if it's part of the state try, then it just becomes, if people are accessing it in statelessness, it becomes part of the proofs. Whereas if not, then your client in statelessness has to have this data locally and have gotten it from previous blocks or somewhere else and the proofs.
00:42:11.762 - 00:42:30.634, Speaker B: So it just kind of adds complexity there. Whereas obviously we have that complexity with the 256 block hashes. But reducing that actually makes, I would want to eliminate these things. So stateless is cleaner and has fewer dependencies rather than add to them. Yeah.
00:42:30.672 - 00:42:30.874, Speaker A: Okay.
00:42:30.912 - 00:42:32.140, Speaker B: I think that makes sense.
00:42:38.640 - 00:42:54.610, Speaker A: I guess given the amount of back and forth on the design on this, it probably makes sense for us to discuss it next week on the CL call once people have had a bit more time to wrap their heads around.
00:42:58.180 - 00:42:58.640, Speaker B: Yeah.
00:42:58.710 - 00:43:10.596, Speaker A: Does that seem reasonable? No. No breakouts. I don't think this words are breakouts. We can do this one, I think. Yeah.
00:43:10.618 - 00:43:17.350, Speaker B: And we have a low agenda next week, so we can spend 30 minutes on it. It's fine. Yeah.
00:43:19.640 - 00:43:20.630, Speaker A: Sorry about.
00:43:21.160 - 00:43:22.424, Speaker C: I was just going to say, obviously.
00:43:22.542 - 00:43:32.830, Speaker B: I think that this Eap is pretty important and that we need to come up with a way of getting the beacon block route into the EVM. I just want to come up with the best design.
00:43:35.200 - 00:44:23.484, Speaker A: And I guess in terms of the CFI conversation, I feel like now that Shanghai is completely out of the way, does it make sense to have the next cordes like Yale call focused on just generally what are the things we might want to CFI for Cancun, so that we're not just making this one off decision now, but we can also have two weeks for teams to just look at the set of things that are being proposed and try to think about what's like a coherent set of things we want to do. And even if we don't agree to everything we want to do in the next two weeks, maybe we can flesh out the one or two most important things on the next call alongside four.
00:44:23.522 - 00:44:23.724, Speaker B: Four.
00:44:23.762 - 00:45:00.040, Speaker A: Four, yeah. Just, I don't know. So kind of. We don't just add things CFI one by one and then realize we have like five eips we've committed to without having a higher level view. If anyone strongly wants to CFI this now, we can have that conversation as well. But I think otherwise, having like a higher level conversation in two weeks probably makes sense. Last call for objections.
00:45:00.040 - 00:45:49.400, Speaker A: Okay, yeah, so let's do that. And I guess over the next two weeks as well, we have this ETh magicians thread. I believe all of the eips that have been proposed are either linked in that thread or there's also a tag Cancun candidate on EtH magicians. So I'll make sure to update the first post on that thread with everything that I'm aware about that has been proposed so people can look at that. If there's anything missing there, feel free to ping me either on discord Twitter eth magicians. So I'll make sure to keep that up to date before the next call. And then, yeah, if client teams and just folks generally want to think through the various options, what we should prioritize, I think that would be valuable.
00:45:49.400 - 00:46:09.470, Speaker A: Yeah. Does that make sense? Okay, next up then. I believe it was Perry. You put this on the agenda. The idea of starting devnets with capella Genesis. Do you want to take a note to chat about that?
00:46:10.480 - 00:46:11.180, Speaker G: Sure.
00:46:11.330 - 00:46:11.596, Speaker A: Yeah.
00:46:11.618 - 00:46:46.690, Speaker G: I just wanted to bring it up to see if any client teams have already tested support and if not, if we could start testing support so that we can start testnets from capella instead of bellytex genesis states, and also asking that in the context of if that's supported in all the feature branches for the different features. So I know, for example, we can't start from Capella Genesis on worker yet, but if it does work on EOf as well as on four eight four.
00:46:46.760 - 00:46:47.380, Speaker B: Ready.
00:46:49.270 - 00:46:50.186, Speaker G: For Loadstar.
00:46:50.238 - 00:46:53.830, Speaker D: It should be possible to start from Capella Genesis.
00:47:02.560 - 00:47:04.000, Speaker A: Any other clients?
00:47:09.790 - 00:47:10.202, Speaker B: Cool.
00:47:10.256 - 00:47:14.282, Speaker G: I can also reach out to the client teams separately later on, figure that out.
00:47:14.336 - 00:47:14.940, Speaker F: Thanks.
00:47:16.670 - 00:47:18.554, Speaker A: One other tiny point.
00:47:18.592 - 00:47:38.340, Speaker G: We have the Cheshang testnet that's currently live that was meant to be public and test withdrawals. We'd like to officially announce its deprecation and we hope to take it down next week Wednesday. So if anyone's using it for something active, please let us know, otherwise it will be deprecated by them.
00:47:40.470 - 00:48:12.110, Speaker A: Got it. And I know in the four four four calls, like two weeks ago, we were talking about setting up the longer lived four four devnet in the next few weeks or so. So I suspect, yeah, once we have that up, we can point people in that direction of the main thing to go if you want to test the colline edge features before they go on public testnets. Okay, anything else on Cancun?
00:48:13.890 - 00:48:46.818, Speaker B: Just real quick. The way that we do releases on the consensus specs is their release candidates. Until the rules are actually on main net. The rules and logic is now on Mainnet. So in the next couple of days we'll be releasing a consensus specs non rc release. This will also include the transaction type change to zero x three on four four four. So I believe that will kind of give us a good target for our four four four testnets.
00:48:46.818 - 00:48:48.570, Speaker B: Just an announcement.
00:48:49.870 - 00:48:50.474, Speaker F: Nice.
00:48:50.592 - 00:49:36.220, Speaker A: And this reminds me, we should mark all the eips for Shanghai as final now that they're on Mainnet. So EIP authors of the Shanghai Eips. Yeah, if you can open a PR just to move it to final, given that they've all been activated as well. Sweet. Anything else on Shanghai or capella? Okay, if not, Mikhail, you had posted the engine API PR discussion. There's a bunch of people who chimed in on the agenda already, but do you want to give some quick context on that and we can discuss it?
00:49:37.710 - 00:50:51.780, Speaker F: Okay, so the original problem is that in nimbus and Bezo pair, nimbus and Bezo combination, there was a bug when Nimbus updated with payload attributes, initiating a build process, payload build process, then it resend in the same spot. If I'm not mistaken, the new payload attributes changed with goals. And the problem is that Bezu did not factor in goals into the payload id computation process. So they did not restart the payload process. And yeah, that led to failure proposal. And it appeared that we didn't have anything about that in the spec saying that the payload addition unique, they identify the build process and be anyhow related to payload attributes. And yeah, hence the proposed fix to the spec.
00:50:51.780 - 00:51:46.680, Speaker F: Yes, if I understand correctly, there might be some other Yale clients that are affected and probably guys somebody want to chime in and share more information on that. But with respect to the pics. So the pics is here and there is discussion happening in there so soon to engage El client devs to look at this pr and if it is okay say something or participate in discussion. The idea also is that by having this in the stack, having the statements in the spec, we can then create the hype test and enforce this behavior to avoid such problems in the future. That's it.
00:51:47.530 - 00:51:50.518, Speaker A: Thank you. Lucas, you have your head up.
00:51:50.684 - 00:51:56.774, Speaker D: So unfortunately Nethermind also doesn't include withdrawals.
00:51:56.822 - 00:52:02.140, Speaker F: When calculating the payload id, so we will be fixing that soon.
00:52:08.890 - 00:52:42.390, Speaker A: Okay. And there was a comment, I guess, by Gajinder on the agenda where he said it should not be possible for Cl, since withdrawals, to send two payloads that are two fork chase updated messages that have the same attributes for everything except withdrawals. So he was curious what would trigger that. I don't know.
00:52:47.960 - 00:52:51.210, Speaker F: This should be bug on Cl side. Okay.
00:52:52.220 - 00:52:58.810, Speaker H: Yeah, this is probably a bad cachet head state, like sending the withdrawals for a different head state.
00:52:59.420 - 00:53:00.330, Speaker A: Got it.
00:53:01.260 - 00:53:01.720, Speaker B: Okay.
00:53:01.790 - 00:53:13.070, Speaker A: And so this fix would mean that even in the case where you send this buggy SCU message, you still get a different payload ID because at least one of the attributes has changed, is that correct?
00:53:13.860 - 00:53:15.232, Speaker D: Right, yeah.
00:53:15.366 - 00:53:23.520, Speaker F: And the new build process should be started to factor in this new information. Withdrawals in this case, for instance.
00:53:24.260 - 00:53:40.440, Speaker D: Yeah, but the proposal will still fail because withdrawals shouldn't be any different. Right. So if it's giving different withdrawals, then probably it has calculated the withdrawals incorrectly and the proposal will be eventually failed.
00:53:56.600 - 00:53:57.540, Speaker A: Mikhail?
00:53:58.360 - 00:53:59.110, Speaker D: Yeah.
00:54:04.680 - 00:54:50.810, Speaker F: The failure is because the withdrawals were sent had different values. So yeah, that's failure. So it's basically not because of Bezu only in this case it's because of the bug on Cl side and this behavior on Yl side as well, which didn't catch up with the new roles information sort of makes. But yeah, we'd like to. I think it makes a lot of sense to have this in the spec and enforced by hypedesk as well.
00:54:59.380 - 00:55:24.630, Speaker A: Okay. I guess in terms of next steps here, should we just go ahead and merge this change? I saw Proto had a recent comment as well. I don't have time to read. Or should we maybe leave it open a couple more days just to make sure we address all the issues or like the comments on it? Yeah, close it.
00:55:25.880 - 00:55:32.620, Speaker F: I would, I would leave it open for the next several days, probably for a weekend, then just merge.
00:55:33.760 - 00:55:43.150, Speaker A: Yeah. And at the very laylist then on next week's Cl call we can agree to merge it, but probably just do it before. Yeah. Okay.
00:55:46.660 - 00:55:47.600, Speaker B: Sweet.
00:55:50.100 - 00:56:04.012, Speaker A: Anything else anyone wanted to cover? Okay. And Meryl saying in the comments we can add a hype test for this.
00:56:04.066 - 00:56:04.910, Speaker B: Case, as.
00:56:08.800 - 00:56:09.820, Speaker A: Otherwise.
00:56:12.180 - 00:56:22.130, Speaker F: We can basically work on hype tests in parallel. So it still makes sense not wait for the.
00:56:25.400 - 00:57:44.780, Speaker D: If there's anything else, I have one thing to bring up around an RPC. Okay, so this is about verifying blocks from builders as they go to the relays, and there's a custom RPC that flashpots implemented in their relay. And right now, basically it's just in a fork of gaff. This came up on the webboost community call one or two calls ago, where we'd like to have client diversity at this layer of the stack as well. And what that means here is having other El clients implement the same RPC, something like it, basically having a standard for this, and then everyone supporting it. Is this something that people are interested in doing? Do they see the value in this? Do they think it's a bad idea? I think probably if we all think it's a good idea, then I can go and work on some sort of RPC spec, but I just wanted to get some sort of initial temperature check before I go and do that. So concretely the endpoint is saying, given a payload, like, is this payload valid?
00:57:47.760 - 00:57:55.330, Speaker A: And in practice I assume we would have this in the execution API's repo. Does anyone think this does not belong there?
00:58:01.180 - 00:58:07.720, Speaker B: Okay, wait, would this go into execution or beacon APIs? Sorry, I missed.
00:58:07.800 - 00:58:18.412, Speaker D: No, this is at the El. So there's a separate discussion point for having this, essentially having the CL part be at the Cl, and this is essentially the El part happen at the El.
00:58:18.466 - 00:58:19.200, Speaker B: Ok, got it.
00:58:19.270 - 00:58:23.250, Speaker D: Kind of need both in different places, but yeah, this is just for.
00:58:25.300 - 00:58:25.664, Speaker A: Having.
00:58:25.702 - 00:58:51.450, Speaker D: This be a standard API across eels, because right now, basically all the relays are using network and cap. And given how many blocks come through the system, it kind of harms all the efforts we've done with client diversity at that part of the stack. So it'd be nice to have optionality here. I'll take the silence as compliance.
00:58:51.530 - 00:59:09.180, Speaker A: Yeah. Okay. I think you can just open a PR and the execution APIs repo, which is where we have all this stuff specified. Yeah, Mikhail.
00:59:10.560 - 00:59:54.380, Speaker F: Yeah, I'm just recalling that we were agreed to double check on this thing, which is builder override flag for get payload. So what we have decided on the previous call is that if any El team is willing to implement these heuristics for this type of flag before the next hard work, then it will make sense to spag it out right now and start working on it. But as far there are no attention to this PR. So I'm just reading this as no intention to work on that.
00:59:54.450 - 00:59:56.184, Speaker D: So I think we should just push.
00:59:56.232 - 01:00:17.840, Speaker F: It to the next hard work and include it in the scope of Cancun, of engine API changes that are scopeful cancun. So if anyone thinks the opposite, so that's just go to the PR with a comment.
01:00:20.500 - 01:00:35.750, Speaker A: Yeah, sounds good. And there was a comment on the PR, I think, by Alex, who said it would be nice to see some prototypes of this working before we decided to add it to API. Yeah. So I don't know if any.
01:00:36.460 - 01:00:48.030, Speaker H: On the TL side we can do this immediately. And on the El side, I think Marius has a draft. I don't know how much can that be worked?
01:00:55.310 - 01:01:28.040, Speaker D: And just as some context, the comment there was essentially saying this could all be done outside of having any formal spec or API that we all agree to. And it would be nice just to prove that, you know, if the concern is like some sort of censorship, resistance detection, then actually prototyping that out, ideally, and clients on main net. But yeah, just having some use case beyond like, oh yeah, here's this additional thing that everyone has to deal with for all time moving forward, and it'll probably do something without actually doing the something.
01:01:36.750 - 01:01:54.750, Speaker H: So in my original issue, I added three different heuristics that I think are pretty valid. And they just have a couple of constants that are configurable. So by prototyping them, you mean having some Yale that actually implements some of those heuristics at least, right?
01:01:54.900 - 01:02:03.394, Speaker D: Yeah, it would be nice. I mean, again, this is not a. Because my concern is otherwise. Go ahead.
01:02:03.512 - 01:02:17.800, Speaker H: No, but I think Nisha is actually proposing this as a blocker, which I agree. I think we should have at least some sort of like draft PR of Somyl that actually wants to implement some of these heuristics before we push to have this in the spec.
01:02:22.610 - 01:02:23.360, Speaker B: Great.
01:02:23.810 - 01:02:48.190, Speaker A: Yeah. Okay. Anything else? Okay, then. Yeah, let's wrap up. Yeah. Thanks, everyone. Congrats again on Chappella and talk to you all in this Yale call next week.
01:02:50.400 - 01:02:51.212, Speaker B: Thank you.
01:02:51.346 - 01:02:51.948, Speaker E: Thank you.
01:02:52.034 - 01:02:54.220, Speaker D: Bye. See you guys. Bye.
