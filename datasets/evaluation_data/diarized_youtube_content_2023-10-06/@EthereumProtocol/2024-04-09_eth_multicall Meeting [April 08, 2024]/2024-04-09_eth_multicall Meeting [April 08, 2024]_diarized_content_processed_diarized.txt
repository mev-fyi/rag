00:00:04.120 - 00:00:04.700, Speaker A: Hello.
00:00:06.854 - 00:00:07.594, Speaker B: Hello.
00:00:14.494 - 00:00:18.914, Speaker C: What is different between get logs and ETH call?
00:00:21.494 - 00:00:29.064, Speaker D: Get logs doesn't have to actually run anything because the logs were already run and most clients save them.
00:00:34.524 - 00:00:35.708, Speaker E: Whereas ethcall.
00:00:35.796 - 00:00:49.276, Speaker D: Will execute some code on top of some block and then give you the results. In fact, very annoyingly to most people, Ethcall doesn't actually return any logs and so simulate does.
00:00:49.300 - 00:00:50.332, Speaker E: That was one of the big wins.
00:00:50.348 - 00:01:18.244, Speaker D: Of ETH over Ethcall is that you actually get logs back. It's super annoying when you're writing applications and you don't get logs, but you do get a return value if you execute a contract off chain like the ETH call, and you do get a return value, but you don't get logs if you execute. If you look at a past execution that happened on chain, because it makes building your applications so much harder when the result set is different between on versus off chain.
00:01:20.884 - 00:01:31.708, Speaker C: So in this feature that Aragorn was trying to develop, they actually allow execution of additional code on top.
00:01:31.796 - 00:02:23.424, Speaker D: So, yeah, so what we've been talking to bentig a little bit in our server, and I think I have a better understanding of what they're doing now. It appears what they're doing is they want to be able to say, okay, over the last, you know, 10 million blocks, pick out the set that expressed this log, this logline, and they use that as like an indicator, like a flag, to say this transaction touched this contract. And then from that subset of transactions and all the last 8 million blocks, whatever they're looking at now, rerun those transactions. So run those transactions again, that subset, but with this code at address X instead of the normal code addressed x, and where their replacement code maybe has some new events or new logs, and then return me all the logs from those executions.
00:02:24.804 - 00:02:29.904, Speaker C: So they rerun only the ones that are related to this particular contract.
00:02:30.924 - 00:03:07.098, Speaker D: Yeah, and unfortunately, the way they identify that, because the only information that's stored about what contracts are called are logs that were fired. And so if you call a particular contract, but the log is never omitted for some reason, say the log is in a conditional statement, right. If that condition is not met and therefore the log was not emitted, their technique will skip over that contract, even though maybe it would have still fired the log. And this is necessary to make it so they can do their thing in reasonable timeframe because you can't just re execute 8 million blocks worth of transactions in any reasonable time.
00:03:07.146 - 00:03:07.378, Speaker E: Right.
00:03:07.426 - 00:03:21.914, Speaker D: So they need to narrow that set down the way the technique they used to do that is say anything that had you know, this log in it rerun, but rerun it with this code instead, but using the same state. And so you do need an archive note for this, because you need to.
00:03:21.954 - 00:03:23.614, Speaker E: Be able to execute with the same state again.
00:03:30.134 - 00:04:25.214, Speaker D: So I do think their thing is different enough from ours, like it's serving a different goal. You could, in theory, and we talked them a little bit about this in our chat. I think you could reproduce their results with esimulate, but it would be much more heavyweight and involve a lot more round trips. So with their thing, you just tell the server, look for any transaction that fired these events, and then rerun those transactions with this code replaced, and all that can be executed. Then on the server side, you don't need to have round trips back and forth between the client, whereas if you want to do the same thing with ETH simulate, you would need to basically go to the server, ask it, please tell me all the places where this log showed up in the last n blocks. And then the client would get that, build out a set of ETH simulate calls, and then call an e simulate for every single block that showed up in that set of events with those set of events. And so there's a huge amount of round trips.
00:04:25.214 - 00:04:46.186, Speaker D: If you were to use our esimulate to solve the same problem, versus if you use their stuff to solve that problem, it's a single round trip, and the node does everything, and it can have optimizations that make it fast, whereas they simulate no optimizations. It would re execute everything. It's going to return results that are much bigger. And so I think they serve different enough purposes that I can see value.
00:04:46.250 - 00:04:47.574, Speaker E: In having both separately.
00:04:50.234 - 00:05:18.034, Speaker C: So do I understand this correctly, that if, like, compared one to another, filtering of certain transactions in history running through like block ranges, and rerunning transactions that are only filtered out based on some criteria, would be the, like, essential difference between what is simulate currently does and what their thing is doing.
00:05:18.554 - 00:05:55.566, Speaker D: Yeah, I'd say that's the biggest difference. Just the fact that you can kind of, you tell it, I want you to rerun some subset of things. Here's the filter that will tell you what that subset of things is. And here's the code I want you to replace when you do the re execution and you're giving all that up front and you're telling it, and do this over a very large range of blocks, whereas esimulate, we're basically saying each of these calls is, we're just saying, I want you to run this set of transactions in its entirety, and give me all the results. That's the other thing is they will only give you back the results that are the events they're not going to give. Like if you do an e simulate, you get back the whole blocks, right? You get blocks, all the receipts and every transaction that occurred. Like, it can get pretty bulky.
00:05:55.566 - 00:06:08.234, Speaker D: And so I think what you described as the biggest and the second one, the second biggest thing is just the volume of results that get returned. It's going to be much smaller for theirs because it's very narrowly targeted, what they're looking for.
00:06:10.994 - 00:06:16.014, Speaker C: It will be smaller because it only returns logs. Or what would be the difference?
00:06:18.234 - 00:06:19.842, Speaker E: Yes, it only returns logs and it.
00:06:19.858 - 00:06:33.442, Speaker D: Only returns logs for the specific subset of transactions you care about. And so you still need, in both cases, you still need to execute all the transactions in the block prior to the transaction of interest to you, right? So if your transaction showed up at transaction number 53 in the block, you.
00:06:33.458 - 00:06:34.322, Speaker E: Still have to, in both cases, you.
00:06:34.338 - 00:07:01.374, Speaker D: Still have to execute transactions one through 52. In their case, they execute transaction one through 52, and then they execute transaction 53, return the results and they stop. Whereas in our case, we execute all the transactions one through 52 plus all the transactions 53 plus 54 through 100, and then return all the results for every single transaction. And so it's just a lot more bulky of a just protocol because it's serving very different purposes.
00:07:06.644 - 00:07:20.924, Speaker C: So basically, in terms of collaboration with them, we probably potentially want to align the RPCs somewhat, a little bit closer to make them, or what do we think?
00:07:20.964 - 00:08:17.434, Speaker E: Yeah, I can see some use in just having the shape of the data structures be similar. It's always nice, you know, when you're, as an app developer, it's nice when you, the same data structures are used over and over rather than having completely different data structures, or even worse, two data structures that are very similar, but they're different in how they do casing or how they do naming, just in a subtle way, because then it makes it really easy to get mixed up and get it wrong. A canonical example of this is depending on what client you're using. For a long time it was input versus data as the data field for transactions. Sometimes input, sometimes data, depending on what context you're in. And this was super confusing for app developers because everything else is the same except for that one field was named differently. And so I think with each simulate and their thing, they get logs thing, I think just having similar data structures where it makes sense may be valuable, just in terms of synchronization.
00:08:17.434 - 00:08:20.994, Speaker E: Again, just for ease of use by app developer.
00:08:23.334 - 00:08:29.666, Speaker F: I guess the use case for that is shadow logs. Or is that going to shadow logs.
00:08:29.690 - 00:09:03.050, Speaker E: Things you're talking about the overlay get logs. The use case. Yeah, the use case makes sense to me. So some contract developer writes some contracts and you're using those contracts, or maybe you're building some sort of analytic software that cares what was happening in those contracts, but the app developer or the contract developer didn't think to put in the logs you need. Right. And so you now want to do some, you want to do some big analytics on what's going on internally, but the logs are simply not present. And so you're saying I want things to execute exactly as they did.
00:09:03.050 - 00:09:22.778, Speaker E: I don't want to change execution, I just want to change, I just want different set of outputs. I want you to give me different details about what's happening. And you can do this with tracing. Tracing tells you literally everything that happened, every single operation, opcode. But that's really heavyweight. To do a full trace on 8 million blocks is insane. That would take you weeks.
00:09:22.778 - 00:09:37.294, Speaker E: And so instead the idea here is we can do a much more narrowed sort of tracing that gives you very kind of the deep inner details that you want without needing to do a bunch of really heavyweight stuff on the node.
00:09:38.154 - 00:09:41.186, Speaker F: I guess etherscan and explorers are doing.
00:09:41.370 - 00:10:14.140, Speaker E: That tracing everything, I believe so they have the advantage though is that, well, sort of. So they have their own internal indexes that they maintain. And those indexes, I think, do require them to do tracing on every single call. But if they want to add some new index, they need to retrace the entire blockchain. Right. Because they need to go re do everything and build a whole new index. Whereas if you're trying to do some data analytics and you just kind of want a one time answer or you want to go back and get some data, but you know where to find that data.
00:10:14.140 - 00:11:05.724, Speaker E: You don't necessarily want to retrace the entire blockchain. You want to spend two weeks on a big beefy server just running your, retracing the whole blockchain just so you can pick out one new piece of information. And so I think the idea with this overlay git logs is you can get that exactly what you want at a fairly low cost. And it does come with a disadvantage. And I think this sounds like from my talk with Vantec that this is okay with them. It does come with a minor disadvantage that if the contract of interest to you doesn't always fire a log in the executions where you need the data, you don't have a good way of figuring out which transactions to re execute. And so they're using this concept that we're assuming, starting from the assumption that some log was fired to let us know this contract was executed so that we know to re execute it with this new code in place that will fire new logs.
00:11:05.724 - 00:11:46.784, Speaker E: If you don't have that, like let's say you had some contract that was deployed and it's always called internally and doesn't fire any logs ever, then you wouldn't be able to use this to redo it because you wouldn't know which transactions you need to rerun. Again, disadvantage. But I think the trade off there, I can see definitely why that's worth it. Because that's almost an impossible problem to solve in a reasonable amount of time. Unless you just literally index the entire blockchain with some petabyte index set of indexes that indexes like every opcode and every contract that was called, then maybe you could do something like that. What does trace filter?
00:11:53.804 - 00:12:01.904, Speaker F: But I wonder what this would mean to us. That this doesn't sound something that we can implemented each simulate and it should be a different method.
00:12:04.204 - 00:13:12.094, Speaker A: Yes, I don't think we should manage this, in my personal opinion. So as for, I mean, honestly. Well, Aragon went ahead and merged this, so. But just, just thinking, for just thinking about this method, it seems like yes, you will have to do a lot of re executions, especially when coming from is get blocks. You might think that large block ranges are okay, but that's because we fetch stuff from database. But here you would have to actually re execute all those transactions and blocks, which is quite a lot more overhead. And my second concern is that it's easy to shoot yourself in the foot, because when you're doing the state override, let's say if you add a view function, that's fine, but if you add another function that kind of changes the state in different ways, like it also calls another contract that modifies something, then I don't know, like what are the implications of that exactly.
00:13:14.314 - 00:13:34.854, Speaker E: I think you definitely get weird results out. But from my understanding of the use case is the use case is primarily around basically just more, getting more, extracting more data out of the executions. Not about changing the behavior at all. In fact, I think they explicitly don't want to change behavior. They just want more data out of what happens when those transactions were executed.
00:13:37.074 - 00:13:50.414, Speaker B: So I think this is all about leveraging new Oregon archive index that is known per transaction. This so they can not execute anything beyond what they have to.
00:13:50.534 - 00:13:55.206, Speaker E: So probably what is this new index from ergon.
00:13:55.270 - 00:14:16.914, Speaker B: So before they were indexing divs per block and now they want to. I'm not sure if this release or not, but they will want to index them by transaction. Right. So they can execute one transaction from the middle of the block without. Without executing any prior transactions.
00:14:19.014 - 00:14:25.134, Speaker E: Just to make sure. I understand you're saying that basically they're storing a pre state for every single transaction rather than just a pre state for a block.
00:14:25.174 - 00:14:29.634, Speaker B: Yeah, yeah, exactly. Exactly. That's what they are trying to do now.
00:14:30.694 - 00:14:32.798, Speaker E: More archival version of an archival node.
00:14:32.926 - 00:14:43.278, Speaker B: Yeah, that's Aragon, what, three or four, whatever. I don't know what number they are and. Yeah, so that's probably leveraging that a lot.
00:14:43.366 - 00:14:44.034, Speaker E: Yeah.
00:14:44.624 - 00:14:50.964, Speaker B: So unless you have similar database design, this might not be completely for you.
00:14:52.064 - 00:15:20.436, Speaker E: Yeah. I think if you wanted to do this in a client that did not have such a database, at best you'd have to use an archive node and you'd have to re execute all the transactions up until the one that was interested to you at the very least. I am kind of curious just. I don't know if I'm guessing no one here knows the answer off the top of their head.
00:15:20.460 - 00:15:23.180, Speaker D: I'm curious how big the archival database.
00:15:23.212 - 00:16:01.144, Speaker E: Would be if the index included every contract that was touched as part of each transaction's execution. So for any given transaction, you would index, ok, these transactions were touched within that transaction. Sorry, these addresses were touched within that transaction. Touched meaning either called or had, I guess just called. Yeah, called or had like a non account change or code change. So less than 420gb. No, it's got more than 420gb.
00:16:02.984 - 00:16:03.844, Speaker C: Okay.
00:16:04.864 - 00:16:15.444, Speaker B: Thinking about it a little bit. Well, hard to say. Well, it should be a few gigabytes. Sorry.
00:16:16.024 - 00:16:44.204, Speaker E: I would think it would be measured in ones or tens of terabytes because an archive node which just stores the state, de duplicated state at that for all of history is measured in terabytes. If you then wanted to take every single transaction, I guess. Okay, maybe that's the wrong way to look at it. So I guess we want to look at the total number of transactions in all of history and then kind of average out. Okay, how many addresses were touched on average in each one? And that would be your number. Ballpark.
00:16:49.264 - 00:17:04.526, Speaker C: When you say that, like to keep all the utilized addresses, do you mean index? It would mean not only index the addresses that were directly called by also indirectly, for example.
00:17:04.550 - 00:17:39.760, Speaker E: Yes. And ideally do it have the index both go transaction to address and address to transactions. That way you can say for this address, tell me all the transaction, show me all the transactions in all of history that ever touched this address in some way. And that way, in a single small query, you do a single index lookup and you would get a list. And in terms of performance, you get a very high performance in terms of trading storage performance set. Right. So your index gives you a really fast return result on all the hits of touches of this contract and all of history.
00:17:39.760 - 00:18:06.748, Speaker E: But you pay a lot in terms of disk space. Yeah, true. Blocks has an interesting solution to this. I looked into it a while back. They don't track calls, though, I think. I think they only track when an account was referenced, but not when it was called. I think it's slightly different.
00:18:06.748 - 00:18:15.564, Speaker E: But yes, maybe that's a good kind of proxy for size wise, because I think their thing is on the order of single digit gigabytes for their indexes.
00:18:21.544 - 00:18:25.496, Speaker F: Should we discuss about each simulate forward?
00:18:25.560 - 00:18:26.404, Speaker E: Probably should.
00:18:29.824 - 00:18:31.324, Speaker F: Oleg, do you have updates?
00:18:33.104 - 00:18:53.224, Speaker C: Yeah, currently basically cleaning up the code for the future last review to. For it to be merged. Haven't yet looked into the hash difference between Nethermind and guess, but we'll do this soon on the local hype.
00:18:56.284 - 00:18:57.224, Speaker F: Kashina.
00:19:00.124 - 00:19:57.214, Speaker A: Yeah. So last week I looked into some reports from killery and did some fixes. Basically, our base fee was computed incorrectly when there were phantom blocks in the middle. I also added withdrawals to the block result as well as excess block gas and block as used. Now it is kind of interesting question what to return for excess blob. Guess when there are phantom blocks in the middle.
00:20:01.084 - 00:20:07.224, Speaker E: And it's because the excess blog gas calculation is a function of prior blocks, right?
00:20:07.724 - 00:20:09.664, Speaker A: Parent, basically, yes.
00:20:10.004 - 00:20:13.164, Speaker E: Is it just parent? It's not like parent.
00:20:13.244 - 00:20:17.504, Speaker A: I mean, I guess the parent then depends on the parent. So it's like moving average.
00:20:17.804 - 00:20:25.694, Speaker E: Yeah. So very similar to 1559 pace fee. Yeah, exactly. So we just do the same thing.
00:20:26.594 - 00:20:46.346, Speaker A: That's what I did. So basically I. What I did was ignore the fact that there were phantom blocks. And I just assumed that the last block was the last real block. We just want to make sure we do the same thing on Nethermind as well.
00:20:46.410 - 00:20:48.214, Speaker C: Yeah, we do the same thing.
00:20:48.614 - 00:21:05.302, Speaker F: Okay, that sounds good. Then there was the other issue when the gas price is zero, that the base fees is set to zero on the EVM, but it still returned something else for the base fee. That was something that was different between get another month.
00:21:05.478 - 00:21:06.190, Speaker D: Yes.
00:21:06.342 - 00:21:37.814, Speaker A: Yes. So we have this. We have this logic that if the gas price is zero doing like basically doing if simulate by default we set the base fee to zero for the purpose of exposing it to the EVM. So when the contract calls the base fee opcode, then it returns zero. And we did this because.
00:21:39.794 - 00:21:40.322, Speaker F: It seems.
00:21:40.338 - 00:21:52.574, Speaker A: To be a common pattern to do TX gas price minus block base fee. And it would have returned a negative number in this case, which is like kind of undefined.
00:21:55.114 - 00:22:33.154, Speaker E: Yeah, didn't think about the negative number part. It's unfortunate though, because hypothetically, one could design a contract that is dependent on base fee never being zero because it is technically impossible to reach zero. It can never go below seven, just due to the way the formula works. And so one could imagine a hypothetical contract that, you know, broke very badly if it was ever zero. I think though, that the issue you were worried about about return negative numbers is worse. And so I think I'd rather have the problem of some obscure random contract breaking. And because you can always fix that by just specifying a base fee, right?
00:22:34.974 - 00:22:37.374, Speaker D: Yeah, if you specify gas price, then.
00:22:37.414 - 00:22:44.264, Speaker A: It'S going to end. You can also specify base fee. Well, yeah, you'd have to have a gas price as well.
00:22:49.204 - 00:22:51.264, Speaker F: Does it always set the base fee?
00:22:53.524 - 00:24:33.704, Speaker C: Yeah, we are looking for so there is a comparison between the gas spent and gas owned and gas available in a block. If you would post me this issue once more on Trello, I probably would be able to look at it again and fix it soonish. And also, I had a question. Which hive test currently would provide us with the like as complete pre built block and transactions as possible? Like in which ones do we have? In which test do we have as much defined as possible in terms of like block data and transaction data? So basically I want to check a case when, say, we had every field in the block possible filled up every field and transaction if possible filled up. And if there would be difference in hash between us and guess, then I'll know that there is something fundamentally different. And if there will be no difference, I'll be able to eliminate field by field 1, second, third, three, four, etcetera, and find out the specific field that is subject to change of the hash. So if we could have such a test where we have field as much as possible block and field as much as possible transaction, that would be perfect.
00:24:34.444 - 00:24:43.624, Speaker E: I think the easiest one is just does it need to be contract calls or can it just be a bunch of ETH transfers? I think it's very easy to fill a block with just like thousand ETH transfers.
00:24:48.364 - 00:24:49.624, Speaker C: Two would be enough.
00:24:51.244 - 00:24:53.384, Speaker E: Two they said you won the block full.
00:24:54.054 - 00:24:56.230, Speaker C: Oh yeah, yeah.
00:24:56.302 - 00:25:17.814, Speaker E: So the, each ETH transfer is. I guess there's two way to do this. One, you write a little contract. Even then still the each, the gas cost is going to be 21,000 each. Right. And so if you have a 30 million, I guess you set the block size really low. So if you just set your block limit to 42,000.
00:25:17.814 - 00:25:28.664, Speaker E: 42,000, you can have two trips, two transactions. They're ETH transfers. He's talking exactly 21,000. They don't do anything. It's a very simple version of this test. Would that be sufficient for you? Yep. Yep.
00:25:28.664 - 00:25:38.464, Speaker E: Okay. You don't need anything fancy that the contracts don't need or the transaction don't need to do anything special or be fancy or anything. They just need to have transactions and fill the block gas.
00:25:39.324 - 00:25:42.184, Speaker C: Yeah, that would be perfect. Basic start.
00:25:42.564 - 00:25:46.544, Speaker E: Okay. Is that something that you can do, Kalura, you think?
00:25:47.694 - 00:26:10.766, Speaker F: Yeah, I can do that. I just said that the code that just returns the base fee and at the moment get. And Netherland is disagreeing on the output on this. And get this returning zero and then Nettermount is returning something else. It's returning non zero base fee. And this is what we are discussing with Sina.
00:26:10.950 - 00:26:51.804, Speaker A: Right. And here I would like to point out another quirky, I'm kind of open to changing this, but basically the base view opcode, in this case in gas returns zero. However, when we return the base v as part of the block result, then it's a real base view value, it's not zero. There is this inconsistency that I noticed when Killer pointed out the issue, but I guess I'm open to changing this based on the result of the discussion here, like seeing what we want to do in general.
00:26:53.104 - 00:27:08.264, Speaker E: So help me better understand this negative number issue then if you were to have the opcode base fee that was the same matched whatever you would have returned or whatever you returned as part of the base fee for the block, why would that yield a negative number?
00:27:08.304 - 00:27:09.084, Speaker D: Exactly.
00:27:11.104 - 00:27:18.584, Speaker A: That's when you would do like TX gas price minus block base fee because gas price would be zero.
00:27:18.704 - 00:27:28.680, Speaker E: In a simulation I see you're allowing zero gas price even when the base fee is greater than zero, which in reality can never actually occur. Yeah, exactly.
00:27:28.712 - 00:27:30.484, Speaker A: Gas price should always be higher.
00:27:31.424 - 00:27:45.574, Speaker E: So I guess this is not a problem where in gaffer nethermined you have to deal with this negative number. This is just someone's solidity contract that was designed to under the assumption that number can never be negative would break. And so the solution could be solved.
00:27:45.614 - 00:27:47.126, Speaker D: By the purely by the user, by.
00:27:47.150 - 00:27:55.434, Speaker E: Just if they have such a contract, they could then set the gas price to larger than the base fee. And that would fix it, right?
00:27:59.534 - 00:28:04.584, Speaker A: Yes. If they specify the gas price then that would fix it.
00:28:05.404 - 00:28:50.944, Speaker E: I think I changed my position then. I think having the opcode return the same thing that the block will return is more important because in both cases, whichever route we go, the user can ultimately solve their problem by just specifying the gas price in the base B explicitly. And so I don't care much about if someone's random contract that does something weird like this breaks in any simulate and they have a workaround that allows them to make it not break. I'm totally fine with that, but I feel like it's worse, it's better to have a user, some users have to do that than to have inconsistent results such as your EVML code doesn't match your JSON RPC results. I think my position is now those two should match.
00:28:54.224 - 00:28:59.444, Speaker F: Yeah, I think it's also very weird when I'm getting different basefeed and it's blockage returning.
00:29:03.144 - 00:29:07.044, Speaker A: But we do also have the option of returning zero in the block result.
00:29:10.664 - 00:29:12.456, Speaker F: Yeah, yeah, that's one option, yeah.
00:29:12.480 - 00:29:27.748, Speaker E: But I mean, we already have the code for correctly calculating the base fee for some definition of correct. And I think I prefer personally to keep that. I'm not super strong on that semantics.
00:29:27.796 - 00:29:41.892, Speaker A: Of gas price being larger than base fees, kind of. I mean, it is part of the yellow paper, let's say, or the protocol.
00:29:41.948 - 00:29:52.554, Speaker E: Could we just, it is one of those things like validation mode on versus off. If you have, if we had a validation mode that we all agreed on and it was on, then this would not be a problem. Would that be an accurate assessment?
00:29:53.574 - 00:29:54.394, Speaker F: Yep.
00:29:55.774 - 00:30:21.214, Speaker E: So this is purely a problem of we're trying to make things easy for the user by setting things to zero that normally cannot be zero and allowing and ignoring, not doing validation of things that don't make sense. And so we don't want to just return an error if the user ends up in a situation where the base fee is higher than the gas price, because we want things to just work and be simple. But that then leads to weird results that don't make sense.
00:30:21.594 - 00:30:22.734, Speaker C: Should it work?
00:30:24.234 - 00:31:01.104, Speaker E: This is the whole debate of validation mode versus non validation mode, right? In validation mode we definitely, this should not work. You should just get an error and it should just tell you you're doing something dumb. Don't do that in non validation mode, we're trying to make it. It's the difference of like typescript versus statically typed versus dynamically typed language, right? Diagnose type language stops you early and fast and says no, you made a mistake, don't do that. Whereas dynamic language is like, yeah, sure, go ahead and add your number in your string. We'll figure out, we'll make that make sense, sort of. And then sometimes the dynamic language, you're going to get a weird result out at the end because the language let you do something that doesn't make sense.
00:31:01.104 - 00:31:12.674, Speaker E: And the thing is, the same general line of argumentation here where the validation mode is the statically typed equivalent, stop you early from doing things that don't make sense to the system.
00:31:15.574 - 00:31:38.654, Speaker C: So my question would be then can we have some formulation that would like create default values that are like, that could potentially work with validation mode on and won't like in most scenarios, and keep and place them into validation mode off also.
00:31:39.154 - 00:32:32.028, Speaker E: So I think the problem we run into is that we're going to have invalidation mode off. We're going to have to violate something if we have the goal of making it so users transactions just work. The primary reason for this is we want to make it so people can execute things without needing money to cover gas fees. And since base fee can never actually go to zero, technically it means either we need to allow base fee to go to zero, which again is against consensus protocol rules, or we need to allow the gas price to be lower than the base fee and the transaction still execute, which is again against consensus rules, or we need to. I think those are two options, right? In either case we're violating a consensus rule. We have to choose which consensus rule we violate. Or I guess the third option is we don't allow people to have gasless transactions, but I'm pretty sure we want people to have gasless transactions in non validating mode.
00:32:32.156 - 00:32:43.424, Speaker C: Can we insert like same defaults for transaction that is given to us as gasless, but to fill the gas values for it automatically?
00:32:45.044 - 00:33:27.456, Speaker E: I guess one option is you say the defaults are transactions are not gasless, but we automatically fund every account with some gas money. So you just give everybody ethnic problem there is. You're going to run the weird things where people aren't expecting accounts to suddenly mint ETH out of nowhere. And so you can then be a little smarter about it and say only give ETH to transaction. Only give ETH to accounts that are executing where the account is the to address of a transaction. And that address doesn't have any ETH at all. Like, it balances Ethan exactly zero and only give it enough ETH such that gas limit times max v per gas is the amount you're giving it.
00:33:27.456 - 00:33:53.774, Speaker E: So that would be a way. I believe it's the client developers. It sounds like it's adding quite a bit of complexity to our implementation to do something like that. But that would solve the problem as well. So that way it's you can submit transactions from any address, doesn't need to have ETH in it, but the transactions are fully valid by default. And the reason they're valid is because we're basically covering your gas fee by just minting you some ease beforehand. Unless you explicitly tell us not to mint you ETH, in which case we will then not do so.
00:33:53.774 - 00:34:05.294, Speaker E: If you explicitly say, you know, set this accounts balance to zero as part of the override, then we would not do that thing, and instead you would get bizarre, weird results.
00:34:06.234 - 00:34:09.614, Speaker C: That last part made it all a bit more complicated.
00:34:10.274 - 00:34:42.614, Speaker E: Right. Yeah, I agree with bandtag. It is definitely useful to sometimes call from addresses that have no ETH. In fact, we went to quite a bit of length to make it so that's possible. We're overriding the rule that you can't initiate transactions from a contract specifically, so you can do exactly that. So I guess it's complexity versus violating rule a versus violate rule b. Those are our three choices.
00:34:42.614 - 00:34:49.894, Speaker E: Or take away useful feature.
00:34:51.514 - 00:34:52.746, Speaker B: Would you elaborate?
00:34:52.810 - 00:34:55.294, Speaker C: Which rules? Rule a and rule b.
00:34:55.874 - 00:35:14.444, Speaker E: So we'll say rule a is. Rule a is the base fee cannot go below seven. That's a consensus rule. Like the spec doesn't allow it to go below seven, ever. Rule B is that the gas price can never be less than the base fee of the Bach.
00:35:18.224 - 00:35:34.214, Speaker C: I think the first one where we allow it to be less than 7 may be a bit easier because then all other stuff would still work. Regarding the. Like, always positive stuff like that.
00:35:35.034 - 00:36:13.958, Speaker E: Yeah, like, I mean, I can imagine a contract that breaks in either case. I think both cases are unlikely to be broken. I think it's just we just have to make a call. I'm okay with either, really. You'd imagine a contract that, you know, some math doesn't work if the denominator is zero, like divide by zero error, and they assume that the base fee and that's the denominator of some function. But if that's when you guys want to go with M finder Sina, what would you suggest?
00:36:14.126 - 00:36:37.454, Speaker A: Well, I mean, we already have this implemented as basically we took route b. But I think regardless of which one of these rules we choose, would you have to also specify what to return in the block result?
00:36:42.594 - 00:36:46.934, Speaker F: If base fee is not defined by the block, should be set it to zero? I guess that's the question.
00:36:47.854 - 00:36:48.674, Speaker E: Yep.
00:36:50.614 - 00:37:03.034, Speaker F: Like at the moment it's assumed that it's going according to the normal rules, but now in my example, the user is not defining it, so it becomes that transaction would normally fail.
00:37:10.134 - 00:37:41.704, Speaker E: Just answer Siena's question. My preference is that we should always return the same thing in the EVM and the JSON RPC result. So if within the evm the base v is seen as zero, then we should return zero as the base fee for that action block. Whatever. And that one I do feel a little bit strongly about. I'd argue for a little bit support that one. I have very little care whether we go with options from one or two here for the other.
00:37:45.924 - 00:37:50.984, Speaker F: But should we set the base default base feed to be zero if this user is not setting it?
00:37:52.844 - 00:37:59.104, Speaker E: Yeah, so if we went with one then I would say yes. The way to solve this is just to set the base feed to zero.
00:38:00.164 - 00:38:14.264, Speaker F: That would mean that if you set the validation mode true, then it would always fail unless you set calculate the base view yourself. At the moment we have to multi simulate calculate the base field for yourself so you don't need to calculate.
00:38:17.724 - 00:38:19.092, Speaker E: Yeah, we can make it so validation.
00:38:19.148 - 00:38:19.824, Speaker D: True.
00:38:21.604 - 00:38:46.474, Speaker E: Sets the base fee accurately, but you don't have this other problem or other. This other problem doesn't exist because we would also default the gas price to be greater than zero in validation mode. So both of them would be valid and higher. And then in non validating mode we would allow we default the base fee to zero and the gas price to zero.
00:38:48.174 - 00:39:03.324, Speaker C: I think there is one more approach potentially we could take. We can replace the sender of the transaction with something like system address or. This is not a good idea.
00:39:05.224 - 00:39:26.004, Speaker E: I think we still have the same problem in the end because I think the same bug will show up if the user specifies a source address from, but they don't specify the base fee or the gas price. So we still have to solve this either way. What is the default from address that we're using right now? Is it zero?
00:39:28.624 - 00:39:31.524, Speaker F: Yes, I think that depends on the client.
00:39:32.784 - 00:39:36.644, Speaker E: Oh, we should make sure those are the same. Do we have a test for that?
00:39:38.264 - 00:39:43.632, Speaker F: I mean, ETH call I think it depends on the there, but I don't think we have tests for that.
00:39:43.808 - 00:39:54.764, Speaker E: No, we should, we should get a test that just leaves the from out and has some contract that outputs different results based on what the from address was.
00:39:55.224 - 00:39:56.004, Speaker F: Yeah.
00:39:57.704 - 00:40:20.104, Speaker E: Maybe just something as simple as just return the from address. Does anyone have strong opinions on whether to go with one or two? Here for non validating mode, do we allow base fee to be zero or do we allow gas price to be less than base fee?
00:40:23.244 - 00:40:28.144, Speaker C: Not really. I can go with whatever is comfortable for gas.
00:40:29.404 - 00:40:33.064, Speaker B: I would go with gas price being lower than base fee.
00:40:34.044 - 00:40:46.544, Speaker E: You prefer that one? Okay, it sounds like Cena has a mild preference for that. Just because you already have the code written, you wouldn't have to change anything. So we've got two people that have a very mild preference for two. Let's go with two.
00:40:47.004 - 00:40:52.904, Speaker A: Well, actually I have my preference for one because that's what we have to pay for.
00:40:55.644 - 00:41:03.024, Speaker E: Okay, so Geth has two written another mind would like to write two but has one written. And Geth would like to write two but has one written. Got it.
00:41:14.504 - 00:41:19.324, Speaker F: When those are different, we don't get any negative numbers anywhere if you allow that.
00:41:19.824 - 00:41:55.084, Speaker E: So in the EVM, you would have a negative number, which is basically is likely to overflow in someone's contract somewhere. So someone's contract does gas price minus base fee. They're expecting that number to be positive. It's going to overflow and solidity checks may even say this is a checked math that will always give you a positive number. But in simulate that number could actually become negative and therefore you're going to overflow and it's going to zero. Xff I'm really, really, really big number and then something's going to break. It's all hypothetical.
00:41:55.084 - 00:42:02.584, Speaker E: I can't imagine why someone that's not sure I can imagine people writing contracts to do gas price minus base fee.
00:42:03.404 - 00:42:10.054, Speaker F: But what should tell my call return? Should it return that zero or should it return the normal base fee?
00:42:10.594 - 00:42:17.170, Speaker E: If we went with one, it would return zero if the user did not set the base fee.
00:42:17.242 - 00:42:18.174, Speaker F: Yeah, it's not set.
00:42:18.474 - 00:42:26.814, Speaker E: Yeah. So it would return zero in your little call there. If we went with two, then it's going to return whatever the calculated base fee is.
00:42:32.334 - 00:42:51.194, Speaker F: I think that's what Netherland is doing because that's where it's returning. But I guess with the tiro, the problem is that if you set the validation model true, then I think that would fail. Even if you add the gap, you have to override that block. Basically.
00:42:54.174 - 00:42:59.144, Speaker A: Fail with defaults anyway because gas price should be set explicitly.
00:43:02.244 - 00:43:13.704, Speaker F: Oh yeah, that's true, but yeah, but when you set that then it's the base, other than the base fee, I guess it gets calculated, it's validating the base fee, I guess, with the validation mode.
00:43:20.884 - 00:44:02.164, Speaker E: So the one argument I have for one, which I'm not going to defend this, but there's argument I can think of, is that two is much more likely to happen in the wild. I think one is much less likely to happen in the wild. I think there are probably zero to one contracts in the wild today and possibly in the foreseeable future that have like something divided by base fee or they're going to get a divide by zero error, whereas the number two scenario, I think actually exists in the wild today and is not uncommon. Any contract is trying to calculate gas premium for the transaction that's being executed, which is frequently used in things like meta transactions and stuff like that.
00:44:05.504 - 00:44:16.204, Speaker A: I think this corner case was found by Peter because he has this chad NFT project that basically you can.
00:44:19.164 - 00:44:19.476, Speaker E: You.
00:44:19.500 - 00:44:29.904, Speaker A: Can take ownership of an NFT when the base fee goes higher than the latest claim. So he certainly faced this issue and that's why he fixed it.
00:44:32.044 - 00:44:38.724, Speaker E: I say, and he faced the issue because you guys had number two implemented, right? Yeah.
00:44:38.764 - 00:44:43.144, Speaker A: Number two is the default. Number two is if you don't do anything, then you have number two.
00:44:51.484 - 00:44:56.784, Speaker E: And Lucas, you said you preferred two or you preferred one. Sorry, I forgot.
00:44:57.884 - 00:45:00.224, Speaker B: Well, I prefer two.
00:45:01.364 - 00:45:09.608, Speaker E: Okay. And why don't you like, why don't you like one? What else they are.
00:45:09.696 - 00:45:25.352, Speaker B: We can do one, but I don't know, basically calculation was pretty well defined. I'm not sure if. What, what would it need to change it? Maybe it's not a big deal. I don't know.
00:45:25.528 - 00:45:32.044, Speaker E: Yeah, I mean, I think all of us agree that it's not a big deal either way. We just need to pick one. Yeah.
00:45:32.984 - 00:45:45.294, Speaker B: Just one. Two, where like more in line what we're doing on other potential places. Right. Like was like, felt feels more natural to me, what I'm used to.
00:45:51.354 - 00:45:56.534, Speaker A: Yeah. Let's postpone this decision to next week.
00:45:57.154 - 00:46:02.514, Speaker E: Sure. If someone come up with a good argument either way, by next week, if not, we'll flip a coin and choose one.
00:46:04.014 - 00:46:17.114, Speaker C: Nice. And could you Kilari, please update the Trello with the issues that we are currently having so that I could brush up on them also if there are any.
00:46:17.614 - 00:46:26.874, Speaker F: Yeah, I had some issues with the test RPC and I guess Sina fixed it and then I can run it again. Then I can see what other differences now.
00:46:27.634 - 00:46:28.414, Speaker E: Perfect.
00:46:29.314 - 00:46:35.010, Speaker F: Or did you push the pr to the test RPC? But I guess needs to be merged or I need to refer to that.
00:46:35.202 - 00:46:47.894, Speaker A: You can. You can make those changes just like two lines. You can make it in your local repo and already do the testing until that gets merged upstream.
00:46:48.354 - 00:46:56.034, Speaker F: Okay. Do we have the girls?
00:46:58.774 - 00:47:23.930, Speaker A: Well, there was a topic that kind of was discussed shortly last week about removing phantom blocks. Now, Micah was pretty set against it, but we said we would discuss it this week as well. Just bringing it up in case.
00:47:24.082 - 00:47:29.002, Speaker C: Could you elaborate on what did we mean by removing phantom blocks?
00:47:29.098 - 00:47:46.014, Speaker A: Basically what I mean is we would require the user to provide every subsequent block, as in the number has to be strictly increasing by one.
00:47:46.874 - 00:47:49.514, Speaker C: Oh, why?
00:47:52.974 - 00:48:48.614, Speaker A: So? My reasoning was that phantom blocks are kind of weird, especially in places where we have. Where we have moving averages. For example, in the case of base fee and excess blob gas, these are things that move with each. Like, these are values that get adjusted with each block. And the approach that we took now is kind of unintuitive to the user, I think. Because when you have hundred empty blocks, that means the gaseous is zero. That means, like, for 100 blocks, the base fee should go down.
00:48:48.614 - 00:48:52.494, Speaker A: But we're not actually doing this.
00:48:52.914 - 00:49:21.038, Speaker E: Yeah, I think I see where you and I, our mental models differ for the phantom blocks. I've always imagined them as not empty blocks, but just like, you know, average blocks. Like, here's between block n and block m where you've got 100 block space. In my head, my mental model, I was like, okay, there's some blocks there. They're normal, average, whatever. Meaning the base fee doesn't change because they're middle of the road. Full middle of the road.
00:49:21.038 - 00:49:30.962, Speaker E: Not interesting. Just a bunch of ETH transfers or whatever. Again, just the mental model in my head sounds like maybe it's different. Which is perhaps part of why we're just green on this.
00:49:31.158 - 00:50:14.418, Speaker A: Exactly. I think it's like they are confusing. And we've had to kind of add a whole section to the documentation because of these phantom blocks. And basically it came up for me when implementing beacon routes. So the thing with, and we have that whole complicated thing with block hashes. So very unintuitive. We had to invent a scheme to calculate the block hashes for phantom blocks which is kind of just really hacky, it feels to me.
00:50:14.418 - 00:50:29.914, Speaker A: And we have the same thing now with phantom block. So we would have to invent another scheme for that because beacon routes can be. So we basically contracts. Can access the beacon routes for the last 8000 block.
00:50:34.094 - 00:50:36.918, Speaker E: What do you mean? The beacon route can access last thousand blocks.
00:50:36.966 - 00:51:01.934, Speaker A: What do you mean? The beacon routes contract. So basically users can request the beacon routes for the last 8000 blocks. And that means that when we have phantom blocks, we kind of have to somehow we have to return something there and we would have to add them to this system contract.
00:51:03.194 - 00:51:04.906, Speaker F: Is that a new thing that's coming.
00:51:04.970 - 00:51:10.294, Speaker A: Or that's already shipped? It was part of the last fork.
00:51:12.914 - 00:51:13.586, Speaker F: I didn't know.
00:51:13.650 - 00:51:20.334, Speaker A: I only realized recently that we actually, we need to add support for this in simulate and eat call.
00:51:21.424 - 00:51:41.684, Speaker E: Yeah, because this is the first time that we've had a basically contract update from the system. Right. Previously, contract updates always occur due to transactions that execute. I think it's the first time in Ethereum's history where the system itself wrote to some addresses storage.
00:51:42.864 - 00:51:56.094, Speaker A: Exactly. So at the beginning of block we would the have to do this extra processing to add the beacon route for that block to the actual try.
00:51:57.474 - 00:52:02.442, Speaker E: Right. So I agree with Sina wholeheartedly that.
00:52:02.538 - 00:52:05.042, Speaker D: Phantom blocks add a lot of complexity.
00:52:05.218 - 00:52:44.186, Speaker E: So I'm not disagreeing on that at all. I think for me I just see it as complexity that is worth it for the benefit of being able to have very large gaps of time that are accurately represented. Now, I think the thing that would convince me otherwise, like if I was going to argue against myself here, by devil's advocate, any well written contract should use timestamps for large spans of time. They should not be saying, you know, after a million blocks have passed, do x or have x as some function of number of blocks passed.
00:52:44.330 - 00:52:46.042, Speaker D: That is not the way you should.
00:52:46.058 - 00:53:04.494, Speaker E: Be writing your contracts. You should always be using a timestamp on Ethereum. If you care about is time passed, you should be using time stamp. You should not assume that blocks are 12 seconds. You should not make any assumption about stuff like that. And there are essentially no use cases I can think of where you actually care about how many blocks have passed.
00:53:05.054 - 00:53:13.846, Speaker C: But it would cost more, wouldn't it? To look at the time comparison in comparison with looking into the number.
00:53:14.030 - 00:53:54.458, Speaker E: In both cases it's just two numbers minus each other. So I think the cost should be identical. In one case you're just doing block timestamp minus your saved timestamp, or when you're doing block number minus your saved number. So I think the execution costs are identical in both. I believe the one place I can see you maybe counting blocks is if you've got some system where the number of blocks that have been produced since a previous thing occurred lets you calculate like the security parameter, some security parameter, right. And so you've got some. So your security, your system is a function of number of blocks passed between two points in time and it's not a function of how much time has passed.
00:53:54.458 - 00:54:37.364, Speaker E: So if, you know, a thousand blocks happen in 4 seconds, then you're just as secure as if 1000 blocks happened in a month. And so I can imagine that, but that's a pretty far fetched kind of construction and I don't think anyone's doing that. And so the argument against my position in favor of CNN's position is just that no one should be writing contracts that do, you know, block number minus prior block number. And so we don't need the ability to simulate, you know, large spans of block numbers. We just need this ability to simulate large spans of time. And we can do that by just allowing block n to have a timestamp of x and block n plus one to have a timestamp of x plus a million. And we currently allow that.
00:54:37.364 - 00:54:57.624, Speaker E: And so I think that's my argument against myself. I don't know if it wins. KALARI yes. On Ethereum mainnet today, blocks happen every 12 seconds. Oh no, slots happen every 12 seconds. Blocks do not necessarily happen every 12 seconds. Blocks happen on mod twelve second intervals.
00:54:57.624 - 00:55:34.684, Speaker E: That being said, it is not set in stone. That will never change. And in networks that are ethereum like, but not exactly ethereum, those numbers can change and one could easily imagine. And there are devnets that have that number different, that are Ethereum devnets, not public testnets, but the internal dev ones often change that number as well. And even like I think you can spin up netherminds and geth both with like their out of the box dev testnet and I think it's got 1 second block times. And so just generally a bad practice to use. Assume blocks for 12 seconds, which is so no one should be, no one should be doing that, especially when you have a literal clock.
00:55:36.944 - 00:55:38.644, Speaker A: How does the beacon routes work?
00:55:39.104 - 00:55:49.564, Speaker F: Is every block or every slot need to have a bacon routes? And if you are doing it simpler, creating new blocks, then we also need to calculate some kind of block route for every block as well.
00:55:51.144 - 00:55:57.164, Speaker E: I would love to know the answer to that question. What happens when you have a missed block? Does the beacon root get two updates in one block?
00:56:00.264 - 00:56:08.854, Speaker B: No, only the beacon block that is corresponding to the El block. We'll have the update.
00:56:09.234 - 00:56:22.374, Speaker E: Okay, so if you have a missed slot, you would basically update the beacon routes with a hole or, sorry, you'd update the beacon routes and that would result in a hole in the list?
00:56:23.994 - 00:56:31.294, Speaker B: No, because you can have same height El blocks on different Cl blocks.
00:56:31.674 - 00:56:39.372, Speaker E: Oh, I see. So it's a. Is the beacon roots are a function of El block number, not slot number or index number.
00:56:39.468 - 00:56:42.588, Speaker B: I'm not entirely sure. I would have to double check. Sina, do you know?
00:56:42.676 - 00:56:47.156, Speaker A: I think so, too. I think they are related to El blocks, not Cl.
00:56:47.340 - 00:56:47.804, Speaker E: I see.
00:56:47.844 - 00:56:54.664, Speaker F: Okay, so if you simulate ten blocks forward, do we also need to calculate those roots for those ten blocks?
00:56:56.204 - 00:57:03.534, Speaker E: If we were doing these shadow shadow blocks, then, yeah, we need to backfill. And this is. I agree, Sina. This does add some complexity that I hadn't considered.
00:57:04.194 - 00:57:12.842, Speaker F: I mean, it's just that very normally create new blocks that are one after another and no phantom blocks. Do we need to calculate some beacon routes for those as well?
00:57:13.018 - 00:57:18.986, Speaker E: Yeah, we need to update the beacon root state for each one of those. And we should have tests for that, basically.
00:57:19.090 - 00:57:36.506, Speaker C: Then the question would be that if I'm a user that wants to simulate block number one and block number million, I would need to stick in like, 999. 999 empty blocks with guest price zero for this same thing to work for me.
00:57:36.650 - 00:57:42.418, Speaker E: Yeah. Yes. And you would almost certainly get throttled. Or if you're running your own node, you'd crash your own node.
00:57:42.546 - 00:58:03.494, Speaker B: No, no, no. You don't really need to. No, no. You don't really need to put blocks anywhere. It's just you need to update the state correctly with 999 hashes in the state. So we don't really need to put any blocks. It's just a simple state update.
00:58:03.494 - 00:58:04.394, Speaker B: To be honest.
00:58:06.374 - 00:58:22.914, Speaker E: We have to adjust our JSON RPC API to support something like that. Or are you saying we would use the same API we've got right now, but instead of doing these phantom blocks, we would actually generate some blocks, get some hashes, update the beacon routes, et cetera, et cetera, for each one of these along the way.
00:58:23.454 - 00:58:31.434, Speaker B: Okay, that's what you mean. So why would you generate a block to get the hash? I don't get it.
00:58:32.694 - 00:58:55.954, Speaker E: If we got rid of this, if we just got rid of the whole concept of phantom blocks, then the client, there's no virtual blocks at all. Every single block actually was a thing that was executed. It may have zero transactions and executed execution may have been short, but it was treated exactly like. If you specified every single block along the way, it may have all defaults. That's fine, but you've got, we don't have.
00:58:56.034 - 00:59:01.706, Speaker B: Okay, this is about beacon routes, right? We don't have beacon routes. In ETH simulating.
00:59:01.770 - 00:59:08.026, Speaker E: Anyway, I think we're saying we should update beacon root. Yeah.
00:59:08.050 - 00:59:11.014, Speaker A: So we should add beacon routes to the block overrides.
00:59:11.374 - 00:59:12.086, Speaker E: Okay.
00:59:12.190 - 00:59:13.022, Speaker B: Okay.
00:59:13.198 - 00:59:16.270, Speaker E: And we should calculate it properly if you're not overriding it.
00:59:16.422 - 00:59:18.354, Speaker B: What do you mean, calculate properly?
00:59:18.934 - 00:59:29.998, Speaker E: Meaning as you iterate over blocks, we actually do fill in the beacon root merkle tree, or we have some sort of virtual function that conroute Merkel tree.
00:59:30.046 - 00:59:30.870, Speaker B: How would you feel?
00:59:30.942 - 00:59:40.242, Speaker E: Beacon root miracle, whatever the data structure is the storing all these beacon roots. Right. So in the contract, we're just filling.
00:59:40.338 - 00:59:45.414, Speaker A: Default beacon routes and that would maybe be the zero hash.
00:59:46.514 - 00:59:46.914, Speaker E: Yeah.
00:59:46.954 - 00:59:52.574, Speaker B: What data would you put as a beacon root of a block where you don't have an explicit beacon.
00:59:52.874 - 00:59:57.802, Speaker E: That's what you're saying. So you're saying for each simulate, we literally would just be making something up.
00:59:57.938 - 00:59:58.562, Speaker B: Yes.
00:59:58.698 - 00:59:59.842, Speaker E: To put in here.
01:00:00.018 - 01:00:01.774, Speaker B: Yes, exactly.
01:00:02.734 - 01:00:10.414, Speaker E: And so it's either a random number, like some sort of deterministic random number, or it's just a fixed value each time, like the zero hash or whatever.
01:00:10.574 - 01:00:26.006, Speaker B: Yes. It's either a fixed value. We can have like a consecutive counter because it's basically u in 256 in some other way. So we can have just a counter put block number there as a counter.
01:00:26.070 - 01:00:26.318, Speaker E: Right.
01:00:26.366 - 01:00:26.774, Speaker B: Whatever.
01:00:26.854 - 01:00:27.142, Speaker E: Right.
01:00:27.198 - 01:00:28.674, Speaker B: We can do multiple things.
01:00:29.004 - 01:00:55.424, Speaker E: Yeah. Okay. I think I'm very slowly coming around. I'm slowly coming around to Cena's point of view, just in that this is one of those things where there's like lots of little tiny bits of complexity that are just starting to add up. And beacon roots is just simply the latest one where our phantom blocks are adding lots of little tiny bits of complexity. No individual one is bad. We can solve all of them, but there's a lot of them.
01:00:55.584 - 01:01:06.964, Speaker B: In my OpInion, phantom blocks don't add complexity here too MuCh because you have the same complexity. If you just don't override this, you still have to put something tHere.
01:01:07.824 - 01:01:26.380, Speaker C: And in my opinion, users would just fill in with nearly empty dummy Data. And I think there won't be an issue for the node to process block that is EXtReMElY empty. It won't consume any, any real gas. So, yeah, so let's say.
01:01:26.452 - 01:01:32.744, Speaker A: Let's say we simulate block n and block n plus 1 million.
01:01:33.444 - 01:01:34.224, Speaker B: Yes.
01:01:39.524 - 01:01:46.292, Speaker E: So if we don't have phantom blocks, then you are going to have to do some amount of execution for each of those. No, no.
01:01:46.388 - 01:01:51.260, Speaker B: Keep phantom blocks. If you didn't override blocks.
01:01:51.372 - 01:01:52.784, Speaker E: Sorry, I'm.
01:01:54.564 - 01:02:03.264, Speaker B: Roots. Then you just return empty hash for it. And that's to be done with it. Right. Regardless if you have phantom or non phantom blocks.
01:02:04.724 - 01:02:21.064, Speaker A: So the point here is that you would need to go ahead and do this state updates for all of these 1 million blocks, or you would have to write some specialized code that does this lazy evaluation.
01:02:23.284 - 01:02:24.064, Speaker E: Yes.
01:02:24.964 - 01:02:27.864, Speaker B: And I will draw the lazy one, to be honest.
01:02:28.444 - 01:03:03.664, Speaker E: All right, I agree. I think that's like, from a performance standpoint, lazy one's definitely the way to go here. But that's now, again, more complexity in the clients to support phantom blocks. One more small piece of complexity, but it's additive to all the other complexity we're getting just for support for phantom blocks. And so I think the question that Sina is, the broad question Sina's trying to propose, and I'm slowly coming around to, is that we just keep adding little. The phantom blocks are adding lots of little pieces of complexity, and it's not clear that they provide the commensurate value to users, especially since users should be doing timestamp based stuff anyways.
01:03:04.504 - 01:03:28.336, Speaker A: And my question here is that given that we want to support these features, is it possible somehow to override something in those contracts to make their time span a bit shorter so that they do fit in $200 that I can actually do simulate on without automatic, not.
01:03:28.360 - 01:03:31.118, Speaker D: Automatically, like the things people do with.
01:03:31.216 - 01:04:05.964, Speaker E: Stamps are often complex defi lego things where you're like, okay, I need to calculate continuously or per second compounding interest. And so they have some function that calculates compounding interest over some span of time. And that's going to be very wrong if you don't have the proper amount of time. That being said, anyone doing that should be using time stamps. And so removing phantom block support would not break them if they correctly use time stamps. It only break them if they had compounding interest as a function of block number. Or it's like each block compounds the interest a little bit.
01:04:05.964 - 01:04:14.184, Speaker E: And if you're doing that, which again, is very bad idea, you shouldn't do that. But if you did that, and we don't allow phantom blocks, then you can't simulate into the future.
01:04:17.404 - 01:04:22.224, Speaker F: The block has somewhere that also breaks, I think.
01:04:22.684 - 01:04:24.024, Speaker E: Why the blockage break?
01:04:26.264 - 01:04:34.724, Speaker F: Because if you want to change a transaction that refers a block has, and then you want the block has to match something, then you might need phantom blocks to do that.
01:04:35.624 - 01:05:27.204, Speaker B: Okay, I have a very, very cool idea. Okay, so if someone supplies you the overrides, then you overwrite, right? That's simply simple thing. But if someone doesn't supply you overwrite, you just do nothing. And because this, if I remember collect correctly, this is like a round robin list that you go through it in the actual implementation, then you would just return some data, that is some from some other block, but it would be consistent in different implementations. You just do nothing and you just return it.
01:05:27.904 - 01:05:32.984, Speaker E: So you're saying that we have some kind of placeholder pre built blocks that we just all agree on?
01:05:33.144 - 01:05:43.252, Speaker B: No, no, no. I'm not saying it because you're starting from some state, right, from the, from the main blockchain and it has some values there, right?
01:05:43.328 - 01:05:44.236, Speaker E: Yeah, yeah.
01:05:44.420 - 01:05:49.704, Speaker A: So what if the chain is less than 8000 blocks old?
01:05:52.324 - 01:05:59.344, Speaker B: Then there would be zeros if you read them, right?
01:05:59.804 - 01:06:00.624, Speaker A: Yeah.
01:06:03.844 - 01:06:04.932, Speaker B: So no problem.
01:06:04.988 - 01:06:05.624, Speaker A: Yeah.
01:06:06.044 - 01:06:31.924, Speaker C: I think also to counter Mikkel point, one thing is that if you allow users to override timestamp and at the same time you limit the ability to override block number, it would feel like a pain because you want to override them both together. You want your phantom blocks to exist if you can have phantom time.
01:06:35.904 - 01:07:21.100, Speaker E: So yeah, I mean that is the broad general argument for phantom blocks is allowing users to override anything that includes down to the block number. I struggle to think of legitimate cases where a user actually cares to override the block number. Vantec, if you know of some situations where overriding block number useful, that would be super helpful. I'm struggling to think of cases where you know you want to do that and you know it makes sense not just you're doing something stupid or you should have been using timestamp. We can continue discussing this and it is late. Sorry, I didn't realize it was past our normal stopping time. Perhaps we continue discussing this in telegram.
01:07:21.100 - 01:07:26.824, Speaker E: And like I said, I'm starting to come around so maybe I can re engage with you there. Sina, more on this?
01:07:27.634 - 01:07:28.894, Speaker A: Yeah, sure.
01:07:33.994 - 01:07:42.974, Speaker F: I guess we need to discuss the bigger routes on how we are going to implement those. If you want to add those as well, we can continue that in telegram as well.
01:07:44.434 - 01:07:49.810, Speaker C: Yeah, let's do async and continue next week. Thank you all guys for coming.
01:07:49.882 - 01:07:53.184, Speaker E: See you. Thank you. Bye. Thanks.
