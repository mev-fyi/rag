00:01:26.564 - 00:02:20.084, Speaker A: Okay, welcome everyone to Acde number 184. We have a bunch of stuff to cover today. First we'll talk about what happened on Mainnet over the past day or so. Then there's a couple things we missed last call that were important. So I bumped them up to the top of this agenda to make sure that we can cover them. So the state growth work by the paradigm team and then the two retroactively applied eips that we wanted to finalize but have like bumped for two or three calls now and then just quick shout to the Denkin Eips and what I suspect will take the bulk of the call is talking about Pektra. So there's some updates on 25 37, which is included around gas costs.
00:02:20.084 - 00:03:17.748, Speaker A: Mike had some updates about the aa questions we had around inclusion lists last time. Alex, can you please stay muted? Thanks. And then, yeah, we have a ton of eips that people wanted to discuss and provide updates for Pectra. We'll see how much time we have left when we get there. But I think at the end we probably want to time block a good 1520 minutes to actually hear from the different client teams around. What are we including in the first, what are we including in the fork and trying to build for in our first Devnet? Because we already have a couple of things and I think having like a finalized scope as at least the first 1st set of Devnets will be valuable. So yes, to kick us off mainnet Miss slots.
00:03:17.748 - 00:03:47.544, Speaker A: So we saw a bunch of those yesterday, Terence, before the call you mentioned it as well. So do you maybe want to share an overview of what you saw and we can go from there? Yeah, definitely. Thanks for having me. So what happened yesterday was that, well, let's go back to how like consensus work. Right? So blog reference blobs. But if today we see reference on the block. But if the block never arrived, or in that case the block will never be imported.
00:03:47.544 - 00:04:23.850, Speaker A: So basically on that case that the block is missed, considered to be missed by the client. Right. So we have seen a few of those before. Even last ACBC, last Thursday I mentioned it, on average we see ten to 20 of those instance per day. So we have been looking here and there and then we basically come to conclusion that we have seen those gossip from older client communications. We have seen those gossip from older builders. So we kind of suspect it was like a relay issue.
00:04:23.850 - 00:04:47.186, Speaker A: So anyway, so let's. Sorry, ask the question. Yeah, so those blobs are never been seen. So essentially yes, they are never arrived at this from my nose perspective, it could be dropped by the earlier. It could be dropped by the. I mean, it could be dropped earlier already, but from my note, I've never seen those blobs. Okay, so let's go back to yesterday.
00:04:47.186 - 00:05:24.208, Speaker A: What happened was, I guess there was heavier blob traffic. I think at the highest point we see like 53, like grade per buy. So that's basically ten times more blob rate than basically what we had two days ago. So I think at a high point, it was at one fourth of the code data price, just, just to give a reference on how much higher it was. Right. And because of those, because of those, blobs are appearing more frequently. And because of those, then we are seeing more instance of that.
00:05:24.208 - 00:05:54.536, Speaker A: So at the high point, we're seeing like 20 to 30 those type of blocks. Basically they never have blobs, so we cannot import those. Right. So that kind of result in like two to three miss block per epoch. And then he has becoming a lot more alarming. So we kind of like send a few telegram message to basically the relay to the builder. And then what happened at the end was that blocks rod turned off their relayer and everything started working again.
00:05:54.536 - 00:06:25.696, Speaker A: Right. So right now, we're. Right now, like, there's a strong evidence this is like a blocks rock, like BBN issue. So because of the VDN, they basically drop the blob somewhere along their network. And so basically their relayer has been shut up until they figure it out. So we hope to see a possible motor from them soon. At the end, from the consensus client, there is this also beacon API, basically to allow to basically propagate block.
00:06:25.696 - 00:07:08.708, Speaker A: And that we're also hardening that implementation to make sure that blob can still be propagated. Even the block has been seen on the network. Right. So yesterday's miss block wasn't so much about like, client couldn't handle that type of workload because I basically counted that all the missed block were caused by the block route issue. There's probably like 99% of it, so. But then there is still that fundamental issue that, okay, what happens under yesterday's traffic? I suspect client, yes, may import blocks slower than before, but that's something that I don't really have a strong evidence on. That's something that still remains to be seen.
00:07:08.708 - 00:07:45.042, Speaker A: But, yeah, that's just an update. Thanks, I really appreciate the context. There's a question in the chat from Perry mentioning whether we should reevaluate how this circuit breaker works in this case. Um, because I believe that the current heuristic is five consecutive missed slots and obviously we didn't quite hit that yesterday. So um, yeah. Anyone have thoughts on this? Yeah, so I think even for the circuit breaker you wasn't close to hitting that. I mean, maybe the circuit breaker is too loose.
00:07:45.042 - 00:09:29.444, Speaker A: But then again, right, if you made a circuit breaker too loose, then it kind of becomes grivable. Basically people can just trigger the circuit breaker to basically force the next slot validator to go to local building. I think like perhaps like there, I mean, I know Alice thoughts has been talking about like implementing more relayers specific circuit breaker. Say that today if you see a missed block from a certain relayer and then there's some proof and then you basically disconnect with a relayer, but there's a lot of work to be done there. Yeah, I agree that if like if you go more fine grained than the circuit breakers are now, they have to be relay builder specific, which there are probably, there are certainly like proofs and messages that can be passed around. There are probably actually like heuristics that could be calculated locally without passing around additional information, but those aren't going to catch off. I guess this is maybe a naive question, but why not make it epoch based rather than consecutive? Like, we know in practice, I understand why at the merge we wanted it consecutive, but I think now given what we know about the makeup of the validator set and the different builders, could we do something like find the 95th percentile amount of missed slots in an epoch and trigger it there? It becomes such an attack vector for everyone.
00:09:29.444 - 00:10:44.444, Speaker A: Then I can keep med boost on and get way more profitable locks. So like, any, anything in that direction becomes like very attackable and even the, like, the consecutive ones that we have right now, like five consecutive. Like that could be just like outwardly bought by malicious relay and turn off for everyone. So I just, I think that if we got go any more fine tuned, it just, it probably has to be like actor specific. Got it. This might take. Anyone else have thoughts on this? Do we agree that this is serious enough that like something should have happened? Because, like, as far as I can tell, like, actual users of the network, like, weren't significantly affected? So are we happy to say that, like, this is bad, but it's not so bad that like, emergency shut off valves should be kicking in? I kind of lean in that direction, honestly, barring independent actor shut offs.
00:10:44.444 - 00:11:25.664, Speaker A: Oh, poetis. Yeah. Besides my obvious comment about epbs, I wanted to mention two different things about this incident. One of them is that most probably the fact that there's lack of client diversity within relays made this happen. Second is that Terrence has been talking about these blobs for over a week, and no one paid attention to this. And once it's exploded, it only took a couple of phone calls to get to the right relays to actually look at their logs. This, I think, is unacceptable.
00:11:25.664 - 00:12:12.484, Speaker A: Yeah, those are, I guess, my main two complaints about this, that Terrence has been talking about this and calling everyone, and we even talked about this in ACD, and relays didn't pay attention to the fact that they were losing those blocks. If we could look at those logs, or if those logs were made, probably public, we could do that work. Got it. Thanks. There's a comment by Alex. I get two comments in the chat. One by Alex saying, you know, we should wait for the postmortem before making any decisions on a solution that's reasonable.
00:12:12.484 - 00:13:13.844, Speaker A: And then there's also a comment by Ansgar around the way 1559 works right now makes this worse, because 1559 is base fee based, block based, rather than slot or time based. So it sort of doesn't know about the missed slots. So, like, this is also something that we. We've considered this in the past, but that we should potentially consider in the future is, you know, if we switch 1559 to time based, then it can take missed slots into accounts a bit better. Okay. And some comments that we should have probably had, like, a more formal written warning about this somewhere. Where's the best place to just follow this conversation? I know, Alex, it seems like you were working on, like, relay specific circuit breakers.
00:13:13.844 - 00:14:02.584, Speaker A: You know, if people want to follow this better, where should they go? Yeah, the Met boost community call would be one option. I think many of us here will push Blockstrom and others for postmortem, which we'll share in, like, the usual channels. Yeah. Okay. Anything else on this? Yeah, but I was just curious why the default behavior is not for builders to release blobs once they see their block kind of signed on the peer to peer. Because at that point, Alex had unbounding concerns. But at that point, the content of the block is already leaked.
00:14:02.584 - 00:14:48.224, Speaker A: So even if you have worries that you might be too late anyway, even with releasing the blocks at that point for the block to be accepted, at least you have a shot, whereas. And the contents of the blobs themselves are not the sensitive part. So I'm curious why that's not default kind of fallback behavior. The builders could do this, especially if they've seen the block signed, but I'm not sure clients are in a way that support this out of the box. The contents of the blobs also could be sensitive. That's an assumption, right? They could have, but if the block signed then like the exchange has happened, so it's fine after that. Oh yeah, no, I know, but from like an unbundling perspective.
00:14:48.224 - 00:15:26.924, Speaker A: Sure. Okay. Yeah, because we are kind of packed agenda wise. I think we can probably leave it at that. Have the conversation on MfBoost community call and then if people who attend that can just share any sort of post mortem or things that come out of it with the rest of awkwardevs, that would be graced. That would be great. I read the time based at the same time, but ok, let's move on.
00:15:26.924 - 00:16:12.314, Speaker A: Next up we had the Rets team has been looking into state growth and put out a really good article going into the details. We wanted to cover this for one or two calls, but didn't have the time so I wanted to make sure we got to it today. I don't know, Georgios or Storm, either of you? Yep, Storm is on the call. Storm, feel free to take the stage share screen. Hi there. Let me share the screen and storm ultraworth and train. Okay.
00:16:12.314 - 00:16:55.594, Speaker A: Is my screen visible? Slides? Yes, yes. Great. So hi everyone, I'm storm. I'm a data scientist and data engineer at paradigm. I work a lot with the rest team and something that we think about all the time is how do we use data to navigate different design trade offs. So whether we're trying to decide the optimal value for some parameter, or we're trying to decide between different architectures, data often plays a huge role in those decisions. So today I want to share some data relevant to Ethereum scaling roadmap and specifically on state growth and history growth.
00:16:55.594 - 00:17:34.404, Speaker A: And we think this data could be pretty helpful for figuring out if or how Ethereum should scale beyond its current level. So Ethereum is a really complex system, has many different bottlenecks on how much it can scale. This is a rough sketch of how we've been thinking about the problem. So starting at the top we have the gas limit. That's the ultimate governor of things like block size and ops per block. Downstream of those things we have our familiar bottlenecks like history growth and state growth. And each of these bottlenecks has its own relationship to the nodes hardware constraints.
00:17:34.404 - 00:18:35.794, Speaker A: So each box here is a pretty deep rabbit hole. And today I'm just going to touch on a subset of it. So starting with state growth a couple weeks ago George Rose and I put out this research blog post, walking through some state growth data. So today I'll touch on those results and then after that I'll go through some of the results for our next blog post, which is on history growth. So this right here is a high level view of how all state on Ethereum is currently being used. The total state size is around 245gb on disk for ref at least, and the size of each rectangle here represents the portion of those bytes occupied by different contracts or contract categories. So for example, ERC 20s are the biggest category, and they take up around 67 of the 245gb.
00:18:35.794 - 00:19:45.804, Speaker A: And this isn't probably too big of a surprise, but the biggest contributors to state are tokens, ERC twenty s and ERC 721s. This is because every user balance of every token typically requires its own storage slot, and there's a lot you can learn about Ethereum just by staring at this graph. If you want to play around with it, this is an interactive visualization in the blog post, and you can sort of just click around and find your favorite protocols in there. But to keep it brief, I'm just going to move through these charts pretty quickly. If you'd like more detail, feel free to reach out to me or georgios so that was a chart of the state distribution. This is a chart of the state growth over time and showing the relative contributions from each contract category. So here every vertical bar represents one month since Ethereum's genesis, and the y axis is the number of gigabytes that the state grew during that month.
00:19:45.804 - 00:21:01.144, Speaker A: So you can see that state growth peaked around 6gb a month at the end of 2022, and now it's currently around 2.5gb a month, which is actually the lowest it's been since 2021. You can see that the main reason that state growth is down is the decline of NFT usage shown in blue here, and state growth from ERC 20 has actually been increasing every year for the past few years, and that's shown in green. So if we take the integral of state growth, we can see the total state size over time, starting at zero from Genesis and going all up, all the way up to the 245gb that we're at today. And state hasn't been growing exactly linearly, but it's kind of more linear than you might expect, even going back like four or five years. So from this we can make a rough projection of how big the state might get in the future. And there's a lot of assumptions you have to make in order to do this.
00:21:01.144 - 00:21:48.554, Speaker A: But under a very simple model, we find that existing consumer hardware can sustain current rates of state growth for a very long time, probably decades. And note that I'm only talking about storage capacity and memory capacity here. This isn't saying anything about the reads or the writes to state. Under this framing, I'm considering reads and writes to be part of state access rather than state growth. This is also not saying anything about what would happen if the gas limit were to change. So state growth we often think about as a pretty big bottleneck on Ethereum scaling. But from these numbers, I'm actually not super worried about state growth in the short term or the medium term.
00:21:48.554 - 00:22:49.784, Speaker A: However, the thing that makes me very uncomfortable is history growth, and I think history growth is kind of a silent killer that hasn't gotten as much attention. But even after 4844, I think it's going to be a major problem. And so maybe my hottest take of the day is that EIP 4444 history expiry should be given very high priority for inclusion in the next hard fork, and maybe even sooner than that if it's possible. We should also be looking at other eips that reduce history growth, like 7623 pretty closely. So, diving into some data, this is a graph of Ethereum's history growth rate over time. And like the last graph, the y axis here is showing the number of gigabytes that history grows during each month for each contract category. So a few things pop out here.
00:22:49.784 - 00:23:50.974, Speaker A: One is that the history growth rate is much larger than the state growth rate by almost an order of magnitude. Another is that the main driver of history growth is bridges, which here includes like l, two s and roll ups. And then finally, until the last month or two, history growth. The history growth rate has been like rapidly accelerating for many years now. And note that since this is the rate of growth, if the rate of growth is increasing, then the total amount of history can increase very rapidly. So if we take the same graph and normalize it to 100%, we can see the relative contribution from each contract category. Something pretty interesting here is that there's been kind of four distinct epochs of how ethereum has been used over the years.
00:23:50.974 - 00:24:32.524, Speaker A: So in the beginning, first couple years, very little is happening on chain. Almost nothing, relatively speaking. And of what is happening, it's pretty hard to identify a lot of these early contracts. It's kind of like archaeology. But then around 2018 2019, we see the rise of ERC 20s. Around 2021 2022, we see Dexs and Defi become the dominant use case, and then in 2023, bridges become the largest use case. So this is a good illustration of how things have become more and more complex over time.
00:24:32.524 - 00:25:55.394, Speaker A: So one of the big questions you might be thinking is how do blobs affect history growth? And if rollups are the biggest contributor to history growth, will 4844 dramatically reduce history of growth? So this is the same chart as before, except now we're zooming in so that each vertical bar represents one day instead of one month. And you can see that after Dunkun, history growth from bridges has fallen by about 50% and overall history growth has fallen by about one third. So history growth is still quite large, but it's also a pretty rapidly evolving situation. Everyone saw all the craziness that happened yesterday with blobs, so it's something to keep an eye on before we get a sense of how the blob dynamics will play out in the long term. So I'll wrap it up with a comparison of state growth and history growth and how they compete for node resources. So, as I mentioned before, history growth is a much larger storage burden than state. And the problem is not just that history is big, it's also growing really quickly, even after Den Kun.
00:25:55.394 - 00:26:48.134, Speaker A: So within just a couple of years, the storage burden for a full node is on track to surpass two terabytes. And this is mostly due to history growth. But there's a direct fix for this, which is EIP 4444, history expiry. So this current graph shows where we're going right now. And then here I'm adding in projections of how 4444 affects the storage burden. So the main difference with 4444 is that nodes would only store a year's worth of history. So at steady state, the history storage burden transforms from the red line into the pink line, and then the total storage burden changes from the black line into the gray line.
00:26:48.134 - 00:27:37.874, Speaker A: And you can see that this makes a massive difference to the storage burden and makes things more sustainable for the next few years. And beyond this, like I said, we should also look pretty closely at EIP 7623 to reduce the history growth rate. And Tony has done a lot of good analysis on this, and you should check it out if you haven't seen it. So this research is still a work in progress. We're still in the middle of analyzing other scaling bottlenecks and trying to tie it all together with the gas limit. We also want to analyze the scaling of l two s with the same methodology and see how they might be different to main net. So that's all I have for today.
00:27:37.874 - 00:28:12.404, Speaker A: Again, one of the main goals here is to leverage data to help improve the design of Ethereum and the node clients. So if there's any variation or follow up to this data that you think would be helpful, please reach out. We'd be interested in possibly collaborating. We're going to publish a few more blog posts on this topic, and we're also open sourcing everything that I showed today. So thanks for your attention and thanks to all the people on this call that have already given us a lot of useful feedback. Thanks. Thank you.
00:28:12.404 - 00:29:07.684, Speaker A: Excellent work. And yeah, thanks for sharing the sneak peeks on the history side as well. We had a question from Andrew who has his hand up. Yeah, thank you storm for the great presentation. And I know that you haven't gone into the state access yet, but I'd just like to comment that one problem that I see with the state size is not only the storage requirements, but also also that the bigger the state, the slower the access to it, because it all boils down to something like a b three and the complexity there is logarithmic. Right? So if you have a huge state, then each axis will be slower. That is the only thing I'd like to note.
00:29:07.684 - 00:29:50.404, Speaker A: Yeah, completely agree. We're still figuring out the best way to sort of measure and benchmark that, but it's definitely something we're thinking about. Thanks. Any other questions? Comments? Okay. Oh Lucas, so one comment from me. I totally agree with the the conclusions that EAP 4444 and the call data cost increase should be prioritized. And thank you for a great analysis.
00:29:50.404 - 00:31:39.824, Speaker A: Thanks. Great, thanks. Oh, ah, Ahmad. Yeah, one idea here that pops to mind is that there was a lot of discussion behind dropping serving these historical blocks from P two p network or not. And there was some people that are not big fans of having some nodes being able to serve these historical blocks because they're archival nodes and some others not being able to serve them. But as a temporary solution till we reach something that like a good archival storage for four fours, maybe having something like this is a good transitionary stage. So, sorry, what would be the transition stage exactly? So given that clients can drop blocks that are earlier than certain points, let's say merge or deposit contract or something like that, if we agree on a certain point that all clients can drop before some clients can drop these blocks and some other nodes could opt into keeping them for whatever reason, and they can also opt in to keep serving them on the P two p network if they want to.
00:31:39.824 - 00:32:44.672, Speaker A: But this will make the P two p network. Some nodes have all the blocks and some nodes will be able to serve part of the blocks. And there is an EIP that I proposed earlier that will solve also this issue of finding the peers that will be able to serve you the blocks that you need. Right? I think, yeah, that definitely seems reasonable as like a path to full removal. I think if we were to do that, I would almost call that like deprecated, you know, like kind of like we did with self destruct where we in the fork before we flag that it was going to go away. So you could have something similar here where you say, you know, as of this point, serving, say pre merge block data is optional and some clients might choose to do it, but you should assume it's deprecated. And then as of a fork after that or whatever, then it becomes, yeah, it becomes kind of gone by default.
00:32:44.672 - 00:33:41.498, Speaker A: But there seems to be some pushback about this in the chat. Danny, you have your hand up? Yeah, I just want this. This point's brought up, been brought up many times without doing that and having an alternative distribution method that new freshly clients sinking can utilize. Like I don't know, torrent, portal, whatever. There's just a concern that the quality of service to sync a new node could significantly degrade during that transitionary period. I think if one or two of these distribution methods are well specified and have high quality service, then you could have a deprecation period, but without that then you're just putting the kind of the sinking of new nodes in a very tenuous place during that period. Okay.
00:33:41.498 - 00:34:45.418, Speaker A: And Abad, did you have another commenter still your. Yeah, I would, I would just like answer Danny and some of the comments. So like my idea here is not to keep this forever, that we can have this only as a transitionary period till like we have a solidified archival solution. Another thing is that the rate that nodes will start dropping historical data is not as high as we think it will be. Existing nodes will not suddenly after the fork decide, okay, I want to prune all of this history away right now and data will become unavailable in a single day. I think most of the nodes will opt into keeping the current configuration, keeping the history until they actually need to start pruning because their drive, for example, does not fit any more data. And I might agree with some of those assumptions.
00:34:45.418 - 00:36:10.512, Speaker A: I just doing that transitionary period and having an unknown time horizon I think is potentially dangerous. If it's stepwise and the assumptions are stated and the timeframe is known, then I think it could make sense. Yeah, I kind of feel like we have a path forward and it seems a little weird to me to try and take a different path as an intermediary step when we kind of know what we want the long term future to look like. It's just a matter of rolling out, supporting era, figuring the last things that need to be figured out about how to support the era format post merge, and then rolling those types of things out right then, given we have the era format for pre merge block. I think last time we discussed this, the rough plan was to try and set up some out of network retrieval scheme, whether it's torrents or portal or something like that, and then start, have clients start using that sometime this year and then, and then stop, you know, start serving like pre merge blocks. Start serving pre merge blocks on that network this year and potentially stop serving them sometime next year on the peer to peer network, I guess. Yeah, yeah, sorry, go ahead.
00:36:10.512 - 00:37:12.254, Speaker A: Yeah, I just said that's kind of how I think that we should proceed. What's the best way for people to follow this? Like is it in the discord? Is it the Discord channel history expiry thing? Is it called state expiry or history expiry? History? Yeah, history expiry in execution R and D. Okay, I think we can probably continue the conversation around history expiry. There any other final questions on just the overall presentation before we move on? Okay, well, yeah, thanks again storm and georgios for putting this together. Yeah, this is really good. Okay, hopefully we can get through these next couple things quickly. So these two retroactive eips, first one was 7610.
00:37:12.254 - 00:38:21.804, Speaker A: We discussed this a few calls ago and there were some questions around whether this would be compatible with verkle or not. I don't know if anyone's looked into that or what the current status on this one is. Yeah, I think it is compatible with vertical because we have this consideration that if we switch to vertical, is there any easy way we can determine if the account has ample storage or not? So in order to solve it according to the current breaker spec, because there is no notion of storage root anymore. So it might be a problem, but in order to solve it, we can choose to discard the useless storage during the Virgo translation. Specifically, if an account has zero nons and zero runtime code, then the storage can be discarded. It is totally fine because runtime code is zero. So these storage are not accessible.
00:38:21.804 - 00:39:08.884, Speaker A: And we have 28 accounts in total eligible for it on the extreme mainnet. So by doing this, no account will be eligible for tracking this special scenario. The destination of the deployment has numb storage. And also after the transactions, the EAP can be disabled automatically because in vertical the storage route returned is always empty. So this additional condition is basically useless and the client team can choose to remove it or it's also fine to keep it. So. Yeah, I don't think it's a blocker for vertical.
00:39:08.884 - 00:39:54.924, Speaker A: Yeah, Guillaume. Yeah. So basically everything that Gary said is correct. I mean, the CIP effectively auto disables itself with verbal. I remember a couple weeks ago someone was asking how would 5806 affect this? And I remember discussing it with Adrian. I think he's here, but our conclusion, although I don't remember the details, so I hope he's here, but our conclusion was that it would not happen because. Because you can't really create accounts like you don't have storage.
00:39:54.924 - 00:40:46.214, Speaker A: You cannot write storage when doing a delegate transition. So I think we're completely safe to move. But yeah, Adrian just raised his hand, so I'll give him the floor. Yeah, just confirming what Guillaume is saying is that if 5806 was to be adopted at any point, the current phrasing of the EIP states that you cannot use any opcode that sets storage or that affects your nonce in the context of anyway, so that EIP would not be used to set state under an EOA. So it doesn't conflict with the vertical assumptions or nor with this DCIP, this retractivity ip. Got it. Thanks, Peter.
00:40:46.214 - 00:42:15.510, Speaker A: Yes. So my first thoughts about the vertical is that a lot of these issues come from there being a bunch of slightly annoying edge cases of accounts that couldn't exist because they're very old. In particular, most of these problems have gone away because anything that was created after spirit Dragon was deployed, the nonce got bumped to one if it was a contract created. I think that what I'm hearing is there's a reasonable case for taking the opportunity of the vertical transition to remove these edge cases by bumping the nonce on accounts that have storage and deleting storage for contracts with no code. No nonce. Yeah, so I would choose to remove the storage because they are useless and very safe to delete. I think it's also worth pointing out, but this edge case only exists in the case of a ludicrously implausible intentional attack on Ethereum, so we don't necessarily need to.
00:42:15.510 - 00:43:49.764, Speaker A: We should be designing our semantics here based on what's easy for clients to implement that's reasonable rather than what makes the most sense for people doing it intentionally because someone's probably attacking ethereum if this ever happens. For example, if we delete the storage during the vertical transition, it's possible that someone has the ability to deploy code at that address because they found a hash collision. What happens will depend on whether they do that deployment before or after the vertical transition. Normally I would be against any such behavior, but given that, I don't think this is likely to happen at all, and I don't think it's a thing we should be worried about. We should just be like, what's the easiest, least complex thing we can do that results in all clients having the same behavior? So I guess one thing to clarify is, is the verkle complexly like part effectively would be part of the verkle? Or is this something we need to consider in the case of 7610, my understanding is this is something we would include as part of verkle to deal with this, like to sort of deal with the consequences of this, but just. Is that correct? I think it can be applied right now. And whenever we switch to vertical, we need to think about how to.
00:43:49.764 - 00:44:44.076, Speaker A: I guess in this case, it seems like all this potential concerns have been addressed, and even though we might need to figure out the right interaction with the vertical transition, there should be a way to do it, and it's just a matter of how rather than if. Does anyone like, oppose just retroactively applying this EIP from Genesis? Okay, no objections. And then we had an other similar EIP. So seven. Sorry, what's the number? 7523. This is Peter's EIP, which basically cleared some empty accounts that were missed in the spurious dragon. Hard fork did this before the merge.
00:44:44.076 - 00:45:43.854, Speaker A: And so the EIP makes the assumption that there are no empty accounts on any post merge network or network which applies this or which applies spurious dragon at genesis. And I believe we've had two people verify that Peter's script was correct. Yeah, Peter, do you want to add more context? Yes. So this is the EIP that says that there are no empty accounts after the merge and that we no longer define what the behavior is in the presence of empty accounts, and that everyone should just, when operating in a post merge context, should just proceed on the assumption that empty accounts simply can't exist. And in particular, this allows the deleting of a large number of test cases. It allows some types of special case code to be removed. There are edge cases that have never occurred, which now can never occur.
00:45:43.854 - 00:46:33.898, Speaker A: It's just useful for everyone. Yes. At this point, me and Marius van Deuision have both manually checked that there are no empty accounts in the current head or at any point after the merge. Ma, I think we can just proceed with this. You could always do more checking. I've not seen any demand for more checking and I think that people generally think that me and Marius are both reasonable, diligent people who will have checked thoroughly. Does anyone have any objections at this point? Yeah, because there is a question of is that true for other chains that are EVM compatible? Right.
00:46:33.898 - 00:47:24.714, Speaker A: Because if we enshrine this as a EVM thing and they are incompatible with that, if they have any edge cases that occurred, then that might be a problem for them. Yes, this is a good point. So the thing you've got to bear in mind here is that in order to have an empty account, your chain must be extremely old. The creation of empty accounts was abolished at the end of 2017 and effectively every VM compatible chain is more recent. In particular, any sort of roll up is like the concept of rollups is just way more recent. I have checked quite carefully and I am confident that the chains that can have empty accounts on them are Ethereum, where they have been manually cleaned. Ropston, which is now deprecated.
00:47:24.714 - 00:48:04.502, Speaker A: Ethereum classic. Ethereum Classic I manually cleaned all the empty accounts. I haven't done any sort of checking that I did that correctly on Ethereum classic, but I did like go through and try and delete them all. And also there are some extremely old Ethereum classic testnets that could have empty accounts on them. My position is, given that we've cleaned things off, etc. I don't think we should block stuff on Ethereum because it might break something on an Ethereum classic testnet. And I'm confident that every.
00:48:04.502 - 00:48:34.000, Speaker A: Yes, and I'm confident that like every other context, this applies. The other significance, of course, is the Etc doesn't have the merge. And this specification says that after the merge there are no accounts. So technically it's valid on ET, on Ethereum classic. And as everyone is saying in the chat, like, we're not responsible for Ethereum classic. So it's not like we're throwing people under the bus. Like, we have to think about it, Ethereum classic has to think about it.
00:48:34.000 - 00:49:29.634, Speaker A: And everyone else just has to observe that they launched their chain after the end of 2017 and adopted. There's a question in the chat about chains without EIP, 158 chains, but also chains without EIP. I would have to look into that. I'm not aware of. My kind of opinion is that, yeah, I'd have no, EIP 158 is the same era. So if you have EIP 158 at launch, you probably didn't have anything out, any empty accounts. I will make a note to look into this, but someone would have to tell me what these chains are.
00:49:29.634 - 00:50:01.286, Speaker A: Yeah, someone mentions maybe gnosis. Yeah. My sense is, I think we've talked about this for over a year. At this point, if someone wants to pay gnosis about it, they can also run the same script we've run on their chain and clear stuff. I do think we should just move forward with it. I think the EIP makes it clear what are the conditions under which it should be activated. Um, and, um, yeah, it is like a nice clarification for a theorem.
00:50:01.286 - 00:50:19.446, Speaker A: So I don't know, any objections on moving this forward. And by moving forward, it's like considering it retroactively applied from genesis. Okay. And I guess relatively applied from the mer. Oh, sorry. Yes, correct. Before the.
00:50:19.446 - 00:51:20.014, Speaker A: Yeah, yeah, yeah, sorry. Um, yes. And I guess on this point, I've been thinking, so I think this is like our fourth retroactively applied EIP, if I'm not mistaken, and they're not really documented anywhere. So I think I'll probably put together a meta eip that tries to list those so that when we do make these changes that people should consider and have been decided after a specific hard fork, they're at least documented somewhere. So I'll put together a draft with all the, with the two eips we discussed today and then all the ones we've did in the past, and I'll show that on all core devs in the next week or so. Anything else on the meta eips or, sorry, the retroactive eips. Okay, so we have about 40 minutes left.
00:51:20.014 - 00:52:26.624, Speaker A: There's two quick updates that we have to do that we should cover on Petra included or considered stuff, and then a bunch of updates on potential eips. So we'll do the two included things so far or included in the CFI thing. For everyone else who has updates on, like, a long list of stuff, consider that you'll have probably one or two minutes to give an update on the most relevant new things, and then we'll keep the end of the call to sort of discuss what should go in and how we scope the first devnets. But first, Marius, you did some analysis on the BLS gas costs? This Marius here, okay, he might be on holiday, actually. Oh, okay. So if Merus is not here, anyone else have comments on the BLS gas costs? Otherwise, I just posted the link. We can continue the discussion async if there's no comments or questions.
00:52:26.624 - 00:52:52.014, Speaker A: Yeah, I think. Yeah, go ahead. Yeah, I was going to say that we are going to do some benchmarking for the libraries that we'll be using in ref, which are going to be different than the ones that Marius posted in the benchmark. So, yeah, we'll post these in the discord when we have them. Got it. Thanks, Alex. You're going to say something? Yeah.
00:52:52.014 - 00:53:16.110, Speaker A: Oh, first off, just plus one to benchmark and other libraries. That'd be really helpful. Yeah. So Marius did some benchmarking with his software stack. I think the document's pretty self descriptive. There's kind of an open question around subgroup checks, which is like this cryptography detail of the pre compiles and. Yeah, just the update.
00:53:16.110 - 00:53:49.854, Speaker A: There is networks underway and. Yeah, it's moving along. Got it. Any other questions or comments on that? And the subgroup check was the really expensive part. And by work on going to decide if it must be in there. Right. So the way the EiP is written right now, there's subgroup checks that are essentially required for the pairing pre compile.
00:53:49.854 - 00:54:33.218, Speaker A: They are effectively, like suggested or recommended for the others, but they are very expensive. So I know Antonio is looking at benchmarking them for these other expensive calls. And then it becomes a question of, do we want to mandate them? Meaning higher costs for users or can we skip them? And then it'd be cheaper to use, but then you'd have to be more careful as a user to not shoot yourself in the foot, so to speak. There might be a new version of these hip group checks that are the best of both worlds, where they're cheap enough and then also secure. So, yeah, that works ongoing. Thanks. Yeah.
00:54:33.218 - 00:55:09.274, Speaker A: One comment on subgroup checks. Given that there's like, multiple libraries that implement these operations, we would have to. I mean, the benchmarks for each of them will guide this. Agree. But, like, if there's a newer, faster one, it's only implemented in one library, then I'm not sure we can recommend, like, decreasing the costs, for example. But I guess that should be obvious. Yeah, I mean, the idea would be that every implementation would use the new thing.
00:55:09.274 - 00:55:39.784, Speaker A: Yeah. I mean, if they're all by an order of magnitude, that's an obvious place. So it's looking to. Okay, so we can keep discussing this, I think. Yeah. And last question. Maybe we can just answer in the chat, but someone's asking if there's any links or information they can dig into to understand why we need the subgroup checks.
00:55:39.784 - 00:56:09.244, Speaker A: So yeah, someone has stuff to get sent. Seems like Dan has. So thanks. Next up, so we have cFI'd the inclusionless CIP on the last Cl call. I believe there were some concerns there that were raised around how it would interact with account abstraction. And I believe Mike has an update on those concerns. Hey Tim.
00:56:09.244 - 00:56:30.664, Speaker A: Hey everyone. Thanks. I'll try and be pretty brief here. I have one diagram to share to kind of help visualize this. And I'll start by saying, also Potez has this doc. I'll paste it in the chat that also covers this issue really well. So for more context, I'd point the interested reader there.
00:56:30.664 - 00:57:01.294, Speaker A: Hopefully you can see my screen. So I'll just give like a really high level view of this issue and then talk through these three options at the end, which are three things that I think we can do to resolve it. So the context here is that we're the slot n plus proposer and we're deciding, you know, where to. Where to build our block as the. Or which block to use as the parent. And this is our current view. So we saw the slot n block and we saw.
00:57:01.294 - 00:57:28.452, Speaker A: We basically like the proposal for 7547 is that the inclusion list is composed of two different objects. The first object is the summary. The second is the list of transactions. The point for doing a summary instead of the full transaction list and committing to just the summary is to solve this free da problem which we talk about here. I'll send this link to just so everyone has it. Um. Right.
00:57:28.452 - 00:58:07.672, Speaker A: So we saw the slot n block and we saw the slot n summary, but we didn't actually see the slot n transaction list that corresponded to the summary. Um. And the way that could happen is if we saw the slot n plus one block which included the slot n summary, um. And we confirmed that this slot n one transactions. So this is kind of like part of the inclusion list mechanism is we ensure that the summary is included in the block and that the summary entries are satisfied. But remember, we didn't see the actual slot n transaction list that corresponded to the slot n summary. So now we're kind of here and it's my time to propose a block.
00:58:07.672 - 00:59:00.320, Speaker A: It's slot n two, and my fork choice says that slot n is actually the head of the chain. So now I'm building on slot n for whatever reason, let's just say slot n plus one didn't get enough attestations for me to consider at the head. And in order for my block that proposed to be valid, I need to include the slotin summary in my slotin two block, and I need to satisfy the entries in the slot n summary. But I can't be guaranteed that the. That I have access to all the slot n transactions that were in the slotin inclusion list because I didn't see this object on the network. So, this is the kind of weird situation where if we don't resolve this, then the slot n plus two proposer can't actually produce a valid block that is the child of slot n. They would have to build on slot n plus one.
00:59:00.320 - 00:59:31.010, Speaker A: So, yeah, I guess there's a few issues here. One thing we can do. And I guess so this is kind of. We haven't even touched the issues around 3074 yet. And so I'll talk about the three options, and then I'll describe where 3074 fits in here. Right. So one option is to reorg this slot n block to say, okay, we didn't see a transaction list for the slot and summary.
00:59:31.010 - 01:00:27.914, Speaker A: So we're going to reorg that block. That doesn't seem great. We could not accept the slot n block as a potential head of the chain in the fork choice view, which is how we treat 4844 blocks. So 4844 blocks, we don't accept them into the fork choice view unless we've seen the blobs. And then the third option is to reconstruct the slot n transaction list from the slot n plus one block. And this is actually where the 3074 issues come in. So the reason 3074 causes an issue is because without 3074, we know that the slot n summary transactions are either in the slot n block itself, or they're in the slot n plus one block with 3074, instead of the transaction from an account being included, there's a chance that that account had balance drained by an external.
01:00:27.914 - 01:00:44.162, Speaker A: By an external account. And so in that case, the satisfaction of the summary entry. I apologize. This is, like, complicated, but, yeah, whatever. I'm getting close. Don't worry. That account could have been drained by a different account.
01:00:44.162 - 01:01:49.448, Speaker A: And so, in that case, reconstructing the slot n transaction inclusion list is not possible from just the slot n transactions and the slot n block. So this is the kind of issue. Yeah, those are the three options. And, yeah, I guess there's one thing that can make 3074 work with reconstruction, which is if we can check not only that a transaction from the account has been included, but if we can pull out the 3074 transaction that invalidated the transaction that was originally in the. In the inclusion list, then that works, too. And, yeah, so basically, I think there is a way we can do reconstruction with 3074, but as Marius pointed out, this becomes a slightly more complicated proposition. So, yeah, I guess the last thing I want to say is that, you know, this does feel like it kind of creates some complications around the inclusionless design.
01:01:49.448 - 01:02:25.024, Speaker A: But I do think there's three options that actually do work. And I also don't think that there's going to be, like, a silver bullet to fix this, because this is, like, fundamentally an issue about how transaction inclusion works and how, like, inclusion lists are designed. So, like, I don't think. Waiting. Yeah, I don't think, like, we're going to find a better design that doesn't be faced, that doesn't have these same issues that it has to resolve. And we can make it work with 3074 specifically, which is a nice compromise. The enshrined 4337, things become a little more complicated.
01:02:25.024 - 01:03:09.820, Speaker A: But, yeah, since we don't know we're going to go for enshrined 4337, I think those realities can be worked out in a future scenario. So, yeah, I think we can still do 7547 in electra for those realities. And because we do have three options to consider, I think it'll probably be the type of thing where it's worth writing up a short doc explaining the details here. But, yeah, that's kind of where I see the 3074 plus 7457 relationship. So, yeah, so that was a little longer than I expected. Wanted to quickly shout out the people in the channel who've been working on this, Matt, Marius, Poan, Terrence Onsgar, Daniel and Sean. Lots of.
01:03:09.820 - 01:03:31.772, Speaker A: Lots of attention on this in the past few days, so hopefully we can get it resolved quickly. Happy to take questions online or off. Let's do one question by Danny and then move on. On the diagram if we have. Just to help my understanding, say the slot n plus one block does not exist. I'm the slot n plus two proposer. I see everything that I see except for that n plus one.
01:03:31.772 - 01:04:05.018, Speaker A: So I can't do reconstruction rate. So I don't even really, can I even build on n at that point, right? At this point, you wouldn't consider slot n valid, this block valid because it's the leaf chain, so it's like. Or it's the leaf block, so it's a candidate for the head of the chain, but it doesn't have a full summary accompanying with it. So part of the. Part of the check would be, if it's a leaf block, it has to have a valid il. If it's not a leaf block, then you can maybe do this reconstruction thing. Okay, so on the maybe side, it's like, why.
01:04:05.018 - 01:04:26.538, Speaker A: Why does this quantity matter? Right. This case matters because there is a situation where this block was the head, and then it got reorged. So it's only. It's only mattering in the reorg case. So it's a bit of an edge. Yeah, yeah. But I'm just like.
01:04:26.538 - 01:04:40.400, Speaker A: In the event that impulse two didn't see. Interesting. Because they. Without impulse one, they can't even see n's. There's. What was that? Put us there. You can.
01:04:40.400 - 01:05:15.672, Speaker A: You can make an attack in which you make. You build a block that is not reorbable. Yeah, basically, like, if. If slot n plus one gets built and is not reorderable, then that's a problem. So that's why you send the il to everyone. You send the il to no one, but you pollute with the next slot builder, and then you guarantee that the next slot can just come and it's never going to be reorganized. Okay, I'm sorry, but we are pressed on time, so.
01:05:15.672 - 01:05:52.364, Speaker A: Yeah, but, yeah, thanks, Mike, for sharing the update. And yeah, we can potentially discuss this in another breakout room, so. Okay, so I'll move us to the big list, I guess. Yeah, try to keep it to about 1 minute. And I think all the eips here except one, have been presented before. So assume that everyone on the call has context and try to focus on what is new and or special about the EIP and why it's relevant. Charles, you had two of them that we didn't get to last call.
01:05:52.364 - 01:06:33.214, Speaker A: So 5920, pay opcode, and then 7609, decreasing the cost of tload and t store. Hi, I'm Charles. I work on VYPR. I want to present two kind of ux related eips. The first is pay. I brought it up on a few calls before, but basically it allows you to send Ethereum without transferring execution context. People have pointed out that you can use self destruct for this, but we are trying to deprecate self destruct.
01:06:33.214 - 01:07:43.086, Speaker A: So I don't think we should depend on, like, the particular semantics of how self destruct works right now. I think it's a super important EIP, because right now, in order to send Ethereum, you need to transfer calling context. And this has resulted in a lot of bugs before. For instance, actually, one of the reasons that the whole call gas schedule is so complicated right now is because of these artifacts from, you know, when send had a 2300 gas stipend hard code or something. And if we had had pay, that just would have never happened from the beginning. So it prevents a whole class of reinfancy and other kinds of bugs related to calling context. And in previous calls, the main, as far as I know, agreed that it should get into the EVM.
01:07:43.086 - 01:08:37.704, Speaker A: But the main objection is that the testing complexity is somewhat high. That's the summary. And then for EIP 7609, this is a proposal to make the pricing of trans and storage cheaper and slightly more complicated. Excuse me, I guess. Any questions? Comments on pay? Yeah, so on pay, that got some special attention a couple of weeks ago from the testing team and the Eels team we used as our prototype EIP for testing the EIP process. And we actually have a bunch of tests and stuff written for it already, so we can help a lot with that. Got it.
01:08:37.704 - 01:09:16.566, Speaker A: So if there's no if that resolves the testing objection, then I would really like it to be considered for CFI. Well, yeah, just for clarity, we can do the CFI discussion and inclusion at the end for everything. Um, yeah, just. Okay. Um, and then the other one is transient storage pricing. Um, I think that honestly, transient storage was mispriced to begin with. Um, basically it should be cheaper than, uh, warm storage for a lot of reasons.
01:09:16.566 - 01:09:57.308, Speaker A: Um, it doesn't interact with the database ever. It doesn't interact with refunds. Uh, the, basically the rules around it, around the implementation are much simpler. There's some other weird things, like in the current gas schedule, you can allocate a lot more transient storage than you can allocate warm storage slots. So this EIP fixes that. And one thing I changed is the, I added a little more analysis around how much you can allocate. So there's one parameter coefficient in the IP which is currently set to one.
01:09:57.308 - 01:10:39.504, Speaker A: And basically the amount you can allocate grows sub linearly with respect to the gas limit in a single transaction. It grows at roughly the square root of the gas limit, and it makes it cheaper in most use cases that you want. And it adds better bounds for how much transit storage you can allocate. And I think that's the quick rundown. I'm happy to answer any questions. Okay, if there's no questions, then let's move on. Thanks, Charles for the updates.
01:10:39.504 - 01:11:08.788, Speaker A: Next up, Guillaume, you had two vertical related eips, or at least vertical adjacent eips. So one about historical block hashes in the state, and then the vertical proof precompile. Do you want to give some quick context on those? Yeah. So I have a very quick presentation. Don't worry if I share. I don't know if you guys can see the slides, but it's going to be very, very short. So it's just actually a reminder.
01:11:08.788 - 01:11:58.154, Speaker A: So we had this EIP by Tomasz and Vitalik about storing the state in the contract, turns out. So DCIP was stalled, but we discovered while running the vertical testnet that actually stateless clients need a way to be passed the block hash, sorry, block hashes to be able to execute the block hash instruction. So we created a revival pr against that EIP. So we're effectively completely hijacking this EIP. The short summary for the CIP of the changes at least, is that we changed the way the four gets activated. It used to be number based, but since then we had the merge. So now we do time based transition.
01:11:58.154 - 01:12:47.404, Speaker A: We no longer store the entire history, we just store a ring buffer of 256 blocks. And there was this weird activation period where the would have happened, but you could not use the block hash instruction using the contract just yet. So we got rid of that. We made it simpler. We just say add the fork transition, we will simply copy the last 256 blocks and that will be it. And why do we want it in Prague? Yes, it's required for Ver call, but we, sorry for the typo. We can sort of include some vertical eip this way signal that it's happening.
01:12:47.404 - 01:13:46.660, Speaker A: And it's quite simple. In fact, there's already an implementation running on the vertical testnet, and it's also useful even before verkle. There was this proposal by Peter around the time we were in Istanbul last year, where what I call lite client diversity. So instead of striving to get every fourth person to install a different client, we just make sure that whatever is the majority client just also works as a light client to a minority client to make sure there's no catastrophic super majority bug. So that's pretty much it, yeah. Why? Why would we want that now? Because it could be used even before verkle. And then due to the complexity of the vertical change, it's also nice to be able to do some stuff that is light before, before the actual fork.
01:13:46.660 - 01:14:32.146, Speaker A: And while I'm at it, I'm going to talk about 7545. So the main domain reason is for the main rational or the purpose of this, of this EIP is to abstract the way smart contracts verify proof. Because we're going to switch from MPT to a new tree format. So that means all the proof, the proof format will change. I realize this is the former version of my slide, so I'm sorry if it's a bit cryptic, but yeah, the idea is we want to be able to also handle the transition. We don't want every smart contract in, out there to know, to have to guess when it's performing the transition. So just receiving a blob.
01:14:32.146 - 01:15:41.164, Speaker A: A blob that is a proof of some state that would be on one l one or on an l two allows for, for example. I mean, typically l two contracts should not have to worry about which proof format they have to handle. You can, you can release one contract at any time you want before the fork without having to rush at the last minute to release the contract. A contract that can handle the new proof system right at the time of the fork. And yeah, once again, why in Prague? Because we want to signal that. Because it has to be because Prague is the last fork for Verkle. So if we want people to, especially l two s, but bridges and whoever needs to handle proofs to be ready for Verkle and not have to rush once again, it would have to be released in Prague so that people have the time to update their contract and make sure it's verk already.
01:15:41.164 - 01:16:32.218, Speaker A: And yeah, that's pretty much it. Thanks, Guillaume Vitalik, you have your hand up? Yeah. So first I wanted to just suggest making the size of the buffer 8192 instead of 256. And the reason for this is basically that in practice, a lot of people don't, a lot of applications end up not using the 256 a block hash precompiler that has existed since launch. And the reason basically is that it gives a window that's actually pretty tiny. And so 256 times that twelve is maybe 3072 seconds. So less than an hour.
01:16:32.218 - 01:17:46.834, Speaker A: And in general, we have had outages of the Ethereum chain that are longer than an hour. And roughly one week is kind of the standard length of time that's generally accepted as safe for rollups. It's the time that we've been assuming that a user is going to reliably be able to get a transaction included on chain at least once in that time. And so if we can increase the length to that higher number, then we'll be able to, like, applications would actually be able to guarantee that, like if something happens on chain, then a user will be able to submit something that involves submitting a proof of it before the time runs out. Yeah, there's some support for this in the chat because we're a bit pressed on time. I think we can move on for this, but I think, yeah, generally it seems like there's support to just match the params from 4788, which has 8192. Okay, any questions on the precompiled EIP? Otherwise we'll move on.
01:17:46.834 - 01:18:29.222, Speaker A: Okay, next up on the list, Dano, you wanted to give a quick update on EoF? Sure, let me share. I can't. I have a new zoom, so I can't share. I'll just post this into the chat. So as far as the status of EOF, we're into the point where basically the specs are locked down and we're writing reference tests and implementation. I posted a readiness matrix that shows the evms that are reporting their status of what they're working on. Base is almost completely done along with EVM one.
01:18:29.222 - 01:19:14.174, Speaker A: There's just some issues with txcreate. They were going through some of the final standards for what a transaction might look like. RevM, which is the one that ref uses, is starting implementation, and I know there's old implementations for Geth and Nethermind from the big EOF days back in Shanghai, so there's probably they're not fully blank on that, but I do need to bring it up to date. So that's mostly the status of EOF, is that people are implementing it and we're getting reference tests written. Awesome, thanks. Any questions on Eof? Okay, yeah, thanks for sharing the matrix. And there's a comment by ret that they need to update that table.
01:19:14.174 - 01:19:44.654, Speaker A: Yeah, it is few weeks old and I need to update on the progress material. Awesome. Okay, let's keep moving then. Matt, you had two from, from your end, so EIP 7212. So the r one precompile and then 3074. Yes, Matt. Yeah, sounds good.
01:19:44.654 - 01:20:32.414, Speaker A: Thanks everyone. Yeah, I think I'll be super, super brief in interest of time with 7212. It seems that with the rips and the adoption on l two s, it would make sense to try to include this precompile on l one as well, just so we don't break any compatibility, and we can make sure that wallet development stays the same across the two layers. And that's the same thing with 3074 moving. Spoke to some of the metamask team and did some due diligence on the consensus side as the BeSu team on 3074 and 7212. So hopefully moving to include those and show the support of those from this team to those. So I don't think we need to dive into specifics of 3074 right now, but motioning for support from Basu.
01:20:32.414 - 01:21:11.202, Speaker A: Yeah, that's it. I'll be very brief. Yeah, there's some support comments for 7212 in the chat from Geth and Rhett. Anything else otherwise? On the 3074 point, Proto had the new EIP addressing what he believes is a new edge case. So there's a draft for it, EIP 7664 Proto, do you want to give some, do you want to give some context on this? For sure. Thank you for the time. So this EIP is brand new.
01:21:11.202 - 01:22:28.114, Speaker A: Drafted it yesterday. It's called EIP 7664. What this EIP does is enhancing the utility of access lists such that contracts can basically require a user to statically define a certain input to the contract. This is functionality. We're losing the 3074, where previously you were able to basically enforce that the transaction input data matches the contract call data. And so by using access lists we can allow contracts to require these inputs to be declared in a transaction without coupling it tightly to the account model, to the call stack or anything like that. And also this generally, besides static declaration of inputs, it basically fills this original motivation of the Berlin EIP, EIP 29 30 where the original motivation described that hey, we can use this for statelessness or for these static analysis type purposes.
01:22:28.114 - 01:23:17.384, Speaker A: And this, this implements exactly that and doesn't add additional resources, but fills this gap that 3074 leaves. Thanks. Any questions? Comments? Joav? Oh, you're muted of. Hi. Yeah, so one question regarding. Yeah, can you hear me? Yes, we can now. Okay, so regarding the latest, the latest addition to 3074 with supporting value, that's something that until yesterday I thought it was.
01:23:17.384 - 01:24:51.350, Speaker A: I thought it was safe enough because similar attack vectors existed, similar denier service existed before. But yesterday's problem with blobs made me realize that there may be a problem. So I wanted to bring it up, see if anyone has any comments. So with blobs, we have an eviction protection policy where we don't allow type two transactions where replaced by fee type three blob. But if we allow. If we allow 3074 to drain accounts, to move ethrom accounts, how can we fix the policy for blobs so that we don't get a lot of blob evictions as a denial of service attack? Does anyone have an answer to that? Or thoughts? Why is it different than the current situation? Because you could today have, you know, in number of blob transactions sitting in TX pools and then work with a builder to build a block with n number of transactions from those accounts that are 21,000 gas transfers just sweeping the account balances out and then all of those blobs become invalid. It's a bit more expensive today, but I think the fundamental tax still exists.
01:24:51.350 - 01:25:19.518, Speaker A: Yeah, yeah, that's what I initially thought, but then I. So I realized that the current policy, the current eviction policy doesn't let you replace by fee on a blob. So a blob is more expensive to propagate. But once it's been propagated, you cannot propagate another transaction from the same. You can, you definitely can replace, but it's much higher. I forget the exact number. I think you have to maybe double the base fee of the bulb.
01:25:19.518 - 01:25:44.844, Speaker A: Yeah, I think it was two x, but like, you still are paying this roughly the same amount because I don't think you have the two x. The priority fee for execution. I might be wrong, but like, you're only two x in the blob base fee. So as long as the base fee stays kind of low, you're paying the same amount. Right, so. So you're saying there's no. So there is no concern where.
01:25:44.844 - 01:26:32.394, Speaker A: So invalidating. Invalidating a large number of blobs doesn't become. Doesn't become significantly cheaper with a single transaction. With multiple auth calls, I think it becomes about three times cheaper because I think it's about 7000 gas to go through the auth process, auth call process to sweep the account maybe a bit more. So you're saying 7000 per account to sweep versus 21,000. Yeah, except that with the doing it with an e zero eight on Zapier without 3074, you do need that. You are paying much more for these 21k because you need to replace.
01:26:32.394 - 01:27:05.684, Speaker A: Because the protocol knows that you are actually replacing blob where here. The transaction doesn't have to pay any higher fee because it doesn't come from the same EoA anymore. The protocol, this is only a transaction pool heuristic. The protocol doesn't care if you receive a block, it's not going to enforce that. The block that's going to coming in that that transaction is paying, you know, enough to. Where are the main problem? Okay, I think. Yeah, sorry guys, we're talking about a private builder.
01:27:05.684 - 01:27:43.254, Speaker A: I think we're going to need to take this offline because we only have three minutes left. Yeah, but thanks for bringing up the concern. You can definitely keep discussing it. Okay, so there's two more quick ones to go over. One was so I guess proto on your access key opcode. The general feeling seems to be that people think it's interesting, but need to review it a bit more. You wanted to also bring up the Sse Eip, right? I'd like to give some L2 perspective on the SSe Eip.
01:27:43.254 - 01:28:15.164, Speaker A: So I know this EIP has been discussed. Yeah, it's just that I think it's kind of underestimated. Where 3074. Other 3074? The inclusion disk. These things rely on transaction typing a lot. But then there's a lot more to say for SSE outside of this. The new set of eips, specifically, specifically for transactions today, they're just very difficult to prove.
01:28:15.164 - 01:29:07.486, Speaker A: And to prove attributes of this also applies to receipts. And so like this proofing, this is a big deal for L2, both for scale and for security, for scale. If you think about sharding, like we can sample data all we lack, we cannot sample the EVM execution the same way. They're not there yet. So what this means is that all these roll ups, they're registering their data through the EVM and basically duplicating the registration of this blob data and then pulling information from layer one is all going through the EVM. Well, this all could just be like a non EVM operation. This could just verify through the commitments we already have in the Ethereum chain.
01:29:07.486 - 01:30:16.604, Speaker A: So the state today is basically, we are forcing a certain bottleneck through the ether and we're duplicating data, and we are making it very difficult to prove the attributes that exist in the chain today in the history. Now we can fix this by just improving the mercantization, improving the representation of this data in the history, and then for security, specifically, L2 s end up implementing all kinds of workarounds to verify the data. Today, these workarounds are generally less safe, less sane, really just a lot more complicated than they need to be. And so this SSE ERP fixes this by improving the layout of the data, improving the mercurialization of the data for all the types. And I think this is really important for the future of L2 to be able to scale and to securely verify layer one data. Thanks. We'll take one comment and then try to wrap up, because I know we're already at time.
01:30:16.604 - 01:30:37.704, Speaker A: Guillaume. Yeah, just to add to that. Sorry. SSD virtualization uses SHA 256 instead of KCAC or kit shack. You never know how to pronounce it. And this is also better for ZK Ro. Thanks.
01:30:37.704 - 01:31:14.896, Speaker A: Okay, we're already at time, I think. I was hoping we could have like a bunch of time for client teams to share their preferences today. But we already have stuff included in the fork. Do people want to make a case for anything else being included? As we work on the first devnets? I think we can take 1 minute for that reading. The chat 3074 seems to be the only one that's been like repeatedly mentioned. There's been a couple mentioned of other things. Yeah.
01:31:14.896 - 01:32:15.916, Speaker A: Do people have a strong preference on making this decision now for some of the things? Do we prefer to push this back two weeks and give people more time to digest everything that happened today? Any strong preference may be better to get some opinions now so we can start working on any prototypes we want for the devnets. Okay. I think, yeah, based on like the chat. Yeah, I think it's probably better to wait two weeks to start. I mean, I don't think client teams have implemented everything that's already included so far. So we can definitely start with that and then block off the first, basically block off all of the next AC DC or ACDe. Sorry to discuss any additions.
01:32:15.916 - 01:32:49.768, Speaker A: And I think, yeah, in the next two weeks we should dive deeper into all the stuff that's been brought up today and, yeah. Aim to make a decision then. And I think anything, the implication is like, I think anything that's like not fully figured out or specified two weeks from now is probably not going to make it into this fork. If we want to move forward, does that make sense to people? Okay, thanks everyone. I appreciate you all getting through all of this today. This was a big one. Yeah.
01:32:49.768 - 01:33:02.344, Speaker A: So again, please review everything we've discussed and we'll block off the first part at least of the next call for discussions around what's going in the next fork. Yeah. Thanks everyone.
