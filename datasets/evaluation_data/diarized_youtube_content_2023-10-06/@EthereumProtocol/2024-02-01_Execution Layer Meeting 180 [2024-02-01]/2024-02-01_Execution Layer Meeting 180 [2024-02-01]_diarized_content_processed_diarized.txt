00:02:33.060 - 00:03:08.910, Speaker A: Sweet. Welcome to Acde number 180. So we have a bunch of stuff to cover today, including a very last minute addition about blob spamming, breaking Devnet twelve. But yeah, so first we wanted to chat about the base U mainnet event and base umainet bug. Sorry. And yeah, we've been pushing this back for a call or two now. So I put that at the top so we can get through it.
00:03:08.910 - 00:03:52.140, Speaker A: Then let's chat about Denkun and we'll do the Devnet conversation as part of that. And then obviously sort of reflect on how sepolia went. And after that, hopefully the bulk of the call we can spend going through the three big things we've been discussing as potential candidates for Prague and potentially the fork after. So vertical EOF and 4444. And then lastly, there's three small things people wanted to talk about. We'll see if we have time, but worst case, we'll just at least point to them and folks can chat async about it. But yeah, I guess to kick it off.
00:03:52.140 - 00:03:59.100, Speaker A: Is Matt on the call? Yeah. Matt, do you want to give some context on the Basu agenda item?
00:03:59.840 - 00:04:30.600, Speaker B: Yeah, absolutely. So, as many of you know, January 6 of this year, we had a halting event with basic nodes on main net. It was not all of the basin nodes on main net. It was probably somewhere between 60 and 70% of the nodes. We, of course, have shared a pretty detailed post mortem. So if someone in the chat for my team would love to paste the full link, that would be great. But I'll give a quick overview of what's going on that we kind of discovered.
00:04:30.600 - 00:05:32.350, Speaker B: What we uncovered was that a lot of, kind of freshly funded accounts were experimenting with metamorphic contract deployment in a way that was exposing some edge cases in Beisu's implementation of the state database called bonsai. We've shared tons of information on Bonsai in the past, but we viewed a few of these kind of iterations of metamorphic contract deployment. When I say iterations, it means that they're intentionally different edge cases that we noticed that we had been kind of also slowly patching in various releases. So we first noticed these things happening on some of our canary nodes on Gurley and Sepolia prior to the January 6 event. So we noticed again some of our canaries going down on those other networks. We were noticing some issues and then we were patching them as we were going along. But on the main event on main net, on block 118 million, 947,983.
00:05:32.350 - 00:06:13.140, Speaker B: That was the big event for what we triggered that caused kind of the big halt. What was happening that we noticed was that the pre image that was being created for some of the block, or excuse me, the pre image for that specific block was being written incorrectly for the trilogues. And the way that trilogs work in Bonsai is that it's a delta between the state and one block to the next. And we were incorrectly writing a pre image for storage that was self destructed.
00:06:13.300 - 00:06:14.488, Speaker C: To the trial log.
00:06:14.574 - 00:07:05.960, Speaker B: So when the block was rolled from one state to the next, that would be considered invalid state. We were not parsing that correctly. And then we were considering the block invalid in some cases, which would cause the basic nodes to go off on a fork. And then in some cases we were just simply halting the node. We noticed again that this was happening. We provided some mitigations for users on Mainnet, asking them to resync their node in some cases to avoid that kind of problematic trilog, since we're not generating trilogs as we kind of do, the world state sync. So if you were able to sync past the problematic block, you weren't writing that incorrect pre image to those bonsai trilogs, which means you weren't applying them as you rolled the block from the previous good block to the problematic block.
00:07:05.960 - 00:07:26.048, Speaker B: This was all well and good. We had again, mitigations in place. We were able to put out a patch. How we were able to discover this was kind of a kind of collaborative effort, which was really exciting to see. We were able to get traces of this block from some other teams. So that was absolutely great. We were working with the Nethermind team.
00:07:26.048 - 00:08:22.400, Speaker B: We were also able to use archive consensus layer nodes to create kind of initial states at the block right before the problematic block. So that we were able to again kind of exercise that trilog, rolling from the last good block to the problematic block. We were able to capture that pre image and see what was happening with the self destructed contract storage. So we again generated that initial beacon state, observed that state change, viewed the bug that we were able to find with the implementation of bonsai, and being able to again put out fixes for that. So it was very interesting in that sense. It was also interesting that these transactions were from freshly funded accounts that were playing with these kind of metamorphic contract flows. They were coming all from exchanges with no Kyc.
00:08:22.400 - 00:09:04.572, Speaker B: But at the end of the day, the client diversity angle kind of worked here. There were no additional kind of correlated penalties triggered for a bunch of basic nodes. And again, the kind of client diversity aspect worked and it showed that there can be issues beyond just the EVM implementation. So we were very happy to have support from some other teams. We were very happy to be able to kind of pass to parse some of those self destruct issues that have been plaguing us for a little while. And we're also very happy that when we ship the fork on Mainnet, that self destruct will no longer be something that we have to worry as much about. So that was a super quick overview.
00:09:04.572 - 00:09:53.470, Speaker B: Again, it really had to do with the way that metamorphic contract deployment interacts with pre images in our state implementation. I encourage you to go down the rabbit hole if you want. We have shared the link in the chat and maybe we'll do 1 second, 30 seconds for questions because I know we have a super packed agenda today. Tim, frankly, the second part of the discussion around performance numbers, I don't think we need to get too deeply into that because there's been tons of public discussion about this at this point, and there's been a lot of, frankly, benefit coming out of that discussion. So I don't feel the need to rehash what went down regarding execution layer minority client performance. But that's basically all I have for that agenda item this morning.
00:09:54.480 - 00:10:42.290, Speaker A: Yeah, thanks for sharing and Amazon for sharing the link in the chat with all the details as well. Any questions? Comments? Thoughts? I would just quickly like to say sorry again for sharing those numbers on Reddit. Those were old numbers and on outdated versions of the clients, and I should have checked with the client teams before sharing them, but I'm glad that there was a discussion about it and we have some very nice numbers now and everything looks kind of good with regard to the worst KCVM execution that we can do at the moment.
00:10:44.100 - 00:10:47.250, Speaker B: Yeah, thank you for that. That's great and we appreciate that.
00:10:50.020 - 00:11:23.500, Speaker A: Thank you. Any other comments? Questions? Okay, well, yeah, thanks again, Matt, for sharing the overview. Yeah, moving on to Denkun. First, we had sepolia fork two days ago. Now does anyone want to give a quick update about how that went? Overall, it seems to have gone super smoothly.
00:11:26.730 - 00:11:30.780, Speaker C: Yeah, so we had the sepolia fork a couple of days ago and.
00:11:32.510 - 00:11:32.826, Speaker A: Most.
00:11:32.848 - 00:11:41.200, Speaker C: Of the validators had updated that client. So it was an uneventful fork. We saw finality as well as blobs showing up exactly when we wanted them to.
00:11:43.570 - 00:12:31.930, Speaker A: Anything anyone noticed that was weird or unexpected. Okay, well, I guess that says it all. Yeah, it looked good. And then Holski is forking next Wednesday, I believe so yeah, that'll be your last one to see before Mainnet. And I believe early next week the blobs should start expiring on gorely as well. So I know that was something we wanted to see on the testnets, obviously before upgrading to main net. So yeah, it seems like things are progressing smoothly.
00:12:31.930 - 00:12:41.886, Speaker A: That said, lucas, you posted something about Devnet twelve saying that the blob spam broke it. Do you want to chat about that?
00:12:42.068 - 00:13:23.558, Speaker D: Yes. So I will maybe leave March in the field in a second. We are hunting one bug in Nevermind, which caused potentially the blob pool to grow beyond the limit we set. And Marcin was doing a test like 2 hours ago and spamming the Devnet with blob messages. And participation rates went down from over 80 to below 60. So over 20% and it haven't recovered yet. So we need to definitely investigate Devnet twelve.
00:13:23.558 - 00:13:55.640, Speaker D: And we want to also do similar spamming on testnets. Probably we'll start with Gurley. So we want to coordinate with DevOps and the community about this because maybe we can make some problems there too. So I don't know. And I think we need to look into this before we can, for example, schedule main net date because this is troubling that it happened. Martin, do you want to add anything?
00:13:56.730 - 00:14:27.806, Speaker A: Yes, I used to do such spamming experiment a few times before, but I was always sending six blob transactions. So in Nethermind we are limiting it to transactions. Doesn't matter how many blobs they have. So we had this 16,000. And we know that gap is limiting.
00:14:27.918 - 00:14:28.580, Speaker C: By.
00:14:31.750 - 00:15:13.520, Speaker A: Size, like on disk space. So if there was only six block transactions, then they had like 13,000, something like that. And today I changed it a bit and I sent a mix of six block transaction and one block transaction. So in Nethermind we still have 16k, but other clients have above 20,000 of transactions. And seems like it may be a problem, but I'm not sure. It's just like guessing. So for the first time, the number of transactions sent in spamming experiment was so high.
00:15:13.520 - 00:15:48.940, Speaker A: Barnabas, you have your hand up. Yeah, just want to mention that before we start blobbing on Gurley, we should wait a few more days before we can test the maximum churn limit because it's possible that we will not recover from that. And we only need a few more days to reach the churn limit test. Okay, sure we can.
00:15:49.870 - 00:15:55.162, Speaker C: Ideally we figure out what happened to Devnote twelve before he breaks the next test net.
00:15:55.296 - 00:16:59.110, Speaker A: Yeah, sure. And then POTUS has a comment in the chat as well, saying there's some block proposals that came late and didn't have blobs in them. And it's unclear if it's a prism or a relay issue. This is on sepolia. So I guess based on all of this, it does seem like we have a few different things to investigate and then we'll obviously see a whole ski for king later this week or early next week. Does it make sense to wait until ACDC to potentially set a main net date, assuming that a week from now we've figured out what's going wrong with Devnet twelve, we've seen the churn limit activate on Gordy. We've seen the blobs expire on Gordy and have a better understanding.
00:16:59.110 - 00:17:23.600, Speaker A: Alex is asking to set a provisional date today. So most people would prefer to do mean we can set a tentative date today and confirm it next week. But then if we realize in the next week or so that things are going to take longer, we're going to have to push it back. So I don't know what people's preference is there.
00:17:24.370 - 00:17:43.000, Speaker D: My preference is not to state the date right now because we don't know if we have some kind of issues in the clients to fix. So before we are resilient to this kind of dos attacks, I don't think it's a good time to set a date.
00:17:45.510 - 00:18:36.100, Speaker A: Fair enough. Okay. And a plus one from Terence on prism. Any other thoughts? Yeah, I would be fine with setting a date, but we can also, the thing is we kind of need the execution layer folks to set the date as well. Yeah. Okay. So what I would do is then if we're going to set it next week again, assuming that we've diagnosed and understand the issues by next week, then yes, El folks, please show up on the call next week or you're just going to have to live with whatever date gets set there.
00:18:36.100 - 00:19:13.550, Speaker A: But I think, yeah, it's probably more reasonable to wait until we have a better view of things and then aim to set a date for main net on next week call if those issues are fixed or we at least know when things are going to be fixed. Yeah. And I guess if for whatever reason some El teams can't make the call next week, just leave a comment in the agenda. If you have strong preferences around, I guess, what's the earliest you could have a release out for main net? Yeah, we can discuss that on ACDC.
00:19:14.930 - 00:19:27.300, Speaker C: Yeah. And if you don't have representation also within 24 hours in chat if you have any further discussion, maybe. I don't know.
00:19:40.310 - 00:19:55.846, Speaker A: Okay. And then there's a comment as well about getting some l two feedback. I know there was a roll call yesterday. I don't know if there was any feedback given there. Otherwise it's something we can.
00:19:55.868 - 00:19:58.166, Speaker C: And Starkware is testing as well, right?
00:19:58.348 - 00:21:05.174, Speaker A: Yes, Starquare has posted some blobs. Yeah. But yeah, that's maybe something we can do as well before next week's call is just try to tally up the different LTU status updates. Okay. Anything else on Denkun, then? Okay then moving on. So next up, the main thing we wanted to discuss today are the three potential big things that we could do in either the next fork or the one after, and potentially try to get to a decision about what we prioritize when. So the three to recap were vertical trees.
00:21:05.174 - 00:21:32.062, Speaker A: Eof. And then Eip. Four four four. Even though the last one doesn't quite need a fork, we could decide to just do a small fork and put extra engineering cycles into four fours. So yeah, I guess maybe to kick it off. I believe the virtual folks have a quick update on their work and we can do questions and kind of dive deeper into it after that. I don't know, Guillaume or Josh.
00:21:32.062 - 00:21:34.130, Speaker A: Which one of you is giving the update?
00:21:34.730 - 00:21:35.522, Speaker C: Yep.
00:21:35.666 - 00:21:37.366, Speaker A: Hello, can you hear me?
00:21:37.388 - 00:21:38.520, Speaker C: Okay, yes.
00:21:38.970 - 00:21:57.034, Speaker E: Cool. 1 second. Let me share my screen if I can. I'm getting an error here. Can I drop a link and maybe Tim, you could share for.
00:21:57.232 - 00:22:00.060, Speaker A: Yeah, yeah, no worries. 1 second.
00:22:05.070 - 00:22:06.810, Speaker E: I'll just drop it in chat.
00:22:08.450 - 00:22:42.090, Speaker A: Okay, let me open this up and try to share this. Okay. I'll leave it like this though, because I think if I go full screen it'll break my. Oh, actually wait, I need to change something on this. Oh, Mike says he thinks he can share. Yeah, the problem. So it was visible on YouTube, but it seems like when I start sharing my screen there, the sound just stops coming in.
00:22:42.090 - 00:22:45.020, Speaker A: Okay, Peter, thank you. Peter saved us.
00:22:45.470 - 00:22:51.020, Speaker E: Thank you, Peter. Okay, so we were on slide four, I believe.
00:22:52.830 - 00:22:53.434, Speaker A: Perfect.
00:22:53.552 - 00:22:56.250, Speaker E: We can go back up one previous.
00:22:56.330 - 00:22:57.022, Speaker A: Yes. Cool.
00:22:57.076 - 00:23:24.310, Speaker E: Thank you. I saw a comment from Lucas as well. A good clarification that it does not solve, but the state growth problem, but does provide. I guess it's a partial solution there. We can get more into this. So, yeah, so not spending too much time on the why of Verkel, but just making sure we don't lose track here of why we're doing the work, we're doing. The decentralization benefits I think are pretty clear and have been gone through a bunch.
00:23:24.310 - 00:24:25.002, Speaker E: Perhaps the most direct, but many other reasons why all of this work is valuable. Of course, instant sync again, probably pretty easy to understand for people. Any new node can theoretically spin up, receive a block and the witness, assuming the witnesses are in the blocks and instantly start validating on the performance and scalability front. Perhaps a bit more theoretical, but still I'd say a fairly clear benefit in reducing disk I o, for example, for non builders. And I think just overall the TLDR here is that statelessness, small enough execution, witnesses that can be passed over the network. This will all unlock a lot of cool new things. So we can go to the next slide, Peter.
00:24:25.002 - 00:24:47.350, Speaker E: I don't know if we're having more. Okay, cool, thanks. Here we have high level overview of what's changing with Verkel. Just sort of at a glance to give a sense for why there are so many moving pieces. A lot going on here. And we will spend time diving into each of these a bit. Next slide.
00:24:47.350 - 00:25:54.860, Speaker E: So yeah, before we dive into each element here is just a slightly different view of the main ingredients needed for vertical and how we are doing so far on each of them. I guess as a note on the progress indicators you see below each item, this isn't really intended or this may not accurately reflect the progress of each individual client, but is more so intended to give a very sort of approximate sense for how far along we are on each item. Don't want to give an impression that there isn't still a lot of work to be done, of course, but it's helpful, I think, hope to understand how much progress has been made on each of these fronts. These numbers could certainly still go up or down a bit over the coming weeks. But overall I think we can be fairly optimistic that they are all trending in the right direction. Cool. So now we can quickly go into each of these one by one.
00:25:54.860 - 00:26:37.928, Speaker E: Next slide, Peter. So yeah, going through these, we tried to organize each into three sections. Hopefully that's clear. Section, three sections, what's already been completed, what's left to do. And then at the bottom, trying to anticipate where we might get surprised to the down the tree structure is one of the items that is the most far along. Next up here is a to do is the shadow fork, which will give us more data and even more confidence in our approach. And this should be complete.
00:26:37.928 - 00:27:44.428, Speaker E: Fingers crossed really, any day now. Next slide. The cryptography also rather far along thanks to Kev Gotti, Doncrad, many others for their long standing work. We have two main implementations go and rust Nim also catching up with their own work I suppose. On the potential surprises front we are interested to get more data on performance and potential impacts of 4844 and this may be a good topic for Q and A at the end. But yeah next slide, gas schedule updates making progress here recently. Still a good bit of work and testing to do understand things and one of the next big milestones here will be getting more dapps other large contracts deployed on testnet so we can continue to learn anything else here.
00:27:44.428 - 00:28:55.008, Speaker E: I think we can move on. Yes, thank you transition clearly I think people all understand one of the most complex and trickiest parts of all of this, but has also been our highest focus area, namely Guillaume and others for some time. Really solid progress. We believe we have a viable migration plan, which is a lot and we'll know more once we complete our shadow fork again, thanks to Perry DevOps for all the work there. Next up on the to do is just sort of finalizing the sorry on the transition and locking that in. Reviewing that and a big piece of that will be finalizing the pre image distribution strategy. Next slide witness generation verification live on the Kalstenin testnet is used in the current version of Verkel sync.
00:28:55.008 - 00:29:26.396, Speaker E: Main thing here perhaps is finalizing the actual distribution strategy. Where are people going to get the proofs from? Included in blocks or distributed in some other way out of blocks? Yeah, so I think that's it for witness generation. We can go on to the next slide and the fact sheet. So Guillaume and Ignacio helped to put this together. And actually Guillaume, maybe to you here, if you're right.
00:29:26.498 - 00:29:58.244, Speaker F: So yeah, the thing is, we wanted to present some numbers, but a lot of them depend on the success of a shadow fork, which is stopping us a bit, has been stopping us a bit for a couple of months. But so what we have to offer are mostly educated estimates. So for example, we estimate that the average witness size should be around 150 tree size. So this is actually an upper limit.
00:29:58.292 - 00:29:58.504, Speaker C: Right.
00:29:58.542 - 00:30:31.650, Speaker F: I just want to make that clear. It would be 80% of the current tree size as of yesterday, like when I did this dump. And what needs to be. Yeah, so what's not specified here is that we haven't optimized the compression in the database yet, so there's a lot more to scratch. This is just the surface and we need to scratch deeper execution time. So we ran that. So this is actual data.
00:30:31.650 - 00:31:30.624, Speaker F: We ran that on somewhat older version of the database. So there could always be a surprise there. But yeah, from what we can tell in terms of performance of executing a block, it's like 10% slower. Of course, once we have the shadow fork running, we can get more current numbers. We did not really try to measure the time it takes to produce a proof because, well, there are several reasons, but one of them is we could still decide to go without the proof in the first iteration of the fork, or we could decide to distribute them differently. But once again, once we have the shadow fork, we'll be able to get numbers, both the time and the size, actually. And, yeah, I think the most worrisome part for people is how long it would take to do the conversion.
00:31:30.624 - 00:31:47.828, Speaker F: So we tested on a fairly slow machine. Right. How old is that machine, Perry? The rock five B. No, sorry, not the rock five b. The other one, the nok, is about three generations old. Three generations old, right. And we also tested, indeed, on the rock five b, which is quite slow.
00:31:47.828 - 00:32:29.290, Speaker F: So we are able to handle 10,000 leaves move per block, and it takes about one extra second. And more importantly, it can be done before the slot starts. So that's quite convenient. And we estimate right now, based on the estimated number of leaves and in the future, that we would be able to do just the copying part within two weeks. Then when there's a buffer, there's a required buffer of one week to distribute the pre images. But, yes, we should be able to handle that under a month, basically. And, yeah, that's pretty much it.
00:32:36.540 - 00:33:09.540, Speaker E: Cool. So we can go to the next slide. Almost finished here. Yeah, and this is pretty much it. I just wanted to bookend this by quickly recapping again benefits of vertical, why we believe it's a powerful upgrade, comes, of course, with a good deal of complexity, but thanks to everyone's efforts here, making very solid progress and confident it'll be ready for primetime soon. So, yeah, thank you all. Any questions? I think we can.
00:33:09.540 - 00:33:16.070, Speaker E: The next slide is just q and a, so we can take those now, assuming we are okay on time.
00:33:16.520 - 00:33:19.930, Speaker A: Yeah, let's do questions now if there are.
00:33:25.250 - 00:33:29.280, Speaker E: Any slides that we want to go back to. Sorry if it was too quick.
00:33:32.910 - 00:33:44.240, Speaker A: And I know there's been a ton of discussions in the chat, some back and forth there. Are there more questions or concerns people want to discuss?
00:33:51.320 - 00:34:11.470, Speaker C: Hey, Josh. So the execution time that I think someone also mentioned this in chat that you had mentioned over there, is it the block production time, or is it an execution time by a full node maintaining worker trees? Or is it a stateless execution time?
00:34:12.640 - 00:34:15.230, Speaker E: More the latter. Stateless execution time.
00:34:19.570 - 00:34:22.580, Speaker A: Okay. Lukesh has some questions as well.
00:34:23.990 - 00:35:17.314, Speaker D: So I just want to comment on the discussion there that not to extrapolate, for example, holesky performance on mainnet performance. If we do Holesky Shadow fork, and I'm talking this directly from our nevermind experience because we are currently trying to move to a path based storage and one of our tries on that worked very well on Sepolia for example, which is a lot smaller, but when scaled to mailnet it actually didn't scale well. So until we have a shadow fork on main net and some performance numbers from there, just keep that performance numbers with a grain of salt that it's not something to make any decisions based on it.
00:35:17.432 - 00:35:20.820, Speaker F: So to be clear, the 10% performance is mainet data.
00:35:22.730 - 00:35:31.714, Speaker D: Okay, so sounds interesting, but after you do Holesky shadow fork, I would be very eager to see like mainnet shadow Fork.
00:35:31.842 - 00:35:32.758, Speaker F: Me too.
00:35:32.924 - 00:35:39.754, Speaker A: Yeah, cool. Yeah, but I guess it's main data, stateless data, right?
00:35:39.872 - 00:35:49.200, Speaker F: It's only like the cryptographic overhead. No it's not. It's actual writing to the database. In fact, that's where a lot of the performance goes.
00:35:51.330 - 00:35:52.480, Speaker C: Right? Okay.
00:36:00.270 - 00:36:02.250, Speaker A: Any other questions? Comments?
00:36:06.370 - 00:36:22.610, Speaker C: Can I ask about the status of this? So if we go back a few slides, you had this sort of status situation based on this, are we in a position where we're ready to do this in the next fork?
00:36:23.190 - 00:36:24.686, Speaker F: I'm just sort of like if vertical.
00:36:24.718 - 00:36:44.202, Speaker C: Sync is only five out of ten done, is there a risk that if we allocate this to the next fork that we're going to get delayed on that fork because we have research needs on the vertical sync? And would it be better to push it one more fork so that Verkel can be basically done when people are.
00:36:44.256 - 00:36:46.060, Speaker G: Ready to start implementing it?
00:36:50.670 - 00:36:57.342, Speaker F: I'm not sure I understood the question actually. Which fork are you talking about pushing it back to? Osaka or what would that be?
00:36:57.476 - 00:37:32.230, Speaker A: I guess he's asking about Prague versus Osaka. Like if we decided this was the main priority today, is it even in a state where it's valuable to have all the clients shift to prioritize that? Or is it better to potentially have a few more cycles around development where we sort of get something like the sync from five to an eight and then at that point we prioritize it for Osaka or some other fork?
00:37:33.550 - 00:38:02.862, Speaker F: Right. The problem with this is that as long as you don't prioritize it, people will not work on it. This is exactly what happened with know it went ahead. No one looked at Veracol. I mean no one. Of course some people did that's why we're here. But it didn't get the attention because it was that thing up in the air that people think they will work on when it's scheduled.
00:38:02.862 - 00:38:22.730, Speaker F: So my understanding is that it should be scheduled. It doesn't really make a difference whether or not you schedule it for Prague or Osaka. What matters is that people start looking at it as if it was the next fork. Because if you don't, you're always going to look at two, right?
00:38:22.800 - 00:38:45.120, Speaker A: Yeah. You're going to focus on the other thing that's important. Okay, so you're saying it's not necessarily the most urgent thing to deploy on the network, but it is urgent to get teams attention on this and significant engineering resources in order to actually make progress on all those things.
00:38:47.490 - 00:39:13.500, Speaker F: Sorry, let me qualify that it is quite urgent to deploy it on the network. I mean, of course we should not rush, but we have the current state that is growing. The more we wait, the more the conversion is going to take longer, which means we have this area of time that is a bit more risky than the rest of normal operation. So it is quite urgent to deploy it also on the testnet, but on main net especially.
00:39:14.350 - 00:39:19.386, Speaker A: Got it. Thanks. I believe Enrico, you had your head.
00:39:19.568 - 00:39:25.786, Speaker C: Yeah, he just covered right now. Because my question was, if we postpone.
00:39:25.818 - 00:39:27.946, Speaker F: This too much, is the state growing.
00:39:27.978 - 00:39:32.720, Speaker C: Too much and then affect the transition that makes from maybe two weeks to.
00:39:33.330 - 00:39:38.146, Speaker F: Maybe one month, because the state now is too big and the transition will.
00:39:38.168 - 00:39:41.810, Speaker C: Be very painful, especially for little boxes.
00:39:42.950 - 00:40:10.140, Speaker F: So far, what we see is that the state grows by roughly 25% each year. If we extrapolate, hopefully it stays this way. We have a little bit of time. Also, machines or nodes on the network are going to get faster and faster. Right. The rock five b is probably not going to be there forever. There will be more powerful versions if that's what we want to keep running, supporting, I mean.
00:40:10.140 - 00:40:55.718, Speaker F: So yeah, it's not like it's super urgent, but what you have to realize as well is that it's not just the transition itself. It's every full sync you're going to do in the future. And right now, or more like about June this year, you will already add about one day to a full sync. Just in terms of pure computation, if you wait much longer, you're making your full sync more and more impractical. Now it doesn't sound like very interesting to most people, but full sync is excellent for testing new code or new code. So this is quite useful for core.
00:40:55.734 - 00:40:56.570, Speaker C: Devs.
00:40:59.470 - 00:41:03.242, Speaker A: Location and Dankrad so a few.
00:41:03.296 - 00:42:20.914, Speaker D: Things I want to cover. So I disagree that we cannot work in unparalleled because we work on 4844 in parallel very well, and we had the test nets of 4844 way way before we delivered Shanghai, and we already have some testnets on the vertical trees. So kind of working parallel kind of works. Secondly, I am against foregoing this in Prague if I already were talking about schedule because of two things. I think while the maybe GEF implementation might be maturing enough, I think each client has its own implementation and it's a very hard thing to do in my opinion. For myself, like the state management and state tree is the hardest part in the code base, and if we want to have multiple client implementations polished to the level of what they are comfortable with, I think it will take a long time. So just because of that I'm against and again, talking from experience, for over a year we are working on our path based tree.
00:42:20.914 - 00:43:03.122, Speaker D: We haven't delivered it yet. Hopefully we can deliver something in the first half of this year. But it's taking a long time and it's delaying and delaying. And like I said, for me it's the hardest part in the code base and we don't want only one client to have it implemented implemented well. We want all clients to have it implemented well and on a good enough level. So I don't see that happening early. And I'm talking this as a client that has a vertical tree implementation already that's joined the testnet and that has Veracle synced prototype in it.
00:43:03.122 - 00:43:32.330, Speaker D: So I would say that Nevermind is pretty decent on the level of implementation, but I am really worried about, for example, mainnet performance numbers on Nevermind. I think they would be very bad. So a lot of work need to go there. Yeah, so I don't see this coming in months. It's still way over a year to deliver in my opinion.
00:43:33.790 - 00:43:43.566, Speaker A: Denkrad yeah, I just had a quick question about the transition, which is what.
00:43:43.588 - 00:43:44.554, Speaker C: Is the limiting factor?
00:43:44.602 - 00:43:45.594, Speaker A: Is it I o bound?
00:43:45.642 - 00:43:47.550, Speaker C: Is it cpu bound?
00:43:49.910 - 00:43:56.980, Speaker A: That might be interesting to know for knowing what to expect first.
00:43:57.590 - 00:43:59.202, Speaker C: Which hardware do we need to watch.
00:43:59.256 - 00:44:11.350, Speaker A: When we say is it going to get faster because we get better hardware and also how much worse it gets when the state gets bigger? Do you know that, Guillaume?
00:44:15.440 - 00:44:49.776, Speaker F: Sure, I'm just trying to enroll everything. So the hardware wise? Yeah, it's definitely both. It's I O bound because the rock five B is extremely, extremely slow when it comes to I O. It's also cpu. If we get better cpu we can indeed pack more leaves. But yeah, I'm trying to remember, to be honest. But I think the biggest problem was indeed I o.
00:44:49.776 - 00:44:53.690, Speaker F: Do you remember, inacio, you if you are here.
00:44:54.940 - 00:44:58.244, Speaker D: Yeah, I would say it's mostly cpu.
00:44:58.292 - 00:45:05.050, Speaker C: Bound, to be honest, and also depends. Right, go ahead.
00:45:05.760 - 00:45:07.724, Speaker D: It kind of depends which is the.
00:45:07.762 - 00:45:18.140, Speaker C: Lowest hardware setup that we are trying to push for. Because there's a huge difference between rock five B and any other normal machine.
00:45:19.200 - 00:45:29.120, Speaker F: Right, yeah, now I remembered because rock five B, I think, had extremely slow I O, which is why the performance was bad. But your average machine is actually cpu bound.
00:45:30.420 - 00:45:32.960, Speaker C: And how many seconds is those 10,000.
00:45:33.030 - 00:45:35.360, Speaker A: Leaves on your average machine?
00:45:36.900 - 00:45:38.480, Speaker F: 1 second ish.
00:45:39.540 - 00:45:41.490, Speaker A: And that's parallelized or not?
00:45:54.220 - 00:45:56.460, Speaker F: I would like to answer Wukesh.
00:45:57.040 - 00:46:02.012, Speaker A: Yeah, let's do that. And then maybe let's move on to the next. Yeah. Right.
00:46:02.066 - 00:46:30.870, Speaker F: So I wanted to clarify two things. So I agree that vertical is a bit far off, but that's exactly the point I'm making. It needs to be looked at. So far, it's just a fairly small team trying to make a gigantic thing happen. Yes. I'm not claiming it will be there. I never claimed it's going to be there within a month.
00:46:30.870 - 00:47:10.870, Speaker F: I still think a year is realistic. Maybe not, but it's definitely not going to happen until every client starts looking at it. This being said, there's been a lot of optimizations that have already been worked on that you can immediately import in other clients. So a lot of the work when it comes to optimization has been done. The second thing I wanted to say is it's true that four four and withdrawals were worked on in parallel. But that's the point. Withdrawals were extremely easy from an El point of view.
00:47:10.870 - 00:47:43.100, Speaker F: What I'm saying here is if you schedule some very light items for Prague and then we come in to do Verco in Osaka, that's no problem. Those things can be parallelized. If you start doing something way more complicated, you're not going to be able to do anything in parallel. So. Yeah, that's just one point I wanted to clarify.
00:47:44.240 - 00:48:11.190, Speaker A: Thanks. And yeah, I guess before we start going into more planning discussion, I think it's worth going over EOF and four four so we have the full picture. Yeah. Peter, I don't know if you can stop sharing your screen so we can move on to the EOF folks. Thank you. Yeah, EOF. I'm not sure who's the main person giving an update here.
00:48:13.260 - 00:48:19.938, Speaker G: Is Alex on? I don't see Alex from any of.
00:48:19.944 - 00:48:21.090, Speaker C: The other UF crew.
00:48:21.590 - 00:49:04.738, Speaker G: I'll go ahead and give an update. We've been moving into ship it mode. We're trying to close the door on as many of the spec possibilities as are out there. There is one out there lingering with regard to whether we're going to use variable length instructions, probably not just because of scheduling issues. Irregardless of the merits, it seems like we could get some value out of it. But as far as the main issues with EOF, it's very derivative of big EOF that we tried to ship about this time last year, and it was about this time last year that we pulled it out of Shanghai. It was going to be another big feature in Shanghai because of solidity, couldn't get some of its constructor stuff working.
00:49:04.738 - 00:49:35.494, Speaker G: So over the past year we added some more stuff to add a couple of big high level features. We remove introspection, code introspection from EOF, which was a major ask from high level people in Ethereum. And also we removed gas introspection. And those two things do things like make gas. Schedule changing il two is much easier. So let me paste this into the chat. So we are starting, just started this yesterday.
00:49:35.494 - 00:50:13.240, Speaker G: We started an implementation metrics to get final status on some of these specific eips and nailing down some of the reference tests. We have reference tests written for a lot of these. Again, these are very derivative of what we had this time last year. So we got kind of a good head start on some of this. Featuring, so, of course, my pitch, like I mentioned back in October, November, I would like EOF to ship as part of Prague, targeting Q three to ship in the test nets so that we are on main net by the time we get to Devcon, Southeast Asia. And I think it's achievable where we're at with the spec. If we ship the spec as we have it today, we don't do any major changes.
00:50:13.240 - 00:51:12.106, Speaker G: And my pitch, if you talk to me, I'll probably go way too far in the details of it. But I feel that these fundamental changes for fixing a lot of technical debt in EVM is kind of existential for the EVM in the next couple of years. All the complaints we see about things like we can't increase the code size, the fundamental problems that are fixed in the way EOF works, and there's a lot of other things that are going in. Why can't we have EVM? Max, we really need immediate arguments. Why can we have immediate arguments? And it's like the hole in the bucket song we just need to fix a lot of things at once and this container format fixes it with container without fundamentally changing the way the EVM works. We still have frames, we still have halts, we still have accounts, we still have storage. And probably the last thing I'll say is the engineers that have been working on EVM stuff tend to be different engineers that work on the data stuff.
00:51:12.106 - 00:51:20.598, Speaker G: So I don't think this would stop any work on vertical from going forward. I don't think it would impact and steal any resources from that.
00:51:20.784 - 00:51:32.660, Speaker A: So thank you. Yeah, great overview. Any questions? Comments? Andrew?
00:51:34.540 - 00:51:41.880, Speaker C: Yeah, so my preference is to schedule EOF for Prague and Velco for Osakar.
00:51:45.820 - 00:52:01.470, Speaker A: Thanks. Yeah, before we get into the scheduling, just any technical questions on what Daniel shared or on specifically. Yeah, Guillaume or Perry or whoever that is.
00:52:03.120 - 00:53:00.620, Speaker F: Yeah, I'm not sure if it's a question or more like a feasibility. Well, I guess it's a mean. We talked about this on Telegram yesterday. There's some kind of understanding how EOF is going to impact Veracol. What I would like to see before we make this kind of decision of putting EOF before vertical is to ensure that it runs on the vertical test net. The reason for this is because there's this whole chunking thing happening in vertical. EOf does the chunking differently, meaning we have two code bases that have never been tried before that have to be tested in parallel.
00:53:00.620 - 00:53:54.370, Speaker F: I am not comfortable going with the of first as long as I don't see the feasibility. It's not just the ability to do it. I agree that UF can be done faster than vertical, that's for sure. The question is can it be done in parallel with vertical? That's a different story. But if it does go first, I need to make sure that we don't ship something and paint ourselves in a corner realizing, oh, actually we broke something. So I would say even before we even try to solve this question of who goes first, I mean, clearly probably Verco is not going in Prague, right? Can we get some actual data as to how well they work together? Basically.
00:53:55.540 - 00:54:10.256, Speaker A: And then your main concern, just to make sure I understand, is about the way EOF does code chunking and how Verco then chunks that differently to put data as part of the tree.
00:54:10.448 - 00:54:42.030, Speaker F: Right. It's just that, yeah, basically they do it differently. And I want to make sure that both code path that are going to be entirely new work, fine. If not, or if we can't get enough confidence, I think EOF should go afterwards if we can get that confidence, then no problem. But I don't want to make a claim, and I don't want us to commit to ship EOF before vertical, as long as we haven't answered that question.
00:54:43.360 - 00:55:18.680, Speaker G: So the code chunking with EOF is going to be basically the same. The code sections are going to work exactly the same, like legacy code sections. We're just going to need to add a little extra bit of logic to make sure that we bring the right parts of the header in. And making sure that we can execute EOF code in a chunked environment has been a discussion in several of the past different EOF implementers calls. It's been something that we've been aware of. It's been something that we're building some things design around. One example that might be worth pointing out is the current chunking plan has an overhang byte and then 31 bytes, and that's because of legacy jump test analysis for EOF code.
00:55:18.680 - 00:55:44.530, Speaker G: We don't need that hangover byte because every code is valid. You don't need to do jump test analysis. If it hits main net then the code is valid because we only deploy valid code and all the jumps are going to hit legitimate places. So in that sense we could save a little bit of space with EOF, but it'll work within the current chunking system. We just need to add some extra analysis to make sure all the appropriate parts of the header are brought in. And that shouldn't be terribly hard.
00:55:45.220 - 00:56:00.016, Speaker F: I don't want shoots. That's the thing. I want to see it working. I'm sorry, I don't want to sound extremely rude. All I'm saying is shoots are fine. What I'm afraid of is that we paint ourselves in a corner and we only realize too late. That's all.
00:56:00.016 - 00:56:01.130, Speaker F: That's all I'm asking.
00:56:01.500 - 00:56:06.360, Speaker G: And before we can put it in a testnet of vertical, we need to have it working in clients.
00:56:07.660 - 00:56:08.410, Speaker F: Exactly.
00:56:10.300 - 00:56:15.420, Speaker G: So we need to implement in clients, which is why we're at with the implementation readiness matrix.
00:56:15.920 - 00:56:23.340, Speaker F: Right, but do you have any client that has implemented it? That's my question. You say it's going to be fast, right? Did I understand that correctly?
00:56:24.900 - 00:56:26.240, Speaker G: What was the claim?
00:56:27.140 - 00:56:34.850, Speaker F: My understanding of your claim is that you say it can be done quite easily by different people.
00:56:35.620 - 00:56:47.728, Speaker G: Yes, it'll be done by different people. Right. In Besu, I do not work on any of the Patricia tree stuff. I only work on the EVM stuff. And my understanding is that's the same on a couple of the other clients.
00:56:47.904 - 00:57:03.870, Speaker F: But how quickly can you slap together. And once again, I'm just trying to see where EOf fits. Does it fit before or after vertical? Can you slap together something to run on the vertical testnets in like a matter of a couple of months?
00:57:04.480 - 00:57:05.230, Speaker G: Yes.
00:57:05.840 - 00:57:06.524, Speaker F: Okay, cool.
00:57:06.562 - 00:57:24.480, Speaker G: We're finishing the spec. That's what the implementation readiness matrix is for. We're writing reference tests for this. Geth would need. It probably is the most advanced vertical. So we would need to have geth dig up and update theirs, whoever was contributing that. Or we could have EVM one do it through the EVMC.
00:57:24.480 - 00:57:32.100, Speaker G: This does seem like you're moving the goalposts. To be honest, that's how it feels.
00:57:33.800 - 00:57:38.550, Speaker F: Once again, just making sure we're not painting ourselves into a corner because I fear that's what's happening.
00:57:39.160 - 00:57:52.940, Speaker A: Okay. Yeah, I think we have a good next step here, but yeah, just make sure we don't cycle around this over and over. Yeah, let's move on. Andrew?
00:57:54.400 - 00:58:23.700, Speaker C: Yes. So can we commit to UF in Prague, but with a caveat that we will have to run a virtual testnet that includes UF and also spend some brain cycles on the compatibility. And if it turns out that there is an incompatibility, we just decide then we don't ship Uf in Prague, just like Shanghai.
00:58:24.280 - 00:58:35.160, Speaker A: Yeah, let's hold that again. I want to make sure we get to four fours as well before we do the scheduling stuff. But Dragan.
00:58:37.420 - 00:59:11.430, Speaker C: Hi. I just want to say that storing the bytecode and using the bytecode are totally different things and how you store it, be that in the vertical, be that in the merkel, is totally different than how you're going to use it inside DBM. So I don't see that as the cutting point or dependency between those two. I just wanted to mention that UF in general seems better fit for the vocal tracking than it's for the legacy.
00:59:11.930 - 00:59:15.800, Speaker F: Just a quick answer. In a stateless context it is the same thing.
00:59:18.800 - 00:59:25.090, Speaker C: Yeah, but for the stateless context we will have at least two or three hard pockets to come to that.
00:59:26.020 - 00:59:26.770, Speaker A: Sure.
00:59:32.020 - 00:59:38.980, Speaker C: But even for that it's better to have UF than the legacy because the bytecode is a little bit smaller.
00:59:44.790 - 01:00:11.290, Speaker D: Question mark about Eof and vertical. Because if we store code in chunks in Veraco, would accessing it would we have to revise the cost of accessing this code from the tree just on the gas costs.
01:00:11.630 - 01:00:18.446, Speaker G: That's one of the eips that was posted a little bit ago. So yes, that's in Verquel's plan, but.
01:00:18.468 - 01:00:33.504, Speaker A: We'Re going to need to revise access costs regardless, right? This is not an EOf specific thing. I don't think that matters because you.
01:00:33.542 - 01:00:36.560, Speaker C: Just count like 32 bytes chunks.
01:00:39.640 - 01:00:42.148, Speaker A: EOf execution probably will need less of.
01:00:42.154 - 01:00:44.470, Speaker C: These chunks, but that's the only effect.
01:00:52.400 - 01:01:17.030, Speaker A: Okay, let's move on from EOF. I want to make sure we also have time to cover four fours. And I guess on that front I know we had someone from portal to give an update on their history network, so maybe we can start there and then if there's any other updates on four four, we can do those.
01:01:18.920 - 01:02:10.820, Speaker C: Yeah, for sure. I'll share my screen quick. I wrote an faq for this, but I'm probably just going to go over the highlights and what I think I should share to just kind of make clear what portal is and what our goals are. So a quick TLDR the Ethereum portal networks is a collection of separate peer to peer DHT networks built on disk v five. The current goal is making execution layer data accessible utilizing minimal resources. Currently we're working on three different portal subnetworks, history beacon and state, each with their own timelines and when they're expected to be online. Portal has a verification first approach for Ethereum mainnet data, and this plays a big part into our guarantees over generic solutions.
01:02:10.820 - 01:02:57.284, Speaker C: We currently are building three clients, but we have been in talks with a few other of the EL teams. Some have expressed interests, and we have also heard of other third party teams who aren't really in layer one who have expressed interest. The portal history network, which will be the main like four four network which will provide headers, block bodies and receipts. We kind of have three major milestones for this. When it says quarters, we mean we're planning to get it out by the end of the quarter. The first milestone would be validating all premerge block history. So this would be like the first 15.5
01:02:57.284 - 01:04:08.830, Speaker C: million blocks, and we're planning to hopefully get that done in Q two. The thing holding us back from launching that is mainly just like asynchronous bugs in our code base. And yeah, the second most interesting one, I'd say, is being able to provide and validate all up to the latest everything but the latest epoch of blocks. We're hoping to get that in Q three and providing that validation, people will also have to run a portal beacon client. What is a portal beacon client? Basically it's just a portal network which provides all the data to validate post merge data. We'll be using it in both the state network and the history network, it's more of just like getting the stuff you need to validate stuff network. And then in Q one of 2025, we plan to be able to provide all the historical data.
01:04:08.830 - 01:05:01.390, Speaker C: Is portal the thing I turn on when I need history, or should it be run continuously? We expect users to do both, but we really hope people do leave it on in the background. Portal is meant to be very resource efficient, so we're hoping that it shouldn't really be an issue. Hopefully in terms of resources, it's on the lines or a little bit more than running a disk. V five client, as that's what we're built on. So that's a little, I guess, reference point. Of course the clients may be storing or storing data, or collecting data to store on their node, but that can be limited to how much it does. Like a max basically connection count.
01:05:01.390 - 01:06:17.060, Speaker C: We use an XOR metric, so lookups for data on the network are in login time. Do individual portal network nodes store all the block history? In normal cases, no. And we really recommend against this, because if you store all the block history, people won't be really able to find it easily on the DHT, because we use metric on how close data is to a node, and we'll ask the closest node first. Every person who runs portal will be able to configure their own amount of disk space they want to allocate, so it's completely opt in. Is the portal network robust? What if a node goes offline? So the main way a portal network gets robustness is through replication values on the network. How will this look like? Let's say you have 400gb of, you only need 400gb to store all the history on the network, but you have like a terabyte of total space on the network. You would have roughly around 2.5
01:06:17.060 - 01:06:43.724, Speaker C: replications of the data. The more replications you have, the less likely of a chance that all the nodes that have the data go offline. I believe I've seen a year ago a graph ips had, but also other DHT networks had on metrics on how long nodes stay up, which is kind of interesting. I should post that.
01:06:43.922 - 01:07:12.872, Speaker A: Yes, there's a question in the chat. I think that might be useful in this caucus, is if you can talk a bit, maybe, about the incentives of running portal, whether or not there are any, and if there aren't, what is the plan to get this up and running at scale so that the execution layer can eventually depend on it? Yeah, for sure.
01:07:12.926 - 01:08:10.330, Speaker C: So I was actually just about to get there. So there's this big question. Do you assume nodes will store the data altruistically? The assumption is there's inherent value in this data itself. There's also intrinsic value in having access to this block history and the cost of running a portal node being extremely low, as in you should be able to run our client and you wouldn't even notice it's there. So we are planning to have no monetary incentives. And one of the major reasons for this is because when you bring monetary incentives into the frame, it's kind of like a race to the bottom. Who can provide the state of the cheapest? And normally those kind of incentives don't really align with the health of the network, but who can basically min max the reward for what you need to provide.
01:08:10.330 - 01:09:45.210, Speaker C: So realistically, over time, as the network gets bigger, the amount of default storage a node will use will lower more and more, and that will just decrease the cost of running portal as the network grows, assuming people actually want this. So who do we expect to run these clients? Well, probably we would expect like maybe an El client who wants to provide a good JSON RPC experience. Let's say you have post four four node and someone queries a transaction receipt from block 5 million. If they're running a portal client, they would be able to just take that call, they check their database, it would be a missing hit. But they could check portal to see if that receipt ever existed and then they could return the actual receipt instead of saying, I have no idea if this block ever existed. For people who are going to run portal altruistically, we expect to bundle a portal client with, I guess the Bittorrent clients, what are going to be hosting arrow one files. Arrow one files for anybody who doesn't know is just the archival format for premerged data.
01:09:45.210 - 01:10:49.244, Speaker C: And yeah, so we expect portal nodes to be ran alongside those as they are providing pretty much the same data. So the cost overhead to run a portal node alongside that is also extremely low. I hope I answered that, but yeah, I'm going to avoid any deep things, but on the history network we provide headers, block buys and receipts. We also provide header epoch accumulators which can be used on premerch to get block number to block hash indexes. This is kind of a fuzzy thing. We're going to have it for a certain amount of time, but we can talk about that in detail another time. So we have that roan map, the beacon is in that, and then let's see.
01:10:49.244 - 01:12:00.310, Speaker C: I think the last thing I just want to say is, does the blobs? No, blobs are supposed to be transient. E 44 is a proposal for the expiry of Ethereum main net headers blockwise and receipts, not blob expiry. I just wanted to mention that as I seen some confusion on what will that include? And then I guess I would just want light client to talk about era one because he's kind of like the spearhead of that. And if anyone wants access to arrow one to experiment with, I guess like the file archival format for premerge, we have some posted in the history expiry channel, but if there's any questions, ask me. I'm happy to answer them offline and I'll provide a link to the FAQ and expand it whenever I get more questions. But that is everything I wanted to go over, just like a high level overview of what portal is and trying to focus on any of the main confusing points or pain points.
01:12:01.480 - 01:12:18.650, Speaker A: Thank you. Yeah, there were a couple of questions in the chat, but I think we covered the biggest ones. It's probably worth Matt giving an update on the error files and then doing more questions after that.
01:12:24.140 - 01:13:34.444, Speaker H: The era one format, just to be clear, is slightly different than the era format, which you might have heard of in relation to this format, that both formats have sort of been developed by the Nimbus team. But the era format is focused specifically on providing consensus layer data, which post merge has the execution layer data as well. And so the arrow one format is really just focused on all of the data that we need to provide for historical purposes that occurred before the merge. And so the format of it is execution layer data only, so blocks, bodies, receipts, the total difficulty of the chain, et cetera. As for the format, it's more or less been specked out for the last year, and in the last year there hasn't been a lot of discussion or changes on things. It seems relatively stable. I think the biggest thing that came up over the last few weeks is we're realizing that the compression format that we're using, snappy, is relatively platform specific, or at least implementation specific in terms of its byte for byte guarantees.
01:13:34.444 - 01:14:43.376, Speaker H: And so we can't take a simple hash of the era file format to get a checksum. It ends up changing whether you export with another mind or Geth. And that threw a bit of a wrench into the situation, but I don't think that it's necessarily any kind of deal breaker. I think that generally we should go forward with the format as is, and we should rely on this accumulator data structure which Colby talked about, this is something that the portal network has come up with, and it's basically just an SSD object that allows us to quickly share a route that represents the accumulation of all of the premerge blocks. So the error format has this thing, it does verify across all of the platforms and that will be the main way of validating that the data you download is correctly. I think if client teams want to provide a checksum for convenience, we can do that, but it's not going to be the recommended way to verify everything in terms of next steps for error one. I think we've got some implementations in another mind other people are looking at implementing to clients.
01:14:43.376 - 01:15:49.740, Speaker H: We really just need to go ahead and put the stamp on the spec and then go ahead and ship it and then start trying to get providers to provide this data. Because the spec itself, exporting the data is great, verifying the data is great, importing the data is great, but until the data is available, we can't actually move forward with removing the data on the P to P network. So I'm thinking in the next one to two months we'll see era pr start to get merged and then we'll get the data out on bittorrents. We'll try and find some people to serve the data and we'll have to give it around a year or so to make sure that the availability is to the degree that we want it. And then maybe sometime next year we can discuss actually turning off the sharing of pre merged data over Devp P to P. And it doesn't necessarily need to go into a hard fork, but I think we're probably going to want to roll it out in unison. So it would be a upgrade of the Eth wire protocol most likely.
01:15:49.740 - 01:15:57.360, Speaker H: That is all the updates I have for error one. Happy to answer any questions on that or if you have questions on portal.
01:16:01.420 - 01:16:59.720, Speaker D: So I have one question about okay, it's fairly straightforward. What would happen if a client user would want to have the whole history? He can download era one files from either portal or like a bittorrent at once and get it. But I'm a bit interesting in the more interactive light client use case, right? Especially with vertical now with portal network and four you can store potentially very small amount of history. But how would an El client, existing El client interrupt with a portal network? Would it be through a different process on portal network client or do we expect it to bundle in process? This is like an open question, what do you think about it. What would be the best approach?
01:17:00.620 - 01:17:44.570, Speaker C: I think clients will do both depending on what they want and the use cases they want to support for portal. We're probably not going to be bundling arrow one files into our architecture ourselves. We're going to be supporting a lookup model where you use a block hash and an index of the type of data you want to fetch on the network. One of the reasons for this is to have nodes that have really low requirements. I believe the biggest error, one file is like 600 megabytes. So yeah, I guess the answer would be like both. We'll probably end up seeing.
01:17:45.420 - 01:18:02.860, Speaker D: Okay, so two more questions about it. One I haven't seen in the document, a lookup from transaction hash to for example block body or something. So with having just only transaction hash, we cannot find anything in portal network transaction.
01:18:04.020 - 01:18:13.280, Speaker C: I think in regards to that. So you said you would need a transaction hash to block hash lookup.
01:18:13.860 - 01:18:15.410, Speaker D: Yeah, something like that.
01:18:16.740 - 01:18:42.280, Speaker C: I believe how we're going to be supporting that use case is we're going to most likely make a new portal network just for storing those indexes. So then you could do a quick lookup there and then you could do a quick lookup on the history network is how I believe we would implement that functionality. But I could look into that specific use case more and get back to you offline.
01:18:42.860 - 01:19:17.030, Speaker D: And last question, if I for example in never mind want to prototype integration of portal network for this use case of interactive one, just looking up things on the fly, how problematic is it? Do I need to implement the full portal network protocol or only some parts of it to doing the lookup? And how would it affect the network if I would be only doing the easier part for the lookup? Right, because I'm El and I'm not really interested in maybe doing everything. Yeah, leak ching. Exactly.
01:19:19.500 - 01:20:20.404, Speaker C: Okay, I think I heard all your questions. Basically implementing portal is just cloning the basic routing table and parts from disk b five and then just calling that portal. So implementing portal in that way is pretty simple for I guess prototyping with a client. The recommended thing I would think is bundling an already built portal client with your client and interfacing it using IPC. I believe we have JSON RPC endpoints to provide the data. We have some lower level like portal ones and we're also planning to provide actual Ethereum JSON RPC endpoints. But I'm assuming that layer one clients would want to access the lower level calls.
01:20:20.404 - 01:21:57.012, Speaker C: But yeah, probably something like that. IPC for prototyping and then if you find it's worth the investment, then looking to build your own client and. Yeah, well, in portal we have different types of overlay networks. And for example, to access the history network data, you don't need to run all of the overlay networks that we are going to implement. For example, for the history network, you will need only the beacon portal network, which is used to track the head of the chain. And then if we decide to implement this transaction history overlay network, you will need that to be able to look up forex history data by using only a transaction hash. So it will be transaction index overlay network and for example, to access the state network data, then you will need to run the Beacon network and the history network alongside the state network.
01:21:57.012 - 01:22:12.030, Speaker C: So you have some kind of choice to make there. Which networks you want to run based on what data exactly you want to access.
01:22:14.500 - 01:23:00.316, Speaker A: Okay, we only have about five minutes left. I think we probably have enough context on portal to at least have a rough idea of where it fits in. I guess probably the most valuable thing at this point would be to hear from the different El teams how they're feeling about basically eof vertical and four. Four. I think for Prague specifically, there's a bunch of small eips that we're all considering. So we already have three that are slotted in. So it's probably not worth going into all the small things we can do.
01:23:00.316 - 01:23:29.370, Speaker A: In addition, in Prague. But just with regards to EOF, with regards to Vercol, and with regards to four fours, like, are those things we want to prioritize for Prague, do we want to pre commit something like vertical in the Osaka fork? Yeah. How the EL teams feel about that? Do you feel ready to make a decision? You have a strong view, or do you want to spend more time thinking about this and going over stuff outside the call?
01:23:36.450 - 01:24:18.410, Speaker H: I'm just curious if other client teams feel similar to me here on four fours. It feels like four fours is pretty orthogonal to a lot of the changes we want to make on main net. And it's something that's somewhat pressing for teams. I mean, the history is growing hundreds of gigabytes a year. And this isn't a really big lift to just continue working in that direction, to actually start lowering the history requirements. We're one year away from doing that, probably. So I think it's extremely high priority and really low touch for teams to be putting some resource to that today and getting down that path.
01:24:21.310 - 01:24:50.920, Speaker A: Yes. Seems like Nethermind and Besu agree the work is already happening so it doesn't seem like there's a reason to stop it. And it's just worth being mindful that, like, the time and effort we spent on that obviously reduces bandwidth for other stuff. But if it is a minimal lift and teams are already working on it, we can keep that going.
01:24:52.810 - 01:25:32.420, Speaker H: I really think we're looking at around one month of effort now ish, and we don't have to have every client team. I think if we have two, maybe three client teams do this, that's really what we need to start bootstrapping this era. One distribution network and I can go off and try and works with some people on what to do for the future for era, but I think getting era one off, we're very close to it and there's not a lot of work that needs to be done at this point. So if client teams are good, know, I think Nethermind is working. I'm going to talk to Beisu later. If we're good with doing that, then let's just get that out and then we can come back in six or eight months and figure out how things are going.
01:25:36.310 - 01:25:40.450, Speaker A: Yeah. Thanks, Andrew.
01:25:41.910 - 01:26:01.574, Speaker C: Yeah. As to four four. So in Aragon, we actually have to do a major release first. We call it Aragon three because in Aragon two, we need to re execute all the blocks from Genesis. And for that we, of course, need all the blocks.
01:26:01.702 - 01:26:02.380, Speaker A: Right.
01:26:02.910 - 01:26:34.070, Speaker C: So Aragon three is a few months away, so for us after that, four fourth will be easy. But beforehand we have to do a mammoth engineering effort. And in terms of EOF and velk, I've already said that my preference is to commit to EOF in Prague, with the caveat that if it's not compatible with Verkel, we don't ship it. And to commit to velko in Osaka.
01:26:35.210 - 01:27:47.112, Speaker A: Got it. Thank you. And there was a comment by Nethermind as well around committing to Vercol in Osaka. And like a weaker view on Eof in Prague, and then obviously from the Beisu side, there was a desire to do Eof in Prague. And we sort of have a minute to end on here. It feels like eof sort of has to be considered a bit more in parallel with the other potential eips for Prague, whereas the decision for Vercol in Osaka is a bit more want, I guess. Does anyone object to committing to have Verco be the main thing for Osaka and figuring out on the next call a bit more the exact scope of Prague and what we want to include there, knowing that we already committed the vertical for Osaka? Last chance.
01:27:47.112 - 01:28:46.490, Speaker A: If there's objections otherwise, I'll open a pr for the Osaka spec later. Mean just then to make a point out of it. I think it makes a lot of sense. But I do think just given that it's relatively rare that we are planning for features that are basically a year plus out still and just kind of from historical experience, that sometimes things slips quite a bit, we should at least be open to say revisiting this in six to twelve months. If by that time, for some unfortunate reason it turns out that just Virgil takes massively longer than planned. I mean I very much don't expect that, but I'm just saying it should not be a hard commitment. Yeah, I think that's reasonable and I think basically the thing we also can commit to is like working on it in parallel to shipping Prague like we did for the past two forks.
01:28:46.490 - 01:30:33.190, Speaker A: Which means that in the time until the next twelve months we'll build up our confidence towards is this thing actually ready to ship or are there some issues we didn't see? But yeah, I think based on a lot of the virtual folks'comments and what teams want to prioritize, flagging this explicitly as the main priority for the fork after Prague and having teams start to, I guess mostly continue to invest in engineering resources and develop this probably is what makes the most sense. Yeah. Any other comments concerns? Okay, and we're already 1 minute over time, so I'll just give a quick shout to the other three agenda items, but I don't think we have the time to go over them. First one was specifying the client versions on the engine API. The idea here is that it would maybe help identify what els are working on. There's a lot of conversations on the thread already, but if people want to go there, hopefully we can get it merged in the next couple of weeks then. Peter, you had something around or actually someone asked to review 7523 but I'm pretty sure we agreed to this a while back.
01:30:33.190 - 01:31:42.430, Speaker A: So yeah, so I guess yeah, Powell, maybe if you want to chat in the r and d discord exactly what the ask was, but I'm pretty sure we had agreed to the empty accounts because we already cleared everything on l one and then last one Carl asked about just reconfirming the decision we'd made around reserving the pre compile range for l two pre compiles. I believe there were no objections to that last time, but if there are any, maybe just raise them on the eth magicians thread for this in the next couple of days. Otherwise the EIP is going to move to last call and be finalized. Yeah, I think that covers everything. And then next call, let's focus on the various potential eips for Prague and try to scope that out a bit more. Anything else before we wrap up?
01:31:50.410 - 01:32:07.638, Speaker C: So I just wanted to point out that there's a new pr in the engine API. We're trying to get some data so that we can measure execution, layer client diversity, which is crucial as we move into the dancun fork. So if you haven't seen it, please check it out. Weigh in. Trying to get more awareness. Thanks. That's all.
01:32:07.744 - 01:32:09.520, Speaker A: That's five one seven, right?
01:32:11.730 - 01:32:12.430, Speaker C: Correct.
01:32:12.580 - 01:32:13.438, Speaker A: Okay, great.
01:32:13.524 - 01:32:17.470, Speaker C: Yeah, it's in the notes. It's in the notes for the meeting.
01:32:18.370 - 01:32:39.400, Speaker A: Sweet. Well, okay, let's wrap up then. Yeah. Thanks, everyone. And also, yeah, if everything goes well on the testnets and we figure out these bugs, we'll be setting the date for mainnet next week on ACDC. So please show up there to discuss that. Thanks, everyone.
01:32:39.400 - 01:32:41.382, Speaker A: Thank you. Thank you.
01:32:41.516 - 01:32:42.440, Speaker D: Thank you.
01:32:42.810 - 01:32:43.880, Speaker C: Thank you.
01:32:45.530 - 01:32:58.140, Speaker A: Thank you. Bye. Sa.
