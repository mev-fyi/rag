00:00:03.280 - 00:00:25.194, Speaker A: Hey everybody, welcome to Verkle implementers, call number 15. This is issue 977 in the pm repo. I will share the agenda here in the chat. And in the meantime, anyone from any client teams want to start things off with any updates?
00:00:30.794 - 00:01:20.904, Speaker B: I guess I can start, yeah. So there's been continued work on getting the conversion process to be tested in the test framework. So I assume there will be more details during the testing update. But yeah, I met with Spencer and we made some progress finding the bugs that I had. So that much made some progress. Otherwise I've been working on the testnet. Relaunch decided that instead of launching a testnet and figuring out if everybody could join, we would dump some test blocks that were produced by my code to see if others could actually execute it, which would save some DevOps resources.
00:01:20.904 - 00:01:49.334, Speaker B: So we've been iterating on this and we just came to an agreement like five minutes ago. There's still another fix to include or at least something to investigate, but we're starting to look good. So I assume we'll be relaunching the Testnet like today or tomorrow, otherwise. Yeah, that was. I think that was all for me. Ignacio, you've got anything else to add?
00:01:51.954 - 00:01:53.294, Speaker C: No, not really.
00:01:56.274 - 00:02:01.454, Speaker B: I mean there's the MSM stuff but yeah, we'll be talking about it afterwards. So yeah, that's all for us.
00:02:06.514 - 00:02:47.144, Speaker D: I can go for Ethereum. As Guy mentioned, Ethereum J's has been trying to run the blocks, run the test cases that Guillem has provided before we launch the next version of Destnet and sort of debugging that. So found few issues in Ethereum J's fixed that and highlighted few issues. And I think once they are resolved then I'll basically re verify. Once Danish can game have resolved all the issues, I'll re verify and see if J's also can agree to the tests.
00:02:53.884 - 00:03:25.224, Speaker E: I think I'll go next. So at Netherland also like I was working on figuring out the gas model with game and except for that there was an optimization that was not implemented in the cryptography library in C sharp. So that is now implemented. And now I starting to focus majority of my time in like implementing the transition in Netherland client because Guillaume also said that he's a bit closer to testing it properly. So now it's a good time to start like implementing that in our client.
00:03:37.504 - 00:03:45.244, Speaker A: Any other updates from any client teams? Daniel, has there been any updates from Nimbus that you would like to share? No pressure. If not.
00:03:46.944 - 00:04:11.724, Speaker F: Yeah. So nowadays it's advisor, I believe that they're handling the integration to the Nimbus client. As far as I know, they made some progress, some good progress. It's not a very thorough integration to the Nimbus client because the code is a bit complicated over there. So we're doing a basic integration for now. This is to be going fine. So soon we'll be able to join Constantine, hopefully in the next few weeks.
00:04:16.464 - 00:04:55.754, Speaker G: On Bezu side, we are trying to prepare Bezu in order to be able to join the testnet. So we did some gas cost modification. We also modified in order to have the circular buffer for the block hash. We are also doing some modification in the tricky generation part and we are also working on some performance stuff in order to patch some call in order to optimize the communication between the java side and the native side, avoid doing some copy, etcetera. So yeah, we continue and we hope that we'll be able to, we'll be ready for the new launch.
00:05:09.414 - 00:05:38.114, Speaker A: Anybody else? If no other updates, we can move on. As far as testing updates, I don't want to put you on the spot, Mario, but are there or anyone else, anyone else would like to share any updates on the testing side? Spencer? Yes, thank you.
00:05:38.814 - 00:06:45.274, Speaker H: Hey, yeah, so we now have a transition tool which generates a test and verifies that the vertical tree post state is as expected. So this is good. To do this, we used a sub command that Guillaume added to get so ta ten. The transition tool is a sub command. So a separate sub command that's used to, on our side, generate a vertical tree to compare against the vertical tree that is being generated by the transition tool. So by doing this, we essentially call this sub command maybe like 250 times for this test. So we have generated a test, but it takes two minutes to generate a single test, which is not ideal.
00:06:45.274 - 00:07:49.524, Speaker H: So I think we'll maybe need to update how this sub command works and try and get it down to the 1 second mark. But yeah, so we still have some more tweaks to do on our side to kind of optimize how we're verifying the post state of the test. But in general, what we have now seems to be, seems to be good. And as a general next step, we want to try and test, I guess, the consumption of the test. So now that we can fill a test, we want to check that we can test the consumption. Do we know why it takes so long to generate the test at the moment? Yeah, so essentially it's because we're calling a binary like over 200 times. And with Python, it's not ideal and we could look into optimizing it, but I think it would make more sense to just change how we use the sub command.
00:07:49.524 - 00:07:57.104, Speaker H: Instead of calling it 200 times, we just call it one time. But we can resolve that internally, I think.
00:08:00.504 - 00:08:01.804, Speaker A: Yeah, yeah, Guillaume.
00:08:02.704 - 00:08:32.304, Speaker B: Yeah. Just to, I mean, I understand. Yeah, that's, that's a huge slowdown and I think, yeah, we, we have the right direction. I just need to understand what you would do. So you would pass me an entire alloc, like Genesis, alloc object serialized to JSoN, and I would return that, like the conversion in vertical into JSON form. So key values, is that what you're expecting?
00:08:32.844 - 00:08:33.564, Speaker H: Exactly.
00:08:33.644 - 00:08:33.876, Speaker B: Yeah.
00:08:33.900 - 00:08:37.788, Speaker H: So that's what we're doing on our side. Exactly right now.
00:08:37.916 - 00:08:53.944, Speaker B: And so my question would be, would the state, like the alloc, be considered complete, or would you be willing, or would you want afterwards to be able to add more stuff to it or to change some values?
00:08:56.364 - 00:09:02.624, Speaker H: So far it would just be just a direct conversion. So just an Alec to a vertical tree conversion.
00:09:03.244 - 00:09:34.474, Speaker B: Okay, so I'm just, my question is more about the code, actually. So if you have code, you would pass me the entire code as it is. But what I'm trying to understand is because I would return, you know, executing the block, I would return the vertical tree as is. So I'm just wondering how you would maintain the alloc yourself. Yeah, I guess. Okay. I guess you are actually expect, you know what to expect, right? From me.
00:09:34.474 - 00:09:37.754, Speaker B: You know, what should be in the tree and what should not be in the tree.
00:09:38.254 - 00:10:08.364, Speaker H: Yeah, exactly. So we have a. I guess so when we write a test, we have the post, the post state alec allocation. So what we expect at the end of the end of executing a test. So day, we'd be passing our expectation to you and then we would ideally get the, our expected and vertical tree back. And then we compare that with the vertical tree that death generates with g ten.
00:10:08.704 - 00:10:17.320, Speaker B: Okay. Yeah, no, that should be able, I don't know if I'll be able to do that today. But you'll have it. Yeah, I guess before the end of the week for sure.
00:10:17.512 - 00:10:24.764, Speaker H: Yeah, I think we can also think on it some more, but I think that seems like the best option for me, at least currently.
00:10:37.384 - 00:11:00.024, Speaker A: Cool. Anything else on the testing side of things, Guillaume? Did you have anything else there on that topic? Okay, next up on the agenda, there are some performance updates that Ignacio has been working on, and he has a doc. Yeah, Ignacio.
00:11:03.244 - 00:11:19.294, Speaker C: Yeah. Thank you. I will share my screen a bit let me know when you can see it. Yeah, yeah.
00:11:19.334 - 00:11:20.110, Speaker A: Okay.
00:11:20.302 - 00:12:40.804, Speaker C: So I will just go, I will give like a really brief overview of this argument, because the details might not be interesting for most people in this call. So what I did last week was I call it like taking another iteration on Virgo, three msms. And this work was motivated by a document that got wrote a while ago. Well, I do this I think would be better, which is you won't see that it's another hackmd doc with what I think is a novel idea of how to calculate MSMs with precomputed tables. I had that in my to do list to really see if this can be used for improving the performance for the go implementation, or any other implementation, actually. So what I did in document is basically walk through all the thinking process, and that led to the conclusion that I will share at the end. So basically in this document I go through some like refresher of like what is this MSM thing and how it relates to Berkeley trees, because maybe some readers might not really connect both things.
00:12:40.804 - 00:13:33.684, Speaker C: So basically in vertical trees, if you separate the cryptography into groups, you have all the proof related things, which is the IPA and the multiproofs. And the other thing is mainly doing msms. Like whenever you calculate a tricky for an account or a storage slot, you have to do an MSM. MSM stands for multi scalar multiplication. Whenever we have to do this state migration from the Markov Patricia tree to the verkle tree, and we have to create a lot of new leaf nodes, we have to do a lot of msms for the new leaf nodes. Whenever we have to update the tree, we have to recalculate the root commitment. We have to do this internal no commitment updates, and those are like extra msms.
00:13:33.684 - 00:15:11.414, Speaker C: So that's why we once in a while I think we revisit if we can improve the performance of this part of the cryptography, because it can have a big impact on the client performance. So since the idea of this document is to come up with a new strategy for an algorithm, I go through like what are we doing today in geth? Just to know like what is the baseline or the algorithm that we are trying to beat in quotes? And I go through like, yeah, what are we doing now? And some summary of the cost and the memory usage and things like that. Then I go through like some overview of what is new this, what is this new idea from Godfrey for a reader that might not be willing to read his document, which I highly recommend, but at least to give some like shorter maybe explanation, and simply stress the fact that this new algorithm has two parameters that are quite relevant, which are called t and b. And I just to give like an intuition on like what it means and what it means for the performance of the algorithm and things like that. So then I simply show that, yeah, I did an implementation that you can see in this link, if anyone is interested. I think it's very easy to port to other languages. It's not like a, it's not like a really tricky thing to port.
00:15:11.414 - 00:16:11.674, Speaker C: Maybe it's hard to understand what is happening just by reading code, but it's just like some code that does point additions and doublings in a tricky way, maybe. So I basically did implementation and did some benchmarks to understand better how this looks like. And I put the result in an appendix because it's really long. This is really long, because as I said, this algorithm has two parameters, d and b. And the performance of that really depends on the MSM length. So basically what I do here is, yeah, you have the result. I explain a bit how to read each line of the results of the benchmark.
00:16:11.674 - 00:17:25.884, Speaker C: So for example, for this case, where we have an MSM length of four, and we use the t equals 32 and b equals ten, you can see how long it takes to run the algorithm. And I also added some custom metrics, which also counts the number of point additions and doublings, which is interesting for two reasons. The first one is that this is more a neutral kind of metric of performance, because the time it takes really depends on the cpu. In this case, it's my cpu. And yeah, if you have a faster, slower cpu, this number might change, but the number of point additions and doublings wants. So that's better. And also this algorithm has some particular behavior of, we call it like wrapping windows, which is really hard to, maybe it's not hard, but it's messy to predict how many point additions and doublings might happen depending on which value of t you use.
00:17:25.884 - 00:19:35.404, Speaker C: Because depending, if t divides 253, which is the number of bits of the scatter field, then you will have like some differences. And I also show how much, how big is the table size for this configuration of DMV, which allows you to understand how much memory this configuration is using. Yeah, so I explained with that, then I have another section which says, okay, so you have this long list of results, and like what is this number saying? And I'm here, so I hear, for example, try to explain things like, okay, if you only increase b by one, let's say, and you keep the rest the same and you compare both benchmarks, you can see that, okay, maybe you are only adding one point addition or doubling, and you are cutting the table size by half and what that means and things like that, because you have to understand well the trade off between memory usage and performance here, and most of the document is trying to unpack that because it's a bit tricky. Then I simply say, okay, after we understand how this algorithm works in a vacuum, let's say how we can use that for vertical trees, msms, and I go through the correct, what I think is the correct way of looking at this problem. Because at the end of the day, the thing that we want to do is beat the current algorithm. And the current algorithm already has some optimizations of using like different window sizes depending on where the points, which are the points that you use for the MSM. So it's not like a really simple single benchmark case that you have to do.
00:19:35.404 - 00:21:13.700, Speaker C: And yeah, I simply go through all like each of these cases and explain if like the current algorithm can be better or not. And I show that if that's the case, which configuration configurations for that case might be better. And at the end of the day I say, okay, so it seems like the final strategy is in this case like a hybrid strategy in which for some cases we'll still use this current algorithm, but for many other cases we can use the new algorithm with different configurations. And the bottom line is the following. So if we are willing to use 144 megabytes of extra ram, which doesn't seem that bad, we have that for msms, between length one and eight, we have like mostly the same performance. Here it's saying it's a bit faster, but this is probably noise because it's calling the current algorithm anyway, and for the rest, so between nine and 156, so it's like a whitish range, we have like between 20 and 44% speed up. So I did the same kind of benchmark in a rock five b, just to double check, and we have like a similar rich performance gains.
00:21:13.700 - 00:22:38.714, Speaker C: So that's okay. And if we think that 144 extra megabytes to get these speed ups is too much, there's also another variant where we only use 16 megabytes of extra ram and we get like kind of the same performance, but only for the range of 33 and 256, we are not like getting this lower range. So yeah, I think that 144 megabytes is a good, it's not really that bad. And we are getting like a good speed up for these ranges, then I go through like other more gory details questions on why these implementations aren't parallelized, which I think I would recommend for our clients to read this section because I kind of explain what we do in get regarding parallelization. Yeah, I go through like other questions regarding some tricks that are explained in the got three's document to reduce the table sizes, but I don't think we can use them in our case. But that's still to be. Maybe I have to think about this better, but I don't think we can do that.
00:22:38.714 - 00:23:53.004, Speaker C: And I also explore if all this new strategy affect table initialization times, because since this, this is using precomputer tables, how to do some work at the start of the client. And the reality is that this doesn't change that much for my cpu. The current strategy uses 400 milliseconds to calculate the tables and now we use 528 so it doesn't change really that much. And for a rock five b, the current implementation uses like three something seconds and still under 4 seconds. So the rest of the cpu's will land in the middle of this, some point in the middle. So the conclusion here is that like from these raw benchmarks seems pretty obvious that this is like a good like new strategy. I'm a bit sad that we have to still keep the previous implementation, to not really have like the goal of all this was to not really have to not really like accept that any other case might be slower.
00:23:53.004 - 00:25:35.324, Speaker C: I had that as a constraint for this exploration because if I allow that, maybe like allow some loose of performance in these cases, maybe things can improve in our dimensions. But I just wanted to only make gains or to really simplify the analysis of everything because it is a bit complicated to say that like a 3% loss here, it is worth a better thing here because we probably will be doing more of these cases in real life than these cases. So I just wanted to avoid all that discussion. So what I will do now is basically like polish final implementation of this and try to do some extra benchmarks in go work all, maybe run a replay, a chain replay to really see if we can see any improvement at that kind of higher levels of the stack and yeah, simply like solidify if we can merge this to main it to like to master, let's say. Yeah, that's it. And I want to say that if anyone wants to ask any question after I read the document, if you're interested, just let me know. And when I say a final shout out to Godfrey.
00:25:35.324 - 00:25:48.944, Speaker C: That was the person that created this new algorithm that could allow these performance improvements. I basically just walked the extra mile to pull this from research to practice, let's say.
00:25:54.484 - 00:25:57.784, Speaker A: Awesome. Thank you, Ignacio.
00:25:59.364 - 00:26:00.100, Speaker C: Cool.
00:26:00.252 - 00:26:12.864, Speaker A: Next up on the agenda, Guillaume had a summary of some of his discussions from East Taipei. An update to the spec. Guillaume, hey.
00:26:13.604 - 00:27:07.884, Speaker B: Yep, let me just try to share my screen. Yeah, so I slept together. A very quick presentation is probably not as good as it should be, but I had a couple discussions with, with Vitalik and Jordi and other people at East Taipei and I wanted to share some of that. So it's not actually so much an update to at least. Okay, so I mostly discussed three eips, 4762, which is the gas, the vertical gas cost 29, 35, which is the history storage, and 7445, which is the precompiler. So I'm going to just talk about, yeah, some of the, some of the insights I got. So the biggest one is 4762.
00:27:07.884 - 00:28:22.212, Speaker B: So I asked Vitalik what was the design, because it wasn't clear to me what was the design principle, how did, how did they choose those, how did they choose their numbers? And they, what he told me is that it's, I mean the general insight is that they try to not change anything of the, any of the gas costs. So contrary to at least what I was believing, the vertical cost should pretty much be the same before, I mean the same as they used to be. It's not 100% perfect, but it's pretty much the same. So for example, if you look at s load, you have the, you have the branch read cost and the chunk read cost. And if you add them together, you get the old storage cold storage cost which is 2100. And if you access an account, let's say you only read the code, hash the balance and the nonce, you get 2500, which is about like slightly under the cold account access cost, which is 2600. But if you read more, of course it will be a bit more expensive.
00:28:22.212 - 00:28:23.824, Speaker B: So depending on what you do.
00:28:25.564 - 00:28:25.900, Speaker F: It'S.
00:28:25.932 - 00:29:42.134, Speaker B: Going to be more expensive or less expensive, but it's roughly in that ballpark. And another realization I didn't, or at least I had while talking to Vitalik, is that if you look at the cost of create, for example the field cost which used to be 200 gas per byte, if you have 31 bytes, you get exactly the value of fill cost. And if you check what it costs to write to the state, to write an account to the state, the contract header, so version balance, nonce, et cetera, you will also find that if you multiply it by the fill cost, you find 31,000, which is effectively almost the cost of the old. Well, the cost of the current instruction, like create instruction, and the difference is 1000. And this is what the new cost of the create instruction, like the new static cost of the create instruction is. So, yeah, basically the realization is that the gas cost should be pretty much the same. So there's no huge increasing gas cost with Verkle.
00:29:42.134 - 00:30:47.300, Speaker B: So yeah, I wanted to share that. So this, yeah, this is not, this is not an update to the, to the spec per se, but at least for me, it's a new reading, and I'm going to use it to try to clarify this EIP, which is pretty messy, try to make it a bit more streamlined. And in fact, Josh, with your help, I'll be recruiting you to help me do that. The second thing that was a discussion with Jordy. So Jordy from Polygon, ZKVM and all about 29, 35. So until now we had this conversation that we decided it was okay, or at least mostly okay, to have a buffer of 256 values. But Jordy said, yeah, that 256 blocks, that's way too little for us because of the time it takes them to generate their proofs.
00:30:47.300 - 00:31:24.872, Speaker B: So they would rather have two weeks worth of blocks instead of just 256. So we would still have a ring buffer, but it would be a much bigger ring buffer. I actually didn't compute how many values that was, but, yeah, it's like 256 blocks is just under an hour. So you would have 256 times two. So 512 times how many hours there is in a week. Week. Yeah, I don't want to make the calculation right now, but you get the drift.
00:31:24.872 - 00:32:04.324, Speaker B: So, yeah, the block would just like, the buffer would be much larger, but not so large that it would start having an impact on the overall state size. And I think, I mean, yeah, I wanted to share that. Maybe there's some pushback, but barring some, some pushback, I think we should just change the EIP to do that. Okay. And the last, the last comment, or at least the last proposal for a spec update was with EIP 7545. So it's the pre compile. So once again, it was a conversation with Jordy.
00:32:04.324 - 00:33:15.098, Speaker B: And his comment was that, I mean, he didn't know about this EIP. He just said it would be great if we had this. And their idea, or at least their request, comes from the fact that what they want to do is update their contract to use a precompile, and they don't want to be rushing to update their contract at the moment when when the fork happens. So what they would like is to have a format, a contract, sorry, a proof format agnostic pre compile that they can just pass a blob to. And then the contract is aware of whether or not the forecast happen or where in the, where in the conversion it is. And when that happens, when the fork happens, it, so before the fork happens it will expect an NPD proof, but you know, just the contract itself just receives the proof in the call data or however it gets it, and then sends it to the pre compiled pre compile verifies the MPT proof, says okay, that's valid. And as soon as the fork happens it receives.
00:33:15.098 - 00:33:46.890, Speaker B: So during the conversion it receives both a vertical proof and a NPT proof together. But once again it's trans it. What's important is that it's transparent to the contract. And then once the conversion has terminate completed, you only receive one vertical proof. And that's once again you don't have to deploy the, you don't have like the contract doesn't have to be aware of what's going on. And that's at the l one level. But it's also true on l two.
00:33:46.890 - 00:34:39.454, Speaker B: They want to do the same thing, so they would like to have this eip so that you can deploy the same contract wherever you want. But for example, l one would be switched to vertical, but l two s some l two would not have switched to vertical yet. So they would still use the old MPT proof format or whatever, a new one, a Zk proof, whatever they want, as long as it's the client that knows where it's at and how to handle it. And the contract itself, the same code can be deployed anywhere. That's what they would like. So yeah, that's pretty much it. The bigger change is, well, first it's in the API of course, although Ignacio was already pushing for a change in API already.
00:34:39.454 - 00:35:58.624, Speaker B: So we would just get rid of the version, or at least we would not make the version like the contract would not care about the version, it would be for the blob payload or whatever payload they used that would say this is a version. And the other, I'm losing my thread of thought. Yeah. The other change is that instead of activating it in Osaka, the vertical fork, we would need to activate it in Prague or whatever fork comes before vertical to give enough time to the l two s and other contracts using this precompile to update their code to use the precompile, and then they could go through the fork unimpeded and not having to rush to deliver an update to their contract at the fork block or after the fork block. Yep. So that's pretty much it for the, for the update or at least for the, for note, for sharing what I, what I discussed. I don't know if there are questions, but otherwise I will work on, on updating all these eips to try to talk about them during the next ACD.
00:35:58.624 - 00:36:02.784, Speaker B: Yep.
00:36:05.684 - 00:36:27.364, Speaker C: Yeah, I have two questions. The first one regarding the first slide, or the first topic about that the gas costs of vertical should kind of approximately match the current ones. So this is true like ignoring coal accessing cost, right?
00:36:27.524 - 00:36:27.956, Speaker B: Yes.
00:36:28.020 - 00:36:31.428, Speaker C: I mean, yeah. Okay.
00:36:31.596 - 00:36:34.264, Speaker B: Right. That part was missing, but now it's covered.
00:36:35.704 - 00:37:09.744, Speaker C: Cool. And regarding this last point about the precompile or the contract for verifying proofs, do you know if they have like a desired API for the precompile or the contract? Because it's a bit tricky because we have to include all these, which are the keys that they are expecting the proof to prove and all things like that. Or they are just like talking about their desire, or maybe they already have a proposal for the API or how they use it today.
00:37:10.204 - 00:37:53.624, Speaker B: No, they don't really have at least like Georgie has not described as a very specific interface. He just told me I want to be able to prove that what I'm looking for is in a given state route. That can be. Two weeks ago I would like, I mean, I was going to try to update the EIP itself because it's quite far from this use case, and then approach him again to ask him for input. But yeah, maybe I should ask him first. So, yeah, the short answer is no, but we indeed need to get this API properly designed before Prague, basically.
00:37:54.524 - 00:38:12.584, Speaker C: So. Okay, so, and this means that the first implementation of this precompile should only have cryptography related stuff to the Merkle Patricia tree, right. Not necessarily the work of tree because that will never be used until the fork really happens.
00:38:12.964 - 00:38:25.374, Speaker B: Yes, absolutely. This being said, yeah, we need to make sure that we don't change the interface. That's all. We need to make sure it works basically, before we, we deliver this EIP.
00:38:38.674 - 00:38:39.454, Speaker A: Yeah.
00:38:41.234 - 00:39:03.534, Speaker E: I just have a small question regarding the, with the block hash, should we not create these two different eips, like one for like switching to a ring buffer and then another one to increasing the size of the number of blocks we are supposed, the block hash opcode is supposed to return? Because this seems like two different changes to me.
00:39:06.754 - 00:39:36.222, Speaker B: Right. So I mean, ok. Trying to make sure I understood so you would want two aips, one for the ring buffer and one to increase the size of the ring buffer. Um, we could do that into EIPs, but I'm not sure I understand the logic of. Because, you know, this EIP does not exist yet, so that's why I'm, I'm not sure I understand. I mean, this EIP hasn't been delivered yet, so we could do everything in one go. I don't mind doing so.
00:39:36.318 - 00:40:13.884, Speaker E: Yeah, no, so I also, like, I generally don't mind, but the thought process, my thought process was like, one AIP is just to change how block hash is implemented, and other AIP might, other change might have other issues with other implemented contracts where we have, we are increasing the number of block hash that we provide right now with the block hash opcode. So, yeah, that's the reason I have. But yeah, we can still do it in the same, but still, like right now, the behavior is that, like, if someone asks for a block hash that's not within 256, it returns a zero value, a null value, but then the behavior is changing.
00:40:14.424 - 00:40:14.976, Speaker G: So.
00:40:15.080 - 00:40:24.724, Speaker B: Yeah, yeah, fair enough. Yeah, I would have to think how to format those things, but, yeah, that's a fair point.
00:40:39.604 - 00:40:52.624, Speaker A: Cool. If nothing else on this topic can move on to the final agenda item, bugs ahead of the testnet relaunch. I think that was also you, Giyom.
00:40:55.604 - 00:41:46.014, Speaker B: Yeah, it was not exactly me as much as. Yeah, basically I wanted to take some time because we had this conversation, this backs and forth with most of the client devs. So I think the main course of the vic is pretty much over. But I wanted to see if there were questions about the blocks I produced, the test blocks I produced from other clients that we could discuss right now and not necessarily keep everybody in the call, but just have, because we have this time to talk, we could do this. So that would be like some kind of peer programming session if there's a need. Otherwise, yeah, we just call it off early.
00:41:49.274 - 00:41:56.854, Speaker A: Cool. I guess. Does anybody want to stick around for that? I don't know what the interest level is.
00:41:58.254 - 00:42:03.634, Speaker B: Thing is, we just solved a lot of problems right before the call, so I don't know if it's really useful, but it could be.
00:42:05.734 - 00:42:19.594, Speaker A: Sure. Got it. Okay. I guess we could also maybe coordinate a separate time. If folks prefer happy. Either way, maybe best to do it separately.
00:42:20.414 - 00:42:37.296, Speaker B: Yeah, I guess we should do this. I mean, I know giginder, for example, is waiting for tennis to be able to verify. Ten is just verified, so maybe. Yeah, the timing is just bad. I mean, we fixed bugs, so it's not a bad thing. But the timing was a bit bad for. For this to happen.
00:42:37.296 - 00:42:40.204, Speaker B: So I guess. I guess we can call it a day.
00:42:41.544 - 00:42:56.062, Speaker A: Okay, cool. Then I guess we can end it there, if nothing else. Thank you, everybody, for joining, and, yeah, we will see you next time. Thanks.
00:42:56.238 - 00:42:57.206, Speaker B: Take care.
00:42:57.390 - 00:42:58.394, Speaker C: Thanks, everyone.
00:42:58.694 - 00:43:00.462, Speaker F: Thank you. Bye.
