00:00:00.170 - 00:00:28.530, Speaker A: Tonight's agenda is a fascinating one. We're starting off with Victor Tron, who's going to talk us through the Ethereum swarm platform that he's put together and built and written. And then we're going to take a short break, and we're going to go into an hour long panel. So, warning, you might want to take your comfort break. At that point, we will have a panel about law and the blockchain. So we have four esteemed panelists introduced by Stuart mast.
00:00:28.690 - 00:00:29.830, Speaker B: Where's Stuart?
00:00:31.290 - 00:00:42.860, Speaker A: Back there chatting. Okay, thanks to Stuart for putting it all together, and we'll bring that to you now. But without any further ado, Victor, in your own time.
00:00:50.590 - 00:02:12.614, Speaker B: Can you guys hear me? Is this going to be okay? Sorry, I'm not sure how my medication works, so I sit down better like this. So I'm happy to talk to you here tonight about swarm, which if Ethereum is the word computer, then swarm is the word hard disk, so to say. Why is this important? From the very early on, the founders of Ethereum realized that Ethereum as a blockchain and offering a decentralized consensus engine over a Turing complete system is a fantastic asset. But what else can we do with it? And they soon realized that, for example, we can fix the web, maybe, and realize that if you complement the blockchain decentralized consensus engine with a few other services, for example, a decentralized storage layer and some sort of decentralized messaging system, then you have everything that you might need for the third web, so to say. So tonight we are not only going to talk about swarm, the decentralized storage system, but in general, maybe a vision of how web3 experience would look like. So I would like to present tonight in three parts. Let's see how it pans out.
00:02:12.614 - 00:03:29.754, Speaker B: In the first part, I would like to drill through very quickly some low level details and especially offer it as a kind of finger practice of how Ethereum can serve as a fantastic platform to architect, a system of incentives where participants in a community, in a network, are forced to behave according to certain rules that are conducive to the desired operation of the system. This is what I'm going to talk about in the context of swarm incentives. In the second part, my colleague Nick Johnson is going to talk about the Ethereum name service, which bridges the gap between these low level parts that I'm going to mention. Hashes and chunks and crazy things like this connects it to the level that's meaningful to the user, basically the semantic web. I'm kind of abusing this term right now. And finally after that, I would like to kind of expand the discussion a little bit. Maybe better do it in an interactive session.
00:03:29.754 - 00:04:49.368, Speaker B: We see how it goes, where I'd like to indulge in certain, maybe fairytale, maybe somewhat science fiction visions of what the web3 experience could look like, and what an ecosystem full with base layer services like swarm and Ethereum can bring, and what value it has to users. Okay, back on track. So before we go into what swarm is, maybe a little bit of history is needed. So in the old days of the web one, what did you need to do if you produced content and wanted to make it accessible? Basically, you had to have a web server which you operated yourself. You had to pay for the maintenance costs, even though no one cared about your content. And if you happen to be lucky and get, say, slash, dotted, or if your content became suddenly popular, look what happened. Exactly the opposite of what you expected.
00:04:49.368 - 00:06:38.700, Speaker B: It's not that you become more available, but actually you got almost like ddos. And if somehow your server survived this, then you paid horrendous amounts for bandwidth use. And if you were less lucky, then your server crashed and even those who were able to review it before couldn't any longer. But at those times, you have had relatively extensive control over your own content. Now, the infrastructure problem of the web server maintainer was somewhat rectified in the times of web two, where cloud providers came up with relatively clever infrastructure that alleviated this problem and offered content providers technology that allowed them to scale according to their popularity. So they basically put clever load balancers in front of servers who took care of spinning up instances as a response to a certain content popularity. However, this came at a certain cost, and as we all know, content producers are more or less forced into what my colleague Danielle likes to call it, the faustian bargain of content hosting, which is that in return to these very cheap and sometimes even free services that scale and help content producers to host and serve their content to their audience.
00:06:38.700 - 00:08:04.200, Speaker B: On the way, they gave up quite a bit of autonomy over the data. It's not only that it's in there, in this centralized stylos power to control the use space, but they often aggregate and collect user profile data that then they can sell on to third parties. And this kind of monetization option has a lot of negative consequences on how the web works. So how can we fix some of these problems? Quite early on, there were solutions which were based on this idea of peer to peer. So in these cases, the, the content is distributed among appears in a distributed network of nodes. And you can see that if you provide some sort of way to take care of redundancy in the network so that certain pieces of data are replicated well enough that the dropout of certain nodes in the network don't count too much, then you have a situation where your system might end up being more resilient, more fault tolerant, and even zero downtime, which is very rare with centralized solutions. And often you get this deal at the fraction of the cost of what you would have with centralized systems.
00:08:04.200 - 00:09:17.330, Speaker B: Of course, the problem with this, that some of these solutions from early on, like Napster and the like, were somewhat centralized anyway, so that led to, because they were used for activities of copyright infringement, the powers that be went after them, and most of these athletes were shut down. And ultimately what remained is Bittorrent, which is a fantastic solution to some of these problems. However, it still has a missing ingredient, which is incentivization. Basically, it's not solved how nodes that help distribute content, so to say seeding content, what incentivizes them to do so. And here there's a problem of content producers not very easily being able to share the cost of sharing their content with their audience, with the consumers. There's no frictionless way to do that. And so this is where Ethereum comes in, and this is where swarm would like to make a difference.
00:09:17.330 - 00:10:36.624, Speaker B: And our team was mainly concentrating on the incentive system, somewhat due to the fact that in the fall, when we had certain austerity problems in Ethereum, we cut certain projects scope. And since in terms of file distribution and sharing some other solutions already they exist, notably ipfs, we concentrated more on aspects that were unsolved. So the instance system was such a focus. And that explains also why that was the first bit that we came up and wrote two papers about. So these are the Easter sphere orange papers, the first two in the series that we published. And in order to understand them, maybe I just quickly give an overview of how swarm works. So from the point of view of the user, the swarm is a document storage system and content delivery network, which offers a serverless hosting solution where you can retrieve data based on the familiar uris.
00:10:36.624 - 00:12:25.030, Speaker B: But underlyingly what swarm does is that it chops your data into tiny little pieces and some are scatters. You guys also find this extremely hot here, or is it just me because I'm sweating like a pig? Okay, yeah, sorry. It might affect, might affect my brain as well, a little bit. So anyway, so the storage problem is basically reduced to the problem of storing chunks over a network. All the higher level assemblage of data and linking and referencing are kind of higher level layers. Now what happens to chunks? What happens when someone uploads content to the swarm? How do you get data in? Just double click on it. I think it's going to work.
00:12:25.030 - 00:13:26.988, Speaker B: This is a Mac. Yeah. So let me talk, let me talk. And then we figure out the slides next. So what happens is when we ingest a piece of file or a document, we chop it into tiny pieces, like 4 kb little pieces, and send them off to the address that they belong. So what's this address business like? How come a chunk has an address? Well, an address is basically a hash of the chunk. A hash of the chunk is a digest that has the properties that is collision free and uniformly distributed.
00:13:26.988 - 00:14:41.450, Speaker B: So if you have a large number of these chunks, you can be pretty sure that it's uniformly distributed across the address space. Now the interesting thing is nodes who participate in the swarm network also have addresses which is based on the hash of their public key, which means that nodes are immediately linked into, thank you, into payable entities accounts on the Ethereum blockchain. So when chunks are sent out to the network, they basically enter your closest neighbor to that chunk. That's where you send it to first. And through a series of forwarding operations, the chunk will end up at its desired destination, which is at a node that's closest to that chunk address. That node is called the custodian, and that's where through routing protocols, a retriever who wants that content will find that particular chunk. So this is what it looks like, something better.
00:14:41.450 - 00:15:41.036, Speaker B: Sorry guys. So when we chop the data, this is the chunk tree that we end up with. This is a Merkel tree that has properties of integrity protection and allows for random access into the files, which means that for example, you can browse, see a certain directory path in a huge file system without downloading the whole thing, or you can skip and watch a movie from the middle without downloading the whole stuff. So this allows for all kinds of clever things. So how do you retrieve a content? It's the same way. So you basically send retrieve request, and you send it to your neighbor that is closest to the address. And recursively, all the nodes do the same.
00:15:41.036 - 00:16:42.748, Speaker B: So through a series of forwarding hops, the grooting algorithm guarantees that your request will end up with the custodian who will be able to serve you with the data. So interesting thing to note here is that unlike traditional distributed hash tables, swarm is somewhat specific in that it's not only metadata about the whereabouts of a file that's found within the closest node, but it's actually the data itself. And it has very interesting properties following from this. So I have to speed up a little bit, I guess. So far out of. I've not been talking for half an hour, have I? Okay, okay. So I tried to do it for a little bit quicker.
00:16:42.748 - 00:17:47.572, Speaker B: So we came up with this system called somewhat playfully, swap, swear and swindle. And the incentive system has basically two pillars. One takes care of the part of the operation that relates to smooth, low latency retrievals. This kind of corresponds to compensating nodes for their bandwidth and for their efforts to make low latency transport possible. On the one hand, there is a completely different problem which relates to preserving. Make sure that data is preserved even when the data is not often accessed. So if the data is requested by a lot of nodes, then based on the bandwidth's incentive, which pays nodes for serving a particular chunk, they'll be incentivized to keep it and cache it.
00:17:47.572 - 00:19:28.996, Speaker B: Therefore, popular content automatically replicates across the network and therefore serves as the incentive for the swarm to become an elastic cloud which auto scales to popularity. But what happens to things like my backup or my birth certificate that I really want to have in ten years when I first want to access it, but for ten years, I probably never going to care, et cetera, et cetera. So for these type of files and documents, there's another incentive system that's needed, which more resembles an insurance system whereby I buy a promise that you're going to keep a piece of data for me up to a certain time or date of expiry. And I think we should skip some of these details. So the swarm accounting protocol is just a clever way to account for the mutual use of resources. When you pass content, pass chunks. So when chunks are retrieved and you answer to delivery requests, then on every live connection there's a tally that calculates like give and take, basically how many chunks I gave you, how many chunks you gave me.
00:19:28.996 - 00:20:51.680, Speaker B: And on the long run, if we have approximately the same consumption and interest in data, then on the long run this equalizes and our balance will be approximately zero. Now if the telets too much in favor of one peer, then you obviously expect some compensation. And in these cases you send a check to the other person, to the other node. Checks introduce another interesting possibility. You can delay the caching of checks, which saves on transaction cost and therefore allows for all kinds of savings. And as for the storage the most interesting use case that we wanted to cater for is the so called upload and disappear situation. This situation where we really want to upload content and right away settle on some sort of insurance that it's going to be stored and it's going to be available up to a certain expiry date that I want without me ever having to be present and check on the file or download it.
00:20:51.680 - 00:22:14.280, Speaker B: There's various techniques that we use for this that includes erasure, coding for redundancy, proof of custody, audits that check regularly the integrity of data, and repair the data if certain chunks are missing. Because based on the redundancy, unless there's a lot of the data is missing, it can still be recovered. And some of the details I can talk about in the question section. So the most important part is how we use the blockchain for this. So the most interesting point is that when you enter a store request because you want to upload a certain chunk, then the node that first takes over that task takes over the store request and accepts it from you, they respond with a receipt. That receipt is going to entitle the owner to litigate against the swarm in case the chunk is not found. And the litigation is basically sending a challenge as a transaction to the blockchain.
00:22:14.280 - 00:23:14.190, Speaker B: And the blockchain can prove whether the proof of custody that they provide as a refutation for the challenge is correct or not. So we had to come up with ways to represent these proof of custodies in a compact way and easily verifiable from within the smart contract. And also we have to come up with ways to do the proof of custody that helps optimize the problem of proof of custody audits for erasure coded so redundantly coded files. So these were the challenges that we tried to solve with the system we came up with. Okay, so I think I'm going to skip this. And since time is passing, I think I should give it over to Nick right now. Yeah.
00:23:14.190 - 00:23:28.992, Speaker B: And then we come. So Nick, we talk about. You have your side, right?
00:23:29.046 - 00:24:30.340, Speaker C: Hello, my name is Nick Johnson. I work at the Ethereum foundation as of Monday week, but they already have my nose to the grindstone. Not that it's unpleasant talking to you all you fine folks, I'm glad I'm here. I've been working on the Ethereum name service, and this basically provides the core component that maps from computer readable and secure hashes to human readable names. My design goal in building this was to be useful for more than just a single task, much like the DNS system is used for a variety of applications. So why do we need another name service? This may be familiar to some people, and you may already be familiar with Namereg, which is a basic name service that's deployed on Ethereum. It's integrated at a basic level into web3, and it provides basic name resolution properties.
00:24:30.340 - 00:25:10.160, Speaker C: It has a few issues, though. It doesn't support delegation. So paradoxically, for a name service that's deployed on a distributed platform, it has centralized authority. There's only a single contract that can be authoritative for all names. There's a single flat global namespace with no hierarchy. And because there's a single contract, there's no support for different implementations of resolvers that might have different needs or need to resolve different sorts of names. The other problem that name Reg has is that it conflates several different areas that are important to name registration.
00:25:10.160 - 00:26:00.968, Speaker C: On the one hand you have actually looking up a name, on the other hand you have how to register a name which we on the Internet use with registrars and so forth. And finally you have governance, which is who decides who gets to register names, how much do they pay, how are the names handed out, how long can you keep them for? And so forth. And each of these are separate issues that need to be dealt with separately, but they're also largely orthogonal to each other. You can solve one well and leave the others to someone else. So the Ethereum name service focuses on the first step, because it's the one that needs to be agreed on widely by the greatest number of people, and it leaves the rest to future work. So what makes a good name service? In my mind, this is my list of criteria. You may disagree, you may disagree vociferously.
00:26:00.968 - 00:26:38.508, Speaker C: We've been having some interesting ongoing discussions, and I'd encourage you all to join them. So, separation of concerns, like I just outlined, it shouldn't try and do many things as a proposal. Eventually the system will integrate all three of those, but they need to be considered separately and on their own merits. It needs distributed authority. Unlike name Reg, you should be able to have multiple implementations. You should be able to delegate from one to the other, so that you can say this particular subdomain is this service is authoritative. For that it needs support for many types of records you shouldn't have to hard code in.
00:26:38.508 - 00:27:24.700, Speaker C: This can resolve swarm hashes, and it can resolve names of contracts or addresses of contracts. But then if you do later want to add in, say, mail exchanges or I-P-V four or I-P-V six addresses, it shouldn't require changes to the protocol? Well, it doesn't need, but it's very useful if it has compatibility with existing systems. It should be possible to build gateways so that this functions elsewhere. And ideally it should support both on chain and off chain resolution. So when we're resolving swarm hashes, frequently the resolver will be a web browser or some proxy server that sits between the web browser and the swarm. But in other cases it may be a contract on the chain. And if you're doing other resolution tasks, such as looking up names of other contracts, then being able to do it cheaply and on chain is very useful.
00:27:24.700 - 00:28:09.544, Speaker C: So the ENS as a whole only has two major components. There are name services which are on chain, they exist as contracts, and they have a well defined interface that everybody agrees on. And you have local resolvers, which are libraries that are part of a contract, or a browser, or a gateway to DNS, for instance. And resolvers talk to nameservers, and name servers respond. There's no direct communication between the services. So the organization of ENS, it's inspired by DNS, but not entirely like DNS, and also by maker's name service proposal. And they also called theirs Ens, just because we need some extra confusion.
00:28:09.544 - 00:28:58.680, Speaker C: But I've since talked to them and I think I've worn them down, and they mostly agreed that this is a reasonable proposal. Unlike DNS, it's hierarchical, based on name components instead of based on authority delegation. So the boundaries of authority, as you'll see, are strictly according to parts of the name. And it uses relative naming, which means that you can say, this entire namespace is a subtree of my namespace, which will also become apparent in a moment. Any name component can have records associated with it, which can be swarm addresses, or they can be mail exchanges or any number of things. And I just said that. So this is an example of a simple layout of an ENS hierarchy.
00:28:58.680 - 00:29:37.652, Speaker C: Down the bottom you can see we've got a global route, and this is a consensus, really. There's nothing intrinsic to the system that says there's a single global route, but we're assuming for the sake of argument that this is the one most people agree on. And the global route has a single entry for ETH, and that points to the S route, and each box here represents a separate contract on the Ethereum blockchain. So the global route has its own contract, and it has only a single node, which contains, in this case, a pointer just to f. The s route is what we would think of as a top level domain. It effectively is. It takes care of registration and so forth.
00:29:37.652 - 00:30:07.632, Speaker C: Both of these have published public APIs for registration, well known rules. Getting a new top level domain in is obviously quite the involved process, because we don't want to overwhelm people with them, unlike Ayana, and one person laughed. That's good. And buying a domain, for instance. And again, we're getting into governance. This would also be published and well known, but hopefully a little less involved. And then, so the f route, for instance, has entries for my site.
00:30:07.632 - 00:31:03.536, Speaker C: So that's if you're starting at the global route, that's mysite F and the Dow. And each of these have their own resolvers. So up in the top left we can see that the person who owns my site has deployed their own resolver to the blockchain. It has three nodes here. It knows how to resolve WWW and forum, and it has a root node, which is what everybody else points to as the authority for my site. And on the right here, we've got the DAO's resolver, which again is very similar, except that while they still have a record that people point to as the authority for the DAO, they also have their own route and they have their own internal nodes. And the point of this is that while most people might point to the global route as the route, much like we all agree on the root servers of the DNS, except a few very strange people, you can also develop your own route, and it can have your own mappings, and it can have the same set or a superset or a subset.
00:31:03.536 - 00:31:43.072, Speaker C: And in this case, the Dow has their own internal route. It points to eth just the same as everyone else. But for internal use, they have a Dow suffix, which only their users can resolve, and that points to some internal sites, for instance. So here we see how you'd resolve, say www dot mysite s. You start at the root, you ask it for f, it points you to the s root, the one node in there. You ask that node for a reference to my site, and you get the root of the mysite, you ask that for a reference to www, and it points you this time to another node inside the same contract. But the local resolver doesn't have to care about that, whether it's the same contract or another.
00:31:43.072 - 00:32:23.260, Speaker C: And finally, your resolver asks that node, what is the IP address of what is your IP address? And it fetches the first a record and it gets back eight. Eight, which would be a terrible address because that's actually Google's DNS. But anyway, I needed something short enough to fit on a slide. So here we see, for instance, how the Dow would resolve it. And the steps are exactly the same, except that we're starting at a different route and we follow the mapping, and the mapping in this case points to the same location. And finally, how the Dow would resolve internal names. The same process is followed, but this time from a path that can only be followed if you start from their route.
00:32:23.260 - 00:33:03.576, Speaker C: So hopefully I've given you a sort of a broad overview of the basic system, of why I think it's necessary and of what it does. And I hope you're convinced that it distributes authority effectively and that it separates concerns so that we can deploy the system, and then we can have the argument about how we're going to govern f and also about how we're going to register names and how long they'll last for, and whether we should have auctions and so on and so forth. So this is written right now. The ENS spec is available at that URL. It's a draft, but it's fairly complete. It's had a lot of feedback. I'm pretty happy with it.
00:33:03.576 - 00:33:43.590, Speaker C: At least there's a reference name server and local resolver implementations available in solidity at that address. GitHub.com arachnids and there's ongoing discussion. If you're interested in this, and I hope some of you are, then join us on Go ethereum name registry scroll back and read some epic arguments about auctions and other byzantine aspects of how we think we should govern the whole thing. Because although I say where concerns are separated, that doesn't stop us arguing about them all at the same time. And I hope this was useful. Thank you.
00:33:43.590 - 00:33:48.680, Speaker C: Please hold your questions.
00:33:48.750 - 00:36:10.100, Speaker B: Despite the slide, if you don't mind, I take it back a little bit. So I hope you understand why the Ethereum name service is hugely important. It's very, very important because that's the only way that mediates between the immutable content of the swarm, where hashes, basically, hash changes if you add a bit of new information to a file, which means that once you viewed something retrieved based on their hash, if someone changes it, the change is not there. So the only way you have permanence of some semantic thing, like for example, my cv is through a name system where you always register the most current version of what you intend to be the content. And also they are extremely important because they will be entry points to certain databases, which will serve as the decentralized layer for Dapps for example, you would store your posts and who likes what in your social network in these swarm databases, ideally, and therefore all these operations on these merkel trees that constitute the description of a key value store, for example a routing table, or as I said, a database. They all can be described using the same kind of structures, and since they have the merker property, their root hash can be registered on the blockchain. But whole states and whole databases can be easily handled off chain and think about them as basically side chains with their own either consensus rules or it's just user data, which where the sole arbiter or the oracle of what the current state of the data is is one user.
00:36:10.100 - 00:38:02.408, Speaker B: So the most important takeaway message that I want to kind of wrap up with is that first we targeted the basic requirement from swarm that it does not only carry over all the properties that we know, like decentralized storage solutions would have basically zero downtime for tolerance and censorship resistance, but also provide some sort of incentive layer which makes sure that it's self sustaining in an economic way over the long term and provides a stable architecture. So that's our research goal. And of course time will tell whether this stands up to the actual real life and whether it works out in the wild. So currently the status of the project is we have almost done releasing the POC, that proof of concept version 0.2. If it's going to be released, then we're going to open up a test network on the Ethereum modern, and probably invite developers or people interested to join our network and try testing it in proper use cases and promise with proper mass usage. There's a lot of work ahead, and we have basically planned work above for five years for ten people at least. There's a lot of interesting projects in the making.
00:38:02.408 - 00:39:45.900, Speaker B: For example, we're going to have payment channels implemented via Raiden. Hopefully that collaboration works out really well. Where Haikoheis is a project, they provide the payment channel implementation and the solidity contract part of basically a lightning payment network, and Ethereum, which scales very well, and hopefully we can have them provide the routing layer for the swap network that it needs for chained payments. Likewise, we would like to have this database implementation running on swarm. That is also the first step towards doing a swarm based lite client, which is based on the idea that the whole of blockchain, the state data and the contract states are also stored in swarm and only accessed on demand, the same way as a light client implementation would do. And there are various other plans in the making. I don't know how much I should talk about them right now, so maybe I should open up the questions rather because that makes it even a bit more dynamic because I can ramble long forever.
00:39:45.900 - 00:41:03.264, Speaker B: Yes, we are basically sister projects that Marshall and for the same common goal and have web3 as a success and bring back the ethos of the original Internet and provide open protocols that allow for frictionless interaction of individuals without necessarily having to have third parties intermediaries in between. And more technical level. IPFS is basically a lego kit of web3. So it defines protocols and abstractions over typical ingredients and components of such decentralized systems, especially related to sotorage and communication. So for example peer selection, routing, chunking, or the way these Merkel dags are interpreted. And most of these protocols are extremely valuable. And we planning to refactor some of the swarm code to comply with some of these standards.
00:41:03.264 - 00:42:15.880, Speaker B: But we don't necessarily want to follow the already existing IPFS modules in some of our plans. For example, in the routing protocol, we had some quite original ideas how to make it more efficient or similarly with the manifest. The manifests are these JSON objects that describe the routing tables or almost like describe basically directory structure in a serverless setting. These manifests are very similar to what IPFS says as Merkel dags and describe these hierarchical relationships between. So we have a slightly different flavor of that than what they're using. But in general the direction of the true projects are very similar and we would like to synergize as much as we can. As far as I know, storage, they just launched last week, there was a launch party in Berlin which I just sadly missed.
00:42:15.880 - 00:43:25.136, Speaker B: As far as I know they're more centralized in their solution and I'm not sure if they're actually seriously taking it as their goal to be a low latency server of interactive web applications because in our scenario that's a very, very important use case that we basically serve the web like interactive real time applications. So we have to cater for low latency use. I'm not sure if it's Torjay's plan or not, but otherwise the systems are very very similar, especially in terms of the way they use ejasure coding and proof of custody audits is somewhat similar. If you go into the tricky details, of course, which you're probably not concerned with, then of course there's a word of difference, but it's only a handful of people care about. So I don't want to comment when.
00:43:25.158 - 00:43:27.040, Speaker A: You say low latency, how low latency?
00:43:30.280 - 00:43:36.580, Speaker B: I would like to be faster than the web now, but it always depends on a lot of circumstances.
00:43:40.040 - 00:44:10.370, Speaker D: Like ipfs supplements to use. So if I have like a DAP, I made a DAP actually last week and at this moment. So contracts in Ethereum and I serve my assets files just in s three bucket. But you could potentially put into the ipfs. And are there any dependency between the swarm and Ethereum itself, or are they completely separated stuff?
00:44:10.980 - 00:45:09.890, Speaker B: Yes. So deep down they both run on the same peer to peer network layer, which is called dev, peer to peer. So in that sense, swarm is the sister protocol of ETH, which is the Ethereum protocol, the protocol in terms of the scheme with which nodes exchange transactions, like broadcast transactions to each other, accept blocks and send blocks to each other, et cetera. That's the ETH protocol, and the biz protocol is the protocol of swarm. That's what the bees speak between each other, that runs on the same multi protocol network layer called death, peer to peer. That's what holds them, kind of binds them together. Actually, this is a very important point, just to emphasize it.
00:45:09.890 - 00:47:03.024, Speaker B: What we really have here is a fantastic opportunity with swarm, and especially in a context where you have a web browser or a mobile client, swarm basically is just a back end to browsing behavior. So when you visit links, you follow from one link to the other. In these cases, the synergy between a decentralized storage solution and the blockchain is really incredible. So we can finally formalize and link certain browsing behaviors to contractual relationships and agreements. So if you're part of, say, an opt in community where you say that each time you visit a media file, let's say emojen heaps song, and like mp3, then you check whether the smart contract that's triggered by you playing that song is verified is according to your principles. It represents the ethos of fair trade music, let's say, and you know exactly that when you click yes to the question of yes, do you want to pay per play for this song? Then you know exactly that your money will be distributed to the rightful owners of that license, to that song. So the smart contract, somehow described in an accurate and transparent way, the content producers wish how they should be renumerated for their work.
00:47:03.024 - 00:48:05.130, Speaker B: And of course, these systems are entirely opt in, and they are not in the context of copyright law enforcement. But the most important point is that the infrastructure that helps check attribution, check provenance, derivative works, licensing identity, all these things are immediately available just because of the synergy of the Ethereum blockchain, and basically content itself. So substance. So logic and substance. Yes, I can repeat the question. Storage, long term insurance goes up. My deposit you don't want.
00:48:05.130 - 00:49:22.196, Speaker B: So you're asking like, yes, so, so you really don't want to lose your deposit. So the question was like whether there's ever a situation that I promised by giving a receipt to store your birth certificate, or like in particular, probably a little chunk of that birth certificate for ten years, but suddenly I'm lured into another arrangement because I got thrown at a lot of popular content, which probably would serve me better, is that what you're saying? In this case, obviously that's why a promise has to be a promise. Promise means that even if there's better circumstances coming to you, you still keep to your original promise. Otherwise you tend to lose your deposit, and that should be a big enough disincentive. So yes, the answer is yes. We have to be very careful with the actual amount so that it will. No, in cases where you, where you want extra value attached to your asset, you want to insure it.
00:49:22.196 - 00:50:20.132, Speaker B: So to say above this incentive that you get by the nose, potentially losing that deposit, then there are like explicit insurance schemes for that. These will probably be third party roles that take on the role of an insurer for your chunk, and they would probably launch regular integrity audits. They would check whether your files are really available and would only pay you kind of in installments. These kind of systems are very nicely, easily programmable in Ethereum and these smart contracts. You can also ask questions from Nick, I think if you have some one.
00:50:20.186 - 00:50:48.636, Speaker E: For you, one last one for you. So you have custodian nodes, and it's kind of like a sloppy distributed hash. Table. What are the guarantees inside the network? If a certain percentage of the nodes go down, for example, those nodes made promises, then do you have a method to offload the files that only those nodes, like only a few set of nodes now have to the rest of the network, such that you still have assurances and guarantees.
00:50:48.828 - 00:52:36.190, Speaker B: Yes, we have to assure them against these situations with some redundant coding. So certain content has to be retrievable, even if parts of that are missing, especially even if a local area in the network is completely cut off, we still have to be able to retrieve the content. And the standard solution for this is using erasure coding, which is just a very basic idea that if you have error correcting code, so basically you inflate the coding of your data to a bigger data blob, which has the properties that any particular proportion of that data that you see is enough to reconstruct your original content. And if you distribute it in a redundant enough way, or this extra extended content, then you can make sure that your data is recoverable. Just also to say that, please close to the team, because obviously I have to mention that it's not only my work, it's a big team behind it. So it's Daniel, it's Aaron, it's Nick and Joette. So a lot of people from the Ethereum foundation and the halo of Ethersphere, so thank you very much for them as well.
