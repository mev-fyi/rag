00:00:00.440 - 00:01:44.575, Speaker A: SA and we should be live. Cool, let's see what's the wrong link. Let me grab the agenda. Okay, yeah everyone, so this is consensus layer call 144 and yeah, I think people have kind of been pretty heads down with the pector Devnets. That being said, there's quite a bit on the agenda today, so yeah, let's go ahead and just dive right in. First we'll start up with Electra and I'm not sure if there's anything to speak to around DevNet3. I think we killed it this week but if there are any comments to close us out there, this would be a good time for that.
00:01:47.675 - 00:01:57.745, Speaker B: I was actually just waiting for ACDC just to confirm that I can shut it off by the end of today. Unless there is a new position, I will proceed with it.
00:02:01.005 - 00:02:01.785, Speaker A: Anyone.
00:02:09.765 - 00:02:19.865, Speaker B: On Devnet 3? We still found a proposal issue with Grandine. I'm not sure if they want to still debug that or whether they just want to focus on Devnet 4.
00:02:22.895 - 00:02:42.880, Speaker A: That's a good question. Is anyone from Grandin on the call? Okay, yeah, Barnabas, I would just say use your judgment. If you can't get in touch with them by the end of the day, I think it's fine to move ahead and close out Devnet 3.
00:02:43.053 - 00:02:46.337, Speaker B: Sounds good. Cool, okay.
00:02:46.510 - 00:03:05.525, Speaker A: Otherwise then Devnet 4, we've all been very busy getting this ready. I think I even saw a message on Discord earlier. A good number of clients I think are ready to go, but I don't think we've launched the Devnet quite yet. Anyone have any more information on the status of DevNet4?
00:03:06.705 - 00:03:54.095, Speaker B: Yeah, I just saw that Silus just joined in so maybe we can circle back for 1 second to Devnet 3. So Silius, would you be okay if I shut off devnstree by the end of today or are you guys still debugging something regarding the non proposals box? Sorry, I just joined so I think. I hope that we'll be ready. I think, I mean we pass all the tests and there could be some integrational stuff that we didn't test in cortosis so. So yeah, if we are lucky we will be ready today. Otherwise tomorrow. No, the question is whether we want to keep Devnet3 around.
00:03:54.095 - 00:04:22.213, Speaker B: Are you still actively debugging on it? Oh, sorry, I don't know. I think if you. What was the outcome when you nuked the server? Was it running okay, we can get back to that. Okay, yeah, let's do offline. Yeah, but it's still not proposing. That's the conclusion. Okay, okay, we will check then.
00:04:22.213 - 00:04:55.395, Speaker B: Okay. Okay. Regarding DevNet 4, I also have an update for that. So we have two execution layer clients working Geth and Ethereum js we have a branch for nethermind but there seems to be some formatting bugs. And for the CLSI we have Lighthouse, techoprism and Nimbus working. And Lodestar is able to attest but cannot propose and Grandine cannot propose.
00:04:58.175 - 00:05:03.047, Speaker A: Okay, awesome. So we've launched the DevNet or this is just.
00:05:03.191 - 00:05:16.065, Speaker B: No, this is just local kurtosis test. Gotcha. We can. So I would like at least one more EL to come online and then we can probably launch it tomorrow possibly.
00:05:17.765 - 00:05:19.865, Speaker A: Okay, yeah, that works for me.
00:05:21.685 - 00:05:22.325, Speaker C: Cool.
00:05:22.445 - 00:05:34.273, Speaker A: Any els on the call who aren't quite ready, but maybe you could speak to readiness by tomorrow. R is not quite ready but I'll.
00:05:34.329 - 00:05:36.273, Speaker C: Sprint on getting it ready today so.
00:05:36.289 - 00:06:08.915, Speaker A: We can launch tomorrow. Cool. Yeah, I think that would be nice just to have a week or 2 of Devnet 4. We did also want to have like a public test. Net based on Devnet 4 for Devcon so the deadline's approaching. Cool. But yeah, it sounds like there's been a lot of great progress on Devnet 4 with the various implementations, so that's really cool to see and yeah, everyone just keep at it.
00:06:08.915 - 00:06:23.607, Speaker A: Anything else on DevNet 4? Yeah, just to reiterate, it sounds like we'll aim for a launch tomorrow pending. Yeah, another one or two els who can join. Any other.
00:06:23.671 - 00:06:30.265, Speaker B: Yeah, and we can always just add them later on as well. Just make some deposits and include them.
00:06:30.925 - 00:07:22.645, Speaker A: Definitely. Cool. Any other comments on Devnet 4? Okay, if not, then we can move on to a number of other things. So at this point, yeah, I effectively want to think about, you know, how do we get to a feature complete pectra so you know, we can start thinking about mainnet down the line. There are a number of open issues that we still have. You know, every call there are fewer and fewer, which is also good to see. One of them Here is this PR 3900 and this was a refactor to some of the attestation work.
00:07:22.645 - 00:08:11.179, Speaker A: We had discussed this last call and basically said, you know, there are some like nice security benefits here. It's also possible that implementation wise, because of how core the attestation type is, this could be a lot of code change for clients. So we're going to give it another cycle to assess that a little bit better and then make a call on this change. So yeah, I mean there's definitely grounds to have this as an update for EIP 7 549. I always get that mixed up with peer DOS, but yeah. So I guess any other CL teams here have. Have you had time to look at this or think about this? I think the PR is pretty much ready to merge.
00:08:11.179 - 00:08:15.055, Speaker A: We just need to agree that we want to move ahead with it. Yeah. Enrico.
00:08:16.515 - 00:09:17.655, Speaker D: Yeah, I think like last. Last message from Dapp lion in. In the attestation PR is kind of a good point to me and considering that we had our spike implementation, it kind of says to us that is doable, a quick implementation of that. That doesn't really impact anything. It's not very nice currently, but we could go in that direction for sure. But essentially what that plan says is that the DoS attack that is supposed to be blocked is not. Is a problem that remains there anyway, which is kind of weird and to me is something like this is an argument that convinced me.
00:09:17.655 - 00:09:31.065, Speaker D: I don't know if anyone has thought about it and if this is true, probably makes the entire change worthless. You think, guys.
00:09:33.845 - 00:09:54.275, Speaker A: Right. Yeah. Has anyone had time to take a look? I would need to go review a little bit more myself, but I hear what you're saying. I don't think Yossik's on the call and he was pushing for this. Any other seals have any input here at the moment?
00:09:57.815 - 00:10:12.075, Speaker E: Well, basically I would say that like the general lighthouse buy was. Yeah, we didn't see the benefit of the change, so probably not worth doing for the reasons Dapline brought up.
00:10:19.945 - 00:10:21.165, Speaker A: Okay, thanks.
00:10:22.385 - 00:10:23.125, Speaker E: So.
00:10:23.545 - 00:10:47.645, Speaker A: Right. Okay. I think then I'll try to circle back with Y offline and get a better sense of. Yeah. What he thinks in light of this recent comment and yeah, otherwise it does not sound like there's a ton of support to move forward and if anything it's one less thing for us to do so we'll get to Pectora sooner rather than later.
00:10:49.945 - 00:10:50.433, Speaker B: Okay.
00:10:50.489 - 00:11:28.497, Speaker A: Yeah. Thanks everyone. If there's no other comments there, we can move to the next agenda item. This one was around a number of P2P changes that I think kind of came out some of the discussions from Interop earlier this year. I think a nice way to think about this is to go ahead and roll out this PR with Pectra itself. I guess here I'm just calling it out for everyone to take a look. I don't know where we are here.
00:11:28.497 - 00:12:13.215, Speaker A: There's a lot of conversation, but yeah. Have any CL teams had a chance to look at this PR feel strongly one way or another about merging it. From what I recall, there was pretty strong support for this direction at Interop. This PR would be, I think, a pretty like, maybe not invasive, but somewhat substantial change to the networking layer, which I think would then would apply, you know, a good bit of testing, which again, I. My understanding is that the VR justifies the cost, but it's just something to think about.
00:12:19.595 - 00:13:09.575, Speaker D: From us. We discussed that internally and things like this is. Yeah, there will be several, several changes to go for and seems like a little bit late to target backdrop for that. Could be better to target the next one, but this was to make things mostly stable and don't add additional things to do for the next fork. It's been there for a while and we were trying to keep Electra stable enough and having that merged now seems like big change.
00:13:11.995 - 00:14:02.685, Speaker A: Right. Okay, this one was also coming from Jacek, so yeah, unfortunately he's not here, but yeah, we got a thumbs up from Terrence as well. Okay, I'll try to reach out to him as well about this and get a better sense of how much he wants to push for this and on what timeline. Cool. Then next up we have a bug fix to some of the max EB things. This is PR3979 and I think here the ask is just to take a look. It does look like there was another issue with withdrawals handling.
00:14:02.685 - 00:14:18.125, Speaker A: And yeah, as far as I know, this is a straightforward bug fix and it should be merged. So yeah, client teams, just take a look if you have a moment. Oh, Mikael's here. I don't know if you want to add anything else.
00:14:20.685 - 00:14:29.585, Speaker E: Yeah, thanks Alex, but yeah, I don't think it's quite straightforward, as you mentioned, so just take a look please, for the proposed fix.
00:14:33.925 - 00:15:55.245, Speaker A: Cool, thanks and nice find. I think we definitely need a round of testing and review around the specs and one way to help that is to get to a stable PETRA spec. So again, I think we're migrating to that regime and hopefully there are cycles over the next month or two to really flush out any bugs. Okay, next up, we have an issue or at least a proposal to think about as a Z support for the builder APIs. Here, let me grab a link to at least this doctor here. This came out of some of the request handling that we were talking about with these execution layer requests going from the EL to the cl and there was a lot of discussion around how do you structure these? Like, is there a JSON encoding, is there an SSZ encoding? And in any case the same Question also comes up with the builder APIs, because then if you're a proposer going to talk to say some nav builder, they have to communicate these to you as well. And the builder APIs right now go with essentially this JSON encoding that deviates a bit from the engine API.
00:15:55.245 - 00:16:44.159, Speaker A: And this point was raised as a way to kind of at least have the option for both, kind of get the best of both worlds via direct SSD support. The question here for the CL teams at least is that you would need to support this in your client to talk to medboost and from there there's other infrastructural things that we need to change. But I guess the first point there is these other players like Relays and builders generally also support this. It's lower latency for them, which they like. And yeah, so it would make a lot of sense. The question then is though, you know, the question then is that you need CL buy in as well. So the proposal concretely would be to go ahead and say this is like a, you know, sort of official part of Pectra.
00:16:44.159 - 00:17:05.565, Speaker A: You would expect to have your client have support for this with the Pectra hard fork. And yeah, I think that's the context there. Anyone disagree or. I feel like that's unreasonable to aim for? Dustin asked. Oh, someone was going to talk.
00:17:05.945 - 00:17:29.135, Speaker E: That was me. I was gonna ask, I was gonna ask. It looks like the PR is meant to be backwards compatible, so are we expecting like relays to support like this in a backwards compatible way? Which would mean like this would actually be, I guess not mandatory for clients. Not that I don't think we can or will implement it, but.
00:17:30.995 - 00:17:49.215, Speaker A: Right. So my understanding is that you set essentially like a header in the request that just signals you can work with this transport and then you go from there. And yeah, the idea is to have the sort of the existing again this like JSON encoding, then also just the option for sse.
00:17:52.495 - 00:18:29.355, Speaker E: Okay, gotcha. Yeah, Also I think hectora would be a good timing to address this or to test this as well. And because I think this was proposed two years ago already and even though it's backwards compatible, I think having a stricter timeline maybe pushes it a bit. And yeah, CLS don't even need to implement this, but of course if you need want to profit from the latency gains then because we only need to implement the client side. So it's mostly important that Relays and Sidecar software implements this.
00:18:38.175 - 00:19:06.925, Speaker A: Right. So it sounds like there's general agreement that this is Interesting. One thing we'll definitely want is support of this and mebust. So yeah, that's something I can work on with the relevant parties and then separately, yeah, CL teams, just keep this on your radar. I don't think it's a huge lift implementation wise. And yeah, we can go ahead and also get the PR merged. That's a good first step.
00:19:06.925 - 00:19:21.695, Speaker A: So have that into the builder specs and then, yeah, from there let's aim to have support for this along with Pectra, and that can be part of the Pector testing as we move along to mainnet. Does that sound good to everyone?
00:19:25.475 - 00:20:08.055, Speaker E: There was one more question also we discussed on Discord, which was basically who will profit from this latency gain? Because I mean, builder could just eat it up with playing timing games, for example, and then in the end there's no real latency gain. But I think since this is also. It's in the response of the get header request and also in the when you submit the blinded block. And the main latency benefit would be the second part when you submit the block. So. And since we have a strict timeout on the header implemented in clients, I think timing games would not be a profit from this extra latency gain.
00:20:13.795 - 00:20:47.215, Speaker A: Right. I mean, also with SSE encoding you use less bandwidth, which will be nice in any case. And it also does harmonize with where we might ultimately want the engine API to go, which would also have an SSE transport. So I think it makes sense. Even putting aside that question. Okay, I can move ahead the spec things there. And yeah, let's just keep this on our radars for Petro.
00:20:47.215 - 00:21:59.225, Speaker A: Okay, next up, I think we just generally have like a sort of notice, especially for application developers who happen to be listening. So we have this concept of generalized indice, which basically correlates to. With the SSZ again, serialization scheme that we use on the cl, there's a corresponding merkleization scheme. There's a way to identify nodes in this Merkle tree for each type that we have on the cl and it facilitates light clients and you know, anything that looks like a light client for current purposes, it looks like the beaconstate then will cross this like key number of like a power of two, in this case 32, which basically means that the current leaf structure gets pushed down a layer and so these like fixed numbers all change. The concern here is that if you're writing an application against these things, there's no way at the moment to be abstract to them. And so you need to also update, say Your smart contract. So yeah, I think the ask here is just to flag this again.
00:21:59.225 - 00:22:33.055, Speaker A: If you're using things like say 4,788 which is the parent beacon block route EIP that we've had or yeah different things like this. If you reason about these generalized indices in some other way in your smart contract right now with Pectra there will be a break and change. So presumably this makes sense to you. If it doesn't, you probably don't need to worry about it but please be aware and yeah Mikhail, you had the comments. I don't know if there's anything else you wanted to add to that.
00:22:33.795 - 00:23:42.295, Speaker E: Yeah, I just want to add that etan proposed potential solution to not cross this mark this time we could group some new fields into. Yeah into some logical groups based on more on request stuff like withdrawals, consolidations and deposits. So there is going to be three groups and we just put every new introduced field in one of those buckets. But yeah, from my perspective it's kind of like artificial. More artificial but that's one of the potential solutions. So not introducing this breaking change right now and maybe in the future once we have a stable container we'll break this thing just once and we'll not have to deal with it after those containers are introduced. So yeah, that's one of the workaround for this problem just to use kind of logical grouping of the new fields.
00:23:42.295 - 00:23:56.515, Speaker E: But I'm kind of like personally inclined not doing anything unless there is a strong reason for that. Like something is completely broken. So that's just a comment I wanted to share.
00:23:57.455 - 00:24:27.455, Speaker A: Yeah, no that makes sense. And yeah that's a good option. If it was like a huge issue. I guess it's worth reaching out to some of the like, for example users of 4,788 and ran out by them. But otherwise as yeah it's probably good just to have this be known and move ahead as is because yeah we could think about restructuring the beacon states but then there'll just be some more code thrash which is not ideal.
00:24:28.355 - 00:24:59.915, Speaker E: Yeah and basically yeah even even though we are relying on stable containers introduced in the future, this is going to be a break and change as far as I understand. So these four 788 Beacon Block routes. Yeah like the applications using them will have to be updated anyway at some point in time whether we have stable containers or whether we're just increasing the number of fields in the beacon state in the future. So in any possible way it will be. It will happen.
00:25:00.455 - 00:25:58.085, Speaker A: Right. Okay. So next up, I suppose I just wanted to give a brief update on this breakout on EIP 2537. This is entirely an EL concern, but there was a breakout call that we had and we kind of came back to this core question of how to structure the APIs to the precompiles. And there's a bit of a trade off here between user safety and essentially cost or performance with how we think about some of these features. Like if you want to get into the details, there's a notion of a subgroup check that is very important for the cryptography. But yeah, then the question is just like the right way to structure these essentially the inputs to each precompile.
00:25:58.085 - 00:26:38.065, Speaker A: So I think I'll just flag that for now. Again, if you're listening, and especially if you expect to be a user of these features on mainnet, say for example, you want to verify zero knowledge proofs that use this curve on the chain. Also the use case of validators and signatures. So this EIP does let you reason about validator signatures trustlessly in evm, which is really cool. And that all being said. Yeah, please reach out if you think this concerns you. We would love input to help answer the question around the right way to structure the some of these details of the eip.
00:26:38.065 - 00:27:29.307, Speaker A: Cool. Okay, that was everything on the agenda for Electra. Any other comments about that for the moment? Otherwise we have pure DOS next up. Okay then, yeah, the blobs, our favorite Ethereum feature. So I think to get started. Well, okay, even before that, I'm assuming there aren't any substantial updates with the Parados dev nets from what I've seen. So I believe there was a breakout this week.
00:27:29.307 - 00:27:39.815, Speaker A: And yeah, I think clients have been working through maybe some syncing issues there, but otherwise I'm not. I don't think there are any significant updates on the. On the Devnet status.
00:27:42.275 - 00:27:43.095, Speaker B: Nothing.
00:27:43.995 - 00:28:31.175, Speaker A: Cool, thank you. So, that being said, there are things we can think about in parallel. One of them is this PR which pairs actually with this engine git blobs v1 method that has recently been merged and there's a question around clarifying sort of the P2P spec here for an honest node. Last call. We discussed this and essentially we got to the point that, yeah, this PR makes sense. I think the key question here being like if you get a blob in this way, say from your local mempool, should you then treat it like you've received it from gossip from a peer, meaning that you then gossip to other nodes? So yeah, I think at this point everyone's in agreement on this being the right call and so we'll go ahead and merge this. I'll bring it up now just to serve as a final call.
00:28:31.175 - 00:29:15.195, Speaker A: Any comments or questions on this? Okay, then I think we will look to merge this even later today. Cool. And then I guess related to this, I was curious if there are implementation updates around this feature. We can think about ways to structure this where it does provide significant family savings to nodes. I know last time clients had been looking at this engine Giblobs V1 and there is a variety of different states of progress. Any clients care to give an update on this now? Yeah, Terrence.
00:29:15.865 - 00:29:26.297, Speaker F: Yeah, so we had our implementation since last week and then we have been testing it on the wild and I wrote a report yesterday.
00:29:26.401 - 00:29:26.857, Speaker C: Let me see.
00:29:26.881 - 00:30:23.873, Speaker F: I can drop into the link, but the TLDR is that like the heat rate surprised me. To me it's really well, so I was able to recover like one blob sidecar per 12 seconds over four days. And that's pretty good if you really think about it because like say today that the target is three. That's kind of assumed that you can get like 33% of the blob cycles just over the main pool. And then because of that the import time of the block turns out to be much faster because typically my node waits about like 100 to 500 milliseconds waiting for the block and now that time is just basically gone because I can import the block faster without waiting for the blob cycle over the while. And then so that's one benefit I did see that I have received less blobs like P2Pmessage over the gossip stuff. And I think that's pretty good as well.
00:30:23.873 - 00:30:31.673, Speaker F: But I cannot, I couldn't quantify how much like bits per second or megabits per second that saving is.
00:30:31.729 - 00:30:32.161, Speaker A: Right.
00:30:32.273 - 00:31:11.725, Speaker F: And then the last thing I observe is that because of that I am the fastest node on the network in a way that I receive the blob the fastest. It turns out to be sending more blobs out, right? Because now it's kind of the trade off because I am on the other end of the spectrum, that I am in a way receiving Blob the fastest. Therefore I reduce my download bandwidth but in parallel since that I want to serve nodes because of that it tends out to be using more update. I think that will change when more nodes have this feature and then yeah, that's basically my update.
00:31:13.585 - 00:31:22.765, Speaker A: Cool, thanks. Is there some way to think about throttling Your use of the uplink, assuming you get a blob from the mempool in this way.
00:31:23.225 - 00:31:31.713, Speaker F: Yeah, Lighthouse has a write up which I haven't reviewed, so they're thinking about the similar concept from the peer does perspective. So it does make sense to kind.
00:31:31.729 - 00:31:33.685, Speaker C: Of play the trade off fast.
00:31:34.105 - 00:31:45.305, Speaker F: To kind of play the trade off in a way that if today you're the fastest node, you kind of want to throttle your off upload. So you can wait if you wait a little bit before you broadcast the blobs.
00:31:48.485 - 00:31:49.585, Speaker A: Enrico.
00:31:51.125 - 00:33:05.861, Speaker D: Yeah, we implemented it and we also released it and is currently working on mainnet with Bezu Correctly we got problems with nethermind and yeah, waiting for other EL clients to release and test. Currently the tech implementation due to what Terence said is not using it immediately when you see the blocks. So we wait at least half a second before doing this just to avoid. Yeah, to be the super fastest node, the faster node that then has to be has to publish because actually we. Yeah, essentially this waiting delay is happening on the attempt to try to recover instead of recovering and then wait before publishing. So yeah, this was the first since we were pretty one of the first client implementing it. So we decided to go first in this direction.
00:33:05.861 - 00:33:43.715, Speaker D: And by the way, we saw a lot of nice recovering. So we are systematically grabbing blobs from the el and this reduced all the attempts that Taku tried in the previous implementation through recovering via rpc. So we were historically pretty aggressive in trying to recover late blobs and now though this thing went down a lot because we try EL first. So far, so good for.
00:33:46.775 - 00:33:47.199, Speaker C: Cool.
00:33:47.247 - 00:34:07.485, Speaker A: Yeah, that's really exciting to hear. Okay, anything else on that sounds like yeah, progress is underway I guess. Again, I don't know if There are any ELs on the call, but yeah, the more ELs that have this implemented, the better.
00:34:09.665 - 00:34:30.088, Speaker E: I was curious Terence, if you know how many of the blobs are in that period were expected to be seen over the public bim pool? Because 30% seems lower than I would have guessed because I had the impression that 80 or 90% of blobs were coming over the public bin pool.
00:34:30.256 - 00:34:31.000, Speaker B: Oh, oh yeah.
00:34:31.032 - 00:34:40.192, Speaker F: But then the problem here is that the relayer and the builder, they purposely broadcast the blob sidecar before they broadcast the block.
00:34:40.248 - 00:34:40.512, Speaker A: Right.
00:34:40.568 - 00:34:58.051, Speaker F: So because of that I'm sure the number would have been higher if today they do the reverse. But since that I am receiving blob cycle first before I receive the block most of the time it kind of. Yeah, it kind of messed up with the number here.
00:34:58.243 - 00:35:03.331, Speaker D: Yeah. Essentially he's not even trying to get it from DL because you got the blobs first. Right.
00:35:03.403 - 00:35:05.375, Speaker E: Okay, I see. Yeah.
00:35:08.275 - 00:35:12.095, Speaker A: Doesn't that kind of defeat the point of this being a balance saving thing though?
00:35:14.075 - 00:35:19.467, Speaker E: Well, it does not if the stat is meaningfully different between relay blocks and homebuilder blocks.
00:35:19.491 - 00:35:19.603, Speaker A: Right.
00:35:19.619 - 00:35:32.243, Speaker E: Because home builders do not do this and we primarily want to help with the peak bandwidth for their blocks. Right. So it would actually be really interesting to see that stat broken out between the two types of.
00:35:32.419 - 00:35:55.465, Speaker D: And what is important is the local building here. So when the relayer is involved it doesn't really matter because anyway they will. They will gossip everything by themselves. But when the proposal builds locally, this is where things get interesting and the local recover really matters.
00:35:57.845 - 00:36:08.785, Speaker A: All right. Terrence, did you have a chance to look at this in the write up? You had the breakdown between relay blocks versus local blocks?
00:36:09.695 - 00:36:12.355, Speaker F: I do not. So I will follow up on that.
00:36:14.375 - 00:36:28.075, Speaker E: This feels a little weird because you know, the nodes have already done this work to get the blobs over the transaction pool and just because the relays are sending out first, we're not able to take advantage of that work.
00:36:32.455 - 00:36:43.643, Speaker F: I also suspect that will mostly likely be the case because of the timing game. So the relayer, their incentive is basically to gossip the block as late as possible to the into the four second mark.
00:36:43.699 - 00:36:44.107, Speaker A: Right.
00:36:44.211 - 00:36:53.675, Speaker F: So today if we ask them to maybe send a blob after then they would just presumably just censor the blob. I presume so.
00:36:53.795 - 00:36:54.291, Speaker C: I don't.
00:36:54.363 - 00:37:03.429, Speaker F: Yeah, I think that's kind of. Yeah, I, I think that's kind of. I, I think, I think that's a hard one to be fair.
00:37:03.557 - 00:37:34.885, Speaker D: What I think it will happen is that the relayer even start not sending anything over the P2P in terms of blobs, just sending the block when the majority of the network just is able to recover from the local yale while sending the blobs and the cls will anyway send. Will anyway recover and send over the P2P the blobs so they will do their work.
00:37:38.145 - 00:37:48.245, Speaker E: Is there a way to receive a like blob sidecar header and then before requesting the full blobs from the clptp ask the el.
00:37:54.795 - 00:38:40.005, Speaker A: There isn't today. I mean we could think about that but it also then is going to complicate things because you like the delivery becomes like partial. Right. Like you could have this header state or you could have that or plus payload state meaning the blob side bar. Right. Well, in any case, I think this was really helpful. And yeah, if we could keep drilling into these different metrics of how this actually works live on mainnet, that would be super helpful to get a better understanding.
00:38:40.005 - 00:39:36.635, Speaker A: And yeah, maybe there's something downstream like client was suggesting to deliver these bandwidth savings with this feature. So one point regarding that, I guess I'll. I don't have a strong view on it yet, I guess. But right now it's explicitly marked as optional. And I understand this. I think it makes sense as introduced in this PR to be optional because it had not been explored the way people are exploring it now. But if we continue to pursue a certain direction that is being looked at regarding increasing blob space with, and here's the crucial part with the reliance, the key assumption of it is okay precisely because of get blobs, and it would not be okay without get blobs, then I would suggest that the optionality of that be reassessed.
00:39:36.635 - 00:39:43.205, Speaker A: Makes sense. Zes.
00:39:44.465 - 00:40:42.285, Speaker B: Yeah, so I'm interested in kind of implementation detail here where when we receive blops from P2P, this is like event driven, so whenever we receive we just get it. And for EL we just mostly polish. So let's think that this optimization will be implemented. So the current thinking is that we just poll EL in order to get the blob that EL has whenever we see the block, for example, or there are discussions that there may be some other way to fetch the blobs from el.
00:40:45.665 - 00:41:12.965, Speaker A: No, I believe it's just the first path. So I mean one way to think about this is it's like another availability sort of avenue for the node. So you get a block and there's some blobs attached. You need to know that the blobs exist, they're available and gossip is one way, but the mempool is another. The idea being that at least currently most blobs are in the public minpool. At least that's sort of our working assumption. The data might suggest otherwise.
00:41:12.965 - 00:41:26.165, Speaker A: In any case. The idea then is that you don't really need to wait for gossip from a peer. You can just go to your local mempool, because in some sense you just already gossiped this at the EL layer, so the CL doesn't need to wait.
00:41:26.755 - 00:41:52.051, Speaker B: Yeah, but so according to the data, it looks like if CL block arrived then I just ask EL once and I should get the blobs. So there is no some, no need for some extra, more complicated, more even based approach here, Is it correct?
00:41:52.203 - 00:42:29.345, Speaker A: Well, the issue that just Came up was an airplay between this feature and the relays and like the external pipeline. Because basically, I mean, it sounds like what was happening is that the relays send out the blobs before you even really think about those, just due to the way like the blocks and blobs are disseminated from the relay. So, yeah, to the extent that we also want this, like, to the extent we think this will be savings for bandwidth for all nodes, even if you're talking to relays, then we'd need to think about some refinement of the mechanism to actually let us get that, because we can't get it right now.
00:42:30.325 - 00:42:30.957, Speaker E: Okay.
00:42:31.061 - 00:42:32.065, Speaker B: Okay, thanks.
00:42:37.405 - 00:43:04.885, Speaker A: And yeah, maybe just to close out the point, like, we were probably mostly concerned about nodes not using the relays. So, you know, I don't think it's necessarily a blocker, but it is an interesting interaction between these two different parts of. Of mainnet. Cool. Yeah, thanks everyone. That was, I thought, really helpful discussion. Next up.
00:43:04.885 - 00:43:29.367, Speaker A: Yeah, this one might be a pretty straightforward. Yes. No, the question is just rather to rebase pure DOS onto Electra. This is certainly the intent. And then it's just now a question of timing. I don't know if this came up on the breakout this week, but I believe this PR is ready to go. So it's just kind of coordinating with client teams and.
00:43:29.367 - 00:43:52.135, Speaker A: Yeah, making sure that everyone's okay with moving ahead with this and the specs. Dustin, I'll assume you're answering yes to this question. Any other client teams want to hold off? Okay, we got to do it. That's good. Doing it. Okay, cool. Anyone not ready?
00:43:54.035 - 00:44:18.545, Speaker B: Anyone working on peer death ready? I think this is going to delay the next by potentially a couple of weeks, two to three weeks, even.
00:44:19.365 - 00:44:21.105, Speaker A: The next paradox DevNet.
00:44:21.725 - 00:44:25.305, Speaker B: Yeah, if we're going to be requiring this.
00:44:28.685 - 00:44:32.385, Speaker A: Was anyone at the breakout call? The Parados breakout call this week?
00:44:34.845 - 00:45:00.975, Speaker B: Yeah, the general consensus was that it's better to do it sooner than later. So everybody seemed to agree with that. So that just the two branches don't get too far apart from each other. But yeah, the major issue is I think that we don't have a stable peer desk network right now. So just going to complicate things a bit more for Peer Desk going forward.
00:45:02.995 - 00:45:59.935, Speaker A: Right. I mean, we will have to do this at some point and so the question is just like, yeah, do we take the pain now or later? I would lean towards merging it. Okay, how about this? I can try to reach out to different CL teams and in particular the people working on Peer dos on those teams just to do like a final check the next couple days. But yeah, otherwise if not then I would expect to see this merged definitely before the next AC dc. Cool. Let's see what was next. Okay, so yeah, to round out the blob segment of today's call, I believe Francis was here and even wanted to give a presentation on some of his analysis.
00:45:59.935 - 00:46:02.155, Speaker A: Are you here Francis?
00:46:02.735 - 00:46:45.005, Speaker C: Yes. Let me share my screen. All right. Okay, so more blobs. I believe everybody, most of us on this call have already reviewed this doc, so I'll try to be quick and just reiterate down some of the important points I guess. So the analysis is broken into like I guess three parts. Firstly like why we want to like increase the blobs and secondly like what in the what's the current problems? And then thirdly like what's the kind of like solutions to alleviate the concerns for those problems.
00:46:45.005 - 00:47:43.511, Speaker C: So the first section is like why the urgency? So I these two pictures, I just took them from last night and you can see that the blobs are trending towards the target, the three blob target already. So that comes to the first point. Like the existing consumers are like continue to scale and base on open mainnet have plans to increase their sale and increase in like significant increases in blob usage. For example base has increased started to increase like the gas per second from like late September and it has increased from 10 mega gas per second to 40 megas per second as of yesterday. So a lot of those changes are kind of like eaten up by the base scaling as well. The second point is that there are new chains trying to join and use blob space. For example Unichain like recently just announced and they are aiming to launch with blobs as DA later this year for their mainnet.
00:47:43.511 - 00:48:36.475, Speaker C: So that's kind of another new blob like requirement. The third part will be the loss opportunities for other L tools. If they see that okay, it's already our target and the DA is going to be super expensive then then they are probably going to reevaluate it and maybe go with RDA solutions or some other things. So I guess too quote metallic we cannot afford to let the momentum slip. I'm moving more L2s over to using blobs. So what are the concerns today? There are very legit concerns about different aspects of the network and I think from what I gather there are two main concerns. The first one is that solo stakers, their proposed blogs can be like reorg due to their low upload bandwidth because they cannot gossip out their blobs and blogs like in time for them to be attested.
00:48:36.475 - 00:49:38.335, Speaker C: So that's like the first concern. The second concern is that if we increase the target number, that could potentially impact the network like syncing performance. Especially for the unhappy case where the network does not finalize and everybody needs to catch up with syncing the historical blocks, but also have to keep up to tip of the chain. So there might be some added bandwidth there that might push things over the fence. So what we are trying to do is to take a look at this situation in terms of block bandwidth, in terms of bandwidth from a first principle like our perspective and try to really see, okay, in the worst case scenario, what are the upload bandwidth needed to be able to foot like fulfill the network requirements. So before that I'd like to reiterate these things. Some of the things that we are not talking about to just focus us on the like same thing, the same problems.
00:49:38.335 - 00:50:20.537, Speaker C: So first we are not going to focus about like block based fees. We believe that it's important here, but they are kind of like orthogonal to the analysis. And with the current flop market close to being saturated, we believe that the entire kind of like fee discovery will happen very, very soon. A second part is that we will not talk about like solo stakers who have me being able to. So this is specifically for the solo stakers who are trying to propose in that situation. So there, I think for this situation there is broad community consensus that with me being wayward, the burden of distributing blocks and blobs is offloaded to the like relayers. So not the solo status.
00:50:20.537 - 00:51:16.309, Speaker C: And we're not going to talk about any wait, timeout and fallback to local building situation because it should happen very, very rarely. And the assumption is that download bandwidth here is not the bottleneck. At least we didn't hear anybody complaining about they cannot download things like in time, et cetera. And in most places download bandwidth is much higher than upload bandwidth. Okay, so this is like the things that we are not going to talk about and the things we are going to focus on is like a solo staker that does local building and tries to propose their blocks to the entire network. The true bottleneck here is basically I think the number five step where the after it's kind of like the proposer, the solo staker like builds its own block. It needs to like gossip out both the block and blobs over the P2P layer on the consensus layer side.
00:51:16.309 - 00:52:04.853, Speaker C: And it has to happen within four seconds of the slot to get majority attestation. And theoretically this should be a little bit faster so that it can propagate or disseminate properties throughout the network. So with that we have done some first principle analysis. We using very conservative numbers and some of them we use the empirical data that we observed from the network and also using the worst case scenario data first load to illustrate the need. So for example, for stations, the worst case upload bandwidth we've observed is 300 kilobytes per second as shown. Sorry, yes, I can. Okay, never mind.
00:52:04.853 - 00:53:09.523, Speaker C: I think it's here like 300 like kilobytes per second for the upload bandwidth. And there are other points that I'm not going to go through point by point just in case of time. But one thing I want to call is that for the calculation we are using 2 seconds of the window of transmission for the CL layers to broadcast all the things out blocks and blobs. So some people have mentioned that maybe 2 seconds is not enough. But the general sense of this calculation is that to showcase, okay, what are the kind of approximate network bandwidth we need with different blob numbers and then we can lead to the optimizations we have and see what are the benefits of those optimizations. So we don't need to actually look at the numbers here, we just need to know that, okay, for different blobs, like the more blobs you have, the more like upload bandwidth that is required. So what can we do here to basically make it safe to increase the blob target and blob number.
00:53:09.523 - 00:55:02.445, Speaker C: And I believe we've discussed extensively about the engine like get blobs v1 that is already showing like very promising results from both the Lighthouse design on ZP address Devnet and also from Terence who just did the analysis from his own nodes on the network, which showcases that the block import time is much faster and there are benefits of the reductions on download bandwidth and for the upload bandwidth. I think we will continue to monitor the network and once more and more nodes with this new kind of changes being like online, then we should hopefully see that the upload value will like decrease as well. The point here with the engine get blob 3.1 is that with that the CL layers doesn't have to broadcast the blobs out during that stringent time window, thus relieving the like peak bandwidth, peak upload bandwidth they need. So if we might use that and deduct all the CL needs to gossip out the blobs, we can see that with five blobs target and a blobs max the actual peak upload bandwidth need is even lower than three blobs although you still need to gossip out those blobs. But it doesn't have to be that quickly and there are optimizations that has already been talked about a little bit prior basically mean that okay, we can potentially like delay gossiping out blobs a little bit later so that it doesn't mess up with the blog gossip and also maybe intentionally save some like block bandwidth if you can. If all the other people can get their like blocks like from their local EM pools.
00:55:02.445 - 00:56:02.743, Speaker C: So the client readiness parts I've done some like search by myself. This property is not like entirely accurate but for the Yale side I believe nevermind and Bethel has already done the implementation for the engine is get block V1 guest has a PR ready and Aragon hasn't implemented yet for the CL sides. What I know is that Prism and Lighthouse are about to release a new version that supports utilizing this new API. So the point here is that it seems like most of the clients are ready or almost ready and if we make it kind of like into the hard fork then everybody like after hard fork should have this change and should have or potentially will have a lot of savings for the network bandwidth. The second point is to disable publishing. I think this is more about CAD TECU and they are already releasing that already so I'm not going to talk about this extensively. And the third part is implement.
00:56:02.743 - 00:57:31.135, Speaker C: I don't want P2P protocol messages and from the multiple researches that we've seen it seems like with that we could result to 30 to 50% transfer reduction and some of the latency reduction as well. But I understand that these are kind of preliminary numbers and the results will we will see it on the main net to be sure. But these are definitely promising advancements I guess. So there are like another one which is proposed by Ben Adams I believe and we to suggest that we adopt like P2P like quick so that we will have like a better RTT latency and I think this could be a good add as well. So with all the improvements I'd like to go through the proposed options here. So we believe that with the suggested like modifications if we take it at the face value right now it will be safe to increase the target to 5 and max to 8 without basically increasing the load on the various like solo stakers. And also because the blobs are almost close to being saturated, it would be great to Have a little bit more like 66% more like blob capacity just for the entire to space or for Ethereum to scale as a whole until peer desk comes into play like after picture.
00:57:31.135 - 00:59:36.329, Speaker C: And regarding the work, I believe that with the required work is like implement this one engine, get roles v1 implement I don't want message which are already being done mostly by majority of the clients so you shouldn't add a lot of extra work or workloads on the client teams and hopefully they could be done within one or two weeks of time windows. And with that there are other options like we don't increase or will we just increase the target to 4 to give a little bit more abroad capacity to the network. So the actual path forward that we want to recommend is that I think the right approach with this is like let the client teams to implement this and roll it out to the network and then we use the real network data from Mainnet to see okay, how much bandwidth did we actually save? Let's say if we see that with this we saved like 30% of the network bandwidth for upload link then with the extra like blobs it should be like on par with pre prior requirements or even lower then you should be safe to increase the blob number at that time. So the proposal is that plan teams like implement the mentioned features and changes and make them make the releases releases as soon as possible. We let it run on mainnet and observe for some period of time and answer the questions like okay, can we get more blobs directly from EL without having to download them from the CL layer also open we can do and have all the data ready and then if and when we observe the real benefits to the metrics we care about which hopefully we will then we officially commit to the change. And on the off chances that we don't see enough improvements we fall back to just increase the target to 4. I believe that's it.
00:59:36.329 - 00:59:39.405, Speaker C: Any questions, comments or suggestions?
00:59:47.715 - 01:00:43.065, Speaker A: Thank you. That was really great to see. There was also a lot to digest. But yeah, I mean I think the different options you laid out at the end could be really promising fast forward. Anyone else have any feedback at the moment? There are a number of you that left comments on the doc I saw. Okay, well so I do think this doc helps ground the conversation around like potential options for changing the blob parameters in PETRA and otherwise. Yeah, it's like you're saying Francis, if we have a little more time to gather some data that will help inform.
01:00:43.105 - 01:01:29.625, Speaker E: The decision ASGHAR yeah, I was just wondering so it seems general consensus that we want to kind of make a call on this a bit later into the fork process. But I was wondering at what point would it make sense to if we at least think there's a reasonable chance we want to include a bub increase that we add that to the, to a devnet ideally I would argue for like the most ambitious version of it, say I don't know, 6, 9 added to the devnet Just so we already have kind of make sure double check that the actual kind of making the change on the technical side does not run into any issues. Obviously that itself wouldn't give us like much data on like you know, the real world kind of constraints. But still would that make sense or would we want to wait with that until we made the final decision?
01:01:36.535 - 01:02:02.215, Speaker C: I think how can we do this? Can we maybe soft commit to this change to this increase on the call so that we can both do it on the devnet like in picture a bit later and also try the changes on the mainnet to gather information from both sides and with that we could have more data and more basically informs like information to make the decision like finally.
01:02:04.875 - 01:03:11.735, Speaker A: Yeah, I mean I don't know what exactly we mean around soft committing to have this in pectoral formally, but definitely I think everyone has soft committed to, you know, answering the question. So you know, from there there's the question of yeah, do we want to touch the BLOB accounts at all? And then if we do, there's a question of how Barnabas brought up this comment and 7742 I think is really our answer to this. Anyone feel. Well, I guess one thing is maybe just to confirm, has anyone started thinking about 7742 implementation or is that not really where we're at at the moment? The strategy here would be to have 7742 because it becomes far simpler to actually change these numbers and then from there we can think about actually changing them. Especially I think this is more attractive if we want to think about experimenting with different numbers say on a devnet. My understanding is that it's really hairy to actually go and make changes to these just given implementations today.
01:03:18.115 - 01:03:37.575, Speaker B: I think this can potentially push the hard for back a month or even two months doing that. There's not a single client that has any implementation with 7742. So if we do want to increase the blob count in picture then we really need to lay on 7742.
01:03:40.115 - 01:03:57.135, Speaker C: I'm curious is it possible to just do a hard increase, hard coding increase for now and not adding like 7:42? Because it seems like it's not entirely ready and it does require extra bandwidth for both SEO and IO to implement that.
01:03:57.715 - 01:04:11.935, Speaker B: As far as I know, there's not like a single constant that needs to be bumped up, but it's like multiple different things and multiple different libraries that have to be recompiled and stuff. So it's not a very straightforward change.
01:04:21.365 - 01:04:44.835, Speaker A: Yeah. So I don't want to distract people from dead four, but we could consider on the next AC DC revisiting this question around 7742 with the idea being that we are going to make some blob target or max increase in Petra. Does that sound reasonable to everyone?
01:04:49.535 - 01:05:18.325, Speaker E: But just one last question on this. If we say that even now already, basically if we were to start prioritizing it, it would possibly create a one month delay or more to the hard folk. Wouldn't that mean that if we only think about this in two weeks now, we are basically adding two extra weeks of basically time on the critical path of shipping this if we decide to still include it into the ad fork. If this is the case, we should probably just make the decision as early as possible.
01:05:19.785 - 01:05:52.501, Speaker A: Right. And so I mean that's the thing, like if a client team has time to go ahead and start with 7742 before the next call, that's great. But again, we should focus on deadnet 4. And this is what we had said with this original split decision was like we focus core Pector stuff. There is an, you know, for very good reasons. The point made that we should really think about the blobs in Pector as well. So we're just here at this moment in time and yeah, I mean that seems reasonable to me.
01:05:52.501 - 01:06:33.155, Speaker A: I don't think, you know, just adding 7742 is going to add like a two month delay. You know, don't hold me to that, but that sounds like a lot of time, especially if it's like the exclusive focus of all the different CL teams. So that being said, it seems reasonable because you know, we'll launch Devnet 4 tomorrow, there might be bugs and then it's, it's a whole thing. So I mean, yeah, that being said to your point on Zagar. Yeah. Various CL teams and EL teams, if you're listening, if you have bandwidth, like the sooner you can start thinking about 7742, the better. If you could even have it implemented in the next couple weeks, that Would be amazing.
01:06:39.175 - 01:06:59.745, Speaker E: Was there any, Was there sinking concerns with 7742? Because if we don't have the. If we're being driven by the cl, we need to validate the blob. Limit the BLOB target. Are we going to have the same types of issues that we're having with requests when we're optimistic?
01:06:59.785 - 01:07:00.565, Speaker C: Syncing?
01:07:01.345 - 01:07:12.725, Speaker A: Yeah, I believe we work through these questions with the EIP already and so I think, let's see, I think we settled on having the target and the header for this reason.
01:07:18.665 - 01:07:19.565, Speaker E: Got it.
01:07:21.835 - 01:07:24.107, Speaker B: Both actually in the header.
01:07:24.251 - 01:07:26.211, Speaker E: Yeah, we need the max.
01:07:26.403 - 01:07:28.427, Speaker B: We need the target and the max too.
01:07:28.491 - 01:07:29.455, Speaker E: Yeah, yeah.
01:07:30.235 - 01:07:36.531, Speaker B: Especially if you want to. Especially if you want to have different values. Not just always half of it.
01:07:36.563 - 01:08:02.065, Speaker E: Yeah, yeah, yeah. That's a good stab at implementing it like a month ago and it seems okay. It's not a one line change by any means, but I would much rather do it than hard coding new values in. But I am curious like from a CL perspective how difficult it will be to drive this new API.
01:08:18.575 - 01:08:35.155, Speaker C: It seems like from the like chat people have not like really looking to like implement it, but they are down to start implementing today. So maybe we can do this I think and understand a little bit more while like people are trying to implement it.
01:08:44.585 - 01:08:45.257, Speaker A: Sure.
01:08:45.401 - 01:09:07.035, Speaker E: I'm just curious like what. When do we cut a Pectora spec release for mainnet and we started forking test nets because we're talking about making this decision later but I feel like we're right now already rushing towards one into four test nets.
01:09:16.455 - 01:09:29.595, Speaker C: I guess. Do we have any like operations or like objections to doing it this way? Just try to implement the Samsung for two and prepare DevNet for target and max increase.
01:09:36.085 - 01:10:12.625, Speaker A: Yeah, I mean if the client team wants to correct me, please do. But my sense is that yeah, I just wouldn't want this to become a distraction with the other picture stuff. So like my sense would be kind of what I said. I mean look, the other way to think about this is we can have latency and make a decision on ACD next week. That actually might be the way to do this. And yeah, if we can get implementations, the sooner the better. I just wouldn't want this to become a distraction for the other PETRA stuff.
01:10:14.605 - 01:10:40.265, Speaker F: May I ask what other PETRA stuff are there? Because my understanding is that death net 4 consists of all the EIPs right. For Petra A so if today you're defnet 4 ready, you are mostly maintaining the future set right funding bugs and stuff. So now the question is that whether do we include this EIP and have a Devnet 5 or do we just.
01:10:40.373 - 01:11:09.255, Speaker A: Cut Devnet 4 short? Basically, I expect we'll have a Devnet 5, we'll launch Devnet 4, even say tomorrow there'll be bugs that will take up some attention to fix. But yeah, I mean, okay, like do we want to commit to moving ahead with this today? It does feel like process wise. I mean, yes, like I think we all agree, but like, yeah, like I.
01:11:09.295 - 01:11:11.135, Speaker B: Think who doesn't agree to include this?
01:11:11.175 - 01:11:11.955, Speaker E: By the way.
01:11:15.015 - 01:11:21.035, Speaker A: What is, what is this? It's 7742 and then a commitment to change the BOB count somehow. Right?
01:11:22.595 - 01:11:41.135, Speaker B: Well, I would, I would commit to 7742 and then we can discuss all the changes later on. But 7742 will be a requirement for Peer Desk anyway, so might as well discuss about including that and then pressing the blob limit once every client has actually implemented it.
01:11:47.045 - 01:13:13.525, Speaker A: Okay, I do think we'd want to have all the ELs also buy into this, but we could go ahead and say today, are there any cls who are opposed to 7742 and moving that to, I think we call it scheduled for inclusion now. Okay, anyone disagree? Okay, I will assume that's unanimous consensus then. So right, if you have time, please move ahead and start looking at 7742 implementation and I'll be sure this is on the agenda for next week. And again, I don't really expect any surprises. But then, yeah, we can move ahead and yeah, just keep moving towards a blob increase for picture. Okay, we have a little time left and there were some ask for another set of EIPs. Are there any other final blob points to make before we move to those? Anything with peer dos or blobs or scaling.
01:13:13.525 - 01:13:57.475, Speaker A: Okay, cool. So there were two that were kind of related. I'll just go in the order they're on the agenda. So the first one was 7783, right? Yeah, let me double check actually. Right. So yeah, 7,783. So this was on last week's ACD call and essentially it's just having some more structure around how the gas limit would change, assuming there was a targeting within the protocol that would raise it or lower it.
01:13:57.475 - 01:14:01.475, Speaker A: Let's see. Is Julio on the call?
01:14:02.395 - 01:14:03.139, Speaker E: Yes.
01:14:03.307 - 01:14:05.935, Speaker A: Oh cool. Yeah. Do you want to say more?
01:14:07.555 - 01:15:08.411, Speaker E: Yeah, actually I wanted to say more in context of 7782 because this is not a CL specific EIP I just wanted to give a small introduction. So yeah, so basically instead of increasing gas limit in a cliff like manner, you increase it gradually and you can kind of pick away some of the. Some unknowns. One of which could be for example if you guys don't remember, if you guys remember the 413 status code error and also yeah this AIP is not part of the protocol so it's also easy to implement. It's not an art fork. Yeah, so it's basically, it's basically, it's Basically a counter UIP to E7782 and yeah, that's pretty much it. Next week at ECDW give an update on it here.
01:15:08.411 - 01:15:15.295, Speaker E: I just wanted to present it. I don't want to waste people's time on any leap too much here.
01:15:17.595 - 01:15:22.615, Speaker A: Cool, thanks. And yeah, I know this was the next one paired with this one was 7782.
01:15:23.595 - 01:15:24.331, Speaker E: Let's see.
01:15:24.403 - 01:15:29.665, Speaker A: I think, yeah, this is from Ben and it looks like you're here on the call. Would you like to give us an overview?
01:15:30.245 - 01:17:10.605, Speaker B: Sure. So the idea of rather than raising blobs or gas directly at least initially would be to decrease the slot time from 12 to 8 seconds which would also in effect increase the blood count and gas limit in aggregate. However, rather than loading any increases on a single validator by speeding up the slot time, any increases are shared between one and a bit validators and rather than. So again this was brought up in ACD but it's more of a consensus change than execution change and I suppose one of the most important questions would be how difficult technically a change of changing the slot time would be some. Oh, one of the other benefits is for instance based roll ups where their latency is directly dependent on layer one latency outside of things like doing pre confirmations and other more complex changes. So yeah, any feedback on what changing the slot time would be?
01:17:15.875 - 01:18:14.455, Speaker E: Yeah, so I. So just to repeat what I said also in the last ACD about this is that probably this is a good change to have eventually because it makes the chain more lively. But by talking around with some people, especially from the distributed validator camps, they told me that this EAP is punitive because. Because it actually still increases bandwidth quite significantly and another concern I personally have is that it may slow down development of singles at finality because one of the approaches being researched is basically trying to just brute force, just brute force with some smart strategy of aggregating the signature. So having lower lot times will not help there that much. And yeah, so these are Just my concerns from last time at acd. I just restated them here.
01:18:21.875 - 01:18:22.735, Speaker A: Yeah.
01:18:25.395 - 01:18:32.085, Speaker C: Yeah. I just dispute a bit the fact that this is mostly an ACL change. Like yes, the change happens in CL.
01:18:32.275 - 01:18:33.809, Speaker E: But the history is going to grow.
01:18:33.857 - 01:18:42.497, Speaker C: 30% faster and while the performance itself is not really linked to the size of the history because at least in.
01:18:42.521 - 01:18:48.937, Speaker E: Guest we move those old blocks outside of the DB if it's going to.
01:18:48.961 - 01:18:54.201, Speaker C: Take more space on the disk And a typical guest plus lighthouse install is.
01:18:54.233 - 01:18:56.605, Speaker E: Like 1.3 terabyte at this point.
01:18:57.265 - 01:19:01.753, Speaker C: If we start making the chain grow faster and faster, which by the way.
01:19:01.769 - 01:19:13.865, Speaker E: The chain is the big biggest size hog, I think we're going to have a lot of validators out there that are going to have to upgrade much sooner than anticipated.
01:19:14.365 - 01:19:15.803, Speaker C: Is it 50% faster?
01:19:15.890 - 01:19:18.833, Speaker A: Like once, once you get there it's.
01:19:18.920 - 01:19:21.431, Speaker C: 50% but until now it's 30%.
01:19:21.517 - 01:19:24.028, Speaker E: Okay, my mass is probably terrible.
01:19:24.115 - 01:19:38.525, Speaker B: 33 on the history growth that's significantly decreased since blobs because a lot of, a lot of that's call data that has moved off to blobs and they're not 444 also.
01:19:42.025 - 01:19:46.913, Speaker E: Yeah, okay that's possible but just I.
01:19:46.929 - 01:19:52.325, Speaker C: Don'T think we should jump into this without making some concrete measurements.
01:19:55.835 - 01:20:30.023, Speaker E: I have the measurements. I mean I basically asked for the guys from Atheist. I mean I talked with the guys from ETH Staker and they basically told me that whether you increase throughput of GAS or not in around six months Validator, in order to comfortably run we need to switch anyway to 4 TB. So I mean since you're. You're basically. Since the hardware works in power of two if that gonna happen anyway, might as well just you know, increase throughput right? Like if you don't do anything it you still increase the requirements. If you do nothing, increase the requirements.
01:20:30.023 - 01:20:40.675, Speaker E: So just make some people happy. I guess that's. But yeah, so that's the answer to your question but I think it should be discussed in ECD probably if anything.
01:20:45.495 - 01:20:48.835, Speaker B: Is there any feedback from consensus?
01:20:53.905 - 01:21:55.465, Speaker A: One thing I would say is the slot time is very critical to many, many things in the protocol and also with implementations. One concern I would have with this is that it might sound pretty straightforward but then ultimately has a huge implementation load. Sort of like with the attestation IP we have in Pectra where it only sounds like. It sounds pretty self contained but then again just and how critical this concept is to the protocol and its semantics, it implies a lot of work. So yeah, I agree with others that we should sit more with the actual performance benefits and weigh that against what we're actually aiming for. One path forward to de risk the implementation question would be to, yeah, work on a devnet or something off in parallel, just to get a sense of how much work this would actually be. Francesca?
01:21:57.245 - 01:22:53.035, Speaker E: Yeah, I just wanted to say that one thing that might not be obvious is just how much it kind of has a potential get in the way of future changes to the cl, which you might think be things that we haven't agreed upon might be even things that we haven't even talked about on ACD because they're just research at this point. But there's just like a whole bunch of things that we might potentially want to in the future, from EPVs to IELTS to all kinds of stuff that do interact with the structure of the slot a lot. And yeah, just compressing the slot might really not be good for that. I'm not saying that we can never do it, but just I think it's something that we should only do it for in like an extremely deliberate way, like really being sure what we're doing. So if this is really about just increasing the gas limit or blob throughput and so on, then I think in the short term it would be much safer to just do that as opposed to do it in this way.
01:23:11.945 - 01:23:14.565, Speaker A: Was that helpful, Ben, in terms of feedback?
01:23:15.825 - 01:23:16.545, Speaker B: Yeah.
01:23:16.665 - 01:23:21.125, Speaker A: Or I mean, yeah, I guess from here, like would there be a place to continue the conversation?
01:23:23.865 - 01:24:27.105, Speaker B: I mean, yeah, there's a on eth Magicians of a thread and I do think it's so I originally raised the EIP because there's been a lot of talk about changing slot times, but nobody's actually put anything down to focus and center the discussion. So sort of putting out there as something to consider and think about because it does impact quite a lot of things. Based roll ups, for instance, is one thing that seems to be seen as a more aligned roll up. But then that has the disadvantage of being quite dependent on L1 speed compared to other types of rollout.
01:24:36.525 - 01:25:28.565, Speaker A: Okay, any other comments on this? For now my client has a question. How hard is it for cls to change slot times in general? Which I think is a good question. And that's kind of what I was getting at is it could be the case that, yeah, it sounds simple but is actually more complex than it sounds. Do any sales today have a sense of this? Like is it as easy as just changing a constant or some configuration, or would it be, you know, more invasive than that? Enrico says it's not simple. Yeah. Julio says it's not. Which I think just lends credence to the other arguments made that we should be very sure about this and do it very intentionally if we do do it.
01:25:36.025 - 01:25:36.505, Speaker E: Cool.
01:25:36.585 - 01:25:57.255, Speaker A: Well, thank you. We are almost at time. There was one final thing, just more administrative. So. ACDC on November 14th. This is right in the middle of DEFCON. I would propose we go ahead and cancel that call because I think most of us will be there.
01:25:57.255 - 01:26:30.645, Speaker A: I'm happy to have it if we want it, but. Yeah. Would anyone be opposed to canceling that call? Given that. Yeah, most of us will be at DEF CON and if anything can just. Yeah. Chat in person or at various times through the week. Anyone opposed to that? Otherwise, I'll move ahead with canceling that one.
01:26:30.645 - 01:27:05.145, Speaker A: Okay, great then. Yeah, I think that was it for the day. Unless there are any other final comments, we'll go ahead and wrap up. Enrique, you unmuted. I don't know if you had something to say.
01:27:06.925 - 01:27:07.357, Speaker D: Just.
01:27:07.421 - 01:27:07.757, Speaker F: Bye.
01:27:07.821 - 01:27:08.425, Speaker D: Bye.
01:27:08.845 - 01:27:13.665, Speaker A: Okay, then, that's that. Thanks everyone and I'll see you around.
01:27:15.325 - 01:27:15.733, Speaker E: Thanks.
01:27:15.789 - 01:27:16.269, Speaker A: Bye.
