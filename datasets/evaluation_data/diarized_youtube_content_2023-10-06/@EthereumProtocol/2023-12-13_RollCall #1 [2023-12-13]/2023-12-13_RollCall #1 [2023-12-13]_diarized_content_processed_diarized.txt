00:00:00.810 - 00:01:09.490, Speaker A: All right, let's get started. So welcome everyone to roll call number one. The original plan was to host this in Istanbul, but due to the circumstances that didn't end up happening. But I think a lot of us managed to still get some fruitful discussions in a bit more piecemeal while we're over there. So today we want to start by discussing some of the new rips that have been proposed and then get into some of the other topics. So we'd like to start with the SeC P 256 R one pre compile which is being moved from being rip, sorry, EIP 7212 to becoming Rip 7212. But basically the EIp will be deleted and recreated as an rip and the idea then being that we can move it into final call.
00:01:09.490 - 00:02:10.854, Speaker A: So asking for comments on what the status of that? Well, basically comments or objections to the rip as it stands. So yeah, I'd like to hand over to the clave team to discuss if there have been any changes or basically where all of this is from their perspective. Hello. Thanks for including in the agenda. I hope everyone is fine. I'm sharing the EIP page in the chats. Currently we are okay with the specifications as the quarters and we received some feedbacks and recently we have changed the address of the pre compile implementation and changed it into the next available address which is proposed in one of the other pre compiles but not active yet.
00:02:10.854 - 00:03:35.240, Speaker A: I have asked to the mat the light clients and he suggested to use this address b and there is no other change and I think we are ready to go to the last call and discuss with the roll up teams about any possible change and we would love to implement any decided change to the Rip version. And we are also currently waiting for the Rip, I mean specifications. For example, how should we propose our existing EIP as an Rip? What should we do as numbering as the file structure? Should we delete the EIP? There are a few discussions in the EIP editors section in the discord. We are also following it and ready to do any change and discussions. Thanks Ulash. Yes Dano. Dano, if you're speaking, we can't hear you.
00:03:35.240 - 00:04:02.918, Speaker A: Oh, all good. I was just saying that we're excited about it. Cool. Can you hear me? Yeah. Okay. My mic was on the wrong system. So with the b one address, what are the implications for the layer one teams? Will they be obligated to ship it at b? There was already l one proposals for BLS twelve where b was one of the proposed numbers.
00:04:02.918 - 00:05:28.320, Speaker A: What happens if l one decides to do another pre compiled next, that is not r one. Are they obligated to skip b and go to c? How will that impact compatibility? Would something in like the thousands be a more appropriate? Yeah, I'm concerned about b one. Oscar suggested that to blocking that address for the future for this specific pre compile, right? Yeah, I think basically we'll see this kind of happen more and more frequently in the future anyway, that we have pre compiled that start that ship on L2 first, but still are a very reasonable candidate for layer one. And so I think standard procedure should really be to basically just block the pre compile address on layer one. So even in the case that layer one never ships something, that these never also never ships something conflicting. But of course we are not in a position to fully commit to this here. So this was basically the only remaining item on my list before moving this ip to final would be to double check with the layer one teams and get them to commit on basically blocking this address once the ip turns into final.
00:05:28.320 - 00:06:14.032, Speaker A: But I'm open to suggestions. If, for example, you think there's a strong reason why you'd want to have ips pre compiled, be in a different range, even if they potentially could target mainet at a later point, then we could discuss that. So yeah, I think that should be discussed on the ACD call, but I think that actually brings up a second question. Is pre compile coordination registry, because there are going to be chains that have pre compiles that matter just to them, roll ups, and there are ones that we want to share. We also want to make sure there's no collision. So I think the second thing that should fall out of this is some sort of a pre compile registry rip. Yeah, I think that's a great idea.
00:06:14.032 - 00:07:47.666, Speaker A: I think the very pragmatic first step on this would just be that we wanted to have ips, basically have a section where they indicate which L2s have implemented them once the IP turns final. Of course that's not as good as central ip as one place where you can see all of them. So I think that as a second step should also follow the only question there is. Do we list basically all of the ips that are just individual to specific roll ups or only the standardized ones? Harry? Yeah, I think I had posted a little bit about this in the chat after last time. I really like the idea and I'm wondering what other people think about here, actually trying to get sort of names like, let's say three namespaces of pre compiles. There's the current implicit namespace, which is basically the all zeros prefix, which is used by l one development. But we could potentially just have sort of like a different prefix, like zero x 10 zero, and then one, two, three and then two and basically have the two other prefixes being one being a rip fully owned prefix where if Ethereum wants to, it'd be awesome if they also adopt the rip and just also have it in that prefix.
00:07:47.666 - 00:09:10.500, Speaker A: I don't think people really care about pre compiles being incrementing from zero and the other being a roll up specific prefix where roll ups could put pre compiles and have a guarantee that neither the EIP process or the rip process would collide with theirs, and then we could just never worry about this again. Yeah, I think the idea of having separate namespaces makes a lot of sense. There's much easier to maintain registries and that kind of thing. Anskar yeah, I would say just as a pragmatic kind of next step then, because I know that the r one pre compile authors really want this to get to final as soon as possible. So I would propose that we basically bring this up as a topic on the next orchodevs call. I'm not quite sure that could be because of the holidays, then I'm not quite sure when that is, but basically make a decision over there whether people prefer kind of just using the normal range and blocking them or having a separate range for ips, with the understanding that Mainnet would then also ship in. They say the r one pre compile if we ever were to ship that on r one, also at the IP address, basically just confirm with them and then get back to the IP authors, update the IP, and at that point then if there's not been any other update to the IP, move it to final.
00:09:10.500 - 00:10:01.620, Speaker A: Yeah, one of the really cool things about pre compiles is that we have as much space as we need. It's not like opcodes where it's sort of contentious as to the availability and you'd need extensions, et cetera, et cetera. So this just giving us a really large namespace seems very possible. Hai chin, I hope I'm saying that correctly. Yes, that's correct. So actually my question is like, so far the r one pre compiled case that it's potentially like that the layer one can also pick up this pre compile in the future, then that means putting the different namespace will be weird. Like if the layer one eventually pick up this pre compile, I think it's not that bad.
00:10:01.620 - 00:10:51.634, Speaker A: It means that when you're calling into it on the layer one, it would also be at this other address. I think it's more just like delineating so that we don't have conflict, as was already mentioned by someone earlier on. I don't think most people care what the address is of a contract they are calling into, so then it shouldn't matter all too much. So one small thing I would float here. The r one pre compile is mostly useful for account abstraction wallet contracts and those are created at create two addresses that can be the same across all chains. I don't think it's unsolvable. Right.
00:10:51.634 - 00:11:53.350, Speaker A: So you can still make a contract that multiplexes on chain id to call into one address on roll ups and another on l one. But if you want a single contract to exist both on l one and on roll ups, then it makes it somewhat simpler if it exists at a single address. Yeah, we could definitely solve it with some kind of switch here, but that would be, I think just if we can avoid that, that'd be great. Powell, you mentioned in your comment that it's not quite as simple as that in the chat. Do you mind elaborating? Yeah, I mean, this is kind of the tax you pay for pre compile every time you call something. Even if you don't use a pre compile, you need to know if it addresses a pre compile or not because it affects many other aspects. So this information is kind of critical.
00:11:53.350 - 00:12:55.082, Speaker A: And if you overcomplicate it, how you determine if an address is in pre compile, then everyone has to pay this additional cost. If you use whole addresses like the prefix in the beginning, then you need some kind of map or, I don't know. For now it's just a range from one to some number, and the number changes by the hard fork number, more or less. It's not, I think, big deal, but something to consider that if you introduce gaps, that changes how it works currently. So there are some number of complications that I think it's not like showstopper. I think it makes sense to have the same pre compile on the same address. So some reservation is fine.
00:12:55.082 - 00:13:56.010, Speaker A: But yeah, I'm not sure I want to use the whole address space for a pre compile. So the number of trade offs. One example is that a pre compile is always kind of worm access. So you need to know how much you will pay. If you even check the code size of some account, you need to know if it's a pre compile or not, and kind of breaks, all of this abstraction. Okay, thanks for explaining some of those complexities, Jordy. Yeah, one thing that I would like to put on the table is what would happen if instead of finding a specific address, we define the address of the equivalent smart contract written in solidity or in VM.
00:13:56.010 - 00:15:03.460, Speaker A: I mean, you have, let's say a canonical smart contract written, for example, signature verification, whatever. I mean, you do it in solidity, you have a smart contract, which is very well defined. You have the definition in VM, it's very clear, it gives compatibility. So for any other roll up, if they don't want to implement something specific, they already have something. We know for sure that we don't have any consensus problems. And the only difference here is the gas price. I mean, maybe there is some chains that decide that certain smart contracts have a different gas price, but for me this has the advantage that we don't need to deal with addresses, but has also the advantages that all the precompiles that are standardized, they come with a very clear definition, have a very clear consensus, so that there is no doubt if you implement it in one way or the other.
00:15:03.460 - 00:16:24.198, Speaker A: I think an idea like that is very cool, but it does require all implementations to be correct and audited and that kind of thing, which adds, I guess a little extra complexity, because now we both need a really solid compiled version as well as the pre compile, as in like solidity compiled or whatever your favorite language is. I think that's a little complicated. It also would certainly complicate the checks that Paul is talking about, because now these would be deployed all over the place. But I do think these are interesting directions to go in different consensus things. I mean, different interpretations of the pre compile from different chains, different people imagine that there are some hks that in one chain is done in one way, in the other chain is done in the other way. So I'm afraid on this, and having a canonical standard EVM recompiled that does that. I mean, this is the standard is the canonical.
00:16:24.198 - 00:17:18.640, Speaker A: I think this is a good thing. So then in the case of the EVM implementation, differing from the pre compile due to some kind of bug somewhere, what is the correct version? How do we address something like that? The thing is that it's precompiled with this any chain. If you change the code, then you change the precompile address. So the precompile is defined by that address. If you change a single line of code because you fix, you correct or whatever, you will have a different address. So there is no doubt what you are executing, but it's like a compiler smart contract. I mean, it's a smart contract that's just doing that.
00:17:18.640 - 00:17:59.740, Speaker A: You can deploy that smart contract in any chain. You know that this smart contract is going to work. So you get compatibility out of the box to all the chains with a very clear way it's done. And then the only thing you do is just for acceleration. Okay. Because you have some specific implementations, then specific roll ups can decide to just lower the price of some pre compilers by lowering the price. I think it's much easier to deal with different interpretations, different versions, different chains, some chains that implement something, the other chains that don't implement something.
00:17:59.740 - 00:18:58.690, Speaker A: Just trying to simplify. Yeah, I think there's all valid points. I think there's still a lot of complexities we need to hash out there in terms of exactly how we do these progressive pre compiles. I think let's table these progressive pre compile discussion for now, and we can have this discussion in a different context later. But yeah, definitely interesting things to think about here. Andreas? Yeah, just a question from a peer developer experience. Right, and solidity or compiler point of mean you have chains that implement it, others don't do they implement it exactly the same way.
00:18:58.690 - 00:20:05.430, Speaker A: So I think that's actually a big problem. So a canonical way of doing it, I think is absolutely required. Otherwise you're going to screw with developer experience security. You're doing r one signature on one, it doesn't verify on the other chain, even though it should, especially if you're doing like bridging and stuff like that. There's all kinds of things that really require saying no, it has to be done in a particular way and only that way, otherwise you're breaking things all over the place. So to be clear, part of the purpose of this whole standards effort is to have that one canonical way. So the idea behind having this r one go into final call now is that this is the implementation that we'd like to ship.
00:20:05.430 - 00:21:12.540, Speaker A: It should be the same across all chains, modulo some weird bug. But yes, if you are going to do it, do it this way, or if you're going to do it differently, then have it at a different address kind of thing. Monscar? Yeah, I also just wanted to briefly say on the topic of progressive pre compiles, because I think that this is an idea that has come up before. I don't think it's quite in time to take this into account for say the r one pre compile here and which address it should use, but it's something worth following up. There was the idea, because we of course wanted to have the in person event in Istanbul, which unfortunately we could not do. We had a few topics and the progressive was on the agenda. If I remember correctly, there was an idea that we could starting maybe after the holidays in January, we basically have a few breakout calls, optional breakout calls centered on each call center on one of these topics from the agenda, or additional topics if they come up by then, maybe using the same time slots, kind of on the off weeks where we don't have this call for people interested.
00:21:12.540 - 00:22:26.190, Speaker A: So I think those would be a good forum then to have kind of more in depth conversations around how to do these things. But I don't think it would be a good idea to kind of block deploying kind of a pre compiler that's ready to go now until we have figured out this topic. Ahmad yeah, just looking into that from the compiler perspective and developer perspective, it is of course a good thing to follow a canonical way, especially for l two s. But there is a reason why there is a separation between what is to be implemented on l one and what's to be implemented in l two. And one of the reasons is that not everything that is implemented in l one makes sense to be implemented in l two. Like given for example 4844, blob, gas fee opcode, and like for example the 4788 a pre compile, et cetera, et cetera. So for these it does not really make sense.
00:22:26.190 - 00:23:15.500, Speaker A: I'm not trying to stir up things here, but EVM equivalents, et cetera, does not make sense in these cases. Right. In this case we need to look into. Okay, probably here we need to know what we're compiling into is this smart contract being compiled into an l two, and if that's the case, certain opcodes needs to be known that they're not compatible with an l two. For example, these opcodes are made for l one. So I'm all for canonical way of doing things, but it should be known that this is for l two and does not necessarily apply to l one all the time. Yeah, I think that's a very valid point.
00:23:15.500 - 00:24:20.558, Speaker A: And basically the idea that we're going to have ultra native compilers or maybe feature flags for certain rips, et cetera, I think makes a lot of sense in the compilers in terms of what this would look like. And I think that's something we're going to have to figure out on this call for all the tooling moving forward. Powell, I'm not sure, I fully agree, but I really like this idea to have at least kind of reference EVM implementation, and I know some l two s already are doing this. They have pre compiles implemented in solidity or EVM, and they just deploy the code at a given address. So I don't think the address deploying on is so big deal. I mean, the benefits of that is much greater, in my opinion, to have that, because if you don't have a capacity to implement it in whatever the back end you have, you can just use EvM. For this reason.
00:24:20.558 - 00:25:10.120, Speaker A: It also works great as a reference spec. Otherwise we need to reach out to some other sources to understand what to implement. Exactly. And in the specific topic of this r one, we actually want to have implementation evmax. I don't think this will be like for deployment, but at least for the spec might be good use for it, and we want to check how well it works in this context. Yeah, thanks for that. Regarding the namespaces, I think it's less of a question whether all l two s will implement feature of l one or vice versa.
00:25:10.120 - 00:26:14.000, Speaker A: But on the other way around, do we want to prevent ever merging features from l two s into l ones or the other way around? And I think there is a consensus that we don't want explicitly to block merging. We don't want conflicts. So it makes sense to have one namespace and one place, whether it's rip or the core dev or someone that will do the minimal work of managing this namespace of numbers. It's not that someone is going to be neither l one nor l two s to have thousands of numbers. There's going to be a small, maybe tens number of numbers. And so you know that this namespace will always be pre compiled and just have this unique numbers authority like Yana on the ITF on the Internet to manage these numbers to make sure that they never overlap. I think that's one of the charters of having these rap.
00:26:14.000 - 00:27:18.512, Speaker A: Eventually, when cross execution cross compatibility between networks will be added, it's going to be simpler than separate namespaces. Yeah, I think that's a very valid point. I think Anskar is getting to that in the chat as and yeah, having a single registry probably in the EIP's repo of this would make a lot of sense then. Yeah, I think. But since dwell mentioned how it's done on the Internet, I think having a namespace and no collision for possible future merging is important. But maybe what Harry suggested of having also a range in a range which is private and is never merged. This is probably okay, the equivalent for this is the private address range for ips.
00:27:18.512 - 00:28:25.440, Speaker A: There are certain ips that you are allowed to allocate in any way you like, and you know that it doesn't need to be compatible with other networks, whereas the rest of the IP range is allocated. So I think the equivalent here would probably be to have a certain prefix under which things can be different on roll ups. And this is roll up specific stuff. And anything outside this particular range is a non conflict zone. I mean, if that's a path that people would like to go down, to be able to have separate things that people are deploying on their individual l two evm implementations, I think that's totally fine. This would create quite a lot of branching and divergence in terms of implementations, then in terms of what gets deployed. But if there's demand for it, then sure, I don't think there's any reason not to do that.
00:28:25.440 - 00:28:58.940, Speaker A: Harry. Hey, yeah, I just wanted to add in terms of, and I kind of wish we were doing all of this like three years. Know, I can speak to both arbitrum and optimism. Already have kind of custom chain specific pre compiled. I'm not sure about other roll ups, but I wouldn't be surprised. Arbitrum, unfortunately, this is definitely a regret. Just used like some, I think we're in the 200 range or something.
00:28:58.940 - 00:29:47.120, Speaker A: Rather than namespacing optimism, I am pretty sure namespaced all of their pre compiles behind some specific prefix. Oh yeah, 420. Well done on their part. So yeah, this is a bit kind of like there's already this issue. Yeah, but this is it. But the past pre compiles are all known at this point. So if we choose to standardize it this way, we could have a document that covers all the current ones that are used by any roll up and just not add any more of them without using the new namespaces.
00:29:47.120 - 00:30:42.740, Speaker A: Yeah, I think that's going to be one thing that we're going to have to just manage going forward. On everyone's front is that there already is a lot of deployed code, and a lot of people have had to make practical decisions in order to actually deploy their l two s. And so this means that we're going to need to think about these things and handle them as they come up. Anyways, moving on, the next topic I'd like to touch on is Rip 7560, and that is the native account abstraction one. I believe we have someone who wants to come on and talk about that. Yeah, I think I will present about it. So Rip 7560, this is heavily based on the previous ERP 4327 that we made for account abstraction.
00:30:42.740 - 00:31:44.460, Speaker A: So I'll give a brief overview. Somebody is not very familiar. What we try to solve with ERC 467 for account abstraction is to allow smart contracts to feel like an actual account native for the users. And we try to achieve it without introducing a consensus change or changes to the evm. So the way it was achieved, backtime was done with deploying the entry point contract. And this entry point contract is a very important contract that was audited many times. And this contract coordinates basically all the process that is happening with the account obstruction and the block building is separated and blocks of transactions that are abstracted that we called user operations is delegated to also separate parties which are bundlers.
00:31:44.460 - 00:32:24.992, Speaker A: Put all of them together into a single large ethereum transaction of a legacy type. Right. Can you hear me? Yes, we can hear you fine. Yeah. Okay, great. So the RiP 7560 takes it one step forward by introducing basically the same principles of account abstraction, ERC and of user operation as a new transaction type. So in general, we propose to create a new transaction type.
00:32:24.992 - 00:33:39.484, Speaker A: We want to reserve number four for this transaction type. And this transaction defines multiple additional fields to define the sender, to extract away signature. And we have some additional features like multidimensional non support and a gas abstraction and execution abstraction built into this transaction type and validity ranges. So the transaction is a little bit bigger. And another step that we are taking is we want to introduce multiple top level execution frames for this transaction. So this transaction will not just include a single call that is done from an EOA to the target, but this transaction type can include up to five, I think, or six top level frame execution frame. First phase of this call is to establish if the transaction is generally valid and eligible to be included in a block.
00:33:39.484 - 00:35:24.860, Speaker A: And the rest is the actual execution of this production debugging. So we do define a number of interfaces and these are solidity interfaces. These are very similar to how they look like in the ERC, but with some changes that were probably necessary. Another thing that we realized while working on it is it is pretty complex task for the block builder and the one who operates the mempool and is filling a block to put this transaction in a block sequentially because transaction can change the state, which means that they can affect each other's validity. So a single transaction can flip a flag, then the next transaction is not valid. So as far as I know, this is not addressed in the previous account abstraction EAP 29 38, if I'm not mistaken, that we need to isolate the code that is running in validation from being modified also by other transactions in the same block. So what we propose in the rip is to cut these transactions like in two parts basically, and apply the validation part together before execution.
00:35:24.860 - 00:36:42.250, Speaker A: So if you have three transactions, then you're running transaction one, validation, transaction two validation, transaction three validation, then you're running transaction one, existence, transaction two attacks, and so on. I think I should post the link to the EAP itself, rip itself. And in addition to that, we did define a very extensive list of storage rules, which means that we want to limit the validation code and we are doing it in order to protect block builders. So if block builders want to break these rules, they are free to do so. It's just that they might be exposed to some kind of denial of service attack because they allow the validation code to invalidate a block, basically. And these rules belong to an ERC that I think is relevant. So I will share it as well here.
00:36:42.250 - 00:39:01.104, Speaker A: And what else should I mention? Yeah, I think that multidimensional nons, this is something that we already do in the ERC, but we want to make it a more native part of the blockchain. So we propose to introduce 256 byte long non that will be split into two parts and a sequential 64 bit value and 192 bits of arbitrary data. So multiple to enable smart contracts to have multiple channels, like smart accounts to have multiple channels of execution in parallel. If you have separate groups of people interacting with the accounts at the same time, it seems like we will achieve that with a pre deployed contract that manages this as part of actual Ethereum state, which is another part that is it another thing that is a very important part of account abstraction for us is gas abstraction. So we allow so called paymaster contract to accept paying a fee for a given transaction. And in that case the entire payment of gas is subtracted from this contract and not from the sender. And because we are adding gas abstraction and validity ranges, we are considering extending this to eoa contracts as well.
00:39:01.104 - 00:40:22.892, Speaker A: Because eventually if we introduce a way for eoas to inject code into their addresses, that would make a lot of sense for them to do it with a type four transaction that is subsidized by a paymaster and has lead range and benefits from other additions from this transaction type job. Did I miss anything you wanted to say about the rip? And if there are any questions, first, not about the rip, but I wanted to add something about the motivation behind it. So this rip is actually, this is actually what triggered the creation of this group of the rip process of roll call. Because we saw that when we worked on ERC 43 seven, we wanted to standardize account obstruction so that wallet developers can build once and deploy everywhere. And this worked great. But then some layer tools wanted to get better efficiency and other benefits of having native account obstruction. So we've seen roll up starting to enshrine a version of 43 seven, which they adapted for their needs in different ways.
00:40:22.892 - 00:41:15.980, Speaker A: And then we started seeing wallets that only support one chain. Like for example argent stopped supporting EVM chains and switched to supporting only one chain that has native account obstruction. So this motivated the creation of the rip process and also of this particular rip, which is meant to standardize native account obstruction for those, for chains that wish to have the benefits of native account obstruction. So just a background for how it all started. Thanks Alexander and Jov. Does anyone have any questions relating to this rip? Yes, Stas. Yeah, hi everyone.
00:41:15.980 - 00:42:27.054, Speaker A: First I want to say thank you. The EAP is actually very comprehensive and I think it's very good. But I have one thing which makes me bit uncomfortable is that one of the introductions for this AP, in order to improve the UX for users or other for users of wallets, is that takes origin for type four transactions will no longer be guaranteed to be an AOA. And right now. Okay, firstly there might be other protocols which depend on it, normal defi ones. But if we even skip this case, then the majority of existing roll ups, I mean like arbitram, optimism and error as I think all of them use checks for ticks, origin for communication between Ethereum and l two. And basically it works like if it's a contract, then its address is alias, and if it's an UA, then there is no alias applied to the sender address to the message sender.
00:42:27.054 - 00:43:51.290, Speaker A: And so I'm curious, is there already a plan to resolve it? Maybe. Just from my perspective, it looks like if we do it naively and on Ethereum we implement the same proposal which we have in the rip, then the assumptions might break and refactor in the code, like changing it in a way which for instance does alias in always for every account, including new ways. Well, it might be a massive breaking change for the existing integrations with such roll ups too. Yeah, thanks. I also replied this comment pull request. It's indeed a problem for how aliasing is done by some of the roll ups in the layer one bridge contract. I think it will have to be resolved in some way in these contracts regardless of this particular rip, because it also conflicts with a few other eaps that are being proposed.
00:43:51.290 - 00:45:33.960, Speaker A: So then I started thinking about what was the rationale behind this exception. Aliasing is done for every address except for ones where a message sender equals Tx origin. And if I understand correctly, if I understand correctly, the reason for this is because the EOA on the other side is not able to do unaliasing which normal contracts can do. So in that case, I proposed in my reply to the comment in the pr, I suggested some ways of handling it in this layer one contract, and I think that rollups should probably consider changing this exception regardless of this particular I need to, I've not spent enough time catching up on this stuff, so I will do that after this call. But just to add a little color in terms of kind of the current state of why aliasing exists and kind of what the point is, is basically we early on discovered the issue, which is that just because you control a contract address on one chain doesn't mean you control a contract address on another chain. And so aliasing was introduced in order to avoid issues around that. Yeah, TX origin equals message sender was used in order to sort of exempt eoas from aliasing.
00:45:33.960 - 00:46:42.428, Speaker A: It's a messy system and certainly interesting to look at within the constraints of kind of what can be changed without breaking existing code. But fundamentally, the hard problem is address name spacing, and that would be the thing to kind of consider in terms of any potential alternatives. Yeah. So the context in which aliasing was needed was because the contract on layer one, you could deploy a contract on layer one using the same deployer and create to a different address. Since some roll ups, including arbitrom, actually used the message sender from layer one directly on L2, it made it possible to make calls from a contract on L2 that are not actually in the code of this contract. And aliasing solved that. And for that reason it seems to be not unneeded for EOA.
00:46:42.428 - 00:47:46.900, Speaker A: However, this will break as soon as there's a way to add code to an EOA. And there are several eips that have been proposed for doing this, starting with. So there's EAP 5003 that adds code, there's the set code, EAP 6913. Then there's the transaction type for adding code, let's say EIP 7377. So any of these eips is going to bring back the same vulnerability, the same potential vulnerability, because you'll be able to have a case where something looks like an eoa on layer one, it looks like an eoa on layer one when you start a transaction and then it actually does something to an address which is a contract on L2. So we are back to the same vulnerability. Therefore I think that before any of these eips is considered on layer one, we'll also need to change how eliasing.
00:47:46.900 - 00:48:32.672, Speaker A: We'll also need to remove this exception in all of these contracts, otherwise they all become vulnerable. Thanks yov. Pooja. Thank you Yoyav, I have a question for you with respect to the category mentioned here. And before I get into that, thank you for providing the background for the proposal, because I was trying to understand how Rip process are being evolved. So as per my understanding, RiP is similar to core interface networking ERC proposals which are standard track type. And core proposals are basically proposals which are proposed for the Ethereum mainnet deploying here.
00:48:32.672 - 00:49:59.870, Speaker A: So if we consider this as an rip, should it be rip, I mean standard track rip instead of core, what is the motivation behind adding it as a core proposal? Rips that do change the core protocol currently on L2, and later maybe in someday on layer one, should they be considered core or is core only layer one? Actually, I'm not that familiar with the definitions here, but maybe Carl and Ansgar are more familiar with what should be used there. I think this is one of the issues we need to figure out, and it's something I'm hoping to do sort of as we go. The point of the rip process is to have a place for roll ups and extended EVM users to be able to make core changes, should they seem relevant to them. And so I'm not particularly concerned with what we call it. If people really feel like we need a new name, we can come up with a new name. But yes, we definitely should be able to make core changes to these things. That is something that is separate from the IP process.
00:49:59.870 - 00:50:26.790, Speaker A: Of course. That's helpful. Thank you so much. I'm here to just learn about the process of evolution and we can perhaps discuss this later on some governance call. Yeah, thanks Pooja. I think that brings into light many questions we're going to have to work out with this whole rip process and how it fits in with eips, which I think we've seen it a few times already. Okay, then moving on.
00:50:26.790 - 00:51:56.800, Speaker A: Light client mentioned he'd like to touch on EIP 30 74 and get a bit of a temperature check for things. I still kind of have a question on the other rip that nobody really answered in the chat. Okay, just curious what the temperature check is for l two s adopting the account abstraction rip if it's not supported upstream in clients, because as it's written right now, my guess we haven't talked about this internally with Geth is we probably wouldn't support it. It's just kind of a large change across the client. And if that's the case, how do other l two s feel about it? Would there be some spectrum of things that we could do to make it easier to support? Has anyone thought about this? For testing purposes, we started implementing our own. We have a fork with a pull request that implements it. Now, I know it's not going to be merged into a get in its current form anytime soon, but isn't this the case for any rip? Every L2 uses some patches on top of Geth in order to implement the modifications.
00:51:56.800 - 00:53:06.330, Speaker A: Get also doesn't have any of the pre compiles that are used by different roll ups, so I think this would be the case. The same question should probably apply for any rip. Well, there's two aspects of that that's true, but in one sense they're trying to have the smallest diff between the upstream client and their client. And there's also the idea that having an additional pre compile is kind of a nicely abstracted away thing. You've got this file and then in some map you have to put the address in there and link it to the implementation, the pre compile. So supporting that is really simple in terms of merging upstream or, sorry, not merging upstream like doing the updates that are happening upstream and merging them back into your repository because you're not having conflicts between those implementations, whereas this is going to be a new transaction type. As a maintainer of a downstream project from an L1 client, you're going to run into a lot more rebasing issues as you are developing your feature and then trying to get the upstream changes that are happening.
00:53:06.330 - 00:53:51.512, Speaker A: It's just very different than a pre compile. Right. So let's hear some L2s. What's your take? And like Clan already kind of said it. I think probably the two biggest considerations in terms of making changes. One is difficulty of upstreaming and how much it's likely to create merge conflicts, just in that staying with upstream is important for all sorts of reasons. The other is overall there's level of likelihood that sort of it will actually eventually get in is certainly kind of the other.
00:53:51.512 - 00:54:39.320, Speaker A: And God, I had another point, but it's slipping my mind, so I'll probably come back in with it in a second. We have a variety of deviations already. Oh, sorry, I remembered the other, and this has bitten us a little bit before. I think the biggest risk for roll ups that are kind of directly building off of geth that are making modifications is implicit assumptions in the Geth code base that might change and silently cause issues which has happened before. And with this sort of stuff that's pretty deeply messing with the system seems risky since doing those merges is high precision. So yeah, it's doable and it's a very valuable thing to implement. So it's certainly something we'd be quite interested in, but also pretty scary.
00:54:39.320 - 00:55:53.534, Speaker A: Onstar. Yeah, I just want to say that I think that's another good example of basically, I don't know how right now we are still in a situation where all the tooling and the compilers, the clients, everything still natively targets layer one, when of course over time, L2 is where most of the continued iteration and most of the use activity will be. I think specifically a big part of the motivation of this standards process is to basically create some sort of common core of more advanced EVM features. And so those will necessarily over time diverge more and more from layer one EVM. The idea is not just, I think to just basically just have eips that are going to go to mainnet anyway just like five months earlier for L2s, then I feel like having an entire process for this just wouldn't be worth it. So I think the idea really has to be to have a more advanced common core that over time will include features like transaction, parallelization, state expiry, all these kind of things that make sense for the VVM, but we might not ever want to bring to layer one just for simplicity. And so I think we'll have to figure a lot of these questions out.
00:55:53.534 - 00:57:23.206, Speaker A: In general. Clearly the right choice here is not to have every single L2 team maintain a separate implementation on top of gas or something of the same feature that everyone supports in the exact same way. There would have to be some sort of standardization, either with L2 specific client teams emerging over time, or I don't know, maybe there could be some sort of common core patch that is collaboratively worked on or something. But I feel like it's not a trivial challenge, but it feels like this is something we have to figure out anyway if this IP process is supposed to be to go to meaningful places. Yeah, how do l two s and people feel about figuring that out sooner rather than later? Because I just imagine that we're going to come up with these ideas and you're going to have people going off building projects around different amounts of changes that would happen to l one clients, but we haven't really figured out how to deal with the situation that the reality is a lot of l one teams will not support things that are going to be pretty complicated. So the r one curve, that's something that I think that we could probably support relatively easily with the configuration flag, just because there's very little overlap with our system and it doesn't really require much maintenance and we don't have to work around it much. But adding a new transaction type is something that is kind of painful for us to do because we have so many different places where we have to deal with all the different transaction types.
00:57:23.206 - 00:58:52.982, Speaker A: And so it's not something that I see that we would be doing if it's not supported on l one. And so we're sort of talking that there needs to be another client sitting between the l one maintainers and the consumers of these l two teams. So it feels like we need to have this discussion sooner than later to figure out how we're going to do it. And if we're not going to do it, then what can we do if that's the actual reality? Yeah, I think that two things here that come to mind. One, personally, I'd definitely be in favor of the idea of kind of like a common core sort of fork of geth that is actively contributed to across the various, well, whoever's consuming it and kind of implements core rips would definitely be happy to contribute there. The other would be, it'll be interesting to think about, and this would be interesting in terms of a question on the guest side as to sort of willingness of accepting upstream changes that add essentially abstraction in the interest of making it easier to maintain that fork. I could imagine a variety of things that we could try to basically not insert the complexity into Geth, but insert various hooks into geth kind of for the purpose of being built on by that other implementation.
00:58:52.982 - 00:59:31.426, Speaker A: So that's kind of level of buy in there. I'm completely on board with the idea of the Geth team can't be responsible for maintaining everything for all the. That makes total sense. But if essentially it's of enough interest that it would motivate sort of incorporating changes that are low enough on maintenance to sort of make life easier for the kind of core fork. Right. I don't want to speak for the whole team. I'm personally much farther on that spectrum of being supportive of adding abstractions that make it easier, adding reasonable abstractions that make it easier for l two teams to interact with this software.
00:59:31.426 - 01:00:12.610, Speaker A: And we sort of did this for, we have this 1559 configuration now and our chain config that was proposed by Proto last year. And it took a while for us to, as a team, agree upon that and get it through, even though it was a simple change, just because we had to overcome this exact thing that you're talking about. Like, do we want to add an abstraction that makes our lives slightly harder, but it makes the l two lives easier and we did end up accepting it. So I think that the appetite is there. It's just always going to be like a delicate balance of how much complexity does this abstraction add. Because at the end of the day, we're just very focused on shipping a very safe client for l one. And I think most core teams are pretty focused on that.
01:00:12.610 - 01:01:49.376, Speaker A: And it's going to just be a dance with figuring out how to do this in the right way. But having a team that sits in between us and l two s that is focused on maintaining that core client that is implementing rips, I think this is a good idea and I would really like us to spend some time figure out how can we make this a reality. Okay. Definitely some interesting things to think about there in terms of how we can manage sort of divergence in sensible ways. Taishin has a good comment in the chat that's worth, I think, taking a look at too, in terms of lessons learned and changes made at Scroll Anskar. Yeah, I just wanted to say that, as Matt said, I think for now, while we are still in the process of figuring this out, of course there are changes that are small where this might still be feasible to have, say, gas support, especially if there's at least a good likelihood that they'll also head to layer one reasonably soon. So, like the r one, pre compile, and then there are changes like kind of enshrined four three seven, which probably it feels like it would be more important for us to basically have figured out this more fundamental approach here before it would actually be feasible to ship them to L2.
01:01:49.376 - 01:03:30.450, Speaker A: So I'd just be curious, obviously this will take some time to figure this out and set a process for this up, maybe question to the four three seven team, what kind of timeline are you kind of right now thinking about roughly? Of course. I mean, timeline is always a hard question, but for this kind of enshrining for four three seven, what do you think is kind of realistic here? And do you think that this basically will be the main blocker in terms of timeline, so we don't have a timeline for the enshrined version yet. We're still working on it. We recently published this rip and asking for comments, and we've been collaborating with several layer tools on some changes that are needed there. It's not close to being finalized, but at the same time we are working on a geth patch that implements it, and we are trying to make it as modular as possible so that it can be easily detached and doesn't have too many conflicts. I think that's important, but as for the timeline for doing this, we are very much dependent on L2s. We're going to see which of the layer tools that are looking into it would like to spin up, for starters a test net in order to experiment with it.
01:03:30.450 - 01:05:14.890, Speaker A: So the timeline for experimenting with this is less up to us and more up to the layer tools that are interested in it. Okay, thanks yov like client do you still want to touch on 30 74 in the context? I don't know if anyone else is on the call who wanted to mention it because otherwise I can say okay, just something to keep in mind. Since we are discussing both the full account obstruction rip and 30 74, it's important to keep in mind these are not mutually exclusive, despite some memes going on Twitter. So they are not proposals that try to solve the same thing. No need to go into a full comparison here, but I'm going to post something about it. But keep in mind that these are not mutually exclusive or try to solve the same problem. Yeah, that's definitely true, and I think a lot of the problem here, I think stems from the misunderstanding of exactly what account abstraction is.
01:05:14.890 - 01:07:00.620, Speaker A: It's become a really diluted term that sort of just means more functional accounts. But really the origin of the term came from this idea of abstracting the way that you authorize things to happen from your account. And it just so happens that a byproduct of that is that it allows us to do neat new things like batching transactions and having social recovery, having spending keys, those types of things. And so 30 74 is really not an account subtraction proposal, even though it's lumped in with that group of ideas, because it looks and feels like the other account abstraction and proposals that exist which have the features just because they're allowing smart contract wallets to do things. So 30 74 is really focused on giving EOAS smart contract wallet like functionality. And there's a long history about how 30 74 came about and how it's kind of evolved over the time and we don't have to get into it on this call, but I have been talking with a handful of different l two teams and developers, people who are interested in having a renewed interest in 30 74 who want to experiment with different ways of creating account abstraction like frameworks and building systems that are really inexpensive for users to onboard to and use that are effortless to interact with. And so they have said that it's something they would like to ship on their l two s, but they want buy in from the rip process.
01:07:00.620 - 01:08:25.366, Speaker A: I don't know if any of those people are here who want to say that their project is pretty supportive, maybe they could chime in. But I would just ask how can we move forward with an EIP proposal if that is kind of the case that it is supported by the rip folks? Oscar? Yeah, I just wanted to briefly say that I think more as a general comment that I think it's important for the IP process to basically that we should not make the distinction too much between, say, supported by the IP process isn't necessarily, I think, a meaningful distinction. I think we should be open to standardizing whatever some subset of rops are interested in. So, for example, I mean, I'm one of the quarters as well of 30 74. I'm personally somewhat skeptic that now is still a good time to ship it, but I think it's a perfectly fine candidate to turn into an IP and then to see to what extent L2s, there could be some subset still want to ship it or something, or maybe everyone or maybe no one or something. But I don't think basically we should not turn the roll call, the IP process too much into a blessed by L2s. I mean, I think it makes sense to have some sort of temperature check as part of the process because why not? But I think basically there should not be a blocker of adding an rip that it needs to get broad support by different people.
01:08:25.366 - 01:09:07.600, Speaker A: So I think definitely 30 74, it's going to be the same question. It's already an EIP, so how do we convert it into an IP? So we just need maybe a standardized process for this. But other than that, I think it's a perfectly good IP candidate. Yeah, I think we should hear from the L two teams how they feel about that specifically, because I don't know if that's how they're feeling, that subsets of people are going to adopt things. But Paul raised his hand. Hey, yeah, hi. Yeah, sorry without elaborating too much, I would say that polygon is very seriously considering 30 74 potentially as early as early next year.
01:09:07.600 - 01:10:17.860, Speaker A: And I would say that just kind of also it gets into this question about the way that we approach certain things. And there's definitely at least two different tracks, right? There's areas where we're trying to come to consensus on standardizing implementation across as many roll ups as possible, and then possibly with the target of getting to l one, basically. And so the SECPR one kind of built in is basically our good example of that. 30 74 may be an area where it's an opt in type thing for the roll ups. And then the main goal basically is just to maybe for one thing not step on each other. And then also if we are going to implement something across several roll ups, but not all, then to the best we can manage it, basically, not to the best we should manage it should be compliant, basically it should be a standard so that there's consistency across roll ups about how things behave as well. That's an interesting aspect of 30 74, which like I said, some roll ups may choose to implement, some may not, but the roll ups that do should be consistent obviously.
01:10:17.860 - 01:10:55.694, Speaker A: Thanks, Paul. Harry. Hey, yeah, no particular comment on 30 74. I just wanted to kind of ask and confirm. So the rip process is opt in in terms of kind of roll ups kind of adopting any particular rip, which makes total sense, but I wanted to check, I'm assuming rips need to be entirely consistent with each other. Like it needs to be technically possible to adopt all rips that you can't have actual conflicting ones. That would be sort of my only concern here.
01:10:55.694 - 01:12:51.582, Speaker A: If there isn't sort of enough buy in would be like is there going to be some other standard that comes along and it doesn't make sense to do both? Yeah, this is a really good question, but I personally don't think this is something we can guarantee just conceptually. There's some specific invariants or guarantees that we can make. Like for example, I think it's good that we would guarantee that there's never a conflict on a specific pre compile address or ideally even though of course we have a smaller range there on opcodes or transaction types or something. But then in terms of just being able to have like a completely modular field where basically we could guarantee that there's never two ips that are mutually exclusive. I don't think just conceptually, because that's not something that a standards process could enforce because there can always be just say two different flavors of something that's clearly competing, that each supported by a few roll ups, and then the ip process should definitely be able to be still a standardizing forum for those competing standards. Any other teams thinking of supporting 30 74? Okay, yeah, not a roll up team necessarily, but I just want to represent that there's significant interest on the application developer side. I think Joe Wong isn't here, but he's super interested in it for Astaria.
01:12:51.582 - 01:13:37.218, Speaker A: And then I'm hacking on some new projects right now and definitely would be interested in it as a primitive for UX improvements as an application developer. Cool. Yeah, thanks for chiming in. I guess that's pretty much all for 30 74. I'm wondering more generically how teams feel about rips being accepted by a subset of roll ups. I mean, it's going to be almost impossible to get every role to do everything that everybody wants to do, but part of the point of the process is to standardize and agree upon implementations. And you could imagine this world.
01:13:37.218 - 01:14:26.180, Speaker A: We've already been pretty focused on EVM equivalents. You could imagine we have an l two EVM standard and every roll up supports the l two EVM standard. And we're kind of saying that we're not going to go 100% that direction. We're going to go to a place where some roll ups support some of the rips. And I just want to confirm that that's also how l two teams feel about it. Or are you still thinking that everybody needs to kind of be supporting everything because we want to have l two EVM equivalents? So I think the important point here is that one is a subset of the other. If it emerges that people would like to have a l two EVM standard, and that is something that is worked on, we can maintain l two EVM or l two clients around that.
01:14:26.180 - 01:15:58.614, Speaker A: I think that's all great and is definitely an outcome that I think would be really cool to see whether we can get there, I think is a different matter and very much depends on l two team's appetite for that. It certainly seems like having kind of a common core upstream repo that implements stuff definitely makes it a lot more likely that things will get widely adopted, which certainly seems like a good goal, not something the process can enforce, but certainly something that we can work towards. Yeah, I mean, just to pick on 30 74, because I've talked to some teams about it and we're on the call partially because they were interested in getting some feedback about how other teams felt about this proposal, hypothetically let's imagine that polygon and optimism want to ship 30 74. Is that enough of a quorum for you guys to ship? 30 74. And then the teams who are kind of lukewarm about shipping and this is just an arbitrary ap, you can do this with the r one curve or whatever else are the other teams. Do you feel like that means you need to implement it because two other large roll ups have said they want to. How are you guys thinking about that? I think there'd be a strong argument for it because I don't think we have anything against it.
01:15:58.614 - 01:16:47.590, Speaker A: I think it's just sort of not that much on the top of our radar, but to the degree that it's a thing that has momentum for it and could be common tooling that could target it and work on all that would certainly be really nice. Yeah, I think a lot of it is sort of like there's a relatively similar issue to the L1 EIP process of just sort of like mental bandwidth towards pushing changes. But to the degree that there's kind of momentum, I don't think. We're certainly not against it in any way. I'm also just asking maybe more generally. Imagine arbitram and polygon. You guys have some arbitrary eap that you're interested in accepting.
01:16:47.590 - 01:17:50.970, Speaker A: If you were the only two roll ups that were supportive of the thing and the other roll ups were like, for whatever reason, they aren't that interested in it, or it's just not top of mind for them, they can't do it right now. Maybe one day in the future, would you still feel comfortable moving forward, even though it's just you and one other roll up? Doing the thing depends on how breaking it is to the overall experience. There are some sorts of changes that are kind of like value add, but don't create incompatibilities, except for people who are opting into that feature, and I don't think there's any concern about that. Like a simple example of there is like a pre compile. I don't think we're too worried about that. I think much more worrying would be anything that sort of fundamentally messed with an invariant that caused you to sort of create subtle incompatibilities that weren't based on kind of clear opt in from developers. Maybe something like 1153 would be harder for a small section of roll ups to ship.
01:17:50.970 - 01:18:47.470, Speaker A: Yeah, I guess I'd have to say the precise answer is it depends. It really is probably very specific, at least until we probably get more experience in kind of chewing our way through some of these proposals and trying to figure out how to sort them. Actually, I think another good example might be like. Actually another good example is probably like and very apt example is like account abstraction changes that break the message sender equals tx origin assumption, which is not a good assumption that people should be making, but is a thing that's kind of in normal code. So I think that's actually a prime example of wouldn't want to break that unless there was a good number of people breaking that. Well, because it seems to have questions. Sorry, go ahead.
01:18:47.470 - 01:19:22.822, Speaker A: I'm sorry. Oh yeah, no problem. You were talking about not wanting to break invariance. It sounds like you're talking about invariance at the smart contract layer, like code already deployed on chain not breaking it. But you're not necessarily talking about invariance at the client layer. Yeah, no, I'm thinking primarily invariance that a developer comes along and deploys a piece of code that they expect to function the same on all chains. And I wouldn't want to have arbitram have a deviation that caused their code to execute differently, which.
01:19:22.822 - 01:19:59.410, Speaker A: Yeah, I think kind of TX origin and message sender assumptions is probably a good example of a place where you wouldn't want to have kind of very different assumptions on one chain than a developer is expecting. Dano, you've had your hand up for a while. Yeah. I would say as an EVM developer the type of fragmentation that scares me more is opcode fragmentation as opposed to pre compile fragmentation. Because the pre compile space is much larger. It's easier to handle pre compile that's there or not there in your code. But if there's opcodes that have different meanings on different chains, that's going to have tool chain implication.
01:19:59.410 - 01:20:45.230, Speaker A: If there's opcodes that are there and that are not there, that's also going to have tool chain implications. Push zero. Who here is living this nightmare? That is something that some people support, some people don't is not something that I think is positive. So I'm wondering if maybe features that may be supported by a subset that a solution that involves using pre compiles instead of opcodes is explored. For example, adding new block metadata could be queried through a pre compile. I'd be interested to see if auth and auth call could be implemented through pair of pre compiles and that would be a lot more tolerant to fragmentation across the chains if some support it and some don't. Thanks, Dano.
01:20:45.230 - 01:21:58.774, Speaker A: Anskar. Yeah, I just wanted to say that I think this is going to be a very important conversation that we'll have to have anyway going forward. I think because for history kind of context, I think initially the idea for roll call coming out of last year was to start really with something that looked much more kind of governance like to have a proper L2 EVM that folks up from layer one EVM and really does everything in standard way and try to get all the L2s to opt into that kind of in its whole. But talking to teams, it really seems like there's limited interest in basically giving up sovereignty over the virtual machine to some kind of consortium process. So I think kind of really the only way to make progress here is this more opt in approach. And I think still think that any feature that's supported by two or three chains identically, rather than everyone just doing their own slightly different thing, is still a win, even if it's not opted in by everyone. And I do think though that there's still a lot of value in trying to then beyond that figure, find things like some sort of common core.
01:21:58.774 - 01:22:45.450, Speaker A: The question would be, would the common core only be all the changes that are supported by literally everyone, or is it more the changes that are supported by the majority and so that it's still worth to have that all implemented in one client, but it's still activated by flags or something. But of course if it's not supported by everyone, then you can't really have tooling targeted because solidity couldn't just say, hey, you compile to L2 common core because then it's still not guaranteed that everyone supports this. So there might be value in having like a minimal common core instead. That's those features literally supported by all the L2s. Obviously I think it's very early. There's a really good answer right now, but I do think that this is going to be very important moving forward. Otherwise there's very limited use in a lot of these features if they're not actually properly supported by the tool chain.
01:22:45.450 - 01:24:08.710, Speaker A: Yep, very relevant points. Thanks Anskar. Moving on, just because we're running short on time, it has been discussed on the l one calls, ACD calls recently Denkun timelines and testing of all the test nets there. So the void four four should go live on goalie early next year, probably February to my understanding, January, February, followed by Sepolia and Holsky. So if you are an Altu and you haven't been testing your deploying blobs on four, I highly recommend targeting these test nets particularly the goalie one, as Gorely is going to be the candidate for sort of changing the sizes of what these blobs look like potentially. So yes, please look at testing into these things because this impacts timelines for when mainnet can ship these things. Yes, mami.
01:24:08.710 - 01:25:03.130, Speaker A: Yeah, one concern we have with Golly is acquiring goalie if it's quite difficult these days. So is there some kind of way for L2 to acquire some for testing? That is the big issue with Gorli. I can look into if there is any spare gorilli going around, especially for those who don't know, Gorli will be deprecated after this Denkun fork, so there's no point in hoarding anymore. So I will ask and get back to you. If you do need Goliath for testing, please ping me and I can see if I can help out there. So I think those are the major topics. One last thing is the call for rip editors.
01:25:03.130 - 01:25:46.680, Speaker A: If you are interested in this, I think it's important to have buy in from many teams, so please reach out to me if you're interested in becoming an rip editor. I think it's important that we have lots of representation and I think that's pretty much a wrap. So thanks everyone for joining this roll call. The next one will be the second week of January and of course we'll all chat async before then. And yeah, see you all then. Should be able to find these recordings on YouTube if you've missed anything, but it's been great. Thanks everyone.
01:25:46.680 - 01:25:53.820, Speaker A: Thank you. Thank you. Thanks. Just.
