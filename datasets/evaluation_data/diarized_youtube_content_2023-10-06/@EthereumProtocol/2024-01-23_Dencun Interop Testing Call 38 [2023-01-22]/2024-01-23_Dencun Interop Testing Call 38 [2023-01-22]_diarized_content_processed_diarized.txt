00:03:28.740 - 00:04:01.324, Speaker A: We are live for this testing. Call number 38 it. So I have too much on the agenda. I guess having an update on how the spamming is going on devnets would be a good start then. If there's any concerned thoughts about client teams on anything. And I think finally would probably make sense to just touch on the call schedule we want to keep for these. The next one is right between the two test net forks, so it seems reasonable to keep it.
00:04:01.324 - 00:04:19.300, Speaker A: But yeah, I want to open it up if people may want to make this a monthly thing or change the cadence as we sort of wrap up the fork. But we'll also be starting a new one, so maybe we stay on this two week cadence, but yeah. Perry Barnabest, you want to kick us off with some Devnet spamming updates?
00:04:21.320 - 00:05:04.720, Speaker B: Yeah, I had shared the initial curly analysis a couple of days back, so just a follow up on that one. Most of the stats look pretty much exactly the same. There's no real difference. So I think whatever takeaway people have from that post still holds true over the weekend. But in general, if someone wants to poke around, we still have the data here. We haven't changed much about our strategy. The blob spam are still running and I think once we have confirmation from layer two people that they've started organically inducing load, then we'll slowly reduce our spamming.
00:05:08.190 - 00:05:09.660, Speaker A: Got it. Thank you.
00:05:12.750 - 00:06:31.074, Speaker C: Also last week, Marcin from the Nethermine team began very heavy blob testing and we saw an increase in memory usage in pretty much all the clients. I just linked to the dashboard for this, and we saw this very high memory usage even after the blob swamming was finished. We don't quite know what the reason for this was. Probably just the blob pool was absolutely hammered for all this period of time, but we had a bunch of clients just om crashing because we have eight gigs of ram for most of these machines and they were just not enough for this small devnet, for Devnet twelve. So if we're going to see some spamming on the main app, we might going to see some big problems where those machines that run with 16 gigs of ram will not be able to keep up and they're just going to owe them. But that's also going to be a very expensive operation on main net because keeping six blobs every slot is going to be not cheap.
00:06:31.202 - 00:06:33.042, Speaker A: Right. It's exponentially more expensive.
00:06:33.106 - 00:06:35.058, Speaker C: Right, exactly. Yeah.
00:06:35.164 - 00:06:39.500, Speaker B: And I think there was something specific that the vets team had to look into as well. Right?
00:06:40.590 - 00:06:40.906, Speaker A: Yeah.
00:06:40.928 - 00:06:42.540, Speaker C: But they pushed out a fix.
00:06:42.910 - 00:06:43.980, Speaker B: Okay, cool.
00:06:45.650 - 00:07:09.490, Speaker C: We had issues with gas. We had issues with rest, you said, and with Bezel also. So it was mostly the execution layer clients, but also just Cl. Clients also ramped up in ram usage during that period.
00:07:12.090 - 00:07:28.140, Speaker B: Yeah. And one other thing we noticed was that Nethermind wasn't respecting the max blobs to store, so they were just basically eating up disk space until they just didn't have any. And they fixed that. Now.
00:07:31.710 - 00:07:36.730, Speaker C: They were using like 50 gigs more than every other client.
00:07:38.530 - 00:07:38.846, Speaker D: Yeah.
00:07:38.868 - 00:07:42.160, Speaker B: And it was a clear regression, so should be fine now.
00:07:49.400 - 00:07:50.550, Speaker C: I think you said.
00:07:52.600 - 00:07:54.852, Speaker D: Yeah, yeah.
00:07:54.906 - 00:08:10.450, Speaker B: I think Marcin posted about it on interop. I'll just try to find a message. Yeah, we mainly noticed because another notes were running out of space.
00:08:12.600 - 00:08:41.000, Speaker C: Yeah. Another thing is we began spamming. Not spamming, sorry. We started depositing 25,000 validators into Gurley. So this is just to check the main default for the max per apple activation churn limit, which should be set to eight. So currently it is eight on Gurley and it will be nine with 589,000 or something like that. Validators.
00:08:41.000 - 00:08:55.280, Speaker C: And we going to try to reach that in the oncoming weeks. And these are calculations. It should happen right after Sepolia and right before Holski forks dancun.
00:08:57.140 - 00:08:57.504, Speaker D: Yeah.
00:08:57.542 - 00:09:02.900, Speaker B: The deposits already done. If you check the pending queue, there's like 24,000 validators in pending.
00:09:06.280 - 00:09:22.730, Speaker C: Right. But we wouldn't reach if we see some mass exit. So we would like, ask every team not to exit any of their validators until we can test this churn limit. So that's like another two weeks or so.
00:09:29.400 - 00:09:32.580, Speaker A: And do we know when the first blob will expire?
00:09:35.240 - 00:09:37.056, Speaker B: It should be the eigth of February.
00:09:37.168 - 00:09:39.590, Speaker A: Okay, nice. I.
00:09:56.480 - 00:09:59.500, Speaker B: I think I posted a message somewhere, actually.
00:09:59.570 - 00:10:14.230, Speaker A: No, sorry. So this will be right after Hosky Sepolia is supposed to fork on the 30th. Right. So it'll be right after potentially between our two test nets and potentially after the last one.
00:10:18.660 - 00:10:26.390, Speaker B: Yeah, I think we basically had everything in plate to make a decision on the eigth of February about main net.
00:10:31.090 - 00:10:49.662, Speaker C: Okay, so the 5 February is going to be expiry, and the 30th we fork sepolia. And the week after that, we fork Oski, remember? So we should have expired blobs by the time we fork Oski.
00:10:49.726 - 00:10:50.310, Speaker A: Yes, exactly.
00:10:50.380 - 00:11:01.260, Speaker C: We should have the deposits processed by the time we also fork Oski. So we should be good with Folski. Oski should be just sending the check at that point.
00:11:06.410 - 00:11:07.350, Speaker D: Sweet.
00:11:12.060 - 00:11:16.280, Speaker A: Okay. And Alex is saying there seems to be some issue with blob scan.
00:11:18.240 - 00:11:26.380, Speaker C: Yeah, it's on our side. The issue is purely on our side. They have a guardian instance running somewhere.
00:11:28.560 - 00:11:40.080, Speaker E: Yeah, I was just looking last night and some things looked off or just were broken. But as long as we're aware, that's like the main thing we have, I think to look at blobs without hitting APIs.
00:11:41.780 - 00:11:42.096, Speaker A: Yeah.
00:11:42.118 - 00:11:44.812, Speaker C: Blobscan.com just resolves not girly.
00:11:44.956 - 00:11:45.312, Speaker D: Yeah.
00:11:45.366 - 00:11:54.020, Speaker C: So if you go to blobscan.com, you will get in top left corner. You can see that it syncs to the network is cited girly.
00:11:54.360 - 00:11:58.980, Speaker B: But I think that stuff is broken still. They're not showing any of the statistics.
00:11:59.140 - 00:12:28.370, Speaker C: Yeah, they said that they are fixing that today. It was some reorg issue that they were not able to process and they have some chart issue also. It should be all fixed by today, hopefully. And our RPC endpoint is offline now for Devnet solve because I had to resync the RPC endpoint and it hasn't finished yet.
00:12:41.270 - 00:12:42.130, Speaker D: Sweet.
00:12:46.110 - 00:13:15.480, Speaker A: Anything else on testing and general readiness that people want to cover? Are all the teams going to have a release out today or tomorrow? Ideally. Okay, I'll take the silence as a yes.
00:13:19.930 - 00:13:22.470, Speaker C: Are we expecting one release now for both forks?
00:13:22.550 - 00:13:25.690, Speaker A: Correct. One release for both test nets.
00:13:28.130 - 00:13:28.880, Speaker B: And.
00:13:30.850 - 00:13:32.800, Speaker A: We'Ll announce both in the same blog post.
00:13:36.040 - 00:13:41.380, Speaker C: Can we then just merge in the Hosky prs?
00:13:42.440 - 00:13:44.280, Speaker A: Which Hosky prs?
00:13:45.500 - 00:13:47.064, Speaker B: Yeah, I think there's just this one.
00:13:47.102 - 00:13:48.040, Speaker A: Outstanding.
00:13:48.380 - 00:13:49.610, Speaker C: There's just one.
00:13:51.020 - 00:14:02.220, Speaker B: Everything else should have already been. No, everything else has been marched in. This one's just a modification to set the max epoch churn limit to eight, which is the same as mainnet.
00:14:03.360 - 00:14:04.350, Speaker A: Got it.
00:14:05.440 - 00:14:07.230, Speaker C: We just didn't have it present.
00:14:08.180 - 00:14:08.960, Speaker D: Yeah. Okay.
00:14:09.030 - 00:14:10.860, Speaker B: The field was missing earlier.
00:14:11.020 - 00:14:11.440, Speaker C: Yeah.
00:14:11.510 - 00:14:38.650, Speaker A: Let's definitely merge it. It's been open for four days. Yeah, I'd say let's merge it today or at the latest tomorrow morning if there's any objection on. Yeah. Who can merge this? Who's like, okay, it's merged. Awesome.
00:14:45.180 - 00:14:45.832, Speaker D: Okay.
00:14:45.966 - 00:15:25.880, Speaker A: Mario has an update about invalid block with a k, not to be testing on clients. Sweet. And then, yeah, I would definitely keep our call two weeks from now so we can review how the testnet went, potentially have some blobs expire, depending on the exact time that happens, and then we'll see after that one if we want another one two weeks after that. That might be right before main net, so there might be value. Let's figure it out then.
00:15:26.410 - 00:16:11.000, Speaker B: I think there's one other talk if we have the time, I just want to go through, which is the attack net doc. So this is the tool where we can control a lot more parameters in the network. So you can imagine you can do network latency, packet drop, split network views, messing with time on the network and stuff like that. There's a couple of test scenarios we wrote down that we think would be a good use. But yeah, the question is still what's most useful for the client teams and what data do you guys want us to collect and how do you want us to show it to you and so on. So it would be great if you can have a look at the doc and then either dm me or leave comments or whatever you can.
00:16:32.210 - 00:16:33.098, Speaker D: Sweet.
00:16:33.274 - 00:17:03.810, Speaker A: Any questions? Comments? Okay. And yeah, it's probably worth posting attacknet more broadly in the r and D discord somewhere, so there's more ice on it. We'll do it. Okay, anything else before we wrap up?
00:17:06.260 - 00:17:08.080, Speaker C: When do we start testing pector?
00:17:09.620 - 00:17:19.860, Speaker A: Soon as we have implementations. We already have three eips that have been CFI, so I'm just waiting on client teams.
00:17:21.080 - 00:17:23.350, Speaker C: Any news from client teams on that?
00:17:26.980 - 00:17:32.390, Speaker A: No, I doubt anyone would seriously have started. I was joking. To be clear.
00:17:36.040 - 00:17:57.180, Speaker B: Have we heard back from any more l two s about when they're using four h four on Goldie other networks? Because I think we're also starting to see some more messages about hey, my library isn't working and so on. So I think starting earlier makes more sense for that.
00:18:09.090 - 00:18:12.990, Speaker C: Is the op stack Devnet going to be on gurley?
00:18:17.010 - 00:18:19.554, Speaker E: Yeah, gurley, because it's the only one that's forked right now.
00:18:19.592 - 00:18:21.540, Speaker C: Right, right.
00:18:39.380 - 00:18:40.240, Speaker D: Sweet.
00:18:46.570 - 00:19:14.670, Speaker A: Anything else before we wrap up? Okay, well thanks everyone. I'll be monitoring all the releases pages on GitHub and pinion you tomorrow midday, if I haven't seen anything there yet. And yeah, let's get the last few testnets done. Talk to you all soon.
00:19:18.240 - 00:19:21.660, Speaker D: Thanks, bye everyone. Bye.
