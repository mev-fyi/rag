00:00:04.080 - 00:00:22.085, Speaker A: All right, let's start things off. Welcome to virtual implementers call 17. This is issue 1024 in the PM repo. Thanks everybody for joining. Starting things off as usual with any client team updates. Would anyone like to start things off?
00:00:25.185 - 00:01:08.491, Speaker B: Yeah, I can start. So yeah, some weeks ago Custodian6 was launched. So some hours after it was launched I was deploying some contracts and mostly to check this Witness Explorer thing that I showed in the previous call was working correctly and for a particular transaction I saw something really really weird. At first I thought it was related to the Windows Explorer because it was pretty new, but then I realized that was a get back that we have. So yeah I fixed that and we relaunched the testnet.
00:01:08.603 - 00:01:09.255, Speaker C: The.
00:01:09.835 - 00:02:39.395, Speaker B: Yeah, the testnet and I think since then things are working correctly. After it was relaunched I redeploy all these contracts and tested that the effect was working and seems to be the case then Guillaume was syncing a new get node until like to have like a more recent state so we can do replay benchmarks and see again like performance bottlenecks, maybe get some insights regarding witness sizes and things like that. Because our previous replay state was pretty old so we will probably work on that soon. I think we still have to rebase some branches things like that. But yeah we are going in a good direction. And also we had some time to talk with many people involved in the EOF implementation as you share ideas regarding how a world where Merkle trees and EOF would exist and how they will work together. And later in this call I will do some small presentation to share those ideas and to maybe get more feedback from people.
00:02:39.395 - 00:02:42.075, Speaker B: That's it for me.
00:02:55.825 - 00:04:45.285, Speaker D: Any other I can go from Ethereum just side. So we basically tried to stateless sync the constraint in six network after it was relaunched and so basically we were able to sync most of the box but in few of the blocks we found some issue like 6128477544487 and so basically block hash access was missing on this and I confirmed that. So our block hash ring buffer was of the correct land of 8192 and then in 8915 block ext code hash was missing data. Basically we do isempty block check over there and which basically also looks into the balance and once and I think that data is not bundled as of now and this is something that we have talked about previously and maybe could be addressed in could be decided and addressed in future Testnet how it could be handled and in block 6989 basically for self destruct to beneficiary address ETHJS is doing is doing the beneficiary balance access which is not. Which was not present in the witness and this was also basically discussed with Guillaume and will be decided how to handle it in the future. So these are sort of updates from Ethereum. JS Gabriel, anything you want to add?
00:04:48.745 - 00:05:20.655, Speaker A: No, not much to add here except that we've fully migrated to the Verkle cryptography wasm. We used to use a Rust Virgo wasm but we've migrated to our own report that we maintained that uses Kev Andre's library as underlyings. So this paves the way for an actual vertical draw implementation which will be able to provide a stateful sync as well. So I'll probably begin working on that as well. So yeah, that's it for my addition.
00:05:25.675 - 00:06:02.415, Speaker E: I think I can go next regarding the Costnet so another mine failed on block around like 30,000. I think I know what the issue is. I'll debug it probably this week. Except for that we are working on transition. So I ran some tests locally and computer test and everything seems to be working fine. Next we'll be testing the transition on Kertosis because as far as I remember I'll just mention that you can now test transitional kertosis. So that's the next step and once I fix the bug for the question and test net then I'll resume testing the vocal sync as well.
00:06:17.235 - 00:06:19.015, Speaker A: Awesome. Anyone else?
00:06:21.485 - 00:07:12.005, Speaker F: I can go for Nimas. So Advaita and Agnish are not present. I'll represent them. So they have made progress with integrating or implementing local in our execution layer clients. So we have a problem in our client that there's some technical dev there around the database area. So that's what's been sort of slowing us down during the last few months. So we decided to split the effort into two basically and the first is to kind of sort of circumvent the current database and just come up with something very basic for constant integration.
00:07:12.005 - 00:07:48.425, Speaker F: It won't be used in mainnet, it will only be used in Constantin. But at least we can proceed with the integration going forward. We're thinking of brand new modern thinking. Started implementing a brand new database that will support both Verkel and Merkel. Ground up with a nice feature that it will be very storage efficient. So that's an ongoing effort. It will take a while but after that hopefully like we'll be able to really support Verkal.
00:07:48.425 - 00:08:47.625, Speaker F: Totally. So next up for us is continuing like with the basic effort to integrate with consonant and then we'll also change the gas costs. We've also been looking at the E2 layer, at the consensus layer to integrate as well into Constantine. And there we encountered some other issue that like apparently worker is currently based on the Capella fork, which is a bit problematic for us. It's. It's a bit buggy. So like I'm asking that on behalf of whether there's any plans to switch to the Deneb port in the consensus layer.
00:08:49.525 - 00:08:50.785, Speaker G: Not immediately.
00:08:53.485 - 00:09:03.145, Speaker F: Okay. Because that would have made our lives easier and we could have integrated it quicker on top of the nab.
00:09:07.285 - 00:09:49.205, Speaker G: Yeah, that's understood. But unfortunately guess is is going to have a hard time rebasing. I mean, by the way, this is something that I would like to add to the to the Go update. We have started merging some PRs into Master, but everybody is aware of how long it takes to get stuff merged into Geth. So until this happens we would have to go through yet another, if not like two really difficult rebases and time is better spent somewhere else. So I understand the discomfort, but. And we would like to fix that, but it's going to take at least a couple months.
00:09:50.345 - 00:09:51.165, Speaker C: Okay.
00:09:52.465 - 00:09:55.045, Speaker F: All right. Well, that's it from our part.
00:10:04.995 - 00:10:05.491, Speaker H: Cool.
00:10:05.563 - 00:10:06.615, Speaker A: Any other updates.
00:10:08.955 - 00:10:12.275, Speaker I: I can. I don't know if you already shared something, Joshua.
00:10:12.435 - 00:10:14.075, Speaker A: No, I have not. Thank you again.
00:10:14.235 - 00:10:54.585, Speaker I: So, just on business side, we started to join the testnet, the Devnet. So we have a gas cost issue in the block 645 in a specific transaction. So I asked if someone can share me the trace of the transaction to understand what is the difference. Because I tried to look at the modification in the spec and I don't see any problems. So I think I am missing something. So yeah, if someone can share a trace, it will help me. We have also Thomas that is working on some optimization in Bezu.
00:10:54.585 - 00:11:14.045, Speaker I: We have more people joining the team. So we have Gary that will help me to work on gas cost and he will also start to look at the proof implementation with. So yeah, pause moment. Mainly trying to change the testnet. Yes, Guillaume.
00:11:15.825 - 00:11:25.033, Speaker G: The trace you need like did I understood correctly that you. You just need a gas cost? Because if you do, you can use Ignacio's block explorer.
00:11:25.169 - 00:11:25.417, Speaker C: No?
00:11:25.441 - 00:11:34.885, Speaker I: Yes, the gas cost of each opcode. Because I look at the explorer of Ignatio and I don't see any difference. So I think I'm missing something else.
00:11:36.025 - 00:11:36.425, Speaker B: Okay.
00:11:36.465 - 00:11:47.325, Speaker G: Weird. Yeah. Okay. I can try to generate you this trace. But yeah, I need to go home first and it's going to be like Wednesday.
00:11:47.945 - 00:11:55.005, Speaker I: Okay, no problem. I will continue to look. Maybe I will find something before. But yeah, thanks.
00:11:58.465 - 00:12:12.865, Speaker B: I should also say that that blog number that you are referring to is exactly the block number where I deploy, deployed and executed this more complex contract. So it is probably something a bit more weird.
00:12:15.365 - 00:12:16.785, Speaker I: Yeah, I think.
00:12:19.525 - 00:12:19.877, Speaker H: Yes.
00:12:19.901 - 00:12:25.185, Speaker E: I was just saying that I think I can generate the trace and send it on the channel by tomorrow probably.
00:12:26.405 - 00:12:27.785, Speaker I: Okay, thanks.
00:12:35.935 - 00:12:54.915, Speaker A: Cool. If no other updates, we can move on. I don't think there's anyone from the testing team here this week. I shared their update, their abbreviated update in the chat. If anyone has anything else they'd like to add as far as the testing side of things go, otherwise we can.
00:12:56.725 - 00:13:19.421, Speaker G: I don't know how different it is from the update you shared. Let me just quickly have a look. Yeah, basically they want to. Yeah, they start using the new. The new input that we gave them seems to work, but they've been delayed. They will start. They're trying to get something ready by the interrupt.
00:13:19.421 - 00:13:20.905, Speaker G: That's what he wrote me.
00:13:22.775 - 00:13:36.155, Speaker A: Perfect. Cool. Okay. Well then I guess next up is the captain updates V6 status and V7 wish lists.
00:13:41.695 - 00:14:19.915, Speaker G: Yeah, I think but I've been disconnected from things that V6 doesn't really have any problems, so. Okay. Galindo reported some issues that we can discuss if need be. Otherwise we can discuss them offline. For the seven my wish list would be to implement. Well, okay, first confirm that there are some issues on the DevNet 6 and also, and also fix them. But I know AAVE and Uniswap were talking about, you know, running their scripts to Deploy on the testnet.
00:14:19.915 - 00:15:01.075, Speaker G: So I would like to do that on DevNet 6 if we could because I think it would be. It would be quite useful to find some like more bugs and try to keep Defnet 7 as the most. As the most stable testnet we've had since number three, I think. And yeah, another thing that I would like to have on the DevNet 7 would be try to implement the field cost which so far was not, was not implemented. I think it's. I think it's time to tackle this before, before the interrupt so that we can have some proper estimate of what the gas costs really look like.
00:15:11.415 - 00:15:12.195, Speaker A: Cool.
00:15:13.015 - 00:15:13.487, Speaker G: Okay.
00:15:13.551 - 00:15:22.735, Speaker A: If nothing else on that topic. Next up, a presentation from the Portal Network team. Milos.
00:15:24.115 - 00:15:25.459, Speaker H: Hi, can you hear me?
00:15:25.627 - 00:15:26.375, Speaker A: Yes.
00:15:26.795 - 00:15:31.295, Speaker H: Okay, I'm. I'm sharing the screen. Can you see the slides?
00:15:34.555 - 00:15:35.299, Speaker A: Yep, perfect.
00:15:35.347 - 00:16:02.469, Speaker H: We can. Okay, cool. So I already shared the link. Maybe somebody can also add it to the chat. I share it on a discord channel as well. Basically I'm doing a research on how the worklet state can be stored in a portal network, meaning the archive state, entire archive state. I will give a bit of background on what portal network is for the one that are not familiar.
00:16:02.469 - 00:17:02.383, Speaker H: Basically the portal network is decentralized API for accessing the archive node kind of API for the Ethereum state. It's actually three networks in it or sub networks, beacon history and state. And part of this research is focused on the state network. Currently what is being in development is the is the state markle current basically current state of the Ethereum network and how it works is that every node in the network has a node ID and every content has a content id. And basically every node stores the content that is close to its own id. That way you know where to look in the network specific content like you know which other nodes is very likely to have it. And for the Merklet tree and the current Ethereum state, there are three types of content.
00:17:02.383 - 00:17:59.255, Speaker H: The accounts tree node, which is basically the tree node of the main tree contract storage tree node, which is the tree node that belongs to contract storage and the contract by code. So very simple structure, nothing too complicated there. I'm going to skip the slides for the worker tree because I assume everybody here is familiar with that. So the naive approach for the worklet case would be to basically do the same store every tree node, but that has a problem. The problem is also present with the Merclet tree, but is less noticeable or less impactful. And the problem is that if you have a node, the tree node that is updating only one element from one block to another, the content ID will be completely different. So it will store it on some completely different space in the network.
00:17:59.255 - 00:18:59.809, Speaker H: But you are duplicating how much storage the whole portal network actually needs. And if the branching factor is 256 as the case of the workload tree, you basically have a lot of waste in memory in that way. For the Merclic tree, the branching factory is 16, so that's still present, but not as as noticeable, let's say. And that's one of the main problems. And the solution that I came up with was to split each node basically into two layer mini kind of tree. So every node is sort of having 256 children. There will be two layer nodes where basically they would accumulate the witness thanks to the patterns on hash and where basically on the lower level each node will just be a 16 consecutive values and the one level above will just combine and add them.
00:18:59.809 - 00:19:57.165, Speaker H: We can see that the others are basically just pedestal hash with 16 elements and everything else being set to zero and the last commitment is just the sum of them. So there is no pedestal hash involved. There is needs to be a bit of, let's say careful engineering here because the commitments that are being passed through on the lower levels are actually the elliptic points, not the scalar field values. But it should work, it should be possible to do it that way. This process can again be split into three or four layer trees with four elements. But that also increases the lookup value on the portal network because you would have more layers to go through to find the value that you're looking for. But it's something that might be explore of how long that would take versus how much saving in memory would actually bring, et cetera.
00:19:57.165 - 00:21:26.875, Speaker H: Another approach that was being considered which is more similar how the execution layer clients stored the state and that is that the same path, the same node on the same path would be stored within the same portal network node or client. And so basically the same node will be responsible for the whole versions of the same three node. That one breaks one of the main assumptions and that is that the hotspots that it creates hotspots. Basically some nodes would have way more activity for particularly popular like states that are updated very frequently. And basically that breaks like a symmetry and linear growth between the radius that portal network node wants to store and the storage size it actually will need. There was also an idea that we basically discussed even for the mercla case is to together look at the epoch where the epoch not in a sense of Ethereum layer network, but portal network kind of epoch, where the epoch would be longer period and just the nodes would store the three nodes within that epoch and the local changes. So basically something in the middle.
00:21:26.875 - 00:22:20.195, Speaker H: This has a problem because every time there is a change from one epoch to another, you have to duplicate the entire three states across the network, which is very bad. This is probably some approach that you might consider more when we go into storing only the header of the state, which will be probably stored in a different way and optimized for that specific use case. And this is probably the path that we will go for that. But for storing the archive state this is probably not good enough. And that's relatively short presentation from my side on what we are doing and how we plan to go forward. Basically I just wanted to give an Update and maybe ask if there is anything that anybody sees, let's say wrong with this approach or any suggestions or anything like that.
00:22:23.865 - 00:22:26.405, Speaker A: Looks great. Guillaume.
00:22:27.825 - 00:22:53.345, Speaker G: Yeah, I have a couple of questions. So, you know, the whole structure was basically made so that it would make things easier for the portal network. So precisely because you requested this, like this is why we end with this tree structure. So I'm just a bit confused why you need to come up with these two layered tree structure. And also isn't that going to be extremely expensive to compute proofs?
00:22:55.845 - 00:23:41.665, Speaker H: Well, it depends. Like the thing is, like, I'm not sure what we will need proofs for that. The first thing, the second thing is if you have a tree node and most likely from one block to another, only one value will change in that three node except the root and maybe a first layer or first two layers. So then you want. You will have to duplicate the entire 256 values and store it somewhere else in the network. And that's a very high duplication factor of how much total storage the whole portal network would need to keep the entire archive state. So I guess that's why that's, that's the benefits of, that's the idea of splitting it into multiple layers.
00:23:41.665 - 00:23:45.585, Speaker H: And what was the other question?
00:23:47.765 - 00:24:09.595, Speaker G: Yeah, just why? I mean the. Okay, the proof. Right, you answered the proof. I mean my question was like, are you, are you no longer interested in the structure? You lobbied for us. Okay, maybe lobbied is a big word, but you asked us to, to adopt, basically.
00:24:11.815 - 00:24:14.875, Speaker H: Can you clarify which one?
00:24:15.975 - 00:24:59.485, Speaker G: Right, so this whole single tree structure with 256 values and everything merged into a single tree. That was a request by Piper. Right. The reason why we have this, we no longer use the per account. I mean there are. Okay, now we found that we would, we could adopt this structure for our own purpose and make a lot of optimizations based on it. But until Piper made this request, the whole point was that like the structure of the tree used to be an account tree and storage trees below them.
00:24:59.485 - 00:25:17.075, Speaker G: So we changed the whole structure to, to adapt it to the portal network. I mean, I'm just wondering, are you suggesting we change the whole structure of the tree ourselves or is it just an internal portal network representation? That wasn't clear to me. This 16.
00:25:19.175 - 00:25:58.815, Speaker H: Right, right, right. No, this would be just the portal network internal representation. So how the structure would be stored within the portal network. And this approach, I can probably go back there with this approach. This would basically represent one node of the worklet tree, how it will be represented within the portal network and the end up commitment would be the same as it currently is because it's just linear algebra combined together. So there will be no change or this doesn't require any change of the worklet tree structure. This will just basically be how it will be stored within the portal network.
00:25:58.815 - 00:26:51.421, Speaker H: I now understand what you mean between like embedding the state, the smart contract storage within the same tree. I actually wasn't there, I wasn't part of the portal network at that point, so I'm not really sure all the reasons. I mean it definitely helps us from having more unified storage across different areas and stuff. So it's still useful. This proposed change doesn't actually affect anything regarding the workletry design of the like for the execution clients or anything like that. This is just how we would actually store it within the portal network. And regarding the proof, it would basically be that we in portal network would most likely not use this kind of proofs.
00:26:51.421 - 00:27:57.335, Speaker H: We have a system where currently with merclet tree, whenever the data is gossiped within the portal network, the data that is gossiped has to be proven that is valid. And the way it currently works with the mercletree, you pick one leaf that you want to store that was updated and then you gossip basically that leaf and all of its parents at once. And how it works is that the node responsible for that leaf would receive the leaf, verify the proof and then it will just remove the leaf and gossip the parent of that leaf to the rest of the network. And that would sort of like precursor propagate all the way to the root. So basically when we gossip we don't actually. We actually want to have all of the parents already provided there so that we can recursively gossip them. And for the worklet tree I guess we would probably preserve the same logic which means we don't actually benefit much from the from the ZK proofs that we can actually get from the worklet trees.
00:27:57.335 - 00:28:35.265, Speaker H: We didn't do much of the work in that area of whether we can optimize something there. But that's the initial thinking. We would still just propagate the leaves and all of the parents with a full with the full content such that we also have to. Because we also have to propagate them through the network as well anyway. So that's kind of like the thinking that we might actually not. But I agree this kind of proving scheme would probably not work if you want to concise proof for the in a later stage and you will still have to iterate through the tree to find the node that you're looking for.
00:28:37.955 - 00:29:09.173, Speaker G: Right, okay, thanks for clarifying another question I had. But you already kind of touched to it just now, you know, like the top of the tree. Like your problem is the churn at the top of the tree. But we already have this kind of information attached to the block, right. So if you see your information inside the block, that means it has been touched. But if you don't see it there, it's. You need to find where it was last touched in the block.
00:29:09.173 - 00:29:43.425, Speaker G: So yeah, you could, you could like if you want to write an archive node or. Okay, not a node, an archive network, you would be able to get those nodes directly from the proof. So you could get it by doing some archive of those proofs themselves instead of reinventing your own, your own method. I mean it's just food for thought for you. You know best what you need. But maybe there's some potential saving for you either in design or storage by looking at what's in the blocks already.
00:29:44.645 - 00:30:55.105, Speaker H: Yeah, I briefly was thinking about that. The problem is that the portal network is designed such that the nodes in the network can join and live kind of randomly and they were not going to go and fetch all historical blocks and keep updating themselves. So there has to be a mechanism where you can just Jo and then the content that is close to your node ID should be given to your city, to you through some means. And you can keep following what is happening currently with the head of the network. But if you disconnect like there is no strong requirements or expectations that every node will observe every block header all the time and have everything updated. And that's why I didn't actually find a way to actually fully use that information in this kind of distributed structure. But definitely something that I was looking into and still thinking about like, because we will have, we call them bridges that will actually going to seed the data into the network and they are probably going to use this information very heavily.
00:30:55.105 - 00:31:03.885, Speaker H: But from individual modes point of view, it's a bit harder because they don't have the same expectations I'd say from them.
00:31:05.745 - 00:31:27.705, Speaker G: Right, yeah. One last question in this regard. The churn that you expect how like what is the depth that you expect it to be a problem? Is it like depth one, depth five? Depth? Well, five is going to be hard to reach but for a vertical tree. But yeah. Do you have any idea of where the churn starts being a big problem?
00:31:29.445 - 00:32:25.965, Speaker H: I mean nothing seems to be a big problem per se, because like I'm not sure what you mean by a problem because we will just store it the way we kind of designed the storage layer to work. The problem is the idea is that if you want to find a balance on some archive states like million blocks away in the past, you would know the state route and you would have to traverse the tree. And you would do that by issuing the queries through the portal network for fetching each tree node on the way to find the leaf that you're actually interested in. And if we do this splitting, then the deeper the tree is, the more query you need to make on the portal network. Meaning it will take longer time to find information you're looking for. So that's a trade off. Like technically we can go all the way to the binary tree, but then that would just take way too long.
00:32:25.965 - 00:33:02.515, Speaker H: And on the other hand, if you have like a big branching factor, then you increase the demand from the whole portal network storage, like combined storage of the entire portal network. You increase the requirements of how much it should be to support everything. So there is like a trade off on latency versus the total global storage. There is no like a strong requirements that it has to be smaller than this or bigger than this. It just like depends on like a preferences and trade offs.
00:33:05.455 - 00:33:23.915, Speaker G: Okay. Yeah, I'm not trying to redesign your entire thing right now. Well, you putting you on the spot. One last question I would have, but you know you were also talking about potentially meeting in, in Prague. Would you by any chance be in Berlin before or after?
00:33:26.055 - 00:33:37.755, Speaker H: I, I'm not, I don't think anybody else from the team would be there. I know the entire, almost entire portal network will be in Prague, but I don't, I'm not sure if anybody would be in Berlin.
00:33:38.425 - 00:33:39.245, Speaker G: Okay.
00:33:39.785 - 00:33:40.121, Speaker C: Yeah.
00:33:40.153 - 00:34:16.475, Speaker G: Okay then let's see. But yeah, I think there's some iterations like we could try to iterate on that if you're interested. One question, last question I had because I don't want to hog the entire call. Would it be possible for you guys? Do you really need to have every single block or could you just have the history for every, say every thousand blocks and then ask people to resync the node based on the proof from the blockchain? Would that be a model that would be realistic for you guys?
00:34:17.975 - 00:35:17.307, Speaker H: I mean we would have to think of the use cases of like, it depends, like how exactly would that be stored? I guess we currently weren't thinking in that direction. That's one of the ideas that something similar, let's say is this idea of Having epoch and path, let's say every 10,000 blocks or something. And then every node would store the state of a tree node at the beginning of that epoch and all the consecutive updates. The issue with that is that we encounter is that we would have to reduplicate at the beginning of every epoch. We would have to reduplicate the entire basically state where basically we would have too much snapshots that we will store across like the entire portal network. Like we can. We will still have to do some calculations to figure out how much exact.
00:35:17.307 - 00:36:01.895, Speaker H: Because as you keep adjusting the epoch, like making it bigger, that problem becomes smaller. But then the problem of the hotspots and having some notes having to do way more work than the others becomes more exaggerated. So there is also a trade off in that design. So it depends on the use cases. What exactly would that be? It's something that we kind of want to explore. But this approach of using the path for the content id, not just like a commitment is something that we are most likely going to pursue for storing the head of the block. That way you can achieve maybe off one request for the head state.
00:36:01.895 - 00:36:10.895, Speaker H: And that's something that we are. That's going to be a completely separate discussion and research. And that area, it's a bit lower priority for us at the moment.
00:36:15.075 - 00:36:16.443, Speaker A: Cool, awesome.
00:36:16.619 - 00:36:17.643, Speaker H: Thanks a lot.
00:36:17.819 - 00:36:40.945, Speaker A: Thank you, Milos. Okay, since we only have 22 minutes left, we can try to get to the rest of this quickly. Next up on the agenda, there was a topic around the 32 byte chunking ideas for Verkal, I think Ignacio, maybe was this you on the agenda or you.
00:36:41.885 - 00:36:43.021, Speaker G: I think it was me.
00:36:43.133 - 00:36:43.709, Speaker A: Okay. Yeah.
00:36:43.797 - 00:37:09.115, Speaker G: Interrupt. Interrupt me if you had any. Yeah, if you thought that was you. Yeah, basically not much to say. What. I accept that in order because of all this EOF considerations we had, I was. It's something that had been talked about with Pavel a long time, a long time ago.
00:37:09.115 - 00:38:18.973, Speaker G: I wanted to finally take some time to try to implement it, to see if we could get more information as to the benefits of packing everything, both to evaluate the impact that UF would have and also to see if it wouldn't be an optimization that is interesting for legacy bytecode as well. So I want to do this. The question I had for the community would be do we want to put the. So in this model, what we would do would be to put the byte. Instead of putting the byte at the beginning of each chunk, like the push data byte at the beginning of each chunk, we would put that in a buffer somewhere and the question is, where should this be? Either we put it at the beginning of a 128 block. Sorry to like the first 128 leaves. The problem is we have to somehow store the actual start offset.
00:38:18.973 - 00:38:41.925, Speaker G: Or I guess we could compute it from the size. That would be possible. But then we probably that has a gas cost impact because we would need to access the size every time we need to touch the code, which is not currently the. The case. Sorry. Actually it is in the current eip but would not necessarily be the case in the. In the future version of this eip.
00:38:41.925 - 00:39:17.841, Speaker G: Or we could put it in the reserved. Well, some of it, because that's the problem. We could put some of it in the reserved leaves of the header group. But in case, even if the code is currently 2024 kilobytes, we would not. We that would not fit in the. In the available space. So we would need to find a way to overflow or we add yet another offset and then that means that every time we read the code we have to go get it somewhere else.
00:39:17.841 - 00:39:34.255, Speaker G: So that's an extra cost. I honestly, I'm not sure which one is the best approach. They all have their drawbacks. I wanted to know if anybody had any opinion as to where this section, let's call it section would have to go.
00:39:41.075 - 00:40:14.285, Speaker B: I mean, I agree that it's very hard to predict. Probably we will need to test the two ideas of like putting this information in the header and not put in there. Because yeah, it kind of depends on the contract, I guess and the length of the contract. Right. Because the amount of bytes that you need is. Depends on how many chunks the contract has. So yeah, it sounds like it will depend on each contract, really.
00:40:16.465 - 00:40:16.913, Speaker C: Yeah.
00:40:16.969 - 00:41:16.875, Speaker B: And also like another thing, another thing is that if we pack together each metabolism bytes, my feeling is that whenever you read one of them, you will be indirectly like loading the metabyte of tanks next to each other. And I don't think that's entirely useful because this metabolic byte is only needed whenever you. You do a jump to really check that this jump is correct or valid. And the chance that you, whenever you execute a chunk, you will jump to a chunk that is really next to that chunk. I know how probably probable that is and how much you will leverage this other metadata bytes that you have read already. I know if that makes sense. It was a bit of a tongue twister, but.
00:41:17.735 - 00:41:28.383, Speaker G: No, no, it's only. It's totally understandable. Yeah, that. That's the biggest problem. And in fact that's the. That's the problem with the whole section reading in eof. I mean, this is what we.
00:41:28.383 - 00:42:12.955, Speaker G: I'm sure we will discuss that in just a few minutes, but I think it's still worth being tried so that we. We have concrete data. So I don't know where to start, but I'm afraid the answer is we have to start all two or all three, depending on how you look at it. I'm just not sure where to start really. I think, yeah, I think I will first try to do it, put some of it in the header and then somehow use the same technique for the overflow. Yes, it's definitely going to be extremely expensive to. If you have to read an extra section just to every time you read a new chunk.
00:42:12.955 - 00:42:37.975, Speaker G: Sorry, a new chunk every time you read another chunk. And in fact this is exactly what Vitalik said in the EFV0PR. But yeah, okay, let's say unless there's another anybody else disagreeing, I will start with that. I will put it in the account header and at least it will serve as a base point and we can try to optimize afterwards.
00:42:44.755 - 00:42:51.815, Speaker A: Cool. Okay, last up on the agenda, a presentation on Verkal and EOS from Ignacio.
00:42:54.195 - 00:43:01.737, Speaker B: Yes, I will share my screen. Can you see my screen?
00:43:01.931 - 00:43:03.145, Speaker A: Yep. Perfect.
00:43:04.085 - 00:44:05.735, Speaker B: Okay. Yeah. So these last days we have been chatting with some people from UF, trying to exchange a bit of our thoughts regarding how a world where Verkol and UF will live together will look like and which are like some concerns and things like that. So the idea of this presentation is mainly like sharing all these ideas for others to be aware. So a bit of context that probably everybody knows already, but EOF is being considered for Prague and Verkol is being considered for Osaka. Prague comes before Osaka. So the idea is to, yeah, try to see how these both AIP interact, interact with each other and try to see the impact of them on Verkal users, on Ethereum users, on the Verkle Tree spec implementation and also see if this might put some.
00:44:05.735 - 00:45:27.865, Speaker B: Create maybe some problems for UF adoption. So it's mostly like an exploration of all these topics to give like a really, really quick DLDR umbrella trees only like focus on the discussion of this presentation. The main motivation of Oracle Trees is to make stateless clients viable and for that to happen, Oracle Trees needs to introduce this concept of an execution witness, which is basically all the data that you need for replaying a block in order to verify it because stateless client do not have any Ethereum state. So you need to provide them all the needed data to execute the block. The important part for this topic is that Burkle trees now include the smart contract code as part of the state in the state tree, which is something that it isn't true today. The smart contract code is usually like in the client's database, so it's not really in the tree. So with that point in mind, there are like two really obvious conclusions.
00:45:27.865 - 00:46:29.625, Speaker B: The first one is that the bigger your contract is, the more gas you will have to pay. At least compared to the gas that you pay paid today, which is 200 per byte because you had to write all this code into the tree, which is more expensive. So that's the first one. And the other like insight in quotes is that whenever you execute a transaction, the more bytecodes that your transaction execution needs, the more gas you will have to pay because we have to include all this bytecode in the execution witness. This is a cost that is like new for users today. They don't have to pay for all this, of course. And this is like something that contract developers and compiler devs will have to think about because it's like a new optimization dimension that they should account for.
00:46:29.625 - 00:47:24.319, Speaker B: But the bottom line for Burkle trees perspective is bigger contracts and needing more bytecodes will mean paying more gas. Okay, so that was like a TLDR for broker trees. If we do the same for eof, which I'm not an expert on, so have that in mind. The idea of EOF is to add more structure to the smart contract code. So it introduced some new sections like the header and type sections. It also includes a lot of new instructions and ideas to make the code execution more efficient. More efficient and also allows the client to avoid doing some random checks, just like the jump desk analysis, because I think that every jump could be valid.
00:47:24.319 - 00:48:49.305, Speaker B: So that's like a reasonable amount of work that clients shouldn't do at runtime. And all this structure allows also to make like the EVM like spec more, evolve better because you can add and remove features or instructions without breaking existing code. So all these benefits have also or might also have some effects such as like maybe bigger code because we have to include like new sections. And also maybe having to read more bytecodes while executing transactions since we might have to read totally or partially these new sections. Okay, this is like open questions. So if we merge together these two insights of UF and Burkle regarding gas concerns, we could say that EOF might increase the coal size and might need more by cost to be read during contract execution. And these two things are exactly the things that Burkle trees make more expensive because we need to add the contract to the tree and also include the vehicles in the witness.
00:48:49.305 - 00:50:48.635, Speaker B: There are a lot of caveats here because eof, as I already said bring new efficiencies. So for example the shamdesk instruction can be avoided. So the contract size might be smaller from that angle and that might offset these new headers that will be included. Also there's like a lot of work that compilers can do to really optimize for the code layout or how these UF sections are placed in the tree such that you have to read the fewer number of chunks possible. Now how much all these efficiency gains or potential optimizations can offset the new costs. Like the thing that we are trying to figure out, a potential idea to explore this a bit is to try to come up with a client that have both at least a minimal implementation of EOF and BURQL trees, launch a testnet with that client, or maybe just run a simulated chain locally and try to deploy a set of reasonable contracts both compiled with legacy code and with eof and just like execute transactions on these contracts and compare the GAS costs. So ideally we will like to have compilers that are already accounting for both EOF and Verkle trees optimizations, because even if you have compilers optimizations only for vertical trees, there will be missing considerations that will make sense for UF and if you have only EOF compilers that have optimizations only for UF that will be missing all the cold chunking optimizations.
00:50:48.635 - 00:52:15.735, Speaker B: So asking for this is very hard because this is a lot of work and compiler teams have really probably limited bandwidth. But yeah, at least even if we don't have these optimized compilers, at least with this kind of test we can have like an upper bound of the GAS difference. And if we try to do this kind of testing, maybe we can have some feedback loop to improve the EOF and Burku specs so they can place better with each other. There's another angle apart from GAS cost risk, which might be EOF adoption risks, which this entirely depends of if EOF versus Legacy has a big GAS difference. So kind of depends. But if there's a GAS difference that is reasonable, maybe users will simply try whenever they deploy a new contract to deploy with legacy code and with EOF and simply do the deployment with whatever setup it uses less GAS for them. And that might be a bit annoying because this might make harder to stop accepting legacy code in the future, which I guess is like the idea, because if that isn't the case, we will have to accept that we will have two modes of contract execution forever.
00:52:15.735 - 00:53:39.015, Speaker B: And I think that will be, I mean not entirely nice for complexity reasons for clients, I guess not only for full clients, but also for stateless clients. We should also be aware of these like two modes of contract execution. Regarding Merkle tree complexity risks, one open question which is a bit related to what Guillaume was talking about before, is we have to figure out how code chunker will work for eof, which will probably be different than for legacy code. So there's like a chance that we will have like let's say double spec for legacy and EOF executions in the vertical tree spec because depending on the contract type we have to explain how chunking works differently and things like that. Also EOF introduces a bunch of new instructions and depending on these instructions we will have to figure out probably new gas costs for them if there are any that indirectly risk the tree state. For example, today instructions like balance and block hash indirectly require state from the chain. So these instructions also require some extra gas cost considerations because they increase the witness size.
00:53:39.015 - 00:55:17.345, Speaker B: And finally, due to the complexity of eof, there's like a reasonable chance that this can delay Osaka Fork. And while this happens, the state growth is still of course happening and maybe the market, the state transition between the Merkle Patricia tree and the purple tree will take a bit longer. And yeah, we will, we will prefer this to take the short amount of time as possible because there's a lot of logic happening and there's like a risk here. So I think that it's fair to say that it might seem that EOF and Burkle tree implementations are non overlapping because they are similarly touching different parts of heights. But the point here is that their effects are really composing with each other, mostly like the gas cost effects. I think that the original UF assumption was that doing all this code structure changing wouldn't affect much gas cost because the code wasn't really part of the tree. But since Verkol trees need to include calling the tree for stateless clients, then we have this situation in which These seemingly different EIPs interact with each other at the gas layer.
00:55:17.345 - 00:56:26.495, Speaker B: Like an open question is maybe more minimal version of EOF can help here? I don't know. Kind of depends what a minimal version of UFO even means and if it really like helps with any of this problem. Just to clarify, again, this presentation isn't about saying that there is a problem or a big problem. The only thing that we are trying to share here is that maybe we don't know and a lot of these questions might have like it depends answer and it maybe will be helpful to really find out if any of these things are things that will backfire in the future. And yeah trying to have more like real data to support or simply discard these concerns. So that's it. And also like the idea to share all this with other core devs and get their feedback and ideas about this these things.
00:56:38.835 - 00:56:40.895, Speaker A: Any questions or feedback?
00:56:41.675 - 00:57:08.245, Speaker J: Well, I have a question. You mentioned that this is the first time and I got away of the EOF and you mentioned that and the goal of EOF is to no longer keeping. No longer keeping the codes in the tree in the Virgo tree. I mean no longer kept in the storage. So how could a block builder.
00:57:10.545 - 00:57:10.833, Speaker C: A.
00:57:10.849 - 00:57:39.285, Speaker J: Block builder to know which code always transcript to execute or to verify. Because then because even if the block the block builder owns the whole story the entire storage and there's no code included in the tree. So how did the block builder know which code will be triggered in the transaction to build a block?
00:57:41.745 - 00:58:01.767, Speaker B: So I'm not sure that question is related to this topic. Like block builders will have all the states of the chain so if the contract is an EOF contract or a legacy contract they will know anyway. And I'm not sure this is related to EOF or vertical really not not.
00:58:01.791 - 00:58:24.317, Speaker J: For Verkho just to make sure that if there's, if there's no code, no code starting the trees, installing the storage. So also in which form whether could be presented to the block builder or.
00:58:24.461 - 00:58:45.905, Speaker A: Something else maybe just since we're at time we can continue discussion Async unless anyone wants to. I'm curious I guess to stay on the topic of EOF if anyone else here not to yeah cut the conversation off but I know Dano is here and I don't want to put you on the spot Dano, but curious if you have any quick thoughts.
00:58:46.245 - 00:59:23.035, Speaker C: Yeah are the slides available? Because there are some things I can clarify. I think the biggest thing to mention about EOF is that it's core, it's just a container format. We don't change fundamentally how the EVM works. So you know one of the big concerns about how are we going to keep two versions of evm it's going to be the same operation execution loop and you're going to be asking the same questions like instead of asking well am I Shanghai? Am I Byzantium? When you Operate an opcode, you might have an extra check to say is this frame eof? If so, execute. Otherwise don't execute. A lot of the core logic in there is going to be absolutely the same. That was one of the goals of eof.
00:59:23.035 - 01:00:50.207, Speaker C: The big goal of EOF was to build this container format for the EVM so we can do things like add operations that use immediate arguments, which is impossible to do safely with the current legacy system. There have been, you know, high level requests that include other large goals of the EVM future into it, which include things like removing the ability to introspect code and removing the ability to introspect GAS while you're running, which will open up a lot of things like the ability to willy nilly change the GAS schedule without fear of breaking contracts that are deployed. If you can't read the gas, then the solution is just always provide more gas. So those were some of the things, that's why, you know, it looks like there's a lot of opcodes, but what we're doing is we're just replacing opcodes that in a way that can handle this container format and they operate exactly the same, just they might use immediates, they might not have GAS introspection, they might need to use a data section to get their embedded data. So that's where a lot of these things come from, is taking existing behavior patterns and containerizing it. There was a prototype Mega EOF compiler written by Solidity before Mega eof, before big EOF got pulled from Shanghai. And in some tests I've done there I've seen between 1 to 3% gas, not gas opcode size reductions of bytes.
01:00:50.207 - 01:01:28.603, Speaker C: But I don't know how that's going to translate to operational. There may be less bytes, but there may be more operative more effectively. So the biggest open question we won't really get answered until we start getting quality implementations from Solidity and from Vyper against the current EOF spec. I'm hopeful it's going to be less, but it might be large, but it won't be, you know, anything outside of the 1 to 3% I'm expecting. Either way it's, you know, going to be statistical noise for a lot of these. So I'm not expecting any major large size or operational GAS cost gains. Anything that is costed in the header tends to be offset elsewhere.
01:01:28.603 - 01:01:44.535, Speaker C: But because of contract size restrictions you really can't, you know, multiply those gains within a contract. So it's, it's going to be mostly a wash is my expectation. I think Those are a lot of the high level concerns out of there that what my expectations are going on right now with.
01:01:46.925 - 01:02:07.221, Speaker G: I know we're over time. We please share the last slide again because there were a couple of questions that I'd like to go over. The fact that it doesn't change. I mean the fact it doesn't change the anything about the execution is demonstrate Demonstra. Sorry, demonstrate wrong. But okay, how is it wrong?
01:02:07.293 - 01:02:08.745, Speaker C: What are we fundamentally changing?
01:02:10.325 - 01:02:18.205, Speaker G: We just showed you that the gas cost was going to change, so it was going to be affected. So yes, it's demonstrated. So that's just demonstrated to be wrong.
01:02:18.325 - 01:02:29.625, Speaker C: That's detailed, fundamental models the same. What I'm saying is the model is the same. The details might change, but the model is going to be the same. We're going to burn gas, we're going to have message frames, we're going to call it contracts.
01:02:30.965 - 01:03:03.279, Speaker G: Right. So the devil is in the details. But okay, let's save this one for the acd. My question was like the one I'm interested in is, I know you've been asked in the past to do EOF like a minimal version, and then someone swooped in at the last minute and asked for the big version of eof. But I'm still. Let's say the context has changed. That was before Cancun.
01:03:03.279 - 01:03:29.565, Speaker G: Now Cancun is there and Verko is next. What could we get as a minimal version of uf? Let's say EOF comes in three version, like three forks or two forks. What could we get that would still be useful and at least be small enough for scope that we could study it and not go back and forth on the question forever?
01:03:30.185 - 01:04:07.341, Speaker C: So the first thing you do in three forks, you're going to have to maintain three more growingly different versions of the vm. And if we have to break the contract format multiple times, then we'll have to support those multiple contract forms forever. One of the requests is if we're going to break these things, then we break it once. And there's only one difference between an EOF contract and a legacy contract. We don't have a legacy contract. And then a container contract that has GAS introspection and then a container contract that does not have GAS introspection, still has code introspection, and then one that has everything that we need. So if we have those three different container things, that's going to be a huge technical debt.
01:04:07.341 - 01:04:45.237, Speaker C: So the first requirement that is a lot of credence is if you're going to break Every break things break as much things that one time is possible. So we need to get banning code introspection and banning gas introspection and preparing for address based expansion in the first go. Otherwise forever we're going to be supporting two or three different contract formats. I mean, we're already supporting two different versions of the Access List transaction because they shipped like just a few months apart. So we got Access List and we've got 1559. We're trying to avoid this by doing it in one big go. Otherwise everyone, everywhere is going to support all three versions forever.
01:04:45.237 - 01:05:21.109, Speaker C: So to split it up into smaller parts actually increases the surface area of what downstream consumers are going to have to support. It also increases the surface area for testing in the surface area for attacks. There might be some design issue in V2 that's not there in V3, and then we have to worry about all the details on that. So while it might make it easier to deploy and get out to do it in three different steps, it significantly increases the risk and surface area of where things are going to need a change. So that's, that's my concern with splitting it up. I mean, I was, I think that wouldn't. That was the initial pitch.
01:05:21.109 - 01:05:26.115, Speaker C: When you start looking at the security maintenance issues, it creates more of a problem than it solves.
01:05:27.855 - 01:06:06.415, Speaker G: That's a fair enough assessment. Although I would counter that, yes, it's. We want to break as little. Well, we want to break stuff as few times as possible, but if the breakage is orders of magnitude bigger than each individual break breakage, I'd rather have, you know, several smaller breaks than one big break. But. Okay, so just, just to be clear, I understand your, I understand your, your point and it's completely fair. But just to have my question answered, the minimum, minimum version of eof, what would that be?
01:06:07.315 - 01:06:51.075, Speaker C: Minimum version of EOF would be one the compilers would actually write compilers for. And I think that includes we could probably get rid of the TX Create and opcode, but I think everything else there, we've tried to pare it down so we only put the minimal features in there. And if we cut much more out of it, compilers won't pay attention to it and they're our primary customer. If compilers will compile the eof, it's a dead feature. That's we. Back in Berlin, we had subroutine opcode proposed to go in and it got pulled very late because Solidity came in and said, hey, we won't use this. So the real litmus test on how much can we cut is how much can we cut before solidity and Viper will say we're not going to use this.
01:06:52.735 - 01:06:53.199, Speaker B: Okay.
01:06:53.247 - 01:06:55.983, Speaker C: And I think what's there.
01:06:56.039 - 01:07:18.605, Speaker G: Okay, so in your view, like at most we could. We would still have to ship 18 instructions and on top of that. Okay, just something. I remember that story there was. That was not EOF at the time, that was EIP, if I'm trying to remember. 617 or maybe that's the. Anyway, yeah.
01:07:18.605 - 01:07:29.915, Speaker G: Why 615. Thank you. Why do they feel this? Do you know? Like, why do they feel that static jumps are not good enough for them?
01:07:32.295 - 01:07:36.869, Speaker C: What do you mean static jumps? Dynamic jumps. Why are dynamic problems?
01:07:37.027 - 01:07:38.881, Speaker G: Right, so yes, sorry. Dynamic jumps.
01:07:38.953 - 01:08:04.645, Speaker C: It's impossible to analyze in static time. You can run coded there that has order N squared order exponential complexity. Jumps in analysis to prove it's safe. Because if you jump from it from something on the stack, you can introduce data from outside of the code to determine where that's going to jump and you can write these horrendous compiler bombs that will absolutely freeze up your block production.
01:08:06.405 - 01:08:22.985, Speaker G: Right. So sorry, that was actually not my question, but thanks for the refresher. That was useful nonetheless. My question was why? Why? Do you know why Solidity. And I think it was Christian at the time who said, 16, 615 is not going to make him because we're not going to do that. Why would they not do this?
01:08:24.005 - 01:09:03.105, Speaker C: So one of the issues had to do with immediate arguments. When I do my EOF presentations at conferences always start out with showing how if we were to add an immediate for static jumps today, how it would change the interpretation of other contracts that may already exist on the blockchain. And if this is known ahead of time, trolls will come in and create problems on the chain. We've seen this before in some other spaces we try to preserve, such as preserving the EOF00 header. We wanted to have it just be EF, but someone came in and put in EOF emojis because they thought it would be funny. So people are going to come and control the chain. And then the Shanghai attacks were absolutely there just to slow down the chain.
01:09:03.105 - 01:09:33.945, Speaker C: So we can't just add in static. You just can't add in immediates. We also can't add in code separation, code and data separation because there's no effective way to show what the difference is between data and code. You could jump into code at the end and there are a lot of contracts out there that interleave their code with Their. With their static stuff and we can't just shut that down. So we need to figure out, you know, it's some of the existing patterns that are in use. We can't just invalidate because they're already out there live on the chain.
01:09:35.485 - 01:09:55.997, Speaker G: Yeah, okay. Yeah, that clarifies things. I will try to talk to Daniel because, you know, last time I talked to Daniel about this stuff, he seemed pretty psyched about it. So. Yeah, I mean, okay, Daniel is not Christian. It's Daniel's problem. It's no longer Christian's problem.
01:09:55.997 - 01:10:25.799, Speaker G: So opinions might change. I guess I will. It's understood that the compiler, you know, whether or not they want to go over the threshold and implement them will require some like an effort that is. It's going to be worth the effort. I will talk to Daniel in Berlin when I'm back to Berlin and see what would be the cutoff for them and then we can continue that.
01:10:25.887 - 01:10:26.159, Speaker C: But.
01:10:26.207 - 01:10:30.075, Speaker G: Okay, thank you. I understand the context now.
01:10:31.415 - 01:10:32.155, Speaker C: Thanks.
01:10:34.055 - 01:10:47.439, Speaker A: Thanks, Dana for joining the call. You're always welcome to come back to Future VIX if you'd like. Cool. And perhaps we can do another call if folks want to on this topic of EOF and Verkal.
01:10:47.527 - 01:10:48.999, Speaker C: It'll happy to do one.
01:10:49.127 - 01:10:57.123, Speaker A: Yeah, cool. It'll have to get brought up on ACD as well, but yeah, thanks again Dano for coming and just helping to share your thoughts here.
01:10:57.259 - 01:10:57.579, Speaker G: Cool.
01:10:57.627 - 01:10:57.931, Speaker C: Okay.
01:10:57.963 - 01:11:03.055, Speaker A: Well, yeah, sorry we went a bit over. Thanks everybody for joining and see you again in a couple weeks.
01:11:04.715 - 01:11:05.675, Speaker C: Thanks. Thanks.
