00:00:00.280 - 00:01:06.434, Speaker A: Hello and welcome everyone to roll call number five. That's issue 1025 in the Ethereum vms repo link to the agenda is in the chat starting off, we had some discussion points on rip 7212, the r one precompiler. The first is that there was a spec clarification. This hasn't fundamentally changed anything for anyone, and I believe Goulash has checked with everyone who's implemented it to verify that this is correct. But basically it was a little ambiguous what to do when a signature did not verify. And so that has been clarified in the rip. So it hasn't like because this is final, we obviously can't change anything there, but this was something that needed to be updated.
00:01:06.434 - 00:02:17.580, Speaker A: The second is there's been some discussion about the pricing of the r one pre compile. That's so first of all, as this is a final rip, that means we cannot change anything about the pricing. If we were to do anything like that, it would need to be a separate rip, and those who wish to adopt that could adopt that. The reason that some people are interested in changing it, at least to my understanding, is that the r one curve does not have the same glv endomorphism that you can use on k one, which makes verification quite a bit more expensive despite the curves being rather similar. So because of this, it might be worth having a more accurate reflection of the work involved. And this is particularly true for some of the Zk roll ups who get closer to the asymptotic costs of these these operations. Anyway, just a consideration so we can have some discussions about that.
00:02:17.580 - 00:02:37.504, Speaker A: And if people really do feel that the pricing is not correct, then that would be worth flagging, potentially having a second or another rip in relation to that. Anyone have other thoughts or additions on pricing or the spec clarification that happened?
00:02:39.314 - 00:04:35.264, Speaker B: I can make a bit comments here. Arbitrum team has raised this concern about pricing of the precompile with this message coming from their benchmarks, and the optimism team shared that it's not a big issue for them and if any of the other teams are not very comfortable of the current guest pricing of the contracts. As you said, we can talk about any proposal and talk about how can we change. But in current situation, it looks like most of the teams are okay with the current implementation and for the clarifications that we have made on the proposal, only Kakarot team's implementation was incorrect due to the current spec and thanks to their team we could fastly update their implementation and it's currently merged and all of the roll ups are aligned with the current spec and the implementation. And lastly, since for the last roll call the implementation has merged for optimism code base already live on Polygon Mainnet. And yesterday ZK sync team has launched on their testnet and possibly we will have on Mainnet next week. And congrats to them, it's the first ZK roll up implement.
00:04:35.264 - 00:04:41.444, Speaker B: They will be first Zklab implementing the precompile on on Mainnet.
00:04:49.964 - 00:04:54.704, Speaker A: Great. And everyone who's deployed things as very cool.
00:04:55.084 - 00:05:01.984, Speaker B: And I can share one of other concern of the arbitrum team, if it's okay.
00:05:02.764 - 00:05:48.502, Speaker A: Yeah, one moment, just quickly on the pricing again. So pricing is something we've seen change for mainnet opcodes before. It is something that there's no right answer for, particularly for crypto things where there are like many very different implementations that are very comparative. And when the whole point of a pre compiles to subsidize the cost of performing some action on chain, there aren't any clear answers. It's just to raise that there is a potential point of discussion, and if you have thoughts on that, it is. It would be worth raising them.
00:05:48.678 - 00:05:49.462, Speaker C: Yes.
00:05:49.638 - 00:05:52.438, Speaker A: On the other point I believe you want to talk about.
00:05:52.606 - 00:07:04.664, Speaker B: Yeah, my reference implementation on get use native crypto library of Golang and they have recently deprecated a function function that checks if the point is on the elliptic curve and the method is still using in their low level APIs and still can be used at the same way in all package. But arbitrary team has concerns that one av may not be used this method as it's deprecated. It's more mark that's duplicated. Now I just want to raise this concern and take attention to different roll up containers. And you can consider changing this method from native Golem library to another implementation. Or just. I would love to hear what, what do you think about this duplication?
00:07:24.044 - 00:07:52.674, Speaker A: I mean the, for any given precompile rip, the implementation should remain largely orthogonal to these kinds of concerns. So it's definitely worth flagging. And it is something that should potentially be changed if people are worried about in their own implementations, but it doesn't change the way that the RFP works.
00:07:54.034 - 00:08:01.094, Speaker B: Yeah, and it's just my reference implementation. Yes, we may see lots of other implementation, even in get.
00:08:02.594 - 00:08:03.974, Speaker A: Yes, exactly.
00:08:18.294 - 00:08:18.630, Speaker C: Yes.
00:08:18.662 - 00:08:20.214, Speaker A: Renaud, you want to say?
00:08:20.294 - 00:09:22.086, Speaker D: Yeah, I don't know if this is a problem or not, but the fact that r1 is slower than k one can be seen just at the pseudocode level. It's just that there is a mathematical optimization which is called glv, glv optimization, which asymptotically it cuts the cost by 50%. In practice it's less. And this is why if the same implementer implements the two curves using the same machine and targets k one will be faster than r1. So this is not related to I am targeting arm intel or this, or this. It's just that k one is faster than ED, which is faster than p 100 256, and the ratio between the computational time will be the same for each target. It's just that.
00:09:22.086 - 00:10:05.964, Speaker D: Okay. Depending on the implementation, one can be more efficient, but it's a mathematical property that you're able to cut by two the size of the ladder on k one. So it's not related to you can bench, of course. But I can also say that when you will have to implement it in a ZK circuit, k one will be faster. So I don't have to wait for the benchmark to know it. It's just that k one has mathematical properties that enable to have some faster, some faster execution.
00:10:09.744 - 00:10:43.424, Speaker A: Yeah, sorry if I wasn't clear about that earlier. My points about benchmarking were just that there are no clear answers exactly for gas costs. But yeah, the endomorphism definitely makes a difference here in terms of cost. Any further points on 7212?
00:10:51.884 - 00:10:52.436, Speaker C: Nope.
00:10:52.500 - 00:11:27.824, Speaker A: Okay, moving on. The next topic is rip 7696, which is a alternative method of achieving foster crypto within the EVM, particularly by offering a pre compile for generic double scale multiplication. So I believe that someone is here on the call who wishes to present.
00:11:28.404 - 00:11:32.064, Speaker D: Yeah, I would like to share my screen, but I cannot.
00:11:32.564 - 00:11:51.384, Speaker A: Absolutely. Yeah, I do need to enable that one. Stand by. One moment. I believe you should be able to share screen now.
00:11:51.504 - 00:11:57.924, Speaker D: Yeah, perfect. Can you see it?
00:12:00.984 - 00:12:02.604, Speaker A: Not yet. I see a wallpaper.
00:12:03.384 - 00:12:16.484, Speaker D: Okay. Okay. I think my PDF reader just crashed. Okay, there we go. Can you see it?
00:12:16.644 - 00:12:18.664, Speaker A: Yes. Okay, cool.
00:12:19.364 - 00:13:22.718, Speaker D: So rip 7696 is about generic double scalar multiplication, which is a core operation for signature verification, but many other cryptographic protocols. So this is what you require if we want to verify either EDDSA ECDSA perform steel signature and so on. So right now we have several different rips for each time targeting a different elliptic curve. So question is, why should we choose for each new use case? And how hard would it be to address generosity? And this is something that we would like to propose and we want to push it progressively. So this is why we perform the solidity implementation. To provide proof of concept, we will compare it with the current implementation and address what it would mean for node and ZKE EVM. So right now we are talking about rap 72.12.
00:13:22.718 - 00:14:21.268, Speaker D: So there are several startups which started to build upon using acanth abstraction in conjunction with p 100 256, mainly for sweet UX. So candid wallets clave which is here. The demo is very far over the 256 and smooth which I belong to. Another use case is using the SGX for the settlement of L2. So this is something zksync and scroll and taco have been implementing. So what happened is that SGX previously was implemented over a BN curve, but recently it has switched to p 100 to 56, which is good if 70 to twelve is implemented. But also we should notice that we are really dependent on the GAfam choice for the choice of implementation.
00:14:21.268 - 00:15:11.774, Speaker D: So right now the part of Fido that is implemented is 300 to 56, but we have no clue that at some point they will not do the same that was done from SGX. So as for instance 250 519 is already part of t 50 of Edo two. We don't know if, I don't know, maybe next year we will not be able to sign using P Android 2556 but have to use ED instead. So this is a limitation of zero ip being non generic. There are also some criticism around this curve. It's not transparent. There is actually a bounty to find the seed which has been lost.
00:15:11.774 - 00:15:55.974, Speaker D: It's also weak considering misuse. So it's not deterministic. Why the edds aspect is it's not NPC friendly and it's slower than k one. So we just talk about so what we are pushing. So we want genericity. It would also enable to implement rap 7212 but ed other more general protocols. So rings nature still address as specified in ERC 5564, palakur vesta baby jujube which is used in the framework.
00:15:55.974 - 00:17:02.596, Speaker D: And actually if you look at what we are doing, there is many use of circum, but sometimes we are using a Zk verifier while a simple classic verifier would be enough and we are using a hammer to crush a fly. And also if you have double scalar multiplication you can implement Pedersen hashing. So this means you can have a friendly Zk hash function. Because hash functions are expensive for Zk L2, then this is something that will be reflected by the proposal of increasing the cost in EIP 7667. Other use case are bridges to other ecosystems. Stagnet has its own curve cosmos Solana so many of the concerns are the code complexity. So actually if you want to have a generic curve implementation, all you need is a mongomoure multiplier and implement the ACC formulas.
00:17:02.596 - 00:18:01.364, Speaker D: So this is something that you should already have in most library or Zke EVM implementation. So what 7696 includes, there are two hub codes. So if you look and compare it to 70 to twelve, the difference for the first code mainly lie in the introduction of the curves parameter. So p ABN, which are specific fields for each curve. And we propose also another precompile, which taking only four extra values enable a speedup which is comparable to glv we will talking about. So mainly GLV is performing four basis multi exponentiation. And with this extra data we can restore performances comparable to what we reached with the k one.
00:18:01.364 - 00:19:00.942, Speaker D: So those optimization are independent of the language. If we look just at the complexities, the number of operations, it would be the same independently of is it implemented in a node, in a circuit or in solidity? I will talk about the result next. And a question I've been asked about why pushing this instead of a full generic mSm. Actually I think a full generic MSM might be too complex to have a chance to be implemented. While if you look at the solidity code of the proposal, it's close to 100 lines. So of course full generic MSme would be superior, but too complex I think for single rip. So last year we had several results about solidity implementation.
00:19:00.942 - 00:20:26.054, Speaker D: So what we want to say now, if we look at the best 70 to twelve implementation, considering the performances was reached by FCL last year, and what we were able to do, actually we managed to have a faster implementation that what FCL was doing, being specific, but for a generic curve. So if we can do it in solidity, it can also be transposed to other languages. So mainly what appears on the two last lines are the cost for the two precompiled in full solidity. So for circuits, just to give an insight. So there is also problem when pushing one eIp or rip, introducing a new curve each time, it's a pain, because it means that a dedicated ZK circuit will have to be implemented by the L2. So this is something ZK sync has done. In Yule we have contact with linear, actually the plan to integrate with solidity, which mainly is quite the same as using Yule.
00:20:26.054 - 00:21:09.020, Speaker D: And we think that pushing something that is generic would avoid to redesign a new circuit each time a new use case or modification is pushed by the Gafan. Yeah, so as I said, Montgomery multiplier is what you need to implement those. It's already coded in GnarC, actually. They pushed. Yeah, and the casing, it's also what they did to implement 70 to twelve. It could be modified to be generic. So that's it.
00:21:09.020 - 00:21:12.024, Speaker D: Because I had only five minutes, I think I've done more.
00:21:16.724 - 00:21:38.196, Speaker A: Thank you very much. I have a few questions. One is on, you mentioned using, providing the extra like four bases for speed ups where possible. Do you have any insight into the gas of that? Because obviously then you'd have to supply the extra points versus the speed up there.
00:21:38.340 - 00:22:22.714, Speaker D: Yeah. So first you can do this off chain. Actually, this is what is done when we provide the public key for your 70 to twelve verification. Actually you already have to make some kind of wrapper to transmit from the result of the enclave to the blockchain. So this can be done off chain. So on chain is something like, I think it's 120k gaze, which would be a one time computation, but it would be clever to perform, to do this off chain instead.
00:22:23.934 - 00:22:42.106, Speaker A: Okay, the next question I have is how does this compare to something like EVM Max in terms of the functionality, good gain, and sort of the gas, the gas comparisons, you have any thoughts on that?
00:22:42.290 - 00:23:28.424, Speaker D: Yeah, it's kind of different because we would have to have a full EvM max. EvM max implementation is that EVM Max is more generic, but it's not to be used by a non cryptographer guy. It means that using EVM maximum you can design a lot of cryptographic primitives. It's a powerful rip, but it's intended for cryptographer only. I think if I ask you, what would you do yourself or developer with EvM Max? It's not obvious. So Evmux, it's a very good rip. Clearly if we add it, I would use it for many use cases.
00:23:28.424 - 00:24:24.424, Speaker D: I think that using EvM Max you could also, for instance, implement lattice based primitives. It's another toolbox, EVM Max. It's a kind of trade off between EVM and e WASM, which at some point was a very, very good project, because EVM language is far from the machine, which, which is complicated. And why do we need all those precompiles? Because we do not have e wasm. So EVM Max is very good. It's kind of a trade off between iwasm and Evm, but I would say that it's a lower level tool than a precompiler.
00:24:25.944 - 00:25:11.984, Speaker A: So outside of the implementation issues around EVM Max, because you obviously have to, for example, implement Montgomery multiplication on channel like in solidity using EVM Max. So assuming the implementation side of things was not an issue, that is, someone wrote a really robust EC multiply library or something like that. Do you have any insight into just sort of the gas costs? Like how much worse is it to do to implement like Montgomery multiplication using EVM Max versus to sort of emulate this RFP of yours?
00:25:15.524 - 00:25:20.262, Speaker D: I would need some off chain time to, to make an evaluation.
00:25:20.398 - 00:25:33.834, Speaker A: But yeah, I mean, I think part of the issue is you also don't have those concrete numbers for EVM Max too. And if we do, they're rather old numbers. So it's isn't a fair answer there. I was just wondering if you had any thoughts.
00:25:34.134 - 00:25:41.754, Speaker D: I could, I could, I could answer it. Not right now, but give me one or two days and I will have you the answer.
00:25:42.814 - 00:25:57.334, Speaker A: That would be really helpful. Thank you. Dogan in the chat asks, can you repeat the gas costs for r1 using ECMO add?
00:26:00.114 - 00:26:19.254, Speaker D: Yeah, yeah, it's actually, it's the same for foreign curves. So what we had last year was two hundred k, and using the second opcode, we were able to reach 160k. So it's 20% less.
00:26:23.434 - 00:26:24.294, Speaker A: Okay.
00:26:24.794 - 00:27:49.424, Speaker D: Right now, this line was the fastest on chain verification, first line of FCL. And we can keep the same performances being generic, or cut it by 20% using the extra dice. But this 20% will be closer and very close to 50% for ZK implementation. And also at node level. In fact, at node level, you should have the same performances as K one actually ZK, because when you perform a single operation on ZK on a circuit, you have a very high number of operations just to implement a modular, modular multiplication. And you are very, very close to the asymptotic limit. This is something that we were able to measure on stagnet last year before when we started to work on the l one, before 4337 came to Ethereum.
00:27:52.604 - 00:27:53.344, Speaker C: Great.
00:27:54.564 - 00:28:12.884, Speaker A: Anyone else have further questions on this topic? All right, thank you, Renaud, for the presentation.
00:28:13.784 - 00:28:14.192, Speaker B: Thank you.
00:28:14.208 - 00:28:56.944, Speaker A: Very cool idea. Yeah, I mean, the idea of having very generic implementation, or very generic ways of doing things is the extensibility of. That is very appealing. Next topic on the agenda is a bit of a discussion of what has been happening on the l one side of things. There's. Since the last roll call, there's been lots of discussion around what gets included in the next l one fork, Hector. So I'm going to hand over to Ansuka, who has some points you'd like to bring to everyone's attention on the ultra side.
00:28:58.364 - 00:29:48.268, Speaker C: Yeah, thanks. I thought it was maybe just a nice addition to roll call, to use this to basically give brief summaries over the parts of the awkward procedures that over last month that were somewhat relevant for L2. So I just prepared that for today. So there are kind of five different individual topics I'd want to touch on quickly. The first three are concerning the upcoming Pectra hard fork towards the end of this year, and then two of them kind of are more on the road beyond that. So on Pectra, the first one, and that's a very recent one, is on account of section. So basically there, there's the AP 3074 that maybe most here are familiar with.
00:29:48.268 - 00:30:36.904, Speaker C: That was after a lot like long debate accepted for the upcoming Pector hard fog initially like a month or so ago. There has been quite a bit of debate since though, because there are some drawbacks of the EAP that not everyone likes. And so specifically yesterday there was a breakout call on the future of a kind of structure on layer one. And now there is an alternative EIP, a very new one, and it's basically just like an iteration of 3074. It's called 7702. And so basically I just wanted to signal here for people maybe not following this super closely, that layer one is very likely to ship one of those two in the upcoming hard fork. And so that we will have basically enhanced UA features on layer one.
00:30:36.904 - 00:31:21.360, Speaker C: And what that means, of course, is that given kind of just like the default setting power of layer one that it still has, and given that UA's of course still make up most of the user base, that this will basically create a pretty strong kind of forcing function towards users expecting this functionality. So if you're L2, you have not looked into this deeply yet. I would recommend that you have a look. You basically have some sort of UA to account abstraction strategy in place. Presumably the decision between 3074 and 7702 will be made over the coming weeks. So I'll give another update on the next roll call. But also you can, for example, tomorrow's all codeps.
00:31:21.360 - 00:32:20.044, Speaker C: This will probably be a big topic, so if you want to listen in on that or maybe even come and contribute that, that also is worth it. And as last point on that account, observation topic, of course, given that the layer one version of such a spec change usually is like a pretty strong default telling point for also the variant that ends up landing on there too, now would be the time to look into. Is there maybe any modification to the spec that you would want to make it more useful in your context and then come and try to participate in the layer one governance process around that as well. I could imagine specifically on the ZK side for example, it would be really helpful to know if there's any maybe small modifications that will make it much easier to have that also implemented on the ZK side. So that's on the kind of search front and I'm not sure how to do this. Maybe after each topic I'll briefly ask if there's any kind of question. So if there's any questions specifically on the conduct section, now would be the time.
00:32:20.044 - 00:33:39.844, Speaker C: Okay, otherwise I'll move ahead. So the second point I would briefly mention, we already talked on this call earlier about what's happened to twelve one pre compile, but just wanted to mention that it's now also officially considered for inclusion for Petra on the l one side. And that's especially exciting because that would be the first time that some feature that was first shipped as an IP and it's already finalized of course as an IP would then later on come to Mainnet. So there's a lot of open questions that we would then basically have to figure out both just with the basics. Will it use the same EIP number as the IP, but also more interesting things like will l one use the spec as is? Will there be some small changes? For example, we talked about the gas price and these kind of things like if they are changing basically how does that impact the relationship between layer one and L2? These kind of questions, they'll all be figured out in the coming weeks depending on of course if it actually ends up being accepted for pector or not. But again, if this is we of course always going to, I'm usually participating in these all code of calls if I'm trying to represent the kind of the L2 perspective as best as possible. Carl as well of course if there's any ever anything specific, it would be great to also have people directly from the site participate.
00:33:39.844 - 00:34:21.316, Speaker C: But again, just want to spike that third one, last one, that one just a quick update EOF, just because that's a bigger change. So if you at some point would want to support that, that would probably require some lead time. That one is still considered conclusion for pectora, but is more contentious. So for now I would not necessarily expect it to be included in the hard work, but that might change in the coming weeks. And if we end up including it, then that's also something we, at least you should have a strategy around. Would you want to ship that then as well? Of course at some point solidity, for example, then would start using that as the default. So if you want to still be compatible with us solidity versions, you probably have to also support that.
00:34:21.316 - 00:35:26.124, Speaker C: So at least having some sort of surgery for that in place, and that I would recommend. So those are kind of the three main points on the upcoming picture fork that I wanted to briefly summarize. And then beyond pectoral, there are two more things. One would just be local trees. We've talked about this in the past here on roll call already a little bit, but now with pectoral mostly locked in also worker, which will likely be the upgrade after pectoral basically is starting to be more and more top of mind for people. There's definitely active work, which means that a us L2 should really probably start to have some sort of worker strategy, like are you going to move over to Berkeley as well? Once Mainnet does that, are you not going to do that? What are the implications of, for example, forever being on a different commitment scheme then, then layer one? These kind of questions. Are there maybe again same topic as with a counter section, are there maybe small modifications to worker that would be really desirable from your point of view so that then it would be more attractive for you to move to.
00:35:26.124 - 00:36:25.356, Speaker C: And those would probably have to be raised on layer one relatively soon, because I think we reasonably soon get to the point of having basically like a more or less spec walk in for Virka. So now would be the time to dive in and pay attention to this. And then the last topic to bring up would just be four, eight, four and post 444 block throughput scaling. So just as an update, we are now what, two months or something after the fog roughly, we still haven't reached basically the target throughput 444, which means the blobs are still effectively free on Mainnet, which is nice, but of course that won't last forever. So basically the way layer one thinks about blob scaling is that it will basically be a gradual scaling. So we are already working on data availability sampling, which is the technology that will unlock more substantial scaling. This might or might not arrive in time for Pectora.
00:36:25.356 - 00:37:15.004, Speaker C: If it doesn't arrive in time for Pector, we would probably just have like a small bump up of the existing kind of numbers to give us a small kind of constant throughput increase over what we have today. And then once we have data availability sampling in the form of peer does, that's a specific technology that we're using there, then we can kind of like have probably also gradual as we kind of get experience with that, but more more ambitious kind of throughput increases. The reason why I'm bringing it up here, I mean for one of course, is that that's maybe relevant for you to plan out your data availability strategy. But also I do think that there's a little bit of uncertainty on the l one side around the demand levels for blobs. So as I was saying, we still haven't even hit the target for 4.4 yet. And of course this has multiple dimensions.
00:37:15.004 - 00:37:59.996, Speaker C: One is just the amount of throughput that existing. There are ones that already use on chain DA, ethereum l one on chain DA can basically make use of. But then also there might be other chains that today do not use ethereum on chain DA, but are considering switching over to it. But they of course would need some sort of reliable, credible path where they can trust that that won't blow back up in cost. And so having a good feeling for how much demand is there beyond the current levels would be very helpful. I think. Right now, for example, I keep pushing for this as a priority, but I don't think all core devs agree that basically it is a big priority given that we don't see the demand yet.
00:37:59.996 - 00:38:34.914, Speaker C: So I think basically just clearly voicing that there would be demand beyond current levels and that should really be a priority and that business scaling should really push at the fastest possible speed. I think if you think that's the case, of course, also the other way around, if you think that current levels are basically that we can take it easy and focus on other things first. Both of those perspectives I think would be very valid. And so basically, like making those more visible for layer one I think would be, would be very helpful. Yeah, so those were, those were the points I had. Yeah. I hope I'll keep doing these summaries on the next roll calls if they're useful.
00:38:34.914 - 00:39:34.204, Speaker C: Yeah. Are there any specific kind of questions for any of these specific points? Yeah, Roberto, I definitely agree that my assumption is also that we will basically reach the target probably even before pectora. I would say that it's almost inevitable that there will be some small bump up in throughput in pectora, but it could be very moderate, it could be very conservative if there's no clear signal that basically more is necessary because it might be pre peer does, so it would just use the existing technology, just basically pushing it a bit more to its limits. So if there's no clear signal that there is strong demand, that I would expect that the Petra bump will be relatively small.
00:39:37.294 - 00:39:52.994, Speaker E: Yeah, all that makes sense. Anzagar, thanks. Just wanted to note we plan on increasing bases throughput a bit more because we're currently at target. So we're going to consume a bit more of that space. And I also see a lot of l two s that are coming online, so it seems very likely we'll be hitting that target quite soon.
00:39:59.414 - 00:40:49.620, Speaker C: Yeah, makes sense. And I think basically the sooner l one basically actually starts reaching that target level and the block base fee goes below, it goes beyond zero, I think the more likely it is that we get a bit more of a throughput increase, both already in pectoral, but also that we. I think that one is not even the one that I'm mostly focused on. I think the more important lever here is to what extent the work on peer does gets prioritized by clients already today. I think to the extent that they feel that there's some priority that should be put on this, we'll just get there sooner. And that's the one that's going to unlock a substantial increase. Okay? Yeah.
00:40:49.620 - 00:40:51.544, Speaker C: And that's all from my side.
00:40:55.744 - 00:41:50.674, Speaker A: Thanks, Anskar. That's all we have on the agenda for today's call. Does anyone have any other topics they would like to discuss before we bring this call to a close? Going once, going twice. Okay, thank you everyone, for attending. Next roll call will be in roughly a month's time and see you all there. One quick last thing to flag is that Ansuka and I will be looking into spinning up breakout calls again after next roll call. So if you have any topics that you'd like to see discussed there, please reach out to either one of us.
00:41:50.674 - 00:41:53.074, Speaker A: And yeah, thanks everyone for attending.
00:41:58.574 - 00:41:59.054, Speaker D: Thank you.
