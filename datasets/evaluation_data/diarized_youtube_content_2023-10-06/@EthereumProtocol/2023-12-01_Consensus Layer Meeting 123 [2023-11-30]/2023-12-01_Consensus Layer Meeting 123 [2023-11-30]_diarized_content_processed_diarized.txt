00:04:17.490 - 00:04:55.290, Speaker A: We should be live. This is the all core dev intensive layer, call 123. This is issue 9116, the PM repo. We'll hit anything on the NEB. So, testing and Devnet updates, as well as a very minor spec release, hopefully going out tomorrow. A clarification on maybe some ambiguity by root request, just for quick discussion. And then a couple of process items from Tim.
00:04:55.290 - 00:05:56.110, Speaker A: And if you have anything else, we can get to it as well. Okay, so first up, what's going on with the nub testing and devnets? Did I see that there's a Devnet that's been launched? Does anybody want to give us an update? Do we have anyone from the DevOps team here? Did I start the call too early? Trying to train you all to show up on the hour. Okay, we will table that for now. Any testing updates worth discussing today?
00:06:00.280 - 00:06:55.280, Speaker B: Yeah, I can talk about the hive updates. There's been no major issues found so far, just a couple of ones, but these have been mostly test related. But so far the three consensus clients that I've tested are working correctly. No major issues so far. I will keep just adding test cases in the blobber side and also in the builder side to see if I can find anything, but so far, nothing worth bringing up. Okay, and another one is maybe after. I'm not sure how long it would take us, but I think probably this week or next we might launch some equivocation tests on the devnet with the blobber.
00:06:55.280 - 00:07:04.730, Speaker B: It's basically ready, but I guess that partish would like to have the Devnet more stable right now before launching any.
00:07:05.820 - 00:07:11.800, Speaker A: Attack testing makes sense. Perry, Barnavas, you want to give us an update on Devnet?
00:07:12.620 - 00:07:12.984, Speaker C: Yeah.
00:07:13.022 - 00:07:15.880, Speaker D: Barnavas, you want to do initial updates?
00:07:16.540 - 00:07:58.410, Speaker E: Yes, we did a non finalization, kind of. We took one third of the network off because we had to recycle the archins for that network. And we found a couple of bugs in a few clients. I'm not sure if Terstak is here to discuss that. Another thing is, Devnet twelve is very stable right now. We just started running Blob tool and yeah, they now have blob net twelve. And most of the clients are onboarded except for some.
00:07:59.340 - 00:08:22.608, Speaker D: Yeah, I think we found like an exit related issue on Devnet eleven as well as something on reds. There was also an interaction between Nimbus and Reds that had an issue in Devnet eleven. They're all being fixed right now, so we'd probably keep eleven up until they're fixed, and then once that's done, we'll reproduce. We'll try reproducing all the issues on twelve.
00:08:22.774 - 00:08:29.500, Speaker A: Was that the finalization? The bug with respect to the finalization issues, was that on eleven or 1211?
00:08:29.660 - 00:08:38.820, Speaker D: Okay, but I don't think there was any finality related issues in itself. It just happened the same time we submitted the exit, so we assumed they were related. They weren't.
00:08:39.560 - 00:08:40.608, Speaker A: I see.
00:08:40.794 - 00:09:02.670, Speaker E: The thing is, the exits would have probably been handled fine, but Nimbus would have voted on incorrect. It would have just said some of it were invalid and maybe it worked away. We don't really know what would have happened if there had been no non finalized state.
00:09:05.440 - 00:09:32.170, Speaker D: Yeah, and just to clarify, the issue is not like a main net related one at all. It's scoped to just Dankun testnets. And the other topic we wanted to bring up was now that we have Devnet twelve and it looks like it's forked without any fanfare, should we start planning girly shadow fork? And we want to know what sort of timeline we can plan for that.
00:09:39.030 - 00:10:28.250, Speaker A: Yeah, does anybody have any thoughts on that? I guess, importantly, what's the prism timeline look like to getting on Devnet twelve? Is anyone from Prism here? We might have lost Terrence because of the time change. It's now 06:00 a.m.. Oh. Preston, do you have any perspective on Prism's readiness for. Hey, I'm not totally sure exactly on timeline. I think we're still a few weeks away. I'll have to check with Terrence and follow up with you guys.
00:10:31.370 - 00:10:33.862, Speaker D: I think on the Dankun call they.
00:10:33.916 - 00:10:41.900, Speaker A: Said that they were two to three weeks away. Yeah, that sounds great.
00:10:42.430 - 00:10:46.890, Speaker E: Is it wise to bother with the shadow then without prison?
00:10:49.810 - 00:10:53.070, Speaker D: Yeah, I'd also wait for prison before we do the Shadow fork.
00:11:01.300 - 00:11:03.440, Speaker E: The shadow fork during Christmas?
00:11:15.280 - 00:11:24.930, Speaker F: Yeah, we should definitely do at least one before the end of the year, I think. Yeah.
00:11:28.420 - 00:11:48.104, Speaker A: Mars is asking what's going on with timelines in prism, Preston. Or do you have any perspective on what the major hurdles are at this point? Yeah, I'm not totally sure. Okay. What it.
00:11:48.222 - 00:11:48.916, Speaker E: We're.
00:11:49.028 - 00:11:57.580, Speaker A: Yeah, I don't know. I have to follow up offline. We can get Terrence or Potez to give us an update in the discord.
00:12:02.010 - 00:12:19.182, Speaker D: In the meantime, what all do we want to see on Devnet twelve? I know Mario spoke about adding blobber and doing some equivocation stuff. We're going to be doing that for sure. We'll submit exits, deposits. There's already blob, spam. Anything else that we want to do.
00:12:19.236 - 00:12:24.160, Speaker A: Any other scenario, assuming we'd run a slasher as well.
00:12:25.010 - 00:12:27.300, Speaker D: Yeah, we definitely be to slash as.
00:12:28.790 - 00:12:47.320, Speaker A: Something Mario probably said this, but just the equivocations with just the sidecar headers will be like a kind of new path for the slashers to pick up getting messages from that gossip topic exclusively or those potential multiple gossip topics. So that'd be interesting.
00:12:48.410 - 00:12:51.400, Speaker E: Which of the client slasher right now.
00:12:53.310 - 00:12:56.170, Speaker A: I believe lighthouse and Prism have a slasher.
00:13:02.270 - 00:13:32.260, Speaker D: Yeah, that was just a question we had as well. Does the slasher need to be updated or is it just assumed if Lighthouse is able to track the chain, it can also produce the new slasher proofs. Yeah, Lighthouse, the thing that we have running on Devnet right now, it does include the block headers from the blob sitecars into the slasher as well. So we should be detecting any proposal slashings that come through the way of.
00:13:32.790 - 00:13:34.520, Speaker F: Blob sitecar as well.
00:13:36.170 - 00:14:16.700, Speaker A: Cool. Well, we can give it a test shot. Yeah, I mean if we have time. Doing non finality, taking portions of the network offline and that kind of stuff is always a good see what's going on. Also, if we're going to be running Devnet twelve for multiple weeks, it might be worth knocking on a couple of l two s doors to make sure their stuff, if they've been doing development work, works on it.
00:14:19.960 - 00:14:33.770, Speaker E: Yeah, I think we're going to be running probably end of take another two weeks to get on boarded then I don't see a reason to relaunch it during the Christmas week.
00:14:38.510 - 00:14:42.474, Speaker A: And what's our mev boost builder situation on Devnet twelve?
00:14:42.512 - 00:15:16.870, Speaker E: Look like we have 225 liters running. It's only three nodes, but they seem to be working somewhat. Okay, we don't have map law working because rot is doing some rework on that that is currently on hold. So we don't have many juicy collections. So local builds are preferred over mev build blocks. As soon as we have running, I think we're going to see a lot more memory related.
00:15:26.210 - 00:15:28.930, Speaker A: Anything else on Devnets and testing?
00:15:34.750 - 00:15:42.650, Speaker E: We have the channel. So maybe he can discuss the Nimbus bug that was covered on Devnet Eleven.
00:15:49.020 - 00:16:16.150, Speaker C: Yeah, sure. So, so this. So Nimbus essentially was in the not validating the not using the correct fork to validate voluntary exit messages on not the state transition, but the.
00:16:18.680 - 00:16:19.252, Speaker A: Sort of.
00:16:19.306 - 00:16:53.884, Speaker C: Integrating blocks into its trusted database. So that ended up stalling progress in it for Devnet eleven. There was a smaller issue that kind of jointly Nimbus and there was a Nimbus ref kind of both had issues there, but that was much less serious. This is a consensus issue, but fortunately.
00:16:53.932 - 00:17:03.490, Speaker A: Owing to Deb, is that something we can catch in the consensus test vectors, or is it a bit too stateful for that?
00:17:04.900 - 00:17:53.212, Speaker C: Well, that's the issue. That was my first instinct as well. And I think the latter at least this is, or say, because of Nimbus's architecture. It's the latter because the state transition function is actually correct. So when I got the state and block, the pre state and the block and just we have a ZCLI analog, it applied, it applied correctly because the state transition function, you know, spec compliant. But there's an optimization that Nimbus has, all the clients have, I guess this batch signature validation and the way that's implemented is outside the scope of the state transition. And honestly, that's probably going to remain that way for this part of it.
00:17:53.212 - 00:18:34.832, Speaker C: It varies a little bit. It is an interesting question in general where the architectural boundary should exist with any of these things, but I think stateful is a good example. And in particular, what happens here is that by the nature of what it does, is it collects signatures across the entire block, like all of the signatures in the block. And this is something that there would end up being a tension between performance and sort of test coverage, which we already see, by the way, the epoch transition white houses published a while ago.
00:18:34.886 - 00:18:38.960, Speaker A: This thing about well optimized version.
00:18:39.620 - 00:19:20.076, Speaker C: And right now there's a really obvious tension between the test coverage right now. If you do that, then you lose a lot of test coverage unless you very carefully. This is why Nimbus hasn't done this as much, only some, because to achieve that level of performance means losing a lot of the test coverage of the individual sort of components of that. In the consensus spec tests here, we made a different decision and it ended up biting Nimbus, I guess, in this case, but where the performance was not feasible to leave on the table for the epoch, for the no.
00:19:20.098 - 00:19:42.788, Speaker A: And I understand it'd be interesting given as the commenced layer is further integrated into Hive and we use additional tools like kertosis. If we can try to capture some of this in there a bit further upstream or downstream, whichever your perspective is.
00:19:42.954 - 00:19:52.810, Speaker E: We just on kurtosis because it takes a long time for an exit or a deposit to take place most of the time.
00:19:57.820 - 00:21:05.212, Speaker C: But I would agree though with the idea of further testing. Obviously more testing is always good, but systematically figuring out how to incorporate more client functionality into tests that are run in maybe more automated ways, whether it's hive or kertosis or something else. And there are practical challenges there, obviously, with the docker image building and et cetera, just timing wise. But if these can be figured out or things can be factored well so that more of these tests can be run, I think that would be definitely useful. The other challenge here, and this is something that is worth, is like how standardized, and this is actually, I think maybe a policy. This is not just like a software engineering question. How standardized should the behaviors be? It's been observed before in a number of contexts that all of the clients, as kind of table stakes, they all correctly pass the consensus spec tests.
00:21:05.212 - 00:21:28.680, Speaker C: They always do with all the devnets and testnets. Anything outside of that is a little bit up in the air. Is that desirable or not? Or are these things that are intentionally kind of left ambiguous? Or are these things what should be really nailed down by a spec versus what should be left deliberately vague?
00:21:29.900 - 00:22:32.270, Speaker A: Yeah, I agree with that. But still, some of these, like when the statefulness kind of gets in the way of processing valid or invalid blocks, I think it's still like spec. It's hard to argue that if you're doing batch operations that they should fail when they pass individually. But I agree that when you get to the edges, especially when you start talking about responding to network things and stuff, it gets really hard to throw that all on the conformance test. I don't know if we've had a consensus. We probably have maybe not live, but most of the issues around consensus have been around things that work in isolation, but then don't work in a more stateful environment regarding caches, optimizations and that kind of stuff. So at least some sort of more bulk operations in high worker toast is maybe good.
00:22:33.440 - 00:23:10.890, Speaker C: I'd agree with that. I think another possibility is the gossip validation. I think it's important. So really crucially, at least for me, there's a hard boundary between tests which depends on network and tests which don't. Network dependent tests are fail, I'm sorry, but they just do. And I want parts of at least the Nimbus CI. And in the Nimbus CI I definitely kind of push for this actively where they ever fail because of network weirdness, or they can't access some external server other tests, that's fine.
00:23:10.890 - 00:23:40.768, Speaker C: In terms of the EF test, I think that's a useful boundary as well. And what I would also push for what I'm getting at here is to say that the things like gossip tests, there are many tests which actually could be done. There are bulk tests. There are more integration tests, but they do not depend on external access. I would say it's a reasonable assumption. And here's call. So if somebody disagrees with me, tell me or tell say so.
00:23:40.768 - 00:24:37.364, Speaker C: But I think that people have or can have refactor things so that there is something like a gossip validation function. There is some oracle in the software that can say, is this gossip message good or bad or ignore or reject if this is the case given, and that function probably is not doing any network calls itself. And so I think one possibility is having a more formal test suite around gossip validation. The only thing I would say there is that what Nimbus does deliberately, and I think a deliberate ambiguity, is the accept reject condition. Reject ignore conditions. By reordering the tests for performance reasons, you can get different versions of those. Difference is the purity scoring and that.
00:24:37.364 - 00:24:45.830, Speaker C: We'll just take that. That's fine. But if it starts becoming, well, no, you failed a test because you didn't put the signature verification last.
00:24:46.220 - 00:24:52.564, Speaker A: Well, yeah, you might have to wrap. Ignore, reject is the same for this purpose.
00:24:52.612 - 00:24:57.256, Speaker C: Yeah, but otherwise, I think that's an example of what you're talking about where.
00:24:57.278 - 00:25:15.250, Speaker A: I think, yeah, even those can be moderately stateful. I mean, at least time, like what is the system's time becomes a condition in there that is not part of the state transition. That's stateful. There might be a couple of other things in there, though you need to be careful about.
00:25:16.100 - 00:25:20.880, Speaker C: That's true. We deal with that with fork choice, though. No, to some extent. Like with a fork choice test.
00:25:20.950 - 00:25:38.764, Speaker A: Yeah, it had to be baked in there. So it could be baked in there. Although some of the ignore conditions are. Did you see this other message? So it's like you'd have to give a suite of messages rather than just a prestate. So it is a bit more stateful.
00:25:38.832 - 00:25:54.124, Speaker C: I'm actually wondering. The other part is ignore is reject can certainly be tested. I hadn't really shared that because that should be just straight up right by their nature. Once reject always rejected all of that stuff.
00:25:54.162 - 00:25:55.150, Speaker A: So that should be.
00:25:55.840 - 00:26:23.652, Speaker C: I see what you're saying, though, nor actually is a real problem. And I would say probably, I would say out of scope. I would suggest for a first version of these, it's just too picky. And it also resolves the ordering issue to some extent, the test ordering issue. If you only pay attention to reject at all. Anyway, that's an example. There are others.
00:26:23.652 - 00:26:38.808, Speaker C: I don't really go too down to that one particular rabbit hole too much. But one can find other examples where I think one could expand the scope and certainly a nimbus that, by the way, would capture off the batch validation stuff because. Sorry, go ahead.
00:26:38.974 - 00:26:52.450, Speaker A: I was asking a question whether reject is even valuable, because reject are probably deterministic with respect to kind of things that would be caught in the state transition. It's really those ignores that are probably more.
00:26:53.300 - 00:27:30.590, Speaker C: Yeah, well, nimbus handles this differently, though. I can't speak for any other clients, obviously, but the nimbus will tend to use the batch validation at that layer even just so. At least for Nimbus, it would be a useful kind of additional set of testing, that it is stateful in a different way. But I see what you mean that it probably won't in general catch a huge amount. So that may not be the highest roi thing to add. There may be others. That was just kind of a spur of the moment thought.
00:27:30.590 - 00:28:31.180, Speaker C: But I think in general, but it was a way of saying, I do agree with you that finding larger, more stateful chunks, if that state can be managed. Another hazard or risk of this, I will suggest, is for people who have seen, I know this is not the ACDE call, but for people who have seen the execution tests, they take significantly longer to run in general, I think, and in their complete vastness. And part of the reason is, there's several reasons, there's more forks and all that, but part of it is just that they kind of lean much more into this idea of doing these integration tests with full on states that take a while. They are slower tests. What the trade off there is CI wise for people, I don't know, but that's another consideration.
00:28:37.640 - 00:29:09.650, Speaker A: Yeah. Okay. How or if and how should we continue this conversation? I know we've been ramping up clself with respect to hive and also digging a bit deeper into some of the stuff we can do with kertosis. So maybe it's just a matter of making sure some of the issues that we do run into are documented so that Mario and others are aware of them and kind of can see if they can be integrated well. But maybe there's something else we should do here.
00:29:10.900 - 00:29:54.080, Speaker C: I would say in terms of things which are not sort of to be completed on this call, but sort of having a complete set of what are the protocol actions that need to be tested. So in particular for here, what triggered this was in the wild, as it were, Daneb exits. That was the new thing in Devnet eleven, which approximately triggered this. So whether that was intentionally chosen for this purpose or not. I'm not sure the previous devnets could have done this. We've had previous devnets, but being more systematic maybe about saying what are all the possibilities.
00:29:55.620 - 00:30:28.350, Speaker A: Yeah. And seeing if we can catch them in some sort of CI instead of having to remember to trigger them on a devnet and catch them a little bit later. Okay, does anyone from testing want to chime in here? And Dustin, is there anything to document on your end so that we better understand this? Or is it simply triggering a bunch of exits on the NEB? Or is there more details that are worth making sure the testing team is aware of?
00:30:29.040 - 00:30:50.380, Speaker C: I don't think so. I think it's basically just more exits on the net. Not quite yet. We haven't released the. I haven't made the pr for this yet. We'll just kind of pointlessly stall nimbus until then. But that's certainly my priority.
00:30:50.380 - 00:30:59.584, Speaker C: And as soon as that goes out and is running on the Devnet twelve or pive or anywhere else, I think the net bag sets.
00:30:59.712 - 00:31:14.120, Speaker A: Yeah. Cool. And I don't mean to hound on this particular issue. It's much more just like us all being mindful of that. There are these stateful paths that we need to be trying to make sure we test and think about how our testing.
00:31:14.620 - 00:31:48.500, Speaker C: No, I appreciate that. Absolutely. And I agree with it. As the system becomes more mature and people optimize more and sort of bake more assumptions, kind of just over time into the software, deviate from the python pseudocode spec increasingly, I mean, gradually, but this will mean, obviously it's been happening from the very beginning as the merge, but this will become more typical.
00:31:49.320 - 00:31:51.670, Speaker A: Mario, you had your hand up. Do you have anything?
00:31:52.600 - 00:32:09.340, Speaker B: So I think a write up would be very helpful. And also if we can also discuss this on the Denkan interrupt testing call to bring it up again and just see how we can continue. We can pick it up there and see ways that we can implement this in the tests.
00:32:12.160 - 00:32:18.670, Speaker C: Okay, so just like a hack MD or when you say, what would you like to see for the write up?
00:32:19.920 - 00:32:39.156, Speaker A: I think a quick hack MD, the high level things that happen on the network and anything that's maybe interesting to document with respect to nimbus, like that, that was a bulk verification path or something. Because then that might imply other types of bulk verifications that we should be thinking.
00:32:39.258 - 00:32:39.972, Speaker C: Right, I see.
00:32:40.026 - 00:32:40.292, Speaker A: Okay.
00:32:40.346 - 00:32:53.480, Speaker C: Right. What can be generalized from it? Because obviously this will, I will say it's not solved quite yet, but it's, let's say, resolved. The mystery has been so sure okay, thanks.
00:32:53.550 - 00:32:54.810, Speaker A: Perry. Do you have anything?
00:32:55.980 - 00:33:33.504, Speaker D: Yeah, I think most of what Mario was saying as well, but we'd probably be talking to see which parts we can put into hive and which one we want to put into kertosis. Because one of the things, I guess, over the last few issues we've seen is that we just need some tool that can systematically execute a certain number of integration related behaviors. So, exits, withdrawals, make sure there's a Mav block produced by every client, make sure there's a block produced by every combination, and so on. And that sounds like it's something we can just integrate into ketosis, so we'd probably solve it by a mixture of hive and ketosis, but, yeah, we'll be discussing.
00:33:33.552 - 00:33:34.412, Speaker A: Yes, my intuition.
00:33:34.496 - 00:33:35.560, Speaker D: Give an update.
00:33:37.100 - 00:33:38.810, Speaker A: Okay. Anything else on this.
00:33:42.380 - 00:33:43.556, Speaker C: I'm not familiar?
00:33:43.588 - 00:34:09.170, Speaker A: Yeah. Cool. All right, cool. So we'll keep it kind of in testing land, but if some broader progress is made on strategies and stuff here, by all means, bring it back to autocorredevs. Terrence, you hopped on? We're trying to get a perspective on prism timelines and any kind of major hurdles that still exist. And also if there's anything we can do to help.
00:34:09.860 - 00:34:47.596, Speaker G: Yeah, sorry I was a bit late. So I guess apology that we underestimate how many people were actually offline and on vacation this month. That's why Prism is slightly delayed because of dev connect. And the Thanksgiving week, we ended up redesigning a bunch of stuff because of Devnet twelve. So we added a new fire storage layer and then a new caching layer, and that's why it's taking a while. Because without those major revamps, if we just implement the pr as it is, it would have been faster. But yeah, we're still, like, I would say two weeks away.
00:34:47.596 - 00:35:08.150, Speaker G: But then again, we can join that net twelve, which I think you just launched this today as well, so. Yeah, you don't have to wait for us. But yeah, we will join when we're ready. Probably in two weeks, but, yeah, we should be ready for Goldie Testnet once it's out, but, yeah, that's pretty much it.
00:35:12.270 - 00:35:18.560, Speaker A: Okay. Thank you. Any questions for Prism before I move on?
00:35:22.980 - 00:35:31.368, Speaker B: Yeah, if you guys have a branch when we're ready, just let me know. Also, I can already have some integration tests.
00:35:31.564 - 00:35:47.670, Speaker A: Absolutely. Thank you. Okay. Anything else related to testing and Devnet? On the dev, we have a tool.
00:35:47.740 - 00:36:05.498, Speaker D: That can reliably do reorgs. Now the question is just, does someone have an issue that was triggered by a reorg that we can toss in the tool to see if it can catch it. Plus that issue being fixed so we can see that it can't catch it after it's fixed.
00:36:05.594 - 00:36:50.240, Speaker A: Right? So like an old commit as well as an old release and a new release. Anybody have a reorgan related issue from the past that they can point Perry to if you can't remember? Can you please ask your team and then knock on Perry's door and Perry maybe drop this in the discord for a broader ask. All right, anything else related? Testing and devnets on Deneb.
00:36:55.860 - 00:37:16.330, Speaker F: Not quite the devnets, but we're going to put out the Gordy blog post today around Gordy validators being exited the later of three months after the fork or one month after mainnet last call. If anyone has thoughts, comments on that. But otherwise the post should be up in the next hour.
00:37:19.340 - 00:38:11.574, Speaker A: Great. Okay, we have a minor spec release that is slated to come out tomorrow. This primarily fixes some testing in fork choice testing as well as we're working on getting this when clients conserve block inside cores by root clarifications on like May should, et cetera. I had a review I put up last week and then did not click submit, so it's a bit my fault. Yesterday I realized that. So we're doing some final stuff here. I don't believe there's anything shocking, but hoping to get it done today and get a release out tomorrow.
00:38:11.574 - 00:39:37.770, Speaker A: If not, release might come Saturday, Sunday, Monday. Are there any comments on this issue 3551 that would be kind of the most. We did talk about it on a call a couple of calls ago, so I don't think there's any surprises, but just wanted to give you a chance to service any comment or concern before this is finalized. Otherwise this is not breaking with respect to the devnets so it won't affect you can and should roll out changes with respect to this to Devnet twelve, but it's totally interoperable on the networking layer. Okay, there's actually one other clarification that might make it in there. This is pr three five four, and I wanted to just make sure that this is capturing what is actually happening. The byroute requests don't actually say that if you're responding to route ABC that you would need to send messages in that order.
00:39:37.770 - 00:40:20.930, Speaker A: I've assumed that is certainly the case that you're matching route zero to message zero, root one to message one. You're not doing a like you responded in some random order. But I might be totally wrong here. So before this goes in, does anybody have a comment on whether these are supposed to come in an order. Also maybe some clarification on duplicate request. The same route and the same request. So like root AAA should not be saying it should not be valid.
00:40:20.930 - 00:41:39.700, Speaker A: Or maybe you should return just a once. Yeah, exactly. Well, if that is set, then the order should not consider it and also the duplication could be removed. But if we want the order, I feel like duplication should not be allowed and then we respect the order. Yeah, well the order can also be weird, right? Because if it's ABC and you don't have a, you can respond B and C, which is the correct order by some standard, but you still have to do the matching properly, right? Yeah, the verification would be much more. Yeah. Um, is anyone aware of how this functionality is actually handled or is it kind of ambiguously handled? And maybe there's different functionality across clients at this point.
00:41:39.700 - 00:43:05.458, Speaker A: Okay, it doesn't seem like this is top of mind and that the people on the call have clarity on how this is currently done. We'll drop it in the discord and try to get clarification from some of the more networking folks on teams and get this clarified. Sir. Obviously things work, but some of the ambiguities here might be cause of issue in weird edge cases and be worth clarifying and cleaning up. All right, anything else on this one? This is not going to go in the release tomorrow. Okay. Any other Deneb items or just general spec items? Okay, Tim had a couple of small items related process.
00:43:05.458 - 00:43:06.340, Speaker A: Tim.
00:43:07.750 - 00:43:42.938, Speaker F: Yeah. So smallest one, adding fork folders to the PM repo. So we currently have things like devnet specs and whatnot, like live randomly on hackmds. We've had these main net checklists for a few of the forks that also end up living in random folders. So my suggestion is every fork, we just create a repo. Sorry, every fork, we just create a folder in the PM repo for that fork and put all those things there. And when the fork is over, we just move it to the archive folder.
00:43:42.938 - 00:43:48.786, Speaker F: So pretty uncontroversial, I think. But if anyone.
00:43:48.888 - 00:43:55.486, Speaker A: There's no even standard files that go in it, it's just no files. Irrelevant.
00:43:55.598 - 00:44:25.202, Speaker F: Yeah, so I'm not saying we have to create a maintenance. Yeah, but just we tend to. And now they either end up in hack mds or random places. So it's just like a default place for people to shove stuff that's relevant to the fork that we're working on. And usually we don't struggle to find them during the fork, but it's kind of nice that this way we'd have them in that repo forever after the fork. Yeah, sorry. Eat panda ops.
00:44:25.202 - 00:44:36.606, Speaker F: Hackmd is not random. Even if we keep using hack mds, we can just even link them from there and note that, yeah, you could.
00:44:36.628 - 00:44:39.726, Speaker A: Drop a doc that said relevant notes if you didn't want to put it in there.
00:44:39.828 - 00:45:21.194, Speaker F: Yeah, that was the first. I just wanted to wait until I merged it to see if anybody had comments, but it doesn't seem like there's any objections. Next one. So the other thing that's kind of weird right now is there's nowhere to know what's actually in a fork, especially for combined forks. So on the el we have things in these random little markdown files. So this is the one for Cancun. Obviously on the cl, the beacon chain spec has a list of the eips, but there's not like a single place where we have these are all the eips going into the fork.
00:45:21.194 - 00:45:51.960, Speaker F: The two place where they're listed for each half of the network are kind of hard to find and not necessarily obvious. So my other proposal is we bring back meta eips to just list what eips are in a fork. So we used to do this for some reason. We stopped a few years ago, I forget why. Yeah, right. The Ethpanda ops hacking is maybe the easiest place to find all the eips, but this is something that people.
00:45:53.770 - 00:45:54.086, Speaker A: On.
00:45:54.108 - 00:46:51.126, Speaker F: The outside aren't necessarily super aware of. The other place the eips end up being listed at the end of the process is like on the hard fork blog post, which is kind of weird to have to rely on the blog post to figure out where to look in the eips repo or in the yeah, my proposal would be we add a meta EIP, literally just as a list of the eips included in a fork. If we know the fork is going to be coupled, like Dineb and Cancun, we just have a single eip. So we don't have the duplication of eips like 444788. If we don't know that the forks are going to be coupled, we can just have one per side. In worst case, if the forks do end up merging together, we just merge to eips or something. And we can either do this right now for Denkoon or start for the next fork.
00:46:51.126 - 00:47:45.264, Speaker F: But it does feel like something where having a single list of eips for the fork somewhere is valuable. Especially now as well, that we split the EiPs and ERC repo. We can imagine starting to add more custom fields to the Eips and whatnot and aggregating them into a single fork EIP. So any thoughts on meta Eips and whether we should do one for Denkoon or just wait to the next fork? Okay, one comment to do it ASAP. I can get it done this week. Okay, two yeses, that's all I need.
00:47:45.382 - 00:48:01.988, Speaker A: I'm certainly not opposed. We're happy with how we bundle things into the specs. Yeah. And I'm not saying we should remove them. I know, but a matter reference somewhere, especially because of the cross layer, makes sense to me.
00:48:02.154 - 00:48:15.512, Speaker F: Yeah, and I think I can do this now, but I would link the specs to many. The bot lets me so saying like these are all the Eips in the fork. This is where the fork is defined for the Cl.
00:48:15.566 - 00:48:16.728, Speaker C: The sort of fork is defined for.
00:48:16.734 - 00:49:39.380, Speaker F: The El type of thing. Okay, so I'll draft it this week and then this is less urgent and just something for consensus layer folks to start thinking about if they want. So we have on the El side this weird status of considered for inclusion for eips, which are things that basically client devs think we should maybe do, but end up being like a superset of what goes into a fork. It's not clear to me if having the status is a plus or not. In the past it was helpful because if somebody comes on the call and they want a signal of like should they keep working on this EIP or not? This is like a soft signal we can give them, but it's also let the confusion of like okay, now we have this big list of stuff we might do or might not do. And so I was thinking on the El side, either we should remove this if it doesn't add value and just add confusion, or if we keep it, maybe it makes sense for it to also be used on the Cl if people want. And again, now that we have the split EIP process, you could imagine making this like a status of an EIP.
00:49:39.380 - 00:50:37.950, Speaker F: If we keep it where it's like something gets proposed for a fork, it can be signaled directly in the EIP that it's being proposed. So I have a link to eat magicians in the agenda. There's a bunch more context there, but yeah, I don't think this is a decision that needs to be made now. But as we start planning the next fork, figuring out, do we want to keep this state that's higher than random ideas from random people. But prior to accepted in the fork, which is roughly what CFI was, but I think it might be worth getting a more formal definition and updating it if both sides want to keep using it. Yeah, that's all I have. So I'll get the meta eip done and I'll merge the fork repo Pr.
00:50:42.960 - 00:51:05.240, Speaker A: Well, any other questions or comments for Tim Dano? Do a temperature check on the idea of putting fork fields on all of the eips and having placeholder fork fields for proposed so we can filter eips by that field to see when it was implemented.
00:51:07.340 - 00:51:23.550, Speaker F: Yeah, I'd like that. Personally on the EIp side, I'm not sure what's the best way to do it, but it would be nice to be able to know Berlin or Capella and get all the eips that were in that.
00:51:42.650 - 00:52:19.536, Speaker A: Okay, anything else on this one? All right, any further comments or discussion points for today? Okay, great. Talk to you all very soon. Thanks, everyone. Thank you. Thank you. Bye bye. Bye, guys.
00:52:19.536 - 00:52:23.930, Speaker A: Bye bye. Sa.
