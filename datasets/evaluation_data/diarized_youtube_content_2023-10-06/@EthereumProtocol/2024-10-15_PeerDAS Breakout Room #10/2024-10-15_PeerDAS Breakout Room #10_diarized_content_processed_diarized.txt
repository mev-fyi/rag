00:02:29.275 - 00:02:29.855, Speaker A: Hello.
00:03:37.745 - 00:03:52.341, Speaker B: Hello. Hey everyone. This seems to be working. Yep, yep. Hey, Great. Yeah. Shall we again today.
00:03:52.341 - 00:04:50.195, Speaker B: So, yeah, I think we are ready to start most people on the call now. Yeah, I can quickly start with updates from Lighthouse. In the past two weeks we haven't really made a lot of progress in peer das. The Lighthouse node has forked from the main chain and we've been looking to the issues and it seems to be related to how we do block lookups and it seems to struggle a little bit when there are multiple forks going on. So we've been looking into it, but we haven't made too much progress. Yeah, hopefully we can get it to sync again with the network. But yeah, I think leaving the devnet running is good for now because we try to see if we can sync our fixes to the to the rest of the network.
00:04:50.195 - 00:05:15.705, Speaker B: Other than that, we've also been working on the gap blobs v1 stuff. So less related to peer DAs. But yeah, that's currently where our focus is. Hopefully get the get lovely one stuff out first and then we'll get back to focusing on peer net. Yeah, that's pretty much all from Lighthouse. I will pass it to maybe Prism.
00:05:18.165 - 00:05:48.477, Speaker C: Hey. Hey everyone. Yes, during the last two weeks, yes we work on several stuff. First one is when running a data column by range request against full node. Sometimes this node does not return data columns correctly. It was a bug which was noticed by the TECU team. There is no issue for supernode, only for Prismful nodes.
00:05:48.477 - 00:06:55.957, Speaker C: And so we work on a fix on this bug. Also we work on optimization during the sync before. For example, if I don't know Prism node needed Poland 5 for block 10 and 7 for block. Let's say 15. So the present was requesting column 5 and 7 for blocks 10, 11, 12, 13, 14, 15, which is like a lot of waste of resources from the peer we use and for the overall network bandwidth and we work on an algorithm to really request by range only codons we really need. Also currently Prism is only able to think about data columns from peers which custody a super set of our own custody columns. Basically it means that Prism Prism node is only able to fetch data columns for super nodes which is an issue and we work on this.
00:06:55.957 - 00:07:51.935, Speaker C: Now Prism is able to sync for every node and also we have sometimes an issue during the sync when we're at the end of the sync, especially if there is a lot of different forks. Prism node could be stuck on a bad fork and so could be unable to finish a synchronization and so we work on this. That's pretty it. And yes, about the DevNet, it seems that other Prism nets are currently up running and proposing blocks. And that's all for me. Next one. I don't know if Teku is here or any other client.
00:07:58.075 - 00:09:19.095, Speaker D: Hey, I can go follow stock. So I, I don't really know the reason why the folks happened and maybe but I know that once the fork happened and some of the Lodestar nodes fell out of sync because they were restarted or whatever, they couldn't catch up with the fork that was up to the. Up to the slot head. And so I debugged and figured out the reason for that was that Lodestar was for example taking blocks from one fork which was not synced up till head and then it was trying to get the data from the other fork because the first fork itself might not have the data. And probably that was why it was stuck in the first place. And so even with the Lordstraw restart and rain saying LORSA was not catching up and that was particularly the reason for that. Although I think we have in our, in our logic of free instinct that basically we don't group the peers, we separate keep the peers separate for different folks.
00:09:19.095 - 00:09:56.625, Speaker D: But I need to know what is going on because it should not have happened that it fetches blocks from one fork and in the next round basically when the peer shopping happens, then it picks up the peer from the other forks. So I need to debug why that is happening, but I think that is the reason why none of the restarted load star nodes were able to sort of sync up even after the data wipe. And yeah, so basically this was a particular reason for it.
00:09:58.845 - 00:10:03.865, Speaker B: So are you saying that Lodestar nodes are able to sync from scratch to head.
00:10:06.735 - 00:11:02.203, Speaker D: So in for example, so lorsa node should have been able to screensh from. Because there was one fork I think in which there was also one lo node, one lighthouse node or I don't remember which other nodes. So there were two, three nodes which were still running unfinalized and still basically those are the nodes which were to the head of the slot. And I tried to make lots of basically sync of that. But again this problem was occurring where it was not able to sort of final sort of fetch a batch properly, download a batch properly, because that is what it was doing. And I did verify that the nodes there was. There were a few super super nodes in the synced fork and I did verify that they had the data that was needed to sync.
00:11:02.203 - 00:11:56.865, Speaker D: So that is what I Figured out was the reason that it was picking blocks from the unsynced fork and then it was and obviously didn't have the data. So it was then trying to get the data from the synced fork peers which obviously did not have it because it might because it was a rain sync and obviously they shouldn't be serving it anyway. So all that was correct behavior. But yes. So the bug in the lordsto is that it should have when it goes for reshuffling and to get data, for example, whatever data I didn't get in the previous round, it should not select the peer from the other fork and that is something I need to debug because as such by default it should not be selecting peers from the other fork.
00:12:03.485 - 00:12:13.185, Speaker B: I see. Yeah. Thanks Katinda. Who else wants to go? Maybe Solius.
00:12:13.965 - 00:12:50.845, Speaker E: Yes, hello. So handling from Fellowship program was working on Pyridas. He did the reconstruction because we didn't have that before. However, it seems there is still some bugs that prevents us even with syncing the Peer Net 3 network. So I mean from Genesis. So yeah, I think we will keep debugging. So we are not very ready at the moment.
00:12:50.845 - 00:12:53.405, Speaker E: That's all.
00:12:56.625 - 00:13:22.375, Speaker B: Thank you Solius. Have we got any other client teams on the call? Nimbus, Teku, anyone else? No, it doesn't seem like we have them. Yeah. Then I guess we move to devnet updates. Do we have anything from DevOps?
00:13:26.675 - 00:13:40.915, Speaker A: Not really. I mean network is broken. I'm happy to roll out any new images or delete the database and try to sync. I don't think we have any update from our side though.
00:13:48.935 - 00:13:57.355, Speaker B: Cool. Yeah. Before we move on, is there anything else that anyone wants to talk about regarding the devnet?
00:13:59.575 - 00:14:06.305, Speaker A: There's a proposal of relaunching with only supernodes. Not sure if that's something that we want to consider.
00:14:18.125 - 00:14:33.215, Speaker B: Does anyone have any opinions? We've kind of mentioned ours on the. On the Discord chat. Not sure if anyone else wants to have any thoughts about running a super node only network.
00:14:41.155 - 00:14:48.375, Speaker D: I mean I would basically like to wait a week so that I can sort of figure out the bug. Otherwise I'm fine.
00:14:50.755 - 00:15:03.075, Speaker A: Yeah, the relaunch wouldn't be like on this week. If they do relaunch it, it would probably be next week or even the week after. It would be very good to get this chain back to finalizing before we do that. I think.
00:15:09.175 - 00:15:39.075, Speaker C: As I said on the. On the Discord. So there is one third of super node in total on the network and currently there is 1/3 of sync super node as well on the network. So it seems that being a super node, not super node does not really help to think currently on the devnet. So I'm not sure that running a full super node network will help.
00:15:39.975 - 00:16:28.257, Speaker F: I mean I think one thing about that is that to begin with, the fact that the network breaks into pieces, it might not be the case this time, but it can always happen. Also because of the fact that there are full nodes. Like especially if we. It's like a relatively small DevNet and it's two thirds full nodes which I think on, on, you know, in the actual validator set that would never really happen. It seems like just making things like unnecessarily hard to me. Like yeah, it's, you know, it doesn't seem like in this case super nodes are doing any better, but on the other end it's, you can't really know that the reason that the more complicated situation happens has nothing to do with the fact that there's a bunch of full nodes. At least that's my.
00:16:28.257 - 00:17:12.755, Speaker F: Yeah, I mean I, I'm not, I'm not involved in debugging the devnet, so definitely not going to impose this opinion. But yeah, it seems unlikely to me that, that it wouldn't be easier to. To just get a dev networking with. With super nodes like that should really look a lot more like 4844 basically. And like yeah, it feels like it should be. It should be like a good first step. Like once you have that working then you know that most other problems should be because of partial availability as opposed to always wondering like is this because of partial availability or is this just some kind of, you know, random bug that is doesn't really have anything to do with peer dos.
00:17:12.755 - 00:17:13.375, Speaker F: Actually.
00:17:20.685 - 00:17:30.305, Speaker A: I think that's a good point and possibly we could do that for the next step net to reduce the scope. Reduce the scope at least.
00:17:41.695 - 00:19:23.645, Speaker B: Yeah, yeah, I agree that using full nodes kind of make everything a bit harder, but I also think that we should probably wait for a bit to debug the current devnet issues and see what they are and then maybe when we do the next launch we could consider having more validator weight in the supernodes and if we need to test full nodes, we can always like run full nodes locally for testing. It feels like it's. It's probably good to give that a try because having full node only network on us small peers like small nodes a bit difficult to really figure out what's happening. It's hard to find peers in the Right Columns. If we don't rely on super nodes, especially when we have fork, it get even worse. Any more thoughts on this topic? It seems like we just need a bit more time to debug the current devnet now and see what we want to do with the next devnet. But it seems like a good suggestion to consider for the next devnet.
00:19:23.645 - 00:19:54.675, Speaker B: Cool. Then maybe we can move on to next topic. Is there. Yeah, there's nothing on the agenda anyone like so it just wants to check if anyone wants to have any discussion regarding spec.
00:20:04.815 - 00:20:37.525, Speaker A: There was a discussion topic last week about the reconstruction, the speed of reconstruction and whether clients should reconstruct only finalized epochs or everything when they are forward syncing from Genesis. I think this came from the fact that Prism was thinking I don't know, one slot per every second or something like that due to the KCG commitment completion overhead.
00:20:38.305 - 00:21:49.875, Speaker C: Yeah, unless I'm mistaken, it was not about reconstruction, it was about verifying the proof. The KJ proof during the yeah, yeah, yeah. So good points. So yeah, so currently Prism has two, let's say two issues. First one is yeah, regardless the block is finalized or not during the sync we do run the all the verification including K verification which take really a lot of time and the majority of time spent during the sync is actually spent into the verification. Also because we don't verify KZJ proof by batch but secondary which is yes, we lose a lot of time in this so it's a bug we have to fix. But also yes, the question is do we really have to verify these K proofs for finalized block because they are already finalized and so normally they are already safe.
00:21:49.875 - 00:22:25.485, Speaker C: So the ideas was to when thinking was to not verifying proof for finalist block and verifying proof only for block which are not yet finalized. I don't know what other clients do I not for example Lighthouse. Do you verify KZ proof or finance block or not?
00:22:27.465 - 00:22:52.105, Speaker B: Yes we do at the moment, but I haven't really noticed any issues with the verification time. I have looked at the verification time myself. A single verification time looks okay, but I haven't looked at like what's how long it takes like on average for a block. So maybe something that we can look at next. Yeah, but curious to hear if any other clients have any issues with verification times during sync.
00:22:53.485 - 00:23:11.515, Speaker G: Just curious. When you are verifying proof during sync, how are you calling the verify batch function? Like how many like how many proofs do you verify per verification function?
00:23:12.775 - 00:23:48.085, Speaker C: Yeah, yeah, it's one issue in Prism we do not batch the verification, we do it basically block by block, which take a long time while. We should be able to batch multiple. Sorry, we do verification column by column, and we should be able to batch verification, and it should really improve our verification time. So it's one issue we have in Prism.
00:23:48.825 - 00:24:11.245, Speaker G: Okay, that makes sense. I'm wondering. Okay, so you're doing column by column. You can upgrade to doing it. Verifying the entire block in one call. That would be the next step. I'm not sure if there is a step after that where you verify multiple blocks in one go.
00:24:11.245 - 00:24:21.365, Speaker G: You know what I mean? Like, maybe you can pass multiple logs at the same time in the function.
00:24:23.135 - 00:24:27.315, Speaker C: Yeah, I have to see, I have to check.
00:24:31.655 - 00:24:52.765, Speaker B: Yeah, but I think back to the question whether we really need to verify. Yeah, I don't. I'm not sure, but I feel like it would be best if we verify and not rely on the majority even though it's finalized. Like, we. That's what we do with block. So I feel like we should also verify the kzg.
00:24:56.585 - 00:25:47.221, Speaker F: If you're someone that doesn't have the whole data, I think you definitely should verify because otherwise you can't know that if you just download a single column, you have to know that it actually matches the commitments. So I think it's really like a must to verify even if it is finalized, if you have the whole data. I think in principle, you could not verify the proofs and just kind of check the. Like you take a whole blob and you could just compute the commitment and check that it matches the original one or something like that. And maybe it's faster than verifying all the proofs. Like, you could just kind of forget about the proofs and just checked equivalence or like correspondence with the commitments. And then you could only maybe check the proofs if you actually, if someone requests, like if someone wants the column from you and you need to send it to them, you could check the proof just before doing that.
00:25:47.221 - 00:25:56.825, Speaker F: But I don't know, this might just be like, more complex than just checking the proof. And I'm not sure if it's that much faster to do it this way.
00:26:00.205 - 00:26:04.105, Speaker A: That would require you to backfill all the blobs.
00:26:09.695 - 00:26:21.915, Speaker F: Do clients not eventually get blobs for the whole period? Like the whole two weeks or whatever period, Even if, like, even for parts that are finalized?
00:26:30.095 - 00:26:36.075, Speaker B: I think we do, and I think that's the requirement to sync back to 18 days.
00:26:41.885 - 00:26:46.025, Speaker A: But you could start validating even without that. No.
00:26:46.445 - 00:26:47.701, Speaker B: Oh, yes, yes. Right.
00:26:47.733 - 00:26:47.925, Speaker D: But.
00:26:47.965 - 00:26:51.745, Speaker B: But we have to Serve the data if we can. So.
00:27:14.455 - 00:27:17.275, Speaker A: Could you not do the verification based on request?
00:27:20.775 - 00:27:28.413, Speaker B: As in like request column from peers. Do we not do verification? Was that the question? Yeah.
00:27:28.429 - 00:27:36.905, Speaker A: So basically you would only do the verification if someone requests that column from you instead of verifying while you're thinking.
00:27:40.245 - 00:27:56.355, Speaker C: On Prism, we only verify when we receive the column. If we run a colon by round request or colon by root request before storing the colon in the store, we run the verification. But once it's done, we never run the verification again.
00:28:03.775 - 00:28:05.555, Speaker B: Yeah, it's the same for Lighthouse.
00:28:09.175 - 00:28:17.555, Speaker A: Is it a big deal then that it's 25 seconds for like, what was it? 25 seconds per.
00:28:21.755 - 00:29:05.215, Speaker C: Actually, I think if I run a verification by batch and sticking the verification linearly like I do today, I think it will really solve the issue. It will take, I don't know, I have to do the math, but something like 2 seconds instead of 25. And so I think we can consider there is no problem anymore. And so yes, we can continue to verify for finalized block. It won't be an issue. Especially if as you said in the discord, a lot of people will probably sync from a checkpoint sync and so the sync won't last long.
00:29:07.755 - 00:29:12.916, Speaker B: Hey Manu, when you say 2 second, how many block is that?
00:29:13.028 - 00:29:13.472, Speaker F: Sorry?
00:29:13.583 - 00:29:20.161, Speaker B: For the 2 second time that you mentioned, like how many blocks is how many blocks?
00:29:20.313 - 00:29:31.993, Speaker C: No, I just. I just said two seconds like that. It's. It's not. I don't know. I just know we have. I just know for.
00:29:31.993 - 00:29:55.335, Speaker C: Let's say something like 64 blocks we had. I had 25 seconds of verification and. And so I Hope that for 64 blocks we can have two seconds of verification. But yeah. Really don't take this figure for scientific publication. I don't know.
00:29:57.115 - 00:30:15.355, Speaker B: Okay, I see. Yeah. So maybe it's worth for us to test out the optimization and see how fast we can get the verification to when we do batching. I don't know if we do bashing on range sync. I don't think we do at the moment. So that's something we can optimize for.
00:30:16.535 - 00:30:59.445, Speaker C: Actually, Justin just did some napkin math. So he said. Yeah, he said so yes, for 1,192 columns. So he got verifying column by column. He got 40 seconds on his computer and after batch it takes only 3, 3 seconds between 3 and 6, between 3 and 4 seconds. So it's quite a huge time 10 improvement using batching.
00:31:04.475 - 00:31:24.615, Speaker B: Nice. Cool. Then I guess we. That seems like a good next step to try to like implement the batching, the optimization on the other clients as well, I guess. Next topic, we could talk about metrics. Katya.
00:31:26.195 - 00:31:57.069, Speaker H: Yeah. Hello everyone. I would like to proceed with this pr. I will share in the chat. So I've split the big PR into small parts. So the first part is more about the DAS process verification reconstruction and I feel like we are more or less agree on this part. Do you have any.
00:31:57.069 - 00:32:21.385, Speaker H: I would like to share the screen and make a short demo of how it works. Here is the link for the dashboard if the clients have the access to the dashboard. Can you see my screen?
00:32:23.355 - 00:32:24.175, Speaker B: Yep.
00:32:25.515 - 00:33:52.023, Speaker H: So currently the SpiritOS metric specs dashboard is here in my folder but I hope maybe we will move it into consensus class later. So what we have now we have, we have filters where we can use super nodes or full nodes or both and we can choose the client as well and even the node separately. For now we have, for example, how can we use this dashboard? This is Lighthouse and we can see for this group of metrics that have some issues. I mean you don't only use logs. I think it's not easy to use logs because logs contain a lot of messages. But here you can see strictly to the point what issue you have. For example, for Lighthouse now we count the requests before gossip verification here and here we can see that there are some issues with verification and we see the failures as well.
00:33:52.023 - 00:34:46.402, Speaker H: And you can easily compare your metrics with other clients, for example Lighthouse and Techo if we do here. Oh no, sorry, not edit but view. So these are for supernodes and you can filter them and you can. Oh here today techo has issues for this metrics. Let's check full nodes. Oh no as well. So for example, you can compare your values with the values of other clients and sometimes it can be helpful.
00:34:46.402 - 00:35:11.795, Speaker H: For example Grandin, which is in the building mode, actively building mode it uses. So I would like to hear from you, what do you think of this group of metrics? They are about reconstruction, computation, inclusion, proof, if you have any opinions. Or we can maybe merge already and implement.
00:35:17.695 - 00:36:17.527, Speaker B: Yeah, I think lion mentions a suggestion last time to. To look at the metrics and consider like which ones are necessary to be part of the spec and which one are just nice to have in individual clients because yeah, I think the more standard metrics we have it will be harder for clients to change things. So it's a bit more rigid. So I kind of wanted to figure out whether we need all the metrics, are all of them going to be useful for every client or and is it worth comparing these metrics between clients? If it is, then sure we should include them in them all. We're just trying to figure out whether these make sense for all the clients. For example, the processing request total. I don't know if that's something that the older client track things this way because I think this is a bit closer to how we do it in Lighthouse.
00:36:17.527 - 00:36:20.505, Speaker B: I guess maybe other clients do it similarly as well.
00:36:24.605 - 00:37:19.723, Speaker H: At least in Teco I've implemented it for techo. Yeah we have the same now but probably for full nodes. It depends on how many sidecars your custody. And another note, if you implement the metrics with these names you will see them on the dashboard at once. And another one I would like to. Sorry, I just skip from the topic that currently Cortosis also has this dashboard. So if you just want to check and run it locally, yeah you can find it.
00:37:19.723 - 00:37:28.535, Speaker H: It is named peer DAS Nemetrics so you will find it easily. You launch Grafana in Cortosis.
00:37:37.605 - 00:38:07.215, Speaker B: I guess maybe now this is broken down into individual PRs. Maybe we could try to look in at least the first PR this one you have on the screen and maybe we can. Maybe all the client teams can have a look and see if it's. If everything's okay. Because I think Katja has been raising talking about this PR a few times. I think last time we had this call it was quite big and we decided to move out the P2P and the gossip stuff. I think now this is broken down to just peer das.
00:38:07.215 - 00:38:28.555, Speaker B: I think it's a bit more easier to maybe just discuss on these items that's raised. See if we want to add any more or even just modify any of these. I think maybe we could look into targeting getting this merge in the next one or two weeks if people agree.
00:38:43.065 - 00:39:24.595, Speaker H: At least I encourage you to try maybe at least locally. And if you have like I think Prism Team has this one metric that should be just renamed. Yeah and you'll see it on the dashboard. And please feel free to add the your issues here in the comments if you have issues about these metrics. And yes, Jimmy said we have another PR which is more we have a hot discussion about lipid P metrics and gossip sub metrics. So I think we will just move it to the next call.
00:39:29.055 - 00:39:45.275, Speaker A: Yeah, I would focus personally on the peer desk specific metrics because that doesn't require any renamings from table metric names. And all of these should be new metric names anyway for every single client.
00:39:48.215 - 00:40:19.015, Speaker H: And maybe if you have. If you Use any specific metrics that can be helpful for other clients. Yeah. Also feel free to share in the comments what would be nice to have as well because I just. I use the dashboard often and I can see the issues when they appear. So for me it looks like another helpful thing. For testing.
00:40:29.805 - 00:41:12.335, Speaker B: For Lighthouse, we also have a few more metrics that could be interesting. Maybe I'll. Yeah, maybe I'll come into this afterwards because when we're doing the Gap LOPS experiment, we were using a few more metrics and for example the computation second one. Sorry, the Beacon data column cycle computation seconds. That metrics will also have labels for it. I don't know if this is something all the clients want to implement because then we can kind of look at the numbers based on the blob count as well, so that sort of thing. I'll see if I can add a comment on the PR afterwards.
00:41:13.325 - 00:41:54.705, Speaker H: Yeah, thank you. And regarding to the seconds, I've experimented with the milliseconds and seconds and it seems like seconds are enough. Let me quickly show you. So these are seconds and these are milliseconds. Milliseconds seems too detailed. I don't know if you agree or not, so we can also discuss it but yeah.
00:41:56.925 - 00:42:12.905, Speaker B: Yeah. Sorry, this might go into too much details on the metrics, but I wanted to ask whether the bucket is also part of the specific. Because I think that would. If we're going to use the same metrics, the bucket will be quite important to also be part of the spec.
00:42:14.805 - 00:42:58.695, Speaker H: I think we can. We can proceed with it. I don't know, maybe later or we should do it now because for example I'm currently implementing histogram for techo and techo doesn't even have an opportunity to change packets at the moment. So this is what I'm trying to solve. Yeah. But I think it should be helpful as well. What? What How I see it, we can later add buckets as well and probably the average values so everyone can see.
00:43:15.885 - 00:43:33.497, Speaker B: Nice. Is there anything else or otherwise? I think the client teams should review this PR and add feedback the next week. Thanks Satya. Thank you.
00:43:33.521 - 00:43:37.565, Speaker H: I will share the second PR as well in the chat. Thank you.
00:43:38.145 - 00:43:45.125, Speaker B: Nice, thank you. Any other spec discussions?
00:43:56.155 - 00:44:14.895, Speaker A: One quick question. Has anyone started working on rebate branch on top of Vectra? It's probably still a bit far fetched but just a temperature check.
00:44:19.245 - 00:44:34.345, Speaker C: Yeah. In present pectoral is in the develop branch and we rebase regularly, let's say every two or three weeks. We rebase appeared as a branch on top of develop branch which include picture changes.
00:44:45.415 - 00:45:06.035, Speaker A: I'm just checking if we target Alpha 8 for if we do a relaunch with super nodes only. So that would be basically required I think a rebase for every client. But if it's too much work then I don't think we should spend extra cycles on.
00:45:16.025 - 00:45:27.121, Speaker B: Does anyone know like what's the state of the HR Devnet like how stable they are and whether they're launching the next one soon?
00:45:27.313 - 00:46:04.165, Speaker A: Yeah, so DevNet 3 is very stable. It's 100% participation right now. DevNet 4 is targeted to be launched by the end of this week hopefully. So there's still client implementations that needs to be done but as of this pack is kind of frozen as of now. But I think we might have at least one more DevNet before we can go on to fork topics next. But that's going to be a small change. Like I don't think we're going to have anything major added to picture.
00:46:04.165 - 00:46:27.345, Speaker A: We could of course wait till picture is absolutely stable and is rolled out before we do the rebates. I think the longer we wait the harder it's going to be. Possibly.
00:46:53.215 - 00:47:05.835, Speaker B: Great, thanks. Any other topics to discuss? Anything else anyone? Yeah.
00:47:08.135 - 00:47:35.475, Speaker H: Yeah I would like to make it clear these metric specs are the most for more for developers or for users because I find it super helpful for current testing process and I'm not sure like if it's needed for users for nodes operators.
00:47:36.295 - 00:47:46.275, Speaker A: I think it's more developers at least right now I don't see any users getting too much valuable information from KCG commitment computation time for example.
00:47:48.105 - 00:48:31.895, Speaker H: Yeah, I agree and that's why I want to point this out. Probably we shouldn't take a lot of time for these agreements or namings like to help the process of testing like to make it more effective. Yeah so I would encourage to try this metrics. Thank you. I mean it's important to agree but yeah just to keep it in mind that it's not for final users as for example Bacon specs. It's for our inner testing process.
00:48:42.925 - 00:49:21.245, Speaker B: Yeah, yeah, yeah, I agree. I think definitely more useful to have these metrics early on then later on. Great, thanks. Thanks. Katya. Yeah, I think last week, Sorry in the last breakout call Xiaoye and Josh sent out a form to fill out for the peer das contributors. I don't know if that's been finalized or.
00:49:21.245 - 00:49:55.495, Speaker B: But if you haven't filled out then just a reminder that the form. I think the form was shared on Discord earlier. If you haven't filled out. Great. Anything else, anyone? If there's nothing else, then I guess we can finish our call earlier today. Yeah. There's also daylight saving here in Australia.
00:49:55.495 - 00:50:39.555, Speaker B: I don't know about elsewhere, but I know that Europe is also having it in two weeks, so I think the time might suck a little bit for some people. Maybe we can revisit this in two weeks time. If anyone has any thoughts, feel free to share it in the Discord Channel. Thanks for the link, Francesca. Cool. Yeah, thanks everyone. We'll keep chatting in on Discord and we'll talk to you guys in two weeks.
00:50:42.695 - 00:50:43.635, Speaker A: Thank you.
00:50:43.935 - 00:50:45.471, Speaker H: Thank you. Bye.
00:50:45.623 - 00:50:46.487, Speaker C: Have a nice day.
00:50:46.551 - 00:50:46.703, Speaker H: Bye.
