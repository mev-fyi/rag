00:00:03.320 - 00:00:17.845, Speaker A: All right, let's get started. Welcome to Virco Implementers Call 23. I have shared the agenda in the chat. This is issue 1121 in TM repo.
00:00:20.105 - 00:00:20.465, Speaker B: Cool.
00:00:20.505 - 00:00:32.975, Speaker A: We don't have a huge agenda today. Yeah, anyways, so sorry. Starting off with updates from client teams. Does anyone have any updates they would like to start off with?
00:00:42.035 - 00:01:21.515, Speaker B: Okay, I guess we'll start. We've been working on the testing. Ignacio has some updates about this afterwards and also making some progress on the next test net. So we finally managed to get the branch with the all the Kenyan gas cost cover. So we've been testing it. It mostly works. We still have an issue updating the kurtosis to be able to build a test net, but once this is ready we should be able to to start.
00:01:21.515 - 00:02:09.927, Speaker B: Of course there's this new approach, but I'll let Ignacio talk about it where we would rather make sure everybody is able to replay the test before we even try to start a test net. But yeah, at least GETH is getting ready. Oh, and also I've been working on the fill cost for not the next test net but the one after. Yeah, so this is starting to get some stuff merged into Geth. So hopefully Barnabas just asked how is the rebase going? Well, it's painful, as expected. But yeah, the field costs are exactly that. We are trying to implement them directly on Master so that we don't have to rebase well to go through very, very painful rebase.
00:02:09.927 - 00:02:13.435, Speaker B: So yeah, that's what we'll be doing in the next few weeks.
00:02:19.385 - 00:02:49.621, Speaker C: I think I can go next. So from nethermind side for the test set we have everything implemented and I'm just waiting for test directors from VM so we can test like if the things that get generates can nethermind also process them as expected. And apart from that, we've been working on a few cryptography improvements and a major refactor to implement the transition. So might be able to implement a test transition in like next few weeks.
00:02:49.733 - 00:02:50.625, Speaker B: So yeah.
00:02:56.365 - 00:03:53.265, Speaker D: I can go next. So in the Bezu team we are working with Thomas, we are trying to modify how we are saving data in the database because we want to try to implement a flat DB based on the stem of the tree. So we are investigating this part. We have also a new guy and this new guy will work on the GAS cost implementation of Bezu. So if you want to contact him, it's Luis Pinto and we have also Gary that will continue to work on the transition. So for the moment I don't think BEZU is ready to is ready to join the new Devnet because we didn't implemented everything on the gas cost. But I hope soon we will be ready.
00:03:53.265 - 00:04:24.465, Speaker D: I just wanted to ask a question, Guillaume. Do you think it will be able to have at the beginning of the DevNet one transaction per block and each transaction testing a specific part of the gas cost or something else? I think it will be more simple to debug have only one transaction with a specific use case at the beginning. After we can have more complex block. But just at the beginning I think it would be nice.
00:04:25.645 - 00:05:06.565, Speaker B: Well, so this is exactly where we want to have the test. One block with very few specific transactions testing one specific aspect of it. So normally when you join the test net you already went through all this testing, so you wouldn't have a lot of very difficult debugging anymore. I mean, okay, you will probably still have a lot of difficult debugging, but at least the basic stuff will be covered. Yeah, I guess we would have to ask Barnabas or Perry if he's here, but I don't see a problem creating sending like regular targeted transaction. But like I said, we've got the test framework for that now.
00:05:06.945 - 00:05:07.825, Speaker D: Yeah, yeah, okay.
00:05:07.905 - 00:05:08.385, Speaker E: Okay.
00:05:08.465 - 00:05:18.045, Speaker D: If you have the framework it will not be a problem. But just if the framework is not completely ready, it will be nice to have this fallback.
00:05:20.665 - 00:05:23.737, Speaker F: Yeah, I can give the FJS update.
00:05:23.881 - 00:05:24.921, Speaker B: So we should be ready.
00:05:24.953 - 00:05:50.575, Speaker F: With all the gas costs that were discussed in Kenya, would be great to have these test sectors that we can test for the actual test. And yeah, as Karen was saying, I think the individual like gas cost changes are best tested as standalone test factors rather than test itself. Yeah, that's it on our side.
00:06:00.125 - 00:06:23.255, Speaker G: So, from Aragon, I was working on getting up to speed with the changes up to the last DevNet, which is DevNet 6. I wanted to ask if the DevNet 6 is still up and running before DevNet 7 comes up. All the changes I've done, I have no way to test it because I'm not getting any peers.
00:06:28.875 - 00:06:30.935, Speaker B: We should check, but it should be running.
00:06:32.275 - 00:07:00.445, Speaker G: Yeah, I go for something like a Folkmon or Dora. It's there. But I think when I try to ping any of those list of boot nodes, I don't get any replies to pings. I have tried various quirks and VPNs. Perhaps those are closed off for some reason, I don't know. Anyways, give me two minutes.
00:07:02.465 - 00:07:02.801, Speaker H: Sure.
00:07:02.833 - 00:07:22.555, Speaker G: You can update it on element or something. Yeah. So when the tests are available I can start implementing the next Incremental set of GAS cost changes for DevNet 7 as well. Thank you.
00:07:32.175 - 00:07:36.435, Speaker A: Sweet. Any other updates, client teams or testing set?
00:07:42.145 - 00:08:05.775, Speaker H: I can quickly on the testing side just pinged in the discord. We have our latest fixture release in the vertical tree migration ER&D channel. But yeah, I think Ignacio was going to go over some some of the testing stuff. I can jump in after Ignacio if there's anything else to cover.
00:08:08.715 - 00:08:16.655, Speaker A: Awesome. Thanks Spencer. Cool. No other updates. Then we can move on to agenda item two.
00:08:19.355 - 00:08:20.459, Speaker B: Okay, cool.
00:08:20.627 - 00:08:24.775, Speaker A: Ignacio and an update on the test framework stuff.
00:08:28.155 - 00:08:38.365, Speaker E: Yeah, let me share my screen. Can you see it?
00:08:39.705 - 00:08:40.485, Speaker G: Yes.
00:08:41.585 - 00:09:11.215, Speaker E: Okay. Yeah. So I want you to give an overall update of the state of Verkle trees tests. It has been. This has been work in the last weeks. I want to thank Spencer for taking some time or been making a lot of fixes in the testing library. I know the testing team has a lot of work to do regarding Vectra so yeah, just wanted to mention that.
00:09:11.215 - 00:10:20.915, Speaker E: So the situation today is that if you want to look at any kind of what is happening on the spectres for Verkle, you should see the burkle slash main branch in the execution spec test repo. So all the action around BURQL test is happening here. If you want to look at the existing tests regarding BurQL you can see them in the test/burql folder. So yeah, I like all these tests that I wrote are like separating by eips so you can maybe explore them a bit later. If you want to look at Some examples for EIP6800 you can just like take a look at. For example this is called chunking tests that I wrote. So the idea here is that I think I did a pretty decent coverage of these EIPs.
00:10:20.915 - 00:11:14.865, Speaker E: Of course like anyone is invited to really see this in more detail and add more test cases or things like that. But I wouldn't say that's very much useful now apart from running whatever exists maybe like in some months people can review this in more detail or add more test cases. Yeah, there are like some 4,762 tests. I separated them usually by instructions. So if you are. If you know about the IP you will. I think that this organization will make sense again you can take a look at any particular tests and hopefully also will make sense.
00:11:14.865 - 00:12:15.095, Speaker E: And some minutes ago Spencer created a new release. So yeah you can see a bit more detail about what happened here. But in the assets you can see these two tarballs. One is called Verkle Conversion fixtures, let's say, and the other Verkle Genesis fixtures. I will explain a bit more what these fixtures are about, but there are already fixtures that you can run today. So that's mainly from the Execution Spec test repo side. From the GET BURQL fork side we had to do many changes both for filling the tests, for consumption of tests.
00:12:15.095 - 00:13:53.375, Speaker E: So for test consumption, I mean the changes aren't really that complicated apart from what you might machine like having like a new fork in your configuration that is called Verkal and just filling the pre state in a Verkle tree instead of a Merkle partition tree and things like that, which is pretty obvious. For the filling of tests that's a bit more convoluted, mainly because the Execution spec repo doesn't have a pure Python implementation of Purko trees and some of the cryptography. So we had to create some new, let's say CLI helpers such that the testing library has to call these new CLI commands to do some kind of like regeneration and things like that. It isn't like the end of the world if you want to allow your EL client to fill tests, but it can be a bit confusing and maybe in some months it can be much simpler. So I won't go into much detail about that. We also have a new CI pipeline in GET where now for each PR that we do, we fill and consume the tests. I left the link here if you want to look at this later because some of you might want to do the same for RDL clients.
00:13:53.375 - 00:15:37.815, Speaker E: Here we have like the job for filling the tests, for consuming the tests. This has been pretty useful also on each pr this workflow also uploads the generated fixtures so you can use like the fixtures from the release in the Execution Spec test repo, which let's call them like stable fixtures. But for some weeks it might be probable that we will be fixing many things in Geth. So maybe you will be more interested in these uploaded fixtures on each PR that we do or if you are more like bleeding edge fixtures. So yeah, so the way that this CI works now is basically filling tests and consuming them and just checking that on each PR or branch this isn't broken and doing some kind of, let's call it branch self coherency because it's pretty obvious that on each PR you should be able to fill and consume those tests. What is a bit more interesting, and we will try doing soon, is also running tests from stable fixtures because it doesn't make much sense to Only run tests that you fill on the same branch. But that can be tricky for some weeks because we might be doing fixes and those stable fixtures might be wrong.
00:15:37.815 - 00:16:45.555, Speaker E: So yeah, this is something that's pretty common maybe when you are starting. But yeah, I just wanted to make this distinction about one thing is catching regressions by running tests from stable fixtures which usually you will pull from the execution spec test repo. Another thing is kind of testing self coherence. So regarding fixtures that exist today, here's a quick summary. I will separate them in two big groups, what we call burql Genesis which is basically saying these are tests where the pre state and the block execution happens only in a Burkle tree. So it's kind of a post conversion world. So for this setup we have all these vertical related EIP test vectors that I showed earlier which are around 161 fixtures.
00:16:45.555 - 00:17:55.449, Speaker E: And we also were able to kind of backfill or re existing tests, let's say a test from Shanghai but run on a Burkle fork. So there are a lot of already existing tests that are also run in only like in a Burkle tree. So that's pretty interesting. I mean despite these tests aren't specifically targeting like the new mechanics of some parts of Burkle, we might still catch bugs by running them on a Burkel tree. So these are like many more tests. The other big group is the overlay tree test vectors which I don't know if many Yale clients have this already. And this basically means running tests under the setup of having the pre state in a frozen MPT and doing the block execution in a vertical tree.
00:17:55.449 - 00:18:52.573, Speaker E: Right. This is the stage of the fork in which we freeze the mpt, we run blocks in the vertical tree, but we haven't started yet the conversion. So that's a whole completely different setup which is basically testing this overlay tree where you have to read new data from the Merkle tree but if it isn't there you have to go to the Merkle partition tree. So there are a lot of things that can go wrong there. So for this setup we have like a really simple multi block test that's pretty kind of dummy, but we also could backfill all existing tests. So this is like the same backfilling from here. But the difference is that all the pre state of existing tests are in the frozen MPT and whatever happens in the block execution happens in the Verkle tree.
00:18:52.573 - 00:20:09.805, Speaker E: So yeah, that might catch other kind of bugs and we get this like for free in the sense of bug filling already existing tests. And another thing that we do in the CI is filling and consuming tests from previous forks, which is basically just to check that the client can still fill, let's say a test from Shanghai and consume it on that fork. So yeah, all these burql changes haven't really broken executing blocks pre verkle. So regarding next steps from the execution spec test library side, we want to finish supporting what we call witness assertions. This is a new construction whenever you write tests in where you can do something similar to post state assertions. Just so the idea is that as a test writer you can assert that the generated witness in the feeling has whatever you expect for your test intentions. So similar to the post states.
00:20:12.345 - 00:20:12.657, Speaker B: And.
00:20:12.681 - 00:21:25.945, Speaker E: We have to fix some needs about the generated fixtures having the post state output which is usually useful for debugging the tests apart from just like checking the state route. Right. So what I would suggest as a way to go to DevNet 7 is kind of having people running these fixtures and gaining confidence that we are agreeing on like stuff before we launch it. And I would say would be useful to go in some steps. The first one that I would suggest is only running the fixtures related to 6,800 which are around 23 fixtures. This already cover a lot of things on the burkle tree workings like all these changes that we did for basic data encoding in a single leaf and things like that. Then we can jump into running all these backfill tests which are not many more.
00:21:25.945 - 00:22:26.055, Speaker E: After we have these witness assertions working, I would invite people to run all the 4,762 tests. I want to wait for this mainly because until we have this ready, I'm not like 100% sure that the tests that I wrote are actually like satisfying what I intended to test. And also these witness assertions will actually catch any guess bug. So yeah, and these are like more like 100 fixtures. But to be honest, like if you are greedy you can like run everything like 600 fixtures. But this might be a bit noisy but in any case it will be testing that whatever other clients are doing matches whatever geth is doing. So there's like no harm actually in running everything.
00:22:26.055 - 00:23:25.895, Speaker E: And yeah, just see what happens. Right. But I kind of suggest going in these steps just to be a bit more have a better like signal to nose ratio of testing. Yeah. And yeah, another next step is basically implementing new test vectors for state conversion. So a while ago, okay, so some weeks ago I created this EIP where I kind of Describe all this state conversion with an overlay tree that we have chat about for many a long time. I had this haagemd doc where I did a high level description of all the testing cases that will be useful for the cip which is like not entirely trivial.
00:23:25.895 - 00:24:14.285, Speaker E: So the idea is that yeah, I will be implementing all these tests so we can also have state conversion test vectors for later if our clients are planning to implement that which like state conversion is something we have to do whatever happens with like whatever target tree structure we use being vertical or binary or whatever. So this is like 90% work that it will be useful in the future for. For sure. And yeah, that's it. I know if Spencer want to add something else that's all from my side.
00:24:19.505 - 00:25:01.825, Speaker H: I think that that was perfect for my site. I just add that if anyone needs help consuming the tests, just send me a message. So I know Guys is considering the test using their EVM block test commands. I'm not entirely sure how other clients consume tests specifically, but if you guys run into issues with that, just let me know. On the execution spec test sites, we do have a alternative way to run the tests within Hive using block rlp. So if you wanted just a standalone way to run the tests immediately, we can help you with that as well.
00:25:13.005 - 00:25:54.875, Speaker A: Thank you Ignacio and Spencer. It's good stuff. Cool. If nothing else on that topic. Up next on the agenda, it was suggested that we do a quick sort of update or Sorry, more of like a sort of status check to see where teams are in terms of gas cost updates, some of the stuff that we discussed in Iota and just sort of seeing a doing a test net readiness check kind of. I don't know if folks want to just post in the chat where we are or anyone wants to chime in. Guillaume or anyone else that has any thoughts on this.
00:25:58.935 - 00:26:26.365, Speaker B: Yeah, we're. We're getting ready. We're. Like I said, there was this kurtosis issue that we couldn't test test nets that start at Virkogenesis. But we're going to fix this with DevOps like this week I suppose so as far as we know, we're ready. But of course the only way we know for sure is if other clients agree with us.
00:26:33.585 - 00:26:39.525, Speaker C: Nethermind is also ready for the testnet we have implemented. I think we have implemented everything.
00:26:40.315 - 00:26:41.095, Speaker B: Yes.
00:26:50.235 - 00:27:01.255, Speaker D: I said we still have some modification to do in the GAS cluster, so I hope we will be able to do that more quickly thanks to the framework.
00:27:08.165 - 00:27:16.625, Speaker I: Yeah, I was talking there with my mic muted. Yeah, what Karim said we are working through the implementation at the moment of the gas costs.
00:27:17.445 - 00:27:32.865, Speaker A: So. Cool. So I guess on the Aragon side still a work in progress as well. And ethjs same.
00:27:39.875 - 00:27:42.975, Speaker H: I think Gabriel should be able to update our progress.
00:27:47.795 - 00:27:49.375, Speaker E: Can you repeat the question?
00:27:52.515 - 00:27:54.375, Speaker A: Sorry, did you say something? Gabriel.
00:27:56.115 - 00:27:57.775, Speaker F: Can you repeat the question?
00:27:58.605 - 00:28:05.945, Speaker A: Oh yeah, sorry. Just seeing where each team is on updating latest gas cost schedule.
00:28:07.125 - 00:28:08.261, Speaker F: Yeah, so we are.
00:28:08.333 - 00:28:09.501, Speaker A: We should be up to date, but.
00:28:09.533 - 00:28:16.025, Speaker F: We haven't implemented the DES vectors. So once we do that, be more confident.
00:28:17.965 - 00:28:58.005, Speaker A: Cool. Okay, great. Alrighty. Sounds like we're getting there. If nothing else, then yeah, we can move on to the last agenda item, which is Guillaume giving sort of an overview. There's been lots of discussions or some discussion at least recently that I'm sure everyone has seen around. Just sort of the various paths that we can take and some folks feeling like maybe there should be more of a push towards binary trees versus verbal.
00:28:58.005 - 00:29:16.885, Speaker A: And so just wanted to share some thoughts as we. Several people have been sort of having those conversations and just evaluating and rehashing old conversations. And Guillaume of course was an author on the original binary tree eip. So yeah, I guess I'll just hand it over to you, Guillaume.
00:29:18.035 - 00:29:40.695, Speaker B: Thanks. Yeah, so I do have a presentation that some of you have already seen, or at least you've seen the older version of this presentation. Can I share my screen? Apparently not. Let me try again. It's always like this. Yeah, okay. Right.
00:29:40.695 - 00:30:11.535, Speaker B: Can you see my screen? Yes. Okay, cool. Yeah, so the BASIC team has already seen that. But yeah, there's been a couple updates that they might still be interested in. So like. Like Josh said, we've like there was an ACD two weeks ago where people were asking if it made sense to keep working on Verkol, if we should to the next. The next technology.
00:30:11.535 - 00:30:41.809, Speaker B: This is really. I mean clearly I am convinced that the answer is yes, it makes sense to work on Verko. But yeah, I would like to share some of the elements that we base our thinking on. So can I change slides from. No, I have to change Windows. Okay. So as a starting point I want to remind everyone that there was a.
00:30:41.809 - 00:31:01.485, Speaker B: Like the Verge includes a lot of steps. Vertical trees are really the first step. It has always been planned to do something else afterwards. Snarking and then stalking Ethereum. So that could be a lot of. A lot of. There are lots of ways to do this.
00:31:01.485 - 00:32:12.345, Speaker B: Clearly there has been a lot of progress on the Stark front recently. This being said, there's still a lot of advantages that Verkal still have over binary hashes. The first one, and I would say in my view the main one is that it stops or it slows down the state of the, sorry, the growth of the state. Because not only don't you, not only do you not need to store every single hash in the tree thanks to the homomorphic properties of polynomial commitments, but on top of that you do not. Yeah, you, you can reduce the arity of the tree so you have a smaller tree, a tree that is wider but shallower. And since you don't need to store every sibling like sibling hash, you're saving quite a lot of data. So that's, I would say, the advantage that other proposals don't really compete with.
00:32:12.345 - 00:33:20.415, Speaker B: And then there's of course the compatibility with snarks, which might or might not be an advantage depending on how fast each side of the ZK world develops the fastest. And there's this proposal by hand that was made two ACDs ago based on state expiry. And the advantage of his proposal is that it really fits, it fits really well into the vertical model. So if we want to implement state expiry, which is also a way to, to slow down state growth or at least to deal with state growth, there's a pretty much ready made proposal for this which others don't really have. But yeah, the other big advantage of vertical trees are hashing, sorry, our proofs. The proofs, proof size, if you have. So I'm taking those numbers from some presentation that Dankwood made.
00:33:20.415 - 00:34:40.015, Speaker B: So this is my source, but you have about half a megabyte of fixed proof size and then you need to pass the witness and with Verko we have actual numbers of replay that you might have seen in the previous Vic and Witness plus proof are depending on the encoding method we choose just about above 400 kilobytes on average to just under 400 kilobytes on average. So this is, this is really where Verkal shines and this is really where the strength of Verkal is, because these are proofs that you can not. Sorry, that you can reuse. Unlike for example, if you just snark the whole block execution. Yes, you know the block execution is correct, but there's nothing else you can do with that. Now on the other, the other side of the coin is that in vertical the hashing and the commitment computation are pretty, are pretty slow compared to regular arithmetic hashes. The quantum resistance aspect has gotten a lot of attention recently.
00:34:40.015 - 00:36:10.997, Speaker B: So clearly everything that is based on elliptic cryptography is going to be broken and apparently because I'm not a Stark expert, but apparently it plays better with Starks. So yeah, that's basically the choice we have up to some extra piece of information that we'll talk about at the end. But yeah, the question is what if you, if you look at the amount of work you have to do to switch to binary tries, what would you have to do? Well, the design and there's been some conversation with the research team, the EF research team about this. What we want is to keep those two things, both Verkal and binary trees, as compatible as possible. So what we would change is of course the proof system. We would be using Merkle proofs or some kind of ZK proof that does that and we would be using, we would be getting rid of polynomial commitments, replacing them by hashes and key computation, also getting rid of person hashes and using some type of arithmetic hash. But most of it won't change because most of those changes are actually targeting or at least designed to bring about statelessness.
00:36:10.997 - 00:36:46.665, Speaker B: They're not Verkal related. So the gas cost that disincentivize growing the block size or the blocks witness size, these will remain because we don't want to grow the size of the block. The single tree structure is the same thing. It's meant to, to simplify the proof, to make it, to make it a bit smaller. So there's no reason to change that. So that would be a single unique binary tree instead of a unique vertical tree. And in fact that was already the case in 3102 which is the binary, the initial binary tree proposal.
00:36:46.665 - 00:37:37.995, Speaker B: So this is. Most of that stuff is actually older than Verkal. The conversion itself would remain because you would have to rehash first you're merging a lot of trees into a single one and on top of that you are changing potentially the hashing method. So you would have to rehash the whole tree. So pre mesh distribution tree conversion, all of that remains and in fact all of that has been is also preceding vertical trees. The conversion like the overlay method was proposed EIP2584 if I'm not mistaken, was proposed to perform the conversion from an NPT to a binary tree and then the sync. So in my view it's pretty much the same because it's only dependent on the structure of the tree.
00:37:37.995 - 00:38:11.153, Speaker B: Tanish might disagree on that and he's the one that has worked on the topic the most. So I'm happy to hear a dissenting voice. But from what I can tell, it's going to be pretty much the same thing. Right. So I'm adding some information. What would the structure of the tree look like? Well, basically instead of being nodes of width 256, you have a binary tree. You would still get the value.
00:38:11.153 - 00:38:59.835, Speaker B: So because the binary tree tends to expand, hence the increase in size, I have to put a lot of dots to make everything fit into one picture. But every time you see three dots that means there's an entire subtree there. And what happens is that you would get the 256 values that you hash, that you hash over and over, you get the top of the suffix tree, you hash it in with the stem and the leaf marker, sorry and the subtree marker. Exactly what we do in vertical trees right now. And that would be your structure. So the hashing method is a bit different. It's not just a binary hash, but it's pretty much the same idea.
00:38:59.835 - 00:39:43.463, Speaker B: And yeah, so the structure is the same. You still get the 256 groups, you still get the opening like the cost for opening a group, the cost for accessing an individual leaf, the non deletion of leaves, et cetera, et cetera. Everything is the same now. Yeah. The question is how do we make it future proof? Well, clearly Starks recently have been good at, have been good at hashing. Quantum resistance has been a question recently, but there's still a lot of issues. So for example, Star security is conjectured.
00:39:43.463 - 00:40:38.065, Speaker B: I got some pushback from Netherlands mind on this, but from what I understand this is the case or more exactly this is the case. As long as you want to keep the proofs small enough. The performance has improved, but it's still not there. We can't imagine every single validator building their own Stark proof within the four seconds they have to build a block. So we might need to change. Like this would entail a lot of changes in the, in the whole Ethereum model, not just the storage tree model. If we, if we decided to get rid of this requirement and we also can't rule out that snarks could potentially get much faster as well, in which case there's no clear cut path anymore.
00:40:38.065 - 00:41:45.355, Speaker B: And when it comes to quantum resistance. So the NSA has what I'm saying recently but not too long ago they created a recommendation, a new version of, I forgot what they call it the cnsa, what it stands for. But it's basically their recommendations for the cryptographic toolbox that everybody should have. And you can see that for example they themselves don't Expect much to, or at least they want firmware signing, like everything that is going to be hardware that has to be ready before everything else to be ready by 2030 and everything that is more like software to be ready by 2033. So there's really no rush in my view, to try to fix quantum resistance in the state right away. Because even when quantum resistance is. Quantum resistance becomes a real problem.
00:41:45.355 - 00:42:31.605, Speaker B: People are going to attack communications, they're going to attack a lot of things before they attack the. Attack the state itself. So it's not like everybody's going to have a Quantum computer by 2033. It's more like very big companies that have better things or better interest than hacking the Ethereum state will, will start, we'll start creating those, we'll start using them. But yeah, like I said, there's no. Of course there can always be a surprise discovery tomorrow, but from what we can see, there's plenty of time to, to work on, on quantum resistance and getting it right. And then.
00:42:31.605 - 00:43:07.705, Speaker B: Yeah, then alternatively just rushing and potentially getting it wrong. There was something that was missing from the initial presentation that has been, once again, thanks to netherminds for pointing this out. It would be to create some kind of Verkel equivalent using something called hash, which is quantum resistant. So I had to talk with Gauti, so I hope I'm not saying anything stupid, otherwise please correct me, Gauti, but.
00:43:08.565 - 00:43:10.665, Speaker I: It'S called itine, not hashtag.
00:43:12.125 - 00:43:45.181, Speaker B: That's a start. Thank you. So, itine hash. So, yeah, it looks a lot like the cryptography we currently have in Verkol, except it's not using elliptic curves, so it's using lattice based cryptography, which is quantum resistance. It still has the properties of homomorphism, which I was talking about at the beginning of the. Of the presentation, which are really desirable for state growth or to stem state growth. Supposedly faster computation.
00:43:45.181 - 00:44:44.757, Speaker B: Although, yeah, got you told me to check some stuff, so I don't know for sure, but yeah, the big problem is the size of their commitments. So quoting my conversation with Gautier, that's between the commitments. Each commitment would be maybe 2 kilobytes, maybe 8 kilobytes. So that's way too much. And that would completely destroy the benefit of having improved computation, like faster computation, because you have to store this in a database and that would kill performance. Yeah, but nonetheless, I think that's an interesting point to study and maybe in the end we get ITA as a hash instead of elliptic curves. But yeah, so as a summary, there's a Lot of very desirable properties that we can have now.
00:44:44.757 - 00:45:34.905, Speaker B: And I like to say done is better than perfect. Even if we switch to binary choice today, we will get the exact same issues that people don't like. The change, gas cost, the conversion, everything. We are also in a period where a lot of discoveries are made, so it might make sense to wait before changing, before finding the perfect technology. But until we do, we have something that works. It's vertical trees. And in case we do find the perfect technology by next year, because those two structures have been designed to be fairly similar, it's not unrealistic to do a last minute swap of the cryptographic primitives because that's the only thing that changes.
00:45:34.905 - 00:46:06.235, Speaker B: And yeah, so that's, from what I understand, quantum, yes, quantum could happen tomorrow, but most likely not. And I think there's no reason to panic and it's not a good reason to change the primitive right now, especially when it's so easy to change it. Well, so easy. Relatively easy to change it in the future. Yep, that's pretty much it for the presentation. But the point was also to figure out if people have questions, people have worries. So yeah, we hope to answer those.
00:46:06.235 - 00:46:16.921, Speaker B: Yes.
00:46:17.033 - 00:46:17.925, Speaker A: Karthi.
00:46:18.625 - 00:46:59.525, Speaker I: Yeah, just a quick comment. I wanted to sort of add to this kind of tldr. I mean, I'm not sure about this MSA site or this national government side that you have sort of shown regarding sort of the timeline for the post London transition. I mean, I mean, first of all, this is a bit of outdated data. I mean, given probably, I mean, I don't know from when this is, probably from before 2022. I mean just from the, just from this. And you have to be aware that these kind of transition timelines are sort of not just taking into account what needs to be done, but also sort of what can be done more directed towards a bit slower changing users.
00:46:59.525 - 00:47:59.849, Speaker I: I'm of course not an expert in quantum computers and I don't have, I'm not a clairvoyant knowing when quantum computers will actually arrive. Just to inform you, we've had an interesting talk with Scott Aronson last week. Sort of basically the godfather of quantum computing in some sense about sort of his estimates and so on. And it seems that a lot of people in the area are estimating now that quantum computers are arriving, potentially are arriving much earlier than was previously anticipated. So this 2030, I mean, this is the timeline for actually changing, not the timeline for actually emergence of, I don't know, quantum computers in the hands of a few and Given this kind of information, I think this is. I would feel uncomfortable with 2030 as a time. I mean as you said on your last slide already, I mean we can always change and so on.
00:47:59.849 - 00:48:08.805, Speaker I: But just saying that this is way ahead and we still have at least 10 years time. It's not a sense that we're comfortably defending.
00:48:11.985 - 00:48:14.153, Speaker B: I didn't get quite get your last sentence.
00:48:14.329 - 00:49:33.012, Speaker I: Oh well, I mean the question is really how much time do we have for sort of to get sort of to transition to post quantum secure schemes and sort of they see and as any timeline is sort of suggesting that yeah we comfortably have like at least 10 years time to make every change in order and do it in orderly and fashion and so on. Of course this is a bit of a prediction that it's hard to sort of know that it's hard to really know. But given what sort of happened, the speed by which things have picked up, by which is mostly improvements in the fidelity of actual quantum systems, not much size, but rather the fidelity of the qubit from sort of the benefits of the games that have been made in the last couple of years and also some kind of what sort of result the experts seem to think then I mean we could actually have a quantum computer by that time frame that by the truth by 2033. Let's put it like this. If everything proceeds and if the key indicators are proceeding at the same speed as they have before and they have actually progressed kind of linearly. Ish. And then we would actually have a Quantum computer by 2033 just to give 2020.
00:49:33.012 - 00:49:55.315, Speaker I: By 2030, 2033 we would actually have a quantum computer probably if everything progresses continuously linearly in the measured as measured by some key performance indicators for this we would actually get them by this time. Not sure that this is an accurate prediction, but just to make a bit of a warning that I think the NSA timeline is a bit optimistic.
00:49:57.255 - 00:50:38.085, Speaker B: Yeah, so I was in the presentation too by the way. That's not. I mean I heard what he said. There was a lot of caveats in that presentation. He was also insisting on the fidelity thing, the aspect because people keep for example talking about having thousands of qubits but they are so extremely unreliable. And he insisted on the fact that you needed to ensure. Yeah basically that they could remain in the state you want them to remain for a long time, which is what they're struggling with.
00:50:38.085 - 00:50:54.845, Speaker B: And so currently they batch qubits together. I read a bit more on things. So once again it's just me reading so far the shor algorithm. The biggest number that has been factorized by the shor algorithm is 21.
00:50:56.025 - 00:51:20.235, Speaker I: Yeah, but that's misleading. That's kind of misleading because the point with this kind of looking at fidelity is or quantum material that once you reach a certain threshold about a certain fidelity threshold, what you can sort of do is you can actually start using quantum error correction. And this quantum error correction really has a threshold. Once you hit a certain threshold you can pretty much do anything.
00:51:21.135 - 00:51:24.439, Speaker B: Right. Right. Yeah, but okay, my.
00:51:24.607 - 00:51:36.779, Speaker I: You can only factor the number, I don't know, 21 or whatever. That, that's kind of not really telling you sort of what's the biggest number that can be factors kind of not really informative for this kind of decision. You need to take.
00:51:36.947 - 00:52:03.485, Speaker B: Well, I mean the, what it tells you is that it's very hard to, to factor bigger numbers and. Yeah, like apparently. Okay, so I read some numbers. I can find a source for you if you want. They were saying you would need to be able to have 20, 20 million qubits to be able to use error corrections and everything in such a way that this breaks to 248 bits RSA. That's, that's okay, I'll find resource, I'll share it with you.
00:52:04.305 - 00:52:10.685, Speaker I: Well, just two things. First of all, we're not trying to break rsa. We want to break elliptic curves, which is easier.
00:52:10.985 - 00:52:11.725, Speaker B: Right.
00:52:13.505 - 00:52:25.115, Speaker I: Secondly, the number of qubits you need depending on the quantum, on the, on the error from the quantum error correction should actually depend on the fidelity. So that number sort of, that blow up number actually depends on.
00:52:27.335 - 00:52:27.775, Speaker G: Yep.
00:52:27.815 - 00:52:45.795, Speaker B: No, agreed. I mean maybe I could. I just interpreted completely differently but in my view that's the big question. I don't think it's that optimistic. But just to answer the question about the CNSA timeline, this is the recommendation. Celestial. I don't know where it comes from, but it hasn't been updated, that's for sure.
00:52:45.795 - 00:53:00.695, Speaker B: And this is the recommendation that the NSA does to American administrations and companies to make sure that by 202023 they will be quantum secure. So clearly they don't expect something sooner than this.
00:53:01.795 - 00:53:04.095, Speaker I: Of course, having them.
00:53:05.275 - 00:53:05.963, Speaker B: Sorry.
00:53:06.099 - 00:53:08.595, Speaker I: Or the NSA is the only ones having them.
00:53:08.755 - 00:53:26.985, Speaker B: Right. Or even worse, they, they know it will never, it can never work the quantum computer, but they know they can break lattice based cryptography and therefore they want everybody to start using it because. Yeah, we don't know for sure. That's. But yeah, okay, fair enough, fair enough. Points.
00:53:27.925 - 00:53:59.321, Speaker A: I guess just to summarize because when I have a few Minutes left. Obviously, this is an important conversation. It's a very big and complex conversation around quantum timelines. And I guess maybe to summarize or sort of try to frame the question correctly, in my mind, I think it's mostly around, like, by what date should we have the protocol generally not just state. Not just sort of Verkel, but what date should we have the protocol generally. Quantum resistance. I don't know, Gati, if you disagree with that sort of framing.
00:53:59.321 - 00:54:21.545, Speaker A: But I think to me, it's first, like, okay, what is that? Of course, we cannot predict the future, but what is that date? And then working backwards from there, we can use that date to inform the question of Verkal and whether or not Verkal is still viable. How many years do we need to. What is the minimum lifespan of Verkal for it to make sense to ship?
00:54:22.525 - 00:54:51.111, Speaker I: I mean, the question is. I mean, that's one way of phrasing it. I'm not sure whether it's the right one, because, I mean, you can also ask sort of for the question about what would be the minimum timeline for us to actually transition away from verbal? I mean, if we went to verbal, how long would it take us to sort of to shift from virtual to. I don't know, to change it? And what. How much work is it also to change from vertical? Say we shift vertical.
00:54:51.263 - 00:55:19.395, Speaker A: Right, Right. But. But if we can work backwards, we can figure out all those things once we have sort of. If we can come to some kind of agreement internally. I mean, this is maybe a conversation for acd and I guess maybe I would say this is a separate conversation around quantum timelines that we should. That maybe we should bring back to acd now that we can't discuss it here, of course, but. And just like a broader conversation around quantum that we should have, I think as a community and core developers, researchers, everybody.
00:55:20.895 - 00:55:22.839, Speaker B: Someone else wanted to have say something.
00:55:22.927 - 00:55:27.943, Speaker A: Yeah, yeah, sorry, I think. Michael. Michael, did you have your hand raised? Sorry about that.
00:55:28.119 - 00:55:31.191, Speaker B: Yes. Yeah, thanks for the presentation.
00:55:31.303 - 00:55:45.495, Speaker E: You mentioned that itai's commitments are just too large. Could you give some number so that you know, the size of the commitment would need to fall below that number to be acceptable in size.
00:55:48.315 - 00:56:22.985, Speaker B: That's a good question. But it's roughly 64 bytes. Anything bigger than this, we're going to have to do database performance to figure out where the cutting point is. We know that 32 bytes, 64 bytes is fine. We know that 8 kilobytes is not fine at all because this used to be the size of a node, of a Verkel node. So somewhere in between there's a cutoff. But yeah, I could try to benchmark this and give you a proper answer, but yeah, the target would be 64 bytes at most.
00:56:23.925 - 00:56:24.945, Speaker E: Thanks a lot.
00:56:28.715 - 00:56:43.595, Speaker I: Just a question. How much does this depend on the arity or any kind of other computation? Because I mean, the question is, of course also what kind of nodes, how many internal nodes you would have to store. Right. Or can you say something about that?
00:56:43.755 - 00:57:18.583, Speaker B: Yeah. So the two things that kill performance are on the one hand, how much data you write per node and how much you jump around the disk. So if you increase the arity and you don't store all the associated commitments, you just stored. So you just stored the commitment to a node, but not the commitment to every single child, which is what homomorphism lets you do. Which is great. But yeah, so the more data you write, the more the database will churn. Basically, it needs to unpack, like it's built as a tree itself.
00:57:18.583 - 00:57:41.785, Speaker B: So it needs to unpack a tree, make room, shuffle stuff around. It's very poor performance. And yeah, so if you increase the arity and you don't store the data, you can afford to make the commitments a bit bigger. But not like 8 kilobytes bigger, because 8 kilobytes. This is what we used to write. And that was destroying performance.
00:57:58.175 - 00:58:13.567, Speaker A: Okay, well, we are at time. Any last thoughts before we break? Okay, cool. Thanks everybody. See you next time.
00:58:13.711 - 00:58:15.665, Speaker B: Yep, thanks. Bye. Yeah.
