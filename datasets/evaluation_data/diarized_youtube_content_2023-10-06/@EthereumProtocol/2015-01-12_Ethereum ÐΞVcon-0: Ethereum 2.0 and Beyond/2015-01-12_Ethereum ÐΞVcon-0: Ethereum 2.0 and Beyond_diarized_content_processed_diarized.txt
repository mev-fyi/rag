00:00:15.530 - 00:00:59.754, Speaker A: This is the last presentation for the day, for the week in Python. That's index negative one. Unfortunately, c plus plus and tiltlic supports that particular feature. One of the many reasons why it's inferior. Basically, here I'm going to give, in part, some of my own technical, economic, philosophical overview of where I see the future of OES. In part, Ethereum, in part other blockchain related and decentralized computing related, related technologies going in the next two years, five years, 20 years. So in part, the kind of perspective that I generally have on this whole space is that I see a lot of interesting ideas.
00:00:59.754 - 00:02:27.030, Speaker A: I see a lot of definitely very cool math, definitely very cool economics and cryptography and so forth. But the challenge is trying to figure out exactly how a lot of this stuff can end up actually being useful. A lot of the ideas that I have are around trying to figure out sort of what is the sort of fundamental thing, what is the fundamental set of parameters that this technology fills in a way that's better than any other technology that's available. And so in those areas, those areas where crypto does well and where other areas don't exist at all, those are in theory going to be the areas where you can sort of look for practical applications that are stable long term. And by stable long term, I mean stable in the sense that people will use them, not just because, oh, hey, cool, it's bitcoin, or hey, cool, it's decentralized, yay, I got to keep my data, but rather because they are, in some meaningful sense, better and people actually care about. They'll have properties that people like and the weaknesses will be so small that people won't care about them. An important point is that everything we're doing here might end up being completely useless.
00:02:27.030 - 00:03:20.934, Speaker A: There's a large probability of that. When you're in a startup, you kind of have to accept that, well, when you're doing any kind of technology, you kind of have to accept that. How many failed designs have there been for fusion reactors now? But the way that you do development is you sort of assume that your path is going to succeed, and you try and go down that path, and then maybe if you see some fork, you pick the branch of the fork that's more likely to succeed. But you sort of imagine things are going well, and if it all fails, then okay, the bottom threshold you can't go below is basically the amount of time you've wasted, and that's fine, but hopefully you'll discover something that's actually great and useful. So how do I even move forward here. Yeah. So I got a couple of sort of secret in this presentation.
00:03:20.934 - 00:04:06.822, Speaker A: I got a couple of sort of secrets in the Peter Thielean sense of the word. So first one is the history of software development is a history of human beings deliberately employing progressively less and less efficient software paradigms because we like them for other reasons. So 1980s assembly, obviously the best and only true way to write code, beats some of these by like a factor of several hundred, machine code and hex. Machine code and hex, even better, actually, even better to just make an ASIC. So, yeah, 1920s Asics, forgot that one. Then we invented this thing called c plus plus, which adds a whole bunch of overhead. It makes things several times slower.
00:04:06.822 - 00:05:17.650, Speaker A: And yet for some, and yet for some reason, for some strange reason, it won. Then we invented Python and JavaScript, which are, oh my God, even worse. But people like them because in this case it's because they make development easier. And so the idea is there's an established history of people, in some respects, sacrificing efficiency, in some cases sacrificing efficiency by over two orders of magnitude because it provides some ancillary benefits, and because as technology improved, it turns out that for most use cases, the amount of computation that we actually do is pretty trivial. And so slowing it down even by like a factor of 10,000, actually isn't all that bad. So the sort of theory that may be correct that I'm working on here is, well, what if decentralized consensus protocols are sort of one of, sort of an element of this trend? So not the next element of the trend, because this trend is about making things easier, and this trend is about making things trust free, but sort of falling in a similar direction. So first off, one thing that people get mixed up about decentralization is there's two kinds of decentralization.
00:05:17.650 - 00:06:02.646, Speaker A: Well, there's several kinds, but these are two kinds. So in general, if something is decentralized, what that means is that nobody controls it, and that fact by itself mainly has political benefits. Well, so I would also add in the benefit of reliability. But in general, the fact that nobody controls it is to a large extent the reason why things should be decentralized as a category in itself. Now, within decentralized there's also this concept of things being distributed. So decentralized and distributed are quite often used as synonyms. In this case, I'm making a distinction, and chances are there are going to be people who try and make different distinctions, because all these words are not too clearly defined.
00:06:02.646 - 00:06:43.870, Speaker A: But here the distributed means that the work is actually split up between many parties. And when decentralization is the distributed kind of decentralization, you can actually see unambiguous technical benefits from making things distributed. So you can provide here. And those technical benefits are so large that even centralized entities are willing to adopt the paradigm. Best example, World of Warcraft, in order to World of Warcraft actually seeds updates for their platform using torrents, and a whole bunch of other software does that. Ubuntu distributes Linux using torrents. A whole bunch of people distribute their stuff using torrents.
00:06:43.870 - 00:07:26.380, Speaker A: So that's one kind of decentralization, the other kind is consensus. So consensus is somewhat different. So consensus, nothing is being split, work is just being massively replicated. So when work is massively replicated, that's always going to be less efficient than just doing everything on one server. That has to be done for other reasons. In general, how do you think about consensus? The basic properties that it has are it's reliable, guaranteed to keep working. I think in the long term you'll definitely be able to count on it being much more reliable than any one particular service.
00:07:26.380 - 00:07:58.790, Speaker A: One particular service is in a lot of cases likely to either shut down, but transparent, globally accessible, not controlled by anyone. So it's not going to shut down. It's also particularly not going to become evil. It's not going to start installing CRM, it's not going to change its API, it's not going to try and throw captchas into its API because it wants to remarketing revenue. But for all this, or at least in Ethereum 1.0 and bitcoin 1.0 and every other crypto 1.0,
00:07:58.790 - 00:08:45.134, Speaker A: the price you pay is it's 10,000 times slower. If it's a C plus plus implementation, then that's 10,000 slower than C Plus plus and maybe a million times slower than assembly. And if that's a Python implementation, then it's 100 million times lower than assembly. So less efficient in theory, but the idea is that it's more efficient in practice due to the lack of monopoly rent. So why is it that theoretically bitcoin is inferior from a purely technical standpoint to just running everything on a centralized server? The reason, it's pretty obvious why bitcoin there's send a transaction, it has to get replicated by a few thousand nodes. You got all this sort of wasteful proof of work being done. PayPal one server send done.
00:08:45.134 - 00:09:33.438, Speaker A: PayPal might do some replication internally for just sort of technical redundancy purposes but even still, that's going to be much more efficient than bitcoin, which provides both technical redundancy and political redundancy. But in practice, for some reason, bitcoin charges five cents and PayPal charges twenty nine cents. So this interesting paradox that less efficient in theory and more efficient in practice. Now what I will say is that this idea of monopoly rent is something that can appear in many ways. So you could have monopoly rent because of prices. PayPal charges 3% plus currency for foreign transactions, plus 2.5% in currency conversions, and also intangibles.
00:09:33.438 - 00:10:29.410, Speaker A: So things like privacy, freedom, whoever creates the software might sort of take some extra effort to impose their own ideological vision and so forth. So another sort of, sort of way of thinking about it is decentralization is a commitment strategy that an application developer can use to commit not to being a jerk forever. So that's roughly sort of what the advantages of consensus are. And so the way that we can make this kind of architecture actually useful is that, okay, we know what the advantages are. Now let's see if we can shrink the disadvantages to the point where it don't count anymore. So what are the two disadvantages? One of them is scalability. So fundamental problem in all these architectures, every full node has to process every transaction.
00:10:29.410 - 00:11:46.958, Speaker A: So the idea is, well, if we want to improve on this, let's split things up, so that's not the case anymore. But we want to split things up so it's not the case anymore, while maintaining shared security. So as Dominique Williams, the founder of pebble, put it, we need to scale out and not scale up. So if you read the sort of bitcoin foundation, scalability roadmaps, they all talk about scaling up and they all talk about how, oh, things are going to be fine if it all gets 1000 times bigger, because look at this data, we'll just have full nodes on really powerful computers and so forth. Whereas I think if we want things to really remain decentralized, I think a sort of more horizontal approach where you actually lose the property that every full node must process every transaction might actually be more appropriate. So the idea is 10,000 x transaction probably too much slow down to make this sort of consensus death practical. In a lot of cases it's fine for money right now because people are used to paying money in order to transfer money, in most other cases for something like Internet forum, name registry or whatever, sort of timestamping a document, whatever else, people are used to that stuff being free.
00:11:46.958 - 00:13:03.590, Speaker A: And so going from free to going to be a bit hard for people to accept. So you might be able to sort of remove the psychological part of it to a large extent by sort of making the payment happen in the background and replacing the dollar sign with a sort of bar that goes from 100 to zero, and then you just have to pay a dollar once in a while to fill it up again. I was actually just reading some predictably irrational by Daniel Rielli, and he kind of points out how you can actually remove much of the psychological association with money by sort of creating a separate measurement thing that's kind of one step removed from money itself. But even still, you're not going to get rid of the fact that people are used to paying nothing, and it'll be hard to get them to pay something. But if you knock it down to, say, a 200 x slowdown, then, well, people already do a 200 x, take a 200 x slowdown voluntarily when they program in Python instead of assemblies. And so if people are willing to do that for ease of development, maybe they might be able to do that because they like the benefits of things being decentralized. Actually, another sort of one of those sort of Peter Tillion secrets about decentralization that I missed is that I think to a large extent, people will like application developers will want to use decentralized paradigms, basically because they're too lazy to manage their own servers.
00:13:03.590 - 00:13:54.578, Speaker A: That's actually a serious issue that I've had. I've actually tried to maintain a multisig bitcoin wallet at multisig info, and it just kept on crashing so many times that I had to keep on babysitting and restarting node js. So just because of that alone, I am willing to turn it into a Dap. So at 200 x, consensus becomes viable for a large number of things. And of course you don't need to take a 200 x slowdown for everything. As Gav pointed out in one of his recent presentations, you can quite often put what you really want to do is you only want to put business logic into consensus and everything that's not connected to business logic. You want to put on the sort of user interface layer inside of an individual, inside of the browser, or inside of a whisper swarm type protocol, a distributed hash table and so forth.
00:13:54.578 - 00:14:44.200, Speaker A: So how do you do it? We kind of talked about this, but just to sort of go through it quickly again, solution one sharding split the state into substates. And the idea is that block makers sort of build on an edge, and when you build on an edge, you process things in both vertices along the edge. And you also sort of move messages along. And if you need to send a message from one vertex to a distant vertex, then the message sort of stays in outboxes and eventually as minus mine edges, it sort of makes its way across. You can think of it as being kind of like, kind of like a it protocol, except it's obviously on a chain, and tree chains is not quite the same. It's more sober. This sort of currency specific paradigm that tries to actually split up debits and credits, hypercube chains basically, is this.
00:14:44.200 - 00:15:29.778, Speaker A: And the other sort of key point that it relies on is this idea of jury selection, that for any particular edge, the verification set for that edge is taken from the entire value pool of the entire system, so that everyone in the system is statistically protecting each individual block, even though only 200 users are actually protecting each individual block. So that's one approach. So what happens? What would this look like in practice? Ethereum 2.0 becomes a hypercube much target transaction fee, maybe knock it down from $0.05, maybe zero, one cents. Ideally much greater use of on chain mechanisms. So that's one approach.
00:15:29.778 - 00:16:07.886, Speaker A: Other approach is this multichain idea. So multichain many blockchains, most of them. There's going to be some blockchains that have many dapps. Sometimes it makes sense to have your dapps be on the exact same execution environment as all the other dapps, because you're interacting really heavily. Sometimes you just want your dap to be by itself because it's cheaper. Chains can interact either by explicitly interconnecting with each other or by the sort of tier nolan type decentralized exchange. And you do common security via the sort of consensus as a service paradigm where you have a chain that has consensus, a chain that specializes in voting on data availability.
00:16:07.886 - 00:16:45.978, Speaker A: And all the other chains, they have this selection rule that at each block height, the only block that's valid is the first valid block at that height, where you define valid by number one, the thing voted that the data is available, and number two, that the height, or number two, that there aren't any proofs of invalidity of it. And you determine first by the timestamp that this consensus as a service thing specifies. So that's the other approach that you can take. You need to standardize very little. Maybe for like clients, you might need to standard. If you want one like client across the entire system, you might need to standardize more. But this is sort of more of a sliding scale.
00:16:45.978 - 00:17:13.110, Speaker A: So what does it look like Ethereum 2.0 blockchain equals the Ethereum 1.1 blockchain. So exactly the same thing. So what we'll call 2.0 is like a set of tools for spinning up new blockchains. And it's probably a new sort of high level language that building on solidity, or maybe if we were building on serpents, I would probably call it Hydra, just because, you know, hydros have multiple heads, and if you cut one off, more of them grow and so forth.
00:17:13.110 - 00:17:51.490, Speaker A: The idea is you would have this sort of high level language you could easily use to compile to any kind of crypto economic structure. So you would have one keyword, and you could either compile to code on a blockchain, you could compile to an independent blockchain, or you could compile to an off chain auditable computation protocol and so forth. And ideally, it would sort of be one language that makes it easy for people to sort of pull in any of these different approaches. So that's the other approach for 2.0. So for 1.1, consensus is an issue. So proof of work has problems.
00:17:51.490 - 00:18:45.874, Speaker A: As we discussed, centralization is one mining centralization, mining pool centralization, waste. Given that quite a large number of our user base does sort of comprise of generally sort of idealistic environmental type of people, I think if the bitcoin community, or the crypto community in general sticks with proof of work, I think once they realize we are going to alienate a large portion of that crowd. So just for that purpose alone, it might be worth abandoning proof of work. And obviously we ourselves don't like resources being wasted, creates two conflicting interest categories. That's actually something that we missed, because with proof of stake, you just have stakeholders. Well, you have stakeholders and users that don't hold stake. But two categories here you have three categories.
00:18:45.874 - 00:19:35.330, Speaker A: You have stakeholders and miners and people who don't have waste. And the more categories you have with conflicting interest to some extent, the more complicated the analysis becomes. So, proof of stake, basically, I would call nothing at stake problem solved, in the sense that we've discovered an upper bound which is equal to a lower bound, which has this weak subjectivity criterion. So hopefully, no need to either argue that it's not possible, even with weak subjectivity, or even try to come up with clever ways to avoid it. The science has been, I think, settled to a partial, to a rather substantial degree, challenges solidifying the details. So what would ethereum 1.1 look like? So before 2.0,
00:19:35.330 - 00:20:04.702, Speaker A: a more sort of more moderate thing. 1.1 so proof of stake maybe some variant of this sort of slasher 2.0 idea always needs some proof of work for antidos eventaries. So the idea with eventuries is if you want a Dap or if you want a contract that does something that would only be actually processed at some point in the future. So you could imagine a contract that says after 30 days do this, then you want that event. So that would have to be an event.
00:20:04.702 - 00:20:46.978, Speaker A: And in order for that event to actually be part of actually process at a later point in time, it would have to be stored as part of a state in the meantime. And so that would be an event tree. Minor improvements. Making the patricia tree binary instead of hex is probably better on chain data structures. So treeps for example. Actually treeps might be a bit too complicated, maybe even heaps, just plain old heaps because they're useful for markets. Because the problem right now is that if you do a sort of market order book on chain, you actually have log cube down overhead because you have a heap which is a login on top of a patricia tree which is a login on top of a level DB which is login, and we can knock it down to n squared and that's a log squared.
00:20:46.978 - 00:21:25.560, Speaker A: That's already a substantial improvement just by making some first class data structure support for EC schnorr better signature scheme that supports sort of implicit multisig without actually being multisig. And yeah, so events trees already mentioned alarm. Yeah. So privacy, that's actually the other major disadvantage that consensus computing has over centralized servers which know in a centralized server. Okay, fine. If you provide your data, Facebook has your data and they're going to do whatever they want and whatever the NSA wants with your data. If you put your data on a blockchain, then everyone's going to do what they want with your data.
00:21:25.560 - 00:22:20.438, Speaker A: We do advocate privacy as a use case for dapps because we know that we can use the blockchain only for very specific things. And we can design dapps in a sort of model where things are stored encrypted and things are not done in consensus by default. So by default it's a system where things are encrypted stored in the sort of swarm cloud encrypted with your private key, and your private key actually directly controls things. So that's fine. But the problem is that there are going to be many situations where we simultaneously want privacy and consensus. So for currency that's actually easier because we can do things like merge avoidance where basically your wallet pretends behind the scenes that you have 100 separate accounts. You can do coin join, which is decentralized mixing, view centralized mixing.
00:22:20.438 - 00:23:03.986, Speaker A: You can do blind mixing with open transactions, more complex. Dapps they generally require a more complex state, and that state would have to intrinsically have a concept of account, and then you can't use these tricks. So in that case, if you're doing things on chain, you're basically giving it all away. So solution basically secret cheering what we're calling secret cheering dows Nick Zabil actually invented this concept back in 1997. I believe he called it the God protocol. So that kind of tells you that this is actually damn powerful stuff, potentially. So instead of doing decentralized consensus computing by replication, you do decentralized consensus computing with secure multiparty computation.
00:23:03.986 - 00:23:56.438, Speaker A: So the way that works is you sort of take some data, the state is stored in the sort of secret shared form, and there are ways that you can do computation on secret shared data such that the result is secret shared. But at the same time, no individual ever learns anything about the intermediate state or the end state. So blockchain like security, with server like privacy, is the idea. What's the cost? You pay one network message for multiplication, so even worse than blockchains, but you can parallelize it. So this is not something that we should advocate everything to run on, but it is something that might be useful for some applications. And with business logic, a 200 times slow down times another 100 x slow down really isn't all that bad if you're just doing like 20 computations. This is something that might be worth building on top of ethereum 1.0
00:23:56.438 - 00:24:53.126, Speaker A: or 2.0. Interesting thing about this mechanism, actually, is that the privacy part is vulnerable to a 51% attack. It's not quite perfect, but I guess some people might actually like the fact that the privacy is limited, because perfect privacy has some dangerous properties. You can do scary stuff with it, but here you actually have this sort of necessarily built in property that if there is this sort of supermajority consensus, that something deserves to be revealed, then that thing will get revealed potentially after the fact. So, interesting next idea we all love, but nobody understands. Dacs das daos. So that's another one of those areas where there's sort of a whole bunch of different ideas.
00:24:53.126 - 00:25:48.906, Speaker A: Nobody's exactly sure of what they are. And I've been sitting there trying to figure out exactly in what way are these va stars going to be useful? What niche do they feel that existing institutions or existing mechanisms don't satisfy already so one answer is corporations. And you actually could put in governments, they exist to quickly form and stabilize complex equilibria, complex equilibrium. Simple equilibria, we all know. So, for example, something like if you have a tribe, a tribe in a forest, then they can have a sort of a social norm, which is a kind of a kind of equilibrium where nobody kills each other. Because if one person kills some other person, then everyone else will ostracize them or will attack them. And if you refuse to ostracize someone who has been ostracized, then you sort of join the group of people that get ostracized, and you get ostracized yourself.
00:25:48.906 - 00:26:24.690, Speaker A: And that's actually a way that a whole bunch of societies actually work. And you could argue our society has some component of that, and that's relatively simple. The problem is the modern world requires us to be able to build these kind of equilibrium norms that are more complicated. So here's one analysis of a company. So let's say you have a set of customers, c. Now, each of these customers pays a dollars to a set of researchers, r, and they each pay b dollars to a set of manufacturers, m, and so forth and so forth. Now, problem is that these transfers include payment for public goods.
00:26:24.690 - 00:27:14.680, Speaker A: So the researchers in this case are not producing a product for customers. They're producing a public good, which the manufacturers and the customers pay for. So this whole thing is not workable piecemeal. You can't have a sort of pure market mechanism or a pure mechanism based on a simple market in which R and M would both get appropriately subsidized and you would end up in a case where r generally gets underfunded and manufacturers manufacture relatively to the optimum crap. So what a company is, is a company generates a combined transfer profile. So it says that, okay, the combined transfer profile is that the customers pay $12, researchers get $2 per product, manufacturers get $5 per product, and so forth. And then it sort of solidifies that particular profile via a network effect.
00:27:14.680 - 00:28:14.982, Speaker A: The existence of a network effect is what prevents that equilibrium from sort of sliding down into, well, the customer is realizing, well, what if I skip out on the components of the payment to the researchers? Because the whole thing is sort of mashed together into this bundle and it all sort of works together. So, da stars, there's this chart that actually made, thanks to Dave Babbitt, made it onto Wikipedia. So the idea is that these sort of autonomous organizations, and the idea is that so far, most research in automation has been in automation at the edges. So assembly line robots are one example. Just general tools are all partial automation, laptops are partial automation. But the thing that we can do, and the other sort of paradigm that we've been under exploring, is this idea of automation at the center and humans at the edges. So humans are still performed.
00:28:14.982 - 00:29:32.018, Speaker A: There's a lot of tasks that have to be done by people. People have a lot more creativity, at least in most cases, than machines do, at least unless, well, maybe not in art, because to be fair, fractals are pretty, but more creativity, more intelligence, the ability to understand the real world and so forth. But then you have ideas, you either partially or completely, or do something to replace the management component at the center, and you sort of replace it with some kind of system that generates equilibrium by itself. So one issue is that there are generally two types of complexity. So human organizations are good at generating equilibria that have this sort of subjective type of complexity. So if you think about the legal system as one big social norm, a question is what constitutes fraud? What does lying by omniscient count? Even what constitutes violence? What's the exact bright line between pollution and just dumping a bunch of sludge onto my house? But blockchains are better at sort of computational complexity. So highly complex state transition functions, having a sort of turnkilly programming language, having me in creating all these really complicated apps with a whole bunch of lines of code.
00:29:32.018 - 00:30:27.334, Speaker A: And the question is, maybe the sort of new opportunity is creating equilibria, creating social norms that are subjectively and computationally complex at the same? Just it is a theory. It might end up not being a particularly useful avenue once again, but it is sort of an interesting way of formalizing the idea dapps. So we as developers, we like dapps because it part political reasons, we like privacy, we like decentralization, we like the idea of nobody being in control. And we wrote the dapps, we appreciate the cool math that goes behind them. Normal people like dapps, if they do something useful. So what dapps could become actually useful. So in this case, useful, it doesn't necessarily have to mean useful in the sense of providing value in some particular, or being usable in some puritanical sense of the word use.
00:30:27.334 - 00:31:10.290, Speaker A: It just means useful in the sense that attract people who will be willing to use them, and particularly for reasons other than these two. So realistically, it's going to be games at first, easy to write, accessible or understandable by anyone. Probably going to be the majority use case of ethereum, I think, at least initially, just because they're fun. You can write them quickly, people can play them. Yeah, you can do battleships on the blockchain now. And the reason why I do battleships on the blockchain is first of all, you have these sort of games with crypto tokens involved. The decentralization part helps to make sure that the game is fair and that the server doesn't have any kind of advantage and so forth.
00:31:10.290 - 00:32:00.786, Speaker A: Computational resource markets. So mesh networking is, or the general category of a market for bandwidth, for. Market for personal bandwidth is interesting for a long term vision. I've made several speeches where I talk about this idea where you uproot the entire ISP system and replace it with a big global incentivized mesh network where you have the ability to form a company whose sole purpose is to maintain one wire going from Vancouver and Canada to Melbourne and Australia. And there's a bunch of these sort of single wire companies and maybe some multi wired companies around the world. When you connect to the network, use Dexter's algorithm, automatically find the shortest and cheapest path to whatever server you want to connect to. And whoever ends up serving along that path, you just use them so it completely decentralize everything.
00:32:00.786 - 00:32:24.310, Speaker A: Problem is, he can't change the world all at once. He needs some near term, minimal viable products. So one of them is sharing a wifi connection. So here's a problem that I've had many times. I go into a new country, I open up my phone, I try and connect to some roaming network. No network works, not even the roaming networks. Now I go into, I go onto the streets and I try and look for a wifi connection.
00:32:24.310 - 00:32:34.202, Speaker A: Ooh, free wifi. Free wifi, free wifi. Okay, free wifi. Yay. Landing page. Enter your mobile number. So two cases.
00:32:34.202 - 00:33:21.054, Speaker A: Case one. Case one, I have a phone. Case one, I have a phone and it's connected to a network. If it's connected to a network, then that means that I probably also have data, and then I don't need your stupid wifi. Case two, my phone is not connected to a network and this thing's bloody useless. So what do I do instead? Well, if I have one of these net mesh networking apps, what I can do is I can actually open up the little app and I could go into some crowded place and hey, look, here's 100 people, and at least some of them are already connected to some of these wireless networks, some of which are locked, some of which are sort of free and so forth. And I can just pay somebody whatever one cents a megabyte in order to just basically access a proxy for me.
00:33:21.054 - 00:34:14.490, Speaker A: This is something that people could benefit from by the millions today if it was available another near term NVP decentralized VPN. So I've kind of noticed that when I was in China for a few weeks, just because a lot of the vpns are centralized, they're actually blocking access to the VPN sites. And when I try and log on to Google with a VPN now, what happens is Google actually detects that you're logging on through a VPN and they don't let you log. Pretty decentralized vpns could actually make a substantial difference, and it's not going to work as a volunteer service. I think you need to incentivize high quality, and the way you incentivize high quality is with incentivization. Now, the reason why you don't want to use Tor is because Tor is just way too powerful for most normal people. Normal people don't need three hops, they just need one hop.
00:34:14.490 - 00:35:04.058, Speaker A: Look, they're just interested in protecting your privacy to some minimal extent, getting around some restriction. They don't necessarily need NSA proof, sort of either military grade or silk road grade protection or whatever. They're just trying to have some minimal level of privacy that they're fine with. So this might actually work substantially. File storage, another one so near term NVP content, addressable web. I won't get into that, because I'm sure Kuan talks about it. A sort of nice use case of this is that one of the things that was pointed out about proof of stake is that you lose this ability, in theory, for anyone to be able to get some quantity of coins by mining them.
00:35:04.058 - 00:35:43.862, Speaker A: And the sort of substitute that you can have is you can have a sort of decentralized cloud storage market, where instead of mining by actually mining, you're sort of mining by renting out your hard drive space and storing clouds for other people. And that's a quick way to earn whatever number of Finney a day that you're going to get. Cloud computing projects how Golem presented this week. So that's just a number of possibilities. So, nice things about computational resource markets are in computational land. You can cryptographically prove that certain things happen. So file storage, you use Merkel trees, you can prove that you're storing a file for cloud computing.
00:35:43.862 - 00:36:20.840, Speaker A: You can do various sorts of auditable off chain protocols. So you could even do crazy Merkel tree stack trees hashing to be able to very efficiently prove that you computed something in a valid way or eventually it would just be skip. So the idea is that these sort of cryptographic protocols, the blockchain, can run verifiers and they can make events directly conditional on verification. Security deposits are king. So that's another point. Thank you, Vlad. So, not for coming up with this particular quote, but for just bringing to my attention the fact that security deposits are rather important.
00:36:20.840 - 00:36:54.970, Speaker A: They are. So that's stablecoins, another interesting use case. So, problem is, right now, cryptocurrencies are volatile. The reason why they're volatile is that for currencies, price is basically demand divided by supply. Now, it's not a general economic principle that holds generally, just say price is dependent on demand and supply. But for currencies, if there's twice as many units of a currency, then the natural thing to do is just for all prices to go up by a factor of two, and then everything's sort of in the exact same equilibrium. So, current model that we have in most crypto environments, one coin, stable supply.
00:36:54.970 - 00:37:19.494, Speaker A: Now, the demand is always volatile, and still volatile. Demand leads to volatile price. All bitcoin users must participate in speculation as part of their bitcoin use. That's one of those. And no, going mainstream will not solve the problem. So those people who think that it will naturally go, naturally become extremely stable when it becomes mainstream. This is gold, as mainstream as you can get.
00:37:19.494 - 00:37:47.026, Speaker A: Top over bottom is about a factor of 4.6. Meanwhile. Meanwhile, if you look at the ratio between two fiat currencies, top over bottom is about 1.5, usually. So probably not good enough. And once again, I think this might be one of those cases where people are willing to regress slightly for ideological reasons, but not this much. So one idea is you sort of create stable assets with a self adjusting monetary policy.
00:37:47.026 - 00:38:24.414, Speaker A: Two ways of doing it, endogenous assets. So endogenous means, instead of trying to stick. So the exogenous approach is you try and use this sort of decentralized consensus on real world data to try and perfectly track the US dollar, CNY and so forth. Endogenous approach is that you don't try and stick to any particular fiat currency. You just try and maintain some concept of stable value. And you try and use various, you make estimators based on things like the mining, hash rate, things like transaction fees. You could try and build in other markets, so you can build in a file storage market, and then you can have an estimator sort of listen to the price on that market now.
00:38:24.414 - 00:39:07.914, Speaker A: So you create some stable assets. And theory is that you have sort of two coins, one volatile coin, one stable coin. And that's how networks would work. And the sort of innovation without speculation. I just put that in there somewhat ironically, because it's blockstream's slogan in their sort of quest to make bitcoin of bitcoin maximalism and making bitcoin the one currency of all networks. And I argue that bitcoin is also speculative and just as speculative as some of these crazy alts. And so if we really want to decouple the sort of volatility part from the cryptocurrency part, the volatility is always going to remain because these are assets that have no intrinsic value.
00:39:07.914 - 00:39:42.370, Speaker A: It's unavoidable. The thing that we really want to do is we want to try and specialize the volatility away. So we want to create a mechanism where people who wants to gamble can gamble, but people who grandma that just wants to keep her retirement savings safe can also fairly reliably do that as well. So those are just some of the ideas that I think we'll be seeing as far as dapps go, as far as scalability, consensus. So sort of the future of the space. So could be the future, or it could be a bubble. You decide.
