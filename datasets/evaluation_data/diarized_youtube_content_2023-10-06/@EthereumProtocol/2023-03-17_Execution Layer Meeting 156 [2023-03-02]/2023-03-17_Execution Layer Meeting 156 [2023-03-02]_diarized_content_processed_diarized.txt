00:03:30.660 - 00:04:11.996, Speaker A: Good morning everyone. Awkward to have 150 today. We have a bunch of things on the agenda. The first ones are around Shanghai, so we saw Chappella go live on Gordy and we can recap that, figure out if we want to set a main net date today. Then Terrence and POTUS had the pr around local block building. So it makes sense to go over that after that. I've tried to basically list out all the stuff that's been proposed for Cancun so far to kind of give people a picture.
00:04:11.996 - 00:05:01.330, Speaker A: We discussed SSD last time and wanted to come back to it. And then there's kind of all these other things that have been discussed. So it might be worth if not like touching on every one of them, at least the biggest ones, and at least putting it out there so people know sort of what's being proposed and we can start having those conversations and then we'll try to keep a couple of minutes at the end. Pooja, I know cathoders have put out a report around El client diversity and just basically what node operators feel they like and dislike about the various. Yeah, I guess to kick off. So we had the Gordy fork this week. Does anyone want to give a quick recap of how things went? Yes.
00:05:01.330 - 00:06:38.960, Speaker A: Nice. Thank you. Anyone else have any thoughts they want to share about Gordy or anything they noticed beyond that? Okay, so if not, it seems like it went relatively well. So I think we can probably set a date for main net. I had proposed three in the agenda. So basically April 6, twelveth and 19th. There's a couple of comments in the chat around April twelveth.
00:06:38.960 - 00:08:05.318, Speaker A: Does anyone think we should not do April twelveth? Otherwise we can go for that. I picked out a slot and epoch number. I'll copy paste them in the chat that I'm 90% sure, 99% sure end up on a historical roots boundary. I'll double check it right now. And there's a comment in the chat about hearing from Basu. So does Basu have any issues if we it. Okay, so people like the twelve.
00:08:05.318 - 00:08:56.830, Speaker A: There's a comment in the chat about like, it's in the evening for the EU and that's unfortunate. But if we want it to cleanly map to historical roots boundary, it needs to divide by twelve by 8192. Sorry. So this is why it's kind of a weird time and you get like one of those every day and a half or something. Yeah. Oh, I. Sorry.
00:08:56.830 - 00:09:23.908, Speaker A: Thanks obs. Can you speak someone? Not me. Okay, it's working now. Okay, so sorry about that. People on YouTube. Let me just post a comment, so I'll just really quickly recap and we can upload the full zoom later today. But in short, Gordy went well, saw a drop in participation because of unupdated clients.
00:09:23.908 - 00:10:18.120, Speaker A: But even regardless of that, BLS changes and withdrawals went well and the network reached finality and then everything was fine. And we were just discussing the fork date for main net and it seems like people like April Twelveth, so the comments, the chat reflect as well. And we were about to start talking about whether or not the time can be a bit better. But it seems unlikely because we need an historical roots boundary which happens every like 27 hours. Sorry YouTube. But yeah, April Twelveth is what's being discussed right now. So I guess, yeah, this current epoch is late for Europe.
00:10:18.120 - 00:11:17.372, Speaker A: How big of a deal breaker is that? We can always go like 27 hours later, which would be the next later, yes, which would be like the 14th or something. No, but that would be later in Europe, right? Well, oh yeah, actually it'll be worse. It'll be like plus 3 hours or something you go day before. But also it's kind of nice that we have this forcing function that slowly rotates and would be different next time so that we get a nice distribution across time zone. Okay, we'll get nethermind some energy drinks. Any objections? So this would be epoch 620-9536 on main net. Okay, so cool.
00:11:17.372 - 00:11:55.160, Speaker A: We have a date for Chappella. We'll give teams like a week. So to put out releases, I think it would be nice, ideally if on next week's call we could have the team releases out, so we can announce them there. But then at the very latest, I think if early the week after that we can put out the announcement. That would be great. So that'll give people like a proper two weeks to upgrade. So from today, I guess we have one, two, three and almost four weeks to the fork.
00:11:55.160 - 00:12:20.370, Speaker A: Anything else on Chappella? Okay, if not, next up, Terence Pokus, you had pr you wanted to discuss. I'll post it in the chat. But if either of you want to go, the floor is yours.
00:12:28.030 - 00:12:30.570, Speaker B: Yeah, sure, I can get started.
00:12:30.640 - 00:12:31.014, Speaker A: POTUS.
00:12:31.062 - 00:13:19.306, Speaker B: Feel free to add if I miss anything. So basically, one property that will be really nice between the Cl and the El interaction is that the El basically determines how Cl proposed pandos blocks based on some sort of censorship resistant metrics or monitoring. Basically you can look at Mempool and look at just transactions to see whether, based on certain things, to determine if the transaction is monitored, to determine transaction is being censored. And the one nice thing about this is that it's not required in a way that if the EL doesn't implement it, El can just return us.
00:13:19.328 - 00:13:19.754, Speaker A: No.
00:13:19.872 - 00:13:51.306, Speaker B: And then if Cl doesn't want to use it, Cl doesn't have to be used. So right now it is phrased as completely optional. But anyway, the PL is out there, it's open for feedback, and then we also have a presale implementation PR as well. On the Cl side it's relatively simple to implement. I guess majority of work will be on the El side, but like I said, this currently phrase as an optional.
00:13:51.358 - 00:14:20.280, Speaker A: Thing, I'm pretty supportive of this. It's a very small change that can then be very much iterated on by els and cls as they see fit, and really does open up the design landscape to try to ensure that we have censorship resistant and other potentially nice properties of how the mempool is being utilized.
00:14:24.940 - 00:15:14.410, Speaker C: Lucas, I have talked with some El devs and some Cl devs and got some feedback that I wanted to mention here. Everyone on the El side told me that it would be absolutely trivial to hard code a false in that return, which means that they could just immediately implement this. And on the Cl side there's no change that is actually, it's from everyone that I've heard that it was absolutely trivial to implement this. And I've got some feedback from the CL folks saying that they'll be allowed to either use it or not use it. And actually there's some diversity as to how to use this, which also makes me happy because having diversity on when we're going to fall back to local execution or not is something that is quite interesting in itself.
00:15:17.580 - 00:15:21.130, Speaker A: Thanks Lucas, did you have your hand up about.
00:15:23.820 - 00:15:43.650, Speaker D: Yeah, but this is just the first step, right? And in order for it to make any sense to be used in the future, we need to also think about the heuristics we would potentially use, just even like discuss them, because we can add them and not never use it, right, or use it in some useful way.
00:15:44.100 - 00:16:44.820, Speaker C: Can I list the ones that I had in mind? I mean, you can add as many as you wish and you can have them configurable or not. I just focused on the stubbing of false because if we manage to implement this right now, then we will avoid bumping a version on the engine API. If it's absolutely trivial, then I would be in favor of implementing this right now. But as for the heuristics, the one that I listed in the sites I think are sort of like the most dramatics for the kind of implementations that I have in mind. One of them is if you have seen a transaction that is constantly being reorked, whenever this transaction appears in a block, then that block is reorganized, then that's a very strong indication that there's censoring going on. There are other heuristics that you can apply. If you have seen a transaction that is paying twice the current priority fee, and that transaction has not been included in three blocks, it's still valid, is paying twice as much as any other transaction, then you should include it, and then you should fall back to local execution.
00:16:44.820 - 00:17:34.390, Speaker C: If you have seen something that worried me, and the reason why I'm opening this pr is that the flashbox community is going towards having the relayer not check the execution payload. This is something called optimistic relayer. And this screws up some statistical analysis that I wanted to do on censorship. And the reason this screws up is that we may see some blocks that are invalid, that contain transactions that could contain transactions that were censored. So that's another heuristic. If you see one transaction that whenever it appears in blocks, that block is invalid, you should also fall back to local execution. But I'm sure smart people will come up with better heuristics or combinations of them.
00:17:37.000 - 00:17:41.812, Speaker A: Thanks, Marek. And then Lucas, quick question right now.
00:17:41.866 - 00:17:46.810, Speaker D: Do you mean before Shanghai or what do you mean by right now?
00:17:47.580 - 00:18:38.650, Speaker C: That would be optimal? If it's absolutely trivial, as everyone has told me, we could do this in less than one day and have it in the next release and test it even in Devnets immediately. If it's not possible, then I don't mind any delay because I think this is very simple so it can be shipped later. But the thing is that this would require more complexity because we will require like bumping the engine API and then coordinating, either using on the CL side an endpoint to check that the endpoint is supported on your El, or other sort of complexity combinations. If it's absolutely trivial, then I think it's easier to just for us it's absolutely trivial because we don't need to do anything. And then for the El, if for the yel it's just sending a fault, it's trivial, then I would suggest to do it now.
00:18:42.690 - 00:19:18.746, Speaker D: So one last thing. For me in general, I think I'm far, but that I don't like that it creates another complexity on the El and another responsibility on our side to implement, maintain and have these heuristics. And this is potentially something that could be done even outside of El, right? It doesn't have to be inside of El and integrated in the API. That's the question that does it make sense to have it in the I.
00:19:18.768 - 00:19:20.170, Speaker C: Can reply as to why I think.
00:19:20.240 - 00:19:21.900, Speaker A: The Yale is the right place.
00:19:23.170 - 00:19:58.230, Speaker C: And the reason I wanted to do this is when I realized that you can unchain with very mild assumptions, assume with probability of one in a million that there's been no censorship if you know that two consecutive blocks haven't been missed. And this can be verified on chain with the assumption that the validators will fall back to local execution when they see censorship by forky. And the only way that I can see how to do this is if the CL has access to the member.
00:20:01.150 - 00:20:14.270, Speaker D: But one more question, but this is basically optional functionality on both CL and El layer, right? So El can also always return false and Cl can always ignore even if it returned true.
00:20:14.420 - 00:20:34.980, Speaker C: That's correct. But just the fact that this is a possibility allows the theoretical assumption that certain percentage of validators will be using it. And with those assumptions you can actually prove things statistically, which if we don't have the option then you cannot prove anything. I think this is a big difference in UX for me.
00:20:42.670 - 00:21:44.560, Speaker A: So I guess there's a couple comments in the chat about being supportive, but not for Shanghai, so I guess it seems unlikely we would get this in now. Does it make sense to continue the conversation on the PR itself if it's something that's not going to happen like in the next couple of days? Okay, perfect. So let's continue the conversation on the PR. But yeah, clearly it sounds like something there's interest from any of the client teams to support anything else on this before we move on. Okay, so yes, next up, Cancun. So I went over the ETH magicians thread yesterday to try and look at all the eips that have been proposed so far. I've updated the ETH magicians thread and also listed them on the agenda here.
00:21:44.560 - 00:22:38.820, Speaker A: And I guess the obvious thing as well is four. Four. Four is the only thing we've officially included so far. So that's something we should take into consideration when talking about Cancun. And maybe the first thing to cover is all of these SSZ proposals. So last week, or, sorry, two weeks ago we discussed this a bit on the call as well, but it seems like teams didn't have a ton of context. But basically because we are introducing SSZ as part of 4844 on the execution layer, it's worth figuring out do we want to introduce more at the same time? At the very least, what is the format we want to go with so that even if we don't introduce more SSD objects, whatever we do introduce in fort for four is consistent with what we'll introduce in the future.
00:22:38.820 - 00:23:02.690, Speaker A: So I don't know if any of the client teams have had time to look into this in the past couple of weeks. If anyone has any kind of updates they want to share on this. Okay, so I guess.
00:23:07.080 - 00:24:23.740, Speaker D: Ethan and I kind of had a discussion, I think it's two weeks ago now, about this, and we kind of came to the conclusion that we're trying to optimize for different things. Yeah, it was in the discord and hashtag Charlote data. So I think my focus was to make the transaction as small as possible, while his focus was to make it as easy to verify by light clients as possible. And yeah, basically there are two different ways we can do the SSC format, or at least two different ways proposed. And one is smaller and one is easier to verify. And so I think we kind of have to make a decision what we want to prioritize.
00:24:26.480 - 00:24:29.470, Speaker A: Got it. Lucas, you have your hand up. Is it?
00:24:30.260 - 00:25:05.204, Speaker D: Yeah, because this is about only ssding the roots. Right. But the question comes up, should we SSD storage and network formats too? Might it be easier to have just one format? Maybe we shouldn't. Maybe we should keep it RLP, right? It is right now. So that's the question. And the other question is, how does this affect cancun timelines?
00:25:05.252 - 00:25:05.512, Speaker A: Right.
00:25:05.566 - 00:25:16.510, Speaker D: Because if we are cramming more and more, and this is not trivial in my opinion, then yeah, it depends when we want Kancom two to come in.
00:25:20.160 - 00:26:05.610, Speaker E: The network format does not really have to be bundled. It's only about the storage format. For actually not even the storage format is bundled. Like everyone can store the transactions how they want. The format that is affected is how transactions are represented as part of the consensus execution payload. And the size difference that Marius mentioned is primarily mean. One of the goals is to allow clients to use JSON RPC while having a way to verify that what they receive is correct.
00:26:05.610 - 00:27:22.240, Speaker E: And there are three items in there, namely the transaction id, the sender of a transaction like the from address, and for transactions that deploy a new contract at contract address. Those three items, they are currently not part of the raw transaction and they can be computed from the raw transaction with SECP, public key recovery, and some RLP and some ketchup. So if we want to include those three items. They either need to go into the receipt tree or into the transactions tree. The size differences are mainly if we decide to put it into the transactions tree, then of course the transaction that includes the extra information is a bit bigger than the other one, but the receipt is a bit smaller. And if we do it the other way, that we put this extra information into the receipt, then the receipt is a bit bigger one way than the other. So ultimately the total size is probably comparable.
00:27:22.240 - 00:27:38.730, Speaker E: But it doesn't affect the networking. That one can still be RLP if that's what people want. But it means that you have to convert back and forth between one representation and the other whenever you receive a message and when you send it.
00:27:41.520 - 00:28:07.140, Speaker D: And this conversion is kind of costly, actually. So we try to have as little conversions as possible. So we store the transactions in the same format that we send them on the network.
00:28:07.960 - 00:28:55.280, Speaker E: Okay. I mean, then we can just update the network to exchange that in the same format as well, to avoid the conversion. But then either the receipts or the transactions are getting a little bit bigger, depending on which one we choose. There is also one conversion that we cannot avoid. It is when we convert from a transaction before, like a raw transaction that's in the mempool. That one is using whatever format it originally was signed under, so that one cannot be changed. That one is always an RLP, or it's this blob transaction with the network wrapper.
00:28:55.280 - 00:29:26.604, Speaker E: So when those are bundled into a new execution payload, they need to be converted a single time. And when they are validated as part of new payload, they need to be converted back so that you can check the signature against the original representation. So there is this during new payload and when bounding a new block, like new payload get payload where the conversions cannot be avoided, regardless of which SSC format we choose.
00:29:26.802 - 00:29:39.390, Speaker A: I think that's ok, because we already have to do that to calculate the Sig hash anyways. Danny, you can.
00:29:40.100 - 00:29:42.544, Speaker E: No, I just wanted to say. Danny has his hand.
00:29:42.662 - 00:30:26.850, Speaker A: Yeah. Okay, thank you. I'm curious, before we go deeper on the technicals here and start talking about decision points, have we done an analysis or an ask of the community as to what this is going to break? I imagine it breaks tooling to a decent amount and then it certainly will break some on chain contracts, which I imagine many of them are upgradable. For example, some l two s. But have we done analysis of. Is there anything we're fully breaking by doing this transition, which not to make a claim that we must not break anything, but I think that to make a decision about how to proceed, we need to know the answer to that.
00:30:28.740 - 00:31:18.640, Speaker E: The only analysis that I made there is to see whether what use cases would break. JSON RPC would still return the exact same data. It could just be extended to also include a proof that it's actually correct. But if you have applications that depend on the current Merkel Patricia try route, those would essentially break because that legacy route will be removed. I'm not sure how to gauge which projects are affected concretely. I hope that the high profile ones have a mechanism in place to upgrade if they depend on this functionality.
00:31:19.140 - 00:32:01.310, Speaker A: Right. And I know l two s that do have such a great functionality, it's at least worth at this point if we're seriously considering this. Yelling into the void to see if anyone comes up with something that we can't patch. Yeah, Tim, there might be some automated analysis we can do here, but it's hard to catch everything because this is like a relatively. Utilizing proofs against a root is not obvious to automatically. Okay, that's what I figured. So yeah, we can definitely make a call for people to show up if this breaks things.
00:32:01.310 - 00:32:16.020, Speaker A: And yeah, it might be worth looking if there is some sort of fancier way that we can know potentially transactions in the last year or what's been deployed. Peter.
00:32:18.440 - 00:32:57.856, Speaker D: Hey. Just a very slight diversion. So previously it was somebody asked whether it makes sense to bundle this together with Cancun with 4844 or not. I just wanted to react that in my opinion, based on just the discussion of the past few minutes, you can see that this is a very deep rabbit hole. And I do think essentially we really don't want to half as this. I think the advantages are super nice, but the question is, what do we want to transition over? And my $0.02 would be to.
00:32:57.856 - 00:33:48.050, Speaker D: I don't see this going in along with 4844. I would much rather just gather everything that needs to be done so that we can roll this out in one big go and update everything that's needed. And I think that's a completely separate effort and it won't really fit inside one problem. Yeah, but one thing that we should just be aware of is that the blob transaction is kind of written in a way. So that is also kind of upgradable to this new version. But I guess if we do the blob transaction similar to how we do the other transactions, then it should.
00:33:49.940 - 00:34:06.950, Speaker A: So are you saying. I'm not sure I understand if you're saying that we need to be sure of the format today because blob transactions will use it? Or are you saying that it's fine to change the format of blob transactions in the future? And basically.
00:34:09.180 - 00:34:25.656, Speaker D: I think Maris was trying to say that since block transactions are already introducing partial introducing dependencies on SSE, let's make sure that whatever block transactions do will be compatible with the final dream.
00:34:25.768 - 00:34:53.190, Speaker A: Yeah, I agree. That's like the minimal decision ideally we would make is even if, and we might not have 100% certainty here, there might be a format that's better or worse for some reason. But if we can have some confidence that whatever we implement in SSV for four, four works and is forward compatible, then that's better than having to rechange it.
00:34:54.920 - 00:36:02.860, Speaker E: Eterna, there are three parts of this proposal, and I think for each of them it can be decided separately when they should be included. The one that has to be in cancun if the blob transaction is SSC is signature scheme like EIP 6493 because we need a signature scheme that cannot ever conflict with a future transaction type that we introduce or with an RLP transaction type. And the other one that's kind of orthogonal is the withdrawals route. That one, the idea was to already convert it with Chappella, and that one was only delayed because it was already late in the cycle when this was proposed. So that one can be moved into Cancun without any controversies. The only one that is controversial is transactions route and receipts route.
00:36:04.880 - 00:36:25.910, Speaker D: I wasn't really referring to controversy, I'm referring to simply work and making sure that nothing blows up. So my main issue is that 4844 is huge, and anything that's sufficiently complicated beside it will be really messy because people will need to juggle two very complex things at the same time.
00:36:30.680 - 00:36:50.110, Speaker A: Right. But I guess what Ethan, are you saying is that we need a signature scheme basically because we introduced fort for four, which is an SSD type. So we need some signature scheme that we agreed to in order to make sure there's not like conflicts with future transaction types. Is that correct?
00:36:50.640 - 00:37:32.520, Speaker E: Yes. The signature scheme is for transactions in the Mem pool. So that one is regardless of how you format the SSC transaction later as part of the execution payload, you need to know how do we sign? Those mean it's probably good to have the same signature scheme for all SSC transactions, even if the individual transaction payloads can be whatever. I mean, there is no reason to align those before they are accessible through APIs.
00:37:34.220 - 00:37:35.290, Speaker A: Got it.
00:37:37.820 - 00:37:57.410, Speaker D: Currently 44 four is spec to use the SeC P 256 signatures and can't we just keep those? And then whenever we do this SSD switchover, then we also switch the signature scheme. Is there a specific problem with doing that?
00:37:58.500 - 00:38:03.920, Speaker E: Do you mean like keep it at ketchup zero x five plus SSE encode?
00:38:06.120 - 00:38:06.870, Speaker D: Yeah.
00:38:08.040 - 00:39:06.820, Speaker E: One issue with that is if it's SSC, you start getting problems when you look at it across different networks, including private networks, because with the SSE transaction, the chain id does not always end up at the same offset as it would if it were RLP encoded. So you can have a signature on a private network that is type five, but some RLP format that accidentally serializes to the same hash as, as the SSE transaction on the main net for type five. I'm not sure how big that risk is, but it could be that if both are using the same hashing and the same transaction type, but different encoding strategies like SSE versus RLP, that there may be a conflict.
00:39:10.380 - 00:39:18.490, Speaker D: I'm not really following, because currently with typed transactions, essentially the first byte is the type.
00:39:22.480 - 00:39:47.990, Speaker E: Yes, but that one is network specific. Someone could create a private network like a L2 that's EVM compatible and they could define their own type five. Maybe they already have a type five and we don't know about it, and someone signed transactions on it and uses the same key on main net as well. It's only across networks where you have those problems.
00:39:49.960 - 00:39:57.784, Speaker F: But it's like hash collision, which is supposed to be extremely rare. So I guess the probability is very low.
00:39:57.982 - 00:40:23.600, Speaker E: Not hash, it's the serialized unsigned transaction that could collide. And that one is not a mathematical property. It's just you can construct an RLP object that has the same encoded value as an SSE object, as an SSC encoding of a different transaction.
00:40:28.340 - 00:40:46.020, Speaker D: I mean, that sounds exceedingly improbable to have an RLP something be valid both as an RLP and SSE decoded stuff and be actually meaningful.
00:40:47.820 - 00:40:56.010, Speaker E: Sure, I mean, that's the open question whether that is a real risk or. Yeah.
00:41:03.340 - 00:41:50.860, Speaker A: It. So I guess just based on this conversation, does anyone feel strongly, I guess, that we should do the full SSD overhaul as part of Cancun? Or is this more about finding the minimal set of changes we want to do that are forward compatible? Basically what Peter was saying, because that can help just kind of prioritize the future discussions where obviously we want to understand the full implications, but there's a difference in do we start implementing this wholesale now, or are we trying to find the minimum we can do for Cancun? Andrew?
00:41:52.080 - 00:42:11.120, Speaker F: I would concentrate on the minimum because we have to weigh. I agree that with Peter that it is a big change. So we have to weigh the benefits of the SSD overhaul versus the other potential candidates. Our throughput as developers is limited.
00:42:17.270 - 00:42:41.182, Speaker A: Okay. Yeah. And plus one for the Nethermine. So I guess we probably can't take a decision on the exact format right now, but I think if this is something that teams can look into in the next couple of weeks, that would be good. So we can start aligning on what's, like, the right minimal scope for SSD. For Cancun. Yeah.
00:42:41.182 - 00:43:07.990, Speaker A: Does that make sense? Okay. I'm cautious about the breakout rooms now because it feels like there's all of what every day. But do you think we need a separate call to discuss this, or can we have teams review it async in the next couple of weeks and potentially discuss it on the next all core devs?
00:43:10.410 - 00:43:14.514, Speaker E: I think that's fine. Like, just discussing it as part of the ACD?
00:43:14.642 - 00:44:14.860, Speaker A: Yeah. And I think if we get to the next ACD and there's been literally zero progress, then maybe we can schedule a breakout room. But, yeah. Okay. So, like Andrew was saying, I guess there are a lot of things being considered for Cancun, and, yeah, I wanted to list them here, and it's probably worth going over them kind of quickly to at least share kind of stylus updates and get some sort of temperature check from different teams about the efforts. But high level, everything around self destruct, the EVM max proposal solidity had proposed EIP six six three, which is this unlimited swap and dupe instructions. We had EIP 59 20, which we discussed quickly last time, the pay op code.
00:44:14.860 - 00:44:57.750, Speaker A: There's everything around eos, obviously, 1153 and Moody had an update about the status in the agenda, and then the two others that came up last time were the BLS precompile and the beacon state route opcode. So this is roughly everything that I believe up to yesterday was proposed. I guess on the self destruct side, I don't know if anyone has an update here. We did have this deprecation warning as part of Shanghai, so I'd be curious to see how strongly the people feel this is a priority. Where do people think we are in terms of specs and whatnot?
00:44:58.670 - 00:45:57.606, Speaker F: Yeah, Andrew, I think that removing self destruct is very much a priority. So, yeah, we would like to do it as early as possible. I was also thinking that there was this analysis by Jupiter who suggested that there was one smart contract pine core using self destruct ephemerally. So, for a contract was created and self destructed within a transaction. So if we just keep that special case, if we allow, we keep self destruct. So if we keep ephemeral self destruct and deactivate non ephemeral self destruct, that might be a nice compromise. I don't think we have a nip, but if people here agree that it's kind of a viable option, then I can create a nip, or somebody can create a nip.
00:45:57.606 - 00:46:01.710, Speaker F: But in general, I think removing self destruct is top priority.
00:46:06.280 - 00:46:36.522, Speaker A: Okay. Anyone else have thoughts? Guillaume. Oh, sorry, I accidentally raised my hand. Yeah, I mean, apart from the fact I also think it's a priority. Yeah, not much to add. Sorry. I guess maybe one question for you while you're unmuted, to be clear.
00:46:36.522 - 00:47:09.922, Speaker A: So say we wanted to do vertical trees and not Cancun, but the fork after that, we need basically self destruct to be removed prior. Right. If we didn't do this in Cancun, it means at the very least we would push out vertical trees, another fork. Right? Yeah, exactly. And I will probably spontaneously combust as well. No? Yeah. If self destruct needs to happen in Cancun, otherwise vehicle trees will be pushed.
00:47:09.986 - 00:47:14.220, Speaker E: Indefinitely because there will always be something to come before.
00:47:15.310 - 00:48:35.140, Speaker A: Okay, so I know there were some proposals around, basically what Andrew was saying, kind of allowing some sort of ephemeral transactions to use self destruct. I know Alex from Ipsilon also had some proposals. So I think that would probably be the most important thing here in the next couple weeks is if we can come up with a spec for something that does deal with the multiple edge cases, that doesn't break the contracts that use it the most and that are most popular today with this and have an actual thing we can look at. We did review another spec last call and there was a lot of complexity in it. So yeah, it does feel like something where having a clear stack is the main blocker. But assuming we had that, I assume people would be pretty on board with moving forward with it. Do we have a self destruct channel or something like that? I guess we could use the execution dev channel at deverities to discuss this in the next couple of weeks if we don't have a self destruct one.
00:48:35.140 - 00:48:50.630, Speaker A: Yeah. Okay, next up, EvM Max. I don't know. Is Jared on the call? Hey, Tim, you are here. Hey, you want to give us an update?
00:48:51.070 - 00:50:06.560, Speaker G: Yeah, hey, I'm just joining. I just wanted to just raise awareness. So recently, in fact, yesterday I posted EIP 66 90 on Ethereum magicians, which is just a variant of 66 one, which was the latest proposal update proposal in 66 90 is the attempt to decouple the proposal from EOF. Not to say, but basically it would be great to get more people just like clients specifically potentially looking into implementing this. And I think a great start would be 66 90, because I think that's the easiest to implement. But EOf also brings benefits. I think that.
00:50:06.560 - 00:51:38.160, Speaker G: So I would say as far as advantages go, if you compare it to something like EIP 25 37, of course you're not going to be able to implement BLS operations as performantly as pre compiles. I mean, that's kind of obvious, but we get several other benefits. Like for example, like I've noted in in both of both of the recent eips, this would basically allow us to replace the mode XP precompile for all of the inputs that I've seen it used on when I've scanned the chain, and then other things like the ZK friendly hash functions such as Mimsi for example, I implemented MimSI and it was 80% cheaper than the Zircom lib implementation, which was being used in production by tornado cache. So I would just say, just to sum up, it would be great to get more eyes on this, and if we do or don't want to move forward. Yeah, it would be good to hear something.
00:51:38.930 - 00:52:13.050, Speaker A: Thanks. Any thoughts comments on EVMX? Okay, so I linked both your two new proposals in the agenda, Jared, so people can go and comment on that async cool, great, thanks for the update. And I guess Marist, do you have something to say?
00:52:13.120 - 00:53:13.070, Speaker D: Yeah, so one thing that I would like to highlight is that evmax is not only cool for BLS twelve 381, but can also be used in a bunch of other use cases. So I think we should even consider this even if we decide to implement the BLS precompiles. I think the BLS precompiles, which we probably will talk about, are very important and much needed. But EVM max or modular arithmetic within the EVM is something that has a lot of use cases and beyond only the BLS precompile.
00:53:15.490 - 00:54:05.280, Speaker G: Yeah, that's a good point. I probably should have mentioned that as far as I could tell, and maybe somebody can correct this if this is wrong. But as far as I can tell, basically any elliptic curve could be implemented with this proposal. And another thing to consider is that if you have a higher width base modulus, I know there's some curves that are at like 768 bits or more. As far as I can tell, this would actually be pretty close in performance to what you could hypothetically get with a pre compile, just because modular multiplication has quadratic complexity. But yeah, good point, Marius, thanks.
00:54:08.470 - 00:54:55.600, Speaker A: Yeah, and I think several years ago we had a very similar discussion around, I think this is the Berlin fork where we discussed BLS versus EVM 384 at the time, and there was like a similar concern around should we do one or the other? And we ended up not doing BLS because EVM 384 might happen soon. And I think it probably makes sense, given the importance of BLS, to consider it also separately and even if there's redundant functionality, that might be one of the few cases where it's worth it relative to delaying BLS if we are going to do EVM Max at a later date.
00:54:56.950 - 00:55:31.830, Speaker G: Yeah, I guess just from the user's perspective, what really would be the difference? I mean, you're saving a bit of gas, but it would be great to quantify and maybe extrapolate out from how people are currently using BN 128 and just, I don't know, assume that they'll just move to BLS and then extrapolate out exactly what we're getting in terms of savings overall with pre compiles.
00:55:31.910 - 00:55:32.442, Speaker A: Right.
00:55:32.576 - 00:55:44.270, Speaker G: Which is hard because I don't have all the EVM Max operations or all the BLS operations implemented in Evmax, but it's fairly easy to guess how much they'll cost.
00:55:44.340 - 00:56:29.680, Speaker A: Right. And I think maybe another way to frame this is like, assume we do BLS in the next forks and EVM Max in the fork after. What's the benefit of users to have access to BLS six months, twelve months early? Right. And maybe they eventually move to using it natively in EVM Max or something. But I think that's the other trade off that's worth considering, is if we do both, but not at the same time. One way to ask this question is how would things be different if we had had BLS in Berlin several years ago? Yeah, regardless of just like the raw gas cost, I guess.
00:56:32.450 - 00:56:47.618, Speaker G: Well, if it's not factoring the raw gas costs, wouldn't EVM Max be strictly like disregarding the raw gas costs, wouldn't evN Max be strictly better in that case? Because you can just implement more stuff.
00:56:47.704 - 00:57:03.820, Speaker A: Right, but if you get BLS sooner. Right, that's the thing. I mean, yeah, the cost is not just the gas cost of each call, but it's being able to use it in six months versus twelve months.
00:57:04.350 - 00:57:32.180, Speaker G: Yeah, I mean, I'm biased, but I would say Vmax seems simpler to me than BLS. And I would implore people to take a look because it's really not that complicated. All I had to do was adapt a few hundred lines of assembly code from blast to get the most performant implementation anyways. Okay.
00:57:35.510 - 00:58:00.570, Speaker A: Cool. Yeah, just want to be mindful of time. Anything else on evmax? Okay, yes, we can continue that discussion on the threads. And then I guess the two things it does touch on is EOS and BLS. But does anyone have an update on EOS?
00:58:06.420 - 00:58:08.130, Speaker H: Was like when on the call.
00:58:10.020 - 00:58:21.172, Speaker A: No, he is. Oh, I think he was for a. Yeah, he is. I'm here. Okay. I don't have much of an update, honestly. I haven't been working on EOF.
00:58:21.172 - 00:58:53.650, Speaker A: I've just kind of been running the EOF breakout rooms, which we've got another one scheduled for next week. Yeah, I don't know. EOF 1.1 is kind of taking in a lot of the thoughts around the discussions in Austria and what EOFV two might look like. And that's sort of coming together. And I think that they've got this implemented in Geth now. I just haven't followed it super closely in the last couple of weeks.
00:58:53.650 - 00:59:16.280, Speaker A: What's the best place for someone to follow the latest on Eof? Like, if they want to just understand? Probably the hashtag EVM discord. Okay, cool.
00:59:16.430 - 00:59:48.770, Speaker F: Andrew, I have a small technical suggestion. So if we want like EOF, whatever, 1.5 or whatnot, with Vitalik's idea of non observability, that will be quite different from EOf one. So my suggestion is, for ease of referencing, create a single EP for this new EOf so that we have everything that we want in a single release in a single EP.
00:59:53.080 - 01:00:53.496, Speaker H: So there's multiple facets that are easier to understand in single eeps. But I think a single combo eep that says point to these five is probably what we might wind up getting because some of these are fairly separated and we put them together in one giant document and it looks like impossible, but you break it up into the five key component parts and it's actually, the whole is greater than the sum of the parts. I think we could write one that would help add to it. But I mean, like the stuff, like the observability, if the current plan we're going to stick with, that's still up a discussion. If we do the one one in Cancun and the 20 in Prague, where in one one we remove key features that would be necessary to preserve observability. And in Prague, we would ship the non observability preserving versions of things that we're taking out. I think a giant e would obscure some of what we're doing in the step work in there, and it'll make it harder to explain.
01:00:53.496 - 01:01:18.610, Speaker H: So I appreciate the need for to be able to get your hands around the full scope, but we still need to have clear seat. The structure of EEP does not really lend itself to five different subsections because there's like one big. The rationale and the implementation are often tightly coupled. So we should have a combo eep to point to the subcomponents. But I don't know if a full eep of everything together is going to be a good idea.
01:01:23.470 - 01:01:24.410, Speaker F: Understood.
01:01:27.470 - 01:01:48.110, Speaker A: Okay. Anything else on EOf now? Okay, so I guess maybe next up, we did touch on this already a bit, but BLS recompile. I don't know if there's anything to add here. We've discussed it several times, but in case there's any updates or comments people want on this.
01:01:51.520 - 01:02:23.620, Speaker G: Yeah, I'll just say that, kind of echoing what we said earlier in the call, this curve is really important to Ethereum. So I would really strongly suggest we consider shipping the pre compiled, even though there's very exciting work with. You know, we could go ahead and get the curve today, and then once EVM Max is ready, we ship. You know, there are plenty of other use cases for evmax beyond this that are also really important. But this curve is used on existence layer, so it's just something we should do ASAP, in my opinion.
01:02:27.560 - 01:03:20.330, Speaker H: Dana, the one thing I would ask if we do that is that we take another look at the gas prices. We did Berlin. We priced it against a gas metric of 35 million gas per second, which is about 28 nanoseconds per gas. Performance wise, that provides an upper limit as to what we could expect local clients to perform. And it makes the BLS ones the lowest point in the whole EVM architecture and capsis at 35 for a lot of performance considerations. I can explain why that's going to be the case, but I prefer if we were to reprice the gas on assumption of something more like 50 gas per second. 50 million gas per second, which would result in higher gas prices, but at the same time would allow the conceptual maximum throughput of the EVM to increase up to 50 billion gas per second.
01:03:23.180 - 01:03:40.690, Speaker G: Yeah, I think it's worth revisiting the EIP, especially if you want to very seriously consider it for Cancun, a question I have, since we're all here, does anyone feel strongly about this EIP 25 37 having a large number of pre compiles versus, say, just one?
01:03:45.620 - 01:03:47.490, Speaker D: Yeah, I kind of do.
01:03:49.460 - 01:04:03.370, Speaker G: I'm just wondering, because it'd be pretty easy, I think, to rewrite the AP where basically there's a type switch in the first byte, something like that. And the question then is, is one more palatable than the other?
01:04:07.500 - 01:04:09.684, Speaker D: What do you mean by having a type switch?
01:04:09.812 - 01:04:21.310, Speaker G: Well, just like, rather than say like nine eips or, sorry, nine pre compiles, you just basically call one pre compile. But the first byte says, okay, this is an ad, this is a bowl, this is a pairing, whatever.
01:04:23.840 - 01:04:37.810, Speaker D: I don't care about that. But I think maybe we should think about not doing all nine pre compiles. Or how many of them can I actually need it?
01:04:38.440 - 01:04:41.990, Speaker G: All of them. I think if anything, we actually should add two more.
01:04:47.090 - 01:04:52.800, Speaker D: Okay, I don't like that. Yeah.
01:04:55.090 - 01:04:56.350, Speaker A: Dan, granted.
01:04:58.790 - 01:05:26.890, Speaker D: Yeah, I just wanted to comment on the gas costs, because if the benchmark were done two years ago, I think the BLS libraries are also significantly faster now. So it's likely that gas costs from back then would be an overestimate rather than underestimate now, even if we want to target 50 million gas per second, or rather what's been done recently, I think is more just compared to existing pre compiled sent opcodes.
01:05:27.390 - 01:05:38.240, Speaker A: Right. Then it's probably pretty easy to re benchmark them because we just ran benchmarks on every client for the 4844 pre compile, so it shouldn't be hard to do it for BLS as well.
01:05:40.290 - 01:05:41.040, Speaker D: Yeah.
01:05:43.590 - 01:05:44.530, Speaker A: Daniel.
01:05:45.750 - 01:07:05.354, Speaker H: So I would prefer to keep all nine, or possibly eleven pre compiles. One of the reasons is right now, pre compiles, they do one thing, each pre compile does one function, and we don't have any pre compiles that have like a switch bit to do different things based on what's coming in. So we would also have to test, in addition to the nine or eleven forks coming out of that pre compile, we need to write test cases to test the switching logic, which would increase the testing load. It's not necessarily that bad, but I think the real concern that I have with it is by putting them all into one precompile, we're masking the true complexity of what's being asked to implement this and what it really means to the EVM that we're doing nine different functions behind one precompile. So I think it's being more upfront and clear about what the complexity of what's going on is, because originally this started out as a arbitrary length and arbitrary distance, arbitrary bitwidth and arbitrary sized set of precompiles that in theory you could have enabled anything at random times. And the issue there is that they were going to, in theory you could turn more stuff on as we go later, it makes a compatibility problem, because what if we add more switches and we add 1314 and 15, then all of a sudden code out there, we're going to have to have different versions of the pre compile, whereas before, if you switch on version 13, the pre compile would fail. Then all of a sudden it works.
01:07:05.354 - 01:07:14.350, Speaker H: I think switching inside the pre compile is the wrong place to add the complexity weekly held. I can be talked away from that, but that's why I prefer multiple pre compiles.
01:07:19.490 - 01:07:21.120, Speaker G: Yeah, that makes sense to me.
01:07:24.050 - 01:07:57.580, Speaker A: Yeah. And we did switch it out to multiple precompiled when we were testing bls for Berlin. So it feels like it's probably a mistake if we say we're going to rebundle them together and then start reimplementing, and then we'll come to testing and figure out it might be better to separate them. Yeah, unless something has changed. Probably makes sense to keep them separate, I think. Rand, was your hand still up from last time or was there anything else you wanted to add?
01:07:58.190 - 01:07:59.386, Speaker D: Oh, yes, sorry.
01:07:59.568 - 01:08:18.794, Speaker A: No worries. Okay, anything else on BLS? Okay, next one. So, 1153. I don't know if there's any champion, if Sarah or Moody are on the call. Oh yes you are. Hey.
01:08:18.852 - 01:08:19.474, Speaker I: Hi Tim.
01:08:19.592 - 01:08:20.260, Speaker A: Yes.
01:08:21.030 - 01:08:44.970, Speaker I: Yeah, I posted an update in the agenda, so feel free, people can look there. But only new update is that we have the tests now merged in the Ethereum test repo. And then. Yeah, just again calling out that the implementations in Geth, Nethermind, Bezu and Ethereum js have been run against this suite. And those are all merged as well. Thanks.
01:08:45.120 - 01:09:07.616, Speaker A: Nice. Thank you. Anyone have questions, comments on 753? Okay, almost done. We have three left. 47 80. This is the Beacon state route in the EVM, I guess. Yeah.
01:09:07.616 - 01:09:11.730, Speaker A: Lucas, I just saw your chat message. Do you want to share more?
01:09:12.420 - 01:09:26.790, Speaker D: No, I'm just saying I'm pro, including 1153 into Cancun. Most of things are done, so why should we postpone it? I don't see any point.
01:09:29.160 - 01:10:17.670, Speaker A: I guess because we have a bunch of open eips. I probably wouldn't make a decision today about including any of them, but yeah, we can definitely do that in the next couple of calls. Yeah. Anything else on 1153? Sorry, before we move on to the next one, I very much support it. It will save a lot of gas and save a lot on attack surface. We've just got to get ways to use storage that aren't so hard to get right. So thanks very much for pushing this forward.
01:10:17.670 - 01:10:54.990, Speaker A: Cool. Anything else? Okay, next one. So 47 eightyes. That's the beacon state route in the EVM. We also discussed this briefly last time. I think Danny had a comment plus running this as well before he left. Alex, is there any updates? There anything you wanted to share?
01:10:56.480 - 01:11:43.890, Speaker G: Yeah, I mean, I would just also plus one, this along with the BLS pre compiles or just BLS arithmetic generally is really important for a lot of staking pool use cases, which given the predominance of lido and sort of the fragile position that one dominant staking pool provider adds to network, I think it's really important to encourage and support alternatives. So for that reason, I think this one, and also some sort of BLS solution is like really do quite quickly. Yeah, the IP itself, four, seven, eight, I think is pretty ready to go. I can make a pass on it before the next all core devs, but yeah, this one should be pretty lightweight. And yeah, I think generally there's pretty good support for it, so I think we should include it.
01:11:44.660 - 01:12:22.976, Speaker A: Thanks. Any comments, thoughts on 47 80? Okay, two to go. Next up, six six three. So this was mentioned by the solidity team right before we actually. Right when we were discussing EOf for Shanghai and they mentioned that this would actually be very helpful for them. In addition, I don't know if anyone on the call is a strong supporter of six six three or has any updates or concerns to share.
01:12:22.998 - 01:12:34.950, Speaker H: I think it's important to point out that it's going to be Eof only because of the use of immediates. And this is the sort of stuff that EOF opens the door for in evolution of the EVM. So generally I'm in favor of it.
01:12:35.560 - 01:12:42.872, Speaker A: When you say eof only. So you're saying we should do six six three in like EOFV one.
01:12:42.926 - 01:12:53.496, Speaker H: Basically we can't do it without EOF, so we do it in either EOFV one or it comes in later. We may as well do it now while solidity is rebuilding their compiler.
01:12:53.688 - 01:13:09.170, Speaker A: Okay, this might be a stupid question, but given this is like an EIP from 2017 and EOF, I guess they've adapted the EIP recently to reflect this.
01:13:11.060 - 01:13:23.812, Speaker H: Yeah, they've called out EOF. I think at one point they were going to do the immediates until the issues with immediates was shown to be a security problem. And then it was adapted, I think to be stack based, but now they're back to immediate based.
01:13:23.946 - 01:13:26.710, Speaker A: Okay. And eos enabled them to do that basically.
01:13:27.340 - 01:13:28.184, Speaker H: Exactly.
01:13:28.382 - 01:13:53.890, Speaker A: Got it. Okay, any other thoughts, comments? About six. Six three. Okay. And then last one. So we discussed this briefly on the last call, but was the pay op code is the other one that's been proposed. I don't know if there's anything new since the last call that anyone wanted to bring up about that.
01:13:59.120 - 01:14:21.060, Speaker H: I'm skeptical on this one. I don't see what it adds that provides significant value. Some of the rationale is based on, well, there's a radio security exploit, you can do this stuff anyway, but I don't know if the answer to that security exploit is to open the door wide open and make it cheaper. Weekly held, I can be talked back, but generally speaking I'm skeptical.
01:14:22.120 - 01:15:44.406, Speaker A: Got it. Any other thoughts, comments? Okay, so we made it through the list, I guess high level for client teams. It's probably really important in the next couple of weeks to start thinking through all of these and what makes the most sense to potentially bundle together, I think for the sort of three bigger efforts. Well, I guess the four bigger efforts, right? Like for SSD, self destruct, EOF, EVM, max like thinking what's reasonable subsets to consider for Cancun is probably the most important thing. And then for these, I guess smaller eips, whether they make sense bundled with four, four four, and then potentially some of the larger, other things. Yeah, and we can keep discussing this in the next couple of weeks, but any other comments or thoughts about Cancun generally? Okay, sweet. So last up, Pooja, you've put together a survey these past few weeks for node operators.
01:15:44.406 - 01:15:47.020, Speaker A: Do you want to take a couple of minutes to walk us through that?
01:15:49.170 - 01:16:06.660, Speaker I: Yes, thank you, Tim, for that. So exactly two months ago we floated a survey for Ethereum validator users to share their experience about client node running and what can be done to improve present network distribution. Tim, if you can maybe pull up the.
01:16:08.390 - 01:16:21.400, Speaker A: Yeah, I posted it there. If you want to share your screen and give a quick overview, we have some time. We can do that as well. I can put it up if you want otherwise, but might be easier if you're the one talking.
01:16:22.570 - 01:16:40.280, Speaker I: Sure. I just lost my listing. I got it. I hope my screen is visible.
01:16:43.120 - 01:16:44.350, Speaker A: It is, yes.
01:16:48.800 - 01:17:43.500, Speaker I: Okay. Are we able to see the medium blog post? Yeah, we do all right, so a couple of weeks. Not exactly a couple of weeks. Actually, exactly two months ago we floated a survey for etherm validators to collect their feedback and thoughts on what they think about present client distribution. What are their experience with working with those clients? Like if they are staking or they are running client node for any different purposes, they have responded quite generously by sharing all their thoughts. The survey sample size was really low in comparison to the present active validator on the network. It is less than 1%, but we believe that the data provided by them could be useful for client team to understand what are the requirement of users, what they like and dislike most about every individual client.
01:17:43.500 - 01:18:45.280, Speaker I: So we received this response from about users from about 40 countries and obviously staking is the most prevalent reason for running Ethereum network node. The representation of chart here is different from what we do see@clientdiversity.org that's because again, of the sample size of the data, that may not be the exact may not be consistent with the percentage that is being shown there. However, the information shared is almost similar. Like Prisma Lighthouse are the favorite client on the consensus side and Geth is predominant on the execution side. We did receive a response to different questions in terms of how many pairs are being run by a single validator, like are they planning to have backups of it or not? I have tried to put together some comments. Obviously all the responses could not be added to this report.
01:18:45.280 - 01:20:14.670, Speaker I: So we have tried to add multiple repeated comments by user and we also ask questions about if they have to switch clients for client diversity or for any purpose. What would be the reason you would like to switch and what could be done for you from client teams that you would consider running minority client like client which is not in the majority network shareholder right now. So these are the responses that we have tried to capture and there is one very interesting thing like what can be done, what features should be added by different client team providers for you to help out with the staking of Ethereum with the help of what can be added to help you continue doing the staking on Ethereum blockchain. And they have provided quite a lot of input in that direction, like they would love to have an ELCL client packages and their testing could be more thorough. One Click could be really good to include more users and all. There are some suggestions on mev boost site as well. So I would encourage client teams to maybe go through the report and other users to understand what features are being provided by these clients, what can be suit their particular needs.
01:20:14.670 - 01:20:38.790, Speaker I: As I mentioned earlier, we could not add all the data points, but we have tried to include multiple comments. But if any client team is interested in learning about what all comments did we receive for individual clients, please reach out to me and I will share the responses received from users. Yeah, that's all. Thank you.
01:20:40.760 - 01:21:01.550, Speaker A: Thank you. Yeah, there was a comment on the chat. I'm not 100% sure about this, but basically some of the numbers of clients use end up being greater than 100%. But I assume this is because people were able to say like they use more than one client, right? That is like they use like guest and base two or something like that.
01:21:02.240 - 01:21:10.320, Speaker I: That is correct. So the questions were multiple answers. So they can choose more than one client if they are running multiple nodes.
01:21:11.460 - 01:21:30.810, Speaker A: Got it. Any questions, comments, thoughts on the report? Okay, well thank you Pooja and yeah, I guess we can wrap up then. Thanks everyone and talk to you soon.
01:21:32.540 - 01:21:33.530, Speaker I: Thank you.
01:21:34.300 - 01:21:35.128, Speaker G: Thanks everyone.
01:21:35.214 - 01:21:35.656, Speaker D: Thanks.
01:21:35.758 - 01:21:37.288, Speaker A: Bye everyone. Bye everyone.
01:21:37.454 - 01:21:38.600, Speaker D: Thanks. Bye.
