00:07:12.180 - 00:08:11.004, Speaker A: This is Alcredith's consensus layer, call 130. This is issue nine, eight, seven in the PM repo. I just shared the link. Anything related to Denub, then we'll move on to a quick discussion around local block boost defaults and we have a lot of things to talk about in Electra and then some networking stuff to discuss. So quite a packed schedule. Before we jump into it, Trent, you had something you want to share? Yeah, so for the past couple weeks, like starting in beginning of February, actually that's a month and a half I've been collecting submissions for this thing called denkun diaries. I just dropped it in the chat and it's just a compilation of core dev perspectives looking back on the last two years and looking forward to Petra and just future ways that we can better improve the processes.
00:08:11.004 - 00:08:49.390, Speaker A: So check out the link. I think it's a really useful resource. If people have been around, they'll know that I've done this for the merge and also the beacon chain launch. So just adding to that collection of historic record, hopefully we're starting to build institutional memory of how core Dev and how the community around it works. So thanks to everybody who submitted, I think it's a really nice snapshot of sentiment and hopefully we're learning as we go, but I'd love for anybody to check it out. If you weren't included for some reason or you missed the deadline, please just DM me and we can get you added to it. Thanks.
00:08:49.390 - 00:09:18.976, Speaker A: Great, thank you. Okay, we still have Deneb on here. One item from Tim. Yes. If you wrote an EIP for the fork that went live, you should move those to final. I've opened PRs for every single one of them that were not final. I think Alex Stokes was the only one who did it.
00:09:18.976 - 00:09:55.910, Speaker A: So shout out to Alex, they're all linked in this PR. So again, if you're an author on one of those eeps, please just take a look. I think pretty much all of them only require an. Yeah. And then hopefully we can get all of those finalized in the next couple days. Yeah, cool. It also needs an approval from editors, from an author and an editor, right? Yeah, once, once we have all the authors approved, I'll bug the EIP editors to get them to approve the batch.
00:09:55.910 - 00:10:24.286, Speaker A: Terrence? Yeah, I guess this is somewhat dynamic related. It's probably easier to use this forum. So we have been seeing blocks that has KZG commitment, but the blobs never arrive, so it means these are unavailable blobs block. So by definition these blocks never gets imported. So they are reordered. But consider these are actually somewhat valid blocks. It's pretty interesting.
00:10:24.286 - 00:11:03.350, Speaker A: So if you're running a validator. So I kind of presume this is a builder issue ready, because standard clients have been tested fairly well. Say today, if you're a validator, you are missing blocks, but then for some reason, your blocks are not getting heard, then please take a look. So here are some numbers. So on March 18, we have 3, March 19, we have 10, March 20, we have six, and 21, we have four of those incidents. And I finally captured those blocks. It turns out that no one's capturing those blocks by default.
00:11:03.350 - 00:11:29.240, Speaker A: So I have four of those blocks and I will look into them and find out what's going on. Yes, I also have the slots as well. I can share those. Yeah. Interesting. It might be worth doing a sandy check through block print if they are all the same client. Well, yeah, if they are all the same client, that might be telling them maybe it's not a builder issue.
00:11:29.240 - 00:12:20.678, Speaker A: Maybe you can also tell if they're a builder. Any questions for Terrence? That's interesting. Okay. Something we don't have to get into today, but something that I think if and as we do have any sort of meaningful data analysis on performance, blow distribution times, things like that, please add them to the call. I think people are eager to get an insight into how things are performing. Are there any interesting metrics or things worth sharing right now other than looks? Okay. Primarily stuck to four eight four and.
00:12:20.764 - 00:12:53.940, Speaker B: May not perform blog post on some of the metrics we're seeing. I've just linked it. Just have a look, I guess. I think one of the thing that we wanted to keep a close eye on is that the P 95 value for blob arrivals with respect to a slot are getting relatively close to 4 seconds. This was based on the first day of data, so we still haven't done a follow up analysis after the week, but that's something we want to keep an eye on.
00:12:59.960 - 00:13:02.340, Speaker A: Got it. Potent.
00:13:03.180 - 00:13:55.240, Speaker B: Just looking at this blog post about resources usage, it might be slightly misleading. We changed the timeline for attestations to be included on the net. So we're allowed to include attestations from more than 32 slots ago. And both, at least prison and tech will have this problem that they would just remove these attestations from the pool and recossit them if they see them again. So someone is sending these attestations very late. We do not know who, but certainly prism and Teco will just keep resending them and this blows up CPU and mean. Certainly bandwidth has increased because of the plot fork, but the numbers we're getting are just exacerbated by this bug.
00:13:55.240 - 00:14:22.080, Speaker B: This is already fixed on prism I suppose from Tecla as well. Yeah, I can confirm. And we have merged a couple of fix to actually accommodate the change, reflecting all the configuration around different pools and gashes that we have. So the next release should have that fixed.
00:14:28.710 - 00:15:22.360, Speaker A: Got it. Any questions on either Perry's metrics or the attestation kind of amplification relay? But sorry, I'm working on you. I can't mute him, surprisingly. All right, cool. Anything else on Danken?
00:15:28.610 - 00:15:44.690, Speaker B: I have a small cleanup PR that I think is also linked to the agenda regarding just renaming of blob base fee to blob base fee per gas, basically to make sure that the unit is more aligned with what it conveys.
00:15:51.750 - 00:16:10.760, Speaker A: Where is this PR? This is to 8316. This is a clarification or this is a bug fix or this is just a naming change?
00:16:11.210 - 00:16:22.250, Speaker B: Yeah, it doesn't affect any of the headers. So basically it just sort of renaming of some helper functions and references.
00:16:26.190 - 00:17:28.690, Speaker A: I see. Okay, so maybe the four authors should take a look. And if anyone else wants to chime in, please do anything else on Dankoon. Okay, I did notice on some of the charts that the AC has gone above one a couple of times, but I think last time I checked is still sitting there. Okay, just on that note, I did a very crude look at the blocks and it seemed like we're posting about 30% of the target number of blobs per block. On average, it was just over 1000 blocks. But it makes sense why the base fee is glued to one right now.
00:17:28.690 - 00:18:01.220, Speaker A: Yeah, and that's what I've been saying. We're kind of still on the average of one ish over time. One ish blob per block. All right, cool. Let's keep our eyes peeled on interesting developments here and surface. Anything here that's relevant to either client stability or potential future improvements. Next up, frederick has to leave at halfway through the call, so he has some comments around local block boost defaults.
00:18:01.220 - 00:18:02.120, Speaker A: Frederick?
00:18:03.120 - 00:18:10.540, Speaker B: Yeah, I'm in the sauna, but I hope the microphone is working fine. Otherwise I'll go outside.
00:18:10.610 - 00:18:11.470, Speaker A: Sounds good.
00:18:12.420 - 00:19:27.610, Speaker B: So yeah, this is basically about doing a client change, which should hopefully improve censorship resistance. If you look at one of the pages that Tony created censorship pix, we can see that. I think it's around 64% of all the external block builders are censoring blocks. Basically by not adding certain transactions. And about 51% of the relays also block these kind of blocks. And since basically today I think it's around 95% of all the blocks being built are done using relays and external builders, this poses a bit of an issue because some of the blocks will be censored. So basically what we could do to kind of help mitigate this a bit is to adjust the default local value boost to 10%.
00:19:27.610 - 00:20:46.530, Speaker B: Currently, in all clients this is set to 0%. But by changing it to 10%, it means that the local blocks will receive a 10% boost. So instead of if there is a block that can be built locally for zero zero four eat and an external block comes in and can be built by zero 00:41 eat, then the external block would be used in these cases, but by setting the default to 10%, the local block would be prioritized. In that case, the way that clients have implemented this differs a bit. Some of them have a boost on the external blocks and some of a boost on the local blocks, but this can always be changed by a flag. So even if the default is set to 10%, a user who don't want to do that could always change it to 0%, or they could set it to 100% if they want that. And the current status here, there are some PRs that's been made.
00:20:46.530 - 00:21:18.360, Speaker B: They're linked on the issues for this call. Nimbus and Teku have already merged these changes. The other clients have PRs for it and are thinking about approving them, I think. And the EF testing team are also implementing some additional tests for this, which was based on some feedback from the Prism team. So yeah, that's it in a nutshell.
00:21:20.000 - 00:21:51.828, Speaker A: Terrence. Yeah, I'm supportive of this change. I guess one question I have was that this will also, I guess, break some testing per se. Right. If before we're just adding a little bit of builder bit that's higher than local bit, now we have to make sure we add this at 10% more. So that's one thing. The second thing is that can we get some testing on Hive for this test case? Just because for the prism side, I don't think we have end to end tests that cover this scenario yet.
00:21:51.828 - 00:21:53.860, Speaker A: So I hope Hive can do it.
00:21:54.010 - 00:21:59.370, Speaker B: Yeah, I think Mario sharing can probably mention something about that. He created a ticket for it.
00:22:00.300 - 00:22:25.170, Speaker A: Yeah, I created a ticket. I'm going to work on making Hive aware of this flag so we can reliably configure it depending on the test case. It's going to be very simple. Basically, just if you are setting it to a high number, a higher than normal number, we should expect that the blocks are locally built and the other way around for the other case. Super simple. But yeah.
00:22:26.580 - 00:22:35.812, Speaker B: Not all clients have the same parameter working the same way, I guess. So that this is the only thing, of course.
00:22:35.946 - 00:22:45.588, Speaker A: So probably what we'll have to do is that we have a high value that's going to be transformed into a client specific value.
00:22:45.674 - 00:22:46.276, Speaker B: Yeah, makes sense.
00:22:46.298 - 00:22:51.568, Speaker A: And set into the flag protest.
00:22:51.744 - 00:23:28.340, Speaker B: I want to be the voice of descent. We should stop specifying these kind of things. We should stop doing this in a coordinated fashion. The reason this wasn't merged in prison to start with, even before the merge, when we started designing this, was that it would be controversial if we had a value that was bigger than zero and now we're coordinating to have something. And even thinking about testing this, I think clients should be free of selling these kind of things however they want. They are configurable by the user and we should be less afraid of actually acting on this in our beliefs.
00:23:33.810 - 00:23:55.572, Speaker A: Right. I guess the testing infrastructure can and should probably be neutral to defaults. It should just in the event that this flag exists, be able to still test building. Right. Can we check that balance? Sorry. Please. Yeah, in my opinion, yes.
00:23:55.572 - 00:25:02.570, Speaker A: It's going to be like, just in the case the flag exists and it's set, it functions properly. That's the idea behind the test. But agreed. I mean, there's some of these things that there's a reason other than just kind of network resilience, that many clients, different clients exist, different design, different philosophy, different failure modes. Sean? Yeah, I generally agree with Lotus, and it should be sort of a recline thing and a reason not to set it to 10%, for example, would be that it would be kind of an unexpected default. And it's kind of like taking advantage of the fact that a user might not be aware of this feature, whereas a better approach, something maybe in Lighthouse we could do this would be to require a user to set that value if they're using a builder. So it raises awareness of the flag without, I guess giving an opinion towards what it should be, and that might have a similar effect.
00:25:02.570 - 00:25:30.874, Speaker A: I am curious maybe some of those that do some analysis on this. Obviously we know that MEP is quite chunky, so a lot of the margin you get on most blocks is pretty small. Do we actually know a function of call it 5%, 20%, whatever that margin is like, how many blocks additionally would be locally built well, but we are.
00:25:30.912 - 00:25:37.614, Speaker B: Seeing relays playing some games that actually are making this margin as small as possible.
00:25:37.732 - 00:25:38.254, Speaker A: Right.
00:25:38.372 - 00:25:42.000, Speaker B: So over time we're seeing this margin being getting lower.
00:25:48.000 - 00:26:28.130, Speaker A: I see. Okay, are there any other comments on this other than just kind of surfacing awareness and knowing that there is a design space for clients in their default and ux around this parameter? Yeah Tim, but that's kind of this absolute value, right? You need to map that to a percentage. But yeah.
00:26:34.530 - 00:27:04.550, Speaker B: I think it probably depends a lot on the current market conditions as well. If it's a scenario where a lot of transactions are being done, you can probably make a bit more juicy MeV blocks and then the margin for might be higher compared to a bear market, for example. But yeah, I don't have any stats.
00:27:05.770 - 00:27:35.520, Speaker A: That's why that absolute number is kind of tough to deal with because the market can change quite a bit, whereas the percentage that a builder usually gets on top of a local build might actually not have the same type of market function, market dynamic function. But we have a pack schedule. Is there anything else people want to talk about? I know there's still some active stuff going on. Chat which.
00:27:42.260 - 00:27:43.856, Speaker B: No, nothing for me.
00:27:44.038 - 00:28:19.550, Speaker A: Great. Thank you Frederick. Okay, Electra, before we get into iOS and Max TV, I mean, there's a number of small proposals that might filter in. For example, blob gas increase, think committee slashing maybe is a small proposal. But these two big ones, we've gone back and forth. We've thought one or the other was buried and they resurfaced at various times. But we're definitely at the point where if we're not making a decision today, we really need to be making decisions soon.
00:28:19.550 - 00:29:02.890, Speaker A: There is the intention to have functional prototypes and devnets of Electra at some point in May, and we're closing at the end of March. So I just want to contextualize. These are medium if not large items that we continue to discuss. We can't continue to kick the canned on the curb for too much longer. Otherwise I think the default just becomes no, I guess indecision is a decision. I'm lost of opinions on these, but I will leave it at that. I think it's time to make a call.
00:29:02.890 - 00:29:37.910, Speaker A: I'm also not going to be here for three months starting at the end of this call, so you might have to make the decision on me anyway. There have been a number of breakout calls. There was an inclusionless breakout call. I think there is certainly some conformance on design and some of the subtle decisions around the design. Mike can you give us an update on where we stand here? Yeah, for sure. Thanks, Danny. Hey, everyone, hopefully you can hear me.
00:29:37.910 - 00:30:12.112, Speaker A: Yeah, so just linking the comment and the issue which links to many more links. So kind of links all the way down. Yeah. As Danny mentioned, there was a breakout room. The recording is there. It was pretty well attended and I thought quite useful. Terrence took some notes that are linked in that HeCMD, and I guess the main takeaway was just everyone coming to consensus on the set of features that we want for the kind of.
00:30:12.112 - 00:30:52.732, Speaker A: The last big thing was discussing whether or not to bundle the inclusion list with the block on gossip, and we all agreed to do the unbundling version. And yeah, that's kind of where we're at. A few more docs that I linked in that comment. POTUS has the doc on bundling that I just mentioned. Terrence had a short doc on the relationship to blobs to inclusion lists. This isn't part of the proof of concept, but it's kind of an interesting thought experiment as we kind of move down the path. I wrote a small doc called the case for Electra that's more of like the meta argument for why this feels important.
00:30:52.732 - 00:31:32.730, Speaker A: And then the POC spec is still being developed in this link here. I'll link this one actually in the Zoom chat because it's kind of the source of truth. Trent put his camera on too. That's funny. So, yeah, I guess it feels like a lot of progress has been made, I guess in the last week. Would love to continue jamming on it in the room. And I guess a lot of the client teams have started implementing and would definitely be happy to hear from any of them if they would like to chime in here.
00:31:32.730 - 00:32:16.200, Speaker A: Seems like generally the implementations are going well and uncovering lots of nuances about the spec that we're kind of ironing through. The only thing I'll bring up today as far as technical issues is there was this one question about the relationship between enshrined four, three seven and inclusionless. We've kind of talked about this once before, but this was kind of rebrought up in today's context, and I don't think it's a blocker. From what I can tell, there's nothing. Yeah, Il has unknown with AA. Yeah. So I don't know if you saw my latest message, Terrence, in the chat, but I don't think this actually is a blocker.
00:32:16.200 - 00:32:21.690, Speaker A: I think we can deal with it. I don't know if it's worth going through the exact mechanics here, but.
00:32:25.500 - 00:33:02.660, Speaker B: Mean if we have execution layer clients that are actively working on accounts abstraction. Let's just phrase the problem abstractly here. What it is, it imposes a condition on account abstraction that forbids an account from being swept from being swept by a different account. If there are designs that are the half chances that are people like looking at, that will allow an account to be swept out by a different account, by a transaction coming originating from a different account, that would break our current design of inclusion lists.
00:33:06.120 - 00:34:11.060, Speaker A: Did you see the message? So I asked Vitalik about this and he said basically one way around this would be to just check a few things. You add another condition, which is not only if the nonce is being reused, but also if the balance of the account decreases, then you can ignore that transaction for the inclusion list. Even if there is an AA world where you can sweep an account without sending a transaction from that account. We could just have this other check on the inclusionless transaction condition. Does that make sense? Right, that's what I was thinking too. Yeah, I sent the screenshot of the chat to the I don't think it gives free DA because you can just drop the transaction in the inclusion list because the summary entry is satisfied by the transaction or by the balance of the account going down. I think it gives you free ability to fill the IL with useless transactions that you sweep.
00:34:11.060 - 00:35:20.190, Speaker A: Well, you can leave the IL empty already. Il is, yeah, but if you nodes that are trying to construct ILS that some transactions might be good to put in the aisle, and then once they make it in the IL, taking space for maybe other transactions which might have been better, then you can just sweep them and remove them. Yeah, I see what you're think. I agree with Francesco's point in the chat that you could already try and stuff really high paying transactions into the summary and then use a different transaction with the same nonce to replace those transactions, I think you still have to pay the. So like, it still feels fine. Maybe there's a slight subtle difference here, but yeah, maybe we can handle it offline. Yeah.
00:35:20.190 - 00:35:43.350, Speaker A: So one of the attempted things to assess over the past month on this was engineering complexity. Does anybody want to chime in on engineering complexity? Maybe noting any sort of concerning points of design or unknowns, or maybe comparing it to some type of work we've done before so that others on this call can better understand the relative complexity?
00:35:51.450 - 00:37:21.030, Speaker B: I am working on the IlpoC, so I can basically add few points over here. So basically it is a little bit different than blob availability because the next block, if you get the next block which is valid, then basically the previous block IL gets satisfied. So when you are trying to do range sync, basically you don't really need an IL. And when basically you are synced to head and are doing gossip and at that point the IL availability really matters. So that is the only gotcha that was around this. And apart from that I think I found that giving status or a flag in folk choice was actually more convenient because then it would help me forward sync blocks easily through blocks by range and basically I could just track the status that okay, I need to basically check that whether they will have the valid child in the end or not, the tip will have a valid child in the end or not. Or I will basically then have to start looking for and importing the IL corresponding to the tip.
00:37:21.030 - 00:38:05.970, Speaker B: But apart from that I think these are the moving parts and then the engine API integration that I'm going to start with get, but it doesn't seem that challenging. And basically I think once we wrap around our head over the concept that I basically is forward and the availability is a bit different and basically just handle it that way. I think it could easily be developed, it could easily be targeted in electra.
00:38:13.910 - 00:38:18.690, Speaker A: Thank you. Any other perspectives on relative complexity or any questions for Kajinder?
00:38:25.940 - 00:39:50.700, Speaker B: I can jump in and saying that I started working on the POC implementation and I got so I become more and more confident on the implementation complexity in generally and the only thing is still in the back of my mind is the IL signing compared to the block production. If we want to sign the EL, it's convenient to sign and send over the network and yel before the actual block production. It will then also require some additional API calls and different other timing between VC and VN. So I'm currently thinking about for the PoC just having the same at the same moment. Just sign the block and sign the yell together. But this might be another complexity in the API and integration between VC and getting if I'm not mistaken.
00:39:52.720 - 00:39:56.460, Speaker A: Poto's asked why is that? But I'm not sure at what point in the conversation.
00:39:58.240 - 00:40:48.660, Speaker B: I'm not sure how technical wants to be here, but I didn't understand Rico's point that we need extra API methods to send the inclusion list before the slot. Can you repeat here? I haven't checked. I didn't understand why sending the inclusion list before the start of the slot would require extra API methods. It seems to me that it's the same. You're just going to request them from your execution layer before, right after you send the SCU of the previous. But you need to sign that. So currently the block production is started this block before.
00:40:48.660 - 00:41:19.488, Speaker B: Right. So at the moment of the block proposal, the VC just requests the block and you sign the block. Okay, but you're also mentioning that it's also possible that you could sign, you could sign the inclusion list for the slot. The thing is that you're going to sign that thing anyways. You're going to sign it at your slot or before the slot. So that API call is going to be there either way.
00:41:19.654 - 00:41:19.984, Speaker A: Right.
00:41:20.022 - 00:42:03.950, Speaker B: But you need to request it through the will do that. You will do that anyways during your slot. Well no, because now we have this concept of block content going through the API and like we have for blobs, you got a block plus blobs and plus other things. So we could put the yale inside of it. And then the VC on the same API call, you sign the block. You're saying that instead of having two different calls, you're going to have to overload the call for the block to return a different envelope with two different messages that you sign. Yeah.
00:42:03.950 - 00:42:57.372, Speaker B: Okay. If we want to make the flow, it's the same. We can at the moment if we want to release everything at the same moment, but if we want to have the freedom of signing and sending over the network in different timing, we need to design a new API. I think we can't avoid that. Yeah, I've been implementing IElts on lighthouse and I agree with porters. I think it is quite useful complexity wise if you are able to send the Il before the start of your slot, because you don't have to touch any of the existing block production logic at all. So it would be a significantly smaller refactor and it would be an addition.
00:42:57.372 - 00:43:55.616, Speaker B: And complexity wise I would prefer that compared to overloading the current block and blob proposal methods, unless just at the beginning of the slot your head changes. Right. So that basically can cause you to basically request Il and sign it and send it. But I guess that can be taken care of. And I also agree that independent design is better because what generally happens in block production is that it's not just the execution layer that PN wait for, they also wait for builder, which basically sends the block a little bit later into the slot, which I think they are always trying to target 1.5 2nd or more. So it would be faster, definitely just at the start of the slot just to request an IL and send it.
00:43:55.638 - 00:44:49.576, Speaker A: Over independently all right, let's table this. It's interesting that we're at the point of debating some of these nuances, which is certainly if this is to go in, we'll have plenty of time and time to do that. I don't want to attempt to make the aisle inclusion call without Max B discussion. I will say before we move on to MaxDb discussion that at least a couple of individuals and loadstart seems as an aggregate signal that they would like to see this in this work. But I think that we generally have a feel on the status update. It seems like a lot of people have opened this up and taken a look. Let's take a look at Maxib where that stands.
00:44:49.576 - 00:45:02.010, Speaker A: There was a breakout call yesterday and there is to be another one on Friday. There were two because of time zone considerations. Phil, you wrote up some notes. Can you give us a high level to start?
00:45:03.580 - 00:46:23.650, Speaker B: Yeah, so yesterday we had a breakout room in which we had participation from Lodestar, from lighthouses there, and also prism. Overall, it seems like at least between the three of us, there seems to be some general consensus on inclusion for this, and mostly because we have a lot of active validators on here and it's going to get to be pretty concerning if we don't deal with this. So it seems like we have consensus on that, but we do have some remaining decisions to make in the spec, which is what these follow up calls are for. There is a issue open by mark for these two, and there is an agenda for the next one which I'll post here as well. And then if you can come to these meetings with some of your thoughts. I believe Mikhail had also updated Dapline's PR in regards to maxib with some additional points in regards to slashing, but yes, that's basically the TLDR. Not sure, Mark, if you want to add anything to.
00:46:26.100 - 00:47:22.400, Speaker A: Yeah, so I think from Lighthouse's perspective, what I've seen is that we've prioritized Max TV in that inclusion list, but are totally open to doing both in the fork. I believe that's basically the same idea from Lodestar and Preston on Prism expressed a similar sentiment. And then I talked to Phil yesterday and he had similar sentiments. I would say so, yeah, I think that. Go ahead. Yeah, can I jump in? I'm curious. Prism's current perspective on the complexity of Max EV engineering complexity.
00:47:22.400 - 00:47:29.830, Speaker A: That has been something that's been brought up a number of times, but I know that the team is going to kind of reevaluate. Can somebody speak to that.
00:47:33.120 - 00:48:02.390, Speaker B: Is Preston here. So if Preston is not, I can just say that he's in charge of doing this and he has an implementation that it's almost complete and he's very confident that it can be included, and he's confident that we can target Maxib for end of this year, which was one of the hard requirements that we had on any ID.
00:48:03.560 - 00:49:14.670, Speaker A: One of the takeaways from the meeting seemed to be that MAXDB required a little bit more of making decisions around or cleaning up the spec, I guess. But it seems like less engineering work to actually implement than inclusion lists, whereas inclusion lists is a little bit of the opposite. The spec is a little more stable, but it's a little more engineering work. But yeah, the things that remain on the spec to decide are, it's like we have several good options. It's almost reaching the point of bike shedding to where we just kind of got to pick one. Nothing that we can't fix. Okay, Mark, you were echoing that a number of teams were signaling both, but did you also say that those teams were signaling both with a preference for Max EB in the event that it was one or the other and consistent across.
00:49:14.670 - 00:49:54.792, Speaker A: Preston did I believe he said that at least him personally would preferred Maxey B over Electro? That's my opinion and Sean's opinion in Lighthouse. I don't know about Pon, and then I think lodestar Phil, you might have expressed a similar opinion in terms of priority, but. Oh, did I say Max? Yeah, I think you meant idols. I meant inclusion list.
00:49:54.846 - 00:49:55.450, Speaker B: Yeah.
00:49:57.920 - 00:50:54.072, Speaker A: But, yeah, I think everybody expressed that sentiment, and I think that's in the notes. Maybe, Phil, you can correct me if I'm wrong, but, yeah, I think that's what I remember. I certainly remember lighthouse prism saying that, and I know that Paul said it. I don't know about Paul's priority, but he did say that maxi b seemed fine to. So there's a couple of things I want to bring to the table. Obviously, being able to handle the engineering complexity of one of these in isolation is something at a certain point, just when we're smashing many things together, spec wise. One, the spec build is more complex.
00:50:54.072 - 00:52:27.228, Speaker A: Two, the testing of that on every single path is more complex. The getting to the point where you're even at testing for clients takes more time. So certainly these things can sometimes compound in unexpected ways, and I want to keep that in context. Yes, they're moderately isolated in terms of the components that they're touching, but I wouldn't naively just say the complexity is additive in terms of shipping an entire fork. So one, I just want to keep that in mind if and as people are pushing for two, I also do want to bring up and reiterate there is a third moderate to major R d thing going on, which is pure dOS, pure data availability sampling. My intuition is that one of Maxi beer ILs going into a lecture probably does not greatly detract from the paralyzation of being able to do pure dos R D, but that in the event that they're both, that we're now beginning to trade off and probably not really being able to tackle all those three things in parallel once. That's not to say that some of that will happen, right? The networking experts are different than people that are touching database this or that.
00:52:27.228 - 00:53:17.242, Speaker A: So there is parallelization that can happen, but intuitively it feels like we probably going over a tipping point on a lecture where that's going to take out most of the oxygen in the room. And so I just want us to be very conscious about these dynamics when attempting to make this decision. I think we did a little bit and there was, because peer Das can be parallelized provided that teams have. That was kind of why the other two proposals were being pushed more. That's all I'll say about that. That was just kind of a sentiment. Yeah, agreed, agreed.
00:53:17.242 - 00:54:55.900, Speaker A: I do think that they're just. I worry that the kind of mounting complexity of electra will almost certainly across some teams, if not all, begin to be a kind of a balance between resources aside, human resources at times, if not the entire time it. Yeah, I guess I'll just say something. I do agree that at the end of the day there's only so many engineering hours, so including both is sort of a tacit acceptance to deprioritize peer desk to some degree. I do think though that we have the capacity on lighthouse at least. So, yeah, those are my two cent. Are we? So at the outset I said from a timing perspective in relation to what we're attempting to do in and by May, we're already behind in attempting to make these decisions.
00:54:55.900 - 00:56:26.332, Speaker A: And so kicking the can down the curb for another two weeks puts us certainly in a tough position to try to hit some of the May targets. So implicit there is today's the day. Is there additional information that we are going to gather the next two weeks that's going to help us make this decision or are we in the place to make this decision today? That's a go or no go on both of these moderately large, if not large proposals, Sean. Yeah. So I would just say it seems like the further we've been getting into the POCs, the more confidence there's been from a development perspective that just implementing these two are doable. I definitely get your point about especially in the spec and the spec tests, like flushing all those out at the same time, covering all the edge cases that might take more time, but I think from a development perspective, definitely achievable in the clients and on the inclusion list side. One thing that I didn't bring up earlier and I'd like to get the perspective on is the amount of complexity in the execution layer versus the consensus layer.
00:56:26.332 - 00:56:43.700, Speaker A: Is this something where by making this decision here, it's really 90 95% of the complexity is over here and it's really kind of a small ask on that side? Or is this something where it's more like 50 50 40 60 in terms of complexity split?
00:56:46.120 - 00:57:16.800, Speaker B: I think it's not even clear right now. Today we started discussing that. Now the EL is going to have to go back and check if the certain balance decreased in the previous block to validate the inclusion list. We are just learning about these issues now. This is in part because at least I can speak for myself, naive, not knowing about execution. I really thought that this transaction would revert instead of being just not includable.
00:57:21.150 - 00:57:43.042, Speaker A: Yeah, I think we can analyze the damage, I guess, kind of risk of this new account abstraction thing in the next day or two. I don't think it should fundamentally change anything, honestly. And as Onskar mentioned earlier, enshrined 457. Is this only matters in enshrined 457?
00:57:43.176 - 00:57:55.140, Speaker B: No, but the thing is that even if we do include this and we not consider at all what will happen in the future with account abstraction, this increases complexity on the El side for implementation of this.
00:58:03.860 - 00:59:04.510, Speaker A: Do we Matt or Marius, could you talk a little bit about El complexity or if any of the ref guys, I think for ILs it's not too bad. On the El side, I worry slightly about having to keep track of balance changes from the previous block, but none of this stuff is intractable. It's just a bit more work, bit more things to think through. Yeah, super happy to work through the implications of that specific AA thing, I guess. Apart from that, though, it seems like everything's well understood. I'm definitely not an El dev, so trying to guard my language as much as possible. I think the work on the El side is maybe like 20% of the.
00:59:04.510 - 00:59:08.930, Speaker A: Yeah, that's the number I was going to give to 80 20.
00:59:12.700 - 00:59:31.440, Speaker B: From my theorem JS perspective. I think ILs should be easy to implement on El site. So els have all the capability. They have dummpool, they can gather transactions and just bunch them up and easy to verify. Again, like executing a payload.
00:59:38.780 - 01:01:15.930, Speaker A: Are we at the point where these specs could be built into Electra with 7002 and the other things that are already? Or is it a couple more weeks of R and D of PoC of design decisions? It sounds a bit like the latter. I don't know. I just get the impression that everyone more or less agrees that these things can be done. I don't that there's probably not major unexpected implementation problems that are going to come up. And at this point it might be a question of what do you prioritize more between the two or between yeah, I guess complexity shaping all that complexity, I guess makes the decision on whether or not we go for both, and if we have to go for one, then it does kind of depend on what do you prioritize. That's at least my read. One interesting thing is that the anti censorship parameter changes from Frederick earlier in this call.
01:01:15.930 - 01:03:31.252, Speaker A: If that really moved the needle on quality assurance of transactions, economically viable transactions making into the block, then can I wait, is there any intuition or data on that? On how much we can move the needle with smaller things? Because if that can be moderate, even moderate, not even massively, substantially moderate, I would make the case for being afraid of the complexity of both of these things together and knowing how timelines go and knowing how tough it is to test these systems and tabling that if there aren't some moderate wins to be had there or in other places, then maybe it's worth taking the complexity now. Yeah, Poto is just to contextualize. We're talking about what to include in the electric work. Obviously there is another major R and D stream which is pure dust, so that will occur in some amount in parallel, but it is there in the trade off space. The more that goes into lecture, the more that that parallel workstream will not get us as much fire. What's oop out of protocol? Mike says my two cent don't depend on oop for Cr. I do agree, but on what time frame, right? Like if we can help reaffirm the norm of Cr and be in a reasonable spot for a stretch of time with smaller changes, then it advises time for the protocol to then take on that complexity.
01:03:31.252 - 01:05:25.470, Speaker A: That's at least the argument I would make. Yeah, so the aisle decision can't be made without the execution layer. We do have execution layer this year. But if there was an attempt to go on the side, this would, I think, have to surface in a week on the executionary call to do an affirmation on that side. Certainly the decision on Maxi B, I believe, is almost entirely in isolation. Tim says we can see if I start with and everything else, and see how we feel about that. Tim, you want to speak to them, right? Yeah, so I think for two reasons, I guess I'm proposing that one, obviously there's high uncertainty about whether we can fit ILS and MaxCB in the same fork, so I should probably just start working on it two like, yeah, MaxCB is clearly just CL thing, whereas ILS does have implication on the EL side as well.
01:05:25.470 - 01:07:02.030, Speaker A: So it'd be good to sanity check with the other EL stuff that we are considering. The teams feel like there's bandwidth. So if we include Max Eb, we have the CFI thing that we've used for the EL in the past that basically signals things we are strongly considering for the fork but not fully committed to yet. We can put ielts there, work on a first Devnet with Maxib, all the other stuff we've already included. If next week we make some decisions on Acde about small EIPs, new opcodes or whatever, we can have that in part the first devnets. And also I think this gives us a couple more weeks to figure out some of these spec level issues for ILS, where if like two to four weeks after that we have a first devnet or two with everything else, a better understanding of the IL complexity, then we can decide to merge it all together, or potentially to push IELTs out to the next fork or something like that. So is the proposal to finalize Max EV spec to build into the electric spec while over the next two weeks to continue the IlPOCs and go no go in two weeks based off of that, and at which point we're either building into the spec or we're setting aside.
01:07:02.030 - 01:08:29.420, Speaker A: Yeah, and I don't know if I'd want to put two weeks, but maybe two to four or something. I think the data point that we should be looking for is implementation readiness on everything else alongside the IL spec readiness. So if in four weeks we have a working Devnet with MAxCB and people are feeling very confident about the whole thing, then great, maybe we could add IL and that's additional complexity. But if in four weeks we're in a spot where it feels really hard to even get what's already done implemented, then yeah, that's a signal that ILS is probably additional complexity that we can't take on. My only concern there is would there ever be a case where we then decide ILS are more important and switch it out with Maxb? Probably not, because of my attempted aggregation of signal that Max Ev is the preference. I know that you feel otherwise, but I think that's what most people have echoed so far in the chat. Yeah, but I think it is worth emphasizing though, if we go that route, it gets hard to pull out Max CB because we've merged it with anything else.
01:08:29.420 - 01:10:05.000, Speaker A: Yeah, and again, my read has been, for the majority of people in this call, if it were one or the other, Maxi B. So begin the spec build with Maxi B. Just from a spec build complexity standpoint, that's going to take the next ten days, next at least week, and I don't really want to build those two things into electric at the same time. So there's kind of a sequential thing that has to happen. Anyway. I'm going to say again, I personally believe that these two things together, we've entered into a much more complex space than we have been discussing and intended to over the past couple of months, so just do so knowingly. I'll also say at this point in the process, we're also always very confident and excited to take on complexity and there's a lot of work to do.
01:10:05.000 - 01:11:14.176, Speaker A: Okay, we do have a current plan. I'll just say it. Maxi V spec to be worked on, gotten into a very stable place and integrated into the electro spec build as soon as possible. ILPLCs, final design decisions and things to be done over the course of the next one to four weeks and make kind of informed decision on Il inclusion in the two to four week time horizon. Probably more like four, based off of continued understanding of complexity, POCs, electric PoCs, et cetera. Okay, thanks everyone. Tough conversations.
01:11:14.176 - 01:11:15.152, Speaker A: Potos.
01:11:15.296 - 01:11:29.130, Speaker B: Yeah, I just wanted to confirm that. It seems that it's not really certain, but it seems that we're aligned with prison. That maxib is a yes, and if we do commit to Maxib, then IELTs would be a no.
01:11:33.930 - 01:12:45.590, Speaker A: Okay, good to know. I think that we can still have the conversation in two to four weeks, and you all can if you stand firm in that, echo that at that juncture. But good to know going into it, any other comments on these two proposals before we move on and the implicit proposal of peer dos that is in competition with them? Just a quick update on that. Very active work on specs and sub initial prototyping and eager to have other teams really at this juncture jumping into certainly the spec and design discussion and prototype as well. Okay, we're not going to make it through this whole schedule today. Ansgar, time based blob gas increase proposal. Can you give us just a quick perspective on that, Ansar or mike?
01:12:46.350 - 01:13:32.262, Speaker B: Yeah, sure. Basically, initially we had like a big debate about the right throughput level for it before we ended up going with three target six max. So maximum of 750 block trade off, of course PTP load and the validated disk space. Yeah. The intention of course is that we want to scale up DA from here to full time sharding over the next three to five years or something. Most of that we will have to do via actual sampling methods so peer does and then potentially steps afterwards. But for peer Das, we will need to have an ERP anyway that increases the throughput because that's the only kind of in protocol change that that needs.
01:13:32.262 - 01:14:32.886, Speaker B: Also separately we could even before Pierdas already try to just basically make the most of the current headroom that we have. V Six was of course cautious by Electra. I was thinking we'll have like six to twelve months of experience with kind of three six and how stable the network will be with that. We have this EFP by Tony 7623 to increase the call data cost. So that will decrease the worst case normal block size from roughly two megabytes, almost two megabytes today, to roughly half a megabyte at count gas limits. So an idea of course for that is to both just basically protect against worst case attacks, but also to give us a bit more headroom, both to increase the L1 gas limit, but also hopefully some extra bot throughput. Of course for bot throughput it's not just about worst case, it's also about average case considerations.
01:14:32.886 - 01:15:20.966, Speaker B: But still from both these perspectives, both for potential peer does, but also already using the headroom that we have today, there was the idea of basically having an EIP for potential kind of increase in the bob throughput. So we created one just mostly as a physical sharing point for conversation around this. It's currently in draft, there's a link in the agenda. The idea would be, although of course we could change that to maybe pace in the block throughput increase gradually. So the county IP proposes to go from count 36 to a total of 816. So 1 MB target, two megabytes max over a span of four months. Basically every month ship one extra target blob.
01:15:20.966 - 01:16:29.470, Speaker B: Basically right in base case we could use that still with full download logic and then gradually scale that and make sure we have this emergency escape hatch, that if we see any problems with the network, we could fork and basically cancel those further future increases. But also if we are successful in shipping theaters on a similar timeline to lecture, then we could of course, just seamlessly use that as the kind of throughput increase that would be powered by Das instead. Yeah, so the idea is, of course, kind of like it's a little early to make the decision now. It was more to now have an EIP that we can specifically reference whenever we talk about this. And my personal belief at least is that basically sticking to this rough meme of trying to three x the Ethereum da every year from now on, over the next three to four years until we are at dank shading levels would be a very nice thing that we could provide for L2s that would be roughly reliable in terms of the timeline for increases. And this would just be a first step on that path.
01:16:31.730 - 01:17:33.540, Speaker A: Thanks, Samskar. Just quick context. I am a co author on this. I would support something in this direction. One, assuming we get solid data on four, four in production, two, in the event that Tony's called data price repricing scheme does go in, and three, I'm certainly a proponent of this time based increase timing and maximums to be debated in this case. And also in the case that we're doing pure DOS type or database sampling type things really going from testnet simulations, testing analysis to mainnet is, I think, always going to come with quite a bit of uncertainty. And so this time based approach where we do have the escape hatch of kind of an upgrade to cancel, is, I think, a tool that's going to be quite useful as we're changing things around data.
01:17:33.540 - 01:18:22.680, Speaker A: We have a lot on this agenda. This is certainly just to kind of open up the dialogue. Does anybody have additional comments or questions at this time? I can't figure out how to raise my hand. How does the kill switch on this work? If we decide that we want to stop the progression of increasing the limits, that would be just a quick fork. It would be an additional EIP and a fast upgrade. So that should be considered when thinking about automated time based increases. Essentially at these various junctures, you get new main net data.
01:18:22.680 - 01:19:20.470, Speaker A: You can certainly design some sort of like kill switch where it's like how the gas increase is done, essentially consensus voting or something like that. I would try to avoid that complexity and governance point. All right, anything else on this one? Thank you, Anskar. Next up, a couple of EIPs were dropped in by it's on sync committee slashing. This is in draft status. Do you have anything you want to comment on here other than existence?
01:19:22.890 - 01:20:47.518, Speaker B: Yeah, it's like essentially the current state of the research of what was previously in consensus specs. There was an issue where I already made this proposal last year, and for the light client data backfill there is an open PR, but because they affect consensus, I think it's appropriate that they are also tracked in EIPs. The light client sync committee shings. They are especially interesting together with maxib because maxib allows the sync committee to have a larger total balance. So larger total balance there means that more ETH can be slashed in case of an attack. Right now, many use cases, even if the slashing is perfect, are limited in how much they can secure because there is only 512 validators, even if they are slashed down to zero. It's not that much if it's like a bridge that secures a lot, but with maxib this could become interesting to follow and light client data backfill.
01:20:47.518 - 01:21:23.870, Speaker B: What this one enables is eventually decentralized chain synchronization for the beacon nodes, similar to what we have on the EL with Napsync. We can build it for the CL as well once we have that, because it allows to sync the trusted block route decentralized and then also enable snapsync from there on, getting rid of the servers like checkpoints, and those are no longer necessary.
01:21:29.260 - 01:22:06.420, Speaker A: One thing that's worth contextualizing, at least in my mind, on the committee slashings, is that there have been at least a number of R D efforts to attempt to zk the FFG portions of the beacon chain to get these bridges that end up having the full crypto economic security with kind of the default slashing, default consensus and just bypassing this team committee. Do you have any perspective on whether these are likely to come into production fruition in reasonable time frames, or are they still pine sky edges?
01:22:07.240 - 01:22:39.870, Speaker B: From what I was told and what I heard, I didn't check personally, but it's still quite far. Definitely post vertical as I understand, but I'm not an expert in this area, so if such a transition could happen earlier, of course that would be much greater than the student committee, which is still a heuristic, so to say, with the CK proof. You know that it's correct, right?
01:22:43.820 - 01:23:59.050, Speaker A: Yeah, I'm going to knock on a couple of doors I heard even a year ago, moderately promising results, but I haven't heard of any of these projects actually finishing. Any questions for Tom or intuitions or opinions on where these two might fit in relation to electric. Okay, any further comment on these? Okay, we do have a number of PDP discussion points. There is a new proposal up by Agent Anton on network shards, which would be in relation to kind of how nodes are selected for ad net and future das distribution. Can you give us a brief on.
01:24:08.750 - 01:25:03.780, Speaker C: Just give it. Yeah, I'll just give a quick overview of maybe just all the things that I raised just to save time. Essentially, I just wanted to make some of the teams aware of these things. So there's this PR which we've called network shards, something that Anton suggested, but it currently isn't super important for what we have right now. But when we start having extra gossip subtopics that we need to form like stable backbones for, this introduces a new concept that essentially allows us to tag a node ID or a peer ID to which topics they should subscribe to. And it allows us to have essentially an easier way of keeping track of those peers to make the topics stable. So essentially I'm just trying to get some attention from client teams to have a look at that PR and just signal whether they think it's a good idea, bad idea, so that we can kind of progress and move forward with it.
01:25:03.780 - 01:25:46.990, Speaker C: The second thing that I suggested is the I don't want message, which is in gossip sub. It's in the gossip sub specs at the moment. The implementation, at least for us anyway, is quite small, but it promises to significantly reduce some of the bandwidth on gossip sub. We're planning on starting to test this practically now in Lighthouse, but it's one of those things where we need the entire network to upgrade before we start seeing some of the effects. So if you have some spare bandwidth, it's probably worth the effort to try and implement this into your client and you'll share the benefits of it. It should be fairly easy to understand if you just have a look at these two PRs.
01:25:48.290 - 01:26:01.060, Speaker A: It's backwards compatible, but the more people that do it, the more aggregate effect it's going to have on the network. Meaning you could roll around Lighthouse independently, but you're just going to be a bit in isolation and usage of it.
01:26:01.510 - 01:26:15.666, Speaker C: Exactly. Yeah. So it's entirely backwards compatible. We'll probably try and release it into lighthouse and see if it is useful, but we only have X percentage of the network. The more clients that do it, the better the effects.
01:26:15.698 - 01:26:28.890, Speaker A: Yeah. And is this another, is it in a state that you would release it yet, or do we need to go through some p to p hurdles to kind of get it into the spec. I see it's not merged.
01:26:29.230 - 01:26:56.360, Speaker C: Yeah. So it's not merged. In my opinion, the general premise is pretty much outlined in the specification that you can go and implement it and you won't have any issues. There's some parts that aren't specified and there's some discussion around some DOS vectors that can come up because scoring hasn't been introduced into the spec. But I think that's not a huge hurdle for not implementing at least the base version of it.
01:26:56.970 - 01:27:00.120, Speaker A: But like the message format and things like that.
01:27:00.650 - 01:27:36.020, Speaker C: Yeah, that's all specked out, so that should all be fairly good. Yeah, I'll take questions on all three after this, if anyone has any. But the last thing is mplex. I think there was a discussion about this earlier on where some of the client teams only had McLex and we're going to upgrade to YaMX. Mplex, I think in Lipidp is now being deprecated. Zhao is on our networking team may have some extra things to say, but just curious about any client teams, whether they still require amplex, because our plan is to kind of deprecate it at some point. Yeah, that's it for me.
01:27:38.500 - 01:27:43.760, Speaker A: Let's take the last question first. Is anybody still relying on solely on NPlEX.
01:27:46.260 - 01:27:56.180, Speaker B: For Teco? Our implementation of Yamox is not really production ready yet. So yeah, we still rely on MPEx.
01:27:57.820 - 01:28:00.170, Speaker A: And it looks like Loadstar does as well.
01:28:01.260 - 01:28:08.010, Speaker B: Yes, we're still working on our Yamux implementation right now, just dealing with some performance issues with it.
01:28:10.300 - 01:28:31.010, Speaker A: Given the deprecation of implement spec, can we start talking about a timeline to deprecate it in our spec? Four, six months end of year? Is there any intuition on when we can do this?
01:28:36.710 - 01:28:47.398, Speaker B: I wouldn't be able to give you a confident answer today without talking to Cayman. I will have to get back to.
01:28:47.404 - 01:28:56.890, Speaker A: You on this dapline. The only reason would be if there are security concerns on why to rush it, which I do not have good visibility.
01:29:05.300 - 01:29:24.488, Speaker C: Oh yeah, Zhao has been looking at it, I think a little bit in more detail. Okay, maybe not.
01:29:24.574 - 01:29:53.760, Speaker A: Yeah, Zhao and age. If there are particular security or optimization or other reasons to do this on fast timeline, can we surface that outside of the call? And if not, obviously we have a lot of different engineering items going on. Maybe both Teku and Lodestar. If there's not a reason to rush, they can take a look at their potential timelines.
01:29:55.540 - 01:29:56.850, Speaker C: Yeah, we've had a few.
01:29:58.340 - 01:30:14.410, Speaker B: Sorry, but I would just ask for us to move on with the spec, because on the spec and Plex is still required. If you could move into optional would be a great update already.
01:30:17.820 - 01:31:16.150, Speaker A: Yeah, we certainly could, but we would just know that that would fragment communication in the network. As of now, given Teku and loadstars, maybe it's worth opening up a PR that makes it optional to galvanize the conversation in one place. I have a good question. On network shards, obviously we use discovery to find validators, in particular aptitude subnets already live. Is this compatible with that? Or if you implemented network shards, you now would have an issue finding peers that happen to implement it. Meaning, is it backwards compatible?
01:31:17.290 - 01:31:49.070, Speaker C: Yeah, it's backwards compatible. Essentially. I don't plan on changing any of the discovery things, at least until all the clients have kind of upgraded. You wouldn't get any benefit if you tried to do the discovery kind of optimization at the moment. Yeah, we're still using the same discovery we've always used for forever, even though we currently have the current iteration of the node ID to add a station subnet that's in the spec at the moment, but it is backwards compatible.
01:31:51.350 - 01:31:54.580, Speaker A: But once everyone does upgrade, you would make some changes.
01:31:55.750 - 01:32:08.920, Speaker C: Yeah, once everybody's upgraded, we'd probably have two. One, initially I would try and use the fast optimization where you search for the prefix and then fall back to just the standard one where we just search for everything.
01:32:12.410 - 01:32:16.970, Speaker A: That's not a spec change, that's an engineering change. Once you have confidence that the network is upgraded.
01:32:17.390 - 01:32:18.380, Speaker C: Yeah, exactly.
01:32:18.830 - 01:33:46.910, Speaker A: Got it. Yeah. I mean, given that we want to use something similar with data availability, this layer of abstraction does make a lot of sense to me, essentially making it a tool we can reuse, analyze, and understand. Any other questions on any of these? The I don't want has been close for a long time. I think that at least a couple of teams implementing this could get this over the edge, and especially when thinking about network load in relation to blobs and other things like that, I don't want to make at least a moderate impact. Seems very worthwhile, especially for the relative simplicity that is. Any other questions, comments on any of these networking proposals? I was not confident we're going to make it through the agency.
01:33:46.910 - 01:34:14.640, Speaker A: Any other discussion points, closing remarks, and with a couple of minutes to stare. Thank you everyone. Alex Stokes will be running this call for the next few months. Please don't give him too much trouble. Take care.
01:34:16.210 - 01:34:18.502, Speaker B: Thanks guys. Thanks so much.
01:34:18.516 - 01:34:23.350, Speaker A: Thanks, Danny. Thanks. Bye. Thank you guys.
01:34:23.500 - 01:34:24.450, Speaker B: Thanks. Bye.
