00:00:16.570 - 00:00:48.954, Speaker A: The next panel, and the panelists may come up now. Vitalik, Vlad Zamfir, Gavin Wood, Martin Beasley, is that the others have been introduced already as an Ethereum researcher and also the core Javascript implementer. And finally, Dominic Williams, the founder, co founder of string and Definity. So without further ado, 1115 and twelve ton and I'll please take it away. Gavin, you'll be moderating then?
00:00:48.992 - 00:00:49.482, Speaker B: That's right.
00:00:49.536 - 00:00:50.800, Speaker A: Okay, thank you.
00:00:52.370 - 00:00:53.280, Speaker C: Thank you.
00:00:54.050 - 00:01:10.994, Speaker D: Yeah, scalability, the burning issue really at the moment, one that blockchains are often criticized for, along with privacy. So I'll start by getting the guys to introduce themselves.
00:01:11.192 - 00:02:20.890, Speaker C: Vitalik yeah, so, founder and chief scientist of the Ethereum project. But regarding scalability specifically, I've been working on trying to figure out how to scale Ethereum for well over a year alongside people like Gav, Vlad, Dominique and a few others. And I think we've generally sort of converged on a fairly small set of strategies that we roughly agree are going to essentially take us from here to there. That I won't get into too deeply because that's kind of the purpose of this hour and several future presentations. But I've been involved in trying to figure out how to move from sort of very simple, every node processes, every transactions, type of proof of work, blockchains. The first kind of step forward was litecoin protocols that have been in Ethereum pretty much ever since the beginning. Since then, Vlad and I were talking about things like hub chains, hub and spoke chains, twelve dimensional hypercubes.
00:02:20.890 - 00:03:15.740, Speaker C: I wrote a sort of very large paper on the topic about six months ago that I'm probably going to do a bunch of major revisions to. So that's in general sort of figuring out how to especially really make blockchains. And I'm using Ethereum because if you can make Ethereum scale, then you can basically make any other protocol scale by writing it in Ethereum code. So figuring out how to go towards something that can actually process the extremely large number of transactions that's going to be needed, especially if we're really serious about this whole sort of moving blockchains beyond currency and talking about slock and Iot and a node for every device type of vision and seeing if we can make it happen.
00:03:22.750 - 00:03:50.280, Speaker E: Hi, I'm Martin. I implemented Javascript client and I enjoyed hanging out and talking about scalability. It's been really fun to see the ideas progress from like hypercubes and tree chains to where we are now. I think we're headed pretty rapidly, as Vitalik said the ideas in the space are converging and it's really exciting to see.
00:03:52.490 - 00:03:54.520, Speaker D: Dominic, does it work?
00:03:55.130 - 00:04:45.430, Speaker B: Does it work? Maybe. Hi, I'm Dominic. I'm an entrepreneur and engineer, currently co founder of fintech startup called String, and founder of the definity Scalable Blockchain project, which is focused on advancing sort of blockchain science with a particular focus on distributed virtual machines, the kind ethereum is. Before that, I created a game, a computer game, back in the UK, and then moved to the US about three years ago. The definity project itself came out of a now defunct cryptocurrency project called pebble, which was looking at price stable, scalable cryptocurrencies back then with a focus on micro payments.
00:04:49.950 - 00:05:30.194, Speaker F: Oh, there we go. I've been working on proof stake, obviously for like a year, and I've also been working on scaling a little bit. Before I started working on proof of stake, I was working on proof of work only scaling, and I still remember my first idea. It's called proof of proof of work, where you would have like a concise proof. There was a lot of proof of work somewhere else. And since then we've come a long way. We had a period of time where I really thought that the correct way to scale would be to have a lot of blockchains and find a way to that they can reinforce each other's security by interoperating after the fact called like the multichain approach.
00:05:30.194 - 00:05:50.970, Speaker F: And ever since then, we've been moving more and more towards these kind of scaling solutions that provide a single development environment, rather than having developers deal with different chains explicitly. And I think that's one of the key design principles that we have in our current scaling approach.
00:05:51.950 - 00:06:08.500, Speaker D: Cool. So Vlad, one of the things you've been working on recently is indeed Casper. So the move from proof of work to proof of sake, what do you see as being important in terms of scalability with the work that you've been doing on this?
00:06:09.430 - 00:07:33.230, Speaker F: Sure. So Casper being a single blockchain doesn't directly solve the scaling problem, but there are a couple of things that are in Casper that are necessary for scaling solutions, one of which is the production of evidence and the idea that we can go and punish particular nodes for producing invalid blocks. Because in a scaling solution where not everyone processes every block, there could be a small shard or group of validators who produces an invalid block and we need to go and reprimand them somehow from producing this valid block. And we have this in Casper and it's a necessary part of any scaling solution, because one of the key problems to any scaling solution is how do we deal with invalid transitions. Another kind of important thing for blockchain scaling more generally is that it's really important that our scaling solution is kind of economically efficient and that it shouldn't be that we require even more electricity consumption than bitcoin takes today. To scale bitcoin up to more transactions, a second and proof of state kind of represents an economic improvement in terms of the cost of processing transactions.
00:07:34.450 - 00:07:39.390, Speaker D: Cool. Dominic, you've been working on pebble recently.
00:07:40.850 - 00:08:54.550, Speaker B: Pebbles kind of defined as. Yeah, definity addresses the scalability challenge. And interesting, Vlad just mentioned how he started with lots of separate chains and having developers interact with lots of chains. That's kind of how pebble started out. We had consensus groups, and when a transaction would send, for example, currency from an account managed by one consensus group to another consensus group, it happened late, it was propagated lazily, and the recipient, the receiving consensus group, would try and validate the other consensus group. And I think just sort of necessary process to go to. But you start off trying to address the problem like that, and you realize that the whole system becomes very difficult to reason about, because unfortunately, if one of these chains or one of these consensus groups becomes corrupted, potentially it can export bad state transitions, bad currency, bad data, or whatever it is to the other groups.
00:08:54.550 - 00:09:47.954, Speaker B: And so you realize that that slightly naive approach to scaling doesn't work. And that was one of the reasons that pebble was never released, because I wasn't satisfied with that. So that's one of the fundamental challenges and the approach that we take is to. I was glad to hear compositionality mentioned earlier in the previous talk to, first of all, separate the different kinds of processing that needs to be done in a kind of distributed crypto ledger. And so when we look at a crypto ledger, we see three different requirements. We see global consensus, global agreements on what the state of the state exactly is. We see validation, which is the validation of transitions, which is distinct, by the way, to consensus.
00:09:47.954 - 00:09:58.602, Speaker B: Right. Because you can agree on invalid state, you can have everybody in the network agree this is the state. But that state could have been updated in an invalid way.
00:09:58.656 - 00:09:58.874, Speaker D: Right?
00:09:58.912 - 00:10:49.930, Speaker B: So you have a validation layer, and then finally you have a storage layer, which stores the state. So the storage layer will obviously need to be sharded in the same way that you'd shard a large website, for example. And that's more of a static thing, because you can't move large amounts of storage around. So the problem becomes if your bottom layer is sharded, your storage layer is sharded, how do you stop one of these shards becoming corrupted in some way, and then exporting bad data to the other shards? So you actually have to have a global validation layer above the storage layer, and scaling that validation layer is one of the key challenges. But there are ways of doing it, and then once you've validated data, you can reach agreements on it in the consensus layer.
00:10:51.410 - 00:11:07.474, Speaker D: Definitely food for thought later. Martin, you've been working on the Ethereum Javascript client mostly. Would you like to give some thoughts about how that might be made into some scalable system?
00:11:07.672 - 00:12:27.066, Speaker E: Yeah, I'm not really sure if the JavaScript client directly has much to add to scalability, but I do want to say to give some intuition about what we're doing with scalability, or want to do, is that right now we have a universe, and every actor in the universe, every interaction in this universe takes the same amount of time. So you can think of it, everybody is the same distance away. And this works fine when we have a little universe, but if we want to have a really big universe, we need to overlay some sort of spatial, it doesn't have to be real space, but we have to add distances, if you will. It's sort of a higher level way of just thinking about what we want to do with scalability. So what does that mean right now? When one contract calls another contract, it's always a synchronous operation. And if we have different times, maybe that contracts could take, because if one contract calls something from further away, it should take longer. So thinking about it from this way, sort of interesting.
00:12:27.066 - 00:13:10.140, Speaker E: It means necessarily there will be an asynchronous environment in which contracts might call. Of course these aren't fully fleshed out ideas, but sort of thinking about the properties that scalability we have and then mapping it onto our current system is interesting, I think. One other thing, like Dominic said, separating different layers, having one layer that does state transitions, and then another global layer that comes to consensus on what all the state transitions is, seems like pretty good path forward. So yeah, cool.
00:13:11.470 - 00:13:33.886, Speaker D: Vitalik. Now at the moment we have a system that has a number of properties, Martin mentioned, one with synchronous calls between contracts going forward into a viable scalable solution. Which of these properties do you expect to see being relaxed?
00:13:34.078 - 00:14:30.980, Speaker C: Right? So I think there is a few properties that blockchains right now provide, whether it was an intentional design goal or accidentally. So one is this concept of synchronous completely global state transitions. So an ethereum contract can theoretically call any other ethereum contract. The specific ethereum contracts that it calls can be decided at runtime. Those contracts might call other contracts. Essentially, you theoretically have pretty much no limits on what the resulting state is after one transaction got applied, except for the implied kind of restriction that comes from the gas limit. So another key point is that absolutely every transaction, absolutely every account, has the same level of security, which is that in order to reverse the transaction, or in order to censor some particular application, you needs to perform a 51% attack on the entire system.
00:14:30.980 - 00:15:50.558, Speaker C: There is the question of exactly what kind of properties the blockchain provides. There is the block time. So blocks come every 17 seconds approximately on frontier. So out of those, the question of which ones you want to sacrifice, I think really depends on your application. So, for example, if you're talking about a currency application, and we have some case where Ethereum, let's say, has mass adoption, and I have some Ethereum based tokens, such as, let's say Digix Global has their gold coin on Ethereum out by then, and I wants to pay for a coffee using $2 of gold, then that transaction does not need to necessarily have an entire $20 million of security backing it. So one route that you could take is you could try to relax security and basically say you pay for security and your transactions can be as cheap as they want, but then their security is also going to be lower. Now, of course, one other category of application where reducing security is okay, or rather the other property that blockchains provide, is the existence of this concept of global state that everyone completely agrees on.
00:15:50.558 - 00:16:56.046, Speaker C: So one important category of applications is proof of existence. So basically, this idea of using the blockchain as a timestamping tool, and basically you publish a piece of data, a bunch of people's pieces of data, get aggregated into a Merkel tree. The root of that Merkel tree goes into a block header, and you have this kind of proof that's kind of permanently etched into existence. And you can use this Merkel branch as a receipt to sort of prove to anyone else forever that this particular object exists at this particular time. That's something that's actually much easier to scale than either currency systems or like folds or incomplete smart contract platforms. And so the way you do that is basically by sacrificing the idea that even every node needs to receive, needs to receive or be able to receive all the data sacrifice, the idea that you need to worry about double spends, or that two people creating the same record at the same time is a problem. Another category of things that's worth thinking of compromising on is synchrony.
00:16:56.046 - 00:18:11.394, Speaker C: So in general, with ethereum right now, any contract can theoretically make a huge set of changes. And therefore if you get two transactions right now, it's virtually impossible to process them in parallel in the general case, because theoretically they might end up affecting the same account, and the way that they affect that same account might be noncommunutative. And you can't necessarily sort of detect those cases beforehand because of various corollaries to the halting problem. So one approach that you could take is you could say that contracts in some cases, so let's say if you split up the space into what we call shards, then contracts that call contracts that are sort of physically close to them by some metrics, they can do that synchronously. But if you want to call some call a contract that's say very far away in this metric, then basically the concept of the event of making a function call and the event of processing that function call in a separate shard might, needs to be sort of separated from each other in time. And that's something I'm actually going to be talking about quite a bit more this afternoon. So I think depending on which application, which one of those you want to compromise on, it might be different.
00:18:11.394 - 00:18:48.040, Speaker C: The challenge with Ethereum, of course, is that we are trying to create this sort of decentralization for dummies kind of platform, and we want to sort of protect developers as much as possible from having to even think about these issues. So I think, and a really important question for us all to be trying to figure out is what are the nicest, the simplest, and sort of the developer friendliest possible abstractions that we can give developers in order to let them sort of choose what trade offs they want among the different choices to the extent that they have to, and at the same time still make development as easy and intuitive as possible.
00:18:48.890 - 00:19:18.446, Speaker D: Thanks, Martin. JavaScript is nice as a language in that it gives you an awful lot of latitude with what paradigms you can use. Can you imagine any alterations to contract programming, perhaps even the virtual machine that would facilitate paradigms allowing you to program on a system that Vitalik's talking about?
00:19:18.628 - 00:20:17.940, Speaker E: Yeah, definitely. Asynchronicity is very important in JavaScript. It was designed to run in a single event loop, and you don't want to stop that event loop. So if you don't program asynchronously in JavaScript, you get really poor performance. I don't think that this necessarily holds just for JavaScript, but the asynchronous design patterns I think are very relevant here. So I would imagine if we had, from a developer's point of view, if you're a contract developer and we had an asynchronous development, when you're calling to some part of the state that's really, really far away and it's going to take a while, it would be sort of like getting a promise back in JavaScript or a callback. It could work the same way.
00:20:19.830 - 00:21:13.380, Speaker D: Cool. When we tend to think about scalable solutions normally in some sense at least, they revolve around not all nodes processing all transactions or storing all data, or even being aware of all data. This brings into the limelight a problem known as data availability, or proof of availability, whereby nodes that are not privy to the transactions that some other nodes have validated must ultimately work with those transactions. Could you give some proposals lab on how to address data availability in general?
00:21:14.150 - 00:22:16.920, Speaker E: Yeah, I think this is still an open question, kind of. But one example, Vlad has a scheme for proof of existence, right? So we have this data, it's all mercalized. So one simple scheme, right? To give you maybe some intuition and how we can build more complicated and maybe better schemes. But we have all this data merklized, so we can query the nodes that are supposed to be storing the state and ask them for random keys and make sure that we get back a correct proof merkel proof. And if we don't, we could punish them in maybe a slasher style thing where we take their deposit, whatnot. So I think that's sort of like the core of how most data availability schemes work.
00:22:17.470 - 00:23:32.586, Speaker F: Okay, sorry, 1 second. It'll turn on in a second. Okay. Another thing that's important for data availability checks is the idea of escalation, where if some nodes claim the data is available and produce proofs, and other nodes claim that oh, I actually can't download this data, that's why I can't produce proofs. So the nodes who can't download the data would basically sound the alarm, causing more nodes to attempt to download the data. And then if there's still disagreement about whether the data is available, we can escalate further until we get to basically all or a very large portion of the nodes trying to download this block, which may or may not actually have been published beyond some attackers nodes. And then we would have to resolve in the end with what we call subjectivity, where basically nodes go with whether or not they personally were able to access that block, as opposed to with the results of voting, because voting alone can't really resolve this question in an environment where an adversary might control very large proportion of the notes, who's voting?
00:23:32.618 - 00:23:34.106, Speaker E: Are the validators voting?
00:23:34.218 - 00:23:41.490, Speaker F: Yeah, the validators are basically saying whether or not they're able to download this block, which some validators are claiming is not available.
00:23:41.640 - 00:23:59.420, Speaker E: Right. So the important part though is the people voting have stake. So it's not just anyone, but also sort of a fun attribute to this protocol is that if you see that some dad bad data.
00:24:01.150 - 00:24:01.914, Speaker F: If you see.
00:24:01.952 - 00:24:12.830, Speaker E: That someone's making a bad claim and you counter it, you would get some reward. So this all has incentives to work out, hopefully.
00:24:15.010 - 00:24:43.334, Speaker D: So, one particularly useful tool in pursuing scalability seems to be cryptographically pseudo random sampling, in order to select some nodes ahead of time to determine what precise role they will play in the consensus algorithm. Dominic, would you like to comment on that?
00:24:43.532 - 00:25:49.158, Speaker B: Yeah, sure. So I think we're going to see that randomness is really the tool that solves all of these big scalability challenges. So, at least in the systems I've been playing with, you have a top level chain that not only records consensus, but also generates a kind of random heartbeat. And that random heartbeat can be used to drive the organization of the wider network. It turns out that's very powerful. So just circling back to this validation problem, there was construct came up with a year ago called validation taps, which is very similar to the system these guys have been working with and works something like this. Let's say you've got a state storage shard, and it has a transaction come in that transitions, that state, it needs to be validated, and that transition will be mapped to what's known as a validation tower.
00:25:49.158 - 00:27:03.774, Speaker B: And a validation tower is bit like a kind of chain that grows each successive level on it is built by a randomly selected set of nodes. And this can be a very small set of nodes. For example, five processes in the network, and those five processes will be passed the transaction and any other transactions mapped to that tower. They have to build a level that validates that those transitions are okay. But in addition to that, they also have to validate some number of levels below them. So for example, let's say they have to validate nine levels beneath them, okay? And let's say that data is passed through the validation tower when it's been validated by ten different levels. Now the trick is that we say those nodes that have been selected to build a new level in the validation tower become inactive until the level that they've created has been buried this number deep, so they become economically inactive.
00:27:03.774 - 00:28:07.826, Speaker B: So if an adversary controlled that group and created a bad level that validated data that nobody else would validate, he could become stuck. Now, of course, the adversary would like to be able to have his nodes selected to build, let's say, the next ten levels, right? But of course, the selection of the nodes that build each level is determined by the random heartbeat. And this tower grows in lockstep with the random heartbeat. And if a level is ever missed, then the validation required of all remaining levels that aren't validated resets to the full depth. So what does this give us? So, for example, in the bitcoin blockchain, we have, I think, approximately 10,000 nodes, and every single node is supposed to. They don't actually, but they're supposed to validate every single transaction that makes it into the blockchain. That's a very sort of naive and inefficient approach.
00:28:07.826 - 00:29:48.146, Speaker B: But we might imagine with something like validation towers that we could get the same level of security with 50 nodes, right? Let's say that we have five nodes that build each successive level, and you need to get a transaction buried ten levels deep, right? And the reason for this is that the probability of the adversary having five of their own nodes selected from some big universe of nodes, in this case, we're saying ten times in a row is vanishingly small, right? And every single time, the adversary hopes this is going to happen and builds a bad level, which perhaps validates some silly transaction that pays him a trillion dollars or something every time he tries this. Of course, he's having to spend money because his nodes become inactive until they're buried ten levels deep, and it's never going to happen. So the resources of the adversary will quickly become exhausted, and we can get the same kind of security from just these 50 nodes that bitcoin can get from, say, 10,000 nodes. I think that as a technique, it's an interesting one, but it kind of illustrates how a lot of these problems that seem very intractable can be solved using randomness. So I think that the generation of pure randomness that an adversary can't manipulate or predict is going to be very important. And luckily, there are ways of doing this using adaptations of certain kinds of threshold signature techniques. And in fact, actually, anyways, proof of work is a kind of randomness.
00:29:48.146 - 00:30:16.130, Speaker B: It's not ideal because it's too easy manipulated, but when you create that, you find that nonce that gives you the leading zeros in your proof of work, the resulting hash is pretty random. So in principle, you could even do this with a proof of work, a top level proof of work blockchain. It's not, not ideal, but in principle you could even use that the randomness generated by just a proof of work blockchain to drive validation towers.
00:30:16.710 - 00:30:59.306, Speaker D: Cool. Great. Way back when, probably about 15 months ago, we were talking about a notion of Ethereum one 1.5, which would sort of go halfway towards scalability, if you remember, to some degree, it was built around the notion of having separate chains that linked back to the main chain, from which proofs could be given that transactions happened or that state was in a particular state. Would you like to comment on and explore the notion of this Ethereum 1.5 idea still? And no, not really. You're done with that now, right? Vitalik?
00:30:59.338 - 00:32:10.406, Speaker C: I mean, the basic notion, as I evolved, there's a bunch of things that work on Ethereum 1.5, but one of them is this hub and spoke chain notion, where basically you'd have a hub chain and you'd have many spoke chains that theoretically could be sort of rely on or benefit from the hub chain's security in some sense. So the particular thing that I come up with was a sort of symmetrical merge mining technique, where basically every miner would have. This was back in the proof of, at the time we were focusing on proof of work. So every miner would choose a hub, a spoke index to make a block on, and then they would make a block, and then that block would point to both the previous block on that spoke and the previous block in general. And that block would count as a block on the spoke and as a block on the hub at the same time. And so the theory was that you could have a system where transactions that sort of happened within a particular spoke could potentially be extremely cheap, because there aren't too many miners validating actually processing them.
00:32:10.406 - 00:32:28.490, Speaker C: And theory is that every miner would be kind of a full client on their kind of home spoke, but they'd be a light client on every other spoke, so they'd still have some degree of access to it. But at the same time, if you wanted to have interoperability between spokes, you could. But that would go through the hub.
00:32:29.790 - 00:32:41.582, Speaker D: Could you see that model scaling indefinitely through compositionality, a sort of hub and spoke? A hub hub is a hub as well as a spoke, and has further spokes, maybe.
00:32:41.636 - 00:33:49.778, Speaker C: But I think we also needs to kind of think more rigorously about exactly how it is all these systems are secured. Because the problem with scalability is that there are so many ways to do things that look like you're solving scalability, but where you're actually sacrificing a fundamental security property. So one example is right now, the sort of de facto model of cryptocurrency scalability is that, well, we have hundreds of altcoins, and if bitcoin hits its limit, and if the politics don't get figured out and the limit doesn't get raised, then people can just sort of move over and do stuff on litecoin and dogecoin instead. The problem with actually sort of institutionalizing that as the scalability paradigm is of course if you have a thousand chains, then each chain is a thousand times weaker. And particularly if you want to maintain the property of having a currency that exists on multiple shards, then one of these shards could sort of get overwhelmed. And then, as Dominic said, it could start sort of exporting bad data to the other shards. So that's one bad way of solving scalability.
00:33:49.778 - 00:34:31.710, Speaker C: Another bad way of solving scalability is basically merge mining. So merge mining is definitely a decent thing that has a lot of uses that was used, and still is used to secure namecoin. But the problem with using it for scalability is that basically let's imagine that we have a world where people wants to have basically a gigabyte worth of transactions every block. One approach is, well, let's just bump up the block size limit to a gigabyte. Done. Every full node has to process a gigabyte every ten minutes. Another approach is to say, well okay, we're going to merge mine 1000 side chains, and each of the side chains is going to have a block limit of a megabyte.
00:34:31.710 - 00:35:21.026, Speaker C: But then the problem is that for a side chain to be secure, it has to be merge mined basically by almost all of the miners. And so almost all of the miners still basically have to process a gigabyte every block. So that is one of the solutions that sort of doesn't actually solve the problem. So the challenge is now, of course, side chains definitely are, they do have value, but even by blockchain themselves, I think they've always been marketed from the start as a functionality solution and not a scalability solution. So for scalability, we have to figure out exactly how the validation is going to work. I think the solution definitely is going to involve some kind of global validation involving random selections. But ultimately, actually I do think this sort of compositional recursive approach is the way to go.
00:35:21.026 - 00:35:32.440, Speaker C: And also, if you're doing global compositional recursion, then the number of spokes per hub just might as well be the computer scientist favorite number of two.
00:35:34.250 - 00:35:34.758, Speaker D: Cool.
00:35:34.844 - 00:35:35.480, Speaker C: Yes.
00:35:36.970 - 00:37:10.920, Speaker F: So kind of tying some of this discussion back to my presentation about making these systems work in a context where majority of nodes might be rational and colluding. It's really important to have randomness that has one of these security models where any one, or any small number, preferably one node who is including with everyone else, could cause the entropy to be is a. There is like Randao project that is working on one of these systems where everyone who is part of the random number generator has to place a security deposit and reveal a commitment. And if they don't reveal, then they lose their deposit. But any one of them could stop everyone else from secretly sharing their commitments in order to produce randomness. Additionally for the validation thing, also, if a large percentage of the nodes are colluding, then this kind of escalation and subjectivity become really important parts of the solution. One thing, though, that I'm hoping that we'd talk about a little bit more is the idea of, to what extent can we provide a general purpose platform that we can reason about how the shards communicate to each other by requiring developers to kind of give us more information about what their contracts will do when they deploy them.
00:37:12.010 - 00:37:15.320, Speaker D: Sure. Martin, you'd like to comment on that?
00:37:18.220 - 00:37:32.144, Speaker E: Yeah, I guess there's several methods of doing that, and I think you're probably thinking of more of a formal proof, like seeing what contracts it's going to call out to and doing type matching. Is that what you're thinking?
00:37:32.182 - 00:37:32.768, Speaker D: Right.
00:37:32.934 - 00:38:07.690, Speaker E: Yeah, I'm not convinced yet. That's the way to go, because if you have a general purpose turning complete virtual machines, and at some point you don't know what calls it can make, and it should just be built into the protocol to be able to handle that, even though that there will be some asynchronicity and it will take longer, but those will have to be things you can program in at a lower level.
00:38:08.140 - 00:38:30.528, Speaker F: Sure, but I mean, if the developer is willing to tell us, okay, this is a key value store that acts in this way, then we will be able to better manage the different shards of that particular application, because we'll have information about the fact that transactions are commutative, for example, if they don't affect the same key.
00:38:30.614 - 00:39:05.752, Speaker E: Right. So, yeah, sure. We can imagine your situation where we have lots of transactions that are totally right for Pacific contracts. Yeah, that's a really interesting concept. That'd be cool. But don't you think you probably should start from the general view and then add the optimization later? Or do you think that you can start from the view where we have this metadata, if you will, on the contracts?
00:39:05.896 - 00:39:51.500, Speaker F: Well, I kind of think that there's some extent to which it's like a fundamental problem that you can have a contract call every other contract, and that if another contract relies on the data they get from this one contract, and then if you're using asynchronous calls and that call was created from an invalid block, and then that block later gets reverted, the question of whether or not that invalid data is used by these other contracts may make application developers, developers suffer more than if they just. Because then they have to deal with locking as opposed to, and like monitoring other shards, as opposed to just kind of having more restrictions on what you can program.
00:39:51.650 - 00:40:19.650, Speaker E: Right. So there's two separate issues there, though. One, there is containment. You're saying assuming a valid state transition gets through on another state, some state. Right. And then how do we revert that? And then Vitalik has, what is it called? The cone of the cone. You have to revert if a bad.
00:40:19.650 - 00:40:21.376, Speaker E: Yeah, the dependency cone.
00:40:21.408 - 00:40:30.810, Speaker C: Well, I used to have the dependency cone concept. I think over time I've come to realize that the dependency cone is going to hit 100% of the state so quickly, you might as well not even think about it.
00:40:31.340 - 00:40:45.148, Speaker E: Yeah. So if you had a bad state transition and you had to revert anyways, that would be. So I don't know if we need to consider the first part of that.
00:40:45.234 - 00:41:01.360, Speaker C: Yeah, I think this depends also on sort of what your underlying security model is, because if all of these shards are ultimately sort of under one header chain, that you still have a global sequencing model. So you can still know that if one thing gets reverted, then everything after will get reverted.
00:41:04.360 - 00:41:28.024, Speaker D: So to what extent do you think we will be able to keep the notion of absolute truth, which is to say a global lockstep state, fully objective going into the future? And to what degree do you think we may have to fall back on sort of this notion of a relative truth, depending on where precisely in the.
00:41:28.062 - 00:41:39.870, Speaker C: System you happen to be by global absolute state, are sort of morkle receipts of asynchronous operations in progress allowed to be part of the state?
00:41:40.560 - 00:41:43.330, Speaker D: In principle, yes.
00:41:43.700 - 00:41:46.960, Speaker C: If so, I think that's fine. We can still have global truth.
00:41:48.820 - 00:42:11.240, Speaker E: Aren't we already sort of like at a situation where we don't have immediate global truth? The global truth becomes more final after time goes by. So we start off with possibly vague global truth, then we get some more blocks, and then we have a more solid version of global truth.
00:42:16.140 - 00:42:38.928, Speaker F: I kind of think that we should definitely explore the possibility that using stronger type information, we can have concurrent execution of transactions where we don't actually have a global consensus on the order of transactions because we see that we don't need it because of something about the particular applications that are deployed on the platform.
00:42:39.094 - 00:42:51.430, Speaker D: Okay, let's explore that. So would you see the inherent linearity of the blockchain as we have it so far, to be something that can be discarded going forward?
00:42:52.120 - 00:43:30.050, Speaker F: I think that if your application is so general purpose that it can affect any part of itself, then you can't really get away from a linear model. But if you have more information about what the application does, for some applications we can definitely get away from the linear model, from some applications we can't. And so we stand a benefit on a scaling side if we can identify how strictly transactions need to be ordered to an application from information given by that application.
00:43:31.220 - 00:44:02.764, Speaker D: So in principle, sharding the state and allowing different sets of nodes to process each different part of the state in parallel would relax the notion of this inherent sequentialness linearity. To what extent do you think that can be taken in this sort of fractal line notion that Lucius presented mean?
00:44:02.802 - 00:45:09.600, Speaker F: So if you have one application across many shards, and that application requires a strict ordering, then it will need to kind of lock and wait for messages across different shards anyways. So just the fact that we have shards doesn't mean that for any particular application we can deal with concurrency because certain different applications can inherently allow for more or less ordering of transactions that affect their state. And yeah, I think multi shard applications are something that we're going to need to think kind of long and hard about, because as far as an application is all within one shard, the story is pretty clear between shards. The application developer might have to do more work to deal with the logic there if the transactions really aren't globally ordered and affect the so basically the shard will have to be big enough to include all of that application state if that application needs a global order, and if we process transactions concurrently between shards.
00:45:11.460 - 00:45:13.360, Speaker D: Dominic, any comments?
00:45:14.440 - 00:46:46.000, Speaker B: Yes, I think even with. Is that working? Yeah, even with sharded architecture, there still has to be a single linear record of overall state transitions. The network scalable network is probably best thought of some kind of pyramid, you know, and at the top of the pyramid is the global chain, and at the bottom of the pyramid are the storage shards. The transactions themselves will have to be asynchronous, which means that effectively, some kind of event passing paradigm, very simple example might be, with just a simple cryptocurrency, that if you've got two wallets, one is on shard a, one is on shard B. The wallet on shard a wants to send some money to the wallet on shard B, a transaction is created that really consists of two parts, a debit and a credit. And first of all, the debit part of the transaction is submitted on shard a that's validated, and only when it's validated is the credit part of the transaction actually submitted on shard B. But even though there's a process by which a smart contract might send some message to another smart contract, well, the creation of that message has to be validated.
00:46:46.000 - 00:47:05.910, Speaker B: While it's being validated. That message is in some kind of queue. The queue itself is part of the state, right? So that can be validated too. So there is some ordering not only of these state transitions, but also the message passing, which is a form of state transition. Anyway.
00:47:08.680 - 00:47:23.880, Speaker D: So I'd like a quick poll. Do we think that for ethereum 2.0 scalable ethereum, we will be able to retain our exact same execution environment?
00:47:26.060 - 00:49:00.844, Speaker C: The property that I'd like to achieve is that, first of all, I do think that simultaneous synchrony and very high scalability is not achievable for fairly simple computer science theoretic reasons. But the properties that I do want to keep are, number one, if you do not want properties that do not exist right now, then you should not lose anything that exists right now. So one example of that is that applications that are within the same shard should maintain the exact same development model as much as possible. Another one is ideally, every time there is a fundamental trade off. Basically, I think good design should, instead of making the trade off for everyone, sort of presents the trade off. So one possibility, for example, would be to come up with a mechanism that offers a choice between, let's say, offers a trade off between transaction fees, for example, and the maximum size of the sort of radius within which you can do synchronous operations. So I think it's not so much 100% of a and 100% of b are probably not possible, but the choice between 100% of a and 20% of b and 20% of a and 100% of a and a whole bunch of stuff in between, I think completely viable, Watson.
00:49:00.992 - 00:49:38.560, Speaker E: Yeah, so property, still a little jet liked properties. I pretty much agree with v right there. We want to keep the same environment programmers have contract developers, but we need to add this property or notion of talking somewhere far away. And how do we inject that? Like it can't be totally synchronous or we have to sacrifice something. So I like the property of it being asynchronous.
00:49:41.640 - 00:50:13.340, Speaker B: Pretty much my own view on it is I think the sort of transition to Ethereum two will take longer than people expect, just because the protocols become much more complex when you start dealing with scale out systems. Having said that, I think Ethereum surprised everyone, me included, with the speed of its implementation and the fact that it's been so stable in production. So yeah, way to be pleasantly surprised.
00:50:14.800 - 00:51:31.130, Speaker F: Yeah. So I think that we're still in a stage where the most that we can imagine that we can get out of a scaling solution is growing. Like we can imagine scaling solutions today that offer us more guarantees than the ones that we then like the kind of first order scaling solutions like the one in Vitalik's paper, which, by the way, I recommend everyone read. I think it's super compelling for Ethereum 2.0. I think we can have a lot of the same kind of semantics for the developers, and I think that there could potentially be relatively small changes that will only affect applications that span between shards. But I think that in the long run we should not be bound by backwards compatibility if it means sacrificing scalability and general purposeness of scaling solutions. For example, the goal of providing transitions at different levels of security is one that cannot be done without adding semantics to the development environment, because developers will need to say whether this needs to happen at a low level security or whether it needs to happen at high level security.
00:51:32.380 - 00:52:09.684, Speaker D: So it seems that we've identified three potential sort of points that may need to be sacrificed in some sense. On the one hand, there's synchronicity, then there's the fees that need to be paid, and then there's the speed at which the transaction actually gets fully executed within the state transition system. Do you think that we'll be able to work toward a model whereby the programmer is able to trade these three off against each other as they choose? Or do you think by and large it's going to have to be set up front?
00:52:09.882 - 00:53:48.160, Speaker C: That's definitely at least my goal. To what extent that's going to be possible, especially between asynchrony and fees, I'm fairly confident I haven't spent too much time thinking about the idea of kind of letting developers choose security levels, mainly because I think up until now, as I go, myself and all of us have been working in this model where sort of, it assumes that everything is going to work, and allowing even one fault is sort of considered unthinkable. And of course, the reason is that if you don't have that model, and once again, to the point where the network is supposed to survive, supposed to survive successful attacks, then applications all have to start worrying about, well, what if someone moves a trillion ether into this account, prints a trillion ether out of this account here, then you have contagion problems, and then different applications have to start dealing with this. So there are, I think, possibilities. One idea that I've had is this idea of having kind of sort of islands of decreasing security where things that are sort of in higher security levels can filter down, but you can't filter up, at least within one transaction or without some sort of particularly special set of rules. But that, I think, is going to take more work, particularly because outside of the decentralized crypto economic context, people aren't as used to the idea that while operations can just randomly fail, that's almost a biologist way of thinking about things to some degree.
00:53:49.780 - 00:53:50.720, Speaker D: Martin?
00:53:53.700 - 00:54:59.130, Speaker E: Yeah. One interesting thing with asynchronicity is you probably could also pretend to have synchronous calls by pushing all your work up front and downloading, forcing the validator to download, access, blot and Clayton download other substate that didn't have and then run it. You could, I suppose, make it look like it was running synchronous, but the whole transaction would just take longer. So that's one trade off. I hadn't really thought about the different levels of security and then taking longer for the state transition. Well, I don't think the programmers would have access to the state transition, so I'm not sure how that would work.
00:55:03.040 - 00:55:35.220, Speaker B: Yeah. So the configurable validation is a really important area. I think that with scalable blockchains, the cost per transaction becomes very low. The main reason you'd want to reduce the number of levels of validation is for reduced latency. Okay. There are some complexities. One of them is that if every transaction pays a fee, right, every transaction involves a currency transfer.
00:55:35.220 - 00:56:43.390, Speaker B: So you can't really easily reduce the validation on it because even though the transaction fee itself might be 1000th of a cent, it could be like, whoops, I'm the miner, and I've just credited my account by $1,000. So one way of addressing that is to have probabilistic fees. So you only pay your fee one every hundred times. So your fee is 100 times what it would be, but you only pay it one every hundred times. So then if you have reduced, then if the application developer specifies a lower level of validation 99 times out of 100, they're not going to pay a transaction fee and therefore they get that reduced level of validation and they get the low latency. But one time out of 100 they will pay the transaction fee and consequently actually that transaction will run at the full level of validation. But that's a kind of simple way of allowing the application to developer to, for example, get the reduced latency 99 times out of 100.
00:56:43.390 - 00:56:46.348, Speaker B: It's not perfect, but it seems to.
00:56:46.354 - 00:57:25.250, Speaker F: Work just kind of quickly. I think that we can isolate shards from each other and have low security transitions. Basically you can think of it as kind of like a two way peg where there's only so many tokens that are in that shard and the other shards know how, how many tokens are in there. And if something goes wrong, then the loss is bounded and we don't need to escalate to deal with it. We can just accept invalid transitions there by kind of isolating the rest of the system from the failure of that shard. But anyways, our time is up close.
