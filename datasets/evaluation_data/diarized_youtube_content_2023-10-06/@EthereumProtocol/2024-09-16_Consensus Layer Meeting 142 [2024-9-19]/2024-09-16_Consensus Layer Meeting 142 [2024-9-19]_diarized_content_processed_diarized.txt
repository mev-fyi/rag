00:01:16.735 - 00:01:44.945, Speaker A: Okay, we should be live. I assume the chat on YouTube will let us know if there are any issues. But yeah, let's go ahead and get started. So to kick us off, we have the agenda here. This is issue 1154 in the PM repo. This is consensus layer call 142. Thank you everyone for joining and yeah, there's quite a bit to get to today, so let's just go ahead and dive in.
00:01:44.945 - 00:02:09.955, Speaker A: So to get started, let's think about Petra and perhaps we can just quickly go through any updates for DevNet 3. DevNet 3 has launched, which is super cool. I think we had some bad block testing that we did, which caused a little turbulence and it'd be interesting to hear an update from the results of that. Anyone have anything they'd like to share?
00:02:11.175 - 00:02:20.621, Speaker B: Yeah, we had Marius, bad block generator and mainly the chaos seems to be limited to Blazer. Is someone from the Bezel team around?
00:02:20.693 - 00:02:21.945, Speaker C: Maybe give an update?
00:02:23.485 - 00:02:40.533, Speaker A: I can give an update, yeah. So the fuzzing yesterday created signatures that were 0x00. Bezo has a rule internally that a.
00:02:40.549 - 00:02:43.705, Speaker B: Signature has to be at least one.
00:02:44.165 - 00:02:51.247, Speaker A: So we reject any transactions with those signatures. I'm currently updating this, so I've opened.
00:02:51.271 - 00:02:59.071, Speaker C: A PR to fix the issue and to accept the signatures as once that's.
00:02:59.103 - 00:03:01.055, Speaker A: Fixed, I think we should be able to join again.
00:03:01.095 - 00:03:13.439, Speaker C: Definitely. Cool.
00:03:13.567 - 00:04:02.669, Speaker A: Any other Devnet 3 issues? Otherwise I was looking the other day, it looked pretty healthy, so that was exciting to see. Okay, cool. Then we will move to the next item on the agenda. And yes, this is discussing the split of PETRA into possibly two forks. We had discussed this on last week's ACD and kind of had asked at least for everyone to have a view on their preference for how to split this into two for reasons that were given there. And yeah, I guess we'll just go ahead and open the floor. Yeah, Etan put this in the chat.
00:04:02.669 - 00:05:10.345, Speaker A: Thank you. So I had a document here that was essentially one way to split up what we have now. My thinking was essentially like, yeah, I mean, I guess where to start is like Petra again is like one of the largest forks that we have scheduled, as it currently is ever in Ethereum's history. So for many reasons around reducing risk and just overall success of actually executing a hard fork, it makes a lot of sense to split it into two. I think also given just development progress of other features we've been discussing for pectra like peer DOS or EOF, there's a pretty natural split between what's on DevNet 3 today and the other things we've been discussing. So yeah, I guess maybe to get us started with the conversation here, does anyone disagree with what I laid out in the notes? Is there like a different proposal that anyone would like to discuss? Are there eips maybe that you would want to see in like this first picture that aren't in this note? Happy to hear what people think.
00:05:16.335 - 00:05:16.671, Speaker C: Yeah.
00:05:16.703 - 00:05:17.435, Speaker A: Andrew.
00:05:19.415 - 00:06:20.585, Speaker C: Right, so we have an arrogance perspective on the vector split. I'm basically I'm posting the write up. It was written by our team members, Somnath Some not I own the call, maybe not. And I can, if not I can briefly summarize our position. So we basically were against it and there are a few reasons. So on the face of it it makes sense. But essentially we worry that splitting Pector into two will lead to, to full to two fully blown forks and people will use it as an opportunity to include more aips into the second fork.
00:06:20.585 - 00:08:10.707, Speaker C: There might be interactions between various eips of the two forks and that will introduce a lot of interference and last minute decisions. And we also want to get our priorities straight. So if we think that Verkal is important, then we should prioritize Verkal rather than kind of add other random EIPs. And also if we try to do, if we basically, if we implement many EIPs, it might be successful in some sense, but it actually also detracts resources from our engineering resources on working on other improvements to the clients like tackling things like state growth or CO deficiency and so on. So trying to cram as much IFS into say two forks, it may be not exactly a success story. And basically we propose to if EOF in fact run. Given that actually there is a lot of momentum on uif, I'm thinking that like if Peer DAS is not ready, we can have SEO only fork or if Peer does require some trivial changes.
00:08:10.731 - 00:08:13.691, Speaker A: On the outside, that should be fine.
00:08:13.763 - 00:08:24.845, Speaker C: Because it won't hinder our progress on Docker much. But yeah, I would rather, I agree with Somnath and I would rather avoid Pector too.
00:08:27.465 - 00:09:22.005, Speaker A: Yeah, I mean I think that's the key risk here is like we would say, okay, pector's big, we'll split it into two and then we essentially don't have the discipline to keep the second part as is. And then yeah, it becomes a whole new separate fork. There's a delay, you know, with that just necessarily and that's not great. So I don't know the way I would mitigate that risk is we Just commit even today to saying hey, we will break it into two. But essentially there's not any room for new features. If there's some EIP that complements something in Pectra A or Picture B, then sure, those would make sense. But the point is not to just push everything to a second fork or push some things to a second fork and then open the can of worms of hard fork scheduling again and say six months.
00:09:22.005 - 00:09:46.455, Speaker A: Any other thoughts? Gaeta?
00:09:50.235 - 00:10:56.245, Speaker C: Personally I'm also in favor of the split just because like I'm looking at it from a peer do perspective and at this stage it seems like that it got descoped so much that it essentially like the subnet das but the DES part is also removed now, so it's like just splitting the data among notes and hoping that it sort of works out with availability without any sampling. So I'm not sure, I mean not fully removed. But yeah, it just appears to me that it's not like ready ready as all the other features that have already been implemented like the attestations the validator requests, Mac cb. I don't think that delaying all of that, which is beneficial for users today, is warranting to wait until both peer DOES and DOF are fully ready and fully battle tested.
00:11:00.225 - 00:11:33.725, Speaker A: Yeah, I mean I think that's the justification for the two forks and then just to surface several comments in the chat, even like onsgar's here, that is the last message. Essentially it's just like hey, if we like this will this will be most successful if we commit to a very strict scoping. And so that really means like yeah, there's no room for like it's not that we are picking Petra, then we're going to pick the next fork and then now we can decide what goes to the next fork. It's like kind of acknowledging we've already decided that we have two hard forks worth of EAPs and this is what we should.
00:11:37.595 - 00:11:38.019, Speaker C: Yeah.
00:11:38.067 - 00:12:40.215, Speaker B: So A, I, I don't agree with like being strict on the scope and B even if I did agree I don't even trust that we would be strict on the scope. We, we set to scope Petra very early, very early much a few months before we went to Kenya and we, we were very clear that we wanted to have a very small fork because it didn't need to delay vertical. That was a commitment, that was an agreement that we went all teams went at Async and proposed a list of changes. Prism didn't even pay for Max CB on the first change and then we had this Monster fork that now we're trying to split. If we were about to have this large fork, then we would have pushed for a very different set of EIPs. So I find it not only unfair, I find it just simply wrong to already be committed for the next two forks because this fork actually is for any practical matters, a two for a two fork.
00:12:47.635 - 00:12:48.615, Speaker A: Oh yes.
00:12:50.515 - 00:14:00.785, Speaker C: Yes. I think I kind of in line my thinking kind of line with photos and I think why we can just think slightly differently. We can just say, look, we have these features from the list that are quite ready and there are a couple of features that doesn't look that will be ready very soon and will delay and let's ship these features in the next four. I mean, for myself, this thinking that it's possible to split the Pectra into two pieces and the second piece will be exactly the same as we think now, I think it's a bit unrealistic. So if we just change the thinking where we just simply say that, look, there are a couple of features, a few features that needs to be pushed to the next hard fork and that's all. So that, that would be my, my idea.
00:14:04.365 - 00:14:46.975, Speaker A: Right. And yeah, just to chime in with what Marius is saying here in the chat, like, there's at least two things here. One of them is just should we focus on Petra being essentially the DevNet 3 set of features? We work over the next couple of months to like get everything polished, start thinking about testnet timing on the way to mainnet. The other thing is, okay, yeah, the scope of say the second fork. I hear everyone that, yeah, it could be tricky to not want to put new things in. I would lean towards again keeping the scope very small just because then that's going to maximize our chances of actually shipping a second fork very quickly. With respect to this first one, say Pectra A.
00:14:46.975 - 00:15:12.933, Speaker A: So that all being said, let's maybe take these separately. So is anyone opposed to saying that the next hard fork, call it Pectra A or even just Pectra, will essentially be the Demet 3 set of EAPs? I think there's general agreement on that point, but correct me if I'm wrong, I'm very much in favor of that idea.
00:15:13.069 - 00:15:13.533, Speaker B: I think.
00:15:13.589 - 00:15:22.375, Speaker C: Yeah, I think Aragon was the only team voicing their opinions that we should split the fork.
00:15:24.875 - 00:15:26.575, Speaker A: Gotcha. Ita.
00:15:28.515 - 00:15:48.545, Speaker C: I mean, generally, yes, it's just this, the 7688, which I think should be in the same one that adds these validator requests like the reindexing on the CL side.
00:15:52.045 - 00:16:14.095, Speaker A: Right. So yeah, again I think it's going to really complicate things to like think about adding more things to Pectra A. But we can have the conversation. Does anyone else feel strongly like in particular different client teams? Would love to hear your thoughts on this. Like the seven. Seven. I think it's six, eight changes.
00:16:14.095 - 00:16:23.095, Speaker A: And yeah, etan, maybe just to add a little context, this is the stable container set of features, but essentially just with like the CL scope.
00:16:26.555 - 00:16:31.867, Speaker B: So my perspective is that like a big benefit of splitting the forks, like.
00:16:31.891 - 00:16:32.987, Speaker A: A reason to do it is that.
00:16:33.011 - 00:16:44.941, Speaker B: We have something that's finalized and so like we could do it quickly. So I would be against adding stable containers to that especially because there weren't like there wasn't like a huge push.
00:16:45.013 - 00:16:48.741, Speaker A: Outside of the like core dev calls.
00:16:48.773 - 00:17:01.825, Speaker B: I guess from people who want that feature. So yeah, I think it makes sense to just like go with Devnet 3. Don't add things, it might actually ship quickly. That'd be sweet.
00:17:04.015 - 00:17:07.035, Speaker A: Yeah, I tend to agree myself, Andrew.
00:17:08.655 - 00:17:41.463, Speaker C: Yeah, I think it makes sense to split pure CL changes like PI does into a separate fork or that might entail some minor changes. Okay, I'm not sure about say peer does entail some trivial changes on the EL side that shouldn't be that bigger problem. Why? Basically I worry about hindering the progress on Verkal. So we are on the outside, at.
00:17:41.479 - 00:17:42.215, Speaker A: Least I think so.
00:17:42.255 - 00:17:53.815, Speaker C: We need some headspace to concentrate on Verkal. That's why I propose to keep EOF in Vectra so that we can ship it.
00:17:53.855 - 00:17:55.355, Speaker A: But I'm happy to.
00:17:56.065 - 00:18:01.645, Speaker C: Yeah, to have PDAs and other CL changes in a split fork.
00:18:03.785 - 00:18:17.485, Speaker A: Okay. My understanding is that EOF is not quite to the same sort of developmental Milestone as the DevNet 3 EIPS. Anyone disagree with that? Take.
00:18:19.105 - 00:18:37.055, Speaker B: We have patches out for get to pass all the ESP tests and we have BASU and Rep are ready to implement. Nethermind isn't far away, so we're not too far from able to execute a DevNet with at least three. But we aren't 100% across all the clients yet.
00:18:40.755 - 00:19:45.695, Speaker A: Okay, thanks. So, I mean essentially I like I understand the urgency to ship eof, let's say, but I just think given all the other considerations, it would make more sense to keep it out of this first spectra. Okay, so what I'm hearing is that we're generally on board with Devnet 3, essentially moving on to be the next hard fork. Yeah, obviously there are other things to consider, but I think that's the direction we should move in. I don't know if we want to then turn to talking about scoping the second fork. It might be a little premature. Yeah, I think the counterpoint to saying that it's premature to scope the second fork was what you mentioned earlier, that if we don't do it now, the commitment to that fork will we risk scope creep of, you know, the same exact problem we had with this first Petra.
00:19:45.695 - 00:19:50.015, Speaker A: Right? Yeah.
00:19:51.755 - 00:20:12.685, Speaker B: Yeah, I think we shouldn't work on the scope of the second fort just yet precisely because I mean like Matt just said, it's going to be to be a creep. But more importantly, there's going to be things that change. We're just going to add some bike shedding before that. You know that is not necessary. What we need to do now is ship that fork.
00:20:14.825 - 00:21:08.897, Speaker A: Right. And you know, to address Matt's point that was just made, I like the way at least I would think about it is like essentially I would try very hard to stick to Pectra B being just peer DOS and sort of the blob set of EIPs along with UF. That being said, it'll be a conversation and just going from the chat and things that here it sounds like it will be hard to sort of commit to just that for the second fork. But I think we should all keep in mind that the more we touch or change the second fork, the longer it will take to ship. Hopefully we have it sooner than nine months. Marius. Okay, so I guess this is final call.
00:21:08.897 - 00:21:17.965, Speaker A: Let's move ahead with Petra being Petra A which is essentially DevNet 3. Can we all live with that?
00:21:18.785 - 00:21:27.980, Speaker B: Well, just to clarify that we have a number of things to modify those EIPs, so we don't really mean DevNet 3, we mean DevNet 4, but with those EIPs.
00:21:28.042 - 00:21:29.729, Speaker C: Right. Or 5 or whatever.
00:21:29.897 - 00:21:41.205, Speaker A: Yeah, this is a good point. So there are a number of open points still, even with the DevNet 3 scope. Some of them we'll get to later in the call. But I essentially mean yeah, just like that EIP set or that feature set.
00:21:44.065 - 00:21:50.445, Speaker C: So where do you want to put stable container stuff into picture two or which one?
00:21:51.825 - 00:21:59.045, Speaker A: Well, I mean I think it's a conversation with everyone here. I think yeah, Vector 2 is a natural target.
00:22:01.745 - 00:22:21.445, Speaker C: My understanding it's as good as ready in most clients and it actually has user interest as well. Like contract developers have problems with this today, so pushing that back seems kind of pointless. It's also CL only.
00:22:25.575 - 00:23:02.345, Speaker A: Right. So I mean the question for me, I would think is just like what, like it. Even if it sounds ready and it's maybe a relatively small set of, you know, say code change, just doing this like again kicks us down. Like it moves us further away from getting to like a stable pectra with like a spec freeze and all this. And you know, that's not the end of the world, but I think it's a decision that we should make intentionally. Are there other client teams other than Nimbus who feel strongly about this? I believe it's 7768, the stable container EIPS.
00:23:07.485 - 00:23:31.269, Speaker B: Oh, sorry. I feel like even though most clients have implementations, maybe all clients at this point, there's still a degree of like testing. Like we have to update the spec tests, ad specs, spec test specific to it, and then just the fact that it's not as battle tested, we'd have to do fuzzing on it. So it's like it does ex add.
00:23:31.317 - 00:23:33.605, Speaker A: More work to what we have, even.
00:23:33.645 - 00:23:41.925, Speaker B: If we're mostly like dev complete. So in my opinion we should put it in the second pector fork. But yeah, it's my opinion.
00:23:44.105 - 00:23:45.605, Speaker A: Thank you, Guillaume.
00:23:46.865 - 00:24:09.815, Speaker B: Yeah, so I did implement the containers on my own library like two months ago. Yeah, it looks cool, but I just wonder if it's worth shipping before we actually need it. And. Okay, I for one haven't understood why we need this. Okay, except for Verkol actually. But apart from that, is there an actual need right now to ship this?
00:24:11.915 - 00:25:27.095, Speaker C: It's the 4,788 smart contracts and client applications like Rocket Pool has a decentralized staking pool and they want without any multi sig to be able to prove, for example, that the validator was activated. So they want to prove relative to the beacon state route and that the validator is now there, that it's not slashed and access that information from the smart contract. So what they have to do today is that when unrelated stuff gets added, such as like consolidation requests or whatever, the size of the structure changes of the beacon state and they have to keep updating the code every single fork, even if only stuff changes, that doesn't affect them. Right. It's like if you install a new OS update and you have to recompile all your apps every time, even if none of the APIs change that you're actually using. So that's what's being addressed here. It's a benefit for the couple users who want to use like verification in a smart contract relative to the beacon state.
00:25:27.095 - 00:26:36.365, Speaker C: For ourselves, the core developers, we don't benefit that much from it. Like we can optimize a couple things by having fewer hash computations if it's like an optimized implementation. But the main benefit is for like those developers and indirectly also the users of the ecosystem because it's easier to create trust minimized applications when you have lower maintenance effort. But I agree it's not like that. The prime most important feature. The only reason why it's is a good timing in the picture is because this re indexing re indexing on the beacon state is already happening for other reasons. So all those teams that rely on EIP4788 have to update their code, have to redeploy the contracts based on a new indices and that's why the timing is like tied to those request EIPs.
00:26:36.365 - 00:27:03.415, Speaker C: We can obviously also ship it separately but it means that users will have to go through the migration step yet another time. Yes, the deposits and the withdrawals are more important of course like but I don't think they are mutually exclusive. Like the stable container is a CL only feature.
00:27:07.155 - 00:28:11.215, Speaker A: Right. So then the question becomes like do we for this particular set of users say a4788 do we like potentially delay pactrate even more to account for them? And I don't know, I think maybe to recenter things a bit. I'll echo what Tim is saying here. The whole idea with the split is to ship the next hard fork as quickly as we safely can and I think what that means is we really stick. We really should stick to a very strict scope. I think it's already going to be tricky to get to say spec freeze of first spectra in the next couple say month or two like next couple weeks like asap just as is and like yeah, I hear that it's, you know, the dev nuts for the EIP has been going like, you know, code's generally ready or mostly ready like I hear that but every little thing will just push out timelines further and further. So yeah, I, I hear all of that.
00:28:11.215 - 00:29:48.419, Speaker A: I would lean towards deferring to a later fork. Okay. There's a number of different views in the chat. Yeah, seems like we're kind of slow on this essentially. Do we keep things as is or do we want to think about including this SSE EIP into the first spectra? Okay, there's more chat going on. Sorry for the silence, I'm trying to catch up. I mean maybe to ask this question like if we do commit to pectra a being DEVNET3 +Polish is that the.
00:29:48.427 - 00:29:49.255, Speaker B: End of the world.
00:29:49.875 - 00:30:29.165, Speaker A: Like, yeah, I understand that there will be a little more downstream code change for users, say, of these generalized indices. Do we? Yeah, I guess. I mean, it's tricky. But I really do think the right decision here is to think, you know, with a broader scope and just say, hey, yes, this is a sort of consequence of this decision, but ultimately we'll get to a faster vector A and just keep momentum generally higher for the protocol, broadly. Yeah.
00:30:30.985 - 00:30:45.497, Speaker C: Yeah. I'm just gonna say whatever I said in the chat, which is I feel like it's either we split whatever is CFI and slated for inclusion in current picture and not add anything extra to.
00:30:45.521 - 00:30:51.865, Speaker B: Picture B, or we're gonna end up with basically scoping a whole other fork.
00:30:51.985 - 00:30:55.085, Speaker C: After this picture fork.
00:30:56.425 - 00:30:59.785, Speaker B: And. And we might. And everyone is going to end up.
00:30:59.825 - 00:31:04.497, Speaker C: Wanting to add eips to that picture B fork and we're going to.
00:31:04.601 - 00:31:06.809, Speaker B: Might. We might end up with picture C.
00:31:06.857 - 00:31:13.065, Speaker C: And goes on like, I don't know. I don't think this is the right way to go.
00:31:13.185 - 00:31:15.121, Speaker B: Either we are strict or we are.
00:31:15.193 - 00:31:19.195, Speaker C: Totally not, or we stick with whatever we have.
00:31:19.235 - 00:31:19.763, Speaker A: Sorry, Which.
00:31:19.819 - 00:31:23.055, Speaker B: Which was. Which is sticking with one fork.
00:31:25.955 - 00:31:53.595, Speaker A: Right. And that's the thing is, like, I think everyone agrees that the current fork is just too big. Right. So then I think a split makes sense. And then from there, I think if we do split, what makes the most sense in terms of, again, like very broadly, from a very broad perspective, keeping everything moving and sort of our shipping velocity high, adding more things to especially Pectra is going to be quite risky. Mark.
00:31:55.095 - 00:31:55.655, Speaker C: Yeah.
00:31:55.735 - 00:31:59.623, Speaker B: So if the main difference, like there's.
00:31:59.639 - 00:32:00.967, Speaker A: A difference here between people who think.
00:32:00.991 - 00:32:09.479, Speaker B: We can and we can and. Or that we won't be able to limit the scope. And then there's a split of people that believe, like, it's worth splitting up.
00:32:09.527 - 00:32:11.275, Speaker A: If we could limit the scope.
00:32:12.985 - 00:32:14.889, Speaker B: I guess, like, the main question as.
00:32:14.897 - 00:32:21.817, Speaker A: To whether or not we can do this is are there enough people that are willing to just not, like in.
00:32:21.921 - 00:32:24.209, Speaker B: Nine months or whatever, however many months.
00:32:24.377 - 00:32:25.729, Speaker A: When people are talking about it, are.
00:32:25.737 - 00:34:03.005, Speaker B: You willing to not let people put anything in? You know, like, if you're on the team that you want it split up and you want us to limit the scope, can you commit to just in these calls, not letting anyone add anything? So one, I think, I think there's two different things or more than two different things, but basically one is, do we want to ship something relatively quickly? And I think the only path there is we split Pectra along The lines of what's already been implemented. There's a bunch of stuff that different people want to add for different good reasons, but I think as soon as we open that can of worms, it becomes pretty much intractable. And yeah, So I think DEVNET 3 is like the right selling point for if we want to split today. I think the other question is if we do picture B, there's. It's clear that peer dash should be the main thing there on the, on the CL side. On the EL side, I guess there's like some disagreement between the relative priority of EOF and Verkal, and that's probably the biggest deciding factor. And then I think the other thing for picture B is like, do we want a bunch of small things come in and then.
00:34:03.005 - 00:35:06.625, Speaker B: And yeah, sorry, by DevNet3 I mean like the EIPS in DevNet3, not like the actual spec as is. I know there's like a bunch of changes on all of the OR most of the EIPs. I think if we, if we commit to like, yeah, assuming we split the fork on the CL side, it does feel like pure DAS is the obvious next thing to work on. On the EL side is a question between Verkal and eof and then there's a question on both sides between do we want like a bunch of other small things to come in the fork or not? And then it doesn't seem, I think it doesn't seem crazy to like try and resolve those things separately. One is to say like, okay, do we want to ship something as soon as possible? And. And then two is to say, assuming we split it, we're obviously removing EOF from this fork. Do we want this to be the focus of Vectra B? And there's some comments in the chat right now saying that they strongly support that.
00:35:06.625 - 00:35:31.755, Speaker B: And then C is assuming we have Peer DAS and EOF in Vectra B, are there other small changes that are not proposed yet that we think are actually quite valuable to include and how do they affect development timelines? And I think the thing that becomes hard there is like drawing the line of what's like a small change versus not, but they're effectively like three sequential decisions.
00:35:32.495 - 00:35:33.275, Speaker C: Yeah.
00:35:36.935 - 00:36:28.805, Speaker A: Yeah. And I think just going sort of by that framework, like, it does seem like, yes, the SEC features are important. Like in isolation, they do seem to kind of fall in this bucket of things that are not quite ready. Say, if we want to slit off today, if only just because it has been part of core Pector devnets yet, there will Be tooling, updates, testing security. There are many things that will need to be part of that process before we want to say this is now in Pectra A. And yeah, it's hard for me to see how we could sort of accomplish the best vision of all of this if we include it in this first fork. So to that point, like, I would propose we go ahead and say again, pectra A is DevNet 3 plus the Polish it needs.
00:36:28.805 - 00:36:41.495, Speaker A: We can have the conversation to put SSE IPs into the next work and probably handle the scoping conversation there. A different day. Guillaume?
00:36:42.835 - 00:37:14.435, Speaker B: Yeah, I just had an idea. If you want to be able to keep some flexibility for Pixra B while still making sure it doesn't blow up, you could introduce a rule, for example, that if you add any ip, you have to remove two from what is already scheduled. This way, presumably only the battery IPs that everybody can get behind are going to be scheduled. And I think it will make people think twice about pushing new stuff.
00:37:18.135 - 00:37:26.875, Speaker A: Yeah, I think that's going to be really hard to do in practice, but it's an interesting way to think about it. Andrew?
00:37:29.295 - 00:38:03.457, Speaker C: Yeah, I'm just thinking that I don't see why we should join a prds with say Vectra B on the L side. I'm thinking that they can be developed quite independently because prds requires minimal if non EO changes. We on the L side we'll have to discuss, assuming we do this in fact transplant. And we'll have to discuss whether we do Verkal first or EOF first because.
00:38:03.521 - 00:38:04.673, Speaker A: Yeah, there is a dependency.
00:38:04.729 - 00:38:35.165, Speaker C: But other than that, I would develop, say we decide to do Verkal before eof, then I would develop, I would have two sets of testnets for EOF and for Verkal, and then they can progress quite independently. We still have to do some, okay, say for Verko, that might require some minimal CL changes and the same thing for Peer Desk. And we'll do that, but the bulk of efforts will be quite independent.
00:38:38.945 - 00:38:39.233, Speaker B: And.
00:38:39.249 - 00:38:43.125, Speaker C: Then we basically deliver whatever, whichever is ready first.
00:38:46.785 - 00:39:00.145, Speaker A: Right. Okay, so how do we start talking about dropping EOs? I. I don't know if we necessarily did.
00:39:01.285 - 00:39:20.105, Speaker C: Well, my point is that we should have a discussion like honestly, we'll have to discuss whether we do Verkal first or EOF first. We'll have to have this discussion on the EL side. But I was just saying, assuming we prioritize Verkal over eof.
00:39:22.245 - 00:39:34.285, Speaker A: Right. And I mean, again, I would just echo what's in the chat here, like I think we can just see which is ready first. From my somewhat limited perspective just right now it seems like EOF is much further along than Verkal.
00:39:36.305 - 00:39:37.649, Speaker B: I wouldn't be so sure.
00:39:37.817 - 00:40:15.365, Speaker A: No. Okay, so again, before this explodes and we just spend the whole call talking about a future fork before we've even decided the first one, I would like to anchor us to make a decision on Petra today and I would strongly suggest we have the understanding that there is like, you know, features from current developments that would make a lot of sense in the next work. But yeah, I think we can have that conversation especially in the context of an EL call, even next week or sometime soon in the future. Dragon.
00:40:17.945 - 00:40:58.165, Speaker C: I think we are starting again make a decision like in the call, this kind of decision where there is a lot of team that needs to give their opinion. I think they're best done in like in Rita form so we can compare the stances of all teams. For example, this call we started talking about vertical before uf while the main point of the split was to speed up the prag. So we are jumping from one topic to another topic depending on the various teams incentives that priorit find. But the main point gets lost in that.
00:40:58.245 - 00:40:59.013, Speaker B: Hey, good morning.
00:40:59.109 - 00:40:59.633, Speaker C: How are you?
00:40:59.679 - 00:41:00.625, Speaker B: Good, how are you?
00:41:01.685 - 00:41:22.285, Speaker C: I would, yeah. I will ask teams to, as Oliver already put it last week in writing, say your like stance on this and we can compare it and be a lot easier to see the various teams where we are basically going. That's it for me.
00:41:22.825 - 00:41:30.605, Speaker A: Yeah, thanks. That makes sense. Mark, we'll take your comments and then I think we'll try to move ahead.
00:41:31.225 - 00:41:35.473, Speaker B: Yeah, it does seem like, like Tim pointed out there's multiple decisions that go.
00:41:35.489 - 00:41:39.655, Speaker C: Into this one decision and so I mean could we.
00:41:41.235 - 00:41:44.547, Speaker B: I like what he said, that we need the teams to get together and.
00:41:44.571 - 00:41:45.227, Speaker C: Do things in writing.
00:41:45.291 - 00:42:10.905, Speaker B: So could we commit to a timeline for deciding the Pector B scope today? Like not, not the Pector B scope but just like by some time we will decide it and then it won't be changed after that point. And then we all, we'll get together, put out our arguments, debate and then we'll have a call and we'll decide.
00:42:12.765 - 00:42:48.835, Speaker A: Yeah, I think it's. Yeah, I think it's tricky to kind of make a deadline just given everything. You know, if we want to try to have a view on Pector B in the next two weeks, that would be great. We could soft like I don't know, like I think everyone. This is top of mind for everyone. So like I think naturally it will happen pretty soon. But yeah, it's, it's hard to just like declare a deadline and then just assume that that will happen given all the uncertainties.
00:42:48.835 - 00:43:35.505, Speaker A: Maybe a soft. Well, yeah, I mean I can just say two weeks from now and then we'll see what happens. I think there's going to be like, I just think there are a lot of uncertainties into like Pector B. And so yeah, it sounds like at least just from the conversation on this call, it sounds like the conversation is a little more complicated than like just split current vector into A and B which is just where my hesitation is coming from. But generally, yeah, I think the faster we can make a decision the better. So there, there are other things to get to on the agenda today. So yeah, let's recenter and there seems to be agreement to split current Pectra somehow.
00:43:35.505 - 00:44:25.225, Speaker A: I think there's very strong support to have again this like devnet3 plus polish idea be the Pectra fork and then downstream we can figure out what comes next. Can we all agree to this? We have a thumbs up. Yes, there were other thumbs up in the chat to this idea earlier. Okay, let's go ahead and call it. Okay, thank you everyone. And yeah, I guess sort of the takeaway for everyone else is be thinking about Pector B and expect to have this conversation in the next few weeks. Okay, thank you.
00:44:25.225 - 00:45:16.857, Speaker A: Okay, let me look at the agenda. So there are some blob things to discuss and yeah, it's a little, we might kind of jump around a little bit. But I think the right way to move forward now is then to turn towards this sort of Pectra A scope. There are a number of PRs open still in flight for some of the Pectra features and let's just touch base on a few of them right now. So this first one I had, Mikael had this pr. Let me grab a link to it here. He and I were talking about this and you know, one thing at least that I'm trying to do is even for like the sort of again, Tektra A scope, simplify things as much as we can.
00:45:16.857 - 00:46:17.535, Speaker A: I think there still are some open questions around where we can simplify what makes sense and how best to do it. One thing that came out of conversation with Mikael and I is a way to change how we're handling the move to consolidations for a validator. The way it's currently spec'd is that you can essentially top up as a validator and if you have a Credentials changed in your deposit message, then it would like signal to the chain to move you to this new sort of class of validator. This kind of touches a bunch of other things around. Weeks of activity, deposit handling, deposit flow. And yeah, it ended up being a little tricky. One simplification here is actually to change how we handle this and essentially rather than have top ups and the deposit flow be the, let's say infrastructure to surface that signal from a validator, we move it somewhere else.
00:46:17.535 - 00:46:54.715, Speaker A: And in particular the suggestion was to move it to the consolidation pipeline. And one way you could think about this is just if I have a consolidation coming from the el if I just say source and target validator are the same one meaning sort of like a self consolidation. We kind of piggyback on that as a signal to switch to this compounding state. That was a lot. Is Mikael on the call? He might have other. Yeah, I don't know if you have anything else you want to add. This one's kind of hairy but I just want to bring it up because it does kind of change how we're thinking about this particular feature.
00:46:57.495 - 00:46:58.455, Speaker C: Thanks Alex.
00:46:58.575 - 00:46:59.639, Speaker B: Pretty much it.
00:46:59.807 - 00:47:19.051, Speaker A: And yeah, it also entails an update to the next PR that is on the agenda to the 6110 PR. It's been updated with removing the switch to compound from the deposit pipeline. So I think we should wait for.
00:47:19.123 - 00:47:22.627, Speaker C: This one we're talking about now to.
00:47:22.651 - 00:47:46.745, Speaker A: Get merged and then merge the 6110 one. That's good fun. Okay. Yeah, sounds good. And I put the second PR in the chat. These are kind of coupled because again it's just trying to simplify the deposit flow along with this consolidation flow. And again this does just kind of point to a lot of the complexity even we have in Petra a today.
00:47:46.745 - 00:48:51.343, Speaker A: So that all being said, I think the ask here is just recognize that we are changing this feature a little bit in terms of how you would move from a validator to a consolidated validator or consolidating validator. So yeah, please take a look at the PR that would be 3918 that I put in the chat here. And the idea is that this one would be merged and then it kind of should further simplify this next PR3818 and this has been an issue even from earlier this year, even like interop around handling the deposits and making sure that the load on the beacon chain cannot become too much. So these are core changes that we need to resolve. And yes, please take a look. I don't Know if there's anything else to add, Mikael, on the 60 win in PR? Nothing on the PR itself, but just wanted to mention the performance stuff. Yeah.
00:48:51.343 - 00:49:01.703, Speaker A: Some devs were concerned about having this pending deposit skew which will be entirely.
00:49:01.759 - 00:49:05.487, Speaker B: Rehashed every epoch processing, so it can.
00:49:05.511 - 00:49:15.343, Speaker A: Cause some probably performance issue. And I just want to ask if we want to check this and do some performance measurements because before Electro we.
00:49:15.359 - 00:49:25.579, Speaker B: Did not have the lists in the beginning state that are that we remove elements from the beginning from.
00:49:25.667 - 00:49:35.803, Speaker A: So that was kind of the concern and this is why we wanted to explore some alternative queue designs, you know, to make it always append and not.
00:49:35.939 - 00:49:38.975, Speaker B: Removing from the beginning thing.
00:49:39.315 - 00:49:56.245, Speaker A: So yeah, just wanted to not ask to forget about this concern. Maybe someone already did some performance measurements and can just share them. Yeah, I'm not sure if.
00:49:58.465 - 00:50:49.955, Speaker C: I mean I look at the performance quite regularly and hashing is like one of the big things that we do with the state, which takes actual time. The other one being shuffling. Right, Those are the two big thieves there. The third one is slightly different, which is basically signature verification and replay of deposits. It's also something we discussed, I think in Kenya, but I would be loathe to add too much hashing because it affects processing in places which are already significantly heavy, such as, you know, processing. And often it can't be avoided, it can't be cached away, it can't be, you know, tweaked.
00:50:54.495 - 00:51:45.655, Speaker A: Right. So I think the takeaway here is that we should look at this and this would be a nice target for DevNet 3, DevNet 4 again around this first picture set of features and the polish we do there just to get it on people's radar. I think there are at least potential other concerns around not just the deposits, but also consolidations. And what is the other one? There's essentially like a number of queues that we're adding to the state and they're kind of all relevant here. So when I say devnet 3 + polish, this is kind of what I mean is I think there will be a number of things that we need to. You know, ideally we could even get benchmarks and implementations today and use that to inform where we go from there. So something to keep in mind.
00:51:45.655 - 00:52:35.289, Speaker A: Okay, so yeah, there's those two PRs again. Please take a look when you have a minute. Next up, there were two more PRs again in this pectra, a scope around the attestation updates. So as far as I understand, essentially like the point, actually let me grab them for the Chat. But essentially it's a number of PRs to make handling attestations easier in clients. Just given the changes from. Let's see, it's 7549, I think the attestation EIP, sorry if I forgot the number.
00:52:35.289 - 00:53:32.525, Speaker A: But essentially these are simplifications, at least as proposed for that feature set. And we had a bit of a temperature check on the last call and I guess, yeah, I'm just bringing them here again as another temperature check. If we are serious about having these in Pectra, then we should make a decision and, and keep these moving along. There were some comments on the PRS here around again concerns of complexity, you know, and again we just kind of get back to the same question. Like yes, we could imagine sort of a perfect world, but we might have to stick with a good enough world. Anyone here? If you've had a time, a chance to look at these PRs, it'd be good to discuss how we think about them. Terence, I started implementing 3900 which is a single attestation.
00:53:32.685 - 00:53:34.861, Speaker B: I think it's very nice to have.
00:53:35.013 - 00:53:40.733, Speaker A: Like, it just feels right, it's very clean. But I do worry about like the complexity.
00:53:40.829 - 00:53:48.037, Speaker B: So it's just one of those things that from the spec perspective it looks very simple but once you like add.
00:53:48.061 - 00:53:49.821, Speaker A: It to the code base then you.
00:53:49.853 - 00:54:22.055, Speaker B: See this cascade of changes that you have to make. I mean but I do think this is much like easier in terms of complexity versus the attestations one previously. But yeah, I just wanted to make a minor warning on that. Another, I also do have a question, just on the spec side, it only makes changes to the PPP interface, but as a validator I need to be signing and sending those attestation out. Right. This is also part of the honest validator duty change.
00:54:26.675 - 00:54:48.295, Speaker A: Right. With the implication being that this might sound like a simple change, but it might end up being quite a bit more work than it sounds like, which is also the issue had with the CIP in the first place. So that's now the call to make. Yeah. Where do we sort of draw the line on good enough versus shipping the perfect solution.
00:54:50.895 - 00:54:52.591, Speaker B: So yeah, Eton was working on this.
00:54:52.623 - 00:54:54.835, Speaker A: From our team and he had similar feedback.
00:54:56.535 - 00:54:58.687, Speaker B: I think if we wanted to shift.
00:54:58.711 - 00:55:03.639, Speaker A: The spectra one quickly, we probably shouldn't include this change just because it's like.
00:55:03.687 - 00:55:16.521, Speaker B: A lot of extra work for like not any tangible gain in the clients. Although I think it's good from like the spec perspective. Yeah, I was going to say something.
00:55:16.553 - 00:55:17.313, Speaker A: Else, but I forget I was going.
00:55:17.329 - 00:55:18.005, Speaker B: To say.
00:55:20.985 - 00:56:07.185, Speaker C: Okay, I can answer the question, at least for the honest validator part can be updated, doesn't have to. You can convert between the two formats freely. The networking part, like what we put on the gossip channel, is important for two reasons. One is the DOS possibility that we close with single attestation, which is very easy to exploit today if we wanted to, if somebody wanted to. And that sort of closes the gap. So that's the immediate kind of benefit that the PR gives. It's a security update benefit, basically.
00:56:07.185 - 00:56:32.625, Speaker C: And at the same time it's also an efficiency update because we'll be spending less CPU processing attestations if we include it, which those that follow performance like hashing takes time. There's a lot of attestations to process and the more validators we have, the more that particular point grows.
00:56:36.165 - 00:56:53.505, Speaker B: So. So if validator doesn't do this, then are you always expecting they're just like nos basically will convert to a different format and just forward it again? So you can assume there's like honest behavior in the P2P for this.
00:56:56.285 - 00:57:08.671, Speaker C: To be honest, I didn't, like, I didn't really consider it. If I, if I was looking at it after your comment, I think I would include the honest validator change as well.
00:57:08.703 - 00:57:09.275, Speaker A: Right.
00:57:09.815 - 00:57:56.125, Speaker C: I think that's reasonable. But it doesn't matter for, let's say, the bulk of the pr. Right. It's, it's, it's, it's better to have it in validated respect as well. I agree, but it's not critical. So if we wanted to have kind of a minimum impact on clients change, then you can actually do all the translation in the gossip layer. And then many of these, you know, cascading code changes concerns can be swept under the rug, I would say.
00:57:57.825 - 00:58:00.209, Speaker A: But when you do the translation, you.
00:58:00.257 - 00:58:03.577, Speaker B: Still have to do some CPU cycles and compute.
00:58:03.641 - 00:58:03.937, Speaker C: Right?
00:58:04.001 - 00:58:12.201, Speaker B: So is that less than today basically? Because when you do the translation, you still have to get the committees. You have to like figure out like.
00:58:12.273 - 00:58:14.649, Speaker A: Which bit to flip in the aggregation.
00:58:14.737 - 00:58:20.335, Speaker B: Field, for example, or. Yeah, something like that. So there is also compute there, right?
00:58:21.515 - 00:59:05.635, Speaker C: Not really. It's all information that you have. I mean, in order to know whether you're supposed to create an attestation to begin with, you have to load the shuffling because that's what determines which slot you're actually sending the attestation in. So that is all information that you have at hand when you're doing that processing. I agree, it's kind of pointless to not make the honest validator change, but it's not strictly necessary. That's what I'm trying to say. And it doesn't affect the prime benefit either, which is that nodes validating the single attestation can do the signature check before doing the shuffling with this change.
00:59:05.635 - 00:59:26.285, Speaker C: Those that produce it, they have to get the shuffling regardless. So it's kind of, again, like. I appreciate your comment. It's really good. And I think we should. I should update the pr, but that's, you know, that's the nice to have of this change.
00:59:27.105 - 00:59:28.673, Speaker A: Yeah, I think if this PR were.
00:59:28.689 - 00:59:37.165, Speaker B: To go in, I think preferred on this validator change as well, just because it feels right to me. But yeah, other than that, I have no strong opinion.
00:59:38.585 - 00:59:41.685, Speaker C: All right, I'll just throw it in there. I think I kind of agree.
00:59:43.385 - 01:00:36.545, Speaker A: Okay, so then to move forward on the dock, if you can add the validity guide changes to this pr, I think that would help people sort of think about it and see the full scope. And I think I'll try to get maybe another client team to look at implementation, at least to like weigh in and then we could try to make a decision on the next cl. I think that's a nice path forward, unless anyone disagrees. Okay, cool. So there are those. And then next on the agenda. Okay, I want to be mindful of time because there's some other things to discuss, but I did want to bring up again this PR from Felix to think about how we handle the request and the engine API.
01:00:36.545 - 01:00:58.545, Speaker A: Yeah, I feel like on the last call we were pretty close to saying this was good to go. There was an update requested around, I think some of the ordering semantics which I believe has been added. Can we go ahead and agree to this change or are there any updates or questions we have about this change?
01:01:13.495 - 01:01:19.447, Speaker B: There is probably an idea to keep requests, to not keep requests on the EL at all.
01:01:19.551 - 01:01:22.287, Speaker A: So because they are now kept by.
01:01:22.391 - 01:01:41.419, Speaker B: CL and only keep the requests root in the EL block header as a commitment that will be validated by the EL given the requests sent from the CL via Engine API, that's probably the.
01:01:41.507 - 01:01:51.347, Speaker A: Alternative to this pr. So EL can compute this request route. So this commitment in a way it.
01:01:51.371 - 01:01:54.575, Speaker B: Wants and it will not affect anyhow the CL part.
01:01:55.325 - 01:02:38.167, Speaker A: I don't know how much do we want to explore towards this idea, but yeah, at least it can be an alternative. Okay, thanks. Okay. Yeah, I'm not sure we're going to make a decision on this call. I would hope we can next week also, you know, this one goes cross layer, so also want The EL perspective as well. So that might be that for now. If there are any other comments on this set of changes, now's the time.
01:02:38.167 - 01:02:40.155, Speaker A: Otherwise we'll go ahead and move forward.
01:02:50.265 - 01:02:51.085, Speaker C: Okay.
01:02:53.145 - 01:03:24.699, Speaker A: So next up were a batch of things, essentially looking now like, okay, we have picture a sort of, again, this DevNet 3 plus different Polish things that we've been discussing. There are a number of other PRs. They're all pretty simple, sort of one by one, so maybe I'll just call them out. But there's this one too. Get rid of this, get payload bodies, V2 set of methods. I'll just post this here. Let's see.
01:03:24.699 - 01:03:51.861, Speaker A: Mikael, you had this pr. I don't know if there's anything else you would like to add. My understanding was that basically we added this for Pictra when it wasn't as clear how we'd handle the request and it's just actually simply no longer necessary. So it makes sense to go ahead and delete, which is what this PR does. Yeah, basically. And it's just, you know, announcements to do the check if anything is missed and we should not remove those methods. But I think we should.
01:03:51.861 - 01:04:58.235, Speaker A: If there is other opinion, please let us know before the next call. Okay. Yeah, I lean towards merging this even sooner than the next call, but yeah, we'll at least give it a few more days. So please take a look and add any feedback to the pr. And then otherwise there was a batch of things essentially again around polish for Petra and kind of saying, you know, with the DevNet 3 feature set, what do we need to do? There's a bunch of things around APIs and connections between the Beacon node via our client, there's the builder pipeline, there's a bunch of sort of peripheral things that are critical and need to be addressed. So I called some of them out. What is relevant here? There was this PR for the engine API, which just reflects how we're going to handle the request and that's in progress.
01:04:58.235 - 01:05:40.345, Speaker A: Another one would be the changes to the beacon APIs. I think there are still a few things there, but let me just drop this in the chat so people can take a look at that. And otherwise let me grab the builder specs PR as well. Give me one second. This one is here. So point being is Moving ahead with DevNet3 +Polish as Petra, now is the time to be thinking about these various APIs. So Beacon APIs, builder APIs, all of the other things.
01:05:40.345 - 01:06:23.527, Speaker A: Another one that is relevant is the remote signer and web3signer. I don't know if anyone here on the call is more involved with that and would like to give an update there. It would at least be nice to get an acknowledgement that someone is keeping track of it. Okay, that's okay. We'll follow up Async with that and great. So yeah, again, takeaway here. Just be aware of these PRs.
01:06:23.527 - 01:06:37.155, Speaker A: There are some sort of minor changes to all these APIs to reflect the latest Pectra EIPS, and that's something we very much want to get settled as quickly as possible so that we can facilitate Petra mainnet as quickly as possible.
01:06:39.495 - 01:06:40.315, Speaker B: Okay.
01:06:42.175 - 01:07:11.565, Speaker A: That was more or less it for Petra. If there's nothing else, we can move on to pure dos. And we do have, I believe, a presentation around the blob counts which then also just kind of funneled back into this fork scoping discussion. But maybe we'll quickly just start with any devnet updates for pure dos. I know they've kind of been in progress for a little while. I don't know if there's anything to update there first. Yeah, Arnoduck.
01:07:13.745 - 01:07:25.235, Speaker C: Yeah, there's one more spec PR I didn't mention, which is the TFTFP timeout thing to get rid of it. I haven't seen anyone against, so maybe we can just go ahead with it.
01:07:26.815 - 01:08:04.475, Speaker A: Okay. Yeah, I haven't looked at this in a little while. Yeah, how about I'll just keep track of this and we can discuss either at the end of the call or probably the next call. But yeah, I agree that this generally looked like it was moving in the right direction to merge. I don't know if anyone else has comments on this right now. Okay, we got it. I like it.
01:08:04.475 - 01:08:36.725, Speaker A: But yeah, this would be, I think pretty in scope for the polish of PETRA and otherwise. If you're listening. Yeah, client teams, take a look at this pr. There's been a good bit of conversation already, but yeah, I believe we discussed this interop and yeah, it seems like everyone was on board. So just one other thing to keep track of. Okay, thank you. Any peer DOS, DevNet updates?
01:08:41.075 - 01:08:42.535, Speaker C: We are working on it.
01:08:42.835 - 01:09:09.324, Speaker A: Okay, great. So yeah, there are progress and again, yeah, it just kind of reflects that. Yeah, it's a little bit earlier than in the development pipeline than some of these other things we have going on. Thanks for that. Cool. So then the next thing on the agenda, there is quite a bit to get through today, I think. So is it Francis? I think Francis should be on the call.
01:09:09.324 - 01:09:17.956, Speaker A: I think they wanted to give presentation. Oh, hey. Yeah, I'll just hand it over to you. I think you have want to talk about blob limits for.
01:09:18.140 - 01:09:29.784, Speaker B: All right, cool. Do you mind I share my screen? That might be easy. Okay. Can everybody see it?
01:09:31.975 - 01:09:33.075, Speaker C: Yep. Yes.
01:09:33.575 - 01:09:52.519, Speaker B: Okay, cool. Thank you. So hello everyone. My name is Francis. I'm from the base team. I'm a protocol engineer there and it's really great that I have the opportunity to talk to you guys about this. So like I think we have a lot of time to discuss about like the potential stability about predictor A and predictor B.
01:09:52.519 - 01:11:00.905, Speaker B: It seems like the consensus alignment right now is that pectoral A will take the current Devnet 3 like ready EIPS and ship and we will put peer Das into picture B and potentially ship later. So our main point here is about the robbed capacity for airfields in general. And if we are putting peer DAs into the next hard fork which is kind of like 9 to 12 months later, then we think that the current capacity 3 target blob will not be enough for L2 to scale in the coming months. Especially with L2 continuously to grow. And with base we have this scaling plan to increase our target gas per second from 10 mega gas to 25 megas per second till the end of the year. So the point is that the capacity might not be enough and we would like to recommend a slight modification to the current proposal like in scope for PETRA A which is that we integrate a blob number increase into petra. So like how do we do that? There are two like ways which is like pretty.
01:11:00.905 - 01:11:59.713, Speaker B: I think it's pretty straightforward. The first one is like let's increase the target and max for the blob number. So we said we can set target to 4 and max to 8 and or we can do just a target increase and keep the max blob number to 6 which helps keep the like worst case scenario like in check. We understand that there are like a lot of concerns around the network bandwidth essentially their effects on these solo stakers. So we did some like very initial analysis and we would like to share it with everybody here. So regarding the network bandwidth we are like to be clear we are using 6 target and 12 max as a lucky for administrative purposes and all the number here right now are average network bandwidth. So there's like really good concerns about the peak bandwidth and we can get into that a little bit later.
01:11:59.713 - 01:13:15.395, Speaker B: But right now let's talk about the average. So for the past week the CL client just for the just P2P data and the Yale clients they are using like 400 kilobytes and 140 kilobytes inbound like data for the path week. And for the outbound Data it's around 670 kilobytes in total. And we have some data from ExpandOps team's dashboard that showcases the entire data coming in and out of the container, which includes the RPC and Beacon API traffic, which you can see it's naturally a little bit more than just P2P data. But the point here is that P2P data is the main data in and out of the CL&EL client. So if we are adding three extra target blobs to the network, we are going to have 54 kilobytes of data addition to inbound and 384 kilobytes of outbound data in addition. And if we add them together in total, the P2P data in after the blob increase will be 300 kilobytes and the total P2P data out would be 1 megabyte per second.
01:13:15.395 - 01:14:52.035, Speaker B: And this is where I think it's really important to talk about not only average like data, but also the peak use cases or the worst cases. So if we say that okay, everybody has to disseminate the blobs within two seconds or like four seconds as the current spec defined, then we can in worst cases without any other change like to like too detailed analysis, we can say that okay, we can say okay for the blob out data it could be like 3 megabytes per second or like even like if we have to decimate them within two seconds it could be like six megabytes per second like counting everything into that two second slot. So I guess the question here is that is that reasonable enough for solo stakers who are like bandwidth constraint. I did some kind of like really, really quick search around the upload upload bandwidth across the world. It seems like for the like 200 country their average upload like speed would be like 3 point something megabytes per second per some like testing network testing websites. So this is just like for thoughts, but we believe that if we keep the max in check and only increase the target, you'll be like definitely reasonable in terms of like a target blob increase. So the second point is like for imposing a higher minimum price for blurb data.
01:14:52.035 - 01:15:54.575, Speaker B: The current argument is that like the validators are not fairly competent compensated for their compute storage and bandwidth required for providing the data availability. And we believe that the problem is definitely legit. But addressing them by constraining The DA capacity is a step in the wrong direction. Instead we would suggest that a different approach which is that we can work on something that both ensures us to have a healthy market for blob space but also keeps the target blob number unconstrained for L2 scale. And that I believe can be orthogonal to like increasing the blob targets in this hard fork and can be worked on in parallel maybe into the next hard fork or the next one like after that. I believe that is all my presentation. I curious about your guys thoughts on like if you are okay with like going with either of these like choices or if you have any other concerns.
01:15:54.575 - 01:15:56.825, Speaker B: Thank you.
01:15:58.005 - 01:16:35.485, Speaker A: Great, thanks. Yeah, that's really nice to see and thanks for putting that together. Yeah, these are interesting options and it's something we should discuss obviously. Again the data facility of Ethereum is one of our most important features. So it's very important to support it where we can. And one way we can do that is think about changing the blob numbers even in Pectora. To that I would have to say I'm going to maintain my earlier position which is just that you know we really should leave Pectora as is work on scope freeze and not think about changing anything else.
01:16:35.485 - 01:17:14.575, Speaker A: That being said, I will just kind of summarize I think what's kind of in the chat in particular like Osgar here had a nice message that changing the MAX I think has a lot of implications. Changing just the target is a little more reasonable and is like a much more sort of narrowly scoped change. Yeah, I think that's my take. Would lean towards leaving things alone for Petra and then just focus on all the BOB things in the next work after Petra. I would be open to a target adjustment. Anyone else want to chime in?
01:17:22.115 - 01:17:40.887, Speaker C: I wonder if it's worth like what is the progress on peer to peer improvements? Like it feels like there's still a lot of things that we could improve on peer to peer to reduce this bandwidth and then it would probably be.
01:17:40.911 - 01:17:45.795, Speaker B: Easier to justify increasing both target and max.
01:17:49.575 - 01:17:51.635, Speaker C: Is there any update on these things?
01:17:55.695 - 01:18:14.495, Speaker A: My sense is that that's kind of been thought about under the sort of work stream of peer dos. So we have the state there which is still pretty early. Any other client teams? Have you, have you looked into this at all? I think there's like a basket of different things we could think about to help the bandwidth consumption.
01:18:15.315 - 01:18:19.259, Speaker C: I mean the main thing isn't wasn't there. I don't want discussion where we said.
01:18:19.307 - 01:18:23.415, Speaker B: Like we reduce the peer to peer.
01:18:25.475 - 01:18:53.985, Speaker C: Factor on blobs especially in order to reduce the bandwidth. That seemed like a very promising direction. So I wonder if that's like are we further on that it's been merged in the lib b2p spec but the lib b2p spec is moving very slowly at these in these days. Nimbus has an implementation. I know Lighthouse was working on one, I don't know about the others. Right.
01:18:58.655 - 01:19:13.023, Speaker B: I think both Lighthouse and Prism have the I don't want implemented already. Like yeah, because the underlying LDP both Inco and Rust have either one invented.
01:19:13.079 - 01:19:17.315, Speaker C: Already I think and have we seen improvements from that or not?
01:19:20.055 - 01:19:37.875, Speaker B: I don't know it has been deploy already or not but yeah, maybe the client team can talk about it. I think we haven't shipped it so it's for the next release. So we can only measure this impact on this in several weeks. It takes a lot of time to users to actually update.
01:19:38.295 - 01:20:42.477, Speaker C: Right. But would it be fair to say if we get like a large saving from saving from this then we can commit like giving that to Altus in the form of a target and max increase. So I can just give a random number. It's very easy. We have a dashboard that tracks how much we save and it's on the order of 40 kilobytes per second right now For I don't want on the mainnet node with you know, standard Nimbus settings at least which may not sound like a lot but it's like compared to those numbers that were presented it actually isn't nothing either and this number will grow with the number of peers that implement. I don't want so it depends on everybody deploying it. It's a feature that that needs to deploy it.
01:20:42.477 - 01:21:02.345, Speaker C: Is there any client that doesn't like isn't going to implement this in the next few months? Okay, sounds like there isn't.
01:21:03.965 - 01:21:06.865, Speaker B: I think there are some but. But I don't know which one.
01:21:08.015 - 01:21:12.315, Speaker A: It sounds like we don't know right now. But you had your end up, right?
01:21:13.095 - 01:22:28.465, Speaker B: Yeah, I had three unrelated or somewhat related comments. So for the first one to get a good measurement you don't really need all clients. You need the largest node operators that are running essentially Prism and Lighthouse to update and then we're going to get statistically relevant numbers and I think regarding your comment, I think everyone would agree that if we have a good measurement and we have like a good experiment that shows that home stakers are are capable of taking an increase then then I think it's fine, we can go for it. Although we've been already in situations in which builders were sending target or even under target number of blobs and home stickers since the mempool was full were sending six blobs and homestakers were hurt because their bandwidth was taken. So I think even measuring this is non trivial. We need to be clear that the average bandwidth is not the right number to look for. We have currently when you submit your blog you have less than two seconds to broadcast to everyone and everyone to validate and consider it available.
01:22:28.465 - 01:22:39.517, Speaker B: So again I repeat that 7732 is very far in advance and it gives you 12 seconds instead of 2. So it solves most of these problems as a no brainer.
01:22:39.701 - 01:22:42.293, Speaker C: Right, right, understood. I.
01:22:42.389 - 01:22:45.865, Speaker B: Well, right. Yeah.
01:22:46.405 - 01:22:47.229, Speaker C: Prep does.
01:22:47.277 - 01:22:48.701, Speaker B: There's also the other one which is.
01:22:48.733 - 01:22:54.563, Speaker C: That like some clients have implemented that you consider blobs that are in the.
01:22:54.579 - 01:23:03.455, Speaker B: Mempool as available as well. Like I think that could also be a big improvement especially for home builders who are only going to consider mempool blobs.
01:23:13.995 - 01:23:15.495, Speaker A: Do you have something to say, Francis?
01:23:16.675 - 01:23:39.305, Speaker B: Yeah, like I think all the concerns make sense. I'm just curious like if everybody's okay or like open to maybe let's just keep the max to be the same so that it fixes the worst case scenario for all these solo stakers to be the same as currently and we increase the block target to a higher number, maybe four or five.
01:23:42.125 - 01:23:47.745, Speaker C: So I have an additional suggestion. I think what Porto has mentioned that.
01:23:49.465 - 01:23:52.361, Speaker B: That the, that currently some builders don't.
01:23:52.393 - 01:23:56.925, Speaker C: Include blobs and then home stakers pick up the slack is an artifact.
01:23:58.865 - 01:24:04.537, Speaker B: I'm not saying currently. Yeah, yeah but like that is an artifact of blobs having low tips.
01:24:04.721 - 01:24:19.337, Speaker C: So I would encourage especially roll up teams to consider just right now maybe already increasing their tip size because right now like everyone who includes blobs is, is essentially already subsidizing blobs because they.
01:24:19.361 - 01:24:21.905, Speaker B: Are not being fairly compensated for the.
01:24:21.945 - 01:24:24.445, Speaker C: Increased offering risk that they're taking.
01:24:25.425 - 01:24:27.905, Speaker B: So if we had like sort of.
01:24:27.945 - 01:24:31.713, Speaker C: Like some big roll ups agree that.
01:24:31.769 - 01:24:33.281, Speaker B: Like it's a good idea to pay.
01:24:33.353 - 01:24:37.697, Speaker C: Say $5 of tips per blob. I don't think that comes down to.
01:24:37.761 - 01:25:12.365, Speaker B: Anything significant per transaction but it would probably make a huge difference for the builders on like considering whether they should include blobs. Yeah, I think this is definitely like great suggestion and we agree that to have everybody like to have the network fairly compensated for the blob space is like Something we are okay with. So definitely we can do that. But like to maybe a little bit question about this. Are we saying that the builders are currently not including blobs as they should?
01:25:14.435 - 01:25:16.419, Speaker C: Well, everyone is free to build their.
01:25:16.467 - 01:25:17.459, Speaker B: Blocks as they like.
01:25:17.507 - 01:25:17.739, Speaker A: Right.
01:25:17.787 - 01:25:19.363, Speaker B: But some builders have at least in.
01:25:19.379 - 01:25:21.019, Speaker C: The past chosen just to not include.
01:25:21.067 - 01:25:22.851, Speaker B: Blobs at all because they said well.
01:25:22.883 - 01:25:24.915, Speaker C: There'S no incentive to do it because.
01:25:24.955 - 01:25:59.003, Speaker B: They don't really pay any fees. Are we talking about like flashbots? I think previously. So if that event this was an issue a few months ago and most builders have fixed this issue. Flashboards was one with the serializing issue that is already fixed. Rsync was another one that was pushing three blobs when the home stakers were pushing six at the time. But then they moved into pushing blobs as they were. Geth itself had a problem in the way that they computed the tips.
01:25:59.003 - 01:26:43.455, Speaker B: That favors or it doesn't favor some rollups that have more or less execution. All of these issues I think are sort of orthogonal to what were the discussing here. Yeah, yeah, that makes sense. So if you. I believe that makes sense and we've talked with like Crashbox team to fix that and the other things might have been like fixed already like down the pipeline or if they haven't, they would be probably to do it down the pipeline. But like as why we said we are definitely okay with like increasing the tips to like maybe help with builders to include that into their blocks so that sort of takers can take less of the burdens on that.
01:26:50.075 - 01:27:26.825, Speaker A: Right. So we are almost at time and maybe just to get to some resolution for the current topic. It does sound like we need a lot more data I think to understand what is safe sort of modific implication for the blob parameters would be especially without pure dos for scope reasons and again just like general momentum reasons. I think we want to touch less rather than more. That would kind of motivate thinking about just the target. And separately. Yeah, I think there's a lot of work here, especially if we want to roll out gossip sub changes and all this.
01:27:26.825 - 01:28:05.061, Speaker A: That's going to take time. Yeah, so it sounds like we should move ahead on those separately. I don't think we'll necessarily make a final call today and yeah, we'll definitely. I think it's very top of mind for everyone. So we'll definitely revisit this before pectoral hits May not. Okay, yeah, we're pretty much at time. Any closing comments otherwise? Thank you all for joining.
01:28:05.061 - 01:28:13.065, Speaker A: That was quite a big call, but a very important one. So, yeah, if there's nothing else, we'll go ahead and wrap up.
01:28:22.215 - 01:28:23.195, Speaker B: Thanks, everyone.
01:28:23.855 - 01:28:29.135, Speaker A: Yeah, cool. Thanks. Next time. Bye, everyone. Thank you. Bye.
