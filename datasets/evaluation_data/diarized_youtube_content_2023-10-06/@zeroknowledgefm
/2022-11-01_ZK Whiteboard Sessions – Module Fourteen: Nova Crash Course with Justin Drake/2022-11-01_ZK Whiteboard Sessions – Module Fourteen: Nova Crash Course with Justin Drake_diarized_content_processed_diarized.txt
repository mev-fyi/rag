00:00:12.160 - 00:00:39.048, Speaker A: Hey everyone, welcome to another episode of the ZK Hack Whiteboard series. I'm Brendan, I work at Polygon and I'm here with Justin Drake. Justin is an ethereum researcher. He is, I would say, a polymath focused on crypto economic design, zero knowledge proofs, hardware acceleration, vdfs, anything that touches the Ethereum l one. So welcome, Justin.
00:00:39.144 - 00:00:40.860, Speaker B: Yeah, thanks for having me, Brendan.
00:00:41.300 - 00:00:43.888, Speaker A: And what are we going to talk about?
00:00:44.054 - 00:00:47.600, Speaker B: So the topic for today is Nova.
00:00:48.580 - 00:00:50.316, Speaker A: What is Nova?
00:00:50.508 - 00:01:32.912, Speaker B: Good question. So Nova is a fairly recent construction that was published on eprint in around March of last year. So it's just over a year's old. It was invented by Srinafsetti, and I kind of think about it as a pre processing step for snarks. So it's not a snark in and of itself, but it's a very powerful technique that when used with a general purpose snark system, can give us kind of the best of both worlds in terms of very fast prover time and very fast and cheap verifier time.
00:01:33.046 - 00:01:45.060, Speaker A: Cool, sounds great. All right, so it's a snark and it's used for batching or recursion. How does it work?
00:01:45.130 - 00:02:22.332, Speaker B: Yeah, so one way to think about it is as a generalization of BLS signatures. So if you have BLS signatures, s one, s two, all the way up to sn. The idea is that if these signatures are all signing over the same message, you can aggregate them and only have to verify the aggregate. And we use this actually on Ethereum today on the beacon chain. And this gives us roughly a 1000 x improvement in verification time.
00:02:22.466 - 00:02:30.156, Speaker A: Because instead of verifying end signatures, you essentially pay a cost to just verify many with sort of one operation.
00:02:30.268 - 00:03:14.988, Speaker B: Exactly right. And you can think of the signature as being a statement. The statement being, I know a private key that corresponds to this public key, and I'm signing over this message. And you can ask yourself, okay, can we generalize the statement to be any NP statement? And this is exactly what Nova does. So you have n statements, and instead of having to verify each of them individually, you fold them together. So the term that is used is a folding scheme. You fold them all together and then you only have to verify the last folded instance.
00:03:15.164 - 00:03:15.890, Speaker C: Cool.
00:03:16.580 - 00:03:39.364, Speaker A: I guess for the non complexity theory heads out there. So I have these computations, and in normal sort of snark terms, I would have to generate a snark proving that each one is valid. And so instead I don't generate a snark, I just take the statements I use Nova to fold them together, and then I can just verify the folded version.
00:03:39.492 - 00:03:52.684, Speaker B: Yes, because snarks are very heavy machinery in the grand scheme of things. And here, basically, it's a way to feed less work to the snark prover and do more preprocessing work up front.
00:03:52.802 - 00:03:53.516, Speaker C: Very cool.
00:03:53.618 - 00:04:44.348, Speaker B: Yeah. And so one of the constraints here is that every statement has the same structure, just like BLS signatures, every signature is a signature. The good news is that in the real world, we have structure everywhere. One example is vdfs, and we can talk about this verifiable delay functions, but maybe a more general purpose example is a virtual machine. Think of the Ethereum virtual machine, or think of a RISC five cpu or a mips, where basically you have cycles that are repeating over and over again, just like a real cpu, where the configuration of the transistors is fixed. That's the structure. And then what you feed into the transistors is what changes.
00:04:44.348 - 00:04:45.672, Speaker B: That's your witness.
00:04:45.816 - 00:05:08.820, Speaker A: Okay. It wouldn't work if I was doing maybe some elliptic curve group operations here and then hash functions here. But in reality, what we're often concerned with is verifying the execution of some virtual machine that's doing the same thing at every step. And so we can exploit that repeated structure with Nova.
00:05:09.160 - 00:05:33.148, Speaker B: Exactly right. So in just the same way that the Ethereum virtual machine kind of gives you escape velocity for programming whatever you want on top of it, but the EVM itself is fully fixed and immutable and ossified and difficult to change. Here's the same thing. You can have a very fixed and ossified virtual machine that allows you to express whatever statements you want on top of it.
00:05:33.234 - 00:05:33.820, Speaker C: Very cool.
00:05:33.890 - 00:05:43.212, Speaker A: So what's sort of nova being looked at for today is the Ethereum foundation currently working on a project with Nova.
00:05:43.276 - 00:06:17.016, Speaker B: Right? So we're working with threenaf on the VDF project. So VDF stands for verifiable delay function. So we have the three letters. And function just means that it takes an input and produces an output. This is not one of the most important letters. This is maybe the most important letter, which is the delay part. So computing the output of the function takes time.
00:06:17.016 - 00:07:00.756, Speaker B: And the way that we mimic the notion of time with computation is using sequential computation that cannot be parallelized. So you have an input, you perform computation on it, which is sequential. That computation might take, let's say, 1 ns. If you do a billion such steps, then you simulate it one second's worth of delay. And the verifiable aspect means that if you're a verifier, a weak verifier, for example, a blockchain, you want to be convinced of the correctness of the output without having to do all the heavy work of doing the sequential computation. You want to be able to immediately verify the output.
00:07:00.948 - 00:07:17.432, Speaker A: So you could prove to me that something took you five minutes, but it would only take me a couple of milliseconds to verify that proof. So if I'm like a validator on a blockchain, I don't have to do something for five minutes to ensure that you were bounded by some delay.
00:07:17.576 - 00:08:07.432, Speaker B: Exactly. And the way that we use Nova for VDFs is by having a two part construction of VDF. So we start with what we call a protovdf. And the idea of a protovDF is that you have computation which is inherently sequential, where going in one direction is significantly harder than going in the other direction, and the computation is reversible. And one way to do it, for example, is by taking roots in a finite field. So if you take, let's say, the fifth root, and you just do that again, take the fifth root, fifth roots, fifth root. Taking fifth roots in a finite field takes, let's say 100 operations.
00:08:07.432 - 00:08:55.464, Speaker B: But verifying a fifth root only takes a few operations, because you only need to multiply a number with itself four times. And so going in this direction is about 100 times slower than going in this direction. And what we do basically, is that because we have this asymmetry, we stand a chance for a snark like prover to actually prove the easy direction, which is equivalent to proving the hard direction. And what we do is that we basically take batches of these fifth roots and we fold them together, and then at the end we only have to check one batch.
00:08:55.592 - 00:08:56.124, Speaker C: Cool.
00:08:56.242 - 00:09:22.980, Speaker A: So to generate the function outside the circuit, it took you a long time, but at the end you're able to hand me a snark that I can verify very cheaply. That shows that the VDF allows me to verify the other. Like, where do you see Nova playing into the Ethereum ecosystem?
00:09:23.640 - 00:09:33.720, Speaker B: Right. So one of the big themes nowadays, and is, I guess what you're working on as well at Polygon is Zke evms.
00:09:35.900 - 00:09:37.220, Speaker A: What's a Zk EVM?
00:09:37.300 - 00:09:48.300, Speaker B: Good question. So it's basically the EVM, the Ethereum virtual machine, which is snarkified. So let me write down EVM.
00:09:50.960 - 00:09:54.232, Speaker A: Ev, ethereum virtual machine.
00:09:54.296 - 00:10:30.596, Speaker B: Right. So VM is a standard term which means virtual machine. You're basically simulating something like a cpu. And e stands for ethereum. It's basically the Ethereum variant of the VM. And it was designed to be useful in a blockchain context. And what we're trying to do basically is make the job of processing transactions, or more specifically, verifying the validity of state routes that progress with transactions.
00:10:30.596 - 00:11:03.124, Speaker B: So you have batches of transactions that come in, these are called blocks, you feed them through the EVM, and that progresses the state, and then you identify the state with a state route, and you want to prove the validity of these state routes. And if we use a SNOC like proof system, we get these very easy to verify proofs that might take a few milliseconds to verify, and that bypasses all the work that you have to do to kind of naively re execute all the transactions one by one.
00:11:03.242 - 00:11:03.910, Speaker C: Cool.
00:11:06.440 - 00:11:17.930, Speaker A: So if I'm running my full node, I won't have to execute every transaction in a block. I'll just download a proof, download the updated state and check the proof. That sounds great.
00:11:18.400 - 00:12:08.330, Speaker B: Exactly right. And because the EVM is a fixed vm, it has this structure, it has these cycles that repeat and repeat. And we can use this technique of folding. And if you zoom out, I kind of think of Nova as being a pre processing gadget for Snox. So Snox is very heavy machinery, which you only want to use if you really, really need the benefits of Snox. But what you can do, basically is take this repeating computation, and then without having to deal with Snox at all, start folding and preprocessing that. And then you have a much smaller statement that you feed into your snark prover so that you get a tiny, tiny proof at the end.
00:12:08.860 - 00:12:52.372, Speaker A: That's really interesting. Maybe we could draw that out. I think that's an important point, because with snarks, we're paying for succinctness, right? Like we're paying to have small proofs. And so you're basically saying, look for ethereum transactions in a block, we don't need to generate an individual snark for each one and sort of pay the cost to shrink the verifier time. Instead, we can use folding, reduce the size of the statement that we're trying to prove until it's more efficient to sort of snarkify it and make.
00:12:52.426 - 00:13:41.620, Speaker B: Exactly. So you have two kind of tools at your disposition. One of the tools is a snark, and that's extremely expensive relative to the other thing. And basically what it gives you is compression. You can compress into something which is absolutely tiny to verify, but you want to feed as little work as possible to this compressor. And so what you have is basically a hybrid system where you combine the compression with something else, which is the folding. And this folding stage kind of dramatically reduces the amount of work that you have to do in this compression stage.
00:13:41.620 - 00:13:57.160, Speaker B: And so really, you get the best of both worlds. On the one hand, you get extremely fast proving, or should I say folding. And on the other hand, the output of the compression is a very small and succinct.
00:13:58.460 - 00:14:29.628, Speaker A: This is, this is sort of our approach. We use fry instead of Nova, but at Polygon, we're trying to exploit this trade off in a very similar way, where proving ethereum transactions is the expensive part. And so we don't want to compress, and we want to use a configuration that allows us to maximize proving time, or I guess, minimize proving time and maximize efficiency. And then we can compress at the end when we post to the blockchain and verification time starts to matter.
00:14:29.734 - 00:14:59.160, Speaker B: Right. So I guess what you're saying is that let's use two snocs for both steps, but these are snarks that are optimized for different things. This one's optimized for fast and cheap verification, this one for fast and cheap proving. But I guess the realization of Nova is that you don't even need a Snox. This is overkill. You can achieve something. You can get the power of Snox by just using folding.
00:14:59.240 - 00:15:00.012, Speaker C: Very cool.
00:15:00.146 - 00:15:03.432, Speaker A: All right, let's dig into Nova.
00:15:03.576 - 00:15:17.200, Speaker B: Yeah, so I guess we can look at some of the nice properties that folding has. One of them is that there's only multi experuniations. There's no ffts.
00:15:19.880 - 00:15:23.620, Speaker A: So why is it nice to avoid ffts?
00:15:24.440 - 00:16:06.690, Speaker B: Right. So, one reason why it's nice to avoid ffts is that you're doing one thing, you're just doing multi experientiations. And if you want to accelerate a snark, for example, in the extreme build a snark asiC, you want your ASIc to be fairly simple. So that's one advantage. Another advantage is that you don't need much memory. Like these ffts can be memory hungry, but the multi exponentiations, they're much more streamable, which is also good for prover performance. And I guess another advantage is that the steps that you're folding into each other can be very big if you need to.
00:16:06.690 - 00:16:26.776, Speaker B: And again, because you don't have this constraint on memory. And so if you take an approach where you have a very EVM equivalent circuit. That could be a humongous circuit, each individual step. And you're not limited by the memory constraints of the FFT there.
00:16:26.878 - 00:16:33.552, Speaker A: And we're also not limited in our choices of elliptic curves. Right, because we don't need multiplicative subgroups for dfts.
00:16:33.636 - 00:16:42.856, Speaker B: Exactly. Yeah, that's a great point. And actually, the curves that we are working with also don't need to be pairing friendly.
00:16:43.048 - 00:16:43.790, Speaker C: Nice.
00:16:47.280 - 00:17:21.044, Speaker B: Um, and so actually it turns out that you can use sec p, which is the exact same curve that is used today for ethereum signatures and for bitcoin signatures. You don't have to invent exotic things. One thing that you do need, however, is what's called a cycle of elliptic curves. And sec p has kind of this dual friend, which is called sec Q, and these two combined form a curve.
00:17:21.172 - 00:17:25.928, Speaker A: Okay, so how does a cycle work? Maybe we could diagram.
00:17:26.024 - 00:18:34.528, Speaker B: Oh yeah, for sure. The way that you define an elliptic curve is with basically what's called the base field. So you have a field f, and then that allows you to define an elliptic curve, which then generates a scalar field. So you have another field here, ffp. So this would be Fq, this would be Fp, and it turns out that there's an impossibility result whereby the p and the q have to be different. And so the question is, can you have two curves such that the relationship between the base field and the scalar field is symmetric for the two curves? So one curve has fp as the base field, the other one has Fq as the base field, and vice versa for the scalar field.
00:18:34.614 - 00:19:00.244, Speaker A: And this is useful because we are defining our constraints in the scalar field of the elliptic curve. So we're sort of programming in that field, but we're verifying our proofs in the base field. And so we want to be able to efficiently, we want those fields to match so we can efficiently verify base field operations of one curve in the scalar field of another, and vice versa.
00:19:00.372 - 00:19:24.236, Speaker B: Exactly. So one of the things that I haven't touched on yet is folding. Sorry? Is recursion in addition to the folding. So it turns out that when you fold a statement into another. I lied a little bit when I said you only need to verify the very last folded statement. You also need to verify that the folding was done properly.
00:19:24.348 - 00:19:24.768, Speaker C: Okay.
00:19:24.854 - 00:20:13.708, Speaker B: But checking that the folding was done properly is very, very cheap. And so what you can do is you can actually do it as you go along in your circuit, you have your initial computation, which is fairly large and then you add just a little bit of overhead to perform the recursion. And as you said, in order to verify that the folding was done correctly, you're working in the base field because you're working with elliptic curve points, but the constraint system is using the scalar field. And so really what we're going to have is we're going to have this back and forth between the two fields and get the efficiency on both sides.
00:20:13.804 - 00:20:14.448, Speaker C: Cool.
00:20:14.614 - 00:20:25.028, Speaker A: So when we're folding, maybe we could diagram that we're not sort of compressing all at once. We're like folding one into the other and we have this chain of.
00:20:25.114 - 00:21:22.330, Speaker B: Yes. Okay, so let's draw this. The idea of Nova is that you're going to have what's called a running instance. So this is the instance in which you're going to fold the statements progressively and it starts effectively empty the trivial instance. And then in red is where I'm going to be doing the real work for some repeating function f. And then the next step is to fold the real work into the running instance to produce the next iteration of the running instance. So basically we're folding these two into here, and the node here is where the folding happened, and then we're going to do more folding of real work.
00:21:22.330 - 00:21:25.556, Speaker B: And the pattern repeats.
00:21:25.748 - 00:21:42.408, Speaker A: So the red arrows are kind of that repeated computation that we sort of drew earlier. And the black is, I think it's sometimes called an accumulator in the literature, but basically tracks the progress of our folder.
00:21:42.584 - 00:22:35.150, Speaker B: Right. You could call it an aggregator if you're using the terminology of Billis aggregate signatures. Or you could call it an accumulator of sorts. Now, as we said, there is a cost to verifying that the folding happened properly. And so what we're going to do is this little trick where we're going to augment the function f to become a function f prime. So there's a prime here in green, and we're going to have f prime verify that the folding done here was done properly. And so basically each step that does real work is also checking that the folding was done properly for the previous step.
00:22:35.520 - 00:22:36.124, Speaker C: Okay.
00:22:36.242 - 00:22:52.608, Speaker B: And then, now my statement is true. My initial statement is true, which is if you verify the very last folded instance, then you're kind of unraveling everything, checking not only the validity of every single iteration of f, but also that the folding was done properly.
00:22:52.704 - 00:22:54.532, Speaker C: Yeah. Cool.
00:22:54.666 - 00:23:03.624, Speaker A: So what are some of the concrete kind of performance characteristics of Nova? You said it's really fast, right?
00:23:03.822 - 00:24:09.160, Speaker B: So there's no ffts, you're only doing multi exponentiations. And the question becomes what is the constant? So if you have n constraints, for example n r one cs constraints, you have to do basically you have to do two multi exponentations of size n, okay, so basically the constant is two, which is at least a factor of two, then the next best thing. And if you take a proof system like plank for example, or graph 16, you'll be several times faster than them. And also you don't have to do the ffts, so you're winning. And also because you don't have a pairing friendly curve, you have more performance. So you might have, let's say, two x more performance here, you might have, let's say, I don't know, four x more performance here. Here you have no ffts.
00:24:09.160 - 00:25:00.232, Speaker B: So that might give you like simplifications and a bit less work to do. And there's another benefit, which is that the so called recursion overhead is very, very low. So verifying that the folding is done properly is dominated by two scalar multiplications. Okay? And if you were to write this as a circuit, which is what we are doing here, with this function f prime, it's on the order of 20,000 gates. So it's a very small recursion overhead, which might give you a further boost here. So potentially you can have a proof system which is roughly ten x faster on the folding than if you were to use a snark.
00:25:00.296 - 00:25:17.250, Speaker A: Naively, yeah, it makes sense because we want to minimize both the cost, sort of the proving time for our statement, but also if we're doing recursion or aggregation, we need to minimize the cost of actually performing the.
00:25:17.860 - 00:26:01.040, Speaker B: Exactly, I mean, and zooming out. One of the trends in Snox is recursion. Recursion is extremely useful for many different things. It allows us, for example, to break down very large statements which are too big to true for our snark provers, and break them down into small chunks. It allows us to have distributed proving, where you give out work to different people or different cpus, or different cores within a single cpu. So you unlock parallelism and distributed proving, and you also make it potentially easier to build hardware because you can feed these chunks.
00:26:02.580 - 00:26:03.570, Speaker C: Very cool.
00:26:06.180 - 00:26:49.756, Speaker B: Let's see, what are other advantages of Nova? I guess one of them is that there's no trusted setup, so it's the transparent proof system. And the general approach is also post quantum upgradable, meaning that if you want to use lattices instead of Peterson commitments, then you get the same construction which is instead of being based on the discrete log assumption, you might base it on something like the LWE assumption. So it's a different assumption, the one which is thought to be post quantum secure.
00:26:49.868 - 00:26:58.216, Speaker A: So for the folding step, we just need some additive homomorphism or some structure, and we can get that with lavists.
00:26:58.348 - 00:27:28.170, Speaker B: Exactly. The one constraint that we need is a linearly homomorphic commitment, a vector commitment scheme. Right. Now, this technique of taking linear combinations is used, for example, with polynomial commitments. But here we're not even working with polynomial commitments. We're working with something which is less powerful, which is purely a vector commitment. So that's kind of another advantage as well here.
00:27:28.620 - 00:27:29.416, Speaker C: Cool.
00:27:29.598 - 00:28:13.256, Speaker B: Okay, so we've gone through the high level stuff. Let's start looking at some of the details. And I guess what we can do before diving in too much into the actual details, maybe we can start with a warm up example. So, let's start with very simple statements which don't give us all the expressivity of Mp, but it will give us an idea of how things work. So, remember how I said that every statement needs to have the same structure? So let's define the structure. Let's say that we have some matrix a. It could be any matrix.
00:28:13.256 - 00:29:27.090, Speaker B: It could be one that's randomly generated, for example. And we have statements of the form axi equals some vector, for example, the vector which is all ones, which I'll write as bolt face one. Now, let's say that as a prover, I claim to have two such vectors which satisfy this. I have x one and I have x two. And I'm going to tell the verifier what the commitments of these vectors are. So when I have a line on the top, I mean the commitment of x one and the commitment of x two. So the claim really is here that I know two vectors, x one and x two, that corresponds to this commitment, such that both x one and x two satisfy this.
00:29:27.090 - 00:30:15.408, Speaker B: Now, the verifier is very lazy, right? He doesn't want to do much work. And so the naive approach would be for the prover to send over x one, the verifier, to check that it matches the commitment. Same thing for x two. And on top of that, not only is there a lot of communication cost, but there's also computational cost, because the verifier needs to do this matrix operation. So instead, what happens is that the verifier says, I'm going to give you a random number. And this is a classic cryptographic thing where we have an interactive game so the prover sends these commitments. So that's step one.
00:30:15.408 - 00:31:18.416, Speaker B: The prover sends the commitments, the verifier. So the verifiers on the right side, the proverbs on the left side, the verifier just sends a random number r. And then the prover will basically take, both the prover and the verifier take a random linear combination of these vectors, so they take x one plus r x two. And the commitment corresponding to this vector is going to be the same thing, but with a hat. So it's going to be x one plus r x two committed. But because of the linear homomorphism of the commitment that we talked about earlier, this is actually equal to x one plus r x two. And so this is something that the verifier can compute.
00:31:18.416 - 00:32:00.692, Speaker B: And now the proverb just needs to send one single vector, which is the opening of this thing here. The verifier checks the opening and it can check this operation. So basically what we've done is that we've halved the amount of work. And what is the cost? The cost is basically one scalar multiplication plus an addition, which is very, very cheap for the verifier. And this technique here, what we've done is that we've folded two instances, but we can fold a million, a billion instances and still end up with this very, very cheap thing at the end.
00:32:00.826 - 00:32:26.220, Speaker A: And these commitments, we're just using the Peterson commitment to generate x one and x two hat what we're doing is we're saving the verifier just needs to check a times this term, instead of checking ax one hat and then ax two hat.
00:32:27.440 - 00:33:23.692, Speaker B: Yeah. So instead of having to check ax one and ax two individually, which requires one kind of receiving x one and x two kind of downloading all this data and then two doing the computation, you only have to download the data once for this random nonlinear combination and do the computation once. Right. Okay. So I guess the question is, how do we go from this toy example, which hopefully gives you the intuition to something more expressive? So it turns out that one of the standard ways to express an NP statement is with what's called r one cs. So r one cs, how does it work? You have three matrices. These are square matrices a, b and c.
00:33:23.692 - 00:33:58.484, Speaker B: And these are going to define the structure of your computation. And then the equation that you have is going to be a times a vector z times b times the vector z equals c times the vector z. And so this round circle here means that you're taking the element by element multiplication of vectors.
00:33:58.532 - 00:34:02.868, Speaker A: So the dot product of the. Because vector. Vector, dot product.
00:34:03.054 - 00:34:04.350, Speaker B: Exactly. Yes.
00:34:06.080 - 00:34:10.670, Speaker A: Sorry. No, not the dot product before the sum product. Yeah.
00:34:11.680 - 00:35:06.040, Speaker B: So it's element wise multiplication. So a, b and c are matrices. A times z, b times z, c times z are vectors. And this vector times a vector is iterative. Now, it turns out that this trick of taking a linear combination, let's imagine that z is going to be a random linear combination of a z one plus a z two with a random coefficient. It turns out that if you plug in a z like this, and you expand it out, you get cross terms. And by cross terms, we mean kind of when you have both indices, one and two, that are involved.
00:35:06.040 - 00:35:45.130, Speaker B: And so what we want to do is actually generalize this r one cs to something that srinaf has called relaxed r one cs. And it's relaxed because you allow two things. One is that you have an extra vector e, which we call the slack vector. And then we have an extra coefficient here, u, which is a scalar. And so now your witness for the statement is z, u and e, and the structure is still a, b and c.
00:35:47.100 - 00:35:57.890, Speaker A: And so these terms just let us cancel out the cross terms that might come up when we are taking Hadamar product of this.
00:35:59.940 - 00:36:55.330, Speaker B: Yeah, exactly. So the term e here will absorb the cross products. So it's kind of a more flexible r one cs. And this flexibility allows it to. To take the shape of what we want. So, basically, what we want, we want to be in a position whereby proving two statements of this form. So, basically, where we have a z one times b z one u one z one e one, and proving the same thing, but with a two a z two b z two equals u two cz two plus e two.
00:36:55.330 - 00:37:21.080, Speaker B: Proving these two things is equivalent to proving a statement of the form A-Z-B-Z equals U-C-Z plus e, where z is this random linear combination.
00:37:21.980 - 00:37:22.730, Speaker C: Cool.
00:37:23.200 - 00:37:30.200, Speaker A: Yeah, it feels weird to be going back to r one cs after so many years in planck constraints.
00:37:30.280 - 00:37:33.340, Speaker C: This is interesting, right?
00:37:33.410 - 00:37:57.940, Speaker B: So we started with r one cs, but it turns out it's not flexible enough to do the folding. But it turns out that you can have much of the expressivity of other things, like the plunkish arithmetization or things like pluckup using relaxed r one cs.
00:37:58.760 - 00:37:59.750, Speaker C: Very cool.
00:38:01.560 - 00:38:12.760, Speaker A: All right, how do we adapt this technique? I guess it's straightforward, adapting it to this relaxed.
00:38:13.280 - 00:38:25.772, Speaker B: It's straightforward. But really, what I have to do is kind of tell you what u and e must be in order for this equivalence to be the same.
00:38:25.906 - 00:38:26.456, Speaker C: Okay.
00:38:26.578 - 00:39:43.336, Speaker B: And you can just write it down and do the arithmetic. But I guess I can just tell you what it is like. U is going to end up being u one plus r u two. So very similar looking to this guy. And then e is going to end up being something like e one plus r squared, e two plus the cross terms. And the cross terms are going to look like a z one times b z two plus a v two b v one minus U-C-U one cz two minus u two c z one. And every single cross term are basically crossing the ones and the twos and everything cancels out.
00:39:43.336 - 00:39:48.216, Speaker B: And everything works out nicely when you plug it in.
00:39:48.398 - 00:39:49.130, Speaker C: Cool.
00:39:49.660 - 00:40:04.840, Speaker A: So when we actually write constraints for this form we're defining like our sort of program, is it A-B-C and e, or is e part of our witness? How does that.
00:40:04.910 - 00:40:07.212, Speaker B: So e, e is part of the witness.
00:40:07.276 - 00:40:07.890, Speaker C: Okay.
00:40:10.980 - 00:40:54.028, Speaker B: And I guess one important question here to ask ourselves is what is going to be the cost for the verifier to keep an updated version of Eu and Z? And actually for z and e is going to be commitments of these things, the verifier, just like here, basically the verifier is only working with commitments. These commitments are very succinct and so they're very easy to work with.
00:40:54.114 - 00:40:58.432, Speaker A: But as long as we have our linear homomorphism, we're sort of set and we don't have to keep.
00:40:58.486 - 00:41:47.410, Speaker B: Exactly. And so we can have a look how many scalar multiplications are there? Because that's going to be the dominating thing. So there's going to be one, one here where you're multiplying by r in order to update z. The addition is very cheap. You're going to have one here to multiply by r squared. And in addition, what the prover does is that it's going to send a commitment to these cross products. So we're going to define t, which is all these cross products, and the prover is going to send a commitment to t.
00:41:47.410 - 00:42:51.440, Speaker B: And so really what the verifier has to do is compute, sorry, there's an r missing here, is going to have to compute r times the commitment of t. So we end up having three scalar multiplications in the recursive verifier circuit. And this is something that I'll draw very soon. When you're folding an r one cs, something which is not relaxed but really the standard r one cs, you end up in a position where e is zero, right? Because that's the standard r one cs where e is zero and u is one. And so you end up not having to do this. This goes away in practice. And this costs about 10,000 gates constraints.
00:42:51.440 - 00:42:59.216, Speaker B: This is about 10,000 constraints. And so together the whole verification circuit is about 20,000 constraints.
00:42:59.328 - 00:43:00.084, Speaker C: Cool.
00:43:00.282 - 00:43:11.992, Speaker A: And we can sort of see why we need that repeated structure because this only works if a, b and c are sort of different for each statement. Then that doesn't work.
00:43:12.126 - 00:43:12.772, Speaker B: Exactly.
00:43:12.926 - 00:43:14.508, Speaker A: That identity doesn't hold.
00:43:14.674 - 00:43:15.390, Speaker C: Cool.
00:43:16.320 - 00:44:17.670, Speaker B: Okay, so let me try and provide some intuition here because we have lots of symbols. Let's go back to this diagram. So we have a running instance which gets updated with the instances where you do the real work. They get folded in. And where does the verifier circuit come in? Basically we want to check that the work of doing the verification here is correct. And so the function here does verification. But one thing that I haven't shown yet is how do we work with the cycles elliptic curves.
00:44:17.670 - 00:45:26.160, Speaker B: And so here really we're not going to have one running instance, we're going to have two running instances. And the way that I think about it is with a slightly staggered running instance. So doing this, this arrow, this Green arrow is inefficient because you have a mismatch in the field, the, the base field in the scalar field. And so instead we want to be working mod p here, here we're working with the prime p. Here we're working with the prime q. And what is efficient is to be, to be so here, because the only thing that you're verifying here is just the folding. I'm going to have a small arrow because there's very little work to do relative to these red arrows where a lot of the work happens.
00:45:26.160 - 00:45:56.280, Speaker B: And so this small arrow is going to prove the validity of this guy and then this guy here is going to prove the validity of the folding here and then same thing happens again. These prove the validity like this and these guys provide the validity like this.
00:45:58.750 - 00:45:59.500, Speaker C: Cool.
00:46:02.030 - 00:47:35.542, Speaker B: Great. So I guess one thing that we've achieved here, like the term that we have here, is what's called incrementally verifiable computation IVc. And one of the downsides here is that you have this extremely sequential thing going on and this is not great if you want to have lots of parallelism or even better, lots of distributed provers that all collaborate on one task. And it turns out there is a generalization of this diagram which gives us PCD proof, carrying data where you can have multiple provers kind of jointly collaborate and you can unlock parallelism. Do you want to see kind of the diagrams? Okay, so here, I guess the basic building block is this triangle here where the folding happens. And this is what basically defines the verifier circuit that powers the recursion. And the question is, can we have a more generalized triangle that unlocks recursion? And the answer is yes.
00:47:35.676 - 00:47:44.186, Speaker A: So sort of the basic building block that we use for recursion, whether we can have it have two arrows instead or sort of more than two.
00:47:44.288 - 00:48:20.486, Speaker B: Exactly. So here we have two input arrows. Why don't we try four input arrows? Okay, so let's see. We're going to have two running instances in black. And we're going to have two instances where real work happens. We're going to fold these into their own. So this is an instance folded of these.
00:48:20.486 - 00:49:14.964, Speaker B: This is the folded instance of this and this. And we're going to fold these two into yet another folded instance. And this is going to define our basic green triangle. And now basically we want to have a pattern where we can recursively fit in this triangle so that all the inputs and outputs match. And one way to do it is by basically having another triangle that come right next to it where. Okay, I'm going to have to erase, erase this guy. So we have two triangles.
00:49:14.964 - 00:49:47.590, Speaker B: Each triangle produces a running instance. So this produces a running instance. This produces a running instance. And now in order to fit the pattern, we need to have two instances that do work, two red instances like this. And now we can complete the pattern like this. And as you can see, we have a new triangle here and it.
00:49:49.480 - 00:49:50.950, Speaker C: Perfect like this.
00:49:53.560 - 00:50:07.204, Speaker A: In reality, I think we would have another parallel running instance because we need to have our cycle verifying this efficiently. But yes, not to overcomplicate.
00:50:07.252 - 00:50:07.896, Speaker B: Yes.
00:50:08.078 - 00:50:10.824, Speaker A: Okay, so this makes a lot of sense.
00:50:10.862 - 00:50:11.930, Speaker C: That's very cool.
00:50:12.560 - 00:51:10.344, Speaker B: And so basically, just to give a little bit more explanation of what's going on here, is that we have these instances that do the real work. We're folding them, but we also want to prove the validity of these green triangles. And this is done in the same way with basically instances that sit outside the triangle. And this is what allows us to do the recursion. So this red line is going to prove the validity of this triangle, the folding of this triangle. And this red line is going to prove the validity of this triangle here. And so what we've done basically is we've unlocked this very fast preprocessing stage, which is parallelism friendly and distributed and friendly to distributed provers.
00:51:10.472 - 00:51:35.488, Speaker A: Yeah, because if we're trying to process transactions in a block, we can divide the transactions in the block across many nodes that can all prove in parallel. And then we can use the folding scheme to end up with our desired single statement that we can then wrap and we can activate the compressor.
00:51:35.664 - 00:52:20.900, Speaker B: Exactly. And I guess one question we can ask ourselves is, what kind of compressor do we want at the very end? So we're working with Peterson commitments. So a very natural thing that we can use is bulletproofs. You need to do a little bit of work, basically change the bulletproof scheme a little bit to work with this extra slack vector. Srinav himself has also a variant of Spartan, which kind of works well as this final wrapper. And there's another project that the firm foundation is working on, which is to have the final wrapper be basically a stock and use fry.
00:52:23.400 - 00:52:30.372, Speaker A: What's sort of the motivation there? Because we're kind of sacrificing succinctness a little bit with Fry.
00:52:30.436 - 00:52:50.536, Speaker B: Right. So the reasoning here is that we want the final proof to be friendly to the EVM. And here we're using curves that are not EVM friendly. So we're using, for example, the pasta curves. That's what actually is being used in production, the cycles of pasta curves.
00:52:50.648 - 00:52:51.116, Speaker C: Okay.
00:52:51.218 - 00:52:51.868, Speaker B: Yeah.
00:52:52.034 - 00:52:52.750, Speaker C: Interesting.
00:52:56.160 - 00:53:08.340, Speaker A: But we could use sec P and sec Q, but then we can't really use bulletproofs with those.
00:53:08.490 - 00:53:20.992, Speaker B: Okay, so the problem with, I guess, sec P and sec Q is that even though the signature scheme uses these curves, I don't think there are opcodes.
00:53:21.056 - 00:53:21.476, Speaker A: Yeah.
00:53:21.578 - 00:53:26.760, Speaker B: So there's the ECU recover opcodes, but I'm not sure there's all the other opcodes that you need.
00:53:26.830 - 00:53:34.460, Speaker A: Yeah. That are exposed. So just being able to do, like, doubling and addition, we can't.
00:53:34.880 - 00:53:36.750, Speaker B: Right, yeah, exactly.
00:53:38.160 - 00:53:50.370, Speaker A: Wow. So we've gone through Nova, we've learned about folding recursion. We've moved from IVC to PCD. So where can people go to find more?
00:53:51.060 - 00:54:28.124, Speaker B: Right, so, as I mentioned, the scheme is fairly recent. It's from March 2021. And there isn't that much educational material. There's the paper on eprint. There's 120 minutes presentation by Srinaf on YouTube. And there's also about ten pages in the Justin Fowler's book. And I actually just learned that there is a Justin Fowler kind of study club as part of ZK Hack, where a group of people, including Justin himself, are running through the whole book.
00:54:28.124 - 00:55:12.200, Speaker B: So if that's of interest to you, do join that discussion so, yeah, that's in terms of the educational material. And I feel like one reason I was excited to talk about Nova is because not many people know about it. And I think there's a lot of potential there to help with zkvms potentially. And one of the exciting things is that not only has Srinav written an academic paper, he's also written in rust, very high quality code, an open source implementation which you can find on GitHub, GitHub.com slash Microsoft slash nova mitlicensed.
00:55:13.280 - 00:55:13.596, Speaker C: Yeah.
00:55:13.618 - 00:55:21.180, Speaker A: So Srinath is one of the best, I think, academic sort of cryptography engineers. His implementations are always super, super fast.
00:55:21.330 - 00:55:58.356, Speaker B: Right. And not only is it fast to start with, but we've also been collaborating with supranational. That has made it even faster, both in terms of having a super optimized cpu, implementations of the pasta operation, pasta cloud operations, but also a GPU implementation, acceleration for the most consuming operations, which is the multi exponentiations. So you can take something which is off the shelf and extremely fast pre processing for your zkevm.
00:55:58.468 - 00:55:59.128, Speaker C: Very cool.
00:55:59.214 - 00:56:13.944, Speaker A: So we can see Nova in hopefully VDFs in the Ethereum base chain, potentially a future ZKVM. And yeah, whatever future work sort of comes from that.
00:56:14.082 - 00:57:26.496, Speaker B: Yeah. I mean, there are some questions, I guess, for implementers. One question might be, what is the best final proof to use? Like one of the downsides of bulletproofs, for example, is you get succinctness of the proof, but you don't get succinctness of the verification time. The verification time is still linear in your step size. And so I think the work that we're doing with the Mina foundation and the Nil foundation to use basically fry as the final step is going to be a useful primitive for the whole space. This project is relatively ambitious and it's still in the work, and it might take a few more months to really be polished. But actually, the reason why we're working on frying, I guess, pasta multi exponentiations is because the Mina blockchain, its proofs, uses the pasta curves.
00:57:26.496 - 00:58:46.616, Speaker B: And we want to build a bridge between the Mina blockchain and the Ethereum blockchain. So we kind of allow us to have code reuse with the Nova project there. And I guess another question is around having to use r one cs, right, as we talked about, and people have been used to custom gates that are very expressive. People have been used to things like pluckup and for example, for custom gates, one of the things you can do if you have degree more than two gates, you can reduce them down to degree two gates, which you can do with rncs. And Srinath also believes that he's been talking to a PhD student quite a long time ago where basically the claim was that you can do pluckup queries fairly cheaply, even with r one cs, something like five constraints for one pluckup query. I need to look into the details here. And it's possible there's other teams also doing kind of independent work to kind of make these techniques be friendly to more arithmeticizations than just on cs.
00:58:46.728 - 00:58:47.436, Speaker C: Cool.
00:58:47.618 - 00:58:54.812, Speaker A: Very cool. Yeah. I really was surprised by how simple and sort of easy to understand the scheme is.
00:58:54.946 - 00:58:55.292, Speaker B: Right.
00:58:55.346 - 00:58:56.436, Speaker A: I think that's cool.
00:58:56.498 - 00:58:57.110, Speaker C: Future.
00:58:57.800 - 00:59:11.250, Speaker A: Well, great. Thank you so much, Justin, for joining me. Thanks to Anna and Tanya from the Zkhack team. And, yeah, we'll stay tuned for more on Nova. Yeah.
