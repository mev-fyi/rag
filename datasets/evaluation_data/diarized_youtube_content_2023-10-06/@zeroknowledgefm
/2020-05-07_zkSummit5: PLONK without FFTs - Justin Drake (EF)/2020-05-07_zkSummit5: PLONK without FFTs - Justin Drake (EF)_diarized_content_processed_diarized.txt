00:00:00.570 - 00:00:14.586, Speaker A: All right, well, welcome to the next session. We're here with Justin Drake, and he's going to be talking about plank without ffts. Please let us know if you can hear us just in the side chat.
00:00:14.698 - 00:00:16.030, Speaker B: Testing, testing.
00:00:17.010 - 00:00:28.120, Speaker A: If you can see us, if you can hear us. They seem to be here. Hey, there we go. Very good. Okay, so, Justin, do you want to take it away?
00:00:28.650 - 00:01:16.600, Speaker B: Yeah, sure. Thank you. Okay, so I'm going to talk about plonk without ffts. So, basically, trying to do know starting from the Lagrange basis and ending in the lagrange basis without having to do these basis changes. And I'm going to introduce some new ideas, some new techniques, but some of the ideas will mean that the construction of planck needs to be changed a little bit. So it's not strictly speaking planck, but it's more of a plonk inspired construction. And the goal is to.
00:01:16.600 - 00:02:38.510, Speaker B: One of the key themes in this talk is around ASICs trying to solve, address scalability of circuits and try and tackle large circuits and use ASICs to help us. So the talk can kind of be summarized in this one slide where basically we have these ffts, these fast Fourier transforms, and we want to try and trade them off for something else, which I call flCs, fast linear combinations. So that's maybe not a term you've heard before, but basically by a linear combination, I just mean multi exponentiation. But I'm kind of trying to use the additive notation, which I think there's probably at this point, rough consensus that it's better than the multiplicative notation. At least it's clearer. And the fast here, basically is in reference to the algorithm that we're using. So instead of doing the naive linear combinations, we would use fancy pippinger like algorithms that are quite a bit faster.
00:02:38.510 - 00:03:19.798, Speaker B: Okay, so we're going to try and take the ffts and make them into flCs. So my talk will have two parts. One will be kind of try and explain why we care so much about removing ffts, and the other part will be like an actual construction, more technical. And I will present the key ideas in the second part. So, just to give a little bit of context where we are today in snarks, I'd argue we're in a great position in terms of the verifier. The verifier is super succinct. The proof is super short, super cheap to verify on a cpu or in an EVM even.
00:03:19.798 - 00:04:31.490, Speaker B: And we also have universality for Planck, but the big bottleneck to reaching kind of mainstream adoption with many more applications is basically prover speed. And we want to try and accelerate the prover for many reasons. We want to try and have tackle larger circuits, we want to lower the proof latency. So basically the time it takes to generate these proofs relative to the negative computation. And we also want cheaper proofs in terms of a dollar cost. And this desire to accelerate provers has kind of materialized more recently, where there's a big focus on the actual constants involved in the provers. So breakthroughs such as pluckup and turboplunk, and actual applications like loop, ring and code that have invested kind of significant engineering in speeding up these proofs.
00:04:31.490 - 00:05:30.280, Speaker B: Okay. And one of the things that's probably easy to make a prediction about is that we're going to see exponential growth in the circuits that are being tackled. So we started with zcash, which is kind of a baby circuit, and gradually we're building up the size. And so here we're roughly in this column where on the order of two to the 25 gate, maybe a little bit more than that. But in the grand scheme of things, we're still tiny. And I think there's lots of room to grow to bigger circuits. And maybe a tentative roadmap here is we could try and grow the circuit size by 32 x every four years, which is, I guess, a bit more aggressive than Moore's law.
00:05:30.280 - 00:06:29.446, Speaker B: Okay, so where are we today in terms of prover time distribution? So we have basically three components in the prover. We have the witness generation, the FFTs and the linear combinations. And where we stand today is that the witness generation is kind of a pretty essential step, which is not super paralyzable. But the good news is that it's quite a small part of the prover time. And then we have the ffts, which are significant, but not the main bottleneck. And then we have the linear combinations, which tend to dominate the proverb. So one of the things that I'm suggesting that we can do is we can trade off the ffts, which is the yellow part, for more linear combinations.
00:06:29.446 - 00:07:27.962, Speaker B: And here in this trade off, it might seem kind of disadvantageous at first, because actually you have to do more linear combinations. So as a result, the witness generation becomes only 1% versus 3%. Let's say you have to do three times more in terms of linear combinations. But the good news is that you only have to worry about two things as opposed to three things. And then if you introduce a linear combination ASIC here and get on the order of 1000 x, speed up then we can get to a point where actually the linear combinations will only take about 10% of the total proof time. And really we'll be in a position where the witness generation dominates. And this is kind of where we're in the position we want to be.
00:07:27.962 - 00:09:12.062, Speaker B: And once we're there, I expect we'll find all sorts of clever tricks to speed up the witness generation, which hasn't been a focus of today's optimizations. So one natural question to ask is, are FFT so bad? Can't we just have two asics? If you want to tackle very large circuits, one ASIC that does the linear combinations, and one ASIC that does the ffts. And one kind of immediate argument off the bat is that if we go with one ASIC, as two ASIC, the ASIC cost and complexity just goes down by roughly a factor of two, maybe more, because you have more moving parts where you have to transfer the data between. So a very significant simplification, which is on the order of $5 million. Also in terms of savings, and also because we're simplifying the RND, we can deploy with just a linear combination ASIC, and get results immediately, well, get results sooner into the market and kind of have a more gradual timeline where we introduce FFT asics, maybe down the road. So one opportunity maybe, to introduce them would be, let's say in 2030, when we need them for quantum security. Because as far as I know, all the quantum secure snark schemes require mean.
00:09:12.062 - 00:10:19.890, Speaker B: If you watch Ellie's talk, he might give you some intuition as to why this is required. And the other thing is around cost. And this is maybe one of the bigger arguments for trying to remove ffts, is that basically, as we'll see in the next slide, for the types of circuits that we'd be looking at on the order of 1 billion gates and above, and the type of hardware that we have, which is namely asIcs. If we were to design both FFT and FLC Asics, we'd be in a position where 90% of our system of the hardware is dedicated to FFTs. So basically, ffts start becoming the bottleneck. It's the opposite situation to what we have today. And so we can have kind of the same performance at ten x less cost if we remove ffts.
00:10:19.890 - 00:11:45.722, Speaker B: This is the relative cost, very roughly speaking, of ffts versus flcs. So if you focused on CPUs, which is this first row, well, you'll see that on CPUs, roughly speaking, the FFTs and the FLCs are on the same order of magnitude, one is not ten times less than the other, or vice versa. Whereas if you move to Asics, then suddenly, if you look at in the ASiC world with a billion gates, then the FFTs will cost ten x the work relative to the FLCs. So, let me try and justify this table a little bit. So, the first thing that may be worth noting is that as the size of the circuit grows, basically, ffts have a natural tendency to start dominating over the linear combinations. And the reason is that there's this log squared divergence, where there's a log factor that comes from the overhead in the ffts. And there's another log factor that comes from pippinger.
00:11:45.722 - 00:12:45.614, Speaker B: It is basically the speed up of pippinger and the linear combinations. And so you see, as I'm growing, the circuit size here, the relative proportion of the FFTs versus the FLCs starts growing. And the other consideration is that cpus today are already pretty good, in relative terms, at doing the, the linear combination. Sorry, the, the ffts. And the reason is that the operations are over these fields, as opposed to being group operations. So, the arithmetic that you have to do is 256 bits, which cpus are actually fairly well optimized to do. And it's like plain, fairly simple and straightforward modular additions and multiplications.
00:12:45.614 - 00:13:51.234, Speaker B: But when you work with linear combinations, you have to do these group operations, which involves, for example, if you work with BS 381, you have to do these 381 bit arithmetic. And there's many more moving parts in a single operation. So there's more opportunity for ASICs to speed things up in the linear combinations relative to the FFTs. And also, as we'll see, there's a much more favorable parallelism story for linear combinations versus ffts. So, roughly speaking, we're looking at ten X more speed up for the linear combinations versus the FFTs if we were to go for ASics. Okay, so just to quickly go over this parallelism point. So, parallelism is kind of a spectrum.
00:13:51.234 - 00:14:54.114, Speaker B: There's things that you can't parallel at all, like the vdfs, which people would say are inherently sequential. And then you have things like ShA 256, proof of work, which are embarrassingly parallel. And the work for snarks kind of falls in between the spectrum. So we have the witness generation, which is the least parallelizable. But the good news is that the amount of computation is not too bad. We have the ffts, which are kind of in between here, where you have to do these memory shuffles, and things can be paralyzed, but there's stuff moving around and then you have these linear combinations, which are extremely paralyzable, not quite as paralyzable as shad 56, because if you use Pip and J, then there's some constraints on parallelizability. But it's much more straightforward to paralyze linear combinations.
00:14:54.114 - 00:15:48.570, Speaker B: And it's paralyzed Fourier transforms. And just to, to give a little bit of flavor of the parallelism in ffts. Probably the most appropriate project here is visic distributed zero knowledge. And they tried to parallelize the computation of ffts over very large circuits. And one of the things they did is they tried to minimize number of these memory shuffles. So they used kind of an algorithm called z's algorithm. But one of the things that we found, basically looking at these graphs, is that you have to pay a constant factor, which is not super favorable to benefit from this parallelism.
00:15:48.570 - 00:16:46.960, Speaker B: Okay. And this is kind of my final slide on the motivation, and then I'll get into the nittygritty of the construction. So if you remove ffts, you can start dealing with larger circuits, you have more scalability, and you have this AsiC strategy where you can focus on the linear combinations and not so much on the ffts. And the other final advantage that I wanted to highlight is that you can go beyond the two addicity limits that the curves have. So for example, BS 1281, I think, has a two edicity of 32. And so generally, you want to choose your toadicity to be a lot larger than the size of your circuit so that you can do these, these ffts efficiently. The good news is that in these constructions, you don't need a 200 c to be that high.
00:16:46.960 - 00:18:00.694, Speaker B: And if we go back kind of to the one key idea behind this no ffts idea is actually the root idea is a no divisions idea. So you want to try and do snarks without any divisions. And in addition to having no ffts, you have two other kind of consequences if you do a little bit more work. One is that you can have snarks, which are sparse, circuit friendly. So if you have a circuit with lots of zeros, then you can significantly accelerate the proofer, and you can also have binary circuits, friendly provers. So in terms of applications here, if we have lots of branching in the circuit, something which is traditionally known as being very expensive, then you can start thinking of doing branching cheaply. And you can also start thinking of doing and do zkvms, basically trying to do a vm inside the snark, where you might be calling one operation at a time.
00:18:00.694 - 00:18:39.620, Speaker B: And then the rest of the circuit is all zeros. You could look at accelerations there. And also for binary circuits like shaft 56. One way to accelerate things is the pluckup idea from Aztec. But here you have a potentially more generic approach to handling binary circuits. That just works for all binary circuits. But this will be for a future talk.
00:18:39.620 - 00:19:36.114, Speaker B: Okay, I'll just check if there's any questions so far. Okay, there's a bunch of questions from Vitalik. Can't you argue that FFTA six are going to be developed anyway? Because there will be people that want to use Fry? Yes, I agree. Fry seems like a general purpose enough ingredient that we can count on them existing soon enough. Yes, I would agree. And maybe stockware will be the one pushing forward these FFT asics. I think Starkware's model right now is that they want to kind of make a business model out of proving.
00:19:36.114 - 00:20:54.634, Speaker B: And so proving would be done by them, potentially. And I think, as a company, they can afford to own to rent out data centers. So, one way to scale out is just to go on AWS and just get as many computers as you can. And that is definitely one way to parallelize. But if you want to take a different approach around provers, where you want anyone in the world with a reasonable amount of hardware to participate than the incentives of stockware, well, you need some sort of different type of incentives. So Nick is asking, what is the dependence on the security parameter for FFT versus FLC? So, not sure I completely understand the question, but you don't have to play around with security parameters. As far as I can tell, the construction that we'll present now basically doesn't change the security parameters.
00:20:54.634 - 00:21:45.950, Speaker B: It just keeps everything the same in terms of security parameters. It just removes the ffts completely, and then it adds, basically, more linear combinations as a trade off. And then we have a question from Alan which says, can the FFT be represented as linear combinations? If so, why can't FFT benefit from Pippidger's algorithm? Yeah, I don't know how to represent ffts as linear combinations. As far as I can tell, the best algorithm to do ffts is n login. And if you could use pippinger to accelerate ffts, that would be a great breakthrough, but I don't know of any such things. Okay, so let's go into the construction. So, the starting point is the Planck equations.
00:21:45.950 - 00:22:15.910, Speaker B: And it looks intimidating because we have all these signals. But let's try and dissect what's going on here. So, we have the various polynomials, each of the gree at most n. So they're polynomials in the Lagrange basis. And there's a few basic operations that you're doing on these polynomials. By the way, these equations should be equal zero. So there's four equations and each one should be equal to zero.
00:22:15.910 - 00:22:57.590, Speaker B: Modulo the vanishing polynomial on the roots of unity. So what are you doing? Here you have some multiplications. So you're multiplying two polynomials and you have additions. So these are two very important operations that you're doing on polynomials. And then another kind of interesting thing to note is that you want to do shifts. Basically you want to shift with wraparound all the coefficients. If you think of the roots of unity as being a circle and the evaluations lying on that circle, you kind of want to rotate the circle.
00:22:57.590 - 00:23:27.066, Speaker B: And the final thing that you want to do as well is the prover wants to commit to the wire polynomials. So there's two types of polynomials. There's the preprocessed ones and the non preprocessed ones. The preprocessed ones are already committed to. So there's no proof of work there. But the ones that need committing you actually need to do the commitments. And so there's various ffts hidden in these operations.
00:23:27.066 - 00:24:23.230, Speaker B: And I'm going to try and convince you that you can do all these operations without any ffts. So let's look at commitments first. The key idea here is what I call the Lagrange SRs. So when you're given an SRS, the structured reference string, usually you're given the powers of tau. But instead of using the powers of tau, which is not Lagrange friendly, it's kind of in the monomial basis. What if we were to pre compute ahead of time, kind of trustlessly, the SRs and the Lagrange basis? And if we were to do this and we were given these points as approver, then it would be kind of trivial to compute the commitment of the polynomial f as a linear combination. Basically you just do an immediate linear combination of size n, which does not require an FFT.
00:24:23.230 - 00:25:17.726, Speaker B: I mean, one question that comes to mind is, given the powers of tau, is there an efficient way to compute the srs in the Lagrange basis? And it turns out there is. What you do is you consider this polynomial f, which has basically as coefficients the powers of tau. And then if you evaluate this polynomial f on the roots of unity, you get as an output on the evaluations, the Lagrange srs. And so basically there is an FFT there, but it's a preprocessed FFT. So you have to pay mlog n group operations to get this srs, but it's a one time thing. And then you cache this as approver, so that allows you to do commitments without any ffts. The next thing we want to do is additions.
00:25:17.726 - 00:26:00.160, Speaker B: So we have three polynomials, a, b and c. And the prover has given commitments to these polynomials to the verifier. And the verifier basically wants to be convinced that a plus b is equal to c as polynomials. So this would be addition, point wise addition of the coefficients in the Lagrange basis. And cat a has this kind of, this superpower which is called linear homomorphisms. And so you just need to check that the commitment of a plus the commitment of b equals the commitment of c. And this will convince you that a plus b is equal to c.
00:26:00.160 - 00:27:13.080, Speaker B: Okay, we need to do shift now. So if you have two polynomials, a and b, and the prover has given commitments to the verifier of a and b, the verifier wants to be convinced that a and b are kind of related in this shift way. Well, there's a very standard technique which is just evaluate at a random point. So the verifier will send a random evaluation point to the prover and you can just check this identity on the random evaluation point. And so the question then becomes, how do you do evaluations without ffts? And how do you do openings without ffts? So evaluations can be done using the baricentric formula. So you kind of have this formula which tells you how to evaluate f at any point given the evaluations here in the Lagrange basis. And it's a linear sum here.
00:27:13.080 - 00:27:53.154, Speaker B: There's not even a linear combination in the group. This is just a field linear combination. And for the opening proof, you want to compute the commitment of f of x minus f of z divided by x minus z. And this is what this part is here. And again, you use the same trick of the pre computed Lagrange Srs. So this is just an FLC, a fast linear combination, again without ffts. Now there is a question around divisions here.
00:27:53.154 - 00:28:28.814, Speaker B: You have to divide by omega to the power I minus z. It turns out that you can do batch operations. I don't have much time, so I'll go fast. But it turns out that the inversions are not very costly, even in the field. Okay, so far, notice that there's been zero changes to plonk. The only thing that I've been telling you about is implementation details like, if you want to do evaluations, use the baricentric formula. If you want to do this, use that.
00:28:28.814 - 00:29:04.534, Speaker B: But here, it's kind of the hard part of removing ffts. And here I need to modify planck a little bit. So, let's focus on the problem statement here. So, I have three polynomials. A times b equals x, ab, and x each of degree n. And I want to show that on the roots of unity, point wise, a times b is equal to x. So H is going to be the root of unity.
00:29:04.534 - 00:29:52.170, Speaker B: And Z of H is going to be your vanishing polynomial. Okay, so what we're going to do here is we're going to use the fry decomposition. So, what does fry give us? It allows it to take a polynomial f of degree n. And split it into its even and Od parts. And one of the really cool properties is that the even and Od parts have half the degree of f. And they're kind of naturally defined on a domain which is itself half the domain of f. So, if f is defined on n, roots of unity, f of e and fo are naturally defined on a subgroup of size of size half.
00:29:52.170 - 00:30:26.386, Speaker B: And that's because of the x squared here versus the x here. So, this is the decomposition that we're going to use. And the key idea is that, as in fry, we want to do a logarithmic number of rounds. Where every round we have a Hadamar check of size n. And we want to convert that into a Hadamar check of size n over two. And then n over four and n over eight. Until we get to a point where either we go all the way where we have a constant polynomial.
00:30:26.386 - 00:31:09.294, Speaker B: Or we kind of abort halfway through, or something like that. And the FFT is so small that it's negligible. Okay, so this is kind of the key theorem or the key lemma, I guess. So, if you have a Hallamor check of the form a times b equals c plus d. So, I've taken the x above. And just for convenience of presenting the theorem, I've split into two parts, the c and the d. And I have kind of this dual check a times b bar, where b bar is basically the rotation of b 180 degrees.
00:31:09.294 - 00:31:59.140, Speaker B: So this minus sign will basically flip the roots of unity, which is equivalent to 180 degree rotation. If I have these two Hadamar checks on h, the roots of unity, then this is equivalent to the same kind of statement. Where everything in the statement is basically half the degree. So the a prime, b prime, c prime, and d prime each have half the degree of a, b, and c, and the domain has half the size. And the reason why we have this cross product. Sorry. This dual check is so that we handle cross products.
00:31:59.140 - 00:32:49.998, Speaker B: And this is the definition of a prime, b prime, and c prime. So, basically, we do the typical thing with fry, where we decompose into two parts, and then we reassemble them using verifier randomness. So the r here is verifier randomness, and we get something which is half a degree. Okay, so, I think I'm almost out of time. So I'm just going to focus on giving a little bit of intuition behind this proof, and then I'll just stop there. So, here's some of the intuition behind the proof of this fact, which allows us to basically remove the ffts. So, we have these polynomials, which I'll represent as circles.
00:32:49.998 - 00:33:14.486, Speaker B: Basically, the circle is the root of unity. And you have the Lagrange basis. So you have evaluations on the circles, and you have the green, the blue, and the red polynomial. Turns out that circles in Google Slides is difficult. So, I'm just going to use lines. I'm going to flatten the circles. And then I use the fry decomposition, where I decompose each polynomial into two pieces.
00:33:14.486 - 00:33:47.010, Speaker B: Here, instead of using the even and od decomposition, I'll use the left and right decomposition. So, left, right, left, right, left, right. This equation here can be written as two equations. Left times left equals left, and right times right equals right. And then I want to do the reassembly using verifier randomness. So, here I've actually denoted the randomness alpha. So, I'm going to try and reduce these two equations into a single equation.
00:33:47.010 - 00:34:33.438, Speaker B: So, I'm going to have left plus alpha right here for the green, left plus alpha right for the blue. And here, I'm going to do the same thing here. Left plus alpha squared, right. So, basically, you see that this red left corresponds to the green left times the blue left. And then this alpha squared right corresponds to alpha right for the green plus times alpha right for the blue. But there's these missing cross product terms. Okay, so what we want to do is this is the reason why we want to consider pairs of Hadamal checks as opposed to single hadamal checks, is we kind of want to consider the dual check where we take the exact same thing, but we rotate b 180 degrees.
00:34:33.438 - 00:35:31.574, Speaker B: And when we rotate b, we're kind of flipping the left and the right, left and right become flipped. So, here on the blue part, the left and right become flipped, everything become flipped. And now we kind of have the red polynomial replaced by a green polynomial. And so these cross product terms can be fed back in to the original equation. And you can do the same thing here. So, basically, this is showing in diagrams how we can go from Hadamal checks of degree n to Hadamal checks of degree n over two by doing this Friday composition, and then folding everything back in to a single hadamal check, taking care of the cross product terms. Okay, I am out of time, so I will stop.
00:35:31.574 - 00:36:08.482, Speaker B: But this is kind of the summary where there's these operations that catif commitments need to do, which is commitments, openings and evaluations. And there's some two key ideas here, the Lagrange Srs and the baricentric formula. So you can do category without ffts, no problem. And then if you want to do planck without ffts, then you kind of need this Friday composition idea, and you need to avoid these divisions. And there's a few optimizations, which I'll skip. Cool. Thank you.
00:36:08.616 - 00:36:38.794, Speaker A: Thank you. Hey, Justin, one thing is you can actually stay in this room if you want. So what I'm going to do now is I'm going to go on to the next session, and I'm going to pull everybody into that session with me. But if anyone wants to remain, you can actually go back up to the schedule and come back to this session, Justin, if you want to keep going. I know there was a few questions, so I'm going to go to the next one. We'll see you there. And if people want to pop back.
00:36:38.794 - 00:36:41.230, Speaker A: And, justin, you can stick around for a little bit.
00:36:41.300 - 00:36:42.734, Speaker B: Cool. Sounds good.
00:36:42.772 - 00:36:44.158, Speaker A: All right, see you soon.
00:36:44.244 - 00:38:28.660, Speaker B: Thank you. So one of the questions which was asked is, what goes wrong when you use a left right decomposition rather than even OD? And I actually think that you can make it work with the left right decomposition. One of the things which is possible that it's possible that it's basically less efficient. And the reason is that if you go to the, you go to the, you know, to the left right stuff here, you basically. Sorry. If you go to, if you look at the Friday composition, you, you need to show that, basically you need to do some sort of consistency check where you want to show that the even part and the OD part really do correspond to the polynomial that you started with. And so to do that, you need to evaluate polynomials at the point z and at a point minus z and at a point z squared.
00:38:28.660 - 00:40:17.900, Speaker B: So you basically have z minus z and z squared and it turns out there's these nice batching things where you can batch the openings of many polynomials at these points. But if you were to use the left right decomposition, I think that you would need to basically evaluate at more points than just these three. You would need to evaluate at possibly z squared z to the power four z, or no, maybe it would be more like w times z and then w to squared times z w to the power four times z. So basically you need to do these shifts when you construct the left and right, and these shifts will mean evaluations at more points, so slightly less optimal construction if you use left and right. Okay, one question from Kev is, are there any efficiency proof of time and proof size comparisons between the original planck and this modified version? Okay, great question. Yes. So one of the tradeoffs is that instead of having a constant size proof, you have a logarithmically sized proof, because you have these logarithmic amounts of interaction, and each interaction will cost you at least one group element.
00:40:17.900 - 00:41:01.936, Speaker B: So you go from a proof size which is maybe half a kilobyte, to several kilobytes. Let's say if you try and really optimize everything, then maybe three, four 5. Good news is that in terms of computational costs, you only have to pay the same amount of pairings. So only two pairings in the modified scheme. And the reason is that you can batch everything. All the pairing operations can be batched. The other thing that you need to consider is the number of g one linear combinations or multi exponentiations.
00:41:01.936 - 00:42:14.470, Speaker B: For the prover here, the number of g one exponentations is going to grow, is going to maybe triple. But because the exponentations are quite a bit cheaper than the pairings, my back of the envelope calculation is that actually the cost for the verifier is only going to be something like two or three x than the normal plunk. If you're measuring. For example, in terms of gas usage on the EVM, this plank without FFTs will only be something like two or three times more expensive to verify. So yeah, even though the increase in proof size sounds scary, I actually don't think this is a problem in practice. And the reason is that the cost that you pay per byte to go on the EVM is tiny, and this is a reflection of real world realities. The fact that bandwidth is so cheap so oftentimes, if your proof size is 3 kb, it's not.
00:42:14.470 - 00:43:14.260, Speaker B: Bringing it down to is not significantly worse than half a kilobytes. The real killer in these snarks is the pairings, which stays constant, and then the next killer is going to be your g one experimentations. But these are still very much under control, and it only increases the computational cost by two or three. Okay, Nick asks, really a question. Does this modification yield an asymptotic improvement, improving time? Okay, great question. And the reason it's a great question is because there's different people have different views on asymptotic. So you have the complexity theorists like yourself, Nick, and then you have the kind of the engineers more like Zach.
00:43:14.260 - 00:44:26.472, Speaker B: Basically, the complexity of the whole scheme is basically going to be basically dictated by the complexity of taking these linear combinations with Pippinger. And if you ask a complexity theorist, he'll tell you that the complexity of pippenger is actually super linear. It's not sublinear, whereas an engineer will tell you that it's sublinear. I guess the answer is yes, it does yield an improvement. So you're basically going from the FFT, which is n log n, down to the complexity of pippinger. Pipinger is going to be, if I believe, is going to be something like n lambda divided by log n lambda. And I think that's slightly.
00:44:26.472 - 00:46:17.502, Speaker B: Well, it's super linear, but it's a little less than ffts. But if you take a pragmatic standpoint, and you say, okay, we have this lambda term, which is fixed, let's say, to 128 bits, and then you consider pippinger to actually be n divided by log n, then actually, the bottleneck now, from a computational standpoint, is the Witness generation, which is linear. So, just reading out the wire values, just basic things like that, these linear bottlenecks, which are fundamental of the bottlenecks. So, yeah, if you want to take a more pragmatic standpoint, then this scheme is basically a linear time proverb, Kev, is when would this protocol be more favorable compared to the original plonk? So there's going to be two factors here to consider. Factor, maybe three, even factor number one is, what kind of hardware do you use? Until. If you use cpus, then this scheme is not helpful to you. Unless your circuit sizes are enormous, which if there are enormous, then cpus just won't work.
00:46:17.502 - 00:47:07.770, Speaker B: Like your proving time will be days or months, and it's not practical. So if you're using cpus, there's no advantage whatsoever. If you use asics, then the advantages are very clear. As I hoped I explained in the talk, the. And the reason is that asics are ridiculously good at doing linear combinations. And so the next bottleneck will become ffts. And unless you're ready to do two things, one is invest in an FFT ASIC and two is to have lots of these ffts ASIC to compensate for the computational bottleneck relative to the linear combinations, then you probably want to go with this scheme.
00:47:07.770 - 00:48:00.206, Speaker B: One of the things that I don't know about is somewhere in between. If you accelerate your linear combinations with gpus then it's unclear. We need to look at the concrete numbers of what is the performance of linear combinations versus ffts on gpus. But my general rule of thumb is that as we move forward in the maturation process of this industry, we're going to have circuits of sizes, billions of gates. At that point. When you reach the billion gate level then this seems like it seems to be a very clear win. But of course it will take several years until we can match it at this .1
00:48:00.206 - 00:48:40.060, Speaker B: of the reasons being that we probably want some form of hardware acceleration to get to a billion gates. Thanks everyone.
