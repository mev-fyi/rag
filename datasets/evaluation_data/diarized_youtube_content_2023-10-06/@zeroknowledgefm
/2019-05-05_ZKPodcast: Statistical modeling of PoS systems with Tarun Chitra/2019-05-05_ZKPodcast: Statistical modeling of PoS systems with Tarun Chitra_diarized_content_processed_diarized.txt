00:00:06.570 - 00:00:18.990, Speaker A: Welcome to Zero Knowledge, a podcast where we explore the latest in blockchain technology and the decentralized web. The show is hosted by me, Anna and me Frederick.
00:00:24.890 - 00:00:35.430, Speaker B: In this episode, we sit down with Tarun to chat about his work on asics to modeling, high frequency trading, and the nuances of statistical modeling for proof of work systems in blockchains.
00:00:44.110 - 00:01:21.810, Speaker A: Thank you to this week's sponsor, Trail of Bits. Here's a little story. This fall, at the world's most competitive undergraduate security competition, students faced a test they needed to enter a giant can of La Croix, modeled in Second Life to break the post quantum cryptography defending an enchanted typewriter. Trail of bits. JP Smith was the architect behind this bizarre challenge, and his write up is on the trail of Bits blog right now. Check it out at blog trailobits.com. We're going to add the link in the show notes.
00:01:21.810 - 00:01:33.722, Speaker A: And now here's our conversation with Tarun. So we're sitting here. It's Frederick Tarun and I. Hi, guys.
00:01:33.856 - 00:01:34.490, Speaker B: Hello.
00:01:34.640 - 00:01:39.580, Speaker A: Hey, Tarun. Do you want to start off by introducing yourself?
00:01:39.890 - 00:02:16.102, Speaker C: Yeah, sure. I'm Tarun Chitra. I live in New York, and I am the founder of Gauntlet Networks, which is a company that is aiming to bring simulation tools from a lot of different other fields to the blockchain space. Basically, I've spent a lot of time being on the sidelines in this space since 2011, and then sort of at some point realized that there were just things that I liked doing that also mapped to things that I felt like were needed in this space.
00:02:16.236 - 00:03:00.326, Speaker B: So, simulation tools, it's actually a thing that's been, I've seen popping up a little bit, and it almost feels like, you say a lot of people coming to this space that have a different background than the founders of the crypto era, where it's now more like serious engineers who have built things like no existing tools and frameworks, and they come in and sort of bring that whole thing in. I know a couple of companies who are working on simulation, both like network simulation, but also like game theory simulation, things like that. What specifically are you trying to build?
00:03:00.508 - 00:04:29.518, Speaker C: Yeah, so we have basically a combination of network simulation and game theoretic simulation. So the design of our code and system is kind of based on the experiences that I had in computational physics and also in high frequency trading. If you want to properly simulate the game theory, as well as the networking behavior of a blockchain system, you have to combine sort of figuring out how to quantify interactions between different agents that have some type of utility reward potential, some type of measurement of how progress they're making locally, as well as understanding how actions they take propagate through the system, and how you can correctly model that type of propagation and those interactions, and the impact of those interactions as it flows through the system. So a sort of simple example is the quintessential selfish mining example. So, in selfish mining, what happens is a miner will withhold blocks, and if they have enough hash power, then they keep a private chain that they publish to the rest of the network whenever someone else publishes a block. And then their private chain beats the other chain, because in expectation, they have some advantage of, say, an extra block or 0.5 blocks.
00:04:29.518 - 00:05:59.198, Speaker C: Now, an interesting kind of little wrinkle to this is after the original paper came out on selfish mining, people did a number of more detailed analyses that showed that the actual network latencies that you incur in an actual blockchain system can dramatically change the efficacy of selfish mining. So figuring out how to both measure the reward of hey, I'm mining a private chain, as well as the cost of, hey, I have a lot of uncertainty from both the network and also potentially, what measurements I'm making of what everyone else is doing. And figuring out how to balance those two is a unique challenge to blockchains that kind of combines a lot of different types of existing simulation. But it's definitely a novel challenge, which is why I was drawn to it. One of the more interesting things is a lot of the academic papers, especially of the more recent high throughput protocols, have really focused on a lot of probabilistic arguments that require a lot of assumptions. And in practice, those assumptions are potentially never true for things like avalanche and algorand. And so the only way to really know that the choices you make of parameters or architectures is kind of to try to come up with models for the network, as well as the types of rational and irrational behaviors that you can think of.
00:05:59.198 - 00:06:08.014, Speaker C: You may not be able to think of every possible class of user, but your goal is to try to construct as many as possible and have them play as many games as possible.
00:06:08.132 - 00:06:26.598, Speaker A: This is something that we really want to dig into in this episode, but I think before we do that, I think it would be really cool for you to tell us a little bit more about your background. You just mentioned that you had been in physics and in finance. Can you tell me a little bit more about what you were doing there and maybe what you've learned there that you've brought into this?
00:06:26.764 - 00:07:19.658, Speaker C: So when I graduated college, I studied math and more or less electrical engineering. It's kind of a degree called applied physics, which ends up being closer to electrical engineering than the physics. And I was going to go do a phd in string theory. And basically I got this weird job at this billionaire funded private research institute called D. Shaw research in New York. And 21 year old me was like, why wouldn't you want to live in New York? Why go live in some californian suburb immediately? So I ended up working there for five years, and that dissuaded me from going to grad school. I started working on sort of a more theoretical physics project.
00:07:19.658 - 00:08:04.194, Speaker C: We were building Asics, kind of to do these large scale simulations of proteins and generic sort of condensed matter physics. So I worked on running really long simulations of glass forming materials, which, a very slight aside, your childhood has probably been filled with lies where everyone told you that there are three or four phases of nature, but in reality there's at least 30, there's probably eight in the room you're in. There are tons of phases of nature that exist. In your screen right now is liquid crystal. Glass is something that's neither a solid nor liquid. So anyway, no one knows why glasses glass is particularly weird. Any liquid can become a glass.
00:08:04.194 - 00:09:04.860, Speaker C: So that's why people wanted to study it. So I first worked on that, and then I started doing more cs and sort of more like high performance computing, kind of low latency stuff, touching our asics. We had like weird custom compiler and all this other stuff, and we did some machine learning on our hardware also. So I really did a lot of things that I feel like I would have done in grad school somehow, but I ended up doing at this place, and it was awesome. And then a lot of people from that place ended up moving to high frequency trading. So in high frequency trading, people build this kind of weird high performance computing architecture to do billions of simulations every day of how you think the rest of the market is behaving and how your trader behaves. And you kind of do these game theoretic simulations, although you don't do that many agents, you really try to focus on optimizing your agent relative to historical data.
00:09:04.860 - 00:09:26.158, Speaker C: But it's kind of an interesting challenge where it mixes things like alphago, where you're kind of like learning to play this game with something that has a lot of statistical uncertainty, like markets. So I went and did that, and I learned a lot. I don't know if I would go back to that industry, but I learned a lot about engineering, I learned a.
00:09:26.164 - 00:09:31.682, Speaker A: Lot about finance, I guess you learned a lot about modeling, or had you already learned that in the previous thing?
00:09:31.816 - 00:10:49.462, Speaker C: The previous thing taught me a lot about modeling interactions between things. And then the finance thing taught me a lot about how you model rationality and irrationality and accommodate for irrationality. In the previous episode with Axel from vest, there was a bit of questioning around the notion of like, should we only be modeling rational agents versus having some model for irrational agents. In finance, at least in high frequency, you try to model yourself as rational as possible, and then try to model the rest of the network as irrational as possible, and try to optimize your rational strategy conditional on treating everyone else as irrational. But in reality, you kind of have to have this mix of strategies. I think there's definitely some sort of impossibility theorems to whether you can ever perfectly optimize these types of games where you assume arbitrary strategies. So if you really want to measure game theoretic rewards in kind of the byzantine, fault tolerant land, there is a whole complexity class called PPAD, which gives you a ton of problems, decision problems that are potentially worse than NP that you have to solve if you wanted to kind of mix like modeling economic rationality with BFT style behavior.
00:10:49.462 - 00:11:44.714, Speaker C: So what you learn from that is that you have to do statistical methods because you're never going to solve these things exactly anyway. But how I got into blockchain is actually a weird bitcoin, really is a weird story. So in 2011, when I was working at DHR research, basically, so we were building asics, and there were very few people building asics in 2011. Pretty much the only other ASIC builders were networking companies. So, for instance, at your telecom company or your cable company, they have these huge custom hardware pieces to do ffts, fast Fourier transforms. And that type of stuff has forever been like, as the population grows, you need to optimize that more. But other than that and academic projects, there weren't many asics.
00:11:44.714 - 00:11:46.666, Speaker C: Most people went to intel.
00:11:46.778 - 00:12:04.420, Speaker A: This is actually just as a side. This is super interesting to hear about asics, not in the context of blockchain, because that is the way that I've often heard about them, and I think a lot of our audience probably has too. So that's really kind of neat that you were working on these early asics for very different purposes than the way that we think about them today.
00:12:05.270 - 00:12:29.990, Speaker C: The story I'm going to tell wouldn't be very weird right now, but it was weird in 2011, and the reason it wouldn't be weird now is now there's a lot of competition for ASIC space between self driving car companies. So for Lidar. So when I say self driving car company, I'm including big car manufacturers also. But anyone who's in that ecosystem, as well as machine learning processors competing with.
00:12:30.080 - 00:12:38.398, Speaker B: Google's TPU, everyone now has an ASic in their iPhone that does the face scanning thing to unlock your phone.
00:12:38.484 - 00:13:41.394, Speaker C: Exactly. But Apple has weird foundry relationships, so they have special access, I'm sure, to weird TSMC things before they launch. But yeah, there's a ton of even small companies have access to this. But the interesting thing about the ASIC world is if you have an order that's not super large and you want to go to the most kind of the smallest feature size, like the smallest physical size per transistor, you oftentimes can't get one of the big manufacturers, TSMC or Samsung, to listen to you unless you have an Order of around a billion. So what you have to do is you basically go through this supply chain where there are these aggregators who aggregate a bunch of orders and then make it a single order on a single wafer and then send it to TSMC or Samsung or something. So that was how we made chips. So we made like this one year, $100 million of chips, or a little less than that.
00:13:41.394 - 00:14:10.006, Speaker C: So it wasn't big enough to merit us TSMC even saying hi to us. So basically we have to go through these suppliers. And what happened in 2011 was we had this supplier that Kind of started ghosting us. We had a big order out with them of over $10 million. They just kind of ghosted us. And then three months later, they're like, oh, we'll give you like a 10% discount. Sorry for the delay.
00:14:10.006 - 00:14:21.322, Speaker C: And we're like, what? Really? Come on, you got to tell us what happened. And that was one of the first ASIC mining companies that eventually kind of got destroyed by bitmain or acquired by bitmain.
00:14:21.386 - 00:14:28.818, Speaker A: Wait, the supplier was they couldn't deliver because there was some miner that had bought all of them.
00:14:28.904 - 00:14:33.314, Speaker C: Yeah. So some minor must have paid more than what we paid, and they just removed us.
00:14:33.432 - 00:14:34.660, Speaker A: Oh, wow. Okay.
00:14:35.430 - 00:14:56.586, Speaker C: So it's kind of like they front run us. Let's say we paid x million dollars. They were like, we'll pay you two x million dollars to our supplier if you do it. Put us ahead. Which is kind of just crazy in 2011. That's crazy right now, that's kind of par for the course. Everyone you talk to in hardware seems to say similar things.
00:14:56.688 - 00:15:08.878, Speaker A: But basically what you're saying is that was the first time you were working at this Asics company because of some supplier fuck up? Kind of that you were made aware that this was happening?
00:15:08.964 - 00:15:09.694, Speaker C: Yeah.
00:15:09.892 - 00:15:18.770, Speaker A: Bitcoin was very real miners trying to grab as many asics as possible. And I guess before that point, what did you know about bitcoin?
00:15:19.190 - 00:15:57.354, Speaker C: I mean, I'd seen the paper, but I hadn't really followed the forums. I hadn't really looked at lib bitcoin. I mean, it wasn't even called lib bitcoin then, right? It was just like a bunch of random c plus plus. Actually, I think the first version was like mainly c files. I don't think there was any c plus plus then. But I'd casually go open GitHub and look at it and people had talked about it, but I never actually started really poking around and looking at things until then. And then I kind of stayed on the sidelines in the sense of I didn't really contribute development wise.
00:15:57.354 - 00:16:43.014, Speaker C: So I started just reading academic papers. I think one of Jotan Semplinsky's paper of red balloons and bitcoin was one of the first academic papers I had remember reading. And it was really interesting because it was the first paper that was like, hey, there's a lot of surprising game theory, algorithmic game theory here. That's like a little nuanced. And it was the first kind of introduction, I think, to the more academic audience of this. The paper that really had me following the literature a lot more was the ghost paper from 2015. Basically, that was the paper that led to kind of a lot of the improvements that let Ethereum get to kind of the block production rate that it had, because it basically found a way to make these uncle blocks kind of useful.
00:16:43.014 - 00:17:26.122, Speaker C: So people don't like uncle blocks now, but at that time, it was really revolutionary that you could get the speed up and get the same security guarantee by just not wasting blocks that were on a bad fork. And it came from a game, theoretic considerations. And they gave probably the most rigorous probabilistic proof that I've seen, because they actually analyzed the entire net, try to analyze the network at that time. At that time, even now, I'd say people try to avoid analyzing the networking in this space. Other than that paper, they gave a very precise networking model, which was really novel. And a lot of people use that result, cite that result. So, yeah, so I just kind of followed this stuff for a while.
00:17:26.122 - 00:18:21.478, Speaker C: And then I think the paper that really made me start writing simulation code for fun, just like in 2017 was probably Algorand because some of the claims were like a little. In particular, appendix C of that paper has like a little bit of trickery there that it doesn't seem like it's actually as stable as claimed under the type of asynchronous assumption they made. So that kind of got me into writing simulations for fun. And then at some point, I started talking to layer one projects and I was doing consulting. And then after a while I started realizing everyone kind of has these same set of problems. And there's a lot of tools from other fields for doing computer aided design. So whether it's computer aided design of an airplane wing, whether it's computer design of an economic system, there are a lot of these tools.
00:18:21.478 - 00:19:03.386, Speaker C: But blockchain protocols, especially ones that are trying to achieve high throughput, have a lot of moving knobs that you have to turn. There are a lot of parameters you have to tweak, and you're never going to get, like, perfect guarantees the way you do about the underlying cryptography. When you add in a lot of the uncertainty of the actual networking and also the actual game theoretic behaviors of maybe there's no maybe someone's not rational. So, yeah, so that kind of led to me starting gauntlet, and now there are six of us, and we're basically my co founder. He used to work on self driving cars at Uber, so we try to cover as many simulation backgrounds as possible.
00:19:03.568 - 00:19:04.522, Speaker A: That's amazing.
00:19:04.656 - 00:20:05.418, Speaker B: I find it interesting that you point this out, that a lot of people just don't do the simulations or don't actually look at the network as a whole. I found that as well when reading papers and reading research. And I find it kind of worrying because they always just say something vague, like, under this assumption, this thing holds and there's no backing of. Okay, can we make that assumption? Is that an accurate assumption to make or not? And I think a lot of these assumptions are not actually that accurate. And in a lot of cases it might be something like, yeah, we have subsecond latency. Okay, well, do you actually have subsecond latency in a super adversarial network or in a network that tries to send satellites to space to communicate blocks? Maybe that's not actually true. So, yeah, it's tricky, and I'm excited to see this sort of simulation space kind of flourish and come up to speed.
00:20:05.418 - 00:20:41.034, Speaker B: If I understand correctly, you've looked at quite a bit at like, proof of stake. And when people talk about proof of stake, especially like on the consensus layer. It's always this hesitation and people saying that it relies on these game theoretic principles. I don't actually know if those are true. It's so much more complicated than proof of work, and I don't actually know if this is secure or not. Do you think your simulations, or simulations in general, could help bring some confidence to whether or not proof of stake actually works?
00:20:41.152 - 00:21:24.370, Speaker C: Yeah, absolutely. I think there are a lot of ways to show different types of metrics for incentive compliance, and the way, at least at a high level, that I like thinking about. The difference between proof of work and proof of stake is proof of stake is collateralizing a lottery ticket with energy, whereas proof of stake is collateralizing a lottery ticket with a digital asset. And the harder part about reasoning about that is with the energy. It's like, kind of a one way function. It's irreversible, right? It's a physical process that, you know, just strictly increases entropy, whereas with the digital asset, you don't actually know that. And the key is, like, you don't want it to be reversible, but you have to spend all this effort, like, trying to force people to coerce the irreversibility of the asset.
00:21:24.370 - 00:22:24.422, Speaker C: So, I think simulation helps you a lot in trying to both measure security measured in this kind of sort of high level notion of whether a digital asset can be made irreversible, but also in kind of helping one think a lot about how the actual distribution of stake looks. So, in proof of work, right, the distribution of stake is really your distribution of hash power, what percentage of hash power you have. And the genius of bitcoin was basically to come up with a simple online estimator for how much you think the rest of the network hash power is. So difficulty adjustment in bitcoin is basically you constructing a really simple statistical estimator of what you think the true network hash power is. And based on that, estimating what the difficulty is. And everyone who's honest does the same thing. In selfish mining, what you actually do is you withhold something so that everyone else can misemates the difficulty.
00:22:24.422 - 00:24:01.354, Speaker C: And a lot of the follow up papers to selfish mining basically prove that when you make the most of your returns in selfish mining is around one of these anomalous difficulty adjustment periods, where difficulty is kind of this geometrically decaying curve, and we're approximating it by this linear step staircase. And those deviations are when the worst type behavior happens. For proof of stake, the hard part is that we're doing many of those types of approximations with proof of work. The difficulty adjustment is kind of like the crown jewel. And I think that simulation helps you figure out these types of things of like, am I approximating this curve really well? Because you can show that your system asymptotically can tolerate maybe in the limit of an infinite number of transactions, or like a really high transaction volume or really high number of shards, or some type of limit that it actually obeys these properties, which is what a lot of the papers do. But in practice, people are using some type of finite approximation method, and in pretty much every other field, you have to test those, to get quantitative bounds on those, you have to do some type of testing, whether it's Monte Carlo, whether it's to some extent fitting a statistical model to it. And in the case of proof of stake, a really cool thing is that monitoring the evolution of the stake distribution is built into the system, whereas in proof of work, it's kind of not as clear, because you don't actually ever know the true hash power.
00:24:01.354 - 00:24:47.800, Speaker C: You don't actually know the true apportioning at any given time. You're kind of guessing, you're estimating it via difficulty, but someone could be withholding a bunch with proof of stake. You actually do know the whole stake distribution. And then you can actually start to talk about modeling, like how close is this to random? Does this look like a certain stochastic process that we know represents a random stake distribution, or a sort of random stake distribution, or a less random stake distribution? And you can ad hoc go back and analyze the entire stake distribution and re optimize what you do in the future. And that to me is the fact that you have the ability to evolve the security model in proof of stake, that you sort of don't in proof of work, because the data is kind of hidden in terms of who actually is contributing hash power.
00:24:49.550 - 00:25:06.538, Speaker A: So, this is interesting that just before you mentioned statistical security, but when we think of security, usually in this space, it's code security. Do you see maybe too much of an emphasis on code security? Do you think that people aren't paying enough attention to the modeling?
00:25:06.634 - 00:25:56.250, Speaker C: Yeah. So let me give kind of like the example, quintessential example of this outside of the blockchain space. So, in trading, algorithmic trading in general. So like when machines are doing all the execution of trades, one does not release a trading strategy because the code is correct. Let's suppose that you have a trading strategy that just says, add up the last ten prices, divide them by five, and if that's greater than one send a trade. That code could be perfectly correct. It's a very simple program, right? But it actually may just perform poorly because there's actual raw market data and you're interacting with everyone else's utility function, reward function, whatever, right? And someone else can realize that you're doing that and then do something that's exactly the opposite in trading.
00:25:56.250 - 00:26:51.406, Speaker C: Kind of one of the crown infrastructure jewels that people build is called backtesting software. And backtesting software lets you take historical data, replay the historical data, and inject the things you're trying, like where you placed an ad order, where you canceled, where you know what price you bought at a lot of different things, or maybe you bought something in a correlated product. It basically lets you statistically check, like, on average, over all the historical data. Even when I shuffle it and do all these other things, this thing has an expected return of X or Y. And if you think about it, things like selfish mining are giving you bounds on expected rewards. They're not saying they're going to work every time, even though the code is not supposed to let you do it, there's still kind of this hidden state that you didn't think about. And in real life systems, when you're building these engineering systems, there are oftentimes you cannot really predict all the states that your system is going to go through.
00:26:51.406 - 00:26:57.490, Speaker C: And your best bet is to kind of come up with these statistical methods for doing this.
00:26:57.560 - 00:27:07.318, Speaker A: But do you still think that there's not enough focus on this? Do you think that code security and audits, do you think that companies tend to invest more in that direction right now?
00:27:07.404 - 00:28:43.806, Speaker C: Yeah, so I think they invest in that direction partially because there's not as much use yet. I think once you have start having more significant use of a token where its security is controlled by not only the on chain security of how much does it cost to do a double spend, but also the off chain security of how much collateral can I short this asset by in a derivatives market with low leverage costs, the moment you start adding in all of those factors, you start saying, like, okay, we can't really just model it simply, but there's just not enough use such that if I want to buy a short ethereum derivative that is close to the size of the market, like if I want to buy a $10 billion ethereum short, it's impossible to do right now, right? Basically, no one would underwrite that and it just never would really work out. But if there's a lot of volume on these things, you will actually start to see short markets in the normal markets, that happens, where the derivatives market can dominate the underlying. The biggest example of this is the S and P future, where the S and P future trades ten times the volume of actually the S and P 500 itself. That ten is fluctuating, but it's significantly more, it's a multiple. And that's just because you have lower collateral costs. To get $100,000 of future exposure, you may have to put up $5,000 of collateral.
00:28:43.806 - 00:29:23.610, Speaker C: And that happens once there's a lot of utility for something. You'll start seeing those derivatives markets grow as much as the underlying, and you're now going to have to expand your definition of security, and that's going to be statistical. So I think in the beginning, the code audits are super important, but they're complementary because you need to make sure your code is running correctly. That's just like a given. But after that, now you have to start thinking about these exogenous notions of security and exogenous notions of how people are going to interact with you. And that's really where the value of kind of these techniques for finance, or self driving cars, or even fluid dynamic simulations.
00:29:23.690 - 00:30:25.538, Speaker B: This is something that comes up when talking about proof of stake specifically, like all the time, because you always have this notion of you put up a stake and if you misbehave, that's slashed and you lose that stake. But what if you short on an external market, and so you put in a huge short and then you misbehave so that the price goes down, and therefore your short gains you more money than your slashed stake, then you're still net profitable by misbehaving. And the best answer I've heard to this so far is just like, yeah, we'll have to make the stake large enough that that secondary market can't outweigh it, or that we assume some sort of behavior of users on this network to not let the price crash, or. I don't know, I never heard a really good argument for it. It's just these kind of weird assumptions around how it's going to play out.
00:30:25.624 - 00:31:08.126, Speaker C: Yeah, definitely. I mean, there's kind of a weird hidden hand that's like holding this from not happening right now, which is bitmex fees and derivatives fees. The only derivatives market with a lot of liquidity, in actuality is Bitmex. And BitMex's fees are like daylight robbery compared to the normal centralized markets, mainly because they don't have any viable competition. People have tried but have not been able to successfully get market makers to show up. So the fact that the fees are so high means that the entering secondary market transactions costs you. You have a high fixed cost for some potentially large variable costs, but also potentially large negative costs.
00:31:08.126 - 00:31:10.878, Speaker C: So people don't want to take that uncertainty risk.
00:31:10.974 - 00:31:40.470, Speaker B: I think that's the best argument that I've heard against it, is the uncertainty risk. So you can argue that, yeah, even if you were able to take a big short and blah, blah, blah, at the end of the day, you're never actually sure that you're going to come out on top. Like when you're making this play and misbehaving, getting slashed, you're assuming that the price will drop enough, but what if it doesn't? And so you're taking on this huge risk. And the argument is that people aren't willing to take this risk.
00:31:40.550 - 00:32:15.830, Speaker C: Exactly. So in trading, people would always ask about this thing called the St. Petersburg paradox, which is this kind of famous game by Daniel Bernoulli, who's this famous mathematician and physicist. He basically said, suppose you have a game where you flip a coin. If it's heads, I double your money, and if it's tails, I take away half. If you write out the expectation of this, it's going to actually be infinite, of how much reward you get, because it's going to be one half times two plus one, four times four plus one over two to the n, times two to the n, that's infinite. But the variance is infinite too, and it actually grows faster than the mean.
00:32:15.830 - 00:33:14.054, Speaker C: And so the moral of this thing is that you actually have to somehow measure how fast your variance of your return grows relative to your mean. And in normal portfolio theory, that's something called a sharp ratio. So the simplest way to really start quantifying these types of adversarial behaviors in these systems is to start developing measurements of volatility, or uncertainty that a certain strategy takes. So even if a strategy is dominant in the sense that it's mean, goes infinity faster than everyone else's mean, if they have to take a huge amount of uncertainty, then they probably won't do it. And in derivatives markets, everything is about measuring those uncertainties. And the cool part about the blockchain space, especially as people are going online making more decentralized derivatives, is that there's an opportunity for kind of a new version of black shoals, a new version of how people measure volatility, to show up. Because now your volatility depends on both the networking and the on chain asset.
00:33:14.054 - 00:33:46.422, Speaker C: And the correlations to the onchain asset. And there's a sense in which proof of stake is really the financialization of proof of work. When you make a bond for the reward that you get in the future and have a slashing condition, you've kind of turned a bond into an option. And if you start actually trying to take a lot of the normal financial modeling for different types of assets into proof of stake land, you start to see that there's actually a lot of similarities and you can actually learn a lot about how to design these things from those.
00:33:46.556 - 00:33:53.810, Speaker A: Frederick, in your example, you mentioned that if someone flashed the price of the token goes down or the market shrinks.
00:33:53.970 - 00:34:04.550, Speaker B: If they misbehave, then one could expect, like if they misbehave by including a double spend, for instance, would the price of the token go down because there is now a double spend in the blockchain?
00:34:04.630 - 00:34:07.574, Speaker A: Yeah, like how there was a double spend in etc.
00:34:07.622 - 00:34:10.618, Speaker B: And the price didn't go down. So maybe that's not a problem.
00:34:10.784 - 00:34:21.920, Speaker A: I was going to ask you, is it just because something bad has happened to the network and we are expecting the price to go down, and not irrationally, only go down by 7% and I think then bounce back?
00:34:23.730 - 00:35:19.582, Speaker C: There are also lots of interesting attacks against, sorry, there are three attacks that people already talk about. Number one is nothing at stake, which is that it doesn't cost you to vote on a bad fork, whereas in bitcoin you're spending hash power to vote on a bad fork. Number two is long range attacks. In the early part of the stake distribution, there might be a few people who have overwhelmingly high stake in the system. You bribe them to kind of like make a really long chain, and because they have all the stake, they can notarize everything. And the third is a grinding attack, which is you have all of these random number generators that you need to draw from this distribution. But what if someone has high compute power and can basically sample all the random numbers and basically figure out which random number seed gives them some slight advantage? So for the first two attacks, suppose the liquidity cost, transaction cost was, let's just say zero for this thought gadonkin.
00:35:19.582 - 00:35:39.974, Speaker C: Basically transaction cost is zero. You actually want to always vote on all, nothing at stake, possible branches. You just make sure that you have an outstanding future that you roll over. It doesn't cost you any money to roll over the expiration date under this assumption of no transaction fee. And the transaction fees are very key to whether this works or not.
00:35:40.092 - 00:35:41.798, Speaker A: But in this case there are none.
00:35:41.894 - 00:36:31.242, Speaker C: Yes, in this case there are none. So you basically can commit nothing at stake attacks all the time. You don't mind getting slashed because you're basically making up in the other market. But the more interesting thing is that all of these thought experiments run in this world, where people assume that the underlying is independent of the derivatives market. There's enough infinite liquidity in one and finite on the other or one way or the other, but they're actually intrinsically linked. And so that's really where it gets interesting in terms of how you have to model different users, because sure, I took a short position in the derivatives market. There's a counterparty there, and that counterparty was able to lend this asset, which means they held this asset for some reason, which means they have some expectation of it, which is correlated to the actual underlying price.
00:36:31.242 - 00:36:56.706, Speaker C: So these kind of complicated loops are actually very hard to prove things about. It's very hard to prove a formally verifiable theorem about these types of systems, because they depend on a lot of randomness. And we don't know the correct thing we're drawing from. We don't know the space of possibilities. To say, we put this probability distribution on this space and we took a sample from it. In real life. What happens is those probability distributions are changing a lot over time.
00:36:56.706 - 00:37:25.770, Speaker C: And certain things that were accessible at time t are not accessible at time t plus 100 years. So there is some intrinsic security, even in the nothing at stake world that comes from transaction fees. So modeling that type of stuff is also very important. A kind of related note is that high frequency market makers in the equities markets would not be making any money without exchange rebates.
00:37:28.110 - 00:38:08.540, Speaker A: What I really like about what we're talking about is it does hearken back to the conversation that we had with Axel that you just mentioned earlier, and that was an episode that we recorded, I think, back in October, where we went through the fundamentals of POS and we talked about some of the attack vectors. Do you feel like in bringing a lot of these models and these modeling concepts to this space, do you find the space complex? Or is it quite simple compared to these other, maybe more established industries where you require modeling things like self driving cars or physics modeling with asics that you were doing back in the day?
00:38:09.550 - 00:39:25.886, Speaker C: I think the modeling is more complicated, but the technology is simpler, because in all honesty, there just is not a focus on low latency or high throughput yet, because people are still trying to figure out what cryptographic primitives they want to use. Kind of what architectures they want to use and how they're going to construct these things. It's more like in the space of people are still designing stuff so they don't really know there's nothing to optimize towards because we're still kind of trying to figure out the main first cut. But I do think that the desire to get to high throughput, especially with this desire to get visa level transaction rates out of public blockchain systems, will basically require taking in all of that knowledge. People are now starting to be like, oh wait, maybe we shouldn't use raw TCP packets when we're sending blocks. We should actually just have UDP streams and do what Google does in Chrome to make buffering faster Google quick. So like, I've heard a bunch of layer one projects talk about like, oh, we're now rewriting our networking stack and doing weird custom UDP stuff.
00:39:25.886 - 00:39:39.478, Speaker C: So it's getting to that point. I think we're at this inflection point where now we're starting to crystallize on the design space and we're kind of getting to the optimizing time and that's what we're kind of really looking at it.
00:39:39.564 - 00:40:26.478, Speaker B: I think you need to get something out and start experimenting and do simulation and analysis in parallel. Like you were saying, switching from TCP to UDP streams or you even shouldn't do that in your first cut. It just don't optimize. What you don't know is going to be a bottleneck. And I think that depends also very much on what kind of scaling design you're going for. Are you trying to get as many transactions as possible on one single chain? Then you need to do one design. If you're trying to do sharding and to sort of break up, you can have pockets of high or like pockets of low transaction, but the system as a whole has a lot of throughput.
00:40:26.478 - 00:40:46.406, Speaker B: Then you have a completely different design again. But I think ultimately, even in a sharded system, eventually you want each shard to be very fast. So you then go back again to trying those low level optimizations. So yeah, I agree, it's so early that we haven't really even decided on any particular architecture.
00:40:46.518 - 00:41:27.714, Speaker C: But that being said, in the proof of stake land, there is a lot more optimization there because people who have proposed protocols have a ton of parameters in those protocols for different types of malicious behavior, you are deducted. This and the reason simulation and optimization tools are really important and interesting, I think to a non specialist, I think the easiest way I've always thought of explaining this is, in the US, we have a recycling tax. So when you buy a bottle or a can, you pay $0.05 on purchase. And then when you recycle it, you get the $0.05 back. So that value, that $0.05,
00:41:27.714 - 00:42:12.050, Speaker C: actually varies a lot from state to state. But you now do the thought experiment of, how do you choose that number? Let's suppose we made that tax $5. So that's significantly more than any beverage, right? Like a can will cost you, let's say, a dollar. So what do you incentivize? You incentivize people to a double spend. If the recycling tax for slashing gives you a reward of $5, you are much more incentivized to break open whatever the recycling machine is and try to figure out how to send the same can in a bunch of times. And you also probably cause demand destruction, because you just made the prices go up five x for a can. On the other hand, if you make the tax a millionth of a cent, people are not incentivized to actually go recycle.
00:42:12.050 - 00:42:49.646, Speaker C: Or actually, what you see in New York, for instance, is that homeless people collectively go into trash cans and find all the cans that people just threw out and then go recycle them. At some point, that's not incentivized. So you have to somehow choose these parameters such that you get behaviors where people are really dissuaded from being too far from the rational assumption, because the uncertainty is just way too high. And so that's the stage at which I think proof of stake is now kind of hitting this optimization, the switch from architecture to optimization.
00:42:49.758 - 00:43:10.890, Speaker A: Also, I guess both proof of work and proof of stake would be modelable. But here, because it has nothing to do with this sunk cost of electricity, there's a lot of, like. It's far more in the pure game theory, I figure, because it's just like numbers, right? It's just like slashing amounts.
00:43:11.230 - 00:43:36.350, Speaker C: Yeah. That's why I say that proof of stake is just the inevitable financialization of proof of work. A lot of diehards would say that's a bad thing. I don't necessarily think that's a bad thing. I think it's just like, in order to get throughput, you have to give up something. And what you're giving up is you have to now control this entire lifetime of the asset, as opposed to lifetime up to electricity burn.
00:43:36.510 - 00:44:07.286, Speaker B: The only thing I regrets with the switch from proof of work to proof of stake is you're removing this cyberpunk element of using your computer to heat your apartment to get some cybercoin thing. And yeah, you don't have this cool sense of buying a bunch of hardware and sitting in your basement and tinkering. It's more just like buy some asset on an exchange and put her up for stake.
00:44:07.478 - 00:44:47.606, Speaker C: Yeah. Well, I guess the more interesting question also, and this is something that I think simulation ends up being useful for too, is token distribution models are now getting wacky, and the token distribution for proof of stake tokens and also sort of layer two things that look like proof of stake, like tcrs, sorry, token curator registries completely determines whether you hit a stable equilibrium or not. No matter how rational everyone is, if you give all the tokens to a small number of users, it's inevitably going to have this plutocratic effect. And so this is another thing you have to design, whereas energy is not quite that somehow nature gave us whatever distribution we have.
00:44:47.628 - 00:45:02.234, Speaker B: And that's it going back a little bit to engineering. I want to focus in a little bit on this. What are you guys actually building and what are you building in? And how do these simulations actually run?
00:45:02.432 - 00:46:04.210, Speaker C: So we have a c plus plus binary. We have a way of specifying the networking of the whole system. We have a way of letting you specify ddosing, so like removing parts of the graph, we also have ways of specifying a compute model for each agent in the system. And we're making a domain specific language for defining how an agent actually behaves. So it will be a scripting language that we compile down to our c representation. The closest analog I can give you to that is there is a custom compiler from Google for Tensorflow called XLA, which they open sourced recently, but it wasn't open source for a long time. That basically is a way to take the data flow graphs from say, tensorflow run, or whoever's generating your deep learning dependency graph and figuring out how to map it down to kind of like a machine optimal thing.
00:46:04.210 - 00:46:30.822, Speaker C: So we're working on that. So that with the idea of trying to get to 100,000 agents and 100,000 blocks in under ten minutes with agents of reasonable sophistication, they're not like stateless, they actually are evaluating some. And that's a single core for a single parameter.
00:46:30.886 - 00:47:16.018, Speaker B: Let me see if I get this straight then. So basically you define your network and your agents in this domain specific language. It compiles down, it gets executed by your c plus plus binary. But how do you define the behavior of the agent, then if I'm thinking about how would I simulate parity ethereum, for instance, I feel like I would actually need to run a parity ethereum node because it's so complex and how the transaction queue works and what type of transactions and how long they take to process and everything depend a block. Import time is a hugely important part of propagation time, for instance.
00:47:16.194 - 00:48:22.010, Speaker C: So the idea is to take as much of the actual raw timing data. The data we're using right now is just stuff from Falcon. So Falcon is this bitcoin and ethereum forwarding project that kind of turned into this company called blocksroute that's building this forwarding thing, but they had public data on actual latency numbers. Our goal is to figure out how to make a probabilistic generative model that generates how these things look without all of the fine details. With the idea that if you have a generative model that you're continually training to real data, then the generative model is removing some of the fine microscopic details, but getting the high level details that give you kind of the correct average statistics so that you can actually measure variances and means. But you're not going to necessarily get, like, exact timing behavior correctly. You want the moments of the statistical distribution to be the same, or roughly approximate versus things that are totally, totally random.
00:48:22.010 - 00:49:26.266, Speaker C: So the key is not to get every little piece of a client correct so much as to get the statistical properties of networking correct. So, one result that kind of led me down this path was I started simulating bitcoin on different network graphs, and I was able to kind of cause a phase transition under some conditions. So what do I mean by that? So take a graph of a bunch of miners. Each of them is growing their own bitcoin tree. So I say tree here because I'm including all the forks. If you look at the heights of those trees, if you can basically construct graphs whose kind of like max cut has very high weight, such that you basically get the heights to diverge statistically. So the height correlation function, like, even if the heights are close, they're actually fluctuating around each other in a way that's uncorrelated, which means that the two sides of the graph are basically on different forks, and they never revert to the same fork, even though it's like having this constant reorg thing where we're always on the opposite reorg statistically.
00:49:26.266 - 00:50:23.282, Speaker C: And so once I saw that, I was like, okay, so clearly you can already get a lot of these effects if you remove some of the microscopic details, just from trying to simulate the latencies between clients and between miners. And that type of stuff is where the physics analogy comes in, because the simulation I did, the first kind of crappy, slow bitcoin simulation I did, looks a lot like simulating something called the icing model in physics, where you have a grid of points. So the Ising model is a model for magnetism. So, basically, it's a model where you have a grid, let's say it's an n by n grid, and at each grid point is a spin, which is just a plus one or a minus one. And you provide some local rules for interacting, how those spins are together, where you say, opposites attract, alikes repel. And this is a very local rule. And you can kind of measure the energy of the system.
00:50:23.282 - 00:51:25.190, Speaker C: And what you find is, when you write the energy, you add in a temperature term, and what you find is, like, below a certain temperature, the only state that you can ever reach is everyone is aligned, which is when you're magnetized. And then above a certain temperature, there's enough kind of weird fluctuations that it looks like it's totally random. And so, with bitcoin, you can think of everyone agreeing on the same everyone in the whole network agreeing on the same branch as being in that magnetic state, where everyone is kind of, like, aligned in the same direction. And so I was kind of trying to find ways of computing metrics that would show for bitcoin. Are there adversarial graphs for which that happens? Or can you show that for random graph ensembles, like random networks for different models of the Internet? Like that would or wouldn't happen. And the fact that you can get those type of results from removing some of the microscopic details suggests that you can get a lot of the averages. You need to get statistical results about the game theory without actually having to get every single timestamp correct.
00:51:25.260 - 00:51:33.350, Speaker B: So, it's sort of like you're using some real world data to test the protocol, but not any specific implementation of that protocol.
00:51:33.430 - 00:51:33.626, Speaker C: Yes.
00:51:33.648 - 00:51:52.894, Speaker B: So, I wouldn't use your library to try to optimize parity bitcoin. Run this thing, fix a thing, and run it again and see if it was faster. It's not that type of benchmarking or simulation thing at all. It's much more about testing the protocol, game, theoretical properties, that sort of thing.
00:51:53.012 - 00:52:29.066, Speaker C: Yeah. And also getting kind of slas on things. So you can now measure expected round trip latency. This type of testing, minus the financial part, is not that different to how Google tests spanner where they actually have a simulation where they remove the actual spanner client, but they try to replicate all the messages spanner sends and then measure what is the expected latency when someone goes on Google Cloud and performs a query. It's just a way, of course, screening a little bit of the detail away, but making sure you have just enough to not distort certain statistical properties such that you can measure how the game theory is working.
00:52:29.248 - 00:52:38.586, Speaker A: So this project Gauntlet came out of this work, I guess. Can you tell me a little bit about what's next for you? What's next for Gauntlet?
00:52:38.698 - 00:54:02.554, Speaker C: Yeah. So, right now, we're just honestly focusing on building out the infrastructure, getting all the statistical tools that are generic to all protocols, implementing a bunch of basically all the tools needed to implement most proof of stake protocols, making sure that we get correct latencies, correct data, get kind of all infrastructure work. We, as a company are basically trying to more or less have kind of like a developer tool product where you can put it in your CI, and we'll go run the simulations. We are really trying to make sure that we get all layers of the stack. So we're also trying to get to the point where we can do statistical testing of contracts, not just the layer one protocols, especially because a lot of contracts just end up recursing layer one protocol mechanisms within them, like tcrs, kind of are a weird staking game, where instead of one staking ensemble, you have a tree of staking ensembles. It's like the fact that there's actually a lot of recursion and the types of algorithms that are being used suggests that if you can really figure out how to categorize these, you can do this. So, yeah, we're just working kind of more directly with layer one projects right now and trying to make sure our software works and then figuring out where we go from there with that, with.
00:54:02.592 - 00:54:14.830, Speaker B: The code and the tools that you're building, what's the plan going forward in terms of what do you intend to open source, or how do you imagine that people actually use this and find it?
00:54:14.900 - 00:54:59.974, Speaker C: So we're planning on open sourcing some of it and then basically selling just the service of us running it more, because there's, like, a lot of annoying distributed infrastructure. You would need to actually run it and get the statistical results and get the dashboards and stuff like that. We'll open source stuff so it gets audited, but then we'll sell the service of running it. And our goal is to really be a neutral third party so we don't want to be partial to any particular project or any particular contract. So we promise never to hold any crypto assets. So even if we are paid in crypto assets, we will not hold any. Because I think if you're trying to do something like this to give people this guarantee of security, you really have to be impartial.
00:54:59.974 - 00:55:02.390, Speaker C: And it's very hard in the space if you're holding.
00:55:02.470 - 00:55:07.870, Speaker A: Yeah, tricky. I mean, in other markets, how do they actually create impartiality?
00:55:09.490 - 00:55:47.510, Speaker C: It is very hard to actually be kind of a neutral third party. You only develop that by engendering the trust of every person that you end up working with individually. I think that's, from our perspective, really important because we really view the whole ecosystem and the whole space as providing this really amazing tool for reinventing law and how people interact with the law. But we just don't really have any strong opinions of which thing will work. We kind of are like, whatever the data says is what will really follow. And so that's kind of our core. Those are our core values.
00:55:47.590 - 00:56:28.434, Speaker B: Cool. Anna, your point of how is this handled in other industries? I think that's actually one of the things that I like about the crypto space is that people, there are a lot of players who actually try to be impartial and try to say things like, we don't hold any crypto and we're just trying to do this or that. Whereas when I worked in the financial industry, financial advisors are the least impartial. They are so partial. They are being directly paid by funds to shield those funds. The same problem exists everywhere and it's not always addressed.
00:56:28.562 - 00:57:04.622, Speaker C: Yeah, one thing I actually find kind of funny in the crypto space is that people in crypto land seem to get angry about these coin. Do you guys know about FCoin? It was this exchange that paid the transaction fees back to miners by 150%, and that incentivized miners to just watch trade themselves because they would get more. This is like the short market thing, except on steroids. And so it got a lot of transaction volume on the exchange for a little while, but then they removed the reward and no one did it. So everyone, people were writing all these blog posts. They're like, oh, this is crazy. There's no neutrality.
00:57:04.622 - 00:57:42.090, Speaker C: The exchange is partial to miners, but all markets that have any significant liquidities offer some type of tiered system where the more volume you have, the easier it is for you. And there's a rich getting richer phenomenon, just like in proof of stake, because someone took that early risk to get all that volume. Someone took that early risk to become the delegator. So there is this kind of delicate balance between ensuring there's liquidity and pure uniform genie coefficient zero, sort of. So that's kind of the cool part about the space, is it's exploring those boundaries.
00:57:42.170 - 00:57:46.578, Speaker A: And I think maybe to wrap up what's coming up in the space that's actually exciting for you.
00:57:46.664 - 00:58:45.060, Speaker C: I'm just really excited to see these experiments attempt to get scale. I think one of the things, the things that really stood out, ignoring sentiment on definity. I do think that threshold relay is really an amazingly simple way of generating randomness, and it's kind of shocking that you get that kind of scaling property for free. So I'm really excited to see, will it actually be possible to do this distributed key generation? All these little technical details that have existed in academia for a long time are going to be tested by reality. And to me that's really exciting because I believe some of them, at least one of them is going to work. I don't have a lot of confidence that because there's so little data on how these systems go from theory to engineering, I just don't feel the confidence to say any of them is better than the other. I'm just excited to see the data.
00:58:45.750 - 00:58:50.486, Speaker A: Well, hey, I want to say thank you so much for coming today to talk to us today.
00:58:50.668 - 00:58:51.766, Speaker C: Yeah, thanks for having me.
00:58:51.788 - 00:58:52.806, Speaker B: Thank you very much.
00:58:52.988 - 00:59:00.582, Speaker A: I guess we will add links to whatever we can in the show notes. Maybe a link to your website, but maybe a link to your twitter as well.
00:59:00.636 - 00:59:00.998, Speaker B: Yeah.
00:59:01.084 - 00:59:05.090, Speaker A: Well, thanks again and to our listeners, thanks for listening.
00:59:05.170 - 00:59:05.970, Speaker B: Thanks for listening.
