00:00:07.210 - 00:00:13.600, Speaker A: We're happy to have Alex and Sachin from ZKP two P for this first talk of the of the afternoon. So yeah, please enjoy.
00:00:14.530 - 00:01:05.794, Speaker B: Thank you so much. Let's go ahead and get started. We actually wanted to start off the talk by updating the title of our talk. So over the weekend we were discussing how the development of our application shaped our perspective on the overall ZK app space and found that to be a little bit more of an interesting talk than just digging much more deeply into the application that we built. So we updated the talk, the title of our talk, and why give a talk if you aren't going to share some bold takes? And so we're presenting credibility proofs, our ZK's transformer. This is an advocacy for developing more apps with credibility proofs and sharing the takeaways from developing our own not sure how many of you guys out there are just starting out in ZK since this is the summit, but we did want to span multiple scale levels in case there were others who were starting out just like we were at the last summit. Hopefully this artifact can bootstrap the first few months of app development for you.
00:01:05.794 - 00:01:58.226, Speaker B: So, a little bit about us we worked together on set protocol, which was an EVM based Defi protocol focused on on chain asset management, and we were among the first teams to explore smart contract based financial services. And this is talking before USDC was even launched by Circle. This means that we've spent the last six years exploring DeFi and crypto economic incentives, and our observations on the current value of smart contracts can be summarized into three things. You get near instant settlement of complex transactions. You get global price discovery, which is true when you have markets on chain open to anyone with an Internet connection. And lastly, you have payments, which are now enabled by liquid stable coins in multiple currencies. Where does that to do with ZK? We'll come back to this in a second, but as a brief aside, I wanted to comment on AI, which is having its moment as a technology trend, and deservedly so.
00:01:58.226 - 00:02:46.818, Speaker B: But I would posit that in 2010, it was somewhat challenging to follow along as an application developer. When someone said that their application was powered by AI, it often ended up being some sort of rule based execution. So in other words, when someone said that they used AI, you knew that it doesn't work. Because when it works, you give it a name, right? It becomes deep learning, it becomes predictive analytics. More recently, in generative AI, it's transformers diffusion. And now with the branch for each modality pair and I think it's a good time to bring this up because ZK is going through where I posit AI was for much of the last decade and suffers from a lot of obscurity while boasting a deep set of value propositions that expands almost every week. So I think it's safe to say that many people on the outside looking in don't really know what the takeaways are.
00:02:46.818 - 00:03:37.726, Speaker B: But like any technology trend, there are going to be diamonds in the rough at any stage, depending on how one looks at it. So what's the first thing that people see today? I'm not going to get into it if you're at a conference named ZK Summit, I'm sure you've seen this before, but in short, it's unclear. It's unhelpful, and the papers keep piling on confusing nomenclature, which I understand is exciting for research but not framed for adoption. So how can we clarify this? So through discussing this talk with Suchin, we settled on the classification of applied zero knowledge proofs that made sense to us that we wanted to share. This is extended from Ian Myers, who some of you may know worked on zcash and Alio. So what does that same graph in AI look like for zero knowledge proofs? I would begin first with confidentiality applications. So this is a fancy way of saying private payments.
00:03:37.726 - 00:04:03.498, Speaker B: It's done with a zero knowledge proof and an accumulator. In practice, this is a snark and a Merkel tree. This is deployed today and takes the form of zcash, Monero and tornado cash. It's largely explored, but hasn't gained a lot of transaction or is being censored outright altogether, as we're finding out. Next, you have scalability applications. These are your roll ups and Zke EVms. You know how this works.
00:04:03.498 - 00:05:05.820, Speaker B: You have a server that compresses transactions into a proof and submits that to the blockchain. So if your blockchain is slow and can only do ten transactions, each of those transactions is now a proof of 10,000 transactions that were signed and executed correctly. Thinking here, being that some of the most popular blockchains are slow and that faster blockchains are going to unlock a new wave of applications, this makes some sense, but mostly it's tailwinds from DFI and NFT usage in previous cycles. And there's a ton of venture funding behind this, which is actually great because they're ultimately improving performance for anyone else that's utilizing general purpose narcs improving systems. But to me this leaves out the most interesting parts of zero knowledge proofs, which are found in the third and last section, which I'm the most excited about, which is credibility apps. The proofs used here involve leveraging signed data returned from existing systems and using it to construct verifiable statements about off chain or real world interactions. These data sources can be API calls, they can be emails, electronic documents, and you can even construct complex statements recursively from multiple of these.
00:05:05.820 - 00:05:55.210, Speaker B: But the tie off app classifications, I wanted to share another angle of them as overlapping, because as we know, applications are often leveraging more than one of the features of zero knowledge proofs in practice. So the boundaries are not strict in terms of limiting values that are utilized. In fact, a lot of the applications live at the intersections of two or more of these value props. And of course in somewhat of like a cheeky sense, you can always say something like a scalability focused application is always making a credibility statement about the underlying batch transactions. And likewise, when you make a credibility proof, the data can optionally be masked by the circuit providing confidentiality. But in the end, it's helpful and important to really simply classify your application and use that to decide how much of each of the value propositions to capture, without feeling the need to maximize all of them. So back to credibility.
00:05:55.210 - 00:07:06.926, Speaker B: The proofs are interesting because you're suddenly able to rely on statements, on chain harnessing consensus, and then using that to execute additional logic. But when used in a well designed application, they have the potential to be something bigger altogether and disrupt an existing inefficient workflow. So going back to defi and smart contracts, you can essentially insert these credible statements into a smart contract that settles a complex transaction that may include reconciling payments. The most easily understood of these is an escrow transaction, in which case you have a signed document that can represent a proof of a closing statement, and then the smart contract may settle that transaction instantly, sending goods to users and then forwarding any money to any of the other counterparties. This is a really, really powerful combination and a reality today, depending on how complex of a statement you want to make. And of course, if you're willing to live with the trust assumptions that you'd have to take on. So with this perspective in mind, I wanted to reiterate the title of the talk and make the claim that credibility proofs are ZK's equivalent of the transformer, which we all know was the pivotal innovation that led to chat GPT, and that disrupting the right workflow and with the right ux will probably be ZK's own chat GPT.
00:07:06.926 - 00:07:52.926, Speaker B: And we think that this is closer than the community probably thinks. And I want to encourage more devs to think about proving complex statements and building great applications around them. And of course this would be no fun if I was just saying this without doing it myself. And so the rest of the talk will be laying out some design questions and how we answer them with our own application. So a little bit about ZkP to P in a nutshell, it is a peer to peer fiat to crypto on ramp we utilize a smart contract to escrow a digital currency like USDC. Then sometime later a user who wants to on ramp can on ramp without collaborating with a depositor by first sending them fiat through a payment channel such as Venmo. Then they generate a proof which represents a receipt of the payment, and finally they submit that proof on chain to claim that original deposit of USDC.
00:07:52.926 - 00:08:38.766, Speaker B: So this is going to be a very light introduction. I don't want to spend too much time talking about the different mechanisms that we built into the protocol, but for the purposes of this talk, you can treat this as an altruistic service maintained by someone who simply wants to see more people utilizing USDC. It was actually born at Zkhack in Lisbon and we thought it would be awesome to close the loop with an update on an app. So the key workflow here, of course is onboarding, and the solution seeks to improve on existing pathways that are either associated with extremely high fees or incurring high latency before granting access to your funds on chain. So under the hood, we piggyback off of a protocol called DKIM, which is short for domain keys. Identified mail. All the data in the email included in the header.
00:08:38.766 - 00:09:29.234, Speaker B: Parameters like the from the to the subject and the hash of the body are all signed by the sender domain and can be verified using a publicly available domain key. It allows anyone to verify that the email message was not faked or altered, and then using regex, the contents can be used to make statements about the actions of the recipients. So for us, when a Venmo transaction is executed, emails are sent to both the sender and the receiver, and either of these can be used to generate a proof of the statement that the sender actually sent the money. All right, so with the apps basics in mind, I wanted to share some questions we encounter when building our own apps and then dig into our design decisions. And hopefully this gives you a window into what's possible today and where there's going to still be some room for flexibility when building credibility apps. So the first is going to be asking what proving system you should use. That's a natural question to ask.
00:09:29.234 - 00:10:32.034, Speaker B: It sounds obvious to say that you should pick one that allows you to optimize your uX, but today there are a plethora of proving systems to choose from. That all tout proving performance, trusted setup mitigation, and quantum resistance, which can be overwhelming for application developers to consume. It's going to be tempting to optimize for performance as a proxy for Ux. However, when we thought about it, we decided that performance wasn't actually going to be a bottleneck. And this makes some sense when we think about the slow proving times relative to how the workflow is actually performed in real life today, for example, on ramping so putting my defi hat back on the likes of compound and uniswap found so much usage because they replaced traditionally complex tasks like taking loans and creating new markets. And so even paying $1,000 in network fees and waiting for a few blocks is actually a drop in the bucket. And so therefore it's somewhat of a safe bet that users will tolerate latency if their workflow is materially improved.
00:10:32.034 - 00:11:28.770, Speaker B: That said, you still want performance where you can, but not at the cost of other side effects that can degrade the experience. And for on ramping, we decided that the best experience would be to allow on rampers to complete the flow without introducing asynchronicity, which we discovered as a byproduct of exploring halo. Two more on this later as we explore the stack. The second is whether privacy is actually necessary and what to do about it. If not so privacy is obviously a pillar when discussing ZK applications, where the current solution for preserving privacy is a proof generation in the client. But not all applications require privacy to be first class citizens, and we came to the conclusion that for on ramping, it probably wasn't. This is somewhat drawn from the observation that zcash and Monero haven't really taken off, while USDC, which is pseudo anonymous and actually even has built in censorship, sees increasing volumes.
00:11:28.770 - 00:12:43.242, Speaker B: So if you're not sure or don't have a strong inclination about this, you can actually think about making this a choice to users to trade off privacy preserving client side proofs which are slow with fast server side proof generation, and you'll see what technologies we chose to achieve this in our stack. And the last question was around identifying dependencies that we would encounter and how we would handle them. So applications in this class will by definition be dealing with external dependencies since you're trying to prove a statement from data from a part of some other system. And so in our case we have critical dependencies on the domain keys used to sign the emails, which can change and of course the email templates themselves since we rely on regex of the emails. This affects your applications because you have to handle routing proofs appropriately to ensure service delivery between key and template updates. And this also affects your deployment process because the proofing system that you ultimately choose may require trusted setups, which would potentially have to be done frequently and are operationally intensive. Luckily, a lot of these are somewhat solved problems whereby managing domain keys and email template updates can be solved with proper versioning as well as on chain registries when deployed.
00:12:43.242 - 00:13:02.040, Speaker B: And in the case of trusted setups, there are emerging projects like potion that seek to make this as simple as possible and actually come complete with the UI. Cool. So with that, I want to hand things over to Sachin who's going to share some takeaways exploring the circuit development side of things and what factors were considered in making the design choices that we did.
00:13:02.490 - 00:13:03.240, Speaker A: Cool.
00:13:04.170 - 00:13:45.890, Speaker C: Yeah, so the learnings that I'm going to share are largely applicable to all credibility proof based apps. We will share concrete ZKP tube examples as required. So when building ZKP apps, there are a multitude of ZKP languages, DSLS libraries proving systems that are available at your disposal but for production zk apps within these six parameters are the most important ones that affect your decision on deciding the tooling for your ZK app. Those include dependencies. So if you include any libraries that you are using like RSA, EDDCA, signature verification, et cetera. Development experience primarily on how easy is the language to pick up and trusted setup. These parameters affect your development and go to market speed.
00:13:45.890 - 00:14:17.070, Speaker C: Next, we have the parameters that affect the user experience the most, which includes proverb performance. Proving key size is important if you are doing in browser proving and on chain verification cost. Let's go over some of the options that we have. I'm assuming some initial knowledge and experience with these languages here. Okay, so option one is circum using the Grot 16 proving system development parameters. First, it has mature implementations. Our main dependency is EK email had a mature and stable circum implementation.
00:14:17.070 - 00:15:03.120, Speaker C: It is easy to pick up and experiment with. It has JavaScript like syntax and a decent devex and it does require trusted setup. Next UX parameters. It has super fast server side performance using rapid snark, slow in browser performing proving for large circuits, keys are in gigabytes for large circuits and it has the cheapest onchain verification cost because it uses the Grot 16 proving system. The second option that we have is halo two which is based on the proving system. Dependencies like RSA and ZKML are largely a work in progress and unoptimized. It is hard to pick up, well easy if you know rust beforehand, but the vanilla hello to API is still a bit overwhelming, so I would suggest to use the hello to lib by Axiom instead.
00:15:03.120 - 00:15:53.840, Speaker C: They have done some great work in simplifying. The API does not require any trusted setup because it is based on the plank proving system UX parameters. Well, hello Two has fast client side proving, although in browser proving for circuits with degree K greater than ten isn't really feasible due to the wasm bind gen limit. Keys are very large and might exceed tens of gigabytes on chain verification gas cost is high for circuits that are large. Verifier exceeds the bytecode size limit. So to bypass that you would have to do server side aggregation, which introduces a lot of latency in the app. So when should you choose circum over halo two? The first obvious is if your dependencies are in circum then you're forced to do that, and otherwise if your circuits are small.
00:15:53.840 - 00:16:44.366, Speaker C: I think it's the hands down the best option because it offers you fast browser proving time due to optimized was improver and you get privacy for free. But when you're building with real world apps, as Alex alluded to earlier, everything is in sunshine and rainbows. And more often than not when you're building credibility proof based apps, your circuits would be big. And that's when you have to review your choices. And the decision mainly to whether or not choose one proving system over the other boils down to whether privacy is critical for your application. If privacy is not critical for application, in that case we suggest you should use Circom and you get fast server side proving using rapid snark. But if privacy is critical to application, then you need to perform client side in browser proving, for which you could use halo two.
00:16:44.366 - 00:17:26.170, Speaker C: But then you run into the two problems that I described before, namely the WASM memory limit and the large verifiers. In order to bypass the WASM memory limit, you would have to break down the circuit into chunks. For example, RSS signature verification could be broken down into two components. The first one is hashing the input using the shah circuit, and then performing the exponentiation using the RSS circuit, and then combining the results of both of those on chain. To bypass large verifiers you would need to do server side aggregation which obviously introduces latency. Note that these are tough engineering problems. You could also use halo two if you are going to continuously update your circuits.
00:17:26.170 - 00:18:10.486, Speaker C: Not having to do trusted setup every time does help. Or if you need to leverage recursion which is a rather complex topic and you wouldn't need it in most cases at the beginning. So which language did we choose for ZkP two p? Well first of all our circuits are quite big because Venmo emails are quite lengthy and you need to hash all the data in the email. And as Alex mentioned earlier, privacy is not critical for on ramping. Hence we went with Circom which offers us fast server side proving. It also gives you low latency synchronous on ramping flow for a better UX. It does also give you optional slow in browser proving and the cherry on the cake is the ZKML.
00:18:10.486 - 00:18:46.270, Speaker C: Circum implementation was quite mature and stable, but circum has its cons. It requires trusted setup. Well that is not an issue because we can automate that using portion from PSE. It has slow in browse approving for large circuits. Fine. I guess it's more of a feature rather than a bug because it allows users who choose privacy over latency to still use ZKP two B key sizes are large. Again, that is not an issue for server side proving, but for in browser proving we have an engineering solution to just chunk and compress the keys.
00:18:46.270 - 00:19:12.238, Speaker C: It is not great, but it works. A quick look at the Zkp two p metrics. Our latest circuits have 5.9 million constraints. It takes around 40 seconds for server side proving using rapid Snark 400K for gas for onchain verification. But if you want to do in browser proving, you would have to download 1.7 gigabits of compressed proving key and it would take around nine minutes for proving on the client side.
00:19:12.238 - 00:20:09.658, Speaker C: That's just the cost that you have to pay for privacy. Now if privacy isn't absolutely critical to your application, then instead of making decisions for your user, you should give your users a choice where they get to choose between privacy and speed for maintaining privacy you can use in browser proving using halo two and for speed and low latency, stick to server side proving. In circum we are envisaging this flow where we offer both these options to be the state of ZkP two p proving. Yeah, let's talk about how to build secure credibility proof based ZK apps. Next, note that these learnings are specific to circum and may not or may not always apply to halo two. The first concept to understand here is that unlike completely on chain protocols, all the computation in Zcaps isn't happening on chain. Instead, part of the computation is done off chain in the circuits and the rest is done in smart contracts.
00:20:09.658 - 00:20:49.820, Speaker C: Hence there's a shared responsibility between the circuit and the smart contracts to keep the system secure. The general framework to think about it is that if f is a costly function to compute, then you would compute F in the circuit and produce a snark which provides computational integrity. Now, for security, you need to do two checks. The first and the obvious one is to you would need to verify the snark, which basically gives you the guarantee that f was computed correctly. But that alone is not sufficient because it could have been computed on any input. So you would also need to check that the inputs and the outputs to the f were correct inside the smart contract. Let's understand that with an example.
00:20:49.820 - 00:21:32.986, Speaker C: So in ZkP two p, as Alex mentioned earlier, we verify the DKM RSS signature. Among other things. Signature verification is costly, hence we do it inside the circuit. The circuit makes the RSA modulus public, and inside the contract we check that the RSA model matches the stored Venmo modulus on chain. This basically authenticates that this blob of data was signed by Venmo. Next, nullifiers nullifiers are used to prevent double spending of example, in Zkp two p, it prevents users from using the same payment email more than once to prove an off chain transaction. Generally, nullifier is constructed using hash of unique identifiers sourced from the data.
00:21:32.986 - 00:22:24.380, Speaker C: It is specific to every application and you need to put some thought into it. But for credibility, proof based apps where you are proving an off chain statement coming from a web to service, it becomes tricky because if the nullifier is sourced from the web two data, this basically allows the web two service that produced the data to de anonymize you. For example, in Ziko p two p, we use the hash of the DCAM signature as nullifiers, but that means Venmo can de anonymize onchain users. But that is a trade off that we are willing to make because we need nullifiers to prevent double spending. In zkaps, proofs are generally meant for some specific onchain state. Again, example in zkb two b, an off chain payment proof is generated to unlock a specific onchain escrow. Now, in order to prevent front running and MeV attacks, we need to tie the proof to that specific on chain state.
00:22:24.380 - 00:23:19.530, Speaker C: We can do so using this neat little trick of accepting the piece of data as an input to the circuit and then adding a fake square constraint on top of it. For example, in the code snippet here taken from the ZKP two circuit, we are tying the proof to the specific order ID that we want to collect the escrowed funds from. Finally, a quick slide on how to think of upgrading your zcaps a year from now. So, given the high paced research around proving systems, which is evident by the need of two ZK summits every year, you should expect to rewrite your circuits in the newest, fastest proving system to offer the best UX possible to your users. Both us and the ZKML team are expecting to do the same a year from now, and rewriting all the circuits is one of the risks of developing ZK apps right now. But we are optimistic and we are willing to take the bet. With that, I'll hand over the stage back to Alex.
00:23:22.850 - 00:24:10.750, Speaker B: All right, so just a bit of a quick recap. We discussed how zk applications can be bucketed into confidentiality, scalability and credibility apps to better understand the ecosystem of which we as a team believe that there's huge potential around applications focused on credibility. As we show with ZKP to P, the ecosystem is probably more capable than what most people are expecting right now and is equipped with a pretty rich design space that's waiting to be explored. And so expect you guys will all see a lot more interesting apps that will emerge shortly. And so with that, go out and build. We want to give a special shout out to Ayush and Sora from the ZKE email team for building ZKE email and sharing all of their insights with us. And also a big thanks to Andy from the PSE for supporting our development.
00:24:10.750 - 00:24:15.440, Speaker B: And with that, I want to thank you for choosing our talk and open things up to questions.
00:24:18.210 - 00:24:33.250, Speaker A: Great. Thanks Alex. Thanks sachin. So yeah, I mean, you've done my job. We do have time for a few questions, so yeah, please raise your hand. I will pass by the mic for any questions. Come on, be creative.
00:24:33.250 - 00:24:35.880, Speaker A: Yes.
00:24:40.170 - 00:25:05.150, Speaker D: You put a lot of emphasis on the trade off space between speed and privacy. Where do you think that distributed witness proving and kind of threshold based things fall on that trade off space? And do you think that in your experience looking at this for consumer apps, it represents an interesting middle building ground?
00:25:10.090 - 00:25:19.574, Speaker C: So like, yeah, your question is about distributed proving. Could you repeat the question, like secret.
00:25:19.622 - 00:25:22.250, Speaker D: Shared proving between different provers?
00:25:23.150 - 00:26:02.946, Speaker C: Yeah, I mean, you could do that, but you need a network for that. I believe you need a network of validators to perform that, I guess. Multiparty computation. Our focus is more on the UX, whether the user is ready to reveal parts of the data to be able to send the proof fast on chain, or maybe use it on a mobile device. Because in browser proving is possible, even though slow. It is possible on your laptops, but it is not possible on your phones. So you would choose the option for speed there rather than privacy.
00:26:02.946 - 00:26:09.766, Speaker C: Otherwise, if you want to preserve your privacy, then you'd go for the other option, which is client set proving.
00:26:09.878 - 00:26:21.674, Speaker D: Thanks. Makes sense. Just a second question. How are you thinking about surfacing privacy? Like what privacy trade offs a user is making when they're picking.
00:26:21.802 - 00:26:53.670, Speaker C: So if you take the specific example of ZKP two p, the user is willing to share their Venmo ID with the server who's producing the proof. They are willing to share all the contents of their email, including the. From the to maybe the name, the bank account they use, and the Venmo Id. Not with everyone on chain, but only with the server, which is producing their proofs.
00:26:54.670 - 00:26:55.082, Speaker A: Cool.
00:26:55.136 - 00:26:55.740, Speaker D: Thanks.
00:26:57.070 - 00:27:24.290, Speaker B: In a prior iteration, we actually first had the onramper specify a specific order of how much USDC they wanted on chain, and they were able to pass along encrypting keys that were then pushed on chain, and then the offramper would be able to use that to encrypt like Venmo handles. But then we decided that in this iteration, we wanted to experiment with a much more synchronous flow for the on ramper to not have to go through that process. So they're just tapping into available liquidity that's on chain.
00:27:25.110 - 00:27:25.860, Speaker C: Cool.
00:27:28.630 - 00:27:32.360, Speaker A: Yeah, last one.
00:27:34.650 - 00:27:54.220, Speaker E: Yeah, you had on the slide that one of the risk is to just having to keep on rewriting your circuits in whatever the newest proving language is. Are you guys investigating looking at something like Noir, for instance, where you can kind of swap out proving back ends? Have you guys looked into.
00:27:56.450 - 00:28:33.000, Speaker C: We haven't looked into noir specifically, but our team member did develop the dependencies that we need for writing ZKML or ZKP. Two P in noir are missing right now, and one of our team members did write RSA in Noyer. So that's one of the dependencies that we need. Also about rewriting circuits. A year from now, some of the languages are moving towards swapping out backends very similar to noir. So in that case, we might not need to rewrite everything from scratch. And we could stick to the same front end but use a different back end.
00:28:41.210 - 00:28:44.100, Speaker A: Okay, guys, thank you very much. Thanks for the talk.
