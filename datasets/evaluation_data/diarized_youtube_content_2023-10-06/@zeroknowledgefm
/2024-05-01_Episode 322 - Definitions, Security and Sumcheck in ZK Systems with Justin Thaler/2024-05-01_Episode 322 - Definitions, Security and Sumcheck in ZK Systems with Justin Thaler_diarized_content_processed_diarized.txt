00:00:05.520 - 00:01:03.248, Speaker A: Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online. This week, Guillermo and I chat with Justin Thaler, associate professor at Georgetown and research partner at a 16 Z. Last year he published a blog post called the 17 misconceptions about snarks which made waves in the ZK space, and in this episode we revisit a few select points. We then dive into Jolt, a new ZKVM which had been initially described along with Lasso in 2023 and has recently been implemented and is open to contributions from the community. We talk about how his work on some check protocols influenced this work, how it compares to others in the space, and more.
00:01:03.248 - 00:01:51.206, Speaker A: Now before we kick off, I just want to share a little bit about Zkhack Krakow, an upcoming IRL hackathon happening May 17 through 19th in Krakow. Zkhack is an educational hub and another project that I'm a part of. We actually mentioned that on this episode with Justin because Justin's been running a longstanding study group over on the Zkhack discord. But Zkhack also does IRL events and as mentioned, the next one coming up is on May 17 through 19th and it's happening in Krakow. We already have an amazing group of hackers set to join us, some fantastic sponsors offering prizes, a great venue, and we're hoping that this event, like our previous hackathons, showcases the next generation of ZK applications. So if you're in the field or looking to jump in, I hope you'll join us as well. Ive added the link in the show notes.
00:01:51.206 - 00:01:52.686, Speaker A: Now Tanya will share a little bit.
00:01:52.710 - 00:02:39.952, Speaker B: About this weeks sponsors Alio is a new layer one blockchain that achieves the programmability of Ethereum, the privacy of zcash, and the scalability of a roll up driven by a mission for a truly secure Internet. Elio has interwoven zero knowledge proofs into every facet of their stack, resulting in a vertically integrated layer one blockchain thats unparalleled in its approach. Alio is ZK by design. Dive into their programming language, Leo, and see what permissionless development looks like, offering boundless opportunities for developers and innovators to build ZK apps. This is an invitation to be part of a transformational ZK journey. Dive deeper and discover more about alio@alio.org. dot Namata is the shielded asset hub, rewarding you to protect the multi chain.
00:02:39.952 - 00:03:20.354, Speaker B: Built to give you full control over sharing your personal information, Nemata brings data protection to existing assets, applications, and networks. Nemada ends the era of transparency by default, enabling shielded transfers and shielded cross chain actions to protect your data, even when interacting with transparent chains. Nemata also introduces a novel shielding rewards system. By holding your assets in a shielded set, you help strengthen Nemata's data protection guarantees and collect nom rewards in return. Nemata will initially support IBC and Ethereum based assets, but will ultimately serve as a single shielded hub for all assets across the multi chain. Learn more and follow Nemata mainnet launch@nemata.net dot.
00:03:23.814 - 00:03:28.294, Speaker A: Today Guillermo and I are here with Justin Thaler. Welcome back to the show, Justin.
00:03:28.414 - 00:03:29.822, Speaker C: Thanks for having me. Happy to be here.
00:03:29.878 - 00:03:51.632, Speaker A: Hey Guillermo. Hey, yeah, Justin, last time you were on the show, I introduced you as an associate professor at Georgetown. Now I think you have another kind of role we can talk about, which is research partner at a 16 z or z. Sorry, that was so canadian. But yeah, it's really great to have you back on the show and I want to talk about what's happened since you've been on.
00:03:51.768 - 00:03:52.472, Speaker C: Sounds good.
00:03:52.568 - 00:03:58.240, Speaker A: So you joined a 16 z. Tell us a little bit about that. What's it like working there and what are you doing there?
00:03:58.392 - 00:05:02.782, Speaker C: Sounds good. Yeah, I think last time I was on was just a few weeks before I started full time. So I got started there right when the research group started, they ran like a few months after day one summer hit. And the group was, I think four people to start. And they brought in a whole bunch of visitors for the summer, really spanning the blockchain discipline, a bunch of cryptographers, economists, even political scientists, mix of PhD students and professors. So I was there actually just for about half the summer and it was super exciting, just like drinking from a fire hose, I guess. Tim Refgarten, who runs the group, was trying to model the summer after the Simons Institute for the Theory of computing at UC Berkeley that runs these like semester long focuses on a research topic and also there, it's just like there's so much activity you just can't keep up.
00:05:02.782 - 00:05:38.112, Speaker C: You have to actually be careful. You have time to like talk and think and do work rather than just like consume the content that is occurring and being produced. So that was just a great experience. Kind of sucked me deeper into sort of efforts to deploy these snarks, deploy these Ek proofs. Yeah, I think it sort of cut both ways. Excitement in this technology was already high and rising. So I think there was a lot of interest both in the firm, to have someone with my expertise there, and I was super excited to be more involved.
00:05:38.112 - 00:06:31.444, Speaker C: So I started full time in January. I'm going to leave from Georgetown right now. I'm still able to sort of get my fix for teaching. I'm still running my reading group over Discord on the ZK hack channel, planning to start a brand new fresh pass through my book soon, hopefully in the next couple of weeks. So I've been off discord for a few weeks, but I'll get back on and sort of announce that soon. And, yeah, it's been a wonderful, it's been a little over a year now, going on a year and a half, and there's so much just excitement about blockchains and snarks in particular throughout the firm. My colleagues in the research group are wonderful, and I've gotten to work very closely with this engineering team that's there as well, which has also just been like a dream come true for me.
00:06:31.444 - 00:06:40.354, Speaker C: Sort of what's led to the recent jolt implementation that we'll probably talk about today. So I'll say more about that then. But it's just been a wonderful experience overall.
00:06:40.814 - 00:06:49.046, Speaker A: Who's on that team with you? Are there any names we know? Yeah, and I mean, some of these folks, actually, I know some folks from your group presented at ck ten.
00:06:49.190 - 00:07:08.410, Speaker C: Yeah. Yeah. So you probably know very well Joe Bonneau, who's kind of a generalist cryptographer. He kind of co authored the first book, as I understand it, on kind of the science of blockchains and the cryptography underlying blockchains in particular.
00:07:08.522 - 00:07:11.334, Speaker A: Yeah, he's been on the show before, too, actually, I think.
00:07:12.474 - 00:07:47.938, Speaker C: Yeah. And he's done really groundbreaking works like introducing the notion of EDFs and with colleagues and things like that. So Valeria Nikolenko is on the team, and she does a lot of work with signatures. And NPC uses snarks as well. In some of her research. We form the cryptographers of the group, and the others are economists. So there's Tim Roughgarden, there's Scott Kaminars, who's at Harvard Business School, Pranav Garamidi and Miriam Bharani.
00:07:47.938 - 00:08:10.582, Speaker C: So Tim and Scott are faculty, and Miriam and Pranav are sort of full time grad, student level researchers essentially, but there's no separation at all in the group. Everybody works together, and the same we also have. Dan Bonet is closely affiliated with the group.
00:08:10.758 - 00:08:15.638, Speaker A: I was going to ask. Yeah, I know he's been involved with this for a while, right?
00:08:15.806 - 00:08:48.426, Speaker C: Yeah. So he's not full time or anything like that, but he's closely involved. Talk to him weekly, and he visits over the summer. And there's also a political scientist closely affiliated with the group, Andy hall. We sort of get him for one day a week, and he works a lot on governance of daos and blockchain protocols and stuff. And I expect the group to continue to grow and add more areas and branch out as we go. A postdoc, Joachim new, is joining soon.
00:08:48.426 - 00:09:05.438, Speaker C: Well, he's very broad, but he does topics like consensus research and things like that. So just a wonderful group and works very closely with other parts of the firm that it's just been a fantastic experience. I sort of can't believe my luck at getting to be a part of it.
00:09:05.606 - 00:09:18.230, Speaker A: It sounds like there's also an engineering team. And actually, the folks who have spoken at CK ten, at CK summit ten, I don't think you've mentioned them yet. They're the folks you work on jolt and stuff. Who are they? How big is that team?
00:09:18.422 - 00:09:31.758, Speaker C: Yeah, so there's an engineering team. I think it's roughly the size of the research group. So I think I listed, depending on who you count, like six to eight people, and the engineering group is in that vicinity. I've never actually counted on my hands.
00:09:31.886 - 00:09:32.714, Speaker A: Got it.
00:09:34.814 - 00:10:10.524, Speaker C: They have dual roles, just like the research group doesn't just do research, that they help the portfolio companies a lot to the extent possible. They also work on developing open source tools for the whole community to use all sorts of other things in the firm as well. And so it's been a real pleasure to have Jalt be one of the main open source efforts to be involved with that. It's just been very exciting and productive, I think.
00:10:10.694 - 00:10:38.088, Speaker A: Cool. You just mentioned your book, proofs, arguments, and zero knowledge proofs. I want to just sort of mention this before we dive into any of the topics we wanted to cover today. I think you've run three cohorts of a study group in the Zkhac discord. I know last time we had you on the show, we talked a lot about that and that book and where it came from and where it was at. So I'll definitely link to that episode. If people want to just learn about this book you've written, or actually, you called it a mono.
00:10:38.176 - 00:10:39.192, Speaker C: What was it called?
00:10:39.368 - 00:10:39.976, Speaker D: Monograph.
00:10:40.040 - 00:10:44.968, Speaker C: Monograph. Monograph. We could just call it a book now. I guess that it's been.
00:10:45.096 - 00:10:47.088, Speaker D: I think it's a book now. I think it's officially a book.
00:10:47.136 - 00:11:14.096, Speaker A: Not a monograph I true peer review three times. So some form of peer review so far. So I know what you mentioned in that episode was that you, like, by doing these study groups, you actually get to see how the book is interpreted by people who are reading it for the first time often. And through that, you've been able to make it, like, refine it and change it. You were just saying you're gonna run another one. This is very cool. I just want to kind of highlight that because I think people are going to be into it.
00:11:14.240 - 00:11:16.416, Speaker D: How long is it again? Like, how many weeks?
00:11:16.440 - 00:11:21.816, Speaker C: How many weeks? Yeah, I mean, it varies based on, like, basically how many off weeks we need to take, but.
00:11:21.920 - 00:11:22.608, Speaker D: Oh, I see.
00:11:22.696 - 00:11:27.112, Speaker C: One pass through the book normally takes about like eight months, it seems, at this point.
00:11:27.168 - 00:11:27.656, Speaker A: Wow.
00:11:27.760 - 00:11:28.432, Speaker D: Oh, okay.
00:11:28.448 - 00:11:28.616, Speaker C: Okay.
00:11:28.640 - 00:11:37.460, Speaker D: This is like, this is like not messing around type thing. Sorry, I apologize. I've never done one of them. I did, like a very, very short session at some point, which was like three days or something.
00:11:37.492 - 00:11:50.268, Speaker A: Yeah, you did a short. You did a study group. Mini Guillermo, Justin, a light edition. Justin's proofs, arguments and zero knowledge proofs are the ZK hack study group. Like, Maxi, for sure.
00:11:50.436 - 00:11:53.380, Speaker D: I should do that. I should just, one of these days should just do it. It'd be kind of fun.
00:11:53.412 - 00:12:05.500, Speaker A: I think it might be the longest one we did do. There was a group that did the moon math manual, and they also spent like, at least six months. But I think yours may still be the longest running one and definitely the longest.
00:12:05.532 - 00:12:06.700, Speaker D: Who is writing Moonmouth?
00:12:06.772 - 00:12:26.694, Speaker A: Well, it's a book. It was produced by the least authority group and Mirko was the author. But they, like, in our group, they've just read through it, so there's a facilitator. And together the group kind of reads a chapter every week and then gets together. They're actually running the moon math manual again right now. And the whiteboards. Study group.
00:12:26.694 - 00:12:36.470, Speaker A: There's a lot of study groups going on in Zkhac. If people aren't aware, definitely go check that out. And I guess, Justin, if you're going to start yours off soon, this is kind of the perfect time to let people know.
00:12:36.582 - 00:13:24.594, Speaker C: So sometime in May, I plan to start it up again. And one benefit of actually coming to the sessions, or we try to record them and get them on YouTube, although I don't actually track if they make their way to YouTube or not. But there are a lot of questions about what people are actually building specific systems and things which isn't discussed as much in the book or certainly the book can't keep up to date with the many changes to what people are building every month or so forth. So you sort of get that benefit. And I do my best to keep up and give accurate answers. It's hard with how much things changes, but yeah, so that's a benefit of the sessions over just reading the book, I think. And also the book is now already out of date in certain ways.
00:13:24.594 - 00:13:27.014, Speaker C: So that's another benefit showing up.
00:13:27.974 - 00:13:57.292, Speaker A: Interesting. Since you've been on, you also published this essay called 17 misconceptions about snarks. And I feel like that really made the rounds in our community. It affected, I mean, it definitely impacted me a little bit. Like use, I mean, the first point is using ZK to mean succinct. I think it was the first time, for me at least, it was very clearly articulated that we've been saying it wrong. When we say ZK snarks, we're often actually describing snarks that are not ZK.
00:13:57.292 - 00:14:11.004, Speaker A: And yet it's all been lumped under the ZK umbrella. And we've talked about this a few times on the show, and I think there's kind of now like an acceptance of, we're still going to keep using the term. But I think you were the first person to really, for me at least, make that distinction very clear.
00:14:11.164 - 00:14:40.754, Speaker C: Yeah. So I think I was not the first person from my perspective. Right. I think people were talking about this before I put it in the post. I guess one point I'm not sure I had heard much before, which is maybe my main concern is that ZK has a precise technical meaning about basically protecting privacy of the witness. And so my main concern is that one day someone caused something. ZK.
00:14:40.754 - 00:15:05.954, Speaker C: It's not ZK. And so there's a privacy leak. And that's really my main concern. I mean, I do think it's helpful to be precise about what property of a proof system you really care about. And often it is the sickness and not the ZK property. I also just like the word succinct, but I know it's not going to, that's not going to catch on quite the same way ZK has.
00:15:06.074 - 00:15:19.532, Speaker A: I think succinctness has. I mean, it's emerged a lot more. I think people do use the term more since you've published this blog post and. Yeah, definitely in the last year. But yeah, I think is that big umbrella term CK still seems to hold?
00:15:19.668 - 00:15:43.328, Speaker C: Yeah. And so I expected it will. And it's, I don't think it's the end of the world as long as a more people are aware of the way it's being used. Right. I mean, I've also, like, talked to people who, you know, know nothing about snarks and got very confused about this. So that was another concern. And, you know, so if there's more awareness, I guess that can help.
00:15:43.328 - 00:15:57.404, Speaker C: And also, now I think people will say ZK and then be more prepared to clarify that they don't actually mean ZK. And that can also help. So I'm not gonna be super rigid here, but I think it was definitely useful to say something.
00:15:57.904 - 00:16:17.954, Speaker D: I think the crappy version of this that I have in my head is ZK. The acronym is a different type than zero knowledge. ZK, the acronym is, like. It just means, like, vaguely everything unless it is combined with a secondary thing. But, like, when you say zero knowledge, you really do mean zero knowledge.
00:16:17.994 - 00:16:24.530, Speaker C: That's it. You know, I actually haven't heard that before. So that. That distinction. So maybe. Maybe I'll feel. I'm gonna.
00:16:24.530 - 00:16:32.066, Speaker C: You know, now every time somebody says it, I'm gonna think, like, did they say zero knowledge or did they say ZK? And maybe it'll be okay. Yeah.
00:16:32.090 - 00:16:38.842, Speaker D: Yeah. Well, now it's like, now it's like a people problem, right? As opposed to. It's like, what kind of person is this? You know?
00:16:38.858 - 00:16:39.034, Speaker C: What?
00:16:39.074 - 00:16:40.552, Speaker D: Who would say that, right?
00:16:40.698 - 00:16:56.868, Speaker A: Looking at these 17. I will not go through all of them because, well, first of all, some of them are too in the weeds. I don't fully understand all of them, but I went through and highlighted a few of these because. When did you publish this blog post? Actually? It's like a year ago, right?
00:16:57.036 - 00:17:08.544, Speaker C: Yeah, something like that. I remember it was a couple months before the lasso Joel papers came out, and those are already seven, eight months ago, so, yeah, a little under a year ago. Ten months, something like that. Yeah.
00:17:08.624 - 00:17:37.554, Speaker A: I sort of mentioned the first point, which was using ZK to mean succinct. Another point you made was sort of the snarks versus starks, which I also think in the time. I mean, I think it was already happening when you wrote this, obviously. But I think in the time since, I think that blending of those things and that sense that, like, I think I've asked this on a recent podcast, like, which one wins? And it's, like, both or neither. Like, they're just. They're a mix in a way now, so it's hard to make that distinction.
00:17:37.704 - 00:18:29.184, Speaker C: Yes. And I think they'll sort of get blended even more. As with recent developments, with like, you know, binius now having polylorithmic proof size without recursion and things like that. So if they blend enough, then the whole, I guess, concern goes away that I expressed in the post. But at the time, and even today, people often treat them this distinctly when they're not really, because all of these are deployed non interactively, which does make a stark, technically a special case of a snark. And of course, people use stark to also mean a particular protocol, as opposed to any protocol satisfying class of properties, which is really how the term was defined in the research literature. I guess my two biggest concerns.
00:18:29.184 - 00:19:36.546, Speaker C: One was mentioned in the post, which is the acronym stark in the research literature actually doesn't specify if the protocol is non interactive or interactive, and whether it's non interactive or interactive drastically affects the appropriate security level of the protocol. And so I find this kind of very, I almost call it dangerous, where if you talk about what's the right security of starks, is this number of bits of security appropriate? Well, firstly, bits of security is a sort of underspecified term, but even ignoring that, the answer is completely different depending on whether it's deployed interactively or non interactively. But today they're exclusively deployed non interactively. So that was one concern, something I didn't say in the post that I keep encountering since the post. People think starks also encompasses post quantum security, but the term as defined says nothing about post quantum security. Anyway, I think that this distinction, which is not really a distinction in practice, has caused a lot of confusion. I'd like to stop the confusion.
00:19:36.730 - 00:19:41.386, Speaker A: Okay, but which one do you choose, Justin? What do you call this thing now?
00:19:41.490 - 00:20:04.314, Speaker C: So, I mean, my preference, as I said in the post, as long as these are all deployed non interactively, a stark is a transparent snark with certain efficiency properties. So let's just say snark. And if you want to emphasize that as transparent, let's say it's a transparent snark and leave it at that. That's my view. Obviously there's going to be all sorts of disagreement about this is pretty controversial.
00:20:04.354 - 00:20:21.178, Speaker D: Topic, but I actually have two questions for you. Yeah, I feel like Snark and Stark have become like tasting notes. You know, it's like you use it to gesture at something that vaguely feels like appropriate, but it's not quite like a real distinction in any deep way.
00:20:21.226 - 00:20:23.010, Speaker A: Is that a wine reference, Guillermo?
00:20:23.122 - 00:20:29.294, Speaker D: Me making wine references in public, that's crazy tasting. Absolutely. I would never do such a thing.
00:20:29.634 - 00:20:30.850, Speaker A: A hint of oak.
00:20:30.962 - 00:20:51.660, Speaker D: Yeah, a nice bit of nuttiness right here, you know, but I do want to. I'm curious about this notion, and this is probably really obvious. It's not super obvious to me, which is you said that starks in the non interactive setting have non obvious security properties. Again, explain it to me like I'm a five year old that happens to know a lot of math, but is otherwise an idiot, so.
00:20:51.772 - 00:21:02.124, Speaker C: Right, okay. So I think you're asking about this distinction I was drawing between appropriate security levels when a protocol is deployed interactively versus non interactive, and which one is.
00:21:02.164 - 00:21:03.716, Speaker A: More secure, actually, in that?
00:21:03.780 - 00:22:11.572, Speaker C: Yeah, I mean, it's not really a moral secure. So let me just explain. So, when people refer to bits of security for a snark, roughly what they mean is the logarithm of the minimum number of steps, computational steps, required to attack the snark. So, to find a convincing proof of a false statement, when people say bits of security when they're talking about an interactive proof system, essentially what they mean typically is, what is the maximum success probability that the prover can achieve on any given run of the protocol, regardless of how long it runs for? Even this can vary a bit. With an interactive protocol, no matter how much work the approver puts in. It might be the case that it has one in a trillion chance of convincing the verifier to accept a false statement. That probability is so low because at the very last second, the verifier chooses a random value, which the prover just doesn't know until all its messages are set right?
00:22:11.628 - 00:22:11.956, Speaker D: Correct.
00:22:12.020 - 00:23:23.506, Speaker C: So, in that setting, it might be reasonable to have the prover have, like, a one in a trillion chance of tricking the verifier, basically because it doesn't know if it's going to succeed until it actually runs the protocol. And with probability one minus one over trillion, the verifier rejects. And everyone in the world sees that this was a cheating proverb, and it got rejected. Whereas in the non interactive case, the attack can happen silently offline. The prover is just doing, in its own head, a linear search over possible proofs until it finds one that the verifier accepts. It's just very different settings in this non interactive setting where the attack happens silently in the previous head and no one knows it's happening, I think you really want minimum 100 bits of security if you're protecting anything of value, if you really want the attack to be truly infeasible today and have a little bit of buffer in case better attacks are discovered. I even prefer more than 100 bits of security, but I won't complain about 100 bits in the interactive setting could be totally fine to be way, way fewer number of bits of security, no? Sure.
00:23:23.506 - 00:23:23.810, Speaker C: Yeah.
00:23:23.882 - 00:23:44.224, Speaker D: In the interactive setting 1 trillion is good enough, but in the non interactive setting it gets. So I see the point fundamentally difference is one can carry an offline attack in the non interactive case, pretty much that's it versus the other one. In a pure probabilistic sense, the sampling is happening independent of whatever you do.
00:23:44.264 - 00:23:45.484, Speaker C: So you're screwed.
00:23:46.584 - 00:23:55.704, Speaker D: That is the argument for saying, hey look, we might want to extend the amount of security for the offline case, for the non interactive case, right?
00:23:55.744 - 00:24:58.938, Speaker C: I think everyone agrees that 40 bits of security, that's one over a trillion, is completely insecure in the non interactive case, but could be totally fine in the interactive case. So I'd really like a term to be precise about which of the two cases we're discussing, because I think security is way more important than performance, and I prefer the term to be precise about that. And technically the stark term is not precise. And actually I think that had confused discussion significantly. And this was one of the when I started looking more closely at what people were doing in practice, as my first summer at a 16 z and I wrote this blog post, I actually had a couple people reach out to me after the post. Not the 17 misconceptions post another post about security, saying that they thought that some starks had been deployed interactively and hence 80 bits of security was plenty, plenty, plenty, plenty, a huge amount of security. And that was not true.
00:24:58.938 - 00:25:24.982, Speaker C: And if these individuals didn't know what projects were doing, like no one does other than the projects themselves, I found it actually quite disturbing that basically these things were ostensibly protecting quite a bit of value, and actually nobody knew apparently what was really the protocol was doing. So that sort of inspired me to try to talk about it. I see.
00:25:25.118 - 00:25:28.966, Speaker A: Makes sense. I'm going to keep going through this list with the highlights.
00:25:29.070 - 00:25:30.030, Speaker C: Sounds good.
00:25:30.222 - 00:25:42.634, Speaker A: Number five was snarks targeting r one cs cannot support lookup arguments. I'm assuming that this is potentially in reference to the work that you then started to do on Lasso.
00:25:43.454 - 00:26:41.778, Speaker C: Partially, I'd say this. This one kind of fed into a paper I have on customizable constraint systems, which predates Lasso and Jolt, and actually for ZkVMS is actually rendered irrelevant by Lasso and Jolt, basically because jolt avoids constraint systems almost entirely. The constraint systems that it does use are so simple that it really doesn't matter what kind of constraint system you use. We use r one cs, but really they're just like those simplest constraints you can come up with. You can use anything. But the customizable constraint systems paper was pointing out a couple of things basically known some check based snarcs, spartan in particular, just directly supports not only r one cs, which is how it was described for Roncs, but you don't need any new ideas, you just have to understand the protocol and you'll see it actually can prove statements about much more expressive constraint systems. And that's sort of CCS.
00:26:41.778 - 00:27:05.802, Speaker C: Customizable constraint systems are sort of what we thought was the cleanest and most expressive constraint system that something like Spartan could prove statements about. But yeah, there were some works that were developing some check based snarks also looking at non subject based snarks for r one cs and sort of didn't let the subject based snarks for r one cs have access to the lookup arguments.
00:27:05.898 - 00:27:06.330, Speaker A: I see.
00:27:06.402 - 00:27:42.964, Speaker C: And so they claimed significant speed improvements over something like Spartan, when I think the speed improvements are really just due to the fact that they didn't let Spartan use lookups and they did let the new stuff use lookups. So that's the point I was trying to highlight there. And now of course that jolt is all some check based and uses Spartan four r one cs. Everything else is lookups. I don't think any of this is unclear anymore, but at the time of the misconceptions post, I think it was unclear.
00:27:43.124 - 00:28:21.852, Speaker A: I should also say the sentences I'm saying you are arguing in this article that these are incorrect. That snark targeting r one cs cannot support lookup arguments is an incorrect statement, right? Yeah. Another incorrect statement that you believe is incorrect is believing that fyet Chimere transformations generally lead to no major loss of security except when applied to the sequential repetition of a base protocol that actually has come up on episodes since the Fyet Shamir transformation as a place of potential security loss. And that's just interesting that you brought that up back then, even for me.
00:28:21.948 - 00:28:29.228, Speaker D: Can you unpack that sentence in a nice way? I think I know what it is. I can map it to something that I understand.
00:28:29.396 - 00:28:40.080, Speaker A: And just to say it again, the misconception he's saying here, people believe that there is no loss of security when using Fiat Shamir, but that is incorrect.
00:28:40.272 - 00:29:07.288, Speaker C: Yeah, so I'm happy for the opportunity to say something here, because actually, I think the blog post this is sort of buried in the post. It's very technical. I think it's really good to speak about it right now. So fiat Chimere can lead to security issues in two ways. Okay? One is basically, like, it's just not implemented right. And this comes up over and over and over again. And I mention it sort of as a PSA in the post, but, like, people have been aware of this for a long time, and somehow the mistake kept happening.
00:29:07.288 - 00:29:29.158, Speaker C: For fiat shamir to be secure, you have to fiat shamir hash anything that the adversary might control, and often people don't do that, and that makes it attackable. Okay? And so people will call that weak fiat shamir. I mean, I think we should just maybe just say, like, that's not fiat shamir. Like, don't do that. But people call weak vyat shamir.
00:29:29.296 - 00:29:35.962, Speaker A: I've heard that. Yeah. But, like, I've heard it just, like, used. Do people just use weak fiat chimere? Sometimes?
00:29:36.058 - 00:30:02.064, Speaker C: Yeah. I mean, I'm not sure why anybody would intentionally do it just because fiat chimere hashing is, like, rarely a bottleneck. So I don't know why you would want to kind of risk. You know, I think the general message for practitioners is, like, when in doubt, hash it. I mean, there are some settings where, you know, if you're hashing, like, a whole description of a constraint system, and the constraint system has no short description. Like, that can be really gross, but typically, it's not a bottleneck. So, like, when in doubt, hash it.
00:30:02.064 - 00:30:46.520, Speaker C: If the adversary might control it, hash it. And so that, you know, that's really just a bug that some practitioner, some papers forgot to write down, like, hash this as well. And so people implemented, like, the paper, word for word. And so trailabits discovered this bug in a bunch of snarks. And depending on context, this might or might not be a completely catastrophic vulnerability. Paul Grubbs and his colleagues had a paper. Paul Grubbs is a faculty at Michigan recently looking at kind of a systematic look at snark implementations, and found this bug all over the place, and I believe, actually found another exploitable deployment of this bug anyway.
00:30:46.520 - 00:31:33.344, Speaker C: So that is one major way fiat Shamir Khalidza lost his security. That is not what this item of the post is about. This item of the post is about the fact that even if you do implement it correctly, the number of bits of security in the interactive sense of the protocol you're applying fiat Shamir to to make it non interactive. Often people just assume that the fiat Shamir protocol has the same number of bits of security as the interactive one you started with. And as we discussed before, these two words, bits of security for interactive and bits of security for non interactive don't necessarily mean the same thing. But yeah, so they assume it's the same number and it's not always the case. And so there's really like a canonical example where there actually is just a catastrophic loss of security.
00:31:33.344 - 00:31:56.808, Speaker C: Even when you implement fiat chimere correctly, which is you take a protocol with like one bit of security, you amplify the number of bits of security by repeating it sequentially once, then twice, then a third time, then a fourth time. Right. That drives the security down to say, 128 bits of security. If you repeat 128 times and then you fiat Shamir, that that won't be secure at all. And like people do know that.
00:31:56.896 - 00:31:59.328, Speaker D: Is there an easy explanation to see why it's not secure?
00:31:59.456 - 00:32:25.520, Speaker C: There's a simple attack. It's sort of called a grinding attack, but like basically in the interactive setting, the prover would have to sort of get lucky on all 128 repetitions all at once. Has to get lucky all at once. But once you fiat Shamir it, you can sort of attack the first iteration independently of all the others. And it'll only take a couple of attempts to succeed to find a convincing proof just for that iteration.
00:32:25.712 - 00:32:28.456, Speaker D: I see. And then you can continue grinding the rest.
00:32:28.520 - 00:32:33.320, Speaker C: Yeah, it's like you defeat one and then the next, and then the next, and you don't have to get lucky all at once.
00:32:33.392 - 00:32:33.784, Speaker D: Got it.
00:32:33.824 - 00:33:01.596, Speaker C: Right. And so, but I think people generally think, oh well, that's the main thing to be worried about. If you're not doing sequential repetition and you do strong feature me or not weak featuring, you should be fine. This post is pointing out that there's a really subtle and amazing attack, which I learned about from a paper of atema et Al, and through the folks from Nethermind, which I have a paper with, which I'll sort of plug briefly. But I think they discussed it on this podcast at some point.
00:33:01.700 - 00:33:07.948, Speaker A: Totally. I was about to say this is where I think we covered exactly that. We'll try to dig that up. The Nethermind episode. Yep.
00:33:08.076 - 00:33:42.194, Speaker C: Wonderful. So I just recently talked to a couple experts recently who were blown away by this attack. They literally read my post, they didn't quite understand the attack in the post, and then I sketched it for them over email and they were like, oh my God. So this is when you take a multi round protocol and it's crucial. Like there has to be two or more rounds so the attack doesn't work if it's like a what? You know, just two messages it's got to be like, you know, three or four or more messages. You know, say it has 64 bits of security. You're not happy with that, you know, because if you fiat Chimera, like, that's not secure.
00:33:42.194 - 00:33:56.440, Speaker C: So you repeat it in parallel twice to get 128 bits of security. And then you fiat Shamir that this is not sequential repetition, it's parallel repetition. And then Fiat shamir, okay, that doesn't work.
00:33:56.632 - 00:34:03.512, Speaker D: So here's the first dumb question. What does it mean to fiat chimere? Two parallel instances. What is the initial hash?
00:34:03.648 - 00:34:23.719, Speaker C: Right. So the parallel repetition of the base protocol is just a new interactive protocol. Forget about Vietromere. So it's like you have a protocol with 64 bits of security. You want to reduce that or improve that to 128 bits of security. So rather than running it once and then running it again, you're going to run it twice? Yeah.
00:34:23.751 - 00:34:25.239, Speaker D: Like simultaneously or something?
00:34:25.391 - 00:34:44.009, Speaker C: Yeah, exactly. That is a new interactive protocol. It has the same number of rounds as the original protocol. Unlike the sequential repetition where you would have doubled the number rounds, this is the same number of rounds. Okay, now fiat chimere, that, you know, that's just an interactive protocol. It makes sense to fiat Shamir, fiat chimere, that will be no more secure than if you hadn't done the repetition.
00:34:44.081 - 00:34:48.289, Speaker A: At all, if you hadn't done that parallelization basically, right?
00:34:48.321 - 00:35:01.585, Speaker C: If you hadn't done the repetition, like literally repeating it once as long, if you started with a two round protocol, you repeated it once, like in parallel, you know, so there's two repetitions in parallel. And you fiat chimera, you might as well have not done the repetition at all.
00:35:01.649 - 00:35:03.333, Speaker D: Interesting. Okay.
00:35:03.744 - 00:35:31.938, Speaker C: And again, the attack is very simple. It roughly amounts to attacking the first repetition on its own. You sort of use round one, like the first verifier challenge, to attack one of the two repetitions. And you only need about two to the 64 attempts before you defeat one of the two. You're not going to defeat both of them at once. You're only going to defeat one of them. But then you still have another round left to attack the other repetition.
00:35:31.938 - 00:35:44.074, Speaker C: And I know, I know that you don't get all the details when I'm saying this on the fly, but I'm going to try to go back to the misconceptions post and add just, I think a little picture of the two rounds repeated.
00:35:44.194 - 00:36:26.246, Speaker D: The point is, okay, fundamentally what's going on is these sequential constructions, these parallel constructions can always be reduced to just instead of attacking this much larger space, like square of the size. Actually, what you're doing is you're doing something two times the size because of the fact that you can exploit the structure of it such that instead of having many parallel instances, each of which is independent, in reality, what you have is we have a single instance which is sequential, and they're all fundamentally sequential in some way. It just requires that you attack a single independent instance. You only need to the 64 at any point in time, or, like, attempts or whatever.
00:36:26.270 - 00:36:41.678, Speaker C: I think that's the right. Yeah. That's the right intuition. Yeah. And so, you know, and the general message of, you know, this part of my blog post, I mean, mainly it's. It's a. It's a PSA, because this is like a huge foot garden where someone at some point is, like, gonna blow their foot off unless everyone is aware of it.
00:36:41.678 - 00:37:15.492, Speaker C: That's right. The kind of. The right way to use fiat Shamir in practice is to first show that the thing you're applying fiat Shamir to satisfies a stronger soundness notion than, like, just standard soundness called round by round soundness. And this captures the intuition that if the prover in the interactive protocol kind of has to get lucky all at once, like, in one round, like, it can sort of get a little lucky in the first round and then a little more lucky in the second round. I see. Then when you apply Fiat Shamir, it's fine. There's no loss of security.
00:37:15.492 - 00:38:06.428, Speaker C: And so with the nethermind folks and my postdoc, Alex Block and others, we showed that Fry is round by round sound. It actually hadn't been shown before, so we actually didn't know in the random oracle model that Fiat Shamir, Fry was sound. It's not that I actually thought it wasn't, but we should know that it is, at least in the random Oracle model. Then, actually, a group from starkware showed the same thing, basically the same result around the same time. With Nethermind, we had some other results that, you know, other things people were doing that potentially could have been attackable because they had more than one round are also round by round sound. And so it's all okay, right? There was no new attack in the protocol. We were just sort of showing that Fyat Shamir was safe when it hadn't actually been shown before.
00:38:06.516 - 00:38:22.560, Speaker D: Here's a very quick, dumb question before we move on to the next point is, what is the intuition for round by rounds on this? I know you said, oh, you can't get too lucky, and essentially, you can't get lucky. In one and get a little bit lucky in the next. And then like at some point like that adds. What is the, like, more clear definition of it?
00:38:22.592 - 00:38:55.990, Speaker C: I don't know if there is serious. I guess that is most of my intuition. I mean, I think it sort of rules out this sequential repetition canonical example. You know, if the prover just has to, you know, it has a probability half of getting lucky in the first repetition and then a half of getting lucky in the second repetition and so on. And so, you know, it can kind of get a little bit lucky in each repetition. And that's exactly the situation where this grinding attack on Fiat Shamir works, where, you know. Yeah, and Rambaround sound basically says that grinding attack can't work.
00:38:55.990 - 00:38:56.754, Speaker C: Got it.
00:38:57.374 - 00:39:02.638, Speaker D: It's just essentially like the definition is explicitly to rule this, like particular attack out rather than like.
00:39:02.766 - 00:39:06.462, Speaker C: That's my interpretation of it. That's my. Yeah, that's my interpretation.
00:39:06.638 - 00:39:31.674, Speaker A: All right, I'm gonna move on to misconception number 1414. All right, so we are, we are skipping like, as I mentioned, I'm just. I selected a few of them. I have two more. So just before you mentioned Binnius briefly, number 14 was the misconception is the key to better performing snarks is working over ever smaller fields. And did they prove you wrong? What happened there?
00:39:33.734 - 00:39:46.258, Speaker C: Great, great. So great. So this predated binius. So a little context there. So binius was at least partially inspired by lasso and jolt. And this post predated last one, Gerald. Right.
00:39:46.258 - 00:40:03.378, Speaker C: So at the time, you know, this was really sort of a fight between elliptic curve based snarks, meaning, you know, the commitments can be elliptic curve, so the field's big and, you know, like hashing based snarks and, you know, using fry in particular. And the, you know, the field is small.
00:40:03.426 - 00:40:07.866, Speaker A: Small field, folks. Yeah, like the flunky threes and stuff.
00:40:08.010 - 00:40:51.184, Speaker C: Yes. And so you could have summed up a lot of my points, but not all of my points by me just saying, hey, curve based snarks can be faster than the hashing based snarks, even though they work over big fields. And jolts proves that. Correct. Because it's at least as fast as the current hashing base ekvms or what have you. But there's another point I was trying to make, which is that in certain applications, you really don't want to work over the small field because the statement you're proving is natively defined over a big field. And in particular, this comes up whenever you're proving a statement about elliptic curve cryptography.
00:40:51.184 - 00:41:46.124, Speaker C: And so I think the main application people always have in mind now is Zkevms, and they're a big bottleneck is like catch hack hashing, say. But if not for that, another big bottleneck would be proving knowledge of digital signatures. And so if in a world where the hashing was the bottleneck, the signatures were, you actually probably would want to work over the big fields. And you could imagine a world where one day we're able to work over whatever field we want, when we want to, and glue everything together. And then what I'm saying would be, right, like big parts of the snark will work over a big field. And then I guess the final thing I'd say is just the underlying techniques that led me to try to make this point. And really I was trying to push back on what I thought was just a general idea which has validity to it, but it's just been taken too far.
00:41:46.124 - 00:42:16.660, Speaker C: But the techniques that kind of led to the points I'm making, the sum check protocol, and all of these things, they led to Binius. What binius has done, in my view, and this isn't a controversial view, is it's taken the small fields to their extreme, where now the characteristic is two. And I actually don't like the small fields term. It's small characteristics. You work over 128 bit field, but it's like you do as much, you keep as much of the prover's work as possible over the base field or something like that.
00:42:16.732 - 00:42:28.756, Speaker A: So, wait, but would you still stand by the fact that going back to the statement, the key to better performing snarks is working over ever smaller fields being incorrect? That was what you were saying. Would you adjust that?
00:42:28.860 - 00:43:16.278, Speaker C: So I would say two things. One is it depends on what you're proving. And this whole thing about proving statements about elliptic curve cryptography, it's not like some isolated application, it's like half a blockchain is just proving, you know, digital signatures authorizing things, right? And like today's digital signatures are based on elliptic curves. That's not like some small thing, right? So it depends on the application also, you know, like these, these folding schemes that get, you know, the overhead of recursion very low. What that, what that basically means is you can break big computations up into really tiny pieces and, you know, prove each piece separately and aggregate the results. And the aggregation step won't be the bottleneck because the folding lets the recursive aggregation be so fast. That can be really important.
00:43:16.278 - 00:43:39.388, Speaker C: If proverbspace is a major concern for you. There's been efforts to also do that with hashing based snarks, but they still have downsides. That's another reason the curves could stick around. I don't think the matter is settled, but I will say I'm more bullish on the small characteristic fields thanks to the binius advancements, than I was when this post came out.
00:43:39.556 - 00:43:42.492, Speaker A: Cool. Yeah, because that one, when I was going through it, definitely.
00:43:42.588 - 00:44:19.604, Speaker C: That got the most pushback, I think. Yeah, that one got the most pushback. But ultimately I think the developments since then have sort of fleshed out what I had in my head a little more. I couldn't talk about Lasso and Jolt, which I did in my head. In my head at the time. They have these nice properties that while they work over a big field, the prover only commits to small values like numbers between zero and two to 20 or something. That's a significant performance boost for the techniques that do use the big fields and heavily exploited to see the performance out of jolt that we see today.
00:44:19.604 - 00:44:28.024, Speaker C: And that was something I couldn't talk about in this misconceptions post that I can now. So that was something that was in my head that I wasn't able to.
00:44:28.024 - 00:44:36.476, Speaker A: Say in the post was one other thing that maybe didn't factor into your thinking then what? Like was sort of where hardware would be at or were you thinking about hardware?
00:44:36.580 - 00:45:07.020, Speaker C: No, I wasn't thinking about hardware too much. You know that the small characteristics field does sort of push that point forward heavily. I think these fields are nicer for hardware. I mean, you know, when I was thinking about performance comparisons, like to the extent that existing snarks are using vectorization in the small fields, like I had that in my head already. But I wasn't thinking too much about FPGA's and Asics, which these small fields also seem friendly to. Especially characteristic too.
00:45:07.172 - 00:45:31.302, Speaker D: I mean, I will say what is it? Apple Arm has an instruction for multiplication of two polynomials that are whatever represented value and date or something like that. I think it just exists on every Apple computer and iPhone, which is pretty cool. I think it's for Reed Solomon encodings actually is what it is. But it has direct applications of course to potentially some of these types of ZK proofs.
00:45:31.398 - 00:46:01.244, Speaker C: Yeah, even on these cpu's. Because AE's uses the field two to 128, they have built in operations. They don't use the same basis for that field that Binius liked to use, but you can still leverage it, things like that. The statement that just came out of my mouth. I had no idea about ten months ago that only came from talking to the folks behind Binius, right? That's way deeper knowledge of hardware than I would have any idea of.
00:46:02.744 - 00:46:25.980, Speaker D: I'm going to see if I can find the specific instruction, but it's definitely in the Apple manual somewhere. It's not even that for AE's, actually for small multiplications of polynomials, because I think it's specifically for QR code or something like that, and some sort of syndrome decoding type thing. It's very weird. I did not know about this. I think I posted about it in the discord sometimes, but it's like there's many messages there.
00:46:26.052 - 00:46:33.344, Speaker C: It's fun to just piggyback over these things that came into the hardware for totally different reasons. It's nice when you can take advantage of it.
00:46:33.964 - 00:46:55.168, Speaker A: The last point was number 17, which is and I think this might still be true, the misconception being snarks are performative enough today that they should now ossify. Do you feel we are any closer to actually getting our our standard snark or any sort of subset of a snark becoming a standard?
00:46:55.256 - 00:48:10.024, Speaker C: So I do. So first let me say, I think since this post came out it's been made clearer, more concrete, like just how slow. I mean, that's my terminology. You could say exactly how fast or slow a lot of snarks are, and in my view there too slow for a lot of their intended applications, but I also think they can get a lot faster still. I'm still hearing this view that there's some more performance juice to squeeze, but it's not much and we should start focusing on hardware speed ups, focusing on standardizing snarks. I'm actually quite optimistic now that we can reach a point where that might make sense quite soon. I actually didn't think that would necessarily be the case when I wrote this post, but I think we can actually so, as I've said very recently when discussing Jolt's performance and other ZKVM's, I think a ZKVM today is 500,000 times or more slower than just running the VM without the ZK part, which means succinct as we've talked about before.
00:48:10.024 - 00:49:24.114, Speaker C: But I actually think we can get down to about 10,000 times slower rather than close to a million times slower, and fairly soon. And then I think it won't be too slow, and then I think it will make sense to focus on hardware and if not standardizing, I've sort of had mixed thoughts about standardizing, but it's going to take a lot of effort to reach a high level of confidence in the bug freeness of various implementations. These are very complicated cryptographic protocols. I mean, people know what I'm saying, that the Ethereum foundation is launching a competition with $20 million of prizes to push this forward. And so it'd be a lot easier if we sort of converge on not the right snark, but a right snark so that effort doesn't have to be repeated over and over and over again. I mean, hopefully we also get better at making the effort, like, lower and reproducible anyway, so. Yeah, but my view as to that final number 17, that snarks are too slow and can be improved and must be improved, is only stronger held view today for me than ten months ago.
00:49:24.494 - 00:49:39.412, Speaker A: But it sounds like you think we're closer to being able to ossify at least a little bit, at least hit a step change where probably not forever, but for a period of time, we all start using the same thing.
00:49:39.508 - 00:50:39.624, Speaker C: I do. I mean, it might be overly optimistic, but we could talk about this a little bit later. I mean, I think some of the recent discussion on Twitter, especially like following the release of Jolt, might give the impression that there's more disagreements than there actually is. And I'm actually sensing like, a shift where certain views or techniques I've been advocating for for a very long time and, like, getting very little traction on actually people seem to be coming around to, and if they do, then of course I have a view of what I think is the right approach. And if people come around to that, then great, then we've converged. I'll also say it's never going to be one size fits all. I think for large enough statements there'll be a right way to do things, but you'll still have settings, and I've been encountering them in certain applications where the statement being proved is pretty simple.
00:50:39.624 - 00:51:21.608, Speaker C: You can describe it in a couple million constraints, and you want the verifier to be really fast, and there's no room in the application to aggregate proofs or prove many of the statements at once. You really have to just prove one small statement and have the verifier be fast and the proof small. In that setting, my preferred techniques don't necessarily help or make the verifier cost too big and recursion would be a bottleneck or something like that. So you can't recurse to bash the verifier cost down. So that will always be an issue. It's never going to be just one snark for everything. But most applications, the statements are at least somewhat big.
00:51:21.608 - 00:51:24.664, Speaker C: And then I think we can converge in at least those settings.
00:51:24.784 - 00:51:55.558, Speaker D: It's funny because right now everything is super general purpose. We're just throw the kitchen sink at it. Like we want, like this thing to work for everything. But one could imagine, like the existence of specific protocols for things that are very common, that are extremely efficient. Like you can provide a sink certificate for it and then verify a sink certificate in some, like, very, very efficient way, and like a secondary problem. So it is kind of interesting. I feel like right now it's just like the, this is like the, the Moore's law thing, right? It's like CPU's.
00:51:55.558 - 00:52:27.852, Speaker D: CPU's just got better and better and better. Everyone was like, why the hell do you need anything other than a CPU? We just wait two years and this thing gets infinitely better. But at some point we're going to probably develop the GPU and the FPGA version of this, where it's like, okay, no, we need to do this one thing very efficiently because we do this one operation, linear algebra or whatever, 50 million times. Let's get good at that and defer a succinct proof of that, or a ZK proof or whatever you want to that special device for that one specific case where it turns out to be extremely useful.
00:52:27.988 - 00:52:43.348, Speaker A: All right, well, now I think it would be a really good time for us to talk about Jolt and maybe Lasso and jolt. I know last year when you announced this work, you shared it in a pair. I think it would be great to go into both of these. So, Justin, what is lasso? What is jolt?
00:52:43.476 - 00:53:19.038, Speaker C: Great. Yeah, I think Lasso is sort of a technical hammer, so I don't think we have to talk too much about it. Back in August, or when we sort of announced Lasso and Jalt, like Lasso was implemented and jolt wasn't. So what is lasso? It's what's called a lookup argument. And there's two ways to view a lookup argument. They're completely equivalent, but one is it lets the prover, an untrusted prover, commit to a bunch of values and then prove that each of those values lies in some predetermined lookup table. Some predetermined table of values.
00:53:19.038 - 00:54:08.168, Speaker C: Okay. A completely equivalent way to look at it, really. This is equivalent to a variant called what I call index lookup arguments, but the details aren't super important, is it's a way for a prover to prove very efficiently that a bunch of committed values are all the outputs of a simple function. So it's like the prover commits to a billion values and then very quickly proves that for each value that's committed, that actually equals f of xi. The I've committed value is f of x I, where f is known to prove and verifier and xi is also committed. Okay, so I actually prefer the second view, although it's sort of. I think I'm the only one who takes that view with that view.
00:54:08.168 - 00:54:42.722, Speaker C: It's just. It's a really efficient snark for, like, super highly data parallel computation. So evaluating the same simple function f over and over and over and over again. Now, what jolt is, is something called a zkvm, specifically for something called the RisC V instruction set. It's a snark that lets approver prove that it correctly ran a computer program on some. Wanna witness where the computer program itself is specified in RISC V bytecode? Risc v assembly. This idea of designing a ZkVM for Risc five was.
00:54:42.722 - 00:55:14.150, Speaker C: Was pioneered by a project called RiSC Zero, which had incredible foresight to see the major benefits of using an instruction set RIsC V that was not designed by snark researchers, but rather designed by the computer architecture community and came with all sorts of great tooling and compilers and things like that. Jolt is basically just another way of proving the same statements that RISC Zero is already proving today and has been proving a year or a year and a half or something like that already.
00:55:14.222 - 00:55:25.654, Speaker A: So, just so you know, Justin, I don't know if you know this, but, like, I mean, I'm an investor in both risk zero and succinct through ZKV and as an angel, and I think, Guillermo, you also have.
00:55:25.734 - 00:55:27.646, Speaker D: I'm also an investor in Risk Zero.
00:55:27.830 - 00:55:54.482, Speaker A: We know. We know that group quite well. But actually, I just want to stop you, because just before you jump into jolt in more detail, I actually want to ask you one thing about Lasso and these look up arguments, because I feel like there's been this line of work. There was. I don't know what the first part of this is, but it's culk and CQ. I know that that's like one line. Is Lasso in the same line of work? Is it an addition on CQ, or is it using.
00:55:54.482 - 00:55:58.962, Speaker A: Is it for something else? Is it coming from different, like a different track?
00:55:59.018 - 00:56:24.528, Speaker C: Yeah. Great question. So it's inspired by that line of work. I don't know whether to call it part of that line of work or not. So essentially, we looked at that line of work, all of those works, starting with caulk. Okay? So none of them use the soundtrack protocol, and some of them seem very specific to KZG commitments in particular. And it's not clear that they would work when KZG commitments don't use fiat Shamir, it's not clear that they would work if they did use feature.
00:56:24.528 - 00:56:31.684, Speaker C: So we basically, we looked at what was going on in those protocols, and we said, well, let's use some check.
00:56:32.424 - 00:56:34.712, Speaker A: Okay. And change it for that.
00:56:34.768 - 00:56:34.984, Speaker C: Yeah.
00:56:35.024 - 00:56:35.400, Speaker A: Okay.
00:56:35.472 - 00:56:38.044, Speaker D: It was like, justin's favorite tool is like, how about.
00:56:39.544 - 00:57:38.734, Speaker C: Right? And this is. And by the way, this is, you know, in my view, I've been trying to get people to use some check for a long time, kind of going on 15 years, like, before I even, like, knew what snarks were, really. I was working on half of a snark, you know, snarks without the commitment schemes. And, you know, if. I'll be thrilled if people just start, like, reaching for some check, like, they don't have to use jolt or whatever. I mean, obviously, it'd be even better if they decide to do that, but as long as people start using subcheck, I'm thrilled, you know? So we particularly took inspiration from Baloo, which clearly had sort of a matrix vector multiplication going on in there, and then we use some check to prove that matrix vector multiplication. And the way we did that was very similar to something that was happening in the spartan paper from the 2019 to 2020 era.
00:57:38.734 - 00:58:08.978, Speaker C: And we just have a few more observations on top of that to squeeze some extra performance out. Basically, make sure the prover only commits to small values, not big values. So that's lasso, and it comes with some other nice benefits. Like, for a lot of the tables, lookup tables people use in practice, Lasso is sort of unique. No one has to commit to the table. I think anything that doesn't use the semcheck protocol is not going to have that property. So that's lasso in a nutshell.
00:58:09.066 - 00:58:09.706, Speaker A: Got it.
00:58:09.810 - 00:58:28.378, Speaker D: Very dumb question here. Before we get too far, you said it only makes sense for a simple function. F. How do you quantify the simplicity of a function? Actually, what does it even mean to be simple here? I know you did early work on this in a very specific way, which is polynomial degree of weird Boolean functions, but I'm just curious what it means here specifically.
00:58:28.546 - 00:59:02.394, Speaker C: I guess the way I'd say it is one way to view lasso is from the verifier's perspective, it is a reduction from evaluating f many, many, many times, once per lookup to evaluating f just once, really. It's not evaluating f itself. It's evaluating the multilinear extension of f. But whatever. Basically, you want f to be simple enough that that multi linear extension evaluation is fast for the verifier. Got it. There are some functions, f, where that multilinear extension evaluation would be extraordinarily expensive.
00:59:02.394 - 00:59:04.604, Speaker C: And so it's a very technical notion of simple.
00:59:04.644 - 00:59:15.684, Speaker D: So it's something like, if I may, it's like the support of. So it's the number of nonzero coefficients in the multilinear extension is like the complexity of the function that we care about here. Is that right or is that.
00:59:15.724 - 00:59:36.774, Speaker C: Yeah. And it doesn't have to be in like, you know, it could be in any basis coefficients in whatever basis. It just needs to be some quick evaluation procedure. Having few coefficients in your favorite, you know, basis for multilinear polynomials is one way to do that. Yeah. I mean, it turns out that all of the tables that we need for Joel to have this property, so no one has to commit to these tables and things like that.
00:59:37.514 - 00:59:52.546, Speaker D: Bitwise operations certainly will have this property and things like that, but you could imagine much more complicated operations to have a sparse basis. You might need a very complicated basis in which you're doing all of the work in the reduction or something.
00:59:52.610 - 00:59:53.234, Speaker C: Yeah.
00:59:53.394 - 00:59:54.530, Speaker D: To this new basis.
00:59:54.602 - 01:00:11.266, Speaker C: Exactly. And in a theoretical sense, this is not a practical protocol, but there's a section buried somewhere in the lasso paper, or like sort of any function with small circuit complexity, it has some reasonably low degree extension that can be quickly evaluated. I believe that. Right.
01:00:11.330 - 01:00:13.882, Speaker D: I was like, this feels like right up your og alley.
01:00:13.938 - 01:00:14.178, Speaker C: Right?
01:00:14.226 - 01:00:18.002, Speaker D: Like, this is like defining the complexity of this feels like.
01:00:18.178 - 01:01:03.086, Speaker C: Well, kind of, yeah. So this has always been a thorn in my side, where my favorite protocol is like, sort of, for the verifier to be fast, you either need, like the multilinear extension. Like I said, things get nice for the pre reviews multilinear extensions. But the verifier is going to be fast, mainly if the multilinear extension is quickly evaluable. But what does it mean for the modular extension to be quickly valuable? And explaining this to people has been a 15 year thorn, but it's very related to how error constraint systems are by design, very structured so that the polynomials capturing the constraint system are quickly valuable. This is completely analogous to that. Yeah, that's the gist.
01:01:03.086 - 01:01:04.406, Speaker C: Okay. Okay. Okay. Got it.
01:01:04.470 - 01:01:27.702, Speaker A: Cool. Let's go into jolt a little deeper. You sort of put it in the same category as the RisC zero ZKVM. You didn't mention it, but it would be similar to the SP one sync's Vm. It's funny. I'm not actually sure. Is Jolt the name of the ZkVM, or is it the name of the snark system that you've actually created? Is there any sort of.
01:01:27.702 - 01:01:35.382, Speaker A: Because, for example, SP one we know is using Plunkett. I actually don't know what risk zero is using. I'm guessing girl 16.
01:01:35.478 - 01:01:37.054, Speaker D: It's a combination of a bunch of stuff, I think.
01:01:37.134 - 01:02:07.902, Speaker C: Okay, so, firstly, so RisC Zero applies a stark that they implemented sort of themselves, to an error capturing the Risc V CPU that they also implemented themselves. SP one feeds an error that I guess they implemented themselves, I think, building on some other projects as well. They feed it to plancky three, and then plaque e three is sort of the stark for the air, the stark backend for the air that they feed the planky three.
01:02:08.038 - 01:02:08.534, Speaker A: Okay.
01:02:08.614 - 01:02:46.614, Speaker C: With Jolt, I would say jolt is like the ZKVM, but agnostic. It's really the polynomial IoP in the ZKVM, if you will. So I'm very happy to switch the polynomial commitment scheme in Jolt to something like. Still call it Jolt. I'll call it Jolt with Binius, instead of Jolt with Hirax commitment or zero morph commitment or something like that. So, Jolt is really the polynomial IOP, capturing the Risc V CPU execution. And I can go into a little more detail about how that polynomial IOP works and how it compares to SP one s and risk zeros, if that would be helpful.
01:02:46.694 - 01:02:53.998, Speaker A: Would you say that's where a lot of the work that you've. The innovation is on this polynomial IOP side of things?
01:02:54.086 - 01:03:01.246, Speaker C: Yeah, I mean, Jolt really just is a polynomial IOP, and plug in whatever commitment scheme you want to turn that polynomial IOP into a.
01:03:01.430 - 01:03:02.142, Speaker A: Interesting.
01:03:02.238 - 01:03:18.158, Speaker D: What are the differences? Right. Like, what is the difference between kind of sticking a big brain risk five virtual machine into some horrible representation versus this. Why is this representation so nice? I will ask very dumb questions, but I'm just curious from this one.
01:03:18.326 - 01:03:52.364, Speaker C: Yeah. So let me give a brief overview of the jolt polynomial IOP and sort of compare it to the pre existing zkvms and go from there. So, just as a CPU contains several components, so does jalt really mirroring those components, and I think the other zkvms probably function in a similar way as well. So CPU just repeatedly runs fetch, decode, execute, fetch decode, execute, fetch, decode, execute. Then the only other thing going on in a CPU is reads and writes to random access memory, basically.
01:03:52.444 - 01:03:55.712, Speaker D: There's some registers in there, too, but I don't think you have those in the ZKVM.
01:03:55.868 - 01:04:06.688, Speaker C: No, we do. We do. So risk. Yeah, so Risc five has 32 registers. Like, you know, so we have to. But. Yeah, so the registers are part of like the fetch decode.
01:04:06.688 - 01:04:14.288, Speaker C: You know, the execute is telling you, like, you know, what are the source registers for the instruction? What's the destination register? You know, where should the output of the instruction be saved and stuff?
01:04:14.336 - 01:04:19.528, Speaker D: Yeah, people usually have been. I know they stick those in ram somewhere instead of actually having like some dedicated.
01:04:19.656 - 01:05:25.026, Speaker C: Yeah, exactly. So ultimately, like, you know, the snark techniques, we have to, you know, verify that the registers are being read and written correctly, like, are really no more efficient waving my hands a little than just general random access memory. So, like, there's not really a reason to separate the registers from the rest of memory necessarily, right? There are, you know, there are some in the weeds reasons that you can benefit from that. But anyway, yeah, so jolt has the following components. Okay, so firstly, the newest thing in jolt, I would say, albeit sort of the one I'm tied to like the least in my personal hopes and dreams or whatever, is handling the execute part. We do it purely with the lasso lookup argument. We say, if you need to evaluate this instruction on these inputs x and y, we just treat that as a lookup into the evaluation table for that instruction, which is exactly this view I was saying, where lookup arguments are great for evaluating the same simple function over and over and over again.
01:05:25.026 - 01:05:57.214, Speaker C: What does a cpu do? Execute, execute, execute, execute, execute. It's a match made in heaven. That's how we handle execute is just straight up lasso. This lookup table is enormous, size 264, 228. But it's so structured that no one ever materializes the whole table and so forth. People were already doing similar things, just without quite this perspective and without using sumcheck, which I think is key to getting the prover performance really high. That is one part of jolt.
01:05:57.214 - 01:06:32.854, Speaker C: We handle the random access memory with what's called offline memory checking techniques, which also are quite popular already today. But again, we implement those offline memory checking techniques with some check. We particularly use something from spice, which is a variant of something from Spartan. Anyway, and then finally, I'd say we handle fetches. Again, memory checking. We handle the decode part. With r one cs we have this constraint system, which again, it's a minimal constraint system.
01:06:32.854 - 01:07:25.946, Speaker C: There's something like 40 ish constraints per step of the CPU, and I think there's 80 witness elements and they're all very small committed per step of the cpu. That's doing things like if you fetch the program counter from the right memory cell, you then have to decompose it into several different fields or something. And making sure we do the decompositions right. That's the gist. That's all a jolt. And I would say the main differences over existing zkvms in my view, of course, the fact that it uses the suncheck protocol everywhere to minimize, especially the amount of data committed by the prover, is that's really what I'm personally connected to. The main takeaway I want from people to take from jolt.
01:07:25.946 - 01:08:06.712, Speaker C: The other things I'd say is it does more everything is this very minimal r one cs and offline memory checking. Everything is offline memory checking other than the r one cs. That's a powerful thing. I think that's where we get like you can specify a new instruction for your instruction set, just with very few lines of rust code. That's just very human readable, specifying how you decompose, executing the instruction into executing instructions on smaller inputs or something like that. I think that's valuable. But I want to be clear about something.
01:08:06.712 - 01:08:46.299, Speaker C: I guess doing everything through lookups, which under the hood is offline memory checking, at least as the way jolt and lasso do lookups, I think that's very powerful. I think for some instructions there's more performance to be had by going through a circuit implementation of the instruction. I think in most cases that performance overhead may not be worth the extra complication. It's very, very clean and simple to just do everything through these lookups. So I don't necessarily think people will only do things through lookups forever and ever from now on when they design zkvms, but there are definitely benefits to doing it that way. It's definitely nice and clean and simple.
01:08:46.451 - 01:09:06.063, Speaker A: You talk about designing a lot of this system around some checks. Does that sort of hint that one could also design a lot of systems, these systems around another paradigm? Could it be a folding based ZK VM? Could you just use another one of these techniques that people like to kind build around?
01:09:06.363 - 01:09:45.923, Speaker C: Yeah, so, I mean, one thing I'd say is like the state of the art folding schemes do use some check in them. Right. So that's like hypernova. And, you know, I guess the protostar like line of work is, you know, doing something like, but not literally doing some check. And so I think a sum check more is like a hammer. And the main thing to be aware of when you use that hammer is that the polynomials arising, like, under the hood in your protocol are multilinear, or at least multivariate instead of univariate. And that has ramifications both in terms of what commitment schemes you can use.
01:09:45.923 - 01:09:59.499, Speaker C: And there's much more subtle ramifications that I think people are slowly starting to understand. It's just a hammer for designing snarks where the prover commits to minimal amount of data, is my view.
01:09:59.611 - 01:10:29.086, Speaker A: Well, I guess what I was trying to say here is it's almost like you choose a track and then you build around that track. I do kind of wonder if you could, like, is there a chance for you to share with existing ZK vms and the ones that are, like, with the RisC V instruction set? Yeah. Can you actually mix and match? You sort of mentioned you could maybe use binius instead of what you have right now, but how much overlap in what exists out there is there with jolt, or did you have to build it again? Basically, did you have to redo the whole thing?
01:10:29.190 - 01:11:00.788, Speaker C: Yeah. So I think you basically do have to redo the whole thing, which is why it was a heavy lift for Sam and Michael and Avarsu to build this proof system. They really couldn't reuse much of anything. Fortunately, they were able to start from archworks and spartan, but nothing else, really. Some curve libraries, too. I guess it's a little hard to explain, but the fact that the polynomial IOP is just different, it means you have to start from scratch. The components of the proof system are.
01:11:00.788 - 01:11:45.220, Speaker C: There is a polynomial IOP, there is a polynomial commitment scheme, but the polynomial IP is totally different. And the commitment scheme needs to be at least a little bit different because it's multilinear instead of univariate polynomials. Maybe this is in the weeds, but people have been trying to use fry as a multilinear polynomial commitment scheme so that they can glue things together better an aside. But, like, I'll take this as an opportunity to revisit. I was saying before, I think there's more agreement out there than kind of the Twitter discourse might lead people to believe. So there's a new proof system, Stu Stwo, that starkware is working on, right?
01:11:45.292 - 01:11:46.180, Speaker A: Oh, yeah, yeah.
01:11:46.332 - 01:13:06.094, Speaker C: There were just some new numbers presented at the recent. You know, ZK Summit, and there was a ZK accelerate event right after that. So I was looking at the stew numbers, right? And like, in my blog post about Jolt, I had a pretty lengthy discussion about whether something like proof system like SP one would be able to do recursion efficiently if using like a snark, unfriendly hash function. And I was expressing concern that this recursion with something like Blake as the hash function would be too slow and would bottleneck the prover. And in the presentation about Stu, one of the presentations, there were very, very, very good numbers reported for proving Blake. And I was worried all of a sudden, like what? You know, did I miss something? You know, have I, you know, like, underestimated, like, the performance of some proof systems? But, you know, there was, there was Twitter discussion like, Stu is apparently getting mileage out of using a lookup argument that is using the sumcheck protocol that was inspired by Lasso and quite similar in many ways to Lasso. Actually, these good performance results are consistent with, like, what I was saying in the blog post is I think you'll need the sumcheck protocol to prove these hash functions fast enough that you can do recursion quickly.
01:13:06.094 - 01:13:32.994, Speaker C: And so there isn't actually any disagreement here. I agree with everything. And so, yeah, I mean, in my view, the key components of jolt are just like everything. It's kind of exploiting some check to minimize how much data the prover commits to, and then it's handling as much as possible with offline memory checking based lookups. And I'm more tied to the sumcheck part than to the lookup part. I think they're both powerful. Yeah.
01:13:32.994 - 01:13:56.086, Speaker C: So that's my view of what jilt is and how it compares to the existing proof systems. And there are ways to glue it together because Stu is only using some check, I think, in the lookups, everything else is based on starks and univariate polynomials. So there are ways to do it. Like, I'm hoping the community sort of fully embraces some check and like, yeah, some check through and through. It's not just one component of the proof system, if that makes sense.
01:13:56.150 - 01:14:23.706, Speaker A: Interesting. What's the plan for something like Jolt? So you've done an implementation, actually. I mean, this was interesting because you announced it last year, and then just recently, actually, I think the day of the CK summit, we saw the announcement for jolt again. But I was like, I wasn't clear when I saw that. I was like, what is that? And I guess that was, that was the implementation that was like, this is now ready to use, but what is, I mean, is this just like an open source library that you're sharing? What are you doing with it?
01:14:23.810 - 01:14:45.528, Speaker C: Yeah, so it is just open source. It's for anyone to use and build on. I mean, there's a lot of building left to do. So right now there's not what's called continuations implemented. So it takes like the entire execution trace. So there is five program, and it just proves it all at once. And that's very space intensive for the prover.
01:14:45.528 - 01:15:23.978, Speaker C: So what people will typically do is they'll chunk up the execution into smaller pieces and kind of prove each piece separately and aggregate the results. So we don't have any of that implemented yet. We want to switch the commitment scheme over to Binius and all sorts of other stuff. So there's a lot left to build. We're still figuring out the best way to go through that long process, and we're hoping that the community as a whole will contribute. And really, I think the initial version of Jolt that we just released, there weren't great ways for the community at large to get involved. It sort of just needed a core team to get it to this point.
01:15:23.978 - 01:15:51.894, Speaker C: But I think now there's sort of a more modular path forward. So we'll hopefully figure that out. But it's very important to us that this is this kind of open source. And eventually the small team that reached this point just can't build all of it and can't, like, keep doing this forever and ever. So it's very important that sort of the community as a whole kind of get involved and eventually probably take ownership in some way. We'll see.
01:15:52.014 - 01:16:13.464, Speaker A: Well, I'm kind of curious why it's coming out of a 16 z. Research that was also a thing where I was like, you guys are building stuff and it's open source, you're sharing it, but do you imagine spinning it out into something, or is it really just for the teams in your portfolio, or is it just for the community? I'm curious how you think about it.
01:16:13.544 - 01:16:44.382, Speaker C: Yeah, no, it's for the community as a whole. I mean, a 16 z will never monetize this. Of course, we'd love portfolio companies to use it, but we'd love for everyone to use it. The perspective of people at a 16 z is that the firm as a whole succeeds or fails based on the entire ecosystem. It's not about any one project. And so if we can really move the needle for the whole ecosystem by sort of these sorts of efforts. It's worth it.
01:16:44.382 - 01:17:04.286, Speaker C: And so I was very lucky to join the firm when I did. The engineering team had just gotten built up. Along with the research team, I managed to get a lot of interest and excitement from Michael and Sam, two of our engineers. They didn't know anything about snarks. They had to learn it from scratch. It's nice that had this book that they could kind of get like a private pass through the book.
01:17:04.430 - 01:17:05.014, Speaker A: Oh, yeah.
01:17:05.094 - 01:17:18.990, Speaker C: Nice. I mean, the whole thing has been sort of unbelievable for me that they've managed to pick this up and implement it, and it was like, just so much work for them. But, yeah, I mean, the hope is that it benefits the whole community, and we'll see how it plays out.
01:17:19.182 - 01:17:41.094, Speaker A: I just love this. I can see this. It's like you have been on a mission to make some check the thing, and you are now getting it into the hands of people in a way I guess you maybe couldn't have imagined, like, a few years ago because you were really in that research area where you might have championed it in research, but you need people to implement it and buy in. And here you actually get a chance to see it.
01:17:41.134 - 01:17:46.046, Speaker D: This is the, like, spoon. Here comes the airplane. To, like, the child who, like, doesn't.
01:17:46.070 - 01:18:21.264, Speaker C: Want to be fed, it's like, no, you will be fed some checks. Right. And I say, I mean, you know, this whole effort, like, would have never happened if, you know, the risk zero had the foresight to. I wouldn't know what risk five was when, you know, when risk zero launched. Right. You know, the sort of the interest. The fact that I was able to take, like, three engineers time because Noah worked on the developer interface, which was also a lot of effort to take all that time from a very small engineering team for, like, ultimately, like, not ten months if you include lasso in there.
01:18:21.264 - 01:18:29.904, Speaker C: Like, you know, the fact that just all these projects had already built so much and made so much progress, like, it just can't happen conceptually.
01:18:30.064 - 01:18:30.682, Speaker A: Yeah.
01:18:30.808 - 01:18:54.910, Speaker C: So anyway, as just a professor, I think there's no way we could have. I could have gotten the engineering required to just sort of demonstrate that this. This stuff really, you know, has the benefits that it does. Because, you know, engineering is for. As a researcher, it's like, it's a confounding factor, right. It's like something could be ten times better sort of intrinsically, but the engineering makes a 20 x difference, and you can't tell now, you know, totally. It gets enough.
01:18:54.910 - 01:18:55.566, Speaker C: Yeah.
01:18:55.710 - 01:19:09.296, Speaker A: Funny. Was there ever a moment as they were implementing it where you weren't sure if your results would actually play out, because now you have some metrics that are good, but what if they hadn't been?
01:19:09.440 - 01:19:48.748, Speaker C: Yeah. I won't say that I had no stress at any time, but it was always the case that. So I had very detailed calculations, largely based on cycle counts, especially because we're using curve based commitments. Everything the prover does is fieldwork or just, like, you know, plumbing stuff that, you know, like, I sort of ignore in my calculations. Right. So either, you know, I was making a mistake in my calculations, or the plumbing was just going to be too expensive, and I was pretty sure the plumbing wouldn't be too expensive. So I either I had, like, a fundamental misunderstanding and my calculations were missing something really important, or just, like, the plumbing is overwhelming.
01:19:48.748 - 01:19:56.930, Speaker C: And so until I either discovered a really big mistake or the plumbing was just seeming awful, I was not too worried.
01:19:56.962 - 01:19:57.786, Speaker A: You were not worried?
01:19:57.850 - 01:20:24.944, Speaker C: Yeah. But now Michael and Sam suffered through all of this without that faith. Right? So I'd say even a month ago, we were five x, six x slower than today. Oh, wow. Yeah. And so I said we had, like, a functioning version of jolt after, like, back in December. And so I thought that I was surprised how long it took to get the engineering in the state that it needed to be in, but I never had a doubt it would get there.
01:20:24.944 - 01:20:38.376, Speaker C: But, like, I kept telling Sam, kept saying, like, oh, we're gonna get, like, at most, another two x. Even. Even back when we were, like, ten x, and I kept being like, no, we're not. It's gonna be better. And eventually it was. And, you know, there was. And there's.
01:20:38.376 - 01:20:40.312, Speaker C: And there's still something on the table there, so.
01:20:40.408 - 01:21:01.106, Speaker D: Right. Yeah, it is funny. The engineering does. It's like, you can test individual components, but it isn't until you kind of really, like, put all the nuts and bolts together and, like, really make sure you oil every bearing and stuff, that it's clear that it all works. It's very easy to jump ahead the five steps, but there's some real glass chewing that happens in the middle.
01:21:01.210 - 01:21:25.834, Speaker C: Oh, yeah. There's a lot of structure that existing libraries don't leverage. The r one cs we deal with is super structured and Spartan was made to handle arbitrary r one cs, and it was 60 times too slow to begin with. You know, we. It took a while to figure out exactly why it was. It was quite so bad, you know, what structure it was missing and things like that.
01:21:25.874 - 01:21:30.402, Speaker D: So, yeah, it's like materializing the matrices in, like, one row order versus column.
01:21:30.458 - 01:21:32.314, Speaker C: Order will screw you if they're spot.
01:21:32.354 - 01:21:40.610, Speaker D: You know, like, this is the kind of stuff that's like, oh, like, yeah, I mean, of course it's like if you do it right, it's gonna work, but takes a real glass cheer, right.
01:21:40.682 - 01:22:17.744, Speaker C: And I don't want to minimize, like, that stuff is super important, but I think there, there is a perspective out there that like, you just can't know how these things will run till you build it because of stuff like this. And I think that's too strong. I mean, you know, it's always possible. I just got lucky with jolt and like, you know, this stuff didn't turn into like a sticking point that kept us from getting where I thought we would go. But, you know, my general sense though is that with enough glass chewing, you will get where you think you're going if your calculations were detailed enough to figure out what, you know, where you think you're going. And that, that has been my experience so far.
01:22:17.854 - 01:22:31.740, Speaker A: Cool. I really see this justin now as like this mission, just this like force of will to get this sum check out there in the world and it, and it's, and you did it. So congratulations on that.
01:22:31.852 - 01:23:13.022, Speaker C: Thank you. I could, I definitely could not have done it without a lot of help along the way. So let me express appreciation to everyone and, yeah, I mean, I think at this point I get to work with lots of new people who weren't working on some check before and that's super fun and I feel much better. I feel like I could disappear today and the community will wind up in a great place. There's so much ability and motivation and capability out there and I've been trying to just point it in what I think is the right direction and I feel like it's pointed now and that's great. The process can just play out.
01:23:13.178 - 01:23:34.918, Speaker A: Very cool. Thank you so much, justin, for coming on the show and sharing with us what you've been up to since last year. Going through the 17 or some of the 17 misconceptions about snarks and then sharing with us, Lasso and then now jolt and basically your mission to get some check into the hands of as many people as you can get it into. I love it. So cool.
01:23:35.086 - 01:23:37.334, Speaker C: Thanks for having me. Pleasure to be here.
01:23:37.374 - 01:23:38.996, Speaker D: Yeah, super, super fun. Thank you.
01:23:39.110 - 01:23:46.464, Speaker A: Thanks. Alright, I want to say thank you to the podcast team, Rachel, Henrik and Tanya, and to our listeners. Thanks for listening.
