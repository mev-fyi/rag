00:00:07.450 - 00:00:52.190, Speaker A: I got my friend to draw this title slide for me and I swear the talk just goes downhill from here. So my name is Jacob, I work at Polygon Zero. I'm an engineer, and my job is basically to make things fast. One of the things that we're really, really proud of at Polygon Zero is donkey two, which is like a stupid fast zero knowledge proof library. We can do recursive proofs in less than 200 milliseconds, which is really, really fast. Well, how did we get there? Well, there's a lot of mathematical tricks involved in doing this, but there's also a lot of low level hardware work that we had to do in order to bring it to this particular speed. In particular with hashing.
00:00:52.190 - 00:01:38.826, Speaker A: Even now, we still spend roughly 50% of our proof time just on hashes. And this is after all the stuff that I'm about to talk about, it used to be way, way higher. So I'd like to introduce to some of the tricks that we used in order to make hashing particularly fast. In order to make plunkey two particularly fast. The hash function that we use is Poseidon. And the way it works, if you haven't seen it before, is you have some sort of an input. So s is for state, and this is a vector of length twelve, because in our particular case it can be a different length, but we have technical reasons to make it twelve.
00:01:38.826 - 00:02:20.682, Speaker A: And there's a few transformations that happen to this before hello. There's a few things that we do to it in order to kind of scramble it up. So it's difficult to just find things that will collide. So the first thing is, so we call this the constant layer, right? So we take the state vector and we just do element wise addition by constant, by some random constants. And this is fairly easy. Then we get to s boxes. So that stands for substitution box.
00:02:20.682 - 00:02:47.518, Speaker A: And this is basically exponentiation. This is still element wise, but it's an expensive operation that adds a little bit of security. And the MDS layer. The MDS matrix is a type of matrix. This is basically just matrix multiplication. And the point is to mix all the elements up nicely. So every index affects every other index.
00:02:47.518 - 00:03:39.060, Speaker A: So the state vector makes its way through all the layers and then back up again a bunch of times. Each set of all three layers is called a round. And eventually, once we're tired and we don't feel like doing any more rounds, we get some result out, which is also length twelve. So one thing that we really need if we want to do these kinds of algebraic hashes quickly is a field to work with that's very fast. We need to be able to do your exponentiation and multiplication matrix multiplication, probably not addition like that. That's always easy, very fast. In order to do hashing quickly, we use something called Goldilocks field.
00:03:39.060 - 00:04:22.762, Speaker A: Let me explain what a field is like in a finite field. You basically take all your integers, right, and you kind of roll them up into a little circle. So when you go, you kind of wrap around. So five plus one is six, six plus one becomes zero. And it turns out that multiplication works like a similar way. And interestingly, it's from this rolling up that the term ZK roll up comes from. Now, the number of elements in this circle has to be prime.
00:04:22.762 - 00:04:57.574, Speaker A: So it was an interesting challenge that Tamesh here mainly did to find a really, really good prime to use. Here. It's seven here, but we obviously can't use seven. We used the Goldilocks prime. My favorite prime is 57, but the Goldilocks prime is my number two favorite prime. And this is the prime that we use for the Goldilocks field. It's that which looks a little bit ugly.
00:04:57.574 - 00:05:47.100, Speaker A: Fair enough. But then you see this and it's kind of nice. Or if you express it in hexadecimal, it's that. And it turns out that since we have machines that are 64 bit, but also often support 32 bit operations, this particular structure ends up, oh, there's supposed to be another zero there. Ends up just permitting some tricks that happen to work on this particular prime field. And it turns out that a lot of algebraic operations are fast. Each Goldilocks prime Goldilocks field element fits in a 64 bit word.
00:05:47.100 - 00:06:21.314, Speaker A: So for example, this is an example of a Goldilocks field member. Here's one, but that's also one. So we have redundant representations. Addition. Well, we take two things and we do the normal addition, but if it overflows a 64 bit word, then we need to compensate for that. So in this case, like, we have a carry that kind of came out of here, and if we get a carry that turns into this, this is just two to the 32 minus one. So you can think of it as the 32 bit integer.
00:06:21.314 - 00:06:52.640, Speaker A: That's just all ones. And you just add that on in order to compensate for the overflow. Occasionally this might overflow again, in which case you just kind of repeat, and that can't overflow. In assembly, it looks like this. Ten points. If you can read this, it's nice and short, and you don't actually have to keep this constant in like a register anywhere it falls out of here. I'm not going to explain it.
00:06:52.640 - 00:07:43.380, Speaker A: Multiplication. If we want to multiply two elements of the gold locks field, we first start with like a full 64 bit by 64 bit 228 bit multiplication. And then we need to figure out how to make, how to reduce this so it fits in 64 bits. Well, what do we do? Firstly, bear with me, we break it up into three chunks. So the low half goes here, then the high quarter goes here, and the kind of middle quarter that's not the one that's part of the low half goes here. Like we've just basically just rewritten this. But it also turns out that this is just negative one and this is just two to the 32 minus one.
00:07:43.380 - 00:08:32.970, Speaker A: So it's just a subtraction and an addition. This can be very easily computed with just a bit shift and a subtraction. So I know that the example might be a little bit, look a little bit complicated, but it's really quite easy. You do a full multiplication, you split it up, subtract and add, and it's really, really fast. So let's get to the actual hashing bit. We looked at the structure of the Poseidon hash and let's talk about s boxes. So exponentiation, I kind of missed a detail before because there's two kinds of s box layers.
00:08:32.970 - 00:09:32.830, Speaker A: We have the one that's used in full rounds where we basically, in our case take every element of the state to the power of seven. And we have the one in the partial round where we only do it to the very first one and we just don't care about the rest. So let's focus on, well, firstly, this is basically how do power of seven, we can break it up into a square, a multiply, another square and a multiply. So it's just four operations to do that. Let's talk about the full rounds. One thing that you might notice here is that we are doing the same operation to a lot of data. So you might naturally think, well, can we use vector instructions for this? So what do I mean by that? Normally when you have calculations executing on a computer, you use scalar instructions.
00:09:32.830 - 00:10:18.058, Speaker A: So your computer normally does kind of one thing at a time. So say you have two inputs, like just two numbers and you want to add them together. A scalar instruction will just take those two numbers and spit out one output. Done. But a lot of architectures have vector instructions that let you do one operation but to a lot of data at once with just the one instruction. So in our case we have phone numbers, we want to like two vectors of phone numbers, I should say add them together, we get a result that's a vector of four numbers. So can we use this? Well, the first issue is this.
00:10:18.058 - 00:11:24.770, Speaker A: So AVX two, which is like the vector instruction set in X 86, doesn't actually have a full multiplication instruction. So straight up like we know this is at least going to be difficult. However, it does have a 32 bit by 32 bit full multiplication, so maybe we can do something with that. Well, if you think about full 64 bit multiplication, what you can do is just split up both factors into the high and low halves. Do like 432 bit full multiplies like pairwise and put them together and it's like I've done this and the code is that don't try and read this. My point is it's messy and you might say okay, but if that's on four elements, it's just one instruction to do it on one element. So surely this is actually slower.
00:11:24.770 - 00:12:01.150, Speaker A: And yes, you're right, we might as well go home. No. Yes, so full multiplication is actually slower with vector instructions. However, remember that full multiplication is just the very first step of Goldilocks multiplication. Reductions are actually way faster in vector. And in fact, interestingly enough, squares are faster in vector as well. Just because you get to reuse some of the like, you don't have to recompute a bunch of values.
00:12:01.150 - 00:12:52.958, Speaker A: So we do actually use vector instructions for the s boxes and it does give us a really nice feed up. Now for the partial routes, we can't do the same thing, right? Like we're applying one operation to kind of one thing, we don't have enough work to apply vector instructions. Well, firstly, we do have to do it in scalar in order to minimize the latency. That's kind of what we're bound by. But there is another thing we can do, which is to try and hide that latency a little bit. If we look at the MDS matrix multiplication, that directly follows the sbox layer. So basically imagine that each of these things is a number.
00:12:52.958 - 00:13:30.170, Speaker A: I just couldn't be bothered actually putting stuff in there. And this is just a matrix that we're multiplying this vector by. What we can do is just split both of them up and perform matrix multiplication on the remainder of the state vector. And like the part of the matrix that's actually relevant. And this can run in parallel with the s box. And after we actually have s zero to the power of seven, then we can tack this on, which is just going to be faster because it's less work. All right, now let's talk about the MDS matrix.
00:13:30.170 - 00:14:22.140, Speaker A: An MDS matrix is a matrix that meets particular security criteria that I don't know about, but we need to multiply by it. The matrix that we use is this one. The numbers don't mean anything in particular to us, but you might make some observations about this matrix. Firstly, you might notice that it's circulant. Like each row is just the row above it just kind of rotated by one. It's also all powers of two, and all the values in it are small. How is this useful? Well, if you have a circulant matrix, there's just way fewer constants for you to keep track of.
00:14:22.140 - 00:15:43.122, Speaker A: On architectures where you do have to keep them in registers, you don't have to use as many of them. Powers of two mean that you get to use bitships instead of multiplication, which on some architectures might be helpful. And because the values are small enough, we can actually prove that we will never need to do a reduction from more than 96 bits. So that just makes the reduction at the end a little bit cheaper. And once you have an MDS matrix, there's a few things that you can do to actually make the computation faster. So firstly, vector instructions work great on this because there is actually a lot of parallelism in matrix multiplication. Just inherently you do have to be very careful about how the data is moved around, because in vector instructions, normally what happens is a vector instruction will do things element wise, like it can add an element of a vector with the corresponding index of the other vector, but you can't do an addition in between different indices.
00:15:43.122 - 00:17:03.354, Speaker A: So you have to use other instructions to move the data in the registers in order to be able to play your arithmetic vector instructions. In our AVX two implementation, there's a lot of hacks that we had to do to actually make the moving data around thing fast, and once we had that, the rest was trivial. You should use multiply accumulate if it's available and if it's fast. So in particular, AVX two doesn't have multiply accumulate on integer, and I couldn't get it to work. Well, I tried doing this in floating point and it was a bit of a disaster. However, the Apple M One actually has a 32 by 32 to 64 bit multiply accumulate, and it can do eight of those per cycle, which is really, really high throughput. And that means that the M One is actually really fast at computing MDS matrix multiplication and the last thing that I will say if you're doing this kind of thing is with this kind of matrix multiplication, you can separate out the high and low halves of the input.
00:17:03.354 - 00:17:53.294, Speaker A: What I mean by that is basically this equation, right? Like just by linearity, if you extract the low half of each element of x, like this is just a vector of 32 bit integers. Now, you can apply the MDS matrix. You can do the same for the high half. Multiply that by to the 32 linearity, and you will get the same result out. And what it means is that you never have to deal with values that are greater than. That would take up more than 64 bits. If you have a 32 bit value, like if you do a bit shift or multiplication by a small enough number, it will fit in 64 bits, and you don't have to worry about things like, okay, how do I do addition with carry in vector, which is doable, but it's annoying and you don't want to have to do it.
00:17:53.294 - 00:18:34.890, Speaker A: So this is actually quite crucial to making MDS multiplication fast. That's basically it for me. Look, I introduced you to a bunch of tricks that we had to invent in order to make our Poseidon hash fast. None of them are actually, like, particularly profound, but the thing is that each of them adds a little bit. And it's all of them combined that make our hashing really fast and in turn, mean that our prover can be sub 200 milliseconds.
00:18:44.610 - 00:19:04.450, Speaker B: It's perfect. We have plenty of time for Q and A. So, five minutes. How about. No? Just repeat the question, if you please. So, what's the speed up over a standard sighting hash? Is it like two X or three X?
00:19:05.220 - 00:19:30.140, Speaker A: The question is, like, what is the speed up over a standard implementation? It would depend on the architecture, but you're looking at two to three X, maybe five. Look, I haven't checked in quite a while, and it also depends on how much you've optimized your integer implementation that doesn't have these tricks.
00:19:33.360 - 00:19:37.120, Speaker C: Brenda, do you see room for future speed ups? Improvement?
00:19:39.540 - 00:20:18.270, Speaker A: Is there room for future speed ups? Yes. I have a whole list of things to do next that I didn't want to talk about, because I haven't done them yet. But there's a bunch of assembly tricks that are perhaps not really interesting unless you're an assembly nerd like I am. It would actually make it faster. Doing the MDS matrix multiplication in foyer space would actually yield some improvement, but you have to be very careful about it so your Fourier transform doesn't end up actually costing you more.
00:20:20.480 - 00:21:09.070, Speaker C: Sure. Yeah. Just to extend on that. So the idea is, in particular that the circulant matrix has a Fourier transform, which is just a single vector in it. So you end up having essentially a dot product of two vectors rather than a matrix multiplication, which is part of the motivation for using the circular matrix in the first place. But then if you're in Fourier space, you don't necessarily have that nice MDS matrix form of powers of two. So instead of doing the search for an MDS matrix that looks like that, you take a random MDS matrix and then take the Fourier transform of that, and then look for examples which have a very nice Fourier transform and use that instead, you end up with essentially a sort of dot product in the Fourier space with small coefficients as well.
00:21:09.540 - 00:21:11.810, Speaker A: Half the stock is Hamish's work, to be clear.
00:21:16.340 - 00:21:26.900, Speaker C: All right, time for one more question. All right, so all of this depends on this very particular field that we're working in, is that a field is actually very common in practice.
00:21:28.360 - 00:22:17.460, Speaker A: Does it depend on the particular field that we use? A lot of these optimizations are enabled by the Goldilocks field. And the reason why is you can do reductions with very simple operations. And vector instructions tend to be, well, they tend to have higher latency, for example, than scalar instructions, but they also tend to be much simpler, just because you don't want to have that much die space devoted to them. The fact that the Goldilocks field has a simple reaction is really, really useful for this. And if we had just an arbitrary prime, like a lot of these things just wouldn't work because you wouldn't be able to vectorize them properly. If your reduction evolves a multiplication, you're screwed.
00:22:19.160 - 00:22:22.176, Speaker C: On the other hand, having a nice NDS matrix is going to be helpful.
00:22:22.208 - 00:22:23.812, Speaker A: No matter what you're using.
00:22:23.946 - 00:22:34.230, Speaker C: Like, a lot of the standard NDS matrices are just these. I think they're the Koshi ones. They're essentially random entries. And then your multiplication is as slow as it's ever going to be all the time.
00:22:38.590 - 00:22:45.270, Speaker B: All right, so thanks for coming in, and you can find Jacob afterwards.
