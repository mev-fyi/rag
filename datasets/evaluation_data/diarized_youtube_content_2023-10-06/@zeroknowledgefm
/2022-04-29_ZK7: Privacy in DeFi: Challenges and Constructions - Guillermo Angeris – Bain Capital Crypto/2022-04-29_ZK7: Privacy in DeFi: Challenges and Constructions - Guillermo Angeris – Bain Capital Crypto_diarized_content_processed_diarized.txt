00:00:06.970 - 00:00:12.750, Speaker A: I want to introduce Guillermo to the stage to talk about privacy in Defi. Welcome to the stage, Guillermo.
00:00:16.770 - 00:00:17.374, Speaker B: There we go.
00:00:17.412 - 00:00:18.206, Speaker C: All right.
00:00:18.388 - 00:00:21.950, Speaker B: And I've actually never used a pointer before in my life. I feel very fancy.
00:00:22.450 - 00:00:23.054, Speaker C: Okay, cool.
00:00:23.092 - 00:00:39.298, Speaker B: So, actually, this talk is going to be quite weird. And in fact, I suspect I'm going to offend approximately three groups of people while giving it. So one is cryptographers. I apologize in advance. I am not a cryptographer. I did statistics and optimization theory. Number two is going to be people who actually do differential privacy.
00:00:39.298 - 00:00:55.934, Speaker B: So this talk is going to be a caricature, so to speak, of differential privacy. In some ways, it's going to be very powerful. In some ways, it's going to be completely useless. So keep that in mind. And I guess number three is actually measure theorists. So if anyone. Has anyone taken measure theory here by chance, I'm so sorry, it's a bad call.
00:00:55.934 - 00:00:59.662, Speaker B: I wouldn't recommend it, but it happens, and there's only so much you can do.
00:00:59.796 - 00:01:00.238, Speaker C: Okay.
00:01:00.324 - 00:01:09.490, Speaker B: With that. You have been warned. I apologize. If you throw tomatoes at me, I think someone's going to have to clean them up, so please don't. But otherwise, you can tell me off stage.
00:01:10.150 - 00:01:11.300, Speaker C: Cool. Okay.
00:01:14.150 - 00:01:52.318, Speaker B: Again, I've given one warning, I will give a second warning. But this first one was the most important one. So we're going to talk very high level about what privacy is. We talk about privacy in a very generic sense. Often we talk about there exists no privacy, or there exists like kind of an infinite amount, or since notion of zero knowledge or things like you cannot, it is very difficult or if not impossible for you to find or do a thing that kind of uncovers the actual true action at the bottom? Differential privacy. And all these notions are kind of a framework to think about something in the middle. So you get to specify the trade off you want.
00:01:52.318 - 00:02:42.414, Speaker B: Often that comes at some cost, potentially economic. And in particular, you've given me your security budget or this notion of whatever, how much privacy you want to leak given. And I can tell you, roughly speaking, what you need to do and how much economic costs it will come, or how much it will cost you economically to actually receive this privacy. Okay, so again, as I mentioned, we're going to talk about what it is in this very weird sense that most people, I suspect here, are not used to, unless you've done differential privacy. And the thing is, how do we actually define it in a useful way where you have this notion of a trade off? The specific thing that I'm going to focus on and the reason why it's going to be potentially, I might get yelled at, is because we're going to focus on really clean definitions. If you've ever read a differential privacy paper, I would say that's unfortunately not the cleanest thing ever. But it's fine.
00:02:42.414 - 00:03:28.158, Speaker B: It's interesting. Okay, so the biggest warning here is many of you probably implicitly know a lot of this stuff, right? And this talk essentially will work on you probably know it, in fact, in a quite different context, but you know it implicitly. You have this framework in your head when you're thinking and speaking of when something is private, but you probably know this framework in more generality than we need for most applications we care about. And specifically, as kind of Anna mentioned in the title of the talk, is we're talking about Defi. Okay, so let's get to some definitions, hopefully not actually, it gets vaguely interesting, and then afterwards actually gets interesting. So we'll start with some disclosure mechanism. That is a totally random thing I just came up with.
00:03:28.158 - 00:04:03.350, Speaker B: But here's the definition. It's a function, we're going to call it t, which maps your action space. So this is mathcal a to some publicly released state space. So for Defi, you have to know things about what you're doing, right? So you have to have some notion of kind of shared or global things that everyone knows. So it's public, even though at the end of the day, what you want is you want kind of individual actions to remain private. And that's what we're capturing here. So, t, as I've kind of mentioned, can at the moment be pretty much anything.
00:04:03.350 - 00:04:30.580, Speaker B: So, one example is a CFMM, or a constant function market maker. It takes a trade, and we're going to call it an action, and then it just reports the price, right. In order for you to trade with something, you'd better know what the price of the thing you're trading at is, otherwise you're going to have a bad time. On the other hand, you could do things like, oh, maybe you borrow a loan. The loan is for some amount, right? And in particular, this is your action. And then what's released to you is, roughly speaking, perhaps the interest rate of that.
00:04:30.950 - 00:04:31.700, Speaker C: Cool.
00:04:32.070 - 00:05:08.118, Speaker B: So, okay, that's one of the definitions. So we have this protocol. It has some disclosure mechanism, which all it does is it takes an action that someone has done and then reports kind of the post state public information. So Alice, our protagonist of every story, will take some action, right? And in particular, what are we saying? Well, that action goes to some smart contract. And the smart contract now reveals some public information, which is given by s and Eve, our ever loved adversary. Her point is to reconstruct that action, a given in the underlying state. S.
00:05:08.118 - 00:05:18.162, Speaker B: Right. This is just the normal classical game we all love about trying to screw privacy. So the biggest question, of course, is what can we learn?
00:05:18.216 - 00:05:18.434, Speaker D: Right?
00:05:18.472 - 00:06:02.698, Speaker B: Like, what can Eve learn about the action given the publicly released data? So big warning. Another warning as well, is we're not going to talk about history, actually, we're going to see how to directly encode history in kind of this framework without actually explicitly requiring it. And we're certainly not talking about anonymity. So anonymity can be achieved without requiring kind of, you can have fully public state and have people who interact with it be completely anonymous. And I will not talk about this notion at all when I talk about privacy, I mean specifically reconstructing the action that was taking at some point in time. This is a really important distinction, because surely you can do things like correlate behavior and things like that, but I'm not touching that at all. This is just about reconstructing actions.
00:06:02.698 - 00:06:38.710, Speaker B: Okay, so what does Eve do? Of course, here's a very intuitive notion, is Eve looks at the pre image of this disclosure mechanism on the publicly released data. In some sense, anything that kind of has been done and gets mapped to a public state, you can only equivocate between those items. Okay, so first obvious fact is that the pre image on s, sorry, I wrote t inverse here. It really does mean pre image. If your use of topology or whatever is not empty, why? Because it contains the true action that was actually performed.
00:06:38.870 - 00:06:39.338, Speaker C: Okay?
00:06:39.424 - 00:07:09.846, Speaker B: And here is the second intuitive notion. Just wait 1 second. But the larger this set is, in some sense, the harder finding the underlying action actually becomes, right? I mean, this is kind of obvious, right? Like, I have a bunch of things that map to one thing. If I go backwards, I try to reconstruct what happened. The larger that set is, the more things I could have done. This is in some sense very similar in spirit to the notion of anonymity set, perhaps, and things like that. But this is a much more generic notion here.
00:07:09.846 - 00:08:11.240, Speaker B: We're talking about kind of very general and somewhat distinct mechanisms, okay? And we're going to quantify what it means to say, like, something is large or small. So here's the unfortunate sad news. In defi, unfortunately, the set is very, very small. In fact, it is so small that it is exactly one element, right? So, for example, in constant function, market makers, you can show that the set, and this is not obvious, not immediately obvious, is a single element. The reason why it involves some solution to some nonlinear differential equations and showing that that has a unique solution. But that's besides the point. But the question is, of course, that I ask you, and perhaps I might be asked, is, what can we do in these, so to speak, worst case outcomes? So, before we get there, finally, there is one last definition, maybe.
00:08:11.240 - 00:08:41.538, Speaker B: I think it's last definition. There is some notion of the size, right? We've talked about, oh, these sets, hemi, large and small, and that's very vague, and no one actually cares. What are we talking about when we say size? So here is where the measure theorist, the single person, sorry. Here is where you might yell at me, but I'm going to call mu a measure, but in fact, it will not be a measure at all. There will be exactly one property it has, and that is, if a is a subset of b, then the measure of a had better be smaller than the measure of b.
00:08:41.624 - 00:08:42.162, Speaker C: That's it.
00:08:42.216 - 00:08:42.530, Speaker B: Sorry.
00:08:42.600 - 00:08:43.106, Speaker C: I know, I know.
00:08:43.128 - 00:08:56.566, Speaker B: You're shaking your head. I'm sorry. I didn't mean it. I mean, I do mean it, like a little bit, but not too much. So, yeah, apologies again. So here are a set of things. If you don't care about privacy at all, here's a perfectly reasonable measure to talk about.
00:08:56.566 - 00:09:30.994, Speaker B: It's the silly measure of mu of a is zero for any a. Congratulations. You have achieved nothing at all with doing this. So let's talk about something slightly more interesting, which is the uniform entropy, right? Which is the logarithm of the size of the set, right? Roughly speaking, that's how many bits you need, how many bits of additional information you need in order to pick out a single element from the set. Assuming you uniformly sample an element from a. Okay, probability is a perfectly reasonable notion of measure. And this is what one might actually call a measure, but it's actually not as interesting.
00:09:30.994 - 00:09:41.190, Speaker B: And you can imagine a number of other possibilities, right? There's other entropic things that we can do which are, for example, not necessarily uniform over the entropy, or the entropy is not necessarily uniform over the distribution that we're talking about.
00:09:41.340 - 00:09:42.038, Speaker C: Okay?
00:09:42.204 - 00:10:16.450, Speaker B: So now we can talk about size and Privacy very cleanly. It's Super simple. And what is it? Well, we're going to say that the privacy of some disclosed data s is going to be what? It's going to be the measure of the pre image of the given set, right. As we kind of mentioned before, intuitively, the larger that set is. Of course, it makes sense that you're going to have more privacy. Here is a perfectly reasonable definition. You've given me a measure of privacy, and I'm giving you a number which says here is how private this specific disclosed state is.
00:10:16.600 - 00:10:17.106, Speaker C: Okay?
00:10:17.208 - 00:10:45.994, Speaker B: And now we could talk about a privacy mechanism. What's the privacy of a mechanism? Well, it's the smallest possible, if you take the infamum over all states, if you take the smallest possible privacy, overall possible resulting states, if that thing is large, right. By definition, if that set is large, then there exists no publicly disclosed data that will ever yield kind of easily distinguishable actions. Right? So does that make sense so far? I think.
00:10:46.112 - 00:10:47.194, Speaker C: Okay, cool.
00:10:47.312 - 00:11:16.206, Speaker B: So now let's get to the interesting stuff. We've given an extremely general framework, right? It's not even clear you can say anything interesting about it at the moment. It's like, okay, fine, you gave me a measure mu on some stupid thing. All right? So let's do something with it. Okay, so what does alice need? Going back to our original protagonist is she needs to make sure that the pre image of this global state, of this shared or public state, is large for a given action.
00:11:16.398 - 00:11:16.994, Speaker E: Right?
00:11:17.112 - 00:12:03.454, Speaker B: So from before, what does that mean? Well, we're going to measure the size of the set using mu in particular. If that thing is large, then we're great. Unfortunately for us, that thing is not large for almost every kind of defi application that we can think of, unless you're careful, and we'll see what careful means. So the worst case is that your pre image is singleton, in which case the smallest possible thing that happens is, congratulations, it's your measure of a single element. And this is common in many DFI applications. So can we improve on this idea? Okay, so first kind of simple notion is, hey, what happens if we can batch actions here? I'm going to be completely and perfectly abstract about what batching means. So we have some notion of a batching operator.
00:12:03.454 - 00:12:30.330, Speaker B: It doesn't need to be associative. It doesn't even really need to be anything. I am simply assuming there's some canonical ordering when I write that thing. But the point is, I have a function, right o plus, which takes a bunch of actions and then just gives you one output action at the end of the day. So the protocol, intuitively, what it's doing is it's taking n people's actions or transactions, adding them all together, and then only publicly releasing information on the batched action.
00:12:30.670 - 00:12:31.430, Speaker E: Right?
00:12:31.600 - 00:13:16.620, Speaker B: So that means that recovering any individual action is probably pretty damn hard. Why is it obvious? Well, here is the kicker, right? If you have a zero action, so if you have an action such that it actually doesn't do anything to the set itself, then there are many, many possible ways of getting the same batched action, right? You can imagine someone does zero, Alice does a bar. Or you can imagine Alice's a bar or zero, and then someone else's a bar, or any number of possible combinations, depending on the structure of your aggregation operator. Okay, so we're going to talk about the set of possibilities for an individual player. We're going to call that p. So p of a bar essentially is all of the set of outputs which are consistent with the released with the known action, a bar. And now we can say something very interesting.
00:13:16.620 - 00:13:43.042, Speaker B: If the set of possibilities for any batched action is large, if it's true for every single batched action, then necessarily we have privacy under the measure mu, right? So here's the proof. It's kind of silly. So we have a list of actions. A one through an alice performs some action. So we're going to call Alice player I. The protocol aggregates that action into a bar and reveals some public state. Then we're going to.
00:13:43.042 - 00:14:35.918, Speaker B: Sorry I lied to you about the last definition. This is the last definition. I'm actually pretty damn sure of it. So we're going to define the set of possibilities over a set, which is just the union of the individual possibilities for every element of the set, right? So that kind of means, like if we have a number of consistent actions for an individually batched thing, the set of possible actions over kind of the set of all possible things is just the union of the individual actions, the union of the possibilities of the individual actions. Okay, so what is the possible set of actions that Alice could have taken? Well, it's p of the pre image of S. And so we have the kind of obvious lower bound that the measure of p of t inverse of s is always going to be greater than or equal to the measure of the possibilities of any individual action. And this bound is tight in exactly one spot.
00:14:35.918 - 00:15:30.022, Speaker B: And that is when t inverse of S is a singleton. So from there, we can prove a slightly simpler or slightly more complicated inequality by minimizing first over the actions on the right, so on the right hand side, and then minimizing over the set of possible states on the left hand side. Actually, you can even derive a strictly better inequality. But the point is, this is kind of a fancy way of writing, it does not matter kind of how bad your mechanism for release is if you're batching, the worst anyone could ever hope to do to you, Alice or Alice's of the world is essentially picking the worst possible action. And if that thing is large for any action, then necessarily that infimum on the right hand side is also large. So in some sense you have privacy independent of how bad or shitty your public disclosure mechanism actually is. Okay, so what's the conclusion of this? This is pretty simple.
00:15:30.022 - 00:15:37.666, Speaker B: Batching never hurts. Well, it might hurt, but it only does not hurt privacy. With many players, it's somewhat reasonable assumptions.
00:15:37.698 - 00:15:37.846, Speaker F: Right.
00:15:37.868 - 00:16:16.340, Speaker B: Then batching in fact, is actually quite beneficial. Right. But here are two problems that we wrote some very nice little symbols, but they're not obvious, is a, it means that you have to define a batching operator. And in a lot of protocols, it is really not obvious how to define a batching operator in any way that actually preserves the properties you might want. And even if you did, you have the second problem B, which is that the UX for users actually suffers. So in the case of, for example. So here's a particular case of batching is if you're trying to trade with a decentralized exchange, one simple way of doing it is you simply add everyone's trades together and then you pay out, kind of.
00:16:16.340 - 00:17:01.470, Speaker B: You have delta one, delta two, delta three, which are how much money people are trading for USDC to eth. And then you simply pay out. So you pass this through one mechanism. You trade this complete large trade, and afterwards you spread the rewards out, or the payoffs back to individual players in proportion to how much they paid it. So that's a pretty reasonable mechanism, but that's really bad for people like arbitrariors and traders, which you want to actually keep your protocol afloat, because now it means that they have to think about the behavior of other players, which they do not know until the information is released in order to actually perform any sort of optimal action. Okay, so here's another possibility. It's actually adding randomness.
00:17:01.470 - 00:17:25.846, Speaker B: This is kind of the classic one, differential privacy. So here's where we're going to get to. It has one benefit, which is you don't actually need anyone else. You don't need anyone else to ensure that you have kind of an anonymity set. It also allows the user to potentially have a knob to control a privacy trade off. Right? More randomness you add. Perhaps the worse it's going to get for the user, but the less randomness you add, it's better for the user, but you have less anonymity.
00:17:25.846 - 00:18:10.758, Speaker B: And so in some sense, it lets you set a privacy budget. And so to say, I'm willing to pay this much for anonymity, at most this much. And it's like, all right, here is how much you expect to save in terms of bits of entropy, for example, if we're using that measure, of course, as we all know, or maybe most of us know, it is really hard to achieve kind of private randomness in practice. This is, like, another thing that's pretty much not obvious. Okay, so last but not least, so we'll write f of w. So it maps the action space to the action space, and w is just randomly, uniformly sampled, so little w is uniformly sampled from the big space. L is going to perform some action, and then the mechanism is going to map that action to another action under this noise model.
00:18:10.758 - 00:18:47.314, Speaker B: So I don't really care what f is in this case. You're going to have to tell me what that is. But the point is, the state that's released is this kind of perturbed state, right? It's a state that's not actually exactly what Alice wanted to do. It's something else that gets performed. And the point is, at the end of the day, s is now kind of perturbed under t and under f. And so to succeed, and this is where things are going to become really useful, is Eve has to find an action from this kind of inverse image of this perturb function and also this pre image of s. But she does not know w, right.
00:18:47.314 - 00:19:17.134, Speaker B: She doesn't know the randomness that's been picked for the user. Okay, so this is me. The last slide. I think math, or second to last slide of math, is the uniformity over w is really, really useful. And the reason you can extend this to non uniform, but here is why. Essentially because it's uniform, eve pretty much has to decide between kind of the union of all possible actions under both the map that you have from the action to the state, and also under this kind of noise function. Right.
00:19:17.134 - 00:19:44.066, Speaker B: Because it's been uniformly sampled, you kind of really can't tell apart. We can make this very rigorous, but right now, be a little bit hand wavy. You cannot tell apart kind of between these w's unless you have some prior on what Alice going to do. And that can be encoded directly in the measure. So given that Alice performed some action a and the global information s was released, then we have the following easy bound, which again, is tight in the case that t inverse, or the pre image of t is a singleton.
00:19:44.258 - 00:19:45.094, Speaker C: Okay.
00:19:45.292 - 00:20:29.534, Speaker B: And finally, we can kind of refine that inequality and say something a little bit more interesting, that the left thing that we have here, this unfortunately complicated looking expression, is actually really simple. It's just the privacy of your protocol is bounded from below by kind of this infreum game of, in fact, just how large the pre image of f is under w. So there's an unfortunate thing about this model, which is that the adversarial power is actually not exactly obvious. Right. It depends on exactly how you define mu and what you want to prove about mu, except in one very special case, which I started the talk with, where mu is entropy, you can give very strong guarantees. And that case is, of course, differential privacy.
00:20:29.662 - 00:20:30.146, Speaker C: Okay?
00:20:30.248 - 00:20:49.366, Speaker B: So, anyways, in conclusion, and if there's any one thing that you get out of the talk that's not a bunch of unions and f inverses and crap like that, is, there is a simple framework that you can reason about privacy, right. This is kind of how people think about privacy very heuristically, without actually needing all the complicated parts of thinking about mutual information and all the stuff. It does include a number of important cases.
00:20:49.398 - 00:20:49.546, Speaker D: Right?
00:20:49.568 - 00:21:21.880, Speaker B: So if you kind of take a large step back, you have this interesting, very simple framework. Large, you can reason kind of how bad is it for me to do x? And the last thing is that, at the end of the day, really, the one thing that actually ends up mattering is just how large are your pre images, right? In other words, how large are in a mini set, how large is your confusion set? Any number of these things. The point is, that is the cleanest and simplest representation, I think, about privacy, and unfortunately, papers don't always make that obvious. So with that, thank you so much.
00:21:27.390 - 00:21:29.340, Speaker A: All right, questions.
00:21:30.830 - 00:21:39.174, Speaker E: Thanks for the talk. Quick question. Batching. Is there any fairness assumption for the actorhood or the part of the protocol that does batching?
00:21:39.302 - 00:21:47.274, Speaker B: So, the only fairness assumption that there would be would be under the choice of w. If you do this randomness, everything else is kind of independent of the protocol.
00:21:47.322 - 00:21:47.870, Speaker E: Right?
00:21:48.020 - 00:22:02.190, Speaker B: So the assumption is that the protocol itself doesn't release. Like, I'm kind of throwing this under a large set of things, but the point is that the protocol does not release global state, okay? Right. If the protocol is global state, you're kind of screwed, and you're dead on arrival.
00:22:02.270 - 00:22:03.060, Speaker E: Thank you.
00:22:04.630 - 00:22:06.020, Speaker A: Any other questions?
00:22:07.590 - 00:22:08.498, Speaker B: The measure theorist.
00:22:08.514 - 00:22:17.640, Speaker F: The measure theorist? No, everything's good with me, but actually, my mother was a measure theorist. So you had gone into the weeds. It might have been a problem.
00:22:19.690 - 00:22:20.630, Speaker B: I'm glad I got the.
00:22:20.700 - 00:22:35.162, Speaker F: Yeah, no, it's good. It's all good. I had a very simple question, actually. The second to last slide, maybe, where you're talking about whether when you can actually prove these mounds. You said something about when you have an entropy, that's when you can. Is that an if and only if tight?
00:22:35.226 - 00:22:54.306, Speaker B: That is. Well, it depends what you mean by tight. And you have to be really careful when you specify tightness, but the answer is generally yes. Right? So the assumption kind of goes something like the following. How do you prove that something is statistically hard? You can prove that you require a number of bits in order to be able to reconstruct it.
00:22:54.328 - 00:22:54.466, Speaker D: Right?
00:22:54.488 - 00:23:28.558, Speaker B: This is like thanos inequality, or like Lacom's inequality, or all these learning tools that are really complicated and no one likes to talk about them. So the answer is if the only information that's released is kind of included within that mutual information term, then the answer is an if and only if, right? In fact, the answer is there exists no measurable function mapping kind of the state s. Back to the action. A which succeeds with this measurable function is a random variable which succeeds with probability. That's non negligible on the moon in the size of the problem that you're talking about.
00:23:28.644 - 00:23:35.390, Speaker F: Okay, and does that apply to any kind of entropy here? Are you specifically doing Shannon entropy, or is it ransop? Any other.
00:23:35.460 - 00:24:07.766, Speaker B: In this case, it's specifically the mutual information. Although you can use Rani entropy, there's like any number of bounds that you can use. So actually the kind of two main ones are total variation, which is like weird notion of entropy, that's not really entropy. But if your total variation between two distributions is really small, you can't tell them apart. So that's one way. And the other way is if you're telling a bunch of things from one big set, right? So the way to think about it is you have a thing that's drawn from a set roughly, uniformly randomly. You have a thing that is a channel, so it's y and it's what gets publicly released.
00:24:07.766 - 00:24:33.458, Speaker B: And then you have kind of an estimator for x, right? Then what you care about is a mutual information between X and the estimator, or in fact, actually the lower bound or upper bound on that is the mutual information between X and the release information. And that lets you say something extremely clean about the existence of not just computable, but in fact measurable functions, which can reconstruct what the true thing was at the beginning. So, thanks.
00:24:33.544 - 00:24:34.180, Speaker C: Yeah.
00:24:35.450 - 00:24:37.334, Speaker B: Is there. Any last questions?
00:24:37.452 - 00:24:43.570, Speaker A: Any other questions on the stream? Oh, in the stream?
00:24:43.730 - 00:24:44.722, Speaker B: Wait, in the stream?
00:24:44.786 - 00:24:45.462, Speaker A: I don't know how to.
00:24:45.516 - 00:24:47.506, Speaker D: Yeah, we're networking.
00:24:47.618 - 00:24:50.506, Speaker A: Oh, tell us. I didn't even know that was possible.
00:24:50.688 - 00:24:51.130, Speaker B: Yeah.
00:24:51.200 - 00:25:02.170, Speaker D: Oh, yeah. It's 2022. So the question in stream is, can you give any examples of usage of randomness based differential privacy in DFI?
00:25:03.090 - 00:25:36.838, Speaker B: Well, is it better if I show a paper that we've written? Is that a cool thing to do? So the answer is, unfortunately, I think at the moment, no, as far as I know that anyone has actually implemented it. But there are some constructions that you can implement. They have absolutely garbage constants. So maybe don't read it. But the point is you can do it, you probably shouldn't do it. Or if you do do it, be extremely careful and actually work out the constants correctly, which we did not do. So unfortunately, the answer is no.
00:25:36.838 - 00:25:49.960, Speaker B: But there are some notions, like batching, for example, penumbra, that are actually done. And we suspect probably give you a good amount of privacy under basic assumptions of your peers who are working with you. Okay, cool.
00:25:51.090 - 00:25:59.100, Speaker A: Any last questions here? All right, I think that wraps up. Thank you so much, Guillermo, for the talk.
