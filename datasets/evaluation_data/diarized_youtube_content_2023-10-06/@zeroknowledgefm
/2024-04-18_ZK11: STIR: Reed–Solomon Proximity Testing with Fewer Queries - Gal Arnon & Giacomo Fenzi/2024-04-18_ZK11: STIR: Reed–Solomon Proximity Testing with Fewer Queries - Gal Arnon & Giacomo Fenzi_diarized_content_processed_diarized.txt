00:00:11.200 - 00:00:57.388, Speaker A: Hi everyone, and thank you for being here. So, my name is Giacomo, and today I'm here with Gao and we're going to be presenting stir rich Solomon proximity testing with fewer queries. This is joint work with Alessandro Chiesa and Elon Youkev. Let me start to give you some motivation on why the clicker works, some motivation on why you should at some point care about this result. And the object of what we're trying to improve are what I affectionately call Starc and friends. And by this I mean all these protocols which end up being constructed by combining an interactive oracle proof with a BC's transformation. So roughly, this yields snarks which are secure in the random oracle model and can be associated with only hash functions.
00:00:57.388 - 00:01:37.774, Speaker A: So with no other cryptography, this has some benefits. One of them is that these snarks tend to have a transparent setup, so no ceremony is needed. They tend to have relatively fast proving. They can enable using fields which are not tied to public key crypto primitives such as elliptic curves. And furthermore, they are post quantum secure in the quantum random oracle model. All of these benefits made like Starx and fan to be widely deployed in practical applications such as rollups and zkvms. And of course, the list that we have here is by no means exhaustive.
00:01:37.774 - 00:02:31.692, Speaker A: And looking at how some of these snarks are deployed in practice, they tend to follow a very standard recipe. Usually combine a polynomial IOP, which might be tailored to some kind of arithmetization or one cs, and then this polynomial IOP is compiled into a standard IOP. And the way that usually this gets done is that the polynomial IOP gets the messages replaced by Reed Solomon encodings of the messages. Then soundness is conditional on the verifier being able to check that these oracle messages are being sent, are actually close to low degree functions. And the way in practice that this is done is by the fry protocol. So these overall IOP. So the combination of these ritual encoded IoP and the fry protocol then gets compiled via the bassier's transformation.
00:02:31.692 - 00:03:39.402, Speaker A: To give you a stark, when you look at the argument size and the complexity features that come of the IOP, roughly we see that the Fry protocol accounts for more than 80% of the argument size in our experiments. So what we aim to improve with this work is exactly this part we want to give an improvement on Fry. Let me start by giving you some backgrounds of the objects that we have in this investigation. So rich Solomon codes and interactive oracle proof of spreadsimity. So rich Solomon codes, which we denote like this, in this talk, are essentially one of the most foundational codes that are known, and they're parameterized over a field, an evaluation domain in some degree. The way that the encoding of a sum code acts is that you interpret your message as the coefficient of a low degree polynomial of degree D, and then you encode by evaluating over this larger evaluation domain. For the purpose of this talk, you can think of this evaluation domain to be around four times larger than the degree.
00:03:39.402 - 00:04:46.102, Speaker A: And this ratio between the degree of the polynomial and the evaluation domain size is, is the rate of the code. In particular, if the evaluation domain is move forward has got some nice features. This encoding can be done relatively efficient with a fast forward transform with Solomon code and now an iop of proximity for its Solomon codes. The idea that you really want the verifier to be able to determine whether some function which it has oracle access to is a Reed Solomon code word, or if it might be like far too from it. The prover and the verifier, they interact with each other, sending oracle messages. And at the end you want that if this function really the rich Solomon code, the verifier accepts, while if the function is delta far from this rich Solomon code in terms of having distance, the verifier will reject with very high probability. And the reason that we want these, we can only give this proximity guarantee compared to more traditional soundless notion is that we want the verifier to make few queries with to both the oracle function and to the proof oracles.
00:04:46.102 - 00:05:20.754, Speaker A: And this essentially will lead to a more efficient argument down the line. So what is the fry protocol? Most people might have seen it. I'm going to give a very brief refresher. So Frye protocol is exactly an IoP of proximity for this problem. So roughly you have a function that you want to test for proximity and the proven verifier interact. The verifier sends some randomness and the prover will send you some oracle g one. And this oracle is supposed to be the folding of the function around the randomness.
00:05:20.754 - 00:05:55.260, Speaker A: And by the verifier. We will see more formally what this means later on in the talk. Roughly, you can think of the fry to be recursively reducing the size of the problem being tested. So at each step you continue this process recursively. At each step you recursively test a claim that a code word belongs to a smaller code. And in particular this k is the folding factor. At each step, the degree of the polynomial, and the degree and the size of the evaluation domain grows down by a factor of k at the end.
00:05:55.260 - 00:06:31.232, Speaker A: The prover just sends a witness polynomial for it. The verifier needs to perform some consistency checks, which, due to the smooth structure of domain, it cannot perform at the end, in what is called the query phase of fry. This protocol has got quite nice properties. It only has logarithmic number of rounds. The proof length is linear in the size of the evaluation domain. And the number of queries is also suitably small. You can think of in this expression that the number of queries is roughly security parameters time log d to get lambda bits of security.
00:06:31.232 - 00:07:10.768, Speaker A: And in this case, we are really talking about round by round soundness. Which is the notion that you really need if you want to have a sound IOP when compiled with BC's transformation. So with this fry, how do we improve this? So our result is stir, which stands for shift to improve rate. And it is a new protocol for an IOPP for reed. Solomon codes with the same round and proof length as Fry, but with significantly better queries. We will compare it in the next slide more accurately. But essentially, you can see that the dependency on the security parameter changes from logd to log logd.
00:07:10.768 - 00:07:35.884, Speaker A: And this translates to a rather larger practical improvement. And again, this is still the round by round security. So this is the comparison. So the first thing that I want to stress is that stir can be used wherever fry is. And I argue that you should. So stir is meant to be a drop in replacement for fry. So wherever you use fry, you can use stir.
00:07:35.884 - 00:08:25.474, Speaker A: So it has fewer queries than Fry. So you can see that if you look at these parameters, like the dependency on the security parameter is better in store compared to Fry. This is really where the benefits of STR really shine. Because fewer queries when you compile means that you have to send fewer authentication paths, which leads to smaller arguments. And further, if you have a fewer authentication path, you have less merkle paths to verify. So your verifier ends up performing less hashes and ends up being faster. So, to give you a rough query comparison, if we look at targeting 100 bits of security with fry and stir, which can be boosted by proof of work in both protocols, fry makes around twice as many queries aster.
00:08:25.474 - 00:08:54.444, Speaker A: So this is a rather, if you pardon a joke, a stark improvement. So let me talk a little bit. We make some claims. So we decided to do the sensible thing and actually implement it. So we have an implementation of STR, which is open source on my GitHub. So we use our courser as a backend, we used for the benchmark, a 192 bits field. And we think that our implementation is reasonably optimized.
00:08:54.444 - 00:09:38.414, Speaker A: We've implemented both Fry and stir in order to make sure that the comparison was as fair as possible. And our implementation is modular over both the choice of field, the choice of HME rush and Mercolash. And I claim that at least for academic standards, it's also a decently well written implementation. So feel free to have a look around and let me know if you like what you see. So maybe this slide is what you've been waiting for. What does this translate in terms of concrete efficiency? So we're pleased to say that the stir achieves better argument size and very far hash complexity compared to Fry. On every degree rate pair that we benchmarked.
00:09:38.414 - 00:10:19.756, Speaker A: This improvement ends up to being larger as the degree increases and the rate increases. So, to give you like some concrete numbers for degree two to the 24 and rate one four, like fry proofs are on an order of 170 kb, while stir are around 100 kb. While in terms of ashes, the verifier of fry will perform around 3.5 thousand of ashes while stir draws this down to 1.8. If we increase the degree end rate, you can see that fry proof ends up being around 500 kb, while stir are around 200. So more than a two heads improvement there. The number of ashes also drops down from 10,000 to around 3.8
00:10:19.756 - 00:11:01.778, Speaker A: thousand, which is almost a free hertz improvement. We achieved this by only paying as very modest, like a very small increase in prover time in the non bottleneck part of the computation. So we expect that when you're using fry in when using storing practice, the difference improver time is not noticeable. Because essentially you will be using storing a batch context. And the cost of the initial FFT will dominate. Compared to where stir is slightly slower, you can see that the argument size is significantly better. So verify ash complexity and the verifier time for a larger degree is also improved.
00:11:01.778 - 00:11:18.674, Speaker A: Not that in particular, this is also because we use the blake free as the hash function. If you use as lower ash function, the verifier time of a stir is better across all parameters. So I'm going to now lead it to gal, who's going to give you a rounddown of the techniques that we use in STR.
00:11:20.214 - 00:11:57.984, Speaker B: Thank you. Okay, so now we're going to take a look at the technical level of how STR works. So, the main idea of STR is that a code with a smaller rate. So larger domain is easier to test. So the smaller your rate, the fewer queries intuitively, you should have. So, Str works in iterations, just like Fry. And the Str iteration reduces testing proximity to a code with degree d to testing proximity of a code to degree d over k, just like fry.
00:11:57.984 - 00:12:34.980, Speaker B: But unlike Fry, the evaluation domain is smaller by a factor of two instead of k. So, in practice, this means that the rate is now two over k times the original rate. So the rate has improved if k is larger than two. So think of k is equal four, for example. And then the rate improves by a factor of one half in every iteration. So after every iteration, the next code is actually easier to test than the previous code. So this iteration also amplifies distance, in a sense.
00:12:34.980 - 00:13:06.504, Speaker B: So if the function is delta far from the. From its code, then the new function f prime is going to be far from the new code, except with probably one minus delta to the t, where this, the new distance is almost maximal. This square root can be turned into just rho prime by some assumption, just like in fry. And t. Here is the number of queries that we have. We have a different number of queries for. For every iteration.
00:13:06.504 - 00:13:48.654, Speaker B: So the round by round soundness errors, if we do this thing recursively, turn out to be one minus delta to the t zero in the first round, and then in every round. And so in round I, you have rho I to the ti, roughly. And because. So if we want lambda bits of security, we have to set. So t zero is set with regards to delta, but every ti is set with regards to row I. So since this row I becomes smaller and smaller, this ti becomes smaller and smaller as well. And then we can improve the number of queries done between each iteration.
00:13:48.654 - 00:14:18.770, Speaker B: So, in order to understand how the protocol works, we're going to have to introduce a few techniques. The first technique that we have is folding. Those of you who know how fry works, this is exactly the same folding that they have. You start off wanting to test a function, a reed Solomon code of degree d, and you end up with degree d over k. And the evaluation domain is also smaller by a factor of k. So it looks like this. You have this function f that you want to test.
00:14:18.770 - 00:15:04.894, Speaker B: The verifier samples a random field element, and this defines a function, the folding of f at alpha, the field element that the verifier chose. And this folding has two significant properties. The first is that it's local. So if we want to sample the folding at some location, and we only have access to f, then we can query f at k locations and do some small computation and then derive the folding from that. So we only need f in order to compute the folder. The other property is that it's distance preserving. So if f is delta far from its code, then with high probability, the folding is also going to be far, delta far.
00:15:04.894 - 00:15:48.284, Speaker B: The second technique that we need is quotienting. So this is a technique to roughly enforce constraints on a function and also has this property of amplifying distance. So we have a function f, which is supposed to be a polynomial. We have a set, a function, ants, which is a list of claimed evaluations of the extension of f as a polynomial on some set s. And we define the quotient of f at ants to be f minus. The low degree extension of ants over the vanishing polynomial at s. The vanishing polynomial is the low degree polynomial that is zero, the non zero, low degree polynomial that's zero on s.
00:15:48.284 - 00:16:25.494, Speaker B: And this has two important properties. It's local, just like folding was, in the sense that if you have f and, you know ants and s, you can compute this quotient at any location. And it also has a consistency property. So if you take a look at f, it might not be a polynomial. It might be delta close to a lot of polynomials. So there might be a whole list of polynomials that it's close to. And, uh, the property that the quotient has is that if for every polynomial in this list, this, the polynomial does not agree on the set, on the, with the function answer.
00:16:25.494 - 00:17:08.944, Speaker B: So there's at least one point where they disagree, then the quotient is going to be far from the Reed Solomon code word. So if, if all of the functions disagree on n's, then this quotient thing is going to be really far. The last property that we need is the last technique that we need is out of domain sampling. And this is a technique to at least intuitively move from list decoding to unique decoding. So imagine a prover sends you some function. It's claimed to be a polynomial, but it might be very far from one. In fact, you might have only a list of polynomials that it's close to, just like we saw before.
00:17:08.944 - 00:18:01.124, Speaker B: And we want to work with only one polynomial. We want to enforce that it works for one polynomial. So the verifier is going to choose a random field element, and the prover will send field element back. And because this list is not very large, and by the fundamental theorem of algebra, there's going to be at most one polynomial element in this list that has the property that it evaluates to beta on alpha. So basically this means that the prover has to choose a specific single polynomial, and it sort of commits to using that polynomial in the future. And the way that we enforce this sort of commitment is by quotienting by the point alpha beta. So now that we have all these techniques, we can build the stir protocol.
00:18:01.124 - 00:18:48.594, Speaker B: Uh, so I'm gonna, uh, describe one iteration. So we start off with the function that we want to test. Then the verifier is gonna choose folding randomness. And the prover will answer with a function g that is claimed to be equal to the extension of the folding. But on l prime, which is some, uh, uh, large, some evaluation domain of size l over two, size of l over two, we're going to then, because this thing might be close to a lot of polynomials and we want to constrain this, the proverbs actions, we're going to do an out of domain sample. We'll see how this affects the protocol later. And now we want to actually test that g really is the extension of the folding.
00:18:48.594 - 00:19:37.900, Speaker B: But the problem is that this l prime is even going to be disjoint from the evaluation domain of the folded function. Sorry. So how do we test consistency? Well, we're going to do it with quotienting. The verifier is going to sample a bunch of locations and then quotient g by these locations in order to enforce consistency between the polynomial that is close to g and the folded function. We're going to see exactly what this means in the next slide. So, just to reiterate, the way that the protocol looks is you send folding randomness. The verifier sends folding randomness.
00:19:37.900 - 00:20:38.674, Speaker B: Then the prover answers with something claimed to be the extension of the folded function. Then we do an out of domain sample, and the verifier samples a bunch of locations in the folded function, queries it and quotients the function based on both the outer drain sample and these answers. So, let's have a very quick high level soundness analysis. The claim is that if the original function is delta far from the code, then except with probability one minus delta to the t f prime is going to be really far from its respective code. So first of all, the folding that we did at the beginning already claimed that this preserves distance with high probability. And now we have the prover sends g, which has a list of polynomials that are close to it, and we do out of domain sampling. So this sort of forces the prover to choose a single polynomial.
00:20:38.674 - 00:21:47.224, Speaker B: So there's a unique polynomial that agrees, that has the property that it evaluates to y zero on x zero. These are the out of domain sample points. Then the verifier chooses a bunch of locations in the folding, and then we're going to compare them to this single polynomial that the prover committed to. So if at any point we have the probably that v on xi is not equal to y I, then by the quotienting property, the quotiented function, which is f prime, is going to be really far from the code, because there's only one polynomial that v, and it doesn't agree on this, on at least one of the points that we're quotienting. So we have this quotienting property. So the folding we've already claimed is delta far from the code. And so, because this polynomial is part of the code, it's also delta far from the polynomial v.
00:21:47.224 - 00:22:35.976, Speaker B: And so the probability that f is going to be f is close to the code, is bounded by the probability that all of the points have agreement with the polynomial, which is at most one minus delta to the t, where t is the number of points that we chose. And that's quick analysis of Wiester works. Okay, so I'll conclude now. So what did we see in this talk? So, we saw a few techniques that were used. So, folding, quotienting and out of domain sampling, and we put them all together to construct stir. And then we saw a very quick soundness analysis. And there's a bunch of stuff that we haven't seen that exists in the paper.
00:22:35.976 - 00:23:24.658, Speaker B: So, for example, when we do this quotienting, we actually mess up the degree of the polynomials. And Str only works with polynomials of specific degrees. So we actually have to correct the degree in some sense. So we have a new way to do degree correction. There's also in the paper a high soundness compiler for polynomial IOP's, which is a builds on a compiler that was not concretely efficient in a previous paper. But now we have concrete efficiency, and we also have round around knowledge soundness for this compiler, which is quite nice. And talking about round soundness, we have a clean analysis of round our own soundness for stir, which I think is also, we also worked very hard to make it be well written.
00:23:24.658 - 00:24:12.848, Speaker B: I hope so. This should be helpful for understanding it, and is also very important for security in the non interactive setting. So, what's next for these ideas? Hopefully, next time we have this summit, people will talk about stir with small fields. So there's the new circle Starc work maybe it can work with stir, maybe things with smaller, for example, binary fields. We're not sure. There's also questions about breaking the log d query barrier. So there is a theoretical work that achieves log log d queries rather than log d queries, but it's completely inefficient.
00:24:12.848 - 00:24:34.454, Speaker B: There's a question of maybe making that efficient somehow. It would be very exciting if we managed to do that. And we just want to see people use stir as much as possible. So that's it. Thank you. If you want to see the paper, it's online on eprint. And there's also Giacomo's great blog post.
00:24:35.714 - 00:24:59.022, Speaker C: Awesome. Great. Thank you, guys. We do have a couple of minutes for questions. If anyone has a question, you do have mics on the seats, so please just raise your hand and I'll talk you through how to access that. If there are no questions, that's fine, too. Anna, you've got a question.
00:24:59.022 - 00:25:00.886, Speaker C: Just turn your mic on. Hello. Hello.
00:25:00.950 - 00:25:24.564, Speaker D: Yes. Not exactly a question, but we actually just. We recorded an episode of the show. You guys are going to be on the show soon. But I just sort of wanted to highlight something because even when we were going over it on the show, folding in this context, in the fry context, is different from some of the other folding that we hear about. And I just want to kind of double check that.
00:25:26.504 - 00:25:27.964, Speaker A: Different kind of folding.
00:25:29.824 - 00:25:30.564, Speaker D: Great.
00:25:31.984 - 00:25:37.644, Speaker C: Okay. Yeah, lovely one just behind Anna as well. So just turn on the button, Anna, if you can turn your mic off.
00:25:39.704 - 00:26:03.436, Speaker E: I caught a very quick remark about poly IOP compilation. What does that mean in this context? What is the polyiop fry? And I assume STR is just a proximity test. There's not a polynomial commitment scheme there unless you do extra stuff. So did you do that extra stuff and just an explanation?
00:26:03.580 - 00:26:35.004, Speaker B: Yeah, we did extra stuff. So, first of all, when I say polynomial, I mean for univariate polynomials. So polynomial IOP is for univariate polynomials. And then we have a proper compiler where you give up in the theory, we don't have an implementation, but where you give a polynomial IOP, where the prover sends universals, and this compiler turns it into a proper IOP by combining it with.
00:26:36.584 - 00:26:40.738, Speaker E: So it's just a univariate scheme based on quotienting, essentially.
00:26:40.856 - 00:26:41.554, Speaker B: Yeah.
00:26:42.494 - 00:26:55.514, Speaker A: Yeah. But compared to previous compilers, he achieves, like, better soundness. So the idea is that, like, combine some of the tricks that you've seen here to get, like, a better way to compile univariate polymorph IOP's into IOP's, then you can like BC's.
00:26:57.454 - 00:26:58.046, Speaker B: Cool.
00:26:58.150 - 00:27:04.274, Speaker C: Great. Lovely. Thank you. Just turn your mic off. Any more questions in here? Yep. Karthik, just turn yours on.
00:27:05.614 - 00:27:17.602, Speaker F: Hi. So if I were to use this for small fields, I should just simply use the extension field for the challenge. And then it's the same level of soundness depending on which you might need.
00:27:17.618 - 00:27:19.362, Speaker C: To hold your mic closer to your mouth. Yeah.
00:27:19.418 - 00:27:31.854, Speaker F: So if I use small fields, then I just need to use the verifier challenge with the extension fields. And there is no change if I want to use it for a 64 bit or 32 bit field.
00:27:32.694 - 00:27:49.206, Speaker A: Yeah, exactly. So for like, the. If you want to use mold fields, you can do everything as in fry. So you just sample the challenges over the extension field that you'll be fine. We believe that, like, you can also, like, apply some of the techniques from the circle. Works directly to stir. Like so.
00:27:49.206 - 00:27:54.318, Speaker A: But yes. So in general, like, whatever you can do with fry, you can usually also do with stir.
00:27:54.406 - 00:27:57.034, Speaker F: Okay, cool. Awesome. Thanks for the great talk.
00:27:57.494 - 00:28:05.776, Speaker C: Great. Thank you. I think we'll leave it there. Kathy, if you turn your mic off, that'd be great. Thank you so much, guys. We're going to move on. Thank you very much.
00:28:05.776 - 00:28:06.564, Speaker C: Cheers.
00:28:07.224 - 00:28:08.164, Speaker A: Thank you.
