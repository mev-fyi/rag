00:00:07.770 - 00:00:35.986, Speaker A: So, hi, everyone. My name is Erica Botazi. I work at PSE, and I'm a technical lead at Suma. Suma is a proof of solvency protocol. And today I'm going to explain you how we use Nova inside Suma. So, I think the main takeaway today is that we are not really only using Suma to performance. So most of the time, you see a different backend that you can use to speed up your performance.
00:00:35.986 - 00:01:20.034, Speaker A: But actually, in this case, we are going to use Nova to increase the security of our protocol. So we are leveraging the characteristics of Nova for an application, basically. So, before I want to give you some overview of what proof of solvency is. So, we are operating in the context of centralized exchanges from like the two main pillars are assets and liabilities. From the point of view of centralized exchanges, the liabilities are the deposit of the user. So these are denominated in different currencies. And the main thing here is that these deposits do not live on chain, so they just live in the database of the exchange.
00:01:20.034 - 00:01:58.610, Speaker A: And on the other side, we have the assets. So these are actual cryptographic assets that are controlled by the wallet of the exchange. They do live on chain. And the rule, theoretically, is that they should map at least one to one the deposit of the user for each currency. So the proof of solvency is like a cryptographic proof that is generated by the exchange that is proving that these assets are greater than the liabilities at a specific moment in time. So it's important that the specific moment in time thing is like just a snapshot in time. So this guarantees the solvency of the exchange.
00:01:58.610 - 00:02:46.338, Speaker A: How does it work? So this is not really just specific to Suma, it's more like how generally work. For every protocol, decentralized exchange will first commit to their asset at a specific moment in time. T they will commit to their liabilities. So this commitment can have more or less privacy according to what you want to do, like some ZK stuff later or not, but you can think as basically a nash or like a root of a Merkel tree. At the third step here is the proof of solvency. So the exchange will generate a proof that the assets are greater than the liabilities according to the commitment performed before. And this is given out to the work, so everyone would be able to verify it.
00:02:46.338 - 00:03:39.170, Speaker A: Everyone will be able to see that. But there's like a further step. So, as you see, the first step is that the exchange will generate a proof for Alice. So this is like a proof specific for each user that is telling Alice that she has been included correctly in the liabilities commitment. So correctly means that she has been included with the correct balance. So you can see already like a problem here, because if Alice doesn't perform the verification, so if Alice is not involved in the protocol, then the centralized exchange can basically fake the liabilities at step two and get away with it. So the security of this protocol relies heavily on the user verification.
00:03:39.170 - 00:04:35.230, Speaker A: And the problem is that we cannot really avoid that, because there's no general consensus on the state of a user in a centralized exchange. It's just basically unilaterally decided by the exchange. So this is an issue. And the second, I mean, the consequence of that is that if the user doesn't perform the verification, the centralized exchange can cheat. And how can they cheat? They can remove the balance from the liabilities commitment, or they can, or like the consequences, that they can achieve a valid solvency without actually having enough assets to cover all the real liabilities. And as I said before, this is a process that is related to a specific moment in time. So ideally this should be performed very frequently, and there are like many rounds of proof of solvency performed.
00:04:35.230 - 00:05:25.198, Speaker A: So realistically, what's happening now is that some exchanges are doing it on a weekly basis, but actually with our protocol, we can also achieve it on a per minute basic basis, potentially. So, yeah, something else that I want to point out in this slide is that each round do not really talk to each other. So are kind of independent entity that are not related. There's a very interesting concept that was described by Costas in this paper. That is the failure probability. So this is basically theorizing the probability that approver misbehaves while going undetected. So in this case, the prover is an exchange, and the misbehavior is like understating or removing the balance of some user.
00:05:25.198 - 00:06:11.618, Speaker A: So this graph is basically like the distribution of this failure probability. We are going to focus on this point in red just to like, it's randomly chosen. But just to give you an idea, it means that we are operating on the red curve, which means that this exchange has 1 million user. The x axis 2% means the amount of users that are performing the verification. So in this case it will be twenty k. The c parameter indicates how many users, like what percentage of user the exchange cheated on, like how many user have been manipulated in this liability tree. So is 0.1%,
00:06:11.618 - 00:06:56.846, Speaker A: which means 100 user have been manipulated their balance. And on the y axis we see the failure probabilities. In this case, it's around zero point 13. So it means that there's like a 13% chance that the exchange can perform this malicious action and go undetected. So as we said before, there are many rounds. And as you can see, for each round, if we keep this assumption that there are like 20K user performing the verification, we see that the failure probability is always the same, 13%. So now finally, we get to Nova, what we achieve using Nova.
00:06:56.846 - 00:07:25.370, Speaker A: But first, very brief introduction to nova. So, Nova, as you know, is a folding scheme. We have like three main components here. The z is like the state. So is a state that can start with zero and then is updated at every step. So like z, one is like the state updated on the instance f. So f is like the function that we are applying at every step.
00:07:25.370 - 00:08:29.978, Speaker A: And on the right, you see like the W, which is the private witness applied at every step. So if you go at the end, we have like zn, which is the state after n steps. And we also have the proof PI, which is basically a proof that is able to verify that this all state transition from zero to n was performed correctly according to this instance function f. So how we apply nova in proof of solvency. So we cape as the z is like the round of the state of each round, the function instance f that is applied at every step is the inclusion, and the private witness is the proof of inclusion for a specific user at each round. So you can see like basically every step in this nova proof is a round of proof of solvency. Eventually you get this proof PI, that is still very fast to verify.
00:08:29.978 - 00:09:22.090, Speaker A: It's comparable to like a proof of inclusion that we used in the previous design, but it contains a lot more of information. So rather than containing just the proof of correct inclusion in a single round, it now contains the proof of inclusion in several rounds up to round n. So now I want to give you what is the added security that we gain using Nova. So, we start round one. There are still like 20K user performing the verification, and the failure probability is still the same. So it's the same scenario as before. But what is the difference here? So, let's consider that we are operating in the hypothesis that for each round, there are still 20,000 user performing the verification.
00:09:22.090 - 00:09:52.310, Speaker A: But this user do not repeat. So it's just like a simplification to let you understand what's the benefit. So now for round two, there are 20,000 user performing the verification. So the failure probability is still 13%. But if you see like this 20,000 new user are actually performing the verification also for round one. So we kind of decrease the failure probability of round one to 1.6, because now there are 40,000 user performing the verification.
00:09:52.310 - 00:11:13.140, Speaker A: So we see that the point in red is kind of now going to zero, which is the failure probability. And if we apply another round, we see that for the latest round, the failure probability is still the same, but it decreases for the previous round and it decreases even more for the first round. So now we get into a situation where for round one we have 60K user performing the verification and the failure probability is close to zero, basically, and it goes asymptotically to zero. So what are the conclusion of this presentation like? Thanks to Nova, user can verify their correct inclusion in multiple rounds all at once. So I think also a more general takeaway is that Nova is not only useful for speed, for increasing the performance, but you can actually leverage this IVC characteristic to apply to your application logic. So in this case, the user verifying approve at the most recent round is actually decreasing the failure probability of any round in the past. So it's like the exchange is more accountable for every kind of malicious action they perform.
00:11:13.140 - 00:11:46.880, Speaker A: So they are more likely to be detected. And this can lead to a more secure, let's say crypto world, where exchange do not blow up. And last thing is that this wasn't only like a theoretic application, but we actually implemented that in summa. So this cure code is like the PR that basically applies changes. Thank you, thank you for that. Any questions? We have one here.
00:11:49.730 - 00:12:08.280, Speaker B: Thank you Enrico, for the presentation. On one of your last slides, you were talking about detecting exchange exchanges are more likely to be detected. Is there a way such that the exchanges cannot cheat in the first place, where they cannot even take the assets from onchain, unless they generate a proof of some sort.
00:12:09.370 - 00:13:07.922, Speaker A: Okay, the problem is that let's go back to maybe this slide. No, not this slide, this slide. So the point is that ideally when they generate this proof, we should try to create a system where they cannot cheat. So they can generate this proof only if everything is correct. But for how centralized exchange are designed, there's no really way to enforce the correct state on the liabilities. So they can just say, okay, yeah, I remove your balance from my database and you have no way to verify that this happened. So let's say ideally they could pin each user database on chain, but it's basically then building a decentralized exchange.
00:13:07.922 - 00:13:25.280, Speaker A: So I think this is one of the constraint that you need to operate with when dealing with centralized entities where the database is basically just an opinion of this centralized entity. Any other questions?
00:13:28.290 - 00:13:40.334, Speaker C: You mentioned that the property of decreasing probability comes from IVC rather than Nova. So have you considered any other implementations of IVCs, like fully recursion atomic accumulation?
00:13:40.462 - 00:14:43.270, Speaker A: Yeah, we actually tried to do aggregation in Lo two, which basically, I mean, it gives the same property, but at each step f, you're not only verifying f, but you are verifying a proof from the previous step. But it's way more expensive maybe, I don't know. To give you an idea like aggregation in lo two, for this small circuit, which is just a bunch of ashes, will take maybe between 32nd and 1 minute for each step. But while Nova is very, almost few hundreds of milliseconds, so it was better for performance, it was also better for, I guess, developer experience. I want to mention that this whole nova thing was applied using Nova Scotia by Nalin, so it's a very great tool if you want to experiment with Nova.
00:14:43.690 - 00:14:47.474, Speaker C: Also, where are the proofs produced at the final layer?
00:14:47.522 - 00:14:50.534, Speaker A: Verified where? Yeah.
00:14:50.572 - 00:14:52.738, Speaker C: Like in which context? On chain?
00:14:52.834 - 00:15:14.782, Speaker A: Yeah, okay. Yeah, good point. In this case it's off chain because I don't want to go back again. But the user verification can be performed locally. It doesn't need to be on chain. But if you wanted to do it on chain, I think this might be problematic. You need some other compressor like some snark that is compressing this Nova proof.
00:15:14.782 - 00:15:34.600, Speaker A: So you cannot just take the nova proof and verify it on chain because it still is succinct. But it's not succinct enough for on chain verification. One more quick question. Otherwise, oh yeah.
00:15:36.250 - 00:15:49.020, Speaker D: For this proof of solvency, do you have any assumptions on, for example, the failure, for the failure percentage, do you have any assumptions on every user holds the same amount of money or that sort of thing?
00:15:50.350 - 00:15:53.050, Speaker A: Sorry, can you elaborate more?
00:15:53.120 - 00:16:04.866, Speaker D: Yeah. In the assumption of the failure mode, do you assume every state depositor has the same amount of money? I guess in my head I'm thinking one user could have a huge amount of money, but they never bothered to check it.
00:16:05.048 - 00:16:45.130, Speaker A: Okay. Yeah, okay. I think this failure probability, you can check the paper, but I think they assume that the probability that a user verify is equal no matter their balance. So yeah, I mean, in the real world, of course there are users that are more incentivized, given their stake, to perform this verification, and users, that won't matter. So now this failure probability graph assumes that there's an equal incentive for each user to from the verification all right. Thank you very much, Erico.
