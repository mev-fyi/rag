00:00:04.250 - 00:00:34.600, Speaker A: All right. Hello, everyone. Welcome back to this edition of ZK Study Club. Today we're very happy and excited to welcome JP Almasant and he is the chief security officer at Taurus. He's going to be speaking to us today about zero knowledge proof security in practice and specifically some of the lessons that he's learned or things that he's observed auditing several different projects in the zero knowledge space. So, JP, it's a pleasure to have you over to you.
00:00:34.970 - 00:01:03.230, Speaker B: Thank you, Alex. Well, thank you for inviting me. I'm very happy to be part of the club now. So I hope this talk will be useful to you and to the ZK community. Some part of it might maybe be obvious to some of you or you may find that some part are wrong. So I'd be happy to hear your feedback, your questions. And if you don't learn much from me, maybe I will learn from you, which is also a good thing.
00:01:03.230 - 00:01:05.918, Speaker B: So don't hesitate to interrupt me.
00:01:06.084 - 00:01:27.910, Speaker A: And actually you just said that. So now I'm going to just start by interrupting you. And I think the reason I want to interrupt you is because some people may not be aware, and I want to make sure everyone who views this later is aware of your background because you have a background in cryptography before doing this work. And I think some people may know of some of the algorithms that you've worked on. So do you want to just make sure, can you briefly some of the prior work that you've done, just for context?
00:01:29.470 - 00:02:00.690, Speaker B: Yeah, well, my background is in academy crypto, symmetric crypto. But the last five years or so I've done quite a few security assessments. I don't like to use the word audit, but I looked into many components of blockchain technology, bit wallets, consensus protocols, and more recently zk circuit, zk proof systems. But yeah, I'm cryptographer by training, before crypto was cool, before this blockchain business. So I'm a bit more old school than some of the folks.
00:02:01.190 - 00:02:04.658, Speaker A: And I think if I'm not mistaken, you worked on Blake, is that right?
00:02:04.744 - 00:02:09.814, Speaker B: Yeah, Blake too. And Siphraj is maybe what people want to know.
00:02:10.012 - 00:02:11.670, Speaker C: Crypto is always cool.
00:02:11.820 - 00:02:38.990, Speaker B: Yeah. So yeah, guys, don't hesitate to ask questions, interrupt me. I see some familiar names in the list. So very happy to have you all here. So I will talk about zk proof security in practice. So in practice, I mean implementations. So more specifically, it would be about non interactive ZK arguments as opposed to interactive protocols that happen to be ZK.
00:02:38.990 - 00:03:31.966, Speaker B: And I will focus more particularly on ZK snarks. So as you know, they are one class of non interactive ZK arguments. They're generally full succinct, meaning that the proof is of constant size and verification time is essentially linear in the circuit size. As Alex mentioned in the introduction, this is based on my experience. So, looking for bugs either on a, let's say profession, capacity, being paid to other stuff, or just for fun, for my own interest. So my experience is mostly with proof systems that use the graph 16 proof system as used in zcash and FIco and many other projects. And more recently I had a chance to look into the technology of Alio, which uses Marlin as part of their proof system.
00:03:31.966 - 00:04:08.474, Speaker B: But Marlin is just one of the components. They also use graph 16, but a lot of what I'm going to say applies to other pro systems. So maybe planks and egg factile inimate and to some extent to stocks as well. So why I think it's a cool topic, why it's important? I believe so. First of all, because from the perspective of blockchain projects, it's a major risk. So Zkesnacs in practice are relatively new, relatively complex. So it's typically the recipe for nontrivial bugs, for new classes of bugs and bugs that are hard to find.
00:04:08.474 - 00:05:13.826, Speaker B: And there's a lot at stake in terms of money, in terms of tokens, in terms of data and privacy. So for example, for Zcash it's essential to ensure that the privacy of their users are protected. And for the layer two protocols that use snarks or ZK snarks in ZK rollups, it's also extremely important to have some security assurance. And maybe from my personal perspective, I think that zk technologies are the most interesting crypto by far today because it's well used in practice. First of all, it's real world crypto, as we say today, it's deployed at scale. There's a lot of interesting papers, a lot of good code being written, and it's intellectually interesting and refreshing. As a cryptographer that's been around for maybe 15 years, I've looked at so many new ciphers, new secure channel protocols, new end to end encryption protocols, and it's quite boring at the end.
00:05:13.826 - 00:06:13.300, Speaker B: It's always the same, but differently. So here it's a very new world that is relatively simple but complex at the same time. So by simple I mean that it's non interactive. So you just send one payload essentially, if you compare to MPC protocols that are highly interactive, like threshold signing protocols, where you have to think and to reason in terms of asynchronous communication. What happened if this message is not received? How do you securely abort the protocol? I don't like reasoning in this context, and the same for consensus protocol to some extent. But Zksnards are also complex in the sense that many moving parts, many non trivial components as part of Zkesnards, for example perimeter commitments. And it's kind of multidimensional in terms of security, because there's the famous three in Ocean sound as complete as zero knowledge, and some notions that are more tied to the application.
00:06:13.300 - 00:07:19.066, Speaker B: So if you do not it of a technology using Zksnocks, it's not only about the proof security, it's about is the proof securely used in the context of the application. So what does it mean to think in terms of security of Zs narcs in practice? So I think generally the most critical notion is soundness, the most likely to be compromised, and also maybe the most critical in case it's compromised. So soundness is about making sure that proofs which are not valid are always rejected. So you want to ensure that from the attacker perspective, it's impossible to forge a proof from scratch, to alter a proof in the sense of malleability, take a valid proof and turn it into another valid proof that would be accepted, and also to prevent the replay of proofs. So you may think of cases where different nullifiers were accepted as valid to replay a proof. Second, zero knowledge. So I don't know at this point.
00:07:19.066 - 00:08:11.210, Speaker B: Maybe some of you might disagree when I said that zero knowledge is less important, less critical and soundness. So happy to hear counterpoints here. But one reason why I rank zero knowledge a second is that there's a little leeway in leaking information because you have a succeeding proof. So there's little freedom in leaking a lot of information. And even if on paper, ZKNs is compromised in the sense that maybe some internal component is not as ZK as it should be, and maybe the proof is not valid in that sense. But in practice, exploiting that to leak information from the proof itself is not always trivial and not always feasible as far as I can tell. And the last one, completeness.
00:08:11.210 - 00:08:50.360, Speaker B: So in most of the cases it's more about, well, deny of service availability, usability. In case, for example, valid proof is not accepted, then the user is not happy and it might be exploited in indirect ways. And if you have systems like decentralized private computation, DPC systems where arbitrary circuits are likely to be accepted if certain types of programs are not accepted, then it could be a problem as well, but presumably not as critical as compromise of soundness at this point. Do you guys have any comments, questions?
00:08:52.970 - 00:08:55.190, Speaker D: There is a comment in the chat.
00:08:55.690 - 00:08:58.006, Speaker A: Yeah, it just came in.
00:08:58.108 - 00:08:58.710, Speaker B: So.
00:08:58.860 - 00:09:19.726, Speaker A: Well, Victor, I can vocalize it. I might as well just say you're asking if there's like, if we disagreed. And I guess I was thinking like, oh, well, what about ZkkyC systems that some people are proposing right now? I can imagine that you would actually care more about zero knowledge than in soundness in that case, if you had to choose, because people's personal identity is.
00:09:19.748 - 00:09:22.222, Speaker B: Being protected by these kind of systems. Right?
00:09:22.276 - 00:09:23.586, Speaker C: That's just kind of example.
00:09:23.688 - 00:09:24.820, Speaker B: Yeah, that's an example.
00:09:25.990 - 00:10:07.310, Speaker C: I would distinguish between the importance of meeting the privacy properties and the actual likelihood of them being broken, because I think you're right, JP, that in practice, there's quite little scope for the proof itself to leak much information. So yeah, it's important to have a proof that it doesn't leak information, but if privacy is going to leak, it's probably going to leak from other parts of the high level protocol rather than from the CK proofs, right?
00:10:07.460 - 00:11:25.702, Speaker B: Yeah. And as you mentioned, we should see it in terms of likelihood, tax impact or impact varies a lot depending on the application. Okay, so now this part just briefly, from my perspective, as someone who likes finding bug and found a few, maybe the problem for blockchain projects, when you want to hire people and you want to have an external view on your project, maybe you would agree that a common problem is that auditors have limited experience because the KPIs and projects are relatively new, limited knowledge because you want people who understand both theory to understand what's happening and what's the risk and who understand the implementation. And oftentimes you read the paper, you look at the code and you're like, okay, what is that? Because implementers know a few optimizations, a few tricks that are not mentioned in the paper. So you need people who can understand the code and match the code to the specs. And it's not always easy. And unlike some other types of protocols, we don't have a checklist, or there's no official, no documented checklist of all the bugs that can happen, all the things that can go wrong.
00:11:25.702 - 00:12:22.718, Speaker B: So as an auditor, you build this based on your experience. But I think depending on the team, there's very few public resources online that will tell you, oh, that's the kind of bug that you want to look for. And likewise, there's very little tooling and public methodology rated to zk proof. I think most of us will agree that most of the bugs, most of the interesting bugs have been found not by external auditors, but internally by internal audits, or accidentally if you took the case. I think the bug that Ariel found in Zcash in 2019, this bug was very hard to catch, for example. So I think this calls for a completely new approach from the auditor perspective. I think that what works is to have a highly collaborative approach with the designers.
00:12:22.718 - 00:13:35.146, Speaker B: So, for example, have regular video calls where the developers, the guys who wrote the code, walks you through the code and explain what's going on, and the answer the auditors question. And to have, for example, a chat channel where auditors can ask all the questions they have. If we think we found a bug. So before writing is in the report and spending time, ask the guys, okay, is it normal, is my understanding correct, of what can happen? Because oftentimes I would find something that makes me a bit nervous. I'm like, okay, this shouldn't be this way, but sometimes the developers told me, no, that's here for a reason, we're aware of it, but look, we can explain why it's fine. And also, what I found very helpful was to do a threat analysis, a kind of threat model, where I have a clear view of what are the risks, what input is controlled by the attacker, what input depend on the context, and what depends on the configuration. So what are the degrees of freedom from the perspective of the attacker? Ultimately, you want to report useful bugs, useful risks, and the developers have to prioritize mitigation.
00:13:35.146 - 00:14:12.538, Speaker B: So you want to tell them whether this can be exploited or not. You don't want to tell them a list of things that as a cryptographer, you don't like to see. And also, what I found extremely valuable is to try to play myself, write some code, play with the code that I'm given to review, not try to call a complete proof from scratch. But you learn only by doing. You don't learn by reading code only. So this practical experience is super important. And I've tried to find information about bugs, about potential risks.
00:14:12.538 - 00:14:46.614, Speaker B: So, of course, in public disclosures, what people document in write ups, what you may find in CTFs. I also looked at the reports by other teams. I found very little useful information there, to be honest. But that's useful in itself. And also looking into the issue trackers of different projects, looking into the PRS, and it's not always clearly tagged as a security issue. Sometimes you have to look in the issues and think, okay, this might be a security bug. And then you look into the comments and you see this.
00:14:46.614 - 00:15:00.474, Speaker B: It's a security bug. But searching is not easy. And also, some people from the community have been very helpful, some of you in the attendees. So, yeah, it's essentially the reconnection step.
00:15:00.512 - 00:15:34.760, Speaker C: That is, have you found difficulty in determining what a protocol is supposed to do, whether it's documented well enough? I know that this is something we pay a lot of effort on in Ccash. It's basically my job to document the protocol, but I haven't seen some of the documentation for other projects. And I'm wondering, how do you go about auditing a project where the documentation is sparse, shall we say?
00:15:35.930 - 00:16:53.620, Speaker B: That's difficult, because the first thing I want to understand is the worst case scenario from the project perspective. What are the key assets, what they want to product? And for example, if you have inputs that come from the protocol, nonseas nullifiers, or things like this, I want to understand who can control which input and how they come into play, how they interact with the proof system. And what I also want to know is which parameters I fixed are fixed by design, and which parameters can potentially be changed by the users or the attackers or the application. But just having a clear documentation or the protocol that matches the code is hard to find. Some projects will tell you, okay, we have the documentation, but it's for the previous release. And what's also extremely helpful is to use consistent notations between the specs, the paper, and the code, because otherwise you have to do this mental mapping between the nolation in the code in the paper, and it's nightmare all the time. Okay, thank you.
00:16:53.620 - 00:17:53.426, Speaker B: So I can give very general examples of things that could go wrong in the very general workflow of a proof system. So you have a program, a computation, then you describe it in terms of a circuit. There's a message step where you turn it into a one cs or ir model, depending on your proof system, and then the actual prover gets you approved, which is integrated in the application. That's maybe an oversimplification. So, first of all, if your program is not secure in the first place, then no matter how much ZKs, no matter how much crypto you throw at, it won't turn it, it won't be secure. So you want the program to be secure. You want the crypto primitives to be the right ones with the good security parameters, and then you want the circuit to be equivalent to the program, which sounds obvious, but in practice, it's not always easy to realize and not always easy to test.
00:17:53.426 - 00:18:51.830, Speaker B: I've seen a number of projects having vanilla version, so a non circuit version of the logic, and having the ZK, the circuit version, and trying to compare the two to have some kind of test vector. So that's very useful, I guess. And then of course, when you translate the circuit in a constraint system, then you want the constraint system, the cs, to really enforce the constraints defined by your circuit. That's what the definition of the constraint system. And then in the proof itself, a lot of things can go wrong, for example, having insecure components or not having the right properties. Let's say if you have a common naskin, you want it to be hiding, and it's not fully hiding might be a problem. And even if your pro system is safe in itself, then you have to make sure that, for example, the application prevents the replay of previous proofs, for instance.
00:18:51.830 - 00:20:24.100, Speaker B: So a few examples of how you could, for example, compromise soundness. So if your constraint system is not effectively enforcing certain type of constraints, certain type of arithmetic operations, if your keys are not sufficiently protected, if your secrets are protected, if the secrets from the trusted setup, if you have a trusted setup, if they're not cleared after the ceremony, in terms of zero knowledge, you have the risk if you treat private data as public data. So in general, it's pretty clear from the application perspective what is private, what is public. But it might happen that internally you treat private data as public variable would be a problem. And what I call metadata attacks is if you obtain knowledge, if you gain some leakage on the internals based on the usage patterns of the user. So from the public on chain activity, you could get some information on the private variables based on the usage patterns that you see on chain and completeness. So what can potentially go wrong? So your constraint sensitizer might not like certain types of sockets, might not like certain types, certain number of variables, and might fail to give you a correct constraint system.
00:20:24.100 - 00:21:47.246, Speaker B: Also, if you have multiple gadgets, you want to make sure that they compose correctly between each other. That in terms of, I don't know, NDNs, in terms of data type, they're all compatible, so to speak, and maybe more project concerns, because ultimately everything is software. So you might have bugs that have little to do with crypto, with zk proof, but just classical software bugs, for example, encoding bugs, bugs and dependencies. So what we now call supply chain bugs to not only the trusted setup, but also in all your dependency graph, making sure that there is no, of course, no vulnerable dependency, even though it might not be a huge problem depending on how you use it, but have some level of assurance on who has control on the code of these dependencies. Because if I see that a small project with three stars on GitHub is used in the ceremony of Alio, then an attacker might be tempted to sabotage it using this dependency. And last but not least, so the onchain software, if you have verifiers of proof running on chain, so we know that every other day there's a big new disaster in terms of smart contract bugs. That's how it is.
00:21:47.246 - 00:22:34.640, Speaker B: But yeah, smart cont. Well, what I want to say is that it's already quite hard to do off chain zk proofs. Doing it on chain is even more complex. So I tried to break down the different components as a kind of software stack, starting from the bottom to the platform to the application, and trying to distinguish the different types of inputs. That's typically how I would try to reason if I had to edit a project. So what input is really clearly adverse oil, what is directly coming from the protocol, like the block number for example, and what is hard coded by the configuration. So for example, what type of curve you use, the number of variables, this kind of thing.
00:22:34.640 - 00:23:45.010, Speaker B: And I tend to observe that the biggest risk in terms of ZKs are the application level, whereas the biggest risk in terms of completeness and soundness are the lower levels. And each layer has its own type of risk, its own types of subcomponents, starting from the top here. So in the application you may try to distinguish the key management NOS management issues. Also, as an auditor, I usually like to look at the unit test at what is tested, what is not tested. Do you test the failure cases or only the success cases? How do you define the interface? Are there any potential side channels that could be exploited? To what extent can value be replayed? So of course in the proverb and verifier, there's a whole lot of components. So hash functions, PRF, algebraic hash functions, algebraic commitments, randomness, randomness interfaces. You might have non uniform distributions to simulate of course all the miracle tree perform membership logic.
00:23:45.010 - 00:24:46.470, Speaker B: That is quite complex, but now people tend to get it right. Also the fiat shamir construction. So here you want to make sure that you hash everything that needs to be hashed, and non trivial components such as hash to curve polymer commitments, where we sometimes found some bugs, and at the very lower level. So you rely ultimately on your platform, your language, the version of your language, the runtime, your hardware, the microcode, all the dependencies, the randomness coming from your system. So I'm quite happy to see that today the randomness situation is much better than the ten years ago. We tend to have quite reliable crypto random generators, but you can still find some cases. For example, I've seen recently that the wasm bindings, the wasm use of randomness for certain types of architecture was not perfect in case of a known architecture.
00:24:46.470 - 00:25:57.150, Speaker B: So we still find some time to time randomness issues. What's important, maybe from a designer perspective and from a documentation perspective, is the contracts between your different components. What are the preconditions that you guarantee and what do you guarantee to the coder of your function? A very typical example is the group membership checks. So when you receive a number that is supposed to be a scalar in a given range, where do you make this check? So you can make it everywhere all the time, but you don't want to make the check at every place. So what I see many projects doing is when they instantiate a group element from integer, they do the check once, and then this group element will always be enforced to be an actual group element. And likewise, if you need to check that group element is nonzero, you want to make sure that, well, typically the right thing to do is to do it as early as possible when you receive it from the application. But it doesn't hurt to do it multiple times.
00:25:57.150 - 00:25:59.630, Speaker B: In case a function is used in different contexts.
00:26:01.090 - 00:26:08.430, Speaker C: We tend to make another type for the non zero thing if we're using a typed language like rust.
00:26:08.590 - 00:26:15.300, Speaker B: Okay, yeah, that's the ideal case, because then you're sure that it won't be zero.
00:26:15.610 - 00:26:18.790, Speaker C: In general, refinement types are really useful in crypto.
00:26:23.850 - 00:27:06.820, Speaker B: Okay, so to finish, I'd like to go through a few examples of bugs. So, not my complete list, and not all the bugs that have been ever found, and maybe not the most interesting ones. So at the end, I'd be happy if you guys can share some examples of bugs that I you've been aware of. So this one is the one I mentioned before, the case of a missing overflow check. So here in that case, a nullifier was not verified to be less than the modulus. So different distinct nullifiers would be accepted that ultimately would reduce down to the same modular value. And the exploitation scenario here was double spend.
00:27:06.820 - 00:27:50.930, Speaker B: So another missing overflow check. So in that case of a public input, which project was it? And another one. So this one is interesting. I think Kobe mentioned it to me recently. So in this project, the check was initially set here at line to 22, but the developer removed it because they believed that it was not necessary, whereas apparently this check was indeed necessary. And I think that this change was not accepted. So you also have to be careful of people who want to optimize the code to save some gas and remove some useful checks.
00:27:53.750 - 00:28:54.470, Speaker C: I've got a good example. In the orchard protocol, which is about to activate on zcash, an earlier version of the circuit, there was this requirement in the spec that a particular value needed to be a non zero group element or non identity group element. If you know the zcash protocol, it's GD. And we had used a type for these values which excluded zero, and that was checked correctly. But what we didn't do is the circuit didn't include that check for non zero. So all of the witness calculation was enforcing this, but the circuit itself wasn't.
00:28:56.650 - 00:28:58.870, Speaker B: And how did you find the bug?
00:29:03.050 - 00:29:27.730, Speaker C: An auditor, it was kedit, actually. They didn't noticed the bug. They actually thought that we had implemented this correctly, and that they said in the first version of their report that this is a potential problem, but you've implemented it correctly. And so I checked it, and no, we hadn't.
00:29:30.070 - 00:30:33.878, Speaker B: Yeah, so for another type of bug. So this one is an R one cs constraint constraint system, and it's a case where the inverse property of an element was not enforced, so you could submit to an element that was not the inverse of the one it was supposed to be the inverse of, and the proof would have been accepted because it would not have enforced that constraint. So I think it was also reported as a rustec advisory. It wasn't in hard work. So, I mean, they have very good code base, and I think in every project, you should not judge the quality of a project by the number of bugs they have, because it might just tell you that people have been doing a good job looking for bugs in the first place, was projects for which no one reports bug. Well, maybe it means that no one is looking for bugs. I just don't want people to get.
00:30:33.884 - 00:30:36.920, Speaker C: The more bugs I find, the more confident I am.
00:30:39.850 - 00:31:27.734, Speaker B: This one credited to Kobi again. So it was in Merkel tree computation, and it was more language issue. The wrong symbols were used for the equality check, and then the equality check was not done. So the Merkel route was not correctly compared. So an incorrect Merkel route could be accepted, and this could be used to get invalid props accepted, and it was fixed quite easily. So this is the bug I referred to before. So this kind of bug are relatively rare, because I don't mean to say that there's no bugs in the papers, but the bugs in papers are quite hard to find.
00:31:27.734 - 00:31:50.240, Speaker B: So in that case, I at Gabison found a problem in the paper, in the description of the trusted setup, and the problem was that some values that were actually toxic and supposed to be clear were not cleared and it could later be exploited to the double spend ultimately. Is that correct?
00:31:51.170 - 00:31:52.160, Speaker C: That's exactly.
00:31:54.450 - 00:32:27.674, Speaker B: So the zcash team did a very nice job, first of all, disclosing this to the project, reasoning the same code base, because now we have the problem that once you find a bug in your code, that it might not only be your code. So in terms of disclosure, what do you do? Do you quickly fix it for you? But if you fix it and disclose it, then some people might exploit it to exploit other projects. So you want to try to be as responsible as possible, but also to protect your own project. So that might not be easy to manage.
00:32:27.872 - 00:32:38.880, Speaker C: Yeah, we had to basically define some cut off in market cap of the projects that we wanted to inform.
00:32:39.890 - 00:32:47.220, Speaker B: But if I had not found that bug, I wonder when it would have been found, if at all.
00:32:50.150 - 00:33:04.406, Speaker C: The issue was that the BCTV 14 paper didn't have a complete security proof. I think it would have been found one way or the other, because it.
00:33:04.428 - 00:33:09.740, Speaker D: Wasn'T just a security proof. Right. Like the protocol wasn't in the main body of the paper.
00:33:10.190 - 00:34:00.570, Speaker C: Yeah, exactly. It was an appendix, and it was assumed to be secure based on being similar to PhGR 13, which did have a correct security proof. Well, correct as far as I know. So I said I think we were always going to try to produce a rigorous proof and check it thoroughly. So it's a matter of time before that bug was found. Yeah, I think that we didn't really have a lot of choice but to use VC 314 because it was basically the only improving system that met our performance requirements when we launched cCash.
00:34:01.710 - 00:34:11.758, Speaker B: I wonder if we shouldn't spend more time reviewing proofs and papers generally, as opposed to only code.
00:34:11.844 - 00:34:13.760, Speaker C: Yeah, definitely.
00:34:17.890 - 00:34:30.318, Speaker B: When papers submitted conferences, reviewers are doing their best, but oftentimes part of the proofs are in appendix and it takes many eyes to review proof correctly.
00:34:30.494 - 00:34:52.860, Speaker C: So I think part of the issue in that paper, it was trying to do a lot. It was trying to do too much, actually, because zero cache is a complicated protocol in itself, and the proof system that was a modification of PhGR 13 should have been probably a separate paper.
00:34:56.910 - 00:35:15.780, Speaker D: To some degree. To some degree. I don't even really think it's the job of reviewers to be finding these kind of cryptographic bugs. Like the job of a reviewer is to decide whether a paper is interesting or not, and it's more the engineering community that has misunderstood that.
00:35:16.630 - 00:35:23.410, Speaker C: I agree. Yeah. I don't rely on peer review to find bugs, because it doesn't.
00:35:24.550 - 00:36:23.634, Speaker B: And I've seen in the case of MPC protocols, of interactive protocols, some proofs being written with certain communication assumptions which are not necessarily correct in practice. So the next bug, this one is more the application level, was disclosed by a stack, and it was about the nonsense management, about the choice of nonsense. So the proof system was fine. Apparently it was just the way inputs were chosen. So like in the first example, they use nullifiers instead of account nonsense. And in the second one, the nonce was not included at all in the encrypted. Not this bug.
00:36:23.634 - 00:36:41.050, Speaker B: Well, don't want really to call it a bug, but it's a scenario whereby you cannot completely break your knowledge, but you can exploit some usage pattern information and leverage it to try to compromise the ZkNs.
00:36:44.530 - 00:37:16.680, Speaker C: I have to be careful about how I phrase this, but this paper is a very poor quality paper. They do not understand the protocol. The original attacker relied on a property of the protocol that it doesn't have. And when we tried to communicate with them about this, let's just say that that communication was not very productive. The problem wasn't on our side.
00:37:17.950 - 00:37:24.874, Speaker B: Yeah, the paper is a bit surprising in terms of the claims being made.
00:37:24.912 - 00:37:31.470, Speaker C: But the claims are, I'm pretty sure, false.
00:37:32.850 - 00:38:07.000, Speaker B: But nonetheless, this type of attack might be extremely relevant if you know that two inputs might be similar or might have some type of relation. And I don't know if the security proofs take these potential leaks into account. For example, in case of shield transaction, you know the potential relation between two amounts. You know that this one is bigger than that amount.
00:38:08.910 - 00:38:43.250, Speaker C: Yeah. It's certainly very difficult to analyze a protocol that has a transparent aspect when you're trying to analyze it for privacy. We've taken that into account when we're designing the unified addresses for the new version for Nu Five. The next upgrade is zcash. So the default privacy policy for that, if you're using a unified address and a transaction is for everything to be private.
00:38:46.890 - 00:39:43.138, Speaker B: No, maybe not the last bug. So this one was kind of zero knowledge bug that was kind of impacting zikiness, but not very clearly. So they were missing some random blinding factors. So I think that on paper, it kind of compromised your knowledge in terms of the proof not being valid anymore, but I'm not sure. And I don't think it was really leaking data in the proof, in the blanc proof. Maybe plonk people will notice better than me, but that was my basic understanding of this bug. It doesn't mean it should not be fixed, but this one was again in a disclosure from Aztec.
00:39:43.138 - 00:40:04.350, Speaker B: So it was a bug in the mercury competition. That was a DoS bug, but a kind of bad one in the sense that it could potentially freeze the ability to verify zklapse. So maybe I can skip the technical details. It's a tree computation bug.
00:40:06.050 - 00:40:14.918, Speaker C: That blog post, I think it wasn't entirely obvious from the summary of the post, but there were two bugs there. I think the other one was for soundless bug.
00:40:15.034 - 00:41:27.990, Speaker B: Okay, and maybe this last is not because it's interesting, not because it's clever, but it's one I recently found in the implementation of the Cairo language. And it's a problem that some valid signatures are being rejected. And my understanding is that the developers believe that the risk was negligible, whereas from my perspective it's not that negligible, it's to the power -54 so, meaning that you can forge a signature that could be actually valid, but rejected by the onchain contract derived from the Cairo implementation. I haven't yet got the feedback from the Cairo team. I'm curious to know what I think of this, and actually the debug was that they enforced the solen values to be less than two to the number of bits in the exponent in the modulus, as opposed to being less than the modulus. And I don't know why they do this. Maybe to save gas.
00:41:30.750 - 00:41:49.114, Speaker C: They'Re doing it in a circuit, or they're doing outside the circuit, because doing a check against a specific value rather than a power of two a circuit is about, there's an optimized way to do it that depends on the number of set bits.
00:41:49.242 - 00:41:50.878, Speaker B: Yeah, that was my guess.
00:41:51.044 - 00:41:55.954, Speaker C: Yeah. It's roughly about one and a half times faster. So maybe it was a performance type.
00:41:56.072 - 00:43:05.430, Speaker B: Yeah, and other types of issues that we've noticed or that we've thought about. For example, in protocol commitments, Peterson hashes making sure that your bases are unique and correctly generated. We've seen situations where the padding, when in the hashing or commitment, there was no padding at all, because you accept a value of fixed size, but at some point then you start using it with a value of variable size, then you want to have padding, otherwise it's completely insecure some small bugs that we noticed was, for example in the poison hash function linear algebra step. So I think one vector matrix product was computed as matrix vector or the opposite. So it wouldn't reduce the security, would just make two implementations incompatible. And also when you fiat chamia stuff, so make sure that everything from the transcript is correctly included in the fiat chamber hash. I mentioned the composability before side channels.
00:43:05.430 - 00:43:56.710, Speaker B: In most cases, people try to, people make some effort to avoid blatant constant time leaks. But most projects, many projects will tell you that they have no guarantee that it's constant time, and it's not designed to be fully constant time. Regarding with respect to the private values, the question is when? How could this be exploited or not? Same for the potential ram leakage. If you have some value that is not zeroized, to be honest, I don't think you have to worry much about it. But at the same time, I don't want to tell people, okay, don't worry about it, because they will find the one case where it's important. And another big class of risk is the speculative execution leaks or these spectre type of attacks.
00:43:58.430 - 00:44:16.362, Speaker C: Go back to that page. So there's a really interesting example of the fiat shama problem. It was in an election protocol that I think was used, or going to be used in swiss and australian elections.
00:44:16.506 - 00:44:17.200, Speaker B: Yes.
00:44:19.010 - 00:44:47.190, Speaker C: So they weren't hashing the instance. Swiss law has really strict transparency requirements for software used in elections. So it's found in the swiss election, but the australian laws are the opposite. It's basically security by obscurity, enforced by law. And so, yeah, don't do what Australia did for their election.
00:44:47.930 - 00:46:02.314, Speaker B: This squishy voting business was quite a story. So, yeah, to conclude on this, well, this talk comes from a frustration I had of not finding bigger bugs than I found, and also from the realization that there hasn't been that many bugs found. So I tried to understand why. So maybe because all the people in this ecosystem are pretty smart and they know what they're doing, they write secure code. Presumably the use of domain specific languages like Carol Noir will make it easier for people who are less experts to write safe code, because DSL will do part of the work for them. And also, maybe one reason why we didn't have many critical bugs is that there's a very narrow attack surface in terms of what the attacker can control and what the proof exposes as a result. In terms of constant size proof, however, what is still worrisome is that very few people understand Zksnarks at the level needed to find bugs.
00:46:02.314 - 00:46:45.662, Speaker B: So even fewer people can really understand the code and have the time and knowledge to find these bugs. We probably need more tooling. I know that there are some e forcing going, some people trying to do fuzzing, differential fuzzing. I don't know if formal verification can help. Usually not a big fan of verification, but doesn't mean it's completely useless. And also, I think it's just the beginning. In terms of the ZK technologies, there will be much more projects using zk proofs, so which means much more value at stake, which means more incentive to find bugs and maybe more incentive to withhold the bugs and exploit them as opposed to disclosing them.
00:46:45.662 - 00:47:15.560, Speaker B: So we want to avoid the same situation. And in the Defi web three business where you know what happens. Thank you for attending. Thank you for the discussions. And I would like to thank some people and companies who helped me with this course. So Alio protocol labs, Kobe, Adrian, Lucas and Matilde, and happy to use the time left we have for taking more questions.
00:47:17.630 - 00:47:22.102, Speaker A: Darren, did you have your hand? It might be a holdover.
00:47:22.246 - 00:47:23.690, Speaker C: Yes, it's just left.
00:47:23.840 - 00:47:24.540, Speaker B: Cool.
00:47:26.270 - 00:47:44.586, Speaker A: AJP, thank you. Very, really fascinating talk. On your last point, you mentioned vulnerability and high ROI for people to exploit bugs. Don't open a short position that then later gets liquidated because no one knows about the vulnerability.
00:47:44.778 - 00:47:45.790, Speaker B: Just joking.
00:47:46.450 - 00:48:10.390, Speaker A: We shouldn't joke about this, since for anyone who's watching this in the future, this is a joke about, I think, what is it, $625,000,000 hack of a smart contract bridge that the hacker opened a short position for planning for the vulnerability to become more widely known, and apparently it wasn't for six days, and his short position was liquidated. Kind of hilarious.
00:48:12.330 - 00:48:17.162, Speaker D: Anyway, on a centralized exchange, which is going to be found.
00:48:17.296 - 00:49:27.926, Speaker A: Yeah, but anyway, back to the topic. JP, thank you very much, and we can start questions? Does anybody from the audience have any questions to start us off? I have a couple, but I want to turn it over to folks who are listening first. Okay, I'll start with mine. So, JP, at the very end you mentioned a couple things about tools that can help with, aside from auditing, which is kind of an active preventative measure, that's kind of a weird way to put it, but someone to go in and look at your code and audit it. You mentioned these tools such as fuzzing and formal verification. Are you seeing, or do you think in your mind starting to be more standardization around some of these tools? I know at Alio we kind of like built an in house fuzzer for Leo, which is our specific language. But it feels like to your point, in this talk, there's many of these same similar bugs, in fact, multiple projects, and it seems like it's crying out for some kind of industry or higher level tooling that people could use.
00:49:27.926 - 00:49:32.322, Speaker A: But have you seen anything like that standardization around these tools, or is it still pretty infant?
00:49:32.466 - 00:50:20.150, Speaker B: Not really budget tools, but I kind of recall some people mentioning ID to standardize encodings of constraint systems, of polynomial constraint systems that could be compatible with different implementations of, I don't know, graph 16 or oplunk or Marlin. What might be also useful is to have a kind of standardized interface to allow for differential fuzzing. So let's say you have two implementations of Marlin, and you want to make sure that given the same parameters approved, that is accepted on the first instance, will be accepted on the second, and likewise will be rejected on both. So I have no idea how difficult.
00:50:20.220 - 00:50:20.920, Speaker C: It is.
00:50:23.450 - 00:51:43.650, Speaker B: But it proved very helpful in the context of basic crypto libraries with projects like witcherproof, and also going back to earwitch proof. It's a project where you fuz an interface, but it's a kind of smart further in the sense that, let's say you fuz ECDs interface, you will know which type of cases are error cases should fail, for example, if you send a noise zero signature. So if you know that you're talking to a graph 16 or plunk or mal improver, you would try to send a malform proof that is created in such a way, maybe to fool the system or to make it crash. So try to cover all these edge cases. But before doing this kind of fancy fuzzing verification stuff, it's not about how much audits you throw the project, it's about your internal security SDSE process, starting with things as basic as test, as unit test, and as documentation and integration tests. So first have writer of unit test, maybe new CI pipeline, and then start doing some fuzzing. But don't waste your time investing in formal verification before you have a good pipeline.
00:51:47.590 - 00:52:33.890, Speaker C: An issue with fuzzing cryptography is that often the cases that you're trying to hit have very low probability when given random input, even to the point where it's difficult to get to the point in the code where the bug actually is. I know a potential way of getting around that would be to use, say, an SMT solver to try and solve for inputs that get you further into the program. Do you know of any other approaches do you think that's the practical approach?
00:52:35.670 - 00:53:26.850, Speaker B: Yeah, I agree that fuzzing. I've seen people trying to fuz hash functions, which is completely useless because straight line programmer. What I also found very useful. I've seen some projects organizing internal audits. It's like for one week you create small task force and people who know the code and they really focus on finding bugs and maybe developing some new tools. And if you hire external auditors, you want to make sure, well first of all they have the capability to help and maybe that they get enough time to understand what could they looking at what can go wrong. And that you don't start directly by looking at the code because it's not useless, but not a good return investment.
00:53:27.430 - 00:53:55.514, Speaker C: Yeah, I would say you have to spend as much effort developing tooling as writing the proof system and circuits in the first place, because otherwise you're not going to be able to understand or visualize your circuits. Yeah, that's definitely been a problem with zcash. We do have some tools, but not enough and not enough time to build.
00:53:55.552 - 00:54:22.050, Speaker B: Them and knowing where to focus your effort. Because let's say a project with the complexity of audio, with the multiple layers of complexity and the multiple probing systems, then you might have perfectly fine ZK proofs and constant systems. But then at the application layer you forget one minus sign or you forget one variable.
00:54:23.930 - 00:55:05.010, Speaker C: Yeah, we just found a bug in orchard. It was actually a completeness bug, but we'd made a change to the protocol so that instead of having one incoming viewing key for notes, there were two possible incoming viewing keys and we hadn't updated the implementation. So sometimes it was using the wrong key and therefore the proof verification was failing. So we had a little bit of a panic when we thought this was some complicated, subtle bug in the circuit, but actually it was just using the wrong instance.
00:55:05.590 - 00:55:21.720, Speaker B: Don't you think there's some level of security by obscurity because it's so hard to approach, it takes so much experience that most likely these bugs will be found by the teams, by the developers, as opposed to by bad guys.
00:55:22.650 - 00:55:58.370, Speaker C: I mean in principle a bad guy. If they're willing to put enough time into it, they can learn a protocol, they can learn an implementation, and they can find anything that you could find. It's really difficult for them. But how do you know that, for example, that the attacker isn't some former employee or an auditor for that matter? There are enough people who could potentially understand this stuff that you do have to worry about that, even if it's unlikely.
00:55:58.950 - 00:55:59.266, Speaker B: Yeah.
00:55:59.288 - 00:56:58.806, Speaker A: I think it's interesting, going back to the point that you talked about, both of you, with regard to tooling. It does feel like that because many of these systems are so nascent, there's an opportunity to build better tooling, as you said there, just to even visualize in some cases, like what is happening. Anna, I was thinking about the grants, the kind of zk grants program that you run. Something like that feels like a perfect thing for some kind of grant for a team to go and build something that would be beneficial for all. And I think at the end of the day, what do they say? At least not with regard to things like tools like fuzing, but with regard to testing. Certainly these basics that we kind of, like all software engineering, are just generally good software engineering practices. I think it feels like there's still quite a long way to go just with regards to kind of enforcing and doing the basics across the board, and it's difficult.
00:56:58.806 - 00:56:59.960, Speaker A: Go ahead.
00:57:00.650 - 00:57:17.722, Speaker C: Choice of programming language. I mean, a lot of code now is being written in rust. There's still a lot of code in C plus Plus. C languages without strong type systems. Yeah, it kind of makes me nervous how much?
00:57:17.856 - 00:57:28.830, Speaker A: Yeah. Actually, on that point, I have a question. I think it's audited systems that are both Rust and C Plus plus. Is there any other languages represented, JP, that systems you've audited?
00:57:29.330 - 00:57:55.926, Speaker B: Well, I've seen some projects using go or I think even JavaScript or typescript, but most of them are in rust. Now you have zebra from zcash, like all the arcworks is in rust. I think Rust is very good language for this type of project. Got it. Yeah.
00:57:55.948 - 00:58:10.410, Speaker A: I was just curious. You said basically you answered the question at the end there. I was curious if you had kind of a preference or you thought that certain languages were more potentially practical for this problem. And sounds like, as most people have already concluded, rust is certainly one of the better ones, if not the best.
00:58:10.560 - 00:58:15.270, Speaker C: Yeah, I mean, there is Ocaml being used by o one labs.
00:58:15.430 - 00:58:23.090, Speaker A: Yes. We don't have Isaac here, unfortunately, to tell us the benefits of Ocaml, which I'm sure many. I'm not very educated.
00:58:23.430 - 00:58:36.920, Speaker C: I really like Ocaml. If you wanted a garbage collected, safe language, you could do a lot worse than Ocaml. Obviously, a little bit more difficult to find people who are familiar with.
00:58:38.970 - 00:59:36.402, Speaker A: This brings. Goes back to your point there around the expertise and JP, you made this point like the security by obscurity, just simply because it's too obscure for all but some tiny sliver of the population to ever understand what's actually happening under the hood. And yeah, it feels like. Do you view this as something that maybe this even extends beyond zero knowledge? But just cryptography in general, do you think these systems get more complex as there's more moving parts, as protocols kind of extend up the stack? How do you address that problem where there's very few people? And you even mentioned this when you said, hey, the best position people to spot bugs are the teams themselves. But in many cases, even the teams that are working on separate pieces of it, and there's sometimes no one actually seeing all of the parts as they're fitting together. So how do you think you address that?
00:59:36.556 - 01:00:48.670, Speaker B: Well, what scares me is what I call state machine bugs, because ultimately you have a stateful system and entering after a sequence of events in a state that is insecure. But first of all, you have to know what is in a secure state and how this insecure state will interact with your permix system and what role your programming system will play. And if I take the case of Alio, it's an example where I was really trying to reason about, okay, what type of viable combination, what type of input would be wrong to have here? And how can we make sure that this would be rejected, detected? It was very unclear to me because it's like when you analyze consensus protocol on paper, it's ready to be easy to understand what happens. And then you simulate it, you're like, oh, shit, it's crashing. What happened? And there are many types of bug that you can only observe by running it in practice because it's too nonlinear to anticipate what's going to happen, there are way too many different possibilities.
01:00:50.690 - 01:01:17.110, Speaker C: The bug that I just mentioned about and the orchard circuit, the way we actually found which part of the circuit it was, well, it wasn't a circuit, but which part of the instance was wrong. It's just commenting out bits of the circuit and seeing whether it passed or not. Some kind of automated way to do things like that. Really useful.
01:01:18.110 - 01:01:57.480, Speaker B: What also might happen is what you've seen with crypto, because suddenly when crypto becomes easier to use, you have better interfaces, better libraries, then everybody starts using it, even people who don't know what they're doing. So perhaps the same might happen with EZK proving systems when they become much easier to use. Where you have all these DSS, everybody can be a cryptographer, everybody can do zk systems. And I've seen some projects already that look a bit sketchy to me. They claim to use zk technology, but you don't know what they're doing, and you don't know if they know what they're doing.
01:02:00.570 - 01:02:30.990, Speaker C: It's inevitable that more people will learn the technology, and therefore there'll be a larger pool of potential attackers. So it's kind of a race against time to build this tooling, these languages that allow us to avoid the bugs before that happens. It's definitely not a bad thing that more people are learning how to do zk stuff, even though it does increase the pull of attackers.
01:02:32.450 - 01:02:55.830, Speaker A: Yeah. Well, maybe now is a good time to highlight some of the great work that Anna and Kobe have been doing on the ZK hack side. Trying to onboard new people into this world and getting them interested in. Yeah, hopefully some of those people will go on to start cool projects, and hopefully some of them will build tools that help make things more secure. And maybe some will even become auditors. Go work for you, JP.
01:02:59.150 - 01:02:59.900, Speaker B: Cool.
01:03:01.950 - 01:03:21.018, Speaker A: Anyone else? I guess we're down to just a few folks here. Anyone else have any final questions for JP? All right, awesome. Well, thank you very much, JP. I really appreciate, we all really appreciate you coming on.
01:03:21.104 - 01:03:22.354, Speaker B: Zeke Study club today.
01:03:22.392 - 01:03:26.130, Speaker A: I think everyone learned a lot and I really enjoyed the conversation. So thanks again for being here.
01:03:26.280 - 01:03:27.280, Speaker B: Thank you, Alex. Thank you, everyone.
