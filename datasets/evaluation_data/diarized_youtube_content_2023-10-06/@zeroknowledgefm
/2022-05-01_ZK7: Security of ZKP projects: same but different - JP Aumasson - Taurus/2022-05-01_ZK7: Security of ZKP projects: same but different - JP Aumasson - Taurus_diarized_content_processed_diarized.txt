00:00:06.890 - 00:00:16.734, Speaker A: So I want to introduce JP Omosson. Yes. Who's going to be talking about the security of ZKP projects. Same, but different. Please take it away.
00:00:16.772 - 00:00:50.138, Speaker B: Thank you. Anna. Hello. Hello, Samit. It's my first summit, so I'm a bit intimidated, but I kind of like the event. It's quite different from the usual pure security hacking events. So like Anna said, that's the official talk, the official title of my talk, the unofficial title is should you pay for security audits? I don't mean to offend the people who do security audits, I'm doing audits myself, so that's pretty much why I'm here today, and it's based on my experience.
00:00:50.138 - 00:01:35.658, Speaker B: So doing security audits for zk projects in the last few years, and before that, I've been doing a lot of security audits, a lot of crypto audits, and I can tell you that's very different. And I even found approaching ZK projects more difficult and more confusing than complex systems such as MPC, TSS, threshold signing. Even though with threshold signing you have quite complex notions. You have the interactivity to handle, but getting rid of the interactivity somehow makes things more complicated. So I was confused, I was frustrated. I was frustrated not to find more bugs than I expected. I was like, oh shit, I suck, I do a shit job.
00:01:35.658 - 00:02:18.502, Speaker B: And I ended up developing a kind of new methodology and trying to understand why I was so frustrated. So that's the talk. So generally, why do we need to study zk security? So it's not just about security on paper, security proof, sound as completenet ZKs, it's about real world security, so to speak. So of course for the projects using ZK snarks, well, if you don't have zk security, then, well, that's kind of pointless. And there's a lot of value, a lot of money at stake and a lot of reputation at stake. So it's important for you, but it's not important for me. What is cool for me as a cryptographer is that I've been doing crypto for 15 years.
00:02:18.502 - 00:02:57.670, Speaker B: And when I told you some people, they're like crypto before bitcoin, I was like, yes, cryptography. And I have seen a whole lot of different applications of crypto in my life. Open stuff, non open stuff. And I find what we're talking about today by far the most interesting because it's, let's say, intellectually challenging. It's nontrivial, there are many moving parts, you have subcomponents as complex as polynomial commitments. And all this stuff is subject to papers, relatively well written to code, and is deployed at scale. And if it fails it's a disaster.
00:02:57.670 - 00:03:38.290, Speaker B: But rarely seen this in crypto. And that's also a know annoying as an auditor because I think like David was commenting before, what you implement is rarely exactly the same logic as in the papers. That's why I love his project cargo specs. So what does it mean? ZK is now security in practice. So it depends a lot on the application, it depends on what are your assets beat, your transaction data, your user privacy. It depends on how you're using your pro system, it depends on your private inputs, your public inputs. But I tried to find some common denominators.
00:03:38.290 - 00:04:35.878, Speaker B: So I find that soundness, that might be obvious to everyone, but it was not to me initially. Soundness is oftentimes the highest risk in practice, because if you accept invalid proofs then well again you have a little problem and it's the most obvious attack vector. So it's not just about complete forgery, it's about malleability of proofs and it's about replay of proofs. So hashtag nonce for example, then we have zero knowledge, the ZK. But if you have a constant size proof, you have only so much space to leak data. And I've often found some big difference between a failure to be zk on paper and an actual exploitation of ZK in practice. So I've seen situations where people were telling me oh yeah, we need these blinding coefficients, because otherwise the proof doesn't work.
00:04:35.878 - 00:05:28.790, Speaker B: I'm like okay, but without it, how can you exploit it? Like not sure, but as a security guy I find this a bit weird that I've seen many cryptographic designs that were overdesigned by the designers because they wanted to make their life easier when writing the security proof. And I have the feeling that it's a bit the same situation with zk proof sometimes. So completeness, it seems to be the easiest to achieve and the least annoying if you fail. But I'm pretty sure there are situations where if you fail at completeness, then you are in quite big trouble. So who's finding the bugs? Different types of people try to put the main categories here. So oftentimes it's the people who know the code the best. So the people who write the code, people who review the code in the organization, people who run the test and so on.
00:05:28.790 - 00:06:16.598, Speaker B: Sometimes it's also people from other projects, concurrent projects, who might reuse the code, frog the code, or just look the code for inspiration. And sometimes it's also external editors who get hired and paid by the projects. And you have DNE, so it may be the users. That's when it's too late. You deploy something in production and accidentally or maliciously, people find and may exploit some bugs and vulnerabilities. So your goal, if you're in a project like, I don't know, I don't want to give the names, but all the sponsors that you can see here, your goal is to make sure that the ABC guys will find the bugs before DNE. And what's the biggest challenge when you're a security auditor? So you have limited experience because it's relatively new.
00:06:16.598 - 00:06:49.326, Speaker B: Nobody has ten years of experience in plunk. You have limited knowledge. If you're generalist doing many security audits, you have most likely not spend as much time studying ZK proofs as people who wrote the code. And you don't have a body of knowledge, a corpus of bugs that you can refer to as a checklist. There's very few documentation about this. And you have limited tooling, limited methodologies, limited write ups online. So you have to use your brain, which is hard from some people.
00:06:49.326 - 00:07:29.638, Speaker B: And the documentation. Oftentimes I would ask the clients, okay, what's your documentation? We have some documentation, yeah, cool. But it's completely outdated. And there's very little documentation that is purely about security, purely about threat modeling, about attack vectors. It's documentation about maybe how to use the code, what the code is doing, but not in terms of security requirements and security goals. Maybe because it's obvious for these guys, I don't want to blame them. So what I found to work and what I've tried to do differently for this type of project is to do more interactive collaboration with the developers.
00:07:29.638 - 00:08:17.498, Speaker B: So have more discussions, organize occasional calls, house them with questions on slack. Why do you do it this way? What does that mean? I don't understand this part. Please explain it to me, this kind of thing. And also try to do the threat model myself. Try to understand what was the main value, what were the main assets in the application that I was looking at, what are the attack vectors, what are the capabilities of an attacker, what you do in any security engineering project normally, and your experience. So I say you don't learn by reading, you learn by doing. So try to do stuff myself, write some code, write my own circuits, try to play with, for example, the Carol language, the Leo language, the Zkhack challenges, this kind of stuff.
00:08:17.498 - 00:09:05.754, Speaker B: And I did a whole work of, let's say archaeology, trying to find the public information about the bugs. So it's not just about looking for write ups, but try to dig into the GitHub issues, the GitHub PRs, and see what patch qualifies as fixing a security bug. So to give you a general idea, if you break down, if you do the usual breakdown in terms of computation down to integration in the application, there are 10,000 ways to fail. If your computation if your program is not secure in the first place, then no matter how good is your proof, it's not going to turn your insecure program into a secure one. If you use an insecure cipher, it's not going to become a secure cipher. If your circuit is not equivalent to the program, then again you have a problem. It doesn't depend on the proof itself.
00:09:05.754 - 00:10:01.198, Speaker B: And if your constraint system is built in a way that it will fail to enforce certain constraints, certain types of constraint, even if your program is safe, then again it might break sound s completely. And in your proof system, where you have multiple moving parts, you want to make sure that you have a good choice of primitives, parameters, and a kind of consistency across the components. So if you are shooting for 256 bit security, you want to make sure that you have 256 bit security across the board. Well, most of the time you have 228 bit security anyway. And then even if all this stuff is secure and safe, then how do you use it in your application? How do you generate the keys? How do you pick the nonsense, this kind of stuff? So how to break zkesnocks so you can break Sars? Obviously that's quite obvious to you. You can break zero knowledge. I've seen cases, for example, if you treat data that is private, as public in some way, then it might leak information.
00:10:01.198 - 00:11:02.910, Speaker B: Of course, at the application level, as I will discuss later, you can imagine scenarios whereby usage patterns might leak data, might leak information on the data that is predicted, even though it's zikian on paper, and completeness. So there are also quite a few ways to fail at completeness. But I'd say more precisely, it's not just about these three notions, because ultimately you write software, and software has bugs, even if it's in rust, even if it's in OCML. And you have different ways to write code, you have, like Moxie says, you can optimize for writability, you can optimize for readability. As an auditor, I prefer the code that is optimized for readability that is commented, and that use variables with names that make sense so you can have software dependent bugs. You can have things like buffer overflows, even in rust. You can have side chunnel timing attacks, which may or may not be exploitable.
00:11:02.910 - 00:11:39.990, Speaker B: And you can enter in some insecure state that can just jeopardize the security because you might be locked into an insecure state. And then all this supply chain, what we call supply chain security these days. So, in short, dependencies platform, your CI pipeline, your review pipeline, so all the boring stuff, which is super important, and last but not least, the on chain situation. So I heard that some smart contracts were found to have bugs last week. Yeah, this happens, too. So I put a pyramid. I have a friend who loves pyramids.
00:11:39.990 - 00:12:25.290, Speaker B: So I put a pyramid. I hope you like it. And I try to understand, let's say I try to break down the proof system in different parts in terms of what security notion could be impacted. And when I audit something, I also try to identify what are the attack vectors, what can be completely manipulated by an attacker, what cannot or what can partially be influenced by an attacker. Most of the time, most of these ZK failures could be at the apiction level. It doesn't mean it cannot happen underneath, but it's, in my experience, mostly at the apiction level. And whereas soundness and completeness are more about the proverb itself and what is underneath.
00:12:25.290 - 00:13:17.750, Speaker B: So all the way from the platform security, from the OS you're using to the arithmetic, the elliptical arithmetic, the field arithmetic, and the actual prover that combines the primitives that you're using. So, speaking of primitives, it's an example of all the components you might have in something like maybe plonk or Marlin. So, hash functions, algebraic hash functions, PRFs, max commitments. It's crazy to me. When I looked at this, I'm like, man, the goal of these guys was to put all the possible crypto primitives that exist and do something of this. You could argue that people like stark that do pretty much everything with only hash functions, but it doesn't mean that it's simple or easy to review. So you're quite different components, quite different security notions.
00:13:17.750 - 00:14:25.342, Speaker B: And if, for example, you have some polynomial commitment being used, that's supposed to be binding or hiding, and you see that some place it's not completely hiding or binding, you're like, oh, that's a problem. And then you think about, okay, how can I exploit it as an attacker? And so, maybe I know too little about Zika Pros, but oftentimes it was not trivial to me how to exploit a problem in one subcomponent but this should still be reported as a problem to be fixed. Another problem that is common in all software development is the contract between different component, between functions and their callers and their colleagues. Well, maybe the poster shyle of this is where do you verify that the scalar you receive lies in the right range? There are multiple ways to do it. I've seen many recent projects doing it in very interesting ways. For example, you can enforce that this is under well below the order of the group, that it is nonzero. But I've also seen some horror stories along these lines.
00:14:25.342 - 00:15:11.230, Speaker B: So as a developer, I think you need to make it clear which component is responsible of checking the sanity of whatever value where you receive, especially attacker control value. And it doesn't hurt to do the same check twice or thrice. Okay, I rushed a bit because I only have so much time and I want to go through this collection of bugs, because some people collect NFTs and I collect bugs in decay proof systems and I don't make NFTs of this. Maybe I should. So this is the favorite class of my friend Kobe, and he helped me find some of these. So this is, maybe you're familiar with this kind of bug. You receive a scalar, so in that case a nullifier.
00:15:11.230 - 00:15:48.726, Speaker B: And this should be unique per payment, per shielded payment. But if it's bigger than the order of the group, then you can have different numbers that reduce modulo down to the same value. So something non unique becomes unique. And when you check that it's non unique, you say oh, it's fine, it's not unique, but then it becomes the same. So it results in double spend. Another case, in another project about the same, about the same situation. So found by Kobe, another one, this one not a nullifier.
00:15:48.726 - 00:16:32.438, Speaker B: So again, in a different project, I think it was a public input. Another one, another public input. I think you understand the point. You understood the point. This one is in an R one cs constraint system. It was in the rust, in the Arqus, the rust arcworks r one CS software, and it was failing to enforce that one element was the inverse of another. And this could have directly been used in matter sending to forge proofs, because you just took the numbers and one is not inverse, where's it supposed to be? And then it's validated by this bug.
00:16:32.438 - 00:17:12.150, Speaker B: I don't mean to say, oh, look at these idiots, they wrote this bug. No, my point is just to say, look, the kind of bugs you can have, and if all these smart people write the code with these bugs, it tells you a lot about the complexity of these systems. So this one was a funny one. It was not really in the hash function, it was in the verification of the hash. So instead of using the equal operator, it was using this, which is not equal. So it was not checking equality. So just two characters and complete disaster.
00:17:12.150 - 00:17:48.146, Speaker B: Yeah, it was in tornado cash, I guess. So this is the famous bug that Ariel Gabizon found in Zcash a few years ago. And interestingly so, back then, Ariel was working for Zcash. So again, it also suggests that people who know the Beza code that are the most qualified to find these very subtle, very complex bugs. And it was actually a problem in the paper itself. So they implemented what was in the paper, but the paper was not completely safe. So this happens as well.
00:17:48.146 - 00:18:24.518, Speaker B: Another case, a recent one. So there was a very nice series of write ups from trial of bits, where they found several occurrences where the fiat Shamir hashing of the transcript of the proof was not including all the public variables. I think it applied to plonk, to the azure proof system and bulletproofs. So some implementations of this, not the system themselves, of course. And in one case for bulletproofs, it was also a shortcoming of the paper. The paper was not describing all the stuff you had to Fiat Shamir to be secure. So, long story short, in your Fiat Shamir.
00:18:24.518 - 00:18:53.170, Speaker B: Harsh. You should put as much as you can, it doesn't hurt much. I don't think it hurts to put too much. This one was a documentation writer by Aztec, I think in the context of their internal audits. In that case, it was a mishandling of the nonsense. They were not using the right type of nonsense in their payment, in the encrypted notes specifically, and privacy was impacted. So it was a bug of zero knowledge.
00:18:53.170 - 00:19:43.634, Speaker B: That, from my perspective, can be seen as a bug in the application, because it's not in the proof system, it's how you choose the inputs that you give to the proof system. This very nice one, very nice work by Ramero, Bonet and Patterson. So maybe you're familiar with this. A few years ago, these were timing attacks. So side channel attacks on Zcash and Monero. So that, again, did not contradict the security claims of the paper, but that exploited some, let's say, real world artifact, piece of information, correlations to leak data on some secret values. And they were also using timing leaks to get oracles.
00:19:43.634 - 00:20:10.218, Speaker B: So an oracle is just something that you interact with, and that tells you something that you don't know. It might be a secret value or it might be whether some stuff is valid or not, or if some decrypted message has some specific pattern. And I think they have three or four different types of attacks. Very interesting. It's a nice paper, I recommend that you read it. This one in plunk up again. So yeah, that's maybe one case where I was a bit confused.
00:20:10.218 - 00:20:47.286, Speaker B: I mean, the people who know plunk much better than me, they probably have the answer, but blinding factors were missing the randomizers and I was like, okay, so you don't get full Zkns here. And I think like Adrian explained to me that it's not very clear, but it's still not clear to me. So this one case, I was talking about kero with someone before, I know very about Cairo. So one day I just opened the code. I'm like, okay, this is open source. Oh, it's Python. And the only stuff I know is crypto.
00:20:47.286 - 00:21:12.306, Speaker B: So I look at the crypto, I look at the signature, and I found what I think is a bug in the signature system. But maybe I'm just missing something. So I don't know if there's someone from kero in the room or someone who knows kero. Well, I would be happy to discuss this. I created the GitHub issue and I don't think anyone responded at this point. So yeah, just to conclude, I know. What's the time? Situation looks okay.
00:21:12.306 - 00:21:54.586, Speaker B: 1 minute. Perfect. Okay. So surprisingly, I've been frustrated not only not to find many bugs myself, but not to see more bugs being found. Maybe just because it's written in rust, or maybe because people are so smart, that would be great news as well. So yeah, maybe it's because people have been developing robust frameworks from the beginning, projects like what Zikash is doing, like Bellman, like the arcworks suite of projects. There's also many efforts to make it easier to write safecode, for example, to domain specific languages, Carol Leone, wire, zinc and others, and gadgets or chips that other projects can reuse.
00:21:54.586 - 00:22:52.430, Speaker B: So if something has been, let's say, battle tested, so to speak, it might make sense to reuse it in your own project. And also in practice, when you look at how the proof systems are used, there's very little attack surface for an attacker. It's not like an Apache server that will receive whatever you send it. In practice, you cannot control much information as an attacker, which is a good thing. So why being scared? Well, scared is maybe a big word, but I tend to believe that we are still in a relatively well immature state. I mean we haven't had this proc system for years and I wouldn't be surprised if one day we discover a big class of bugs that impacts all the plunk and plunk variants across the board and it's a complete disaster. I hope it doesn't happen, but I would not be surprised because we have very shortage of tooling.
00:22:52.430 - 00:23:50.994, Speaker B: And I mean no offense to the project, but the way people think about security, what you learn at security school is that the first thing you should do when you start a project is to set out the security requirements, what you want to achieve in terms of security, rather than do it exposed, so to speak, rather than doing security after the fact. I don't think we will ever learn this, but that's fine, that's how it is. Also, many more people are coming to the ZK summit, many more projects are being created, many more, let's say assets, depend on zk proof security. So there will just be economic incentives for attackers to invest in breaking these systems and maybe to withhold the bugs they found. So to conclude, okay, I'm concluding I think we need more, because you need to conclude with open questions and messages for the next generations. So we need more testing, more fuzzing. I've seen very little fuzzing.
00:23:50.994 - 00:24:37.880, Speaker B: I know that Guido Franken did some, I believe, but we needed more smart fuzzing. Fuzzing specific to the API. Again, I mentioned the project from David and also we need more write ups like Trail of bits did, which I think is very useful for many people. And last but maybe not least, maybe we can use insights from the people from the field of hardware engineering where you have a high level language, so a very log of VHDL and you transform it into a netlist which is essentially a circuit. And they have a whole body of literature about testing bugs tooling associated to this. So I wonder if we can reuse some of their IDs experience and tools to improve the state of zigaprofs. Thank you very much.
00:24:44.650 - 00:24:52.460, Speaker A: So I think we could do one question. We are kind of catching up right now, but yes, let's do one question.
00:24:56.990 - 00:25:56.938, Speaker C: Hi, thank you Jean Philip, for great talk. Actually I just want to add a bit of information to your slides where you describe this soundness and completeness, things that could be missing. I think one important also aspect, kind of part of the security that are interesting for us in zero knowledge aspect is availability. The thing is, zero knowledge proofs are often used to transit a state from one, well, from one state to another. State and we provide a proof that the transition is correct. But the thing is, in order to produce further proofs, we must know what the state is, and the data that needed to reconstruct the state must be available. And some bugs that we did find in real applications is that some data is not really provided.
00:25:56.938 - 00:26:09.274, Speaker C: So the proofs are correct. Everything is complete, everything is sound, but the data is incomplete. So that in the worst case it might not be possible to reconstruct the state in the variant.
00:26:09.322 - 00:26:13.422, Speaker B: Yeah, it makes a lot of sense there. Good point, Dimitri. Thank you. Cool.
00:26:13.476 - 00:26:14.700, Speaker A: Well, thank you so much for the talk.
