00:00:08.730 - 00:00:44.634, Speaker A: You said that in the schedule. It was a little bit also about the information of the metal labs itself. I will keep this part short. Luckily for me, Aviu from Starkware already explained a lot about all the solutions which are possible to make for level two networks, for transaction, for batching and some other forums. I will just say what we do exactly. Right now we're working on a level two solution, or just side chain how people call it, which is using ZK snarks for transaction aggregation and scaling of the public chains. And public is important here.
00:00:44.634 - 00:01:23.510, Speaker A: We need existing primitives, so we use what we have in Ethereum right now. Our goal is universal solution in some sense, and I will speak a little bit about it later. Initially, we finalized our set of features for a first version about two weeks ago. Would be transfers would be deck settlements in a form of atomic dependencies with advisory chains and state channel opening, funding and closing without escalation. So for best case scenario, we want to move in the field of privacy to hide at least the transaction amounts. But for now we optimize for efficiency. So this is not in the first version.
00:01:23.510 - 00:02:04.126, Speaker A: Also, decentralization is important for such a level two network. It will be required for someone to aggregate the transactions, and we want the party who would have to aggregate these transactions, which can be called in different settings, like in plasma. Those entities are called operators. We call them validators and doesn't matter, they just make the proofs. We want those parties to be more than one or more than one finite set of entities which are known upfront. So we want to have wider participation for this network. But in principle we work in both practical part and just some fundamental part to have further progress and direction for advancements.
00:02:04.126 - 00:03:02.210, Speaker A: And my personal goal, or more or less what I want to do is to have some form of arbitrary kind of as a smart contracts in a level two layer. But for this we need recursion, and I will not stop on this today. So universality is important and that's why actually presenting it here today, first time we call for initiative for more or less universal ZK snarks for now. Standards for all the public chains in a sense of tooling for users to design circuits, backend for proving so those can be interoperable, like reference verification contracts, reference batch verification contracts, and also spark maybe some reference functionality for some kind of networks. And if you want to know something, please use this email. There is no website, so just email and we will answer your question directly. So why universal universality is important? Because we work in a public chains.
00:03:02.210 - 00:04:05.706, Speaker A: And even while you can do compression of transactions infinitely, so you can just prove a lot of transactions, one state transition from another state route to another. You need still to care about the worst case scenarios when someone needs to reconstruct the whole state from some part and to be able to have the procedure of movement funds back to the main chain when there is no cooperation from the operator. This can be more or less eliminated if there is some form of decentralization, but it's not final. So for this part, I will just give actually the concrete numbers. Every transaction which is aggregated by us using snarks starks will consist of two parts. One is how much you pay per verification, how much you pay for public data, which is required for now to solve the data availability problem. For our case, if you go full scale, fully filled blocks, payment will take 150 gas per transaction, just per verification exchange will take, well, actually twice more.
00:04:05.706 - 00:04:47.480, Speaker A: But let's put some margin for now, and now for data availability. Now, even without the privacy, which will require additional costs, it will cost 750 gas per public data to be able for every participant of the network to reconstruct the state for worst case scenarios, which we should always carry about in a public and decentralized networks when some party just refuses to cooperate. And yeah, it's already a lot. If you want to put 1000 transaction, you multiply it by thousands. Already quite a lot more and more and more. You take also network capacity, and if you use 6 million proof for starts, you take more network capacity. And then we have another crypticitties or whatever.
00:04:47.480 - 00:05:35.000, Speaker A: And actually if you just go into the there is research on this, how much data is transferred between Ethereum nodes, how much more you can push, and actually just much more difficult than just say let's just reduce it ten times. Fortunately, it also depends on the network level. And if you want to have the decentralized network and keep it assuming decentralized, you cannot estimate that every node which actively participates has a gigabit of network capacity. So we cannot estimate this. So just saying that let's put the price for data ten times less is not going to work. So that's why something universal is more or less ideal. So you can have a wide functionality for as many users of the network as possible, whether it's dexes, payment solutions, state channels, what l four is working on.
00:05:35.000 - 00:06:32.294, Speaker A: So you always pay per any proof of any transaction aggregation, you pay the fixed price no matter how many transactions you actually have the block and how many transaction, actually just dummy transaction, which is for padding so this price is fixed and you still put always the same price, whether it's padding transaction or not for public data. So if your blocks are underfilled, it's actually hurt the network in both levels. So if you don't have the network, we can operate at the maximum efficiency and simultaneously be properly filled. So that's why there is a need for some universality. You cannot just scale efficiency, there will be just cryptic. It is again, but best part is actually much more interesting, especially for me and I think for people who actually join us here today. So for sonics, I want to repeat, the sonics is another proof system as it has a common reference string.
00:06:32.294 - 00:06:55.214, Speaker A: Yes, it has a common reference string. It requires trusted setup. But this trusted setup is first of all universal. You run it once and you can use it for any circuit up to the given depths and it's updatable. If you don't trust the previous trusted setup, you contribute on top of it. And let's say you trust yourself. And please, I don't think any credit for any implementation.
00:06:55.214 - 00:07:47.230, Speaker A: I make. I make improvements, I make new primitives, but all credits should go to the original people who made this idea. So please, their names also, just to show how Sonics and standard snack like Gross 16 are different, let's say you want to play a game, a card game. For gross 16, you take a deck of cards, you shuffle it under the table, give it to another participant of the game, you shuffle it under the table and you get the deck on the table and you play the game with this deck. And for every game you need a new deck. So you need to go over all this procedure again and again. For Sonics, you shuffle the deck, you give it to another entity, and at the end of the day you get the shuffled deck on the table and then you magically clone it and use it for a game.
00:07:47.230 - 00:08:46.738, Speaker A: And if you want to play another game, another kind of game, for example, just another round, you clone it and you play it. And you only need to do a shuffling once, well up to games which require only 52 cards for example, to play. So both of these proof systems can potentially start from rank one constraint system. Also, Sonics use multiplication gates and linear constraints in their paper and in their description, but they can all start and this can be reduced. Main parameters for graph 16 is just number of constraints as many people familiar with for Sonics in a paper it describes that the most important parameter is the number of multiplication gates. Actually, number of linear constraints is also important. So here you have to care for more numbers and well implementations, since sonic implementation was not there yet, but the implementation of the gross 16 in terms of prover and verifier and batch inventory verifier is there.
00:08:46.738 - 00:09:41.894, Speaker A: So we can hope for something like 200 gas per proof in a batched mode of five transaction, five proofs checked at once if there is one public input, for example. Now about the efficient implementation, everyone knows what that for row 16 or in principle, you want to run some FFT. So you want the scalar filled with a number, large number of fruits of unity. Your pro role will work in both small and large groups in g one and g two, usually called the limitation is available. A lot of well operations are highly parallelizable up to the very large degree, so can be done efficiently. You can scale it to more cpus if you want in your large beefy server. If you want to prove the large circuit over a small time with sonics, operations are parallelizable, at least a large share of them, maybe, except of one.
00:09:41.894 - 00:10:33.442, Speaker A: But it doesn't take a lot of time. As I tested, the program implementation requires only multi expansions in g one, which are usually like four or five times less expensive, which is potentially also good. The problem is that for f of t for some polynomial multiplication, which you need for sonics, well, the number of multiplication gates. I will show how it's related to the number of constraints, but potentially sonics limit your circuit size for a given scalar field, unless, well, because you need efficient polynomial multiplication for which you just need an efficient FFT. So just concrete example of the conversion. If you go to the MIMC circuit standard one, just open the Bellman repository, find one. If you convert it originally it has 644 constraints.
00:10:33.442 - 00:11:13.858, Speaker A: If you convert it, you get 968 multiplication gates. I didn't put linear constraints here, I actually forget about it. And roughly the conversion is trivial. You take one rank, one constraint, system constraint, you convert it into three linear constraint plus a multiplication gate, and also to allocate any variable, private or public, you need like one half of the multiplication gate. So this formula is more or less universal. So you see, it actually just grows with a small factor. But still then there was a part of the paper which was much more difficult to understand, which was about help procedure and help procedure, whether the proof is ascent or not.
00:11:13.858 - 00:11:50.602, Speaker A: So I will go into these details just one step after another, the same way it's more or less structured in the paper for just understanding of the audience. So let's say there is no bunching, there is no requirement for sustained proof. There's just one proof which someone needs to prove. So proverb does three polynomial commitments plus a signature of correct computation. Virtually the signature of correct computation. Its verification in this name form requires the number of steps which is equal to the number of multiplication gates. So the proof, as every fire in the setting is not suspend.
00:11:50.602 - 00:12:39.182, Speaker A: So we need to go further. What was called the help procedure. You can make some entity an untrusted helper, helper so can be anyone. This entity can aggregate m proofs and m signatures of credit computation and produce one signature of credit computation. So basically you reduce the verification work from m by n to just n, still not suspend. The suscent procedure called unhelped. Yes, you can make the signature of correct computation suscent also for some tradeoffs such as increase the number of multiplication gates up to the three times, we will go into it later.
00:12:39.182 - 00:13:38.862, Speaker A: And yes, your verifier is now suspend what you can do and you can mix and match, you can take both. So this most likely is going to be an optimal procedure. So you make NM independent proofs over one circuit, you aggregate the signature of credit computations, and then you transform the signature of credit computation into a synth form, and you get the verifier whose work is actually, well, still linear in the number of proofs which you need to verify. But now the signature of credit computation is at least a synth. So this one is more or less like a batch verification of the snarks, the standard gross 16. Now benchmarks which were not in Paris, because some people could have seen this presentation kind of in Paris. This is a concrete example, a test circuit with UShA 256 rounds, which just leads to this number of multiplication gates and linear constraints.
00:13:38.862 - 00:14:45.430, Speaker A: I mean, one rank, one constraint, system constraint will transform in one multiplication gate plus three linear constraints, which is quite obvious in my machine, just a laptop, modern MacBook, twelve virtual cores, six physical ones. But virtual cores actually do help. When you run the procedure for graph 16, it's 0.6 seconds per proof. So it's for five proofs, let's say 3 seconds with sonics, single proof with just production of the proof itself, plus what's called the advice, a commitment to the signature of competition, as unfortunately 9 seconds. The part which is called the helper, the untrusted part, which anyone can run as something like 12 seconds to aggregate five signatures, correct computations. And this work is linear in number of computations you want to aggregate, so it's not amortized.
00:14:45.430 - 00:15:28.226, Speaker A: So the reduction, just plain implementation of the reduction procedure. Even also, we made a good progress on how to actually implement it. It's not in the code yet. So let's say the worst assumption from the paper, which will increase the number of multiplication gates three times. So we just scale all the results previously by three, which is a good assumption actually. And so we go, we go to the synthetic part of the benchmark. Part of this is synthetic, so only for proof calculation part for succinct signature of correct computation, which requires permutation arguments, actually three permutation arguments, grant product arguments, and well, formatness arguments.
00:15:28.226 - 00:16:26.040, Speaker A: Everything is from the paper, if you want to go there. Just the same names for those. Not the most efficient domination yet, because it just has a procedure which can aggregate internal parts, it's around 60 seconds, which, let's say 30% can be shaved easily just by aggregation. So in total of 226 seconds, it's almost 80 times more. Quite unpleasant for now, but now let's focus. But we optimize not for time, but for a price per proof, and for optimization for a price per proof, because you can always scale all these proven techniques with well up to of course some given degree, but quite far, I would say you can just spend more computing power on it, and it just basically will be more per one proof if you want to fit it at the same time. For this, well, we work on the parts, which should help us with this.
00:16:26.040 - 00:17:37.406, Speaker A: Before it, there was a whiteboard session about the implementation of GPU progress with some benchmarks and very early steps on. So my estimate from what already we have already, without any professional GPU design experience, we could already achieve up to twice the performance in multi exponentiations and multi exponentiations is a procedure which is present in both gross 16 and Sonics proof systems. So it takes in Sonics 70% to 80% of the proverb time. Those can be efficiently sped up by GPU and speed up here, either you spend less time or you need less resources in terms of price to fit into the same time window. So here, everything we just call the speed is actually optimization for a price. So if you say, is it very cheap? What we have is, well, 35 seconds, still quite a large multiplier. But first of all, one should remember why we want to have sonics in the first place.
00:17:37.406 - 00:18:36.862, Speaker A: This is a part where you don't need to have a trusted setup per any user application. As I mentioned, I want to have some kind of smart contract logic in layer two chains. For this, some crypto primitives are needed which are not yet available in Ethereum, and I work well we work for Ethereum as a level two chain and that's why I say that there are limitations. If the primitives are available, if recursion cycles are available, why the set of curves is available? Well, we can briefly the sonic proof inside of the circuit with the circuit itself being either sonic gross 16 or something else. So we can efficiently mix and match and just provide the functionality. Which would be interested for the end users either we use sonic, so the top level is gpu, so we just have users who prove some their private smart contracts uses the cpus. Just an implementation should be done, tested.
00:18:36.862 - 00:19:16.750, Speaker A: And what I actually think is over this year, maybe next year there will be another step further in common reference string Susan proof systems. So I expect the next implementation will be even more efficient. And we already see that, well, yes, 25 times more, but still manageable. And worst case, because we still live in well, we work for public blockchains, we will need to go for worst case scenarios. Even if the sonics are not that efficient, we have the plan just for what we do. Yeah, thank you. I try to leave more time for questions.
00:19:16.750 - 00:20:28.366, Speaker A: If someone has talk is quite involved, but I hope that it's a zero knowledge summit. People know what we will be talking about. So please anyone, it nothing one in the background. Can you detail a bit more what's the difference between the multiplication gate representation that you use and r one cs and how complicated it would be to translate? Well, first of all, the translation procedure is not that difficult. In r one cs, you basically have the multiplication of two vectors of variables plus addition. I mean, you trans multiplication of linear combinations element wise. These parts each like usually represented a multiplied by B minus c is equal to zero, where a, b and c is linear combinations.
00:20:28.366 - 00:21:25.090, Speaker A: You just make one linear constraint in sonics, which is for a, and you make a new variable which will represent all this, a combination. You take the same for b, you take the same for c, and now you have basically ab equal to c, which is your multiplication gate. So the transformation procedure from r one cs to what sonics use in the paper for multiplication gates and for linear constraints is trivial. Linear constraint is just, well, it's linear constraint. Multiplication gates is just three variables, a, b and c, which are a multiplied by b is equal to c. But now not in terms of linear combinations, but in terms of individual variables in your constraint system. So any other questions? Then let's thank our speaker again and we'll have a small announcement from Anna.
