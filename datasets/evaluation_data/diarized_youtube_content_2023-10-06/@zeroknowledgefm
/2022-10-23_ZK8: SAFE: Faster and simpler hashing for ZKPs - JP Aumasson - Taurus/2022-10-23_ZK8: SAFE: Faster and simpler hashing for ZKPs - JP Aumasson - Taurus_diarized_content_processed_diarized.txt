00:00:06.730 - 00:00:26.886, Speaker A: So, hello. Welcome to my talk. I know it's pretty sad day for hash functions, isn't it? No. The merge proof of stake. They're trying to kill hash functions. Do people hate hashing? It's bad for the planet, but they won't succeed. We'll keep on hashing stuff and hashing in plenty of ways.
00:00:26.886 - 00:00:47.222, Speaker A: And I have this topic of my talk, hashing and fast hashing. And the great news is that we have some time, I think. So, Ari, that's the last talk of the session, right? So the conference finishes at 630. It's three. So 3 hours of hashing. So hashing is everywhere. Not just hashing one message, but doing a lot of stuff with hashing.
00:00:47.222 - 00:01:16.110, Speaker A: And in ZKP, in zk proofs in particular. So, commitments. In every single talks today, you hear about commitments, polynomial commitments and so on. You hear about miracle trees. When you don't know what to do, you use a tree, the fiat shamir, of course, when you want to turn a protocol from interactive to non interactive, you hash stuff. And of course, when you want to generate, to make things unpredictable, like in fiat chamir. And just for simple hashing, of course, pseudo random functions.
00:01:16.110 - 00:02:02.514, Speaker A: So the random permutations, to some extent, DrBGs, so deterministic random bit generators, exhaust extensible output functions. So there's so many hash functions, I don't know myself, all of them. And if you look at a snark, at a protocol like Alio, so you have hashing everywhere. If you look at stocks, you have even more hashing. So I don't need to convince you that what Ethereum people are trying to do, killing hashing, they won't succeed. So I've been designing hash functions for quite a while. So this is a joint talk, a joint walk with what Dimitri was here and the main instigator of this project, and more than ten years ago, in 2008, 2007.
00:02:02.514 - 00:02:44.546, Speaker A: So Dimitri and I were doing our PhD and working on hash functions, trying to break hash functions. And then we designed some hash functions with more or less success. And the game at the time was to make hashing faster and faster meant well. You implement in software, in C, you're an academic, you don't write very good code, but you try to make it as fast as possible. And you measure in terms of cycles per byte to make it independent of the frequency. But in the ZK summit context, we don't only care about the speed of vanilla hashing. So, vanilla means when it's run as it's specified, not in a circuit not in terms of constraints or error, or ONCs, you name it.
00:02:44.546 - 00:03:32.830, Speaker A: But in the context of ZKP, the performance matters even more than the speed in pure software, because people were spending days trying to make their hash a bit faster, hashing at 10gb/second instead of 8Gb/second and then we're faster, man. Look at the read and write speed of your drive, look at your bandwidth, look at your network speed. You don't care about being ten gigs or 20 gigs, it depends. But so in the context of zk proofs, an important metric is the number of constraints. So be they r one cs, like in Marlin or air, or other types like in Planck and all the Planck family. But in general, if you have many r one cs constraints, you will have many air constraints as well. So that's what is sometimes called algebraic complexity.
00:03:32.830 - 00:04:54.474, Speaker A: And of course, the goal is to minimize the cost, the proof generation. And if you use a simple general purpose hash function such as Blake two or shard two five six, it won't be ideal, let's say. So that's why people came up with the concept of zk friendly hash functions, so functions that would be well suited for the use case of zk proof systems, where most of the time we work with algebraic structures, so hashtag finite field, typically polynomials and so on. But most of the time you're going to work with objects that will lie in a finite field, and for example, something that is straightforward in a normal computer, like the XOR operation, the most stupid operation of all time, then it's quite painful to do on finite fields, because you have to think in terms of encoding, in terms of converting your finite field representation in terms of exor, and that leads to many constraints, and that's very annoying. But at the time when I wanted to implement Blake two in Leo, so the language of Aleo, I was stuck at the part where I had to do exor. I was like, damn, it doesn't work. So, yeah, even though I like Blake two, I don't hate Blake two.
00:04:54.474 - 00:05:24.050, Speaker A: Of course I designed it. It's really not what you want to use if you are going for an efficient circuit, an efficient pro generation. So people came up with a bunch of designs. Poseidon is the most famous one. So one of the co designers, Mitris Hereswell. And Poseidon is orders of magnitude faster and smaller than Blake two or shadow five six. So Poseidon was designed, I believe, in 2019, right? Whereas Blake two was designed in 2012.
00:05:24.050 - 00:06:00.666, Speaker A: It depends how you count it. And shy two five six was designed even earlier. Some people are making the argument that Poseidon and other algebraic hashes, they're a bit new, they're not as well studied as the other one. So we want to stick with shatter and black two. But I think Poseidon is safe, at least the parameters that are defined. So, of course, another way to make hashing faster is just to reduce number of competitions, reduce the number of rounds. From my perspective, many hash functions are also overdesigned.
00:06:00.666 - 00:06:43.194, Speaker A: But that's another question. So let's go back to Poseidon. You see, it's using Ali, onoma, dusk, falcon, penumbra, you name it, many projects. But people designed other hash functions, such as rescue, such as Griffin Neptune is a variant of Poseidon mimc. So sometimes to optimize, maybe for the field size, or to optimize for the constraint type. So if you see there's also sensimilia by the zcash people, and if you just look at the numbers here. So Blake two, shutter, five, six, not zk friendly hashes, big numbers here in terms of error product, in terms of r one, cs or plugup constraints.
00:06:43.194 - 00:07:15.462, Speaker A: And you see there's really an order of magnitude between these guys and zk friendly ones. Okay, but they're fast, natively in a vanilla mode. So zk friendly hash is good. It's presumably secure, it's presumably efficient, and it often works like this. So it's a sponge. A sponge function. Do you know what a sponge is? So it's a sponge function instead of hash function.
00:07:15.462 - 00:07:45.658, Speaker A: But it's a bit misleading, because ultimately a sponge function gives you a hash function. It's a way to construct it. So it's a very simple way. Instead of having to design something that takes something big and shrinks it to something small, you just design a permutation. So something that maps in a reversible way an input of some fixed size. So in kitstack, it's 1600 bits, and in post item, it depends. But it's easier to design as a cryptographer.
00:07:45.658 - 00:08:06.770, Speaker A: So you look at the block cipher, you look at the block cipher, it's a permutation. With the key, you remove the key, you get a permutation. So you take your block cipher, you remove the key, you get a hash function. And with this permutation, you can create a hash, but you can create other stuff. But the way it works is that you just exorb your input in. You permit you exor. You permit you exorb.
00:08:06.770 - 00:08:48.594, Speaker A: So, of course, you just exorb part of the state, not everything. Otherwise, it's completely broken. And there's another mode that is called duplex. So here you don't just send input and collect the output, you can combine the two. You inject, you absorb some input and then you read, and then you can write, write, and then maybe you want to read to get some output. So the typical use case for this is a PRng whereby you get entropy from your system, you introduce entropy, and the read option is when you get a cisco or whatever that needs some sort of random bytes. So maybe you can also think in terms of fiat chamber at this point.
00:08:48.594 - 00:09:14.766, Speaker A: Okay, I send my write my commitments and I read my challenge and this kind of thing. And you see the word pad. Pad, pad, pad. So pad means that you have to encode your input in a certain way. And pad means adding some additional data, for example, the encoding of the size of your input to get the security proof working and to get a secure design. But in practice, it's not so easy to do. I told you it's easy.
00:09:14.766 - 00:10:18.094, Speaker A: And then I told you that. I tell you it's not easy. What is this guy saying? So, in crypto, like many times, it depends. It's deceptively simple in the sense that once you have to implement it, there are many questions that come to you, like well, the encoding, the parameters, the interface, how do you define the API? You take five different implementations of Poseidon, you get six different APIs, six different ways to encode the input, to assign the arguments to the input, and also what we call the domain separation. So if you have objects of different type of variable size, you need some encoding to say, this is the start of my first object, now I'm going to the second object. And maybe there are different types of objects and you want to avoid trivial collisions, whereby if you have an object of two bytes and an object of one byte, then you can also see it as one object of one byte and another object of two bytes. But if the three bytes have the same value, then in these two situations you end up with the same hash.
00:10:18.094 - 00:10:52.102, Speaker A: So domain separation is quite important. And the padding. So in the real world, in the vanilla case, non ZK, we don't really care. But in the ZK space, every single optimization is useful because something that looks small in paper can translate in terms of hundreds of constraints. So there are simple ways to simplify it. Some people started specifying the hash function in terms of field element instead of bits. So there are some papers that talk in terms of algebraic capacity.
00:10:52.102 - 00:11:40.990, Speaker A: So you know the capacity concept in a sponge. So capacity is the size of the part where you don't write and don't read and just define it in terms of reading and writing field elements. And also, if you assume that the input has a fixed size, so you take a fiat shamu, you know in advance the number of elements that you're going to provide, you know their size, because the size of your field is not going to change well, normally. So there's a lot of things specific to the ZK use cases that we can leverage to simplify out the designs. Okay, so maybe you don't trust me, maybe it's fud, it's not that hard. There's no bugs in hash functions. So to prove that I'm right, I did the following experiment a few days ago, and I did my slides.
00:11:40.990 - 00:12:43.120, Speaker A: I picked a uniformly random Poseidon implementation on the GitHub website. Well, not uniformly random, because I picked on the first page or second page of the result, and I gave myself five minutes to find a challenging. It's like, oh shit. But the first implementation, I didn't find any bug. I looked for the usual know domain separation, and even sometimes people consuse matrix, vector and vector matrix product in Poseidon, but the first was fine. And in the second implementation, which I will not cite because it's from a friend, I found a bug. And what I can say is that they were trying to implement multiple instances of Poseidon, to spot multiple instances, and to estimate the security level depending on the parameters, the parameters being capacity, rate and width, site of the state and the competition was wrong, was wrong in a way whereby they overestimated the security.
00:12:43.120 - 00:13:24.286, Speaker A: If I remember correctly, they derived it from the width, not from the capacity. So you could get an instance that the program believed was maybe 128 bit secure, whereas it was not so well. The upshot is that even very smart, experienced people can get stuff wrong. If I write cryptocode, I will get stuff wrong all the time. So we need to minimize the risk, minimize the errors, and bring what the people need. Safe sponge API for field elements. It's not a new hash function, it's just a way to simplify, standardize the boring stuff.
00:13:24.286 - 00:13:59.850, Speaker A: So we're an implementer, the API, the encodings, all these things that you hate doing because it's not really cool, it's not really interesting, it's not really deep mathematically, but it has to be done. And we also get rid of the padding. Do you know that in terms of padding, there's shastri, there's ketsak might think they're the same. No, they're not the same because the padding is in Shastri. In Shastriketsack is not the same padding as non chasri ketsak. And I think that in Ethereum they use a non shastri getsac. So the old padding.
00:13:59.850 - 00:14:01.402, Speaker A: Yeah.
00:14:01.456 - 00:14:02.060, Speaker B: Okay.
00:14:02.750 - 00:14:04.570, Speaker A: Crypto is so confusing.
00:14:05.550 - 00:14:09.580, Speaker B: It launched like three days before NIST did the standard.
00:14:10.370 - 00:14:14.602, Speaker A: Really? Yeah. Blame nisT.
00:14:14.666 - 00:14:15.600, Speaker B: 30 hours.
00:14:17.970 - 00:14:44.434, Speaker A: So no padding, no problem. We get rid of the padding. It doesn't come completely for free. We have a workaround. We use what we call I O pattern, which is essentially a description of the course you're going to make. So you might say I want to absorb ten bytes and then I want to read two bytes and so on and so forth. And it will be beneficial to everyone if it's kind of defective standard, because it will make implementations interoperable.
00:14:44.434 - 00:15:35.762, Speaker A: It will give a common baseline to implement this kind of hash function, and also for people who are interested in hardware. So what Omar presented this morning, I think it will also be easier if we have a single way to do hashing so that you don't have to create a new chip for every single hash function use case. So then again, it's not anything very deep or new. It's not a completely new mode, it's just a twist over the duplex mode, and it's not a new permutation. So there are a lot of good works in the permission space. I think Dimitri will present a new one in 45 minutes. But the flip side is that you can use any permutation with safe.
00:15:35.762 - 00:16:00.282, Speaker A: So you like Poseidon, use it. You can use any finite field. So our goal is to be as generic as possible. So the API has to be quite simple. So there's essentially four calls start where you initialize the sponge. So, looks a bit complicated. I O pattern, domain separator.
00:16:00.282 - 00:16:46.298, Speaker A: But the I O pattern is just a description of the coast you're going to make again. So absorb, squeeze, absorb, absorb, squeeze with the size and domain separator is just fancy name to say personalization or another string that will make the function different. So if you have the same I O pattern and different domain separators, you will get different initial values, which means that the functions will behave completely differently. For any same input, you will get completely different output. So absorb and squeeze. It's write and read. As I mentioned, however, the difference is that you can provide arbitrarily many bits to each of the skulls.
00:16:46.298 - 00:17:50.530, Speaker A: So if the rate of your sponge so if what you can write between two permutations is, I don't know, just three bytes, you're not limited to three bytes here because it will distribute whatever you have to read or write across as many small absorbs or squeeze as possible. What I want to say is that if you look here, if I can absorb three bytes per call, I can do three. So you can call our absorb with nine, and it will take care of doing the two permutations. And likewise for the squeeze. Okay, and finish is the conclusion. So why do you need to call finish if you can just squeeze stuff out and call it a day? So finish will do maybe two things, so it will erase the memory. Want to use the mem zero or equivalent to safely erase the memory to make sure that nothing is left in your cache or whatnot.
00:17:50.530 - 00:18:36.846, Speaker A: And it will also verify that all the calls that have been specified by the I O pattern have been executed. So the goal is to prevent what cryptographers call misuse. Maybe you define your function in a way that you have to do another three Epsom calls and write and read at ten bytes. And the finish call will validate that all the calls that had to be made have been done with the right length parameter. So you can see it as a safety net in your protocol to ensure that you have used this correctly and that you computed the I O pattern correctly. And there's no padding at all, no reason to pad stuff. But there's a caveat.
00:18:36.846 - 00:19:13.710, Speaker A: So the state is stateful. Of course you have the permutation state. So it's the elements of your permutation. It might be four, six, eight more field elements, depending on the size, depending on the security level, the permutation, of course, so the function that will permit the elements and two counters, so just small numbers between zero and the rate. So they can be eight bit values, positive eight bit values. And then we also need a hash function. Why do you need a hash function for a hash function? It's just to bootstrap the design.
00:19:13.710 - 00:19:51.178, Speaker A: So when we compute the initial value, we take this I O pattern. So this encoding of the usage of the hash function, we encode it with a very straightforward serialization system, and we hash it down to 128 bits to write these 128 bits in the initial value. But you don't need it at runtime. You can just pre compute because you will know in advance what you're going to hash, what you're going to read and write. So this can be part of the design. You don't even need to have it as part of your code. You can pre compute the initial value and you won't need a hash function at all.
00:19:51.178 - 00:20:28.360, Speaker A: And likewise the tag, the initial value is derived from the use case, but you can also hard code it even in the code. That's what I mentioned. You derive it from the I O pattern. And just to give you an example, they're repeating what I mentioned before. So if you, there are three I O patterns. So the first one, you absorb three bytes, three elements, sorry, you see, I'm not in film mode, I'm still thinking in terms of byte. You absorb three elements and then you read one.
00:20:28.360 - 00:21:09.730, Speaker A: And in this case you absorb two elements and you read one. In the third case, you absorb two, then you absorb one again and you read one. So which ones are equivalent to each other? Sorry, one and three. Right. One and three, because here you're going to absorb two plus one equals three, and here you're going to absorb three in one call. But for the safe mode, it will be equivalent. So it will aggregate these two values and it will just see this as one single absorb with three elements, which means that you're going to have the same initial value, which means that you're going to have the same output at the end.
00:21:09.730 - 00:22:22.810, Speaker A: So you have this degree of flexibility in how you organize your calls. So you can see start as committing to your I O pattern, and then the finish as verifying the commitment, as opening the commitment. Okay, so you can see this thing we did as a middleware between the applications and the sponge function and the sponge. If you have, I don't know, Poseidon, ketsack or whatnot, it can, or it should only expose this very minimal low level API where you initialize the capacity with the initial value you permit, of course, and read and write elements. So you can read or you can add and you don't need anything else. Are you a problem? Absorb, somebody switch on and off again, kind signal. Okay, so let's move to concrete use cases.
00:22:22.810 - 00:23:01.590, Speaker A: We'll be very straightforward. So, merkel tree, the simplest merckle tree you can imagine two leaves and one root. So each leaf is one element. So you absorb the first one, you absorb the second one, and then you squeeze out one element. But before that you'll have verified, you'll have defined your initial value and then you verify that it is correct. In the paper, we provide some test values using the shastri two, five, six hash functions, if you want to verify it. So that's the case of a basic, non interactive to interactive protocol.
00:23:01.590 - 00:23:49.138, Speaker A: You have some elements of the proof typically commitments. The proof are commits to these two values. Then the verifier will send some challenge. So in terms of safe, in terms of sponge, this will be an absorbed, this will be a squeeze, this will be a read, then a write, then a read, and so on. So, some limitations of this very nice mode that we design. The first one is it doesn't support what is sometimes called variable input, variable length input. If you don't know in advance the size of your message, then you cannot define the I O pattern.
00:23:49.138 - 00:24:29.006, Speaker A: Therefore you don't have an IV, so you can hash. So it's a use case that sometimes happened, but we specify how to handle it. You will most likely need some padding, but you'll be better off if you know the size in advance. You will have less overhead. We even describe a way to use what we call infinite length Prng. If you don't know when or if you're going to stop generating output, if you have any stateful system where we read forever, that can also be done. It's a small walkaround.
00:24:29.006 - 00:24:56.294, Speaker A: And likewise for the Aeid. So, authenticated encryption with associated data. You have walkarounds to support input when you download size in advance. So yeah, I just showed Markletree, Fiat chamir. But of course you can do PRF, aka Mac encryption, authenticated encryption, simple hashing. Everything you can do with a permutation. Also the input domain separation.
00:24:56.294 - 00:25:23.282, Speaker A: That depends on your protocol. So let's say you hash stuff. You hash a field element, that's fine. Then you need to hash a string. So you have to encode your string into a field element, but then this becomes a field element. But you're hashing a string, so you have a collision between the case I hash a field element and the case I hash a string, which happens to have the same encoding. So you might want or need, it depends, to add some encoding to signal the type.
00:25:23.282 - 00:26:21.000, Speaker A: You can add an extra byte, you can add an extra element. So then it really depends on your protocol's constraints and specifically what is controlled by the attacker, what is potentially modifiable by the attacker, what is determined in advance, what is random. So there's no single right answer here. Also, you need a hash function, like I mentioned before, but it's not a problem. We have good hash functions, and maybe last point that people raise is, but where's the security proof? Dmitry and I are very convinced that it's secure. Shame on us if it's not. But more seriously, I don't like writing security proofs some people do it very well.
00:26:21.000 - 00:26:43.280, Speaker A: The change compared to the original design is very small, and it will be fairly easy to show that our mod with the I O pattern instead of the padding has exactly the same security bound as duplex and whatnot. And I think some people might be working on it, right?
00:26:43.810 - 00:26:44.560, Speaker B: Yeah.
00:26:46.370 - 00:27:20.906, Speaker A: Work in progress. Yeah. So proof in the full version of the paper, as we say. So, yeah. If you'd like to learn more, we have a page online, a hackmd page. The reference code is Filecoin's implementation in the Neptune project, which is a small twist over Poseidon. So if you're a hash function designer, you can focus on creating a permutation, which is by far the hardest part, and just reuse this mod for the rest.
00:27:20.906 - 00:28:08.540, Speaker A: If you're specifying a protocol, it will help a lot the implementers to specify to write it in terms of safe calls, absorb, squeeze, instead of having different terms, different notation, different language. Also, it will make the code reviewers very happy and implementers likewise. So if you have to code it in software, hardware might be convenient to use the same wording, the same API, this I O pattern, because it will also make your library interoperable with people who choose to use the same protocol. So we'd be happy if it becomes a de facto standard. We don't have a certification program or do this at your own risk. So thank you for your attention. And here for questions.
00:28:08.540 - 00:28:15.226, Speaker A: Yes, would it make sense to use.
00:28:15.248 - 00:28:16.950, Speaker B: Space in the context, maybe with NPC.
00:28:17.030 - 00:28:22.400, Speaker A: Where you're also handling messages for like NPC in the head.
00:28:23.010 - 00:28:30.480, Speaker B: Yeah, or even just like most writing communications, where you're kind of messages you're getting, would I just see Strobe or.
00:28:32.050 - 00:29:16.298, Speaker A: Yes, you can see Strobe as a generalization of a sponge. So it's a stateful object where you can write and read stuff. Here, our goal is limited to emulated some symmetric primitives, but if you are in a case where you MPC case where you want to commit suit stuff, you have this concept of challenge response. It can also work very well because it can save you from having to choose. Okay, what I'm going to hash, because I've seen many bugs where you had to hash a transcript and people were forgetting to put stuff in the transcript. So there are many bugs in TSS, in threshold singing protocol, where. Okay, that's what we hash.
00:29:16.298 - 00:29:36.770, Speaker A: That's what we hash. So if you have something that is stateful and that is hashing, is absorbing all your transcript in the first place, so you have the guarantee that you're not going to forget any value in your hash. That's actually a good use case. Use case. We could have mentioned threshold signing.
00:29:37.370 - 00:29:39.160, Speaker B: Do you imagine using this?
00:29:45.230 - 00:29:48.540, Speaker A: Sorry, I didn't understand. Yeah. Okay.
00:29:54.030 - 00:29:56.250, Speaker B: Is fiat engineer primary?
00:30:02.010 - 00:30:13.600, Speaker A: No, for any, any application. So fiat chamber just happens to be one relevant use case. But fiat chamber is not the only case where you need to read and write about it.
00:30:30.260 - 00:31:08.976, Speaker B: You don't have any overhead because all this I O pattern is just hard for the constant. So there will be constant in the circuit. You have to just, so like. Yeah, it will just, you just have to call aside and like in certain ways. But it's no leftover test, it's the same cost. You just have to call with proper inputs like initialize.
00:31:09.008 - 00:31:27.554, Speaker A: It's pre computation, so it's not runtime cost multiplication where.
00:31:27.612 - 00:31:30.890, Speaker B: Sorry, wait, what is done with the.
00:31:31.040 - 00:31:53.360, Speaker A: Outside of the hash function, which I just copied to the IV? So you see that you pass the hash output as field elements. So if it's 128 bit and you have 128 bit field that have one, otherwise you pass it as your three and then you just copy paste it in there.
00:31:56.210 - 00:32:04.230, Speaker B: You mentioned that underlying field.
00:32:04.340 - 00:32:39.390, Speaker A: Yeah. Well, typically should be a crypto friendly field where the discrete log problem is hard and typically a large primal field. Because if you want a binary field, then Blake is two is fine because it work with bits which are GF two. But yeah, it's typically the main use cases, it's at least 64 bit fields and most commonly two, five, six or more. Cool.
00:32:39.460 - 00:32:42.000, Speaker B: And where is the field?
00:32:45.190 - 00:33:06.920, Speaker A: Yeah, good points. Because we don't depend on the field, we don't encode the field, but we don't need to, because if you have a different field, the function will behave differently, the operations will be different, so you will get a different input. A different output, sorry. So that's why you don't need to explicitly say, I'm using this field, or this group.
00:33:10.030 - 00:33:12.090, Speaker B: Needs to be encoded.
00:33:13.950 - 00:33:31.530, Speaker A: As well. Exactly. And that's the part where if you need to hash objects which are not natively field elements, you need to convert them to field elements. And if you have some ambiguity. Yeah. Right. So can be straightforward.
00:33:31.530 - 00:33:39.934, Speaker A: But these straightforward problems are sometimes the ones that people get wrong. So you want to avoid collisions and two different items mapping to the same field element.
00:33:40.062 - 00:33:45.490, Speaker B: And that depends on that responsibility of hashing the field is on the person who's using.
00:33:45.560 - 00:33:53.430, Speaker A: Yeah. The color of the application. We can anticipate what people will need, strings, images or whatnot.
00:33:56.810 - 00:33:59.510, Speaker B: Can you also hash up your stuff like for example, verification.
00:34:04.210 - 00:34:35.380, Speaker A: You could put whatever you want in the initialization. We have this domain separator which is a generic input type specified as a string of arbitrary size which is just appended to the I O pattern. So if you have one gigs image disk, may want to hash it as well. The good thing with hash functions, you can hash as much as you want. So if. No other questions. Thank you guys and hope you like it.
