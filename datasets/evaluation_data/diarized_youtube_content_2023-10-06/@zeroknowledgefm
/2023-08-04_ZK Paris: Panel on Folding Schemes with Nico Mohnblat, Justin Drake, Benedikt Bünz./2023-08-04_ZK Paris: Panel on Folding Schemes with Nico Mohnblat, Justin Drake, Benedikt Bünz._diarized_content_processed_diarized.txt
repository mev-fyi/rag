00:00:06.370 - 00:00:23.270, Speaker A: Okay. Up next, we have our first panel of the day on folding schemes with moderator and panellist Nico Momblatt from geometry, and panellists Justin Drake from the Ethereum foundation and Benedict buns from espresso systems.
00:00:51.130 - 00:01:14.158, Speaker B: Hi. Thank you very much. Thanks for the intro, Chloe. So, I am Nicholas Monblatt. I'm a cryptographer at Geometry, and I have here today Justin Drake and Benedict Boons joining me on the panel. So we're going to be talking about folding schemes. I have a pretty sort of ambitious schedule for us up ahead, sort of covering the basics, what folding schemes are talking about, performance, talking about security.
00:01:14.158 - 00:01:29.570, Speaker B: I want to also try to talk about what's beyond and what are the alternatives, and hopefully have a bit of an open q a. Before we dive into all this, can I ask you both to introduce yourselves to the audience and actually to myself, because I don't think we've met in person. Benedict.
00:01:30.630 - 00:01:49.980, Speaker C: I'm Justin Drake from the Ethereum Foundation. I do a lot of research on upgrading Ethereum, making it more secure. The layer one. And I guess we might use folding in the context of vdfs at layer one, as well as for snarkifying the EVM, basically making the EVM an enshrined roll up.
00:01:50.910 - 00:02:08.990, Speaker D: Yeah. My name is Benig Buns. I'm the chief scientist and the co founder of Espresso Systems. We work on decentralized shared sequencing. But I also have, I'm a researcher working on zero knowledge proofs. And in recent years, a lot of my focus has been on folding schemes.
00:02:10.370 - 00:02:22.802, Speaker B: So I think, Justin, you started touching upon this already. Why are we interested in folding schemes? Sort of what are the motivating problems that we've seen in the world that led us to think about these problems, right?
00:02:22.856 - 00:03:05.886, Speaker C: I mean, the big primitive that we're interested in is snarks. And I think folding schemes bring us to a point of optimality in many ways. On the one hand, they tend to be extremely simple. So, like congratulations tends to be very low. They seem to reach peak performance, just be optimal from a constant standpoint, even if you're assuming elliptic curves. And, yeah, I mean, this has all sorts of consequences. For example, the fact that we've reached a point of optimality might mean that we can finally standardize, right? Because every six months we come up with a new improvement.
00:03:05.886 - 00:03:11.650, Speaker C: But since we've reached the end of the road, and now maybe we can start building folding asics and whatnot.
00:03:13.690 - 00:03:15.400, Speaker B: Would you like to add anything?
00:03:15.850 - 00:03:58.510, Speaker D: Yeah, no, I mean, I think that folding schemes, they give us this primitive called IVC, called incrementally verifiable computation, which is computation that sort of runs forever. So think of a blockchain. It's a computation that runs forever, but at every step I can actually give you a proof that the computation was done correctly up to this point. And sometimes this is referred to as recursive snarks. This is actually not quite correct. Recursive snarks give you IVC, but folding schemes are something actually much simpler and weaker that still give you IVC. And IVC itself has many applications, succinct blockchains, vdfs.
00:03:58.510 - 00:04:19.530, Speaker D: But even you can build a snark, right? Because you can just formulate your computation as a machine, like as a Turing machine does computation steps. So it just gives you a snark itself. And yeah, I think what Justin said about the constants just being sometimes one is very intriguing.
00:04:20.350 - 00:05:00.280, Speaker C: And I guess one of the things that you can do is because the constants are so good and the overhead of recursion is so low, you can have real time proving. So the end game that I'm thinking is you have a cpu and then you have some sort of coprocessor right next to it, and within one millisecond it gives you a proof of whatever you've proved on the cpu. And another really interesting aspect of IVC is this distributed proving with untrusted provers. So you could have provers all around the world, for example, on the blockchain, and they're all working together and they don't have to trust each other.
00:05:01.610 - 00:05:19.354, Speaker B: Okay, I'll actually come back to that a bit later, maybe towards the end of the discussion for the sort of beyond folding, we've said folding a good dozen times, if not more. I think it's time for us to sort of define what we mean and sort of explain what it is. Yeah. Benedict?
00:05:19.402 - 00:05:19.662, Speaker C: Maybe.
00:05:19.716 - 00:05:20.462, Speaker B: Do you want to take it?
00:05:20.516 - 00:06:13.440, Speaker D: Yeah, sure. I guess we'll get into the naming discussion. But the idea of a folding scheme is that essentially it's about batching. That's yet another name that nobody used, but maybe that should have been the name. Normally when I have a proof, I verify it. But what if instead of verifying the entire proof, I could take two proofs and just batch the verification together? And what I have to do normally in these IVC schemes, I prove that I've checked a proof, but now with folding schemes, I just prove that I've correctly batched the proofs together. And this gives me some new batched object, some accumulator is also what it's called.
00:06:13.440 - 00:06:54.010, Speaker D: And then at the very end of the chain, I just check that the accumulator is correct. And the important property of these folding schemes is that if I check the final accumulator is correct, then this implies that all of the previous things that I folded into this one accumulator were valid, were valid proofs. So I reduce basically checking many proofs into checking one proofs. But the important thing is that I can do this kind of, in this iterative manner, and I iteratively prove that each folding step was correctly, each batching step was done correctly.
00:06:54.430 - 00:07:27.638, Speaker C: I mean, I would go in even further in saying that it is true that we're technically batching these proofs, but the proofs are the most naive proof you could think of, which is like the raw witness, like the instance itself. So really I think of it as batching the instances themselves, the witness themselves. And you're doing almost well. No preprocessing on that. There's no proving step, really, the proving step is delayed to the very end where you want to compress this folder instance to something that may be constant size.
00:07:27.724 - 00:07:56.350, Speaker B: Yeah, I think I've heard you talk about these schemes in the terms of approving preprocessor. Some work you do, and eventually you do the proving, right? So, batching accumulator, lots of, I guess, words that come through this line of work, do they all mean the same thing when we say accumulation schemes, split accumulation schemes, or are there minute differences? Do we care about these differences?
00:07:58.290 - 00:08:39.690, Speaker D: Of course there's minute differences. I don't think that most of them are not that important. It's just historically, like we had these papers in 2020 about accumulation schemes. Well, first of all, there was Halo, which sort of started this work, this whole idea that you could delay some of the verification. And then we had these works on accumulation scheme, and then afterwards Nova came out. But there is one difference, and this relates to what you just said. So oftentimes folding really talks in this language of relations where I fold the witness of a relation, and most of the time that is enough.
00:08:39.690 - 00:09:13.186, Speaker D: Most of the time I will just fold the relation. In accumulation schemes, we do talk about, oh, I'm going to badge the proof verification. And there are schemes like in Protostar, we had this really efficient lookup protocol where usually this works. All of this works for schemes that are algebraic and low degree. So where the verification is some lower degree verification. For r one cs, for Nova, this was r one cs, which is just a quadratic check. So degree two equation.
00:09:13.186 - 00:10:00.498, Speaker D: And then laterworks have extended this to sort of arbitrary degree d equation. But then we also have things like lookup, which isn't inherently a low degree check, but it turns out through one round of interaction, we can turn this into a low degree check and then accumulate that. So that's one example. And maybe there are more where you don't fold, sort of like one witness, but you actually fold a proof. Most of the time the proof is trivial, and I agree with that. But I think just looking at sort of this language of relations does seem to be, at least we have one good example where it is limiting. But folding is definitely a better name than accumulation.
00:10:00.498 - 00:10:02.920, Speaker D: Yeah, sounds better.
00:10:03.450 - 00:10:43.954, Speaker B: Okay, glad to see we can converge on folding. Actually, to your point now, I've been giving this mental model, usually now, for folding schemes, where it's sort of this reduce and combine approach. So you sort of have a strict instance. The problem you start with, and you want to make it into something foldable, right? So in the case of nova, it was relaxed, or one cs. In the case of protostar or hypernova, we now see this thing where you do work through some of the steps of your protocol, and only then do you start accumulating. No question there. So, just echoing your point, I want to talk a bit about performance of these things.
00:10:43.954 - 00:10:57.490, Speaker B: So we said we're optimal down to constant, and even the constants themselves are optimal. How did we get there? That sounds crazy, right? No, you don't seem surprised.
00:10:57.650 - 00:10:58.520, Speaker D: Go ahead.
00:11:01.070 - 00:11:34.114, Speaker C: Yeah, it does seem too good to be true. I mean, what happened for me as a story is I kind of discovered Nova almost at the very beginning, a couple of years ago. And they were very, very useful for vdfs and really got us the performance that we wanted. But no one was looking at them. And the reason no one was looking at them is that they had. Well, maybe that's an exaggeration, but few people, few practitioners were looking at them. They were using halo two, graph 16, fry, whatever.
00:11:34.114 - 00:12:21.250, Speaker C: And I think part of the reason is that they had all sorts of constraints, like on paper, they were not really snarks, right? Because you had to do this final compression step, and the verification time was not very good, because the paper described this compression scheme, which reduced the size of the proofs, but the verification time was still linear in one step. It was only compatible with r one cs, which was not very attractive, and it had various other kind of annoying things. And I think the way that I saw it, as this rough diamond that needed polishing, and over several years, I guess a couple of years we've managed to fully polish it, and now I think we have something which is production grade.
00:12:21.750 - 00:12:38.730, Speaker B: Was it also a case of applications and use cases? Because I think Nova was advertised talking about vdfs and so IVC. The average practitioner isn't building an IVC system. Right. If I'm building just an application, I just need one snark to verify.
00:12:40.510 - 00:12:43.318, Speaker C: I think most practitioners will move towards IoC.
00:12:43.414 - 00:12:45.738, Speaker B: Okay, for every kind of application.
00:12:45.824 - 00:12:56.240, Speaker C: For many applications, yeah. I mean, I think the primary application in blockchains is just zkvms. And once you have that primitive, you can do so many things.
00:12:57.250 - 00:12:58.000, Speaker B: Fair.
00:12:59.250 - 00:13:41.982, Speaker D: I think maybe slightly different perspective on why. It's sort of what has enabled this from a mathematical perspective, to be so damn efficient. We show that this actually works for. It works for any protocol is literally all you need to do is take a random linear combination. That is all you need to do. You take a random linear combination of your accumulator and your new proof or relation or whatever, right? So literally, this is why the constant is one. You take one of them and then you somehow deal with kind of what comes out of it.
00:13:41.982 - 00:13:53.238, Speaker D: But what is just so beautiful is that the primitive is so damn simple, but it gives you something so powerful.
00:13:53.434 - 00:14:24.140, Speaker C: And the primitive that we're relying upon is this additively homomorphic vector commitment, which is kind of the simplest kind of thing, which has like, a little bit of additive structure. And that gives us actually hope that we can patch maybe the last final weakness of folding schemes, which is that they're not quantum secure. And so basically we just need a quantum secure, additively homophobic vector commitment, and then we'll be able to just reuse all the technology that we've built.
00:14:24.830 - 00:14:38.846, Speaker B: I'll add to that. I don't know if it's the final hole in this polished diamond. There's also this thing that we have to use curve cycles, right, which can be quite annoying to either find or work with or reason about.
00:14:39.028 - 00:14:46.446, Speaker C: Right. Alphaleak, there is a paper coming out soon, okay. That improves big alpha leak things.
00:14:46.628 - 00:15:11.094, Speaker B: Okay. So I do want to talk about curve cycles and the bug that was recently shown in the Nova implementation. Before we get there, I also want to talk about more specifics about the performance. It seems like right now the state of the art is kind of hypernova and protostart, as far as I understand. And it seems to be like two camps. Oh, you don't agree with.
00:15:11.212 - 00:15:13.100, Speaker D: I just think protostar is better, but.
00:15:14.830 - 00:15:15.820, Speaker B: I agree.
00:15:17.150 - 00:15:17.466, Speaker C: No.
00:15:17.488 - 00:15:30.960, Speaker B: So there seems to be two camps, the sum check camp, and the kind of random linear combination approach. Is there a superior approach? You just said, do you think protostar is better?
00:15:31.430 - 00:16:14.798, Speaker D: Yeah, I mean, you can just look at the constants. This whole idea in folding schemes is to do as little as possible. And what you can do is you can formulate, so you can view everything as a sum check. If you. Any random linear combination is essentially some form of a sum check. The question is basically, do you run the so called sum check protocol, which is this famous protocol from I think, 1992, which is extremely verifier efficient. Right.
00:16:14.798 - 00:16:42.882, Speaker D: It allows you to do some check over where the verifier only works logarithmic, and allows you to do a random linear combination where sweeping a lot of things out in the rug. But the verifier is logarithmic. But that is not what we need in folding schemes. We don't actually need that. The verifier is efficient. We can do that at the very, very end. We can push that to the decider.
00:16:42.882 - 00:17:17.790, Speaker D: So in Protestar, we don't run the sum check protocol. There's still like some sum over a bunch of terms. That's the random linear combination. But you don't have to do that. You can kind of fold that in, I guess. Where are these things compatible? Well, at the very end, you get some big proof, and you might want to outsource checking that proof or deciding that proof is what it's sometimes called. And this is where we can use standard snark techniques.
00:17:17.790 - 00:17:29.430, Speaker D: And sumcheck is a beautiful standard snark technique that has many applications and is perfectly suitable for these kind of settings. And so that's, I think, where these things are compatible.
00:17:32.490 - 00:17:46.540, Speaker B: Okay, I think now is a good time to move back to the point I just mentioned of curve cycles and the recent bug, I think. Is either one of you able to give a brief overview of what happened?
00:17:48.670 - 00:18:26.918, Speaker C: The non technical overview is that Srinaf didn't prove the scheme with the cycles. He only proved the non cycle scheme. And then he just said, okay, the cycle is an optimization. I won't even bother proving it. Now, I have another story to share, which kind of led to a similar bug, which was actually with RSA based vdfs. We were going to do this amazing kind of MPC with a thousand parties. And basically the authors of this amazing paper kind of had proofs in the base case, and then there was like two or three optimizations that kind of left almost as an exercise to the reader.
00:18:26.918 - 00:18:36.570, Speaker C: And it turned out that the optimizations completely broke the scheme. And so I guess the moral of the story is, always prove the damn optimizations.
00:18:37.550 - 00:19:38.650, Speaker D: It's just to be very precise, what is important is the Nova paper itself actually does not have a bug, because it does not describe this scheme, and neither does, in full fairness, neither does our work on accumulation scheme, our work on Protostar or the halo work. So none of these schemes describe it or any other work that I know, like sangria. None of these works, I think, really describe it formally or improve it. The chain of cycle curves case, they all just mention it. And then in the implementation of Nova, obviously uses a cycle of curves and then you could call it just an implementation bug. But I think that would be wrong because where does it come from? Well, it comes from being under specified and then it's very easy to get wrong. And there was some additional data, and this additional data allows you to break the scheme.
00:19:38.730 - 00:20:09.586, Speaker B: So we agree. Proof of the damn thing, as you just said, justin, who's responsible for this? No, but not to point fingers, but it's more saying like, who should be doing this? Do we expect academics to actually prove every single optimization that's going to be implemented? Or do the implementers need to actually turn to the papers and extend the proofs that are already out there? Do we need some kind of collaboration?
00:20:09.778 - 00:21:11.870, Speaker D: Yeah, it's hard, right? Because the easy answer is like, oh, you should prove everything and blah, blah. But then we probably don't get like five folding schemes papers a year, right? There is a trade off here of trying to make schemes simple to describe and easy to understand, versus ideally, you give a full specification of your schemes and academic papers are not full specifications of schemes. We definitely still need that step, and it's sometimes a little bit of a thankless step. And that is an issue, right. It doesn't get the attention or you don't get the reputation from it that maybe it deserves. Although there's, right, like, there are great, I think zcash libraries does an amazing job of giving very detailed specifications. And I mean, this leads people to use their libraries.
00:21:11.870 - 00:21:56.294, Speaker D: So I do think, yeah, it probably should be a collaboration. I also think that this specific case is a great opportunity for someone to give some generic compiler, which is almost essentially what they did in the Nugen Bonet seti paper, what they did, which is good. They didn't just show like, here was a bug. They also said, here is the actual proof of how it is secure. And yeah, I mean, that is one saving grace is for me, the saving grace is like, people always complain about peer review and they say like, well, peer review doesn't find bugs. And it's like all this academic stuff is bullshit. Like, nobody finds the bugs.
00:21:56.294 - 00:22:47.878, Speaker D: And to some degree that's true. But if something is important enough, then it will get more attention, right? Like you have to just let it sit and marinate a little bit and eventually someone will find the bugs. I think that, yes, the more proving things in your paper, and especially if you describe it in your paper, then you should prove it. There's another story, I think, from one of the snarks that they Pinocchio or one of the other snarks that they use in zcash. And the zcash implementation. I think the zcash bug was also like, you can trace it back to some protocol in the appendix which didn't have a proof, right? So this happens over and over again. You probably shouldn't have extra protocols in the appendix without a proof.
00:22:47.878 - 00:23:17.080, Speaker D: And you should also be ideally very specific about what you do and what you don't do. So for example, if you say like, oh, you can optimize this with cycles of curves, then say specific that in theory we suggest that it might be possible, or that it's likely possible to use cycles of curves. This is not yet proven. We leave this for future work. Right. I think this would be a good intermediate step.
00:23:18.410 - 00:23:52.234, Speaker C: I mean, in terms of reflecting, of how can I kind of try and help avoiding these bugs. I think one of the powers that we have as blockchain projects is the power to signal kind of what is important. And so we can say, okay, nova and folding schemes or a proto star. These are important things, please go break them. I think we also have the capital to have bug bounties. And I think, as you said, with zcash, they have this kind of, this really strong internal cryptographic team. And I think we've tried to do the same thing at the ethereum foundation.
00:23:52.234 - 00:23:58.770, Speaker C: And I think part of the role of this internal team is to bridge the gap between the academic paper and the final implementation.
00:24:00.150 - 00:24:46.930, Speaker B: Okay, I now want to move away a bit from folding and sort of look beyond folding or what are the other options? Because we've said the one requirement we have is actively homomorphic commitments. The problem is those right now, based on discrete log, require large fields. Right. There's a whole other approach, which is let's forget about all this fancy folding, let's go brute force, full recursion. But let's do this with very small fields and very efficient like hardware efficient primitives. Do we have any way of comparing? So here I'm talking about the plonky two type stuff like plonk plus fry and recursion. Do we have any way of comparing these approaches other than just implement and benchmark.
00:24:50.710 - 00:25:24.266, Speaker D: For comparing? I would say that implement and benchmark is the gold standard. I think that's definitely true. I do want to say that. Go back. It wasn't quite the question you asked, but the question that begs itself is, does folding really, and even elliptic curve based folding really have to use large fields? And I think the answer is not necessarily right. So we talked about these constants. The main cost of folding is committing to the witness.
00:25:24.266 - 00:26:25.006, Speaker D: But what is really interesting is you pay cost that is linear in literally like the bit size of the witness. So if your witness is smaller, like, let's say each element is only 32 bits, well, then you will only pay that constant factor, right? In any other proof system that we know, even if you start with small inputs, eventually things will get large and there's no saving of having small witnesses. So maybe we can simulate smaller field operations, especially with techniques like lookups. We could even simulate 16 bits operations. So have something that looks very similar to our cpu, or like a cpu from many years ago. But yeah, sort of simulate these small field operations in a bigger field and then only pay these very small costs.
00:26:25.038 - 00:26:28.238, Speaker B: So you're saying pack more witness bits per field element?
00:26:28.334 - 00:27:06.170, Speaker D: I don't even need to do that. Just have smaller, I mean, just have like you have a full field. But if all of your elements are in the witness, if everything in the witness is just 16 bits, then I don't pay like 256 times group operations. I pay 16 times group operations. And this is very specific about, this is where the small constants really help for these folding schemes where I don't have to pay for, no matter how small your witnesses, I pay a full field exponentiation per element.
00:27:06.250 - 00:27:19.380, Speaker B: I guess holding us accountable for what we just said. Prove it. Is there any security issue there? If we restrict our witness to be on a specific part of the field?
00:27:20.710 - 00:27:37.610, Speaker D: Maybe. I don't know. I'm not saying this is a scheme, right? I'm saying there isn't this fundamental barrier of using small field elements. Yeah, absolutely. This is future work. Section.
00:27:39.150 - 00:28:15.170, Speaker C: One thing I'll say is that on the hardware side of things, we're paying a big penalty because the cpus haven't implemented these large multipliers. And once you go in the ASIC land, then the delta between a 64 bit multiplier. And a 256 multiplier is like much, much smaller. So this will be bridged to an extent with asics. And I guess there's another kind of paper that's coming out. So second alpha leak, I guess, which intends to bridge this gap using lookup tables.
00:28:16.010 - 00:28:33.960, Speaker B: Nice. Thank you both for answering my questions. I kind of want to open the floor to the audience if there are any questions. I see some raised hands already. Yeah. Thank you for the discussion. I have a question about soundness.
00:28:33.960 - 00:28:50.960, Speaker B: So when we fold instances together, we introduce some kind of knowledge soundness error. And my question is, how do these accumulate? And if they do accumulate, does this impose a practical limit on how deep we can go in IVC using folding?
00:28:51.700 - 00:29:22.588, Speaker D: Yeah, sure. So there's a theoretical and a practical answer. The theoretical answer is it's a massive problem. The errors multiply. So even if the size of the extractor grows, so even if it grows by a factor of two, you can only do like less than 100 recursion steps, and suddenly you have something that is exponential. And we cannot prove or say anything. This is the theoretical answer.
00:29:22.588 - 00:30:04.570, Speaker D: The practical answer is like, there's no attack. We have no, like, there's probably not an issue at all. The theoretical and maybe even practical solution to this is to build things in more of a tree like manner, where the issue is the recursion depth. Right. How deep is the recursion? So if I fold a recurs or do IVC, and this is true for all IVC, this is true for recursive snarks, folding everything. If I build things in a tree like manner, I can get sort of the same number of items at the bottom, but like only the logarithmic depth there.
00:30:05.260 - 00:30:11.020, Speaker C: And this tree structure is also the thing that's friendly to distributed and massively paralyzed proving.
00:30:12.640 - 00:30:13.550, Speaker B: Thank you.
00:30:21.030 - 00:30:31.010, Speaker E: First, thanks for the wonderful panel. I do want to challenge a little bit about why we're using IVC as a framework.
00:30:31.090 - 00:30:31.720, Speaker C: Right.
00:30:32.170 - 00:30:52.762, Speaker E: So I think there is another way of thinking this, which is proof current data, which is originally by Alexandra Chiaza. If you read his original paper, he spends some times talking about the negative effect about why IVC shouldn't be the framework of thinking this.
00:30:52.816 - 00:30:53.226, Speaker C: Right.
00:30:53.328 - 00:31:09.700, Speaker E: I guess to me seems that the PCD is actually kind of more general things of thinking about, especially if you have heterogeneous instances from different parties. We'd love to learn you guys ideas about.
00:31:15.190 - 00:31:31.130, Speaker C: Mean. In my mind, the distinction between IBC and PCD is that with PCD you have these untrusted provers. But my understanding is actually these falling schemes are pcds. They have this untrusted prover aspect, so different untrusted provers can all collaborate.
00:31:37.150 - 00:32:19.402, Speaker D: You're completely right. But PCD is the more general primitive. The way that I think of it is, I think the way it is described is that IVC kind of works for line graphs and PCD works for arbitrary graphs. But you can build PCD from folding schemes like we did this back in 2020. The split accumulation scheme works perfectly fine for PCD. Rel very recently had some work extending kind of protostar to be able to handle more inputs. But even if you just have a binary input, you can just make your arbitrary degree dag dag into a degree two dag by just stacking things.
00:32:19.402 - 00:32:32.010, Speaker D: So yes, PCD is really the primitive that we want, though. It's the more general, but we can get PCD from folding. But IVC is simpler to think about. That's the only reason I would say.
00:32:32.080 - 00:32:51.890, Speaker B: This is also maybe something that I got lost in calling everything folding folding originally, like the paper specifically considered the two to one case. Right. You have two things, you fold them into one. The not predecessor, let's say split accumulation considers this many to one case and considers PCD specifically.
00:32:52.950 - 00:33:45.758, Speaker D: Yeah. And there's also something that's not been worked on so much recently, but we did work on in the original split accumulation paper and I'm sure would be possible to get back is there's a very strong notion about zero knowledge for PCD, which is that at any step of the computation you can continue the computation without even knowing any information about what has happened in the past. Right. It's not just that it's untrusted what has happened in the past. You don't even need to know really leak any information about it. The current folding schemes, I think mainly for simplicity, are really not described as zero knowledge schemes, but I'm sure we'll soon see someone adding zero knowledge back. And yeah, PCD is sort of the end primitive that we want with all the nice features.
00:33:45.758 - 00:33:48.626, Speaker D: It's just simpler to describe and it's a stepping stone.
00:33:48.658 - 00:33:48.758, Speaker B: Yeah.
00:33:48.764 - 00:34:00.214, Speaker E: I just want to add a little bit, because from the many distributed system researchers and practitioners, right, I can see PCD more useful in terms of beyond blockchain use cases.
00:34:00.262 - 00:34:00.522, Speaker D: Right.
00:34:00.576 - 00:34:12.000, Speaker E: Maybe we even want to integrate PCD into our distributed system like cloud computing cluster, so that it's more like resilience, at least intuitively to me.
00:34:12.770 - 00:34:14.240, Speaker D: Yeah, that would be amazing.
00:34:17.410 - 00:34:35.430, Speaker B: I think there's one more in the front here. Thanks for the talk. I was wondering, what are your thoughts on using folding for virtual machine type of circuits, like non uniform computation, instead of just focusing on circuits.
00:34:37.050 - 00:35:10.074, Speaker C: Right. I mean, there's. What is it called, supernova, that basically allows you to have multiple circuits that you're accumulating. And so if you have a virtual machines with lots of different opcodes, you might think of like one circuit per upcode kind of thing. Yeah. People are trying to build like a ZKVM using folding. And we have techniques like supernova.
00:35:10.074 - 00:35:11.118, Speaker C: Whatever techniques you have, we have.
00:35:11.124 - 00:35:19.906, Speaker D: The photostar also has a different way of achieving non uniformity, and we also.
00:35:19.928 - 00:35:32.882, Speaker C: Have the high degree gates. Now we have the ccs, we have all sorts of lookup tables. So we're starting to get all the little gadgets that we need to build very complicated circuits.
00:35:33.026 - 00:36:09.620, Speaker D: Yeah. One interesting thing about the constraint systems is actually that you don't even need, like plonk and all of the other constraint systems, they have a lot of structure. But in these folding schemes, it actually suffices to literally just describe completely different verification equations per gate. Right. You can just describe an arbitrary degree depolynomial, and they have to be in no way connected. Right. Normally, in Planck, we have to do.
00:36:09.620 - 00:36:20.502, Speaker D: I don't know, there's sort of different gadgets, and you have to formulate your equation as part of these gadgets in folding schemes. It seems like you don't even need to do that. So it's a very intriguing way.
00:36:20.556 - 00:36:29.890, Speaker B: And, yeah, one more thing I'll add, and correct me if I'm wrong, I think with Protostar, the cost of doing this non uniform ABC is like, at each step, you'll pay for your biggest opcode.
00:36:29.970 - 00:36:47.326, Speaker D: Is that think I. Okay, we need to look in the details. And again, no unproven things, but I think you literally only pay for what you're doing, not just the biggest one. Not just the biggest one.
00:36:47.348 - 00:36:48.846, Speaker B: That's one you're doing specifically. Okay.
00:36:48.948 - 00:36:53.914, Speaker D: I mean, I think we described as if they're all the same size. So it's a little bit more complicated.
00:36:53.962 - 00:37:09.218, Speaker B: But yeah, in that case, maybe the point is moot. But I was going to say it might sort of change the way you want to make your circuits for your vm. You might not want to do one circuit per opcode. You might want to pack them into similar sized circuits to sort of spread out this cost.
00:37:09.384 - 00:37:09.810, Speaker C: Cool.
00:37:09.880 - 00:37:17.420, Speaker A: Thanks. Okay. That was awesome. Thank you very much, Nico, Benedict and Justin, you.
