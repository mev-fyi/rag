00:00:00.170 - 00:00:30.102, Speaker A: Hit record as well. And then we can just start this off. Cool. So this is the third part of the polynomial commitment scheme series that Justin's been doing. Even though it says part four on the screen, it's actually part three for is kind of. We have the first two. I'll probably link to them somewhere in case somebody wants to see those before.
00:00:30.102 - 00:00:35.510, Speaker A: It's probably worth your while to watch that before you start on this one. But welcome back to the study club, Justin.
00:00:36.410 - 00:01:12.590, Speaker B: Thank you, Anna. So, yeah, just to recap where we are. So, in the previous presentations, we gave some motivation for polymorph commitments that they can be used for universal snarks. And we kind of gave an overview of the different constructions and their properties. And it turns out there's a few that you can choose from, and they have different trade offs. And then we kind of explored a little bit how they're actually built. So some of the math behind it and some of the ideas.
00:01:12.590 - 00:02:06.680, Speaker B: But in this presentation, in a way, we're going to move away from polymor commitments. We're going to kind of consider them as a black box, and we're going to see, okay, how can we use them to do cool things? And ultimately, we want to try and build these snoks, but as a kind of intermediate level of abstraction. So between the IOP and the ponymore commitments, I kind of want to introduce this new concept, which I call, I guess, a Ponymore gadget, kind of my own terminology. And it's kind of the Lego blocks, the building blocks that you can reuse reusable blocks to build iops. We have the IOP. Actually, let me just make my mouse a bit bigger. Would that be helpful for people?
00:02:09.850 - 00:02:12.758, Speaker A: We can see it, but, okay, we.
00:02:12.764 - 00:03:13.162, Speaker B: Have the IOP here, the information theory part, and then we have the cryptographic part with the premium commitments. And what I'm suggesting is that we have these kind of gadgets that we introduce as a new level of abstraction. And as I mentioned, these gadgets are useful to build iops because they are reusable. But it turns out that these gadgets can be used to make the polymal commitments themselves better. So we've seen that the basic building block, like, the basic idea of a polymorph commitment, is you have a polynomial, and you want to evaluate it at a point. But what if, for example, you want to evaluate multiple polynomials at the same point, or maybe a single polynomial at multiple points? It turns out that if you use these gadgets, you can enhance the polynomial commitments themselves. Okay.
00:03:13.162 - 00:04:10.538, Speaker B: And one of the key ideas that we're going to use kind of throughout is this short zipper lemmer, which is really cool, really powerful. I mean, basically what it does is that it allows us to take a hard problem, as in kind of computationally expensive problem of comparing polynomials. So if you have a polynomial that has a degree 1 billion, and you want to check that these two polynomials are equal, you kind of need to check coefficient by coefficient. And that's a lot of work if you have a bit of coefficients. So it reduces polynomial equality checking down to checking of one single point, which is chosen at random. So this is what it says here. So f one equals f two with high probability if and only if f one evaluated at z is equal to f two evaluated at z, where z is some sort of random point.
00:04:10.538 - 00:05:04.938, Speaker B: And this is kind of the power of interactivity. So the f one and f two are committed up front, and then the verifier sends a random point, and then you open, and then you can check equality of polynomials. And it turns out that we can generalize the Schwadzlema a little bit. So if you take a bunch of polynomials and you do operations on them. So for example, you have f one, f two, f three, and you kind of want to consider the sum, for example, f one plus f two plus f three, and then you want to compare it, let's say, to f four times f five plus f six, some manipulation of polynomials. Then you can do the same thing. So you have this g polynomial here, which is kind of, which can kind of encode a relationship between polynomials.
00:05:04.938 - 00:05:54.240, Speaker B: You have a polynomial of polynomials, if that makes sense. And then you can check kind of a similar thing. So you can check that these relations between polynomials hold just by evaluating at the single point. And you can also add kind of additional variables. So here, for example, I added this additional free variable, y, if you want the equality. So if you have a multivariate polynomial, you can reduce the number of variables, but you can still keep free variables around if that's what you want. So, yeah, that lemma is used all over the place.
00:05:54.240 - 00:06:46.558, Speaker B: So what I kind of want to do is, as I said, introduce these polynomial gadgets. And at the end, I want you to have kind of a cheat sheet of the various gadgets. So you can just go to this one place and have a synopsis of some of the main ones. And I'll start with a very simple trick, which is used all over the place, which is the range trick. So what you have is a polynomial f, and you have a set s. So s is part of the evaluation domain of f. And you kind of want to show that f vanishes on this set.
00:06:46.558 - 00:07:57.254, Speaker B: So the set, for example, could be the roots of unity. And you want to show that f vanishes on the roots of unity. And basically the trick here that you need to do, and it's kind of written concisely here, is like you need to notice that f vanishes on a range s, kind of if and only if f is divisible by the vanishing polynomial at s. So this z, sub s S is the set z, sub S is going to be the smallest polynomial, smallest nonzero polynomial, which vanishes on S. And so it's going to be the product of x minus si, where the sis are part of S. And you can see that f is equal to this expression. And so if you can somehow check this expression, then you'll be convinced that f is indeed divisible by z of S, and hence that f vanishes on s.
00:07:57.254 - 00:08:31.790, Speaker B: And so this is what the protocol looks like. So basically you commit to f, you commit to the quotient. The prover sends that information to the verifier. The verifier sends a random opening point. The prover will open what he committed to, and then the verifier does this consistency check. And this is a very easy consistency check because we're working with field elements here. It's just one field element, equality and multiplication.
00:08:31.790 - 00:09:26.660, Speaker B: So this is very cheap. And I guess one note here is that in the case where S is the roots of unity, then Z of S is actually very, very nice. It's just x to the power n minus one, where n is the number of roots of unity that you have. And I guess if s was some really awkward set, but for some reason you knew about it ahead of time. So in the pre processing stage, for example, then you could also kind of commit to z of S and then open z of S as well. But in this case, often Z of S is designed to be easy to evaluate by the verifier. So the verifier can check all of this in one go.
00:09:26.660 - 00:10:29.430, Speaker B: Any questions so far on this first basic example? So I kind of walked you through in detail. I mean, this is, yeah, this is a very basic example. I imagine everyone in the chat probably knows it. Okay, so we can show that a polymorph f vanishes on S. Now, it turns out that there's an extremely similar trick, which is quite cool, which is a multipoint opening. So actually you can think of it as a generalization of the range trick, because if you want to show that a polynomial vanishes on S, that just means that when opened at every point on S, it's equal to zero. But what if the openings are nonzero? Well, basically you do the same thing, but in addition to the quotient, you have this remainder.
00:10:29.430 - 00:11:18.854, Speaker B: So you take the remainder of f modulo z of S, and that's going to encode information. Theoretically, this is equivalent, by communicating this polynomial is equivalent to knowing the evaluations of f on S. And so this is how it works, basically exactly the same thing, but we have these extra terms that are communicated. So in the vanishing case, it was implicit that the evaluations would all be zero. But here there's kind of extra information that needs to be communicated because it's possibly nonzero. So you communicate f mod z of S, and it's probably best to communicate it in the Lagrange basis. And what this is, by definition, is going to be the evaluations of f on s.
00:11:18.854 - 00:12:00.270, Speaker B: And then in the consistency check, you check here, you add this extra remainder part, and again, this remainder part. If S is relatively small. If you want to open, generally you want to open at few points because you need to communicate this. And in the context of a snark, you don't want to be opening to a million points. You want to be opening to, let's say a handful of points. So this very much can be evaluated by the verifier. Any questions so far? So basically we have the same trick twice, but really it has a very different flavor.
00:12:00.270 - 00:12:40.960, Speaker B: So one is for ranges and the other one is for multipoint openings. Okay, this is what I mean by taking a polynomial commitment. Just going back quickly to the multipoint opening. You have a polynomial commitment which can open at one point. And here you kind of enhanced it with a simple trick to open at multiple points. And one of the reasons why this is cool is because every opening is expensive, right? So you don't want to do too many openings. So this is kind of a batching trick, really.
00:12:40.960 - 00:13:34.094, Speaker B: And if you look here, you only need to open these two polynomials. You don't have to open. You don't have to open at every single point. You can just open at one single point, just this one point z. Okay, so turns out you can do the same thing for, if you have multiple polynomials, there's also a batching trick, and this is kind of a standard one where you have multiple polynomials, f zero, f to fk. And you just take kind of a random linear combination. But here, instead of taking a random linear combination, we'll actually take a slightly more structured thing, where we're given a random point, which is represented by this variable y.
00:13:34.094 - 00:14:28.690, Speaker B: And then you take powers of it, and the kind of powers of something random kind of looks random. But we can use the Schwartz zip for lemma to convince ourselves that the sadness is good. This protocol is slightly more complicated because we have these two rounds. So we have one random evaluation point, but also just a random challenge. So we have these polynomials, f zero, f two, fk. You kind of want to batch the openings. The verifier sends the opening point, generally after the commitment, and then the prover will send the claimed evaluations, say, I claim that f zero at z, et cetera, up to fk at z are these values.
00:14:28.690 - 00:15:17.700, Speaker B: And then the verifier says, okay, here's a challenge, r, and I want you to open this polynomial here. So, kind of think of it as a random linear combination of the fi, open it at the evaluation point. And now you kind of have two things. You have this one opening, you have this batched opening here, and you have the individual claimed evaluations, and you can do this consistency check as a verifier. And if this consistency check passes, then by the Schwartzer Polmmar, then with high probability, the claimed evaluations were correct. So, I can ask a question? Yeah, go ahead.
00:15:18.390 - 00:15:24.050, Speaker C: Is the intuition here similar to how we batch via Shamir's trick.
00:15:29.210 - 00:15:29.894, Speaker B: Where we.
00:15:29.932 - 00:15:31.574, Speaker C: Have like two variables, and we take.
00:15:31.612 - 00:15:33.974, Speaker B: A random linear combination of the two.
00:15:34.092 - 00:15:35.606, Speaker C: And instead of having the two, we.
00:15:35.628 - 00:15:37.080, Speaker D: Can just show one.
00:15:38.330 - 00:16:10.514, Speaker B: Yeah, this trick is used all over the place. I can't see exactly which one you're talking about right now in the context of fiat chamir. But yes, this trick is used everywhere. Nvds is a good one. I'm familiar. Yeah, it's a very common one. And so here, what you do is, you know where I had this generalized Schwab zipperemma, where I had this free variable y, where.
00:16:10.514 - 00:16:45.790, Speaker B: Well, basically, this is why, part of the reason why I showed this generalized thing is that in this specific trick, you use the y to kind of take the random linear combination. So it's possible you're applying the generalized Schwarzefall lemma kind of twice, maybe. But anyway, yes, you're right. It's a very common trick.
00:16:49.330 - 00:17:08.630, Speaker C: Can I also ask a question? On the previous slide, you said use linearity. What do you mean? I guess you're not going to commit to this linear combination. So, are you something on the commitments that they fare well with linear combinations.
00:17:09.210 - 00:17:58.642, Speaker B: Yeah. So a lot of the polynomial commitment schemes have this linear homomorphism. So if you take f of zero and f of one, and you have commitment of those, you can just take the sum to get the commitment of f of zero and f of one. And here the r's are just scalars. So here you really have a linear combination. So from the commitments of the fi, the verifier, locally, he can compute a commitment to the linear combination, and so can the prover as well. Yeah, it's very likely that this trick can also work without linearity.
00:17:58.642 - 00:18:31.886, Speaker B: So basically, if you don't have linearity, I guess the prover would have to. Hmm. Yeah, I'm not, I'm not sure how. I mean, if you don't have linearity, then you need to kind of prove consistency of a new commitment with your old commitments. But then you kind of lose the benefits of batching, because to prove consistency, then, I don't know, maybe someone in the chat can figure out how to.
00:18:32.068 - 00:19:41.240, Speaker D: I think maybe it would just add another round. So you would have, after sending this r, the prover would send a commitment to this polynomial, let's call it g, which is some r to the ifi, right? I mean, that's just some other, when r is fixed, this is just some other fixed polynomial. And then, yeah, you would need on some random point to prove that really, this is the commitment to g by comparing on some new verifier challenge. And then when you've been convinced that g is the commitment to the right thing, I think you would just open, as you do here. I would need to write it out, but I think basically I would just add another round. So you would still benefit from batching k things.
00:19:43.850 - 00:19:49.046, Speaker B: Right. I mean, it sounds very plausible. I can't see all the details, but it definitely sounds plausible.
00:19:49.158 - 00:19:55.020, Speaker C: Doesn't make sense to me. We'd have to do some quotienting or something as well, since we want to evaluate it at Z.
00:19:58.510 - 00:20:16.560, Speaker B: You're right that in the internals of many of the polyurethane commitment schemes, you would do some quotienting. So you would quotient out by X minus Z. But here I'm assuming you have your ponymore commitment as a black box, and your API is kind of commit and open, and there's no kind of quotient thing here.
00:20:17.110 - 00:20:20.660, Speaker C: It wasn't linear. I can't see how to get arrows into work without.
00:20:21.990 - 00:20:22.498, Speaker B: So.
00:20:22.584 - 00:21:03.600, Speaker D: Right, you say you already have commitments to f one, f and f two, and it's like some nonlinear commitment scheme. And I now send you some g. I've already sent it, and I claim it's a commitment to f one plus f two. I think at this point, if you just check at a random a that the sum of the openings of f one and f two is equal to the opening of the third thing at a. Then I think from the Schwartz polema, it means that the thing I sent is really the commitment to f one plus f two.
00:21:04.770 - 00:21:28.780, Speaker B: Yeah, I see what you're doing. So, basically, what you're doing is that you are paying the cost of opening at some random point for every polynomial. So you lose the batching properties. But once you've done this one time investment, then you can reuse this batching multiple times in the future.
00:21:29.950 - 00:21:48.080, Speaker D: Oh, so, you know what? I didn't really notice that for this consistency check, I was opening everybody in the standard way. So, yeah, I did not notice that I was doing that. So, yeah, maybe it's useful in some case, but not others.
00:21:49.490 - 00:22:25.450, Speaker B: Right. Okay, let's move on, because I've already used almost half my time and another kind of cool trick. And this kind of fits in the table of what I call basis independent tricks. So they work in both the monomial and the Lagrange basis. So it's kind of this degree. Brown. So, let's say that you have a polynomial commitment scheme, which can easily prove, or even, like, a low degree test, which can prove that a certain polynomial has degree at most n.
00:22:25.450 - 00:23:34.266, Speaker B: But in some cases, your n is kind of hard coded. So, for example, if you have a powers of tau, then you can prove that your polynomial has degree at most n. But what if you want to prove that your degree has at most d, where d is less than n? Well, one simple trick is to kind of prove that x times x to the power n minus d times f has degree at most n, and therefore, f must have degree at most D. And this is also helpful in the context of some of these polynomial commitment schemes, which have a logarithmic number of rounds. And the reason is that, basically, for example, if you take the groups of unknown order scheme, you. The. The number of rounds that you do is gonna tell you is gonna give you a bound on your degree.
00:23:34.266 - 00:24:30.850, Speaker B: So if you have, you know, k rounds, you know that your polynomial has degree at most two to the k. But now, if you want to do these batching tricks and you want to prove that multiple polynomials, let's say, have small degree but different degrees, then you kind of need to homogenize everything, and then you can do the batching in a given number of rounds. So this could be a trick useful for that. Okay, so these are kind of the basic tricks and the basis independent. Now I'm going to move to slightly more fancy ones. And for that, I kind of want to make sure we're on the same page in terms of bases. So if you have, like, a vector of field elements, a zero up to ad, there's two extremely natural ways to interpret them as a polynomial.
00:24:30.850 - 00:25:50.650, Speaker B: So one way is to interpret them kind of in the monomial basis, where you have your monomials x, zero up to xd, and then you take this linear combination, and then that's a polynomial that you can evaluate. And then the other really cool basis is the Lagrange basis. And basically what it means here is that the polynomial evaluated at a zero. Sorry, at the zero root of unity is going to be a zero, et cetera. So, basically, these lis are the Lagrange polynomials, and they're designed to kind of be zero everywhere on the roots of unity, except at the one, at the if root of unity. So, if you're familiar with the Chronica delta notation, the delta ij, where it's equal to one if and only if I equals j, otherwise is equal to zero. Well, that might be helpful, but otherwise, just think of it as the polynomial, which is like a vanishing polynomial, is zero everywhere on the roots of unity, except at one root of unity.
00:25:50.650 - 00:26:22.950, Speaker B: And for every one of the tricks that I'm going to present next, there's going to kind of be these two flavors, and the two flavors are going to be different. And what you can do is you can move from one basis to another, so you can do a basis change, but that's going to cost you an FFt. And so you're going to have this log factor overhead, and also it's going to cost you some memory to do the fft.
00:26:23.930 - 00:26:55.714, Speaker A: Hey, Justin, really quick, before you move on the. I just. Having done an episode with Ariel and talking about Planck, Lagrange was highlighted as, like, one of the distinguishing factors that they're using that. Does that suggest that other commitment schemes are using the other one, or is this all new in Planck? I'm just curious if this monomial basis, if it's being used in the other mean.
00:26:55.752 - 00:27:07.698, Speaker B: I guess this is a choice that has to be made at the IOP level. So kind of one layer above. And I think historically you're right. Like, for example, sonic is built on the monomial basis.
00:27:07.714 - 00:27:08.214, Speaker A: Oh, it is.
00:27:08.252 - 00:27:40.190, Speaker B: Okay. And the monomial basis is very natural and kind of what you learn at high school. And so people would naturally go for that. But I think one of the big takeaways in hindsight is that Lagrange bass is just strictly better than the monomial basis, at least for the iops and the point on commitments and all that stuff. It seems to be strictly more powerful, and it seems to just lead to more natural constructions.
00:27:41.890 - 00:27:45.860, Speaker A: Somebody just typed in the text dem are fighting words.
00:27:49.350 - 00:28:54.134, Speaker B: I mean, I'm more than happy to have a debate on this basis is amazing. It's very natural. Actually, my last slide is about plonk, because I'll be able to show plonk in one slide after I showed all the tricks. And also, I'll show you why the Lagrange space is more powerful. Anyway, I'll try to argue. So, this is one of the reasons why the Lagrange bases is so cool. So I used to think, and many people used to think, actually, Vitalik used to think, and Dan Bonet used to think, that one of the key superpowers of the monomial basis is that you can do evaluations in linear time, right? So if you want to evaluate this thing here at point z, then you compute the powers of z, and then you take a sum all in linear time, whereas here, if you want to evaluate it at z, is kind of less clear, because each one of these polynomials has degree n, and you have n of them.
00:28:54.134 - 00:29:22.314, Speaker B: So you have kind of n squared. Okay, you could go from here to here. You could do an inverse f of t, but that's going to cost you a log n factor. So that's no longer linear either. But it turns out there's this really cool formula, which very few people know. I mean, it was mentioned in the Disick paper, but other than that, it doesn't seem to be well known in our space. And it turns out that you can evaluate from the Lagrange basis.
00:29:22.314 - 00:30:00.246, Speaker B: So from the evaluations of f, this is an evaluation of f at the root of unity. You can evaluate in linear time. You have a linear sum here, any evaluation. So this is f of z. So this is the evaluation of f at a point z, you have this formula that holds, which is really cool, and you can think of it as an inner product, right? Exactly the same as the monomial basis. So here, this expression here is the inner product of this vector, the ais by the x vector. So the vector of monomials.
00:30:00.246 - 00:30:47.242, Speaker B: And here is the same thing here you have the inner product of the f to the power I, which you can call AI if you want the same here, the ais, and then you're going to have one over. It's kind of like interesting, right? The symmetry. So here you have powers of x and here you kind of have inverse powers of roots of unity. Anyway, it's just a very cool trick that I think more people should know just to make sure we're on the same page. So we have these two ways of encoding. You can either encode a vector in the Lagrange basis or the monomial basis. And encoding is like a really natural step.
00:30:47.242 - 00:31:48.170, Speaker B: It's like one of the first things you do in your Iop, right? So you have your circuit, which could be expressed in high level code, and then you have your witnesses, and then you want to do this basic arithmetization. And so you kind of want to encode things into polynomials and you have a bunch of coefficients, and there's these two natural ways to encode. So you have, for example, a billion coefficients, d could be a billion, and you encode into one single polynomial. So yeah, the polynomials, you can think of them as information theoretic places like capsules to store coefficients. And then once you've encoded, you can do a query, right? So if you want to query the aif coefficient, you kind of want to do the inverse operation from encoding. You can just evaluate at the if root of unity and you can do something similar in the monomial basis. And you can already see the monomial basis is more awkward.
00:31:48.170 - 00:32:36.426, Speaker B: So you kind of need to isolate, you isolate the if monomial and then you have a left and a right where the left polynomial is going to be of degree. You're going to prove that it has degree less than, less than I. And then this part here must have degree at most, at least I plus one, because you have this I plus one contribution. And therefore you can prove that AI is indeed the if coefficient. So here you can kind of see some of the separation between the bases. The Lagrange is, at least for queries, more natural. And then you have shifts.
00:32:36.426 - 00:33:25.870, Speaker B: So shifts are one of this really awesome thing. So you have this vector of coefficients and you want to shift them. And what is the resulting polynomial? If you shift everything and in the Lagrange basis it's very clean. You just basically multiply the inner part of, basically your input to the function f by a root of unity. And so that's going to be a shift by I coefficients. And you can kind of see that because the roots of unity form a cyclic group. And so you're kind of just rotating everything.
00:33:25.870 - 00:34:07.610, Speaker B: You can think of it as a rotation. And so this is a shift with wraparound. So the very top evaluations will wrap around all the way to the beginning. And then you have something similar in the monomial basis, where you take f and you multiply it by x to the power I, and that's going to shift everything. But again, you can see how it's actually way inferior to the Lagrange basis. And the reason is that this shift here does not have wraparound. Okay, so it's going to shift everything, but the very top coefficients are not going to wrap around to the bottom.
00:34:07.610 - 00:34:50.710, Speaker B: And so if you do want wraparound, which most of the time you do, is kind of a very natural thing to want, you kind of need to do a bit of surgery. You need to take the top bit, kind of need to rotate it, flip it, and then get back in. And so you get a similar mess to the queries. Have a question here. Question from Ivan. Considering baricentric interpolation, how much fft can we remove from the prover? So, if you want to do just evaluation, you can remove all ffts. And actually, if you want to do any operation on the catech commitment scheme, you can also remove all ffts.
00:34:50.710 - 00:35:38.070, Speaker B: So that was one of the realizations that I made recently. Do we have to do it at all, or can we just stay in the Lagrange basis all the time? Great question. So you can stay in the Lagrange basis all the time. And. Yeah, that's something I would recommend. And it turns out that even if you stay in Lagrange bases all the time, for example, in Planck, there are some instances where you do want to do ffts. But I have this write up, which I haven't published yet, but I've showed it to a few people, and I believe you can completely remove ffts from a variant of planck.
00:35:38.070 - 00:36:21.270, Speaker B: The only computationally expensive thing involved is basically multi experimentations. So you have to do a bit of gymnastics to remove all the ffts. But I believe it can be done. At least I have a proof of concept, and I'm hopeful that people will be able to improve upon that. Yeah, you're welcome. Okay, so we have the shift, which is super nice in Lagrange, and if you want to wrap around pretty bad in the monomial basis. And then we have the flip.
00:36:21.270 - 00:37:29.622, Speaker B: So the flip is, as the name suggests, you kind of want to flip the coefficients so you can kind of think of it as like a mirror symmetry kind of thing. And you can just take the inverse here in the Lagrange basis and in the monomial basis is kind of Slightly more awkward. You also have to, in addition to taking the inverse here, you have to do a bit more work to kind of. Basically, when you take the inverse here, you get a LAurenT polynomial, right, where the powers have inverse powers. And so you have to renormalize everything by multiplying by x to the power D. Anyway, flip, I don't think is used that often, but interesting to put there. And then you have a really cool one, which is a sum.
00:37:29.622 - 00:38:25.360, Speaker B: So basically, you have your vector of a zero up to 80, and you want to try and get the sum of the ais. And here it's a really cool idea, which actually is in the plung paper. But I think it was used before it was used, for example, in Sonic. And even before that, kind of the idea of an accumulator polynomial. So you have this g, and you want to kind of gradually accumulate something. And so in this case, we kind of want to gradually accumulate a sum of ais. So instead of taking the sum in one single go, you kind of build it over time.
00:38:25.360 - 00:39:26.034, Speaker B: And so this is where you use the shift. The shift is very powerful. So basically, you have your polynomial f, which encodes the ais. And you want to build this accumulator polynomial such that the g at the zero root of unity is equal to f at the zero root of unity. And then g at the first root of unity is going to be equal to the sum of g of zero at the zero. And the first, and then the zero of the first and the second, et cetera, et cetera. So you kind of accumulate, and so the shift allows you to kind of access kind of a neighbor, right? So this will be the neighbor relative to this guy.
00:39:26.034 - 00:40:08.702, Speaker B: And so the neighbors are related to each other via the addition of something. Is someone speaking? No. Okay. And so this is how it looks like in the picture, just to show you how it works. So you have your f, for which you want to take the sum. You have g, which is your accumulator polynomial, which is kind of uniquely derived from f. And then you kind of want to show that g is consistent with f.
00:40:08.702 - 00:41:04.610, Speaker B: So what you want to do is you want to show that this polynomial q, which is going to be the shift of g. So the g at the next value, the neighbor, minus the current value of g, that should be a minus, sorry, minus the current value of g minus f is going to be zero on the vanishing polynomials, where the s here is the roots of unity. Right. So you want this expression to hold on all of the roots of unity. And if they hold on all the roots of unity, then in particular they must hold for the very last root of unity, and the very last root of unity will contain the whole sum. And then what you do is you just query the g on the last root of unity, and you get the sum.
00:41:04.950 - 00:41:21.430, Speaker C: So I do have a question here. If the s don't add up to zero around the root of unity, how does this work? Because we wrap around, right? Or does it need zs not to be all the roots of unity?
00:41:25.290 - 00:41:26.680, Speaker B: Could you repeat the question?
00:41:28.490 - 00:41:38.140, Speaker C: If f is defined at all the roots of unity, then it needs to sum all of them to zero. If we want this to accumulate around all the roots of unity, we've got to sum to zero, because g wraps around.
00:41:41.150 - 00:41:44.320, Speaker B: So why would they accumulate to zero?
00:41:46.850 - 00:41:52.590, Speaker C: If they don't? How is it true that g of the first utility is equal to g of the first utility? Unity?
00:41:55.190 - 00:42:09.000, Speaker B: So you can enforce that by construction. Right. Okay, so you're saying this expression won't hold on all of s? It will hold on s minus the very first point.
00:42:09.690 - 00:42:12.018, Speaker C: Yeah, it needs the s not to be all the root of unity.
00:42:12.114 - 00:42:14.422, Speaker B: That's a great thing, actually, I think you're right.
00:42:14.476 - 00:42:14.982, Speaker C: Yes.
00:42:15.116 - 00:43:03.718, Speaker B: So we kind of want to take the vanishing polynomial on s minus the very first root of unity. And then on the first root of unity, you have this special condition. Yeah, you're right. And the good news is that if you take the vanishing polynomial on s and you remove the first root of unity, you still get something which the prover can compute easily here. It's still a very nice kind of sparse polynomial. And then here, this trick with the quotient is the exact same trick as the vanishing trick. I haven't done anything different here, but I guess one thing which is different is that I have this point g, and I'm opening it at two points.
00:43:03.718 - 00:43:28.880, Speaker B: I'm opening it at Z, and I'm opening it at WZ. And this means that I don't need to commit the shift of g. I can just open if, if I have a commitment to g, that's enough for me to get openings of shifts of g. And then I can just check the consistency here.
00:43:32.980 - 00:43:40.468, Speaker A: Hey, Justin, just a quick check on timing. Are you good on this? Yeah, I'm good.
00:43:40.554 - 00:44:04.664, Speaker B: Okay, cool. I have 13 minutes. I'm going to finish this okay, cool. Okay, so, yeah, this is the sum trick. And there's kind of an alternative way of doing it, which is like the. How do they call it? Sum check. Yeah, they call it sum check, and I think it's used in fractal and other places.
00:44:04.664 - 00:44:47.304, Speaker B: And basically, if you're working on the roots of unity, there's like an alternative way to get the sum using this formula. But the reason why I like this one here is because it maps well with the next one, which is the product. So you can use this exact same idea of accumulator polynomial, but this time with a product. And this is one of the arguments that's used in plonc. So you have this, what they call the grand product. So I guess the first one should probably be called the grand sum. And the exact same trick happens if you have a vector of A's and you want to know what the product is.
00:44:47.304 - 00:45:20.500, Speaker B: Well, the prover wants to show the verifier that the product is a certain product. Then you can use the same trick, you know, in the monomial basis. This is what sonic does, and it's really ugly. I still haven't fully wrapped my head around how they do it. And Mary suggested not spending any time on it. Yeah. So here again, it's like the Lagrange basis is so much nicer than the monomenal basis.
00:45:20.500 - 00:46:14.880, Speaker B: And then we have this other really cool argument, which also is part of Planck, which is the permutation argument. Actually, that's the p in Planck. And here it builds upon the grand product. So you have sigma, which is a permutation. It's a permutation of the set of indices from zero to d. And basically you want to prove that if you take f, which encodes the AI, and you were to permute the ais according to sigma, then you'd still get the same. And this is kind of the three line kind of synopsis.
00:46:14.880 - 00:47:04.244, Speaker B: But let me kind of give a few more details because it's a really cool argument. So we have this sigma, right, which is a permutation of the ais. And what does it mean to be a permutation of the ais? Well, it means that kind of AI is equal to aj whenever I is equal to sigma j for every pair, kind of inj. It's kind of just a reformulation of the definition. And then you can encode this top line as polynomials. Right? So you have a polynomial, which are degree one. That means they have two coefficients here and here.
00:47:04.244 - 00:48:05.544, Speaker B: And if you have equality as polynomials, then that means that the coefficients match. And so it means that AI is equal to aj if AI is equal to sigma j. This is just an arithmetization step, really from just writing things in polynomials. And then we have this other reformulation in terms of multi sets, right? So here, instead of having many polynomials, you just consider just one multiset for each of them. So this line here is equivalent to these two things being equal as multisets, multisets of polynomials. And then if you have multisets, you can arithmetize that as a polynomial. So we already have the x variable, which is used.
00:48:05.544 - 00:49:02.844, Speaker B: So I'm going to use a new variable, y. And basically, if you look at the product here, for example, you look at the left hand side, you take the product. Well, the root of this product is going to be this multi set, right? Maybe there's a minus sign, there should be minus y here. But if you look at the roots, this corresponds to this and this corresponds to this. And so two polynomials are equal if and only if they share the same roots. And so basically you've been able to encode this permutation thing into an equality check of polynomials. And then by the Schwarzenegglemma, you can kind of replace, you can just evaluate at random points r one and r two.
00:49:02.844 - 00:49:51.420, Speaker B: And then here you just have a single check of field elements. And so we already have product arguments. It's easy to build this with linearity. It's easy to build what's inside here. And so it's easy to kind of build a permutation argument from this kind of arithmetization of sigma. And this is what is used in plunk. And then kind of the final thing, which is just to cover everything in the list, is the inner product.
00:49:51.420 - 00:50:49.230, Speaker B: So you're right. So you basically you have two polynomials, f one and f two. And taking the product here over the Lagrange basis is the same as entry wise products. So if you have a zero up to ad and b zero up to bd, and you multiply the polynomials, corresponding polynomials, you have a zero times b zero in the first entry and the zero of entry, and then aa one, b one in the first entry, et cetera. So it's what's called a Hadamar product, which is kind of an entry wise product. And then you can have an inner product by summing over this Hadamar product. And there's a corresponding way to do it in the monomial basis as well.
00:50:49.230 - 00:51:35.630, Speaker B: And here I just want to kind of show that there's some things that you can't really do in why, at least I don't know how to do in the monomial basis. So, for example, this hademal product we just talked about, you have f one and f two, and you just take the product. It couldn't be any simpler. That's something I don't know how to do in the monomial basis. So it's one of the superpowers of the Lagrange basis. And I guess partly why Planck is built on the Lagrange basis. And then similar thing, inner products come from Hannah Maru, so similar thing here.
00:51:35.630 - 00:53:02.824, Speaker B: And also the lincheck, which is basically like, you have your vector a, and here, this time you want to multiply it by a matrix. So you want to take linear combinations, hence the term lin, which stands for linear. And here Marlin shows you how to do it, I guess, in the Lagrange basis, but I wouldn't know how to do it in the monomo basis. Okay, so this is kind of my final slide that I wanted to get to. So, we have an arithmetic circuit, and what does it mean to be an arithmetic circuit? It kind of means that you have four things. So you have your public inputs, you have addition gates, multiplication gates, and wires, right? And generally your gate will be arithmetic gates, as opposed to Boolean gates, meaning that the inputs and the outputs will be field elements, kind of large field elements. And in plonk, everything is just simple.
00:53:02.824 - 00:53:45.910, Speaker B: So you have your public inputs, and you encode them using the encoding trick in the Lagrange basis and some polynomial I for inputs. And then you kind of have addition gates. So addition gates have left, right and output wires, and you can encode each independently. So left, right, output, you can encode them each as a polynomial, and then you can check the consistency here. This is very, you just need to open at a random point. This is kind of the generalized Schwartz polema that you can use here. So, open at a random point and check consistency here.
00:53:45.910 - 00:54:42.872, Speaker B: And then here you can use the Hadamard trick, hadamod product, and the range trick. So basically you consider ML times Mr. Minus m zero. This should vanish on the roots of unity. In a way, it couldn't be any simpler. And then the big question is, how do you do wires? How do you connect the various gates? And so here what you do is kind of, you consider your big witness polynomial, which you can think of as the concatenation of all your previous wires that were defined here. And then you have a permutation argument over those.
00:54:42.872 - 00:55:36.410, Speaker B: So you consider all the inputs and all the outputs to your gates as one big vector, and then you permute them. And so if you have a connection between this input and that output, well, that means that there's a cycle which contains these two indices, and so you have equality in that way. And there's kind of a simplification of the Planck constraint system. So in Planck, in the real world, they'll come up with kind of optimizations, but that's kind of a simple way of thinking about it, kind of conceptually, where you have these different things, and they're each handled independently by different tricks, and that's it. And I think I'm just on time.
00:55:37.980 - 00:55:52.156, Speaker A: Wow. There was a question. I don't know, did you get a chance to answer that other question? I think it was back on slide like 80, when all. There's somebody who asked it.
00:55:52.338 - 00:56:55.440, Speaker B: Yes. So Olivier is asking, regarding the formula evaluating a polynomial. Given that granch basis at some point z, how efficient is the computation of one divided by z minus w to the I? So let me have a look. Um, yeah, so he's asking like this, this part here, how efficient is it? Well, you can pre compute w to the I, and you can also pre compute one over w to the I. And then taking this sum here, this subtraction is easy. And I guess I think it's the inverse that you're talking about. How expensive is the inverse? And I believe taking the inverse in a finite field is like one exponentiation.
00:56:55.440 - 00:57:01.360, Speaker B: So I guess you're right. You have to do n exponentiations.
00:57:05.380 - 00:57:17.720, Speaker C: So I was just wondering. It means that evaluation is not linear in the degree. General, the time to do evaluation.
00:57:20.160 - 00:57:51.270, Speaker B: Well, I mean, these are field exponentiations, right? They're not group exponentiations. Um, right. So you. What are you saying is that you're. Your unit of cost is a field operation, and here you have. I mean, that's a very good point.
00:57:52.280 - 00:58:06.360, Speaker D: Zach knows this better than me. But there's this batch inversion tricks. I'm guessing you can do it in total o of n field operations.
00:58:10.160 - 00:58:25.010, Speaker B: Right? So I think you can do it. Maybe in the context of a circuit, you can do nondistip deterministic things like that. Like these tricks. Maybe that's what you're referring to.
00:58:25.780 - 00:58:32.210, Speaker D: No, as I understood it, and I didn't see the algorithm myself.
00:58:35.000 - 00:58:35.668, Speaker B: Just to.
00:58:35.754 - 00:58:42.260, Speaker D: Compute many inverses in the field, there's some sort of batching technique.
00:58:44.440 - 00:59:10.284, Speaker B: Right? I mean, one easy batching trick is. So you want to compute one over ab, right? Where a and b are two distinct values. So you want to compute a of one over a and one over b. Well, first, you compute one over ab. So that's one experimentation. And then you take that, and you multiply it by b. You get one over a, and then you multiply it by a, and you get one over b.
00:59:10.284 - 00:59:36.900, Speaker B: So, actually, you're right. You can do two experimentations at the cost of just one experimentation and two multiplications. And I think the trick kind of generalizes. So, if you have one over a, one over b, one over c, you only compute one over ABC, and then you kind of clear the things that you don't like. Yeah, that's a good point, Ariel.
00:59:41.020 - 00:59:41.384, Speaker C: But.
00:59:41.422 - 01:00:00.280, Speaker B: Right. I need to confirm. So, basically, the question becomes, is it more or less than log n or equal to. Yeah, that's a good question. Thank you, Olivia.
01:00:00.360 - 01:00:06.590, Speaker A: Maybe this is actually a question we could put into the study club chat, see if anybody has any ideas on that.
01:00:07.060 - 01:00:09.090, Speaker B: Yeah. Cool.
01:00:10.420 - 01:00:20.340, Speaker A: Is there any other questions or points? I mean, we started a few minutes after the hour, so I think we could stay another minute or two. If you guys have some other questions or comments.
01:00:24.440 - 01:00:46.060, Speaker B: I mean, one thing I'd be curious to know is, are there tricks that I missed out? So, basically, the reason why I built this table is because every paper uses a different set of tricks, and all the tricks are kind of split over many papers. I was trying to put everything together in one place. What have I missed in terms of tricks?
01:00:50.700 - 01:00:57.070, Speaker A: Cool. I feel like this sounds like an open question, maybe, for people to look at.
01:00:58.160 - 01:01:29.072, Speaker D: I don't have an answer to that. I'll give my one main comment that I'm interested. I didn't think of this grand something that you could do it with the way you did it. So I'm interested now to do, like, a sum check, if you'll plug that in, instead of the sum check technique in aurora, in Marlin and fractal.
01:01:29.216 - 01:01:30.004, Speaker B: Right.
01:01:30.202 - 01:01:34.070, Speaker D: I'm curious how that will affect their numbers.
01:01:34.600 - 01:02:10.370, Speaker B: Yeah, that's a good. Yeah, I think I discovered this. I came up with it. I don't know if anyone before came up with it. And the reason why I discovered it is because I was filling this table, and it took a sum in a monomial basis. I was like, okay, how can you do it here? And you can just use the same trick as the product. So, one of the cool consequences of this thumb, which might be relevant for Planck, is that in plonk, you have this problem where addition gates are not free.
01:02:10.370 - 01:03:18.692, Speaker B: And so one of the problems is, if you have like a massive addition gate with a huge fan in, then you're going to have to pay a lot, because you have to build it in terms of small addition gates, which have small fan in, and then the costs really start to add up. But what you can do is, at a constant overhead for the verifier, you can build an addition gate with arbitrary fan in. So if you wanted, you could take a linear combination over every single kind of wire with a single kind of these kind of global gates. Global addition gates. So if you have a circuit which has just a few, let's say a handful of very large addition gates, then really you should consider this trick. I mean, it will increase the proof size by a little bit. Something like one group element maybe, or one group element and one field element per gate.
01:03:18.692 - 01:03:48.290, Speaker B: But it might be worth it in some instances. And the reason is that you can combine this with a selector polynomial. So instead of just f, you do f times q, where q is a selector polynomial, and then you take the sum. And so that allows you to take arbitrary linear combinations over all the wires, which might be interesting.
01:03:50.740 - 01:04:20.700, Speaker D: Yeah. So you'd need like a selector polynomial per gate, I guess, if you're totally honest, I'm more interested to see how this will affect Marlin, because it'll be more sort of a smooth plugin. But, yeah, that's also a good direction.
01:04:26.560 - 01:04:27.310, Speaker B: Cool.
01:04:30.020 - 01:04:39.904, Speaker A: Well, I think if it's okay for you guys, I'm going to wrap it up. But I don't know if you have anything else you wanted to share, Justin, or are you good to.
01:04:40.102 - 01:04:42.740, Speaker B: No, that's it. Thank you so much, Anna, for having me. This.
01:04:42.810 - 01:04:59.928, Speaker A: Yeah, thank you so much for doing this. This third part of this series, I've been hearing so many. I said this to you kind of before this version or before we started today, but I've been hearing so many good things from other people who've been watching the videos. And so I really appreciate you coming on and doing this.
01:05:00.014 - 01:05:09.884, Speaker B: Yeah, boy. I have a suggestion which you can totally ignore, but there's a website called ZKP Science. I think it's run by Strad. You'd have to double check.
01:05:10.002 - 01:05:10.670, Speaker A: Yeah.
01:05:11.200 - 01:05:46.068, Speaker B: And I would suggest you submit this table. Maybe there would be some way it could be added to by people or something, but this common ontology is really great. Yeah. So Daniel from ZK proof asked me if I would write kind of a blog post on this, and I kind of agreed. So we'll have a blog post, and it's also possible that I'll give a two hour crash course on this at ECC as nice.
01:05:46.234 - 01:05:46.950, Speaker A: Nice.
01:05:47.400 - 01:05:48.150, Speaker B: Cool.
01:05:48.600 - 01:06:05.564, Speaker A: All right, so thank you all for joining the study club. I think this concludes, for now, the polynomial commitment series. However, we have some ideas for some follow ups, but we'll chat more about those in the group. All right, thank you all.
01:06:05.682 - 01:06:09.880, Speaker B: Thank you. Thanks. Thank you. Bye.
