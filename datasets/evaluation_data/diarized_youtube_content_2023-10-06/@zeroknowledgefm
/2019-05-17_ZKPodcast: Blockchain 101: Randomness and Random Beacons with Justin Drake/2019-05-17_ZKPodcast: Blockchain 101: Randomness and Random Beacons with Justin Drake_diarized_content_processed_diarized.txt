00:00:06.330 - 00:00:18.750, Speaker A: Welcome to Zero Knowledge, a podcast where we explore the latest in blockchain technology and the decentralized web. The show is hosted by me, Anna, and me, Frederick.
00:00:26.330 - 00:00:56.090, Speaker B: In this week's episode, we sit down with Justin Drake from the Ethereum foundation to talk about randomness, how it's used in blockchain, what a random beacon is, and more. Today we're going to talk about randomness, and hopefully it's a one on one episode that sort of stays within the educational bounds and we can be useful here before we kick off. Hello Anna.
00:00:56.170 - 00:00:56.702, Speaker A: Hello.
00:00:56.836 - 00:01:13.934, Speaker B: We have Justin Drake with us, which actually is on the episode by people's Choice Award. We made a call out on our Telegram channel and asked who the best person to talk about randomness is, and everyone voted for you, Justin. Congratulations.
00:01:14.062 - 00:01:15.540, Speaker C: Oh wow, great.
00:01:16.470 - 00:01:17.970, Speaker A: So welcome to the podcast.
00:01:18.050 - 00:01:19.350, Speaker C: Thank you. Thank you for having me.
00:01:19.420 - 00:01:22.690, Speaker B: Maybe you can just introduce yourself quickly, Justin.
00:01:22.770 - 00:01:37.242, Speaker C: Sure. So I'm Justin Drake and I'm a researcher at the firm foundation and I'm mostly helping out with the Ethereum 2.0 effort, and I've been with the Inferior foundation for about a year and a half now.
00:01:37.376 - 00:01:45.680, Speaker A: When you say you're working on e 2.0, why did your name come up for randomness? Like when we said who's the expert on randomness? Why did your name come up?
00:01:47.890 - 00:02:28.298, Speaker C: I look at all aspects of Ethereum 2.0 and at one point in time I ran down the randomness rabbit hole trying to find effectively the best solution for Ethereum 2.0. And it turns out that one of the solutions that we looked into is so called vdfs, verifiable delay functions. And that is a whole story with a lot of moving parts. But it's one of the research efforts that I'm leading at the Ethereum foundation, and the idea of VDFs is to try and make a randomness beacon which has all the properties that we want for Ethereum 2.0.
00:02:28.384 - 00:03:14.890, Speaker A: We want to structure this episode as a blockchain 101 episode on Randomness, and I think that through the conversation we will probably get a chance to touch on the work that you're doing, Justin. But to start off, I want to define randomness, and the reason I want to do this is that there is some confusion about the term. Here are the standard definitions. Randomness is the lack of purpose, logic, or objectivity of an event. It's a type of circumstance or event that is described by a probabilistic distribution. Randomness has also been defined as having the property that all possible outcomes are equally likely. So, to illustrate that last point, let's take the number PI.
00:03:14.890 - 00:03:58.354, Speaker A: So PI actually is a series of seemingly random numbers. PI definitely has that last property where all possible outcomes are equal. So say you're looking for a specific three digit pattern series. It's no more likely that you would find the series five, two, eight than it would be that you find the series four, seven, six. There's no pattern. So this is great, but it wouldn't necessarily satisfy the definition of randomness for cryptographic purposes. In the case of cryptography, one needs a form of randomness with a very specific quality, that is, that the random number is both unpredictable and kept secret.
00:03:58.354 - 00:04:25.854, Speaker A: With cryptography, one is looking for an asymmetry of information. You want to be able to perform some secure operation without an adversary being able to replicate it or derive the same results. Going back to the example of PI, the result of the calculation of PI has no discernible pattern. The actual number PI is very random. The sequence of numbers appears with equal frequency. Therefore, all number combos are equally likely. And so you could call this random.
00:04:25.854 - 00:05:24.078, Speaker A: But given that the method for generating this number is known and therefore predictable, this would not satisfy the requirements of a cryptographic randomness. So how do you add this unpredictability that you so need? Well, this is a cool feature about randomness. If you take a random number and mix it with another number, random or not, it will have the same level of unpredictability as the most unpredictable of the two. So when you think of a computer generating randomness, what it will often do is look to some outside real world source to generate a random number and potentially combine that then with other numbers. The source of this could be temperature or the timing of some real world thing. For example, Cloudflare published a really great article about randomness and how they generate it. They actually have a wall of lava lamps in their office, and they use a video recording of the lava lamps, which would be fully unpredictable to generate randomness.
00:05:24.078 - 00:05:41.930, Speaker A: So particular pixels on the screen, depending on the color that they take, this could then be used as a source of real world randomness. Cool. So with this, I think we can now jump into how randomness can be used in blockchain.
00:05:42.590 - 00:06:39.514, Speaker B: Yeah, so I think randomness in blockchain is an interesting topic that is related to cryptography, obviously, but also somewhat separate. I think the first time anyone interacts with sort of randomness in a blockchain context is, at least in the Ethereum world. If you're trying to write a smart contract, that does something like a lottery, and they go, okay, how do I randomly choose a winner out of this set of people who are participating in the lottery? And the first answer that they will think of is, I'll use the hash of the last block, because that looks like a random number. And there's actually some problems with this, and we've covered it slightly on the podcast before, but not really dived into it. So maybe, Justin, you can talk a little bit about what kinds of randomness do we have in, say, ethereum today? And what's the problem with that? Why is it not enough?
00:06:39.632 - 00:07:27.622, Speaker C: Yeah, so I think, going back to definitions, randomness, for me, is an unpredictable source of entropy. And one of the big challenges in the blockchain space is that we want to do it in a decentralized fashion. I mean, there's other properties that we care about other than unpredictability. For example, we don't want the randomness or the source of entropy to be manipulable, and we want kind of a steady stream of it. We don't want it to suddenly stop. In the case of Ethereum 1.0, the randomness that we get from the block hashes actually falls under a very large class of randomness beacons, which I would categorize as commit reveal.
00:07:27.622 - 00:08:52.614, Speaker C: And this is actually very somewhat similar to the way that computers derive randomness by sampling from the environment. So the idea is that some external actor to the blockchain will commit somehow, and then they have the choice to reveal that local entropy, which is external to the blockchain, and put it on chain. So in the case of proof of work, you can kind of think of the commitment phase as being burning a lot of electricity to generate a block. And then once you have generated a block, the block hash will look random, it will be a good local source of randomness, which is derived kind of externally to the blockchain. And now the miner has the incentive to publish this block and include it on chain, and basically include this randomness for everyone. The main problem with proof of work and all the other commit reveal schemes is that the miner, or whatever external actor it is, it could be a validator, for example, on proof of stake, has the option to not include that local entropy on chain. And so by doing so, they basically have the option to discard a random number that they don't like.
00:08:52.614 - 00:09:20.414, Speaker C: They just don't tell the world about it. And as such, we say that they have the ability to manipulate the source of randomness. So even though the block hashes look totally unpredictable. And they satisfy the decentralized property, and they also satisfy the liveness property, which is that you will have a constant stream of blocks. They don't satisfy the on manipulability property.
00:09:20.612 - 00:10:10.602, Speaker B: Going back to the lottery example here, that would mean essentially that the miner can participate in the lottery and then start mining a block, and that he keeps churning out new hashes until he finds a proof of work that allows him to publish that block. If he doesn't win the lottery in that block, he can either keep mining and let's just add another transaction, and then keep mining until he finds a block where he does actually win. Or he can say someone else won, I'm just going to throw this one away, and then keep doing that. So he can sort of keep churning to try to gain an advantage in winning. And depending on how large of a mining portion this guy has, he may or may not be more or less likely to win. He can actually affect the outcome of the lottery. On average.
00:10:10.602 - 00:10:18.210, Speaker B: A lottery is one example. But what other uses of on chain randomness do you see there is so.
00:10:18.280 - 00:10:47.430, Speaker C: The use cases of generating randomness on chain kind of fall in two buckets. Buckets. Number one is just using the randomness for consensus layer use cases. And the second bucket is for application layer stuff. So the lottery, for example, would be at the application layer. In terms of the consensus layer, it's very simple. It's simply randomly sampling consensus participants to do certain tasks.
00:10:47.430 - 00:11:18.294, Speaker C: So if you take bitcoin or ethereum 1.0, the main task that needs to be done is block proposing. So extending the blockchain with a new block. And we want that process to be random and fair. But you can have more sophisticated reasons to use randomness at the consensus layer. For example, if instead of sampling a single kind of monopolistic actor for a short period of time, you can sample the whole committee. And so in ethereum 2.0,
00:11:18.294 - 00:11:23.682, Speaker C: for example, we have the notion of a cross link committee or attestation committees.
00:11:23.826 - 00:11:37.580, Speaker A: In this case, though, randomness is used to make choices, sort of of which actors get included. Is that sort of the way it's being used in that case? When you say a committee, I'm just curious what that means.
00:11:38.110 - 00:12:33.180, Speaker C: Yeah, so the idea of committees is that you have a pool of participants and you have a so called honesty assumption. So you assume that some fraction of these participants will be honest. And generally the property is that this pool of participants can be very large, because if you're working in an open, decentralized blockchain, anyone can just come in and become a consensus participant. So generally it's non practical for everyone to participate at the same time. One technique that is used quite commonly in proof of stake systems is that you will have a notion of a statistically representative committee. So if you take a committee which is large enough, let's say 1000 validators, then it will be very representative of the global pool of.
00:12:34.270 - 00:12:45.514, Speaker A: But if you took a small that doesn't completely define where randomness comes in, the way I've understood it is you'd have a larger pool. And in the sampling process, it's almost like random numbers are picked.
00:12:45.562 - 00:13:00.310, Speaker B: I mean, either you do it in some pattern, like we pick the first 100, then the second 100 in a predictable way. But if you wanted to actually sample, you need some source of randomness to pick out random participants from this set.
00:13:00.460 - 00:13:14.618, Speaker A: And I guess the reason that you would do that is by adding this randomness, it cannot be gamed. You can't figure out who will be chosen for that committee and then put your friends in that space. I know it's not friends in this case, but like.
00:13:14.704 - 00:13:33.050, Speaker B: Yeah, but exactly. I mean, if you had 1000 validators and you knew it was 100, then the next hundred you could set yourself up, because there's no built in civil resistance necessarily. You could set yourself up with 100 validators, set them all in a row, and then you know that by this block I will have total control of the production.
00:13:33.210 - 00:13:47.798, Speaker A: And so this is for, we've just been talking about the pos use of randomness, but going back to what you had said originally about proof of work. Is proof of work a random generator? Is that basically what role it takes there?
00:13:47.884 - 00:14:23.626, Speaker C: Yes, exactly. And it's an extremely expensive random number generator. You can think of that basically it's a way to take a pool of participants, in this case the miners, and fairly sampling them one at a time. And not only is it extremely expensive, it could burn, let's say, $1 billion per year. It's also not perfect. It's manipulable. And so it turns out that we can build randomness beacons which are much, much cheaper.
00:14:23.626 - 00:14:32.370, Speaker C: They might burn on the order of $1 million per year. So 1000 times cheaper and also have better properties. They are unmanipulable.
00:14:33.270 - 00:14:57.430, Speaker B: Going back to proof of work and this switch to proof of stake, one of my questions was, so we have this randomness generation in proof of work. We know it's manipulable. So what is a better solution? You just mentioned the random beacon, but really what's the better way of generating randomness? In this case, when in that switch to proof of stake, what changes? What capabilities do we gain?
00:14:57.510 - 00:16:11.326, Speaker C: So in proof of stake, we have three options. The first option is to go with the commit reveal family of randomness beacons. And that's in a way replicating the proof of work, whereby you have participants who commit to local entropy and then they have the option to reveal or not reveal. And all these systems have a so called last revealer attack, which is when you're invited to reveal, and after your reveal, that's going to be the random number that is used by whatever application, then you have the option to reveal, not reveal, and hence you can manipulate the randomness. So there's two other classes of randomness beacons which don't suffer from the manipulation of attackers. Number one is a class around threshold cryptography, and probably the most well known system here is definitive's threshold relay. And the other approach which doesn't suffer from the manipulation, is basically taking a commit reveal scheme and adding a third step, which I call an extraction scheme.
00:16:11.326 - 00:16:29.462, Speaker C: So it becomes commit, reveal, extract. With this final extraction scheme, you basically prevent attackers from being able to make informed decisions about the reveal or the non reveal, and hence not being able to manipulate the randomness.
00:16:29.606 - 00:16:40.214, Speaker A: You've just mentioned a few different sort of mechanisms for creating this, but what is the input? Where's the original randomness coming into this and where does it come from?
00:16:40.272 - 00:17:14.530, Speaker C: So in all cases, the original randomness comes from the external consensus participants. So they will generate randomness in the traditional way using their computer. And so long as the honest players do that, basically choose what looks to them like a random number. Then with the honesty assumptions, you can have what looks random to everyone on the blockchain by going through the protocols.
00:17:14.610 - 00:17:56.622, Speaker B: Basically in like a commit reveal scheme, it's up to each participant what they're committing. I mean, they're basically saying, here's a random value. Trust me that it's random, and you just have to trust them. And what their source of entropy can be up to each participant. So usually it might be cspring or something running on their computer, sampling times probably from their cpu. It's sort of funny, like people can get crazy about this. You mentioned the lava lamp situation in Cloudflare, which is a funny one, but there's an actual device you can buy and plug into your computer that's like a Geiger counter, sampling the background radiation of the universe.
00:17:56.622 - 00:18:29.694, Speaker B: And that's probably the best source of randomness that you can get. But yeah, in the commit reveal scheme I think it's pretty obvious. But to me it's not at all clear. Like in a threshold cryptography scenario, what is the source of entropy? Because there's some Message that's sent out and then everyone just kind of signs off on that, and then the threshold is passed and some signature gets forwarded onward, which is used as the random number. But what is the input to all of this?
00:18:29.812 - 00:19:20.666, Speaker C: So the input is still random numbers generated on people's computer. I guess the difference is that there's a so called secret sharing scheme whereby with these local pieces of randomness, you can generate a shared secret, which is common to everyone, but which has the property that no one knows it. In order to redeve it, you need a certain threshold of participants to kind of collaborate and reconstruct it. So it's a way of basically having a secret which no one knows. It just exists, but it's unknowable and it's split into tiny slices, and each slice is given to a different validator. So it's somewhat similar to threshold signatures.
00:19:20.778 - 00:19:22.510, Speaker A: Sounds a lot like an MPC.
00:19:23.490 - 00:19:53.514, Speaker C: It is an MPC, yeah. So an MPC basically allows for every participant in the NPC to have a piece of secret. And then you want to compute a function basically by preserving the randomness. And the function will be a mixing function which combines all the secrets. And it has another property, which is the threshold property, which is that you can recover the secret if some fraction, for example, one half, will collaborate and.
00:19:53.552 - 00:20:32.230, Speaker A: Reconstruct it for listeners who haven't heard the episode we did on npcs. Npcs are, it's short for multiparty computation. And we'll link to the episode on that if you want to hear a little bit about it. But as I understood it, the inputs are supposed to be random. You're using an NPC to make sure that even if there's like a small proportion of the people or the participants who are malicious and potentially not giving real random numbers or having some coordinated number, even if they did that using an NPC, the system would still be secure.
00:20:32.650 - 00:21:27.906, Speaker C: Yes. So it's actually not too much of a problem if you have a bunch of people who just pick zero, pick a totally non random number, so long as you have one single person who picks a random number, then the shared secret will be random. The bigger problem with the threshold cryptography is that if you have a large enough attacker who can basically go above the threshold, then they can compute ahead of time all the random numbers they can just go through the reconstruction process and predict the future. The other problem with the threshold cryptography is that is a so called liveness problem. So if you have so many people who are offline that you can't even reach the threshold, then you can't generate the next random number and your randomness beacon will stall.
00:21:28.018 - 00:21:45.950, Speaker A: Just thinking before we dig deeper into the. Because we wanted to talk about the random beacon as like a separate entity here, I think I'm still trying to get to the, basically the question I'm trying to come to is why do we hear about randomness using a block header as the input?
00:21:47.170 - 00:21:53.310, Speaker B: That's sort of what I was talking about before with using the hash of the block, okay, of the previous block.
00:21:54.930 - 00:21:57.460, Speaker A: That is actually the block header. Got it.
00:21:58.310 - 00:22:31.206, Speaker C: But there's two ways to use the block headers. One is in the proof of work. And basically the block header will encapsulate the proof of work kind of result of the puzzle, the nonce. And that nonce will have entropy. And so the overhaul block header block hash will have entropy. But in a proof of stake system it's free to come up with tons and tons of block hashes because you don't have to do the work. So you can just shuffle around the transactions and then you'll get different block hashes.
00:22:31.206 - 00:22:35.786, Speaker C: And so in proof of stake, using the block hashes, that's a completely broken solution.
00:22:35.898 - 00:23:00.822, Speaker B: I think that leads us pretty well into my next question, which is what do we use randomness for in proof of stake? Like you say, it's not to create the next block hash. It doesn't work the same way as proof of work. So what do we use it for? And is there a way to bring the randomness from the consensus layer into the application layer as well?
00:23:00.956 - 00:24:00.774, Speaker C: So in proof of stake, you use the randomness for the same purpose as proof of work, which is basically to select a random lock proposer in a fair fashion. So by fairness we mean that your probability of being selected is proportional to some sort of economic resource, and you can go beyond that. So there's the idea of sampling committees basically trying to sample the total set of consensus participants, which generally we call validators, and only take a much smaller committee which is representative of the wider pool and assign them a specific task. And this is how you get scalability with sharding, for example. So in the traditional model Ethereum 1.0, every node will go through all the blocks and have to process everything which is unscalable. And in the Ethereum 2.0
00:24:00.774 - 00:24:48.694, Speaker C: model. What we want to do is we want to find a smart way of basically splitting up the total validator pool into so called committees and assigning one committee per shard so that you don't have this wasteful redundancy. Instead, you're kind of cleverly spreading your security. Well, spreading the validators across the shard. And the idea is that if you have enough honest validators in the global pool, then this property will be preserved in the committees, because the committees are meant to be statistically representative of the global pool. And in order to have this statistical representation, you need to have randomness. And this is where the randomness beacon comes in.
00:24:48.694 - 00:24:55.974, Speaker C: It's the seed to a so called shuffling function, which will allow you to sample committees.
00:24:56.022 - 00:25:10.206, Speaker A: So from what you've just said, as I understand it, randomness is used in Pos for choosing the validator, building a committee, and potentially building a committee on the shards. Or did I misunderstand that?
00:25:10.308 - 00:25:12.810, Speaker B: A committee of validators for each shard?
00:25:12.890 - 00:25:13.146, Speaker C: Yeah.
00:25:13.188 - 00:25:18.926, Speaker A: Okay. Is there any other uses for randomness in proof of stake?
00:25:19.038 - 00:26:07.618, Speaker C: So at the consensus layer, no, that's the main application, just somehow sampling validators. And it could be just a single validator, or it could be a whole committee of validators. But one thing that is totally possible is to expose the randomness that it generated at the consensus layer to the application layer, and then that application layer can use it for all sorts of things. So the most obvious application is gambling and lotteries and games, but you can do other things. So one thing that you can do is that you can start building proof of stake systems on top of proof of stake systems. So for example, if you like filecoin, and that's a nice idea of being able to have decentralized storage. And you want to recreate such a system on top of Ethereum 2.0.
00:26:07.618 - 00:27:24.630, Speaker C: Well, having an unbiasable source of randomness will be a huge help to do that, but it could be used for other things at the application layer. So one kind of use case which fits in quite well with this podcast is in the context of stocks. So stocks are quite large, zero knowledge proofs. And one of the reasons that they're large is because you have the so called fiat shamir heuristic, where you try and reduce an interactive game, which is like challenge response to something non interactive, and you kind of derive randomness from the interactions between the challenger and the responder. What this allows you to do as an attacker is basically try and simulate locally on your computer tons and tons of these interactions until you find one interaction which kind of breaks, which creates a proof which is wrong. And so in order to counterbalance this, you kind of have to increase various security parameters. And so you have larger stocks if you don't rely on the fiat heuristic.
00:27:24.630 - 00:27:42.000, Speaker C: And instead you have a zero knowledge proof, which is more interactive and kind of at each round, you resample fresh, unbiaseable randomness. Then you're able to reduce the size of these stocks, maybe reduce them by a factor of four, something like that.
00:27:42.610 - 00:27:50.702, Speaker B: So then, yeah, the stark would essentially use that on chain randomness generated by the beacon chain.
00:27:50.846 - 00:28:26.698, Speaker C: Right. So someone who wants to kind of prove a statement would get a challenge, which depends on the randomness, and then the proof would kind of be broken down in time in much smaller chunk, where the aggregate communication is smaller than if they were to do it in one go in a non interactive way. But because they're interacting with a blockchain, which is kind of this trustless entity, which doesn't really is not a traditional counterparty, you can still think of it as a non interactive zero knowledge proof.
00:28:26.714 - 00:29:03.466, Speaker B: In a way, it just struck me that having proper on chain randomness, and I mean, this isn't necessarily tied to proof of work or stake or anything, but if you have a huge lottery on chain and someone loses a ton of money, they're basically incentivized to fork the chain before that happened. If it was a proof of stake system, they could put up a lot of stake previous to the loss, and they just continue there. But it's interesting we should explain how.
00:29:03.488 - 00:29:22.798, Speaker A: This lottery thing works, because we didn't really, I think we've mentioned it in previous episodes, but as I understand it, the idea here is that if you could predict what a block number coming up is, or if you see a block number before it happens or something, you could place the bet on that number happening and then make the money from it.
00:29:22.964 - 00:29:44.200, Speaker B: Well, you could do that too. I mean, that's the simplest possible attack. Like a miner can place a transaction. They could front run the lottery, basically by entering into the lottery in the same block that they're winning it. Yeah, but that's like an incredibly dumb design of a lottery. And if the smart contract author just shouldn't be doing that.
00:29:44.650 - 00:29:48.966, Speaker A: So what do you mean then what kind of attack is the lottery you described?
00:29:49.078 - 00:30:17.438, Speaker B: So basically, if they place a bet, then if the source of randomness is manipulable, they might not be able to directly say, like, I'm definitely going to win here and place a bet. But over time, they are more likely to win than lose. So in a repeated game, they'll just keep losing, keep losing and winning, and then over time they'll win more than they lose because they have some influence over the randomness.
00:30:17.534 - 00:30:50.186, Speaker C: Right. So there's kind of two attacks on lotteries. One is where your source of randomness is manipulable because you're using, for example, a commit reveal scheme. You really shouldn't be doing that, because even an attacker with a tiny bit of influence, let's say 0.1 bit, only 10% of the time, they'd be able to kind of reroll the die. That's very, very bad, because basically what that gives them is an edge. So if they have 0.1
00:30:50.186 - 00:32:17.974, Speaker C: bit, then that gives them a 5% edge if they possibate an equal amount to what is currently in the pot. So if you have a $1 million notchery and they participate with another million dollars, well, normally they should win half of the time, but instead they'll have an edge over everyone else, and so they'll be able to basically steal from the general public. The other attack that Frederick mentioned is kind of interesting, which is, well, why didn't you fork the blockchain? And indeed, if you have a blockchain where the economic value is much larger than the economic value backing the kind of the finality of the chain, then that could happen. One of the things that you want to try and do is avoid soft finality. So you want to try and make sure that all the bets kind of have been placed and some sort of initial seed has been committed. And then after this kind of initial phase, there's a very strong finalization process. And then after that you will try and deterministically extract the randomness from this seed, which has been finalized at some point in the future in such a way that no one could have predicted what this extracted random number is.
00:32:17.974 - 00:33:08.486, Speaker C: And so there's basically two ways of doing it. One is using the MPC approach, where you basically have this secret sharing protocol where you split the secret, and then what you do is that you commit to the shares that have been split on chain, and then you have this finalization process which has much more economic value at stake than the lottery. And then after the finalization process, you recombine the shares and then you get the random number. And then the other approach is basically whereby you have a so called VDF verifiable delay function. And the idea there is that using computation using computation that is inherently sequential. You can have a function which basically takes an input and produces a unique output. So it's unique and deterministic.
00:33:08.486 - 00:33:48.790, Speaker C: But the process of computing this function takes a minimum guaranteed amount of time. So let's say at least 1 hour. And so what you can do is you can commit to the input of that function, run through the finality process within an hour, and then go through this process of extracting the output of the VDF. You're guaranteed that anyone who tried to extract the output of the VDF will only be able to do it after the finalization process, and hence they cannot bias the input that happen pre finalization.
00:33:49.130 - 00:34:37.510, Speaker B: Let's get back to vrfs and vdfs in a bit, because I want to sort of tease that a bit, talk about it a little bit, but I think we should do a whole episode on that. I think those are super interesting topics. But right now I wanted to get back a little bit to this idea of a beacon chain. So we've mentioned it a couple of times already. I think this is a term that has sort of evolved over time, and I'm not actually sure. I think in Ethereum, the beacon chain is sort of the same as in Ethereum too, the same as the kind of relay chain or whatever you want to call it. But I've seen designs before of like a beacon chain being a completely separate thing, that its only purpose is to output random numbers.
00:34:37.510 - 00:34:51.510, Speaker B: It's just like a stream of random numbers that you can subscribe to over the network, not really a blockchain at all. So how do you think about beacon chain? What's the definition to you and what's its purpose?
00:34:51.670 - 00:35:38.706, Speaker C: Yeah, so it's a bit of accident of research that happened. So when I was studying randomness, as you say, I found all these so called randomness beacons, which are you can think of as pure sources of randomness. And I wanted to try and expand that concept and have a chain whose main purpose was to generate random numbers. And I coined the term beacon chain. And then people just started using the term for something other than what I intended. They started using it for not just the randomness, but also the other stuff. The reason is that you can't really have a pure randomness beacon in a decentralized context.
00:35:38.706 - 00:36:38.586, Speaker C: You need to have, at a minimum, some sort of registry of, at least in the proof of stake concept, some sort of registry of who will be participating in the randomness beacon. So you need to have a mechanism to maintain that registry, and then you need to have a deposit mechanism so that you can register as a validator, and you need to have mechanism to exit, and then you might have rewards to incentivize participation in the randomness beacon. And then you start adding all these things, and at the end of the day, you start recreating what I think would be a better term, a system chain. So it's basically the core shared infrastructure for all the shards which provide various system services. And those services are randomness, finality, maintenance of the set of validators, processing rewards and penalties, including slashing conditions and all of that stuff.
00:36:38.688 - 00:36:44.138, Speaker A: So your original idea was to try to have a random beacon that sort of sat alone?
00:36:44.314 - 00:37:22.426, Speaker C: Yes. So initially we were thinking of having a so called sharding manager contract. So a lot of the heavy lifting at the system level was done in a contract within Ethereum 1.0. But even though that worked great for things like deposits and maintaining a validator registry, it wasn't adequate for the purposes of randomness. So I was thinking of basically building this external thing called the randomness beacon, whose main purpose was randomness. And then this whole thing snowballed. And then we eventually realized that we want to put nothing in 1.0.
00:37:22.426 - 00:37:48.322, Speaker C: We want to kind of start with a clean slate and put all the system stuff in this external chain. And I think the term is somewhat unfortunate because it gets cryptographers really confused because they're used to random netbeakans and they don't understand. But the general public is fine. They just think of some sort of lighthouse that points the way forward for the rest of the system, and that's perfectly fine.
00:37:48.456 - 00:38:06.182, Speaker A: I'm still curious, though. I understand sort of this randomness beacon. I understand that it evolved, and now you've referred to the beacon chain, but is the beacon chain just ETH 2.0? Does it live somewhere, or is it. The entire thing is a beacon chain now.
00:38:06.316 - 00:38:28.366, Speaker C: Now we kind of have three types of chains. So we have the kind of so called legacy Ethereum 1.0 chain, which is what exists right now. And then we have this totally separate parallel universe, which is also a blockchain, which is the beacon chain. And that's one of the components of the wider ethereum 2.0 system. And the wider ethereum 2.0
00:38:28.366 - 00:38:52.486, Speaker C: system, in addition to the beacon chain, has shards, specifically 1024 shards. And the shards, you can think of them as user level blockchains, like application level blockchains. So it has the user transactions, it has the user state, it has user land blocks. But the beacon chain is all about system stuff. So it has system transactions, system state.
00:38:52.588 - 00:38:57.880, Speaker A: System blocks, and it sounds like it goes way past randomness now.
00:38:58.490 - 00:39:36.470, Speaker C: Yes, it basically does everything and anything that is relevant to support the shard. So you can kind of think of it a little bit like an operating system. So on your computer you have an operating system, and then you have applications that are built on top of the operating system. And the operating system will provide basic functionality. So you can, I don't know, read from memory, write to memory. And one of the basic functionalities that is exposed by the operating system is randomness. And so in a way we're just replicating this thing in a decentralized context.
00:39:37.530 - 00:39:53.142, Speaker A: Is that so? Going back to just the more general definition of randomness in computing, I'm just curious if there's any examples you could give of how randomness actually exists in sort of the usual OS.
00:39:53.286 - 00:40:34.562, Speaker C: So I think the usual OS would basically have some sort of API to various external sources in its environment. So it might read the temperature, it might look at the clock and go down to the nanosecond, it might read user input from the mouse or from the keyboard, or it might use random noise from the microphone. And then it will combine all these things into a very easy to use API for the application layer. And at the application layer you use randomness all the time. So I know if you're doing some sort of simulation, then you would use randomness.
00:40:34.706 - 00:40:41.850, Speaker A: I see. But it's still like feeding a random number into something else that would use that randomness.
00:40:42.830 - 00:41:13.582, Speaker B: The operating system, it depends on which operating system you're talking about. But most operating systems have some sort of built in way to get randomness. Like a developer doesn't really need to care. They just call this random function and it gives them a random number. They don't know where it comes from. And it's the operating system level that does all the interacting with the hardware of all the things that Justin were talking about. That's essentially like in your initial definition, you said the computer reads from its environments.
00:41:13.582 - 00:41:36.474, Speaker B: That's what it is. It's reading temperature, clocks, noise in general in the system to try to generate a random number. In the analogy, like the system chain creates this randomness, it is the source of this randomness and the user level change. They don't really need to care where it's coming from. It's just, I have a random number.
00:41:36.512 - 00:41:59.598, Speaker A: Right here, I guess. But the question I had a little bit was, in the more traditional OS, is there any need for using the randomness itself, in its processing word maybe, is wrong here. Processing is probably not right, but it's like in its own operation, does it use randomness the way that the beacon chain would still be using its own randomness to define the validators?
00:41:59.774 - 00:43:00.598, Speaker C: So you're asking, does the OS use its own randomness for various things? And almost as a lady answer is yes. And the reason is that when you think of it very abstractly in terms of algorithms, then there's two classes of algorithms. There's like deterministic algorithms, and then there's randomized algorithms. And it turns out that the second one is strictly greater than the other one, like strictly more powerful. And so you might have a sorting algorithm which is randomized, or you might have a caching algorithm which is randomized. You do random invictions or something like that. And it turns out that if you want to try and smooth things out, or try and avoid having any particular bias which may come in with the deterministic algorithms, you can use randomness to have a superior algorithm for the job.
00:43:00.764 - 00:43:23.680, Speaker B: So I want to go back actually to a thing that you mentioned. You were talking about these three ways of generating randomness in a sort of proof of stake environment. One is a typical commit reveal, the other one is threshold cryptography. And the last one you mentioned was commit reveal extract. This is actually something that's new to me, and I'm not exactly sure how that works. Maybe you can explain that a little bit.
00:43:24.370 - 00:45:20.866, Speaker C: Sure. So the idea is to take a commit reveal scheme, which is great on all properties except one, which is that it's manipulable, and try and upgrade it somehow by adding this extraction step to try and fix the manipulation aspect. The idea there is that you want to try and use a so called VDF verifiable delay function to add a minimum guaranteed amount of time to extract a number which looks random given any input. So you give it any input and it will take some amount of time, let's say 1 hour, and after 1 hour you have a number which looks totally random if you make sure that the reveal phase lasts less than the delay. So in our case, the reveal phase lasts less than 1 hour, then any kind of bias that you want to try and introduce in the reveal phase by not revealing, well, that is not going to be meaningful in the sense that you will be biasing the randomness in the blind. So you will affect what the final random number will be, but you won't be able to affect it in a meaningful way in the sense that you won't be able to know if you're affecting it in a positive or negative way to you, you'll just be resampling a new number. So you can kind of think of it maybe as a big jar with lots of numbers that are kind of written on pieces of paper, and the piece of paper are folded, and you put your hand in and then you mix, and then you say, okay, I'm going to pick this number, but you don't open it.
00:45:20.866 - 00:45:39.538, Speaker C: And then you take it out and then you say, oh, actually, no, I've changed my mind. I'm going to pick another one. So you put the number back in and then you pick another one. So here you have changed the final outcome, but it was in the blind, so you have no way of actually meaningfully manipulating the final outcome.
00:45:39.634 - 00:45:49.180, Speaker A: I wonder, does this have a little bit to do with the idea of mixing random numbers? Are you combining a random number with another random number here?
00:45:49.890 - 00:45:50.398, Speaker C: Yes.
00:45:50.484 - 00:46:13.220, Speaker A: Yeah. As I've understood it, if you mix a sort of highly unpredictable number random number with a not quite as secure number, you end up with the resulting number will have a highly unpredictable value. So it will actually take the more random of the two and will have that quality.
00:46:13.910 - 00:46:15.202, Speaker C: That's exactly right.
00:46:15.336 - 00:46:21.446, Speaker B: Basically, if you combine a bunch of random numbers, it's not a weakest link property, it's the strongest link property.
00:46:21.548 - 00:46:57.966, Speaker C: Yeah, that's exactly right. And so that's why the commit reveal schemes. They kind of have this minority honesty assumption. You just need one single person in the group of participants to be honest, and they will pick an unpredictable number. And so the combined mix will be totally unpredictable. And so the idea of commit reveal scheme is that you have this commit reveal phase, which generates this mix, which is unpredictable, and then you have this extraction phase on top of it to make it unmanipulable, in addition to being unpredictable.
00:46:58.158 - 00:47:44.802, Speaker B: So going back to the lottery example, I guess, the commit rule extract, everyone commits. You have the last guy in the normal model. He can say, if I commit my value to be this, the outcome will be that he has the total privilege of seeing what the outcome of all of this will be, and so can decide if he wants to put in his value or not. But if you feed all of this into a VDF, and then the outcome of the lottery is decided before the outcome of the VDF, then basically he doesn't know until the outcome. So like you said, he doesn't know if he should commit his thing or not because he doesn't know what it is until this VDF is done.
00:47:44.936 - 00:48:45.878, Speaker C: Yeah, that's exactly right. So in the reveal phase, which is where the bias comes in, because one thing that maybe I should make explicit is that the commit reveal scheme only allows you to reveal the single number that you've committed to. You have no other option than either revealing what you've committed to or not revealing at all. So you have so called one bit of attack surface. When you are an attacker, and let's say you control some fraction of the participants in the commit reveal scheme, you'll have some amount of attack surface, but this amount of attack surface will be tiny relative to the unpredictable entropy which is added by the one single honest participant. And then this unpredictable entropy will overwhelm the extraction process. So it's basically impossible to brute force the extraction process.
00:48:45.878 - 00:48:49.490, Speaker C: You have to go through this slow computation phase.
00:48:49.570 - 00:49:07.274, Speaker B: So we have these three models, commit, reveal, commit, reveal, extract, and threshold. What's the attack model for these three? Like, if I wanted to break the randomness and actually be able to decide what the random number is, what do I have to do?
00:49:07.472 - 00:49:56.742, Speaker C: For commit reveal, you have a minority honesty assumption, so you just need one single person to participate and you'll have this key property, which is unpredictability. Now what happens is that there's kind of a spectrum for manipulation. So if you have everyone is honest, then no one will try and manipulate and you'll have a great random number. But the more attackers you have. So basically, the lower your honesty assumption is, the greater the surface for manipulation. In the threshold cryptography, it's much more clear cut. Instead of having this gradient, this spectrum, it either works perfectly or is completely broken in the context of working perfectly.
00:49:56.742 - 00:50:43.178, Speaker C: Basically you want to make sure that you have sufficiently many people who are honest and online at any given point in time. If you don't have this threshold, then there's two ways in which the randomness beacon fails in very dramatic ways. Way number one is you don't reach the threshold at all. And so that's a liveness issue and you can't even generate the next random number. And that's really problematic, for example, on the proof of stake system, because you need to decide who's going to be the next person to create a block. And if you don't have that, then your blockchain will stall. And that's, for example, what definity has right now.
00:50:43.344 - 00:50:56.640, Speaker B: And there's no way to get out of this, right? Like you can't reshuffle anything because you don't have a random number to reshuffle. What do you do? Do you just have to wait until enough people come online?
00:50:57.250 - 00:51:56.674, Speaker C: So you have various options, you can just wait it out and see if people come back online. Another option would be to just fork at the social layer, the blockchain, and try and restart with some new parameters, with new set of validators. Another option is to try and have some sort of hybrid model where in the default case you will use threshold cryptography, but your blockchain has stored for a whole hour. You can try and have a fallback mechanism with commit reveal. The problem with these fallback mechanisms is that suddenly they introduce manipulation. And the reason is that what you can do is if you're an attacker that's large enough, you will have kind of sufficient votes to go above the threshold. But if you don't participate, you go below the threshold and so you can privately know what the next random number is.
00:51:56.674 - 00:52:06.966, Speaker C: And if you don't like it, then you just wait it out and trigger the fallback mechanism. And that will give you basically another chance at getting another random number.
00:52:07.148 - 00:52:25.500, Speaker B: Yeah, so it doesn't just introduce manipulability of the fallback mechanism. Like if the fallback mechanism is commit real, then we have manipulability there, but also like at the higher level where you can choose if you want to fall back or not, which is interesting.
00:52:26.190 - 00:52:35.790, Speaker C: Exactly. And even forking out at the social layer, that's also a fallback mechanism and also a surface of manipulation.
00:52:36.130 - 00:52:42.720, Speaker A: Is there any difference in the attack surface between the commit reveal and commit reveal extract, or is it the same?
00:52:43.030 - 00:53:02.434, Speaker C: So the commit reveal extract is kind of perfect in the sense that it's totally unpredictable, totally unmanipulable, and has strong liveness in the context of minority honesty assumption.
00:53:02.562 - 00:53:12.540, Speaker B: But it depends on vdfs, which is a complicated topic and we don't really know yet if it has its own attack vectors or not.
00:53:13.390 - 00:53:23.950, Speaker C: Right? I mean, vdfs definitely have their own attack vectors, and there's a bunch of standard assumptions that are made, cryptographic assumptions, but there's also new assumptions which relate to hardware.
00:53:26.370 - 00:54:03.258, Speaker B: Which are. That's the part I'm alluding to. Like if you design a VDF as an ASIC, is your ASIC optimal or not? And that sort of introduces some questions. But maybe we can, we're at time, so we need to wrap up, unfortunately. But I want to do an episode in the future with you, Justin, on vrfs, vdfs, all these things. Maybe we can just give a quick teaser and primer, like what is a VDF high level. What is a VRF, and are they related or not?
00:54:03.424 - 00:54:58.938, Speaker C: So VRF is actually easy, and we've kind of used them in the commit reveal scheme. So basically you want to have a way of generating randomness locally and then presenting it to the blockchain in a verifiable fashion. And so one very easy way to do that is first you commit to a secret with a hash, and then you reveal the hash, and anyone can verify that the secret that you revealed matches the hash. And you can use other functions. So for example, one which we use in Ethereum 2.0 is the idea of using a BLS signature. So the commitment will be registering a public key, and then the reveal will be a signature from that public key.
00:54:58.938 - 00:56:01.214, Speaker C: And so once you reveal a signature, anyone can verify that the signature matches the public key. So a PRF is nothing fancy. We use them all the time. What's much more kind of interesting is the VDF, because that's a new cryptographic primitive. And the idea there is to try and have, in a totally trustless and decentralized fashion, have a notion of minimum guaranteed delay. And by delay I mean physical time delay. So you have a guarantee that a whole hour has passed, and not only has a whole hour passed, but also you have a mechanism of taking an input and producing an output at the end of this 1 hour, where the output is unique and deterministic and looks like a random number.
00:56:01.214 - 00:57:15.560, Speaker C: And the way that these vdfs are implemented in the real world is using hardware. So you have this notion of sequential computations, where every step of the computation depends on the previous step. And so you can't parallelize things, you have to do them one after the other. And so there's no way of kind of cheating the system with data centers full of hardware. And the new kind of exotic assumption that you have with the hardware is that an attacker is not much faster, not too much faster than you are. So, for example, not more than 100 times faster than you are. And so the exciting thing that we're doing at the FM foundation, in collaboration with all sorts of other blockchain projects, is to try and build state of the art hardware that will do this sequential computation and have extremely high confidence that it's effectively impossible for an attacker to build equivalent hardware, which is a hundred times faster than what we're going to build.
00:57:18.570 - 00:57:24.802, Speaker A: So, listen, thanks so much for coming on the podcast, Justin, and helping us to explore randomness.
00:57:24.946 - 00:57:27.734, Speaker C: Yeah, no worries. It was fun. Thank you for having me.
00:57:27.772 - 00:57:30.986, Speaker B: Thank you very much. And to our listeners, thanks for listening.
00:57:31.098 - 00:57:31.770, Speaker A: Thanks for listening.
