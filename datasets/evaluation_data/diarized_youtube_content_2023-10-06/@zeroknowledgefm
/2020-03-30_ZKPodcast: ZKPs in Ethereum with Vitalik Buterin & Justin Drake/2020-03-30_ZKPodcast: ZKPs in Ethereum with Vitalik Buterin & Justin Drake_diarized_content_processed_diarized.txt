00:00:07.930 - 00:00:33.190, Speaker A: Welcome to Zero Knowledge, a podcast where we talk about the latest in zero knowledge research and the decentralized web. The show is hosted by me, Anna, and me, Frederick. So today we're sitting with Justin and Vitalik from the Ethereum foundation. Welcome to the show, guys.
00:00:33.260 - 00:00:33.590, Speaker B: Hi.
00:00:33.660 - 00:00:34.966, Speaker C: Thank you. It's good to be here.
00:00:35.068 - 00:00:45.930, Speaker A: First off, I'd say we should do some intros. Justin, you've already been on the show. You came on to do an episode about randomness, quite different from, I think, what we're going to talk about today. But welcome back.
00:00:46.000 - 00:00:46.620, Speaker B: Thanks.
00:00:47.230 - 00:00:50.542, Speaker A: Even though you've already been on the show, do you want to just quickly say who you are?
00:00:50.676 - 00:01:13.694, Speaker B: Yeah. So I'm Justin Drake. I've been at the Ethereum foundation since late 2017, and I've mostly been doing research on ethereum 2.0. But I guess now that the research for phase zero and phase one is to a very large extent done, I've started to branch out a little bit to zero knowledge proofs.
00:01:13.822 - 00:01:16.998, Speaker A: Cool. And Vitalik, this is the first time you're joining us.
00:01:17.084 - 00:01:45.198, Speaker C: Yeah. So I've been part of the Ethereum foundation since 2014. I've started doing NF ETH one things and then have been doing NF ETH two related research and many other directions of research the whole time. And I guess, yeah, recently, with phase zero and phase one stuff, kind of starting to get finished on the research side, I've been branching off into different things too.
00:01:45.284 - 00:01:49.840, Speaker A: Yeah, I was kind of curious, what do you work on right now? What's your focus?
00:01:50.210 - 00:02:32.640, Speaker C: I kind of jump between a few different things in the very recent time. I've been thinking about the thing that I presented at SPC yesterday, which was kind of ways of moving beyond 51% attacks and recovering from 51% attacks and trying to just make sure that the network can kind of coordinate on what to do in those kinds of situations. Also economic mechanism of things. So like following the Gitcoin Grants quadratic fund, which I think you received quite a bit of money from last round, and just kind of analyzing the economics of that and some other things before that.
00:02:33.650 - 00:02:56.020, Speaker D: So on the topic of the name of the show, I'm curious to hear your guys'first introduction to zero knowledge proofs. What was that? Do you remember when you first came across them? Was it that you immediately got the impact of that and started thinking about applications, or was it just the thing that went by?
00:02:56.390 - 00:04:05.350, Speaker C: I remember hearing about them in 2013. It was either at or around the time of the bitcoin conference in San Jose then and this was back when zero coin, this was the older and more complicated protocol that had 50 kilobyte transactions that used accumulators and log sized ore proofs and other couple x things was published. And I just remember just being really excited that such a thing is even possible. And then soon after that, of course, there was the zero cash proposal that was published, and this concept of what back then was called kind of scip and secure computational integrity and privacy. And the concept was just kind of, obviously, to me, kind of very mind blowing and revolutionary. And so I was doing this kind of traveling between bitcoin communities around the world at the time. And a few months later, I was in Israel, and I went over to the Technion in Haifa, and I interviewed Ellie back then when I was doing bitcoin magazine.
00:04:05.350 - 00:04:08.666, Speaker C: So I've been kind of excited about them ever since.
00:04:08.848 - 00:04:18.830, Speaker A: When you first learned about it, did you go back and do all the research, kind of back into the early papers, or did you really come in when it was more connected with blockchain?
00:04:19.490 - 00:04:40.182, Speaker C: I definitely came in when it was more connected with blockchains, and I knew about kind of the existence of some of those earlier protocols, but they just kind of seemed very kind of complicated. And just the fact that a lot of them weren't succinct in general purpose just meant to me that there was nothing exciting about them.
00:04:40.316 - 00:04:43.494, Speaker D: What about you, Justin? What was your first impression of them?
00:04:43.692 - 00:05:31.510, Speaker B: So I think it was maybe 2015, around that time, zero cash, I had this company building on top of bitcoin and open bazaar. So it was something that felt like dark magic that I was very curious about, but I didn't have the time to learn about them. But my company wasn't doing too well in 2017, and so I kind of decided to take a break and to relax. I wanted to learn about these zero knowledge proofs. So I started writing a blog post, which I published in June 2017, which was about using zero knowledge proofs for scalability. And back then, no one was talking about it. It was all about privacy.
00:05:31.510 - 00:06:08.660, Speaker B: And I happened to be very, very lucky because I emailed Ellie, like, sent a cold email asking for review of this blog post. And he gave me feedback, but he also invited me to the technion. So I went to the Techneon, and I remember when it was the speaker's dinner, and I was not a speaker, but for some reason, Eddie was like, hey, why don't you come? He took a leap of faith on this random guy who I was back then, and I remember the dinner where there was Ellie, there was Zuko, there was Vitalik, and the imposter in the room was me.
00:06:11.590 - 00:06:18.150, Speaker A: Were you the first person to write about that? Are you like this idea of the scaling the zero knowledge proof with scaling?
00:06:19.130 - 00:06:27.846, Speaker B: I think it was such an obvious idea that many people didn't even bother writing it. But as far as I can tell, I was maybe one of the first.
00:06:27.948 - 00:06:35.750, Speaker A: That's cool, because really it was a privacy. Like, the whole idea of zero knowledge proofs, even like the original inception, was very privacy focused.
00:06:35.910 - 00:06:46.490, Speaker B: Right? And I mean, it's natural that privacy is the first application because you have much smaller circuits when you want to do scalability. Then suddenly your circuits blow up and your prover time becomes unmanageable.
00:06:46.650 - 00:06:49.680, Speaker A: And so that's around the time that you joined the.
00:06:53.010 - 00:07:30.838, Speaker B: I mean, I remember having a quick chat with Vitalik. What happened is that I was so excited about zero noise proofs, but then Vitalik, like poured a lot of cold water and he said, but what about the data availability problem? So I went back home and I spent like several weeks thinking about the data availability problem. And after a few weeks I had these ideas, which I think, but at the time, not many people were talking about, namely around RSI accumulators. And so I emailed Vitalik and we had this back and forth and like a few weeks later he offered to join the foundation.
00:07:30.934 - 00:07:32.354, Speaker A: Do you remember that, Vitalik?
00:07:32.422 - 00:07:41.546, Speaker C: Yeah, I mean, in my defense, at the time, half the community was denying that the data availability problem existed, right? We both made progress.
00:07:41.658 - 00:07:42.174, Speaker B: Yeah.
00:07:42.292 - 00:08:27.040, Speaker A: So one of the reasons that I really wanted to do an interview with you guys was that I started to notice that the Ethereum foundation was sort of snatching up some really good zero knowledge proof researchers. And yet, and I think we've actually mentioned this a couple times in previous episodes, we aren't completely clear as to why. Like, what are the researchers doing in Ethereum and what are they working on? Kind of at what levels, at what layers. But maybe before we go into that, I think what would be really interesting for us to understand is a little bit like, what is eth research? Or what is the Ethereum research team? What do you guys do generally? And then maybe we can figure out where the zero knowledge proofs fit.
00:08:27.650 - 00:09:19.600, Speaker B: Mean maybe Vitalik should talk about the prehistory. But from my point of view, in the early days, a lot of the research was done by a very small team, possibly to a large extent, just one person. I mean, it's not completely true, right? There was people like Vlad and Gavin. But when I joined, I think Danny had just joined and the research team was surprisingly small. I was shocked at how small the project was. You had this multibillion dollar project and we were going to upgrade e 20 and there was like three people doing research, as in the theoretical research. We also have this other part of the research team, which is all around prototyping, especially implementing the ideas in python and seeing how that works out.
00:09:19.600 - 00:09:41.990, Speaker B: But now I think we've taken a different philosophy where we try and be proactive on hiring. So I've hired a bunch of people. I think Danny has also made some hires, and now we try to build whole team, so, like a whole team around cryptography and zero knowledge.
00:09:44.010 - 00:09:55.414, Speaker A: When I say e three search, I realize I'mixing it up because there's a website, there's a resource called e three search. This is the Ethereum foundation research team. So what did it look like? Or what's the history of it from your perspective?
00:09:55.462 - 00:11:00.110, Speaker C: Vitalik yeah, so maybe 2014 through 2017 or so. It was myself and Vlad mostly, and Gavin for the first year as well. And then Gavin kind of went off to do parity, which then went off to do polka dot. I think the main focuses kind of from the beginning were kind of primarily proof of stake and charting. These are kind of the things that we said to the community that we would do even way back in 2014, kind of as the answers for, well, what about the blockchain? And of burning a lot of energy. What about the blockchain not being scalable? And we said that we would figure out these things, we had ideas, but we would improve on them. And there was this long period of time, and it was even longer because both myself and Vlad kind of came in not really understanding byzantine fault tolerance theory, there not being good explanations of any of these BFT algorithms, not understanding a lot of other things in protocol design.
00:11:00.110 - 00:12:02.818, Speaker C: But then we learned, and then over time, Vlad started his CBC direction. And then I have started work with what I called minimal swashing conditions, which eventually turned into Casper FFG. So at the beginning, it was this kind of small, motley group of people that were kind of just continually working on these things while jumping between whatever other things that they were doing. And then at some point, I think in 2017, we started to realize that zero knowledge is something that we need to take much more seriously. There was this event at IC three. This is these annual boot camps that we have in the summer. And the one in, I think it was either 2016 or 2017 Zcash people were there and we were there, and I had a chance to ask some of them a bunch of questions about how AZk snarks worked, which was really nice.
00:12:02.818 - 00:12:45.578, Speaker C: That was when the idea for including the elliptic curve pairing pre compiles into Ethereum was kind of born. And there was also this prototype that kind of quickly made a pre compile and then built an application on it. This was kind of Zoe zcash on Ethereum, and a year later that became an actual thing. There was the Byzantium hard fork that added that pre compile and people kind of started experimenting and building on it. And I think just even that being real and just there being the real possibility of building zero knowledge proof based applications on Ethereum just kind of really galvanized a lot of attention from everyone.
00:12:45.764 - 00:12:51.262, Speaker A: And so that's kind of where this idea came, maybe originally, to start focusing a little bit more on zero knowledge proofs.
00:12:51.326 - 00:12:51.746, Speaker C: Yes.
00:12:51.848 - 00:13:18.554, Speaker D: I'm curious what you see as sort of the mandate of the foundation, because some of the research, as you say, is very focused. Like if you look at proof of stake research and consensus research, it's very much about solving a problem that exists on the chain today. But zero knowledge research starts moving more in the direction of fundamental research. Like we're just researching for the sake of maybe we'll discover something interesting.
00:13:18.752 - 00:13:57.602, Speaker C: Yeah, I think at the beginning it was more kind of application oriented. It's like, hey, we want to have an internal zero knowledge competency so that we can do things like building at least the core of, say, anonymous voting systems or just zero knowledge proof verifiers that people can put into their applications more easily and things like that. And then over time, we've definitely started doing more and more of just kind of fundamental research in the space. And I think definitely Justin has thanked him a lot for the progress.
00:13:57.666 - 00:14:42.630, Speaker B: In mean, I'd say mean. Now it's pretty obvious that zone knowledge proofs are this megatrend within the blockchain space. They address both scalability and privacy to an extent, and they unlock so many things. I mean, going back to what is the mandate, the remit of the Ethereum foundation, I think a large part of it is supporting common goods and supporting the whole ecosystem beyond the layer one, even beyond layer two of Ethereum, but like the actual whole ecosystem. And the nice thing about zero noise proofs right now is that it's so nascent that there's quite a small number of experts, and it's extremely collaborative and it's a lot of fun.
00:14:42.780 - 00:14:57.066, Speaker A: What else is like Ethereum research. What else are you guys working on? Because we just talked about the zero knowledge group, but do you separate it into different groups focused on different problems? Are you all one larger team that sort of move around between problems?
00:14:57.168 - 00:15:26.838, Speaker C: There's kind of different sub teams with different levels of kind of formality of their teamness. There's CBC, for example, correct by construction Casper, and there's VlAd and Aditya and a couple of other people there. There's rig, the robust incentives group, which started up fairly recently, and it's doing some economic analysis of the protocol. And then otherwise there's just people working on different problems.
00:15:27.004 - 00:16:04.110, Speaker B: And within the cryptography team, we kind of have a split. So we have kind of the zero notch people, which tend to be designers or builders, and we're starting to build a competency in what I would call crypto analysis, which is basically studying the fundamental assumptions that we're making and stress testing them and trying to break them, because a lot of what we're doing, in fairness, uses assumptions which are fairly recent, fairly exotic. And so one of the hires that we made recently was Mitri Kovretzovich, and we're hoping to grow this script analysis.
00:16:04.710 - 00:16:26.374, Speaker A: So when you say research here, are you also building some of these systems out? Are you just specking them out? I'm kind of curious. When we talk about zero knowledge proofs, I know that there is recently a lot more zero knowledge proof engineering work happening. Is that happening in the research team or is that more outside?
00:16:26.572 - 00:16:54.160, Speaker C: Yeah, so Barry Whitehat's team, for example, has done a lot of things semaphore, the zero knowledge signaling stuff that's done by them, and it's been gotten pretty far recently. And if Macy, the minimal anti collusion stuff, that's also come out of their team, and it's close to having a workable version at this point. So there's definitely specific kind of close to application layer things being built too.
00:16:54.610 - 00:17:08.454, Speaker B: They've also started a perpetual powers of tau. And their powers of tau is somewhat unique in the sense that it's one of the biggest. I forget exactly how many monomials they have, but something like two to the 29 or two through 28.
00:17:08.492 - 00:17:10.120, Speaker C: Two to the 28, as I remember.
00:17:10.650 - 00:17:21.494, Speaker B: Which is pretty big. So it'll take quite a bit of time to run on your computer. But if sufficiently many people do that, we're going to have a nice robust powers of tau.
00:17:21.542 - 00:17:23.654, Speaker A: What did you just call it? What kind of powers of tau?
00:17:23.702 - 00:17:26.102, Speaker B: You just said the monomials.
00:17:26.166 - 00:17:32.574, Speaker A: No, not that perpetual perpetual. Is this like universal? Is this like the same thing or not?
00:17:32.612 - 00:17:42.158, Speaker B: It's the updatable stuff, the universal stuff, which is basically the future, I think all the other constructions, in my opinion, is like the beginning of the end.
00:17:42.244 - 00:17:47.060, Speaker A: Does that mean, like, anyone can still join, though? Is that like an ongoing. Oh, cool.
00:17:47.590 - 00:18:09.510, Speaker B: Yeah. And traditionally we've had these powers of Tao where it's one participant at a time, and so that can be really slow. But there was this recent observation where actually you can do powers of Tao with like ten people working at a time. So potentially we can have powers of tau which are ten times bigger in terms of number of participants or ten times bigger in terms of number of monomials.
00:18:10.810 - 00:18:39.586, Speaker A: There's also some new structures where you don't need the trusted setup, and I want to talk to you guys about some of your ideas on that going forward. But maybe before we move on to sort of the next section, I wanted to ask you about collaborations with other projects. So there are other projects already working on zero knowledge proof stuff, and I know that there's a connection with Zcash and with Starquare, and I'm just curious kind of how those came about and if there's maybe some others that we don't know about.
00:18:39.688 - 00:19:17.194, Speaker C: I feel like I've been friends with the Zcash people in Zuko in particular for many years now, and we've been going to a lot of the same events in the IC three boot camp that I mentioned, has spent quite a bit of time with their whole team, visited the Zcash offices and a couple of their conferences now, and they're a very solid kind of technical team with very strong values, and I have a lot of deep respect for them. And also it's definitely very helpful that they're working on and interested in zero knowledge proofs, and we're working on and interested in zero knowledge proofs.
00:19:17.242 - 00:19:21.834, Speaker A: What does collaboration look like between those two groups, though? Is it just sort of like friendly.
00:19:21.882 - 00:19:29.010, Speaker C: Networks, or is there actual, like, we've co funded things? I think that's definitely been meaningful.
00:19:29.910 - 00:19:47.426, Speaker D: There's some overlap, like the stuff that you mentioned before, where the Ethereum network has implemented stuff like pre compiles to help bring in zcash stuff and help interoperability in some ways, or help to build stuff on top of Ethereum.
00:19:47.618 - 00:20:23.330, Speaker B: I mean, I'm super pro collaboration and I'm trying to push it forward. My talk at Defcon was on collaboration. I mean, one of the teams I have a lot of respect for, and we're very good friends with is the protocol apps team. So when I was a bitcoin entrepreneur building Openbazaar, Openbazar is built on lip P two P. So I got to know them quite a few years back. And the VDF project is also a great way to have more friends because it's such an ambitious project that if you don't have friends, you're going to fail. So at this point, we have four blockchain projects that are contributing funding.
00:20:23.330 - 00:20:28.882, Speaker B: So that's Cosmos, Tezos protocol labs, and DFM foundation.
00:20:28.946 - 00:20:31.410, Speaker A: And this is for the VDF collaboration?
00:20:31.490 - 00:21:17.234, Speaker B: Yes. So we're looking to build hardware. It's like roughly ten to $15 million, but also lots of technical challenges, things like the RSA, MPC and whatnot. But yeah, we're also making collaborations with Chia Solana to an extent, with Nier on education, to an extent. Algorand is helping with the BLS standardization effort, and I'm hoping we can work with even more projects. Projects like definity, for example. But definity has been hard to collaborate with because they're so secretive and they don't have much open source code.
00:21:17.432 - 00:21:24.134, Speaker A: But this is all still on the VDF front, right, what you just described? Or does that also cross over somehow into the zero knowledge world?
00:21:24.172 - 00:21:43.662, Speaker B: I mean, the VDF front is where there's been a lot of collaborations, but one of the other things that I've been actively involved with is the BLS standardization, and here there was a lot of herding cats. Let's all use BLS twelve 381 as the blockchain standard. And through this effort, you can make new friends.
00:21:43.796 - 00:21:57.550, Speaker A: That's kind of an interesting point of collaboration, where it's not necessarily that you're sharing engineering efforts or even sharing research, but you're trying to get some sort of standardization. And to do that, you inevitably have to collaborate somewhere.
00:21:57.630 - 00:22:10.550, Speaker B: Right. And I think one of the future areas for collaboration will be WASM, because we have all these blockchain projects building on WASM. And right now, from what I can tell, there's almost no collaboration, which drives me crazy.
00:22:10.700 - 00:22:11.510, Speaker C: Weird.
00:22:11.930 - 00:22:52.130, Speaker D: Yeah, it's weird. I mean, it's kind of a strange thing where everyone has their opinions because the differences on the bytecode is none. The bytecode wise basically all look the same. So it's like what the host interface looks like. And. Yeah, some projects like near have an asynchronous model and some have asynchronous model, and that makes a very big difference. And it's a fundamental, if you have difference of opinion on things like that, then you're just never going to be able to agree because it's like Mac versus windows people, they're not going to agree, but it's not for any real reason, but it's kind of happening.
00:22:52.130 - 00:23:24.430, Speaker D: So I was at this WASM conference last year and we had some of the Mozilla people over and they were talking about the WASI standard, which a host interface standard to run WASM programs on your computer and interface with like a Unix type interface. And we actually got the Mozilla people to kind of agree that it would make sense to have like a blockchain subset of. And like, I think that's a good thing to pursue.
00:23:26.450 - 00:23:39.406, Speaker A: That leads to another question about this. Are you also seeing collaborations with other organizations? Maybe non blockchain related organizations? Other open source communities and academic groups.
00:23:39.438 - 00:24:14.800, Speaker C: Are definitely the first place to start. We've had very long running collaboration with IC three recently with Stanford CBR, which we were one of the founding sponsors of Andomis ago, and Ethereum project was another one of them. Then academic groups in Israel of various kinds, though that's been a bit informal. Eliza and his team as well, at Berkeley. Yeah.
00:24:15.170 - 00:24:36.050, Speaker B: And VDF is also a great magnet to get academics interested. So we had VDF day number four just before the Stanford belgian conference, and there was about, I think 30% of the room was academics. There were 70 people and they came from nine different universities, from four different countries.
00:24:36.210 - 00:24:43.430, Speaker A: Cool. Actually, one other group that we haven't spoken about yet is starkware. What is the relationship with Starkware?
00:24:44.650 - 00:24:54.230, Speaker C: The Ethereum foundation have paid or give a grant to starkware to basically work on stark friendly hash functions.
00:24:54.390 - 00:25:19.538, Speaker B: Right? I mean, the bigger picture is Starks is like the bet that we want to place for quantum security for Ethereum 3.0. So the cool thing is that zero knowledge proofs are so flexible that they can be kind of this massive power tool to rule them all. So it can do signatures, it can do proofs of custody, it can do vdfs, it can do everything.
00:25:19.704 - 00:25:37.830, Speaker D: How do you mean? We kind of touched on this a little bit before. The difference between snark or generally ZKP constructions that need a trusted setup versus those that don't. How do you see the space of those that don't need a trusted setup? Like transparent proofs.
00:25:38.350 - 00:26:30.762, Speaker B: Right. In the last year, so it used to be, I think, mainly stocks from starkware that were transparent, but now there's competition in the space. So it turns out that if you have a polynomial commitment, you can have universal snarks. And so the question, and the question becomes, can you have polynomial commitments which are transparent? Because then the snark will inherit that property, and it turns out that they are. So you can use the doc, polynomial commitment, which is using class groups. There's no trusted setup there. And just a few days ago, there was this massive kind of announcement in the cryptography space where there's a new type of group of unknown order called jacobian groups of hyperlink curves of genius three.
00:26:30.762 - 00:26:36.810, Speaker B: And they also don't have a trusted setup, and they have very nice performance properties.
00:26:37.550 - 00:26:42.750, Speaker A: Is that coming out of snark research, or is this somewhere else that this exists?
00:26:43.330 - 00:27:30.830, Speaker B: Well, so what happened is that 20 13 19 was the revolution for universal Snox, and now there's like so much attention to these polynomial commitments, because this is a very nice framework to work in. And one of these polynomial commitments that was found recently was the dark ones. And that kind of led to, that led to the need for more groups of unknown order, which are actually the basis for vdfs and accumulators and vector commencements and all these other things. So if it wasn't for all the blockchain applications, I don't think we would have today, this group of unknown order.
00:27:30.900 - 00:27:35.680, Speaker A: And this interest and this sort of attention on it.
00:27:36.770 - 00:27:48.498, Speaker B: One of the amazing things about the Ethereum brand, being an Ethereum researcher, is that you send an email to an academic. The probability that he will reply to your email, excited, is extremely high.
00:27:48.584 - 00:27:56.150, Speaker D: Wow, nice. Yeah. Who was it that came up with the jacobian version?
00:27:58.090 - 00:28:25.966, Speaker B: So I think there's a famous guy called Galbrafe, I believe. Actually, I think most of the credit goes to his PhD student. I forget his name, but one of the interesting things is that Dan Bonet, months ago, I think, in November, he had this intuition that, yeah, we can use hyperliftic curves, but there was this one problem that he couldn't crack, and it seems like these guys came up with a clever idea.
00:28:26.068 - 00:28:29.342, Speaker A: We're definitely after this. We'll go find this article. Sorry.
00:28:29.396 - 00:28:31.226, Speaker D: Yeah, we'll try to find the paper and put it in the show notes.
00:28:31.258 - 00:28:32.030, Speaker A: Totally.
00:28:32.950 - 00:28:56.886, Speaker D: Speaking of groups of unknown order, we just had Luca Defeo on to talk about isogenes, and there's one interesting problem in there that's like, that's what I'm looking forward to in this space, when we can construct elliptic curves of unknown order without a trusted setup. So he says there shouldn't be a reason we can't do this. It's just that no one knows how to do it yet.
00:28:57.068 - 00:29:00.170, Speaker C: Doesn't scoof's algorithm just work on elliptic curves?
00:29:01.310 - 00:29:03.450, Speaker B: You mean in the context of isogenes?
00:29:04.590 - 00:29:29.620, Speaker D: Yeah, but. So the reason this came up is to do isogeny based vdfs you need to get to an elliptic curve of unknown order. And the only way that we currently know how to get there is to do random walks across the space of all elliptic curves using isogenes. That random walk requires a trusted setup then.
00:29:30.070 - 00:29:59.210, Speaker B: So the way that I understood what was said is that basically you want to have an elliptic curve where you don't know the endomorphism group. And so the idea is that you start with a hyperliptic curve and then you do this random walk, and so long as one of the participants is honest, then you'll get to a point where Europe curve has unknown on the morphism group.
00:29:59.360 - 00:30:04.190, Speaker D: Yeah, and he said that there is a way to get that without the random walk.
00:30:04.260 - 00:30:05.520, Speaker B: Yeah, that would be amazing.
00:30:08.530 - 00:30:10.778, Speaker A: Putting it out into the universe, hoping.
00:30:10.794 - 00:30:12.880, Speaker D: That some listener will solve this problem.
00:30:14.070 - 00:30:50.806, Speaker A: Cool. So next up, I want to explore a little bit more how zero knowledge proofs are being used kind of throughout the eth two construction, how you're thinking about them, where they fit. We've kind of covered ethereum research, we've covered some of the collaborations, we've touched on a few of the projects, but you have these great researchers in house. Some of the research, as you mentioned, is going towards kind of like general public good. But I imagine there's also some really interesting applications within the construction, within the ETH two model. So where do they live? Where do they exist?
00:30:50.998 - 00:31:37.750, Speaker C: I think kind of the most obvious place to start subdividing it is kind of thinking about layer one applications versus layer two applications. And by layer two, I don't just mean scaling. Well, fine, there's layer 1.5, there's also application layer things. So at the application layer, and we've been seeing mixers get fairly popular recently, and in Ethereum, there's been a bit of this kind of privacy awakening that's been starting. There was this incident a few weeks ago where I think one of the co founders sold about 90,000 ETH and this transaction was detected and the coin media talked about this. And then there was this other release by some group that kind of traced a whole bunch of Ens names.
00:31:37.750 - 00:32:29.494, Speaker C: And so people are realizing if their privacy is something that they should take seriously. And the problem is that you could consider trying to get privacy by kind of sending your coins to a different account. But then if you do that, then the link between the new account and the old account is still kind of very detectable in the clear. And so what tornado cash offers is basically a ZK scenario based mechanism that allows you to just send coins from one account to another without kind of the link of who sends to whom actually being public, because a zcash kind of style mechanism is used to enforce that. So that's something that's been getting a lot of attention. And then there's some other projects like on eth research. There's one called a zero pool that was published.
00:32:29.494 - 00:32:32.554, Speaker C: And there's also this longer term thing.
00:32:32.592 - 00:32:40.670, Speaker A: Called Aztec, by the way, all that zero pool, not but Aztec and Tornado, they've been on the podcast. We can also add the links in the show notes if somebody.
00:32:40.740 - 00:33:44.926, Speaker C: Wonderful. Yeah. So a lot of progress in the payment space. And then also Macy, this is minimal anti collusion infrastructure. This is basically kind of secure cryptographic voting where you want it to be anonymous, but then there's this additional challenge that you don't want it to be possible to prove who you voted for because then you can bribe people to vote a certain way and they came up with a kind of spec for that that basically kind of relies on a central party for the collusion resistance, but does not rely on that central party for anything else. And the Gitcoin grants, quadratic funding. People actually want to use this, right, because quadratic funding and kind of voting and all of these mechanisms in general have this property that if you can spend $1 of your own money to cause an impact to someone else's money, which is bigger than $1, and you need that in any mechanism that kind of funds public goods and kind of counteracts the tragedy of the commons.
00:33:44.926 - 00:34:06.620, Speaker C: But the problem is that once that property exists, then a malicious recipient can just bribe people to take this action and then they spend one dollars or the bribery gets $10 and then they kind of give $5 back, and that kind of just breaks the whole mechanism. And Macy can kind of hopefully solve this.
00:34:07.710 - 00:34:10.298, Speaker A: All of those that you just mentioned, those are all layer two?
00:34:10.384 - 00:34:12.330, Speaker C: Those are all application layer. Yes.
00:34:12.480 - 00:34:23.374, Speaker B: If we were to look at layer one, it's kind of confusing because if you look at the phase zero and phase one spec, there's no Snox. And we probably won't have snox in phase two either.
00:34:23.492 - 00:34:24.170, Speaker A: Okay.
00:34:24.340 - 00:35:14.750, Speaker B: But the bigger picture is that there's kind of this separation between crypto economics and cryptography. And when I was doing research in sharding. My motto was, if cryptography doesn't work, try crypto economics, because usually you can find a solution that works for crypto economics. But as you try and implement these things, you realize that actually the economics part is the devil. You kind of want to minimize it. So I think moving long term, we want to try and take some of these crypto economic systems which have complexities around fraud proofs and challenge and response, and you have assumptions around rationality and honesty and things like that, and just replace them straight out with cryptography and the whole system becomes simpler.
00:35:15.250 - 00:35:42.842, Speaker A: So now I want to talk about layer 1.5 and what you just said. This idea of going from the crypto economic to cryptography. It's kind of interesting what's happened on one of the layer 1.5 projects. Roll up where it went plasma ish, like something very crypto economic to something cryptography with ZK roll up to optimistic roll up, which is again game theory. So how did that happen?
00:35:42.976 - 00:36:35.270, Speaker C: Yeah, so ZK roll up. And I published this youth research post about it about a year and a half ago. And the goal there was just to kind of show that hey, there exists this other category of scaling solutions and can get you up to back then, 500 transactions a second. Now after we did the call, data gas cost reduction in Istanbul, 2500 transactions a second. And you can do this with these really nice properties, like you don't need to really depend as much on central operators and the system has better ways of recovering from failure. You can deposit and withdraw immediately and all these other nice things. But the challenge with ZK roll up, there were two of them, right? So one of the challenges that we hit upon immediately is zero knowledge.
00:36:35.270 - 00:37:22.214, Speaker C: Prover time is very nontrivial. So I remember matter labs actually implemented a ZK roll up pretty quickly and they said they even got up to over 100 tps. I think it was on main net. But the problem is that the reason that they said they didn't get more is basically because their servers were kind of at full capacity generating these proofs. And the other challenge that the ZK roll up I think had or has is that people want to do more than just moving coins around. And I think this is the big thing that's also led to challenges for a lot of the other ethereum layer two protocols, which is people are on ethereum precisely because they want to do more than just moving coins around. People already have existing applications.
00:37:22.214 - 00:37:59.670, Speaker C: They want to move their applications over. They don't want to rewrite their whole applications in the process. And so the thing that people actually want is basically a layer two that just behaves like the EVM. And optimistic rollup is actually capable of providing that. Right. You have to do a couple of clever tricks, like the optimism team, for example, have this really clever kind of code translator that takes the sload opcode, for example, and they replace every instance of the sload opcode with a call to a contract that checks some Merkel proof. They do the same with Sdor, do the same to kind of wrap external calls.
00:37:59.670 - 00:38:25.660, Speaker C: And then because those other manipulations kind of change the code positions around, they kind of add a dictionary for a jump table. So there's a lot of tricks that you need to do to make it work, but with optimistic rollup, you actually can make something that does actually the same thing that the EVM does, and that's something that a lot of people are attracted to.
00:38:26.850 - 00:38:37.860, Speaker A: But ZK rollup, you're still working on ZK rollup. So where does that live then? How does that fit in? Is it still going to be used for certain cases, or is it more like, once it gets more efficient, it'll be fine.
00:38:38.230 - 00:39:19.966, Speaker C: So the loop ring team, for example, has been making a lot of progress on ZK rollup recently. They published some numbers with a version they released through Webex about a month ago, and they've implemented a ZK roll up for a decentralized exchange. And the statistic that they gave was 0.3 cents per transaction for onchain gas and 0.3 cents per transaction for the prover computing time. But both of those numbers are still very far from optimal. Right? Because on the one hand, the 0.3
00:39:19.966 - 00:39:59.642, Speaker C: cents for unchained gas was there just because they had fairly low volume. And if they have higher volume, then the per transaction gas cost, kind of the amortized amount goes down from about 4500 that it was back then to a theoretical minimum of about 400. And if you have that, then of course the 0.3 cents becomes 0.3 cents. And on the prover side, one of them just told me very recently that they made a factor of 30 improvements in prover time. And so it seems like they've been doing quite a good job of making ZK roll up work, at least for the DeX use case.
00:39:59.642 - 00:40:06.330, Speaker C: Now, the final frontier, of course, is can you make ZK roll up work for a general purpose computation?
00:40:06.490 - 00:40:09.498, Speaker A: And that is still to be determined.
00:40:09.674 - 00:41:12.658, Speaker B: I mean, that's kind of one of the crazy things that I'm helping move the space move towards really, Snox are amazing in all respects now, except probably for the prover bottleneck. So how can we improve the bottleneck there? And actually there's a few ideas here. So one kind of obvious one is hardware acceleration. So we've seen people use gpus, for example. But once you become more ambitious and you go with, for example, a SnOC ASIC, then you can get 1000 x speed up. So, I mean, the rough ballpark right now is in terms of the comparison between native speed of a CPU and the speed within a snark circuit is between a billion to a trillion x slowdown. So we need several ten x ideas to get to where we want to be.
00:41:12.658 - 00:41:52.586, Speaker B: One possible kind of ten x idea that I'm excited about and I'm working on right now is this idea of a sparse snark as an analogy. Think of a cpu. A CPU has about a single core has about a billion transistors, and a CPU has about a thousand instructions that can be called. And when you call an instruction, the CPU will turn on the transistors for that one instruction, and the rest of the chip is turned off. So that's called dark silicon. And the reason is that you want to save on power and you want to prevent the chip from just melting. And maybe we can do something similar for snarks.
00:41:52.586 - 00:42:15.938, Speaker B: So one of the holy grails is general purpose computation. And that means, like, massive circuits, but doesn't mean that you have all the wires and all the gates turned on. So what you can do is you can set almost all the wires and all the gates to zero, and then you want to try and use this sparseness to your advantage to accelerate the prover.
00:42:16.114 - 00:42:25.498, Speaker A: In that case, is it like you're having other things use that sparseness, like use that space? Or is it just speeding it up because you're not using that space?
00:42:25.664 - 00:42:27.562, Speaker B: It's sped up because you're not using that space.
00:42:27.616 - 00:42:28.042, Speaker A: Got it?
00:42:28.096 - 00:43:15.110, Speaker B: Yeah. So one of the typical examples is you want to do a conditional in a snark is like if, let's say, a equals b, or if a not equals to b, like in a traditional snark circuit, you have to pay two extra cost for both branches, and that's very expensive. You have this blow up. But if you can activate only one branch at a time, that's a huge benefit. So going back to what I said around the key is a polymor commitment. That is your starting point. So a few weeks ago, we discovered a sparse polynomial commitment scheme that could use the sparseness, and then you can do so called sparse hademal checks, and that might unlock sparse snox.
00:43:15.270 - 00:43:16.460, Speaker A: What's that called?
00:43:17.310 - 00:43:17.962, Speaker B: Which one?
00:43:18.016 - 00:43:20.262, Speaker A: What you just said, the sparse snox.
00:43:20.406 - 00:43:21.446, Speaker B: Sparse snox.
00:43:21.558 - 00:43:22.474, Speaker A: It's called sparse.
00:43:22.522 - 00:43:33.182, Speaker B: Well, that's how I'm calling them, because sparse polynomial commitments is a very natural term, whichever people are using. And then that kind of extends to the other tools in the stack.
00:43:33.246 - 00:43:35.874, Speaker A: Cool. I like the name.
00:43:35.992 - 00:44:03.530, Speaker B: Yeah. I mean, another idea that I'm working on is trying to simplify the prover. And in particular, when you look at the prover, there's basically three components in approver. There's the witness generation, which today tends to be negligible, like, let's say 1% of the prover time. And then you have the ffts, which tends to be like 20%. And then you have the multi exponentiations, which is 80%. Now, okay, you want to try and speed up the 80%.
00:44:03.530 - 00:44:38.622, Speaker B: And let's say you build an ASIC, but let's say you bring this 80% down to zero. Now you're still left with a 21% or 20%. So you've only sped up the whole thing by five X. So now you need another aSIC, basically, that will do the FFTs. So what if we can just simplify the problem and remove the ffts? And also, the ffts are a pain in the neck because they're not super friendly to distribution. They can be paralyzed, but they have these choke points and not super friendly to full distribution. And they're quite expensive in terms of memory.
00:44:38.622 - 00:44:48.370, Speaker B: So I've been working on a. .0. Commitment scheme where you completely remove the ffts, and then you can do the same thing, a harder mod check with no ffts, and then the snark without ffts.
00:44:48.450 - 00:44:53.114, Speaker A: What do you put in its place, though? Do you just take them out? Why doesn't everybody take them out?
00:44:53.232 - 00:45:31.650, Speaker B: Well, for a long time, people believed that it was not possible to take them out for fundamental reasons. And I think that discouraged a lot of people from going that route. But I came in naive and with fresh eyes, and I guess there was a few tricks that had to be found, actually, Vitalik helped along the way, Dan Bonet helped along the way. And yeah, it turns out you can do it. So if we are going to build this Mark AsiC, which I think we are, and I think it's going to be a collaborative project for the blockchain space, then we will focus on the multi exponentiation. We can just forget about the ffts.
00:45:31.730 - 00:45:42.890, Speaker A: You just mentioned sort of three optimization things. You said, get rid of ffts hardware. So like using asics and the sparseness, is there anything else that would optimize?
00:45:43.630 - 00:46:21.490, Speaker B: Right. One kind of recent idea is this idea of custom gates. So that goes down to how you lay your circuit, and that's a big area for improvement. The hardware analogy kind of works well. So if you're building an ASIC, you have this so called cell library. Now, you could build everything in theory out of Nand gates. But actually what happens in practice is that you have these cell libraries with hundreds of gates that will do more complex operations.
00:46:21.490 - 00:47:07.542, Speaker B: And by using these slightly bigger so called cells, rather than the primitive gates, you get quite a lot of speed up. And so the same thing is happening. And this is something that is being led by the aztec team. So a perfect example is, let's say you have a hash function like MIMC, where you're basically repeating an internal round function. If you have a custom gate for this internal round, then you can really get a lot of performance benefits. And then another really exciting idea, which lots of people are working on, and there's now lots of different ways to do, is a recursion. So recursions help in two ways.
00:47:07.542 - 00:47:44.502, Speaker B: One is that if you do, for example, one layer of recursion, or a constant number of layers of recursion, then you get a constant improvement in the verifier time. So you can take, for example, 1000 snarks and compress those into one single snark. So there you go. You have 1000 x improvement on the verifier time, but you can go even further, and you can have these systems where you always only have one single snok at the very tip of your system. And so that's something, for example, that Coda is working on. It's very exciting.
00:47:44.666 - 00:47:48.420, Speaker A: Yeah, we've done, I think, two episodes on recursion so far.
00:47:48.790 - 00:48:02.774, Speaker B: Yeah. Cool. And the cool thing is that these ideas tend to compose on each other. So let's say we have roughly ten x ideas. We need another five in order to get to where we want to be.
00:48:02.892 - 00:48:07.762, Speaker C: The other kind of thing that I think we want to optimize is range checking.
00:48:07.906 - 00:48:12.378, Speaker B: Yes. Actually, there is a paper coming out in a couple of weeks.
00:48:12.464 - 00:48:15.514, Speaker A: Range checking, yes. I don't know if I know that.
00:48:15.552 - 00:48:21.494, Speaker C: Just to verifying that a number is between zero and two to the 60 of three or whatever, so you don't overflow.
00:48:21.622 - 00:49:05.414, Speaker B: So what tends to happen is that you have your pointy commitment and then you build gadget out of it, so you can have a permutation gadget or a shift gadget or a sum gadget or product gadget. And one of the gadgets, which is used very, very often, especially when you're doing big integer arithmetic and things like that, is a range check gadget, as I understand. So, like, a few days ago, there was this really nice idea, basically showing how you can use point commitments as a range check gadget. And from what I understand, in a couple of weeks, there should be a write up with pretty exciting ideas where you can batch these range checks.
00:49:05.542 - 00:49:09.660, Speaker A: This is sort of unexplored territory, though, right? These are just, like, ideas at this point.
00:49:11.250 - 00:49:34.020, Speaker B: The amazing thing is that there's. There's so much low hanging fruit. So, like, I. You know, we. You've seen, right, in 2019, how much. How many papers were published on the. On the continuous basis, and, you know, even in 2020, we're finding ideas, new ideas every week, pretty much.
00:49:34.630 - 00:49:48.530, Speaker D: Why do you think there is that acceleration in this space? There's so much happening. Is it just that more people are looking at it, that hardware got good enough that we can do it, or. What's the catalyst here?
00:49:48.700 - 00:50:37.190, Speaker B: I think a lot of people with kind of a pragmatic and engineering mindset kind of gave up on zero knowledge proofs as being kind of unicorn magic. That is not really useful. But I think Zcash was, like, the big moment where everyone woke up like, this is actually real. And we've had this education period, which has been somewhat painful. And Vitalik has done a great job trying to educate people on these difficult topics. And now we're at a point where the people with both kind of engineering and research hat on are here to solve questions that are less about, for example, asymptotics. The researchers, they love asymptotics, whereas the engineers, they love constants.
00:50:37.190 - 00:50:41.030, Speaker B: So, yeah, the ball is being handed off to the engineers.
00:50:41.190 - 00:50:46.954, Speaker A: Yeah, actually, that's nice that you just mentioned the papers or the blog posts that you wrote. Vitalik, you wrote, what was it, like?
00:50:46.992 - 00:50:53.126, Speaker C: Four on snarks, three on snarks, one on plonk, three on starks.
00:50:53.238 - 00:50:58.718, Speaker A: Yeah. And I know that those are actually, like, pretty important resources in the community. When did you write the snarks one?
00:50:58.804 - 00:51:01.610, Speaker C: I think it was, like, 2016 to 2017.
00:51:01.690 - 00:51:04.858, Speaker A: Quite long ago. And plonk would have been really recent.
00:51:04.954 - 00:51:05.310, Speaker C: Yes.
00:51:05.380 - 00:51:07.442, Speaker A: And Starks was, like, in the last two years?
00:51:07.496 - 00:51:07.954, Speaker C: Yes.
00:51:08.072 - 00:51:14.126, Speaker A: Cool. Yeah, we'll definitely link those as well. I know that they've been a resource that a lot of people have learned from, myself included.
00:51:14.238 - 00:51:46.286, Speaker B: And I think one of the things that's difficult in our space is that if you take a research paper, you're going to have a 20 to 30, sometimes 50, 60 page thing, which is undigestible. And generally speaking, the intuition, the core part is like hidden on page 15, 2nd paragraph, and is like a few lines. And I think one of the trick is to be able to identify this and remove all the crap. And I think Vitalik has a skill for that.
00:51:46.388 - 00:51:47.134, Speaker A: Cool.
00:51:47.332 - 00:52:05.810, Speaker D: So we've talked about the application layer, layer, kind of 1.5. So what about layer one? We mentioned e three earlier. Is that where some of this is starting to come in, or are you already imagining applications of zero knowledge proofs on layer one?
00:52:05.880 - 00:52:49.300, Speaker C: Yeah, and I think in general we have a long term goal of trying to kind of fit in zero knowledge proofs in places to just remove as much as possible of the need for lots of notes to reverify, the same thing for fraud proofs to kind of COVID cases where people did things incorrectly as part of things like data availability checks, for example. So in a lot of any kind of places where right now we have some kind of more complicated protocol for collectively verifying something without having everyone personally verify it, a lot of those cases are situations where you can just simplify things a lot by just allotting a zero knowledge proof in cool.
00:52:50.870 - 00:52:53.234, Speaker A: Are there a lot of places that you're looking at it?
00:52:53.272 - 00:53:38.894, Speaker B: There's a handful. One example, for example, is single secret leader election. So the idea here is that, okay, so you have these validators, you sample them at random using randomness, but right now the result of the so called lottery is public. And so you're going to know who's going to be called to participate in the future. And so that's a denial of service attack vector, because you can do a network ddos, but there's also other attacks, like you can try and bribe them with a smart contract or whatever. So one of the great mitigations here is you have a lottery where you don't know who the winner is. And there's a very nice scheme that Ramponay and others came up with that we're looking to implement.
00:53:38.894 - 00:54:31.522, Speaker B: And one way to implement it is using a zero knowledge proof, and that's the kind of a clean way of doing it. And then there's this other way, which is kind of using crypto economics, which we can possibly implement faster, get off the ground faster, but it's more ugly. Another place where I think there's a lot of potential for Snox is in witness compression. Right. In phase two, we have this idea of stateless client. Stateless client is when the validators don't store the state, they only store kind of this small digest, so called like, accumulator, which could be, for example, a Merkel route. And to counterbalance the fact that the validators don't have the state, the users themselves, when they make transactions, they need to come with their own state and proves that the state is actually real.
00:54:31.522 - 00:54:57.782, Speaker B: So the Merkel paths all the way to the Merkel route. And in practice, if you were to take, for example, e one today, and you were to go in this stateless model, you'd have a huge overhead in terms of bandwidth for these Merkel paths on the order of ten x. So what if we could take all these Merkel paths and compress them into a single snark and remove this ten x overhead?
00:54:57.846 - 00:54:58.460, Speaker A: Cool.
00:54:59.150 - 00:55:51.238, Speaker C: Yeah. Data availability checks are another one. So this is this other end of branch of research that I've been doing for about three years now. And I published that paper with Mustafa back in 2018, where I have this scheme where basically you take some data and then you erasure code that data using this in a two dimensional Reed Solomon code. And the idea there is that the erasure coding kind of transforms a kind of make sure 100% of the data is their problem into a make sure 50% of the data is their problem. And once you only need to make sure 50% of the data is there, then you can use kind of random sampling to figure out whether or not enough data has been published in a constant time. This is a key technology that we want to use for just kind of scalably validating that the data and blocks actually, that have been proposed, actually has been published.
00:55:51.238 - 00:56:25.218, Speaker C: And the technique that we proposed in 2018 is one that is very dependent on fraud proofs. And in order to make the fraud proofs work, we have to use a 2d code instead of a 1d code. And that has a suboptimal rate, and there's some annoyances in it. But we have a number of kind of alternative proposals. So one of them involves polynomial commitments, and then the other one involves basically just using a snark to prove that a Merkel route of erasure coded data was constructed correctly.
00:56:25.394 - 00:56:31.830, Speaker A: At what phase is that right now, though? Because it sounds like it hasn't been decided. It's like, there's some possibilities.
00:56:32.250 - 00:56:33.698, Speaker C: Yeah, definitely after phase.
00:56:33.794 - 00:56:34.422, Speaker A: Got it.
00:56:34.476 - 00:57:17.158, Speaker B: I mean, like, the final thing that comes to mind is very much related, is around proof of custody. So proof of custody is another mitigation in addition to the data availability checks for the data availability problem. And right now we have a fraud proof based scheme, and it would be very nice if you could have a pure cryptography scheme. So the idea is very simple. Basically, you have data that you want to prove that you have at the time you make a signature. So what you do is you take a secret and you mix it in with the data. So what you could do is you could have a zero knowledge proof saying that you've correctly computed the merkel root of the data mixed with the secret, and that proves that you should have had the data.
00:57:17.244 - 00:57:26.810, Speaker D: Another application, you kind of touched on this when you talked about starks, is post quantum security for the chain. When does that enter the picture?
00:57:28.270 - 00:58:40.110, Speaker C: This is definitely something that we're designing the protocol around, being able to upgrade to eventually. There's some parts of the protocol that are not quantum secure at the moment, but for every single thing that we use, we definitely want to have the ability to switch to an existing and ready quantum safe alternative when we want to. The one that's probably the toughest at the moment is aggregate signatures. So post quantum signature schemes exist, and even hash based quantum signature schemes exist. But kind of post quantum aggregation friendly signature schemes has definitely seen considerably less attention and using snarks, or in this case, starks, which apparently are a type of snark now to solve this problem. So basically, just using a stark to prove the existence of a bunch of lamport signatures, for example, is one of the paths to solutions that we're looking at. Though in order to actually make that be possible, we're definitely going to need some very significant improvements.
00:58:40.110 - 00:58:40.980, Speaker C: Time.
00:58:41.670 - 00:58:55.750, Speaker D: The whole difference between snark and Stark is funny. Everyone has different opinions on what a snark is and what a stark is. Basically, the consensus seems to be that whatever comes out of starkware is a stark and everything else is a snark.
00:58:56.570 - 00:59:14.480, Speaker C: The totally unconsidered mental intuition that I had started with for some reason is like, if it uses elliptic curves, it's a snark, if it uses hashes, it's a stark, and if it uses hidden order groups, it's a dark. And then I started talking to people, and then I realized other people have totally different ideas, and I don't care.
00:59:16.130 - 00:59:41.734, Speaker B: So, on the topic of the quantum secure signatures that can be aggregated, this is a very hard problem. And this is also an opportunity for us to collaborate with academics. So we're currently working with a group called, you know, they have Jean Charles Fouger and Ludovic Pere, both experts in quantum security. So, yeah, we have a team working.
00:59:41.772 - 01:00:00.090, Speaker A: On, like, is there anything in the construction right now that is similar at all to what sello's been doing with this light client construction? You sort of mentioned this data availability. Is that related somehow? Are you thinking about creating sort of smaller light clients using zero knowledge proofs?
01:00:01.330 - 01:00:57.678, Speaker C: That definitely is the sort of thing that we want to do kind of longer term. So I guess if the recursive synarc kind of verify everything in one proof stuff, that verifies data validity. So if you download a block, then it can prove to you that that block is the latest block in a chain where the entire history is correct and possibly even prove to you that that chain is better in terms of kind of consensus priority than any other chain that you've seen. But the thing that can't prove to you by itself is that the data in the chain actually has been published. And even if all the data is correct, you can do lots of really mean things by withholding data. You can basically withhold information that other people need to make their own transactions and kind of send things from other accounts. And so it's kind of almost equivalent to stealing from people.
01:00:57.678 - 01:01:20.870, Speaker C: And what data availability checks let you do is basically kind of randomly sample of all of the data in the chain. And if the data is erasure coded, then you just kind of randomly check that with very high probability, at least enough of the data has been published that if necessary, the entire thing can be reconstructed.
01:01:22.170 - 01:01:54.830, Speaker D: I'm curious to poke your minds on something that is, I know, without naming names, some companies that have now gone, we're not allowed to talk about zero knowledge proofs anymore, can't mention it in the office because it becomes a practicality thing where everyone, instead of actually trying to solve the problem go, zero knowledge proof will fix that, whatever, we'll keep working. Zero knowledge proofs will fix that and just kind of passing the ball.
01:01:55.170 - 01:02:17.640, Speaker B: I mean, I think that might make sense as a policy. I mean, unless you're very serious and you have people that are dedicated to it. I'd say a very similar comment, maybe for blockchains even you have all these companies that say, oh, blockchains will solve everything unless you have at least one blockchain expert in your team. Blockchains is probably not for you.
01:02:19.610 - 01:02:40.558, Speaker A: I've also heard companies, I know some companies where they hired a zero knowledge proof researcher to kind of look into it, but then realized that the space was moving so fast and they maybe didn't have the capacity to fully contribute or fully keep up. So they've sort of taken a step back and been like, let them figure it out, and then maybe we'll add a zero knowledge proof back into our system.
01:02:40.724 - 01:03:13.562, Speaker B: I mean, this is almost the opposite of a problem that we had in bitcoin and ethereum, which is that no one wanted to build stuff on Ethereum or bitcoin. That was high volume, because if they were successful, then there wouldn't be support for them. The system basically wouldn't adapt fast enough. But here's the opposite problem, where the system is just adapting too fast that you can't even keep up with it.
01:03:13.616 - 01:03:30.346, Speaker A: Yeah, and in a way that's sort of true if somebody's going to put in the resources to implement one of these things. And actually, this was a topic that came up with Eth one, this idea that certain. I'm not going to say this right, but certain curves were the pre compiled.
01:03:30.378 - 01:03:47.138, Speaker D: That we talked about before that was added, then Zcash upgraded and not using that curve anymore. So it's kind of pointless that it was added, and now we need to add the next one. That was a particularly unlucky case, but it is a good point.
01:03:47.224 - 01:04:19.920, Speaker C: This is definitely a problem that a lot of projects just have to deal with. Like at some point you have to just choose and say, right now we're building with this set of technologies. And yes, we know that that means that we have to do this stupid multiround trusted setup. And then maybe four months later, and maybe 20 months later, plunk is going to be ready to use, and that whole effort will have been for nothing. But in 28 months in the future, people might be talking about, well, why use plunk when plunk 2.0 is going to be ready?
01:04:20.450 - 01:04:26.974, Speaker A: I hear they just actually add letters before plonk. So it's. Ariel had another one.
01:04:27.172 - 01:05:21.602, Speaker B: This problem of curve that you bring up is very serious, and it kind of goes against the whole universality revolution that we're seeing. And the reason is that for different use cases, you want different curves. So you have BS twelve 381, which is the standard we're going toward. But then you have this other curve, BS twelve 377, and then you have the MNC four MNC six cycle, and then you have the BN two five four cycle, which is the legacy curve, which is the only one that affirm support. So that's what everyone is building on in practice, and I think, unfortunately, this is going to hurt us. Maybe one of the kind of advertisements for the RSA based snarks is that they're in several ways truly universal. So there's no kind of bike shedding on the type of RSA group.
01:05:21.602 - 01:05:52.430, Speaker B: There's only one type. I mean, you can choose the bit size, but you'd think that people would agree on, let's say, 3000 bit RSA group. That's plenty of security. But the other interesting thing about these rsa based snarks is that they're not bounded in circuit size. So you have a powers of towel. It only goes up to, let's say, two to 28, and if you want to go beyond that, you have to restart your powers of towel from scratch, whereas you don't have this limitation with RSA groups.
01:05:53.010 - 01:06:41.020, Speaker C: One more use case of narcs that might be relevant is in Casper CBC. The validity conditions for blocks are kind of very simple conceptually, but they're kind of fairly involved and complicated to actually verify. So the validity condition basically says that for a block to be valid, the block must be the block's parent must be the winner of the fork choice rule executed using all information that has been included into that Block or its history. And there's a really nice and clean kind of mathematical intuitiveness to this, but the problem is that this takes kind of o of n time to evaluate, and snarks are something that can possibly help with this.
01:06:42.990 - 01:07:12.466, Speaker A: We've come a long way. I think we have a way better understanding of what's going on with zero knowledge proofs in the eth two world and at all the different layers. But I think a nice way to close out this episode would be to talk a little bit more about this future idea for it. You sort of mentioned starks and e three, but what else are you looking at in terms of, just like, the larger zero knowledge proof ecosystem? Are there other places that you're just sort of keeping your eye on, or other ideas?
01:07:12.658 - 01:07:24.266, Speaker C: And there's also enough areas of cryptography other than zero knowledge proofs, multiparty computation, homomorphic encryption, code obfuscation. There's definitely quite a lot of use cases for each of those.
01:07:24.448 - 01:07:34.206, Speaker D: Are you hopeful that those will exist anytime soon? Or if you imagine ten years from now, which of these things are, in.
01:07:34.228 - 01:07:46.466, Speaker C: Practice, MPC kind of already exists, but it's not efficient for all things, and I definitely expect the set of things that MPC is efficient for to improve. The question is just by how much.
01:07:46.568 - 01:08:14.074, Speaker B: I mean, the RSA NPC is really pushing the boundary. I think the record for the NPC was actually set by aztec, so they had 173 participants and the ceremony lasted one month. We're looking to do an NPC with over 1000 participants, and if everything goes fine, it should run in about ten minutes. So we're really shattering the records here.
01:08:14.112 - 01:08:17.738, Speaker A: You mean like ten minutes per participant, like on their computer?
01:08:17.824 - 01:08:37.874, Speaker B: Ten minutes total for all participants. What? So the catch here is that all participants need to be online at the same time, and if one of them disconnects, we identify them. So you have identifiable abort and then you have to restart. So that's why I say, in the best case, it's ten minutes, but we'll see.
01:08:37.992 - 01:08:38.606, Speaker D: Whoa.
01:08:38.718 - 01:08:42.660, Speaker C: It's ten times f minutes in BFT lingo, right?
01:08:45.610 - 01:09:26.734, Speaker B: I mean, one thing I'm quite excited about, just from a nerdy perspective, is the I o, like indistinguishability, obfuscation, like a piece of magic which, as far as I understand, can be unlocked by this cryptographic primitive called trilinear maps. So we have bilinear maps with the pairings, which gives us snarks and which give BLS signatures. When you go to trilinear, three is so much better and you can do IO. And just a couple of days ago, there was a new paper by Huang, who apparently is very well respected, with a very promising construction.
01:09:26.862 - 01:09:59.578, Speaker D: That's interesting. I mean, we had Dan Bonet on the show, and at the end he talked about what he's looking forward to and challenges that are unsolved, and he mentioned tri linear maps and code obfuscation as one of the big unsolved problems that he's very excited about. And after that, I looked into it. It's just like, holy shit. This changes the whole landscape of smart contracts. It does so much, it's amazing. I didn't know that there was actual results, and that's super exciting just for.
01:09:59.584 - 01:10:30.182, Speaker B: The listener who doesn't know what it is. One of the consequences is that you can have a smart contract which holds a secret in the code of the contract. So you can have a secret which performs a specific task. So, for example, it will only sign a message if the message has a certain structure. But at the same time, even if the code is public, no one can go extract the secret and sign messages that the contract was not meant to sign.
01:10:30.316 - 01:10:36.934, Speaker A: Some of these ideas, I mean, they're going past blockchain, right? This is not a particularly blockchain focused concept, is it?
01:10:36.972 - 01:10:43.142, Speaker D: They have very interesting and cool applications in blockchain. But no, certainly go way beyond blockchain.
01:10:43.206 - 01:10:55.360, Speaker A: Yeah. Do either of you sort of see or are you excited about zero knowledge proof past blockchain or outside of blockchain? Because I know that that is something that I'm starting to see bubling up.
01:10:56.050 - 01:11:23.714, Speaker B: So one of the things that I've observed is that a lot of smart people that I've tried to hire say, no, I don't want to join because blockchain, I am not fully convinced yet. But then you tell them you can learn about zero knowledge proofs and then they can see a career there because it's an easier sell. They can see that even if the blockchain world would go to zero, the zero knowledge proof of our huge applications.
01:11:23.762 - 01:12:23.382, Speaker C: In industry, I think the blockchain space is going beyond blockchain. And this is kind of one thing that I'm excited about. And I guess what I mean by that is just the communities that are trying to build things on blockchains and trying to build some decentralized finance constructions, Daos and all these other things. There's, I think, definitely increasing realization that if the blockchain isn't the only tool that they should be using and the right solution probably involves using a blockchain for some things. But it might end up including some components of an NPC architecture, some components of something else, and just kind of branching out and just thinking about it as how do we use cryptographic and economic tools to build the things that we want to build?
01:12:23.516 - 01:12:28.070, Speaker A: That's a good way to think about it. I want to say thank you guys so much for coming on.
01:12:28.220 - 01:12:29.670, Speaker B: Thank you so much. That was great.
01:12:29.740 - 01:12:30.206, Speaker C: Thank you.
01:12:30.228 - 01:12:39.226, Speaker A: Yeah. And I definitely want to extend an invitation to both of you to come back and maybe do deep dives on some of these topics that we covered or some others if you have some ideas.
01:12:39.418 - 01:12:41.374, Speaker D: Yeah. Thank you very much. It was awesome.
01:12:41.492 - 01:12:43.466, Speaker A: And to our listeners, thanks for listening.
01:12:43.578 - 01:12:44.090, Speaker D: Thanks for listening.
