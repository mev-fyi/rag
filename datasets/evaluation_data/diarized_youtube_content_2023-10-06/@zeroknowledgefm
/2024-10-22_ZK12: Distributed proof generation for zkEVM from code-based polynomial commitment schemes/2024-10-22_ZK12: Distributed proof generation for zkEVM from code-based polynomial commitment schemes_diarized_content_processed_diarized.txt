00:00:08.039 - 00:00:32.227, Speaker A: So, hello everyone. Today I'm going to be talking about scaling of proof generation. And I'm going to present several techniques that can improve the efficiency of this process. For the case of snarks based on the proximity test for the Reed Solomon codes. So. So let's get started. My talk is mainly devoted to two problems.
00:00:32.227 - 00:01:14.985, Speaker A: They are distributed proof generation and incrementally verifiable computation. In the first case, we have a large computation that is distributed among multiple provers and their goal is to produce a single proof that guarantee the correctness of all their local computations. In the second case we have a long running computation. It means that the input of one prover can depend on the output of another. And moreover, such chain of computations can be infinite. Let me give one motivating example. So let's consider a multi chain system where multiple independent blockchains operate in parallel.
00:01:14.985 - 00:02:00.605, Speaker A: And let's assume for simplicity that blocks are produced uniformly over time. Then our goal is to produce a state transition proof. I mean proof that current state is correct if the previous one was correct. State transition for each separate chain can be proven by separate prover. And it means that we can think of this process like a distributed proof generation. And contrary, if we want to obtain a single proof for a whole batch of blocks, we need to take into account recursive nature of adding new blocks. And this is a situation which is perfectly captured by the notion of ivc.
00:02:00.605 - 00:02:55.725, Speaker A: Now let me go back to the first problem, namely distributed proof generation. So we have a large computation C which is split into several equal parts and there are several provers. Each operates on its own part of the whole computation. Also, we allow provers to exchange data so that they can produce a single proof that guarantee that all their local computations are correct simultaneously. We can achieve efficient protocol for this problem using additive commitments. Using them, we can obtain a commitment to a linear combination of data without actual direct access to this data. But FRY protocol heavily relies on cryptographic hash functions that don't have such a property.
00:02:55.725 - 00:03:48.573, Speaker A: And that's why this problem for that case is so challenging. Basically, many protocols require to send a huge amount of data whose size is comparable to the size of the local circuit. And there is another extreme approach for the FRY based snarks, where every prover can produce an independent proof. Then we can. So given this multiple independent proofs, we can aggregate them all together into a single one. But the problem is that the size of the recursive aggregation, the size of the aggregation circuit will will grow linearly in the number of provers. So before I introduce new approach, let me briefly recall how Frybase snark works.
00:03:48.573 - 00:04:41.715, Speaker A: So, we assume that our computation is represented by our execution trace is represented by a set of polynomials. Then prover uses the reed Salomon code to encode these polynomials. Namely, he computes a vector of evaluations of these polynomials over the large domain. And then he computes commitments send corresponding commitments to the verifier. And the most expensive part of the verifier's job is to check that these commitments correspond to functions that are close enough to the Reed Solomon code. And in order to do that, parties execute batched version of the Fry protocol. Namely, prover gets a random challenge and computes a random linear combination of his local polynomials.
00:04:41.715 - 00:05:41.605, Speaker A: And then he executes a regular version of Fry with this polynomial as an input. Since verifier accepts during this protocol, we can guarantee that this linear combination is close to the Reed Solomon code. And it means that each of the initial polynomials is close to the Reed Solomon code as well. So we propose the following approach. We also allow exchange of data, but we use this interaction to compute a single linear combination for all the polynomials across all the provers. And in order to do that, in order to minimize the size of data to be transmitted, every prover computes a partial linear combination. In order to do that, we introduce some kind of synchronization between provers so they can compute a single random challenge.
00:05:41.605 - 00:06:50.545, Speaker A: Here I should make a reference to the paper on amortization techniques for Fry based snarks, which also uses a single linear combination in order to minimize the size of the final proof. But the main difference is that we are working in distributed settings, which imposes different limitations on our protocol. So here is more formal view of our protocol. So at the first step, Prover computes commitments to all his local polynomials and sends his commitments to the master prover, which in turn derives random challenge and broadcast it to all the provers. Given this random challenge and specific powers, prover is able to compute partial linear combination. And you can see that there is just a single polynomial. And that means that the data required to store it or to send it is n times smaller than the size of his local circuit, where n is the number of polynomials.
00:06:50.545 - 00:07:42.653, Speaker A: And it turns out that this amount of data is acceptable for practice. For instance, if we are using a 32 bits field and the upper bound for the degree of polynomials is 2 to the power of 23, the amount of data is about 32 megabytes. Now, since all the partial linear combinations are collected. Master prover execute the regular version of Fry with the sum of these linear combinations. But this is not the end because we have to prove that the sum is actually a linear combination. So we have to provide consistency proofs for every random point that was used during the Fry protocol. But master prover simply cannot provide this information.
00:07:42.653 - 00:08:32.525, Speaker A: So he asks for it for other provers and gets corresponding response. Finally, we can know that Verifier also can verify this Merkle proofs for every respond. And in that way he can detect any malicious behavior from the provers. So in this way he can introduce some economic incentives or different ways of punishment for malicious behavior. Now let me compare this approach to others. So it is well known that distributed snarks can be constructed from additive commitments and the Pianist protocol is a great example of it. But usually such protocols require public key assumptions.
00:08:32.525 - 00:09:33.625, Speaker A: Fry based snarks protocols, which based on snarks like the Virgo system requires to send huge amount of data whose size is comparable to the size of the circuit. And distributed Fry can be considered like a balance between these approaches. But we can mention that the final proof of the final size for distributed Fry protocol will be a bit larger than the size of the proof of each individual prover. Here you can see a plot where we show a number of hashes hash invocations needed to verify every proof. It is a good measure to estimate the size of the verification circuit. And the orange line is for single proof of one individual prover. And a blue line is for concatenation of m proofs where M is equal to 10 in our case.
00:09:33.625 - 00:10:25.205, Speaker A: And the green line is for distributed Fry protocol output. Now let's move on to the second problem which is proven of correctness long running computation. The picture in this case is very similar to the previous one. But in this case the input of each prover can depend from the output of another. And for simplicity, the circuit is the same for every step. It's easy to see that in that case we simply cannot compute a single linear combination for all these provers because all these polynomials simply are not available simultaneously. And it means that we need to use some kind of recursive mechanism.
00:10:25.205 - 00:11:39.105, Speaker A: And as a starting point we can consider an IVC via recursive proof composition. So IVC prover produces a proof that the prior step, the prior that all the previous computations were done correctly and that there is a sorry IVC prover produces a proof that step function was computed correctly and that there is a proof that the prior step was correct. More specifically, we can combine a circuit which represents a step function and a circuit which represents a snark verifier into a single one single recursive circuit. And then we can use proverbs which takes as input the output of the previous step and the proof that all the previous computations were done correctly. And he launches snark prover. And by doing it, he attests the correctness of all the computations up to the current step. So recursively we have a proof that the entire long running computation is correct.
00:11:39.105 - 00:12:50.727, Speaker A: And the key aspect of IVC is that neither the IVC verifiers work nor IVC proof size depends on the number of steps in this computation. But we can see that recursive circuit contains the entire verification algorithm, which is quite a big overhead for practice. So there is a question. Can we move out some part of this verification algorithm outside of the circuit accumulated at every step and verify just once at the very end? And there is a natural idea to move out the most expensive part of the verification, which is proximity test. And as you can expect, we can try some similar thing. We can try to accumulate partial linear combinations. And a very similar idea is expressed in the paper Accumulation without homomorphism, which was published recently.
00:12:50.727 - 00:13:47.735, Speaker A: And transferring this idea to our settings, we have the following step procedure. So let polynomial G be an accumulator which is essentially a partial linear combination for all the polynomials that we encountered so far. And we want to accumulate a new bunch of functions F1 to FN. In order to do that, we provide linearity proof. It means that we produce a new accumulator polynomial commitment to it, and a short proof that the new polynomial that the commitment corresponds to the function which is close to the linear combination. The size of such proof is asymptotically smaller than the size of the proof of proximity test. And it means that this technique is efficient.
00:13:47.735 - 00:14:33.451, Speaker A: But in order to get this proof short, we used relaxed guarantee. Namely, we proved that the new function is close to the linear combination, but it might not be exactly one. And also it means that after a certain number of accumulations, we have such a big distance. It means that the distance between our current accumulator and the very first function grows with the number of number of steps. And it means that at some point the distance is so big that we simply cannot make any meaningful decision. This is more of theoretical problem. There are several workarounds for it.
00:14:33.451 - 00:15:25.013, Speaker A: But it also leads to practical issue. It means that the number of random queries needed to produce a linearity proof depends on the upper bound of maximum number of steps. And it turns out that at least for some practical sets of parameters, the size of linearity proof is comparable to the size of the proximity test. So it simply means that it's not applicable for these practical cases. So we have these two issues and I'm going to present a new approach that can address both. So here's the oracle batching protocol. We have a verifier and verifier has an oracle access to n functions f1 to fn.
00:15:25.013 - 00:16:21.139, Speaker A: Oracle Access is depicted as rectangular here. The prover in addition has polynomials themselves in a plain text and as an output verifier has a new oracle F new to the polynomial, such that its closeness to the Reed Solomon code means that Every initial function F1 to FN is close to the Reed Solomon code as well. This protocol is heavily inspired by round function of stored protocol, which was presented at ZK Summit 11 this year. So let me go through this protocol. So as you can expect, we are going to compute a linear combination. So the verifier samples a random point and sends it to the prover. Prover computes a linear combination.
00:16:21.139 - 00:17:41.945, Speaker A: But the key difference here is that we are using two disjoint domains for the Reed Solomon code and D prime. So these domains have the same size and their intersection is empty. And the prover computes a vector of evaluations of a new domain D prime and sends the corresponding oracle to the verifier. Then verifier makes an out of domain sample and gets the corresponding respond from the prover, which is equal to the evaluation at this point of of the linear combination. Then verifier samples additional t points from the domain D and again gets corresponding values from the initial functions f 1 to fn. Next, Verifier computes two polynomials vanishing polynomial z which is equal to 0 at each evaluation point, and polynomial Q which is essentially an interpolation of all evaluation points. And finally he constructs a new oracle F nu, which is actually a quotient, which is constructed by using oracle G and new polynomials Q and z.
00:17:41.945 - 00:18:38.755, Speaker A: And this protocol is sound. It means that if any of initial functions F1 to FN is far from the Reed Solman code, it means that if nu is also far from the Reed Solomon code with high probability and we don't have a distance decay here, it means that we can apply this protocol many times. Again we can apply transformation to make this protocol non interactive. And all that means that we can construct split accumulator scheme from this, which we call boil. In that case, accumulator is represented by two parts, short instance part and long weakness part. The weakness part actually is represented by a single polynomial G which is a linear combination of all the polynomials from the prior step. And the instance part is represented by several values.
00:18:38.755 - 00:19:25.555, Speaker A: The first one is a bit B which is used to represent which domain we are using. If bit b is equal 1, we use domain D prime with B is equal to 0, we use domain D. Then commitment to the Polynomial G&2 Polynomials Q& Z which are derived from the execution of the protocol at the prior step. So at every step we are going to accumulate a bunch of new polynomials F1 to FN. In order to do that, we execute non interactive version of the batch in Oracle protocol. So we get a proof and updated version of the accumulator. Namely we have polynomials Q prime, Z prime and G prime from the execution of the protocol and we change the value of the bit B to the opposite one.
00:19:25.555 - 00:20:41.295, Speaker A: And finally, to get the proof that entire long running computation is correct, we need to provide proximity proof for the final quotient polynomial which can be derived from the final accumulator state the size the complexity of this proximity test is higher, but again because we need to run it just once, it has better amortized cost and the final node using. So given this split accumulation scheme, we can use any well known compilers to achieve IVC IVC scheme from this. So let me summarize. We consider distributed Fry protocol which basically makes it possible to compute a single proof for a network of distributed provers. And the advantage is that this optimization can extremely easy to implement. So here's the link to our blog post where you can find more details on that. And later we described Boyle split accumulator technique so it makes it possible to construct IVC which is based exclusively on symmetric crypto assumptions.
00:20:41.295 - 00:21:06.815, Speaker A: And moreover it's practically efficient. For instance it doesn't require non native arithmetic. And for instance comparing with folding approaches, this witness part of the accumulator is just much smaller than the size of the whole circuit because. Because it's just a single polynomial. That's it. Thank you for your attention.
00:21:13.875 - 00:21:15.375, Speaker B: Any one more questions?
00:21:21.835 - 00:21:53.425, Speaker C: So in non anti tip homomorphic foldings the issue that we make market lookups in different points of evaluation domain and that's why having distance is growing. And in your approach do you make lookups in different points for different foldings or in the same points?
00:21:57.695 - 00:22:11.235, Speaker A: The key difference is that we are acquiring points outside of the domain where our Oracle is defined. It's the key difference what exactly? It makes it possible to keep the distance constant.
00:22:12.455 - 00:22:13.675, Speaker C: Okay, thank you.
00:22:20.065 - 00:22:46.021, Speaker D: Thank you for the presentation. I do have a kind of a fundamental question. So, regarding the whole setup, what is the secure assumption about your collision resistance? What are your collision resistance assumptions? So you were talking about the distributed proverbs and there is a master prover. So do you assume that all of these are just kind of not looking with each other and they are workers on your cluster? What's the concrete aggregation setup that you're using?
00:22:46.153 - 00:23:14.973, Speaker A: Actually, it depends. So you can consider different options. So in this case, which I described, I assumed that some prover can cheat and master prover will detect it. So he can detect it before the final proof is produced. But there is another case. So actually if there is distributed proof generation, we might think of all these provers as a single entity. So they trust each other.
00:23:14.973 - 00:23:17.605, Speaker A: I think it's reasonable for some cases.
00:23:17.725 - 00:23:32.065, Speaker D: So masterproover is trusted for consistency, but not for. So it's honest but curious or what? What's the assumption about the master prover? Is master prover allowed to execute different code or do I assume that the master prover is like, honest?
00:23:33.325 - 00:23:43.633, Speaker A: No, no. So the output of the master prover is the proof, so it has to be verified. So we do not trust master proof in that sense.
00:23:43.729 - 00:23:45.965, Speaker D: Right. Okay, thank you.
00:23:51.065 - 00:23:51.513, Speaker E: Cool.
00:23:51.569 - 00:23:53.125, Speaker B: Anyone else have any questions?
00:23:56.345 - 00:24:09.627, Speaker E: Could you talk a bit about the performance and like proof size and proving time? The maybe you have it. I didn't, sorry.
00:24:09.771 - 00:24:39.791, Speaker A: Yeah, so I think we thought about it and we decided that the number of hash invocations to verify the proof, it's a good measure to present because the actual time complexity of the prover significantly depends on the nature of the computation. It depends whether we use lookups or not. Depends on many things. So, yeah, we go with this number of hashing vacations to verify.
00:24:39.983 - 00:24:41.355, Speaker E: Yeah, thanks.
00:24:44.735 - 00:24:45.715, Speaker B: Last call.
00:24:47.215 - 00:24:47.711, Speaker E: Cool.
00:24:47.783 - 00:24:48.255, Speaker B: Thank you so much.
