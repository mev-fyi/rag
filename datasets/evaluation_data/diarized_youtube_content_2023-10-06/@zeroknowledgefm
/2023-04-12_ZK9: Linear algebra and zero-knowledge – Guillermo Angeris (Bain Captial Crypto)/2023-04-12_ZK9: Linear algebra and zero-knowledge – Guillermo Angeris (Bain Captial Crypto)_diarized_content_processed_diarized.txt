00:00:06.970 - 00:00:18.794, Speaker A: Guillermo is going to come up and share a little bit about linear algebra and zero knowledge. Welcome Guillermo back to the stage as a speaker this time and the speaker.
00:00:18.842 - 00:00:38.422, Speaker B: This time, for better or worse. I guess we'll find out next. Yeah, cool. So this is, is work done jointly with Alex, who's actually right here. So if there's any mistakes on the slide, please direct them just straight at him, not me. But I really do hope you catch some mistakes because I'm sure they will exist and I would prefer to not have them. All right, great.
00:00:38.422 - 00:01:17.190, Speaker B: So with that, let's get started. So, very quickly, what we're going to do in about ten minutes is we're going to do a hilarious amount of things, which is namely generalize a number of things that we know in ZK to just way more broad situations. So, specifically, ZK depends a lot on the fact that polynomials have this very nice structure. They have this property that if you test a specific point of a polynomial with high probability, if it's zero, then with high probability, the coefficients of the polynomial must have been zero. Blah, blah, blah. We're going to show that actually none of those results depend on polynomials at all. They're actually properties of something called error correcting codes.
00:01:17.190 - 00:01:50.480, Speaker B: And the idea is to do this with something called, hopefully good notation, linear algebra. And just a tiny, tiny, tiny salt based sprinkle of eccs. Okay, so for anyone who is not well aware of linear algebra, linear algebra is very, very nice. Over the real and complex numbers, everything is lovely. Your intuition makes sense. The sun is shining and the ponies are out. Unfortunately, linear algebra over finite fields is an eldritch monster with a nice smiley face in the front that says, yes, sure, you can do linear algebra, come in.
00:01:50.480 - 00:02:19.346, Speaker B: But it will eat you extremely quickly, so beware. Okay, so, unfortunately, in order to get everything in ten minutes, you have to eat a few vegetables before we can get to the dessert. So let's get started on that. So what we're going to say is we have probabilistic implications. This is kind of a weird thing that only exists in data science, and I'm just going to pretend it exists everywhere else. What does it mean? It means, so we say pr, which is a statement that depends on some randomness. R implies another statement, qr, that depends on randomness.
00:02:19.346 - 00:02:55.822, Speaker B: Qr prime with high probability, if that implication is true, is false, with probability at most p. So if p is very small, it means that the implication is almost always true. Right? So there's a number of interesting downstream consequences. This is just what people call soundness error. But here is like a calculus for these things, which is you can chain implications together. There's a simple variation of this. It says if p implies q with probability p and q implies t with probability p prime, then indeed, kind of as one would expect, you can chain those things and say p implies t, but the probability here is slightly larger.
00:02:55.822 - 00:03:32.766, Speaker B: It's p plus p prime. So this is kind of an extension of the normal notion of implication that we all know and love, which is that if a thing implies another thing and that thing implies something else, then the original and the last thing are implications of each other. So many other things follow. This is kind of going to be the only one that's important, but we have to carry around this extra error term. Okay, so what is an error correcting code? I'm going to just put it out there. A linear code is just a matrix, and the matrix has m columns and n row, sorry, m rows and n columns. We say we encode a vector x into a much larger code word, g of x.
00:03:32.766 - 00:04:18.046, Speaker B: And here's my shitty drawing of an encoding which says you take an x and then you blow it up to some larger message, or in this case, code word g of x. Okay, the trivial code, all it says is, here's my encoding, you give me a message, congratulations, you got the message back. That is a perfectly reasonable encoding under our framework, although it won't be that interesting. But the probably more common encoding in Zk is the Reed Solomon code. What it is is it says if there's anything you get out of a stock is forget all of the linear algebra. What we're going to do is we're going to say x is the coefficients of polynomial of degree n minus one g of x means you're going to encode that polynomial or encode that message into a polynomial and gx of r says, evaluate that polynomial at a point r. That's it.
00:04:18.046 - 00:04:52.970, Speaker B: So this is the one. If you ever get lost in the sauce, I would recommend just go back to this Quan definition, and this will kind of set you back on the right track. But this is true for arbitrary linear codes, and there's many of them that are not just read Solomon codes. Okay, we're going to use exactly one definition from coding theory, and that is the distance D of a code. This is D of a code is a very weird but actually useful thing that says, I'm going to encode all possible messages. So there's a list of messages that are all possible messages. Encode all of them, and then find the one that has the minimum number of nonzero elements.
00:04:52.970 - 00:05:53.718, Speaker B: That number is going to be called the distance. So the idea is, if you think about it, if you have a vector and it has some error, the encoding, if the distance is large, will actually expand that error into a bunch of other places in the encoded vector. And this is kind of going to be useful because if you have some error somewhere, you want to have high probability of catching it by just like checking the output or a very small portion of the output. And that's the point of an error correcting code. Okay, so the trivial code is just an example, has distance one. Of course, any nonzero vector will have distance that's not zero because it needs to have at least one nonzero element, but it's going to have distance exactly one because there's take a vector with only a single one and zeros everywhere else. The read Solomon code actually has this funny little distance, which is you can think about it as message with length n can be encoded into a degree n minus one polynomial, right? So it has at most n minus one zeros and minus one roots.
00:05:53.718 - 00:06:17.666, Speaker B: So because of that, if we think of the set of all possible evaluations of the polynomial, it's going to be zero. And at most the field, which is all possible evaluations, minus the places where it can be zero. So it's going to be nonzero in all of those spots. It's potentially a very, very large number. Okay, we've eaten all the vegetables. Let's get to some interesting results very quickly. We're going to do a bunch of zk in like five minutes.
00:06:17.666 - 00:07:06.502, Speaker B: So the usual zero check. What are we doing? All right, take a vector x, encode it, and then check a random symbol from the encoding check if it's equal to zero. If that's equal to zero, then with high probability the original thing must have been zero. Why? Well, that's kind of the definition of distance, right? The definition of distance says anything, whatever it is, is going to have very large distance d, right? And with high probability, if that thing is non zero, if d is very, very large relative to the size with high probability, I'll hit one of those nonzero entries. So for a read solomon code, this is exactly the same as the one dimensional Schwartz sipple lemma, if one wants to think about it. But the point is you get exactly the same bound that you would expect, which is, what's the probability that a nonzero polynomial value is zero at a point at a randomly chosen point, but it's the degree divided by the field size. So a generalized zero check says, okay, well, I don't want to check if a single vector is zero.
00:07:06.502 - 00:07:25.926, Speaker B: I want to check if a bunch of vectors are zero. What are we going to do? We're going to smash all the vectors together in this particular way. We're going to choose a random row of g. I'll give a diagram for this in about a second. Choose a random row of g, and then use that as the coefficients for a linear combination of these, like zeros of these vectors. Sorry. So you have y one through y n.
00:07:25.926 - 00:07:52.994, Speaker B: You want to check if all of them are zero? I don't want to check if each individual one is zero. What we're going to do is we're going to smash them all together and then check if that resulting vector is zero. To do that again, we'd pick a random row. We use that as the linear coefficients. We smash them all together, and then we check that that resulting vector is zero. And indeed, if that resulting vector is zero, then with high probability, the original vectors must have been zero in the first place. In fact, the probability bound is exactly the same as the original one.
00:07:52.994 - 00:08:31.146, Speaker B: For the original test and for a read Solman code, this is exactly the same thing. The degree of a polynomial, or the number, the length of the message minus one divided by the field size. Okay, the picture, of course, is we have m rows, which use a random row uniformly at random, and we use that row as the coefficients of the linear combination of y one, y two all the way to y n, and then check if that's equal to zero. The high level proof is kind of silly, but it says, look, okay, I'm going to take a linear combination of a bunch of things that are nonzero. Then with high probability that the places that nonzero are going to be kept in the final vector. That's, again, just from the definition of distance. Okay, now you could be like, ha ha, I'm very smart.
00:08:31.146 - 00:09:09.542, Speaker B: What am I going to do? Here's a bunch of math. I have a thing that lets me check if a single vector is zero by checking a random element, right? I also have a thing that says I can take a bunch of vectors and smash them all together into one vector and check if that thing is zero. So why don't we just put the two things together, right? I'm going to take a list of element, a list of vectors. I'm going to smash them together into one vector, and then I'm going to encode that vector and then check if that element is zero. Right. All we're doing is we're kind of concatenating the two previous things that we talked about into just one single protocol. And indeed, what you find is of course, by the fact that we can have these very nice implications.
00:09:09.542 - 00:09:52.746, Speaker B: With additive soundness error, we get the probability of this final check kind of written out in math, not super important, is actually at most p plus p prime. And it turns out, actually, if you guys are familiar with this, this is exactly the Schwartzblemma. And it turns out that this actually shows that it's not tight and it's generalized also to arbitrary linear codes. So it does not depend on the fact short simple does not depend on the fact that polynomials are in the mix. It's actually true for any general linear code. Okay, finally, we'll say one interesting thing that I don't actually think is known that says, let's say I want to check that a list of vectors each belong into some subspace v. How can I do that? Well, sure, I can check that each individual vector belongs in subspace v.
00:09:52.746 - 00:10:25.342, Speaker B: Instead, I can do the same trick that I did before, which is I pick a random row of the big code matrix. I use that as the weights for a vector combination, and then afterwards I check that the resulting single vector is in the subspace v. And that indeed will, high probability will imply that the original vectors must have each been in the original subspace for a read Solomon code. Again, same probability bounds. Okay, finally we're going to get. So you might say, okay, the probability bounds are all the same. That's kind of, you know, maybe we know a few of these things, maybe we don't.
00:10:25.342 - 00:10:57.440, Speaker B: There's some quick sparsity results, and these are going to be very similar to those of the harrow, very similar to those of fry and spirit. So the sparse zero check says, okay, I want to check if a vector is sparse. There's many ways of doing that. Of course. I can go through every entry of the vector x and then check if the number of nonzero entries actually matches the bound that I want. Instead, I can just say, okay, I'm going to randomly sample a bunch of elements from x and check if all of them are zero. If all of them are zero, then with high probability you probably don't have that many things that are not zero.
00:10:57.440 - 00:11:39.210, Speaker B: Indeed, the bound is kind of exponential in the number of tests that you have. So s here is a uniformly randomly chosen subset of the entries that you are checking and p here is the probability, which is exponentially decreasing, and how many indices you test. So this is lehero lite, which says, okay, I'd not want to check if one vector is in this sparse. I want to check if a bunch of vectors are individually sparse. And lo and behold, the same trick happens to hold. You take a random row, you sum all of the vectors using those weights, and then afterwards you check if that one single vector is sparse, and with high probability. That will imply that the original vectors each must have individually been sparse.
00:11:39.210 - 00:12:08.594, Speaker B: This is again kind of weird, right? Like the same trick is popping up over and over and over again. We have it all in the same notation, all in exactly the same way. And indeed, actually you can show that this implies the original claim. And finally, we have this generalized Leharo. So this is almost, but not fully proven. But the point is, a classic thing you want to do is you want to check that a bunch of things are close to a certain vector space. One way of doing that again is checking that every individual element is close to a vector space.
00:12:08.594 - 00:12:44.638, Speaker B: But that's a bad thing to do because that requires a lot of work. So instead you can just take a bunch of linear, random linear combination of all of those things, and then check that the resulting linear combination, which is a single vector, is close to the vector space itself. That will, again, hopefully with high probability, imply that each of the individual elements must have been close to the vector space in the first place. There's a condition on how far, how sparse your thing can be, but that we'll punt it for later. The point is, actually, this part of the spoof is still open, and for technical reasons I'm happy to get into later. So please come and chat with us. This is interesting.
00:12:44.638 - 00:13:20.298, Speaker B: Anyways, so we just did seven checks in a little bit over ten minutes, very fast. So hopefully everyone got it. There's going to be a quiz at the end of this, many of the generalizations here. But the point here is all of these things are kind of straightforward, right? Like you write down the math and everything looks kind of exactly identical, and you're just doing the same thing over and over and over again and a bunch of different things in exactly one framework. So we can replace, the second point here is that we can replace read Solomon codes with general linear codes, and we kind of don't really lose anything. So there might be codes that are more or less computationally efficient. That might be interesting to use in certain parts of proofs, depending on what's needed.
00:13:20.298 - 00:13:45.910, Speaker B: And finally, and most importantly, we can understand many of the systems that are many of these individual proofs that we all end up dealing with all the time in just one single framework. So with that, the paper will hopefully come soon. Although I make no promises. Okay, I make a few promises, but with high probability promises. And then just want to say thank you for listening. And thank you to Aki for actually some of the interesting conversations. Cheers.
00:13:52.010 - 00:14:01.180, Speaker A: Should we take one question? I think I had you start like, does anyone have a question on this?
00:14:02.830 - 00:14:05.370, Speaker B: Yeah, I think everyone understood everything.
00:14:05.440 - 00:14:06.762, Speaker A: Everyone understood, right?
00:14:06.896 - 00:14:10.540, Speaker B: 100% right. Great. Okay. Oh, you got one?
00:14:11.650 - 00:14:14.240, Speaker A: Oh, just 1 second. I think we're going to bring you a mic.
00:14:17.410 - 00:14:35.430, Speaker B: With regards to codes themselves, do they have to be mds or it can be any linear, any code? I didn't even define mds, actually. And that wasn't just out of spite, that was actually just actually not required. What's the prehesion? Thanks. Cool.
00:14:35.500 - 00:14:37.586, Speaker A: Okay, fantastic.
00:14:37.778 - 00:14:38.294, Speaker B: Thank you.
00:14:38.332 - 00:14:41.410, Speaker A: I don't think we really have time for another one anyway, so thank you so much, Guillermo.
