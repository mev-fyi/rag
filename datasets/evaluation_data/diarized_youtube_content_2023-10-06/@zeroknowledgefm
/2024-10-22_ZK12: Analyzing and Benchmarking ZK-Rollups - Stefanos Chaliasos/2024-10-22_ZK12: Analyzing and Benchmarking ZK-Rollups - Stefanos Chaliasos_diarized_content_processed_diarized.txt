00:00:08.800 - 00:01:26.025, Speaker A: So today I'm going to present a joint work with Itamar Adria, Jens Akis and Ben on analyzing and benchmarking ZK rollups. So I will start with a brief introduction on what is blockchain scalability, what are roll ups and what are ZK rollups. And then I will describe what we did our analysis, some preliminary benchmark results for zksync ERA and Polygon zkvm and also then I will discuss a bit the implications in particular for transaction fee mechanism design for ZK rollups. So blockchain scalability is often framed in the context of the blockchain trilemma, which is basically which highlights that we can't have simultaneously scalability, decentralization and security. And blockchains such as Bitcoin and Ethereum have historically focused on decentralization and security. Whereas someone might claim that some of the modern blockchains can achieve all three of them, but that's a different topic. So if we want to scale something like Ethereum, what we can do is we can use rollups.
00:01:26.025 - 00:02:23.725, Speaker A: And rollups are L2 solutions, scalability solutions that basically move the execution of chain. They execute transactions, update the states of chain and then they submit some data. And we will see what data on chain and do some verification of that execution on the main chain, which is what we call on chain. And in particular Ethereum, in the last couple of years it has a very rollup centric roadmap. There have been improvements that are mainly focused for improving the efficiency of rollups. The most important example is proto dunk sharding ape4844, which basically introduced blobs that makes DA much cheaper in Ethereum. So how a rollup works is that we have a completely different blockchain which we call the rollup or L2 or off chain computation.
00:02:23.725 - 00:04:01.585, Speaker A: And in that rollup we get transactions, we mine the transactions into a block or validate them into a block and then we batch together multiple blocks that we will then submit the data for that batch on the L1 and later we will actually verify the correct execution of the transactions for the bots. And we have two main types of roll ups, optimistic rollups, which basically they secure their transactions with fraud proof in an optimistic way. So they assume that all transactions are valid by default and only run some computation, a fraud proof basically mechanism in the event that some observers says that some of the execution was not correct and they have to submit all the data. And typically they have a quite long finality period because you have to Give at the moment it's one week for observers to make sure that have checked all the bots and to basically finalize completely the transactions of the bots. The other option are ZK rollups which are basically using succinct proofs of valid execution that can be verified on chain without re executing them. Verifying a validity proof submitted on the old chain has become much cheaper the last couple of years. We have more options on compressing the data.
00:04:01.585 - 00:04:47.779, Speaker A: We need to publish all the L1 and also time to finality. It could take some minutes or at most some hours nowadays. So now I will give a more thorough overview of how explicitly ZK rollup works. So we have LT users that interact with our L2 blockchain, the ZK rollup in that case and they submit transactions to the sequencer. At that point their transactions are pending. Then the sequencer creates some blocks and batch together all the transactions into a single batch. And the size of a batch could be from some dozens of transactions up to some thousands of transactions.
00:04:47.779 - 00:05:57.103, Speaker A: And at that point we say that our transactions are preconfirmed, have been executed in the L2 and then we commit to that batch in the L1. So we create an L1 transaction that publish the data of that batch, the data for the transactions of that batch in the L1. And at that stage we say that our transaction is committed. In the meantime, the L2 will send the batch to approver and the prover will actually create a succinct proof for the batch. And when that succinct proof is created, we will submit that proof to NL1 smart contract that will actually run the verifier and verify the correct execution of the transactions. And at that point we can say that our transactions have been finalized and we are safe to proceed. So we had a few research questions for ZK rollups in particularly and we wanted to investigate it.
00:05:57.103 - 00:06:57.205, Speaker A: What are the fixed costs? And fixed costs are the cost of a batch independently of the transactions that have been included in the batch. Marginal cost has to do specifically with transactions. And then we have we want to explore the trade offs between fast finality and cost minimization. We will present a complete breakdown of the different costs for zksync era and Polygon with some custom pretty naive payloads. And finally we will discuss a bit the impact of blobs. So here are some of the ZK rollups that have been implemented. We have Polygon, scroll and ZKE Synchera which basically use ZKEVM EVM as their vm and they try to be either EVM compatible or solid decompatible.
00:06:57.205 - 00:08:17.357, Speaker A: And then we have some other options. For example starknet, we have Aztec, which is more like a privacy oriented rollup. So we chose to go with Polygon and Zksync Era because we wanted to focus in scalability solutions, we wanted to focus in systems that are in production and we also wanted to have a solidity compatibility to be able to compare kind of apples to apples, although that's not completely true as we will see. And those were the only two solutions that at that point at least were fully open sourced. So some notes here is that both use two proof systems, one for the initial proving and then they want to convert a larger proof, a stark proof, typically to a smaller, more succinct proof that will submit to the L1. And another main difference is that Polygon submits transaction Data in the L1 publish transaction data, whereas ZKE Synchera uses stated. And finally Polygon uses a large CPU based machine, at least at that point.
00:08:17.357 - 00:09:17.625, Speaker A: And ZKE Sync Era has a more complicated hardware specification where basically they use many GPU based kind of smaller machines. So here is another way to see the landscape and basically the compilation process. And as I said, we want to start from the solidity part to be able to transact to compare similar payloads. So what are the costs? First we have fixed cost which is composed by settlement and proof compression. By settlement we mean two things. First is the transaction to commit to a specific batch on the L1 and that includes all the gas cost, but not the DA cost for that batch. And secondly is submitting the proof and executing the verification logic in an L1 contract.
00:09:17.625 - 00:10:29.375, Speaker A: And secondly, another fixed cost is the proof compressor of proof transformation which is going from a star proof to snark proof that's also fixed independently of the size or the type of transactions in the batch. And for marginal cost we have da, which is how much does it cost to publish some Data on the L1 we have the proving cost which is how much does it cost to prove some transactions. And we also have some other L2 cost that for that work we will say are negligible. So here is our methodology. So we start with some payloads and we opt for some very naive and simple payloads because we wanted to have full control, we wanted to be able to run our methodology for both rollups. Ideally you would want you would like to experiment a bit with get some historical blocks from Ethereum and run them through both, but that's a bit more Complicated. It's something we want to do in the future.
00:10:29.375 - 00:11:15.409, Speaker A: And then we have our metrics. So when we run the payloads through the roll ups, we get the bots and then in the bots we extract the DA we will need to publish in the L1. We also extract the settlement cost, the L1 gas. We will have to pay for both committing and verifying the proof. And then we have the prover. So for the prover what we decided to do is to keep track of seconds per proof and also USD per proof and USD per proof in a transaction. And I will present some results later on.
00:11:15.409 - 00:12:12.453, Speaker A: And finally the configuration for both machines is this one here. I should note that both prices are a bit, not a bit like two or three, maybe even five times more than what Polygon and Ziggy Sinkera Mater Labs are paying because they have better deals with Google and aws. We just got a machine for a month and that was it. So you can bring those costs even lower. So here we break down the fixed costs and I want to start with describing the first major change between the two approaches. Zksync has really huge batches, whereas Polygon works in smaller batches that then actually aggregates multiple batch into a single proof. And it works in that way.
00:12:12.453 - 00:13:02.273, Speaker A: So we can see that ZK synchronous bodies are in terms of some thousands, whereas Polygon are some dozens of transactions. The theoretical match batch size was at that point, maybe it has been improved. And it's also simple ETH transfers. So if you want to do more complicated transactions then that it's much smaller. Then we have all the fixed costs and the numbers here are pretty comparable. We can see again the difference in Polygon's case that you have multiple batches included in a single operation. For for example, typically they submit five batches when they perform one commit transaction, et cetera.
00:13:02.273 - 00:13:57.931, Speaker A: And what we wanted to focus here is the normalized cost per transaction. And we can see that it's pretty low. And as we will see later on, fixed costs are the main cost for ZK rollup transactions at the moment. And we will analyze a bit more how that might change if DA becomes more expensive, if proving becomes cheaper, et cetera. And also we had the normalized cost in the theoretical case where you have as many transactions as possible. And we can see that the number is pretty low for both rollups. So here we have the first results for the marginal costs and we have how many bytes we require per each of those payloads.
00:13:57.931 - 00:15:06.605, Speaker A: In terms for da, our payloads are super biased towards state divs. And that's because, for example, if we have many transactions, that means that if we have many transactions, all those transactions will kind of touch similar state. So state, it's the perfect situation for status. And I want to highlight that result because we will see later how that might affect how you want to actually sequence transactions. But we can see that in extreme cases you can achieve pretty good compression, I will say, by using state divs. Next, here are some marginal cost results for Polygon zkvm and we can see on the left figure the proving plus compression time and how it evolves when we have more bigger inputs. This is I think for transfers or ERC 20 transfers, I don't remember.
00:15:06.605 - 00:15:47.737, Speaker A: And we have the time it requires to do the proving. And that's important because basically our finality time is bounded to our proving time. And on the right side we can see the cost per transaction. And we have two lines. The first line, it's when we use cold data as our da, so that's why it's more expensive. And the second line is when we use blobs. So we can see that especially for blobs, if you manage to fill a bot then it doesn't make sense to wait more time and to detriment your finality.
00:15:47.737 - 00:17:07.767, Speaker A: But it's another trade off and there could be different options on how to handle that. On the Zekiro website, here is the total costs of Polygon's ZK VMs. And so first of all, DA at the moment it's virtually free for rollups in general. So all those results should be taken with a bit of salt because that might change in the future if changes happen or if there is more usage of blobs and more demand and prices go up. And that's why we also captured what would have been the cost if they used cold data with prices before the introduction of proto dank charging, which you can see when we have a larger size of bats, then it's our main cost in case of using cold data. Here we have basically our main fixed cost. We can see that proof compression is pretty cheap already and I think both teams have done an order of magnitude improvement in their proof compression.
00:17:07.767 - 00:18:08.253, Speaker A: And we can see on the left side is the percentage of commit cost when we use blobs as the A and on the right side is when we used cold data. And we can see that the feast cost actually decreases as we increase the size. And that's because the fixed cost per transaction, it's lower because we have larger batches. And also we can see that proving starts to become relevant only if we try to actually minimize our costs, which is the case when we try to have huge budgets. And only that case it starts to be a bit relevant in the cases where DA it's not expensive at all, it's virtually free. And also here is the cost per transaction. And we can see that as we increase the batch size then the cost per transaction decreases.
00:18:08.253 - 00:19:11.409, Speaker A: But that comes at the cost of longer finality times, which might be important for some users, might not be important from some other users. So what are some main trade offs? We have the trade, we have some main takeaways, we have the trade off between fast finality and cost efficiency. And that's especially important when you don't have high volumes of transactions and maybe you want to delay a bit so you provide cheaper prices to to your users. Or you might have to have different mechanisms there. So some people might opt for fast finality and some others are okay with waiting a bit to be included in the next batch. Then we have stated this efficiency and here we saw the extreme case of how much better stated could be. What I want to mention here is pretty close to the next point.
00:19:11.409 - 00:20:04.095, Speaker A: Importance of sequencing. So for state divs, normally in L1 I would say which transactions are going to be in the next block. It's more about MEV in rollups. At the moment we don't have mev, but even if we had, it's a nice question, especially if DA is your main cost to try to put in the next block and the next batch. Basically transactions that will maximize your state diff your state diff's impact. And similarly what we noticed is that we have a similar case for provers. So for example, in Polygon's case, if you have a transaction that does a single chess hash, you will have to pay the cost for proving chase husks.
00:20:04.095 - 00:21:27.595, Speaker A: So what you want to do to amortize that cost is to include as many transactions to fill that chess space in the prover to minimize your proving cost per each of those transactions. And finally we also saw that basically the impact of APE4844 what's great for ZK rollups. There is a lot of discussion if that should change, if that should not change. But we guess that some strategies it will be nice to be developed in case that blobs start becoming more expensive. Then you might want to move temporarily to cold data or if you don't have many transactions and many data to post for your batches but you want to have fast finality. You might want to try to share blob space with other roll ups if that's possible, etc. So some open research questions we have identified is basically what is the impact of this decentralization on the cost? What happens if we have if we start having more costs in the L2? Now we said those costs are negligible but that might change.
00:21:27.595 - 00:22:23.225, Speaker A: What are some strategies for batch size and sequencing optimizations and how you can basically take the best out of your prover and out of the state dips or what other else compression you are applying. We also want to do some metering mechanism evaluation because in some cases the proving cost may not completely captured by the existing metering mechanism. And also we want to try to develop a proving market mechanism specifically for ZK rollups that take into consideration all the results of this work. So that's it. You can find our paper in that and we also have published all the scripts all the results in the GitHub repo that you can find through the paper. Thank you.
