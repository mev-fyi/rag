00:00:06.330 - 00:00:18.750, Speaker A: Welcome to Zero Knowledge, a podcast where we explore the latest in blockchain technology and the decentralized web. The show is hosted by me, Anna and me Frederick.
00:00:25.130 - 00:00:35.910, Speaker B: In this episode, we sit down with Alex and Alex from Matter Labs to discuss how they plan to use zgay snarks to scale up the transaction processing capacity of Ethereum.
00:00:43.950 - 00:01:20.486, Speaker A: Before we start, we just want to speak to you, our listeners, for a quick moment. As you may know, we rely on our supporters, subscribers and sponsors to make this show possible. If you are thinking, hey, I like this show and I wouldn't mind supporting it, then there are a bunch of ways that you can do this. You can share a monthly amount in Dai through our Gitcoin grant or join us on Patreon if you'd rather do this in fiat. You can also support us directly through our bitcoin and Ethereum addresses as well as our new Zcash address. Thank you to the listener who pointed out that Zcash might be an interesting option for our audience. We want to say a big thank you to the folks who have supported us on Patreon and Gitcoin so far.
00:01:20.486 - 00:01:44.474, Speaker A: We really appreciate the support and you really do help to make this podcast possible. If you want to support the show, we have all of our links in the description for how to do that, or if you're a project, please get in touch with us about sponsorships. So now here's our interview with matter. So today we're sitting with the guys from Matter Labs. Hi, guys.
00:01:44.592 - 00:01:47.194, Speaker C: Hey, Ayanna and Frederick.
00:01:47.242 - 00:01:48.080, Speaker B: Hello. Hello.
00:01:49.330 - 00:02:12.502, Speaker A: Today we get a chance to explore a little bit what matterlabs are working on. This will be a zero knowledge episode, which is kind of exciting. It's kind of one of the first times where we talk about how zero knowledge could actually be used for something other than privacy. Instead, we're going to be talking about how it could be used for scaling. So what if we start off with a quick intro? Do you guys want to just tell us who you are and what you're working on?
00:02:12.556 - 00:02:18.050, Speaker C: Yeah. So we're from Metal Labs. My name is Alex. My co founder Alex is also Alex.
00:02:18.210 - 00:02:30.534, Speaker D: Just for the sake of clarity, it's a difference in full first names. Another Alex is Alexi. I'm Alexander. But of course, no one uses a full name, even while it's a distinguisher. I'm Alexander.
00:02:30.582 - 00:02:31.654, Speaker A: You're Alexander.
00:02:31.782 - 00:02:32.802, Speaker C: I'm Alexey.
00:02:32.886 - 00:02:34.046, Speaker A: Okay, got it?
00:02:34.148 - 00:03:03.000, Speaker C: Yeah. So at Matterlabs, we are using zero knowledge proofs to scale blockchains. We met at Defcon last year and we came from different backgrounds to simultaneously to same idea that zero knowledge proof, specifically snarks and starks, the succinct zero knowledge proof techniques can be used for scaling and we decided to use it for scaling Ethereum in the first place because Ethereum is very well suited for this kind of technology.
00:03:03.770 - 00:03:05.480, Speaker A: What were you working on before?
00:03:05.850 - 00:03:31.882, Speaker C: My background is in software engineering. I studied in Kiev and Berlin, and I was in Berlin in a couple of startups as a CTO, building teams, worked mostly in the old new economy on the online things like trade show management and camper sharing platform. I was co founder of Pole Kemper, which is the largest european startup for camper sharing.
00:03:31.946 - 00:03:32.702, Speaker D: Oh, cool.
00:03:32.836 - 00:03:34.850, Speaker A: How about you? What is your background?
00:03:35.270 - 00:04:36.258, Speaker D: Okay, as you remember from CK Summit, in the matter of fact, I'm russian nuclear physicist, my master is in higher energy physics, and my phd is in electrical engineering. Now I work on blockchains as a practical application. So it's just one step at a time from pure science to something practical. Yeah, I was working in a plasma environment for quite a long, so I'm much more familiar with scaling solutions in principle. And I mean, even before at the Defcon itself where we met, I was a member of the plasma panel discussion, like discussing what's happening in there, what can be done and improved. But then we decided, even while plasmas can give thousands of transactions per second, we can use snarks right now to maybe reach not thousands by hundreds, but with another completely different user experience and other trade offs. And we find these trade offs and user experience more important than reaching the ultimate benchmark of thousands of transactions per second.
00:04:36.258 - 00:04:38.242, Speaker D: So we decided to go this way.
00:04:38.376 - 00:04:42.178, Speaker A: So you were in Devcon, you were actually on a panel speaking about plasma.
00:04:42.274 - 00:04:42.822, Speaker D: Yeah.
00:04:42.956 - 00:04:45.910, Speaker A: When you said you're working in plasma environments, what does that mean?
00:04:45.980 - 00:05:07.402, Speaker D: I mean, in a sense that when Vitalik introduced plasma in 2017, in summer, like August, I think, then I was at the first eth global hackathons one in Waterloo, and there the hackathon project was like a first demo of the plasma, which we did over the weekend, which hackathon eth. Waterloo.
00:05:07.466 - 00:05:09.994, Speaker A: Waterloo. Oh, cool. Waterloo.
00:05:10.042 - 00:05:46.054, Speaker D: Yeah. And so from then, actually around this time, there was also snarks blog post by Vitalik. There was a first paper for Starx with two gigabyte proofs, if I remember correctly. So somewhere from there, everything started for me, and since then, I was working continuously on plasma for this minimal viable plasma, more viable plasma. What are plasma cache. What are plasma prime, where I say accumulators all this fancy stuff with every construction having its own benefits and trade offs. And now we have the construction using ZK snarks for scaling, which has other benefits and trade offs.
00:05:46.054 - 00:05:48.330, Speaker D: And for now, I work in this one.
00:05:48.480 - 00:05:52.730, Speaker A: When you guys met, who is the team? That is matter.
00:05:52.880 - 00:06:15.654, Speaker D: Well, I mean, the matter as itself has a little bit longer history than before me and Alex met at the Defcon, but in principle, it's still very small. It's like five engineers. That's it. Engineers in a sense. Hardcore engineers work in GPU provers who know how gross 16 security proof works. Sorry if I'm too technical, you can just say this. Technical, okay.
00:06:15.654 - 00:06:55.410, Speaker D: But yeah, this is how we work. It's very narrowly focused. Very narrowly in principle, every one of us could do almost everything that other person does. For example, I write the code very fast, efficient, but in a hackathon style, maybe not too readable. Alex, another Alex, always tells me this. He works in a production mode, so much more slower, but, well, it's more readable still. I find my code readable too, but yeah, we're all different, but still very well focused and kind of not replaceable, but reshuffable.
00:06:56.630 - 00:07:15.922, Speaker B: I'm curious to hear, coming from the plasma space and then moving into what you guys are doing at matter, is plasma still involved in this in some sense, or have you completely abandoned that and gone Zk? Zero knowledge proofs are the future of scaling.
00:07:15.986 - 00:07:36.154, Speaker C: So, in fact, when we first came up with the first version of the product, we called it zero knowledge based plasma, because we thought it's going to be the easiest way for people to comprehend what we're doing. It's very, very similar to plasma. The difference is that each new block or each new state transition is secured by zero knowledge proofs.
00:07:36.282 - 00:07:48.402, Speaker A: I think one thing I want to do before we jump even deeper into this, I don't think we've ever defined plasma on this podcast. So can you actually quickly summarize what plasma is?
00:07:48.536 - 00:08:23.438, Speaker C: Plasma is a layer two scaling solution, which in principle relies on layer one for security, but it has strictly sublinear communication complexity on the main chain. So we submit less data in terms of the amount of transactions, or compared to amount of transactions to the main chain than we actually process in the side chain. However, if something goes wrong, we always have the main chain as a witness of what the canonical state is by the virtue of commitment, cryptographic commitment, to the new state.
00:08:23.604 - 00:08:33.362, Speaker A: And when you first developed the Zk for plasma, then you were using zero knowledge proofs to do that. Check in point with the main net.
00:08:33.496 - 00:09:20.446, Speaker C: So we still do the normal check in point. We just commit this new state to the main chain, but then we use zero knowledge proofs to secure this in plasma, you rely on game theoretical assumptions. Other people should, or you maybe will monitor all the blocks. And if you find a mistake, malicious or not malicious, then you can go on main chain and protest. You use Ethereum as a court of final appeal, which will govern the conflict. Now, with your knowledge proofs, you can provide the proof that certain computations were correct and that let main chain contract verify this. So you can use zero knowledge proofs for scaling in two different ways.
00:09:20.446 - 00:09:58.986, Speaker C: One way is to have fraud proofs on zero knowledge, which will be like plasma. But instead of having a challenge game interactive protocol of having multiple requests and responses, just provide one proof which says for this hash, the correct state should be this and this, or like this transaction is incorrect. So this is proof fraud proofs. Now, you can also use your knowledge in a very different way, where you provide proofs of validity for every state which you commit on the main chain. And this is what we use because this creates an entirely different user experience.
00:09:59.088 - 00:10:18.322, Speaker A: Well, I'm glad we actually made that distinction, because I think it's important to understand that the general topic that we want to cover today is also like off chain computation and how that would work. Starting from plasma, adding the zero knowledge, did you decide to use it in both ways, or are you only using it in one with the project as it is?
00:10:18.456 - 00:11:13.390, Speaker C: So we decided to use it strictly as validity proofs, although we made some contribution to fraud proofs as well, which plasma can benefit from. But fraud proofs always have game theoretical assumptions. Somebody has to submit fraud proofs in the case of fraud, and if they don't do this, or if the transaction doesn't get mined, then security is compromised. With validity proofs, it's impossible for anybody to commit an incorrect state, or you can commit, but you can never finalize the incorrect state, because the smart contract will only accept a state as final when the proof is there. And this leads to very interesting properties. You have very short finality in a range of maybe a few minutes, and then your funds are at security level of almost the layer one security, and nobody has to be online. You don't have any liveness assumption.
00:11:13.470 - 00:12:01.682, Speaker D: Yeah, the liveness assumption is crucial for any plasma, because there is an assumption that someone is watching a set of watchtowers, or every user monitors his own assets. This is an important part. I mean, there are different flavors in plasma and we can also talk about all of them if you want. But yeah, some of those can benefit from CK snarks to, for example, compress the history in plasma cache. We actually in Singapore ETH Singapore, where we showed the first kind of very rough hackathon style demo, we also simplified our main snark to allow history compression for plasma cache. So other projects can also benefit from it indirectly, not from what we do as a project, but from the code we just release and do it for fun.
00:12:01.816 - 00:12:47.154, Speaker B: So this is curious to me because you're not using ZK snarks or starks or whatever as in like starkware was talking about, or what roll up is trying to do in the sense of using the exponential speed up quote unquote property of a stark, in that you're trying to say that verifying all of this computation is a lot faster than doing the computation. So you have this plasma like off chain thing, and then using the stark to validate that the state transition function essentially was executed correctly. It's not like compressing data.
00:12:47.272 - 00:13:29.994, Speaker D: Yeah, unfortunately this is a misunderstanding. When Ilux mentioned the linear cast, we should separate it further. There is a state transition which is just transition from one state route to another state route, which is a result of applying a thousand of transactions, for example. But it's verified only as one snark proof. The same way it's exponential scaling. Also, we have the public data problem, and we need to disclose some part of every transaction in this block to resolve some worst case scenarios. When the operator doesn't cooperate, for example, or censors you completely, or something else happens, or a data center is completely offline, for example, there should be a procedure where the operator doesn't cooperate.
00:13:29.994 - 00:14:17.834, Speaker D: For this, we need to post a small amount of every transaction in the block to the main chain, and this is a linear part. If we eliminate this or just forget about this for a second, this is the same exponential scaling. There is no principal difference between what we do and the roll up. We just specialize it for set of features, but use the same idea of providing the validity proof for every state transition for every new block in the network. This is a part no one likes to speak about because the data availability problem is hard and it's expensive to resolve. That's why starkware doesn't talk about it. They talk about the size of the proof and how much approval will cost, but never about the public data, which they have to post for the same purposes if they want to have a possible resolution.
00:14:17.834 - 00:14:20.542, Speaker D: For worst case scenarios, we have a.
00:14:20.596 - 00:14:27.970, Speaker C: Sublinear computational complexity on the main chain and strictly linear communication complexity for data availability.
00:14:29.350 - 00:14:34.114, Speaker A: You were just comparing this to starkware. Is that the differences that you're trying to make, or.
00:14:34.152 - 00:15:24.994, Speaker D: No, I'm just. No, I mean, in principle, snarks and starks difference is that your proof size, your verification cost is sublinear compared to the amount of computations which you prove. It's valid for both snarks and starks. Also, people usually compare the kind of size of the proof, the cost to make the proof, or just cost to verify the proof on chain. And this is where Starkvar says we have the 6 million gas to verify the proof for this amount of transaction. We don't say this this way because we calculate both contributions verification for this set of, let's say, thousand transactions, plus the amount of data which we have to send for the same thousand transactions. So we more or less usually say the final number, which will be still required to spend.
00:15:25.192 - 00:15:38.550, Speaker B: Let's dig into the data availability problem a little bit, because I don't think everyone is aware of one, why it needs to be available, but also necessarily what that data is, or like what you would use it for.
00:15:38.700 - 00:16:34.794, Speaker C: So imagine we have a new state. We have a block in the side chain, which applies 1000 transactions and produces a new state. And this state, we compute the Merkle root of the state, we commit it on the main chain. And now, in case of plasma, or in case of zero knowledge based solution, if nobody knows, if the operator doesn't disclose the exact details of those transactions to anybody else, then the route might still be correct. It reflects the correct state transition, but nobody knows the Merkel paths, and they cannot expand the data. Nobody can prove that they own some money at some point of time, because the data is just not available. So in this case, the chain essentially halts, and people need to, in case of plasma, they need to exit immediately, all of them, because they will not be able to continue operation or even to withdraw the money.
00:16:34.794 - 00:18:01.890, Speaker C: And this is a big problem with any layer two scaling solution. Now, with zero knowledge based scaling, we can at least guarantee that the new state is correct. But still, if nobody knows the data, they won't be able, although the state is there and it's correct, they won't be able to prove that they own some money in the correct state. But with zero knowledge, we can solve this in a very elegant way. We post all the raw transaction data, not the signatures, but just the delta of the state change only the very minimum information which is necessary to reconstruct the new state from the previous one, we post it on chain, then we compute a hash of this public data, and then we use this hash as a public input to this snark proof to make sure that the snark not only guarantees the correct transition, but it only also guarantees that the data which is being used for the state transition was made publicly available, in our case through broadcast on Ethereum main chain. Now, we assume that this data is publicly available because every miner in Ethereum needs to use it to generate blocks and full archive nodes hold all this data. But it's much, much cheaper than using the storage because we never need to use any of this data for smart contracts.
00:18:01.890 - 00:18:05.030, Speaker C: We just compute the hash once and then we essentially forget it.
00:18:05.180 - 00:19:10.154, Speaker B: So comparing this to just plain Ethereum, in Ethereum, and a transaction is submitted, it's executed on chain through the EVM and everything else. And that's quite a lot of work. And then out pops the new state that says you have this balance, et cetera. So what you want to do then in your model is you move the execution of that transaction off chain, but you still want to be able to reconstruct that state in some way, and that's why you need to have the transaction available. If the transaction was submitted off chain, executed off chain, but proven correctly executed, that proof that it's correctly executed doesn't really mean that much like what is the result of the computation. That's what I want to know. And so with your scheme, you're committing the transaction on chain so that if someone doesn't share what the result of the execution is, you can go back and re execute it yourself and make sure you can recompute the state from that.
00:19:10.352 - 00:19:45.906, Speaker D: You cannot exactly reexecute it because we don't post the signatures. We don't need to post signatures just for reasons that we prove that state transition is valid. So there was a valid signature, which we know, and it was used for this state transition. But we post them. Let's say our simple state is just two balances, number zero and one. So if there is some transaction from zero to one, and we know what is a previous balance of zero and previous balance of one, what's necessary? Just the amount which was transacted so you can reconstruct. A same logic applies.
00:19:45.906 - 00:20:04.598, Speaker D: If you start from empty balances completely and fill them out somehow, and then for every transaction back and forth, you can reconstruct what would be the final balance. If you just go along all the public data which was available, and then you can just continue this procedure by just expanding the number of participants.
00:20:04.694 - 00:20:23.506, Speaker C: So you cannot reexecute the block, you cannot construct a new valid proof that the block is correct, but you can prove to the smart contract on the main chain that you own some balance at some point, and then you can just rely on the main chain EVM to be able to withdraw this to your account.
00:20:23.608 - 00:20:33.458, Speaker A: I'm still trying to figure out where is the saving here, because what you just described, if you make any transaction, you're still writing something to the main chain.
00:20:33.554 - 00:21:11.170, Speaker D: We don't write it, we just post it as a part of transaction data. There is a difference in pricing for the data which you just submit as a part of the transaction, the data which you store. So in principle, for this state transition, what we do is we write, once we write one storage slot, which is a new route, if we just go to the bare minimum, without the details of the contract, everything else can be downloaded as a part of transaction data from archive node. And if something happens, you start with an empty tree. You follow every transaction which happens as a deposit to this contract. And also you follow every transaction which was happened as a transfer or exit, or partial exit. Doesn't matter.
00:21:11.170 - 00:21:45.882, Speaker D: At the end of the day, you get with a set of balances for every participant of the network as a final state. So the chain will stall if there is no cooperation. At some point it will be what's called the final state. And if you just go along the history from the beginning of time, you will get the content of the state just from the data which was publicly available through the theorem and through the archive nodes. And then you can just do a simple Merkel proof at this was my state at this moment of time. And just this data ensures that you know all the state. So you can reconstruct the Merkle proof.
00:21:45.882 - 00:21:51.406, Speaker D: I mean, to reconstruct the Merkle proof, you need all the state, and that's a part of the deal.
00:21:51.588 - 00:22:06.894, Speaker B: That's where I have the question. Like you're saying that you can't re execute the transactions, but if you don't post the result of the transaction as well, then you have to re execute it to get the final state right, to be able to say, this is my balance at the end, you don't.
00:22:06.942 - 00:22:16.726, Speaker D: Execute it because you don't see the signature. I mean, in our terms, executing the transactions, that you also verify the signature. But all signature verifications were done for.
00:22:16.748 - 00:22:22.870, Speaker B: You already, because you're not executing it on chain, but you're executing it somewhere.
00:22:23.030 - 00:22:41.466, Speaker D: You execute it locally, just one entity in the world can execute it for everyone else. This procedure is much simpler than actually doing all the bookkeeping and making the proofs, because this is just one database of very small amount of rows which you can just add and subtract, which is kind of trivial.
00:22:41.578 - 00:23:02.774, Speaker B: But that's what I mean. You're still re executing it. Not on chain. But that's why you want to make that data available. That's why the data availability thing matters, because if you don't have the data, then you can't reexecute it. And to your point, you're not reexecuting it in the Ethereum EVM model, but within your normal off chain model.
00:23:02.892 - 00:23:09.190, Speaker C: But you only will need to do this in the very unlikely event of complete operator.
00:23:09.530 - 00:23:13.114, Speaker B: Yeah, but that's why you need the data availability as well in that case.
00:23:13.232 - 00:23:13.706, Speaker C: Exactly.
00:23:13.808 - 00:23:41.490, Speaker D: Yes. I mean, we need it only for this purpose, for everything else you could just not post it ever. And if you assume that your operator will live forever and survive the nuclear winter as a serum 2.0 once, then fine, you can just saying that operator will give the content of your account at any moment of time. If you don't assume this and we don't assume this, then you need this worst case resolution procedure.
00:23:42.390 - 00:24:17.870, Speaker B: And the win for the chain is, I think it's a one on one episode from a while ago that talks about the difference between history and state and sort of the different storage models there. And many of the problems that we're having in Ethereum, like why it doesn't quote unquote scale, why it's so hard to sync, et cetera, is because there are so many nodes in the Merkel tree of the Ethereum states. And this doesn't bloat the state, it just bloats history. Which is fine. We don't care about history necessarily.
00:24:19.250 - 00:24:43.000, Speaker D: Well, I wouldn't say no one cares about history. People do care about history. There are proposals how to kind of at some point clean up and forget the history. But then there is no proper way how you can sync with a network. It's a problem which should be resolved at the layer one. And we work with conservative assumptions that it will not be worse than it's now. And that's it.
00:24:43.930 - 00:25:15.498, Speaker B: Right now, all full nodes store all history. And there is discussion around, like you say, how we can get rid of history and not require all full nodes to store all history, but then it's still within the model. It's incentivized somehow to keep the history around by people don't care. I mean, like normal users don't care. Or have to look at history. Like when you sync an Ethereum node, you don't look at history. There's no syncing scheme that actually re executes all the history anymore, but it's.
00:25:15.514 - 00:25:17.514, Speaker A: Always discoverable if you wanted to find.
00:25:17.572 - 00:25:18.594, Speaker B: Yeah, exactly.
00:25:18.792 - 00:25:35.558, Speaker A: So I feel like in previous episodes we've talked about a little bit about just off chain computation, and we hinted at different ways of sort of coming back to the main chain. Do you see this as what's a comparable project to what you guys are doing?
00:25:35.644 - 00:26:22.662, Speaker C: Well, for me, the inspiration of this approach was the coda protocol, the succinct blockchain where they tried to compress the entire blockchain into one snark proof, which I thought we can apply for plasma because having an isolated blockchain is a little difficult, because the coda also faces the data availability issue and they need to solve it still with the same consensus algorithms which other blockchains struggle with. And it's better to rely on something existing and proven than try to reinvent everything from scratch. And also, it's the new philosophy of Ethereum to have the layer one as the data availability and consensus layer, and have all the computations on top of it in layer two and above is.
00:26:22.716 - 00:26:24.230, Speaker A: Like what you guys are working on.
00:26:24.300 - 00:27:16.594, Speaker C: Recursive, no, we decided in favor of a different approach. On the one hand, due to limitations in current Ethereum protocol, because we only have efficient pre compiles for a very specific elliptic curve for bn two, five, six, so we cannot use recursion yet, on the other hand, because it allows us to scale with a lot more parallelism. So what we do, instead of relying on the previous proof, we compute all proofs for all blocks in parallel, so we can make a commitment for the first block, start producing the proof. It takes a while. Meanwhile, we commit the second block and we start producing the proof for the second block, and so on and so on. And then finally first block gets proven, and then we submit the second transaction on Ethereum and finalize this state. So this allows us practically arbitrary scale parallelism.
00:27:16.594 - 00:27:30.194, Speaker C: We can just use as much hardware as we need. So this is a difference to Coda specifically because they have to wait for the previous proof to be ready in order to start generation of the next proof.
00:27:30.262 - 00:27:46.690, Speaker B: I'm very curious to hear more about what kind of transactions do you support? Is it completely generic EVM transactions only balance transfers? Or is it even outside of EVM, like generic x 86 programs?
00:27:47.350 - 00:28:48.770, Speaker C: This is a great question. We want to start with very simple functionality, which is going to be as universal as possible for the first version. So we're only going to support simple transfers of ERC 20 tokens. Atomic swaps of ERC 20 tokens is mutual transfers and also state channel updates because we can provide all the UX benefits also for state channels. With all these state channels frameworks being able to build on top of this layer zero knowledge layer in the future versions, as soon as recursion becomes available to us on Ethereum, we will be able to write arbitrary smart contracts on zero knowledge in snarks. They're not going to be turing complete, they're going to be bounded in computation, and they're going to be somewhat expensive to compute. It will take some seconds, maybe minutes on local laptops or cell phones, but we will be able to support very complex operations.
00:28:48.770 - 00:29:06.380, Speaker C: For example, we will be able to breed a crypto kitty and only spend gas on the state update, like just one k gas instead of currently it costs I guess 200,000 gas in the example.
00:29:07.070 - 00:29:22.430, Speaker A: I think it's almost a year ago we had Eric Tang from Livepeer come on and they were doing like video compression off chain. Could this ever be used for something like that? Or is this in a totally different ballpark?
00:29:24.930 - 00:30:07.626, Speaker D: This part, because we work one step at a time, even while we see that at some point, especially with recursion, we can support much wider functionality, and this functionality is still to be defined completely. Of course, there are obvious technological limitations that your contracts or any execution, like any programs which you write, will not be turing complete. They will have all the limitations of any snark based programs. But still, for now, we cannot even make a very well educated guess about what will be possible. Or even while we understand what's going to be possible, we don't know if people, and how people will use it as a problem. There is no killer feature for Ethereum, there is no killer feature for blockchains. In principle, there is no killer feature for snarks.
00:30:07.626 - 00:30:12.880, Speaker D: I think yachts, if at some point someone makes it, we would want to support it.
00:30:14.450 - 00:30:55.486, Speaker B: Yeah, I think with video compression and stuff like that, I mean, ultimately whatever you compute off chain needs to be provable in a snark or a stark, and so you'd need to write a video compression snark, which seems pretty impossible right now. But who knows? In the future, as snark programming languages get better and we get better abstractions in place and they become more performance, I think video compression is like the worst case thing that you could possibly do. But there's definitely, like, if we can get to a generalized evm or like a webassembly thing, then we can get pretty far with that.
00:30:55.508 - 00:31:05.810, Speaker C: And it's also contingent on hardware. If we have specific hardware for snark, or zero knowledge proof generation in general, then it's going to be much more efficient.
00:31:06.310 - 00:31:31.882, Speaker A: That's actually really helpful, though, to just place this on that sort of time frame. And basically what I was trying to figure out there is, where is it actually at? And it sounds like it's very far away at this point from being able to handle something like video compression. That's fine. It's not a criticism. It's more just like for us to understand at what level this sort of technology is at.
00:31:32.016 - 00:32:22.890, Speaker D: Well, I mean, the entry bar for any snark based application is high anyway. People usually give an example that you can prove the inference step of the neural network using some zigzanaric, even while it's technically possible, I even agree that it's possible. It's another challenge how you can make the neural network which is efficiently provable in snark. I mean, if you try to prove the normal neural network, you will end nowhere. So you need to change the neural network with the primitives which used as numbers in a neural network itself to be able to efficiently prove it. And this is a limitation of snarks, because they work over fields, not over normal numbers, which were used to work with when we write just normal programs. This is obvious technological limitation.
00:32:22.890 - 00:32:42.858, Speaker D: But if your application allows you to work around this and use field elements instead of numbers, for example, and you know how to use them, because there are parts which are quite not so obvious, then sure, you can try to prove your local computations used in TK snark.
00:32:42.954 - 00:32:56.610, Speaker A: So I want to ask a question about the roll up project and how this relates to what you guys are doing, because have you been working like, is this a roll up project? What is roll up?
00:32:56.760 - 00:33:28.382, Speaker C: Roll up is the name of the concept. We were struggling to find a better name. We just settled on ZK rollup, which is currently being used as common name for this kind of solutions. So it's a general umbrella term for zero knowledge based state transitions with data availability solved on chain. I would say this is definition. So I started working on roll up. Yet before I met Alex, I was working with Barry Whitehat, and the ideas we developed there flew into what we implemented with matter.
00:33:28.382 - 00:33:36.678, Speaker C: So matter was just the we, as a research team were the first to actually implement a working version of a rollup.
00:33:36.874 - 00:33:45.542, Speaker A: And who was it initially? Was Barry the first person to kick this off? His name is often associated with it.
00:33:45.596 - 00:34:14.570, Speaker C: Yes, Barry Whitehead. Barry's role was to be the pioneer of zero knowledge in general. He created miximus and created a group. I found him online, and there were a couple of people having discussions about that, making specifications, making some prototypes, and we progressed from there. We discussed different directions. We had different libraries and frameworks which we could use. Barry was working with Lipsnark.
00:34:14.570 - 00:34:56.838, Speaker C: We decided to experiment with Bellman, which is a library written in rust by guys from Zkesh, by Sean Bo, specifically for the most part. And we thought it might be a better fit because Rust is a really interesting language for writing secure applications and for writing cryptographic protocols, because it has a very nice set of constraints. Certain things you can define things which are not possible or are not allowed to do, and then the language itself will take care of these limitations. And it's also more modern, more concise. The programs are shorter. You don't need to write the same code twice, because you can reuse generic functionality of rust. And it turned out to be a good choice.
00:34:56.838 - 00:35:00.358, Speaker C: It allowed that to make the progress much faster.
00:35:00.454 - 00:35:15.300, Speaker A: So roll up was this initial idea, and there's a group of people online who kind of rallied around it, and out of that came the project that you guys do. What is the name of the exact project? Because I know you as matter labs, but what's the project? What's the technology?
00:35:16.150 - 00:35:47.850, Speaker C: So we're working on a network now which we're going to call Franklin. It's going to be the platform for generic execution of zero knowledge on top of layer one. So we call it layer two network, but maybe it's just an extension of layer one. We're not sure how to position it. So, like all projects which are now layer two can just build on top of this. We're not going into any specific functional vertical. We're just going to provide tools for other projects to build on top.
00:35:47.850 - 00:35:58.046, Speaker C: Who can use transfers, they can use atomic swaps, they can use state shells with all the existing tooling, which is currently there, just with better user experience and better security.
00:35:58.148 - 00:36:04.270, Speaker A: And is it always going to have that zero knowledge component, or is it more focused on just general scalability?
00:36:04.870 - 00:36:16.210, Speaker C: No, it's scalability based on zero knowledge, because zero knowledge is the only technology which allows us to scale with its benefits. There is nothing else. It was a breakthrough technologically.
00:36:16.710 - 00:37:14.578, Speaker B: I want to dig more into how this is built and sort of what the architecture looks like, what pieces are off chain, and how they communicate, and some stuff like that. But there was one thing that I wanted to cover first, and something that I talked to Barry about, and I know there are some issues in just how Ethereum works. And with roll up or roll up architectures in general, you get the maximum scalability if you fill up every block with one single transaction. That's very hard to do. And it's also questionable if you have a generic off chain structure that can execute anything, then you just force people to go to that place to execute it instead of executing it on chain. But if you have something like a Dex that focuses entirely on this, then every block just serves this one Dex, and no other application can make it on chain. So obviously this is problematic.
00:37:14.578 - 00:37:16.600, Speaker B: How do you think about this?
00:37:17.050 - 00:37:52.798, Speaker C: First of all, it's about economic efficiency. But of course, you can just wait longer to fill the blocks. Then it will increase the delay, which is going to introduce inconvenience for users. So obviously, yes, this is the correctly identified problem. We want to fit as many zero knowledge transactions into one block as possible. And this was our motivation for starting Franklin. We realized that if every project will go with their own solution, then they are going to all compete for gas on Ethereum, and they will have to wait very long time to fill the blocks.
00:37:52.798 - 00:39:11.914, Speaker C: And for some projects which are not yet, as that massively used, is going to be inefficient, and our monetization of the gas cost per transaction is going to be low. So that's why we decided, let's figure out what are the basic universal blocks, which all projects can build on top. And let's make a very broadly accessible network, so that all these projects, DeXs, DeFi projects, which need to use scaling cryptokitties, simply payment solutions, which need to transfer tokens, can all reuse. And of course, there might be conflict potential between all these projects, because who's going to control this? So we came up with an idea to make a decentralized network. Now, it's important to explain that the decentralization is not strictly needed in our case, because zero knowledge proofs make sure that the state is always correct. You can never commit an incorrect state, but we need it for censorship resistance, because if we only have one operator, this operator will be able to censor anybody they want. So for censorship resistance, we want to decentralize this, and we will have a permissionless set of validators.
00:39:11.914 - 00:39:16.560, Speaker C: Anybody can become a validator, and they will just produce the blocks in turn.
00:39:19.410 - 00:40:05.200, Speaker B: This is what I want to dig into as well. I think a comment on filling up every block. Like I said, if you make this a generic structure that can execute any type of transaction, and it's a proper decentralized network and has censorship, resistance, et cetera, I think it's fine to fill up every block with one transaction. But at that point, if it's truly successful, and this is how people want to build their applications, then it's questionable if we should keep Ethereum as it is, or we just ditch the EVM completely and say this is a blockchain for making data available and validating proofs. And that's the only purpose of layer one. And we don't have to have the smart contract thing going on.
00:40:06.130 - 00:40:41.750, Speaker C: Well, you want the smart contract just to be able to support generic proofs. Maybe in the future we'll come up with better protocols which need to be verified. And you also want to use Ethereum for other purposes as the court of final appeal because you will still have cases where you need to escalate on the main chain for the sake of flexibility. So you want to keep the flexible generic computing functionality on layer one just in case, just for situations where you need to resolve something there. But mostly, yes, this is going to progress towards the vision which you just described.
00:40:41.830 - 00:41:08.274, Speaker A: Do you think that that would mean. Okay, according to this vision, would like the one ethereum, what are we saying? Ethereum one x ethereum that we have today that could become something like this court of final appeal. But then I'm curious, how do you interact with EtH 2.0? Does it matter? Do you need to care? Are you looking at this? Will it change the way that you build things?
00:41:08.472 - 00:41:10.690, Speaker C: No, we can build this solution.
00:41:11.110 - 00:41:57.026, Speaker D: Yeah, with the series 2.0 we don't need cross shared interaction if the user deposits part of his funds or uses what we built for his purposes. User actually never touches the sharding structure as it is after he's already in the network. So for sharding, the only entity which will care will be us as the developers. But if we operate on some shard, yes, we take quite a lot of capacity of the shard, but for purposes of proof verification and data availability. But then we don't need direct cross shard interaction unless there is a deposit access procedure. But well, this will require crosshard interaction anyway.
00:41:57.026 - 00:42:07.918, Speaker D: But other than this, we can operate in one shard and in this sense sharding is very much beneficial. We take much smaller share of the total network capacity just to verify the proofs.
00:42:08.094 - 00:42:45.810, Speaker A: This was something that Frederick and I were talking about just a little bit before this interview, and that was, you touched on it just earlier, the idea that when you go off chain, you actually are no longer in this decentralized mode. All of this could be happening in an extremely centralized way, and then sort of checks into the decentralized court of appeal or the data star uber validator. Somehow I know that's the wrong word here, because it'll get confusing for people. But anyway, but then you sort of mentioned that the way that you want to decentralize it is to create sort of a decentralized network in itself. How do you do that?
00:42:45.960 - 00:43:42.242, Speaker C: Before I explain how we do that, I want to add one more motivation for doing this in the first place. And this is trusted setup, because for snarks with the current iteration, which we use is the protocol is called cross 16, and it requires an application specific trusted setup. This was the biggest problem why snarks have not been broadly used in the past. The only application which takes heavy use of snarks is zcash. And Zcash had two rounds of trusted setup. The second round was more efficient with over 100 participants, but most of those people were either closely affiliated with Zcash or they were just random participants who are not very well publicly known. We didn't see any large credible organizations taking part in the trusted setup, which of course is giving reason to some people to doubt the credibility of the setup in general.
00:43:42.242 - 00:44:54.906, Speaker C: So of course if you're building something very this problem faces, every project faces this problem because if they create trusted setup just for them, just for their application, how are they going to attract large number of credible participants to make it publicly compelling? So now starquare comes and says, hey, let's use starks because we have transparent arguments of knowledge, and then you don't need a trusted setup, which sounds like a very good idea. Unfortunately for blockchains, for this out of efficiency reasons, because the proofs are two orders of magnitude larger than for snarks with hundreds of kilobytes of code, we just cannot use it. Now, the block size or the gas block limit is the main bottleneck on the public blockchains. So the proofs must be short. Otherwise, if your proof is costing you 6 million gas, then you don't have any space to put data availability on chain, and then it becomes a very tricky solution. And you cannot build roll up on starks right now for this.
00:44:55.008 - 00:44:59.146, Speaker A: So that's interesting, but how does that. Let's talk about the decentralized if we.
00:44:59.168 - 00:46:01.600, Speaker C: Have a single network which can be reused by many projects, which only supports generic functionality, not going into depth of every application, but allows any application to build stuff that they need, we can have a broad set of projects who are interested in this, in using this network, who will participate in the trusted setup, which will make the trusted setup much more convincing for the public. And this is what we're striving to do with Franklin. So we're actually in the process of creating what we call a zero knowledge blockchain alliance. The organization which will include these projects and the first condition for participation will be taking part in the trusted setup. And it also will balance incentives, and it also will provide a broader set of validators which is decentralized, which will eliminate the risk that one party controls most of the network and they can censor everybody else.
00:46:02.130 - 00:46:40.640, Speaker A: That's interesting that you consider, because the trusted setup is very sort of broad. Like it can be very broad in its scope and you can have a lot of different individuals from different walks of life that that would somehow give them any power over the network post trusted setup, because isn't it just that you have all of these people weigh in to create the secret key, private key, and then it's used once and then it's sort of set free, then however, the thing is managed after that is not related necessarily to the trusted setup group. I don't exactly understand how that decentralizes it.
00:46:41.170 - 00:47:22.542, Speaker D: Well, let's kind of have step back. Let's say there are ten projects building their own kind of ck based solution actually for a particular application, not for some functionality which can suit a lot of project, but just for one particular one, let's say DaX or something like this. Then ten such projects. Each of those will compete for gas price verification, public data availability. Some solutions which I've seen have some enormous requirements for data availability, unless they actually do it, which will be very expensive. They can be actually called centralized network because they can just not disclose the data. If they all do it, it's another security keys.
00:47:22.542 - 00:47:26.566, Speaker D: And to do it, they all need to do the trusted setup individually.
00:47:26.758 - 00:47:28.330, Speaker C: Each of them will each of them.
00:47:28.400 - 00:48:15.210, Speaker D: Separately and individually just because gross 16 works this way. So if you somehow manage their interest, merge them together, or just make something generic which they all can use, maybe with some limitations, but still usable. It's one verification cost, public data availability which with much smaller data which is necessary, and one trusted setup which can include participation from all those ten entities. So this is high quality trusted setup because those entities are potentially competitors. So they have no incentive to try to come together and try to keep all this large share of entropy. Well, we will participate ourselves, and I mean just destroy our entropy anyway.
00:48:15.280 - 00:48:29.040, Speaker A: But it doesn't matter to me. The trusted setup and all of that is solving a different problem, that is to make sure that the whole system is secure. But that doesn't necessarily decentralize it.
00:48:29.730 - 00:49:22.494, Speaker C: It's a part of decentralization, having more participants in the trust is set up. But let's also deep dive into the technological architecture, how we can make it decentralized. So to decentralize things, we just need to give the right to produce blocks to different participants in turn. I produce the first block, then I pass it to you, you produce the second block, then Alex produced, this is the third block, and so on, and it goes in circle. Now, with plasma, it's very difficult to do because the participants need to share data availability. And if one of them refuses to share data availability, we have something called a speaker listener dilemma. It's non attributable if I say publicly that I told you something and you say publicly, you never received this message.
00:49:22.494 - 00:50:08.474, Speaker C: There is no way for anybody to understand who is right. That's why plasma must be centralized with roll up specifically, because we put all the data provably on the main chain and make it available to everybody else. All operators can just take the data from there and continue producing blocks. And we can have something very simple, some simple randall, some smart contract which determines randomly sequence of who is producing the next block. And this is all we need in order to decentralize. Now, if some of the validators will censor you, somebody else won't, because they will have incentive. Obviously, you will have to pay for your transactions some small fee because they've proven generation costs have some costs.
00:50:08.474 - 00:50:16.590, Speaker C: So somebody will be happy to include your transaction. And this way we will guarantee the absence of censorship, resistance.
00:50:16.750 - 00:50:28.166, Speaker B: Do you intend to set up like a separate blockchain for this, or like a beacon chain to be able to do validator selection? Or do you plan to have that as a smart contract on mainnet we.
00:50:28.188 - 00:51:01.034, Speaker C: Will have a smart contract which determines randomly the sequence of validators. And we will of course, have a side chain. The state itself is going to be stored off the main chain. So we will produce nodes which people have to run, which store the state in some database, and they exchange the state through sending it on Ethereum. Now, as far as randomness is concerned, we are not sensitive to any bias and randomness. We don't need very valuable data functions because all we need to do is we don't even need randomness, we can just produce it in the sequence.
00:51:01.082 - 00:51:21.330, Speaker B: You can do a round robin around the validator sets, assuming it's not too large. So I think that brings the question of how do you add and remove validators from this set? And what's the, I mean, the incentives of being a validator is you get fees, but how do you get civil resistance? Why can't I just sign up with 1000 validators and be the only one ever validating?
00:51:21.490 - 00:51:37.882, Speaker C: Well, for this on blockchains we introduce tokens. This is the simplest and more most obvious version. How we can make it permissionless, how we can allow anybody to just join the network without any bias, without control of anybody else. Right.
00:51:37.936 - 00:51:40.102, Speaker B: So there will be a Franklin token.
00:51:40.166 - 00:51:42.182, Speaker C: Yeah, there will be something like Franklin network token.
00:51:42.246 - 00:52:28.486, Speaker D: Yeah. I mean, just because germans approved requires some capital, it can be not even direct validator participation. But you can just delegate your right to potentially make this share of blocks in this round to someone who is willing to take this capital cost upfront for a purpose of potentially collecting the amount of fees which is profitable for them. We don't want to limit any form of such cooperation, so we'll need to make some form of delegation procedure. But then still the round is fine at time. If you delegated your right to someone and see that this is not the best validator, you delegate your right to someone else. Well, because we cannot produce invalid blocks.
00:52:28.486 - 00:52:52.866, Speaker D: Worst thing can happen, you can commit to some block and never produce the proofs and you just slashed. We don't have all these severe problems like yours protocol with delegated proof of stake, potentially proof of stake protocols in principle, yeah. The worst thing can happen if validator screws up and the next one steps in earlier than should be. And that's the only transition mechanism and.
00:52:52.888 - 00:53:11.930, Speaker C: Error resolution which will pose a small denial of service risk on the system. If I don't produce blocks, then it will be some delay. But then this validator who did not commit the block will be slashed for the amount of time they caused damage on the system and this will discourage such behavior.
00:53:12.350 - 00:53:58.440, Speaker A: One thing that I find sort of fascinating about this project and even exploring it with you guys, we started from this off chain solution and then into this sort of platform of tools. As you go through it, you guys are basically coming into. You're facing the challenges that every large scale blockchain project, not every, but many large scale blockchain projects face. The question of how validators are dealt with the question of should you be delegated? It's been a very interesting journey, even through this interview, to sort of understand where you guys are going and where you've been. Anyway, it's an ambitious project.
00:53:58.890 - 00:54:06.026, Speaker C: Thank you. But luckily for us, we rely on the security properties of zero knowledge proofs, and they solve a lot of problems for us.
00:54:06.128 - 00:54:50.760, Speaker D: Yeah. And just from purely technological perspective, decentralization is not strictly necessary. From the day one, there was a worst case resolution scenario, even if there is just one supervalidator, decentralization as it is, is kind of our decisions that we want to make it this way. So just not even that. This worst case scenario will never happen and, well, should not happen in a decentralized setting. But if we can do it, we don't need a superpower. I mean, building all of the previous parts and continuing provisions in the work is already a lot of work, and being a validator is not the funniest part of it.
00:54:51.290 - 00:54:54.950, Speaker A: Funniest? The funnest, yeah, the funnest.
00:54:56.830 - 00:55:31.318, Speaker B: Yeah. I mean, you do need the decentralized aspect to have censorship resistance, I suppose. But I want to ask one final question around how you guys are building things, because like Anna said, it's an ambitious project. There will have to be validator nodes that are producing proofs and collecting transactions, and you'll need a transaction queue, and you'll need PTP networking and everything else. You said you're building stuff in rust. How are you building out these different components? And which PTP net library are you using?
00:55:31.484 - 00:56:20.934, Speaker C: So, a nice thing about being a validator is that you're not exposed to a risk of operational security, risks, because you never expose your private key on a hot wallet machine. You just produce the proofs, and if the proofs are incorrect, then they get declined. So therefore, the security requirements for us are less. We need to make a lot of effort to make the snarks. The protocols inside zero knowledge proves very secure. But the rest, all these communicational things can be written in languages like Golang, maybe, which take much faster in development. So we use rust for really difficult parts, and the rest we will see.
00:56:20.934 - 00:56:33.386, Speaker C: Rust is an interesting language. Might be a little. It's a matter of trade off, of speed of development and cost of development and the security which we need to achieve there.
00:56:33.568 - 00:56:41.378, Speaker B: So how much is finished? Like, how much is actually built right now and how much is there still on the horizon?
00:56:41.574 - 00:56:49.520, Speaker C: So we have a working. The snark circuit for the first version is very close to, I would say 50%.
00:56:50.050 - 00:57:19.450, Speaker D: Yeah, I mean, we finalize the features which want to put, because actually just limiting the features is a difficult task in terms of implementing the snarks. Well, still not many people really like to do it, but it's not a problem for us. Everything else is much closer to more or less standard software development. Yes, it requires people in principles. That's why we don't hide. We try to get some funding. Even after we got the Ethereum grant, it would allow us to get more people.
00:57:19.450 - 00:57:55.300, Speaker D: Because this part is purely software engineering, it's not too related to snarks directly. I mean, we can cover already the snarky part. In this sense, our goal is to provide the validators a set of tools which basically four lines of code. Not even a code, just four lines for tuning the network. Like to what address you want to get collected fees or something like this. So you can just start and run. Even the communication, like PDB communication is, well, there are wonderful libraries for it in the first place.
00:57:55.300 - 00:58:19.180, Speaker D: Or we can agree that the round is finite, so everyone knows who will be the participant in this round, and they can all agree upfront about what will be the common storage where they put all the data, just even for public to download it. It's not a big difference if it's storage for public or storage between operators. So this part is purely software engineering without touching ZK part.
00:58:19.790 - 00:58:55.990, Speaker C: The ZK part is really difficult, but we also have to explain that we have a few other lines of research. We were also working on the GPU prover, which Alex can present the benchmarks. We were also working on the sonic implementation, which we haven't touched yet, because this is a new promising direction, which is especially important for smart contracts and snarks. And we will have to decide, it will depend on funding, whether we can pursue all the directions or just focus on the Franklin network.
00:58:56.810 - 00:59:06.620, Speaker B: I happen to know of a very good framework that gives you networking databases and validator notes and everything else out of the box. We can talk more about that later.
00:59:07.710 - 00:59:11.646, Speaker C: Well, it's certainly something plug, certainly something we're going to consider.
00:59:11.748 - 00:59:33.410, Speaker A: On what you just mentioned, I actually have a last question, which is about the use of snarks, the snarks that we have spoken about on this podcast. And are you constructing this in a way that you could eventually replace snarks with something else, or is this 100% built around the concept of snarks?
00:59:33.830 - 01:00:11.834, Speaker D: Well, Snarks S is important. We need for verification purposes something which scales exponentially with the number of competitions just to compress this part which ensures the validity. That's why we don't need signatures on chain, we just need the small deltas. So this part is any solution which will allow us to do this. We will be willing to spend the time in research and evaluate how it works. Succinct part is the most important one. I mean, sonics are snarks because they are succinct.
01:00:11.834 - 01:00:53.870, Speaker D: I mean, it's just sonic as a proof system is called this way a gross system. 16 is also a proof system, but all of those are basically snarks. If there is an x proof system, even more and more efficient, with universal trusted setups and potentially other benefits, then we will pursue this direction and we'll check and evaluate how expensive it's to make proofs there, how expensive it's to verify the proof there, and if it suits the needs well, we definitely will upgrade if there are benefits. But sonics are snarks in their essence, just the same way. It's another proof system, but the area is the same.
01:00:54.020 - 01:00:56.042, Speaker A: Do sonic still need trusted setups?
01:00:56.106 - 01:01:27.000, Speaker D: I need a trusted setup, but not for every application, only once. For any application up to the given size and up to the given size is basically the amount of computation which you want to run inside of this circuit. This is so powerful feature. For example, if we want to upgrade the functionality, we would have to do another trust, at least part of it with sonics. We just say this is a new functionality. That's it.
01:01:28.010 - 01:01:34.780, Speaker C: And we can have a much broader set of participants in the trusted setup because everybody will be interested because they can reuse it.
01:01:35.390 - 01:01:39.130, Speaker D: It's not only universal, but it's continuously updatable.
01:01:39.630 - 01:01:46.078, Speaker A: Maybe we're due to do an entire episode just on Sonics, but you should invite Sean Bo.
01:01:46.164 - 01:01:48.526, Speaker D: Yeah, if you invite mean.
01:01:48.548 - 01:01:54.914, Speaker B: If we could get Sean Bo on here, that would be pretty amazing, regardless of what we talk about.
01:01:55.112 - 01:02:02.258, Speaker A: Cool. Do you guys have anything else you want to share with the audience before we sign off? Where can people find you?
01:02:02.424 - 01:02:06.290, Speaker C: They can go to matter labs IO.
01:02:06.650 - 01:02:13.906, Speaker D: Well, just because we code go to the mean, but they will find links.
01:02:13.938 - 01:02:19.030, Speaker C: To Twitter, medium, GitHub, GitHub. Everything from the main site.
01:02:19.100 - 01:02:36.670, Speaker A: I want to say thank you guys for sharing a little insight into what you're working on. And also, I think we've uncovered a few different topics through this conversation. We've talked about plasma, we've talked about on chain and computation. That trusted setup. Again, like the new advancements, this is pretty cool. So, yeah, thanks for coming.
01:02:36.740 - 01:02:37.278, Speaker C: Thank you.
01:02:37.364 - 01:02:38.078, Speaker D: You're welcome.
01:02:38.164 - 01:02:41.658, Speaker B: Thank you. Very much. And to our listeners, thanks for listening.
01:02:41.754 - 01:02:42.490, Speaker A: Thanks for listening.
