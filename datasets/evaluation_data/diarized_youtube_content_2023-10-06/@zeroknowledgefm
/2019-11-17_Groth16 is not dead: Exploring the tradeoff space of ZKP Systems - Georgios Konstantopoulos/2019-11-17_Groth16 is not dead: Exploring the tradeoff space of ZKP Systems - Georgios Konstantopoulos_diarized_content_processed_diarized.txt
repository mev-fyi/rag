00:00:06.330 - 00:00:26.246, Speaker A: Hi, all. Thank you for being here. So, I'm Giorgios, and this is going to be a talk about things that I learned in the past few months. So I haven't been doing this type of work for too long, so please don't judge me. So. Okay, I think everybody in this crowd knows this, so this is like the snark. I know something.
00:00:26.246 - 00:01:26.946, Speaker A: I want to outsource the verification to somebody. And how am I going to prove this computation in a short way? Like, I have to encode this computation in some proof, the toolbox that we're using. So I want to take a very practical approach for this talk is like a bunch of things. So, for the snarks, usually we do pairing based cryptography, some form of accumulator, which usually is some Merkel tree, some vector commitment or something. We have some assumptions. So like all of these protocols that we've seen so far, they're either in the algebraic group model or the generic group model, which my understanding is that currently they're slightly stronger assumptions than the usual ones that we'd have if we want to do recursion, usually you'd need some curve that has some cycle, but Sean may have some breakthrough on that. And usually the thing that's a very hard part to do is properly instantiating the protocols.
00:01:26.946 - 00:02:29.542, Speaker A: So usually you could very easily do it with a trusted party that generates some string, or you can do some multiparty computation to avoid the setup. And the returning view that we have is like that. You have some statement, you make some information, theoretical statement about it, and then you compile it. You use some compiler that go to a snark, and we have this pattern again, which started from the CS proofs, then to probabilistically checkable proofs and so on, where the prover sends some commitment to the computation, to the verifier, and then the verifier starts making queries at different points. And depending on what type of construction you're using, this could be like a polynomial IOP, a linear IOP, like what Ben was talking about, and so on. So this is what triggered this new wave of snarks, realizing that this technique could be applied in this case. So this is a normal snark, and this is like a preprocessing snark.
00:02:29.542 - 00:03:21.494, Speaker A: So basically you have a snark where some of the computation, usually, to have a succinct verifier, you need the way for the verifier to not have to read the whole circuit, but like just a short description of the circuit. And usually this means that the common reference string that you have generated is like kind of in the size of the computation. Yeah, this usually has this problem. So what can we do to fix it? Firstly, the one thing that we know that works there is like the growth 16, and with the previous construction before it, which is pretty fast. So like two group one elements, one group two, and a short amount of pairings. And however, the initial growth paper was not MPC friendly. Then we had a paper in 2017 which showed how to do it in a multiparty computation.
00:03:21.494 - 00:04:18.350, Speaker A: And then we had one more paper, which showed by Mary Molar, I believe, which shows that you don't even need some of the assumptions that were in that paper. However, what we really want is to have a universal setup. And how can we get that? The first approach was, from my understanding at least, was like the tinyram or the von Neumann Tinyram approach, where you have a cpu and you run one computational step in the snark, and then you keep iterating on that. The other approach was to have that you initially have some reference string that is common and random, and then each time that you have a specific computation, you try to derive the specific string from that one. And the one approach was like the one that we had in sampling, and the other one was by GKMM in 2018. However, the common reference string was like too big in that case. And we need something that's like both universal, updatable and practical.
00:04:18.350 - 00:04:56.262, Speaker A: Some review. Okay, we have the polynomial. We want to commit the polynomial. The commitment is like short commitment to it. Ideally, we want this to be very short and independent of the size of the polynomial. When the opening says that I'm going to evaluate the polynomial at some point, and I will also give you, and I will return you, like, an evaluation of the polynomial, plus some proof that I calculated that, attest that I calculated the polynomial correctly, and the verifier will return zero or one, depending on if that witness passes some things. So if you remove the polynomial, this looks very similar to some miracle trigger commitment.
00:04:56.262 - 00:05:47.706, Speaker A: However, what we really want is to have a constant size opening or commitment, which, if you have a constant size commitment, allows you to avoid logarithmic terms in your proverb and verifier. It seems that all constant size schemes require a setup, which is like what the cat bilinear pairing one required. So I have a question, like, if at some point we're able to have a constant size without the setup. So currently we have indeed the category, then we have the dark, which we heard about earlier, which are based on groups of unknown order, you can instantiate them with an RSA number. So depending on what you. Maybe you can use that it's fast, maybe has a setup. Or you can use the class groups, which are like slow, but they don't have a setup, and slow and also big.
00:05:47.706 - 00:06:52.574, Speaker A: And there's like one other approach, which I believe is like what's used in fractal and also in a polynomial commitment scheme by Vasov and Panarin, which is very similar to fry, which is used primarily in starks, where you get though, which works, but it gets you like a logarithmic term overhead due to the metal trees that you're using. So a caveat on the darks is like that, if you notice, like, okay, sure, it is transparent. However, it has this logarithmic term, which if you use a separate tap, it moves over here. And also another part of the darks is that, if you notice here, yes, indeed, it has like a logarithmic term, improvement over the fry based protocols, but it's in groups of unknown order, rather in class groups, if it actually is transparent. And also, this type of operation is much more expensive than hashes. And from a practical standpoint, if the operation is like 100 times worse, it's bad. So we need to figure out some way around it.
00:06:52.574 - 00:07:31.678, Speaker A: What you could do here is that you could have a hardware acceleration, like have some ASIc that's able to calculate that. But at that point, you're kind of like comparing apples to oranges. And now let me talk a bit about the universal setup again from a practical standpoint, because the whole idea of the universal setup is that firstly, it is able to be used for any kind of circuit. And also, if it's updatable, you're able to always trust it in some way. So, okay, I have the public parameters. I have a deterministic derivation for a specific circuit, and I get like proving key, a verifying key, a. And I distribute them to the people that want to use it.
00:07:31.678 - 00:08:09.930, Speaker A: And if I have another circuit, I again do a deterministic derivation, and I get like another system. This works very nicely in the sonic construction. They show how you can do it very efficiently, this derivation. So we're good there. However, there is like an implementation detail, as a researcher would say, with respect to the continuous setup. And let's say that we're in a deployed blockchain scenario. We have a miner who is like the natural party, like, to have both the proving key and the verifying key actually, and let's say that we have Alice that does not trust this setup because reasons.
00:08:09.930 - 00:09:10.374, Speaker A: So Alice goes to the common reference string, updates it in some way, and then gets back like the new proving key and verifying key. The thing here is that if Alice wants to generate a proof for the circuit, who is going to verify that proof? In order to verify that proof, you must also get the updated proving key. So the new structured reference ring must also be distributed to the people that you're trying to prove it. So the issue is that in this case either Bob, in order for Bob to be part of the same system as Alice, Bob must either receive the new proven key or the miner has to keep previous verification keys. And this might seem obvious that yes, of course this must be done if we're on different proof systems, if we're on different instantiations, like how are we going to speak to each other? But from a practical standpoint, the continuous setup is not continuous per se. It's like continuously update. It's updated in discrete time steps.
00:09:10.374 - 00:10:34.934, Speaker A: So if you're in a blockchain scenario, you cannot really have a transaction which says at any point in time update the setup, because it also has the danger that it can cause some sort of race condition where I am preparing a proof for the current instantiation of the circuit, then somebody else comes in, contributes to the randomness, regenerates the new circuit specific reference thing. Then the miner updates their verifying key, and I'm left with a proof from the previous instantiation of the setup. So it would be nice in this case if I was either able to kind of update my already generated proof in some proof, that is, with the new randomness, because otherwise I have to regenerate the proof. So what would you do? I think the solution to that would be to add discrete updates, let's say mini ceremonies every week that the randomness gets updated and your trust model kind of switches from the one time setup to the weekly setup. Again, from a practical perspective, the question I had with this new system was like, okay, we have the system that was deployed on sapling. Is it actually dead? So Sonic was like this initial thing which says, okay, we have an r one cs, we will encode it in a bivariate polynomial. That polynomial has to have a certain structure so that I'm able to convert it with some way to a univariate one.
00:10:34.934 - 00:11:12.046, Speaker A: And then I'm utilizing the polynomial commitment schemes that are univariate and have nice properties and Sonic's performance compared to growth. So this is taken directly from the paper. So with bulletproofs, it does not stand much to compare due to the linear verifying of bulletproofs. But compared to growth, Sonic is much more expensive. And in the help mode, yes, it is kind of good. And it's very practical for blockchain applications in the unhelp or the succinct mode, what you call it, I believe it's like way, way worse. So we can see that it's like 273 exponentiations.
00:11:12.046 - 00:11:52.046, Speaker A: Like it's way too expensive. And it's like three ways that you can run sonic. So either in the help mode, which Sean mentioned, or the full succinct, or you can do client side parallel proof generation. Because the way that the protocol works is that at some point I receive a challenge. And that challenge does not have to change. Like if I have multiple proofs on my, if I want to prove like ten transactions, I can reuse the challenge. So this would be nice if I could somehow, as a minor, you could look at this like the help mode being a synchronous protocol, whereas a minor, I received ten proofs which are already signed, and I batch them.
00:11:52.046 - 00:12:51.630, Speaker A: And the parallel proof generation is kind of like an asynchronous one where I have different z eyes and y eyes, and it allows me to improve my proof generation time. And I think that you could probably utilize the halo construction in this scenario to do this sort of aggregation of the proofs. The plunk, what it does is that contrary to sonic, which kind of is r one cs, it says, okay, we're not really into the rank one constraint system situation, we're just going to use this equation which allows us to do like more complex sequits. So it's kind of like an r one cs on steroids. And the whole issue with the whole hard part of plung is not really like proving multiplication or addition. It's literally showing that the output of some gate gets correctly fed as an input to the next gate. And the novelty here is like the very nice permutation argument based on the Lagrange basis.
00:12:51.630 - 00:14:19.786, Speaker A: And if we compare. So plunk also has like a sibling called Aurora lite by Ariel Gabson, who is watching the live stream, I believe, which has this very nice trade off, in my view, again for the blockchain deployment setting, which has a very much faster prover, but a different bit like bigger proof. So what I'm trying to say is that you have sonic, plonk and Aurora on Aurora Lite, and depending on what exactly deployment situation you want to have in your blockchain or your application, you can pick one depending on which proof sizes fit you best. And again, probably for actual deployment, I think you're thinking between Aurora Lite or Sonic helped and the plunk verifier is here to show this how it's only two pairings and some exponentiations compared to the sonic, which is much more expensive. So now we have Marlin, which is also which was done concurrently, more or less during with the plunk, and the dark work, which, as we can see from the graph, it has more or less about an order of magnitude difference with growth and about forex difference in verifier time. So how relevant this is at this point, the verifier in time. I could argue that after some point, it no longer matters.
00:14:19.786 - 00:15:14.130, Speaker A: You're fast enough to verify it. There's no way you're going to have blocks that arrive like every millisecond, so you're most likely good. However, the proving time, in my view at least, that's a very important thing. So we need to pay attention to the proving time and the proof size. So none of these constructions so far are like post quantum. So what can we do for that? We can, instead of using the current polynomial commitment schemes, we can learn from the starks and the fry, and basically use low degree testing to have some way to prove that we need know the polynomial while using just hashes. And this is what fractal uses, which my understanding is that it's like an optimization plus extension on Marlin, and also the polynomial commitment scheme by Vlasoven Panani, which has the same overhead as the stark.
00:15:14.130 - 00:15:52.310, Speaker A: If we compare the verifier and the provers, they're basically the same. And this is like a screenshot taken from Alice talk in Simon's last month. So we can see that more or less like the verifier time and the approver time, they're within one order of magnitude with each other. So it is kind of good enough already. And the verifier time is like ten to the minus two. So like, okay, how many proofs are you going to do with light blockchains already without any zero knowledge proofs, they already take too long to even propagate the block. So even if your cryptography is good, eventually you run into networking bottlenecks.
00:15:52.310 - 00:16:33.818, Speaker A: And what's really interesting in this is how, for example, fractal and Aurora, which are like the two post quantum snarks, or starks, they have huge argument sizes, like ten to the fourth, to ten, to the fifth as the constraint system grows, and growth 16 and Marlin, which are not like post quantum, you cannot even see them. So they're very small. And if you're building a blockchain again, you want very, very small. So probably these are like, Marlin seems to be like a better choice with respect to that. And some things that I like about growth is like, firstly, it's like rank one constraint system. We know it's r one cs. We have languages that we can use to compile down to it.
00:16:33.818 - 00:16:53.118, Speaker A: We know how to model it. It's great. It already is implemented in multiple backends. We have like circum bellman libsnag using it. There are batch verifiers for it, like algorithms which have better. You can verify the proof faster. There are GPU provers, which there was a big bounty by like around this year.
00:16:53.118 - 00:17:46.246, Speaker A: There were distributed provers, which Howard showed last year. And also there has been like a real world bounty. Like a lot of money is like at stake if you break growth. And my understanding is that currently there was a paper released by Noah and Mary like maybe ten days or less, where they have a very nice inner pairing product argument which they use to make BLS verification, BLS signature verification more efficient for many messages over the previous state of the art. But my understanding is that this could be used for storage. If I'm storing n growth proofs, I can kind of aggregate them and store only log and have total storage size of log n, which obviously it makes the blockchain smaller if you're storing proofs. And my thinking of it is that even if it does not allow you to make proof for proving or verifying faster, it makes your blockchain smaller.
00:17:46.246 - 00:18:56.630, Speaker A: So similar to how you can use, for example, cut through when you're doing mimble wimble to kind of make your chain smaller, you can kind of do some pruning of your chain using this technique. I hope the paper gets updated to include some notion for this. And I've taken this picture from Vitalik's blog post on plonk and added three more pictures. So we see how as you go more to the left, your proof size gets bigger, but your assumptions are improving. So like top left, we see stark and fractal with very minimal security assumptions with pretty big, and as you go more to the right, so like bottom right, previous snarks, it's like growth, which is like tiny but also pet program trusted setup. And what I'm really looking for is some way to apply all the innovations found in Marlin and Sonic and plunk primarily on the growth so that we can somehow get a setup that is at least verifiable. So perhaps you could use the ZK shark construction, which I did not have time to put in these slides.
00:18:56.630 - 00:19:44.360, Speaker A: That's about my talk, and one question I want to pose to the audience is that all these constructions, they've kind of gone off the way of true the pure r one cs, and they kind of add extra variables in their equation to make it more expressive. Like Zach earlier showed how not only are you not at r one cs with fun into and fun out unlimited, you have gates getting inputs from gates that are in the next level. So is it worth it to kind of get rid of r one cs and try to find a more expressive abstraction which perhaps reduces, like if you set some terms to zero to r one cs? I don't know. This is like my question, which I'd like to discuss, and thank you very much.
