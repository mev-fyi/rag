00:00:06.730 - 00:00:18.030, Speaker A: So up next we have Alex from ZK sync, who will be presenting on a somewhat spicy topic, I think, of ZKE EVM compatibility versus equivalence. So give it away for Alex.
00:00:21.730 - 00:01:03.854, Speaker B: Hello everyone. Thank you. Yeah, so I'm from ZK sync, which is the project aiming to bring EVM compatibility to the world of ZK rollups. We built the very first functional ZK roll up on Ethereum, which is now known as Zksync version one, and it's live on mainet for two years with simple payments. And back then it was hardly conceivable that we will be able to do generic programming in such a short period of time. But here we are. We introduced the concept of Ziki EVM One and a half years ago, raised significant amount of funds from different investors led by Andreez and Horowitz last year to implement it.
00:01:03.854 - 00:01:43.850, Speaker B: And we opened the first public testnet for ZKVM in January last year, and now it's very stable and we are about to launch on mainet in just 43 days. We're very committed to this date. So Zksync is a mission driven project. Our mission is to accelerate the mass adoption of crypto to achieve personal sovereignty. And every design decision we make technologically is considered from this perspective. So whatever we're going to discuss, always keep this in mind. This is the central judging point for us, and this is why we had to embrace EKVM.
00:01:43.850 - 00:02:20.360, Speaker B: It was clear already two years ago that EVM kind of is going to win the protocol race across all the standards that were emerging. It became de facto lingua Franco of the Internet of value. That is becoming true with Ethereum and other blockchains. This is like JavaScript. It's going to be sticky for a very long time. There are a lot of tools, services, libraries written for EVM specifically, and it's just going to stick. Alternative l ones are pivoting all to EVM, have to support it.
00:02:20.360 - 00:03:15.958, Speaker B: And ZK, I don't have to explain it at this summit, it's the holy grail. It's the only technology we have that actually breaks out of the blockchain dilemma. So the combination is what brings us to ZkVM. Now, if we are talking about ZKVM, we initially set out to build something compatible with EVM standard. There's been since then a lot of talks about equivalence as a separate concept, like compatibility is not enough. You have to be fully equivalent to satisfy all the developer needs. But the reality is it's a spectrum, it's not a binary switch and Vitalik recently a few weeks ago had a really good blog post about this, introducing several types of compatibility, like degrees of compatibility from just mere source code compatibility to which you could call full equivalence.
00:03:15.958 - 00:04:29.090, Speaker B: And they are nicely put on the spectrum of performance and compatibility. And you can see that we were trading off performance the higher on the compatibility we go. So I want to give a detailed overview of the types and give you some intuition on what are the trade offs, what matters, why it is so from the perspective of ZK environment and where we are with ZK sync. So as you can see, the types are basically just adding one by one features. Type four is the very basic one where we take source code, we can deploy anything written for EVM and it will just work in exact same way as you expect or like basically the same way, right? But you cannot deploy bytecode and the API might not be 100% compatible and you certainly don't have full block root equivalents, but the higher you go up you just add one by one those stages. So there is a really big difference between type four and everything above. Type four is native compilation.
00:04:29.090 - 00:05:38.890, Speaker B: When you take the source code and compile it into risk like architecture, it's not exact risk standard, but it's reduced instruction set. We do it using LLVM compiler from Yule and we use solidity native compiler to get to Yule. And we also have a Viper compiler which compiles directly to LLVM intermediate representation. Then from there to our ZKEVM instruction set which is optimized for being provable in the ZK environment. So it's optimized for us to be able to prove the execution trace of a program that runs the ZkVM opcode set. In order to do this, we had to carefully choose the opcodes to make them really optimal for the ZK environment. So if you're going with an architecture that was designed for PC for phenomena architecture, you will not have this optimal property of having all the opcodes as small as possible, as close as possible in a cause profile to each other.
00:05:38.890 - 00:06:34.380, Speaker B: So that you really have this micro basic blocks. Because in the ZK circuit that proves an execution trace at each step, at each cycle of the circuit, we have to implement all the constraints for this one step. So essentially the cost of each cycle is the sum of all of our opcodes. So that means you want as little opcodes as possible and you want them to be really mini, really basic, like not taking too much complexity, instead implementing the complexity through using multiple opcodes. If you need that and optimizing that at compiler time. So this gives you the lowest possible transaction cost on the proverb side from all the other approaches, like by orders of magnitude, probably by more than one order of magnitude. We will see why.
00:06:34.380 - 00:07:45.454, Speaker B: But this is a really important thing. But the second important thing is it gives you the ability to experiment and go beyond what is possible with EVM and implement new features in an efficient way that is important for user experience and developer experience for user experience. One good example is account abstraction that we support in Zksync version two. So you can do multicol out of box, so you just do one click in metamask or your wallet and you can execute multiple calls. You can support smart contract based wallets out of the box like argent wallets, working natively without having to instantiate them and do complex operations to pay to just get the wallet, et cetera. But developer experience is really important and it's going beyond just like pure convenience. You can take libraries that are written in any modern language that supports LLVM, frontend, like Golang, C, Python, Rust, and you just can compile them natively into LLVM and use them as libraries and connect them to your smart contracts.
00:07:45.454 - 00:08:57.080, Speaker B: And using the really low cost like absolutely pushed to the bottom execution costs. You can do quite complex computations, whether mathematical or compression or doing some transformations, whatever you need, like complex stuff like processing large arrays of data, working on doing, voting for example on certain subset, on a certain snapshot of your blockchain state, et cetera. So this is something you would lose. This second point you would lose if you went for fully VM equivalents essentially for any degree of compatibility above type four. The main disadvantage of this approach is that obviously we won't support opcode level compatibility, so all the tools that work at opcode level won't work. So we will need to re implement them. Luckily there are only a few of them, so mostly it's debuggers and tracers and we can just work with vendors to like there are a few of them that are popular, we can just implement support for ZKVM and they will work also.
00:08:57.080 - 00:09:48.006, Speaker B: Okay, this is type four. And yeah, as I said, we embrace it at ZK sync from day one of our launch initially, because it's really critical to our mission to be able to support real mass adoption. Mass adoption means excellent ux, where users have better ux than they would expect from neo banks and all the web two applications. So you really don't want to bother them with inconveniences with multiple clicks, et cetera, et cetera. And you really want to be able to support use cases that require the absolute marginal bottom of the costs, such as social networks. We're talking about orders of magnitude. So there is a difference between ten cents per transaction and 0.1
00:09:48.006 - 00:10:43.590, Speaker B: cent per transaction. It's quantitative, but it really leads to entirely new use cases being unlocked that are not possible if your transaction costs are in the range of cents and not like microsense milliseconds, et cetera. But all the types above type four are, let's call them Cisc, complex instruction sets because you have to interpret or natively execute evm opcodes. And they are complex. Some of them are simple like add, sub, move, jump, compare would be basically what we also have in EzKVM instructions set. But you have some really ugly ones like push 32 where you're manipulating 32 variables of your stack, et cetera. Or working with storage, working with some pre compiles like hashes.
00:10:43.590 - 00:11:28.934, Speaker B: So there are two ways you can solve. You can try to solve this. One would be to implement a circuit that natively supports all those opcodes. As you can see, the cost of a cycle would grow. Like will it just explode? Because you will have to maintain at each cycle, you would natively have to implement all these complex instructions. And yeah, this would be the most expensive, prohibitively expensive approach in my opinion. Second approach would be to try to interpret Cisc that is actually doable, but it just gives you this order of magnitude or more overhead.
00:11:28.934 - 00:12:47.460, Speaker B: Because if you remember here, like each opcode is really small, it's just like one cycle, right? But then to be able to interpret this opcode, if you just encounter add, you would need to add micro instructions that read the opcode, make a choice, what branch do we have to go compare, execute, and then update the counter. Each of that would be a separate transaction. And then you would add a lot of them for really complex instructions, but you still would add an order of magnitude more to simple instructions. So that is still suboptimal, but it is doable. And in fact we don't want to stop there. It is completely possible to implement this interpreter just as a contract on the natively compiled contract, which you can write in assembly, in rust, in solidity, in whatever is most convenient to write. So this would make Zksync instantly both type four and type three system.
00:12:47.460 - 00:13:50.514, Speaker B: In this classification it would just go up to type three, but we would be able to support both native code compiled into risk ZkVM and bytecode that for some reason people don't want to migrate. Maybe because it's like super sensitive on security and they just want to be, they don't trust the new compiler yet, so they want to use the old tested code, which is bottle tested with a lot of money on layer one. Or maybe it's just some legacy code that just needs to be running for whatever reason. Maybe some small. A good example would be deploying to the same address on multiple chains, right? So you really want to preserve the same bytecode so you can prove that it's the same hash as the same instruction hash of the contract code hash as on other chains like layer one. So that would be doable by just implementing this simple interpreter. In fact, there are these interpreters already written in optimistic roll ups because they need to execute EVM inside EVM.
00:13:50.514 - 00:14:28.030, Speaker B: So like one option would be to take this code and adjust it. Another option is just to write something from scratch and rust, which I think is going to be way more efficient. And then to get to type 2.5, we only need to support like 100% of the API. What that means is all the cryptographic functions, all the hashes, and all the pre compiles. The hashes we're basically going to support from day one when we launch on Mainet, like what we currently have on Testnet, and when we launch on Mainet in 43 days, we will have kchak and shuttle five six natively supported. They are quite cheap thanks to plonk and custom gates and lookup tables.
00:14:28.030 - 00:14:55.980, Speaker B: They're not very, not much more expensive than Patterson hashes or like a Jupyter hashes, so we are actually natively using them. Other P compiles we expect to be supporting by the end of year. The most complex one is probably the pairing computation bn two five six pairings and like maybe other pairings in the future. But there is work going on on this, so we will support them. It's not a big deal. And that's the only difference between type three and type 2.5. As you can see, the performance is essentially the same.
00:14:55.980 - 00:15:51.802, Speaker B: You could get some boost in performance if you avoided normal hashes and would switch to algebraic ones. But if you really want to do that, you're much better off with type four anyway. Then you're probably writing your code optimized for ZK environment. Now if you go on to go up or actually, yeah, before that. It's important to notice that when we start with type four, it's like starting with a really powerful machine that can new MacBook or some new processor that can simulate older systems, mainframes or like MS DOS or older versions of Unix. But you cannot really do the other way around. You cannot take an old like eighty s, ninety s machine and run a modern macOS on it.
00:15:51.802 - 00:16:36.940, Speaker B: So you really have to start at the bottom and go up because you cannot do it the other way around. And this is why we're doing this. But if we wanted to go up higher, it doesn't really make sense for an l two. So the type of two would mean that your gas calculation is exactly the same as in Ethereum at every opcode is metered in exactly the same way as it's done on layer one. And block root hash would include like consensus mechanism and all the storage updates, et cetera. So from the application standpoint it doesn't matter because you don't have access to you cannot introspect state on ethereum. Really.
00:16:36.940 - 00:18:05.298, Speaker B: So having consensus there would deprive you of the ability to experiment with consensus layers in L2. But that could be pro and con. But having guess metering is really disadvantageous and counterproductive because the cost profile in L2s will always be in favor of computation and cheaper storage over layer one. In layer one you have wild difference in different types of resources, how they are metered compared to L2, right? So if you force people to meter it the same way as it's metered on layer one, you will be either not having the advantages that L2 offers you, like really cheap computation, really cheap storage, or you will be opening yourself to a denial of service attack vector. Neither way is good. So the only thing why you would want to use the type two type one systems potentially would be to try to prove Ethereum itself, like every single block of Ethereum and going back to Genesis or like latest regenesis. But that would actually only give you the ability to quickly sync a node not much beyond that, because every new block would be coming really fast every few seconds.
00:18:05.298 - 00:19:18.060, Speaker B: Probably we don't have the proof latency yet for a couple of years to be able to generate the proof so fast for ethereum, especially with all the inefficiencies of type one compatibility. So it's really not a big deal. So if Ethereum wanted to become a fully succeeded chain, what they could do is just reduce the block limit from 12 million gas or 30 million gas to just like half a million gas, make the blocks really lightweight and kind of force everyone into l two s and keep the l one just for the very raise the storage costs really high and reduce the gas limits so that full nodes only work with a few storage slots that are actually commitments from roll ups. And everything else is happening in the roll ups. That's a much easier way to approach this than trying to go for a full type one equivalence. What would be real equivalents? So, yeah, that's why we're starting with type four and going really fast to type 2.5 with this case.
00:19:18.060 - 00:19:42.280, Speaker B: That's it. Thank you very much. Really happy to take any questions. And I also want to add that we are actively hiring, and if you are interested in your knowledge, proofs, or if you want to do any engineering, like whatever, we are open in all roles, both technical, nontechnical. So please talk to me after the call, after this talk. Thank you for attention. Really happy to take any questions.
00:19:47.610 - 00:20:06.766, Speaker A: You, as usual. Does anyone have any questions? Wow, really? All right, well, I have a question, actually. Yeah, so it's not just an l two problem for the metering resources. Actually, it would be essentially any ZK chain or the succinct verification.
00:20:06.878 - 00:20:07.540, Speaker B: Right.
00:20:08.870 - 00:20:26.870, Speaker A: The difference between metering for certain resources also comes from the fact that you now have succinct proofs for certain things. So your computation is. Cost is also different. So it wouldn't just be an l two difference between l one. It would also be the fact that you have, like your state transitions are verified by sync proofs. No? Or is that incorrect?
00:20:28.090 - 00:21:14.802, Speaker B: So the difference comes from the fact that you're trying to measure the costs of running a full node. If you're running in a layer one full node, you have to be able to access storage really fast. So, first of all, you're using computational resources, which are limited because you're running it on a single processor. You cannot even take use of multicore because it's always executed sequentially until we have some really fancy parallel processing of transactions. So that's one thing. And you have to account for the fact that you're running it on tens of thousands or hundreds of thousands of machines, right? So you have to increase to add this overhead there. With ZK, you don't have this because you only run it once on your approver.
00:21:14.802 - 00:21:48.770, Speaker B: It gives you some overhead, maybe 100 x, maybe 1000 x, but that's it. That's flat. And then you can add like 1 million full nodes, and they will all take that. And for the storage, it's coming from the fact that you have to maintain ssds that are really efficient in random access. That's the problem with data availability. When you just put the bandwidth when we push the storage data from storage to call data. All you need to do is just download a chunk of data directly, a continuous chunk of data from the network.
00:21:48.770 - 00:22:01.590, Speaker B: You don't have to jump on disk from point to point. So that is any layer one. Like if you have a succinct layer one, you will probably have the same profile as roll up. Yes.
00:22:01.740 - 00:22:06.360, Speaker A: Cool. All right, we have one more, or at least one more question.
00:22:08.250 - 00:22:32.400, Speaker C: So the big benefit of web3 is the fact that it is permissionless and stuff. How big is the risk, in your opinion, of some big company like starquare conquering all the roll ups and just forcing old web two censorship in web3?
00:22:33.350 - 00:23:08.970, Speaker B: That's a terrific question. The risk is increasing because at l two it's very tempting to have higher centralization because you are safe anyway, right? You can have a single centralized operator who will be sequencing transactions, and then we're all roll ups. So it's the same security as Ethereum. And then you give this one entity the right to standard transactions. So we need to be racing now to decentralize l two sequencers ASAp all of the roll ups. Like whoever doesn't do this doesn't have a right to. They can be doing some corporate chain.
00:23:08.970 - 00:23:25.390, Speaker B: I cannot talk for Starquare. We don't want to become a corporate chain. We want to build a permissionless protocol in the spirit of Ethereum. We are fully aligned ideologically with Ethereum. We're fully embracing the ethos of Ethereum. So we're going to be fully open source, permissionlessly open source. It's very, very important to us.
00:23:25.390 - 00:24:13.162, Speaker B: We haven't open sourced version two yet. We're going to do it after the audits. But when we do, together with the launch, it's going to be MIT Apache license, so anybody can fork it, not just open source for read only purposes and like copyright version, right? So that you can fork it away. If the chain that we initially launch is not fully decentralized, is not embracing the ethos, you should be able to move away. This is why we're supporting priority queues for forced exits and they are going to be working in the batch mode so that you can do mass exits. The way it's going to work is you're going to have some decentralized consensus running the sequencer. It can have some trade offs, it can be less validators, it can be more validators.
00:24:13.162 - 00:24:33.430, Speaker B: But if some chain, and there will be multiple chains, like maybe it's l freeze, maybe it's separate instances, if some of them are censoring transactions from the users, the user should just say, screw you, we're going to mass exit to a different instance run by someone who is not censoring because they have a better decentralization.
00:24:36.330 - 00:24:49.538, Speaker C: Are you aware of any proprietary, I don't know, technologies by starkware that give them benefits over ZK sync?
00:24:49.734 - 00:24:50.858, Speaker B: Proprietary?
00:24:51.034 - 00:24:55.566, Speaker C: Yeah. Well, patented, I don't know, is this.
00:24:55.668 - 00:25:34.086, Speaker B: Thing at this point that is not a thing because what they're building is Cairo. Like it's a completely different direction from what we're doing. They are diversing, essentially diversifying from Ethereum. They are telling you you have to learn this new language, reimplement all your code in it, reimplement the entire ecosystem, all the tools, all the libraries from scratch, and just use those libraries. And then you're going to use it under Polaris license, which is under copyright, and you only may use it with one single sequencer that is designated by this multi signal or whatever engraved in the Polaris license. Right. And then you don't have a choice.
00:25:34.086 - 00:26:03.810, Speaker B: You cannot fork. So like that's a classical lock in which might be beneficial if you are a corporation. You want your slas, you want guarantees, you're going to go there and you're going to use it, and then you're going to have your support or whatever. That's not our way. Our way is to be compatible with Ethereum so that you don't have to learn stuff. You just take existing code, take existing libraries. We support like web free API completely, 100% with other reads writes.
00:26:03.810 - 00:26:33.354, Speaker B: Based on that. We have integrations now with chain link with the graph where you can use those APIs just the same way you would use them on layer one, on other roll ups and you can fork away. That's the most important thing. You should be able to fork away. That's what makes blockchains beautiful. It's a little harder with L2s, with like there are some specifics with upgradability that make it really hard to. You cannot fork away in all situations.
00:26:33.354 - 00:26:48.020, Speaker B: If there are vulnerabilities that are immediately requiring fix, that is still a hard problem that I don't have a perfect solution to. But for normal operation, if you're just being censored by validators, you should be able to mass exit away.
00:26:50.950 - 00:26:56.260, Speaker A: I think this is the last question, unfortunately, because we're about to get to the break time.
00:26:59.770 - 00:27:25.550, Speaker D: But yeah, I just wanted to ask because it seems like in order to allow for the censorship resistance, we need both validators and provers that are acting in the best interests of that cause. But then, given that the, the provers are, like you said, working on an overhead of like 1000 x to what a verifier would be doing, what incentivizes people to run provers.
00:27:26.770 - 00:27:59.734, Speaker B: Provers are actually easier, so the validators are more at risk of centralization, because we want the best ux. Best Ux means low latency for transactions. You want transactions to be confirmed instantly. The progress can be fully permissionless. So once a block is formed by the consensus of the sequencer, anyone can be building that block. So there will be some provisions, some sequence which is expected to run so that you don't have a race, that two people do not work on the same proof at the same time. But that is minor.
00:27:59.734 - 00:28:47.270, Speaker B: Like anyone who has the block data from the consensus can be building the block and the proofs are going to be decentralized. Very broadly, because we are using gpu accelerators, we brought the latency down to just like some seconds, like half a minute a minute, because everything is run in parallel on different gpus. The gpus give you a boost of several orders of magnitude of improvement over cpus. And when I say a boost of performance, I always mean the costs. The latency is going to remain flat, because the latency is determined by the number of steps of recursion. And at each step we do like 20. We can aggregate 20 different proofs, or like more.
00:28:47.270 - 00:29:20.418, Speaker B: And then you just have like three steps that give you blocks of basically arbitrary size of tens of thousands of transactions. Each step takes like 15 seconds on a powerful GPU. And these gpus are abundant. We have a lot of them now freed up this morning with a merge, and they are not going anywhere. And we will always have a lot of gpus, like running on computer gaming and machine learning and all this stuff, which we will be able to reuse. And the best thing about gpus is they are very decentralized. They are in the hands of a lot of people.
00:29:20.418 - 00:29:58.138, Speaker B: So we'll be able to build a network of decentralized provers that just do this job, and we aggregate. So provers are less at risk validators because of latency is more important. But my guess is we will have several instances, maybe like as an l three. And l three s are coming very soon to Zk sync, where some l three s will be optimizing for latency, maybe for high performance trading, but some of them will be optimizing for sensorship resistance. And you will have hundreds of thousands of validators to make sure that you actually always get your transaction included. Great.
00:29:58.224 - 00:30:02.040, Speaker A: Thank you so much. Alex. Let's give it up one more time. Thank.
