00:00:03.450 - 00:00:26.310, Speaker A: Hello, everyone. Welcome back to another session of ZK Study club today. Very excited to be doing a deep dive in some new work called Lasso and Jolt by Justin Thaler. And this is joint work with a few other co authors, which I'm not going to go all the way through them all, but you can read them here. But Justin is here today, and he's going to be presenting this for all of us at ZK Study club. Welcome, Justin.
00:00:26.890 - 00:01:13.940, Speaker B: Thanks for having me. Yeah. So I want to tell you a little bit about two new works called Lasso and Jolt. So there are three and a half hours of talks from a seminar at a 16 z available on these works. And so my goal here is to have a completely self contained talk. Obviously, I'm not assuming anyone has watched three and a half hours of videos prior to this, but at the same time, to make the talk, like, complementary to those three and a half hours. And, yeah, I sort of view this as an opportunity to kind of update the presentation in light of a lot of the feedback we've gotten since we released the papers a couple of weeks ago.
00:01:13.940 - 00:01:56.318, Speaker B: And also just to dive deeper into the weeds since I think the audience here today is a little bit more technical. I did just throw this presentation together, like, in less 2 hours. So I really welcome questions all along the way. I think it'll both be kind of more fun, and it's not like this is a carefully considered presentation anyway. So please just jump in about anything at any time. Okay, so let me start by just telling you what my plan is today, and I welcome diversions or tangents. So I'll tell you first, what is the lookup argument? Because Lasso is essentially a new lookup argument.
00:01:56.318 - 00:02:37.470, Speaker B: Then I'll tell you what Lasso and jolt are. Then I'll give details of Lasso. Details of jolt. Jolt is essentially like a system that builds on Lasso. So Lasso is a key technical component to Jolt, but it is not the only way to build on Lasso. So I'll describe at the end how I think Lasso should be viewed as a tool that exists independently of something like Jolt, can be used in other ways and sort of speculate on how else Lasso might find applications in the future. And as we'll see, there's a lot of lasso that's really not specific to lasso.
00:02:37.470 - 00:03:30.880, Speaker B: The kind of key new techniques or insights in Lasso actually can be applied to other lookup arguments as well. And I'm actually quite optimistic that we might see a future where there are all sorts of lookup arguments being used depending on certain parameter settings and things like that. But all of them are kind of building on the same key insights that I'll tell you about today. Okay, so lookup arguments, what are they? So I actually want to introduce two variants of lookup arguments. The first one is the one that people normally study or normally refer to when they say a lookup argument. But I'm going to call this an unindexed lookup argument. So in this setting, the prover has kind of already committed to a vector a, let's say of length m.
00:03:30.880 - 00:04:13.514, Speaker B: And the lookup arguments lets the prover prove that every entry of a resides somewhere in a predetermined table t. Okay? So that is for every entry of the committed vector a, there is some index bi, such that that entry of a equals the bi entry of t. Okay? So this is actually equivalent to a subset relationship. You can think of t as a set. And the lookup argument lets the prover prove that every element of a is in the set. So it's like kind of like a batched set membership proof. So that's an unindexed lookup argument.
00:04:13.514 - 00:05:34.354, Speaker B: If you read any paper before Lasso that says lookup argument, that's probably what the paper means, sort of. Lasso naturally gives and jolt is going to need something I call an index lookup argument, where here the vector b of indices is pre committed and the proverbs claim actually refers to those indices, right? So that is an index lookup argument lets the prover prove that for every entry AI of a, AI equals the bi entry of the table. Okay? And in this case I'll call a like the vector of looked up values, and b the vector of indices. Okay? And here, just as I said, another way of describing unindex Lookup arguments is like subset relationship or set membership queries. An index lookup argument is really the same as like a read only memory, right? So saying that AI equals t of bi is the same as saying that if you read cell bi of the memory specified by the table, the result, the value stored there is AI. Okay? And at the end of this presentation I'll give yet other ways to view index lookup arguments. I think thinking of lookup arguments as lookups is very useful.
00:05:34.354 - 00:05:48.922, Speaker B: I think other ways of viewing them which are completely equivalent are also useful. And actually it's good to actually kind of formally state and clarify all of the views. Anyone want to ask any questions? At this point?
00:05:49.056 - 00:06:06.350, Speaker C: I don't have a question. So in the index case, do you have the indexes pre computed? So both proven verifier know them before starting the protocol. Or it could be the case that proverb commits the first step, let's say, of the argument to these indices.
00:06:06.710 - 00:06:47.866, Speaker B: Yeah, great question. So the verifier does not know a or b, they're committed. The verifier does know the table, but how the verifier knows the table is going to. Some of the tables we want to think about are enormous and so big you can never write down every entry in them. But the verifier still knows it because it has a succinct implicit description or something like that. I'll say more about that later. So the important point here is the verifier does know the table somehow.
00:06:47.866 - 00:07:29.606, Speaker B: Again, I'll say more later, but does not know the entries of a and b. They are committed by the prover. Now you can either think of the commitments to a and b as like part of the problem statement. So this is just like an accounting question. When I state the commitment costs for the prover in our index lookup arguments, I'm going to just assume that the entries of a and b are already committed, just so I don't have to charge the prover to commit to them. I'm thinking of them as kind of pre committed or part of the problem statement. Yeah, but you could also just think of the very first step of the lookup argument is the proofer commits to a and b and then proves what's listed on this slide.
00:07:29.606 - 00:07:34.540, Speaker B: Does that make sense? Yes.
00:07:35.170 - 00:07:51.966, Speaker D: Another funny question. Whenever you talk about index lookup argument, generally what we do in, as I say, previous papers is that we take a random linear combination of the index together with everything. What's the advantage, besides the random complexity here, of not going through that approach?
00:07:52.078 - 00:07:57.480, Speaker B: Yeah, great. I plan to say more about this later too, but let me address it now.
00:07:58.250 - 00:08:04.070, Speaker C: I noticed that as well, because if you do that, then you can avoid the extra range checks.
00:08:04.730 - 00:09:10.620, Speaker B: Okay, great. So firstly, there are no range checks here. If Bi is out of range, meaning bigger than the table size n, the proverbs claim is just false, and there's just no way for soundness guarantees the prover is going to fail. In the lookup argument, there are generic transformations which I think you're asking about from unindexed lookup arguments to lookup ones, right where you said you sort of take a random linear combination of the indices and the table entries, and that's like a new table, and you apply the unindexed lookup argument to that new table. And there's another transformation which we actually describe, not that I think there's anything new to it at all, in an appendix of the last so, eprint, where rather than a random linear combination, you just pack, deterministically the indices and values together. That obviously requires the field to be big enough that you can sort of pack those two values together without any sort of overflow issues.
00:09:11.630 - 00:09:15.802, Speaker C: That was what I was referring to in regards to look at the extra range checks.
00:09:15.946 - 00:09:59.078, Speaker B: Okay, great. So even in that case, well, whatever. In last one, Joel, we don't use either of those transformations like, we just sort of directly give an index lookup argument. Here is why I think using those transformations can be problematic, and this is a subject that's worth thinking through carefully. This is sort of my initial impression, and I'll say more about this later. Lasso is really heavily going to get mileage out of two properties of these lookup tables. One is that if the values in the table are small, the prover is going to be, like, really fast, much faster than if the values were like random field elements.
00:09:59.078 - 00:11:12.462, Speaker B: And so a random linear combination of indices and values would lose that property, and then the packing thing wouldn't lose the property, would make the table entries bigger, but not like crazy big. But we're going to deal with really big tables that have a certain kind of structure, and that sort of packing together indices and values into the table for some tables might not preserve that structure. I think for some tables it will, and for other tables it won't. I think to build jolt, we would have a problem for some tables we need to deal with. So my sense right now is it's really crucial that we directly give an index lookup argument without invoking one of these transformations. But again, I think it's a really important topic to think through, not just me, for, like, 15 minutes or something. And I think a lot of the ideas or techniques we're putting forward can and will benefit other lookup arguments as well, and can actually potentially improve jolt.
00:11:12.462 - 00:11:16.226, Speaker B: If you can address the issues I just raised about the transformations.
00:11:16.418 - 00:11:19.880, Speaker C: Yeah, it definitely seems well motivated to me.
00:11:20.330 - 00:12:04.154, Speaker B: Cool. Other questions? Okay, great. So lasso is like a new family of index lookup arguments. High level. The prover is about an order magnitude faster than priorworks, and this is the bottleneck for the prover in prior works, tends to be the commitment costs. So basically, there's always some kind of polynomial commitment scheme that gets used, and lasso sort of by about an order magnitude reduces the prover time to deal with the polynomial commitment scheme. And so the benefits are coming in from two aspects.
00:12:04.154 - 00:12:54.640, Speaker B: The prover in Lasso is committing to fewer field elements than prior works, but also smaller field elements than prior works. And actually most of the order magnitude, and it really can be a little bit more than an order magnitude, is coming from. The second thing, that the field elements are all small rather than. So you maybe get a factor of two or three fewer field elements, but a factor of six or to ten from all of them being small. By small, I don't mean that the field is small, although I think this property could help you work over smaller fields, potentially in some contexts. What I mean is, even if the field is big, all of the committed field elements are in like zero one, up to two, to the 20 or something. And that speeds up the commitment schemes, depending on which commitment scheme you're using.
00:12:55.990 - 00:13:12.534, Speaker C: I had a question about this. So the paper doesn't mention zero knowledge. If you were doing this in zero knowledge, would you have to randomize those commitments? Would that mean that they're no longer small?
00:13:12.732 - 00:13:21.240, Speaker B: Yeah, great question. So, yeah, the paper we're putting off dealing with ZK till later, I completely understand.
00:13:22.190 - 00:13:24.890, Speaker C: We did too in the first halo paper.
00:13:25.040 - 00:13:52.258, Speaker B: Great. Yeah, it should be fine to just basically have a handful of blinding factors. So in addition to committing to a whole bunch of small field elements, the prover also commits to a pretty small number of random field elements. And I do not think that that should bottleneck the prover. And I'll leave it at that, just because we haven't worked through the detail.
00:13:52.344 - 00:13:55.940, Speaker C: Yeah, that sounds plausible. The devil is in the detail, but yeah.
00:13:56.390 - 00:15:08.170, Speaker B: And then the sort of generic, which might feel like a bit of a cop out, but you can always compose with a ZK snark at the end, as long as the verifier is fast enough to turn into the circuit or something. Other questions? Another new property of this lookup argument, which I think is more of just a nice thing. I don't think it's completely crucial, is for many, many lookup tables in particular, everything we need in jolt. I'll tell you shortly what jolt is. No honest party has to commit to the table in preprocessing. So, like in prior lookup arguments, the verifier sort of learns the table, really, by being handed a commitment to the table that has to be computed by an honest party. All of the tables arising, like in jolt, just have this nice structure to it that the verifier can sort of process the table on its own with like 50 field operations or something, 100, 200 even for ridiculously big tables.
00:15:08.170 - 00:15:45.250, Speaker B: And finally, lasso sort of naturally supports gigantic tables as long as they have some structure to it. So I'll say much more about this soon. I mean, people are already doing things like this. But in general, if the table has size n, think of n as like two to the 128. The designer can choose some setting of a parameter c. And basically there are two kinds of prover costs. There's sort of a cost that depends only on the size of the table, but not on the number of lookups, and a cost that depends only on the number of lookups, not on the size of the table.
00:15:45.250 - 00:16:05.600, Speaker B: And the bigger c is, the smaller you make, like the table only cost, but the bigger you make, the per lookup cost. And so for table size, 228, maybe you want to set c to be like six or something, and you'll kind of equalize these two costs if you're doing, say, a few million lookups into a table of this size.
00:16:08.050 - 00:16:26.382, Speaker C: Yeah, this question might fit better later on when you get into detail. I was wondering how that was different from doing c, separate lookups and then combining the results to get the thing that you're looking up in the larger virtual table.
00:16:26.526 - 00:16:42.954, Speaker B: Yeah, so let me handle that later. The short version is not different. Up to some details that might be really important. Okay. But conceptually it is not different. Okay. Yeah.
00:16:42.992 - 00:16:45.500, Speaker C: So it's just concrete optimizations on that.
00:16:46.030 - 00:17:30.280, Speaker B: Yeah, you could think of it that way. There's some of these issues that I discussed. Thanks to your earlier question about. We naturally are invoking an index lookup argument or become really important when you're looking to combine the results of the small table lookups into the big table lookup. Okay, so that's lasso in a slide. What jolt is, it's like a new technique for designing zkvms. So these are snarks that sort of let the prover prove it correctly, ran a program expressed in the assembly language of a specific sort of processor, instruction set architecture or something.
00:17:30.280 - 00:18:12.310, Speaker B: Step by step on some witness high level, we think the prover is just faster than prior approaches to ZKVM design. Jolt is not fully built yet. So what we're doing here is just sort of a careful analytic accounting. Basically, we're confirming that the non commitment cost for the prover should not be a bottleneck. And then we're very carefully counting analytically, what is the cryptographic, the commitment cost for the prover and comparing it to prior zkvms. And soon, hopefully, jolt will be built and we will be not just doing that kind of accounting and comparison. High level.
00:18:12.310 - 00:18:48.926, Speaker B: What's going on here is that each primitive operation for the vm will be executed as just one lookup into a giant table, that table being the entire evaluation table for that instruction. So if the instruction operates on like 264 bit inputs, that's a table as size like two to the 128. Now, I think this is like a new way of thinking about primitive instruction evaluations. But again, people are already doing very similar things as Dara's question sort of got. What are people already decomposing big tables into small ones and combining the results?
00:18:49.118 - 00:18:53.310, Speaker C: So the answer is yes, we did this in the orchard circuit for the range checks.
00:18:53.390 - 00:19:11.346, Speaker B: For the range checks. Okay. Yeah. Okay. Yeah. So I will get to how I think the precise way we go about things is a little bit different, but conceptually totally the same. You're just executing the instruction by doing c lookups into smaller tables and combining the results.
00:19:11.346 - 00:19:41.810, Speaker B: I mean, that's it. Okay. All right, so let me give a little more detail about lasso, and then I'll say more about jolt. So this probably, I don't know, 15 minutes about Lasso, and then onto Jolt, maybe 20 minutes. Okay, so a little bit more specifics about the cost of lasso. Before I just said kind of order number of lookups plus table size to the one over c. So more precisely, the lasso prover to do m lookups into a table of size n.
00:19:41.810 - 00:20:36.340, Speaker B: And these are index lookups, commits to three c field elements per lookup. And then there's this fixed cost of something like c times n to the one over c. For a lot of tables, you actually don't get that c in front of the end of the one over c, but it doesn't really matter that much. And all of these field elements are small, say, concretely in the set zero up to, I don't know, two to the 22, to the 22. It depends on how many lookups you're doing and what c is and things like that. And this is assuming all of the table elements are small. Just because you just cannot avoid, like, by definition of a lookup argument, the prover does have to commit to the results of the lookups, right? So if the results of the lookups are huge, two to the 200 or something, there's just no avoiding committing to those.
00:20:36.340 - 00:21:22.338, Speaker B: And the importance of small field elements. If you've looked at the paper, this is sort of emphasized heavily, but is MSM based. So multiscaler multiplication based commitment schemes are just much, much faster when committing to small field elements than to random ones. And in particular, you can basically commit to each of these sort of three c field elements per lookup with just one group operation per field element. So that means if it's an additive group, like one group addition. So that's much faster, just to be clear, than like a scalar multiplication, say 400 times faster typically, or something like that. Okay, now, c equals one is like a special case, so c equals one.
00:21:22.338 - 00:22:12.900, Speaker B: I call it basic lasso. We didn't use that terminology in the paper, but I'm sort of using it in presentations where you don't pay for this three out in front of the per lookup cost. And the proverbs commitment cost is only m plus n field elements in this case, and even amongst those m plus n, many of them are zeros. Okay? So at most, two per lookup are non zero. And in many cases, it's sort of determined based on just how many distinct table elements are accessed. As few as m of them might be non zero. So this is important because committing to zeros is like free in a sense.
00:22:12.900 - 00:22:55.978, Speaker B: It's not no group operation at all. Okay, so, yeah, hopefully this makes sense. Now, let me just take a step back and explain where these costs are coming from and kind of how they compare to other lookup arguments and naive approaches. So it's not so difficult. The natural lookup arguments you would come up with, I think, with a little bit of thought, what they would naturally wind up having the prover do is commit to a vector of size n. That's the table size at most m nonzeros. And so think of these as like access counts.
00:22:55.978 - 00:23:52.754, Speaker B: People call them multiplicities. So for each cell of the table, how many times was it read? Okay, now, it's actually possible with msms to commit to such a vector, like as fast as you could hope, which is basically exactly, sort of the lasso like cost of their m small field elements. You can sort of just ignore all the zeros because they don't change the value of the MSM. So where you run into a problem is that at the end of the lookup argument, the verifier is going to have to evaluate the committed polynomial that we're interpreting this length n vector that's very sparse, as like the coefficients of a polynomial or something. The verifier is going to have to evaluate that committed polynomial at a point, and the evaluation proof is going to wind up paying costs that grow with n the length of this vector. The evaluation proofs don't seem to benefit, as far as we know, from sparsity of the committed vector. And so that's the problem like everyone's grappling with.
00:23:52.754 - 00:24:45.650, Speaker B: And to be clear, there are some polynomial commitment schemes where the cryptographic work of the prover in computing the evaluation proof is sublinear in n, but it's still square root n. So if the table size two is 28, it still just doesn't work. Okay, so there's a great line of work starting with caulk and most recently culminating in CQ, that deals with this issue by pushing all of the processing that depends on n to pre processing, right? So you pay, it's something like n log n group scalar multiplications or whatever. In pre processing, I think it's also specific to Kzg commitments. So you need an srs of size n. And the nice thing after that is after that pre processing, your costs are completely independent of n. So you don't even get this like end of the one over c term.
00:24:45.650 - 00:26:24.370, Speaker B: But I think with them, with cq, it's like seven random field elements committed per lookup, whereas lasso, you do have this end of the one over c term, but it's like if c is six or something, it's like roughly 18 or something, small field elements per lookup, which is much faster than seven random field elements. And so what Lasso does for these big tables is just deal with this issue of this sparse vector of length n in just a completely different way. So what Lasso has the prover do for these giant decomposable tables is the proverb will never commit to a length end vector at all. Instead, what the prover will commit to is the much shorter vector that sort of just lists which entries of the giant length n vector are non zero. And then that's the commitment to these polynomials of access counts is just like, apply a dense polynomial commitment scheme, like a normal one, to the vector that just lists the non zero access counts. And then when the verifier says, hey, I need to evaluate that committed polynomial at a point, the prover just proves that it correctly ran an algorithm that sort of takes the evaluation point and takes the non zeros that were already committed and computes the evaluation given those two inputs. So the evaluation proof is just like I prove I ran this evaluation algorithm correctly on this particular representation of the sparse polynomial.
00:26:24.370 - 00:27:12.282, Speaker B: That's lasso. Okay, any questions? Cool. Okay. And then in terms of verifier costs. So you wind up with logarithmically many. And actually, it turns out just to be log in the number of lookups m, like even for these big tables, n that have huge size n, logarithmically many field operations. And if you're applying fiat chimier hash evaluations coming from the sum check protocol, plus one evaluation of the committed polynomial, so the prover commits to several polynomials, and you can batch all the evaluation proofs into one.
00:27:12.282 - 00:28:12.190, Speaker B: So that's like one evaluation proof for my polynomial size, like n to the one over c. Yeah, and I think these costs are low enough that basically you should ignore them. I mean, some people might disagree with this, but they should be low enough that you can do something like snark composition. This is a little bit dependent on exactly what polynomial commitment scheme you want to use. I think ideally you would use one that has the fastest proofer you can come up with, and the evaluation proofs might be somewhat bigger. And you just want the evaluation proofs to be sort of small enough that it doesn't become a bottleneck to do the composition. And yeah, happy to talk more about this, but that's just my general view is kind of a natural way to design snarks, is get the proofread as fast as possible and the verifier cost, like just low enough that you can bash the proof size down further through composition.
00:28:15.790 - 00:28:40.798, Speaker C: Yeah, I was just thinking it might be useful for some applications if you could prove things about the number of accesses to an element. So for example, if that's just zero or one, then you're proving that the input vector is a permutation of a subset of the table, and that could be a directly useful thing to prove.
00:28:40.974 - 00:29:26.978, Speaker B: Yeah, that's interesting. I mean, the lookup argument under the hood does use permutation check anyway. The main stuff the prover commits to. Okay, in the sqls one case where there's no decomposing big table, on the small table, the prover is just committing to access counts. There is this nuisance that, let's say a cell is accessed five times. So the proverb is going to commit to the numbers like 01234, and it should be possible to only commit to four. And we don't know how to do that yet.
00:29:26.978 - 00:30:13.070, Speaker B: And there's some follow up work I'm going to mention, which is, I believe, going online tomorrow by others that does achieve exactly that by building on the sort of the key ideas I'm about to describe to you using logarithmic derivatives, but it's sort of naturally an unindexed lookup argument, and I don't think these transformations to index lookup arguments would play nicely enough to build jolt out of it. But if you could, and I could easily be wrong about that, I'd love to be wrong. I'd say you should potentially just swap out the c equals one lasso with the logarithmic derivatives one. But keep using how we do the decompositions of the big tables into small tables.
00:30:13.570 - 00:30:15.406, Speaker C: More engineering to be done.
00:30:15.588 - 00:30:24.050, Speaker B: Yes. Cool. Thanks for the questions. Anything else?
00:30:24.120 - 00:30:24.740, Speaker C: Now.
00:30:29.960 - 00:30:33.252, Speaker A: Feel free to continue. No questions in the chat. I'll read them if they come up.
00:30:33.386 - 00:30:51.544, Speaker B: Okay, thanks. Okay, so let me say a little bit more about how these decompositions are being done, although I think I'm not going to go into great detail. So maybe I've already said this. Let me just. Yeah, I think I basically already said this. So. Yeah, we actually give two variants of lasso for big tables.
00:30:51.544 - 00:31:29.720, Speaker B: Okay. One applies to more general class of tables, but is more expensive in the sense that about a third of the committed field elements are random instead of small. And we don't actually know any example of a table where you need the generality of that one. So let's just ignore it from now on. So, yeah, the kind of structure used by the more efficient variance is what we call decomposability. This basically means conceptually what we've already discussed. One lookup into the big table can be answered by doing, say, c lookups into one or more tables of size n to the one over c, where n is the big table size.
00:31:29.720 - 00:31:37.980, Speaker B: And we'll see examples of this where I will cover examples of decompositions in a bit, where this decomposition thing is very natural.
00:31:38.320 - 00:31:49.410, Speaker C: I have a question here. Will the proof size depend on the amount of different tables you're using, or the amount of tables you are using for the compose the big one?
00:31:50.260 - 00:32:19.848, Speaker B: So, proof size, not really verifier time. The verifier is going to have to evaluate the multilinear extension polynomial of each little table at a random point. And so more small tables increases that time. But it's just field work. I don't think it should be a bottleneck compared to other cryptographic operations. The verifier is ultimately going to have to do great.
00:32:20.014 - 00:32:20.730, Speaker C: Thanks.
00:32:24.960 - 00:32:54.310, Speaker B: Yeah. The proof size grows a little bit. Well, it depends how much you're, like, batching things. The proverb commits to certain polynomials for each small table, but there are ways to commit to many polynomials by kind of gluing them together into one, and you can batch the evaluation proofs together and things like that. There are these costs that grow with the number of subtables, but I don't think it's bad for anyone, really. I don't think it really matters that much. Other questions.
00:32:54.310 - 00:33:07.530, Speaker B: In jolt, I think there's a total of something like 45 different subtables, roughly, if that's helpful. Okay.
00:33:08.780 - 00:33:15.128, Speaker C: I'm just thinking about how expensive the verifier circuit is if you need to do recursive.
00:33:15.304 - 00:33:54.650, Speaker B: Yeah, well, okay, so you don't have to recurse on that part of the verifier. Right. The parts of the verifier we would run a recurse on are, like, if you don't like the log size, some check proofs. That's field work and hashes. But Fry is, like, cubically more of those. I think the big thing is, if you want to use a fast proof or polynomial commitment scheme where the evaluation proofs are bigger than you want to post on chain or something, I think that's really what you'd want to recurse on. And everything else, I think is cheap enough, you probably wouldn't mind not recursing on it.
00:33:54.650 - 00:34:23.920, Speaker B: It depends. Yeah. Okay. Yeah. So, I guess the way we take the results of the small table lookups and turn them into the results of the big table lookup is with the sumcheck protocol. So we use the sumcheck protocol, like, in many different places in both lasso and jolt. And that basically makes the combining free for the prover, there's just, like, no more commitment costs.
00:34:23.920 - 00:35:02.910, Speaker B: And this is sort of where, if the result of the small table lookup is like an index and an actual value sort of packed together, you run up into issues. So, if the way you're going to glue the small table indices into a big table index matches the way you want to glue the values together, you kind of have no problem. And that actually is the case for some tables in jolt, but for others, it's not. Right. Like, the way you want to glue indices together. The indices are like a limb decomposition of the big table index. So you want to take a weighted sum of the limbs to get the big thing.
00:35:02.910 - 00:36:08.664, Speaker B: But we often need to glue small table values together, not through weighted sums. And that's, like, my main sticking point, I think, right now, in terms of trying to use unindexed lookup arguments that are not naturally indexed to give something that would let us build jolt from it. Okay, let's see. Okay, so let me now just say what are the key ideas in lasso. Lasso is arguably completely implicit already in a result in spartan. So mainly we're just making observations that clarify benefits of these techniques that I guess, exactly this extent of the benefits weren't previously clear. Okay, so like all known lookup arguments and a lot of snarks that are not lookup arguments, use something called a grand product argument.
00:36:08.664 - 00:36:49.176, Speaker B: And this is just a way to multiply together a bunch of committed values. And if you use a snark that under the hood is using univariate polynomials. As far as I know, the only grand product arguments. Firstly, they kind of have two issues. The main one is the proverb winds up committing to partial products. And there are some other issues I think you can attribute. Like pluckup has a cost that grows with the maximum of the number of lookups in the table size, and I think you can kind of attribute where that maximum is coming from to using kind of univary polynomials under the hood.
00:36:49.176 - 00:37:40.890, Speaker B: I'm not 100% sure of that, so that's another nuisance, and this is not necessary. So there are lookup, sorry, grand product arguments based on the sumcheck protocol that just don't have any more commitment costs for the prover. So I gave one in 2013. It was just this interactive proof called the GKR protocol, sort of optimized for a circuit that's just like a binary tree of multiplication gates. Now, the proof size there is like log squared in the number of things you're multiplying, which again, is better than fry, which a lot of people are using today already. But if that is too much for you, steady. And Lee gave a nice way to sort of reduce that verifier cost from log squared down to something very close to log with like a small increase in proverb commitment cost.
00:37:40.890 - 00:38:17.928, Speaker B: Yeah. So very high level. And I'm not planning on going into more detail than this, but you can basically take some existing lookup arguments and just swap out the grand product argument to use either this GKR based one or the variant from SETI and Lee. And maybe the key observation is the only reason the prover in the current lookup arguments is committing to random field elements is because these partial products involve randomness. Yeah. So you just get rid of that.
00:38:18.094 - 00:38:21.160, Speaker C: Did I get the right link for the paper? T 13.
00:38:22.540 - 00:38:24.552, Speaker B: Sorry, let me check.
00:38:24.686 - 00:38:26.632, Speaker C: Time optimal interactive proof per second.
00:38:26.686 - 00:39:02.100, Speaker B: That's it? That's it. Okay, cool. Exactly. And if you don't care about the very precise optimizations, you're happy with a heavier hammer and extra log factors or something. You could have invoked prior works that still have this qualitative aspect that there's no commitments happening in the grand product argument. Yeah. But the 2013 paper sort of made sure that the proverbs field work is itself linear in the number of things being multiplied.
00:39:02.100 - 00:39:30.856, Speaker B: Okay. Yeah. And I mentioned there's nice upcoming work. I'm mentioning it because I think it should be online tomorrow. I didn't check with the authors if it was okay. So I hope it's okay. Kind of applying similar sorts of insights to the logarithmic derivatives and to get, I think, actually a better unindexed lookup argument than basic lasso, but I don't currently see how to slot it into something like jolt that needs a lookup Argument.
00:39:30.856 - 00:39:53.588, Speaker B: It'd be awesome if I'm wrong about that. Okay. So, yeah, I guess as a higher level takeaway, these techniques have been around. Right. So it's possible the community is just not quite fully internalized, like the power of the sumcheck protocol, which is underlying all of this, to minimize commitment costs for the prover. Yeah. Okay.
00:39:53.588 - 00:40:24.400, Speaker B: So if you want to see details of how basic lasso works, I did cover that in my second a 16 z talk, so I won't focus on that here. I also don't really think it's that important. And then I'm going to say more at the end of this presentation about how I suggest thinking about lasso as a hammer. So I'm not sort of done explaining lasso in some ways. Yeah. Just as a heads up, I'll say more about this. I'm not totally done with Lasso.
00:40:24.400 - 00:40:28.320, Speaker B: Any questions before jolt?
00:40:31.280 - 00:40:37.168, Speaker C: Can you just go back to that last slide so I can notice? That's okay.
00:40:37.254 - 00:40:45.216, Speaker B: Thanks. Okay. Yeah. So let's talk about jolt. Jolt is a.
00:40:45.398 - 00:40:47.270, Speaker D: Sorry. Quick question.
00:40:50.280 - 00:40:54.512, Speaker B: Can you do it with a multiplication.
00:40:54.576 - 00:40:57.972, Speaker A: Gate with fan in, like, d instead.
00:40:58.026 - 00:41:00.936, Speaker D: Of two, and then maybe get the.
00:41:00.958 - 00:41:35.004, Speaker B: Log squared in a bigger base than two? Yeah, great question. So you can. There's a trade off there where I don't believe I should think about that more. You have fewer layers, so within each layer. So I'll probably think offline how to optimize that. It's a great question. The GKR protocol goes layer by layer through a circuit.
00:41:35.004 - 00:42:15.516, Speaker B: Okay. So if you make the circuit shorter but the fan in more, there are fewer layers, but the cost within each layer goes up, and the cost will actually grow up within each layer. The field elements will go up linearly with the fan in the rounds. Actually should not really increase per layer. So I think what's happening, here's my guess. My guess is the number of field elements in the proof will actually go above log squared, but the rounds will go way down. And for composition, those rounds turn into hashes, which is the bottleneck.
00:42:15.516 - 00:42:46.360, Speaker B: So that could be a good way to do things potentially, if you're looking into composition, if you just care about proof size, I'm not sure if it'll help. My guess is it won't just because it'll make the number of field elements potentially go up. But yeah, the number of layers going down, that's actually good for proof size. But within each layer, the number of field elements is going up. That's bad for proof size. I have to have to think about it more. Yeah, I was mainly thinking of the number of hashes and recursive composition.
00:42:46.360 - 00:43:19.860, Speaker B: Yeah, it would definitely help with that. And then another thing you could help with is within each layer, rather than working over the Boolean hypercube zero one to the log layer size, you could work over like 01234. And that would have the number of rounds, it would increase number of field elements sent in each round. But that's just more field ops for the verifier, not hashes. So that's another toggle you could turn if you're looking to minimize hashes for composition reasons.
00:43:21.960 - 00:43:24.420, Speaker D: Yeah, okay, thanks.
00:43:24.570 - 00:43:54.180, Speaker B: Yeah, thanks for the question. Other questions. Cool. Okay. Yeah. So just a quick sort of recap of zkvms, I guess. So the setting here is, you say the proverbs claiming to have run a computer program for some number of steps, m, deliberately, I'm choosing m the same as the same letter that I used to denote number of lookups before, because there's going to be one lookup per step of the computation.
00:43:54.180 - 00:44:55.480, Speaker B: So for ZKVM, the program should be expressed in the assembly language of a virtual machine. Popular virtual machines today. For ZKVMS, there's the Cairo virtual machine, there's RISC five, there's the Ethereum virtual machine, and there are others as well, Midin and so forth. And the way people deal with these today is they'll basically turn the computer program into really the virtual machine, I guess, into a circuit kind of a universal circuit or constraint system. And basically what the constraint system will do is make sure that the prover sort of, at each step of the computation, kind of correctly figures out what instruction needs to be executed at this step, what primitive instruction, and then that the prover correctly executes that instruction. So that's sort of what's happening over and over again. There's some kind of master loop that's sort of getting unrolled for t steps or m steps, where m is some predetermined bound.
00:44:55.480 - 00:45:29.664, Speaker B: And it's just repeatedly like, figure out what instruction to execute on what data execute the instruction. Okay? And what jolt does is it uses lasso to handle the second thing. Execute the instruction with just like one lookup into the entire evaluation table of that instruction. And as I said before, that means if there's like 264 bit inputs to that instruction, this table is size two to the 128. And things get a little bit annoying when you have 50 different primitive instructions. And so they each have their own evaluation table and you have to deal with those things. But here's like jolt in a picture.
00:45:29.664 - 00:46:20.980, Speaker B: I'll try to summarize, this is just I copy pasted from the paper, but I'll try to summarize how I'm thinking about things. So I'd say there are three aspects to any ZKVM, really. I sort of mentioned two of them already, which is like at each step of the computer program, you sort of have to figure out which instructions should be executed next on which inputs you have to correctly execute that instruction. And then the last thing is that first part and also where you sort of store the output. Those involve reads and writes to either random access memory and or to the registers of the cpu. So like RISC five, I think it's 32 or 64 registers. Cairo, I think there are three registers.
00:46:20.980 - 00:47:08.884, Speaker B: So there's really three components here. There's the lookup argument we're going to use to make sure that each instruction is executed correctly. There's what I call offline memory checking and offline memory checking argument to make sure that all reads and writes are done correctly. So anytime the machine does a read into the cpu's random access memory, or writes something to a register or whatever you sort of think of, there would be two separate memories, like the 64 registers or whatever. And however big random access memory is, we need a memory checking argument to make sure the prover is returning the right value for every read, the value most recently written to that register or memory cell.
00:47:09.082 - 00:47:14.560, Speaker C: Okay, is that argument not also implementable using lookups?
00:47:14.720 - 00:47:54.372, Speaker B: Great question. So, lookups are for read only memories, and here we need read write. And actually to make the argument secure, you have to do things a little differently, a little more expensively with read write. So there is a section like an appendix in the jolt paper where I try to brain dump sort of what's known about these things. It probably didn't do a great job. Now, I'll briefly tell you the offline memory checking procedure for read write memory that we're planning on using in jolt. But there are others.
00:47:54.372 - 00:48:57.880, Speaker B: So what Lasso does, or basic lasso, is any read to the read only memory gets paired with account. So the prover also returns account, which, if the prover is honest, is just an access count. How many times was the cell accessed? And anytime there's a read, and the proverb return says, hey, here's the value that lives in the cell you asked to read, and here's the count that lived that was associated with that cell. The verifier will immediately write back to the same cell, the return value, and the same count, but incremented by one, which ensures that if the prover was honest, the count continues to be just an access count. And then you wind up showing like, hey, if the prover, you sort of have a read set and a write set. The read set is like all of the values and counts returned by the reads. The write set is the values and counts written, and you show the permutations of each other if and only if the prover is honest.
00:48:57.880 - 00:50:04.188, Speaker B: Okay, so you just do a permutation check. So by only incrementing the count by one, that's a really nice, simple update to the count. That does not prevent a cheating prover from doing something like answering a read at time I with some value that will be written to the same cell sometime in the future. Okay, so it can kind of reorder, and an adversary can successfully reorder the values, return a value that will only be written in the future, and then in the future, return a value that was written in the past. So to prevent that, you have to change the count update procedure. And what we do, we take something out of a paper called spice that replaces plus one with a certain max operation. So there's like, kind of a global max that the circuit is sort of tracking at all times, and the count after each read operation gets updated to the max of the return count and that global count plus one.
00:50:04.188 - 00:50:12.556, Speaker B: And that prevents this sort of like, going back in time attack. This might have been this paper proving.
00:50:12.588 - 00:50:15.090, Speaker C: The correct execution of concurrent services in.
00:50:17.220 - 00:51:08.050, Speaker B: So we use that spice like count update procedure. That's what we plan to use and how we did all our calculations. But to make this max operation less painful, what do we do? We use lasso. Inside it, there are other approaches, and it's not 100% clear to me that actually the spice thing is the best, but it's certainly among them. One nice thing about both basic lasso, which is really in spark, and about spice is there's no grouping or resorting of the operations. It's just like the prover is sort of on the fly committing to these values and counts, and it doesn't have to sort by memory cell. And some of the other techniques do have these sort of the proverbial groups by memory cell and stuff.
00:51:08.050 - 00:51:12.320, Speaker B: Go ahead. Sorry.
00:51:12.390 - 00:51:32.010, Speaker D: Yeah, before, you were talking about the different ways in which you can look at the lookup protocol, and you told us about the indexed way and the unused way, and now you're introducing the memory checking. And so the first question is, how do you see memory checking with respect to the indexed perspective that you gave us earlier? Do you think there is a difference?
00:51:33.340 - 00:51:52.030, Speaker B: Yeah. So memory checking for a read only memory is completely equivalent to indexed lookup arguments. But if it's a read write memory, that's a harder problem. So that's sort of like indexed lookups into a table that you can overwrite cells of.
00:51:52.800 - 00:52:05.456, Speaker D: Okay, another quick question, in part. And there was already the offline memory checking idea, but that was already taking this linear combination of the various indices for the read and write tapes. So you're not doing that anymore.
00:52:05.568 - 00:52:56.016, Speaker B: Okay. What's happening in Spartan's memory checking stuff? It sort of applies it specifically to get a sparse polynomial commitment scheme. Okay. It is just doing like, so sparse polynomial evaluation. This algorithm I mentioned earlier, where the evaluation proofs, or the prover proves it correctly, ran a certain algorithm to evaluate the committed polynomial at the requested point. All that algorithm does is it literally initializes these small tables of size n to the one over c. And then for each non zero coefficient of the sparse polynomial that's been committed, it does one lookup into each of the small tables and multiplies the results together to get the result of the big table lookup.
00:52:56.016 - 00:53:31.500, Speaker B: And those tables are filled with random field elements no matter what. Right. The tables themselves, in this context, are filled with these big field elements. So all Spartan is doing is, it's applying lasso, literally. Like, lasso is what spartan or spark. That's what spark calls as far as polynomial commitment scheme. But spark is a very specific application to a very specific algorithm and lookup table.
00:53:31.500 - 00:53:49.620, Speaker B: And so all we're doing in Lasso is sort of pointing out that the situation is more general and it applies beyond sparse polynomial commitment schemes. And when you do apply it beyond sparse polynomial commitment schemes, the small tables can have very small entries instead of like random field elements. Was that helpful?
00:53:50.520 - 00:53:56.260, Speaker D: That answered the question, yes, but I want to know more, so I'll leave you talking.
00:53:56.330 - 00:53:57.510, Speaker B: I'll leave you to the.
00:54:00.220 - 00:54:01.192, Speaker A: You got it?
00:54:01.326 - 00:54:42.052, Speaker C: Yeah, I have a question. So, there's a thing that is intermediate in expressiveness between a lookup in a read only memory and memory checking, which is looking up in a dynamically witnessed table. If you use a halo like lookup argument, then it just naturally supports that there are some details, but it doesn't become significantly more complicated. Does that also apply to Lasso? Just consider basic lasso for this.
00:54:42.186 - 00:55:06.360, Speaker B: Yeah. Okay. I might need to know a little bit more about, I guess, what you mean by dynamically witnessed table. In spark the sparse polynomial commitment scheme. The relevant tables are not known ahead of time. They are dynamically determined. They're determined by the point at which the verifier wants to evaluate the committed sparse polynomial.
00:55:06.360 - 00:55:14.352, Speaker B: But once you determine that evaluation point, it's like the tables are determined, and so they're read only, like from that point. So they're sort of dynamically determined, read only.
00:55:14.406 - 00:55:23.160, Speaker C: Yeah, I think that is the same thing. Okay, so, yes, you could use dynamic tables.
00:55:23.340 - 00:55:24.070, Speaker B: Yes.
00:55:25.160 - 00:55:38.324, Speaker C: As the basic tables, and then obviously not directly for the generalized lasso. Yeah, but the big table is constructed from the dynamic tables.
00:55:38.372 - 00:55:40.410, Speaker B: Exactly, that's right.
00:55:41.500 - 00:55:42.250, Speaker C: Interesting.
00:55:44.780 - 00:56:07.532, Speaker B: Cool. Yeah. Okay, so I guess back to the picture. The VM, sort of at each step, figures out what instruction needs to be executed on what inputs, executes the instruction, and saves the results. Somewhere, right. And somewhere in there is getting the input, saving the results. There might be reads from memory.
00:56:07.532 - 00:57:21.556, Speaker B: So, yeah, we use the lasso lookup argument to make sure each instruction gets executed correctly. Once the inputs to the instruction are determined, we use offline memory checking to make sure any reads and writes to registers or memory cells are dealt with appropriately. And then the R one Cs is sort of capturing anything else, but what those things tend to be is how you determine what inputs and what instruction is getting applied to those inputs at each step. So, for example, when we sort of save things into memory or register, we, to the extent possible, want it packed all into one field element. Because the costs of the memory checking procedure grow with the number of field elements we're sort of reading. But then to easily sort of figure out what instruction to execute on which inputs, we then need to kind of unpack like an opcode into its individual bits or things like that. And so the R one Cs is capturing constraints like make sure the unpacking is done correctly, and also it captures within the lookup arguments.
00:57:21.556 - 00:57:59.060, Speaker B: We need to make sure, when we decompose an index into the big table, into limbs, we need to make sure those limbs are really a decomposition of the big. There's no range checks, but of the big table index. So the R one Cs is just capturing all those constraints. Okay, now, in terms of the prover costs, sort of. Surprisingly to me at this stage, all three are about equal. So that does mean, I think that's a little bit bad because it means if you did want to continue to improve things further, you have to sort of improve all three. I'm surprised by this.
00:57:59.060 - 00:58:31.180, Speaker B: I think what's really going on, it's just a little more annoying to do, like offline memory checking is a little more expensive than I would like. Sort of unpacking of field elements into constituent pieces that we need is more expensive than I would like. And so I would have thought that this lookup into a giant table would be the bottleneck. And I guess we've seemed to made it efficient enough that it's not. It's about equal to the other two costs.
00:58:33.460 - 00:58:41.650, Speaker C: Does that depend on the kind of program? Because some programs might do more lookups than others.
00:58:42.260 - 00:59:23.484, Speaker B: Yeah, it does. So there's this interesting thing where I think there are 45, 50 primitive instructions in RISC five. Now, some of them do no lookups. Some of them you only have to do a lookup into like a, you know, table of size, like two to the 65. Most of them it's size two to 128. And then a couple are even kind of two x more expensive than that. And we really only have to pay for the instructions that are actually executed, essentially.
00:59:23.484 - 01:00:00.360, Speaker B: There are some details there. Some programs will be cheaper than others just based on what instructions get executed. And then another caveat, sort of in the reverse direction here. I'm saying, like, some things are sort of more efficient than what we kind of accounted for in the paper is some instructions we sort of have to turn into multiple invocations of another instruction. But this is just like to do division. We have the prover provide like the quotient and remainder kind of situation. That answered your question roughly.
01:00:00.360 - 01:00:01.070, Speaker B: Yes.
01:00:03.280 - 01:00:11.284, Speaker C: It will be interesting to compare the instruction costs in this approach with the native instruction costs.
01:00:11.432 - 01:00:47.960, Speaker B: Yeah, and there's this funny thing where each real lookup into a subtable is sort of cost the same. It doesn't really matter what is in the subtable, because all the subtables are small and stuff like that. So the only place where you get sort of program dependent stuff is just because different instructions involve different number of subtable lookups. Anywhere from zero to twelve actually is the biggest twelve. If we're setting c to be six, which. Yeah, anyway, we plan to set c to be six for 64 bit data types.
01:00:49.740 - 01:00:56.572, Speaker C: Okay, I was going to ask what the useful range of c was. Okay, that's interesting.
01:00:56.706 - 01:01:20.740, Speaker B: Yeah. So I think for 32 bit data types, probably want to set c to be about three. And that's just because your tables will size like two to the 64 and you divide that by three. And now it's like pretty close to two to 20 and like msms of size two to the 20, no problem. And if you square the table size, you double c roughly, because it's table size to the one over c. Got it. Cool.
01:01:20.740 - 01:01:22.150, Speaker B: Okay, other questions.
01:01:25.160 - 01:02:03.932, Speaker D: I have another one, and it's going to be really stupid. So we worry so much about the tables, the size of the elements in our lookup. But at the end of the day, we're going to have to do a sum check of the same size as the tables that we're having to look into. Right. At least the number of elements during sum check, all these elements could be randomized because sum check, at least from the second round, I will have all random elements. What am I missing here?
01:02:04.086 - 01:03:04.900, Speaker B: Yeah, the key point, and this is exactly the power of sumcheck, is the prover is not cryptographically committing to those elements. It's just field work. Right. Now, I will say if you do really want to work over like a small field, an extension field of a small base field, you sort of raise an important issue, which is if you're naive about, if you sort of take the linear time sum check provers today, from the second round onward, all arithmetic is happening over the extension field, if your random elements are chosen from the extension field. So I have some work in progress that can improve over that. And so when you want to work over these small fields, the field work can become a bottleneck. When you work over big fields, I think it just won't be the stuff over small fields.
01:03:04.900 - 01:03:33.150, Speaker B: It's like risk zero is working over baby bear now. So for each committed field element, you can actually pack like eight committed field elements into one. They're using hashing based stuff, right? So the small field is saving you cryptographic costs, right? Because rather than if you use a 251 bit field, if your hash function takes 2256 bit inputs, you can only hash like two field elements at a time.
01:03:34.800 - 01:03:41.388, Speaker D: When you pack elements like that then you need to do at least an inner product in order to show that the packing has been done correctly.
01:03:41.484 - 01:03:59.076, Speaker B: I don't think the packing is being done algebraically. It's just. Well, it is if you like recurse. Yeah. Without recursion, you just do the packing, and the verifier itself does the unpacking. And it doesn't have to be algebraic in nature. If you recurse, then you have to express the verifier as a circuit.
01:03:59.076 - 01:04:39.700, Speaker B: And now the verifier does have to be algebraic. Anyway, maybe I shouldn't have gone into this because the discussion gets complicated. But, yeah, maybe I'll just take a step back and say you're right that the sum check prover needs to compute over random field elements from round two onward. But it's only field work. I think you do want to pay attention to that field work if the field is really small. If not, it really should not be a bottleneck. And even in that case, I know that there's some interesting stuff you can do that hopefully continues to ensure that the field work is not a bottleneck for the proverb.
01:04:40.680 - 01:04:41.910, Speaker D: Okay, thank you.
01:04:42.440 - 01:04:47.960, Speaker C: You mentioned the baby bear field I just looked up. It's just under 31 bits.
01:04:48.540 - 01:05:31.350, Speaker B: Yeah, exactly. So up to issues with. Do you have to pack and unpack in an algebraic way? You can pack like eight field elements into 256 bits. Cool. Okay, I've told you what Jolt is now, and just some brief context. We sort of were pitching jolt. We think it is a realization of Barry Whitehat's kind of lookup singularity vision, because most of the serious work happening is happening in the lookup argument, even though the prover costs actually are sort of split evenly between sort of three components of the system.
01:05:31.350 - 01:07:03.730, Speaker B: And the hope is that this is sort of, let's say it turns the auditing procedure into the following. Make sure that your lookup argument is correct, your offline memory checking procedure is correct, and, yeah, I guess that's a big piece of it. And then once all of that's correct, if you want to change your primitive instruction set for your vm, all the developer has to do is specify the subtables and how to combine the results of the subtable lookups thereafter. So this feels, I think, like a qualitatively different way of building zkvms. It feels like it cuts out the need to hand design optimized circuit like anytime you change the primitive instruction set and things like that, as I've already acknowledged, it is pretty similar to things people are already doing so, I think the key differences here are coming from the lookup argument. So firstly, it's just much lower commitment cost for the prover, so you can just kind of use it more freely at less expense. It's naturally indexed, so I've sort of highlighted this struggle to take unindexed lookup arguments and you can turn them into index, but does it preserve the properties we're really exploiting in lasso and jolt? It's not clear to me that it does.
01:07:03.730 - 01:07:52.960, Speaker B: I think it doesn't actually. And then since our collation technique, turning the small table results into big table results, uses sumcheck, it sort of comes at no additional cost for the prover. And it's not clear to me that non sumcheck based techniques can achieve that. And you sort of put all these together, and I think that's what's unlocking our ability to sort of do all of the risk. Five instructions as just lookups into these decomposable tables. Again, I hope it's a good thing that we're doing things closely related to what people are already doing and maybe just pushing them a little further with optimizations. The closer it is to the way people are already doing things, I hope the easier it is for people to start using it if people wind up thinking it's worthwhile.
01:07:52.960 - 01:08:00.004, Speaker B: Yeah. Let me see how much. Okay, so I have like 15 minutes or so.
01:08:00.202 - 01:08:22.270, Speaker C: Well, let me ask you a question first, because I've been thinking about how I've used this kind of multiple tables, combining the results in existing circuits. How complicated can the combining operation be?
01:08:22.800 - 01:09:00.968, Speaker B: Yeah, I'd say the main annoying ones are less than and bit shifts are surprisingly annoying. But complicated is complicated in terms of writing down the summations. In terms of performance. For the prover, it's all just a handful of additions and multiplications. Handful meaning like C equals six. Probably a total of five additions and five multiplications is the worst you're going to get. Something like that.
01:09:00.968 - 01:09:03.640, Speaker B: Is that right? The shifts, actually. The shift.
01:09:04.620 - 01:09:10.830, Speaker C: Do you mean for all of the combination functions that are used in the n risk zero?
01:09:11.200 - 01:09:29.216, Speaker B: Yeah, that's, the short version is think of it as each is like something like in the order of like five additions and five multiplications. Yeah.
01:09:29.238 - 01:09:54.600, Speaker C: Because I'm thinking of the most complicated ways of useless and probably that one would be sincere where you're doing elliptic curve operations to combine the table lookups. I'm not sure whether something that complicated is useful to consider as an instance of the same approach.
01:09:57.020 - 01:10:05.532, Speaker B: Are you thinking about like what if you did the combining without sum check, but with some other technique? Is that what you're thinking of, or are you thinking.
01:10:05.666 - 01:10:14.492, Speaker C: No, I'm asking what the limitations are of doing it using sumcheck on the complexity of combining functions.
01:10:14.636 - 01:10:56.030, Speaker B: Oh, I see. Okay, so it's all field work for the prover and the verifier. Of course, there's per round, there's going to be hashes after you fiat Shamir it, and there's going to be one round for each subtable lookup, I think. And so the hashes for the verifier are just determined by the number of subtable lookups. And then the number of field elements sent per round is just determined by the degree of the combining function.
01:10:57.520 - 01:11:04.188, Speaker C: Right. Okay, so it's like a custom constraint, effectively. Right. Ultrapunk.
01:11:04.284 - 01:11:07.520, Speaker B: Yes. But without any commitment costs.
01:11:08.340 - 01:11:08.848, Speaker C: Right.
01:11:08.934 - 01:11:09.570, Speaker B: Okay.
01:11:10.340 - 01:11:13.376, Speaker C: And then the degree of that determines the cost.
01:11:13.478 - 01:11:14.432, Speaker B: Okay. Yes.
01:11:14.486 - 01:11:15.890, Speaker C: That makes a lot of sense.
01:11:17.640 - 01:11:45.768, Speaker B: And all of our combining functions, though, are naturally sort of multilinear in the subtable results. So the degree in each variable is just maybe, is it one? It might just be two. Anyway, it seems that just degree one or two suffices for the combining functions.
01:11:45.944 - 01:11:46.670, Speaker C: Interesting.
01:11:47.440 - 01:11:49.916, Speaker B: In each variable. Yeah.
01:11:50.018 - 01:11:58.560, Speaker C: So do you count the total degree in the same way as you count them for a Planck custom constraint? So that's the maximum degree of any term.
01:11:59.140 - 01:12:11.972, Speaker B: Yeah. So, yes, sumcheck. It's the number of field elements sent in round I is the degree of the combining function in variable I or subtable result I. Oh, right.
01:12:12.026 - 01:12:13.350, Speaker C: Okay, thanks.
01:12:17.740 - 01:12:18.760, Speaker B: And Alex.
01:12:21.180 - 01:12:39.084, Speaker A: Your question, how much time do I have left? Well, technically 13 minutes left, but I mean, if you want to go a little bit longer, that's fine, but I just want to be respectful of your time, most especially, but it's up to you. We can go a little bit longer, say, 1015 minutes, if people are interested and you are interested, but it's up to you.
01:12:39.202 - 01:12:51.810, Speaker B: Okay, sounds good. I think I can get through everything I want to say anyway in about 15 minutes. I guess we'll see. Just trying to think if I reorder it.
01:12:53.700 - 01:13:03.396, Speaker A: Yeah, I'll follow your lead. Don't worry about it. Don't worry about the time. Whenever I can queue you when it's 1130, in case you have other stuff. But, yeah, I would say just great.
01:13:03.418 - 01:13:31.688, Speaker B: No, I have no hard stop. Okay. Yeah, so I thought we would. My plan for the rest is, let's see some example decompositions. And then I was just going to talk about lasso, kind of as a tool independent of jolt and how else it might be used. Okay, so, yeah, we'll start with bitwise end, which is the example we sort of give. It's the simplest, cleanest example, like in the intros to the papers.
01:13:31.688 - 01:14:16.220, Speaker B: So maybe you've seen it. But, yeah, it's worth. So, yeah. The fact that the evaluation table bitwise end is decomposable is essentially equivalent to the fact that to compute the bitwise end of, like, 264 bit inputs, you can break each of those inputs up into, say, chunks of eight bits each. You can bitwise end each chunk and just concatenate the results. Okay, so that means just, like, do an eight bit limb decomposition of each of the 264 bit inputs. Each of them turns into a lookup into the smaller table that only takes two eight bit instead of 264 bit inputs, and then concatenating the results is just like a weighted sum.
01:14:17.840 - 01:14:44.420, Speaker C: I have a question. So, if you were doing this just using an existing lookup argument, then the decomposition would come for free with the bitwise and table lookup, because your Xi and yi would be constrained to eight bits in the table. Is that also the case for lasso?
01:14:44.840 - 01:14:52.120, Speaker B: Yeah. So you're asking, like, do you have to do, like, a range check? A separate range check? Yeah. You do not need to do any range checks here.
01:14:52.270 - 01:14:53.290, Speaker C: Okay, cool.
01:14:55.260 - 01:15:23.584, Speaker B: Yeah. So the verifier at the end of lasso has to evaluate the multilinear extension of each subtable. There's only one subtable here. It's like the sort of the eight bit n subtable at a random point. And so here is that multilinear extension. So it's just, it takes 16 variables as input, and it computes this weighted sum of the variables. Each variable has total degree two.
01:15:23.584 - 01:15:37.830, Speaker B: Sorry. Each term in the sum has total degree two. So that's why no one has to commit to even the subtable. The verifier can just evaluate the multilinear extension of the subtable in, like, 25 field operations or whatever.
01:15:39.880 - 01:15:44.264, Speaker C: Do you get a cost benefit from the fact that the subtables are all the same? In this case?
01:15:44.462 - 01:15:58.172, Speaker B: Yeah. For the verifier, yeah. So if there were more subtables, each one of those would have, like, a different extension polynomial that the verifier would have to evaluate at the same random point, but different polynomial. And I don't think there's any.
01:15:58.226 - 01:16:00.588, Speaker C: But it's just one evaluation for each one.
01:16:00.674 - 01:16:08.988, Speaker B: For each one, yeah. Okay. And it's all field work. Yeah. Okay. So that is end. Okay, so here's RisC five addition.
01:16:08.988 - 01:16:48.060, Speaker B: I sort of copied this out of a Twitter thread like right before the talk, so I hope this isn't too bad. So basically you take 264 bit data types x and y. You want to add them together. If the result is bigger than two to the 64 minus one, I guess there's like an overflow bit that I believe risk five prescribes you to just ignore, like throw away the overflow bit. So essentially what we have to do is identify that overflow bit. Okay, so what we're going to do is what jolt does is it will do the addition in the field. So now we just have one field element which might be bigger than two to 64 minus one, in which case we have a problem.
01:16:48.060 - 01:17:28.600, Speaker B: We have to sort of subtract off that overflow bit times two to 64. So we need to use lookups to figure out what is the overflow bit. So all this amounts to is doing a limb decomposition of that field element. So you do sort of the prover commits to the limbs. Now we do do a range check, and the lookup is like basically a range check on the limbs. We add a constraint to the ancillary r one cs that makes sure it really is a limb decomposition of x plus y. And then there's just one more lookup where we take the highest order limb and do a lookup to just sort of pull out the high order bit.
01:17:28.600 - 01:18:06.028, Speaker B: And actually, another way you could have done it is you could have had the proofer sort of commit to that high order bit separately and then have the high order limb just range check it to be like impossible. You can even avoid it. But anyway, there are other ways you could figure out that high order bit that actually might be a little bit more efficient, but it's not going to be a big difference. Questions about this one. Okay. And then again, this lookup table, ultimately all the lookups are just into, like, they're just for range checks. And those are also like MLE structured tables, multilinear extension structures, so no one's committing to the subtables.
01:18:06.028 - 01:18:28.060, Speaker B: Okay, final one, less than. So this is actually like the most expensive table for us in jolt. And here. Yeah. Again, you just take the two inputs, x and Y. We want to know is x less than y? And again, you just sort of do a limb decomposition of them. So that is you chunk each into like eight bit limbs, say.
01:18:28.060 - 01:19:22.312, Speaker B: And now I've written this expression here for what the answer is. If you take the high order chunk and you just compare the high order chunks and the comparison goes a certain way. You know what the answer is, right? I think if you want to know if x is less than y, and in fact the high order chunk of x is greater than the higher order chunk of Y, you already know the answer is zero or something, so you don't even have to continue on to the remaining chunks. But if the higher chunks are the same, the higher chunks might make a decision. Right? If the higher chunk is strictly greater than y or strictly less than y, you already know what the answer is. You have a problem. If they're the same, then you have to move on to the next chunk and figure out, do they make a decision in the ordering? If they're the same, you have to move on to the next chunk.
01:19:22.312 - 01:19:58.410, Speaker B: And that's what this sum here is capturing. So we go like, each term of the sum is capturing. Like, which chunk are we looking at now let's see if that chunk made a decision by evaluating less than just on that chunk. And if not, we move on to the other ones and only pay attention to the other ones. If, like, all the higher order chunks were equal. You know, that's conceptually, I know this is too much to follow online, but maybe you can look at the expression offline. But that's what's going on is you're just solving less than sort of by breaking into chunks and looking at the chunks from the highest order to the lowest order ones.
01:20:00.880 - 01:20:08.190, Speaker C: There's a different way of doing it, which is to subtract the inputs and then compare the results to zero.
01:20:10.160 - 01:20:11.550, Speaker B: That's a great point.
01:20:12.880 - 01:20:16.850, Speaker C: You have to be very careful of overflow. It's more complicated than I've said it.
01:20:17.700 - 01:20:19.632, Speaker B: No, yeah, that is a great point.
01:20:19.766 - 01:20:27.750, Speaker C: I think you end up doing two range checks. So Liv snark had a comparison function that worked this way.
01:20:29.960 - 01:20:38.296, Speaker B: Yeah, now that you bring it up, I remember this, I'll think through. It's possible that because the number of.
01:20:38.318 - 01:20:53.390, Speaker C: Terms in this output is quite large, isn't it? Because, am I reading it correctly? There's a paran around the left of the two to the I minus one, and to the right of the e.
01:20:55.360 - 01:21:09.744, Speaker B: So I think you can. So there are eight terms of the sum, and although the it term involves a product of I minus. Right.
01:21:09.782 - 01:21:12.976, Speaker C: So you've got a quadratic, but you.
01:21:12.998 - 01:21:35.832, Speaker B: Don'T actually pay the quadratic because there's so much overlap between the terms. Like, you can compute the sum with, like an arithmetic circuit of size only about ten rather than eight squared over two. Just because these products are there's only one term that's different in each as you advance. I. Oh, I see.
01:21:35.886 - 01:21:39.070, Speaker C: Yeah. So this might actually be reasonably cheap already.
01:21:41.280 - 01:21:52.848, Speaker B: The thing is, all of the cost is really in the subtable lookups, and evaluating this expression is just fieldwork for the prover, so it almost doesn't matter anyway.
01:21:52.934 - 01:21:54.640, Speaker C: That's an engineering optimization.
01:21:55.300 - 01:22:32.904, Speaker B: Yeah, and it's great. My guess is, since what matters is the number of subtable lookups, my initial guess is there will be range checks coming into the subtraction approach that will wind up equalizing the cost. But I definitely should think that through. I don't think it's like bottlenecks, the proverb performance anyway. But it is a nuisance to have an instruction here that does a lookup into two different subtables per chunk instead of just one. I think it's like the only instruction that does that. So that's just like an engineering nuisance.
01:22:32.904 - 01:22:35.468, Speaker B: And if we can avoid that, that'd be nice. Yeah.
01:22:35.554 - 01:22:48.220, Speaker C: So if I remember correctly, you reduce a comparison on k bits to basically two range checks on k bits.
01:22:50.340 - 01:22:55.876, Speaker B: That makes sense. And I think you're going to get the exact same cost. I'll take that.
01:22:56.058 - 01:23:12.628, Speaker C: Well, because your range check tables just have one input. They're not binary tables, so I think it does end up cheaper.
01:23:12.804 - 01:23:23.548, Speaker B: That's a good point. If this is much smaller table. Yeah. Okay. All right. Yeah. Maybe that will get rid of this annoying factor of two, actually.
01:23:23.548 - 01:23:48.580, Speaker B: So thank you. I'll follow up and let you know. That would be great. Okay, so those were the examples I guess I wanted to cover. This involves this equality subtable and this less than subtable, and like two eight bit inputs. And I just wrote out what their multilingual extensions are. And again, it's like not many field operations to evaluate them at a random point, so no one has to commit to those subtables.
01:23:48.580 - 01:25:20.428, Speaker B: Okay. So the last thing I wanted to discuss was, like lasso, I think you should think of it as a tool, which is at least if you want to avoid anybody committing to the subtables, which it's not the end of the world to have someone commit to them since they're all small. But I think it's nice to not have to have that. Then I think intuitively, what these techniques support is doing simple operations on the bit representations of field elements, but without having the prover commit to individual bits, but rather mainly committing to these limb decompositions. The reason this is sort of simple functions on the bits that are sort of captured by lasso is because these multilinear extension polynomials of these subtables are quickly evaluatable, like roughly when they're simple functions of the bits of the indices. Yeah, so one thing to clarify here is lookups, sort of, in my view, are like all about economies of scale, and there are some exceptions with, like, I'll actually say a little bit more about that in a second. But if you're just doing like three lookups into some table, at least with Lasso, there's some cost that depends on the table size that you're not going to amortize effectively over that few lookups.
01:25:20.428 - 01:26:22.800, Speaker B: So you really want to be doing a lot of lookups before it makes any sense. With something like CQ, you have this nice property where you sort of deal with the table once in preprocessing, and then you can sort of do like three lookups to the table in this snark and three lookups to the table in another snark, and you're never having to pay that cost, that dependent on the table size again after the preprocessing phase. So that is maybe the main exception. But yeah, so I think in general, if you are repeatedly doing simple operations on the bit representations of field elements, that's kind of when you wouldn't want to use Lasso. And it is more general than that if you're willing to have an honest party commit to some things in preprocessing. Okay, I promised at the start that I would discuss a couple other ways to view lookup arguments. My favorite way of viewing them now is as snarks for, I call data parallel computation.
01:26:22.800 - 01:27:42.240, Speaker B: It's probably easiest to just say repeated function evaluation. But here, if you think of the lookup table as jolt does, as just storing the entire evaluation table of some function, then what a lookup argument does is let the prover commit to a bunch of claimed inputs and claimed outputs of the function and prove that the outputs are correct. So literally, it's just like a rephrasing of a lookup argument is just a snark for repeated function evaluation. Okay? And then in settings where you really want a two to depend on f of a one or something, you'd have like ancillary r one cs to confirm that a two indeed is derived from r one cs from a one in the right way or whatever. And so from this perspective, lookup arguments are just like really efficient snarks for computations with repeated structure and with lasso, because there's a cost that grows like table size to the one overseas, you wind up in the situation that people don't normally think about. I think in most people's heads when they think about snarks for repeated function evaluation. So normally you think, well, if your function is on n bits, then maybe you're going to evaluate it like poly n many times or something.
01:27:42.240 - 01:28:57.450, Speaker B: And that's like many repeated evaluations of the function here. But for that end of the one over c cost and lasso to be not ridiculous, you really need the number of evaluations here to be like kind of weekly exponential in the size of the input to f, right? Okay, so that two to the size of that input, that's n. And so you need that to the one over c, where c is like six or three or what have you to be a reasonable cost for the prover to pay. And so, yeah, that means you really need the number of evaluations of f to be at least like two to the input size, to the one over c, to the one 6th or something. However, with virtual machine execution, that's exactly the setting you're in, because these primitive instructions only operate on like 64 bit inputs or 128 bit inputs. But you run the machine for a billion steps, a trillion steps, and it really is reasonable. You basically should think of the input size to f as logarithmic in the real input size, and the number of valuations of f as exponential in that, which is now only polynomial in the real input size.
01:28:57.450 - 01:30:09.816, Speaker B: Yeah, so you could just call lookup arguments like very effective snarks for repeated function evaluation. And so I think Lasso will be useful whenever you have repeated function evaluation. And I'm just going to mention, I think this slide is horrible, I threw together right before the talk. I think there are other ways to identify repeated structure in computations other than going through virtual machine abstractions, just as one direction, which I'm working on with my intern from the summer, Yanuo, and another student, Sriram. There's actually an approach from the hardware community called bit slicing, which is trying to take advantage of the fact that real computers have 64 bit data types and sort of operating on all 64 bits is like the same cost as operating on one bit. So what this means is for a function that's sort of naturally captured by a Boolean circuit, and I think like AES and Sha should qualify. And that's sort of the point of the project, is to kind of make sure this, what you could do is to evaluate on many different inputs like you would for several Merkle authentication paths.
01:30:09.816 - 01:30:57.630, Speaker B: If that's the hash function you used you would take the first bit of each of those inputs and say there are 64 of them, and pack them into one field element. The second bit of each of those, say, 64 inputs, and pack it into another field element. And then you take the boolean circuit for one copy of f and replace each two bit end with an end on 264 bit inputs. And now each output gate, rather than computing one bit of one output, is computing like one bit of all 64 outputs. And then, of course, you'd have to do the sort of unpacking and repacking to get your bits altogether. So this is just another way to sort of leverage data parallelism that Lasso is very well tailored for.
01:30:59.600 - 01:31:11.228, Speaker C: The advantage is that cryptographic engineers are already used to doing this because it is already the best way to optimize cryptographic algorithms on conventional architectures.
01:31:11.324 - 01:32:04.476, Speaker B: Yeah. To me, I continue to be surprised. Others seem to not be, by the extent to which the parallels go between kind of how people design real hardware and how we can design snarks, and you can't push them too far. A lookup argument is not literally just doing a read into memory, literally. But, yeah, this seems like another example where, for slightly different reasons, designing a snark might make sense to use the same exact technique that people are using today outside of snarks. Okay, so I think I'll stop here. But, yeah, in general, I think these lookup arguments are just a tool for any computations with repeated structure.
01:32:04.476 - 01:32:17.156, Speaker B: It does not have to be virtual machine abstraction. So jolt, I hope, is just sort of the first application of these things. Yeah, thanks for all your attention for this talk. Yeah, thanks for having me.
01:32:17.258 - 01:32:51.168, Speaker A: Yeah, thanks very much, Justin. I have a quick maybe question, and then maybe others have questions, but I think one way to think about this is you're taking advantage of kind of the structured nature of these bitwise computations specifically to construct a table. And a question that actually, this is pertutious question he asked me earlier about when we were discussing this work is, if you have to go to bitwise representation anyway to do the table lookup, is there actual savings in real world terms? So I guess that would maybe be a question for you. Like, how would you answer that?
01:32:51.254 - 01:32:57.968, Speaker B: So the question is, we're comparing savings relative to just having the proverb commit to the bit decompositions.
01:32:58.064 - 01:32:58.852, Speaker A: That's right.
01:32:58.986 - 01:33:41.804, Speaker B: Yes. It's a big savings. It's at least an order of magnitude. And it's because if you're using MSM based commitments anyway, and I think there are savings in other contexts, too. But Pippinger's algorithm conceptually lets you commit to, let's say a 22 bit field element, meaning big field, but element is between zero and two to 22 or something with one group operation, which is the same exact cost that it would take to commit to a single bit. Does that make sense? So it really makes sense to try to pack, to the extent possible, at least like 20 to 30 bits into each field element. And when you do that, you commit to fewer field elements, like at the same cost per field element.
01:33:41.804 - 01:33:57.590, Speaker B: And then there are ancillary costs, depending on your setting or snark, that also will go down because you're committing to fewer field elements. So short version is. Yeah, I see a 20 to 30 x speed up from doing it this way. Cool.
01:33:58.700 - 01:34:06.760, Speaker A: Does anyone else have any questions? I know a couple of people dropped it at the half hour mark, but anyone else in the group have questions for Justin?
01:34:09.020 - 01:34:35.316, Speaker D: Yeah, I wanted to understand a bit better this idea of rain checks and these packing ideas. So the idea is that at the end of the day, you're sending bit commitments to all of your values, and then you check the range on those, and then you group them together in order to perform algebraic operations over larger sequences of bits. I don't understand how these packing and range checks ideas work together.
01:34:35.498 - 01:35:33.750, Speaker B: So I think the way to think of it is we never decompose the elements to be range checked into smaller data types than limbs, and the relevant limbs are between 16 bits. And it's however big you're willing to do. Kind of an MSM. Like you go up to maybe 30 od bits to confirm that some field element is between zero and two to the 128. Maybe you decompose it into, I say, 32 bit limbs, if you're willing to do size two to 32 msms. So what is that? That's four of them or something. So four limbs, and you range check each limb, make sure it's between zero and two to 32, with an unindexed lookup argument into the table that just stores the field element zero up to two to 32 minus one, and that's your range check.
01:35:33.750 - 01:35:40.970, Speaker B: And you're never going to decompose the limbs any further into their bits or anything like that.
01:35:42.460 - 01:35:54.940, Speaker D: If at the end of the day, all I do with limbs, maybe I'm doing like, if I'm doing operations on each of the limbs at the end of the day, then the lookup itself will be checking the range, wouldn't it? Why am I doing a range.
01:35:57.040 - 01:36:15.588, Speaker B: In, in general, this gets a little confusing. But in most applications of lasso, we don't need to do any range checks. Okay. Like, the lookup itself intrinsically checks into the subtable. Does a range check. Right. There are some contexts where we literally just want to do a range check and there's no other relevant lookups or something.
01:36:15.588 - 01:36:45.324, Speaker B: And then we will just do an actual range check with lasso. But, yeah, there are different applications of lookup arguments. Sometimes you do want to do operations on the limbs. Other times you don't really care about the limbs. I mean, with less than. Right. We break things into limbs just so that the size of the subtables is small.
01:36:45.324 - 01:37:22.170, Speaker B: We don't actually care about the limbs. We only care about the result of the less than operation. Right. And that's just like, one bit. So when the less than Operation Dyra might have us doing it a better way, but right now we're doing, like a lookup into an equality table and a less than table on each limb, and we collate the results to get our final result of the whole less than, and we just don't do anything with the limbs ever. Is that helpful? We're only going into limbs to keep the subtables small.
01:37:23.900 - 01:37:27.610, Speaker D: Yeah. It makes more sense now. Thank you. I understand better.
01:37:28.140 - 01:37:29.610, Speaker B: Yeah. Thanks for the question.
01:37:31.580 - 01:37:55.628, Speaker A: Awesome. Well, hey, Justin, thank you for being super generous with your time. This is an awesome talk. This is, I think, actually one of the best, most participation I've seen in one of these in a while. So thank you very much for joining us today. Really hoping to have you back at some point. I know you've got some other follow up work, and you mentioned some interesting future directions, which we didn't get to touch on, but especially interested to see as you guys implement.
01:37:55.628 - 01:37:59.332, Speaker A: I think you mentioned there's some work to implement Jolt. Is that right?
01:37:59.466 - 01:38:14.888, Speaker B: Ongoing, yeah. We're hoping, maybe I shouldn't say this out loud. We're hoping in just, like, a couple of months to have some working version of jolt, which should be good for benchmarking. So we'll see how things actually really.
01:38:14.974 - 01:38:31.584, Speaker A: Would be really cool to see kind of the concrete improvements that this could bring. But in general, from a theoretical level, seems like a lot of really great improvements here. So thank you very much for sharing that with all of us in this group, and thank you, everybody, for attending this week's study record.
01:38:31.782 - 01:38:33.790, Speaker B: Thanks for having me again. Thanks for the great questions.
