00:00:07.130 - 00:00:16.750, Speaker A: Next talk is by Yi, who will be telling us about how to add a reflection to the EVM using axiom. Everyone, a round of applause.
00:00:22.690 - 00:01:23.110, Speaker B: So today I wanted to talk about nuke capability that ZK actually enables for blockchain computing. And that's something I want to term reflection. So before we get into what reflection actually is in a blockchain context, I want to review a little bit how humans actually access information about the chain. So the special property of blockchains is that all execution and state, at least at present, is transparent and publicly verifiable. And so as a normal user, you would typically access a blockchain by first finding an archive node, either one that you're running yourself, or perhaps run by a service like infura or alchemy. Then you would sort of query this node for the historic on chain state, and then you would view that in your eyeball. So in my language, archive nodes really are there to feed onchain information to human eyeballs.
00:01:23.110 - 00:02:26.570, Speaker B: And as the ecosystem has developed, users demand sort of increasingly more sophisticated ways to actually understand this information. So at the most basic level, you have a variety of archive nodes that provide answers to JSON, RPC queries. You can choose geth, Aragon, Nethermind, or soon you'll probably be able to choose ret. But as users use more sophisticated on chain applications, they actually want to understand what the raw data they're querying actually means. And so for that, you would turn to an indexer like etherscan, the graph, or covalent. And we're now seeing a new wave of different ways to look at the chain that give very context specific information. For example, there are wallets like blowfish and Stello that will actually simulate a transaction you're about to run on top of the current state of the chain to see if you're actually getting hacked and going to lose all your nfts.
00:02:26.570 - 00:03:12.678, Speaker B: So the general trend here is for users to view the chain with more and more processing on top. Now let's think about the situation of a smart contract. Viewing the state of Ethereum there. It's quite different. If you go to a application like OpenSea, and you try to identify which of the pieces of data on the page are actually accessible to the OpenSea smart contract, you'll find that there's exactly one number on a listing page that's accessible, namely the current owner of the relevant NFT. And this is a more general phenomenon. So there's all this historic information about past states, transactions and receipts on the chain.
00:03:12.678 - 00:04:08.010, Speaker B: That's fundamentally unavailable to smart contracts. And that's not an arbitrary decision. There's a very good reason for this, which is that all validating full nodes need to have this type of information in random access in order to validate a transaction. So we really cannot force these nodes to have to keep historic information and still maintain the ability to run them on reasonable hardware. Secondly, even if we could access all this data, it's very difficult to process it in a blockchain VM. As every smart contract developer knows, the EVM, or actually any blockchain VM, is pretty limited. There are certain types of operations that are made artificially cheap via precompiles, but there's a whole world of other interesting operations that, for whatever reason, are not included as precompiles and become pretty difficult to use in your application.
00:04:08.010 - 00:05:12.878, Speaker B: Again, this isn't just arbitrary choice, this is to maintain the consensus stability of Ethereum or in principle, any other blockchain. So, situation looks pretty grim for blockchain computers. They're pretty limited on historic access and also compute, but they actually have a special weapon to combat this. So unlike most other types of computing, blockchains have commitments from the current block to all previous blocks, and thereby all information contained in all previous blocks. So if you have access to a current or recent block header of Ethereum, in principle you have some type of cryptographic access to the entire history of Ethereum. And so, to sum this up, blockchains have this special property of cryptographic memory. And what does this mean? Well, it means that we can actually allow blockchains to reflect by accessing this memory.
00:05:12.878 - 00:06:23.210, Speaker B: In Ethereum, this would work by first taking the current block, decommitting a pretty long chain of blocks back to a past block containing the information that you want to access, and then providing some merkle persuasion fry proofs to access a specific piece of information about state transactions or receipts. And I like to think of this as kind of a baby version of reflection, because it's quite expensive to access each piece of information. Maybe if you want to find data about one storage slot in the past, that's okay, but if you want to scale this, it's going to be prohibitively expensive. So what we're doing at axiom is adding reflection to the EVM at scale. Using ZK, we wrap that whole process of providing a Merkel, Patricia triproof and a ketchak chain of block headers in ZK. And from this perspective, the verification of all these proofs is compressed using the succinctness property. And that allows us to think a lot bigger and address much more powerful statements using reflection.
00:06:23.210 - 00:07:14.990, Speaker B: So we've packaged this into something we're calling a ZK coprocessor. And what we're building is something to allow smart contracts to trustlessly access and process all types of this on chain data. So let me tell you how that works from a developer perspective. When you query into axiom, we divide your query into some historic onchain reads and compute on top of that. So I already told you that the onchain reads are reaching into the committed memory of the chain. And on the compute side, we allow developers to apply different types of primitives on top of the data that they access. So this ranges from pretty pedestrian things like basic analytics to kind of wackier things like cryptography or machine learning.
00:07:14.990 - 00:07:58.344, Speaker B: And finally, in parallel, we're generating zero knowledge proofs that all of this was done in a correct fashion, and we verify those proofs on chain. The end result of this is a kind of different smart contract computing paradigm. You have a smart contract, you send us a query to a coprocessor. This picture is a picture of an actual math coprocessor from the 1990s. You'll notice it's made by Math Co. So this is kind of a throwback, but in our coprocessor we'll do the reads and the compute. And finally we put the verified result on chain for a smart contract to use.
00:07:58.344 - 00:08:51.796, Speaker B: And so we think this is a new delegated data read and compute paradigm for smart contract applications. So let me talk a little bit about how we specifically achieve this. So there's essentially two tasks. The first is to actually be able to access historic block hashes at some scale. Doing this from scratch for each query would be pretty expensive, even in ZK. So we maintain a smart contract that caches a mercalized form of all historic block hashes on Ethereum. The idea here is that the EVM, for whatever reason, allows you to access the last 256 block hashes, and we systematically reverse that ketchack header chain to cache every 1024 block hashes.
00:08:51.796 - 00:09:51.342, Speaker B: The end result is that our smart contract, which you can view on main net today, actually contains a verified Merkel root of all past block hashes from Genesis. The day we did that, we were the number two gas consumer on Ethereum, which is a rather dubious distinction I hope to not repeat. Now, when we actually fulfill a query about specific historic information into axiom, we can verify proofs against this cache of block hashes. So all we need to do for single queries is Merkel. Patricia, try proofs that are verified in our smart contract against these Merkel roots. So the resulting ZK based reflection type system differs from standard smart contract computing in some pretty critical ways. So our view is that smart contracts are generally powered by consensus.
00:09:51.342 - 00:11:01.316, Speaker B: What that means is that they're very good at providing synchronous access to a pretty limited amount of onchain state. On the other hand, they don't give you any access to historic state, and your compute is pretty carefully metered. On the other hand, if you use a zero knowledge powered reflection type system, you don't get any access to the current state, since it's a fundamentally asynchronous operation. In return, though, you can actually do unlimited compute on historic data, so long as you're willing to pay for the proving and you get async access to the entire chain history. So we think we're presenting a different type of trade off for developers here. All right, so that was a lot of very conceptual talk about new primitives. What can we actually do with this? So we think of this in stages, and I've drawn a rough plot of the amount of data read for an application on the horizontal axis, and the amount of compute that's required for an application on the vertical axis.
00:11:01.316 - 00:12:42.108, Speaker B: The first class of applications are things that are generally possible in the EVM today, whether by doing single storage proofs directly or by adding caching to your application. So this would be things like reading historic consensus level randomness from the block header, verifying your account age, or reading a historic uniswap spot price or TWAP price. If we scale up though, to things which require much more data complexity and much more compute complexity, then we get more novel applications which really are not accessible in the EVM today. This would be things like adjusting parameters of your deFi protocol by applying some basic machine learning algorithms to the historic performance, rewarding your protocol's participants by doing a trustless evaluation of their onchain track record, or actually computing an onchain credit score in a trustless fashion instead of relying on a centralized provider to put that number on chain. And we think that these applications, because the scale of the data read and the compute is so much beyond what you can think of on chain today, they're actually hard to think of. And we think as developers try to use this primitive of reflection more, they'll find new ones. One thing I want to note on this basically made up graph of my estimate of the complexity is that the data complexity of an application is actually intrinsically quite tied to its compute complexity.
00:12:42.108 - 00:13:48.700, Speaker B: Essentially, if we offer you access to a huge number of data points, then it's very hard to use that without a commiserative level of compute. Similarly, if you're going for an application that does very sophisticated compute, for example, some type of neural network inference, you very likely will have to access quite a bit of data in a trustless fashion to even get the input. So at axiom, we're going after applications in this top right corner. So if you have any ideas that aren't on this list, definitely come talk to me after the talk. Now, for the rest of this talk, I want to talk a little bit about what would be next if we finish this entire 2d plot. And what I think could be next is something that I want to call introspection. So we've talked about reflection as the blockchain, looking at itself and, well, what's introspection for a human psychology that's sort of thinking about your actual mind and thinking about what you might do in some hypothetical situations.
00:13:48.700 - 00:15:08.310, Speaker B: And so for a blockchain, I will take introspection to mean doing simulation of what hypothetical activities could happen if you were to do different transactions on chain. Okay, so this will require doing much more access as well as much more compute than anything that I've written on the slide. And in this case, we think that it will require, first of all, something like a ZkevM, and can allow application developers to make much richer statements about blockchain activities. Just to list a few examples of things that are kind of pie in the sky today, but may be possible down the road, you might be able to test whether a hypothetical property would happen to your protocol if a user were to execute some type of transaction. In the context of insurance, we think the highest possible standard for whether you should be entitled to a claim for insurance is when you try to withdraw your money from a, let's say, lending protocol. If you can prove that you are not able to withdraw, then you are certainly entitled to a claim. On the other hand, if you are able to withdraw, perhaps you shouldn't get paid.
00:15:08.310 - 00:16:45.590, Speaker B: Similarly, if you're a white hat hacker, you want to get paid a bug bounty, but maybe you don't trust the team to actually pay you. So there, we think the highest standard would be to prove that you have knowledge of exploiting transactions which would allow you to violate certain protocol level guarantees. If we go more to the infrastructure level, you'll sort of see that this proof of exploit is actually quite similar to the notion of a ZK based fraud or fault proof. In an optimistic roll up, a fraud proof is nothing more than a demonstration that the state that is arrived to on chain, or that is claimed on chain by a sequencer, is actually not the correct application of a sequence of transactions. And finally, in the domain of MeV and block building, we think that doing hypothetical execution of transactions that are being submitted in a mempool to be put in a block, could be a way to actually provide accountability or some sort of performance evaluation for block builders. For example, you might be able to prove that of all the transactions in some set, you actually built some block which is good by some type of arbitrary performance metric. So in general, I think we're very early about thinking about the implications of introspection for applications in the space, and we're pretty excited to see what developers can come up with once this sort of idea is even possible.
00:16:45.590 - 00:17:54.510, Speaker B: So I want to end by coming back to the beginning of the talk and drawing an analogy with what's happening in the more user facing side of crypto on a data access. So we started with sort of off chain access to basic primitives via an archive node. And things in that area of the space have really moved beyond that to tracing archive nodes, to full transaction simulation and all sorts of very fancy indexers that are coming to market now. And at axiom, we're bringing the very basic blocking and tackling applications of on chain reflection via coprocessing for smart contract applications. And as things develop, we're excited to explore the implications of that for onchain introspection for smart contracts. So, just to finish, we're hiring pretty aggressively. So we have a bunch of job listings for both engineering ZK, as well as developer relations and bd roles at our job site.
00:17:54.510 - 00:18:10.370, Speaker B: And we'd love to have you guys join us in building the foundations necessary for smart contract developers to build expressive and powerful trust minimized applications on chain. So I think there's quite a bit of time for questions. So yeah.
00:18:19.060 - 00:18:30.650, Speaker A: As he said, if anyone has any questions, please feel free. Oh yeah, one. Yes, she is coming to get.
00:18:37.980 - 00:18:49.950, Speaker C: Thank you for the talk. You had great slides in there. Could you say a few more words about the architecture? Because it will seem like everything happens on chain. But I guess there are things that happen off chain too, right?
00:18:53.520 - 00:19:25.560, Speaker B: Yeah, definitely. So there are two pieces to the architecture. The first is that we have to keep our cache of historic block hashes. Well, first we have to initialize it. So to do that, we provide zero knowledge proofs of the valid ketchak chain of block headers in groups of 1024. So we do that historically and then we also have to do that on an ongoing basis to maintain this cache. So actually you can look on main net today we're running a zero knowledge proof every 30 minutes to keep this updated.
00:19:25.560 - 00:20:14.600, Speaker B: So for the historic access we did some tricks to actually do a batch import in groups of about 128,000 at a time. Now the second thing is for the query fulfillment on chain. So let me find the right diagram. So what happens here is that a smart contract will make a query into our off chain system. We'll pull the data and run the compute to find the result of that query. And in parallel we'll generate a zero knowledge proof that we did all those actions in a valid fashion. And because we've built this Cache of block headers we're actually able to verify that proof on chain and then check that the appropriate block hash information matches the verified block headers we already have in our onchain contract.
00:20:14.600 - 00:20:24.350, Speaker B: So in this way we're offering essentially delegated data reads and compute to our off chain coprocess. Thank you.
00:20:25.760 - 00:20:27.470, Speaker A: Anybody else have any questions?
00:20:33.600 - 00:21:14.396, Speaker B: Thank you. I have the impression that this system could have huge implications for MeV and Mev simulation. Is this true or is this only an impression? It sounds like you know much more about Mev than we do. So first of all, we'd love to talk to you about that. One area where I think it could have some implications is that we're able to access, for example, the ordering of transactions in an actual block. And I've been told at least that first of all in NMEV this is what people care about, but also that this might have implications on driving some accountability for various systems in MVV. So I'd love to talk further about that.
00:21:14.396 - 00:21:15.550, Speaker B: Thank you.
00:21:21.710 - 00:21:22.780, Speaker A: That's right here.
00:21:24.190 - 00:21:37.326, Speaker D: Hi. And I would like to know what type of proving system are you using? I mean which system or are you using recursion or. I don't know, because it seems that performance is really important here.
00:21:37.508 - 00:21:58.306, Speaker B: Yeah, definitely. So we're using Halo two with the KZG back end. So we're using the same fork of Halo two as PSC and scroll are using for the ZkevM. And definitely we're proving pretty big statements so we make pretty heavy use of non native aggregation and recursion.
00:21:58.498 - 00:22:00.040, Speaker E: Okay, thank you so much.
00:22:02.490 - 00:22:08.010, Speaker B: And if you want to see the gory details, we actually open sourced all of our code so you can check it out on our GitHub.
00:22:14.280 - 00:22:25.320, Speaker E: Could I bring my own data? Meaning at some point in history the hash of my data is included in the chain. Could I bring that data and bring that into the proof?
00:22:27.260 - 00:22:56.160, Speaker B: I think the question is, if you have some committed data that's sort of hidden, can you somehow provide it as a witness in our system today? You would have to do that in smart contract land. But we are exploring ways to allow not just committed data, but just arbitrary private inputs that users provide. Another example where that's interesting is if that data is a user provided signature.
00:22:57.300 - 00:22:58.450, Speaker E: Yes. Cool.
00:23:03.630 - 00:23:09.150, Speaker A: Any last burning questions? Ideas? Notions? Comments? Constructions?
00:23:09.270 - 00:23:09.890, Speaker B: This?
00:23:10.040 - 00:23:16.160, Speaker A: No? Well, one more thank you to ye. This is great. Thank you so much. And I will.
