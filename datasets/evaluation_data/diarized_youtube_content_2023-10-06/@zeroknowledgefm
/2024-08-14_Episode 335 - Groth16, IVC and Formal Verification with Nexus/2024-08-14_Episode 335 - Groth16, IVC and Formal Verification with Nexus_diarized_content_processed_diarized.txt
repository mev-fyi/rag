00:00:05.560 - 00:00:58.108, Speaker A: Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online. This week I chat with Jens Grot and Daniel Marin from Nexus. We start with a catch up on all things grot 16 or groth 16 from the author himself. Then we cover the topics of formal verification in the context of Zkps, the Nexus architecture, the benefits and challenges of building a system from the ground up, folding an IVC, and what these properties offer in the ZKVM context, and much more. Now, before we kick off, I just want to let you know about ZK Summit twelve, which will be happening in Lisbon on October 8.
00:00:58.108 - 00:01:32.828, Speaker A: We are excited to once again bring our one day, invite only ZK focused event to life. At ZK Summit, you can learn about cutting edge research, new ZK paradigms and products, and the math and cryptographic techniques that are giving us epic efficiency gains. In the realm of ZK, space is limited and there is an application process. You do need to apply to be eligible for a ticket, and I've added the link in the show notes. If you want to get that application in, the deadline to apply is today, so it's really your last chance to get that application in. If you'd like to give a talk at the ZK summit, cool. So hope to see you there.
00:01:32.828 - 00:01:35.840, Speaker A: Now Tanya will share a little bit about this week's sponsors.
00:01:37.180 - 00:02:28.718, Speaker B: Alio is a new layer one blockchain that achieves the programmability of Ethereum, the privacy of zCash, and the scalability of a roll up. Driven by a mission for a truly secure Internet, Alio has interwoven zero knowledge proofs into every facet of their stack, resulting in a vertically integrated layer one blockchain that's unparalleled in its approach. Alio is ZK by design. Dive into their programming language, Leo, and see what permissionless development looks like, offering boundless opportunities for developers and innovators to build ZK apps. This is an invitation to be part of a transformational ZK journey. Dive deeper and discover more about alio@alio.org. dot Namata is the shielded asset hub rewarding you to protect the multi chain built to give you full control over sharing your personal information, Namada brings data protection to existing assets, applications and networks.
00:02:28.718 - 00:03:03.410, Speaker B: Namada ends the era of transparency by default, enabling shielded transfers and shielded cross chain actions to protect your data even when interacting with transparent chains. Nemata also introduces a novel shielding rewards system. By holding your assets in a shielded set, you help strengthen Namada's data protection guarantees and collect knom rewards in return. Namata will initially support IBC and Ethereum based assets, but will ultimately serve as a single shielded hub for all assets across the multi chain. Learn more and follow Nemata mainnet launch@nemada.net. dot and now here's our episode.
00:03:06.920 - 00:03:12.144, Speaker A: Today. I'm here with Jens Grat and Daniel Marin, both from Nexus. Welcome to the show.
00:03:12.232 - 00:03:13.240, Speaker C: Thank you, Anna.
00:03:13.400 - 00:03:14.632, Speaker D: Very nice to be here.
00:03:14.736 - 00:03:54.372, Speaker A: So I want to start with you, Jens. I want to do a bit of a catch up, folks might be familiar with you. Your last name Groth, or Groth I'm assuming will be very familiar to a lot of our listeners. And yes, you are indeed the author of the Groth 16 proving system. When we did our initial episode many episodes ago, we actually spoke in depth about the Groth 16 system, and we talked about what had happened in leading up to that, as well as what happened after. But I think it might be fun to catch up with you on that topic. I'm kind of wondering, are you tracking all of the implementations of Groth 16? Yeah.
00:03:54.372 - 00:03:56.244, Speaker A: Is there anything maybe new you can share?
00:03:56.372 - 00:04:29.750, Speaker D: Yeah, so I'm not following all the implementations of Groth 16. I focus more on the science side than the implementation side. But we were recently at the CK proof conference in Berlin, and CK proof is this standardization process where we're trying to standardize zero knowledge proofs. And as part of that there's now an effort to standardize grass 16. So I think that's very exciting. That's the first standard that CK proof wants to get going.
00:04:30.170 - 00:04:39.302, Speaker A: What would that mean to standardize it? Is it to choose one iteration or is it to make it a standard itself? Like just the research part.
00:04:39.486 - 00:05:04.566, Speaker D: So I think still open what exactly it means to standardize. That's sort of like also part of the standardization process to decide. But the goal is to be very clear about what is a graph 16 verification, and also set the standard so people can adopt it in good faith. They know they're getting the right thing. Right. So the standard is a lot about confidence building in serial knowledge proofs.
00:05:04.678 - 00:05:11.810, Speaker C: Yeah. Also formal verification of the verifier to know that exists. Yeah, that was one of the efforts.
00:05:12.110 - 00:05:18.558, Speaker A: Cool. Other than the gros 16 world, what has happened for you since we last spoke?
00:05:18.734 - 00:05:30.478, Speaker D: When we last spoke, I was working at Dfinity, and now I've switched to Nexus, so I joined December 2023, as chief scientist of Nexus.
00:05:30.614 - 00:05:40.930, Speaker A: Nice. And what are you working on there? Like are you creating a new GROS 16 proving system? Are you working at a different part of the research? Yeah. What kind of thing are you doing?
00:05:41.350 - 00:06:23.440, Speaker D: Yeah, so the goal really is to work on zero knowledge proofs, and that's why I joined Nexus because I really wanted to work on zero knowledge proofs. And we are building a CKVM, which we hope to have like working at really massive scale so we can prove to trillion cpu cycles per second. So it's basically getting all of the technologies together. And of course this happened a lot in the space in the recent years. It's really moving very fast. So it's about picking a lot of different techniques and putting it all together and choosing some points in the design space that we hope will get us there to this very high throughput.
00:06:24.340 - 00:06:39.962, Speaker A: That's cool. I want to come back to exactly the types of thing you're working on when we talk a bit more about Nexus and the architecture there. But Daniel, welcome to the show for the first time. Tell us a bit about what you were working on before this and what your role is at Nexus before that.
00:06:39.986 - 00:07:04.240, Speaker C: I'll just say that probably when you asked Jens if he was working on a new growth 16, he wanted to hear like, oh yes, we're working on groth 24 or something like that. Maybe. But interestingly, it's not the case. Maybe in the future. Maybe in the future. I actually do track very closely all of the implementation of groud 16, but that's something else we can discuss later. Cool.
00:07:04.240 - 00:07:53.378, Speaker C: Yeah, so a little bit about my background, studied at Stanford, cryptography was part of the Stanford blockchain club and well, essentially while being at school started Nexus. And the goal is just to provide like a hyperscale proverb for verifiable computation. That's it. To enable verifiable computation and bring it to life in a really sort of like large scale, really professional sort of like fashion. What are you going to consider? Just verifiable cloud computing. And our approach is to essentially combine scientific efforts and breakthroughs from mathematics, cryptography and engineering, as well as cryptogramic efforts, which we can discuss later as well to really build this super powerful machine which we call the Nexus CKBM.
00:07:53.474 - 00:08:00.030, Speaker A: What was it originally? Was that the original plan when you first created this project, or did it have a different focus?
00:08:00.450 - 00:08:37.500, Speaker C: Well, it started about two years ago, but more formally about a year and a half. And the goal was to enable verifiable computation but we were, in fact, taking a different approach for a short period of time of the company, which was essentially crypto economic security, actually a topic that is pretty popular nowadays. The idea there was to use something like webassembly, as well as staking mechanisms to provide a stateless off chain computational system that would allow smart contracts to outsource compute to economically incentivize off chain agents.
00:08:37.840 - 00:08:45.510, Speaker A: Do you put this in the category of ZK coprocessors? Was it more like ZK coprocessors? And now it's a changed.
00:08:45.630 - 00:09:17.526, Speaker C: So we usually don't call it coprocessor. So usually a CK coprocessor is like, you use some general purpose verifiable computation system to outsource compute from a smart contract, but it's only one application of verifiable computation. Our approach has always been fully general purpose from day one. So our goal is to allow developers to build things like CKCO, processors of different flavors, actually, and that is the primary goal.
00:09:17.678 - 00:09:23.438, Speaker A: When you first started, though, when you were talking about this cryptographic security, did you have a more narrow focus then?
00:09:23.614 - 00:10:22.988, Speaker C: Actually, no. We wanted to be able to execute in a pseudo verifiable fashion the execution of any ROS program using this crypto economic incentives. And actually, part of the reason why is because I thought this was like, two years ago, that CK was advancing not sufficiently fast in order for this technology to satisfy consumer demand. But actually, that changed very quickly, pretty much perhaps like three months after we raised our seed round, we focused 100% into CK. And it all started when I read the Nova paper, which was published in 2022. From there, it was very obvious to me that that was the direction, especially around highly efficient proof aggregation, and not only because of that paper, but because there was, like, a stream of papers coming out of academia, et cetera, et cetera, that every couple of weeks they were popping out, and we decided to go 100% into that direction.
00:10:23.124 - 00:10:30.724, Speaker A: I feel like I hear the term verifiable compute a lot, or verifiable computation, but can you walk us through what that actually is?
00:10:30.852 - 00:10:42.134, Speaker C: Yeah, absolutely. So I think when people mention verifiable computation, I actually think about a traditional cloud computing service, just that it comes with a proof that simple.
00:10:42.262 - 00:10:53.990, Speaker A: Okay, and what does the proof represent, though? That is that instead of the cloud, is it instead of a third party kind of guaranteeing it, or is it just. It never was guaranteed, and now it can be. It can be verifiable?
00:10:54.070 - 00:11:38.188, Speaker C: Well, it was never a guarantee. Right now, the proof is something that provides you as a consumer a guarantee, but that you can also share with anyone in order to convince them about something. It's always been sort of like part of our philosophy that developers shouldn't know whether they are computing in a traditional environment or in a verifiable environment, and the user experience should be exactly the same. And the Nexus CKBM was assigned exactly with this user experience in mind. It's like you shouldn't know that this is a verifiable machine, that this is a verifiable world. It should be essentially taken for granted. Of course that's not the case, because performance, but that's exactly what we're working on.
00:11:38.324 - 00:11:53.400, Speaker A: Let's define that and verifiable here. I'm trying to picture a use case of this. A developer creates a rust application. What kind of application would need to be verified or verifiable, or what element of the application?
00:11:54.180 - 00:12:45.674, Speaker C: Yeah, first of all, verifiability is about verifiability of the execution, and we verify that the output actually corresponds to running the program on a given input. This differs from formal verification, where verifiability is static. Here it's a dynamic process, it goes after execution. So essentially you're trying to convince someone that some algorithm being computed is being computed correctly. Or more abstractly, you're trying to convince someone that something is true. And so there are obviously a lot of examples in the blockchain space where you also find super interesting outside of the blockchain space, what the possibilities there are, even though the market is less developed. But answering your question, just a simple thing that you can prove is, for example, properties about your Google account without revealing your password.
00:12:45.674 - 00:13:34.132, Speaker C: For example, I could prove that I have 100 emails from Jens sent last night to me without what happened. Maybe he was too worried about the CKP. What are we going to talk about? So, identity high security computing, of course, all of private computation falls into a realm of verifiable computation. There is also the field of re execution. This is quite an interesting property that has never existed in the history of humanity, right? Which is the fact that if you compute approve for sound computation, then no one in the whole future of humanity has to rerun that computation, because someone already spent a lot of resources in.
00:13:34.156 - 00:14:55.378, Speaker D: Producing this proof to add on this, right? So there has been this notion of double execution, right? If you have to have high confidence in a computation, well, how would you build that high confidence? You would execute it on multiple machines, maybe with different pieces of software developed by different teams of developers and so forth. You would then compare those outputs and see do they match. In that case, you would leave the computation. It's really cool that even though it's much more expensive to just do a double computation, but it's really cool that you can now create a proof, and now everybody has a guarantee that this is the right result and you don't need to worry about was there some random bug during the execution or whatever that otherwise could happen. And then to add on that, if you just do two executions, then you are not able to go to anybody else and convince them that this was the correct result. But now when you have a proof that's verifiable by anybody, then you can take this piece of data and you can go around and show, here is a proof that it's correctly computed and everybody will believe that data. So you can send it around to other systems and can compute on this data and trust the data because it.
00:14:55.394 - 00:15:19.760, Speaker C: Has a proof and everybody in the future as well. So the proof is theoretically valid for an infinite amount of time. And that's why actually going back to our conversation of standards, standards are important because if we all agree on a standard for a final, let's say, proof system like grot 16, then my grot 16 proof hopefully will also convince people living 100 years into the future.
00:15:20.500 - 00:15:42.096, Speaker A: So, like, in this case, you can create a proof, but then people need to trust that the proving system you used is correct, is working correctly. And you're saying if there's a standard around gros 16 or something, and that is sort of an accepted, strong proving system that people accept as correct, then they wouldn't also have to audit the proving system because I guess otherwise they would potentially.
00:15:42.168 - 00:16:16.096, Speaker C: Exactly. Exactly. No, you're totally right. And I think this is something that people don't talk about too often, which is like, people talk about proofs and proving systems, but no one talks about, how do we convince ourselves that any of this makes sense? You can tell me. This is a proof. Here's like a million bytes of data, and the proof system is halo two, so to say. Right? Well, I don't know if the verifier that I'm using is correct, if halo two is sound, if the prover for halo two is not cheating.
00:16:16.096 - 00:16:39.046, Speaker C: So I mean, like, it just doesn't mean to me anything, right? So hopefully formal verification can help us with this as well as more concrete estimates of soundness and completeness properties, proof systems. But just like this is a social problem, how do I convince to you that this proof actually proves what I'm trying to prove. It's very hard.
00:16:39.158 - 00:17:11.832, Speaker A: And so far with Roth 16, the proof of that basically comes because a lot of teams have audited and adopted it. It's sort of like they checked it, they implemented it. Those implementations, we assume, have been extremely carefully checked and battle tested, and then a lot of people use them. So they've made lots of, there's lots of exposure of that system. So if it was attackable, the assumption goes, someone would have already done it. I always find that so odd that that's how we get our trust. In a weird way.
00:17:11.832 - 00:17:16.664, Speaker A: It's just sort of like it hasn't broken yet. Therefore we believe.
00:17:16.792 - 00:18:06.672, Speaker D: I mean, in some sense, this is how the field of cryptography works, right? Yeah, we built our systems based on some assumptions about cryptographic primitives. The discrete logarithm problem is hard, or you cannot break this lattice assumption or something like that. The basis for that trust is essentially people have thought hard about those problems and not found a way to attack them. So there's a social process in getting to the trust. We don't have a process by which we can get trust that does not rely at all on human effort. So I do think this is something that is part of the system. And it's interesting actually, when people talk about proof systems, they say, well, you get mathematical guarantees of truth and so forth.
00:18:06.672 - 00:18:31.280, Speaker D: And that is true. You've sort of made some part of the trust process mathematical, but you still do go back to some underlying assumptions, and you do need to have some human trust, especially if you're not an expert in the field. Right. You sort of have to take some cryptographers at the word that, yes, this is a secure proof system, this is a secure assumption to rely on.
00:18:31.820 - 00:19:55.878, Speaker C: So this is actually part of like our company strategy, so to say, as well, which is like, we recognize that a lot of this math is pretty impossible for a lot of people. And so a lot of trust in a proof system and its implementation, being sound, complete and sound and being secure, comes down to social proof. And for that, it's important that the people developing it are, well, world experts across multiple fields, whether that's compilers, cryptography, obviously to other realms, like formal verification, because otherwise, how will you as a user know that you're buying into a system that actually provides security, which is the core product of a system like verifiable computation? If the people that are developing are not cryptographers, or at least don't have expertise on this, how do you convince yourself? My hope is that there will be a way in the future in which trust in experts, such as the people that we have in our team, is not fully necessary because of new advances as well in formal verification, which can provide undebatable proof of soundness. But that is too futuristic nowadays. Today, we just have to trust that the people that implemented a proof system didn't miss a constraint here and there. And that's pretty hard to do.
00:19:55.974 - 00:20:30.220, Speaker A: I feel like we should define formal verification. I've done two episodes on this at least. I know it's come up a lot, but can you explain what that actually is? Because it does seem like it's a step further than just the social. It's not just building something, putting it out into the wild, seeing if someone can break it. It's actually almost like reverse engineering, creating almost like a mirror project that just makes sure that this executes the way it's supposed to. Yeah, but let's explore that a little bit.
00:20:30.360 - 00:20:58.222, Speaker C: Yeah, I'll just say that I was recently seeing, there was a call that Vitalik gave at the Google event, and he was saying, we want to move out from just trust me, bro scenario. And that's exactly what I'm trying to say. A lot of the systems are like, just trust me, bro. This is sound. I've written all the constraints that I've promised to write. The verifier is correct. No one actually has a time or knowledge to go and check it out themselves.
00:20:58.222 - 00:21:19.130, Speaker C: Like, I could build, like, a prover today and say that it runs at a billion hertz. It's going to be really hard for anyone to contradict that claim because they would need to find a soundness exploit. And so the hope is, like, with formal verification, that that process can actually become automated and that it doesn't rely on social proof.
00:21:19.470 - 00:21:55.268, Speaker D: I think there are several layers of formal verification. So, like, if you look at papers in cryptography, they will often come with a security theorem. There will be a written proof, and this is something that's developed by hand, but it is a formal verification process, right. It's just processed by humans that develop that formal verification of the proof. And we have seen in some cases that there are subtle details and the proofs are flawed and so forth. Right? So it's not a failed proof procedure, but it certainly is much better than not having a proof. Right.
00:21:55.268 - 00:22:02.500, Speaker D: If you take a cryptographic paper that publishes some new scheme and it doesn't have a proof, then it's typically a sign that it's broken.
00:22:02.580 - 00:22:03.400, Speaker A: Okay.
00:22:03.940 - 00:22:05.220, Speaker D: That is my experience.
00:22:05.380 - 00:22:08.860, Speaker A: Like, they haven't been able to provide the proof that it works.
00:22:08.980 - 00:22:38.698, Speaker D: Exactly. So either it's a sign that it's developed by somebody who does not have the technical competence to it or somebody who's too sloppy to do it, or something else. Right. I really is. I think more than 50% of times, if there's no security proof, it's just because it's broken. Now, if you look at the papers that do have security proofs, then they're typically good, especially if it's been peer reviewed and so forth. But we do have exceptions to that as well, either.
00:22:38.698 - 00:23:39.336, Speaker D: Few cases there are really horrible flaws, and it's just completely broken. Quite often there's subtle details, and there's some corner case where things are not quite as you expected from the security seat. So if you go a step further than that, then you would say, humans are flawed processes. Humans do make mistakes. Humans do have a hard time keeping track of a lot of small details. So you could say, well, can we have a computer system that can help us? And you do have formal verification tools like lean and Cock and Isabel hall that can help you really check in detail that a proof is correct. So now you would formalize the theorems you want to prove in their language, and then you would use their proof system to sort of create a proof that this is a correct theorem, and they will have a proof checker built in that checks that.
00:23:39.336 - 00:24:11.394, Speaker D: Indeed, that theorem holds, and it's even been designed, these systems. Right. So I think, kart, you can sort of, like, boil down to. There's a core base, which is actually not so many lines of code, 5000 lines of code, something. I'm making up some numbers, but if you really want to verify everything, you just need to verify that chunk, and then you can expand that out to the full cox system, and that is what you're using to verify. So you're really trying to minimize the surface area that needs to be covered by humans.
00:24:11.522 - 00:24:24.494, Speaker A: I always thought of formal verification as more on the engineering side of things, that you've implemented it, and then you create formal verification to guarantee that the implementation is acting correctly. But is it more on the research side?
00:24:24.622 - 00:25:30.182, Speaker D: So, I would think of it as these tools allow you to prove theorems, and they prove theorems about some model, right? So you have to model what it is that you want to prove something. So you would have to write a model of what are sort of like the system that you're doing and provide some theorems about that system. And then you can prove that. You can actually go from direction of, if you have proof of that, there are some tools that allows you to extract code from that, that implements these systems, but that's sort of like the opposite direction. People are also interested in, can you tie an implementation to the model that you're formally verifying? And that would be really useful, because one thing that happens is that you spend a lot of effort in creating this model and proving some theorems. Now you have an implementation. That implementation goes out of sync with the model you have, because while you're updating something, you come up with some performance trick, but now it's actually a different system that you have than what you have in the model.
00:25:30.182 - 00:25:42.090, Speaker D: It's a very hard thing to keep those two in sync with each other. And people are also thinking about how do you try to make sure that they actually do stay in sync with each other.
00:25:42.860 - 00:26:08.524, Speaker A: That's interesting. I remember, I've spoken with some auditors. I think auditors will sometimes do formal verification tools, and there's almost like this spectrum of the way things get checked. Rust, in a way, as a language, is also doing some sort of check that things are correct. That's like a built in. I don't know if you'd call it debugger, but it's doing something to make sure that your programs execute. You've written it in a way that it's executing correctly, and it will kind of correct you if it's wrong.
00:26:08.524 - 00:26:20.102, Speaker A: Then there were things like fuzzing, and then on the sort of far end there was formal verification. And I guess that's why I thought of it more in the coding, in the engineering, implementation side of things.
00:26:20.206 - 00:26:57.562, Speaker D: Yes. So I wanted to point out that the type checking system in rust indeed gives you some tools to formally verify things. It doesn't allow you to express everything, but it does allow you to express some security properties. And that's exactly the strength of rust. That particular some memory safety issues that don't occur in rust because you have this type checking system. There are also attempts to build in more expressivity of that type checking, what you can prove into rust. There are several projects that have tried to build advanced type checking.
00:26:57.562 - 00:27:11.582, Speaker D: You can annotate in the code that these are the post and preconditions expect from this function or method. And then you can actually, if you have built this system in, you can also use the type checker to check that those properties hold.
00:27:11.726 - 00:28:00.890, Speaker C: Yeah, I will say that here at Nexus, we're like huge fans of Rust. I guess at this point, it's not very unique, but we really are. So, for example, formal verification tools in rust to verify rust code. We've been experimenting with some of those. All of the Nexus in KBM is built in rust, of course, like a Rust SDK that works with async Rust to allow you to compute multiple proofs in parallel. We're working on a system that allows you to write this precompiled extension. So the instruction set purely in rust, it's a little bit mind boggling to think what you have is a piece of rust code that verifies arbitrary Roscoe and might be formally verified with Roscoe.
00:28:02.760 - 00:28:06.540, Speaker A: So I don't know, in rust on rust somehow.
00:28:07.080 - 00:28:34.130, Speaker C: Actually, our team is composed of Mozillians, people that came out of Mozilla, the project. So, for example, one of our team members, actually, his name is Sean, Sean Martell, he designed the rust logo itself. But yeah, we're huge believers into that. And the reasons are multiple, not only from the memory safety and the compiler, but also just the developer experience. It's just amazing.
00:28:34.250 - 00:28:34.866, Speaker A: Nice.
00:28:35.018 - 00:29:21.930, Speaker C: And I also wanted to say, Anna, on the point of formal verification, what you said, this is what we see a few teams doing, that it's like we implement, then we formally verify some subset of the constraints. Interestingly, the approach that we've taken since day one is we first essentially formally define the system. And you will see that in the nexus white paper, we provide a mathematical description of the machine. So quite literally, the machine is represented as a Turing machine, so to say, as an. As an algorithm, as a non interactive succinct snark, etcetera. Right? But it is actually represented as an algorithm, and all of the algorithms are described therein. And how we do it is we compose algorithms from multiple open papers.
00:29:21.930 - 00:30:10.406, Speaker C: Just now, as mentioned, Nova, hypernova, incrementally verifiable computation systems, so on and so forth. And we actually combined this building blocks in a very modular way, so to say, to build a mathematical definition of the machine using these papers. And this allows us as well to prove that the construction is complete and sound, because the proofs of soundness, so to say, come from the respective papers. And then once the machine is mathematically defined, we implement it. Once it's implemented, because it's well documented from a mathematical point of view, this gives us essentially two benefits. Number one, formal verification is simpler because we know exactly what we're formally verifying. We didn't go out and implement something and then try to do it instead.
00:30:10.406 - 00:30:27.248, Speaker C: We already knew what we wanted to implement, and then we implemented it, number one. And number two, we hope that it will allow us to build multiple clients. So not only a rust based SIGBM, but just as with Ethereum's client diversity, for which there are, like, Ross and go clients.
00:30:27.384 - 00:31:05.412, Speaker A: So I want to dive into sort of the proving system and the Nexus system, but before I do that, I just want to. One last thought on sort of the formal verification in this auditing sort of space. Has there been any proposals for, like, AI auditing? Like, if you think of going back into that social model that you described of just like you put it out, you make it as strong as you can. You wait for someone to break it. Can't they just. And maybe not yet, but wouldn't it make sense to just run AI agents at this thing, just throw as many hacker brains at it? Has anyone done that?
00:31:05.516 - 00:31:46.880, Speaker D: So people have used AI tools to find bugs, and there are many different tools that you can use to find bugs. So it's also a question, like, how do they stack up against those other approaches? I think I'm a little hazy here. I don't know this field very well, but my understanding is that AI is not quite as proficient as other tools in finding bugs. So you would probably find more bugs by other testing methods, fast testing and so forth, than throwing an AI at a system. But they do find some bugs, and they would. But I might be different types of bugs. So it does give you a bit more coverage in the bug finding approach.
00:31:46.880 - 00:32:08.268, Speaker D: What it doesn't give you and what formal verification gives you is the elimination of all bugs, assuming your model is correct. Again, there's an assumption here, but under that assumption, I don't know of any other tool than formal verification by a machine that can give you close to 100% guarantee that there are no bugs.
00:32:08.364 - 00:32:38.614, Speaker A: That's cool. All right, let's make the move over to the Nexus system. I feel like we got a little, you know, we got into the verification auditing world, but now I want to kind of dig down into what you are building, the proving system that you are talking about, or even that mathematical model. Now, I'm really curious what that looks like, because you mentioned IVC and Nova. When I think of a ZKVM, I feel like usually there is sort of a central proving system. Sometimes there's some sort of recursion built into it. But tell me what yours.
00:32:38.614 - 00:32:44.086, Speaker A: Yeah, the Nexus, CKVM or the Nexus machine really looks like and what proving systems are in there.
00:32:44.198 - 00:32:49.030, Speaker C: Yeah. Yeah. So first I will say that we have built essentially everything from scratch.
00:32:49.110 - 00:32:49.758, Speaker A: Okay.
00:32:49.894 - 00:33:25.502, Speaker C: You know, it's, it's been essentially taking like zero amount of codes from anyone else except for artworks. Pratush and you know, all of those guys, the folks from Alio, have contributed greatly to the artworks ecosystem. We provided the first open source implementation of the Nova proof system after Microsoft's own prototype from Sreenet SETI. That was great. We decided to essentially re implement it, productionize it, using arcworks tooling. We've always wanted to have a very solid foundation for the CKBM up on which we can iterate very fast. Actually, we did.
00:33:25.502 - 00:34:05.076, Speaker C: We provided an implementation of supernova actually very easily after the implementation of Nova was completed, the implementation of Nova took quite a bit. There were a lot of engineering complications that I can talk about as well, especially around two cycles of elliptic curves. We implemented the cycle fold, essentially techniques which allow for more efficient verification on two cycle of curves. And then we recently announced and released an open sourcing of the hyperdoba proof system. It's more generalized, it has multiple benefits as well as the arithmeticization of the machine. So we wrote in r1 cs, the full RISC five instruction set.
00:34:05.148 - 00:34:05.660, Speaker A: Wow.
00:34:05.780 - 00:34:26.427, Speaker C: As well as the memory checking mechanism, which is based on Merkel trees. All of this is part of r one cs constraints. We also have a part of the CKBM that has what we call the proof compression mechanism, which is essentially after we've computed multiple rounds of IVC. And IVC means incrementally verifiable computation.
00:34:26.523 - 00:34:26.995, Speaker D: Right.
00:34:27.107 - 00:34:35.555, Speaker C: For the audience here, we compress the proofs because the Nova proofs are quite large. And so that as well, we essentially, we've done it from scratch.
00:34:35.627 - 00:34:43.467, Speaker A: What do you use there? Is it a stark? Because isn't it usually like, I feel like there's usually like a snark and a stark. Do you do that, or do you just take another snark?
00:34:43.603 - 00:35:06.620, Speaker C: Yes, people sometimes do like a stark and a snark. We have done what you could consider. This is not entirely precisely true, but what you consider, like, I'm neither a snark nor a stark, more like a narc. Okay, so without the s. Yeah, so not snark, not stark, more like narc, without the succinct part. And that is folding. One can be more precise there.
00:35:06.620 - 00:35:57.512, Speaker C: But essentially we make the trade off on the proverb side of things to not be succinct. The falling proof is actually the size of the circuit that is being proven, which is pretty big, but it allows us to do highly efficient proof aggregation. That's what folding schemes in Nova, Aspernova, and Hibernova are good at. And then we grab that real large proof and actually compress it with a snark. In fact, actually three composed, regressive snarks, the first one of which is a modified version of Spartan developed by Srinath SETI, and then implementation by Chiang Fei Shang in arcworks. And then we apply a couple of other rounds, the last one being, well, groth 16. And this is what allows us to get to the three group element proof size.
00:35:57.616 - 00:36:02.580, Speaker A: Would you say then, are you supernova or hypernova under the hood? And then a groth 16?
00:36:03.080 - 00:36:21.068, Speaker C: Yes. Right now it's hypernova, hypernova, hypernova. And then the. What we call the proof compression sequence, out of which the last part is grot 16. So we don't go directly to grot 16. We need to do some mental gymnastics here and there to compress the proofs through rounds of regression.
00:36:21.204 - 00:36:22.000, Speaker A: Okay.
00:36:22.420 - 00:36:28.060, Speaker C: Yeah. Because otherwise it's too much for growth 16 to handle. There would be, like, a gigantic circuit.
00:36:28.100 - 00:36:33.228, Speaker A: Is this recursion done in hypernova itself, or is there, like, another proof in between there?
00:36:33.324 - 00:36:40.924, Speaker C: Exactly, exactly. So hypernova, nova supernova, and so on and so forth are exactly the same for recursion. Right.
00:36:41.012 - 00:36:41.364, Speaker A: Okay.
00:36:41.412 - 00:37:17.058, Speaker C: And this is actually super important. For example, our machine is able to do IVC, right? Just keep computing, keep computing, keep computing, keep computing within a bounded and reasonably small amount of memory, even your browser, for example. So, for example, you can do recursion on the browser. And this is important because for monolithic snarks or starks, sure. Maybe you can prove a little bit on the browser if, you know, if your computer is pretty powerful. But to do recursion, you do need to essentially use a lot of memory here. That's not the case.
00:37:17.058 - 00:37:24.778, Speaker C: But the trade off that we're doing is that the proofs are large, and so they need to be compressed before they can be submitted to Ethereum.
00:37:24.914 - 00:37:52.196, Speaker A: I see. I want to understand the comparison here with some of the other ZKVM teams that have come out recently. So, the first one I've heard of that had the RISC V instruction set was RiSC zero, and then SP one came out earlier this year. Now we have Jolt. All of those are using Starks, as far as I understand. Or some, like, you know, in the case of SP one. Oh, jolt is not a Stark.
00:37:52.268 - 00:37:54.052, Speaker C: No, no, Jolt is a.
00:37:54.076 - 00:37:55.732, Speaker A: Some check based snark.
00:37:55.876 - 00:37:57.100, Speaker C: It uses a spartan.
00:37:57.180 - 00:37:57.484, Speaker D: Yeah.
00:37:57.532 - 00:38:05.240, Speaker A: Oh, it uses spartan. Okay. So is that. Would you say, of those three, then? Would you. Are you closer in spirit to Jolt.
00:38:05.620 - 00:38:24.070, Speaker C: Because of the large field universe. Yes. So right now the next is sigbm. Exactly. It's based on 256ft field snarks. So yeah, actually we're pretty close to there in terms of the actual field size, but we don't have no allegiance to large fields. Right.
00:38:24.070 - 00:38:32.412, Speaker C: In fact we have been exploring quite actively small field universe. It just has many downsides, but it also has a lot of upsides.
00:38:32.526 - 00:38:44.200, Speaker A: So what kind of research are you doing into the small field? Stuff like it sounds like would you be able to use something like binneas somewhere or is everything too built so you wouldn't be able to like sort of hot swap these things?
00:38:44.320 - 00:39:35.750, Speaker D: So, so indeed we are looking into that, right. We are very agnostic about the techniques and there are many techniques out in the field that we are looking at. So this would be one of them. So, so I see this as a question of which kind of commitment scheme do you want to use to commit to field elements and then the trade offs between large field elements and small field elements and the options you have for commitment schemes because Peterson commitments do require large fields, but then they have nice homomorphic properties and so forth. On the small field side, I don't know quite where the landscape will settle yet. Seems promising, but is that what is ultimately going to win or not? That's not clear. People also looking at lattice based schemes for instance.
00:39:35.750 - 00:40:26.120, Speaker D: But we're definitely looking at that as an option. And I think the question you could ask is what is the cost per bit that we are committing? At some point we have to look at what is the raw efficiency we can get out of the proof system and we want to maximize that raw efficiency. One metric is perhaps to say what is the cost per bit that you are committing? Another metric would be then within the proof system how many bits do we actually need to commit to at all? There are also techniques that try to reduce the number of bits that are committed. GKR has been successful in so like reducing the overall commitment surface.
00:40:27.180 - 00:41:33.840, Speaker C: Answering your question, Anna, I would say that one of the core properties of the nexus CKBM in our philosophy from day one has been essentially the observation that the CKBM can be thought of as a modular stack of components, each of which can be individually optimized. And so for example, Jens and his theory team here at Nexus and others, we have been working extensively on what we call the arithmetication system that is inspired by jolt like techniques based on lookup arguments. What that essentially does is it's work into how to arithmetize a full cpu like RISC five in a set of constraints such that we can pack as many cpu cycles perennial per recursion batch. Another direction of optimization is the prover itself, which is Jens just mentioned. We're agnostic to whatever prover we use. We have built our own prover as mentioned Nova supernova, Hypernova. We've built those from scratch and they couple directly with the circuits being implemented.
00:41:33.840 - 00:42:33.304, Speaker C: In addition to that, we also have what we call the memory checking subsystem, which is essentially an independent module that does not just cpu verification, but memory checking. And there's multiple techniques to memory checking. The nexus one machine that we released back in September silently was using a naive memory checking mechanism that was based on Merkel trees. Right now that's still the case for the Nexus two, but we are about to upgrade as well. Then there's multiple other independent components like the proof compression mechanism that is completely decoupled and independent of all of this. Then there's many other components like what we call the Nexus compiler is a transpilation from RISC five to just more and better and simpler suitable instruction set architecture, which we call the nexus virtual machine. For example, the nexus virtual machine was actually inspired by the Ethereum virtual machine, and it allows us to further to do two things.
00:42:33.304 - 00:43:23.544, Speaker C: Number one, essentially you simplify RISC five in a way that is just better suited for proving, and number two, to simplify it from a human perspective for it to make it easier to audit. There are some discussions here and there about whether that makes sense or not. Right now, the nexus virtual machine is essentially risk five with just tiny modifications. Next iterations of the Nexus virtual machine will be most likely supersets of RISC five. This gives us many, many benefits, but all of these components are modular, from the compiler to the cpu architecture, to the memory checking mechanism, to the prover, to the arithmetic system, to the commitment scheme to the compression system. How we work on is we have multiple versions of those in our library. All of it is open source.
00:43:23.544 - 00:44:01.280, Speaker C: We independently optimize them, we gobble them together, and that is what gives you a given choice of a CKVM that might be optimized for either proving in large computer clusters with a lot of memory usage, or proving on constrained environments like a browser with small memory usage, but perhaps more cpu intents. This has been challenging because not taking code from scratch obviously gives you a lot of slowdown initially, but it has been very helpful to us to be able to iterate fast now that it's.
00:44:01.320 - 00:44:41.642, Speaker A: Been built and it sounds like you had a lot of control, but I want to. So I think on the proving system side what it sounded like was that you have this somewhat modular, you have components and you can use these optimization techniques. So you wouldn't necessarily implement a new proving system, you would just use the technique that some new proving system has showcased and then include it in what you've built potentially if you can. And sort of you mentioned like lookup tables and I'm assuming there's like other techniques, maybe if you decide to do this small field thing I'm guessing you wouldn't just re implement something, but rather you would just try to extract what they've done and add it to your system.
00:44:41.786 - 00:45:21.676, Speaker C: Exactly. So it just gets better and better and better. Right. So from a business perspective, so to say, our hope is that this, like the nexus at GBM, can be thought on the more as an ecosystem for cryptography, where multiple implementations of multiple different schemes and components can be found. And then one is a developer using the SDK can choose the optimal configuration for a given program. The good thing is that as the codebase matures and the system progresses, essentially all of the applications that are compatible with the Nexus CKBM just get better and better and better. So that's all ROS programs for now, right.
00:45:21.676 - 00:45:59.000, Speaker C: That's good, because it gives like the developer like zero friction. Right. This has been also one of our core philosophies is that you should just be able to sell it, run it and get approved, right? Yeah, that simple. And that modularity has always been top of mind. In fact, I would say, Anna, that our story as a company has been fairly distinct from that of others in the sense that unlike other folks that have systems that go like super fast, perhaps initially when they launched, when we launched, we were not ashamed to say that the CKBM ran at one cpu cycle proofed per second.
00:45:59.300 - 00:46:01.480, Speaker A: 1 guess this is slow.
00:46:02.140 - 00:46:21.480, Speaker C: It's very slow. Exactly. So proof one cpu cycle per second. The reason why is we folded essentially one cpu instruction per step. And this was back in September 2023. That was what we internally called the Nexus zero machine. The very first version.
00:46:21.480 - 00:47:02.770, Speaker C: We presented it at Stanford then around December of this year represented the Nexus one machine. The Nexus one ran at about 100 hz on an M two MacBook Pro. 100 hz is fairly slow, or was fairly slow by industry standards, in that people say that they run super, super fast. We care about many things, including recursion and proof compression. It ran at 100 hz. Reason being? There's multiple reasons, but the sizes of the circuits that we were proving are actually super large. In the Nexus one, there were 30,000 constraints, which it happens just to be a lot per cpu side.
00:47:02.770 - 00:47:36.094, Speaker C: Here at the Nexus two, we have essentially incorporated advancements from the jolt techniques. And this is actually something that we were looking into since like more than half a year ago, which is to incorporate essentially jolts arithmeticization techniques to reduce the size of the circuits so that we could pack more cycles per recursion step. This is exactly what we've done. And so that was the Nexus two announcement that we did probably like three weeks ago or something like that. And their hope is to continue going in that direction across many verticals of optimization.
00:47:36.252 - 00:47:36.770, Speaker A: Okay.
00:47:36.850 - 00:47:38.682, Speaker C: In collaboration with other teams as well.
00:47:38.826 - 00:47:42.230, Speaker A: But how fast is that one? You sort of said the 100 hz was one.
00:47:42.530 - 00:48:21.978, Speaker C: Oh yeah. Okay. So with jolt, it gets about 100 times faster than the nexus one. So probably like 10,000 hz in our system. But the problem is that being fully transparent, it also has some downsides, like doing ethereum, verifiability is significantly more complex. So it's tricky to just talk about Hertz without it being a full end to end system. For example, there are some systems, you can make them go super, super fast, but they're just extremely hard to recurse, extremely hard to compress.
00:48:21.978 - 00:49:05.536, Speaker C: So it's like, okay, sure, you can go super fast, but it's not useful to anyone. And then there are systems that are probably is lower, but at least are practical, and you can recurse them efficiently and you can compress them efficiently. That's what we had for the nexus one machine. So our hope is like, how can we combine this, like all of this together so that you have a system that is fast, compressible, regression friendly, so on and so forth. Let me give you another example. Something that people don't talk about is you want a universal verifier. And this has always been the goal with Nexus, is that a single smart contract implementing the verifier for the machine can verify any program execution.
00:49:05.536 - 00:49:11.528, Speaker C: And some systems out there have the problem that you need a different verifier for different types.
00:49:11.624 - 00:49:12.520, Speaker A: Okay, exactly.
00:49:12.560 - 00:49:55.968, Speaker C: And that is huge. If you're a developer, you would have to deploy a different smart contract for each different program. A lot of non sick abms as well in the past had exactly that problem. I myself faced this personally because there was some time when I was a student there that I was trying any possible CK system out there, like circumnor, I think it was in its initial days right there. Also, Leo from the Alio team, great products, great tool chains, etcetera. But the universality is a very important factor in practice for any deployed system. You just want to have 1 bar of fire to rule them all, so to say.
00:49:55.968 - 00:50:01.900, Speaker C: I see this has always been part of our core philosophy. That is like, how do we maintain that while improving performance?
00:50:02.060 - 00:50:18.172, Speaker A: When you talk about the universal verifier though, I guess you're talking here about like the zkvms that are non risk five based, right? Like in the Risc V based. Do those also have universal verifiers usually like the single smart contract, like SP one, risk zero and stuff like that?
00:50:18.356 - 00:51:20.290, Speaker C: Yes and no. For example, the verifier may change when adding precompiles to the architecture, because the circuit that is being verified actually changes. If you're a developer that uses machine architecture that's different from other machine architecture, your verifier will be different and you will need to deploy a different smart contract, which is a big deal right now. The best system, so to say, have a universal verifier for a given machine architecture. There might be a way in the future to even have universality across machine architectures, but that's not the case right now. What I would say is the universality property is not always maintained by RisC V machines. Sometimes, for example, the verifier needs to know some kind of like data from the program, and that this is just like a huge trouble with the specifics of RISC Zero or SP one.
00:51:20.290 - 00:51:43.314, Speaker C: Honestly, we've spent a lot of time obviously here being in this space, reading at it. Honestly, we're not sure. We're not sure about how the systems operate, but what we know is how our system operates with us. It is like that for a given machine architecture. Exactly. For a given machine architecture, the very far is universal.
00:51:43.422 - 00:51:52.314, Speaker A: And this means if someone deployed an application on the Nexus VM, they wouldn't have to deploy a smart contract themselves, it would just use yours, right?
00:51:52.402 - 00:51:57.954, Speaker C: Exactly. Just one universal smart contract. Got it. And that's pretty much it.
00:51:58.082 - 00:52:16.646, Speaker A: Going back to the speed up that you're talking about, what are other areas like? You sort of mentioned the Nexus two comes out, but there's a few. It's like it's faster, but there's some other drawbacks. What's next on your roadmap in that regard, in terms of adding new techniques, are there any areas you're paying attention to?
00:52:16.758 - 00:52:59.984, Speaker C: Well, a lot of exciting things are coming on many, many fronts. Not only is the CKBM as mentioned modular. So we're working on essentially every part of the modular CKBM stack. So our vision has always been to do large scale proof aggregation for the CKBM and to essentially build an ecosystem of applications that consume compute and supply compute. So to allow people to essentially lend their compute to participate in a really large scale computation. So going back to our discussion that Jens brought up about re execution, Anna, are you familiar with a project called SETI at home?
00:53:00.072 - 00:53:01.088, Speaker A: Seti at home?
00:53:01.184 - 00:53:03.344, Speaker C: Seti at home from NASA in Berkeley?
00:53:03.432 - 00:53:04.220, Speaker A: Not really.
00:53:04.760 - 00:54:18.730, Speaker C: This was a project from NASA in Berkeley in which essentially NASA needed a lot of compute to do astrophysics calculation to search for alien life in the two thousands. They didn't have a lot of cloud computing power, so they decided to tell people like, hey, please volunteer your GPU, your Nvidia GPU's, and we're going to do this massive large scale distributed computation in order to achieve this. And they were actually very successful. They got about 100,000 people to supply compute, and they actually broke the record to the world's largest computation ever performed. But they had the re execution problem, essentially, to guarantee correctness of this volunteer network, they had to duplicate or triplicate compute and then use a probabilistic argument to compare results before actually computing the large computation. What our CKBM is optimized to do, our goal is just to prove really, really, really large statements. What our CKBM is optimized to do is essentially to do large scale joint collaborative proving.
00:54:18.730 - 00:54:54.848, Speaker C: That is where the recursion part comes in as a core part of the system, which is can we essentially grab multiple proofs from multiple different people and bring them together into a single proof that verifies a whole large configuration? For the Nexus three, we'll see what we'll have. There are the improvements from the developer experience to multiple parts of the proving stack, but there will be components potentially as well, on what you can call the large scale proof aggregation system. We'll have more details there, but I.
00:54:54.864 - 00:55:12.016, Speaker A: Prefer to keep those sequences I want to hear. So when you say proof aggregation system, I'm wondering, do you have in mind also like a prover network, a prover marketplace kind of scenario? We've seen a lot of the ZKVM sort of point in that direction, so I'm wondering if that's also on your roadmap.
00:55:12.168 - 00:56:00.460, Speaker C: We've seen a few of those conversations. It has always been our go here at Nexus. Essentially, we want to be able to do the largest verifiable computation ever performed. That's what we want to do, let's say you look ten backs into the future. We would want to say Nexus was able to run an extremely, extremely large computation that has never been seen before. While people talk about CK marketplaces, CK networks here and there, we are concerned about the problem of building a really, really, really large scale machine, a single machine, having many people work together as one. So we don't call it market or something like that.
00:56:00.460 - 00:56:13.828, Speaker C: We'll see. I don't want to spoil a lot of things in this CK podcast, but let's say that we are concerned about the problem of making the CK VM as powerful as possible through any means as possible.
00:56:13.964 - 00:56:31.918, Speaker A: Hmm. That's your focus. So you're not focused right now on building a marketplace. Could you potentially work with a marketplace? Like, you talk about this sort of largest verifiable computation possible, a joint collaborative proof. But I guess the proof itself would still ideally be smallish.
00:56:32.014 - 00:56:32.782, Speaker C: Exactly.
00:56:32.926 - 00:56:36.398, Speaker A: Would like, you need a special actor to actually create that.
00:56:36.494 - 00:56:41.826, Speaker C: Exactly. A special actor. And so this actor, we call it the orchestrator.
00:56:41.878 - 00:56:43.234, Speaker A: The orchestrator, okay.
00:56:43.282 - 00:57:14.346, Speaker C: Yeah. So this is a system that orchestrates compute, quite interestingly. So in our white paper, actually, Ana, this is all public in our white paper with define the CKVM, as mentioned, mathematically using multiple mathematical components. And one of those components is called PCD, which stands for proof carrying data, which is a generalization of IVC, which is mentioned, stands for incrementally verifiable computation. So PCD is essentially a distributed, incrementally verifiable computation.
00:57:14.458 - 00:57:22.122, Speaker A: Distributed. What was it again? What's the I? Distributed incremental incremental verifiable computation. Okay.
00:57:22.306 - 00:57:39.570, Speaker C: Exciting. Just like you and Jens and me and maybe a few other friends, perhaps we can do a computation in pieces, right? And essentially user pieces to combine together to build like a proof and then keep computing on it. Keep computing, keep computing.
00:57:39.650 - 00:57:48.130, Speaker A: That's the folding itself, is that like each one of us would do one of the folds or like whatever, one of these iterations, and then they're combined.
00:57:48.290 - 00:58:31.316, Speaker C: Exactly. And then they're combined, and then we can keep combining them and combining them at infinitum, so to say. So actually, Anna, this is something else that we've been super focused on, because I feel like this is something that people don't talk about is the ability to prove unbounded computations. What this means is that the CKBM is able, like if you have enough patience and time to prove a really, really, really large computation in other systems, you have to set up a static bound on the computation. You say, we're going to prove any program up to 100,000 steps. If you want to do that, you can do it. But if you want to prove like 101, it doesn't work.
00:58:31.316 - 00:59:00.680, Speaker C: You need to either. Exactly. It won't work. You need either a new verifier or two proofs or something like that. What we've been concerned of is, can we have a verifier that is not only universal for any program, but also universal for any length of execution? And the folding property, as well as the recursion, exactly allows us to do that. We can keep folding and folding and folding. And this is the whole premise of incrementally verifiable communication.
00:59:00.680 - 00:59:11.200, Speaker C: You might see that CK rollups actually fall very nicely in this category. I think. Sik roll ups are naturally, incrementally computable machines.
00:59:11.500 - 00:59:15.308, Speaker A: Are they unbounded, though? Do they need unbounded computation?
00:59:15.404 - 00:59:54.764, Speaker C: Well, you're going to run your CK rollo. Hopefully for the whole future of humanity, you can use traditional recursion to essentially combine sick roll up proofs. There are some challenges. Maybe the recurse aggregated proof needs a different verifier than the proof for the actual execution, which is a challenge. It would be really nice if Ckrollps were essentially just one proof, and that proof was tiny and it just got updated, updated, updated, updated on every block. You just like have a new proof. You take the old proof to combine them, and that's your new proof.
00:59:54.764 - 01:00:10.588, Speaker C: That's the premise of incrementally verifiable recommendation. Right. It's that you need just one verifier and you can combine the proofs over and over and over. Yeah, that is sort of like our goal to enable this like incrementally machine. Incrementally computable machine.
01:00:10.724 - 01:00:27.678, Speaker A: I want to go back to the unbounded computation because it caught my interest, which is like, what is an example of that? Maybe that. And we can also talk about this re execution. I kind of want to dig into, like, what that would even look like. What's a use case there? What's a use case for unbounded computation? What would that. Yeah, what kind of application needs that?
01:00:27.774 - 01:01:05.476, Speaker C: Yeah, absolutely. Ethereum. Ethereum is an incrementally computable machine, the EVM or any other virtual machine based blockchain system. And it would be very nice, for example, if actually Vitalik has been talking about it. He asked at some point, what if we all achieved consensus on a proof system? Let's just say there's a million out there. What if we decided we synchronize as humans into choosing one? Well, perhaps if we wanted to choose one, it would be one that is potentially number one, very simple. Number two, widely understood as a scientific breakthrough, so to say.
01:01:05.476 - 01:01:40.770, Speaker C: And number three, that is optimized for recursion and incrementally verifiable computation. We would want to have a proof for a block, for this proof to be convincing. Hopefully, this proof system is very simple. And for this proof to be updatable, we would want that if we have essentially a proof for block one and a proof for block two, we can just cheaply merge them together and have a proof for block one and block two. I would say that our main goal has been particularly proving really large computations, like rollups, so to say.
01:01:40.850 - 01:01:52.834, Speaker A: Okay, but. Sorry, Daniel, I keep trying to. It's the unbounded computation that I'm trying to understand, though. It's not the IVC so much. So what is that? Is ethereum unbounded computation?
01:01:52.962 - 01:01:53.906, Speaker C: Exactly, exactly.
01:01:53.938 - 01:01:58.790, Speaker A: Okay, that's what you're saying. Or roll ups use would need unbounded computation.
01:01:59.090 - 01:02:54.172, Speaker C: Exactly, exactly. So if you're launching a rollop, an l two, an l three, even an l one, this machine's are essentially running an unbounded computation that will keep computing, and computing every day, more and more and more and more. One would want to have a proof system that is able to prove unbounded computations without changes to the universality of the verifier. The verifier should remain the same, and the proofs should be easily aggregatable. The unboundedness also helps in another thing, which is on collaborative proving. Collaborative proving makes a lot of sense if you're proving an unbounded computation, because you can keep doing it and keep doing it, keep doing it and use computational resources from multiple people. Yeah, and these people, you know, they might connect, they might disconnect, but you can just, like, reallocate computational resources and keep computing.
01:02:54.172 - 01:03:05.050, Speaker C: So it doesn't matter who is doing the computation. It can be, let's say, stopped for a moment, but restarted by any other agent. This is a discussion about liveness, for example.
01:03:05.170 - 01:03:55.310, Speaker D: I'm wondering if it's useful also to give the original motivation for incremental verifiable computation, because that was for an unbounded computation. And the example was, suppose humanity wants to compute something that will take 100 years, or whatever it will take, you don't know. That's essentially an unbounded computation. And the question is then, how do you trust what happened 100 years ago? And why would you trust that the computation has been correct up to this point in time? Right. And in incremental, verifiable computation, you sort of, like, do the computation in chunks, and once a chunk has completed, you spit out a proof, right? And then you do another chunk, and you spit another proof. And those proofs aggregate all the previous proofs. Right.
01:03:55.310 - 01:04:04.590, Speaker D: So now you know that all the previous chunks of computation are correct, and that led to this point where we are now and what the state of the computation is at this time.
01:04:04.750 - 01:04:32.572, Speaker A: I want to actually, just to clarify there, because what it sounds like is more recursion, just like recursive proofs, like the Mina system, where it always had a proof, and then it made a new proof with new information and the old proof. But in the case of IVC, it also sounds like you're kind of like you're adding lots of different pieces of almost like a partial computation of a proof with an existing proof. Is that what it actually looks like, or is it straight up recursion in the same way?
01:04:32.676 - 01:04:56.812, Speaker C: So it's simpler recursion, actually. So one can achieve recursion in IVC using monolithic starks. There are just some downsides. Falling systems are optimized for, essentially enabling IVC in a very cheap fashion. There are other technical details there as well. But the folks from Mina were the original ones proposed. They didn't call it that way.
01:04:56.812 - 01:05:06.484, Speaker C: They didn't call it IBC. They just called recursion. But essentially, they essentially proposed the first blockchain succinct blockchain IBC system.
01:05:06.652 - 01:05:22.238, Speaker A: Yeah. That's nice you say that, actually. Yeah. I feel like during the IVC boom of last year, I felt they kind of got, like, a little bit forgotten. People needed to remind them that there was a system that did something like this. Yeah.
01:05:22.324 - 01:05:45.354, Speaker C: Yeah. I remember reading the Mina white paper when I was a student, and I had a panel some months ago with the Mina folks. And, yeah, this is what I was telling. And, like, guys, you know, you were the original ones to propose this, and I think this is going to be the future for a long amount of time. We'll see how people call it. But now it's incrementally verifiable computation. Right.
01:05:45.354 - 01:05:50.694, Speaker C: And technically, what we have is an incrementally verifiable zero knowledge virtual machine.
01:05:50.762 - 01:05:51.830, Speaker A: Hmm. Cool.
01:05:51.910 - 01:06:54.446, Speaker C: Yeah. I would say that another topic is just like, will, this proves that we compute for ethereum. Like, convince someone a year into the future or two years into the future, we would want this proofs to be fairly simple, like mathematically speaking, that it could convince anyone, for example, when you say, oh, I use this random proof system, it's like the proof system could be sound, et cetera, et cetera. But it's like, how do you convince other people that this is correct? Such as we had discussed before, we believe that a system that uses very simple mathematical primitives, it's very simple to understand from a mathematical perspective. Maybe it's not as efficient, but it's simple, leads to many advantages in terms of human synchronization. A lot of ethereum is about synchronizing people. We believe that something like that is important in terms of like, standards, so to say, not standards, but more like universally accepted standards for what a proof is.
01:06:54.446 - 01:07:45.328, Speaker C: And so this is important for the point that I was going to make before about liveness, right? So like if for some reason there's a company, right, that is computing a lot of proofs, but then they die or whatever, hopefully the proofs are, number one, is easy to understand, easy to produce, and then anyone can continue producing those proof. So just as, just as people are computing blocks and there's like block proposers and all of that, the same, right, people would compute Nick proofs and they would propose a proof, the proof would get accepted. And there's a way to decide on a proof. This is a different topic of conversation that I am particularly very passionate about, which is sort of like the future of Ethereum. Like Ethereum wasn't designed for serial knowledge proofing. Yeah, right. So what will happen in the next five years of Ethereum? Well, we'll see.
01:07:45.328 - 01:07:55.888, Speaker C: But Ethereum is certainly very here and there. Very ck, unfriendly, I would say, on the side of proving and on the side of verification.
01:07:55.984 - 01:07:56.560, Speaker A: Yep.
01:07:56.680 - 01:08:02.384, Speaker C: So. Well, we'll see. I'm very optimistic that the Ethereum foundation.
01:08:02.432 - 01:08:27.680, Speaker A: Folks will make changes. Yeah, it's funny, because I was going to ask you if you are solely in the Ethereum world or if like as a ZKVM, you could be on any chain. So, yeah. Have you ever thought of deploying elsewhere that's more ZK friendly? It's so, what is the word? It's heresy to suggest such a thing, I realize, but yeah, no, no, no.
01:08:28.420 - 01:09:14.722, Speaker C: So I really like Ethereum. Like personally, I've been following the Ethereum probably since like 2017, but it has huge limitation that, for example, it only supports precompiles for BN 254 operations and pairings. This has been a problem since two years ago. I asked my advisor Dan Bonet back then when I was Stanford, like, hey Dan, is this ever going to get resolved? And he actually asked Vitalik, and Vitalik just said, probably not anytime soon. We're focused on the merge and all of that. It has been the same for years. For example, we would want precompiles for BLS 12,381 so that we could do BLS verification of Ethereum of BLS signatures.
01:09:14.722 - 01:09:35.979, Speaker C: But that just hasn't been enabled. There's been an eap that has not been enabled. We would want other precompiles to be able to do other different things. This would massively simplify our proof compression process, because for now we're bounded to essentially have a system that makes the proofs compatible with BN 254, which is not nice.
01:09:36.059 - 01:09:40.555, Speaker A: Yeah, yeah, yeah, I know. I've heard this from lots of teams that have worked with the Ethereum stack.
01:09:40.627 - 01:10:08.170, Speaker C: Exactly, exactly. Other blockchains, they have their problems as well, just because sometimes they just don't have BN 254 break compile. So it's just like worst. So you have to deploy a full raw spirit of fire. But sometimes those blockchains just happen to be so cheap that maybe doing that is fine. There were recently some innovations on the side of bitcoin and dogecoin. True, right? You saw that, right? To add an opcode for a Grot 16 verifier.
01:10:08.170 - 01:10:59.110, Speaker C: And all of those innovations on the execution environment is good. So, answering your question, our system, even though in my heart Ethereum occupies a special place, our system is fully general purpose. Our verifier can be deployed anywhere. And in fact, one of the core goals of the Nexus project, so to say, is to what we call democratize Ethereum's execution environment. And by that I mean to allow developers to have rollops or other execution environments that are just distinct through the use of precompiles and other virtual machine architectures. For example, this limitation that we've had on precompiles on the EVM would be solved if the EVM was a cKEVM built with the nexus CKVM.
01:10:59.230 - 01:11:03.450, Speaker A: Oh, so you would almost like triple, like you'd put yourself on the third level or something?
01:11:04.670 - 01:11:14.382, Speaker C: No, what I mean, sorry. What I mean is that people have not been able to develop their own precompiles to the EVM and then merge them into the Ethereum protocol.
01:11:14.446 - 01:11:16.542, Speaker A: So maybe they have to move it to the l two.
01:11:16.686 - 01:11:32.870, Speaker C: Exactly like innovation will probably be faster in an l two, where you can enable new precompiles because you don't have to wait for the Ethereum foundation to pass something. So innovation will likely improve on that side by just allowing people to write their own pre compiles.
01:11:33.410 - 01:12:31.812, Speaker D: I wanted to add to what Daniel described for us, the blockchain space, both ethereum, but also potentially other chains. I think we should also think beyond the blockchain space. Our goal is to enable a verifiable computation to be everywhere. So if you made verifiability really cheap, so it was almost at the cost of computing, then you could spread verifiability everywhere. So you could imagine the Internet and all the data so floating around, and the Internet came with proofs, so it was all based on verifiable information, rather than just whatever you believe that the other computers is giving you. So I think the end goal, and that's probably going to be many years into the future, is really that you saw increase the trustworthiness of everything through verifiable computation.
01:12:31.956 - 01:13:14.846, Speaker A: Yeah, actually there's a few other. There's Pluto, who's doing sort of like the TL's web proof thing. There's a few teams, they're kind of like we've been describing them closer. They're not quite like hardcore, they're infrastructure, but they're a bit higher up the stack towards the use case. And I do think Mena, in a way, going back to Mina, I think they also had that vision of being able to prove the Internet, which is a much bigger picture. So I like that you're also thinking about that, because I think there's something important that we do think past blockchain stuff. I think interaction, the intersection between ZK and blockchain, has been epic for the acceleration of Zkhdem.
01:13:14.846 - 01:13:22.050, Speaker A: It's been the funding. I think it's also brought a lot to blockchain. But, yeah, I love the idea that it even transcends that.
01:13:22.170 - 01:13:33.874, Speaker D: I mean, I think there's a lot of things happening in the application space because what people are doing, they're going out and finding these trust anchors. You need to prove something about something.
01:13:34.042 - 01:13:34.898, Speaker A: Yeah, exactly.
01:13:35.034 - 01:14:03.798, Speaker D: You have people that are going out and stripping out passport data using the TL's connection, Google account or some information and so forth. Right. And proving things about that. And it's interesting because this is exactly something that they can do, because we have improved the efficiency of zero knowledge proofs. Right. If you tried to do that ten years ago, you just failed because you couldn't do it. You didn't have the performance to do it.
01:14:03.798 - 01:14:38.440, Speaker D: Right. But now we have the performance to do it. And even that they can take general purpose proofs, right, and throw them at it, right. So it's much faster innovation and prototyping, right. You don't need to so like handcraft a proof system that is efficient enough for this application, but you can just go out and take a general purpose. And I think it's very interesting what the application space and the matching serial knowledge techniques are going to look like. I think we are really seeing very rapid improvements in performance, which is going to unblock a lot of things.
01:14:38.940 - 01:15:22.134, Speaker C: Pretty much everybody in our team, we are primarily excited about the real world applications of CK and to bring it into industry. That actually goes back to our initial conversation about verifiable cloud computing. We care about making it very, very professional, high degrees of observability, what you can consider an elastic cloud that is highly available, highly optimized, very professional. It just computes proofs in connection with ecosystem partners and so on and so forth, which is where the network discussion comes into the future. But bringing verifiable computation to full generality, to the Internet is our primary goal as a company.
01:15:22.302 - 01:15:41.042, Speaker A: Cool. Well, thank you, Daniel. Thank you Jens, for coming on the show and sharing with us all these things. I mean, we covered for formal verification, audits and security all the way to sort of the nexus system in this verifiable compute, what that actually means, the largest verifiable computation.
01:15:41.146 - 01:15:41.730, Speaker D: That's cool.
01:15:41.810 - 01:15:42.594, Speaker A: Thanks for coming on.
01:15:42.642 - 01:15:43.410, Speaker C: Yeah, thank you.
01:15:43.530 - 01:15:45.530, Speaker D: Thanks, Anna. A pleasure as always.
01:15:45.690 - 01:15:52.410, Speaker A: All right, I want to say thank you to the podcast team, Rachel, Henrik and Tanya, and to our listeners. Thanks for listening.
