00:00:07.370 - 00:00:59.694, Speaker A: Welcome to Zero Knowledge, a podcast where we talk about the latest in zero knowledge research and the decentralized web. The show is hosted by me, Anna and me Frederick. In this episode, we chat with John Adler about optimistic rollups and how they compare to ZK roll ups. We also chat about his projects, lazy Ledger and fuel labs. But before we start in, I want to say thank you to this week's sponsor, lease authority. Lease Authority is a security consulting company known for their dedication to pushing the limits on how to build privacy respecting solutions. They are a team of security researchers, open source developers, privacy advocates, and cryptographers specializing in security audits, design specification reviews, and security by design.
00:00:59.694 - 00:01:35.990, Speaker A: They are well known as an auditing firm. They've worked with organizations like the Ethereum Foundation, Tezos foundation, or protocol labs, as well as their work on zero knowledge proofs. They've implemented the zero knowledge access passes, or zcaps, with the distributed storage system. Tahoe laughs. If you are skilled in the area of zero knowledge protocols as well as other advanced cryptography, both for scalability and privacy enhancing tech, then you should get in touch with them. They're currently expanding their team, so email them at jobs@leastauthority.com I've also added the link in the show notes, so thank you again, least authority.
00:01:35.990 - 00:01:54.862, Speaker A: Now here is our conversation with John Adler. So this week I want to welcome John Adler, who's the co founder of Lazy Ledger and Fuel Labs, to the show. He's an applied researcher and protocol designer and also a fellow canadian. So welcome, John.
00:01:54.996 - 00:01:56.298, Speaker B: Thank you for having me.
00:01:56.404 - 00:01:57.620, Speaker C: Welcome, welcome.
00:01:58.310 - 00:02:18.360, Speaker A: And John, you've been actually recommended. People have sort of mentioned your name to me a couple times to have you on the I mean, I think one of the ways we can start this is actually to understand what are you actually working on right now. We usually do the background first, but I want to hear more about what you're up to right now.
00:02:18.730 - 00:03:13.610, Speaker B: Right, so there's kind of two things that I am working on, one being lazy ledger and the other being fuel labs. So lazy Ledger is a new layer, one blockchain that is specifically optimized for just data availability and ordering and it doesn't do execution. And this is a brand new, completely different paradigm to all other blockchains that currently exist and that are proposed. And this allows it to be more scalable while retaining good security and decentralization guarantees. The other thing I'm working on is fuel labs, which is building an optimistic rollup called fuel on top of Ethereum, and it's optimized for performance rather than short term ease of use. So it uses things like the UtXO data model and parallelizable verification so that you can get much higher transaction throughput on the same hardware.
00:03:14.350 - 00:03:22.218, Speaker A: What were you doing before this that would have led to this? How did you discover that these were the problems that you wanted to tackle?
00:03:22.394 - 00:04:06.154, Speaker B: So, funny story is that I initially got into the blockchain space through pushes from my old grad school advisor. I was in grad school at UFT doing formal verification research, and he kind of got really into this whole bitcoin and ethereum stuff. One picture he really likes to show his students is him in front of Mount Gox in know the day that it happened, and there's a guy with a sign, give us your bitcoin or whatever, and then he's right there. So that's kind of a picture he likes to show. So that's kind of where I got my first taste of crypto. Then I joined consensus, I think, two years ago around to do L2 scalability research. So this was plasma channels and stuff.
00:04:06.154 - 00:04:30.850, Speaker B: And while doing that research, just like a month after I joined, Mustafa Albasam had published his paper co authored with Vitalikbuterin, on fraud and data availability proofs. And that kind of set the entire direction of the research that I was doing. It showed that data availability was a big problem, that fraud proofs are real. And this led into the direction of optimistic roll ups in the Icar eventually.
00:04:31.270 - 00:04:35.060, Speaker A: Who was your. You sort of mentioned this professor, but who was that?
00:04:36.070 - 00:04:38.914, Speaker B: Who was it? Andreas Vanaras. Okay.
00:04:38.952 - 00:04:44.802, Speaker A: I don't know if I know him, but I was just wondering, because you didn't say his name, you just had sort of said a professor.
00:04:44.946 - 00:05:00.170, Speaker B: He doesn't do too much novel blockchain consensus protocols research. His group right now is more involved in oracle research and crypto economic stuff as opposed to the consensus protocol stuff that a lot of professors publish.
00:05:00.510 - 00:05:01.498, Speaker A: Got it.
00:05:01.664 - 00:05:33.634, Speaker C: So one of the main things that we're here to talk about today is optimistic roll ups. And this is part of what you do with fuel. And I can see how lazy Ledger also sort of fits into that story. But before we dig into anything super specific, I think we should talk about what optimistic roll ups even are. We've mentioned that term on the podcast before, but we never really had a good explanation of it. So if we just start high level, what is the 30,000 foot view of optimistic roll ups?
00:05:33.762 - 00:06:07.314, Speaker B: So the very, like a one line or one sentence description off optimistic roll ups is that it's like ZK roll ups, which I'm sure most of your listeners are familiar with given the name of this podcast. It's like ZK rollups, but instead of using validity proofs or zero knowledge proofs, you use fraud proofs. That's the one sentence description. And of course this oversimplifies things, and it kind of is the grossly oversimplified version that is missing critical details, that without those details it doesn't work.
00:06:07.432 - 00:06:15.198, Speaker A: Got it. I think we've mentioned fraud proofs before, but can we actually define what that is like? What in this context is a fraud proof?
00:06:15.294 - 00:06:39.958, Speaker B: Sure. So a fraud proof is a proof that something is invalid. So a validity proof is a proof that something is valid, and a fraud proof is a proof that something is invalid. So the way a fraud proof works is you get a claim and then you wait some timeout, period, until you see a fraud proof. And if you don't see a fraud proof within that timeout, then you assume it's valid.
00:06:40.054 - 00:06:46.480, Speaker A: And the fraud proof, is that created automatically or is that created by some agent that's watching?
00:06:46.930 - 00:06:50.106, Speaker B: Someone watching needs to create the fraud proof.
00:06:50.218 - 00:07:44.990, Speaker C: And so I guess this is why they're called optimistic in that I think of optimistic in the sense of optimistic updates on like when you used to build websites and there weren't good frameworks for this, and you sent off an Ajax request, and then you could either update the UI as soon as you sent the Ajax request, or you could wait for the server response to come back and say, yes, this was successful, you can now update the UI. And the optimistic update is you just update it right away, assuming that the server will succeed in its request. And so I guess it's the same thing here where you execute a transaction and then you just assume that it's going to succeed. That's the optimistic part. But the server, quote unquote, which is like the verifiers, they might come back with a fraud proof saying, no, actually this didn't work out, and then you have to roll back your assumption or what you thought was going to happen.
00:07:45.140 - 00:07:47.954, Speaker B: In many ways, yes. That's a good description, doesn't it?
00:07:47.992 - 00:08:03.734, Speaker A: Sort of. And I know that these have been lumped together, I know there are distinctions, but like plasma, wasn't it sort of working under this principle of at first declaring it correct and then having some game theory potentially rebut that? Is this similar?
00:08:03.932 - 00:08:07.234, Speaker B: Yeah. So plasma and channels both use fraud proofs.
00:08:07.282 - 00:08:14.220, Speaker A: I see. But optimistic roll ups are new. I mean, they're considered sort of a class of their own. Why?
00:08:15.470 - 00:08:51.986, Speaker B: Because optimistic roll ups provide us guarantees, a certain cost, but guarantees nonetheless that channels and plasma can't provide. So I can go over them briefly now, which is that channels don't have open participation and they have certain constraints on liquidity. Right. You need things like inbound liquidity to receive a payment. Plasma is inherently permissioned, and you can't have arbitrary smart contracts running on plasma. The reason being is that plasma doesn't use general purpose fraud proofs. It uses specifically an exit game around owned assets.
00:08:51.986 - 00:09:22.830, Speaker B: So if you don't have a concept of ownership for something, for example, the uniswap contract, no one owns it. In that case, plasma doesn't really work very well with optimistic roll ups. It has open participation that plasma has and channels don't. It doesn't have the liquidity constraints that channels have. It's permissionless, and it uses general purpose fraud proofs so that you can run any smart contract on it potentially.
00:09:23.330 - 00:09:29.650, Speaker A: Was plasma ever designed with the idea of actually running smart contracts on it as well? Was that originally the idea?
00:09:29.800 - 00:10:03.978, Speaker B: Yeah, the original plasma paper, I think it described that you could run smart contracts, but the original plasma paper didn't really describe a concrete system. It was more of a very high level, abstract idea. The kind of concrete instantiations of plasma were plasma MVP, minimal, viable plasma, which was only around payment, Utxos, just accounts, account balances basically, and then plasma cash, which was also based around payments and maybe potentially some predicate scripts around that. But none of these were designed around the potential of general purpose smart contracts.
00:10:04.154 - 00:11:07.854, Speaker C: I'm curious to dig into how you achieve these two properties that you mentioned. Open participation and not needing the liquidity. So to maybe recap for the audience, like if you open a channel, you sort of say on chain, each participant perhaps says, I'm going to lock up ten eth. They both lock up ten eth, and then they can both go plus minus ten eth each in their off chain transactions, and they can send a million transactions off chain, but they have to move within these limits that they've set in the beginning. And then when they say, okay, all my transactions are done, they settle back on chain, and then the actual ending balance is moved, like properly settled. So how do you move off chain without actually doing this lockup process and having this, I have to lock up x with this person in a channel. And like this participation stuff, it seems like this on chain lockup thing is sort of inherent to how you take things off chain.
00:11:07.854 - 00:11:11.714, Speaker C: So how do you actually achieve those properties without doing that.
00:11:11.912 - 00:11:43.030, Speaker B: There's a really easy way, and you use what's called a blockchain. You may have heard of this technique, blockchains allow us. I'm saying this partly as a joke, but partly because this is actually what you do. The point of a blockchain is you can order transactions and you prevent double spends. Blockchains allow open participation and they allow participation that doesn't require capital lockup. So we can briefly go over plasma cash in this context. And it might make more sense of why you can't do things like uniswap with channels.
00:11:43.030 - 00:12:36.190, Speaker B: So the way I kind of think about plasma cash, and not many people really think about it this way, which is curious, but you can think of it like a bunch of channels with a particular allowable update. So channels proceed by unanimous agreement. All the channels participant needs to sign off on a new state. So imagine you had some channel construction where one of the allowable updates was one of the channel participants could completely give up ownership to a different user that's not even part of the channel. You have Alice and Bob, and then Alice and Bob sign an update that now it's Charlie and Bob that are in the channel. Now, this doesn't work in reality because Alice and Bob could also sign a channel update and give it to Dave that say, now Dave and Bob are the channel owners. So essentially you can kind of double spend this transfer of ownership.
00:12:36.190 - 00:12:40.874, Speaker B: And the way to prevent double spends is, well, you just use a blockchain.
00:12:41.002 - 00:12:43.902, Speaker A: The off chain has to be a blockchain. Is that what you mean?
00:12:43.956 - 00:13:20.602, Speaker B: Okay, yeah. So what happens off chain has to be a blockchain and plasma cash is that it's a blockchain that orders this transfer of channel ownership in the plasma cash coins. So in the sense, plasma cash is kind of like a bunch of channels. And then you just change the ownership surround, which wouldn't work with channels usually. But if you put these channel ownership transfers into a blockchain, which is what the plasma cash chain is, then it works. So from this, we can kind of see why you couldn't really build a uniswap without a blockchain using only channels is because you have a uniswap. This is some shared state.
00:13:20.602 - 00:13:35.694, Speaker B: If Alice and Bob are trading on a uniswap inside a channel, and then they bring in Charlie and they say, hey, Charlie, here's our latest uniswap. Well, Charlie doesn't know that's the latest channel state, or it doesn't know that they've also brought in Dave, so you need a blockchain somewhere.
00:13:35.822 - 00:14:04.806, Speaker A: I think we've covered at least two, maybe three ZK rollup constructions. And in those, I don't know if it's all of them, but at least two that I can think of. ZK sync and Hermes. They have some sort of, like, validator, even if it's not the traditional validator that we think of. They're kind of these agents that are making sure that whatever's happening in there is somehow correct. And they are like the consensus builder within this other blockchain.
00:14:04.998 - 00:14:05.740, Speaker B: Yes.
00:14:06.190 - 00:14:17.002, Speaker A: So I do know that that mostly exists in ZK rollups, but actually, John, are you familiar with ZK rollups that don't have any sort of validator type role?
00:14:17.146 - 00:14:25.298, Speaker B: I think ZK rollups use provers as opposed to validators in the kind of proof of stake sense.
00:14:25.384 - 00:14:33.986, Speaker A: That's fair to say. I mean, I'm using the word validator here as like, just some block producer, some checking body that could either be.
00:14:34.008 - 00:14:40.998, Speaker C: A committee would be validating the transactions by submitting a proof that they're correct.
00:14:41.164 - 00:14:48.546, Speaker B: Yeah. The validator for most ek roll ups would be the smart contract on Ethereum. It would actually verify the proof.
00:14:48.738 - 00:15:15.502, Speaker C: That's true. Yeah. When you talk about a validator on a proof of stake system, it's the same kind of thing, right, where the validator isn't actually guaranteed to produce something valid. That's why you have all these other economic systems in place. Just like the prover isn't actually guaranteed to provide a correct proof. And so it's the smart contract that verifies the proof. But in a proof of stake sense, maybe the prover is more like a validator.
00:15:15.502 - 00:15:18.174, Speaker C: I don't know, confusing terminology.
00:15:18.302 - 00:15:26.302, Speaker A: Yeah. Okay, but let's go back to your story. So plasma cash, it did have then some sort of group of provers.
00:15:26.446 - 00:15:31.494, Speaker B: Yeah, plasma cash and roll ups would have block producers for the blockchain, okay.
00:15:31.532 - 00:15:32.370, Speaker A: For the separate.
00:15:32.450 - 00:15:51.390, Speaker B: They are blockchains, so they do need some block producers. And plasma and plasma MVP and plasma cache are both inherently permissioned because of the data availability problem, which we can talk later on in this podcast. But roll ups are permissionless, which is a very great distinction.
00:15:52.370 - 00:16:27.298, Speaker C: What implication does that have? Because as far as I know, the permissioned aspect of plasma, and this was widely debated once they were starting to get popular, like, oh, we can't have this because it's permissioned. And then other people saying, well, we can't have it because the only thing you can do is censor people. And if you find yourself being censored, you can always exit back to the chain, and you have this whole sequence of events to prevent any malicious behavior. So is it really a problem that they're permissioned, or is it just that it's unneedlessly cumbersome to have them permissioned?
00:16:27.474 - 00:17:11.586, Speaker B: So before we talk about permissioned, and there's a very good question, actually, we should kind of go over some definitions, and in the not too distant future, we'll probably want to define scalability and scaling as we contrast ZK roll ups and optimistic roll ups. But for now, to answer your question is that we should define permissionless and trustless. So permissionless is just, you don't have to ask for permission. So if you have a single operator in a plasma, it would be permissioned in a roll up. If anyone could make a state transition on the roll up, this could take the form of anyone could be a block producer, or anyone can force some state transition, then it's permissionless. And the other facet is trustless. Trustless.
00:17:11.586 - 00:17:43.082, Speaker B: I actually wrote a paper on this while I was at consensus a while back to kind of try and formally model the word trustless, and I came up with two facets. So one is state liveness, and the other one is state safety, where state liveness is basically you consume your state in finite time, and state safety is no one can consume your state without your authorization. So in plain English, state liveness is basically you can move your coins, your coins can't be frozen, and state safety is no one can steal your coins.
00:17:43.226 - 00:17:43.774, Speaker A: Okay.
00:17:43.892 - 00:18:19.946, Speaker B: And we see that if a system has both state liveness and state safety, that it's trustless. Right. As long as you can continue moving your coins, and as long as no one can steal your coins, then you don't have to trust whoever's operating in that system. And I really wish other projects and other analyses kind of use this framework because people throw around words without defining, unfortunately, much too much. So, to go back to your question is that plasma is permissioned, but it can still be trustless. I see, right, is that you can always exit your coins from the plasma even if the plasma operator censors you. Now why is that a problem? Well, it's a good thing.
00:18:19.946 - 00:18:35.898, Speaker B: Mind you, it's better than having it being trusted. Like a side chain. You have to trust a regular side chain. You have to trust that the majority of the block producers on the side chain are honest. Otherwise your funds can get stolen in plasma. You don't have to trust them. You don't have to trust the plasma operators.
00:18:35.898 - 00:19:10.586, Speaker B: So it's better than a side chain. But the problem with it being permissioned is that the operator can force people to go back to the main chain. And going back to the main chain means potentially it's very costly. Right. As we see on Ethereum today, with gas prices at 1000 gigawa not too long ago, this means that many people, if they're forced to go back to the main chain, they just can't afford it, and the main chain can't actually take on that capacity. Imagine if you have a plasma. Apparently these are supposed to scale to a million transactions per second or some other ridiculous number like that.
00:19:10.586 - 00:19:21.870, Speaker B: You have a bunch of people there, you can't really exit them onto the main chain anytime soon. So being forced out of the plasma chain is not a good thing.
00:19:22.020 - 00:19:30.290, Speaker C: Yeah, to put it in practical terms, it could mean like you have to pay $50 even if you manage to get in.
00:19:30.360 - 00:19:30.690, Speaker B: Exactly.
00:19:30.760 - 00:19:34.402, Speaker C: And so it's just impractical for anyone to actually try to do that.
00:19:34.456 - 00:19:59.046, Speaker B: Yeah. And to kind of give some context in this, just to give some numbers for some intuitions. Is that the paper that introduced the lightning network? I think it suggested block sizes, around 300 megabytes for bitcoin to be able to handle even a small amount of global users. Opening and closing channels. Opening and closing channels. Again, plasma, you can think of plasma cache especially. Again, you can think of it like channels.
00:19:59.046 - 00:20:24.690, Speaker B: So imagine the operator kicks you out of the plasma cache. You now have to essentially close all your channels. The side of bitcoin, that would take 300 megabyte blocks to process this. So as you can see, bitcoin's limited capacity definitely wouldn't be able to handle having an off chain system that suddenly stopped working and having all users have to exit back to the main chain. And similarly, ethereum would be way too overcrowded.
00:20:25.050 - 00:20:57.402, Speaker C: Right. So how do you achieve the permissionless aspect in optimistic roll ups without having that? This in plasma comes from, you have sort of a centralized unit that is the block producer of the quote unquote plasma blockchain. And I think I've seen proposals where you can have multiple operators, but it still, at best, just alleviates the problem slightly. You still have the same core problem. So how do you actually get by without that role in optimistic rollups?
00:20:57.466 - 00:21:32.022, Speaker B: You get by by having data availability. And basically all blockchain problems boil down to data availability, which will be a recurring theme in this call. But in plasma, pretend you have two operators, right? You have Alice and Bob. Alice starts and then she creates a plasma block, but she doesn't tell Bob what's in the plasma block. She tells individual users, here are your transactions, and the plasma block is valid, right? So there's no reason for anyone to exit because the plasma block is valid. There's not a single fraudulent transaction in there. But Bob doesn't know what's in the plasma block.
00:21:32.022 - 00:21:51.214, Speaker B: So Bob can't make new plasma blocks. Right. So now Alice is the only block producer. All problems in blockchain boil down to data availability. So in a roll up, and this is both ZK rollup and optimistic roll up, all the block data is posted on chain. So this means that anyone can produce a new block. They have all the data there.
00:21:51.214 - 00:22:10.758, Speaker B: The only thing you need is a system to select a leader. And you can do this any way you want. You could have something like first come, first serve. Anyone just produces a block. The first person who does it is implicitly the leader. It wouldn't be too efficient, but it works. It gives you all the guarantees that you could want.
00:22:10.758 - 00:22:38.122, Speaker B: You could have something like everyone puts up some bond and then you do some round robin, or you do some randomized shuffling with randao. You could add bdfs in there so that the randomized shuffling isn't predictable. And you can do this all on a smart contract in Ethereum. So selecting a leader is easy. In a blockchain, it's basically just take some group of people and you put them in some order. So it's very straightforward. And data being available means anyone can be a leader.
00:22:38.122 - 00:22:41.358, Speaker B: Anyone can be a block producer in the roll up if they want to.
00:22:41.524 - 00:23:00.790, Speaker C: But then the trade off. And the downside of this is you have to put all transactions on chain. How do you compress that? Or do you try to work around it in any way? Or do you just say, throw your hands up and accept that the blockchain size will grow indefinitely?
00:23:01.450 - 00:23:14.714, Speaker B: So there's kind of two caveats here. The first is that we don't necessarily have to use the same transaction model as the base chain. We can use a slightly different transaction model.
00:23:14.832 - 00:23:20.490, Speaker C: You could do basically whatever you want, right, because you're just putting binary blobs in call data, I assume.
00:23:21.230 - 00:23:26.166, Speaker B: Yes, but the binary blobs still have to represent state transitions.
00:23:26.278 - 00:23:26.614, Speaker C: Yeah.
00:23:26.672 - 00:24:05.078, Speaker B: So you can't just do anything. You still need to have sufficient information there so that someone can reconstruct the state of the system and then produce a new block. So you can't just do anything you want. But that being said, you can use a different transaction format. For example, the initial prototypes for ZK rollup used much more compressed transaction just for simple transfers Alice to bob with accounts where instead of using addresses, you just use some index. People register for an account and then you just use an index. And this is different than like a blockchain, for example, where you need a 20 byte address to show that Alice is Alice.
00:24:05.078 - 00:24:45.974, Speaker B: And you can do this in the L2, you can do this in the roll up because we have an underlying layer one that allows us to send transactions from 20 byte addresses to register a four byte index. So it'd be difficult to do this on layer one, but we can do this on L2 by leveraging layer one. So you can do tricks like that. Instead of having a 32 byte value, you could have a four byte value or even an eight byte value. There's not really any reason that you would ever want a 32 byte value like for ether or some token or something. There's really no reason why you would need such so many bytes to represent a value. So you can have reduced representation of this value and so on.
00:24:45.974 - 00:25:24.334, Speaker B: So this allows you to represent state transitions that are different than Ethereum's, using potentially much smaller transactions. So that's one potential gain. The other potential gain is that you can experiment with different ways of aggregating signatures. So in Ethereum, each transaction needs to have a signature in a ZK roll up. The signatures are implicitly in the ZK proof. In optimistic roll ups, you do need to include signature data. But you can do things like BLS aggregate signatures today that reduce the cost to something like one, maybe two bytes per transaction, which is probably even cheaper than a ZK roll ups.
00:25:24.334 - 00:25:27.254, Speaker B: Verifying the ZK proof, does that answer your question?
00:25:27.452 - 00:25:58.426, Speaker C: Yes, you kind of use a bunch of different tricks that you would use if you were to build a token only like a domain specific blockchain that was only for token transfers. If you were to build that from scratch today, you would use a lot of these optimizations. But instead of building a new blockchain, you're doing this as a L2 solution and sort of embedding the block data in this format within the existing blockchain.
00:25:58.538 - 00:26:30.540, Speaker B: Yeah. So to loop back to your question, you're not inherently increasing the size of the history because you can fit more stuff into the same amount of space. That being said, even if you do increase the size of the history disproportionately, it's not really a problem because history is write once, never read as opposed to state, which is you need random reads into the state. Just append. Only history is very cheap. You can even do that on a hard disk, and hard disks are a dime a dozen. They cost nothing.
00:26:30.540 - 00:26:59.806, Speaker B: And the second thing is that storage, or history rather is prunable, so you don't have to keep all the history around forever on every full node. Once you have the current state of the system, you can just discard the history. You can't discard the state of the system because you need the state of the system to produce and verify new blocks. So in that sense, moving stuff from the state of layer one into the history of layer one means that it becomes more sustainable and cheaper.
00:26:59.918 - 00:27:52.526, Speaker C: Yeah, I mean it becomes harder to sync, but really only for the people that will want to try to sync the optimistic roll up and continue producing blocks. Have you thought about syncing strategies for them? Because on the base chain we have a bunch of different proposals from warp sync to whatever else, there's a bunch of new ones. If they ever get implemented that essentially simplified terms, try to break up the state into torrents tree like a chunk tree that you then just sync those chunks. And for the optimistic roll up like block producer, that person still needs to go through all of history to create their internal state to be able to then produce the next block.
00:27:52.718 - 00:28:27.760, Speaker B: Yeah, that actually segues us very nicely into a topic I wanted to discuss, which was comparing and contrasting ZK and optimistic roll ups, the guarantees they provide and the scalability they provide. And it's exactly because of sync strategies and whatnot. So if you don't mind, then we should cover that topic. Sure. Right. First, before we talk about which one is more scalable, or less scalable, or the same scalable, or anything in between, we kind of have to define what scalable is, or scalability. Again, we should start by definitions first, because otherwise we don't know what we're talking about.
00:28:27.760 - 00:29:33.442, Speaker B: So James Prestwich wrote a thread about this, which was prompted by some comments I had made that ZK roll ups aren't actually more scalable than optimistic roll ups, contrary to popular belief, which we'll cover very shortly. Why that is, and hopefully this doesn't make a lot of your viewers very sad, but that's the spoiler alert. So if you don't like this, then skip to the last quarter of the podcast. So throughput versus scalability, and James Preswich wrote a Twitter thread on this. So throughput is just transactions per second. You have a certain number of transactions and you have a certain period of time, and then you just divide the two is the ratio, and then transactions isn't really the best way of wording it. It should really be worded as some sort of unit of work per time, because just transactions doesn't really tell you anything, because a transaction could be very small, it could just be a simple transfer, or it could be very large, it could be doing, going through ten D five protocols to do some flash loans and some minting of gas tokens, and some burning of gas tokens, and anything in between.
00:29:33.442 - 00:30:00.250, Speaker B: So just transactions doesn't really tell you anything. In the sense of ethereum, it should be gas per second. In the case of bitcoin, it could be like v bytes per second, something like that. But we can say TPS just as shorthand. But it should be known that it's not just transactions, it's some unit of work. How to measure that unit of work is actually very tricky, so we'll leave that as an exercise for the reader. So that's the ruput, but that's not scalability.
00:30:00.250 - 00:30:38.006, Speaker B: And unfortunately, a lot of projects have described that as scalability, and have said our blockchain is scalable because it can do 10,000 transactions per second. But again, that's not scalability. So what's scalability? Scalability is transaction throughput divided by the cost to run a full node. And there's a star next to this fully validating node, which we'll cover shortly, because it's not just full validation. So what this means is you can increase the throughput of a system by increasing the cost to run a node. In other words, require more powerful hardware, and this will increase the throughput, but it'll keep scalability the same.
00:30:38.108 - 00:30:42.479, Speaker A: And by cost here, do you mean cost like energy cost? Actually like dollars.
00:30:42.479 - 00:31:09.300, Speaker B: Dollars. It cost me a certain amount of money to buy a full node, to buy the hardware or to rent it. Got it. Right? Yeah. So as an example, that takes us to the extreme. I'll use Solana. Solana requires you to have a really powerful computer, potentially that gets more powerful every year, multiple gpus and all that stuff running in parallel, a 64 core cpu, a bunch of ram and all that stuff.
00:31:09.300 - 00:31:42.770, Speaker B: And sure it achieves potentially high throughput numbers, but that's not scalability, because the cost to run a node is very high. So it's not any more scalable than Ethereum. Now they have made some optimizations which they should be respected for in terms of things like parallel transaction processing, which allow you to make use of currently unused resources that currently ethereum does not use, but that are there just sitting idle. So those optimizations are good, and they should be respected for them. But requiring a $10,000 computer, that's not scalability, right? That's increased throughput.
00:31:42.870 - 00:31:58.274, Speaker C: Yeah, just as you said as well, that you could do a lot of these optimizations on Ethereum, too, right? Like reducing the transaction bit size, et cetera. Yeah, they're good optimizations. And those are the ones that you would make if you built a new blockchain. But you can have them anyway.
00:31:58.472 - 00:32:20.946, Speaker B: They have access lists, for example. You can add access lists on Ethereum. There's nothing inherently stopping you. It just hasn't happened yet. And adding them will allow some measure of transaction parallelization. So now we'll go to the star in the full node, which is that full node ensures that the blockchain is valid. It fully downloads and validates every single block, and it makes sure it's all valid.
00:32:20.946 - 00:32:48.434, Speaker B: The star here is that it also allows you to do something that a light node doesn't really allow you to do. It allows you to produce a new block. And the original thing is that you had mining nodes and then you had SPV nodes. That's kind of like the Satoshi's vision, if you want to call it that. That was described in the paper. But there's no real concept of a non mining node. There was no non mining node until after a while.
00:32:48.434 - 00:33:23.774, Speaker B: So one of the things a full node allows you to do is produce a new block. And this is where things get interesting. Okay, so we'll cover something like, let's say a succinct blockchain. I'm pretty sure you guys have heard of things like an o of one blockchain or a succinct blockchain, where you get a succinct recursive succinct proof, a validity proof that all previous blocks are valid. This doesn't tell you the four choice rule of the chain, but it at least tells you that all the blocks are valid. So here's the thing. Let's say you have a system and all your full nodes are just doing that.
00:33:23.774 - 00:33:45.454, Speaker B: They're just verifying this zero knowledge proof. We note that the zero knowledge proof tells us that the chain is valid, but it doesn't tell us what the state is, right? It tells us, here's a state route. The state route was created through valid transitions with valid signatures and all that. But it doesn't tell you what the state is. So this node couldn't actually produce a new block.
00:33:45.502 - 00:33:49.810, Speaker A: Is there a way that they are actually writing this somehow, separately on chain?
00:33:49.970 - 00:34:00.520, Speaker C: No, it's not on chain. It's all distributed. The whole stateless concept is you don't store it on chain. You just pass it around off chain and hope for the best.
00:34:00.890 - 00:34:28.818, Speaker B: Yeah. So this isn't the ZK, Rob. This is some hypothetical chain that operated through every block had a recursive zero knowledge proof of its validity in all previous blocks. What I'm describing is coda, or what used to be called coda, right? So you have this chain, okay? So you have your full node, you validate the zero knowledge proof. Great, it's valid. But you can't produce a new block. So what do you have to do to produce a new block? And this is a question for you.
00:34:28.824 - 00:34:31.570, Speaker C: Guys need to download the past data.
00:34:31.720 - 00:34:37.140, Speaker B: Great. You have to download the past data and make sure it's available. And then what else do you have to do with it?
00:34:37.590 - 00:34:38.786, Speaker C: Re execute it.
00:34:38.888 - 00:34:44.790, Speaker B: Yes, you have to re execute it. I'm wondering if you've been reading the stuff that we've been writing or if this was known all along.
00:34:44.860 - 00:34:46.454, Speaker C: This is known all along.
00:34:46.572 - 00:35:15.210, Speaker B: Okay, good. Yeah. So you have to re execute it. Right. And what this means is that, again, scalability is the throughput divided by the cost to run a full node star, where the star is not a full node, that's just validating transactions, it's a full node that can produce new blocks. Then we note that the zero knowledge proof doesn't provide any scalability. You still have to fully download all previous blocks, and you still have to execute every single past transaction to get to the latest state so that you can produce a new block.
00:35:15.210 - 00:35:22.306, Speaker B: So to kind of bust some myths is that there's no such thing as nol one blockchain. Unfortunately, as much as we wish there.
00:35:22.328 - 00:35:46.780, Speaker C: Was, in theory, you could. I mean, I'm not sure if they actually attempt to solve it this way, but in theory, you could just pass the current state around. And then the zero knowledge proof also proves that this particular state route is the correct one. And then with the state in hand, like, you have to download the state still, but you have a proof that this is the correct state route, and then you can proceed from there with the transactions that you've been given.
00:35:47.230 - 00:36:36.326, Speaker B: Yeah, you could do that. But note that that's a stronger assumption than any other blockchain makes, right? Like bitcoin, for example, and even ethereum, they don't assume that you can download the full state from any node, they give you the option of reconstructing the state yourself by syncing the chain's history only. And this is a kind of a fundamental assumption of current blockchains. They don't assume that you must be able to fully download the state of the system, they only assume that you only have access to the history. And of course there's reasons for this, as I said before, is that history is really cheap and state is really expensive, and state can potentially be quite large. Like in Ethereum, the state is almost as big as the history. So obviously you can make certain assumptions, and you can make certain stronger assumptions than currently exist in blockchains.
00:36:36.326 - 00:37:23.366, Speaker B: But then that begs the question, if you make these assumptions, then do you really need the zero knowledge proofs? One example is if you make the assumption that there exists someone that is fully validating the system and that has access to the current state, why can't you just assume that they'll give you a fraud proof? If such a person exists, then they should be able to give you a fraud proof, at which point you don't need all this complexity of validity proofs, because you have this mythical person that's validating these tens of thousands of transactions per second that can give you the whole current state. They can also construct a fraud proof. It's trivial for them to do if they do all that work, right? So we see this kind of counterintuitive result, which is that zero knowledge proofs, contrary to popular belief, don't actually provide any scalability or security advantages over fraud proofs.
00:37:23.478 - 00:37:31.180, Speaker A: Isn't there a proposal for some joint ZK optimistic roll up or something where you'd actually make use of the properties of both?
00:37:31.710 - 00:38:34.370, Speaker B: Yes, I don't remember the order, but I think it was you submit the ZK proof and then you only verify it if it's challenged or something along those lines. I don't remember. But yeah, there's some suggestions around there. So to kind of go back to this throughput and scalability and zero knowledge proofs versus not this hypothetical blockchain that's supposedly all of one but isn't, you do have to be able to fully download all the history and you need to be able to reconstruct the current state. And again, if you assume that the current state can be given to you by someone in the network, that there's someone willing to potentially, because supposedly using validity proofs means you can have a whole bunch of transactions per second, tens of thousands, hundreds of thousands, because there's only one person creating proofs, right? So if you have this mythical person that has a supercomputer capable of constructing the current state and giving it to you at any time, they can just construct fraud proofs and give them to you at any time. So there's not really any assumption model under which zero knowledge proofs actually have higher scalability and more security than fraud proofs.
00:38:34.950 - 00:38:57.346, Speaker C: It's sort of probabilistic finality versus guaranteed finality in consensus systems like ZK is guaranteed finality. Once you're given this proof, it's correct. I assume everything is correct. I can proceed from there. In a fraud proof system, I have to give it some time. How much time is a little bit of like a variable?
00:38:57.538 - 00:39:30.350, Speaker B: That is a very good point. So I want to make clear that when I say that zero knowledge proofs aren't more scalable or more secure than fraud proofs, that doesn't mean that zero knowledge proofs have no advantages. So they do actually have advantages. And you brought up a very good one, which is latency. I wouldn't call it to finality, but latency to on chain interactions. The reason being is that in an optimistic roll up, you commit to an optimistic roll up block, and if it's invalid, then it can be reverted with a fraud proof within some timeout. But if it's valid, it can't be reverted.
00:39:30.350 - 00:40:21.918, Speaker B: So it's guaranteed from the point of view of the on chain contract to eventually finalize. But a user can fully validate the optimistic roll up off chain and then see that, okay, all these blocks are valid. They build upon valid blocks, then it's immediately final. But from the point of view of the on chain contract, the on chain contract obviously can't fully validate an optimistic roll up, otherwise that defeats the whole purpose, then you might as well just run it in the contract. The on chain contract needs to wait a long period of time, and potentially much longer than like a normal peer to peer fraud proof, because on chain there might be congestion and stuff. So you're correct that ZK proofs have the very good advantage that they allow you to have much lower latency when it comes to on chain interactions. So once you submit a ZK roll up block, then you know it's valid, you can immediately withdraw your funds.
00:40:21.918 - 00:41:20.040, Speaker B: In the case of fungible assets, this isn't a very big distinction, because the fungible asset can just be withdrawn with the help of a liquidity provider. But it is very useful in the case of things like cross chain interactions and for doing things like withdrawing non fungible tokens or nfts, because obviously you can't have a liquidity provider for an NFT by definition, because it's just not fungible. So that is one place where zero knowledge proofs are better than fraud proofs, which is know lower latency for on chain interactions. They do have higher latency for committing the block for the first time, which is a disadvantage, but that's kind of orthogonal. So they have one other advantage. And I did bust potentially a lot of people's hopes when I say that zero knowledge proofs don't provide scalability or security, or better scalability or security. But there's another thing which they're potentially good at.
00:41:20.040 - 00:42:29.718, Speaker B: One thing that I kind of glossed over is that when you reconstruct the state of the system so that you can produce a new block, you don't actually have to fully execute every single transaction, you only have to apply every state transition. So imagine you have like a uniswap contract. Maybe this isn't the best example, but some trade, some trading system, right? It might have to look up a bunch of prices internally, but the end result is they're just a movement of tokens from one person to another, or from two people between each other. In the scenario where you want to reconstruct the state of the system, of a system that has zero knowledge proofs. So in such a chain, to get the state of the chain, you don't actually have to execute every transaction, you only need to apply every state transition. Now, if the blockchain only does state transitions, like if it only does simple balance transfers, then we can see that state transition. If that's all the work you're doing, if all you're doing is just transferring balances, then clearly using validity proofs doesn't have any advantage over not using validity proofs, because all the work is in the state transition.
00:42:29.718 - 00:43:22.134, Speaker B: But if your system is very complex and allows for complex smart contracts that potentially read a lot of state elements, right, like it goes in there and reads 100 state elements, then does one simple state transition, then that is an area where zero knowledge proofs are useful. Which kind of brings us back to why they're originally created. It's to provide a succinct proof of some computation. So using it in the sense of scalability that has been bastardized in the modern ZK roll ups and all this other all of one blockchains and stuff is incorrect. But if you use it the way it was originally intended, where you provide a succinct proof of some computation happening and that computation could be very large. Then that is where using validity proofs for scalability has an advantage, because that's what they were built for. You can have very, very large transactions.
00:43:22.134 - 00:43:58.102, Speaker B: Imagine a transaction that consumes a billion gas but only has one small state transition just moves a balance. That's all it's doing. Then of course, using validity proofs does provide you scalability. Unfortunately, such a system doesn't really exist nowadays because having large general purpose computations inside some ZK rollup is just not something that exists today, at least not in an open source, publicly reviewable way. It may in the future. There's some advancements being done here. There's, I think, zinc on the Zksync side, there's Cairo from the star core guys.
00:43:58.102 - 00:44:04.550, Speaker B: So maybe we'll see some interesting things happening on that front. But for now at least, it doesn't really seem like those exist.
00:44:04.710 - 00:45:32.290, Speaker C: I want to take a step back because I think we've dove into describing, I think, everything and how this works, but I want to take a step back and sort of ask the question of where fundamentally does the added scalability come from, quote unquote added scalability. Because it seems to me that if we just take plain ethereum, it is not scalable because it is acting on a global state with a bunch of different miners and full nodes across the board have to re execute all these transactions all the time and it causes a bunch of problems. But what you do in an optimistic roll up, and to some degree in a CK roll up roll up in general, is you are kind of creating a secondary system in which you have fewer block producers that can run faster because you've optimized the underlying data structures in how you act on this roll up. But you still fundamentally put the transactions on chain. You still fundamentally have to build that state in the roll up block producers. And so aren't you just like sort of creating a secondary blockchain within the call data of the first? And the scalability is sort of you're reducing the set of people who are computing.
00:45:32.450 - 00:46:17.906, Speaker B: That's almost exactly correct except for the number of nodes. But everything else that you said is entirely correct, that there seems to be something fishy here. The system I described, the reason that ZK roll ups is not more scalable or secure than optimistic roll ups or validity proofs versus fraud proofs, is that you still need to fully in the worst case, not in every case, but in the worst case. And unfortunately we have to metric our systems based on the worst case because blockchains are adversarial environments. In the worst case, you still need a user to potentially run a full node to reconstruct the state and essentially do full validation, if you want to call it that. So if you have that system and you just put all your transactions in a roll up, it doesn't seem like you're getting any scalability. Why would you? You just have bigger blocks, essentially, or you have the same size blocks and you have no scalability.
00:46:17.906 - 00:47:24.902, Speaker B: And the answer is you're correct. Which is a very bizarre result from someone working in the roll up space would tell you, but you're entirely correct that roll ups don't provide scalability on their own. If you have an EVM and you put it inside an optimistic roll up, so you have an EVM native optimistic roll up, or potentially, in some hypothetical scenario, ZK roll up, then that roll up would still be limited to 15 transactions per second. Unless you want to increase the cost of running a full node, or unless you want to add stronger trust assumption, unless you say, well, the users aren't going to run full nodes, we trust there's some supercomputer out there that can generate fraud proofs, right? But if you really want to make that assumption, why don't you just do that on Ethereum? Right? So clearly there's no scalability to be gained here. So where do we get scalability? So there's two ways, and James Petriz covered this in his thread, which everyone should read if you're in blockchain. So the key things that rollups give you is there's two things. One, it allows you to choose different execution models that potentially are more scalable.
00:47:24.902 - 00:48:13.558, Speaker B: One example is what we're doing at fuel. Instead of using the EVM, with all its problems, we're using a Utxo based data model that allows for parallel transaction validation, and that doesn't have the same state lookup bottlenecks that ethereum has. So this means on the same computer you get more transactions, you're going to get more transaction throughput. So that's an increase in scalability. So essentially you can think of it like segmenting the block space of ethereum, where block space isn't just in bytes, it's, let's say gas. So if half of ethereum blocks used ethereum, half of them were for fuel, it'll be able to process more than the current transactions per second, assuming we don't raise the current gas limit, right, because we use a different execution model for fuel. The other way you get scalability potentially is by creating segmented trust boundaries.
00:48:13.558 - 00:48:50.840, Speaker B: So the example is if there's two roll ups and you only use one roll up, you don't care what happens on the other roll up. Right? So this means that users of one system could get access to the full 15 transactions per second. Users of the other system could get a full 15 transactions per second. And assuming most users don't use both systems, then now you have 30 transactions per second across both of them. And this should sound familiar because that is essentially what sharding provides. Right? Roll ups are very similar in many ways to shards, which should hopefully segue into our next question.
00:48:51.370 - 00:48:59.334, Speaker C: Yeah, this is exactly heterogeneous sharding. E g. What polka dot does e.
00:48:59.372 - 00:49:06.154, Speaker A: Two is supposed to do eventually, but can't yet. This is so weird. I didn't realize that. I didn't think about it that way.
00:49:06.272 - 00:49:24.462, Speaker C: Yeah, let's go into the e two discussion, because e two still doesn't really want to provide heterogeneous sharding. It still wants to provide homogeneous sharding, where you still have the same execution model on all shards. So how do you bring this model into e two and make it even better there?
00:49:24.596 - 00:50:13.742, Speaker B: Okay, so there was a recent post by vitaliputerin sometime in early October about just this topic, which is a roll up centric view of serenity. Unfortunately, it's commonly called e two, but I disagree with calling it e two because that assumes the conclusion it's a separate blockchain that has nothing to do with Ethereum, except there's a few guys there that also worked on Ethereum. It should really be called serenity. This post described a new roadmap for Serenity that was more roll up focused. So rather than having sharded execution as they were originally planning for phase two, they say, okay, let's drop this. Let's just have sharded data availability, which, mind you, has some problems that we can discuss shortly. Let's just have sharded data availability and allow roll ups to use this available and order data to just run.
00:50:13.742 - 00:51:07.954, Speaker B: And the roll ups can run on, for example, eth one or Ethereum. So we kind of see that roll ups do, as we just discussed, do provide some of the same properties of sharding, right? You can run as many roll ups as you want. The roll ups can have different execution models. The only constraint is that the execution model must either be expressible in some zero knowledge proof way, or you must be able to construct a fraud proof that can be interpreted in some way in the EVM if not, then you can't really construct a fraud proof. So those are kind of the constraints, and they are actually non trivial constraints. Not having access to native code and only having access to the EVM are non trivial constraints. But you can still do basically all of DeFi that you could want, all of payment transfers, utxos accounts, anything in between.
00:51:07.954 - 00:51:42.618, Speaker B: You can basically do anything you want. So in this way, Serenity could provide a large amount of data availability throughput. And then roll ups could just run on Ethereum and essentially act as shards. You could have 64 roll ups running, just as an example. And then if a user doesn't want to validate every single roll up because they only use one, then they just validate one. Right? And then you can have some way of having some committees around for these 64 roll ups that again are just leader selection. And you can do leader selection in some smart contract on Ethereum.
00:51:42.618 - 00:51:44.430, Speaker B: It's not inherently complicated.
00:51:44.930 - 00:52:07.666, Speaker A: In what you've described, would you have identical ZK roll ups? Or do we imagine each one of these having this unique property? For example, would there be many ZK syncs, many fuel, many? Like, would there be all of these? I know it's not ZK roll up, but all of these other roll ups, would it be multiples of them? Would it be singular?
00:52:07.858 - 00:52:35.694, Speaker C: I think there would have to be. I mean, the added scalability as described comes from domain specific optimizations. Basically saying, we know in this context we only deal with balance transfers, so we can make the most optimal thing for balance transfers, and then you can make another one that's optimized for nfts or whatever. Right. And so I think that's where the scalability comes from. So you'd have to have domain specific roll ups. Yeah.
00:52:35.732 - 00:52:38.382, Speaker B: So you have domain specific roll ups and you have multiple of them.
00:52:38.436 - 00:52:52.914, Speaker A: Multiple of each one, yeah, because that's the question, is it sort of like, do you have to multiply it? In the example that you had given, you did say that there would be like two roll ups basically running at the same time, which do the same thing. So do we imagine it that way?
00:52:53.032 - 00:53:35.746, Speaker B: The current imagining of the researchers in the space is that there should be multiple roll ups running. Now, if you're asking multiple roll ups of the same kind, like let's say multiple fuels or multiple ZK syncs or multiple hermes, then the answer is, maybe there's no inherent reason not to, especially if they're constrained. Like, let's say you have zksync, which does just as payments, right? Let's say for now. In that case, having two of them doesn't really affect anything because you don't really need atomic composability across payments, right? So you can just have two of them and you don't really lose anything. But it potentially provides you with conditional scalability. Depending on if users use one or.
00:53:35.768 - 00:53:45.310, Speaker C: Both, you kind of need composability and payments, like in the train and hotel problem where you want to pay for two things, but conditionally that both succeed.
00:53:45.390 - 00:54:07.386, Speaker B: Yeah, if they both interact with smart contracts then yes, but I'm saying if it's just like a payment for something that's off chain, then it's not really too important for things like trades or for more composable interactions, then yes, of course you would want to put them in the same execution system. But if it's just like a payment for something that's off chain, then it doesn't really matter where it is, just as long as they accept it.
00:54:07.488 - 00:54:32.298, Speaker C: Sort of back to the same sharding methods where you would have problems perhaps interacting with people on the other roll up. Like if Alice and Bob are on two different roll ups and they want to send money to each other, then either Alice or Bob has to create an account on the other roll up and can't send directly between them, which might or might not be a problem. Depends on how you shard this and how users interact in reality.
00:54:32.394 - 00:55:21.378, Speaker B: Yeah, obviously there needs to be a lot of application development that happens for cross roll up interactions, just like there would be for cross shard interactions, just like there would be for cross chain interactions. So yes, there needs to be a lot of work done there, but it's not impossible. And some interesting stuff that's happening at the forefront of this is Barry Whitehat is also kind of pushing this is doing things like public key registries, if you want to call it that, or count registries, so that all roll ups or all roll ups that conform to some standard can share a common registry of accounts. At that point, account registrations are shared across multiple of these roll ups, which means know Alice can send to Bob. Bob doesn't really need to create an account on the second roll up, he just needs to have an account registered on any roll up stuff like this. But yeah, there does need to be a lot of work done.
00:55:21.464 - 00:55:27.374, Speaker A: Fascinating. Just imagining that starts to get really wild, the cross roll up interaction.
00:55:27.502 - 00:55:52.410, Speaker C: Let's talk a little bit about the data availability because I'm also just a quick question before we dig in. You already teased there are some problems with sharded data availability. But even before then there is this proposal on the table. I think it's been accepted. I don't really follow the details anymore, but to reduce the cost of call data. So how much does the cost of call data actually affect roll up schemes?
00:55:52.850 - 00:56:14.878, Speaker B: Right now it's actually quite significant because it's 16 gas per byte, and if you kind of do the math, it ends up being that in a lot of cases it's cheaper to store something in state and then just doing an s load later than it is to provide the pre image to some hash in call data. It's actually a very significant blocker.
00:56:14.974 - 00:56:21.142, Speaker C: Right. So that's why that proposal is in. I assume it's going to be accepted and put into practice at some point.
00:56:21.276 - 00:57:08.726, Speaker B: So I actually wrote a proposal to decrease the cost of call data. Then I think there might be some other soft proposals floating around. I want to be optimistic and imagine that it'll be included at some point. But the realistic me says that there is opposition from certain Ethereum developers such as Peter from Geth that really don't want chain history to grow, despite the fact that, as I said above, you can do things like pruning and whatnot. And chain history is actually really cheap. So I don't actually think the cost of call data is going to be decreased anytime soon on Ethereum, unfortunately. And part of this is because Ethereum doesn't really have any good scheme for doing anything better than just full replication of the data.
00:57:08.726 - 00:57:23.850, Speaker B: If they had some way of doing error correction or error recovery, then potentially you could do better because you wouldn't need full replication of all the history. You could do partial replication. Which leads us to our next topic.
00:57:24.750 - 00:57:39.870, Speaker A: This is flowing so well. So I guess next up is what we've teased this data availability. Let's go into this. You've sort of explained the issue of it, or the lack of it in certain cases. What are you doing about that?
00:57:40.020 - 00:58:09.114, Speaker B: So this is kind of the singular problem of all blockchain, such as data availability and ordering. If we have those two, then we have everything. So let's kind of do a deeper dive into the problems that it causes. So in the form of channels, for instance, you don't have a globally available ordering on channel updates. So you can't do things like what, you know, Alice sends to Bob. Alice and Bob have a channel. And then Alice signs the channel over to Charlie and know says I'm no longer part of the channel.
00:58:09.114 - 00:58:38.914, Speaker B: You can't do that without a global ordering. Another thing that data availability, the problems that it causes is things like plasma being permissioned. Someone can create a plasma block and just not provide the data behind it. It could all be valid and they just don't provide it to any one party. Then no one can create a new plasma block. It becomes permissioned. Another thing is, in the case of, say, a succinct blockchain or ZK roll up, even right in a ZK roll up, you still need to provide all the transaction data.
00:58:38.914 - 00:59:35.634, Speaker B: And the reason you need to do this is if you didn't have this, someone could move the state over. In this case, you know, the new state is valid because there's a validity proof for it, but no one can produce a new block. And not only can no one produce a new block, which maybe you say that's like a philosophical problem, right? But the issue is no one even knows what their coins are. If no one knows what their coins are, they can't spend them so effectively by moving to an unknown state that is valid, but unknown, everyone's coins are burned, right? So it creates all these problems. And the solution, the naive solution is, well, you just download everything, which is obviously a terrible solution, because downloading everything means, especially if everything is in history, it means that you're doing a whole bunch of work. So can we do better? And the answer is yes, we can. Which brings us into how lazyledger is doing things and how serenity is doing this potentially sharded data availability.
00:59:35.634 - 01:00:32.680, Speaker B: So kind of a brief two minute description of the solution to the data availability problem, which is, well, let's take this data, let's erasure code it. For those of you who are not familiar with erasure coding, it's a way of doing error correction. So you have some data, and then in the simplest sense is you grow the data to twice as big. So if you have, say, 1 data, now you have two megabytes of data plus parity data, like original data plus parity data, it's two megabytes. And as long as you can recover 1 mb, any 1 this two megabytes, then you can recover the original data and by extension you can recompute the parity data. There's some more nuances in there and some parameters you can change, but that's kind of in the simplest sense. So if we have such a scheme, what you can do is, well, you just random sample into this parity plus original data.
01:00:32.680 - 01:01:13.570, Speaker B: Every time you do random sample into there, there's a 50% chance in this naive construction just to get some intuitions, there's a 50% chance you land on some data that is there, that is available. But the block producer, like this malicious block producer, might be trying to hide more than half of it so that the original data is completely hidden. So very high level intuitions is every time you do a random sample, there's a 50% chance you get tricked. That's the intuition. So if you do multiple samples, the chance that you are tricked decreases exponentially. It's 50%, then you do another sample. Now it's 25%, and so on.
01:01:13.640 - 01:01:19.766, Speaker A: If you've been able to sample and it has not shown problems, then the likelihood is lower.
01:01:19.868 - 01:01:26.358, Speaker B: Yes. Then the likelihood that you have been tricked decreases exponentially with the number of samples. And exponential decrease is important.
01:01:26.444 - 01:01:27.414, Speaker A: I understand.
01:01:27.612 - 01:02:10.630, Speaker B: So, doing this scheme requires committing to the erasure coded version of the data and using the 2d scheme. The one I just described here is the 1d. But just to build some intuitions. But the 2d scheme requires a commitment to square root of the block size amount of data, because you need a square root number of rows and a square root number of columns because they're two dimensional. So in total, you need twice the square root of the block size amount of data. So there's some overhead here. But what this means is once you've done this erasure coding scheme, a node can be convinced, and this is a light node can be convinced, that the block data is available by doing a fixed number of samples into it.
01:02:10.630 - 01:02:56.178, Speaker B: Because again, every time you do one of these, you have a 50% chance of being tricked, and that decreases exponentially. So the block could be 1gb, it could be 1, could be one petabyte. You only have to do a fixed number of samples. The only overhead is square root, square root of the block size that must be committed to for each block. So what this means is we have a way of nodes convincing themselves that data is available by doing sublinear work. They only need to download the square root number of commitments, and then they have to do a fixed number of samples so they don't actually have to fully download the block to convince themselves. With very, very high guarantees, very high likelihood that the block is, in fact, available.
01:02:56.178 - 01:03:09.254, Speaker B: Okay. Which is revolutionary. It means that you no longer have to fully download blocks. You can actually do sublinear work, which means blocks can now be made, they can be squared as much large.
01:03:09.452 - 01:03:15.450, Speaker A: And is this what lazy ledger does? You sort of described it before as like, it's an l one in itself.
01:03:15.600 - 01:03:42.286, Speaker B: Yes. So I can cover both lazy ledger and the serenity proposal in this context. So lazy ledger basically does this and it only does this. The only thing it does is it takes data blobs, just zeros and ones. It doesn't execute them, it doesn't interpret them, it just puts them in a block, erasure codes the block and then passes it around. So the only thing you need to do to ensure a block is valid is actually to ensure it's available, because again, there's no execution.
01:03:42.398 - 01:03:50.390, Speaker A: But is it taking, when you describe this, it's not taking this information from another blockchain. It's in itself doing this.
01:03:50.460 - 01:04:19.866, Speaker B: Yes, it's in itself a blockchain. But the nice thing is, any blockchain can make use of this service. For instance, you could take bitcoin blocks. Take that 1. Just put it on lazy ledger as a blob. Lazy Ledger doesn't interpret it as a bitcoin block, but it means that some application running on top of lazy ledger, say like a bitcoin virtual sidechain, full node, could read out all these bitcoin blocks from lazy Ledger and essentially reconstruct the bitcoin blockchain without having to fully download every bitcoin block.
01:04:19.978 - 01:04:20.638, Speaker A: Interesting.
01:04:20.804 - 01:04:51.420, Speaker B: Yeah, so it allows you to have data availability with sublinear cost, which means that you can have a large amount of it. The sublinear cost means that you can have a huge amount of it. Execution always has to be linear, right? Or at least not execution, because as we said, you can do some tricks with zero knowledge proofs, but at least the number of state transitions needs to be linear. You can't avoid not executing something but data availability. You don't have to fully download something to ensure it's available. You only have to do square root work.
01:04:51.790 - 01:04:56.346, Speaker A: I'm trying to imagine how would an application interact with it. Exactly.
01:04:56.528 - 01:05:48.742, Speaker B: Okay, so with lazy Ledger we have a special optimization called the namespace Merkel tree. And this is not something that serenity has. But contrast the two systems, is that every piece of data in the lazy ledger block is associated with some namespace, some application namespace, and then each application essentially just describes its own namespace. So like you could have bitcoin, for instance, it calls itself some virtual bitcoin, let's pretend, and calls itself, oh, I'm namespace twelve, right? So it goes through and it downloads only the pieces of the lazy ledger block that have namespace twelve. And you can provide efficient Merkel proofs to show that just these pieces of the lazy ledger block are for namespace twelve. So you don't have to fully download every single lazy ledger block to extract the application messages.
01:05:48.806 - 01:05:52.698, Speaker A: Do you imagine this functioning though in an l two context as well?
01:05:52.784 - 01:06:34.150, Speaker B: Yeah, so it can help with l two s as well. You can build things like roll ups and optimistic roll ups and DK roll ups and side chains if you even want to. And use lazy Ledger as a shared data availability layer to provide shared data availability for all these systems. For instance, Cosmos zones, instead of having each Cosmos zone, have to have a very strong validator set to prevent corruption. You just put all the blocks for the zones on lazy ledger, and if they're not there, then they're invalid. And if they're there, then if something is invalid, you can construct a fraud proof or a ZK proof. So lazy ledger can be used by Annie and all other blockchains as a shared data availability layer.
01:06:34.150 - 01:07:25.354, Speaker B: And since it only does data availability, it can provide much higher throughput of just data than any other system in the world. Contrasting with serenity and the new roadmap Vitalip Buterin. I think he posted a picture very recently, or he also gave a short talk at the ETH online or ETH global summit, which was like a graph and it's showing if you just do sharded data availability or just data availability in general, you can get the throughput off 25,000 transactions per second. But if you do sharded execution, the throughput decreases to like 1000 transactions per second. So that shows that if you just do data availability and you don't do any execution, you can have much higher throughput off. The critical thing that you need, and the critical thing that you need is data availability and ordering. Once the data is available, once it's ordering, you can build any application you want.
01:07:25.354 - 01:07:28.330, Speaker B: On top of that, you just read out the data from the right namespaces.
01:07:28.990 - 01:07:59.720, Speaker C: I find this fascinating because I have my own internal worldview and everything, but to me you're literally just describing Polkadot, where namespaces are another name for parachains. And what is the exact ledger? That's a relay chain. And so it's sort of coming to the same thing from two completely different angles kind of thing.
01:08:00.250 - 01:08:22.430, Speaker B: Yeah, I mean, ultimately all these sharded blockchains try to solve data availability in some way or another, while also providing things like guarantees around execution. But ultimately what they really want to try to do is to increase the data availability throughput. Are there similarities? The answer is yes, of course there's similarities between lazy Ledger and all sharded blockchains.
01:08:22.850 - 01:08:31.498, Speaker A: Also, beacon chain I don't know if that's still there, but was the beacon chain also supposed to act a bit like this? Lazy ledger construction?
01:08:31.674 - 01:09:14.618, Speaker B: Not exactly. So the beacon chain is responsible only for shuffling the validator set. It doesn't really do much with the shards. That being said, one issue with the serenity design of sharded data availability is higher overheads and more brittleness in the consensus mechanism. But specifically the higher overheads is the current serenity design is basically you have one big block, you segment it into 64 smaller blocks and then each of these smaller blocks you do the erasure coding like I described to get the square root number of commitments and so on, and you do some sampling. But the trick is, as I described earlier, the overhead for each block is square root in the block size. And the number of samples you need to do is constant.
01:09:14.618 - 01:09:49.442, Speaker B: Right. But you have to do a number of samples per erasure coded data. So if you take one big block and segment it into 64 smaller blocks, you're now increasing your overhead 64 times and you're not really gaining anything like ultimately you still need to do data availability checking. And every single node needs to check the availability of every shard. In this example, all the data across all the blocks needs to be checked for availability. So in that sense, there's not really an advantage just splitting up one big block into 64 smaller blocks. You're just increasingly overhead.
01:09:49.442 - 01:09:52.730, Speaker B: You might as well just have one big block and just do a razor coding over the whole thing.
01:09:52.800 - 01:10:24.206, Speaker A: Interesting. I hate to sort of wrap up here because I feel like we could probably continue to chat about the comparison between these systems and get into the nuance even deeper. But I want to say thank you so much for coming on the show and sort of taking us on this journey through fuels work on optimistic roll ups. Its comparison to ZK roll ups, like how kind of, it's turning the ETH one construction into a sort of semi sharded environment and lazy ledger. This is really. And the data availability problem. This is really interesting.
01:10:24.388 - 01:10:24.734, Speaker B: Yeah.
01:10:24.772 - 01:10:25.686, Speaker C: Thanks very much.
01:10:25.788 - 01:10:27.686, Speaker B: No problem. Thank you for having me.
01:10:27.788 - 01:10:30.866, Speaker A: Yeah. And to our listeners, thanks for listening.
01:10:30.978 - 01:10:31.730, Speaker C: Thanks for listening.
