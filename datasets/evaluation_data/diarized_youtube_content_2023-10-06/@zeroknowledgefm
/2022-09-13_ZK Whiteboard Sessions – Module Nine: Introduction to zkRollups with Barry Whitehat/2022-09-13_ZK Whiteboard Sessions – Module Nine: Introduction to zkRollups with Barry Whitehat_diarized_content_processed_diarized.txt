00:00:11.510 - 00:00:29.522, Speaker A: Hello, everyone. My name is Bobin. I'm Prologon, and we're here today recording another episode of the ZK Whiteboard sessions. And here with me today is Barry Whitehead from the Ethereum foundation. And we will be talking about ZK rollups. So I guess the first question, Barry, in general, what is a roll up and why do we need it?
00:00:29.656 - 00:00:32.414, Speaker B: Right. So a roll up is a way to scale blockchain.
00:00:32.542 - 00:00:32.882, Speaker A: Okay?
00:00:32.936 - 00:00:58.486, Speaker B: And we need it because we need to. Because blockchains are really, the amount of transactions they include is quite low. So the way that blockchains work is that there's always this commitment, there's always this state. And the state is basically like, how much money everybody has. So this is like one person, this is another person. This person has like, ten, and this person has, like nine. And then every time we make a new block, we have a state transition.
00:00:58.486 - 00:01:31.646, Speaker B: Right? And we move from this state to a new state. So let's see what the new state looks like. Okay, so this is a new state. And we updated these two leaves. So what's the balance of these two leaves? Okay, so this leaf, let's call this one nine, and let's call this other 110. Okay, so basically what's happening here is like, this user is sending one something to this other user, and this is called a state transition. And the way that blockchains work is that this state transition is validated by all of the nodes.
00:01:31.646 - 00:01:40.018, Speaker B: So there's many, many observers, and they all observe this state transition. And when you see the state transition, you say, oh, this is a correct state transition.
00:01:40.114 - 00:01:46.054, Speaker A: And when you say observe, they actually, I think, re execute the transactions to get the same exact state transition.
00:01:46.182 - 00:01:46.618, Speaker B: Exactly.
00:01:46.704 - 00:01:46.954, Speaker A: Okay.
00:01:46.992 - 00:02:02.942, Speaker B: Yeah. So the way that a blockchain works is that we have this state transition. Everybody is observing and confirming that it's correct. If someone presents an invalid state transition, all the users will ignore it. All the users be like, oh, that's not correct. I don't want to be part of that. I want to be part of the correct one.
00:02:02.942 - 00:02:29.910, Speaker B: And this is how blockchains work. And this is why they're secure. So they're secure because everyone is checking. But the problem is that when we have a state transition and we have many users to check, we want to make the state transition as simple as possible so that we maximize the number of users that can check. The way that we do that is that we just reduce the number of transactions that we have here. We have just one transaction, and this makes it easy for us to validate.
00:02:30.330 - 00:02:38.810, Speaker A: So if we wanted to put a million transactions into a block, some participants in the network would just not be able to keep up to validate all of them.
00:02:38.880 - 00:02:59.130, Speaker B: Exactly. Yeah. But what we do with roll up is we want to put a million, five texts into a block. So let's talk about how we do that. So with a roll up, we have the same situation, but instead of having everybody check, we have something that guarantees the validity. This can be a ZKP or it can be a fraud proof.
00:02:59.210 - 00:02:59.694, Speaker A: Okay.
00:02:59.812 - 00:03:38.350, Speaker B: And basically what happens is that the ZKP makes sure that all of these transactions were done correctly. And this gives us our validity guarantee. But we also need to have a data availability guarantee, because if users don't know the new state of the system, they won't be able to make more transactions, they won't be able to withdraw. And these are two important issues. So in order to guarantee that our data is available, we make sure that our proof also comes with a representation of all the transactions that happen. So let's take an example here. We have a transaction where this person sent $1 to this person.
00:03:38.350 - 00:04:03.494, Speaker B: So what's the simplest representation of that? The simplest representation of that is the. To the from and the amount that was transferred. Right. So we say, okay, so from user zero, from user zero to user one, we transferred one. Okay. And with that information and the previous state, you're able to calculate the new state.
00:04:03.692 - 00:04:04.342, Speaker A: Makes sense.
00:04:04.396 - 00:04:04.950, Speaker B: Yeah.
00:04:05.100 - 00:04:18.742, Speaker A: Now, usually when you send transactions, you sign them. Right. There is also a signature involved. And this is when we are talking about l ones. On l two. Is there a signature as well when we're talking about roll ups?
00:04:18.806 - 00:04:51.060, Speaker B: Yes. So in the context of roll ups, it's really important that users consent to this transaction because on layer one, everyone signs to say, oh, I want to send this much money from me to this other person. So it's important to do that on L2 as well. And we do that exactly the same way. We have signatures. We have signatures, and we use those signatures for users to consent to say that, oh, I want to send one token from me to this other person.
00:04:51.670 - 00:05:12.262, Speaker A: Makes sense. Makes sense. Okay. So to kind of make sure I understood everything, in a Zk roll up, there is a separate state. So we still have the l one state, which is separate. And then there is an l two state, which is now this represents it. And then the validity aspect, it proves that the l two state was transitioned correctly.
00:05:12.326 - 00:05:12.650, Speaker B: Yes.
00:05:12.720 - 00:05:26.430, Speaker A: So I'm just trying to understand a bit more about interaction between l one and l two. So let's say we have an l one smart contract or something. What do we actually push to l one when we have a transition on l two?
00:05:26.500 - 00:05:46.566, Speaker B: Okay, so let's take this example and let's draw the smart contract that's used to execute this example. So we end up with two layers. So we have layer one. This is the blockchain that already exists. This is the one that we talked about earlier where everyone is validating every single state transition. This is the one that we talked about. That's very few transactions per second.
00:05:46.566 - 00:06:18.980, Speaker B: And inside of this we make a second layer. This is that we make a smart contract. This smart contract makes our L2. And basically what this smart contract does is it accepts a ZKP or a fraud proof or a fraud proof. And some call data, maybe call data is not the best word. So there's different words that are used to describe the minimal representation of the state transition. So some people use diff, some people use call data.
00:06:18.980 - 00:06:31.610, Speaker B: What do you think is a good term for this? How about state transition representation?
00:06:31.710 - 00:06:33.490, Speaker A: Yeah, let's go with state transition.
00:06:33.650 - 00:06:57.882, Speaker B: It's a bit of a mouthful, but let's go for it. Okay. So they put the ZKP and they also put the state transition representation. They put both of those things into the smart contract. And the smart contract then basically does the following things. Number one, it verifies the DKP is correct. This is our validity guarantee.
00:06:57.882 - 00:07:25.110, Speaker B: Make sure that, oh yeah, this transition is correct. All of the users consented to this list of transactions. Secondly, it checks that this list of transactions is included in the ZKP. It checks that the state transition representation is part of the ZKP. That's the second thing that it checks. And the third thing that it does is it remembers the new state. So it used to remember the old state root.
00:07:25.110 - 00:07:54.170, Speaker B: And after we do the state transition, we go to root prime. And the smart contract just says, okay, root prime is our new latest state. And so this is our L2. This is our L2. And our L2 is basically like a contract that tracks route the root. Every time we update the root, we give it a ZKP and we give it a state transition representation. And this guarantees validity.
00:07:54.170 - 00:07:57.790, Speaker B: And the state transition representation guarantees data availability.
00:07:58.130 - 00:08:31.046, Speaker A: Makes sense. So just to clarify a few things. So the root is the commitment to the state, right? So by kind of like knowing what the root is, we know kind of that there is a state that is a specific state and it cannot be changed very easily. And that when we execute the smart contract and verify this the route actually changes. Right. Why can we just simplify this and just have a root in there as well? Like what would happen if we don't have the state transition representation? You just have a ZKP and a root.
00:08:31.158 - 00:09:09.014, Speaker B: Okay, so let's talk about that. So in that context, what would happen is that we remove the state transition representation and our users who are observing the system would not know this information. If they don't know this information, if they don't know this information, they wouldn't be able to withdraw or they wouldn't be able to do another state transition. State transition. Because this is one state transition, but we're going to do many, many more. The point is that each of these state transition functions is like a new block. We're processing transactions, but we have more transactions coming and we need to process them.
00:09:09.014 - 00:09:24.214, Speaker B: And if we don't know the data, we're not able to do another state transition and we're not able to withdraw. So that's why the state transition representation is a very important part of the guarantees of roll ups.
00:09:24.262 - 00:09:40.626, Speaker A: Makes sense. So while ZKP guarantees that you cannot do invalid transition, there is still somebody who doesn't reveal the state. They can do valid transitions but hide the actual state. And then it's kind of like you're stuck in that situation where you don't know exactly what the state is. Exactly.
00:09:40.808 - 00:09:43.058, Speaker B: So it's kind of like a ransom attack.
00:09:43.144 - 00:09:51.602, Speaker A: Yeah, makes sense. And then if we don't put the state transition representation, that is technically not a roll up anymore, right?
00:09:51.656 - 00:09:54.002, Speaker B: Yes. Okay. That is correct. Okay.
00:09:54.136 - 00:10:06.886, Speaker A: So roll up, you have to be able to reconstruct the state. And I guess that brings me to a question, like what are the properties of a roll up? Like, what do we want to say? Like this is a roll up and this is not roll up. Like what are the key properties that we want or guarantees that we want to ensure?
00:10:06.998 - 00:10:47.254, Speaker B: So what we want to ensure is that the roll up inherits its security from layer one. This is the key property of roll up because let's look at the situation that we have. We have a layer one that's very secure because many, many people are checking it. Many people are being careful about is this correct? And we want to include more transactions, but we want it to have the same security guarantees. So basically what a roll up does is it says, okay, I want to inherit those security guarantees. The two security guarantees are validity and data availability. So we use the fraud proof or the ZKP to verify the state transition function.
00:10:47.254 - 00:11:02.310, Speaker B: And then we use the layer one to guarantee the ZKP is correct and also to make sure that the data is available. So the key thing about rollout is that we want to inherit our security from layer one. That's the key property they're trying to achieve.
00:11:02.390 - 00:11:30.670, Speaker A: And I guess by security we mean maybe a couple of things. That one, our funds cannot be stolen from a roll up and the same kind of applies. And the second one is our transactions cannot get censored. Right? Yes. I guess we didn't talk about this yet, but I'm curious. So who is actually generating the zkps and putting this on? L1, is there an entity? Is it one entity? Is it many entities? Like how does it usually work? And what are the trade offs of different approaches?
00:11:30.750 - 00:11:53.050, Speaker B: So there are different approaches here. So as you said, there's always an entity that needs to create the ZKP. Yeah. So they have some powerful computer or maybe not so powerful, depending on the CK rollup. And inside this powerful computer, they compute the ZKP. Right. They get transactions from the users.
00:11:53.050 - 00:12:32.710, Speaker B: Our users are here, let me draw some users. Yeah. They get transactions from our users and then they use those transactions to make the ZKP. Finally they send the ZKP on the layer one. The layer one then puts the ZKP into the smart contract and produces a L2 block. So the entity that makes these proofs is, it seems like they're quite powerful because they're getting transactions from users and they're putting them on chain. And we said before that one of the things we want to guarantee is that there's censorship resistance here.
00:12:32.710 - 00:12:36.914, Speaker B: We want to make sure that this party isn't able to censor other users.
00:12:36.962 - 00:12:44.742, Speaker A: And by censorship resistance, basically if I want to get my transaction, if I have the fund in the roll up, I want to be able to make sure my transactions get executed.
00:12:44.806 - 00:13:23.510, Speaker B: Yeah. So this party could theoretically just say, I don't like this user, I'm not going to include their transactions anymore. Right. How can we decide who to be the coordinator? It seems like it's such a powerful position. So there's two approaches to limiting the power of the block producer. And I recommend that we take both approaches, but let's talk about them and see. So the first one is to basically say that, okay, let's just make other people who are also able to make zkps, who also have a computer, they make their zkps.
00:13:23.510 - 00:13:35.580, Speaker B: They make their zkps and they send them on chain. And we have this kind of like competition. And the users are then able to decide, oh, I want to send my transactions to this one instead.
00:13:37.810 - 00:13:40.762, Speaker A: Basically, whoever will be willing to process my transaction.
00:13:40.826 - 00:13:48.400, Speaker B: Yeah, exactly. So this is the first step in kind of like preventing this censorship issue.
00:13:48.770 - 00:14:04.422, Speaker A: And this somewhat replicates like, l one dynamic, right. You have kind of some form of consensus that these guys need to come to before they can send something to. Unless maybe there is a race condition. I'm guessing there are many ways to coordinate these guys between themselves, right?
00:14:04.476 - 00:14:41.550, Speaker B: Yes, there's many ways for them to be coordinated. One of them is proof of stake. Proof of stake is basically where everyone puts down a deposit and you assign people the rights to make a block based upon how big their deposit is. And the reason why this is secure is that basically if this person puts down a deposit, they're losing something. They have, like an opportunity cost with that money. And when they receive transactions from users and they don't include them, they're missing out. They could make more by receiving transactions from other users.
00:14:41.550 - 00:15:25.834, Speaker B: So that's how proof of stake works. And proof of stake is very secure on layer one, because on layer one, if there's some very rich person who is getting the opportunity to make money blocks and decides, I don't want to make blocks for this user, I'm going to censor this user, I don't care that I'm losing a little bit of money, because the amount of money I'm losing is so tiny compared to the size of my stake. It's not, I don't like that user, I'm not going to do it. And if that happens on layer one, we can have what's called a hard fork to recover. Basically on layer one. The other participants say, that's not the rules of the system, that's not the properties of blockchain. We're going to change the system so that you don't have that state anymore.
00:15:25.882 - 00:15:30.978, Speaker A: So you bring a social aspect to this and be able to kind of recover through social means.
00:15:31.064 - 00:16:28.354, Speaker B: Yes. So on layer one, you can recover from this type of censorship attack via social recovery hard fork. Unfortunately, this doesn't work on L2 because if we were doing the same thing on L2 and a certain user and the same attack happened and we wanted to do a social recovery hard fork, we couldn't do a social recovery hard fork of the L2, we would have to do one of the layer one. And historically it's been very rare for there to be hard forks to recover broken smart contracts. And I think that in the future it's going to be very, very difficult to do. It's very hard to justify that because it basically means that everyone has to change their philosophy or everyone has to accept this slight change in order to help this broken thing. So because of that, I think that leader election via proof of stake is not a good mechanism on L2.
00:16:28.354 - 00:16:28.706, Speaker B: Yeah.
00:16:28.728 - 00:16:35.414, Speaker A: Because if somebody controls a significant portion of the stake, they can still censor transactions. Did I send it right?
00:16:35.452 - 00:17:14.242, Speaker B: Okay, yeah, exactly. So instead of doing this, we can have some other mechanisms. So we talked earlier about how on layer one we want to inherit the security guarantees of layer one, and layer one has very good censorship resistance. So how can we inherit this guarantee? Well, one thing that we could do is we could, let me make a little bit of space here, is that we could add something to our roll up and we could basically make this kind of like forced transaction. Right.
00:17:14.296 - 00:17:16.626, Speaker A: Okay. Could you explain what it is?
00:17:16.728 - 00:17:47.040, Speaker B: Yes, again. So let's talk about the context first. Okay. Let's say that we're in this situation. This user is being censored not just by one coordinator, but by all of them. They are running the kind of like the same attack that we discussed earlier, and this user is just not able to get their transactions included. So the idea of a forced Transaction is that this user could, instead of sending their transactions to the block, producers, could send the transaction to the layer one.
00:17:47.040 - 00:18:02.226, Speaker B: And because layer one has better censorship resistance guarantees, the transaction can be included by the transaction will appear here. Because layer one's censorship resistance guarantees are very strong.
00:18:02.408 - 00:18:09.154, Speaker A: So the logic in the smart contract will actually say you cannot execute the state transition unless you execute this transaction. Is that right?
00:18:09.192 - 00:18:23.400, Speaker B: Yeah, that's right. So the next part is that once a transaction appears here, if any coordinator wants to make a new block, they must include the transactions here. That's the rules of this system.
00:18:24.170 - 00:18:30.922, Speaker A: So I guess the only downside of this approach is that you do have to pay the l one costs for the transaction to be able to do this.
00:18:31.056 - 00:18:54.530, Speaker B: Right. So the l one cost is going to be higher. Right. Because we built this whole system, we built all of the roll up in order to be like, oh, we're not able to do a lot of transactions on layer one. So if a user has to put a transaction on layer one in order to get it processed, that kind of defeats the whole purpose. Right. So what we can do, but there's ways for us to mitigate this.
00:18:54.530 - 00:19:32.602, Speaker B: And basically what we do to mitigate this is instead of putting a transaction here, the user can produce a Zkp. The user says, okay, I'm done. I'm going to make a ZkP. And they can put the ZkP on chain. And this would be like a representation of many other transactions. They can get this transaction from this user or from other users, and it's possible for them to sort of compress many transactions together, just like we do with a roll up. So it's possible for the cost of the force transaction to approach the cost of like a regular roll up block.
00:19:32.602 - 00:19:32.894, Speaker B: Okay.
00:19:32.932 - 00:19:43.026, Speaker A: So there is some coordination effort this guy needs to do, but hopefully he will not need to pay the l one cost if he is able to get enough other people to include into his ZkP, right?
00:19:43.048 - 00:20:28.662, Speaker B: Yes. Okay. And this is some important nuance here, is that because transactions are super cheap on L2 and they're really expensive on layer one, if there's like a censorship attack against a user or a group of users, it can be really difficult for them to withdraw their money because their transactions are just so expensive. Like if I have $0.10 on a L2 and the Ethereum transaction fee is twelve cents, I can never do a forced transaction. So it's important that we have these mechanisms to force transactions that are as cheap as the layer one gas cost or as the L2 gas cost. Otherwise, users can be kind of like trapped inside a roll up by a malicious user, a malicious coordinator, block producer.
00:20:28.746 - 00:20:58.522, Speaker A: Okay, so basically, just to summarize, there are a couple of approaches. One approach is you kind of have many block producers, or it's decentralized block production to some extent, and that helps you in some ways, but it doesn't prevent the worst case scenario. And the worst case scenario can be prevented by doing this, forced transactions when, if all else fails, and then you can go through l one and do it. And hopefully the mechanism that you described can make it actually not super costly to do.
00:20:58.576 - 00:20:58.938, Speaker B: Yes.
00:20:59.024 - 00:21:15.840, Speaker A: Okay, makes sense. Now, I think one of the things that is frequently talked about is do we actually need to have many? Like if we have force transaction mechanism, is there any value to having many block producers? Can we just have one block producer and say, that's enough?
00:21:16.770 - 00:21:47.318, Speaker B: Yeah. So this is an interesting trade off, because if you have a single brock producer, you get some other benefits, right? You're able to have different features. Like you're able to have that coordinator kind of give you a weak promise that they're going to include a transaction and they can give that to you very quickly. That's just really nice. Ux people who are used to web two just click a button and it says yes. And we want to have that same user experience for L2. And one way we can do that is if we can have a single block producer.
00:21:47.318 - 00:22:37.062, Speaker B: And the good news is that it's possible for us to use a force transaction and a single coordinator to get approximately the same guarantees. We're inheriting the censorship resistance from the force transaction and we're getting some of the nice benefits of a centralized processor by just having a single coordinator. So it's also possible to have a system where we can transition between the two systems. We can have a single coordinator where we have these nice properties, and we can also have another coordinator who appears and says, oh, I want to be a coordinator too. And we can have a system where one coordinator is able to provide the instant finality guarantees and the other coordinator isn't. There's all these different ways that we can build a system that's able to have this kind of censorship resistance and inherited from layer one.
00:22:37.116 - 00:23:25.270, Speaker A: And I guess you may still want to have, as you said, maybe like a backup coordinator or something like that, or maybe people being able to come in because in case this guy goes down for whatever reason, you don't want to sacrifice the liveness of the system. Right, exactly. Okay, makes sense. So we talked about how do we enforce validity, but one of the things that I'm kind of still curious about is this state transition representation. So you do need to put data on chain. And I guess the question, and I think you kind of explained it, but I want to dig deeper into it, is this actually cheaper than regular transactions? Why is this data smaller than what you would go with regular l one transactions? How we're able to make this more efficient from data perspective.
00:23:25.850 - 00:24:05.606, Speaker B: Okay, so let me talk about the difference between the costs of layer one and L2. Okay, so let's take a look at what's included in a layer one transaction. So layer one transaction has a bunch of data associated with it. It's like two from amount and signature. And the L2 transaction. Basically we can do a couple of things. So we have to basically have the same representation, but we can reduce the cost a little bit.
00:24:05.788 - 00:24:12.178, Speaker A: And this is a simple transaction. We are not talking about smart contracts yet. This is like a very basic payment transaction.
00:24:12.274 - 00:24:27.582, Speaker B: Yeah, exactly. This is very simple. So on layer one we have these costs. For the next section, I'm going to talk about. Okay, let's do optimistic roll up and zk roll up. So let's see what optimistic roll ups will do. So optimistic roll ups have the same thing.
00:24:27.582 - 00:25:03.754, Speaker B: They have two from amount, a signature. So the one thing that optimistic roles can do is they can reduce the size of two from an amount. Because on layer one, two from an amount are like, I think it's like how much is the 256 bits at the very. Okay, let's say it's 256 bits. Yeah, 256 bits from is 256. Amount is 256, although it's like not exact amount is 256.
00:25:03.792 - 00:25:07.370, Speaker A: For this purposes I think it's fine. We can round up or estimate it.
00:25:07.440 - 00:25:11.994, Speaker B: Okay. And signature is much bigger. Let's say it's like 512.
00:25:12.112 - 00:25:12.780, Speaker A: Yeah.
00:25:15.310 - 00:25:38.334, Speaker B: So this is the size of those on L2. We're able to reduce this a lot. We can probably put this down to like 32 bits. 32 bits. 32 bits. And signature still has to stay the same as like twelve. Okay, so we're already getting some gains.
00:25:38.334 - 00:26:23.262, Speaker B: We're reducing their outcomes by having this compression algorithm that we're using. Right. So could the layer one apply these same optimizations? Could the layer one reduce the size of the to address the from address? And it could, but it wouldn't give any benefit because on layer one everyone is executing the computation. And if you reduce the size, they'll need to check that the size reduction was done correctly. So it would just add more computation. So layer one has bigger data footprints because execution is expensive, but in optimistic rollups. And in ZK rollups we have cheaper computation.
00:26:23.262 - 00:26:52.758, Speaker B: The computation is very cheap, so we're able to reduce the data size. The next thing to look. Okay, so let's look at ZK rollup. So ZK rollup is very similar to optimistic. It's the same two from amount, but it doesn't need a signature. The signature gets removed because the ZKP verifies the signature. In an optimistic role, the fraud proof verifies the signature.
00:26:52.758 - 00:27:01.450, Speaker B: So because we're able to put the ZKP, we're able to replace the ZKP with the signature. We're able to replace the signatures with the ZKP.
00:27:01.530 - 00:27:08.862, Speaker A: So we do need to put the ZKP here. Right, but although ZKP gets amortized across all the transaction in a given block, right?
00:27:08.916 - 00:27:23.078, Speaker B: Yeah, we put the ZKP here, which is bigger than one signature, but because the block contains many signatures, it ends up being cheaper. So let me just write up the cost here. So it's 32 bits, 32 bits and 32 bits for the amount.
00:27:23.164 - 00:27:31.874, Speaker A: And then different proving systems have different sizes for ZKP. Some are very small. Right, like growth 16 I think is just cost of a few signatures.
00:27:31.922 - 00:28:03.166, Speaker B: Probably, yeah, starks are quite big, so there's different trade offs there. A stark can include more transactions though. So maybe a stark is going to be better because it's going to have a thousand transactions. There's trade offs here we need to think about. One other thing to mention is that it's possible for optimistic roll ups to replace this signature scheme. They can use something like BLS signatures instead of the current signature. That BLS is a compressible signature scheme.
00:28:03.166 - 00:28:10.450, Speaker B: You can aggregate together many signatures. So it's also possible for optimistic roll up to sort of remove the signature requirement.
00:28:11.450 - 00:28:15.910, Speaker A: Would that require l one to be able to process the signatures efficiently?
00:28:17.770 - 00:28:41.870, Speaker B: Yeah, it would require that, but it's actually impossible to do that right now. Okay. You can do BLS signature verification and actually theoretically optimistic roles would be cheaper than ZK roles because they wouldn't have to include the ZKP or the signature. But it would be cheaper. But it's very small difference because it turns out we can make zkps that include many transactions.
00:28:43.090 - 00:29:11.640, Speaker A: One other thing I want to ask. So in the tree that you draw previously, like where you had a state in the context of ZKP, my understanding is that you only need to put the final state of the, let's say if there are three people involved and I send tokens to you and you send tokens to another person, what needs to go on chain is only that I sent one token to someone and someone else got token. But the intermediate transaction doesn't need to be recorded.
00:29:12.750 - 00:29:28.554, Speaker B: So this is a tricky situation because, okay, so what you're saying is technically correct, but one thing we want to do with blockchains and with ethereum is to make sure that transactions work once and only once.
00:29:28.672 - 00:29:29.002, Speaker A: Okay?
00:29:29.056 - 00:30:11.318, Speaker B: And there's two ways to do this. One of them is that every time someone makes a transaction, you use a new leaf in the tree, and that means that the old leaf is just kind of discarded and thrown away. The other thing you can do is you can use something called the nonce. And the nonce is just like this number that every time it's the count of the number of transactions you made, every time you do another transaction, you increase the nonce by one. So in the case where you discussed, oh, so let me add this here. So ethereum uses a nonce because that makes the tree space, that makes the tree space, it allows for the tree space to be used more efficiently. So then you could remove the nonce, but then you would have to make this number bigger.
00:30:11.318 - 00:30:50.630, Speaker B: So in general, people use the nonce. But why didn't I put the nonce in the data availability before? Well, we have the noncen data availability in layer one, but on L2 we can remove it. Right, because we already have this implicit idea of a nonce. Because we have this from, if there's a from, you know, you have to increase that nonce by one. So we're able to remove the, able to remove the nonce here. So like the optimization that you're talking about, it's possible, but it might not work out as being the most efficient thing, because although you get rid of the nonce and you don't have to put it on chain, you end up using your state tree less efficiently.
00:30:52.250 - 00:31:03.290, Speaker A: Makes sense. I guess it depends on what representation. And that's actually maybe a good point. A roll up doesn't need to follow the same state model as the l one. You can experiment with different state models.
00:31:04.830 - 00:31:14.880, Speaker B: Yeah. And some people have done that using making like a zk roll up. That's a kind of automatic. What is it? Amm stand for?
00:31:15.250 - 00:31:16.842, Speaker A: Automated market maker.
00:31:16.906 - 00:31:22.898, Speaker B: Automated market maker exchange. This state is completely different.
00:31:23.064 - 00:31:28.658, Speaker A: So you can have like an account based system. You can have a Utxo based system or anything in between or something new.
00:31:28.744 - 00:31:48.140, Speaker B: Exactly. There's a lot of space to innovate. There's a lot of things that people are optimizing for that you might want to optimize for. Call data size. Maybe you don't care about that. Maybe you want to optimize for simplicity or something else. There's a big trade off space with many things to worry about or to care about.
00:31:48.590 - 00:32:13.780, Speaker A: Okay, so basic takeaway. We can compress things, and that's why in the previous kind of discussion about validity, we simplified the computation burden. Right. Because you don't need to do all the computational one here. We're also simplifying the data burden, so to say, because you can represent things succinctly. But it does seem to me that you only get a constant improvement here. Right.
00:32:13.780 - 00:32:22.920, Speaker A: If you need to put some data, you can only shrink it so much. You can't shrink it exponentially. Am I thinking about it right?
00:32:25.050 - 00:33:02.414, Speaker B: Yes, I think that that's correct. So there's more advanced compression algorithms. We are only scratching the surface of what is possible that you can have. So there is some interesting thinking here that theoretically, when you put data on chain, you need to put data on chain for new blocks, not for old blocks. So if you wanted to make transactions for, like, if I wanted to send ten transactions every month, to the same people with the same amount. I wouldn't have to put the call data on chain for that. I could just refer to a previous block.
00:33:02.462 - 00:33:02.866, Speaker A: Interesting.
00:33:02.968 - 00:33:22.918, Speaker B: I could just say that, oh, I made this block last week. It's the same block. Just process. Just, this is the same data. So once data is available, it's possible, but that's this kind of nuanced kind of edge case. And I feel like why you're asking is different than that. I mean, that's one thing to be aware of.
00:33:22.918 - 00:33:52.900, Speaker B: But in general there is. So in general, we're moving from a world where computation costs. On layer one, computation is expensive. On L2, computation is like effectively free because we hardly ever execute our fraud proof, and our ZKP is constant time verification. Because of that, data becomes expensive, and data is the new thing to worry about. And it's impossible to compress data infinitely. You can never just get rid of data.
00:33:54.950 - 00:34:12.886, Speaker A: The other reasons you can give up some of the security guarantees to the validium model or whatever else, it's not a roll up anymore, right. But you can say, I'm not going to put all the data on chain, I'm just going to put fruits of the state and the actual data is somewhere else. Like there is something else. But then you don't inherit quite the same security guarantees anymore, is that right?
00:34:12.908 - 00:34:54.950, Speaker B: Yeah, you don't inherit like a validium doesn't inherit the security guarantees of Ethereum. It affects the data availability guarantees, which also ends up affecting the validity guarantee. Oh, no. Does it affect the really? But yeah. So it affects the data availability guarantees. So it means that users can lose access to their funds, and a malicious coordinator could use that in order to extract a ransom or extract high fees from their users. So you mentioned that, okay, there's this problem of data availability.
00:34:54.950 - 00:35:27.454, Speaker B: How can we scale past that? So after the merge in Ethereum, the next major feature to add is ippy 4844, and IPpy 4844 basically adds a lot more call data. An Ethereum block right now, I think, includes order of kilobytes per second of data. And AP 4844 will be like 1 mb/second or one to 16 megabytes per second. So it's going to be this major scalability upgrade. Okay.
00:35:27.572 - 00:35:31.540, Speaker A: And in the long term, like sharding and things like that will unlock even more.
00:35:32.230 - 00:35:48.006, Speaker B: Yeah, that's true, but the sharding has changed so much from when it was originally defined, because sharding used to be like, oh, we'll just have many people executing different things, but now sharding is just like, oh, we're just data availability from many different people.
00:35:48.108 - 00:35:51.862, Speaker A: Makes sense. It's kind of like data sharding, not execution sharding anymore, right?
00:35:51.916 - 00:35:52.470, Speaker B: Yeah.
00:35:52.620 - 00:36:15.680, Speaker A: Okay, makes sense. Okay, so we talked about fairly simple model for the roll ups, where this is just moving some amounts of tokens or whatever, or coins from one account to another. What other types of roll ups are there? Assuming, let's say we want to do something as sophisticated as we can do l one.
00:36:17.490 - 00:36:22.222, Speaker B: So we can have a ZK roll up that supports the ethereum virtual machine.
00:36:22.286 - 00:36:28.580, Speaker A: Yeah, so it's more kind of like a general purpose roll up, not just a specialized roll up that does only transactions. Okay.
00:36:28.950 - 00:36:41.320, Speaker B: Yeah, so let's talk about how that works. So in order to talk about how that works, first we need to talk about how ethereum works. So let me clear this and we can talk about it.
00:36:41.850 - 00:36:57.738, Speaker A: All right, so we talked about fairly simple roll ups where you just transfer tokens from one account to another. But obviously we get a lot of like with l one, we get the ability to write smart contracts and build custom logic and all of the cool stuff. Can we do something similar in a roll up context?
00:36:57.834 - 00:37:13.780, Speaker B: Yes, we can. So it's possible to do that with optimistic rollups. I think Optimus and maveritrum both have optimistic roll ups that are EVM compatible, and it's also possible to do that with ZK roll ups. But it's a little bit more difficult.
00:37:14.470 - 00:37:15.860, Speaker A: Why is it more difficult?
00:37:16.870 - 00:37:46.334, Speaker B: That's a good question. Why is it more difficult? Well, okay, let's look at the robes we talked about before. But when we were talking about the robes before, we had like a Merkel tree, and we did a very simple strange transition where we just this leaf and this leaf, we updated them. So basically we just changed two things and we did the strange transition. And it was easy to make a ZKP for that.
00:37:46.452 - 00:37:53.482, Speaker A: The computation involved here is very easy, and it's fairly simple to put a DKP together that can prove that computation.
00:37:53.626 - 00:38:38.726, Speaker B: The key is that the change is not dynamic, it's constant. Like, okay, we did one say transition here, we're going to do another one where we change another two leaves, and what we're doing is the same all the time. We're just changing two leaves, we're just updating two. And like, because we're doing a constant thing all the time, it's easy. And we can make application specific roles that you mentioned earlier. We can make an automatic market maker roll up where we're doing what Uniswap does, but we manage to encode that in a way that's just updating several leaps, just changing several things.
00:38:38.848 - 00:38:41.338, Speaker A: The logic is fixed, basically, the logic is fixed.
00:38:41.434 - 00:39:30.604, Speaker B: And the problem with the EVM is that the logic is dynamic. I can deploy a smart contract that does anything I want. And if we want to make a ZKP for that, if we want to make a proof about that, we need to dynamically adjust. So there's all these different kind of approaches and mitigations that people have taken to try to allow you to have smart contracts. The simplest thing to do is that to say that, okay, every time I deploy a smart contract, I'm going to make a new ZKP, right? I have one ZKP for this, and I'm going to make a new one for Amm and I'm going to make another one for something else for Ens. But this is hard to do. It means you have to do a whole bunch of different setup phases.
00:39:30.604 - 00:39:37.584, Speaker B: It means that you have to update your smart contracts with the new proving keys, it just becomes really difficult to do.
00:39:37.702 - 00:39:50.128, Speaker A: And I would say also a person who deploys it actually needs to be able to write zkps, right. They need to have the special expertise, which is slightly more complicated than writing solidity smart contracts.
00:39:50.144 - 00:40:48.376, Speaker B: I would say bobin is being very, it's, it's a lot more complicated than I think, than solidity contracts, because solidity contracts are like a paradigm that people already understand. With solidity smart contracts, you're just doing like check, signature, and add this money to this and subtract this money. But with zkps, you're not able to do these things. All you're able to do is just like a plus B equal, equals c. You're just able to check that different numbers. There's this constraint between these numbers, and you eventually use this to compose a signature check or other things. But it's really hard to do, and it's not a good developer experience, especially for the, I don't know, is it millions or tens of thousands or thousands of people writing smart contracts? For the people writing smart contracts, it's really hard.
00:40:48.376 - 00:41:08.590, Speaker B: And one thing that we think about when we think about, oh, how can we scale ethereum is like developer experience. This is like a mistake that a bunch of people made. Originally, people thought that I'm just going to build my thing and people are going to just come and use it. But they didn't really take into account that it takes a long time to teach people how to use a new thing.
00:41:09.920 - 00:41:12.690, Speaker A: And then learning curve here is probably super high.
00:41:14.020 - 00:41:38.120, Speaker B: So this is one approach. Let's talk about some other approaches, because I don't want to spend too much time on this. So this is one approach to just make like special ckps. Another approach. Let's talk about the compiler approach. Oh, I'm going to leave that there actually, because this is so like, we have a system where we have a smart contract for Amm. This is a smart contract and we have one for ens.
00:41:38.120 - 00:42:13.306, Speaker B: This is a smart contract. And like basically we say that, okay, well, let's just make a compiler that compiles them across. Yeah, let's make a compiler that converts this into this. And this works too. But there's some concerns here. Some of the concerns are the compiler security is very difficult, to be sure about that. We have this compiler here that's converting one thing to another.
00:42:13.306 - 00:42:53.946, Speaker B: How do we know that it converts exactly the same? The other thing is that this smart contract is written with a certain set of security assumptions in mind. But moving to a different execution environment, like a ZKP, those security assumptions can change. That's the second concern here, that people are very worried about compiler based approaches, because this could be an issue. There's another approach that just says, let's make a special ZKP and compiler, and let's not use Solidity. Like in this example I was talking about, this was like solidity contracts in this example.
00:42:54.048 - 00:42:59.802, Speaker A: But I also think making a compiler that compiles arbitrary solidity is probably a challenging task in itself.
00:42:59.936 - 00:43:31.238, Speaker B: Yeah, it's not easy. I think the complexity approaches, the complexity of the ZKVM, but yeah, it's not easy. So those are two types. Let's talk about the third type where you're really starting to get close. And basically the third type is where you figure out a whole bunch of the problems related to the ZKVM. And you say that like, okay, well, what's in Ethereum? So there's stack, there's bytecode memory. Yeah.
00:43:31.238 - 00:43:47.194, Speaker B: Memory storage or something. Yeah, storage. Yeah. And you say that, okay, I'm going to figure out some of these things, but not all of them so stacked. Let's do it. Bytecode, good memory, good storage. No, storage is more difficult.
00:43:47.194 - 00:44:16.302, Speaker B: So there's some approaches that kind of basically take storage and they take the storage tree that ethereum currently uses and they move to another one. They have a different kind of storage. Yeah, maybe it's different. Like, let me draw it the way that you draw it. I like the way that you draw them like this. Yeah. So we move to a different storage, storage structure.
00:44:16.446 - 00:44:27.574, Speaker A: And so to be clear, in this approach, we have a ZkP that will actually prove the execution of the EVM, and with them making compromises on different parts of the EVM of how we want to represent them.
00:44:27.612 - 00:44:28.614, Speaker B: Yeah, exactly.
00:44:28.812 - 00:44:36.250, Speaker A: I guess one of the curious things, why is storage, for example, difficult? Like, why would we want to change from Ethereum storage to something else?
00:44:36.400 - 00:45:00.850, Speaker B: Right. So storage is different for two reasons. The first reason is that the hash function that the storage uses is very difficult to implement in a snark. Well, it's not that it's difficult, and it's not that it's expensive either. It's that it's just used so much that the hash function in storage is just used all the time. So people used, I think it was prohibitively expensive. I don't think that anymore.
00:45:00.850 - 00:45:22.082, Speaker B: So there's some really interesting work about optimizing ketchack and things like that. That's really exciting stuff. That's one of the reasons. The other major reason is that this representation of state is very complicated. This is called a Merkel Patricia tree, and it's really hard. It's really hard to implement, and it's especially really hard to implement inside the ZKP.
00:45:22.146 - 00:45:31.930, Speaker A: Inside ZKP is generally like, the simpler the computation, the easier it is to handle. In the ZKP, something that has a lot of edge cases and complex is difficult to handle.
00:45:32.350 - 00:46:01.874, Speaker B: Exactly. Yeah, there's a lot of edge cases. So this is another approach. And the final approach is basically to just be like, okay, we're going to do it anyway. It's to just be like, okay, we're going to figure out storage. So why would people go to this length? Well, one of the problems with not having storage is that the philosophy here is basically that if you change anything in Ethereum, it has these huge implications for other parts of the stack. For example, if you change storage, your deposit and withdrawals are broken.
00:46:01.874 - 00:46:32.554, Speaker B: Also, there's a whole bunch of smart contracts that use storage to look up other things, and if you do that, you'll break those contracts too. So the idea is that, okay, well, we want to have storage as well, so let's make a ZKP for storage as well. And we're working on this. A whole bunch of different people are working on this, and we're thinking it's possible it's just going to take a little bit longer. I think that the key realization here was to just be like provertime doesn't matter that much that developer experience is more important than provertime.
00:46:32.682 - 00:46:54.262, Speaker A: And I think this ties back to, I think there was a blog post by Vitalik about different types of Ziki EVM compatibility. Right. So when you handle everything, this is, I believe, like type one, you basically don't change anything about how EVM or ethereum work. And then there is like type two and type three and which are kind of different levels of kind of pick and choose, like what you keep the same and what you change.
00:46:54.316 - 00:46:54.726, Speaker B: Right.
00:46:54.828 - 00:47:00.002, Speaker A: And then type four is like a different one where you just compatible at solidity level. Am I?
00:47:00.076 - 00:47:48.850, Speaker B: Yeah, that's right. Okay, so another point of doing the highest level of compatibility is it will let us make a proof of validity for Ethereum. So far today we've talked about using zkps for state transitions, and we talked about how layer one, everyone was checking, and the idea is that we can have a ZKP so everyone doesn't need to check. And especially people who have not powerful devices don't need to check or don't need to download so much data. So that's another really interesting, really interesting possibility. If we end up with a single DK roll up or a single EVM compatible roll up, then we're able to make this proof of validity. But if we manage to get multiple, then we're able to have multiple client diversity of different proofs of validity for Ethereum.
00:47:49.010 - 00:48:10.670, Speaker A: Makes sense. Yeah, no, this is like the type one ZKEVM equivalent that basically you can approve the state transition of the main chain of Ethereum. Okay, awesome. I think one other thing I want to kind of get a bit more detail about. We mentioned that it's difficult to build ZKE evms and zkvms in general.
00:48:10.820 - 00:48:12.154, Speaker B: What are the difficulties?
00:48:12.282 - 00:48:25.266, Speaker A: We mentioned hash functions being different or the tree structures being different. Are there any other challenges that you faced or others are facing in building this?
00:48:25.368 - 00:49:06.442, Speaker B: Yeah, so there's a whole bunch of challenges and there's a lot of really fun stuff. Let me talk about one really fun thing, and let me talk also about the actual difficult challenges. Okay, so the actual difficult challenges is the issue is that everything is dynamic, that we have the bicode circuit here and we're going through a bytecode circuit and we're executing things. And every time we execute something, we don't know where we're going to go next. Once we're here, we could go here, but we also could go to here. We don't know where we're going to go. And because of that, it's just really hard to get a ZKP to represent that within a ZKP because a ZKP is just ordered.
00:49:06.442 - 00:49:43.420, Speaker B: It just goes like one step at a time. Like here, just update this leaf, update this leaf, update this leaf, update this leaf. But here you can't do that because you don't know what leaf you're going to do and you don't know where you're going to go. That's one of the indications of it. The other difficult thing is that not only do you not know where you're going to go, not only do you not know where you're going to go, you don't know what you're going to do. If you come here, you're going to do one thing. You're going to do like catch act, and if you come here you're going to check the storage because you don't know what you're going to do.
00:49:43.420 - 00:50:00.814, Speaker B: It ends up that it's just really difficult to have a list of everything that you need to do because Ethereum has a whole bunch of different opcodes that are really expensive or really difficult to implement at snark. So yeah, it's a really fun challenge.
00:50:00.862 - 00:50:29.162, Speaker A: And I think this difference, by the way, you mentioned some things are more expensive, but also in the gas model, right? Some things are fairly cheap, like catch up could be cheap in Ethereum, more or less. And it's an expensive thing in a ZKP context. So you need to somehow account for the differences, because if you want to execute a thousand of catch acts, maybe it's fine in a ZKP context, but here it's going to be much more costly. So there's a cost discrepancy between ZKP and the gas model of Ethereum as well.
00:50:29.216 - 00:50:55.854, Speaker B: Yeah, so that's an interesting question, and it's an interesting topic to think about. My opinion here is that compatibility is so important that you shouldn't even change the gas prices. You should just be like prover. You just have to do 100 times more work and we have to spend 100 times more money on AWS. And I think that that's okay because we're good at scaling. Compute. Compute is like we have supercomputers.
00:50:55.854 - 00:51:13.754, Speaker B: We've been doing computation for hundreds of years. I think that it's easier for us to solve that problem than it is to solve the problem of teaching 10,000 people that this doesn't cost ten anymore. It costs 100 now.
00:51:13.792 - 00:51:31.390, Speaker A: Yeah, but this is like an example of a challenges which are not obvious, but still you need to deal and figure out how you make sure that your execution trace doesn't run out or something like that, while doing something expensive, while it was cheaper. In a kind of original smart contract.
00:51:32.130 - 00:51:58.594, Speaker B: Let me tell you about the fun one. So there's one special opcode that's called xcode size, and basically xcode size. You give it an Ethereum address, it will go and get you the size of that Ethereum address, and it will give it back to you. And this originally was like, oh, I thought that was easy, right? Because we just go to storage. We just look up the contract, we get the size. But it turns out that in storage they don't store the size. They don't store the size of the contract.
00:51:58.594 - 00:52:29.230, Speaker B: So when you go there, you can't get the size. So what you end up having to do, you have to go there, you have to get the code hash, and then you have to get all of the code, hash it together, make sure it equals the code, make sure it matches the code hashed, and then you get the size of the code that you put there. So it's this huge amount of hashing. It's like one of the most expensive things in the EVM. But there's a bunch of approaches to solve it. One approach is to put it into the Merkel Patricia tree to just add the size here. But that changes Ethereum.
00:52:29.230 - 00:52:57.126, Speaker B: What else can we. The actual solution to this is we're going to have a hard fork to include that. We're going to update the state tree to include the code size, because ZKVM is a really important thing to do. The other thing we could do is we could just reduce the cost of catchax so much that it's okay to have that because xcode size is like 100 casts. And the biggest contract, I think, is order of kilobytes. So it's a lot of hashing, but that's fun.
00:52:57.308 - 00:53:03.580, Speaker A: Awesome. It's a lot of interesting challenges, but a lot of people are working on it. A lot of smart people.
00:53:04.210 - 00:53:22.610, Speaker B: Yeah, I'm really excited to see there's a whole community of different people working on the ZKVM. There's Polygon. Hermes, you're working on it too. And there's other groups. There's scroll, there's Aztec. Oh, no, Aztec or not. But there's starkware, and there's matter labs.
00:53:22.610 - 00:53:55.182, Speaker B: There's a whole bunch of optimistic roll ups too. And they're all working on this stuff together. And I think that at the end, the most efficient solution will be combining the implementations of different groups. I think that I want to be part of a roll up where there's a ZKP, but there's also a fraud proof. And there's a ZKP from Polygon Hermes. But there's also a ZKP from Scrolltech, and there's also a ZKP from Starkware. And all of these things are checking the same thing.
00:53:55.182 - 00:54:24.566, Speaker B: And this will give you the security guarantees that we really need, because with Ethereum, we have this velocity. We want to have multiple different client implementations. And when you have multiple different client implementations, it basically says that, well, if one of the clients is broken, the other client will recognize it, and no one will be able to be tricked. We'll be able to fix the other client. So we need to have that too. For roll ups. We need to have client diversity, and we need to have fraud proofs and zkps, and we need to have all different groups working together.
00:54:24.566 - 00:54:24.982, Speaker B: Yeah.
00:54:25.036 - 00:54:32.666, Speaker A: No, agreed. All right, well, this has been awesome. But just to summarize, what are the ZK roll ups and what are the benefits? Like, why do we care of them?
00:54:32.768 - 00:55:11.518, Speaker B: Sure. So roll ups are a way to scale Ethereum. Basically, we use fraud proofs, or zkps, to remove the computation burden. That allows us to worry more about the call data burden, which we're currently compressing. And in the future, we'll have 4844 will allow us to have many orders of magnitude more transactions per second. That's what roll ups are. And that's like the plan to get to the roll up centric roadmap where Ethereum is just roll ups and everything else is just about data availability.
00:55:11.614 - 00:55:29.526, Speaker A: I think the roll up model also allows you to experiment more and have faster transaction confirmation and all of that stuff, because there are different models that you can deploy where you still don't have the same security, but you can, let's say, for short term, you trust, in optimistic way, for a centralized validator or something like that.
00:55:29.548 - 00:55:36.334, Speaker B: Right? Yeah, I found a group working on zkwasm. We could have a ZK was and rolled up in the future. That could be fun. Yeah.
00:55:36.452 - 00:55:38.282, Speaker A: Awesome. Well, thank you so much, Barry.
00:55:38.346 - 00:55:41.200, Speaker B: Yeah, thank you, Bob, and happy to help.
00:55:41.890 - 00:55:42.540, Speaker A: Thank you, everyone.
