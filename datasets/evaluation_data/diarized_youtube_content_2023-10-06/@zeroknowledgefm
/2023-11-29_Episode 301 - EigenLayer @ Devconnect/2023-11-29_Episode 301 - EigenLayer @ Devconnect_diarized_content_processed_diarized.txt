00:00:05.850 - 00:00:48.940, Speaker A: Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online. This week we bring you an interview with Tarun Sriram and I. This was a spontaneous one, recorded during the Devconnect event in Istanbul. In fact, we were sitting just outside the Alio hackathon for the recording, like we were just in the hall. So you as a listener may notice some background noise, people walking by, you know what I mean? General Hackathon sounds.
00:00:49.390 - 00:00:53.198, Speaker B: To kick off the episode, we did a quick survey of the ideas that.
00:00:53.204 - 00:01:28.998, Speaker A: We had discussed at DevConnect, including ZK toolkits, intents, and Da. And then we spent some time really digging into Eigen layer, something I had yet to properly do. Now both Tarun and I are investors in Eigen layer, him through robot ventures and me through ZK validator. And while Tarun clearly, deeply understands the system, as you will hear leading up to this interview, I did not. So you'll hear me stumbling around a little bit and trying to fill in the gaps in my understanding. And unlike other podcasts, this one was very spontaneous, so we didn't have very much time to prep. We are also recording this at the end of the DevConnect week.
00:01:28.998 - 00:02:10.018, Speaker A: We are also a little bit tired. So yeah, here is a more unfiltered version of the show for you. But before we kick this one off, I do want to say a big thank you to all of the folks who came out for the ZK Hack Istanbul event. This was our second IRL hackathon and it ran from November 10 through twelveth. We had over 170 hackers and 59 teams, and 48 of these completed and submitted their project at the end of the weekend. Once again, we were blown away with the creativity, technical skills, and speed of development of these hackers. I will link to a tweet which highlights all of the winners and runner ups and bounties, but a big shout out to everyone who came through, especially the winning teams.
00:02:10.018 - 00:02:49.954, Speaker A: Cats damn fair, anon abuse, ZKVM, and Nil Chronicle. We also had this time around a chewing glass prize, and the winners were one Js Sha 256 and hello Hypercube. The hackers got to vote on their favorite project, and so hackers choice went to KzGcex solvency and in the three days following the event, Devfolio, the hackathon platform we'd used for the event, ran a quadratic voting competition where everyone who had participated could vote on all of the teams. And for that, circom Monolith came in first. So congrats to all of the winners. But yeah, thank you all for coming out.
00:02:50.072 - 00:02:51.534, Speaker B: I know the team was so excited.
00:02:51.582 - 00:03:15.034, Speaker A: About it, we already started planning our next IRL hackathon. We're aiming kind of for May June, probably somewhere in Europe, but even before that we're going to be hosting our next online zkhack. So this is us returning to the puzzle hacking competition and multi week workshops, and we're planning this for mid January. So keep an eye on the Zkhack Discord Zkhack Twitter for more info. Now Tanya will share a little bit.
00:03:15.072 - 00:04:00.022, Speaker C: About this week's sponsor Alio is a new layer one blockchain that achieves the programmability of Ethereum, the privacy of Zcash, and the scalability of a rollup. Driven by a mission of a truly secure Internet. Alio has interwoven ZK proofs into every facet of their stack, resulting in a vertically integrated layer one blockchain that's unparalleled in its approach. Alio is ZK by design. Dive into their programming language, Leo, and see what permissionless development looks like, offering boundless opportunities for developers and innovators to build ZK apps as Alio is gearing up for their main net launch in Q Four. This is an invitation to be part of a transformational ZK journey. Dive deeper and discover more about alio@alio.org
00:04:00.022 - 00:04:03.610, Speaker C: so thanks again Alio. And now here's our episode.
00:04:06.990 - 00:04:29.822, Speaker B: I think we ask this every time we do these kind of live event ones, but what were the themes that you've been hearing? And it doesn't have to be talks. This is maybe what people are talking about. I mean, you just mentioned DA data availability maybe in different forms or. Yeah, do we need it in the way it's been proposed or not? Maybe. Let's start with DA, expand a little on what's been discussed and then we'll talk topics.
00:04:29.966 - 00:05:18.170, Speaker D: I think most of the stuff about DA has just been focused on lowering fees for use cases that are not purely financial or like DFI types of use cases. And I think because Celestia is live, it doesn't feel like the only DA is EtH L1. It feels like there's choice in the marketplace. And I think Sriram is probably the best to talk through the pros and cons of each of the different models. But I think a lot of it is just focused on reducing cost. But I do feel like there's still the reason I would say we aren't fully out of the disillusionment side of the distribution is there just hasn't. It feels like everyone I talk to is basically saying when application, when new application, right? There's tons of new infrastructure, but there haven't been too many new application.
00:05:18.170 - 00:05:21.458, Speaker D: The uniswap moment of this cycle hasn't happened yet.
00:05:21.624 - 00:05:26.866, Speaker B: Let's talk Da for a second, though. You just said Sriram is like a good person to break this down.
00:05:26.968 - 00:05:27.590, Speaker A: Can you?
00:05:27.660 - 00:05:45.030, Speaker E: Yeah, sure. I do think maybe it's the sample of people that I interact with, but it seemed like a lot of people are excited about DA this time. It's really credit to the Celestia people for actually making this happen because it's such a boring concept.
00:05:45.190 - 00:05:46.250, Speaker D: Maybe define it.
00:05:46.320 - 00:06:43.482, Speaker E: Okay, so to define it, the idea is when you have one of the ways to scale a blockchain, and this is particularly the roadmap that Ethereum committed to is to use roll ups. And what are rollups? Roll ups basically offload computation and then create proofs that the computations were done correctly. When you do this, one of the important things you have to do is to publish either the inputs or the outputs to the computation. This data has to be published somewhere. Why does this need to be published? Because if this data is withheld and not made available by the roll up operator, what might happen is funds could get stolen. Because if you don't know the data, then you can't compute what the roll up did. Or if in case of a ZK roll up, what might happen is, even though you don't need to double check the computation, somebody needs to know the state in order to continue the computation thereafter.
00:06:43.482 - 00:07:18.022, Speaker E: So a role of a data availability layer, I think some people are saying maybe it should be called a data publishing layer, a layer which ensures that when data is published, everybody has access to this data. So that's what a data availability layer is. Even though it's kind of a critical piece in the blockchain stack. The identification that this could be kind of decoupled and scaled separately is one of the key insights in the Ethereum roadmap, as well as what Celestia took and built around.
00:07:18.156 - 00:07:53.682, Speaker D: So one thing I think to a person who's never heard of DA before, that oftentimes gets used as an analogy, a very imperfect one, but one that's worth maybe going through is the comparison of dial up or t one to broadband. Right? In terms of single chain dial up versus broadband, like many asynchronous kind of connections capturing data. How do you think about that analogy, especially when it comes to this idea that in blockchains, it's not just about how much data you get, it's also execution environment, there's all these other overheads. Yeah. How do you kind of think about that?
00:07:53.736 - 00:08:54.070, Speaker E: Yeah. So one of the things that the modular landscape does is to decouple things like computation throughput from data throughput. So you can say you can separate the quality of data availability and a consensus protocol by just looking purely at how many bytes per second can it transport, and then on top of which, you're slapping a VM on top of it. And then the VM translates that I can take like 200 bytes per transaction, and each transaction on an average takes this much computation, and you can translate that into a throughput or like a transactions per second for a certain kind of transactions. So the fundamental performance metric of a DA layer is therefore bytes per second. So the comparison with something like a dial up versus like a broadband is quite appropriate. But what's actually happening under the hood is the following thing, which is that in most of the blockchains today, you have highly redundant transmission of the data.
00:08:54.070 - 00:09:28.702, Speaker E: Every node downloads and stores exactly the same data. Therefore, the entire system's performance is bottlenecked by any one node's bandwidth availability. Whereas scaling data availability fundamentally pertains to the idea that everybody doesn't have to download all the data, everybody downloads only a portion of the data using things like erasure codes and KZG polynomial commitments. Even though some fraction of nodes go offline or are malicious, you can still reconstruct every bit of the data from a small fraction of the nodes.
00:09:28.846 - 00:09:44.610, Speaker D: Okay, let's do explain it to a 15 year old or high schooler for data availability, sampling, and also the commitments, because I think understanding the trade offs and just the process in which this is done is important. Before we talk about the security guarantees.
00:09:44.690 - 00:10:16.686, Speaker E: Yeah, imagine you have a bunch of nodes and you want to store some data on this nodes, and you don't want everybody to store all the data. One way you do it is you take the data and then split it up into chunks and then say that data item one, I only store on a few nodes, and data item two, I only store on a few nodes, and so on. If you did this, what would happen is if the nodes that just stored the data, item one, if they go offline, then you lose that portion of the data completely.
00:10:16.788 - 00:10:17.390, Speaker B: Yeah.
00:10:17.540 - 00:11:05.986, Speaker E: So this, in simple schemes like this, where you are redundantly storing data, but only on a small subset of nodes, you have this problem that scalability and security are at odds. If you want more scalability, you will say that only a few nodes store the data, each data item. But that means that if those nodes go offline, you lose that data item. So what erasure codes do is instead they allow you to mix the data item in complex configurations. Imagine x one, x two, x three are different data items. You send x one plus x two plus x three to one guy, x one minus x two plus x three to one guy, x one plus x two minus three, x three to another guy, and so on. So that any three nodes, you get three linear combinations and you can actually find out what is x one, x two and x three.
00:11:05.986 - 00:12:16.230, Speaker E: So that's the basic idea of erasure coding and data availability scaling. I'm going to separate data availability scaling from data availability sampling. The data availability scaling basically means everybody doesn't download all the data, but together the system is still secure and has all the data, even if some of the nodes, or a good fraction of the nodes go offline. What is data availability sampling is imagine you have a network that is running and I'm sitting outside the network and I want to know whether this network is doing its job of actually storing and downloading the data. So how can I verify that? A normal method to verify that would be to say, go and download all the data, and then you will know whether the data items available. Data availability sampling is this very nice idea that instead of actually downloading all the data, you decide which random samples to query for, and then you say, oh, give me sample 30, give me sample 35, give me sample 50. And if you get all of them, you're like, okay, it seems like it's highly likely that all the samples must be available because the guy is giving me whatever I ask for.
00:12:16.230 - 00:12:41.466, Speaker E: So that's data availability sampling. Data availability sampling is a mechanism to scale verifiability. It is not a mechanism to scale the consensus bandwidth in the network. So you can think of these two things, data availability scaling, which is how do I make sure that no node in the network actually downloads all the data versus data availability sampling, which is a verifiability scaling method.
00:12:41.578 - 00:12:55.314, Speaker B: When you say both of these, and it might be very off, but it sort of reminds me a little bit about MP3 s and this sort of curves. Is there any connection between erasure coding or the sampling and that kind of technology?
00:12:55.432 - 00:13:22.514, Speaker E: Yeah, I mean, definitely MP3 is compression technology. But if you look at most of the, say, things like cdroms or where there is a chance that, for example, you may scratch your CD. So data has to be stored in such a way that it's resilient to certain number of erasures. And that's exactly the same technology that is used in data availability sampling.
00:13:22.582 - 00:13:30.686, Speaker B: Nice, sort of bringing it back to the event and the week. Are there any other topics other than DA that you thought were discussed a.
00:13:30.708 - 00:13:41.460, Speaker D: Lot where you were restaking which, know what, we're going to talk about more in a bit. I may or may not have been also one of the proponents mentioning it a lot.
00:13:42.070 - 00:13:56.374, Speaker B: So it's funny, someone said to me, everyone is talking about intents, but I personally have not actually heard much about it. I heard one project that was doing some ZK intense project that talked to me, but otherwise I hadn't heard very much.
00:13:56.572 - 00:14:35.140, Speaker D: Yeah, there were about three events for events intense. I only went to one and I feel like. So I sort of presented this view of Ethereum at a couple of panels today. But if we look at Ethereum's transaction supply chain from 2016 to now, 2016, you had sort of these probabilistic gas auctions. You only had the public mempool. People spammed the mempool to try to get their transactions in. So you kind of have this completely disaggregated transaction accumulation process.
00:14:35.140 - 00:15:24.210, Speaker D: Once flashbots launched, you now had this combinatorial auction where people sent in bundles. So I sent in a group of three transactions, and I say, these need to be executed atomically or not, like I really want all of them executed, or none at all. And that sort of led to some cohesiveness, like there was less of a reason to spam the network, and now you had sort of more ordered blocks. On the other hand, this thing was a combinatorial auction, too hard to do, not centralized. And effectively what you see is you see a bit of an aggregation event there. So we went from totally disaggregated to somewhat aggregated. Then we went in last year at the merge to propose their builder separation, where instead of bidding on sequences of transactions, you bid on the entire block.
00:15:24.210 - 00:16:28.434, Speaker D: This kind of moving from very disaggregated to aggregated is realistically a way for making Ethereum sustainable. It increased the net revenue to proposers, as you did this aggregation process, and it took more of that revenue from mev searchers and people of that form and gave it to proposers. On the other hand, now the users want to rebel against that sort of monopoly power, which came from that. And you can argue that we're now about to enter the disaggregation era of Ethereum and intents and RFQ systems are a way of disaggregating the value extracted by the proposer and returning it partially to the user. And so this sort of aggregation disaggregation process. When I was five, my mom once told me this very wise Cohen, which is capitalism is just bundling and unbundling repeated at infinitum. And at some level I think this entire what we're seeing in the transaction coalescence world is that and the intense model is unbundling the MEV in a sort of more peer to peer fashion versus kind of giving it out proposer.
00:16:28.434 - 00:17:01.854, Speaker D: So I think that's the reason people like it. It feels like it's the thing that has the highest growth rate or highest sort of derivative. Like people think it's making the most progress. If you look at Uniswap X, it's been taking in a lot of Uniswap's volume. On the other hand it's sort of this thing where it's like a nebulous concept. And of course disclaimer. I'm spending a lot of time trying to write some research on this because it feels like this type of thing where like there's the porn, you know it when you see know Supreme Court aspect to like everyone is like this thing is an intent, this thing is not an intent, yet there's no definitions.
00:17:01.854 - 00:17:17.058, Speaker D: And so I think trying to make sense of that is like what I would say is one thing people are doing, but people are making all these software frameworks for writing these types of things that effectively in my mind are ways of avoiding paying the proposers. Wow, sorry for the rant.
00:17:17.154 - 00:18:07.734, Speaker B: No, I'm going to mention a few things that definitely hit my radar. I had been at VM day, so vms, but also because a lot of ZK days or events would also have mentioned sort of vms, these new vms within roll ups. I feel like we are currently also in this moment where there's been a big shift to l two s, especially Sims Amsterdam if we think on that timescale ZK sync. I actually did an interview with Alex a while ago, but they launched in February 2023 so it's pretty recent still. It's like ten months ago. And so many have launched around that. I don't know if there were any before, but just seeing I guess yeah, there was optimism in arbitrum before, but there's just all that shifting value into this l two space.
00:18:07.734 - 00:18:11.226, Speaker B: It's kind of amazing that we're now living in that time.
00:18:11.408 - 00:18:28.400, Speaker D: You waited almost into one of the controversies on my Zkavm panel, which was the first question asked, which is, who's the first ckavm? And of course, one of the answers was, oh, we're all in it together. We're all first. And then immediately after that, someone was, no, no, we were first.
00:18:30.050 - 00:19:11.702, Speaker B: Nice. I do want to mention one more, which is on the ZK front, which was this idea of on the ZK front. If we look at the difference between last year and this, it's the fact that there are now maybe still in Testnet, but there are now environments, sandboxes, testnets, frameworks, where people can start to build and deploy ZK stuff a lot faster. I think that's also why in April of this year, when we did Zkhack Lisbon, people were able to build anything. Otherwise, they're just doing cryptography implementation, which is very challenging. Only a limited number of people can do it. But I think it's been opening, like, the space has been opening up to more noobs.
00:19:11.702 - 00:19:47.334, Speaker B: Kind of like, not first time hackers, maybe like experienced developers, but first time building in ZK. And then this time around, we saw even more of it, and there is even more tools around ZK and how to deploy them faster. I don't know if we're at the point where there's a lot of debuggers built into systems. I know these languages and these frameworks are still super young, but, yeah, that's something that I noticed, especially at the hackathon, but also kind of all week. Oh, I think last time in Paris, I also asked what was the best swag that you saw this week?
00:19:47.532 - 00:20:06.938, Speaker D: Speaking of intense, there is a team called essential, and they give out essential oils. And me, as someone who has a house filled with 500 candles and incense burning all the time, I appreciate a scent based swag much more than a black t shirt with a logo.
00:20:07.034 - 00:20:33.078, Speaker B: Very nice. All right, I think we could shift the conversation now a little bit over to eigenlayer. I feel we've set the scene in describing DA. I think it's funny because I would often kind of put Eigen layer in the DA camp. Somehow it competed with Celestia as it was being proposed, as just like a DA layer. But is that wrong?
00:20:33.244 - 00:20:34.422, Speaker E: No, it's not wrong. Okay.
00:20:34.476 - 00:20:39.370, Speaker B: Correct. And yet that's the problem. The minute I look at the system, I'm like, is it DA.
00:20:40.110 - 00:21:30.566, Speaker E: It's because we're building two things. We're building Eigen layer, which is a general purpose mechanism for sharing decentralized trust. So you can take the staking and the node operators and the economics underneath the Ethereum network. And Eigen layer lets you share that with anybody who wants to consume it, to share it. Imagine you want to build an oracle, or a data storage network, or a new AI inference network, or a decentralized prover network for a ZK. Any of these things, you need many nodes and they need to put in some stake, and then they need to participate in active validation of a certain service. So Eigen layer is a generalized mechanism for anybody to build arbitrary distributed systems on top of the Ethereum trust network.
00:21:30.566 - 00:21:51.086, Speaker E: Okay, that is not at all one of the modules. So we call these avs as actively validated services, and anybody can build an avs to demonstrate the power of the platform. We built the first actively validated service ourselves, and that is called Eigen da, which is a data availability service.
00:21:51.188 - 00:22:03.342, Speaker B: I see. But it's still kind of in the Ethereum camp, right? Does it model itself as like a celestial hub that roll ups are supposed to link into, or is it doing a different kind of da?
00:22:03.406 - 00:22:56.166, Speaker E: Yeah, that's a great question. The way we modeled our data availability system is as an adjunct to the Ethereum blockchain. So you have an Ethereum on the Ethereum blockchain. Let's say you're running your own ZK roll up and you want to post your data somewhere, and Ethereum doesn't have enough bandwidth or it's too expensive, whatever, and you can post it on Eigenda. Eigenda is not a standalone blockchain, unlike Celestia or avail or other things. And it was designed from first principles to be purely an adjacent to Ethereum. What this does, we surprisingly found, is it liberates a lot of the trade offs that exist in building a data availability, because usually when the other blockchains that are serving to be data availability systems also build an ordering service.
00:22:56.166 - 00:23:51.266, Speaker E: And basically it's a blockchain because there is an inherent ordering of the data blobs that have been posted into that system. What we realized is already roll ups have an ordering service, they're relying on Ethereum in the world that we are living in. So what we do is just provide a data attestation service where you write the data to the system. This system gives you a thumbs up saying that the data has been stored and custodied, and then that aggregate signature of the commitment is then posted onto Ethereum. And Ethereum itself has an ordering layer. So you have an implicit ordering of all the data blobs through Ethereum by decoupling consensus and data availability. So we are even more modular than these other solutions.
00:23:51.266 - 00:24:11.606, Speaker E: And this come with benefits and trade offs. And the benefits are very clear when you are a roll up, which is natively on Ethereum, but these other systems offer you mechanisms where you don't have to be on Ethereum for anything, and then you can be natively on celestia or available.
00:24:11.728 - 00:24:21.374, Speaker B: So in the Celestia model, though, they do have consensus and data availability, but they don't have a settlement layer. In your case, it's just the data.
00:24:21.412 - 00:24:23.354, Speaker E: Availability, purely just data availability.
00:24:23.482 - 00:24:28.766, Speaker B: And just one thing, because they had this other project, I think it's blobstream. Am I saying blobstream?
00:24:28.798 - 00:24:28.994, Speaker E: Yes.
00:24:29.032 - 00:24:31.186, Speaker B: Is that similar? Is that similar?
00:24:31.288 - 00:25:00.590, Speaker E: That's not similar. So what that does is that bridges this information from the celestia blockchain. So let's say you are an Ethereum roll up and you still want to consume the Celestia like blob space. So you're a roll up. You go and write your blob into the celestia blockchain, but your roll up contract is sitting on now, you know, you need some kind of a bridge which tells you that what has happened in the Celestia universe and then that information is bridged into ethereum. That's what blobstream is.
00:25:00.660 - 00:25:01.518, Speaker B: Got it?
00:25:01.684 - 00:26:06.354, Speaker E: So one of the key things that these other data availability systems like avail and Celestia were built on is data availability sampling. And like I was saying, data availability sampling is a mechanism to verify from a third party point of view that the data is available. And this is really useful and gives you very high trust guarantees on the system when you're natively on celestia. Because even if all the celestia validators collude and try to sign on a data item for which they did not publish the data, if you run a light node, you will try to sample the data chunks and you find out, hey, this block's data chunks are not available. Even though the validators, a majority of the validators signed off on the block, you will not accept it. And you'll say, reject this block because I am unable to access the data items inherent in it. And if everybody does the same thing, then the blockchain will stall and you can fork the chain and then retrieve it to a correct state.
00:26:06.354 - 00:27:04.094, Speaker E: So this is a superpower that is possible on blockchains that implement data availability sampling. But when this state is bridged into ethereum, because if I'm a roll up on Ethereum, or an Ethereum smart contract does not have the ability to do data availability sampling. So what that means is essentially you have to trust this majority of validators from the other network, from the celestia network or avail network, and the roll up contract. If the majority of these nodes are malicious, roll up contract still makes progress and your money is stuck in the roll up. So this can definitely happen. So the benefit of sampling is not prominent, or I would say nonexistent when you are an Ethereum adjacent layer. And so we built our system around data availability scaling instead of data availability sampling.
00:27:04.094 - 00:27:33.498, Speaker E: What these other systems did is they also made trade offs where even though the system has data availability sampling, there is no scaling of data availability. What it means is every consensus node in celestia downloads the entire block. And there are lots of technical reasons for this, but that's the architecture. And whereas it is highly scalable for verification, it is not scalable to be a consensus node.
00:27:33.674 - 00:28:09.846, Speaker D: So I think one thing that's probably worth talking about if we zoom out a little bit is why can you build such a thing on top of Ethereum? And what does it mean to be sort of reusing Ethereum's trust? There's two main benefits that I've sort of always thought of since, I guess whenever you told me about this 2021 November or something, which is first you don't have to bootstrap your own network. So if you think about something like Celestia, the market cap of the token is really important to be large, so that this consensus is hard to any.
00:28:09.868 - 00:28:11.682, Speaker E: Number going up, right?
00:28:11.836 - 00:28:53.266, Speaker D: For sure. But I mean, you have to be able to bootstrap such a network, right? You couldn't just start telestia tomorrow as a fork and hope that it's secure enough, right? It's actually quite hard to do that, and it's an accomplishment to have gotten to that market cap. Absolutely right. So first off, but I think the interesting thing about eigen layer and restaking in general is you don't have to bootstrap a token. You get to use Ethereum's market cap. And the way you're doing it is you're opting into extra slashing rules and getting potential fees, but also potential extra slashing. But that allows you to piggyback off of Ethereum.
00:28:53.266 - 00:29:00.694, Speaker D: So maybe walk us through the process and why you're able to build da in this way that doesn't rely on a new consensus.
00:29:00.822 - 00:29:52.694, Speaker E: The process for how a staker or operator opts into Eigen layer there are two mechanisms. One is called native restaking. You stake in Ethereum natively, and when you stake in Ethereum natively, you have to set who's the withdrawal address? Usually you'll set it to your own hardware wallet or wherever is the safest place you have, because when you withdraw, that's where the funds go to instead. When you opt into Eigen layer, what you do is to add a step in the withdrawal flow. You say set the withdrawal address to a contract that you create in the Eigen layer system called an eigenpod, which is your own little zone in the Eigen layer universe. And in the eigenpod contract, you then set your withdrawal address to your hardware wallet. So when you trigger withdrawal from Ethereum, the funds go into the Eigen layer contract.
00:29:52.694 - 00:30:00.570, Speaker E: And if you didn't do any kind of malicious activity and were subject to Eigen layer slashing, you will be able to then withdraw the funds into your.
00:30:00.640 - 00:30:02.846, Speaker D: Wallet plus other fees you may have.
00:30:02.868 - 00:30:48.170, Speaker E: Earned for yes, so you will be able to download your fees in the normal mode every few weeks or whatever from the Eigen layer protocol. That's why you're doing all these things. Why take the trouble of opting into other things and risks is because you are actually earning something for delivering these services. So this is the flow for a native restaking. Now that you do this, you become a native restaker on Eigen layer and you can go into the Eigen layer contracts. And because you have the Eigen pod, you can specify what services you want to opt in and operate yourself. Maybe these services are like, I want to run Eigenda, I want to run an oracle service, I want to run a bridging service.
00:30:48.170 - 00:31:31.670, Speaker E: I want to run an intent based architecture. Whatever is the set of services that you are opting in to run, you can decide it's a purely opt in system for all the sides. So you opt in and say, hey, I'm doing this. And then you have to say, who's the operator? Who's going to run this service? You could say yourself and say that, yeah, I download and run these softwares. And each of these softwares now are arbitrary software, like they are not confined to the EVM or anything like that. It's just a binary or a docker container that you can download and run on any computer. So you can start building general purpose services which have nothing to do with the EVM.
00:31:31.670 - 00:32:02.322, Speaker E: Let me explain. So now I mentioned the staker and the operator side. Then there is a service, somebody who's building these new services. They build two distinct things. One is they build a service contract which sits on Ethereum and then talks to the Eigen layer contracts. The service contract does minimal overheads and coordination. What are the coordination things? Number one, who can register into your system? Do they need 32 eth? Maybe they only need three eth because you're a different system.
00:32:02.322 - 00:32:32.830, Speaker E: Whatever. So that's number one, registration conditions. Number two, what is the payment conditions? Or if you opt into my data storage service, you store 1gb of data, you will get one, eat whatever, some kind of a payment condition. Number three, slashing condition. If you say that you're storing data and then I randomly recall you to produce the data, and then you don't do it, you will lose your etH. Something like that is the slashing condition. So this specifies a service from the service side.
00:32:32.830 - 00:33:04.818, Speaker E: So Eigen layer is the coordination mechanism, which helps stakers find operators find services. And then these three together then create a service economy where these services are then offered to consumers. Maybe it's a Defi app which is using an oracle service or an intent service or a bridge or a DA. So that's the overall architecture I mentioned. There are two ways of staking. One is native restaking, which you had to do this withdrawal credential thing. There's also liquid restaking.
00:33:04.818 - 00:33:20.620, Speaker E: You can take an LST, like the coinbase LST or the Lido LST or the rocket pool LST, and then put it into the Eigen layer contract. It's just like any token that you deposit. Now you have a status inside the Eigen layer contracts that lets you participate in this.
00:33:22.910 - 00:33:41.362, Speaker B: Mean, you sort of mentioned it as this underlying general purpose. It's not framework, but it's a space where you can deploy things. You mentioned the DA level, the DA layer, as like one of those applications. Is the restaking an application on top of it as well?
00:33:41.496 - 00:33:46.766, Speaker E: Restaking is what powers the Eigen layer.
00:33:46.878 - 00:33:55.590, Speaker B: Okay, is Eigen layer Da just kind of like you wanted to show what is possible? Or is that meant to be like an actual product that's being used?
00:33:55.660 - 00:34:36.386, Speaker E: Eigenda is a product, okay? It was designed not only to be a proof of concept, but also to be a proof of value, which means it's something that is valuable and useful and delivers fees. Because when you have this complex, multi sided marketplace, your stakers, operators, services, service consumers, like there are four sides at the minimum. In this marketplace, it's very difficult to bootstrap it. One way to do it is to actually build a powerful service, which is fee earning, so that stakers actually have. I see, something to get. So it's not meant purely as a proof of concept. It is a proof of value.
00:34:36.568 - 00:34:38.790, Speaker B: Will you be doing other things like that?
00:34:38.860 - 00:34:50.726, Speaker E: Will we be building other services? We are not intending to be building the other services. We are intending for other people to be building all these services. You briefly asked what types of other services? Maybe I can go into that.
00:34:50.828 - 00:35:15.840, Speaker B: That's kind of what I wanted to find out first. I wanted to understand sort of what is Eigenda, because maybe my initial thought was, oh, maybe you will be building three of these, and they're all sort of feeding into the system. But what I hear is you're building one to create value, so that it's actually like paying kind of through the system to show it's proof. But the other two, would you just put out proposals, or would you be like, oh, no, this is what you could build.
00:35:16.210 - 00:35:36.870, Speaker D: So anyone can build this. And I think an interesting kind of overheard, which may or may not have had some input from me into it is all l ones either die a hero or live to turn into an l two using restaking or their own da layer.
00:35:39.370 - 00:35:40.662, Speaker E: That is an interesting take.
00:35:40.716 - 00:35:47.894, Speaker D: But the reason I mentioned that is, as Sriram can tell you about, there is an l one who's moving to being an l two using restaking.
00:35:48.022 - 00:35:51.530, Speaker B: Oh, yeah, and that's the cello thing, right?
00:35:51.680 - 00:36:57.506, Speaker E: Cello is actually working with us in using Eigen DA to become a roll up on Ethereum. The total data throughput that cello was operating at was, I think, even greater than the total throughput of Ethereum. So there was no chance for them to be a native roll up. Posting data to Ethereum, not only for the low know, cello has lots of users in Latin America and so on, and very low fees, so there was no chance for them to actually be a native roll up, whereas eigenda has extremely good cost economics, and that enabled them to come become part of the Ethereum ecosystem. We are also seeing another trend, for example, near, which is an l one is working with us to build lots of services for the roll up ecosystem. So, for example, roll ups need sequencers, and near protocol already has this consensus protocol and everything already running. So you could actually have the near blockchain work with Ethereum to provide some of these other services.
00:36:57.506 - 00:37:00.238, Speaker E: As an adjacent to the Ethereum blockchain.
00:37:00.334 - 00:37:03.106, Speaker B: Can you describe, let's kind of going back to that initial question that what.
00:37:03.128 - 00:37:33.798, Speaker E: Are the services, what are the categories? So we are seeing, actually it has been amazing to see from our own vision has been open innovation. So we want to maximize the surface area of permissionless innovation. That's really what motivates us in this project. But when we started, we had like a couple of examples of what might be possible as Eigen layer services. And today in the restaking summit, I just gave a talk where I showed like 25 new services in five categories.
00:37:33.894 - 00:37:34.594, Speaker B: Oh, neat.
00:37:34.662 - 00:37:38.282, Speaker E: And I'll maybe give a sense of these categories and what some of the most exciting.
00:37:38.346 - 00:37:42.030, Speaker B: Are you publishing that somewhere? Because maybe we can add that to the sheet.
00:37:42.770 - 00:37:44.126, Speaker D: Do you recorded all the.
00:37:44.228 - 00:38:20.354, Speaker E: We will have a video so I can give a link to that, or just send a picture. For example, one category which is very obvious and where we see immediate traction is roll up services. So roll ups need lots of adjacent services in order to make the roll up economy work. One example I was just mentioning is sequencing. How do you, a single sequencer is like a censorship bottleneck in the roll up system. Do you want to have a small group of decentralized sequencers or a large group of nodes which participate in ordering transactions?
00:38:20.422 - 00:38:24.510, Speaker B: So is that decentralized sequences or that is decentralized shared sequencers?
00:38:27.090 - 00:38:51.138, Speaker E: It could be a shad sequencer or like a non chat sequencer. Yeah, but both of them need decentralization, and so all of them can use Eigen layer to actually build these kinds of. We are seeing many different models of decentralized sequencing being built. But espresso, which is a leading shad sequencer, is also working with us in sharing security from Ethereum, in addition to their own native token.
00:38:51.314 - 00:40:08.250, Speaker D: So as a disclosure, Eigen layer investor, as also Anna, is, I think the 0th order model I had in my head for how this network accrues value despite not having its own token. And layer one is effectively this idea that if you think about all the roll ups in the world, they're eventually going to have to have decentralized sequencers. Those fees have to go back to Ethereum somehow, right? Ethereum is going to be losing a ton of fee revenue as more and more value migrates away. And the main way of, unfortunately, this word is overused, aligning the roll up fees with the proposer incentives and the l one is to actually have a way for the l one proposer to also earn the roll up fees. And the natural way to do it is via something like Eigen layer, because if I use restaking, I'm reusing ETH, I'm earning fees in ETH. I'm sort of in some ways, giving the roll up some of my ETH in exchange for some of their fees. One interesting thing that I've been thinking about a lot is if you look at this model of like Eigen layer for restaking, the different roll ups, plus Dax and ETh just being the place where data is posted, some data is posted, or maybe proofs of validity are posted.
00:40:08.250 - 00:40:11.786, Speaker D: You really do start looking like Polkadot without the auctions.
00:40:11.978 - 00:40:17.826, Speaker B: Yeah. Actually, tarun, you kind of drew this out. You said there's like these three pieces that basically make you.
00:40:17.928 - 00:40:25.220, Speaker D: It really looks like Polkadot, except for the auctions. I think the auctions were just very expensive for the parachains here. This is much more.
00:40:25.530 - 00:41:10.798, Speaker E: Yeah. So the comparison to Polkadot is actually accurate in one sense, which is that basically parachains gave a certain level of programmability while also maintaining shared security. But there was a certain amount of homogeneity which was needed. For example, they all had to be in Wasam, and you have to write your virtual machine on top of that. And not only that, you only share security in the Polkadot model for the execution. For example, let's say you want to build a secret sharing service where you take a secret and then encode it into chunks and then send each node a portion of the secret. You cannot really do this in the shared security model of Polkadot.
00:41:10.798 - 00:41:55.854, Speaker E: So the way we think about it is in the history of blockchain, like Ethereum was created as this Turing complete, general purpose programming language, but it gave you only a programming interface at the level of the virtual machine. And then all the coordination about how the distributed system is managed, how the consensus is managed, and all of it was internalized into the protocol. And what we started, as we were thinking about new ideas for consensus and scaling and so on, what we found is this limited the level of permissionless innovation that could penetrate into these areas. And so if I had a new idea as an academic, we had tens of papers on consensus protocols, and we talked about many of them in the last ZK podcast.
00:41:55.902 - 00:41:56.500, Speaker B: Yeah.
00:41:57.110 - 00:42:26.938, Speaker E: And if I were to go build a new blockchain for each new consensus protocol, that would just be like a completely nonviable way to do things. Whereas what Eigen layer does is give you the first general purpose programmable distributed trust system. So you can say what each of the nodes in this system have to run. You have complete programmability at the level of the distributed system. So you can start building basically anything that requires decentralized trust.
00:42:27.104 - 00:42:39.454, Speaker D: Yeah. For the record, I wasn't saying it's like exactly Polkadot. It was just more. It's funny because I feel like Ethereum was always like, we're never going to have fishermen. And it's sort of like indirectly all.
00:42:39.492 - 00:42:41.600, Speaker E: Of these things are happening actually. Right.
00:42:41.970 - 00:42:44.782, Speaker B: Everything converges to Polkadot.
00:42:44.926 - 00:42:51.300, Speaker D: I feel like Ethereum is very good at taking good ideas from different places and then smooshing them together.
00:42:53.510 - 00:43:10.442, Speaker E: I think there's also something similar to be said for Cosmos and how a lot of the ideas from cosmos also percolated back to Ethereum. There's the idea of interchange security and how Eigenve is related to that. Yeah. So for sure, going back to this.
00:43:10.576 - 00:43:12.074, Speaker B: The services, I want to hear more.
00:43:12.112 - 00:43:46.478, Speaker E: Going back to the set of applications, roll up services, some of the examples are, I mentioned decentralized sequences, bridges, for example, you want to build a super fast bridge between two ZK rollups, each of the ZK rollups only settling on Ethereum every few hours because of the batching efficiencies. But I still want to interoperate between them at a much faster pace. Can I build some kind of a restaked service which knows the state from the other thing and then puts an economic collateral at risk and then starts helping you bridge between roll ups? That's an example you can start thinking about.
00:43:46.664 - 00:43:50.950, Speaker B: Could you do something like coprocessors, that sort of coprocessor model?
00:43:51.100 - 00:43:55.878, Speaker E: That's the next category. You're right on target. Okay.
00:43:56.044 - 00:43:58.280, Speaker B: Eigen layer is taking over everything, though.
00:43:59.370 - 00:44:50.662, Speaker E: So to finish the roll up thing, one more category that we see is like, fishermen know. Tharun just alluded to in Polkadot, the idea of who's watching? If there are a lot of optimistic roll ups, somebody needs to be watching these optimistic roll ups to trigger fault alerts. And today there are like a handful of major optimistic roll ups, and there's lots of extraneous parties whose job involves also watching the network. Because you're an RPC, you are an exchange, you're a block explorer. Whatever your job is, you happen to be watching the network. But in the era of thousands of application specific roll ups, and some of the roll ups are actually building to be highly transient. Like they just open up, be a roll up for a few hours and then vanish, do your NFT distribution and then vanish or whatever.
00:44:50.662 - 00:45:05.934, Speaker E: So these kinds of rollups, who will be watching and nobody knows whether there'll be enough people watching. So a watchtower service is being built where a random group of nodes are selected to watch each roll up and you can spin up tasks and so on.
00:45:05.972 - 00:45:07.050, Speaker B: And that's the fisherman.
00:45:07.130 - 00:45:41.478, Speaker E: That's a fisherman like service on Eigen layer coprocessors is the next category. Like roll ups. The way I define coprocessor is like a serverless lambda. It's a stateless service. I'm sitting in Ethereum and then I want to run an AI inference and then I want to consume the output of the AI inference. Why? Maybe because I want to do intelligent deFi. Like I put my money into a uniswap pool and I don't want to get raided by uniswap X, basically only sending toxic flow into my liquidity provision.
00:45:41.478 - 00:46:23.954, Speaker E: So what I say is I put my money into a pool and say that the price of this pool is modulated by an AI protocol. And the AI protocol looks at all the history of the trades and then tries to adjust the spread so that the toxicity is contained. But I need this AI inference to be highly trusted because if somebody says oh, one, each should be set as $20 and then somebody can come and raid the pool, you don't want that. So you want this AI inference to be highly trusted. So what you could do is run an Eigen layer service where there is enough economic security. Let's say you want to get at least 100 million dollar economic security because in a given day your trade volume is less than 100 million. Then that makes the system like fully secure.
00:46:23.954 - 00:47:11.666, Speaker E: So this is an example of a coprocessor. Another example of a coprocessor might be I want to run a Linux box and like this particular program on this Linux box and get the output and then promise it on ethereum. Or I want to run a database, I want to run a SQL query and then get it back. So all of these things could be either done using ZK technologies or they could be done using crypto economic security. And the trade off here is like what is the excess cost of proving that goes into cryptographic, like a ZKML or a ZK SQL or whatever set of solutions. Or instead of paying for the cost of proving, I can pay for the cost of capital and then borrow the security from eigenvector.
00:47:11.698 - 00:47:29.770, Speaker B: But in moving into the coprocessor space or having that be an option, is there still a reason to create like a very from the ground up coprocessor? Do they still get some advantage in being able to build it from scratch versus building it on Eigen layer.
00:47:30.350 - 00:47:48.302, Speaker E: Is there an advantage of building a coprocessor from scratch? I mean, the way I think about it is it's not a binary question of if you are building on Eigen layer versus building on your own, because Eigen layer is fully programmable. So whatever you can build on your own, you can also build on Eigen layer.
00:47:48.366 - 00:47:52.142, Speaker B: Does it lose anything? By building on Eigen layer the constraint.
00:47:52.206 - 00:47:54.142, Speaker E: That you're suffering, you lose the ability.
00:47:54.206 - 00:47:55.434, Speaker D: To make a token.
00:47:55.582 - 00:47:57.830, Speaker E: No. Okay, that's a misconception.
00:47:58.330 - 00:48:00.786, Speaker D: You lose the need to do it at inception.
00:48:00.898 - 00:48:46.450, Speaker E: Okay, that may be what it does, is this is one of the things that everybody asks me is, hey, if you're saying that you don't need a token for your own middleware or service or whatever you're building, then what would people do? Like, where are they? You know, first thing to observe is that's already true for being an application on Ethereum. Every DAP on Ethereum also has a token. The tokens used for governance or other purposes. In eigen layer, we also provide native support for something called dual staking. So let's say you have a coprocessor and you have a coprocessor token. You can have both the coprocessor token be staked and the Ethereum token be staked. So you're borrowing a sense of economic security.
00:48:46.450 - 00:49:34.622, Speaker E: And how much of each you can decide by controlling how much fee you're willing to share between the two layers. And over time, maybe for bootstrapping, you need a lot of eth, and over time you decide like it's not that beneficial to your system, so you can tune it out to using more of your own token later on. So this is the kind of coprocessor category. But in general, this question shows up a lot. Hey, what happens to my token? And the answer is nothing. Actually, fundamentally, if you look at the economic value of your token, it's coming from the future expected rewards. And if in the future, expected rewards are maximized by not using each security and only using your own token security, you can tune the dual staking all the way to send all my fees to my own token.
00:49:34.686 - 00:49:35.170, Speaker B: Oh, wow.
00:49:35.240 - 00:49:43.240, Speaker E: So you really don't have any loss. It's just optionality. And you can use the optionality in ways that most benefit your own community.
00:49:43.770 - 00:49:56.422, Speaker B: I feel like you already kind of at the beginning, or like a few minutes ago you defined restaking really well, but I feel like I still need to go through the motions.
00:49:56.486 - 00:50:33.746, Speaker E: No, but maybe this will help. I think restaking became a meme and a word, so we stuck with it. But really what we're building is permissionless programmable staking. So when you're staking in any given blockchain protocol, what is happening is you are making a promise to run that blockchain protocol according to the rules. Otherwise you're liable to lose your eth. And each blockchain, when the blockchain is created, specifies these rules, and they're programmed into the rules of the blockchain. But what we figured is it's just stake.
00:50:33.746 - 00:50:59.582, Speaker E: And if ethereum is Turing complete programming language, I can subject your stake to arbitrary slashing conditions so that you now create this general purpose layer for programmable staking where anybody. So it's permissionless programmable staking because anybody can come and create new staking conditions by writing new programs, and then you can bind yourself to them.
00:50:59.716 - 00:51:05.650, Speaker B: I thought, though, when I first heard restaking, that it was somehow in the category of like liquid staking, but it isn't.
00:51:06.150 - 00:51:08.514, Speaker E: A lot of people think they thought that.
00:51:08.552 - 00:51:30.530, Speaker B: Yeah, but as a staker, can you just walk through what's happening if they stake? Normally a staker will be like running their validator and staking their eth to the validator, or they'll be like staking through a pool. In this case, they're staking with a new piece of software. Is it a validator as they know it? This is the part I wanted to understand.
00:51:30.620 - 00:51:35.770, Speaker E: We have two roles, a staker and an operator. The operator is the one who's actually running the service.
00:51:35.840 - 00:51:37.002, Speaker B: It's the validator kind of.
00:51:37.056 - 00:51:37.734, Speaker E: It's the validator.
00:51:37.782 - 00:51:38.042, Speaker B: Okay.
00:51:38.096 - 00:51:47.342, Speaker E: Right. So what the staker does is the staker puts up the money into the contract and then they specify who their operator is.
00:51:47.396 - 00:51:47.758, Speaker B: Okay.
00:51:47.844 - 00:52:29.334, Speaker E: It could be themselves, or it could be like delegated and why they trust the delegate and so on. It's up to them. And then because the delegate is now the operator downloads and runs all these containers that the staker is opting into. Let's say the staker opts into, I want to run this, Oracle and data storage and so on. Then the operator has to actually go and download and run all those services. And by them downloading and running these services, they earn a certain fee, and the staker keeps a certain fraction of the fee and the operator gets a certain cut of the fee. So it's very much like staking any other thing, except it's like an infinitely expansive staking protocol.
00:52:29.334 - 00:52:33.600, Speaker E: You stake once and then you can restake into all these services.
00:52:35.330 - 00:52:46.226, Speaker B: Do you have to sort of stake more? And then a portion of it is going towards regular staking? No, actually, is a portion of it going towards regular staking on Ethereum? That is correct, yes.
00:52:46.408 - 00:52:49.730, Speaker E: You're subjecting your 32 e to multiple conditions.
00:52:50.070 - 00:52:57.982, Speaker B: How do you actually do that under the hood, though? Because the operator still needs to have a validator with 32 e. It can't change the rules of Ethereum.
00:52:58.046 - 00:53:38.690, Speaker E: That's right. So this was one of the early things we had to kind of figure out this hack, which is the thing I was explaining about the withdrawal credentials you stake in Ethereum. Whenever you stake, you have the ability to set the withdrawal credential to your own wallet, but instead you can set it to a smart contract on Ethereum. And in that smart contract, you set your withdrawal power to yourself. So that smart contract is the Eigen layer smart contract. So you stake in Ethereum and then set the withdrawal address to an Eigen layer smart contract. In the Eigen layer smart contract, you say that, hey, I am the one who has the ability to withdraw this money at the end of the day.
00:53:38.690 - 00:53:47.190, Speaker E: And what this does is enables the Eigen layer contract to take away your eth if you misbehave.
00:53:49.210 - 00:53:57.174, Speaker B: But still, I guess the thing that isn't solved here is the 32. Does that mean that anyone using the restaking have to put up more than.
00:53:57.212 - 00:54:12.618, Speaker E: 32 e if you're restaking natively? So this is what we call native restaking. Okay, but you could take like steeth and put up 3.17 steeth into the Eigen layer contracts and then specify who the operator is.
00:54:12.704 - 00:54:50.022, Speaker D: Okay, so I have to say, I remember the first time Shiram mentioned this idea to me. I forgot what name you had for it. This must have been like August 2021 or like July or something like that. And he had the most confusing name for this routing contract that sits in front of the staking, which was middleware, which is like a very 2000s tech company term because it is like middle, like that is what middleware is like. Software sitting in the middle, add extra rules. But I did not understand it until I asked a bunch of questions. I feel your pain, but just going.
00:54:50.076 - 00:55:01.030, Speaker B: So I see this using staked Eth, you're already, let's go through that example. So you're saying either you're going to put up 32 plus something that could be used for the restake.
00:55:01.110 - 00:55:03.774, Speaker E: There's no plus something, it's just 32.
00:55:03.812 - 00:55:07.386, Speaker B: But if you just put up 32 doesn't all go through to the validator.
00:55:07.498 - 00:55:11.120, Speaker E: All goes to the validator. You have to set the withdrawal address.
00:55:11.810 - 00:55:14.800, Speaker B: It's the slashing that you lose on. Okay, that's right.
00:55:15.170 - 00:55:18.434, Speaker E: When you withdraw, okay, you will not be able to withdraw your entire money.
00:55:18.472 - 00:55:23.106, Speaker B: And the yield comes because of those services that's paying down through the system. I get it.
00:55:23.128 - 00:55:28.966, Speaker E: Okay. It's like taking a parlay bet. You lose your bet if any one of those things.
00:55:29.068 - 00:55:35.458, Speaker B: Okay, now that's the native. Now walk me through liquid staking. The liquid staking, one of the easiest.
00:55:35.474 - 00:55:47.226, Speaker E: Ways to participate in Eigen layer. You just take your steeth token or CBE token, put it into the eigen layer contracts, and then specify some operator like Coinbase or whatever, and then let them do their thing.
00:55:47.248 - 00:55:52.942, Speaker B: And again, you have a withdraw access, so that if you do something bad, you get slashed. Yeah.
00:55:52.996 - 00:55:55.770, Speaker E: It's not only a withdrawal access. Your steeth is sitting in the contract.
00:55:55.850 - 00:56:08.690, Speaker B: And yet, basically you allow this to still participate in the system because it's staked ETH. It's already connected to Ethereum and staking in some sense.
00:56:08.840 - 00:56:17.650, Speaker E: Eigen layer contracts are designed to be highly general purpose. I could put in USDC for all I care. Okay, so you can stake anything you want. It's general purpose staking.
00:56:17.730 - 00:56:18.118, Speaker B: Yeah.
00:56:18.204 - 00:57:09.350, Speaker E: So there's nothing specific about St e or LSE. It's just that these are already reward earning, right, because I'm already earning the base layer rewards. And in addition, I'm getting these rewards, making it very easy for people to not worry about the opportunity cost of capital. Whereas if I ask them to put in unencumbered ETH or USD or anything, then we have to worry about, are they making that 5%, 6%? This is like, I already got my four and a half percent, 5%. So this is something on top of it. So that's the economics that makes it more favorable for us to ask for. It's just in the staker's favor to actually stake one of these other assets rather than native eTh.
00:57:09.350 - 00:58:20.250, Speaker E: Another category that we are super excited about is cryptography, which many of your listeners may be super interested in. So all kinds of interesting new cryptographic systems can be built on Eigen layer. For example, if you want to build a system using secure multiparty computation, there's no way for you to build that either as a roll up or as a native smart contract on Ethereum, because you are specifying what kinds of computation each node should do and what specific information each of those nodes hold. For example, imagine a system like penumbra which has state which is dispersed across these different nodes. And you want to build applications on top of this, you can build this on Eigen layer because you have a decentralized network that you can borrow. So you can build something like Panambra as an avs on top of Eigen layer. You can build threshold encryption, right? So let's say you want to build an encrypted mempool where you send transactions, and the transactions are all encrypted to a threshold key, which is in the threshold group.
00:58:20.320 - 00:58:22.286, Speaker B: Yeah. Which would prevent like sandwich attacks, which.
00:58:22.308 - 00:59:05.658, Speaker E: Would prevent sandwich attacks. So these can be built on Eigen layer. You can build threshold fully homomorphic encryption systems. You can start building basically anything that requires decentralized trust can be built on Eigen layer and the proximity to the Ethereum ecosystem. What it reminds me of if this analogy is useful, is if you look at how web application development worked back in 1995. If you wanted to write an application, you have to put up your own server stack, and then on top of it you write your own identity stack, your payment stack, your database stack, and then the specific application. If you're selling a books, you have to then do whatever that particular thing is.
00:59:05.658 - 00:59:10.254, Speaker E: But you're building all of this all by yourself. To me, blockchain development, that was just.
00:59:10.292 - 00:59:15.600, Speaker D: Proof that you're from Seattle, also showing my age.
00:59:18.690 - 01:00:28.530, Speaker E: You know, I remember geocities, for example, used to be there where if you just wanted to put up a dumb web page, you can go to geocities and host it. But if you want to do anything more powerful, you had to do all these things that I'm talking about. That's exactly the situation that I see in blockchain today. If you just want to write a simple smart contract, you can just throw it on top of Ethereum, but you want to do anything more powerful, you have to build everything. You have to build your own token of value, you have to build your own validator network, you have to build your own consensus, scaling all of these things inside your own blockchain. Instead, what's happening in 2023 web application development is you say go to the cloud, use AWS. And on top of the cloud, there are thousands of successful software as a service solutions, and each of them hyper specialized in some particular domain, saying, hey, I am an authorization service for social networks like Oauth, right? I am a NoSQL database for enterprise applications, very specific in the type of use cases that are being dealt.
01:00:28.530 - 01:00:47.194, Speaker E: But these are still more foundational pieces. And what happens is consumer applications integrate a bunch of these SaaS services in the back end and then create an end user application. A typical end user application in the web uses 15 SaaS services in the back end.
01:00:47.232 - 01:00:47.674, Speaker B: Wow.
01:00:47.792 - 01:00:56.890, Speaker E: So when people come and tell us, oh, if you have a lot of modular blockchain, you have to pay a fee on each of these layers. That's exactly how the Internet works, if you haven't noticed.
01:00:56.970 - 01:00:59.770, Speaker B: Wow, so you're bringing SaaS to blockchain?
01:00:59.850 - 01:01:49.950, Speaker E: Yes, unleashing the SaaS era of blockchain, because SaaS is actually open innovation. So our core thesis is open innovation, which is somebody who's super specialized in building, let's say fhe for some particular application should just build that, and an end user application should consume these services as they need, and they all interoperate through a shared security layer, which is eigen layer. So that's our vision. And so what we envision is people building on top of us build these more protocol. You can think of it instead of SaaS, it's protocol as a service, right? Anybody can build an arbitrary protocol and launch it as a service. And now you can concatenate these protocols as a service and then build end user applications. So that's the vision that we're building.
01:01:50.020 - 01:02:12.886, Speaker D: One very big different financial piece of this versus SaaS, though, is that you have dynamic pricing at all times. Versus SaaS is oftentimes mainly. Obviously there's preemptible nodes in cloud, but you need to get to a certain scale for dynamic pricing to work, whereas here you have dynamic pricing from the beginning. And that's like a totally different economic world.
01:02:13.068 - 01:03:01.586, Speaker E: And I don't like that we're trying to change it. So for example, in eigenda you have static reserved pricing. So when you think about something like AWS, people say, oh, how much block space do you have? And nobody asks, aws, how much cloud space do they have? The cloud space expands to fill the requisite demand. And the reason it does it, and does it in a very smooth manner is 70% of the instances running on AWS are actually reserved instances. And there's also the spot instances where you can go and ask right away, give me something which is dynamic pricing, but there's also the reserved instances which give you like a long term correlation which gives you price certainty. Absolutely. Completely different economics.
01:03:01.586 - 01:03:19.946, Speaker E: Eigenda is built on this dual model where you have a spot market where you can go and buy bandwidth in the moment. But as a roll up, I know I need 100 next one year. So I prepay that fee with a very good discount, and then I have access to it.
01:03:20.048 - 01:03:25.374, Speaker B: This reminds me of a very old project. I don't know if this ever turned into the gas token. Do you remember this?
01:03:25.412 - 01:03:27.710, Speaker D: It kind of died because of EIP 1559.
01:03:27.780 - 01:03:56.934, Speaker E: Yeah, I think the difference between things like gas token and this thing is the gas token is like spot futures, where you're actually trading like what is the spot future? Instead, reservation bandwidth is like contract pricing. You have a contract with your oil supplier for the next one year to supply a barrel of oil at this price. So I think contract pricing is directly happening between the seller and the buyer, so it's actually much more rigid in the types of guarantees that you can provide.
01:03:57.132 - 01:04:46.390, Speaker D: Yeah, although I think I would guarantee that if you went and interviewed a bunch of web two companies and you were like, hey, would you be willing to pay cloudflare on dynamic pricing, where most of the time you're actually going to pay ten times less than what you're paying now, but sometimes you're going to pay ten times more? I bet you most people would find that appealing. But if you think about how SaaS payment processing works and how there's not really streaming notions of streaming payments, you effectively have this huge overhead for startups like stripe doesn't offer you dynamic pricing. You have to get to the scale of uber or AWS for you to have dynamic pricing. So there's also this thing though, that I think to me has always been the beauty of crypto, that you can be arbitrarily small in size, but have dynamic pricing if you need it.
01:04:46.540 - 01:05:37.362, Speaker E: Yeah, so that's a really good point. If we only have reserved instances, then small rollups will not be able to use layer. So the availability of these multiple pricing models is useful, but also the cost certainty that a reserved bandwidth gives you, because imagine you're a roll up and you have uncertain data availability costs over the next one year, but you have to go tell your users at Coinbase like how much it's going to cost on a daily basis. How do you go do this? So it becomes impossible. For example, we know, for example, Ray Dalio help McDonald's hedge soy futures and so on, so that their burgers can actually have a constant price over longer timescales. These are mechanisms that are needed to actually build pretty rigid markets.
01:05:37.506 - 01:06:18.866, Speaker D: Yeah, I just think the difference is most non fully native payment mechanisms, which is like almost everything, right? Only at really high scale can you do these kind of streaming crazy things. It's always been quite hard to get to that access to that. And I think, to me, crypto, the beauty of it is that you can have that if you want it, because obviously people love static pricing, right? It's like, nice. But if I can really offer you way more efficiency on average, there's a lot of people who would love to reduce their Cloudflare bill, for instance. Right. If it was dynamic. But it's just a pain in the ass for them to do it because they are not even their own payment provider.
01:06:18.866 - 01:06:36.860, Speaker D: They need their payment provider to offer. And I think that somehow this notion of services that start from being programmable on how they take payment is like, there's a fundamental difference between this type of stuff and SaaS. So hopefully we don't have to use the word middleware again.
01:06:39.230 - 01:06:45.278, Speaker B: All right, well, I think we've reached the end of the episode. I know I've reached now the end of my time.
01:06:45.364 - 01:06:46.494, Speaker E: It's late night here.
01:06:46.532 - 01:06:51.658, Speaker B: It does connect. It is late here. Thank you so much for coming back on Sriram.
01:06:51.754 - 01:06:59.722, Speaker E: Thank you so much, Anna. Thank you, Taran. This is super fun, chatting about all these things with you and looking forward to hang out with you all at other events.
01:06:59.866 - 01:07:06.610, Speaker B: Thanks, Sriram. I want to say thank you to the podcast team, Henrik, Rachel, and Tanya, and to our listeners. Thanks for listening.
