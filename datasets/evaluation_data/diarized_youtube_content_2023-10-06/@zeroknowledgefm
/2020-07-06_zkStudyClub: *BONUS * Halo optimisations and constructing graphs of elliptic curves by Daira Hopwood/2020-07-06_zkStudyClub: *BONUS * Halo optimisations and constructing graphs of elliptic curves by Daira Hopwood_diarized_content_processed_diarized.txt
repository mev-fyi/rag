00:00:00.730 - 00:00:22.430, Speaker A: This is the bonus. ZK study club. We weren't able to finish off Dara's presentation last week, and so we decided to do this. The original presentation is called halo optimization and constructing graphs of elliptic curves. And Dara, where are we? Maybe you can give a little bit of a recap and help us get oriented.
00:00:23.330 - 00:00:39.900, Speaker B: So we basically did the constructing graphs of elliptic curves bit, and we still have the halo optimizations bit to do. I should have switched around the title, really. Okay, do you want me to start?
00:00:40.590 - 00:00:41.340, Speaker A: Absolutely.
00:00:41.710 - 00:02:12.680, Speaker B: Okay, so the story so far is that we defined what graphs of elliptic curves are. Basically you have a bunch of primes that give the fields of definition of your curves, and you define the elliptic curves as edges between those primes. We explained why it can be difficult to construct cycles of curves. And there's a category of curves called complex multiplication curves that solve this problem. So we defined that, and we talked a little bit about latices of endomorphism rings, but all we're going to take from that part really is the idea of an endomorphism, which is it gives you a shortcut to scalar multiplication on the curve. And then we talked a little bit about constructing graphs that include pairing friendly curves and did some live coding for that. Okay, and I think that's a summary of what we did last time.
00:02:12.680 - 00:03:25.200, Speaker B: So now we're going to jump straight into accumulation schemes. A recent paper on this that came out after the halo paper, it's this BCMS 2020, and it basically formalizes the idea behind halo. It's actually a slightly simpler version of what we do in Halo. So the basic idea is you have a proving system. That part of what you need to do to verify proofs is quite expensive. So you don't want to have to do that every time you're iterating the recursive proof system. So what you do instead is you have a way to combine this expensive part of the verification, so that you only need to do that once at the end.
00:03:25.200 - 00:04:35.220, Speaker B: So your circuit here is verifying a previous proof, and it's also combining this accumulator with the accumulator generated by this verification. So that's the idea. But there's a complication. So your proving system can only efficiently do arithmetic in one field, and that's not the same field as the field of definition of the elliptic curve. So if you tried to implement it exactly as shown in the diagram, then you'd have to do lots of wrong field arithmetic, and that would be very expensive. So what we do instead is something like this. So the purple colored things are one of the fields in your cycle, and the orange colored things are the other field.
00:04:35.220 - 00:05:34.070, Speaker B: This convention is actually taken from Isaac Merckler's presentation on the pickles proving system. And these are nicely contrasting colors, I hope if there are any colorblind people here, purple things on the left and orange on the right. And then we have the arrows here, kind of FQ flavored proofs. So the proofs that are easy to verify when you can do arithmetic in FQ. And here, this is a proof that's easy to verify if you can do arithmetic in FP. So you basically just alternate between two proving systems, one defined on FP and the other defined on FQ.
00:05:37.470 - 00:05:57.082, Speaker C: I just want to sort of complain that what frustrated me about, well, maybe to a degree, halo, but also this BCMS paper that I like very much is that they didn't do this sort of extra step of exactly like this diagram.
00:05:57.146 - 00:06:01.374, Speaker B: Because really this is precisely why I'm doing it now.
00:06:01.492 - 00:06:10.850, Speaker C: Yeah, they're saying you're fertilizing Halo, but to really do that, you need to do this moving between two accumulation schemes.
00:06:14.070 - 00:07:43.220, Speaker B: So your proof at the end, you end up with two instances of the accumulation scheme that you need to check and one proof, which is dependent on where you are, whether you're an OD, number of steps, or an even number of steps. And this is just the case for incrementally verifiable computation, IBC, where you have just a chain of proofs that you want to verify. The more general thing is called PCD, or proof carrying data, where you have a directed ecyclic graph of things you want to prove. So this solves part of the problem, but it turns out there's a complication still, because your proofs are actually not just in one field. So your proof consists of some values in one field and some values in the other field. So these are what we call deferreds. So instead of sending them to the other proof system, we defer them until the next step in the same field, and similarly here.
00:07:43.220 - 00:08:56.950, Speaker B: So they basically bypass the next step and they can get checked in the step after. So that way we only ever have to do arithmetic in the right field, not the wrong field. And then at the end you end up with two deferreds, one in each field that you also have to check in addition to the final proof and the final accumulators. Okay, so that's basically it for the incremental computation case, you just have a chain. Let's see how it goes for when you have a general directed asyclic graph. So the arrows here mean that this node on the middle left is dependent on the proofs on the top left here and top middle here, simile and so on. And you can see there's one proof here that is used in two places.
00:08:56.950 - 00:09:57.340, Speaker B: So what happens when we have to basically assign these to one proof system or the other? So, to FP or FQ. So if we do that, we find that we have to add another dummy proof here, which is just wrapping this orange one. And then we have that the proofs alternate along every path, so everything is still efficient. And in this particular example, we only needed to add one proof. To do that, in general, we might have to add any number. So that diagram didn't show the deferreds, but you can see that these, they skip one proof and go on to the next one. So here and so on.
00:10:01.890 - 00:10:07.840, Speaker C: Yeah. So I want to try to get also the basic case sort of clear.
00:10:09.090 - 00:10:12.000, Speaker B: Yeah, go on my head.
00:10:12.370 - 00:10:34.920, Speaker C: Like say you're using the Buddha bulletproof polynomial commitment scheme. I never remember this, but you have like in the verification, you have like a linear number of field operations and a linear number of group operations. Or is that true?
00:10:37.550 - 00:10:46.010, Speaker B: To decide the accumulation scheme only takes a linear number of group operations.
00:10:48.670 - 00:10:55.150, Speaker C: There's no field operations. Oh, for the commitment scheme you're saying it's only group operations.
00:10:55.810 - 00:11:10.690, Speaker B: So let's go back to here. Okay. Yeah. This decider here and here is only doing a multi x, basically a multiscale multiplication.
00:11:13.910 - 00:11:30.970, Speaker C: Okay. So the polynomials in admit scheme, it only has some multi x, but there's also these operations of the verifier that are just in the field, that are just checking some equation.
00:11:32.270 - 00:11:55.780, Speaker B: Yeah. So these are in the circuit here. So we have two things to check. We have the deferred from the previous instance and we have the proof from the other side proving system. And I forget which of these are field operations and which are group operations. To be honest. You have to ask Sean about that.
00:12:01.670 - 00:12:02.420, Speaker C: Okay.
00:12:03.190 - 00:12:17.830, Speaker B: You have some field operations and some group operations, but the field operations are directly in FP and the group operations are in the elliptic curve over FP. So both of them are efficient.
00:12:18.970 - 00:12:27.820, Speaker C: Yeah, I'm trying to just remember if the field operations you also have to accumulate, or is it just always a small number?
00:12:28.430 - 00:12:51.250, Speaker B: So this includes, there's a verification operation that you need to do on the. So the inputs to the operation are the two things that you want to accumulate and then you have to verify it and the output is the accumulator going into the next proof.
00:12:53.430 - 00:13:00.790, Speaker D: Dara, are you going to show us at some point more explicitly, what exactly are each of these arrows here, like the deferred and the accumulate?
00:13:03.130 - 00:13:28.640, Speaker B: I'm not, because I think that was in Sean's presentation. So if you go back to the YouTube video for. I can't remember how much detail he went into, but if I were to explain that, I'd basically just be pointing you to figure one of the halo paper.
00:13:32.190 - 00:13:33.258, Speaker D: Okay, thanks.
00:13:33.424 - 00:13:59.650, Speaker B: Okay. Right. Okay, this is where we were up to, just adding in the deferreds to the graph. So this is one way of doing it. You just add dummy proofs only where you need to. The other way of doing it would be this. So you add a wrapper proof after each node.
00:13:59.650 - 00:15:23.870, Speaker B: So that's more regular, which might be useful for some protocols. If, say, you didn't know the structure of the directed acyclic graph in advance, it might be easier to do this. So this looks kind of complicated when you add in the deferreds, but it's not that bad, really. Each proof has the proof itself. It has a list of deferreds in each field. So the ones that were added in the immediately preceding step and the ones that were added in the step before that. If you see what is that clear? For example, if you just take a slice through any of these arrows, then you would have at this proof, you would have two orange deferreds, one purple deferred, and you would have this proof and then you would have the accumulators.
00:15:23.870 - 00:15:40.762, Speaker B: Okay, that's a good place to. Does anyone have any questions? Because I can't see the chat. No questions?
00:15:40.896 - 00:15:42.750, Speaker A: No chat so far I don't think.
00:15:42.900 - 00:16:52.370, Speaker B: Okay, cool. So the other thing we need to know if we're going to optimize this is what arithmetization, what circuit model halo uses. So it's exactly the same model as sonic. So I assume that people are reasonably familiar with r one cs. In r one cs you have these constraints where any of a, b or c is a linear combination of variables. And in sonic or halo, we have to split those out. So we have each multiplicative constraint, introduces three fresh variables, and then separately we have linear constraints which can refer to these variables that were introduced in the multiplicative constraints.
00:16:52.370 - 00:18:00.390, Speaker B: You can't directly, for example, say a times a equals a, which would be a Boolean constraint on a. You would need two extra linear constraints. So you would have a one times a two equals a three, and then a two minus a one equals zero and a three minus a one equals zero. And that would be your Boolean constraint. But in practice it's pretty easy to translate between that and r one cs. So you don't really have to worry very much about needing different optimizations for the sonic arithmetization as you would for r one cs. It's all the same ideas, and I think for some pathological circuits you might end up having to add dummy multiplicative constraints just to introduce variables.
00:18:00.390 - 00:19:05.386, Speaker B: So if you're finding you need to do that, then just try and reduce the number of them. You can usually get away with the same number of modifications as in I one cs. Okay, so the halo verification circuit consists of. These are the two operations that dominate the performance. So we need multiscalar multiplications and we need to implement fiat chamele. Because halo uses the fiat chameleon paradigm, we start with an interactive proof system and we instantiate the verify. We replace the verify with a hash function, as well as verifying the parent proofs, as it were.
00:19:05.386 - 00:20:08.020, Speaker B: We also need to verify some application circuit. So we need to not over optimize for these two operations. We need the circuit model to be flexible enough to do other things that people might want to do. But as it turns out, people very often want to do elliptic curve arithmetic or hash functions anyway, so it's not too much of a problem in practice. Okay, so how do we do scalar multiplication? We're using prime order short vastros curves of this form. So this is the obvious way to do scalar multiplication for a variable base. I'm just using double and add.
00:20:08.020 - 00:21:08.760, Speaker B: It would take four multiplications for the doubling three to add the base point p and then two, because you need to compute either two a or two a plus p. So basically you conditionally switch on the x coordinate and then separately for the y coordinate. That costs two constraints. So that would cost us four plus three plus two, nine constraints per bit. So this ticket describes how to do it in six modifications per bit. It's based on this paper. So the idea is you just rearrange this to a plus p plus a.
00:21:08.760 - 00:21:51.500, Speaker B: That gives you two improvements. The first of all, you can compute. I'm assuming here that people basically know how to do elliptic curve addition on short fast trust curves. Is that a reasonable assumption? I can go through it if people are not familiar with that. Okay, I'm going to assume that people are familiar with it. Basically.
00:21:51.650 - 00:21:52.508, Speaker D: I'm good.
00:21:52.674 - 00:23:28.510, Speaker B: Yeah, you compute the chord between the two points and then it's obvious from that. So you can save needing the chord between these points p and a because you can compute it from, sorry, between the points a plus p and a because you can compute that from the chords that you had for a plus p. So that saves you one multiplication. And then here you can save another multiplication because instead of doing a conditional here, you just add plus p or minus p. And you can compute this plus r minus p in one constraint because you only have to negate the conditionally negate the y coordinate. And then, so it turns out that you can adjust the scalar so that adding plus or minus p, you can use that to construct an algorithm that works like the one ad. So you end up with six constraints per bit here.
00:23:28.510 - 00:24:04.550, Speaker B: So the conditional negation costs one, this costs three, and then two for this addition because you didn't need to compute the lambda separately. Okay, so we can do even better than that. You can do it in 3.5 modifications per bit instead of six. Hang on, my phone is beefing at me. I just need to track that. There's nothing important.
00:24:04.550 - 00:24:47.300, Speaker B: No, nothing important. So what we're going to do is process two bits at a time, and processing two bits is going to take seven multiplications. So we end up with an average of 3.5. So the idea is to use these endomorphisms on the curve. So this phi function is the endomorphism. And this is easy to compute because turns out you just need to multiply the x coordinate by a constant. So it's very easy.
00:24:47.300 - 00:26:12.288, Speaker B: So you can compute one of these four points dependent on P in just two constraints. So all you do is take two random bits and use those to decide which of these four points you're going to add. So that's what we're doing here when computing Si and then use the same trick as in the previous slide to add one of these four points. You end up multiplying by a scalar that is dependent in some way on all of your random bits. So it turns out that we don't need to pick the scalar that we're going to multiply by directly from the output of the hash. We can just pick the random bits and then multiply by a scalar that's dependent on those random bits. And as long as that doesn't lose any entropy, then we're fine.
00:26:12.288 - 00:26:23.810, Speaker B: So we need 128 bits of entropy in the scalar that we're multiplying by. So.
00:26:25.540 - 00:26:47.290, Speaker C: You'Re looking, I mean, in a Pederston hash context, I can understand that you just want to not lose entropy. But like in a snark verification mean, aren't you given like a scalar a and you need to multiply exactly by a?
00:26:48.800 - 00:27:24.390, Speaker B: It turns out no, if you look at the proof in the appendix of the paper, nothing actually depends on choosing a specific scalar. So it doesn't need to be full length, it just needs to be sufficiently unpredictable. So that's why we only need 128 bits of entropy. And that gives you roughly 128 bit security level.
00:27:30.380 - 00:27:46.672, Speaker C: I'm a little hasty, I guess I'mixing things up. But aren't the scalars, when you have the commitment scheme, aren't they like powers of some x, some random x or something?
00:27:46.726 - 00:27:49.890, Speaker B: Or maybe I'm just, are you talking about the scalars and the.
00:27:54.740 - 00:28:21.020, Speaker C: Yeah, I guess I'm talking about. So I'm assuming, right, this is for like for doing halo. So the main thing you need to do is like this scalar multiplication for the, to prove that the opening of the polynomial is. So I thought there like the scalar is not completely to your choice.
00:28:22.960 - 00:29:29.012, Speaker B: So you're picking these random bits according to Fiat Shamir. It's just a way of picking fiat shamir. Challenges. It turns out that we need the challenges to be squares. But that's fine, because what we can do is compute the scalar that we multiplied by this by in this algorithm and check that it's a square. And if it isn't, then we just rewind the randomness and try the proof again. Okay, so what do we need in order for this to be correct? We need to prove that the mapping from the random bits to the effective scalar doesn't lose any entropy.
00:29:29.012 - 00:30:40.610, Speaker B: So an easy way to prove that is to show that it's one to one, and then it loses no entropy at all. So the first part of we split that into two parts. We show that the map from the random bits to these a and b values is one to one. And then the map from that to the effective scalar is one to one. So for the first part of that, it's proven in the paper. But it turns out that you can simplify the problem by just looking at the first element of these sequences that vary. So what you're to prove that this is injective, you split your random bits into the ones that are affecting a and the ones that are affecting b.
00:30:40.610 - 00:31:34.290, Speaker B: And then, so it's, the function is injective when it gives different outputs for different inputs. So consider the first element of CD that differs, and then you can show that these sums differ modulo four. And using that argument, you can show that it's sufficient to consider these two cases. So k equals one when the sequences are of length one. So you only have two bits, and k equals two. So when you only have four bits. Dara, go on.
00:31:34.740 - 00:31:38.710, Speaker D: Sorry, I'm just getting a little lost here. What is the problem that you're guarding against?
00:31:39.160 - 00:31:59.240, Speaker B: The problem is that you could have collisions in this function. So if you had collisions, then you would have less entropy in the scalar than in the original input bits. So you'd have to increase the number of bits in order to keep it equally secure.
00:31:59.840 - 00:32:03.516, Speaker D: Oh, this is for fiat shamir. In order for fiat shamir to be exactly.
00:32:03.618 - 00:32:03.980, Speaker B: Okay.
00:32:04.050 - 00:32:05.230, Speaker D: Okay, thank you.
00:32:05.680 - 00:33:32.760, Speaker B: Okay, thanks for that question. Okay, so basically this function from the two cases that we need to track are here. And basically these lattices, if you remember in the first part, we said that the generalized scalars that we're multiplying by can be considered as elements of a latice. So this direction, say, from the origin along these red dots, corresponds to applying the endomorphism, and then this from the origin to the right corresponds to just ordinary multiplication. So the fact that all of these colored dots are distinct is just saying that this property here holds. Um, and then, so for the second part where we're going from this pair of values into the effective scalar, we. So we use this argument.
00:33:32.760 - 00:34:29.550, Speaker B: We know that each of a and b is in this fixed range. And so let's just consider this zeta a term. What we're trying to do is proving that the minimum distance between any two values of any two distinct values of zeta A. For a in this range, the minimum distance is greater than another copy of the range. So that means that when you add zeta a and b, the range for b will fit between the gaps and the possible values of z to a. Does that make sense?
00:34:39.690 - 00:34:46.710, Speaker C: Wait, so you're saying a, for it to be injective, a is in this range?
00:34:47.230 - 00:34:54.310, Speaker B: Yeah, little a is in this range. Little b is also in this range.
00:34:54.470 - 00:34:55.770, Speaker C: Oh, same range.
00:34:56.670 - 00:35:59.960, Speaker B: So if the minimum distance between any two distinct values is e to a is greater than this value, then it must be injective, because we can. Basically, what we'll have is say that this is one possible value of zeta A. Then we'll just have a. So let me explain this diagram. This red line here is zero, and then we have positive values to the right and negative values mod q to the left. So our q is something like two to the 250 something. So if I show this video.
00:35:59.960 - 00:36:52.420, Speaker B: Basically what it's doing is plotting all the possible values of zeta A, because it turns out that the next value of the minimum distance will always come near the origin. So if we just show the. So these green lines here are the search space for the next minimum distance. So we're zooming in on the next element of the search space. You can see that these values here are zooming in.
00:36:54.390 - 00:36:58.738, Speaker C: Sorry, the minimum distance between.
00:36:58.824 - 00:37:04.230, Speaker B: The minimum distance between two values of zeta A for a in this range.
00:37:07.450 - 00:37:14.460, Speaker C: Okay, and you're saying if those are larger than, like.
00:37:18.750 - 00:37:24.000, Speaker B: If it's larger than another copy of the range, then we're fine, because it must be one to one.
00:37:25.890 - 00:37:28.400, Speaker C: Well, maybe twice the range. No.
00:37:31.170 - 00:37:54.200, Speaker B: Just one copy of the range. So it turns out that we computed this and it turned out to be two to the 185 or so, just less than 202 to the 185. So this is obviously much greater than two to the 65 plus two to the 64. So we're fine.
00:37:58.010 - 00:38:24.480, Speaker C: I'm just trying to do the proof in my head. So, assume for contradiction that there's a collision. So zeta a plus B equals Z. A prime plus B prime, or some ab, a prime, b prime. And then you move everything to one side, or you move the zeta, say, to one side. So you get zeta a minus a prime equals B prime minus b.
00:38:27.330 - 00:38:28.080, Speaker B: Wait.
00:38:33.010 - 00:38:56.140, Speaker C: It seems like, what would you need? So you want that. It can't be that zeta, like a minus a prime equals B minus B prime. So what you have on the right hand side is not just an element of the set, it's like a difference. So the difference can be maybe twice as large as.
00:38:57.710 - 00:38:58.874, Speaker B: No, because maybe.
00:38:58.912 - 00:39:00.138, Speaker C: Let me just ask you, why is.
00:39:00.144 - 00:40:00.560, Speaker B: It this is a single sided range? Say, for example, that our two values of zeta a were this one and this one, the two red lines here. And then the possible values of zeta a plus B, then are. It's a band which starts at each of these red lines and stretches to the right by two to the 65 plus two to the 64. So provided that this distance is greater than that, then the bands won't coincide. Ah.
00:40:03.170 - 00:40:08.994, Speaker C: So sorry. Wait again. Sorry. Could, could you repeat that? I'm not sure I got it.
00:40:09.112 - 00:40:18.340, Speaker B: Okay, so imagine that each of these lines we draw.
00:40:22.440 - 00:40:26.388, Speaker C: Each of these lines is like an element zeta a. Yeah.
00:40:26.474 - 00:40:46.270, Speaker B: Each of these lines is a zeta a. Okay. And then we can add to that any value b in this range. Right? So, provided the distance between the lines is greater than the size of the range, then it's one to one.
00:40:48.400 - 00:41:14.100, Speaker C: No, I think that shows that zeta a plus B can't equal zeta a prime. But it doesn't show. I think zeta a plus B can't equal zeta a prime plus B prime. I may be getting this wrong, but I think maybe you need to, needs to be larger than twice the range.
00:41:16.120 - 00:41:28.600, Speaker D: Maybe another way to say it is that you put a little ball around each of these lines of radius two to the 65, plus two to the 64, and those balls don't intersect. So any b, you add to any line.
00:41:28.750 - 00:41:54.000, Speaker C: I'm saying this line, the distance needs to be twice because, right, you've got your zeta a and zeta a prime. Yeah. You want to see that? They don't touch. But the thing is, the zeta a can have a b. That helps him. But also the Zeta a prime can maybe have a b prime that helps him go in the other direction.
00:41:56.660 - 00:42:04.000, Speaker B: No, because this range here is from zero. This is not a symmetric range. It's.
00:42:04.420 - 00:42:06.148, Speaker C: Okay. All right.
00:42:06.234 - 00:42:06.772, Speaker B: Sorry.
00:42:06.906 - 00:42:08.470, Speaker C: Yeah, I think that's why.
00:42:09.720 - 00:42:12.230, Speaker B: If it was a symmetric range, then you'll be. Right.
00:42:12.600 - 00:42:24.410, Speaker C: I guess the question. Okay, and I guess you're saying also there's no wraparound, right, because the zeta is about two to the 128. Right. So I think also there's no wraparound ever in this.
00:42:26.560 - 00:42:35.116, Speaker B: That's what we computed here. We computed the minimum distance, taking into account wraparound. No, I think.
00:42:35.138 - 00:42:41.056, Speaker C: Right, because the zeta is like about, well, I guess it depends on the curve. But I think.
00:42:41.078 - 00:42:41.216, Speaker B: Right.
00:42:41.238 - 00:42:44.880, Speaker C: The zeta is about two to the 128, at least in the.
00:42:44.950 - 00:43:14.600, Speaker B: No, that's what makes this problem difficult. So you can construct curves in which Zeta is around 228, but the tweedle curves done that because I didn't know how to at the time. So yeah, Zeta will be two to the, it'll be greater than two to the 250. So that's what makes this problem complicated.
00:43:17.100 - 00:43:19.208, Speaker C: Okay, you're saying the tweedle curves of.
00:43:19.214 - 00:43:24.380, Speaker B: Zeta is almost, if zeta was less than two to the 128, then it would be trivial.
00:43:26.240 - 00:43:34.736, Speaker C: So maybe it's better to work with Zeta because. Right, you have Zeta and Zeta squared. No, I mean, like if so, we don't actually.
00:43:34.918 - 00:43:54.952, Speaker B: So we don't use the endomorphism squared. We just use it once. Because if you look at the algorithm here, we're only applying Zeta to the base point. We're not applying it to the accumulator at any point. Oh yeah.
00:43:55.006 - 00:43:57.304, Speaker C: I'm saying I think not to use both of them.
00:43:57.342 - 00:43:57.930, Speaker B: But.
00:43:59.660 - 00:44:10.108, Speaker C: Could you maybe use zeta squared instead of. Suppose zeta squared happens to be nicer in the sense that it's closer to two, to the 128.
00:44:10.274 - 00:44:10.892, Speaker B: Oh yeah.
00:44:10.946 - 00:44:12.350, Speaker C: Yes, you can only.
00:44:15.360 - 00:45:05.752, Speaker B: Yeah, you absolutely can. And in fact, remember when we were constructing last week, the inverted lollipops? It turns out that when you're constructing those, you can arrange for the zeta values to be half length. So if you wanted to do it that way, you could. That's not the way we did it in the paper. There's also potentially the option to use six values here instead of four. So that would be minus p p minus five p five p minus phi squared. Of p phi squared.
00:45:05.752 - 00:45:58.400, Speaker B: Yeah. So yes, that could work, but it makes this computation of the minimum distance even more complicated. So I didn't do that. And it only actually saves you a fraction of a multiplication per bit. So I'm not sure that it's really worth the complexity. Okay, so that's the scalar multiplication optimization. There's actually another one that we didn't use in the prototype implementation and didn't include in the paper, which is that when you're doing a multiscaler modification, you can share the doublings.
00:45:58.400 - 00:47:20.250, Speaker B: So what you would do in that case is you can get it down to 2.5 multiplications per bit. So for this you have to assume that you don't hit any exceptional cases in the addition. But what you would do is add together all of the base points that you want to add at each step. So suppose that you have base points p and q. You would calculate one of these four points for p, and then another one of the four points for q, you would add those together, and then you would do that for whatever the width of your multiscaler multiplication is. And then only at the end you would do this, you would do the doubling effectively, and I think that costs 2.5
00:47:20.250 - 00:48:33.680, Speaker B: multiplications per bit because it's effectively five multiplications to add each of these modified base points. So you need one for the conditional negation, one for the conditional endomorphism, and then three to add the point. So that's five per base. But then you have to divide it by two because you're processing two bits at the same time. So it comes out to 2.5 for a large number of bases. But you end up having to use the street log relation assumption to say that you never end up adding two points and getting the zero point or adding two points with the same x coordinate.
00:48:33.680 - 00:48:44.230, Speaker B: Okay, so let's go on. First of all, are there any questions about the scalar multiplication part?
00:48:49.640 - 00:48:57.720, Speaker C: Sorry to repeat myself a little, but just to understand so this is scalar multiplication only in the fiat shamir context.
00:48:59.260 - 00:49:28.992, Speaker B: Yeah. You can't really use this for general. Well, you can use some of the optimizations that I described for general multiplication, but this particular one you can only use if you're multiplying by a random scalar. Right. This one here, that gives you six months per scalabit. You can use that for any scalar. Right.
00:49:29.046 - 00:49:34.660, Speaker C: But the endomorphism, yeah, it makes, it's only fiat chamir.
00:49:35.240 - 00:50:16.080, Speaker B: Yeah. So if you use the half length, let's see, you might be able to do something more efficient if you're relying on the half length theta. I haven't really thought about that very much. Okay. And then let's go on to Fiat Shemi. So you have this protocol between, this interactive protocol between approver and a verifier. They both know the instance.
00:50:16.080 - 00:51:25.800, Speaker B: So verifier sends a random challenge, c, one approver sends part of the proof, verifier sends another challenge, and so on. And then at the end, the verifier can decide whether the proof is valid or not. So we want to make this into a non interactive protocol. There's a perfectly standard transformation here, but in this case, what we're going to do is use a duplex sponge for the hash function. So that's a particular hash construction that depends on a permutation. So we choose a permutation that is efficient on our field, and we split the state of the hash function into two parts. So the idea is that this part on the right here will be hidden from an attacker.
00:51:25.800 - 00:53:01.140, Speaker B: So if this is one field element, field element is going to be about 256 bits, at least. So we end up with 128 bit security because there are square root attacks on this. So notice here that we will only need to do one to compute one permutation per challenge. So if we did this differently, if we explicitly hashed all of the previous transcripts, so the things that have been sent in the interactive protocol are called the transcript. So we could hash all of the previous transcript and then use another iteration to get the challenge. But in a duplex sponge, we combine this into one permutation. Is that clear? So this construction is ideal for shamay because it's almost optimal, because you couldn't get away with doing less than one cryptographic permutation per challenge.
00:53:01.140 - 00:53:24.540, Speaker B: And the idea here is that the prover can't cheat by modifying what it outputs, because that will unpredictably modify the verifier's next challenge.
00:53:28.080 - 00:53:37.650, Speaker C: So I feel I'm missing something here. What is the more sort of straightforward thing that this.
00:53:38.980 - 00:54:27.720, Speaker B: The most straightforward thing would be to? You have some general hash function, call it h, and you have a state. So you hash the state to get the next state, but then you have to produce an output from the challenge. So you'd have hash of state, comma, input. Um, and then that. I guess if you did that, then it would be similar to the sponge.
00:54:30.460 - 00:54:39.930, Speaker C: So here, instead of hashing the challenge and I them.
00:54:41.260 - 00:55:02.900, Speaker B: Yeah, this hit hidden part of the state here. The attacker never gets access to that. Sorry. I mean, the attacker never gets to influence that directly.
00:55:06.120 - 00:55:17.610, Speaker C: Yeah, I'm trying to understand more. The left part. So there is a cso PI, and then you feed that into.
00:55:20.540 - 00:55:21.640, Speaker B: The permutation.
00:55:24.720 - 00:55:32.460, Speaker C: To a permutation. Sorry, you mean the sore is like a permutation.
00:55:33.760 - 00:56:00.580, Speaker B: So this is depending on the fact that the sore is. It's not actually a sore, it's addition. But this just needs to be, to be one to one in the sense that any possible PI one gives you a different input to the permutation.
00:56:04.550 - 00:56:26.550, Speaker C: This f is not a compressing hash. It's. Oh, I see. But it's okay that different pairs like c, PI will. I'm just thinking of this plus as a bitwise sore.
00:56:30.250 - 00:56:38.300, Speaker B: So for duplex sponges, it was originally defined as a sore in the paper, but it just needs to be group operation, basically.
00:56:41.970 - 00:56:52.514, Speaker C: So different pairs, c one, PI one, say, will give the same input into f. That's right.
00:56:52.552 - 00:57:10.600, Speaker B: But at the point where c one has already been chosen. So if c one is considered to be fixed, then each different PI one will produce a different input to f, and therefore a different c two.
00:57:14.970 - 00:57:15.720, Speaker C: Right.
00:57:16.670 - 00:57:38.474, Speaker B: An attacker can't manipulate PI one and keep. So what you're trying to protect against is that the attacker knows c two and is able to choose c one to make it work, to make the proof work. Well, the attacker.
00:57:38.522 - 00:57:52.820, Speaker C: Right, it seems like they can say there's a c two they like, so they can for any c one.
00:57:53.270 - 00:57:57.080, Speaker B: Oh, okay.
00:57:59.370 - 00:58:05.320, Speaker C: I was going to say for any c one, they can choose a PI one that will give c two.
00:58:09.930 - 00:58:46.600, Speaker B: So the difficulties they have there. So they can't influence this. So this behaves basically like a random permutation. And therefore, given c one. Yes. They can pick a c two and then run the permutation backwards. They can't actually, because they don't know what this would be.
00:58:47.530 - 00:59:12.190, Speaker C: Sorry, I think I'm missing part of the context because I'm thinking of approver just making a non interactive proof so the prover puts everything into the hash. But you're saying somehow this is a setting where the prover doesn't have control about on this right part. Like he doesn't know the right part of.
00:59:12.260 - 01:00:09.230, Speaker B: Well, he does know it, but it's a difficult problem. If you're only given c two and this value here, then you can't from that compute what PI one would be. You can if you know the output of this. So this arrow here, then you could run f backwards and compute PI one. If you're only given this input and this, then you can't do that anyway. So there's a proof in the duplex sponge paper that basically says that this is a secure hash.
01:00:11.090 - 01:00:15.518, Speaker C: Oh, I see. I was confused if this is a new thing or.
01:00:15.604 - 01:01:00.080, Speaker B: No, it's not a new thing. It's just we're observing that it is exactly the abstraction that you need for fiat chamber. Okay, sorry if that was confusing. So we can apply some optimizations here. So if we were actually using XOR for this operation, then we would have to decompose the inputs into bits. And that's expensive in a circuit, but we actually only need it to be a group operation. So we use addition modulo the field size, and that works fine.
01:01:00.080 - 01:02:11.574, Speaker B: And all of the proofs for the duplex sponge still go through. The other thing we can do is we can compress the inputs, so it's sufficient to have a compression function that works in almost all cases. So let's say that we're trying to input two curve points, which in fact is exactly the case that we need in halo. So the obvious way to do it is using uncompressed points. So that would be four field elements, or we could use compressed points, but then the compression requires decomposing our coordinates into bits. So that's expensive, but we can compress two curve points into three field elements. Probistically, it's just like this.
01:02:11.574 - 01:03:41.300, Speaker B: So we take the x coordinates and then we add the y coordinates. It turns out that that's sufficient to determine the input, and therefore it's a secure compression in this context. So we do have to check that these inequalities hold, but we can check these very efficiently in the circuit. And then if they don't hold, then we could try again with different randomness, but the probability is that it's negligible. Anyway, we don't actually use this in the prototype implementation, but it's described in the accompanying notes, which I posted to the telegram channel. Okay, so the other thing that we can do is the size of this element of the sponge state is called the rate. So that's the size of input that we can take in for one computation of f, and it's also the size of output that we can produce for one computation of f.
01:03:41.300 - 01:05:20.980, Speaker B: So we just choose the rate to be as small as we can so that we have one f evaluation per round of the interactive protocol, as shown in the diagram. Okay. And in practice, that will be log n rounds four, where n is the size of our circuit, basically. And then, so, to do this efficiently, we want to choose an f that we can compute in the circuit, and we use rescue, which is one of the recently developed circuit efficient hashes. So we also considered Poseidon, but there's a paper that it doesn't break Poseidon, but it suggests that its security margin is a little bit less than the designers hoped for. So I think if you adjust the number of rounds for Poseidon so that it looks equally secure to rescue, it turns out that rescue is a little bit more efficient. And it's also more efficient to compute rescue outside a circuit, because it is the true.
01:05:20.980 - 01:06:15.160, Speaker B: No, it's poseidon that's more efficient to compute outside the circuit. Never mind. But you're mainly interested in the in circuit cost rather than the out of circuit cost. So rescue has a parameter, which is this s box function. So we need to choose that. We choose the curves so that that will be a permutation. We can't choose the minimum value of this exponent, which would be three, because that conflicts with using the endomorphism, and it gives us more advantage to use the endomorphism than to choose exponent three here.
01:06:15.160 - 01:06:38.490, Speaker B: Okay, so that's basically it. Any questions about that part of it? The algebraic hashes. Cool. So that was the end of the talk.
01:06:42.380 - 01:06:43.370, Speaker C: Thank you.
01:06:45.980 - 01:06:47.240, Speaker D: Thanks, Dara.
01:06:47.600 - 01:06:48.590, Speaker B: You're welcome.
01:06:51.840 - 01:06:53.790, Speaker A: Ariel. Were you about to ask something?
01:07:00.560 - 01:07:13.510, Speaker B: The questions don't have to be restricted to the second half of the talk. If you want to ask any questions about the first half that you didn't think of last time, then that's fine as well.
01:07:23.660 - 01:07:25.000, Speaker D: Go ahead, Ariel.
01:07:25.500 - 01:07:27.784, Speaker C: No, go ahead.
01:07:27.982 - 01:07:28.730, Speaker B: Okay.
01:07:29.180 - 01:07:45.810, Speaker D: At the risk of belaboring this fiat shamir question, I think you mentioned that you save, like, roughly a factor of two on the hashes there. But I'm a little confused by that, because in just plain vanilla fiat shamir, you're only doing one hash per approver message. So where does the savings come from?
01:07:48.180 - 01:08:32.130, Speaker B: So it's the fact that you would have to. So if you. That the inefficiency comes from the hash mode. So typically, to produce a collision resistant hash, or actually a hash that can be used as a random oracle, you would need to compute the permutation twice or your compression function twice for each instance of the hash. So that's where the factor of two comes from.
01:08:34.180 - 01:08:44.080, Speaker D: Okay, thanks. Maybe yet another follow up question. Why is it important that f be a permutation?
01:08:46.580 - 01:08:49.792, Speaker B: For that, you will need to look at the duplex sponge paper.
01:08:49.926 - 01:08:54.068, Speaker D: Okay, yeah, maybe we should plop that into the show notes or something.
01:08:54.154 - 01:09:04.650, Speaker B: Yeah, I'm sure someone that can look it up and just put it in the chat now. Okay, I'm done. Thanks. Okay, cool.
01:09:07.760 - 01:09:33.910, Speaker C: So I had to miss the end of last time. So right now the thing you're constructing is you have some curve and two curves below it or something, or what is the current instantiation you're focusing on? Is it half pairing? Is it like what.
01:09:50.440 - 01:11:29.992, Speaker B: Specific slide that I'm trying to get back to? Okay, this is it. Halo can just use a very simple cycle of non pairing family curves, a two cycle. So this first part of the talk about half of it was about how to construct two cycles in general. And then I went on to how to construct graphs that include pairing friendly curves. The latter isn't strictly needed for halo, but suppose that you wanted to have a proof system with shorter proofs, or to make the final proof easier to verify. In that case, if you're prepared to make the trusted setup trade off, then you could use, say, half pairing cycle. And what that allows you to do is you take the halo proof and then you verify it in, say, grot 16 circuit or a planck circuit, and then that ends up with a shark.
01:11:29.992 - 01:12:25.860, Speaker B: So you can still, if you want, do the conservative verification of the original halo proof, but you also have the option of verifying the grot 16 or plunk proof. But to make that work efficiently, you need the. So there are two ways of doing it. You can use the half pairing cycle. So for the main recursion, you just wouldn't use the fact that this is a pairing friendly curve. You would only use it at the end to get the shark. Or you could use this inverted lollipop thing and then use this pairing friendly curve to get the final proof.
01:12:25.860 - 01:13:10.050, Speaker B: But if you want a shark, then you can't use the pairing dependent proof system in the recursion itself because that depends on a trusted setup, which is what you want to avoid for a shark. You want to be able to take proofs. Say that the pairing friendly proof system was broken. You want to be able to take your proofs and then regenerate the pairing friendly part of it to accelerate the verification again.
01:13:12.840 - 01:13:24.520, Speaker C: And do you have a favorite or one that you're focusing on in terms of the shark versus the inverted?
01:13:26.780 - 01:13:53.570, Speaker B: For most of what we're doing, it doesn't matter. So most of the research we're doing is how to optimize the verification circuit and how to optimize the discrete log proof system. And that doesn't depend on which of these three cycle constructions you use.
01:13:57.460 - 01:13:58.210, Speaker C: Great.
01:14:00.120 - 01:14:35.550, Speaker B: So the trade off basically is in curve size. So if you use this non pairing friendly cycle, then you can use 256 bit curves for each of P and Q. If you use the half pairing cycle, then they have to be around 384 bits or 400 bits depending on who you ask. Some people say 460 bits, obviously.
01:14:38.020 - 01:14:45.650, Speaker C: I think the number field, Steve, maybe some people still say 256.
01:14:48.180 - 01:16:09.310, Speaker B: I think the community has settled on the idea that 206 bits with embedding degree twelve probably isn't conservative enough for the long term. But yeah, it's kind of a matter of opinion. And then, so for this inverted lollipop, this R and Q can be 256 bits again and then p, depending on what curve family this pairing friendly curve is in. P can be say 384 bits if you use BLS twelve, or it can be 300 and 2340 bits if you use KSS 18. Whereas for this, where both kers in the cycle are pairing friendly, you would need about 768 bits. That was the original MNT four MNT six cycle. Yeah.
01:16:10.800 - 01:16:18.590, Speaker C: The advantage of the inverted lollipop is the disadvantage is the trusted setup. Well, it may be the size.
01:16:21.220 - 01:18:11.000, Speaker B: So if you're using it in a shark, then maybe you don't mind as much about the trusted setup because you can easily prove, say you have an invalid proof for the poring friendly system, you can prove that it's invalid. Um, so let's say you were using this in a blockchain application. So on the chain you would have the discrete log base proof and also a shortcut to verify the pairing base proof. So if you ever get a case where the pairing base proof validates and the discrete log proof doesn't, then you know that the pairing based proof system is broken. So at that point you can either stop the chain if you have enough control over the blockchain to do that and restart it with a fixed proof system, or you can fall back to just verifying the discrete log base proofs and you can potentially do the latter automatically. Although there might be denial of service issues with doing that. If you are relying on the fact that the pairing based proofs are simpler, less expensive to verify.
01:18:13.260 - 01:18:27.980, Speaker C: But how many people, when we've already switched to these universal updatable setups, how many people are really disturbed anymore by the setup?
01:18:31.860 - 01:19:22.560, Speaker B: And I think maybe more than you think, because we don't just have to convince our own community of cryptographers, we have to convince the general public. And there's always going to be some degree of doubt in proof systems that rely on trusted setup. I think. I mean, it depends on your philosophy and your politics, but I know that at least some of the louder critics of zcash make a big deal about the fact that it uses a trusted setup.
01:19:25.700 - 01:19:42.336, Speaker C: Yeah, but that was before, like, if you would describe to them the process of the universal updatable setup, how many of them would give the same criticism? It's not some one time thing between a small group.
01:19:42.378 - 01:21:05.730, Speaker B: Well, I mean, it's kind of moot. If you can make the discrete log set up, if you can make the discrete log proof system efficient enough, then you don't need the setup. I'm thinking it's not just the setup itself that can potentially be compromised, it's the fact that you're using more complex cryptography, because you not only have to be to trust the experts, you have to convince the general public who's using your protocol that this complex moon mass is something that they should be relying on. But that's going to be challenging. Pairing or not pairing. I mean, that's if it is Pedersen, or if it is so if you're trying to convince, say, bitcoin people, they seem to be perfectly okay with bulletproofs, or Monero people are perfectly okay with bulletproofs. To me, performance is going to win.
01:21:05.730 - 01:21:13.780, Speaker B: We're looking at the half pairing, because at least one half we can do gross system.
01:21:13.850 - 01:21:15.012, Speaker C: So that's the idea.
01:21:15.066 - 01:22:11.350, Speaker B: And we think that the gaining performance will be if you can make the verification circuit cheap enough, then the fact that you have a linear time decider is not really that much of a big deal. If you can make it less than a second say, right, especially if you have a protocol that relies on recursive proofs, so that you're actually having to verify fewer proofs. You don't have to verify a proof per spend or per output. You might only have to verify one proof for the whole blockchain or for one block, because obviously you have to redo that verification for each new block that comes in.
01:22:12.600 - 01:22:17.236, Speaker C: Yeah, that's the challenge coda is trying.
01:22:17.258 - 01:22:22.280, Speaker B: To solve also to reduce that. But catching up only takes one verification.
01:22:26.800 - 01:22:39.410, Speaker C: My perspective on it is eventually the person whom you need to convince is the engineer, non crypto expert ten years from now, who needs to implement the system.
01:22:42.900 - 01:22:48.624, Speaker B: They would prefer not to have to implement pairings, because not implementing something is.
01:22:48.662 - 01:22:55.060, Speaker C: Always, they're going to have, like, they have a library of hashes, they're going to have a library of these systems.
01:22:58.520 - 01:22:59.876, Speaker B: Sorry, carry on.
01:23:00.058 - 01:23:38.320, Speaker C: And they're going to be like us, they're going to be optimization and fanatics. So even if you get the DL systems to a forex factor from the pairing system, then they're going to be, from their perspective, they're not going to think trusted, set up philosophical issues. They're saying, okay, this, if you just download this file, which is the universal setup, practically speaking. It'll just be like, if you just download this file, you get this forex speed up. That's what's going to be in their head, probably much more than forex. It's forex in a very optimistic.
01:23:38.980 - 01:24:24.770, Speaker B: I don't agree, because you have smaller curves with the DL only system. If you were right about, and you could convince the rest of the cryptographic community that it's okay to use 256 bit pairing friendly curves, then, yeah, those would definitely be more efficient. But as it is, say, use 384 bit pairing friendly curves, then that's going to cost at least double on your curve arithmetic, maybe three times more.
01:24:27.380 - 01:24:43.300, Speaker C: You're saying that when using 381 bit for the pairing, yeah. All of your arithmetic becomes systems will be more efficient than the pairing systems.
01:24:45.980 - 01:24:48.600, Speaker B: Not for verification, but for proving.
01:24:55.580 - 01:24:56.440, Speaker C: Mmhmm.
01:24:58.320 - 01:25:04.540, Speaker B: Because all of the field arithmetic will be between two and three times more expensive.
01:25:06.480 - 01:25:13.730, Speaker C: But you have this chain of login hashes, right? For example, things like that.
01:25:16.100 - 01:25:28.070, Speaker A: Sorry, this almost sounds like the beginning of a debate type study club. We might want to invite some other voices to this.
01:25:29.880 - 01:25:31.888, Speaker B: Discrete live versus pairings.
01:25:31.984 - 01:25:44.890, Speaker A: Yeah, I mean, do you want to have. Because I feel like this conversation, there's like large camps that actually represent those two sides. So I wonder if we should have everybody join us. Would it be fun? Could we do it?
01:25:46.060 - 01:25:49.484, Speaker B: Do we have people who have strong opinions on this?
01:25:49.602 - 01:25:51.288, Speaker A: I don't know if in this, apart.
01:25:51.304 - 01:25:57.036, Speaker B: From me and Ariel, I think in.
01:25:57.058 - 01:25:59.580, Speaker A: The larger study club there definitely are.
01:25:59.730 - 01:26:28.520, Speaker B: But I don't know in this group, to be fair. I do see Ariel's argument here. Yeah, you can certainly make the argument that universal setup is fine. I think it will be difficult to convince the general public that you're not relying on moon mass.
01:26:28.590 - 01:26:36.892, Speaker A: If you do that, I think eventually the general public won't. If it's almost trusted enough, people won't care.
01:26:36.946 - 01:27:02.820, Speaker B: They won't pay. Well, I mean, objectively, you are relying on moon math, right? It's a tiny proportion of people that can actually understand these things. Understanding the pairing based systems is maybe a bit more complicated than understanding the discrete log systems. I don't know, maybe there's not that much difference.
01:27:02.970 - 01:27:11.380, Speaker C: But I mean, how much do we understand about. Right. We use things without understanding them. Like in society.
01:27:14.220 - 01:28:00.310, Speaker B: We hope that there are experts in those things that understand them. Actually, I think you're not quite right there, because it take a complicated thing like a car. Maybe this is not quite so true now, in the days of electronic engine management systems, but at least 20 years ago, it was perfectly possible to learn car maintenance. And many people did and were able to maintain their cars and understand in some depth how they worked.
01:28:00.920 - 01:28:02.260, Speaker A: But now they don't.
01:28:03.160 - 01:28:06.928, Speaker B: Well, now they don't. But I don't think that anyone actually thinks that's a good thing.
01:28:07.114 - 01:28:11.240, Speaker A: Well, I don't know. A lot of people, I don't think they just sort of don't care.
01:28:11.310 - 01:28:11.592, Speaker B: Right.
01:28:11.646 - 01:28:19.470, Speaker A: Like the cars, in a way, are better. I know that there's car lovers out there who would disagree here, but.
01:28:21.920 - 01:28:22.236, Speaker B: You.
01:28:22.258 - 01:28:51.332, Speaker A: Need more expertise to actually fix them. You need different kinds of tooling. You need all these things, but at the same time, it is more of a black box. But people still consume them and like them and they last long. I don't know. I think for this study club we should probably wrap up, but I think we should continue this conversation in other forms and other places because I would be curious to hear.
01:28:51.466 - 01:29:18.556, Speaker B: I think it's a fascinating philosophical argument about to what extent we should be relying on complexity in the things that we use. And my position is that we're probably using things that are too fragile. I mean, not just in cryptography, but in general, but especially in cryptography, actually.
01:29:18.738 - 01:29:38.530, Speaker A: Well, from Nuttycom here in the chat, some people about the cars, some people don't care. Others now tune their cars via software. Good conclusion there, Dara. Thank you. Can I say a big thank you for doing the bonus. Thanks for coming back.
01:29:39.140 - 01:29:39.664, Speaker B: You're welcome.
01:29:39.702 - 01:29:45.752, Speaker A: This is going to be an amazing three parter, actually, in the end, like three part resource that I hope is helpful to people.
01:29:45.886 - 01:29:47.210, Speaker B: I hope so, too.
01:29:47.980 - 01:29:52.808, Speaker A: So I guess we'll speak to you all in the study club. And till next time.
01:29:52.974 - 01:29:53.944, Speaker B: Okay, see you.
01:29:53.982 - 01:29:54.616, Speaker A: Cool.
01:29:54.798 - 01:29:56.580, Speaker C: Thank you. Bye.
