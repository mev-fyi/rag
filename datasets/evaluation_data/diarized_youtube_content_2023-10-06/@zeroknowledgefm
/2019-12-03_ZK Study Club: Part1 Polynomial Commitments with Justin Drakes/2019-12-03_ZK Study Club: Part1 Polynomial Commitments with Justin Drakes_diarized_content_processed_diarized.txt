00:00:00.650 - 00:00:52.166, Speaker A: I think we can now kick off. There might be a few more people joining as we go, but this is the study club, the ZK study club. This is sort of a thing we've been trying to do every month where we get together and talk about a topic or a paper. And for this session, we have Justin Drake, who's actually going to talk about polynomial commitments. And I think this is such a great foundational topic for us to actually cover in this format. Just so for anyone who's sort of joining for the first time, the way that this is made and the sort of idea is that this is supposed to be a bit more of an intimate presentation, not something like, we're just watching Justin speak. So if we have questions or if there's anything that we want to say, I think maybe, Justin, you give some moments for people to ask, or you can also put, like, in the chat.
00:00:52.166 - 00:01:09.620, Speaker A: Sometimes people will put questions in there if they don't want to interrupt right at that moment. But this should be pretty interactive. So, yeah, I might have to turn off my camera, because my connection is not so good, but I'm here, and I'll jump in every once in a while.
00:01:09.990 - 00:01:10.834, Speaker B: Cool. All right.
00:01:10.872 - 00:01:12.066, Speaker A: Justin, you want to take it away?
00:01:12.168 - 00:02:08.226, Speaker B: Sure. Thank you, Anna. Cool. So, yeah, I'm going to talk about polynomial commitments, and it's kind of a really exciting kind of topic in the context of universal snot, we've had all these recent universal snot constructions, and one of the unifying concepts is polynomial commitment. So, just to give a little bit of historical perspective here. So we've had the initial paper, the cate annull, from 2010, that defined polynomial commitments and gave an initial construction based on pairings. And then kind of in more recent years, we've had kind of other constructions that kind of gave different constructions for polynomial commitments.
00:02:08.226 - 00:02:55.166, Speaker B: So we have the green one here, bulletproof and fry, which, as we'll see, can be thought of as polynomial commitment schemes. And then we also have kind of in the context of these universal proof systems, Hirax and VSQL. And so they fit in kind of the IOP part. So, as we'll see that there's this unifying framework, which is made of two parts, a cryptographic part with the ponyamal commitments, and then information theoretic part with iops. And so in blue, we have the iops, and in green, we have the cryptography. And then in recent years. Sorry, math.
00:02:55.166 - 00:03:56.934, Speaker B: Yeah. So, over the years, we've had more and more kind of iops, and very, very recently, we kind of had this boom of iops, and we've kind of reached an all time high in terms of citations for this initial paper. That was in 2010. And I think we'll see more and more interest in this new construction of polynomial commitment schemes. And by the way, I've shared the slides in the telegram chat, and all of these links are clickable if you want to go and learn more later. Okay, so what is a polynomial commitment scheme? I guess before defining that, I want to define the concept of a polynomial oracle, which is kind of an abstraction of a functionality. And so basically what we have here is we have an approver who has a polynomial, and this polynomial can be very, very big with lots of information to describe it.
00:03:56.934 - 00:04:38.114, Speaker B: For example, a very large degree with lots of coefficients. And so what the prover can do is he can encode his polynomial in an oracle. And then the verifier, he doesn't really care about the details of the polynomial. He only wants to make evaluation. So he has the evaluation points, and he can make queries to the oracle, and the oracle will respond with the evaluation here. So this is kind of the functionality. We have an encoding a query and then a response, and it's the same thing with polynomial commitments, but we use slightly different language.
00:04:38.114 - 00:05:30.894, Speaker B: So we have a polynomial f. The prover will commit to the polynomial, hence the term polynomial commitment. So this will be a binding commitment, and then the verifier can ask to open at a specific point, and then the prover will prove to you cryptographically that the evaluation of the polynomial f at z is indeed what the prover claims it to be. That's the basic definition. If you really want to kind of follow through kind of the nitty gritty with all the fine details. This is the full definition of a polynomial commitment schemes, as appeared in Marlin. And I must admit, I don't follow all the details mean.
00:05:30.894 - 00:06:34.294, Speaker B: I think that's maybe even an opportunity for improvement, for cryptographers to try and find better ways of expressing these concepts formally, because it really is difficult for a non cryptographer to actually understand all the subtleties expressed here. Just a quick question. Is there any intended property of knowledge, or lack thereof, like, the prover doesn't want to reveal anything about the polynomial? Perhaps? Yes. So there's this extractability property, which is that basically, like knowledge, soundness. Basically, the prover needs to know the polynomial not only needs to be able to make valuations and prove the valuations, but he needs to know them. There's kind of another property which is usually discussed, which is hiding, also known as zero knowledge. And here it's actually not super important, I'd say, in the context of this discussion.
00:06:34.294 - 00:07:26.502, Speaker B: And the reason is that you can kind of build in zero knowledge not at the cryptographic layer, but at the IOP layer and the information theory. So you're going to add kind of blinding factors in the polynomials. But it turns out that, as far as I can tell, all the known polynomial commitment schemes, you can also have kind of blinding, I mean, hiding at the cryptographic layer as well, if that's what you want. My philosophy, I guess, is to try and keep the cryptography to a minimum. And so you want to have it very contained, kind of this very small but powerful black box. And then all the interesting stuff kind of happens outside. And so my personal kind of preference would be to put blinding at the IOP layer.
00:07:26.502 - 00:07:41.040, Speaker B: But you can do either, I guess. And you can see here there's kind of a hiding definition, which is this hugely complicated thing that I don't understand.
00:07:44.690 - 00:08:18.330, Speaker C: With hiding for Casa commitments, it's actually very easy because the commitment is very opening proof, and the commitment are both very small. But for some of these larger polynomials, such as those you would get with the groups of unknown order or with bulletproofs, you actually have a logarithmic number of elements. So proving hiding at the IOP layer is a bit nasty.
00:08:21.390 - 00:08:28.626, Speaker B: Sorry, proving. You mean the polymorphism layer is nasty because there's more elements?
00:08:28.758 - 00:08:30.174, Speaker C: Sorry, I didn't hear that.
00:08:30.372 - 00:08:40.260, Speaker B: Do you mean that it's more nasty at the polynomial commitment layer? Because you're saying that the proofs are larger, so there's more kind of surface of leakage of information.
00:08:43.030 - 00:08:59.430, Speaker C: I guess, at both. So trying to sort of hide it in the IOP layer would be sort of a bit confusing because where you should put your hiding blinding values is going to depend on your commitment scheme.
00:09:00.410 - 00:09:17.806, Speaker B: Okay. Because the way I interpreted plonk, for example, the hiding is not a problem in plonk. Okay? Right. Okay. So there's more subtle, then there's some iops which are kind of friendly to blinding, and then there's some maybe that are much more difficult and you'd want.
00:09:17.988 - 00:09:30.740, Speaker C: No, it's not that. It's that carte polynomials reveal so little information that if you want to put the blinding in at the IOP layer, it's quite easy to do. But if you're trying to compile them with different types of polynomial commitments, it's sometimes more.
00:09:31.990 - 00:09:47.400, Speaker B: I see. Okay, so basically you're saying that the blinding at the IOP layer is actually dependent on how you compile. So which polynomial commitment scheme you use.
00:09:48.670 - 00:09:49.514, Speaker C: Yeah.
00:09:49.712 - 00:10:42.294, Speaker B: Okay, interesting. Okay, so this is kind of the big picture that I want to try and describe. So basically you have these three fields. So you have computer science, information theory and cryptography, and they're kind of nice and segregated. So where you start with is on the left side here with your circuit and your witness. Actually, even to get to that point, you might have to do a lot of work. So you might have a high level python code, which you will compile down to a ram program or a circuit, and then you'll translate it to r one cs, and then you might do other things like translating to qaps and blah, blah, blah.
00:10:42.294 - 00:11:45.626, Speaker B: But at the end of the day, you have basically an encoding of your circuit and your witness, which is going to be friendly to the middle bit, the information theory. And what you want is kind of on the right side here to have a universal snark, the kind of spit that is created out of this framework. So what are the other components that go in the middle here? So we have this encoding of the circuit and the witness as polynomial oracles. And here is basically the reason why we use oracles here is to get succinctness, right? Because there's a huge amount of information encoded here in the circuit, in the witness. And if we want succinctness, we need to have some sort of tool to compress the information. And this is what it is here, the polynomial oracle. And then the polynomial oracles kind of interact with an interactive oracle proof.
00:11:45.626 - 00:12:49.410, Speaker B: So interactive oracle proof is when you have a prover and a verifier and they're talking to each other, the prover sends oracles to the verifier. The verifier can query these oracles, and then the verifier can also send challenges in the form of kind of randomly sampled field elements or to the proverb, so is this interactive thing? And out comes the snot. But there's a few missing pieces here. In order to have the randomness, you need a random oracle. And in order to actually kind of have a snock, you need to instantiate the oracle. So the oracles are these kind of abstractions which don't really exist in real life, and you want to instantiate them or materialize them. And so you're going to use a hash function to instantiate the random oracle that's known as the fiat shamir heuristic.
00:12:49.410 - 00:13:56.322, Speaker B: And here, this is the important part, you're going to have a polynomial commitment scheme that is going to instantiate the polynomial oracles. And yeah, there's these very clean kind of boundaries, which makes it much easier to reason about. So there's been this flood of new constructions. But actually, once you conceptualize them in this new framework, it becomes much simpler and clearer in my mind. Does that make sense to everybody? So within each of these columns, we kind of have various compilation steps. So I mentioned, you know, you might go from Python code to a circuit, to r one cs, et cetera, to polynomials. And then here you kind of have this cliff where you encode your information into oracles.
00:13:56.322 - 00:14:59.610, Speaker B: And what you lose here, this first cliff, is a little bit of soundness. So basically you go from having perfect soundness to statistical soundness. And then here there's also a few compilers internal. So you might add your random oracle to remove the verifier, things like that. And then when you want to go past this boundary, you're going to have another cliff, basically, where you instantiate your oracles and you're going to lose also a bit of soundness there. So you're going to go from statistical soundness to computational soundness. Um, and I guess one of the interesting things here is that here, when you encode your circuit into polynomial oracles, because the circuit is fixed, you can actually do this encoding ahead of time in a sort of offline phase.
00:14:59.610 - 00:16:11.870, Speaker B: And this is actually kind of a requirement for succigness of the verifier. So that the verifier has a very small encoding of the circuit. And one of the terms used here for this preprocessing is kind of holography. So you basically take your circuit and you encode it into a polynomial commitment. Okay, so I mentioned that in order to have succinctness, we need to kind of compress the circuit and the witness. And it turns out that the modern framework uses polynomial oracles. But you might ask why polynomial oracles? There's various other things you can try and do to compress, and maybe the simplest thing you can do is if you have a lot of objects, you can kind of put them in a set, and you can ask the queries that the verifier could make is set membership.
00:16:11.870 - 00:17:12.162, Speaker B: Is this object part of that set? And it turns out there's this cryptographic primitive called an accumulator, which basically instantiates this kind of set oracle. And there's RSA accumulators, there's merkel trees, all sorts of things. And so the thing is, there's kind of a hierarchy of oracles that you can choose from. The least powerful is going to be, let's say, the set oracle. But you can try and get more powerful things. So you can try and use a vector oracle, which is kind of a list of elements, as opposed to being kind of unordered set. You can try and have a low degree test, polynomial oracle, and you can also have inner product oracle.
00:17:12.162 - 00:18:03.542, Speaker B: And basically, as you go down this hierarchy, you gain inexpressiveness. So from the point of view of the information theory, the IoP, you can do more powerful stuff. But that comes at a cost, right? Because each of these oracles, they need to be instantiated with a corresponding cryptographic primitive. So, for example, the primitive corresponding to the vector oracle is going to be a vector commitment. And you want to be in a position where you're happy with the costs associated with your cryptographic primitive. And kind of, historically speaking, we've kind of gone all the way down to the inner product. So schemes like graph 16 went all the way to the inner product.
00:18:03.542 - 00:19:02.186, Speaker B: And schemes like bulletproofs also had this inner product argument. And I would claim that there's this barrier of practicality between polynomial commitment schemes and inner products, where the price that you have to pay to go from here to here is just too high and you just don't want to cross this barrier. And specifically the price that you have to pay. So, for example, in the case of pairing based arguments, is that you have a trusted setup, which is non universal. So you kind of have to bake your inner product queries in the trusted setup, which is very expensive in practice, because doing all these trusted setups per circuit is very expensive. And so this is why we're kind of in kind of the modern snarks. We're kind of dialing it down a little bit.
00:19:02.186 - 00:20:02.650, Speaker B: We're having something a little less powerful, which is the polynomial commitment. And it turns out through various recent discoveries like sonic and Planck and Marlin, that actually polynomials are pretty damn powerful, and you don't actually need the expressiveness of the inner product to do Snox. Does that make sense? And the reason why I call it a hierarchy is because for some constructions, there's a natural way to amplify them from one to another. So, for example, you can take an RSA accumulator, and you can amplify it, can make it stronger to an RSA vector commitment. And so that's what Benedict guens and others have done, for example. And then you can take a vector commitment. And then you can amplify that to a low degree test.
00:20:02.650 - 00:20:28.606, Speaker B: And this is basically what Fry has mean. I'll go through the details of fry. But this is effectively what it's done. If you start with a vector commitment, you can get a low degree test. And then it turns out that all the constructions that we know of for polynomial commitments. Can be also made into low degree test. Or vice versa.
00:20:28.606 - 00:20:48.554, Speaker B: You can take the low degree test. And kind of amplify it to polynomial commitments. So you have this nice hierarchy. And then it turned out that all the inner product arguments. Can also be thought of as polynomial commitments schemes. Because, basically, the query that you make. Is going to be a very structured one.
00:20:48.554 - 00:21:07.920, Speaker B: Where you take. So if you want to evaluate at x, what you do is that you take the inner product with one x x squared, x cubed, et cetera. Does that make sense to everyone?
00:21:11.170 - 00:21:16.370, Speaker A: I personally am really, really liking the way you're laying a lot of this out. It's incredibly helpful.
00:21:17.030 - 00:21:36.662, Speaker B: Okay. I can say so far, yeah, there's this tug of war. There's a trade off. And it turns out that the sweet spot, in my opinion, and we've nailed it. Is the polynomial commitment here. And this is why this primitive is so important. Okay, so this was kind of motivation.
00:21:36.662 - 00:22:09.240, Speaker B: And now I want to tell you a bit more about the actual constructions. What do the polynomial commitment schemes actually look like? So we kind of have four different flavors. And these four flavors kind of correspond to four different areas of cryptography and math. So we have constructions that are based on hash functions. And that's fry. So the setup is very easy. You just pick a hash function.
00:22:09.240 - 00:22:38.910, Speaker B: And kind of common to all the discussion here is, implicitly, we have a prime field. A field over which the polynomial is defined. So, f f is going to be a field. H is going to be a hash function. And it turns out that very, very often, we want to do ffts. So, what we're going to do is we're going to pick a subgroup of the field. Which is going to be the roots of unity.
00:22:38.910 - 00:23:07.770, Speaker B: So, generally, you want your field to have so called high two identicity. Which means that it has a very large group, like roots of unity. So w will be the generator, I will be a primitive root of unity in the field. And this is your hash function. So all of this is kind of transparent. There's no trusted set up, nothing like that. And this is what the commitment looks like.
00:23:07.770 - 00:24:05.402, Speaker B: So you have your function, f. And basically, you're going to evaluate it on some sort of evaluation domain. And the evaluation domain is going to be some sort of subgroup, some roots of unity. And here d, it's quite small, but d is the degree of the polynomial. And what you're going to do is you're going to evaluate it on a domain which is greater than the degree of the polynomial. So what you're doing is you're kind of adding redundancy, right? Turns out it's called a read Solomon code, where you kind of extend the, normally a polynomial can be uniquely described by evaluating at D points where d is its degree. But here you're going to evaluate at more than D point kind of k times D.
00:24:05.402 - 00:24:57.050, Speaker B: And k is going to be relatively small, like eight or 16. Okay? And then you're going to take the merkel root of these elements. So this is where the hash function comes in for the commitments. You can see how kind of basic queries can be made here. So the verifier might say, okay, give me the evaluation of f at this kind of root of unity, which I choose, of my choosing, and then the prover will give you the evaluation and the corresponding Merkel path. Okay, so that was hash functions. My slides are a little slow to load.
00:24:57.050 - 00:25:38.620, Speaker B: Okay, the next construction is with pairings. So you have g one and g two. These are your pairing friendly setup. You have generators, g one, little g one, generator of big g one, little g two, generator of big g two. And you have your pairing. And what you also have is a secret, like a field element, which is secret and which is encoded in this so called powers of Tau. So in our case it would be powers of S.
00:25:38.620 - 00:26:37.926, Speaker B: And basically that allows you to construct this commitment here. So the specific powers is going to be powers of s times g one, the generator in g one. And so here, basically what we're doing is that we're encoding kind of coefficient by coefficient using the powers of Tau. Okay? And then we have a very recent construction which is based on unknown order groups. So that's the UO here. And here what you have is a group of unknown order. So something like an RSA group for which you don't know the factorization of the RSA modulus.
00:26:37.926 - 00:27:07.720, Speaker B: So an RSA modulus is the product of two large primes. You can't factor factorization is hard. And in order to know the order of the group, you kind of need to factor to find the two prime factors. So that's one example of an unknown order group. You have a generator G in G, and you have some large integer which we'll talk about. And then this is your commitment. So it's actually somewhat in the same flavor as here.
00:27:07.720 - 00:27:58.706, Speaker B: You're kind of encoding the coefficients. So it turns out that this part here without the g is like a huge integer. So you take your polynomial and you're going to encode it as an integer, like a very, very large integer. And then groups of unknown order, they're kind of designed to commit to integers. And the reason is that because you don't know the order of the group, you can't reduce by the order of the group, so it keeps wrapping around and you don't know where you end. And so it's very difficult for the prover to cheat. So basically, groups of unknown order are great to encode integers.
00:27:58.706 - 00:29:01.420, Speaker B: And so you need to do a little bit of work in order to properly encode your polynomial as an integer. And then you have this final construction, which is based on discrete log groups, by the way, I should say that this is called docs. This is a recent work, also by Benedict Bones and others. And then you have the discrete log group construction, where basically you have an elliptic curve and you kind of have randomly chosen elements in g, which are going to be some sort of a basis, and they're going to be independent relative to discrete logs. And you're basically going to take, like, the linear combination, basically the point in the vector space generated by these random elements. And this is how you encode your polynomial. And this is how basically bulletproofs work.
00:29:01.420 - 00:29:22.000, Speaker B: So we have fry, cate, dark and bulletproofs. Any questions so far? So, I'm just trying to give flavor of. Yeah, I have a question. Like, in the unknown order groups, is q secret? No. Q is public. All right. Yeah.
00:29:22.000 - 00:30:20.962, Speaker B: So q is going to be on the order of p to the power log, d, I believe. And p is going to be basically the prime, the size of your field. So f is a prime field, p is the size of the field. And basically the reason why you want Q to be large is so that the encoding to an integer is kind of well defined. You can kind of uniquely decode it back to a polynomial to avoid these problems of overflow. Basically, you want the individual coefficient here to be very well isolated so there's no pollution between them. But if you take an, you want Q to wrap around every time when you raise it to some power.
00:30:20.962 - 00:31:02.634, Speaker B: Right? Okay, so you have this commitment, right? And it's an algebraic commitment. These last three here are very much algebraic. And so you're going to want to make use of the algebra. Right. So you're going to do additions and you're going to do multiplications on these things. And when you do these algebraic operations, you're going to basically affect these coefficients here. So if you do an addition, let's say you have two of these points, a zero all the way up to ad, and then b zero all the way up to bd, and you take the sum of these two.
00:31:02.634 - 00:32:18.178, Speaker B: Well, now suddenly you're going to have a zero, b zero plus b zero, and then all the way up to ad plus bd. And in the world of polynomials, the coefficients, they're kind of local to every monomial, right? So if you add something in x and something in x squared, then they're never going to kind of interfere with each other. But if, let's say, a zero is very, very close to q, so a zero is large and b zero is large, then suddenly you're going to have an overflow. You're going to have a zero plus b zero is greater than q, and so you're summing two constant terms, but they're going to interfere, they're going to overflow into the x term. Does that make sense? Yeah, I think it clarified that. Thanks. So you have these nice homomorphic properties here, additively homomorphic and also multiplicatively homomorphic.
00:32:18.178 - 00:32:56.150, Speaker B: So it's kind of fully homomorphic, but there's a caveat. And the caveat is that these homomorphisms, they only work if these coefficients are small. And so a lot of the work in docs is just proving that these coefficients are small. But I'll talk about that later. Okay, so we have these four flavors. Now let's kind of look qualitatively like, what can we tell already just by looking at the setups? So the nice thing about the hash function as a setup is that you have very nice properties. It's transparent.
00:32:56.150 - 00:33:45.622, Speaker B: The setup is succinct, meaning that the information that the prover needs to have to generate the proof is tiny, it's unbounded, meaning that you can prove statements that are arbitrarily large. You can have a circuit with a billion gates, a trillion gates. That doesn't matter, there's no limit. The setup is updatable in the sense you could just change the hash function. If you decide your old hash function is broken, that's fine, easy to do, and it's also plausibly post quantum. So hash function, based on your commitments, at least from the qualitative standpoint, are very, very attractive. And then you have kind of almost the opposite, which is the pairing group.
00:33:45.622 - 00:34:25.060, Speaker B: So, pairing group, they're not transparent. You need to do a setup. They're not succinct in the sense that the bigger the polynomials you want to commit to, the more information the proven needs to have. And it gets really bulky, like tens, hundreds of gigabytes, terabytes potentially of information. So not succinct. There's a downside also not unbounded. So if you do a setup which is going to be a powers of tau setup or powers of s, your powers of s are only going to go up to a certain size.
00:34:25.060 - 00:35:04.982, Speaker B: And so even though you're going to have a universal scheme, it's only going to be universal up to a certain size. So that's yet another downside of pairings. And it's updatable. But there's a lot of caveats here. So one is that it turns out that in the world of there's lots of pairing friendly groups. There's bn two five four Bs, twelve 381. And people are looking into recursion friendly stuff like Mnt four, mnT six vs.
00:35:04.982 - 00:35:52.430, Speaker B: Twelve 377. And so basically, for every one of these curves, you have to do a different setup, which is not fantastic, and it's also not post quantum. But as we'll see, kind of the pairing groups from a quantitative statement, they're amazing. And hash functions from a quantitative statement are kind of meh. So ying and yang, it's a trade off space here. So it kind of depends what you care about. If you care more about kind of purism and kind of long term thinking, or do you care about very pragmatic, I want to get stuff done today kind of attitude.
00:35:52.430 - 00:36:36.570, Speaker B: And then you kind of have RSA groups which are somewhat in between. So not transparent like parent groups, but they have kind of a few killer features relative to parent groups. One is that they're succinct. So the setup in an RSA group is one single RSA modulus, and that's it. The modulus is going to be 3000 bits or 2000 bits, so just a few hundred bytes, and that's all you need as a prover in order to generate the proof. So it's very handy, very lightweight in stark comparison to parents. The other advantage is that it's unbounded.
00:36:36.570 - 00:37:11.574, Speaker B: So you really have universality. You're not kind of constrained by the size of your powers of tau. You can have circuits which have a billion, a trillion gates. That's not a problem. On the downside here, it's not updatable. Unlike the ceremony for RSA moduli, we don't know of any that's kind of updatable. So it's kind of a one time thing.
00:37:11.574 - 00:37:53.250, Speaker B: And then you kind of need to trust this one time setup without kind of the niceness of continuously updating. But the cool thing here is that it's not like there's like ten different flavors of RSA groups, like the elliptic curves. It's like there's one type. And so there's no bike shedding here, there's no cleverness. You just do this one setup once, and for just going to read the messages if there's some questions. Okay. And it's also not post quantum.
00:37:53.250 - 00:38:41.090, Speaker B: This is another group of unknown order. So we know of two types of groups of unknown order. We know the RSA groups and the class groups. And from a qualitative standpoint, class groups are significantly better than RSA groups. The reason is that they have this transparent setup, and automatically that makes them updatable. So basically, the setup is just pick a large prime, and you can create a new group of unknown order just by sampling new prime. So that's very easy to generate, these class groups.
00:38:41.090 - 00:40:02.882, Speaker B: And actually, TIA, which is a blockchain project that is looking to use class groups for vdfs, that's one of the reasons why they chose class groups, is because you can update them very often. So if you're worried about the security of your class group, you're worried that an attacker, given some amount of time, can find the order of your class group. Well, if you update it fast enough, then you're making it harder for the adversary, because if it takes, let's say, a whole week to find the order, but you update every ten minutes, then there's no way the attacker can actually disrupt your system, which is nice, and also post quantum. So it turns out that, kind of, as a general heuristic, any group of unknown order is not going to be post quantum. And the reason is easy, because Shaw's algorithm is kind of designed to find orders of things. And so there's little hope that we will find a group of known order, which is post quantum, which is a bit sad. And then we have the discrete log, the final type of setup, which is kind of a strange beast.
00:40:02.882 - 00:40:46.730, Speaker B: It's kind of yet a different point in the trade off space where it's transparent, which is very nice, unbounded, updatable, not post quantum, but it has this kind of big downside, which is that it's not succinct so basically, the amount of stuff that you need to prove the statements kind of grows with the size of your statements. And it turns out there's an even bigger downside, which is that the same thing happens for the verifier. So the verifier is actually not succinct. It takes a linear time to verify these proofs. Any questions here on the qualitative comparison?
00:40:50.110 - 00:41:02.080, Speaker C: Would the setup for the discrete bulb group not be succinct because it is transparent, meaning you can get it from a pseudorandum function, doesn't change the verifier being.
00:41:04.850 - 00:41:59.790, Speaker B: Yes. So I guess it's a question of what Mary is saying is that you can, on the fly, generate your setup, because all you need is a random oracle that will kind of hash to your discrete log group. But it turns out that in practice, I believe this is not what is done. And the reason is that there's a cost associated with actually instantiating the random oracle. So you basically need to run all these hash functions and you need to compute in real time. So I think what's done in practice is that ahead of time, you're going to pre compute everything. And it's even possible that there's like, pre computation goes beyond just computing the basis elements.
00:41:59.790 - 00:43:03.160, Speaker B: Probably someone else in this room knows better than me, but maybe Zach might even know. There's these fancy multi exponentiation algorithms that are sub linear. Do these require pre computation? Well, they require you to compute the generators, because I think in the discrete log setting, you just have a very large number of unique generators, and you do reconfigure just as a point of reference. You can get sublinear commitment schemes in the discrete log setting for both the verifier and for the setup. But sublinear is still not great because the complexity is the square roots of your commitment degree, your polynomial degree. So it's still pretty. Right.
00:43:03.160 - 00:43:39.060, Speaker B: So I think what Zach was saying is like, basically two things. Number one, that if you want to use these fancy sublinear multi exponentation algorithms, you need to pre compute the bases. And so this is kind of a nice justification for the fact that it's not succeed. And then the other thing that just to add to that, the number of bases you need will be equal to the square root of the degree of the thing you're committing to. Okay, I guess it depends on what you mean by succinct. It's kind of succinct, but not like practically. It's still there.
00:43:39.060 - 00:44:08.234, Speaker B: Okay, interesting. Yeah, maybe not succinct okay. And were you saying that kind of, there's like different schemes that are also based on discrete log, but have a different trade off space. So you might have square root verifier time, but you might have more prover timers. Yeah, precisely. Okay. You can get square root of verified time.
00:44:08.234 - 00:45:04.154, Speaker B: That gives you. Okay, so now we're kind of done with the qualitative. We can look into the quantitative. And here for the hash function based schemes, everything is kind of mer, and especially mer is the proof size, where you have this square factor log squared, as opposed to just log. And the reason you have this log squared factor is because, as we'll see, it's a recursive scheme with log n round. And in each round you need to do these commitments, which themselves will cost a log factor. So, log times log squared.
00:45:04.154 - 00:46:04.354, Speaker B: And what this means in practice is that you have relatively large proofs on the order of 100 hundreds of kilobytes, between 102 hundred or maybe between 51 hundred for reasonable seconds. Verify time. It doesn't look fantastic because you have this square here, but actually the verifier is mostly just hash functions, which are pretty fast and cheap. So not too worried about verified time. Improver time is okay, I guess. And then you have the pairing group where here it's like, wow, constant proof size, constant verifier time and optimal prover time for the polynomial commitment scheme. But I must have a little caveat here on the linear time here is that, and we'll discuss this later.
00:46:04.354 - 00:46:52.270, Speaker B: If you start by encoding your coefficients in Lagrange space, then even just to do the evaluation, just that you need to do a Fourier transform, and that's going to cost you a log factor. So this optimal performance is only if you start in the monomial basis, and then you have the groups of unknown order, which are not as good as pairings, but kind of better than the hash function. So you kind of lose this log square and you're down to log, which is kind of nicer.
00:46:54.710 - 00:47:04.906, Speaker D: I hope I'm not too loud with the background here, but I think this may be a little unfair to the hash functions.
00:47:05.038 - 00:47:05.478, Speaker B: Okay.
00:47:05.564 - 00:48:13.354, Speaker D: Because I think there's a sort of origins to apples. I may be wrong, but I think if you look at bit operations, then you're already measuring the hash functions in bit operations here, right? Because I think it's log squared. Because you do this like log d hashes and you're thinking of every hash, right? I think that's what you're doing. And if you look at it's one in the pairing group, if we think of the group operation as one, which I think is natural to do for simplicity. But I think in this table it comes out unfair to the hash functions because I think in bit operations, the middle column is going to go up to o log d and the uo column is going to go up to log squared. I should verify that. But I think that's what's going to happen if you measure everybody in bit operations, right.
00:48:13.354 - 00:48:20.380, Speaker D: Because you need to work with a group of size, at least d. So every operation is going to be like log d.
00:48:24.130 - 00:48:40.180, Speaker B: Interesting. Right. This is kind of oranges to apples, and this is why we have the big O notation, to kind of partly remove the constants. These are different operations. So this is a pairing operation and this is a bit operation. Yeah.
00:48:42.390 - 00:48:51.318, Speaker D: If you say bit operations on the first and group operations on the second and third, yeah, that's fine, I think.
00:48:51.484 - 00:48:52.150, Speaker B: Right.
00:48:52.300 - 00:49:07.660, Speaker C: Another way we sometimes get around that technicality is explicitly including the security parameter in the big O location, which I'm never sure is the right thing to do because it can introduce its own confusions. But it's an option.
00:49:08.510 - 00:49:17.726, Speaker D: But I think for the o of one, you'd also need to include d. You'd also need to think of d as a constant because, well, lambda is.
00:49:17.748 - 00:49:21.920, Speaker C: Always going to be bigger than d, so I think that covers it.
00:49:22.950 - 00:49:23.700, Speaker D: Okay.
00:49:25.110 - 00:49:25.954, Speaker B: Yeah, I guess.
00:49:25.992 - 00:49:36.710, Speaker D: And then the question if the hash function is still log squared d, if you assume lambda is a constant larger than d, I'm not sure.
00:49:36.860 - 00:49:37.558, Speaker B: Right.
00:49:37.724 - 00:49:47.094, Speaker D: Yeah, maybe it is, because maybe it's actually log squared d times like log, the security parameter in the hash function, right?
00:49:47.212 - 00:50:45.434, Speaker B: Yes. So the same comment can be said about the hash functions that as d becomes very large, you need to increase the size of the hash outputs. And so I guess Mary's comment that you just assume the security parameter is greater than the degree, and then you just put the security parameter on the table. Yeah, that's probably fairer. Yeah, for sure. And I guess in the context of pairing groups, one reason why it's conceptually simpler to think of them as constants is because we pick a pairing group which has sufficient security for all circuits, practically known to humankind. And technically, if you want to consider kind of galactic circuits, which are kind of greater than what human can do, then this model breaks down.
00:50:45.434 - 00:51:42.154, Speaker B: But in practice, this is a reasonable simplification, I guess. Okay. And then we have the discrete lock groups, which kind of, instead of being very balanced, like the group of unknown order, where it's kind of all orange, you kind of have more variance, so you have a better prover time, but much worse verification time. There's no free lunch here in this trade off space. These are four very different constructions. In a way it's good, but in a way because you can just pick a construction which fits your application. But in a way it's kind of bad because we need to keep in our minds the whole trade off space and, and make the right decision.
00:51:42.154 - 00:52:54.340, Speaker B: It would be much simpler if we just had one construction to rule them all. And maybe we'll have one of these soon. One reason why I'm cautiously optimistic is because these polymorph commitment schemes, as a construction, they were kind of an obscure thing, but now they're a center stage, so more attention, I think, will be given to them. Okay, have a small question. I don't know if there's a clear answer, either. I asked those of the starcore sessions whether the checksum algorithm in the kind of, the hierarchs and the Libra schemes are kind of instantiable with polynomial commitments, or they're comparable, and if so, then where would it fit in these comparisons? Was that a clear question? Can anyone in the group answer it?
00:52:59.690 - 00:53:52.130, Speaker C: So, one thing I would say about checksum is it is designed for boolean polynomials, as opposed to polynomials over finite fields, and therefore it has quite a different flavor to when we're trying to do things over finite fields and different trade offs for sure. So it makes it quite hard to compare it directly. You will get a proof size which is logarithmic, a verifier time, which is logarithmic, and a prover time, which is, I think, d log d. Okay. Yeah, but you can't use it for the same kinds of functions.
00:53:52.210 - 00:53:55.560, Speaker B: Yeah. Thanks.
00:54:00.980 - 00:54:37.710, Speaker D: I want to make an informal argument why pairings will win, because the picture I have in my head is that in 1020 years, when, if this stuff on the optimistic case becomes standard and everybody's using it, you have some engineer that doesn't know all these nuances we're talking about, and he's just going to pick the most efficient thing. He's not going to pick something less efficient because of some nuance that is very removed from him and abstract. So that's why I think pairings will win.
00:54:39.680 - 00:55:01.600, Speaker B: So the counterargument to that would be that in 20 years you'll be able to buy or build a quantum computer or something that's maybe not large enough to break crypto, but scary enough that cryptographers would frown upon people deploying crypto that is not post quantum.
00:55:03.080 - 00:55:10.740, Speaker D: Oh yeah. My argument is assuming pairings and discrete log is not harshly broken.
00:55:12.840 - 00:56:20.030, Speaker B: Yeah, I would agree with that. Then there is one thing to be said about the hash functions, which is interesting, which is that basically the main way where they're inferior to the rest, is that they have large proof sizes. But it turns out that if you take a historical perspective and you look at megatrends in computing, bandwidth is cheap, and it keeps on getting cheaper and cheaper. So you have this law called Nielsen's law, which is kind of the equivalent of Moore's law for bandwidth, and it says that every, every year you have a 50% increase in bandwidth. And what we found is that the equivalent law for computation, like for raw cpu horsepower, just is no longer exponential. It's a very tiny, very slow exponential, something like 3% every year. So you're comparing 50% increase every year to 3% increase every year.
00:56:20.030 - 00:57:00.650, Speaker B: In ten years time, I made the calculation, 100 kb will correspond roughly to 2 years time. It will be even less. Like 100 kb will be nothing. It will be the equivalent of maybe less than 100 bytes. And so you'll get all the benefits, really, because the only thing you have to pay for is data. But data will be so insanely cheap in 20 years. That's also definitely a way forward.
00:57:04.670 - 00:57:15.760, Speaker D: Well, still, there's a question. If data on a distributed ledger will be insanely cheap. I don't know if that will follow the same law or if you're arguing that it will.
00:57:16.610 - 00:58:11.250, Speaker B: Yeah. So what we have seen is that it has been followed on Ethereum, at least. And the reason is that Ethereum was launched about five years ago. And in the next kind of hard fork, which is happening in just a few days, there is kind of a gas adjustment where we're making data cheaper and we're making it significantly cheaper. We're making it, I forget exactly, but something like five times cheaper, something like that. And this multiple roughly corresponds to the five year time span that has happened between when Ethereum launched and today. And so it's possible that we will make more hard forks in the future to lower the gas price, or we'll have kind of dynamic pricing where the gas cost of data will kind of decay exponentially relative to the other opcodes in the Ethereum virtual machine.
00:58:13.270 - 00:58:37.590, Speaker C: I'm inclined to say that short of crypto analysis making progress, it doesn't make sense to choose a winner, because each of these settings, I mean, you've seen in every single slide. They have different trade offs, which means that different schemes would be suitable for different solutions. So they're all good in their own setting.
00:58:39.770 - 00:59:20.002, Speaker B: Agreed. Yeah. For example, if you look at discrete logs, it doesn't look very attractive because you have this verifier blow up. And the whole point of succinctnesses on blockchains is that the blockchain is very weak and it doesn't have much computational power to verify these things. But it turns out that bitcoiners absolutely love the discrete log setting. And the reason is that they already have the discrete log assumption within their cryptographic stack, in the signatures in ECDSA. And they only care about proving very small statements, things like range proofs, in order to get confidential transactions.
00:59:20.002 - 01:00:17.146, Speaker B: And so if you have a tiny, tiny, tiny circuit, then actually the discrete log setting is very attractive. So, yeah, it's very much going to depend on your personal preferences and on your specific. The size of your circuit and things like that. But the nice thing is that in the IOP world, there isn't so much bike shedding. I'd say it's much clearer how you can compare the iops. So you could say without any question or without any doubts, you could say that, for example, Planck is just better than sonic. As a know, no one will bike shed you on that, but will argue with you.
01:00:17.146 - 01:00:55.510, Speaker B: But here, there's a lot of personal opinion that comes into play at the cryptographic layer. Cryptography, right. Like, philosophically, it's faith, it's like religion, right? If you have these assumptions and no one knows whether they're true, just like no one knows whether God exists or not, and you kind of. You pick your religion and. Yeah, it's kind of a similar thing. So I realized that we're already past the hour, and I don't think I've even gone through one third of my slides.
01:00:58.170 - 01:01:06.890, Speaker A: Oh, wow. Okay. Well, then I guess that sort of leaves us to this part of the thing, which is we decide.
01:01:08.990 - 01:01:09.386, Speaker B: Do you.
01:01:09.408 - 01:01:10.780, Speaker A: Want to do another one?
01:01:11.470 - 01:01:34.098, Speaker B: I mean, I'd be more than happy to continue if people want that as a participant. This has been fantastic, and I would gladly join another session. I really appreciate your ontology. It's helpful. As someone who is not so helpful with all the intricacies of all the papers, it's helpful to see it laid out like this. I second that. Yeah.
01:01:34.184 - 01:01:40.290, Speaker D: I would feel less guilty about slowing you down if you had another session to do more slides.
01:01:41.110 - 01:01:44.040, Speaker B: Okay. Please have another one.
01:01:47.850 - 01:01:54.134, Speaker A: Something be up for. We could talk about this in a dm you don't have to say anything here.
01:01:54.332 - 01:01:58.440, Speaker B: I've done all the work to prepare the slides. That was the hard part. The presentation is easy.
01:02:00.490 - 01:02:02.054, Speaker A: That would make sense for us.
01:02:02.172 - 01:02:19.980, Speaker B: Yeah. I mean, here's a very good point to break, actually, because I've kind of in the motivation and I've given the high level comparison. And then from the next slide onwards, I actually dig deep in the actual specifics of the constructions. So here would be a good place to.
