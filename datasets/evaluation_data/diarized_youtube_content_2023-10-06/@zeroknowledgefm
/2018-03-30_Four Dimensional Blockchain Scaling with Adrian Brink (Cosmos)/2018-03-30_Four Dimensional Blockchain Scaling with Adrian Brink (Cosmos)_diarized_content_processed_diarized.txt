00:00:00.570 - 00:00:28.562, Speaker A: Great. Can everyone hear me? Okay, like in the back. Is this loud enough? Awesome. Okay, so let's get started. Thank you everyone for coming. I'm adrian. I'm working on the Cosmos project and this talk used to be called three dimensional blockchain scaling with cosmos antenna, but now it's four dimensional scaling because I gave the precursor to this talk at ECC in Paris.
00:00:28.562 - 00:01:03.410, Speaker A: And then afterwards in discussions, we realized that we actually missed one of the dimensions. So now it's four dimensional scaling. We may add more or remove them as we go along. So if you have ideas around this, let me know. Yeah. This talk isn't designed to be telling you exactly how to scale your blockchains. It's mostly to give you an overview of some of the possibilities around how you can scale, and that it's like everyone's responsibility to figure out how to scale your specific application.
00:01:03.410 - 00:01:30.968, Speaker A: There isn't a one solution that just works for everyone magically. All right, so this is like a brief agenda. Okay. All right. So I will begin to give a quick introduction of the problems around scaling interoperability. From then on, I'll actually talk about the four very specific ways we can scale blockchains. And lastly, I'll talk about security models.
00:01:30.968 - 00:01:59.030, Speaker A: And security models are really, ideally, we want to decouple the security models from how we scale blockchains because that makes both parts much easier. Yeah. So that's kind of the overview. Yes. Great. And in the end, we'll do a quick recap. So, actually, to start with, I know talks are hard to follow and I might skip over things.
00:01:59.030 - 00:02:25.624, Speaker A: So these are the three main things I want everyone to take away. So if you take away nothing else from this talk, just take away these three main points, which is there are about four different scaling approaches to blockchain. So four different levels at which we can scale. So the first one is consensus scaling. I'll talk a lot more about this and why it's cool. So using tenement consensus. The second one is state machine scaling.
00:02:25.624 - 00:03:18.510, Speaker A: So how we can actually build very scalable business application logic, then third one is interchange scaling. How can we connect multiple heterogeneous blockchains? And the last one is social scaling. How do we actually build systems that aren't stuck at the Genesis state, but can rather evolve throughout time in a non ad hoc way? Then the available security models that you can pick from, it's like sovereign, hosted or plasma style model. And then probably the most important thing is you have to be an optimistic planner. Scaling is everyone's responsibility. It's like every single one of you can make a choice around how to scale your blockchain, your application, and you really are responsible to make that choice. Don't wait for someone else to figure it out for you.
00:03:18.510 - 00:04:21.840, Speaker A: Okay, so quick introduction to scaling and interoperability. So why do we actually need to scale? Right, so ethereum currently supports somewhere between like twelve and 15 transactions a second bitcoin, between five to seven transaction transactions. And really this is globally, right? So we can't have more than seven concurrent users at any given point in time sending a single transaction on bitcoin. When you think about what this technology is trying to be, this is a ridiculous number that needs to go up by orders of magnitude before this becomes relevant to anything in the real world and so on. The scaling part, the interesting thing there from an application developer's perspective is that if you are building cryptokitties, you are competing with someone that just wants to do payments. Like you're all competing for the same computer resources. And in my mind, this isn't a great model where we have to all run essentially on the same CPU.
00:04:21.840 - 00:05:31.188, Speaker A: I have a very specific need that I want to solve, or very specific problem that I want to solve, and I'd much rather not compete with everyone else on the resources over my application. Did I want to say anything else? Yeah. So that's pretty much, scaling is important. Currently we're in a state where this is a bit ridiculous then on interoperability. So when you think about how current blockchains work, right? So like we have Ethereum and you have a bunch of smart contracts, you have bitcoin, you have bitcoin script, but from a user perspective, it is extremely hard to actually use both at the same times. We have many heterogeneous chains that have no way to interact with each other, except if we go through these central synchronization points, which in today's world are really the centralized exchanges. So imagine if you're an ethereum user and you're a bitcoin user, and you want to use smart contracts.
00:05:31.188 - 00:06:36.140, Speaker A: All of a sudden you have to first go to an exchange, wait, most likely like six blocks, like 60 minutes, get confirmation, switch it into Ethereum, then pay out to an ether address, and then start using a smart contract. So like super high latency. And realistically speaking, there's really no reason why we shouldn't be able to use, for example, bitcoin on Ethereum directly or within Ethereum smart contracts. And lastly, the problem with, without interoperability, each one of the chains is a liquidity trap, essentially, or like liquidity is an additive. If you have interoperable chains, the liquidity within each of those chains adds up, whereas in current system it's mostly that liquidity is restricted to each chain. So this is especially problematic. If you're an application developer and you need some sort of liquidity, you can't really pick the one that's best for your specific use case, and you can't even deploy your own blockchain because you have zero liquidity on it.
00:06:36.140 - 00:07:33.604, Speaker A: So in an ideal world, you want to be able to deploy decentralized applications that can tap into the entire market's liquidity. Now to scaling. So the four dimensions in my mind of current of the layers of the stack and how we can scale current blockchains is consensus scaling, state machine scaling, interchange scaling, and then also social scaling. And just going back to very easy example of how Ethereum works, right? Like Ethereum at the bottom is essentially just peer to peer networking. Then on top of this you have some form of consensus layer. And then built on top of this consensus layer, you actually have the application layer. So that's the state machine level here.
00:07:33.604 - 00:08:37.006, Speaker A: And the state machine in Ethereum is really the EVM and then the interchange. So this is kind of like how you can think about it in the Ethereum stack. And then the interchange part is all about being able to move, for example, being able to move tokens from, for example, Ethereum to Ethereum classic. And the last part is around how can we actually foster large developer communities and have an open process to do protocol upgrades in the future? So how do we move the decision making process from like, let's say it's ten individuals that really control most blockchains. To make this like, the actual stakeholders within the ecosystem have now the ability to come to consensus over upgrading the protocol. All right, let's start with consensus scaling. So how many of you have heard about, how many of you know what BFT consensus is? It's like 30% maybe.
00:08:37.006 - 00:09:01.320, Speaker A: All right, great. So currently, essentially all blockchains are running Nakamoto consensus. So this is literally the simplest form of consensus, the chain based consensus. The longest chain is the correct chain. So if you want to figure out what is the correct chain, you just follow headers until you are on the longest chain. Well, hopefully because you have no way of knowing whether there's a longer chain out there. But that's the kind of, I'll come back to this.
00:09:01.320 - 00:09:50.434, Speaker A: So the dark background here is the block time. And so what you usually have to do. How Nakomoto consensus works is you have some amount of time for validation, so like, validate the state transitions over the given state, and then you have some amount of time for network propagation and mining. And I'm really going to leave out the mining part because it's not as relevant. But then you have all this extra time at the end. And this is essentially Nakamoto consensus, the safety buffer that you have to have in case your network latency suddenly goes up. Right? Because what happens if your network latency goes up? So this is my block time, but most other people haven't.
00:09:50.434 - 00:10:27.198, Speaker A: And so I produce the next block. Most other people haven't actually heard about this block by the time I'm already starting the next one. This leads to a phenomenon of split brain. Like a very pronounced example of this is like if you have network partitions and suddenly network latency jumps for small subsets of the network. First of all, this leads to the fact that you need to have always a safety buffer, so your block times aren't optimal. And secondly, it means that you have only probabilistic finality, so you don't have actual finality. You only assume that after a while, you're not going to get reverted anymore.
00:10:27.198 - 00:10:58.720, Speaker A: And at the same time, as an application developer, you now have to deal with the ability to handle reorgs. So this really, when you look at it, this isn't optimal. In the best case, we need a large safety buffer buffer. In the worst case, we go split brain. And if this network latency goes up permanently, let's say, suddenly, the Internet speed goes down by a factor of ten for some reason whatsoever, we will constantly be in a situation where you're split brain. Nakamoto consensus completely breaks down. In this case.
00:10:58.720 - 00:12:04.610, Speaker A: All right, BFT consensus, byzantine fault tolerant consensus algorithms, they actually go back a very long time. So they go back, I think, to research from. The cool thing here is that you don't need the safety buffer. Your block time is literally just the addition of the time you need for validation, plus to tell more than two thirds of the network or of the validators about that block. So your block times are optimal in almost all cases. And the coolest thing about BFT consensus is that you have finality. So as soon as you see one block, if you see a block, you can tell whether it's completely final or whether it will get reverted at some point in the future, which is like a huge deal, because now, as an application, now we don't have to come up with this magic number of six blocks is probably okay, you can say, like, I'm 100% certain that this block is final, and if there should ever be a reorg, someone is going to lose massive amounts of stake due to that reorg.
00:12:04.610 - 00:13:18.620, Speaker A: What happens in the case, though, that the coolest thing probably is that you're always safe. So you're always safe in fully asynchronous networks, meaning that even if you have an undetermined amount of network latency, you never produce blocks, you never lose safety. Your lightness might go up, so your block time might go start approaching infinity, but if you have a block, you always know that it's correct and not going to get reverted. So the way that, you know, how I explained this earlier on, that's like, if network latency suddenly jumps up, that's just like bad here. It's still bad, but at least we have an inconsensus way to adapt to it. So, in BFT consensus, the block time just automatically starts increasing until we eventually hear, for more than two thirds hear the votes for more than two thirds of the people. So that's in my mind, BFT consensus is really the next frontier for how we're going to scale public blockchains, because all of a sudden, we can go from block times of ten minutes in bitcoin, and then block times of 15 seconds in Ethereum to block times of like 1 second.
00:13:18.620 - 00:14:12.870, Speaker A: Just as fast as we can make it on the network propagation level, we can make our blocks, and we have safety, which is like a massive deal great. So there are currently not many implementations of actual BFT algorithms. So one of them is called tenement consensus, and it's the one we've been building for three years now. And it's within an engine called tenement core, and it prioritizes safety over likeness, essentially. Some of the cool things you get is like, you get instant finality. So as soon as the application sees a block, it knows it's final. The actually application logic never sees anything that isn't final, which means that from an application developer's perspective, you never have to deal with the possibility of reorgs.
00:14:12.870 - 00:14:55.378, Speaker A: The only thing you see are 100% final. You get extremely. Oh, yeah. So I totally forgot to say this. So, in BFT consensus, one of the cool things is because you know who's supposed to be signing these blocks, you can construct extremely efficient, light client proofs. So in traditional, like a motor based consensus, even as a light client, you still have to download every header and verify every header, which means realistically, your phone has to come online and do like an update every 15 seconds, roughly. Here we can say that we know who's supposed to be signing these blocks, and we have a reference to the current set of signers of the blockchain.
00:14:55.378 - 00:15:38.600, Speaker A: And as a light plan, I can take any block and just see if more than two thirds of the people that I already trust have signed this block. This means you can build mobile phone apps that have to synchronize with the chain like once every three weeks. It sort of depends on how you put your economic security guarantees. So if you have very long, unbonding periods, this could even go up to like six months. It's a variable parameter, like how often you need light clients to come back online to synchronize. You're always safe in asynchrony, and you're live in partially synchronous networks. And this always sounds scary that there are cases where you might not be necessarily life, but realistically, this only happens.
00:15:38.600 - 00:16:43.798, Speaker A: So this happens if more than one third of the network goes offline. If more than one third of your community goes offline, you probably should stop and figure out why that happened and use social consensus to then figure out how to fix it. Also, by the way, so if the network latency just increases a lot, you'll still be life. It's like you only have to use social consensus if for some reason, more than one third of your validators got nuked, essentially. So it's only not recoverable if more than one third goes offline forever. And yeah, so some transaction throughput, it's like roughly on 64 validators across the world, you get like to 4000 transactions per second. So it's like a massive speed increase, right? If you can now write fast state machines, you get 4000 transactions per second in an always safe network that has efficient, like clients proof.
00:16:43.798 - 00:17:44.800, Speaker A: I think honestly, b of T consensus algorithms will be the future. One of the downsides is that you have quite high network overhead. So every validator needs to talk to every other validator, so you have like N squared network overhead. But some of the cool future improvements are actually like using BLS signatures to resolve this so that we can do signature aggregation at the peer to peer layer. So instead of me having to receive signatures from, let's say, 67 people in the audience, people can always start aggregating these signatures while they talk to each other. So that in the end I might only have to receive, let's say, ten transmissions actually where the signatures are included. We can do something around optimistic pipelining to get block times even faster, and then using DKG constructions to actually fully prevent minor validator front running so that you encrypt the transactions before you send them.
00:17:44.800 - 00:18:05.080, Speaker A: Oh yeah. So practical takeaways for this. There are currently two implementations in the world that I know of. One is called tenement core, and it's the one we've been building. It's been pretty stable for the last year or so. The other one is by parity. They have an implementation of tenement consensus as well.
00:18:05.080 - 00:18:44.770, Speaker A: You should probably talk to the parity people about this. I haven't looked into it recently, but it used to be there. It'd be great if Rob was here, because he actually wrote most of it. Okay, so that was consensus scaling. So there's already a lot of things we can do at the consensus level to make blockchains massively faster. The next layer of the stack, if you think about like I do, it's state machine scaling. So everything that once you have consensus, now we have a bunch of transaction bytes and we're running them against some deterministic state machine.
00:18:44.770 - 00:19:34.580, Speaker A: So generally we should be spending more time on making state machines faster. When you look at the comparison of parity against Geth, parity is about two and a half times faster right now, and just like being able to execute EVM transactions. But this leads me actually to another point. Currently, most projects are building around or on top of the EVM. And in my mind, this isn't a great way to build decentralized applications. Most applications do not need the full feature set of the EVM. If you want to build an exchange, there's no reason why you would need user level scriptability within your application.
00:19:34.580 - 00:20:35.526, Speaker A: The EVM is great if you want to write contracts like smart contracts, actually things that are very low volume and executed maybe once or twice, but isn't a consistent volume transaction throughput. And the other thing is if you need user scripting abilities at the application level. So if you want your users to write smart contracts that directly interact with your set of smart contracts, if your users never write smart contracts themselves, there is very little reason for you to build a smart contract for your users to interact with. You may as well build a specific application, like build an application specific state machine that is just for your use case. And this gets massively faster because instead of running a virtual machine, you compile everything down to. This is maybe another interesting point. Most people say like, oh but yeah, bitcoin isn'turing complete, but Ethereum is.
00:20:35.526 - 00:21:10.240, Speaker A: And so it allows me to build turing complete, it allows me to build any sort of application on top of it. That is true. But every blockchain you build, every single state machine you build is touring complete. Like if you write your own state machine, it is fully touring complete. You can build it in go, you can build it in rust, you can build it in Haskell, it doesn't matter, but you have the full creativity. You can use all your creativity to build an application specific state machine. And I'd like to talk about two examples that I'm currently helping build.
00:21:10.240 - 00:21:55.820, Speaker A: The first one is Ethermint, and ethermint is really it's the EVM, because the EVM has amazing use cases. I just think that most people are currently abusing those use cases. And ethermint is an implementation of the EVM put on top of tenement core. So it's an EVM that runs at about 200 transactions a second. So at about roughly 15 times the speed with the same block limit, block gas limit, it has very efficient, light client proofs. Like everything works as you'd expect it to work, except that it's fully proof of stake. Oh yeah, and the coolest feature about it is it implements the entire web3 spec.
00:21:55.820 - 00:22:43.346, Speaker A: So like all the existing tooling works, it's literally like you change a URL. The other more involved part of the stack is the Cosmos SDK. The SDK is actually how we build ethermin in the first place. The SDK is a framework to make building blockchains extremely easy. It's like you can write bitcoin in like 200 lines of code with it, the same way that Ethereum made it extremely easy to write smart contracts. The SDK makes it extremely easy for you to build your own blockchain. Finally, now we have the freedom, everyone has the freedom to just be able to essentially, in the future it will be that you essentially just install a bunch of modules, and these modules can.
00:22:43.346 - 00:23:50.190, Speaker A: So like, you know how NPM does it with JavaScript and the packages you do NPM install. And suddenly you have this whole bunch of functionality that you don't really understand how it works, but you understand the input and output it gives you. And it's essentially the same thing for the SDK. You plug in a bunch of modules that do things like staking, that give you an account structure, that give you governance, and you just focus on writing the tiny bit of application logic that makes you unique, instead of having to build up everything from scratch, right? Instead of having to write your own networking layer, your own consensus layer, and then your own application layer, plus your own tooling, like your own CLI tooling around that application, you can pretty much offload most of this to an existing library ecosystem and just write what makes your application, or just write your business logic. And for example, one of the cool features of the SDK is that you have an EVM module. So if you want user level scripting mobilities, you just plug in the EVM module. Of course, this doesn't.
00:23:50.190 - 00:24:36.080, Speaker A: There are other solutions like Zcasnax, which you heard a lot about this morning, and state channels. And so those are very valid, awesome things for their specific use case as well, and we're hoping to integrate them eventually as well. Great. Okay, coming back to the third one. Interchange scaling. Interchange scaling is all about the ability for heterogeneous blockchains. So blockchains with different application states, written in different programming languages, written with completely different protocols to be able to send tokens from to each other and back.
00:24:36.080 - 00:25:10.038, Speaker A: I can actually walk you through how it works from a conceptual level. In the beginning, it's an open protocol standard that we're developing, and everyone is very welcome to give us input on it and to collaborate on developing IBC standard. But in the beginning it allows you to move tokens. Then we'll extend it to non fungible assets. So I actually don't like to call them tokens because tokens in my mind have actually this notion of fungibility. So they're more like NFas. And lastly, complex objects.
00:25:10.038 - 00:26:17.054, Speaker A: And at complex objects, things get really interesting, because all of a sudden you can have a scenario in which blockchain A is like a loan blockchain, and people can collateralize their house or their assets on that blockchain. But now, once IBC supports complex objects, you can take that loan object from blockchain A and move it to blockchain A. And so the interest payments could happen on blockchain A, or the loans can be actively traded on decentralized exchanges. And whoever controls that loan object on another chain, once interest payments, for example, start failing, they can bring it back to the original chain and claim the underlying collateral. So once you have the ability to move complex objects, a lot of financial applications get really interesting here. And so conceptually, the way it works is that I mentioned before, with BFT consensus algorithms, you're able to build extremely efficient light clients. So all these chains are light clients to each other.
00:26:17.054 - 00:27:14.990, Speaker A: So let's say you have blockchain A and blockchain B, and blockchain B has an account that only the consensus of blockchain B can unlock on chain A. When I want to move tokens from chain A to chain B, I essentially lock my tokens on chain A to the consensus of chain B. And then I send this proof that, yes, your consensus, my destination chain consensus now controls these tokens, so it knows that it can then give me those tokens on its own chain because we can always go back. We can always say, oh, some other person wants to move those tokens back and then they go to blockchain B and say, hey, I want to move those tokens back. And blockchain B says, oh yeah, right. I have actually the exact equivalent amount of tokens locked. My consensus can spend those tokens.
00:27:14.990 - 00:28:00.430, Speaker A: And the way you do it is that each chain essentially keeps track of each other's validator sets, so it always knows who's supposed to be signing packets. And then we have a special transaction for it's called IRBC packet, which the consensus of one chain signs. And then some relayer relays that packet to chain B. And chain B already knows who's supposed to be signing it. So it can then authenticate that packet and see whether it's correctly signed. And if it's correctly signed, it knows that this is now final on the other chain and can release the so by the. So this is like high level overview of how IBC works.
00:28:00.430 - 00:28:53.610, Speaker A: We currently working on the specfruit and the implementation. It's a lot of fun to start thinking about how blockchains can have accounts on other blockchains. So if you're interested, let me know after. However, there's always this problem with legacy systems, right? Most blockchains right now aren't finality based chains, they're probabilistic finality. So that we have eventual finality, but we have to kind of follow the chain all the time to figure out which one's the latest state. With IBC plus adapters essentially means that we write some sort of finality threshold around a probabilistic blockchain. So in the case of Ethereum, we write an adapter that follows the Ethereum chain and follows the normal protocol, but after 100 blocks essentially signs a message saying I've seen like this block is now 100 blocks deep.
00:28:53.610 - 00:29:38.990, Speaker A: So I'm guaranteeing that it will never get reverted. And I previously used to call this peggy or peg zone, but really it's just an extension of IBC to non finality chains. Or we need some extra way to enforce eventual to guarantee finality even over non finality chains. So in the case of Ethereum, this literally works by saying we run the Ethereum protocol. All the validators of that chain run the Ethereum protocol. They all see some events and then they witness those events, and they only witness them after a certain amount of time, after which they believe that those will not get reverted. If they get reverted, they're punishable as normal proof of stake.
00:29:38.990 - 00:30:28.222, Speaker A: So they have an incentive to figure out what is the best finality threshold to use for Ethereum. And then after that step, it's just normal IBC, where we just send lightline proofs between chains, because now we have this finality guarantee again. And the last thing about interchange scaling is how the hub works. So it's a cosmos hub. It is essentially a liquidity aggregator, or it has two functions. You know how in Ethereum, Ethereum enforces this ability on the user level that users can't double spend each other? So I can't send tokens to you and you because the protocol enforces that. That doesn't happen.
00:30:28.222 - 00:31:42.486, Speaker A: Once you're in a scenario where you have many, many blockchains, there's no way for blockchains could double spend each other because there's no one enforcing this invariance, that you can't spend more tokens than you have. And that's really where the cosmos hub comes in, that it says the hub enforces the invariant, that if I'm blockchain A, I can't just create tokens for blockchain B out of thin air. Actually, the other thing is, it's a liquidity provider. So if you build an application and you need liquidity, you just connect it to the hub and all of a sudden you can tap into all the connected ecosystem. So instead of having to connect to every single chain in itself and figure out how to do secure connection against Ethereum, against bitcoin, against zcash, against Monero, you connect to the hub and the hub gives you access to this liquidity pool of all the other chains. And liquidity is a major problem if we move into a direction of multiple heterogeneous blockchains, yes. Okay.
00:31:42.486 - 00:32:17.250, Speaker A: Social scaling. So this is like super fast. This is like the addition that I made after ethcc because people brought up that point, which is like, it's fair. So we really have as a community, we have to figure out how we can grow the amount of people that understand how these protocols work. We have to learn how to write good readmes, we have to learn how to onboard new people. It's not a good idea that only 20 people in the world understand how these systems are built. It's a recipe for failure, essentially.
00:32:17.250 - 00:33:10.926, Speaker A: It also helps your adoption, the security of your code base. Yeah. So developer social scaling of development is, I think, a huge role that most projects kind of ignore. The other thing is governance, and this is probably the most crucial part. It's like we have seen repeatedly in the recent forks as well in some recent hacks, that we need a way to evolve the protocol or to even come to agreement over state changes we want to make in that protocol. So imagine a scenario in which we could have had governance to decide on what to do with the Dow. And by governance, I mean we don't have, let's say, 50 people in a room that can't decide what to do, but it's rather everyone gets an equal vote, or like the economic majority gets a vote.
00:33:10.926 - 00:34:24.190, Speaker A: So that in order to do protocol upgrades, you need to have more than two thirds of the economic stake behind you. Suddenly it becomes actually a feasible option to do long term evolution of these protocols. What currently happens is mostly you have this rapid development phase where you try out new ideas and then once you're live, you really start to slow down because it's hard to say who is the right now to make changes to that protocol. Is it the developers that push new code? Is it some other people? Is it the miners that decide? It's very unclear of who is responsible for this. And by social scaling, I mostly mean we need a long term ability to evolve protocols because otherwise you're always in this pickle where a new competitor will come up every five years and will have learned everything you did wrong and then upgrade, put it into their protocol, maybe fork your distribution and then run with the improved protocol. Whereas if you actually have the ability to evolve your protocol, you have a much better chance of actually changing the world. Okay, this is like, I think my last slide around.
00:34:24.190 - 00:35:05.290, Speaker A: So that was scaling the four different stages of scaling. So we can either scale at the consensus layer, we can scale the state machine layer, or we can scale at the interchange layer where we. Yeah, well, kind of like orthogonal to this, we should always think about social scaling. As for the different security models, and I think this is very important to realize, you can pick whichever security model suits you best. And a lot of applications have very different requirements for the security model. For example, the first one is sovereign. So like you're your own blockchain, you have to figure out how you do security yourself.
00:35:05.290 - 00:36:07.224, Speaker A: You have to get some sort of distribution of economic value that is willing to secure your blockchain in like a proof of stake way. It gives you the greatest flexibility. It's like a great model if you want to build a core level protocol, like an infrastructure, because it gives the stakeholders complete control over how to evolve the protocol. It's like if you build directly on the EVM, yes, you have the ability to evolve your specific smart contracts, but you have no control of, let's say the networking layer or the state machine or the EVM, the opcodes that you're running. If you are actually using sovereign security model, you're completely independent, you're completely your own community and you do whatever you decide is best. Interesting enough, I think one of the major things that should probably be sovereign are like DEXs, because they probably have very specific requirements that we haven't completely figured out yet. The next model is hosted.
00:36:07.224 - 00:37:01.550, Speaker A: So that's very similar to Polkadot. This is what Polkadot is doing, that I can write arbitrary application state within webassembly, but I can upload it and then I don't have to worry about how the security is determined. So a bunch of people just get, a bunch of validators get selected to be running my application at this state, then they run it, and if they actually fault, they're punishable on the polka dot chain, on the relay chain, I think it's called. And so this is a great model. I think this is like, if you don't want sovereignty and you don't want to have to deal with your own security, hosted is probably a pretty good idea. The last one is plasma. And plasma is a very loaded term, so a lot of people understand very different things about plasma in my mind.
00:37:01.550 - 00:38:48.210, Speaker A: And so plasma is this whole idea that you want to be able to delegate, that you make zero security assumptions on all the child chains and that all the security lies on the root chain. I think it's a great model for certain use cases, but I don't think it's a great model for building or for scaling the future, for scaling the future of blockchains. Because the main issue with plasma, around plasma is that if you trust your entire security to plasma, you might end up in a situation where a lot of people will want to, let's say one or two of the plasma chains start failing. And now everyone has to now think about whether they need to exit to the root chain. But the root chain, of course, has limited throughput. And so now I, as an unaffected plasma user, have to think, wait, if another chain fails, can my plasma operator start cheating me based on the fact that I can't get my exit transaction into the root. So you might see a bottom up collapse of the entire chain system due to the fact that you get market panic and everyone tries to go to the root chain, but you have limited throughput on the root chain and all of a sudden webplaza is excellent is as a security model around IBC, because it allows you to even if I move tokens to another able, even if that chain fails, I'm still able to submit the proof that I own tokens on that failed chain back to the second chain and be able to reclaim my tokens there.
00:38:48.210 - 00:40:01.690, Speaker A: I think applying plasma as like a general security model is risky because you might get catastrophic failures, catastrophic cascading failures. But plasma is great for multiple different blockchains to connect to each other and be able to deal with the failure cases of certain blockchains generally. What I really want to say with this slide though, is we shouldn't be building a future in which everyone uses exactly the same security model. Using exactly the same security model is probably not a good idea, and eventually that security model might be proven insecure and we get massive failures. We should always go for a diversified approach to most things, like not everyone should be building on the same software stack, not everyone should be using the same security model. I think as like a general rule of thumb, it's like if everyone is using the same security assumptions, it is very risky because that assumption might be wrong or might fail in unexpected cases, depending on your application. Those are kind of your options right now, but it depends on your application.
00:40:01.690 - 00:40:16.380, Speaker A: All right. I am very close to the end, actually. I fixed this slide. Okay. Yes. So this is a quick recap. So I hope you remember the three things I said you should remember.
00:40:16.380 - 00:40:56.116, Speaker A: The only things that are actually important to take away from this talk. Those are the same things again. So we talked about the different approaches to scaling. So we've consent to scaling. And ideally we want to use byzantine fault tolerant algorithms there because they give us massive amounts of benefits, they give us sufficient light clients, they give us much higher transaction throughput, they give us close to optimal block times. Then it's state machine scaling. So the ability to actually be building your own blockchain without a lot of hassle, and the fact that most people probably want to be building their own blockchain interchange scaling that can actually move tokens between chains.
00:40:56.116 - 00:41:27.140, Speaker A: So we don't all have to live on the same chain. But it's okay if you all live on a different chain because the chains can still talk to each other. We don't have to pick. This is the only thing we want to support because this is where the greatest amount of liquidity is. We can pick whatever software stack is best for us, whatever security stack is best for us, but we can still all talk to each other. And the last thing, social scaling around, like actually think about how you want to evolve your protocol. Then I talked about the know the available modes of security, so sovereign, hosted or plasma.
00:41:27.140 - 00:42:03.310, Speaker A: And lastly, scaling is really everyone's responsibility. So don't wait for someone else to solve your scaling needs. Think about it like Netflix. If Netflix is writing software that can only serve 100 users worldwide at the same time, they shouldn't be waiting and sitting around hoping that Amazon builds better servers. They really should be figuring out how to build applications that actually scale to the user base they're intending to have. It's everyone's responsibility. You can't just hope that someone magically solves scalability for you.
00:42:03.310 - 00:42:18.290, Speaker A: Yeah. Actively look for design patterns that allow you to build scalable. Dapps. Yeah, again, I'm very persistent on this. Yeah. So thank you very much. This was pretty much it.
