00:00:07.560 - 00:01:01.414, Speaker A: Hello, my name is Uma and I'm one of the co founders of Succinct. And today I'm going to be talking about the role of decentralized proving in the modular stack. So first I'm just going to start with a broad overview of what is the modular stack and then talk about how zero knowledge proofs fit into it. So the modular stack is kind of a new evolution of blockchain architectures. When Ethereum initially launched, you had the main base layer responsible for execution, settlement, consensus and data availability. When Ethereum pivoted to a roll up centric scaling roadmap, you can see that now execution is delegated to the rollups, and they have choices in what execution environment they want to use, and they have choices in how they settle to Ethereum. And then they leverage Ethereum for the rest of the pieces, including consensus and data availability.
00:01:01.414 - 00:01:53.354, Speaker A: And then newer blockchains like Celestia take this a step even further, where they even parcel out, for example, settlement. And then the base layer is only responsible for consensus and data availability. And the main benefit of this modular architecture is that each piece can be really specialized and have advantages from specialization on the component of the stack that they're focused on. So the modular stack of today actually has even more pieces than what I showed in the previous diagram. And so I'll go through them here. But from when a user sends a transaction to some sort of network, there's actually a bunch of different components that a lot of different teams that are even here are working on. So for example, the first part of the modular stack is your transaction gets sequenced and ordered, and there's a bunch of different companies focused on specialized networks for sequencing.
00:01:53.354 - 00:02:49.490, Speaker A: Then there is an execution layer where the block of transactions that gets bundled together in a block gets executed and then posted to a DA layer. And then finally in blue, I've highlighted all the parts that ZK is relevant for, which is the block getting proven, and then interoperability between these different execution layers. And finally settlement to base layer like Ethereum or maybe even bitcoin or celestia. Now one interesting thing to note is that right now, a lot of the stacks that I've highlighted in this diagram today are not actually using zero knowledge proofs. So for example, the op stack today is still a work in progress. They're working on fraud proofs, but today it's just secured by a multisig in most production contexts. And then the arbitrum stack is secured by these interactive fraud proofs in practice and polygon CDK is, I believe, one of the only roll up stacks using zero knowledge proofs in practice.
00:02:49.490 - 00:04:09.996, Speaker A: But a lot of chains that are deployed today use arbitrum or op stack. But obviously we're here at ZK summit. So I do think there's an interesting turning point around the corner for zero knowledge proofs in the modular stack that I'm going to talk about next. So historically, if you're a roll up team and you wanted to use zero knowledge proofs for interop and settlement, you'd have to have a team of cryptographers with a bunch of experience, and you'd have to spend years and years building out a ZKeVM or some other ZKVM. But recently from some work from the Sysync team and a few other teams in this space, including the Ris zero team, and also more recently, the last one jolt work from the Andreessen team have led to this, what I call the ZKVM revolution, where you have these very performant, general purpose ZK vms that let developers write normal rust code and then put that into their ZK rollup. So in particular, SP one, which I have on this slide here, kind of showed a world where instead of having this team of cryptographers and very advanced engineering, you can just have a few people on your team write a ZK implementation of your node in normal rust, and then it can still be very performant. And in particular, we have this novel precompile centric architecture that lets general purpose ZK you be really performant.
00:04:09.996 - 00:05:05.834, Speaker A: And we have some other innovations. And on the right hand side, I have a picture of some code in SP one that implements a tendermint lite client that shows in 40 lines of code, you can reuse an existing tendermint rust implementation and do no advanced cryptography, no specialized knowledge, and just have a tendermint lite client you're ready to use with zero knowledge proofs. And you can do a very similar thing for taking ref a rust EVM client and using it in SP one or some of these other zkvms and getting a very performant ZKe Vm. And so this rise of general purpose performance zk vms, I think, will really catalyze ZK adoption in the modular stack and make it an option to be used everywhere. So, again, with my previous point, I think every rollup will be a Zk rollup in the fullness of time using this maintainable rust code. It's just a lot more maintainable. It's a lot more customizable.
00:05:05.834 - 00:06:05.102, Speaker A: You can keep up to date with the latest changes in the EVM and the developer time is really unparalleled. And because of this, because of this new easiness to use ZK, I think basically it will become the best possible option for people in the modular stack. So today, if you're using something that is a trust has a lot of trust assumptions, obviously that doesn't scale very well. It might be really easy to set up, but it's very hard to reason about because there's a lot of hidden trust assumptions. And then optimistic and economic security systems are better on the trust assumption spectrum. But especially with optimistic systems, they can suffer from slow finality and sometimes they're kind of difficult to program and then in some ways they don't scale super well, because fundamentally, for example, for interoperability, you, you have to have a bunch of nodes in the system, rerun the same computations if you want faster finality. And zero knowledge proofs solve a lot of these problems.
00:06:05.102 - 00:07:14.214, Speaker A: And because it's becoming so easy to use and also performant, I think now is kind of like the turning point for how zero knowledge proofs will fully enable this modular stack to all talk to each other. So hopefully now I've convinced you that zero knowledge proofs will be an important part of how the modular stack scales and interoperates. And now I'll talk more about the shape of the proof supply chain, which is going into deeper detail on how zero knowledge proofs will actually be used by a roll up in the modular stack. So if I'm a roll up that's using zero knowledge proofs, how does the proof go from origination of a user transaction to then being used on chain? You have a bunch of different roll ups that are sending proof requests to some sort of either proving network or some other model for how proofs requests get fulfilled. Then you have a bunch of provers who are working on fulfilling these requests. There's also an optional proof aggregation step where these fulfilled proofs get aggregated into one bigger proof, because on chain verification on ethereum can be somewhat expensive. And then finally these proofs get verified on chain onto whatever settlement layer you want.
00:07:14.214 - 00:08:15.568, Speaker A: They can also be used in a p two p layer for faster interoperability, kind of like some of the ideas Justin was talking about earlier today. And they're also useful in other contexts, for example, even off chain, although our main focus today, especially the modular stack, is primarily for on chain usage. So the proof supply chain has these two kind of very simple steps, requesting proofs and fulfilling proofs. And one big question that I want to talk about here today is what is the final shape of the proof supply chain? So here I've kind of highlighted a straw man proof supply chain where you have a bunch of isolated SaaS companies building proprietary stacks. So in the request phase, basically you have each roll up team choose a different proof provider who can just be a SaaS company and maybe they're running different ZK vms. And then each roll up team will integrate with their own proof providers API. And then the proof provider will be responsible for generating the proof and landing it on chain.
00:08:15.568 - 00:09:00.924, Speaker A: So this is kind of a straw man simplistic model for how the proof supply chain could look. Now, one interesting thing with proofs is that they're self verifying. So you're not actually trusting your proof provider to provide you with something correct, because the proof, if it verifies on chain, is correct by nature of how zero knowledge proofs work. So there's no trust assumptions here, but there's kind of other reasons why this is less desirable than having an open protocol for the entire proof supply chain. So in the rest of the talk, I'll be talking about how the proof supply chain can actually be made better by an open protocol versus kind of the straw man world that I highlighted in my previous slide. And I'll kind of walk through the benefits, but at a high level. Let's start with what the open protocol looks like.
00:09:00.924 - 00:10:03.922, Speaker A: So in the open protocol version of the world, you have a bunch of different roll ups and they send a proof request to a pool of proof requests using some sort of standardized API or RPC or interface or whatever you want to call it. Then you have a bunch of different provers. Now note from the color scheme here, the provers can be run by different companies, run by different teams, in different data centers, use different hardware, and there is an RPC or API for kind of claiming the proofs and bidding on the proof pricing. And then finally, once the proofs have been fulfilled, there's a relay phase where they get relayed on chain. Now, I think the most compelling reason why an open protocol wins is that if you have this open protocol that's unified and provides this unified method for different roll ups, submitting proof requests, it results in two things. The first thing is that if you have this shared standard, then you can start enabling competitive price discovery amongst a diverse set of provers. Today, in zero knowledge proof world, price discovery is very difficult.
00:10:03.922 - 00:11:23.894, Speaker A: So you'll have different proof providers charging for different circuits and different use cases, very different numbers, and there's no unified place if you're a team using zero knowledge proofs to know how much your proof should cost. And at succinct we've even written many different proving services and we don't even know necessarily how to price our proof super well. And so having this unified standard for sending proof requests and then having this unified standard for how different provers across different companies can compete on price will result in better end pricing for roll up teams and eventually it will trickle down to their end users. I also think that it will provide a much more compelling developer experience for these roll up teams to integrate with a bunch of different provers in a world without a unified protocol. If you're a roll up team, you might have to go talk to five or ten different hardware providers or proverb companies and do this individual BD to figure out hey, should I integrate with your API and what's your price and stuff like that in this world with the unified protocol you just integrate once with this unified RPC for requesting proofs, and then on the other side you have this competitive auction built in for you kind of for free. And so the end developer experience for integration is you integrate once and you get this marketplace for free. So the overall developer experience is a lot better.
00:11:23.894 - 00:12:23.914, Speaker A: Another interesting thing about having kind of a network of provers and this open protocol is that the network aggregates demand across all these rollups. Even today we can see that different roll ups have very different activity periods and that activity can be pretty variable over time. And so if you have a unified protocol where you have a bunch of different requests being aggregated, you're aggregating demand across all the roll ups and hopefully smoothening that demand as well. And then each prover who's connected to the network has much better resource utilization because they can service the proofs on the network at that time. And if you have better resource utilization, you have better hardware utilization rates and then you can also start providing cheaper prices. And so the prover network benefits from economies of scale by aggregating demand and aggregating supply, and hopefully that results in better outcomes for either side. And then finally we are in crypto.
00:12:23.914 - 00:13:39.096, Speaker A: So we actually do really care about strong liveness guarantees and censorship resistance. And so if you have this unified protocol with permissionless participation of a set of provers, then you kind of get a lot of these guarantees for. So for example, you can have a bunch of geographically diverse provers participate in the network and you have much stronger liveness guarantees than these closed SaaS services because you have a diversity of companies, a diversity of hardware, data centers and stuff like that. And of course with all of that comes strong censorship resistance, which can be important for a lot of applications. So overall, I think the proof supply chain that the test of time will be coordinated on this open protocol, and hopefully the advantages of the decentralized open protocol will make the end cost of proofs cheaper and have the network be more resilient than an alternative universe where you have a walled garden of these siloed and proprietary proofing services. And hopefully the developer experience is also better for the end users of zero knowledge proofs. And I think what this open protocol will let us do is proving now can kind of be this glue between all the components of the modular stack.
00:13:39.096 - 00:14:38.708, Speaker A: So once the user transaction goes in, it gets sequenced, it gets executed, made available on some DA layer, then hopefully proving is kind of the glue that takes in those inputs and then allows all these different layers to talk to each other through interoperability and then gets settled as well. So now that I've hopefully convinced you that prover markets and an open protocol for proving is interesting, let's dive a little more into the mechanism design for actually how the prover market will work. So first we have to kind of define the goals of what our prover market wants to do. And so here I have my opinionated take on what the goals of an efficient prover market should be in rank order. So I think the foremost priority for all the customers we talk today for using zero knowledge proofs is cost. Zero knowledge proofs today can be very expensive and reducing the cost overhead is generally these teams number one priority. Next important is latency.
00:14:38.708 - 00:15:36.204, Speaker A: So low latency proof generation is very important for fast finality for rollups and bridging use cases, although we generally find that it lies second to cost. And then after that, liveness and censorship resistance are also very important to these end users of zero knowledge proofs. And one interesting thing to note is that in many long tail situations, liveness can be as important as safety. With zero knowledge proofs, generally because the proof is self verifying, you don't have to worry about safety or trust assumptions or things like that. But for example, if a proof system is not live and you're trying to withdraw your money out of a ZK roll up right now, and there's some weird long tail situation going on, liveness can actually be very, very important to an end user as well. And of course we always care about censorship resistance. So one model of the world for how mechanism design in how these proof requests get assigned to provers is proof mining and proof racing.
00:15:36.204 - 00:16:17.388, Speaker A: So this is kind of maybe the obvious thing that you would do. You just say when a proof request comes in. In the proof racing world, you have provers race to generate and fulfill proofs and the fastest prover just gets paid. Now, this approach has some undesirable qualities, which is provers are racing against each other. And so there's a lot of redundant work. And I think this makes this approach impractical when people care about cost, because the redundant work ends up leading to higher computational costs, which end up being passed to the end user. In proof mining world, it's similar to proof racing, but additionally provers have to mine a nonce with their proof to ensure that the fastest prover doesn't win.
00:16:17.388 - 00:17:00.404, Speaker A: So this helps a bit with decentralization where you're ensuring that the fastest person is not always winning. But again, it has similar problems to proof racing, where there's still a lot of wasted work. And so in a world where cost is the number one priority, I think proof mining and proof racing fundamentally won't work out. So given that we're not doing proof mining and proof racing, we need another way to figure out once a proof request comes in, who actually gets to generate the proof. And there's kind of two options in this world that have been discussed. One is a proof of stake style issuance model, and then another one is an auction model. And I actually posted a question to Twitter and got some really interesting discussion on that I think last week.
00:17:00.404 - 00:18:14.264, Speaker A: Ok, so in a POS issuance model world, basically you have a roll up, give a fixed, perhaps with some sort of bonding curve, a fixed amount of reward per ZK proof per block. And in an auction model, you basically run a competitive auction for each proof request. I think a proof of stake issuance model for ZK proof pricing works well when your provers aren't that sophisticated. So basically anyone can run approver and then the proof cost per block is relatively uniform, which is true in many of proof of stake networks, and it's pretty bad if you have an oligopoly of provers. One interesting thing to note about this proof of stake issuance model is when proofs get cheaper, the network doesn't necessarily benefit from the reduced prover costs because there's a fixed reward per proof. In an auction model, if you have very sophisticated provers who are engaged in this competitive auction per proof then usually you would expect that the prover sophistication is pretty high, and then the proof costs can actually be quite variable per proof, which I think is the world we live in. And you might end up with an oligopoly of very sophisticated actors.
00:18:14.264 - 00:18:56.416, Speaker A: But the upside is that when provers get more efficient, then the network benefits because they are engaging in a competitive market. And so the end prover costs are lower. So my prediction, and I think there's still a lot of research going on as to which model makes the most sense. But my prediction is that the end game ZK market structure actually looks like an auction model world. And it actually might look pretty similar to how blockbuilding on Ethereum looks today, where you have a bunch of sophisticated actors in the block building world. You have these very sophisticated searcher entities, and they're engaged in a competitive game to generate proofs with the lowest possible costs. And that is kind of my prediction for how things will play out.
00:18:56.416 - 00:19:42.674, Speaker A: But I know there's a bunch of people doing research on what types of auctions make sense, um, and auction mechanism design in general. And then finally, uh, I kind of want to talk about putting all these pieces together. So hopefully now I've kind of convinced you ZK zero knowledge proofs are important for the modular stack, and it's kind of the end game of how everything will interoperate. Then there will be this like open protocol and marketplace for how each the roll up communicates with approvers. And finally that there will be a competitive auction model for how these proofs get assigned and priced. And so that's kind of what we're building at succinct is this prover network with all those pieces. So I've rederived from first principles how we kind of came to our architecture, and I can talk in more specifics about what we're building.
00:19:42.674 - 00:20:28.158, Speaker A: So our prover network at a high level will support a variety of open source ZK vms and proof systems, including SP one. But it can also support, for example, other open source ZKVMs like Lasso and Joolt that came out and what other teams are working on. And it will provide this unified interface where developers can use these ZK VMs in production, hopefully with one API request and one click. And finally, it will have an auction mechanism that will enable this competitive price discovery amongst a diverse set of provers. So let's walk through each of those phases. So if you're a developer trying to use zero knowledge proofs in general with a ZK VM using Sysync's network. Your first step is you deploy your program.
00:20:28.158 - 00:21:25.416, Speaker A: So you write your program in normal rust code, and then you can just deploy it to the network by uploading your program spike code. Or for those of you who are more familiar with RISC five, an elf file, then if you're a roll up team that wants to request a proof, so say you've done your sequencing and your DA and execution, you've aggregated a bunch of transactions, and now you actually want to request a proof for that block. You'll just send a request to our network corresponding to your deployed program. And the idea here is that you can send a request for any open source DK VM, and we will also provide this unified payment interface. So you have one canonical way that you pay for proofs. Once the proof requests get sent to the network, then that's when the auction starts. So you have a bunch of different provers, and there's an API for claiming proof requests and submitting your bid for how much you're willing to generate the proof for.
00:21:25.416 - 00:22:13.254, Speaker A: And the auction mechanism will price the proofs. You'll have this sophisticated set of diverse provers, and the provers will need to be bonded to be accountable for once they are assigned a proof, not actually generating it instead of griefing the network. Finally, when the proof gets generated and fulfilled, then it can be relayed to whatever on chain environment you want to use, whether that's Ethereum, whether that's Solana, whether that's Celestia, wherever else you want to use the proof. Or you can also use it off chain. So this kind of takes all the pieces that I've been talking about throughout and puts them all together. So right now it's succinct. I think we're building our ZKVM SP one, but then the easiest way to use SP one will be our prover network, and this is under active development.
00:22:13.254 - 00:23:08.000, Speaker A: So if you're a developer wanting to use zero knowledge proofs, you can start by using our ZkVM SP one. And then once our network is live, that's when you can start deploying SP one programs and getting proofs generated in one click. If you're a prover that wants to actually generate SP one proofs or proofs for other ZK vms, you can reach out to us to become a development partner for our APIs that we're developing and start participating in these pricing auctions. And then if you're a ZK person building a proof system or a new ZK VM and you kind of want to leverage our unified interface for payment and unified interface for proof requests. Then you can also reach out to us to be added to our network's supported proof systems. So yeah, all of that's kind of active work we're doing and more details will be released soon. TM and yeah, so you can get in touch with us through this form or on my twitter.
00:23:08.000 - 00:23:20.184, Speaker A: And then also we're hiring for people who want to work on this kind of stuff. That's all I have here.
00:23:20.224 - 00:23:21.964, Speaker B: You can just take this microphone.
00:23:23.984 - 00:23:53.624, Speaker C: Hi everyone. Hiyuma. Thanks a lot. We are from bonus technology and we are very enthusiastic about the work you just presented, as we are also ourselves just recently starting on a very similar, let's say, project. I was just curious, have you guys did any kind of simulations for the economical viability? Because keywords that you mentioned is cheaper, more resilient, and of course that resonates with me personally. But if you could just elaborate a bit more from the perspective of simulations and the cost effectiveness of everything.
00:23:56.084 - 00:24:35.944, Speaker A: So I think we haven't decided. I think the actual auction mechanism is something that I think there's still a lot of very active research on. There's even a talk perhaps later today, or maybe it was earlier today on whether first price auction makes sense, second price whatever. So we haven't simulated that. I think it's most likely that we'll start with a version that's fairly opinionated, with a limited set of provers and a limited set of supported proof systems, and then over time generalize it. So, to answer your question in short, we haven't done much simulation. It'll start in a more closed setting, and then over time we'll take that feedback loop and use it to do the simulation when it makes sense.
00:24:36.244 - 00:24:37.424, Speaker C: Okay, thanks.
00:24:40.114 - 00:24:41.178, Speaker D: Amazing talk.
00:24:41.346 - 00:24:51.694, Speaker C: How do you think about sharding and scheduling the proof generation or the statement itself to a diverse network of participants?
00:24:53.154 - 00:25:33.396, Speaker A: Yeah, this is a good question. So a lot of ZK vms today, including SP one, basically take a really long program or statement and then break it up into sub statements and then combine them all together for one final proof that can be verified on Ethereum. There's two models of the world. One model is each subproof gets sent to the network as a separate request, and each aggregation step also gets sent as a separate request. There's another model where you basically just send, hey, I want a proof for this program. And then the prover does all of that aggregation and recursion themselves. I think either model could work with our system because it's pretty flexible.
00:25:33.396 - 00:25:58.874, Speaker A: Although I do think there are benefits to having a very pipelined sharding and aggregation that come from just having one prover do everything because you just get a lot of engineering benefits from having it in one data center and being able to talk to each other very fast. So yeah, I think either can work and it just depends on what people who are using the network want to do.
00:25:59.734 - 00:26:02.694, Speaker B: Okay, we had a few hands. The gentleman in the base shirt.
00:26:02.774 - 00:26:03.474, Speaker A: Yep.
00:26:04.174 - 00:26:05.502, Speaker B: You have to push the button.
00:26:05.638 - 00:26:22.794, Speaker D: Thanks, uma, that was great talk. Question, why do you think that in action model the price variance is going to be high? Because that's a little bit counterintuitive. Like when you do an auction, people will fuel money, they get kicked out of the pool pretty quickly and the market discovers the price. Thanks.
00:26:24.224 - 00:27:02.684, Speaker A: Yeah, so per Ethereum block, for example, depending on what is in the block, your amount of like the amount of computation in a ZK proof can be pretty different. So you could have something like gas where for example, the gas for an ethereum roll up is mapped to a ZK gas instead. And then it's like a more fixed cost and you have some EIP 1559 type mechanism that could work. But as it stands today, usually most roll ups, gas schedules don't align perfectly with the ZK proving costs. So today it looks pretty variable just because of that.
00:27:03.864 - 00:27:06.284, Speaker B: Okay, the white shirt.
00:27:12.624 - 00:27:40.394, Speaker E: So what do you think will be the dynamics of the demand side? Like you compared to Ethereum, but in Ethereum there is a very clear price pull, but in rollups there is no dynamic. So what do you think will be the reward? Like a bounty system that each roll up will put, or how do you think it will take place?
00:27:42.894 - 00:28:16.184, Speaker A: Yeah, I think the roll up, similar to how Ethereum delegates a lot of such roll ups and roll ups are free to experiment. Perhaps some roll ups the roll up team will just pay for all the proofs. Maybe in some other roll ups the cost will be passed to the end user in the form of like some part of your transaction fee is dedicated to proving. In Tyco, I think they're doing a base roll up where there's some like proof bounty. I think rollups can experiment with different models and then use like the unified API with whatever way that makes sense for them.
00:28:17.364 - 00:28:19.504, Speaker B: Okay, we have time for one more question.
00:28:20.204 - 00:28:21.504, Speaker A: There you go. Yep.
00:28:26.404 - 00:28:36.464, Speaker D: Hello. Hi oma, amazing talk. I just have one question. How do you ensure privacy of fitnesses in the case of decentralized proof of networks?
00:28:37.364 - 00:29:11.314, Speaker A: Yeah, it's a really good question. So for witness privacy, generally you want to generate your proof client side. But I do think there's still a huge role for decentralized proofer networks to play in, because client side, you generate a proof that's maybe not that succinct. So you have small prover time but high verifier time, and then you can use a prover network to basically take that proof and shrink it into something that can be verified on chain. Already a lot of teams do something similar to this, so I think the initial proof will be generated client side and then you dispatch to the prover network, and at that point you don't care about witness privacy.
00:29:13.254 - 00:29:14.494, Speaker B: Okay, thank you so much, uma.
