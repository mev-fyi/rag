00:00:03.520 - 00:00:16.276, Speaker A: Hello, everyone. Welcome to this week's edition of ZK Study Club. Today we're proud to host Zach Williamson of aztec protocol, and he's going to be talking to us about his recent work on Goblin plonk. So thank you very much for being here.
00:00:16.298 - 00:00:37.848, Speaker B: Zach, over to you. Cheers. Thanks, Alex. Hey, everyone. So yeah, Goblin plank. What the hell is Goblin plank? So this is some work that, ah, that we've been doing recently about how to do effect efficient recursive proof composition. There's been a lot of research that's come out lately about folding schemas using sum checks.
00:00:37.848 - 00:01:40.640, Speaker B: So this is adjacent to that, but I think it can be combined in very interesting ways, which perhaps we can discuss at the end. Hang on, how do I advance slides? There we go. So what's the problem? So, Goblin Hog is not a general purpose scheme for recursion. It makes a few assumptions that may or may not hold true for certain applications in the ZK trader space. So the idea is that you have a setup where you have a very resource constrained prover that's constructing zero notch proofs where one wants to preserve privacy, and this prover also wants to perform arbitrary depth recursion. But the expectation, the assumption is that there is also in this network a very computationally well resourced prover available, but they're not trusted, so you don't want to give them any secrets. And also the actual amount of the communications bandwidth between the two previous is quite limited effect.
00:01:40.640 - 00:02:31.776, Speaker B: For example, if you're say, building a privacy preserving layer two, and you have a resource constraint prover submitting proofs to a mempool, for example, that represents transactions to be aggregated, to be sequenced, then you have this model where basically client side, you got a very wimpy prover, you want to be able to recurse. Then once your transaction gets picked up by a sequencer by prover, you have a very resourceful, you have access to a lot of compute, but you can't trust it. So no privacy and the limited bandwidth between the two for the same reasons why Ethereum transactions on a p to pin network can't be too large. So I said we're insensitive. Final proof size. That's actually kind of contradicting what I just said. By insensitive, I mean like tens of kilobytes, not megabytes.
00:02:31.776 - 00:03:24.812, Speaker B: And the final verification costs don't matter because of this resourceful proverb. So yeah, recursion. So just to summarize briefly, the ZK snark recursion problem so if you're defining a snark over elliptic curves, then your circuits will be defined over some curve where the arithmetic of your gates is going to be modulo the number of points in the curve, which will be a prime subgroups. And so to verify a proof within a proof with one elliptic curve, you need to perform arithmetic in the field that the curve is defined over. And that's going to be different to the curve that the native modulus that your circuit arithmetic is performed over p does not equal q. This is the problem, and it's not good. So there are several solutions I'm just going to run through briefly.
00:03:24.812 - 00:04:10.684, Speaker B: Historically, the obvious one is a brute force. You use your native prime field arithmetic to emulate a binary arithmetic, and then you use your emulated binary arithmetic to emulate the prime field arithmetic that you really want to do. But layer on these two abstractions adds a lot of cost, so it's like about 300,000 constraints to verify simple proof within a proof under this model. So it works, but it's very expensive. It's like not good at all. So the next solution, and this is the current one that everyone's using over elliptic curves, the one that's really very elegant, is using halo two curve cycles. So you have two curves, a and b, where they form a cycle, because basically the fields are swapped.
00:04:10.684 - 00:04:58.460, Speaker B: So the coordinate field of one curve is the same as the native field modulus of the other curve, and vice versa. So you can't create these cycles with both of these curves. Pairing friendly, but that doesn't matter because of accumulation schemes that haleopilone did. And it means you can defer, like all the expensive non native elliptic curve operations you need to do to verify a proof within a proof, you can kind of defer them by kicking them into a circuit defined over your cycle curve, where they become native operations, which is very efficient. So this is kind of the best way of doing it so far. But it does have some drawbacks in practice. Your protocol logic is probably wants to be if you're encoding some kind of protocol, you want that to be over one curve, so you only have to deal with one prime field modulus.
00:04:58.460 - 00:05:14.224, Speaker B: And so you're going to use your cycle curve strictly for recursion, I. E. You're going to embed like a snark verify algorithm within your cycle curve. And that's the only circuit you're proving over your cycle curve. Which kind of means that to recurse once over your main curve, you really need to verify two proofs. One over your main curve, one over your cycle curve. It's kind of expensive.
00:05:14.224 - 00:05:49.836, Speaker B: That's lots of hashing to do. And also at each layer you still have a relatively chunky amount of non native field arithmetic to perform. You can delegate the group operations to another cl curve. When it comes to actually verifying the proof over your cycle curve, you have this intersection where you're going to have a snark proof over your cycle curve, and some public inputs of that snark proof will relate to claims you want to make in your main circuit. And so the claim is going to be defined over your native field modulus. The public inputs will be defined over your cycle curves. Native modulus there's non native field matched up there that you really can't avoid.
00:05:49.836 - 00:06:29.280, Speaker B: I'm not aware of any ways of avoiding it. It's better than the brute force method, a lot better, but it's still got some drawbacks. Curve cycles. And so this then leads on to goblin talk, which is very much an iteration. I see it as an iteration on halo two, which is the idea is, can we basically avoid doing this expensive curve bouncing step? And so one of the concepts, key concepts Goblin talk uses is this idea of an instruction machine. I think they're called state machines in other places. I very much pinched this idea from Jordy when I was chatting with him about ZKVM, so this is very much coming from him.
00:06:29.280 - 00:07:53.700, Speaker B: But the idea is that if you have some complex operation you want to do inside a circuit, instead of evaluating the arithmetic in your circuit, you cheat, basically, and you write the instruction you want to perform into a lookup table along with the claimed output, and then you carry on with your life inside your main circuit. And the idea with an instruction machine is you define an instruction set of nasty functions, basically, and whenever you're in your circuit, you need to evaluate one of these nasty functions. You don't. You just write an instruction inside your instruction machine going, here's the inputs to the nasty function, here's the claimed output, I'm going to move on with my life. And the idea there is that. The benefit of that is that you can take that lookup table and convert it into a transcript, multiple transcripts, where you have basically a commitments to the set of like you have commitments to all of the claims for specific instructions, and then you can construct bespoke snark circuits solely to prove the correctness of one instruction over and over and over again. This is very nice and efficient because you can create like a custom, very wide circuit feature of these instructions where you can define the optimal algebraic constraints specifically for an instruction, for example like ketchak or non native field arithmetic.
00:07:53.700 - 00:08:11.168, Speaker B: And generally wide circuits are going to be much more proverbial efficient than narrow circuits because you're doing less wire copying and also every degree of your polynomials, every constraint in a slack circuit has a constant cost associated with computing the opening proof, which goes away, I think.
00:08:11.254 - 00:08:40.184, Speaker A: Dara put in the chat. I'm commenting here for folks who are watching the video. So there's a link to a GitHub issue in the halo two repo, which I will put on the YouTube link, but I don't know. Darren, do you want to mention, you just want to give some context around that comment. If you can't talk, that's okay. Bottom line is, what he said is the best way we had to avoid native arithmetic or public inputs for halo two is in this issue anyway. So folks, I'll link this in the video.
00:08:40.222 - 00:09:04.348, Speaker B: I'm sorry to interrupt. Yeah, I'm not up to date with the latest and greatest in halo two. I'm aware that the noise is a problem. I'm not familiar with the latest on how to avoid it, so. Yeah, cool. So we have this idea of an instruction machine that was fermenting in our heads around. Basically, if you want to do complicated operations in your snark circuit, you don't.
00:09:04.348 - 00:09:32.440, Speaker B: You cheat. You put them in a lookup table, then you make it somebody else's problem. And then the idea is you can construct proofs of these instruction machines to prove all of the correctness of the transcripts. And so what we're thinking with this is, well, hang on a minute. We have a nasty function we need to do, and I'm stealing Ariel's nomenclature here. He was talking to me about proofs of nasty functions. So we have this nasty function what to do, which is non native elliptic curve arithmetic, particular scalar multiplications.
00:09:32.440 - 00:10:15.448, Speaker B: So why don't we just shove those in a lookup table and forget about them and try and find a way of solving this problem at some abstraction layer which is not within a recursive cycle of circuits. So one of the things we do here, another concept we need is this concept of transcript aggregation. So the idea is inside your snark circuit, you want to do elliptical scalar multiplications. You don't, you cheat. Instead you write them into a lookup table. So you can use something like pluckup or one of the stuff that oracle has been doing or whatever. There's loads of techniques, but the basic core idea here is that what you're doing is at the end.
00:10:15.448 - 00:11:07.960, Speaker B: The result of this operation is that you have columns in your circuit which describe the elliptical scalar multiplication operations that need to be performed in their claimed outputs, and that those columns are going to become basically directly translates to polynomial commitments. So you have basically commitments to transcripts at each recursion layer. And so the idea is you need some way of doing transcript aggregation, basically combining, aggregating multiple transcripts so that at each layer of recursion, you're just tracking one transcript. And so this is not a folding scheme, because in goblin plonk, this transcript, the size of transcript grows as you recurse. If you have, imagine you have two snark circuits. They're both needed to perform non native scalar multiplications. The result of doing the aggregation is just, you have a transcript polynomial, which is just the sum of the input transcripts.
00:11:07.960 - 00:12:20.600, Speaker B: So this isn't too much of a problem, because if one entry inside this lookup table is going to be an instruction to do a non native scalar multiplication, so there's not going to be many, very many of those per circuit. So this transcript is not going to grow particularly quickly. The degree of the transcript polynomials is going to be relatively small compared to the degree of the circuit size. And yeah, if you're using multilinear, if you're using subjects multilinear color commitments, this is some really, really nice ways of trivially aggregating transcripts I can get into in a bit. And so the final technique with goblin plunk is this concept of curve transposition, which is very much stolen from EIP 4844, where the idea is you can prove the equivalence of commitments to the same set of data over two different deliberate curves. And so this is very useful, because effectively, what I'm trying to get to here, what I'm trying to build up, is that in goblin plunk, what you want to end up, what you end up with is you're recursing repeatedly over and over again in some recursion stack. And at each layer of recursion, you're running a brute force verification algorithm.
00:12:20.600 - 00:13:11.260, Speaker B: However, whenever that algorithm comes to the point where it needs to do elliptical scalar multiplications, it doesn't do the multiplication. It shoves your instruction into a transcript. And then you combine those transcripts as you recurse. And so at the very end of your recursion stack, you're going to have this chunky polynomial commitment to your transcript that you then need to prove the correctness of. So how do you do this? Well, the trick here is that you commit to the same transcript over your cycle curve. You prove the equivalence of the two committed polynomials like the data within them, and then you construct a proof of correctness for your elliptical scale multiplications in a circuit defined over your cycle curve, where all of the operations are now native scalar multiplications, not non native. And so it reduces the complexity by about two orders of magnitude.
00:13:11.260 - 00:14:07.676, Speaker B: How do you do curve transposition? Right, I've just glossed over this. Well, okay, you have two commitments over different curves over different prime fields to the same information. Well, there's no magic bullet way of doing this, but there is a way that's pretty fast. So what you do is you just use a regular polynomial commitment scheme like IPA to open the commitment of your cycle curve at some random point. And then you construct a snark proof of your main curve, where the polynomial commitments form columns in your circuit. And the circuit is basically a bespoke circuit tailored to do non native prime field operations, where as a public input, you take in the random challenge you used over your cycle curve to open those polynomials. And you literally kind of, in your circuit, you iterate over the coefficients of your polynomial, and you construct the evaluation of that polynomial in the field of your cycle curve.
00:14:07.676 - 00:14:43.470, Speaker B: So you're using non native arithmetic, which is not particularly difficult to do. It requires for each polynomial, each degree of your polynomial, which in this case is going to be an instruction to do. An ellipticoscular mold requires one non native field multiplication, one non native field addition. And you can just combine that, you can just create one big chunky constraint that does that as a primitive operation. So that's a lot of information, big info dump. But this is to try and summarize it all. You want to do your main recursion of a curve a.
00:14:43.470 - 00:15:42.640, Speaker B: And so the scalar multiplication operations of a curve a you need to do when you're verifying, you delegate that to this imagined elliptic curve virtual machine via this instruction machine process, where you're shoving the operations into a transcript. And then at each recursion layer, you're aggregating the instruction machine transcript produced at the current layer of recursion with the previous aggregated transcript. And then at the very end of the recursion stack, you provide a proof of correctness for a transcript to the elliptic curve virtual machine. And this is going to be defined over your cycle curve. And then you use curve transposition techniques to validate that the transcript you shoved into your elliptic curve. VM proof is equivalent to the transcript that was built up in your recursion stack. So there's some funky sequencing you got to do here to make sure that your challenges are generated in the right way so that you can't do rewinding attacks.
00:15:42.640 - 00:16:25.500, Speaker B: But this is the overall architecture. It's not particularly simple or elegant, but I do believe it is very efficient. So to try and dive into this in a bit more detail at each level of recursion. So this red square is basically defined. The idea is that you're constructing a proof of a stark circuit and the blue squares is basically aggregation algorithm. So you're saying okay, as you're constructing proofs of proofs of proofs you want to run a recursive verifier on the previous proof. Technically it's not a verifier, it's an aggregator because it's not fully verifying the proof.
00:16:25.500 - 00:17:13.992, Speaker B: And each layer you're shoving your extensive operations into these transcript commits which are part of the proof. And then once you've finished with your recursion you have this final proof and within that proof you're going to have commitments to your transcript polynomials. And so you run this instruction machine circuit which basically takes as an input the transcript polynomials and what and the kind of the. Sorry, let me, I'm doing things out of order. At the very end of your recursion stack. The next thing you do is you make this ECC VM proof. So the idea is the proven knows, okay, they know what's in the transcript.
00:17:13.992 - 00:18:16.204, Speaker B: They commit to the same information over your cycle curve. So obviously you need to make sure that there's going to be some range checks involved here to make sure that you're not overflowing the modulus in both contexts where you prove the correctness of a bunch of elliptical scale mls over the cycle curve. So it's all nice, it's all native, it's all cheap. And then as part of making that proof, the proof will have to open the transcript polynomials committed over the cycle curve at around a point. So we're going to reuse that and we're going to then feed that information into the, into the instruction machine along with the original transcript. And what the instruction machine is going to do is it's going to produce instructions for a non native fields virtual machine where the instructions are going to be what the non native field operations that need to be performed to evaluate the coefficients of the transcript as if they were coefficients committed over the cycle curve. And then those instructions get fed into a non native field, virtual machine.
00:18:16.204 - 00:18:48.680, Speaker B: You get a proof out and then, boom. At the very end of this, you have four proofs to prove instead of one. But the idea is that basically it's a way of minimizing the amount of work that this prover is doing. Because the idea is that you have these four proofs, they're succinct. You can kick them off to some third party aggregation service to combine, and you don't particularly care about the costs of verifying these four proofs as long as they're succinct. So it's not too painful. Succinct ish.
00:18:48.680 - 00:18:57.550, Speaker B: Anyway, should I stop on this for questions? Because I know I've just gone through a lot of info here.
00:18:58.740 - 00:19:23.956, Speaker A: Yeah, thanks. This is great. Does anyone have questions? So Dara is putting in some comments. We cannot hear you, Dara. So if you've been trying to talk, then, yeah, there may be an issue with your mic. We cannot hear you. You want to type your question? Okay, so Dara's going to rejoin and come back.
00:19:23.956 - 00:19:28.520, Speaker A: Does anyone else, while there is doing that, does anyone else have questions for Zach?
00:19:38.360 - 00:19:39.430, Speaker B: Maybe not.
00:19:40.120 - 00:19:44.984, Speaker A: Okay, yeah, everyone may be still processing this slide. There's a lot of information on here.
00:19:45.182 - 00:19:53.524, Speaker B: Yeah. Quick question. What is labeled as, like, PI, the honk proof?
00:19:53.572 - 00:19:55.656, Speaker A: Does that include the transcript from all.
00:19:55.678 - 00:20:46.920, Speaker B: The instruction machine calls? Okay, so it includes the latest transcript. So the idea is that I'm going to get into this in a future slide, but the idea is each proof contains basically the old transcript and the new transcript. And you can very easily, if you're using multilinear stuff, you can very easily derive the current transcript from the old transcript and the new transcript. And in doing so, you implicitly are also proving the corrects of the aggregation that the old transcript plus the current transcript equals the new transcript. So the alpha proof doesn't contain all of the transcripts of the previous proofs. Well, it contains two transcripts, like the transcript at the previous layer and the transcript at the current layer. And the transcript at the current layer contains the aggregation of the sum of all the instructions that have been added as the proof has been recursing.
00:20:46.920 - 00:20:55.340, Speaker B: Daira, how is your mic? No, we can't hear you. Sorry.
00:20:55.490 - 00:21:03.650, Speaker A: If you're talking, dara, we can't hear you. Srinath just asked a question. Do transcripts grow in size as more steps are executed? I believe you said.
00:21:05.540 - 00:21:53.170, Speaker B: So. I'm making a very implicit assumption here, which is that every entry into a transcript describes an expensive operation to perform and therefore the transcript will grow at each recursion layer. But the idea here is that the overall size of the transcript is not going to be much, much smaller than the degrees of the polynomials that you're committing to for your circuit. Because to give an example, to do a non native scaler multiplication between like four and 20,000 constraints and that becomes one degree of your transcript polynomial. So it does grow. But as long as each instruction represents something expensive, the assumption is that's not going to be a problem.
00:21:53.620 - 00:21:54.930, Speaker C: Can you hear me now?
00:21:55.700 - 00:21:56.496, Speaker B: Yes.
00:21:56.678 - 00:22:06.996, Speaker C: Excellent. Okay, so these four proofs that you get out at the end, what kind of proof are they? How big are they, how expensive are they?
00:22:07.018 - 00:23:03.850, Speaker B: To verify, I've defined scribe some explicit curves here. So the way we're planning on building this is using a half pairing friendly cycle where the red proofs would be fully succinct and one would have seemed very cheap to verify. Like these virtual machine circuits, the assumption is they'll be quite wide for maximal proof of efficiency. So like 70 ish columns, so not trivial to compute to verify. But you're talking like 120 or something scalar moles, possibly for each proof. And then the cycle curve proof will be more expensive because the assumption is the commitment scheme will be IPA and therefore there'll be this linear time step you need to do where it'll be linear and the degree of the VM circuit. The efficiency of the elliptical VM circuit matters a great deal.
00:23:03.850 - 00:23:30.720, Speaker B: The current kind of like architecture we have, at least on paper we haven't built it yet, can do one variable base native scalar modification in eight rows and about 70 columns. So the assumption is that the degree of this build a curve proof won't be that large and can be stomached by whatever verifier is going to be consuming this proof.
00:23:32.340 - 00:23:48.150, Speaker C: So the red proofs here, if you're using the half pairing cycle thing, the red proofs are KZG proofs on the ECC VM. This is an IPA proof. Why that way around?
00:23:49.080 - 00:24:25.410, Speaker B: So it doesn't have to be that way around. The reason we would do that way around is because KCG is much cheaper for the prover to compute an opening proof for than IPA. So it's just going to be, if one is ruthlessly focusing on optimizing approver efficiency for the person constructing this goblin stuff, you'll want your main recursive sec, your main circuits to be over your cheaper curve, which will be the KCG curve. But yeah, there's no fundamental reason you could flip the curves around. I see.
00:24:26.420 - 00:24:38.464, Speaker C: Or you could do one step of a halo recursion to move the pink proof to be a KCG proof.
00:24:38.592 - 00:25:10.370, Speaker B: Yeah, you don't need a half paying friendly cycle. I'm assuming this abstraction would also work for if you're using pastor and vella curves, because I saw your comment where you're saying it's similar to what your guys folks are doing with halo two, but like deferring the other characteristic. Yeah, exactly. I mean, I was looking at halo two and thinking like, how can I avoid verifying this cycle curve proof at every layer? Because I was way too lazy for that.
00:25:11.060 - 00:25:30.404, Speaker C: Yeah, unfortunately, it's too complicated for me to explain. If you look at that ticket, then there's a diagram which shows, it's a data flow diagram showing where everything goes. But to try and explain it in a video, I don't know how I would do that. Sorry.
00:25:30.602 - 00:26:05.276, Speaker B: Fair enough. Yeah, I can't look at the ticket because I'm sharing the screen, but I will look at afterwards and pick your brain on it. But there's also an additional consideration because we're building this honk proving system which is very much like hyperplanc where it's using sum checks, which is really nice for the proverb. Very very efficient, very fast, no ffts. But one of the downsides is that the verifier has got a logarithmic number of hashes to do. So actually verifying proofs is not like the grunt work that the verifier is doing outside of the non native authentic. It's not completely trivial anymore.
00:26:05.276 - 00:26:22.180, Speaker B: And so, yeah, so the idea of having to do verify each day of recursion, verifying another proof over the cycle curve, plus the non native field of authentic was like not good. So we were thinking about, yeah, let's just not bother, let's be lazy.
00:26:22.600 - 00:26:31.000, Speaker A: But just to put it back in the context, you're assuming the verify here is like a roll up server, right? That theoretically has some stronger abilities.
00:26:31.500 - 00:26:44.510, Speaker B: Exactly. If you send all these four proofs to a smart contract, then that smart contract is going to cry and it'll cost you hundreds of dollars in gas fees because of the IPA verifier. So there is that assumption. Cool.
00:26:46.100 - 00:27:09.270, Speaker C: So I guess you can have some kind of hybrid where periodically you compress your application proofs, but you're not doing it on every step as you would in the original halo paper.
00:27:09.800 - 00:27:56.340, Speaker B: Yeah, exactly. Because this is not, the construction is not really succinct because the transcript grows as you recurse. So yeah, as you said, you can just fix the recursion depth to some hard limit and basically then fix the recursion to a hard limit and take that entire protocol, then just put it in a box and say that this is now your recursion perimeter. Where the idea is instead of doing halo two recursion at each recursive cycle, you do goblin recursion for like 100 cycles and then you take all these four proofs and then you recurse those using halo two and then you have a generic accumulation scheme that doesn't have any linear growing term.
00:27:56.840 - 00:28:05.450, Speaker C: Yeah, eventually we're going to work out what the most efficient way to do this is. But everything's all up in the air at the moment.
00:28:05.820 - 00:28:18.060, Speaker B: Yeah, I think another useful application of this is folding. So foldings have become quite popular recently and there's been lots of really cool stuff done with them with like sangria nova, supernova, hypernova.
00:28:20.640 - 00:28:37.232, Speaker A: I'm sorry, just because he's here. Srinath author of I think. Hello Srnath, hopefully you can know, maybe you have some comments at some point about comparing and contrasting or commenting on the applicability of those schemes there. But anyway, I didn't mean to interrupt you, I just want to call that Sirnath is here and he presented I think two weeks ago.
00:28:37.286 - 00:29:30.930, Speaker B: Supernova. Yes. I mean the thing I'm thinking about here is that if you're using something like hypernova where you need to do one, I believe like if you want to use hypertensive for IBC you need one non native ellipse scalar multiplication for each fold, which isn't very much work, but zero work is better. So instead of actually doing that you just use goblin plunk. So each time in each fold you just shove that non native operation into a lookup table and then after say 100 or so folds, maybe you, or at some fixed constant number of folds you then do this transition step to prove the correctness of the scalar moles. And then you go again and it takes an already relatively low cost of holding and basically makes it disappear I think.
00:29:31.860 - 00:29:44.132, Speaker C: Yeah, I just noticed you've got on the slide the sizes of all of these intermediate things in terms of group and field elements. That's interesting. They're smaller than I would have expected.
00:29:44.276 - 00:30:27.460, Speaker B: Well take them with a grain of salt because not implemented them in code yet. So could be making some optimistic, could have missed. Well maybe we should move on then. So in terms of costs. Yeah, so the napkin math here I was thinking each non native scalar multiplication instruction becomes reduces to doing one native scalar mole in the ECC VM and five non native field operations in the non native field VM. And why five? It's because one scalar mole instruction in the VM needs five columns. You have four for your x and y coordinates because you need to split them up into one to eight bit chunks and then you have one and column for your scalar multiplier.
00:30:27.460 - 00:31:21.460, Speaker B: So that's five. But it depends how you slice it. If instead of the VM instruction being do a non native scanner mole if the NVM instruction is do 15 moles required to verify proof or whatever then that would get reduced. But these are like the rough napkin math for plonkish constraints. I reckon that the goblin plonk costs are probably about like 8000 plonkish by plonkish. I'm saying I'm imagining width for ultraplunk because that's the one thing I'm most familiar with. So my claim, which I've not yet proven in code is that the costs of doing one layer of encore ultraplunk recursion reduces to about eight ish thousand ish plonkish constraints plus the cost of the rest of the verifier algorithm.
00:31:21.460 - 00:32:03.824, Speaker B: I think we can be better than that. My hope is all in the cost of one a of recursion. Getting it down to about 8192 plonkish constraints, that's my goal at least. But until I've actually shown that sounds feasible. Yeah, and with this, that basically makes recursion just like a disposable primitive. You can just throw in everything because it's so cheap, which would be pretty cool. I don't know how we're doing for time because I have a few slides for the details for the VM designs, et cetera, but they're somewhat tangential.
00:32:03.824 - 00:32:04.724, Speaker B: We're good.
00:32:04.922 - 00:32:15.050, Speaker A: Well, just so you know, we typically go up to 90 minutes so there's still an hour left in the session. So I think we can proceed. We got plenty of time.
00:32:15.660 - 00:32:17.000, Speaker C: I'm fascinated.
00:32:17.820 - 00:33:03.872, Speaker B: All right, so this instruction machine stuff. So technically, for what I've described, the goblin plunk you don't really need this concept of an instruction machine because you only have one instruction which is do an electric curve scalar multiplication. The reason why this instruction machine abstraction I think is useful is because if you're doing goblin plonk you might as well build an instruction machine into your protocol architecture and use it for more than just non elliptic curve operations. For example, you could have an instruction machine to do care check rounds shah two by six rounds, big integer arithmetic, et cetera, et cetera. So I figured I might as well just describe a little bit how it works. So again, there are some kind of explicit details here which are like, you could generalize this a lot. So I'm saying the transcript has four columns.
00:33:03.872 - 00:33:55.928, Speaker B: That's because we're planning on building it, adding it into a width for tronkesh proving system. So if you had more columns, you probably want your transcript to be wider as well. So the idea is the width of the transcript is going to be the same as the width of your mate of the circuit that you're making a proofseader, where the. What did I say? M describes the transcript size. Okay, one convention we could do is how do you actually get instructions into the transcript? Well, the simplest thing you can do is you just have a convention where the transcripts, the constraints where you're writing into your transcript, you put those at the very start of your circuit, and then you just have a selector polynomial which copies them across. And then if you need those, the transcripts outputs elsewhere in your circuit, you copy constraints. So that's the simplest way of doing it.
00:33:55.928 - 00:34:25.170, Speaker B: You don't even need permutations or lookups. And so this is the idea of an opcode. So again, I'm using, assuming that you have some elliptical morphism to break up a 256 bit scalar mole into 128 bit chunks. That's what lambda here is. So the idea is the extraction will be you have these two one two bit scalars, z one plus lambda. Z two times p, add it into an accumulator a, that will be the opcode. And.
00:34:25.170 - 00:35:36.250, Speaker B: Yeah, so here, this is like a weird work example that I'm even, I'm struggling to understand. So the idea is that as an example of what one could do is the first column here is the opcode value, and then the second and third columns are like, define your x coordinate, the fourth, and the y four, and y two describe your y coordinates, and y three and y four. It describes z one and z two. And so the idea here is that your disillusioned curve instruction would exist in your transcript over two rows. So I think what I was trying to, trying to get at here is that if you have a very complex instruction for your instruction machine that requires more entries than you have room for in your transcript, you can just pack that instruction across multiple rows. And so here's just a little bit of an aside for if you're using a multilinear proving system, how do you aggregate transcripts, because it's really nice and simple. So if you're using a multilinear proving system like hyperplanck or honk, which is still not yet published, but we'll see.
00:35:36.250 - 00:36:34.430, Speaker B: The idea is that your subject requires multilinear polynomials, but multilinear polynomial commitment schemes are not the best. They're very painful. But there's a very nice technique that was applied in the Gemini paper where basically you have a transformation there where you have a protocol which transforms claims about the evaluations of multilinear polynomials to claims about evaluations of univariate polynomials, and you actually commit to your polynomial. So in univariate form via univariate polynomial equipment scheme, where typically the representation is just that, the coefficients of a univaric polynomial are the kind of the evaluate are the vertices of the Boolean hypercube that you are multilinearizing. Just as an aside, this is a really nice way of doing things because. Hello? Was somebody saying something?
00:36:36.400 - 00:36:43.272, Speaker A: I heard that too, but there may have just been a mic misfire. So yeah, keep on rolling.
00:36:43.416 - 00:38:02.688, Speaker B: One of the nice things about these transformation layers is it means that getting shifts is very, very nice. So for plonkish systems, it's very helpful to get to have a cheap way of getting commitments to your shifted input vectors, witness vectors. So if you're representing your polynomials in invariate form, you can get the shift trivially by just multiplying your polynomials by x, which is easy to do. The idea is you have some unit univariatization map which maps univariate polynomials to multilinear polynomials, where your coefficients of your univariate polynomials represent the value the vertices of Boolean hyperkey. Why does this make it nice to aggregate transcripts? Because you literally just kind of just combine them. So the idea is that, let's say at some layer of recursion, you have equipment to the transcript of the previous layer, white, old, and you have equipment to the transcript at the new layer, white, new. And the idea is that you can take the difference between these two to extract the transcript at the current layer, which is kind of pretty easy to do because you literally, just because I'm assuming you've got an additively homomorphic commitment scheme here, you just subtract the old transcript from the new transcripts and that gets you the coefficients that you want.
00:38:02.688 - 00:38:54.810, Speaker B: It's just that they're at the wrong monomial powers. So you just downshift by one over x to the m, where m is the number of entries added in at the current transcript layer. So I'm making some assumptions. I'm making an assumption that the number of entries added into the transcript is known, is part of the verification key, so it doesn't vary for a given second. And then you can just use some simple consistency checks. So you basically say, okay, you derive this transcript polynomial yx by taking the difference between the new and the old transcripts and then just downshifting by x to the m. And then you do some consistency checks where you just check that this output polynomial for all of the degrees that are higher than m.
00:38:54.810 - 00:39:34.832, Speaker B: There's no data that you have, everything is zero. And similarly you do this copy check again, basically where you're saying that for the first mros of your circuit wi are the y equipments to your circuit for the first Mrs. Your transcript polynomial. And your. Yep, polynomials are equivalent because you're writing instructions to your transcript. So I can go into this in more detail, but this is kind of just a bit of implementation detail. It's making a lot of assumptions about the proving system that's being used, which is kind of independent of goblin blockets as a scheme.
00:39:34.832 - 00:40:25.568, Speaker B: So yeah, this stuff is only useful for multilinear sumcheki systems. Vms. I mentioned before that we have the concept of like an electric curve virtual machine and an automated field virtual machine. These are very primitive vms. The only reason they're vms is because I'm making an assumption here that when you're recursing in goblin plank, the circuits that you're recursing over are not uniform, as in they're different. And by extension, the verification algorithms will be different, which that means that by the time you've finished doing a bunch of recursions, you don't know the actual arithmetic that you need to perform in your transcript is not going to be known ahead of time. If it is, then you don't need a Vm.
00:40:25.568 - 00:41:23.800, Speaker B: But to give an example of why this may not be the case, imagine you're recursing and you're verifying multiple hyperplanc proofs or honk proofs, because the verifier algorithm, the verifier has to do with scalar multiplications. That's a logarithmic in the circuit size. So if the circuit size of each circuit in your recursion stack is different, then you get an easy different operation. Similarly, you may have different aspections at different layers with different polynomial identities, which means you may have more wire commitments, more advice wires, et cetera. So the assumption I'm making here is that at the end of your code is that when you have the scram scripts, the verifier doesn't know precisely what operations need to be performed. So that's why you have a VM, so that you can kind of build up the instructions over time. And they can be actually just part of the witness statement.
00:41:23.800 - 00:41:54.050, Speaker B: But these vms are very primitive. They have an accumulator that you can add stuff into and you can like, well, here's an example instruction step for ECC VM. So the idea is like you can set the accumulator to be the result of multiplication, you can add a multiplication into the accumulator. You can do an assertion that accumulator equals some value. So basically like super, super primitive operations. So they're barely virtual machines. Certainly you don't have memory or registers or anything like that.
00:41:54.050 - 00:42:59.540, Speaker B: Similarly, for the non need to field virtual machine, this is an example that can be expanded. But the idea here is that the idea is to evaluate somewhat general arithmetic statement that can be used to represent a large set of non field operations. So this may not actually, I think we're probably actually going to change this by the time we have implemented it. But the idea is that you can do things like add and multiply inside the n at the non field machine, or you can do like basic linear operations and similarly the modulus. The idea is that the actual modulus of the operation you're performing, the VM, is not fixed, because if that's the case, then you get some other nice stuff for free. It means that you can get big into derivative for free, which is kind of useful for other use cases. It means you can do just arithmetic over arbitrary 256 bit moduli, so you can do second gap k one, as well as stuff over your cycle curve and your main curve.
00:42:59.540 - 00:44:00.596, Speaker B: You basically want one row in your noni field version machine to just do some somewhat general arithmetic statement that that covers a large amount of operations. Done a bit of brainstorming about how to do UCCBM. Not yet built it because been kind of busy with other stuff, but our team is planning on actually doing a spike on this soon. So the idea for the uptick virtual machine is you can just do the, if you want, like what other algorithm you're going to implement, because you could just do the normal double and add for doing ECC multiplication. But that's kind of slow, it's kind of boring, and with lookup tables you can do much better. So you can use. Basically my idea is my thoughts are take the best algorithm that exists in the real world and just put it in a circuit using lookup tables where you use this kind of like four bit sliding window non adjacent form method.
00:44:00.596 - 00:45:11.650, Speaker B: You basically take your input scalar that you scalar multiplier and you split it up into four bit slices. But each slice isn't zero to six to 15, it's actually -15 to 15 where the values are od, so -15 -13 minus eleven, et cetera. That's efficient because it means you get, it's still four bits of information, but because inversion negations are basically free for elliptic curves, you would initially need three bits of data in your lookup tables, which is nice. So the idea is with this four array sliding window method. So the thoughts are that to make such a VM circuit, you will have three distinct blocks in your circuit like sets of columns. Your first sets of columns will be all transcript validation, where basically you're iterating through the transcript row by row by row, and you're doing all the consistency checks equivalent, like all of the basic logic, everything except the scalar multiplication. But you're just making sure that if there's a claimed output, it gets added into the cumulator correctly, that the operation, and you're writing stuff to look up tables depending on what the operation is saying.
00:45:11.650 - 00:46:01.680, Speaker B: The second block of columns in your circuit is for scalar decompositions and point table computations. So the idea is you're taking each input scalar multiplier and you're decomposing into this four bit non adjacent form version, verifying that the decomposition is correct. And then you're writing those decomposed values into a lookup table. So you're saying for a given point index and a given round of your scalar multiplication algorithm, you're shoving the value of your sliding window scalar into a lookup table. Then also in this block of columns, you're doing the point table computation. So if your sliding window represents the points p three p five p up to eleven p up to 15 p, then you need to compute these and put them in a lookup table. So that's what the point computation block does.
00:46:01.680 - 00:47:19.930, Speaker B: Basically, the transcript will basically write a value into the section of your circuit, write a point in, and then you'll double a point to compute two p, and then you'll compute three p, five p seven p, et cetera. And you'll put them all in a lookup table that's associated that maps to the various sliding window forms. So those are the first two blocks. Transcript validation and then scalar scaling, decomposition, point table computation. Third block of columns is the actual scalar multiplication algorithm, where basically, my thoughts are, you have enough columns to perform four point additions, and what you're doing is you're basically iterating over your points, pulling out those sliding window values from your lookup tables you've computed, then using those to pull out values from your pre computed points table, again using a lookup protocol, and then you're adding them into an accumulator, and then you're also doing some tracking to make sure that once you've like for a given round, once you've iterated through your point through for all your points, you then need to double the accumulator four times, and then you increase the round counter and you go again. So there's a lot of logic to perform, and it's somewhat tricky to concode it all in polynomial arithmetic, but it can be done, I'm fairly confident of that at least.
00:47:22.060 - 00:47:51.220, Speaker C: So I was going to ask the most efficient algorithms, if you allow variable time algorithms for multiscal modification outside a circuit, are things like Pepinga and Boscosta that involve putting points into buckets. I'd always assumed that those were going to be far too complicated to do in a circuit.
00:47:55.320 - 00:48:33.628, Speaker B: Well, I think that pippidger is actually possible to encode in a circuit. I mean, it's a bit mental, what I'm describing. I think it's sometimes referred to as the Strauss algorithm, which it's not asymptotically optimal, but if you're doing a relatively small multi scale multiplication, so like less than two, five, six points, I believe it gives you better performance in Pippinger because the constants are better. Right, okay. But yeah, it's so, yeah, yeah. But it's just kind of, I guess the devil's in the detail. This is all just paper designs.
00:48:33.628 - 00:48:44.180, Speaker B: I'm making some claims here. I'm making a claim that it's possible to implement Strauss in polynomial algebra by making a claim that it's possible to implement in polynomial algebra. We can look up tables, but yet to be proven.
00:48:46.360 - 00:48:55.640, Speaker C: We know it's possible because the arithmetization is complete. Can you do it without making mistakes?
00:48:59.020 - 00:49:11.500, Speaker B: Not the first time around, but my plan is basically implement the algorithm, put on GitHub, brag on Twitter that it's perfect and it has no flaws, and then I'll get a bunch of reply guys to find all the flaws.
00:49:11.920 - 00:49:13.368, Speaker C: And reply girls.
00:49:13.544 - 00:49:15.224, Speaker B: And reply girls, my apologies.
00:49:15.272 - 00:49:16.690, Speaker C: Reply nonbinary people.
00:49:18.580 - 00:50:01.784, Speaker B: I want all the angry reply people absolutely, by the way. Oh yeah. This isn't 49 columns, it's 70 columns at least, but 70 columns, eight rows. What really matters is the overall area of your circuit, and that area is very tiny. To do a variable based gillimal so beats the multiple tens of thousands you require for non native scalar multiplications. Yeah, so that's the rough idea for the Kerv version machine. The non field version machine, I think is less unknown because I think that the EVM ZKVM folks have done one or two already.
00:50:01.784 - 00:51:25.456, Speaker B: But the way I was thinking about doing it is, the idea is you have two sets of columns. The first set basically evaluates all of the non native algebra, like the algebra for a non native field operation as defined by that weird formula in the earlier slide. And then you have a second set of columns that do the range checks. So the idea is the columns that do the algebra for each non interfield element that they've got, they shove it in a lookup table, say, oh, you do a range check on this, and then you do them in a separate section of your circuit. The reason why you would want to do that is because that way you can get rid of duplicates, so that when you're doing your range checks, you're only range checking unique values and not duplicate values, which will reduce the complexity of the circuit, reduce the amount of work you're doing, pluckup or another lookup table protocol to avoid duplicate range checks. So I'm assuming these column numbers assume that you can do like degree 17 polynomial expressions in your main plunk polynomial entity, which, if you're using ultraplunk or like a plonkish star system that doesn't use sum checks is insane because you need crazy ffts. But I'm hoping that the hope is that if you're using subject protocols because you don't do ffts, it actually might be tolerable to have high degree expressions.
00:51:25.456 - 00:52:42.380, Speaker B: But if that's not the possible, then it just means you need a few more columns so it's not the end of the world. And then you have the transposition circuit. So this is the bit that's kind of complicated. So the idea is, for the elliptical curve virtual machine, the way you want to structure your transcript inputs is going to be in a way that's optimal for the vm, which means you'll want it all on one row. But the way you're writing the instruction into your main transcript is going to be optimal for the width of your main circuit, which means that here for example, if the main circuit is four columns and the Grumpkin lift, the curvevm requires six columns. Then you need to do some funky transformations because what's going to happen as part of this curve transposition subprrotocol is that as part of the Grumpkin ECBM proof, these transcript polynomials are going to be opened at a random point. And then what needs to happen is you need to take the information in your main transcript that's over your main curve that maps to these columns and basically interpret it as if they were part of the Grumpkin transcript columns when you're evaluating polynomials.
00:52:42.380 - 00:53:59.076, Speaker B: So by that I mean like in this example, for example, the values here x low and x high need to be like they combined form one column in your east vm, same with the y low and y high values, and then z one and z two. They're their own independent columns. Then the opcode is an independent column, but here it's been duplicated because, well it doesn't need to be duplicated but there's a redundant value here that's not needed. And so the idea is that this transposition circuit is going to iterate over your main transcript and construct and create instructions for the non native field virtual machine that are consistent with what needs to happen to basically open the transcript information as if they were coefficients of the Grumpkin using CPM polynomials. So here, for example, we're saying let's do multiply. So at row I, let's do a multiply and add row I of the non ended field virtual machine. Let's do multiply and add where over like Zeta and x mod the Grumpkin prime.
00:53:59.076 - 00:55:16.876, Speaker B: So that's basically saying that's evaluating one degree of the x polynomial here in the EZBM. And so the idea is that if m is the number of eziops plus one, then from rows I to m you will want like rows I to m will exclusively be multiplying add over the x coordinates of the information in the ECC transcript. So basically what's going to happen is as you're iterating over the main transcript, you're just taking the x I and x low values and you're shoving them into the first into rows like zero to m of the non to field bm. And then similarly you're then taking actually maybe and then the next m rows in your circuit you're evaluating, multiply and add on zeta and y. So that gets you the Y column. And then you do the same for z one, you do the same for z two, and you do the same for the, for the opcode row. And so there's a bit of a weird transformation that's happening here because you don't want to open the main transcript polynomials, you don't want to evaluate the main transcript polynomials over the cycle curve modulus.
00:55:16.876 - 00:56:36.584, Speaker B: You want to evaluate the polynomials that would get produced by the ECCBM and evaluate those with the cycle curve modulus. So there's a weird funky transformation that has to happen here, but it's very doable. So the end state of the non interfield transcript, after processing the main transcript will be that each virtual machine operation becomes, as I said, like five of these non native field VM operations where you interleave them. So the idea is that the non interfield VM, the accumulator, only works on one polynomial at a time. And if this polynomial was a degree m, then it means that you're going to have five m rows where each block of m rows is going to be working on one polynomial. So either the polynomial for the x coordinates, the polynomial for the y coordinate, the polynomials for z one, the polynomials for z two, the polynomials for the, for the opcode, and yeah, the reason why it's actually m plus one rows for each block is because at the final, you also need to shove in an assert equal opcode every m rows to check that the resulting computation lines up with what it needs to be. So this is possibly a slight overcomplication.
00:56:36.584 - 00:56:47.904, Speaker B: That's to do with the fact that the elite KerVM transcript format is not the same as the format as your main transcript format, this problem will go away.
00:56:48.102 - 00:56:50.016, Speaker C: Could you not just make them the same?
00:56:50.198 - 00:57:26.450, Speaker B: You could, yeah, but you could. I can't quite remember why I was so gung ho on not making the same when I wrote this up. I think the idea here was that actually. Yeah, no, I don't know. Perhaps it's just needlessly complex. If you just make your main circuits with five, this kind of goes away anyway. If the formats are different, then there's a step you can do to transform them properly.
00:57:26.450 - 00:58:01.990, Speaker B: So. Yeah, details about the transposition circuit. I don't know if it's even worth going over this, because I imagine this is like a month old and it's already obsolete, and I haven't even written a line of code about it. But the idea is, yeah, there's going to be a little bit of algebra if you want to do these transformations. It's a bit of a, bit of a cost, but it's pretty trivial compared to the cost itself actually proving the ECC VM, proving an interfield VM. So it's kind of not, that was originally this was an internal talk to my team. So this is an internal development.
00:58:01.990 - 00:59:20.790, Speaker B: But hey, I'll share it with you guys as well. So yeah, this is basically how we're planning on tackling it. Implement the vms first and then add this concept of an instruction machine into our circuit builder code and then kind of use these vms that we've built, that we will build in our recursion code. Instead of actually brute forcing the keb operations, the recursive verifier circuit will just instead just use this transposition composer to build up like an instructor machine transcript. So yeah, I mean, it's fairly complicated, but I think it's eminently doable and at a very high level. Just the idea of deferring non native elliptic curve operations that are required to recurse and then transforming them into a problem over a cycle curve reduces the complexity by about an order of magnitude or more compared to brute forcing, and I think has better constants than doing it at the recursive layer like header do. So yeah, that's the goblin tonk lazy recursive proof composition.
00:59:20.790 - 00:59:27.670, Speaker B: Thanks folks for following along and very very happy to answer any questions, folks.
00:59:29.050 - 00:59:29.782, Speaker C: Thank you.
00:59:29.836 - 00:59:30.440, Speaker B: Awesome.
00:59:30.910 - 01:00:03.582, Speaker A: Yeah, thanks a lot Zach. Man, that was a lot of information in a short time. Thank you for organizing it as well as you did. And like I mentioned, if you want to share this deck with me, I'll put it in the YouTube video. I linked a couple for everyone's just to say out loud here, there's some comments and one of them is reference. Dara referenced the Gemini paper, which I think was about elasticsnarks, and Mikhail, one of the co authors, presented that actually at ZK study club. So I'll link that in the YouTube.
01:00:03.646 - 01:00:04.260, Speaker B: As.
01:00:06.710 - 01:00:19.960, Speaker A: Then Dara also linked a ZK study club on MSM algorithms, which I'm assuming we discuss as pippinger. I'm not sure if we discussed with the other one. Yeah, thank you.
01:00:21.070 - 01:00:26.380, Speaker C: I also mentioned, I think. I'm not sure which of those were mentioned.
01:00:26.990 - 01:00:29.434, Speaker B: Yeah, but great opening up to questions.
01:00:29.472 - 01:01:07.858, Speaker A: Does anyone have any questions or Zach or comments on related mean again, potentially Srinath maybe? I know we talked about hyperplunk a few times in here and hyper novo, so maybe it would be interesting to hear your thoughts on the relevance of this construction to your work. Not to call on you, but if you feel like it would be interesting, but yeah. In general, if anyone has questions, feel free, can go ahead and ask. Stunned silence follows Zach's presentation.
01:01:07.954 - 01:01:16.600, Speaker B: Yeah, hopefully some of this might be a bit clearer if we can print it, instantiate it in.
01:01:18.170 - 01:01:30.320, Speaker A: Guess you have a bunch of, I think, costs in there, but I think, yeah, concrete benchmarks, definitely. I think I mentioned this earlier on the side chat that it would be really interesting to see this concretely instantiated and benchmarking that.
01:01:32.450 - 01:01:39.966, Speaker C: And of course, after implementing something, it's easier to explain it in general, even if people are not reading the code.
01:01:40.148 - 01:02:15.930, Speaker B: Yeah, hopefully in a month or so, we'll have a POC, maybe two months, depending on our time, what other stuff gets in the way. But, yeah, internally, we're pretty bullish on this for taking the pain out of drawing the sting out of the costs for recursion. So, yeah, combined with folding scheme stuff, I think it makes recursion IBC just utterly, utterly trivial even of arbitrary circuits.
01:02:17.230 - 01:02:39.570, Speaker A: Yeah, that's obviously a theme across a lot of development in the space. And, yeah, it's been interesting to see. I mean, this is something that, for Zprize, we're looking at exploring this year how to create some categories around applying different types of recursive techniques for computations that otherwise would be impractical.
01:02:42.630 - 01:02:53.480, Speaker C: So for the zprise thing, would that be sort of define a high level problem and you can use any recursion method to do it, or would it be targeted towards specific.
01:02:55.210 - 01:03:17.194, Speaker A: A. It's a bit of a trade. So the ideal is what you just said, dara, the trade off in running these competitions is always, the more open you make it, the harder it is to judge. Basically, if you wanted to open all the way up, you could say, hey, invent your own new proof system if you want to. But then, of course, I have to make sure that's correct. We have to make sure it has to be audited.
01:03:17.242 - 01:03:17.406, Speaker B: Right.
01:03:17.428 - 01:04:01.706, Speaker A: So then it's okay, maybe you fix it to, like, a handful of proof systems or recursion schemes. So there's this tension between the degrees of freedom you give the competitors and the ease of judging. But the goal this time is to move it up from last time was really all about the MSM's low level, and now this time, it's more about like, hey, what's a problem? We care about? Use any one of this number of schemes to kind of solve this. So we talked specifically yesterday at the steering committee meeting, we met and we talked a lot about Nova and just generally like, hey, how do we work Nova in and giving people more flexibility. So, hey, you're not just stuck with Nova, but any recursion scheme could work. I think that would be good. Back to the point about concrete benchmarks, and there's so many great new schemes coming out for different that I think have a wide range of applications.
01:04:01.706 - 01:04:25.350, Speaker A: It would be good to just concretely see for this application or this application. What are we talking about in real world numbers? Anyway? We're going to be defining the prizes over the next week and we'll release some more info about that. But yeah, definitely planning on sharing that in the ZK study club telegram group. So it would be awesome to get feedback from you all specifically on your thoughts. But in general, that's the tension.
01:04:26.010 - 01:04:36.860, Speaker B: Can I make a very tongue in cheek suggestion for a price category? Absolutely. What about a roll your own crypto price? So a price for building a proving system that does not yet exist in code.
01:04:38.350 - 01:05:01.890, Speaker A: That's interesting. We had not considered that. One thing that we did consider having is having, like, a bug bounty of, like, hey, all these new crypto systems that people are inventing that they haven't been out in the wild yet, in some cases, they're already being used. And so it'd be interesting to see, like, hey, if anyone can find a vulnerability, maybe that's worth having. But we hadn't thought of the inverse of roll your own crypto.
01:05:03.430 - 01:05:10.786, Speaker B: Yeah, it's not great for legitimacy and safety, but it could be fun, though. Yeah.
01:05:10.968 - 01:05:20.518, Speaker C: Most plausible sounding fake paper. I liked your April Fool's Day thing about that site.
01:05:20.684 - 01:05:24.950, Speaker B: Yeah, thank you, Dara. Although at least someone appreciated it.
01:05:25.100 - 01:05:45.360, Speaker A: I mean, I think we should have a prize category for who can come up with the most clever new name for a prefix to plonk, because we have a lot of different planck or plonk variant names. And so I think that could be deserving of its own and valuable to any researcher who's writing a new paper about know, maybe the IACR would sponsor that one.
01:05:47.490 - 01:05:53.410, Speaker B: Cool. I'm down with that. That sounds like fun. Awesome.
01:05:53.480 - 01:06:00.450, Speaker A: Do we have any more questions for Zach or any just discussion topics that people are interested in or want to comment on generally?
01:06:04.530 - 01:06:05.710, Speaker B: Okay, cool.
01:06:05.780 - 01:06:37.610, Speaker A: Well, I think I'll cut the video here in a second, but thank you very much, Zach, for being here. Thank you, Dara, for always being a strong contributor to the conversation. And thank you, everyone, for attending, and we'll see you guys next time. Oh, I guess maybe quick plug. We're presenting some upcoming work or some work that Ariel has co authored, so we'll be doing that one, I believe, in two weeks. Anyway, I'll publish the schedule on the telegram group, but we have some other really cool sessions coming up. Hope to see you guys at those as well.
01:06:37.680 - 01:06:38.138, Speaker B: All right.
01:06:38.224 - 01:06:39.610, Speaker A: Thank you very much, Zach.
01:06:40.030 - 01:06:41.320, Speaker B: Cheers. Thanks, everyone. Take care.
