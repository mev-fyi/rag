00:00:00.090 - 00:01:01.600, Speaker A: All right. Yes, as I said, joshua will take us all through Plookup later and so that'll be exciting. He'll talk a little bit and have a really cool live demonstration. So we'll start off with a non zk related cryptography topic, but one that I'm sure will still be of interest to many of you here. Just go over this project that we call Fervio, which is in collaboration with Osmosis Labs. It's a great collaboration that we've had with them and yes, okay, just having a little technical issue. All right, there we go.
00:01:01.600 - 00:03:14.426, Speaker A: You can follow along because it's hack MD slides. You can follow along with this link and you can browse the slides at your own pace. Although most of the technical details about Fervia will be released in a technical paper relatively soon that we're currently preparing that will have all of the nice math and proofs and other details that will be very interesting. But for right now, I want to go through the high level overview of what we're trying to do and some design decisions that we've made. Okay, so what is Ferbia? Well, there's this problem that blockchains have encountered, especially as more and more finance applications move on to the blockchain. This problem of minor extractable value, or mev, and the idea of minor extract value, which is perhaps a little bit of a misnomer because it's not just miners that can extract value in this way, but pretty much anyone who has some kind of advantageous position on the network where they can control in some way how transactions are executed on the network, including putting their transactions ahead of other people's in a way that can be called front running, censoring transactions, reordering transactions to be more advantageous to the miner or validator in many cases. In fact, these entities don't actually have any kind of formal role in the network, but nevertheless they have better connections and are able to monitor the mempool for incoming transactions or that kind of thing.
00:03:14.426 - 00:04:47.186, Speaker A: And they're able to, because of whatever advantages they have, be able to change how transactions are executed and extract some value. And in cases of front running, this sort of lowers the utility of the network. It makes it less useful for other people, because if other people are trying to do their trades in the network, but they keep getting front run and extracting some of the value of these trades, then there's not as much usefulness of this network for those traders. So there's some great articles on this, some technical research by this flashbas group and some amazing articles on the concept of a deep forest, which I encourage you to read. But many of you may already be familiar with. And solving this issue, or addressing this issue of minor extractable value is going to be important for blockchains going forward. Okay, so what can we do about it? Blockchains are very public, at least if we're not in the zero knowledge area with transactions come in, they're public, they have to be publicly executed.
00:04:47.186 - 00:07:05.242, Speaker A: So what can you do? Well, the most important thing you can possibly do is to at least not reveal the content of the transactions until they're actually included in a block and the block is committed and the ordering of the transactions are committed. Only then do you want to reveal what the content of the transactions actually were, because that way, when the block is being constructed, no one can use their knowledge of the transaction that was submitted to put their transactions in front or sandwich the transactions, or in other ways alter the order of how the transactions are executed. And so broadly, I think people have looked at minor extractable value and said, okay, trying to prevent the content of the transactions from being revealed is the important part. However, the details about how to actually do this are not necessarily so straightforward, and there's actually several different kind of theoretical approaches which people have looked into. One very popular idea of how to address this is using some kind of time lock encryption, usually, but maybe not always, based off of something like verifiable delay functions, where the idea is that you lock your transactions in this time lock encryption and you encrypt your transactions to say, well, I want the deep transactions to only be able to be decrypted, say 1 minute from now, and the hope being that in that 1 minute time they will be included in the blockchain and committed to. And then once the time lock is opened, the transactions will execute in the order that they have appeared in the blockchain. And in some ways, this is a good approach.
00:07:05.242 - 00:08:54.070, Speaker A: It's certainly sort of very compatible with the notions of how proof of work blockchains work. There's some tricky things in terms of how to get the actual timing to work out the way that you want them to. And also, certainly in some applications, having a delay in execution is not so convenient as well. You want your time lock to last long enough to make sure that your transaction gets executed without being front run, but you also don't want your transaction locked for too long, otherwise it might become stale or it might not execute in the way that you want to. So a different way of doing it, which is the way that we've gone with furvio, is to use something called threshold decryption, where unlike time lock encryption, which is based off of encrypting to some future time, threshold decryption is this idea of decrypting via some committee or some group of entities that are able to decrypt. And there was a great talk by Sonny a couple weeks ago on these various trade offs and these different approaches that can be used for preventing minor extractable value, which you can go watch his talk, it's really great. But further into the trade offs here and just describe how do we actually accomplish this with Furbia.
00:08:54.070 - 00:10:20.018, Speaker A: So with threshold decryption, if you're familiar with standard public key cryptography, where a message is encrypted to a public key and then whoever possesses the private key can decrypt them. With the threshold decryption system, it works very similar. You have a single public key that's available to everyone public, and it's broadcast on the blockchain or gossiped in some way so that everyone has it. And messages, which is in this case are essentially transactions, can be encrypted to this single public key. But the difference is that no one actually knows the private key. The private key that's associated with this public key is not possessed by anyone in the world, but instead n different parties own what we'll call private key shares, which are, we've kind of partitioned up this private key in some way so that every subset of at least t parties can decrypt the message, while every subset of at most t minus one parties cannot decrypt the message. And this is related to SOmething CALLed shamir secret sharing, if you're familiar with that.
00:10:20.018 - 00:11:42.910, Speaker A: The idea of how n parties can share a general kind of secret such that at least every subset of least t parties can reconstruct the secret. Well, every subset of at least of at most t minus one parties cannot reconstruct the secret. And the basic concept is the same, except that instead of reconstructing the private key, which would reveal to everyone what the private key was, parties are not going to reveal the private key. They're instead going to decrypt this message. And these parameters are very adjustable. So in a threshold decryption system, you can choose these values of t and n, where if you have n different parties and you want, say, two thirds of them to be able to decrypt, then you can choose t equals two thirds of n, right? So threshold decryption is something that we can build on top of secret sharing. And it's been known for many, many years that you can, and there's been many different constructions for how to build threshold decryption crypto systems, although I'm not sure that there's actually been very many applications deployed that actually use threshold decryption.
00:11:42.910 - 00:13:53.974, Speaker A: Okay, so threshold decryption as a cryptographic concept is well established, but one reason why verifiable delay functions are a little bit newer. It sort of came up in the blockchain context first, but in the threshold, cryptography is actually very useful depending on the kind of blockchain that you're building it on top of. So in a BFT, byzantine fault tolerant, consensus based blockchain, such as for example, those that are built on tendermint or the tendermint proof of stake protocol, there's actually a very nice natural correspondence between who owns the stake in the network or who has been delegated the stake in the network and who should own these private key shares. So in the tenderman proof of stake system, validators are delegated weight by people who own the staking token in the network. And validators can produce blocks by voting using the stake, and more than two thirds weight by stake when they vote to commit a block that causes the blocks to get committed and the transactions in it join the blockchain. Right. And so at least sort of qualitatively, you have this very natural correspondence, the validators which already have been delegated this stake in the network, this authority in the network, to say, okay, if some threshold of stake has voted to commit these blocks, then the blocks become finalized and the blockchain progresses forward.
00:13:53.974 - 00:14:15.760, Speaker A: And so it's very natural now to say that these validators should also have these private key shares and be able to decrypt these transactions subject to the same two thirds weight threshold. And so this makes using threshold decryption in this proof of stake world.
00:14:19.330 - 00:14:19.598, Speaker B: A.
00:14:19.604 - 00:15:37.810, Speaker A: Lot more of a natural choice than maybe using time lock encryption. So here's the basic idea of how we want the transaction model to look like. Validators are going to generate a common public key in some way, a single public key that everyone using the blockchain should know. This public key gets broadcasted in some way. Alice wants to do some kind of transaction, so she builds the transaction in the way that she has her wallet software that builds her transaction for her, and the wallet is going to encrypt her transaction to this public key, so she encrypts the transactions. No one can know what's contained in this transaction, but presumably it can be decrypted later at some point. So once Alice broadcasts her encrypted transaction, validators are going to get this encrypted transaction in the Mem pool, and validators are going to include this encrypted transaction eventually in a block.
00:15:37.810 - 00:17:33.450, Speaker A: Validators, because they have no idea what's in the transaction, are going to have to use just what little information they have to decide when and how to include Alice's transaction. So maybe she's included the gas amount and the fees and that sort of thing, but otherwise, no one can actually tell what her transaction is or what kind of transaction she might be doing, and therefore it makes it much more difficult to front run or get MeV from. And so validators are going to include Alice's encrypted transactions in a block, whoever is constructing the next block, because in the tendermint proof of stake, this is very sort of well described, well specified about how blocks are proposed and voted on. So whoever the block proposer is will scoop up a bunch of transactions from the mempool, add it to a block, and then the network will come to consensus on that block. And what we want to happen is that when the validators are voting on this block to decide whether to commit to it, they should be committing with their decryption shares of Alice's transaction. But essentially the decryption shares of every transaction in this block. But the idea is that we want to make sure that the exact moment when two thirds of stake is voting for this block, that two thirds of the private key shares are also voting for this block.
00:17:33.450 - 00:18:35.930, Speaker A: And therefore, as soon as you reach this two thirds threshold, the block is finalized and you also have enough decryption shares to actually decrypt Alice's transaction and execute it. So that's the model that we're trying to get to. Now, there's some difficulties that happen when you try and actually implement this. The biggest difficulty is that, well, validators have different amounts of stake. And in general, you might hope that every validator has about the equal amount of weight, because that's good for decentralization of the network and other reasons. But in practice, of course, this is not how it works. Validators tend to be very top heavy, and a couple of validators tend to have a lot of weight.
00:18:35.930 - 00:19:43.778, Speaker A: And so two thirds by count of validators is not generally going to be equal to two thirds by stake. Even in a really great world where validators have roughly equal kind of stake, you're not necessarily going to have this exact equality and so if we want to achieve our ideal outcome where the validator can finalize a block, if and only if they can decrypt all the transactions in the block, well then we need to do some adjustment there. That's the outcome we want to get. But we can't just give one private key share to each validator. So the only really reasonable way to do this is by weighting the private key shares. Namely you have more private key shares than you have validators, and higher stake validators get more private key shares. And if you have 100 validators, for example, you might want to give 2000 or 4000 or even 8000 key shares out.
00:19:43.778 - 00:21:10.894, Speaker A: And this will let you approximate the amount of stake that all the validators have in the network. Now, because we're now making the number of private shares, private key shares in the protocol much, much larger. There's a big difference between, say, n equals 100 and n equals 8000. The immediate thing that this enforces is that every part of this protocol now must be big of n log n in performance in both network bandwidth and computation, which otherwise the scaling is just too bad. It has to be at least this kind of quasi linear scaling. And the reason that I point this out as being really important is because it's not necessarily obvious how to achieve this, because remember, if you have n different parties, the communication complexity pair wise is n squared. So if you're sending a MEssage between every pair of entities in the network and you have n entities, then you have n squared messages in general.
00:21:10.894 - 00:22:41.914, Speaker A: And then if every entity must do some computation on the data they receive from every other entity, then it can very easily become big o of n squared bandwidth and computation. And in fact, this is kind of just a general issue with distributed systems in general. But it just becomes kind of even more acute here because we are sort of decoupling the number of validators or number of entities in the network and the number of private key shares, which is now much larger. And so with everything that we do, we have to focus on keeping this performance much better. So the first part of this, of Fervio is so we need to have the validators be able to create this private key where the private key is distributed around to all the private key shares are distributed around to all the validators. But no one actually knows what the private key is. It's very much like a trusted setup in the DK world, right? And so won't go into the details of how we do this.
00:22:41.914 - 00:23:45.786, Speaker A: There's some really great papers that describe the technical details much better. But the basic design decisions that we've made in furvio are that we want to use something called a publicly verifiable secret sharing scheme. So as I mentioned, secret sharing before, the idea that end parties can split the secret among themselves. The verifiable part of verifiable secret sharing is that everyone can verify that their own private key shares are valid. They haven't been distributed an invalid private key share by someone. If, say, there was a rogue validator being part of the protocol, well, you would want that their messages can be detected by the person receiving the messages. But we go a little further than just a verifiable secret sharing scheme.
00:23:45.786 - 00:25:24.270, Speaker A: We want to use a publicly verifiable secret sharing scheme. In a publicly verifiable secret sharing scheme, the validity of every step of the DKG can be checked by every other validator in the network. So this means that the validators are going to engage in this key generation protocol and they're going to share messages with each other. It's not just the recipient of each message that can verify that the message was constructed properly, but in fact, every validator, or even people who are not validators in the network, can check each step of the DKG, check each message and make sure that it's valid. And the reason that this is important in blockchain context is that there's no issues if a validator goes offline. So the reason for this is that if the scheme was not publicly verifiable, then a validator that could send a rogue or invalid message might not be detected. Because if the validator that's receiving that message has gone offline for some reason, either on purpose, or if there was like a denial of service or something, or if there's just any issue that caused them to go offline, this invalid message might not be detected.
00:25:24.270 - 00:26:25.200, Speaker A: And so the distributed key generation gets much more complex when you have to have additional rounds of communication where a validator has to be online and file a dispute if a different validator is acting in a malicious or byzantine way. And so our primary objective here is that we're going to avoid that. We want to avoid that as much as possible. We want to use a publicly verifiable secret sharing scheme where if a validator goes offline, then they know that all of the messages that they would have received, they were online are still good. Let's see. We have a question. Would it be possible to use a hierarchical secret sharing scheme to account for the weights of each validator? And thus reduce the number of shares needed.
00:26:25.200 - 00:27:12.490, Speaker A: That's a good question, and it's one that we struggled with for quite a while ourselves, trying to figure this out and reading all of the literature about secret sharing. So the short answer to this is that you can, but you don't necessarily gain very much. And the reason for this is that is actually kind of quite deep. Secret sharing is an information theoretic construct. It's actually based off of polynomials and interpolation. Right. If you're familiar with secret sharing, is not actually based on like a cryptographic construction, necessarily.
00:27:12.490 - 00:29:39.770, Speaker A: And so the problem with information theoretic schemes is that you don't actually get many opportunities to do like compression or aggregation or anything like that. And so if you have a secret sharing scheme, which you want to have parties have different weight in them, then very oftentimes the size of the private key share is going to grow with the weight, rather than anything else grow linearly with the weight. And so if you adopted a different kind of secret sharing scheme, for example, there's secret sharing schemes, they're not based off of polynomials, they're based off of chinese remainder theorem. There's actually many different secret sharing schemes that people have proposed, but sort of a common thread amongst them is that unfortunately, when you actually try and implement them, it's very difficult to sort of beat the simplest secret sharing scheme of sharing, a polynomial evaluation polynomial, which it's not to say you can't beat that, but certainly a simple scheme is very much more straightforward to implement and optimize. But as we found, we were able to get the secret sharing scheme to be very performant. It's just mostly a matter of dealing with the high amount of bandwidth required because of the high number of private key shares, and also making sure that our implementations are all very fast, computationally, algorithmically fast, this big o of n log n algorithmic complexity. Okay, so the other important property of our distributed key generation is that we actually use a synchronous on chain message passing protocol.
00:29:39.770 - 00:30:58.022, Speaker A: And this is actually quite a bit different than most of the academic literature on distributed key generation. A lot of the interesting work in recent years has been in the asynchronous model, or maybe I should say a partially asynchronous model, where entities are passing messages between each other, perhaps using a gossip protocol. And there it becomes much more difficult to achieve, like a big o of n log n overall complexity and also maintain the synchrony of the network. Synchronizing things in general is just a very difficult thing to do. That's why blockchains are interesting. But since we have control of the underlying chain, it's actually much, much simpler if you use the existing BFT consensus mechanism to synchronize your distributing key generation protocol. And it's just much simpler when the blockchain and BFT consensus are already there.
00:30:58.022 - 00:32:38.200, Speaker A: And you can assume that when a message is posted to the blockchain that it can be received, that it actually is received by everyone, and that there's not some kind of byzantine fault in the network. Now this of course means that there's a lot of on chain data, but we find that not a problem because the data is actually prunable later, and so we're not carrying this data forever. So we're going to run our distributed key generation protocol on the chain, and we have based on this nice, publicly verifiable secret sharing scheme. There's a technical issue which tripped us up for quite a while, namely that publicly verifiable secret sharing based DKG generates private key shares that are elliptic curve points and not scalars. So if you're familiar with how private keys work, usually in elliptic curve cryptography, private key shares are scalars and public keys are elliptic curve points. However, in the DKG that we constructed, the private key shares are also elliptic curve points. Formally speaking, this DKG works over a pairing friendly curve, and the public keys are g one points and the private keys are g two points.
00:32:38.200 - 00:34:14.178, Speaker A: The private key shares are g two points as well. And this is a problem because it becomes very difficult to use existing threshold decryption schemes which are not written with this assumption in mind. They're written with perhaps other assumptions in mind. And so our solution to this is we've developed a new pairing based encryption scheme that supports threshold decryption with these publicly verifiable secret sharing generated keys. Now, just one aside, there was a nice paper this year by Groth, the same groth as in gross 16, who described a PVSS based distributed key generator which actually does generate private key shares that are scalars. However, the scheme that we're using is quite good enough, quite high performance, and simple enough that we can continue using the PVSS scheme that has this private key share property. Okay, what are the optimizations and features that are really interesting and perhaps unique to furvia? First is that everything is being done using fast big o of n login algebra operations.
00:34:14.178 - 00:36:05.880, Speaker A: So if you're familiar with secret sharing in general, or perhaps just inferring things from what I've said, so far, secret sharing involves a lot of polynomial operations, multiplying, adding, interpolating polynomials, evaluating polynomials. There's a lot of these polynomial operations which happen in the secret sharing process, in the threshold decryption process, and they're sometimes, but not always, optimizable using FFT techniques. It turns out that there's actually a quite general field of fast polynomial operations, fast polynomial algorithms which allow you to do all these operations in the required and login speed. And so where we've had to, we've implemented all of these operations and integrated them into our distributed key generation and threshold decryption implementation. And so that's going to allow us to scale up the number of private key shares to the level that we want without destroying the performance completely. Some optimizations that we've done on the decryption share side we have in our implementation one decryption share per transaction per validator, not per transaction per private key share. So this is a very nice optimization which our very own DK hack host Kobe helped point out.
00:36:05.880 - 00:38:00.790, Speaker A: And this is very important because anytime that you can do something on a per validator basis instead of a per private key share basis, you're automatically going to get an enormous speed up because you're going from, like I said, 4000 or 8000 private key shares down to 100 validators. Our encryption and threshold decryption scheme has to be something called key committing. Namely we want to be able to say that either a transaction is guaranteed to be decryptable, or it's detectable that it's invalid. Or another way of saying this is that all valid transactions can be decrypted, verifiably decrypted, using the key that we derive. And the reason it's important is to prevent the censorship of transactions, as you don't want validators decrypting a transaction and then discovering, oh no, I didn't like this one, I want to censor it in some way. The important property we get from key committing is that if a transaction is validly constructed and encrypted, then the decryption shares become available and it must become decrypted because everyone else can verify that if it actually was an invalid transaction, that fact is verifiable to everyone. So before I hand this off to Joshua, part, which I know that everyone is going to be interested in, was the actual concrete performance of this of furvio, both in terms of the bandwidth and the compute time.
00:38:00.790 - 00:39:10.984, Speaker A: So the amount of data that goes on chain for the distributed key generator is actually quite large it's 138 megabytes per epic. So if you refresh your key, say, once per day, you're putting this large amount of data on the chain. However, the trade off is that it's actually prunable. So it's more of an issue for gossiping all of this data around. It's not very efficient to gossip this data while the DKG is running. However, in terms of your long term storage costs, it's actually not important because once the DKG is done and the epic is passed and the key has expired, you can just prune all of this data from your blockchain. And if you have the ability to build your blockchain in a way where it's prunable, then you maybe don't care so much that your TKG uses so much on chain data and then the actual data.
00:39:10.984 - 00:39:58.692, Speaker A: That sort of is important on chain. The decryption share and the ciphertext overhead is going to be quite minimal. Your decryption shares are very small amount, basically 48 bytes per transaction per validator. And so maybe if you have a lot of validators, this can add up per transaction. But as a general rule, it's actually very small. It's much better than if it was again 48 bytes per transaction per private key share. And then the ciphertext overhead from encrypting to this public key, a couple of hundred bytes.
00:39:58.692 - 00:41:32.980, Speaker A: This is also mostly unavoidable performance wise. Rough single core measurements on my laptop, and so it should be much better on actual validator hardware. We managed to push most of the compute into a per block compute. So it takes about 4 seconds of compute per block to sort of do all the prep work for doing threshold decryption for that block, and then a really insignificant 16 milliseconds extra compute per transaction. So if you have many transactions, you're actually amortizing this four second cost over the many transactions in that block. And so even for a very high number of transactions, like 100 transactions per block, you still don't actually have that very much total compute time. And if you parallelize this over many cores, if you have 32 or 64 cores, this should make this quite feasible for validators to do over all of their blocks, right? Some validators might have to upgrade their hardware, but that's an acceptable trade off against running a proof of work minor or something, right? It's not that significant to add some more cores.
00:41:32.980 - 00:42:24.280, Speaker A: Okay, you can check out all of our work in progress public repository. Certainly appreciate any comments or feedback or other insights that you have. Any questions. Let's see. Share papers related to DKG or PBSs. Yes, a DKG in the wild is a great one. There's one that, in fact, that put us in contact with Kobe about this in the first place, on aggregateable DKG.
00:42:24.280 - 00:43:36.780, Speaker A: In fact, our DKG is very much closely related to this aggregatable DKG. Except, of course, the aggregatable DKG is more interesting overall in the asynchronous context, where they get this big of n log n performance asynchronously, which is a much bigger achievement. The fact that we're doing everything synchronously makes the full, aggregateable DKG unnecessary. But in every other respect, our DKG is very similar to the aggregate. Um, maybe I'll share one more paper. This is the groth 21 dkg. It's a very interesting piece of work.
00:43:36.780 - 00:44:09.908, Speaker A: I encourage you to at least read the abstract and maybe understand why it's an interesting piece of work, but certainly it's also quite complex, both in concept and implementation. Okay, I think quite a bit of time.
00:44:10.074 - 00:44:19.864, Speaker C: Yeah, I think. Josh, thank you so much for this talk. I guess if there's any other questions. Joe, do you want to just kind of stick around in the chat if people wanted to ask more?
00:44:19.982 - 00:44:20.888, Speaker A: Yeah, absolutely.
00:44:21.054 - 00:44:40.530, Speaker C: Perfect. And, Joshua, do you want to maybe jump in here? Joe, I think you're going to have to turn off your screen for him, for Joshua to show. Perfect. Cool. And I think when you're not speaking, maybe just mute to avoid any sort of background. Perfect. Cool.
00:44:45.050 - 00:45:47.690, Speaker B: Okay, see, can I make this big? All right, so, yeah, I'm going to present on plunk up, a protocol which combines both plank and Plookup protocols into one. Well, let me just find where I'm at here. So what is plunk up? It's a method for unifying the plonk and pluckup proving schemes. Plonk came up first. Plokup came out a bit later. There was nothing in the Plokup paper saying exactly how these two things code or should be combined. Plokup was kind of its own singular thing, and so we went ahead and tried to combine these and ended up with Ploncup.
00:45:47.690 - 00:47:06.730, Speaker B: Planckup has advantages of Planck, which are relatively small proofs, fast verification, and universal setup with the additional power of lookups to reduce circuit sizes. If you have a circuit unfriendly function, like bitwise operations, most hash functions have some bitwise operations, so implementing them in a circuit is a little inefficient. But with lookups, you can increase the efficiency a lot. The reinforced concrete hash is an example of a new hash function that exploits lookups and arithmetic in a circuit. So to express the reinforced concrete hash in a circuit, you need something that can do both. Plank up is our solution for doing both. We've been following the convention of using the term plankish to refer to a family of protocols that descended from Planck.
00:47:06.730 - 00:47:51.580, Speaker B: If you just say planck, no one knows what you're talking about, because there's so many different kinds. So if we're referring to the family of protocols, we just call them plonkish, so that people know we're talking about a whole group of protocols that have some similarities. I think it was some folks at ECC that started using plonkish in this way. So, some examples of plonkish protocols are Planck, the original turboplank and alter plank, which. I always get confused which one is which, but one is referring to custom gates. One is referring to lookup gates, possibly also custom gates. Like I said, I forget what's what.
00:47:51.580 - 00:48:27.794, Speaker B: Pluckup, which is for lookups only. Plank up, which is our version of lookups and arithmetic. Halo two, which uses planck with a different polynomial commitment scheme, and Faflanc, which just came out. And also, this is the first time I'm ever pronouncing that out loud. And, I don't know, maybe your plonk here someday. There's so many different planks. Everyone should try and make their own plank.
00:48:27.794 - 00:49:38.410, Speaker B: I think it's a good exercise. Okay, so what characterizes a plankish protocol? They can't be characterized by their polynomial commitment scheme or their gate structure, because you can change those. You can swap out the polynomial commitment scheme. You can use KZG or Kate commitments. Halo two uses an inner product argument, like bulletproof style commitment scheme, but you could theoretically swap it out and use something like fry turning planck into a kind of stark. So you can't really call something plankish just by its polynomial commitment scheme or the gate structure, because you can use custom gates, which can be basically anything. So what does characterize a plankish protocol? You have fixed width gates, which could be custom.
00:49:38.410 - 00:50:38.830, Speaker B: You have a universal setup, so you can run one trusted setup that works for any circuit below a particular size. Plonkish protocols usually exploit a Lagrange basis in order to gain some efficiency there. So in a Lagrange basis, you represent your values by points or valuations on polynomials rather than coefficients or some other thing. And they have a grand product argument. The original Planck uses a grand product argument to show that there's a permutation of variables that shows their equality. Plokup has a different product argument. Plonk up combines those.
00:50:38.830 - 00:51:17.156, Speaker B: I just see a question here. What's fixed with gates? With Plonk, you don't get unbounded fan in like you do with rank one constraint system. With r one cs. You can do basically unbounded additions in a single gate with plank or plankish protocols. You don't get to do unbounded addition. You could make your gate have many additions if you wanted. You could make it have eight additions.
00:51:17.188 - 00:51:18.650, Speaker A: If you wanted, whatever.
00:51:19.420 - 00:52:07.638, Speaker B: But it's fixed. So I like to view plunkish protocols like a switchboard. So what I have here is kind of my mental image of what's going on with planck. I've got three different types of gates. Here. We have addition in green, multiplication in blue, and lookup in orange for ABC, and D a one b one c. Be a part of this green row means that they have to satisfy a particular constraint.
00:52:07.638 - 00:53:07.034, Speaker B: In this case, an addition constraint. So I decided to make it a plus b equals c plus d multiplication a times b equals c plus D. And for lookups, then you just look up the tuple ABCD and check to see if it's in the table. All right, so rows are associated with these gates. They need to satisfy these gate constraints. All right, then what you can do is connect them together. I hastily drew these wires on an airplane, but you could plug a kind of patch cable in and connect two variables.
00:53:07.034 - 00:54:00.000, Speaker B: So here I've got c one and a two connected, which means they need to take on the same value. That's how you can connect variables from one gate to another gate. So here I've got a multiplication gate where one of the factors in the multiplication b four also needs to be a part of this lookup. So it's going to be the first element in this lookup row. So c one equals a two, b four equals a five. Before I get too far, I just wanted to give some terminology so you know what I'm talking about. Wires are columns of the proverbs, private inputs to the circuit, and they're vectors of length n.
00:54:00.000 - 00:54:58.080, Speaker B: So if I scroll back up here, I'll try not to do too much scrolling, but if I scroll back up here, you'll see I've got abc and d columns. So we have four wires. The a wire, b wire, c wire, dy, and the length of those wires is six. Okay, gates talked about a little bit but a gate is just a relationship that a row of the proverbs private inputs must satisfy and plonk up supports arithmetic gates and lookup gates. Sometimes these are also called constraints. Sometimes, if I'm really being bad, I might call it a row, but it's not really a row. But you might hear that selectors are vectors also of length n that turn gates on and off.
00:54:58.080 - 00:55:41.550, Speaker B: For example, a one index k of the lookup selector turns the lookup requirement on for row k. A selector can also contribute an auxiliary scalar to a gate constraint. In some cases, like I said, they're vectors of length n. Finally, we have copy constraints. I've already mentioned these. These are like those patch cables that you can plug into the switchboard that force two variables to take on the same value. So these are used to build a permutation which swaps each variable with another that is supposed to be equal to it.
00:55:41.550 - 00:56:32.460, Speaker B: So essentially we run through the same kind of product twice, one with the original value and one with the permuted value. And if those products are equal, then the original value and the permuted value must be the same. We also have a circuit description. This is all the stuff. Selectors, circuit length and the permutation encoding the copy constraints and the lookup table form the circuit description. This is public stuff that you need to prove and verify a circuit. Okay, here's what our lookup table will look like.
00:56:32.460 - 00:57:17.930, Speaker B: The lookup table has columns equal to the number of wires in the circuit. So I had four wires earlier. So I made a table with four columns. And we will take those values of wires in a lookup gate and see if those same four values appear as a row in this table. Okay, I made a lot of slides. I made way more slides than I needed. So I'm not going to go through all of these or not all of them in depth.
00:57:17.930 - 00:58:06.540, Speaker B: So you can see these slides. You can take a look at them later if you want to look at more of these arguments. But I will stop here for the grand product argument. This is what I was talking about before, where we check these copy constraints. So what we have is four wires, A-B-C and D, and we multiply up all of these terms into a big product. And if you look at the numerator and denominator of these terms, they're similar, but not quite the same. Up top you have, can I highlight that part? Not really.
00:58:06.540 - 00:59:03.980, Speaker B: Right here you have like I, beta I stands for the index of the original value. So we index these like basically the first wire a gets one through n, b gets n plus one to two n, and so on. So we have all these indices from one to four n. That's what I is here. And in the denominator, we have almost the exact same thing, except we now use the permutation on the index instead of the original index. So if the product in the numerator equals the product in the denominator, they should all cancel out. You get one.
00:59:03.980 - 00:59:39.510, Speaker B: If you try to mess with any of the values of the wires, it's not going to cancel out. You won't get one. If you try to mess with the indexes, it also is not going to cancel out. Okay. Pluckup also has a product argument. This, I should say, is a modified version of the pluckup product argument. We've changed this a little bit to make it more efficient.
00:59:39.510 - 01:00:41.018, Speaker B: This suggestion here for how to change this was Luke Pearson came up with this, really, really helped with the efficiency of our plankit protocol. Pluckup also has this product. This is really interesting, I think. So I'll try to explain how it works. You have a vector of queries, which is f, and you have a vector that comes from the public lookup table that's t. And you combine those together, you concatenate them to form a new vector, s, which is double the length of either t or f, because you're combining them together. And then you divide s into two halves, which we call h one and h two.
01:00:41.018 - 01:02:05.080, Speaker B: You can see those down here in the denominator h one are all the od indexed elements of s. H two are the even indexed elements of s, and s is sorted to match the ordering on t. Okay, so what happens here is that these denominator factors will run through all of the elements of s. And you can actually see here, if you look, we are not just going through the elements, we're kind of pairing up adjacent elements so that we have a first element and we have the next element, which is multiplied by delta. So all of the pairs of elements as you go through s are run through with these denominator factors. If that pair of elements that are adjacent matches a pair of elements from the table, then one of these denominator factors is going to cancel with this factor here. Sometimes the two adjacent values will be the same.
01:02:05.080 - 01:02:57.414, Speaker B: When they're the same, they are the same because they are a table value and a lookup value, which means a table value and a query value, I should say, which means the query is in the table. And that's why we see it twice. We have a table value and a query value. They're the same. When that happens, if these two, like h one I and h two I are equal, then you can factor out one plus delta, because we also see this one plus delta here. The one plus delta can come completely out of this. And what you have left is epsilon times the single value, which must be one of the queries, which is f.
01:02:57.414 - 01:03:55.200, Speaker B: So that will cancel out with these factors here. So as we run through this concatenated and sorted vector here s, it's either going to cancel out with an element of the table or an element of the queries if the vector is formed correctly. So this will all cancel out and you get one. Okay, I'm going to skip past some of this more technical stuff, and I'll show you these gate structures before we get on to the live coding part. With ploncup, we have two major kinds of gates. We have arithmetic gates and lookup gates. Here's an arithmetic gate.
01:03:55.200 - 01:04:38.540, Speaker B: What you have are the proverbs private inputs. These are ABC and D. You also have selectors, QM Qlro. We couldn't figure out a name for the fourth one, so we just called the fourth one Q four. And QC is a constant that you can add into a gate. So if you look here, this first term has QM times ab. So if the Q selector, the QM selector is of one, then that means we are including a times b in this gate as a part of the constraint.
01:04:38.540 - 01:05:31.020, Speaker B: If that selector is zero, then you won't have that in there. So if you turn on QM with a one, you make this a multiplication gate, or you can turn it off and make it more of an addition gate. The rest of the terms we have qlqr times A and b each, and also qO times c. These stand for left, right and out. So these can scale a, B and C, or they can turn them off completely by making them zero. We have a fourth one for D, and we have a constant also. So this is more expressive in some ways than rank one constraint.
01:05:31.020 - 01:06:09.096, Speaker B: Actually, I'd take that back. I take that back completely, as this is all addition here. A rank one constraint can also express this with also even more additions if they want. So this is less expressive than a rank one constraint, but it can do multiplication and scalarized addition here. So kind of like a linear type constraint with one multiplication. A lookup gate is much simpler. It doesn't have to satisfy a formula like this.
01:06:09.096 - 01:06:57.960, Speaker B: All it needs is that that tuple needs to be an element of the table. Okay, so if you want to use plunk up, you can run a setup with plunk up here we have the code for this. N is the size of the circuit that you want to use so you can generate a setup this way. This is your trusted setup you may have heard about. Then we create approver. Approver can have a kind of tag. I'm calling this one workshop prover.
01:06:57.960 - 01:07:39.060, Speaker B: This initializes a transcript. So workshop prover is also kind of a diversifier so that different proofs for different purposes will have different tags here and will not be able to. They won't verify if you have the wrong tag. All right, then the proofer has private inputs. My circuit here, I should have put a slide in this, but my circuit here is showing pythagorean relationship between private inputs. So there's three private inputs, a, b, and c. And my circuit should show that a squared plus b squared equals C squared.
01:07:39.060 - 01:08:38.552, Speaker B: So here's the proverbs private inputs 512 and 13, that should form a pythagorean triple, if I remember my geometry correctly. And these are added as inputs and become variables in our constraint system. The prover mute cs is our constraint system. All right, once we have variables in, we need to put our gates into the circuit. We have a multiplication gate here, and it has a selector value which is one, and it's two variables, a and a. So this should give one times a times a, so a squared. And then we save that here in VAR a squared, VAR b squared, essentially the same.
01:08:38.552 - 01:09:13.860, Speaker B: Just we're using VAR B course VAR C squared, essentially the same. We're using VAR c. Then we need one addition gate to show that a squared plus b squared equals C squared. The way we actually do that is show that a squared plus b squared plus C squared equals zero. Minus C squared equals zero. So we have our three squares. We could scale those if we wanted to, but we don't want to in this case.
01:09:13.860 - 01:10:07.744, Speaker B: So we're just using a one. And for c, we take the negative so that it adds to zero. Once we have our gates, then you can prove a circuit with a commit key. Ck, you just run prover prove, and that will create your proof for you. And then to verify, you need a commit key and a verifier key. But you just run verifier verify, and that takes proof, verifier key, and some public information here, the public inputs and lookup table, and gives you your result. I think I saw some interesting questions over here, David.
01:10:07.744 - 01:10:53.380, Speaker B: What's the point of reordering t in t prime as well from enabling alternative ordering slide. I will scroll and see if I can find that. Slide. This is one of the slides I skipped. Um, we, we have to sort these vectors, and sorting can be kind of tricky. They need to be sorted in order for this pluckup argument to work. But the prover needs to sort their copy of the table the same way the verifier has their copy of the table.
01:10:53.380 - 01:11:56.760, Speaker B: So you can use like a relative sorting algorithm, or you could also just do this small change to the product argument here. The prover uses this t prime instead, which is their own ordering. It can be whatever order the prover wants. It doesn't have to match the verifier's copy, which is t. And these extra factors show that the prover's ordering is a permutation of the verifier's ordering. So this enables the prover to choose a sorting algorithm of their choice. If you use a relative sorting algorithm, it's a little tricky.
01:11:56.760 - 01:13:02.140, Speaker B: If you take one of any number of efficient sorting algorithms out of the box, you can get down to n log n pretty easily just by tacking on a few extra elements here. We had an issue with the sorting when we first ran through this protocol. The sorting was really destroying our benchmarks. And this allows you to kind of step around that issue. There's a couple of ways to do it. Okay, I think. Are there any questions over the slides before I switch over to the live coding part? I don't see any new questions in q a.
01:13:02.140 - 01:13:59.658, Speaker B: I'll go ahead and switch over now to the live coding. I kind of gave you a preview in some of the later slides. The syntax is going to be slightly different because I'm using a slightly different version of our library. Let's see here. Here we go. Okay, so what I'd like to do here for the live coding portion is show you how to use plonk up to make an XOr table. Xor is a pretty common operation in hash functions.
01:13:59.658 - 01:15:17.442, Speaker B: It's also not that easy to do with regular arithmetic gates main reason is that you have to break up the XoR into a bunch of little pieces, often just bits. So if you have 256 bit element and you want to xor that with some other 256 bit element, well, you have to break that into bits, constrain each bit so that you know it's a bit. So you end up with 512 constraints. Then you have to use a constraint for each Xor, and then you need to collect all of those back together into your output. So because you're working bit by bit, you end up using a ton of constraints. If you use a lookup table, you can work more easily with multiple bits. And what I'm going to do here is make a four bit lookup table.
01:15:17.442 - 01:16:14.200, Speaker B: So each input is going to be four bits, and that'll save a lot of constraints. We're just going to do an xor between two bytes. I wanted to make sure this was not too complicated, or else I'll be making too many mistakes on screen here. So we're going to take a left value, left byte, a right byte, xor them together with our table, produce an outbite. Sorry ayazid, I don't have a repo for you. Okay, so let me explain the different parts here. You've seen some of this already in the slides.
01:16:14.200 - 01:16:54.330, Speaker B: Here's our main n is the size of the circuit. Actually it's bigger than the size of the circuit. It's the next power of two larger than the circuit. This size needs to be a power of two in order to make the FFTs more efficient. So once we have the size of the circuit, we can generate parameters. This is your trusted setup here. So this will generate enough elements in the setup to handle a circuit of size 512.
01:16:54.330 - 01:17:40.724, Speaker B: Next, we have the proverbs viewpoint here. In this closure, the prover is going to generate random bytes here, three random bytes or two random bytes as opposed. The last one is the XOr between the left and the right. These are going to serve as the inputs to the prover circuit. Then the prover can create their prover struct. I've got it tagged at Zkhack workshop. Next, we will create the lookup table with this function.
01:17:40.724 - 01:18:13.120, Speaker B: We're going to fill in that function which was just above. Next, we add the gates to the circuit. We're also going to fill that in. In a function that's above, we create our commit key, which we need to do a proof. We do some preprocessing on the circuit. Usually preprocessing is done once per circuit. Here we're just going to do it on the fly.
01:18:13.120 - 01:19:04.480, Speaker B: Then we also want to make sure that the verifier gets the same public inputs and the same lookup table. So we're going to grab these from the proverbs constraint system and pass that along to the verifier later. And then we'll create our proof. Once we have the proof created, if you look at the beginning of this closure, this will spit out a proof the public inputs and the lookup table. These two are public. Well, all this is public and the verifier will use this information to verify. So the verifier also creates their own verifier.
01:19:04.480 - 01:19:42.284, Speaker B: This verifier needs to have the same exact tag here. Ckheck workshop the verifier appends the lookup table, attaches their circuit. This is going to be the same circuit using the same function. If you notice here, these are the verifiers inputs to the circuit. They can be anything. I just picked some random numbers. I don't think that seven, x or two is one, but I didn't really check.
01:19:42.284 - 01:20:34.130, Speaker B: I just picked some things. Those don't matter actually. So then we compute the commit and verifier key and verifier does their own preprocessing and then we can run this verification here. Okay. And then we'll see at the end if the verifier accepts. So what we really need to do is fill in these two functions, generate Xor lookup table for bit and example circuit. Both approver and verifier use both of these to create the circuit.
01:20:34.130 - 01:21:17.420, Speaker B: Okay, let's see here. I have my cheat guide. Okay, so to generate the table, we'll just use some for loops. These going to run through all four bits. So it's just going to go from zero to 16. I is going to be the values in our first column. J will be the value in our column.
01:21:17.420 - 01:22:02.680, Speaker B: We'll have our composer. This is passed into this function. The standard composer handles creating the circuit. So the composer has a lookup table attached to it. The lookup table is just a wrapper around a vector. So to get to that vector, I'm using the zero here, and we will push a row of values. So our row of values is a row of scalars.
01:22:02.680 - 01:23:12.220, Speaker B: We'll use Bls scalar and use I here we need to pass a U 64, so we'll cast that to 64. 2nd value is from j, you is xor, y and j. This version of the library basically defaults to a table that has four columns. We're only using three columns. So in the last column we'll just give it zero. Okay, and that should do it. This lookup table could be generated by anyone.
01:23:12.220 - 01:24:42.542, Speaker B: It could be generated by the prover, it can be generated by the verifier, it can be published somewhere, downloaded, whatever, as long as prover and verifier agree what the table is supposed to be, xor, then they can generate this table okay, so there's our code that generates our lookup table. Let's continue. Now we need our circuit. So here we will put in the gates and any computations that we need to do in order to clone those gates. So one thing we need to do is take our left, right and out inputs and split them into four bit chunks as our lookup table is based on four bit chunks. So this is just a bit of arithmetic. Um, do this, we'll split in each piece into a high and low part and create a scalar for the high bits.
01:24:42.542 - 01:26:17.100, Speaker B: We'll just take the left input and do an integer division by 16 and we need to cast that as 64 bits integer division. That'll get you the four high bits. If you want. The four low bits do almost the same thing except you switch this to odd. So there we get our high bits and our low bits and this converts them into a scalar. So we'll do this with all three. So it it now our three byte inputs are split into scalars that correspond to their upper and lower bits.
01:26:17.100 - 01:27:56.390, Speaker B: Okay, now we're going to include these as variable circuit composer and add input. Need to do that. Left high that should do it. This will take the scalar and turn it into a variable. I want to be able to refer to the variable later, so I should probably there our scalar input will turn into a variable in the composer and we need to do this with all six pieces. It absurd that I'm typing in front of a bunch of people. I failed typing in school.
01:27:56.390 - 01:29:01.210, Speaker B: It okay. Oh wait a second. Low these need to be changed too. Okay. All right, looks good. There's one more variable we need, which is after we do these x ors, we're going to have the out bits and the out high bits and the out low bits. We want to combine those back together into the out variable here.
01:29:01.210 - 01:30:09.580, Speaker B: So we'll also include u eight. So we will need to convert that into a scalar. Okay, here's all the variables we're using. Next we can add the gates. We're going to use two lookup gates, one for the high parts, one for the low parts. Xor is really nice for this because you can do Xor. You can divide things up into pieces and do Xor on all pieces and then compose them later.
01:30:14.070 - 01:30:14.820, Speaker C: It.
01:30:26.150 - 01:31:30.378, Speaker B: Okay, so we take our composer and do a flick of gate start with our high variables. It as I said, this version defaults to a width four table, but the fourth element may or may not be there. So we need to wrap it in a sum and this needs to be a zero. So there is a zero variable that comes up along with the composer that you can use. In this case, that's really just an empty column we're not using. So put a zero in there. And there's one more input that we need.
01:31:30.378 - 01:32:23.850, Speaker B: This is a public input, which we're not going to use. So we'll just let me label these. This is A-B-C-E. So we are looking up this tuple in the table and then this last one here is public input. You may in some cases use a public input to specify a particular table that you'd like to use. We only have one, so we don't need that. Okay, so this is going to check the Xor relationship between the high variables.
01:32:23.850 - 01:33:20.540, Speaker B: We have to give the out high variable. So we have to compute this ourselves and put it in here, and this gate checks the relationship between them. In this case, it doesn't compute the output for you, although you do have some arithmetic gates that do work like that. All right, and we'll do the same thing with the low side. That's still going to be zero. So I think that's good for our lookup gates. So this checks that the high bits actually xor up to out high and the low bits actually xor up to out low.
01:33:20.540 - 01:34:50.298, Speaker B: But then we need to combine those back together and check them against our out here, which is also capsulated in this variable out VAR. So we need one more gate. This is just going to be an addition gate. We're going to kind of reverse this decomposition that we did up here where we split things into upper and lower. So all we're going to do is take the upper bits for out high bar and multiply them by 16, and that'll give us four zeros in the lower bits. And then we just add out VAR low or outloo VAR to get back to eight bits. So this is an add gate and we're taking the out high bar out low bar and the outbars inputs it.
01:34:50.298 - 01:36:20.950, Speaker B: We want to make sure that our constraint has that 16 times the high bar plus the low bar equals the outfar on the high bar. I'll label all these later, just like I did these today. Our coefficient multiplier on the out VAR, it needs to be one. But because we're trying to constrain these to be equal to one another or all add up to zero, rather we need to make it negative. Then we have two more failures that we can put in, but we're not using these really? So just zero and. Yeah, let me label these for you so you can see what they are. And then these are selectors.
01:36:20.950 - 01:36:52.960, Speaker B: This is the QL selector that modifies a QR selector, which modifies B. Nothing much modifying, because it's one. Then we have the QL selector. No, I already did that. Q o selector. This is a constant. You see, most of our gates, you can add in a constant as well.
01:36:52.960 - 01:37:43.422, Speaker B: And this is public input. David asked, what's the D here? This particular style of gate is written to just have three inputs, so it just automatically puts in zeros for D. We have multiple versions of these arithmetic gates. Some allow you to set every single variable in that long constraint. Some are more focused. So if you only need three inputs, you can just use the three input version. Yeah.
01:37:43.422 - 01:38:36.090, Speaker B: D is the fourth one. David asked, okay, let's see. Will that do it? I think that'll do it. So we just have three gates in our circuit, two lookup gates, which compute the XoR, and one addition gate which repacks those four bit chunks back together into an eight bit byte. Okay, so if I've done this right, and I haven't made any mistakes, we should be able to run this. We'll see. Let's.
01:38:38.070 - 01:38:38.820, Speaker C: It.
01:38:42.870 - 01:39:07.770, Speaker B: Plug up cargo and release. And. Hey, can you see that? Yeah. Worked. So it goes through the steps, generating parameters. That's the trusted setup. Preprocessing then creates the proof.
01:39:07.770 - 01:39:46.148, Speaker B: The verifier does their own preprocessing, verifier does their verification, and proof is accepted. So the proof was accepted because our inputs really did correspond to Xor's and the gates here. Here's where we got our inputs left and rights were random. But then we did the xor between left and right to get her out. It should work. And it did. But what if I mess this up.
01:39:46.154 - 01:39:46.950, Speaker A: A little bit.
01:39:50.120 - 01:40:33.930, Speaker B: Xor, and just add a plus one? Then this should not verify, actually, you'll see what happens. Okay, now, oh, make sure save it first before we compile. Okay, so what happened here is we had a panic. We tried to unwrap error value. Said element is not indexed. This happened while we were creating the proof. So what happened here is the prover.
01:40:33.930 - 01:40:59.010, Speaker B: In our implementation, the prover checks as they're creating the proof that their lookup queries are well formed and actually exist in the table. So this is the prover saying that they're not going to create a proof, they're going to quit because they know it's not possible, because one of these values was not in the table.
01:41:02.950 - 01:41:03.266, Speaker A: It.
01:41:03.288 - 01:42:07.170, Speaker B: Is theoretically possible for a prover to ignore this, try to create a proof. Anyway, if that would have happened, instead of getting an error value here, the proof would just have failed to verify. So this stopped actually before that verification actually took place. Another way I could mess this up is mess with these scalars here. So if I put two instead of one, save our inputs should not satisfy this gate. The Xors will be done correctly, but when packing them back together to make the out byte, it shouldn't work because the lower bytes are going to be multiplied by two. So this should not verify.
01:42:07.170 - 01:43:30.250, Speaker B: Let's check it out. So now you can see the proof was rejected because our inputs did not satisfy this last gate back. All right, in the interest of time, I kind of skipped past some things that you probably would want to do if you were actually designing a circuit. I only ever checked that the out byte is formed from those upper and lower bits here. With this last ad gate, the inputs left and right were never checked. So this decomposition where they're split into high and low, this is never checked in this circuit. If you were really doing this, you probably would want to constrain those or check that the left inputs actually correspond to the high and low that we say they do here.
01:43:30.250 - 01:44:36.690, Speaker B: This computation is not done in the circuit. This is just the proverzone. Like private computations, they can do however they want, so there's nothing forcing them to actually decompose these inputs correctly. Only the outputs are constrained with that last gate. But this actually kind of corresponds to a lot of normal use cases, because often you have inputs to a circuit or a portion of a circuit that are outputs from earlier parts, and so they might be constrained in earlier sections. So I kind of was thinking of that scenario when I was practicing this example circuit, but it wouldn't be too hard. I'm not going to do it now, but it wouldn't be too hard to add checks on left and right the same way that we have them here for out, just by copying and pasting this, all I'd have to do is change the out to lucky red should work.
01:44:36.690 - 01:45:57.800, Speaker B: Okay, that's it. Are there any questions over any of this live coding part? Any questions about the gates, or questions about why I did certain things? Joseph says, great demo. Thanks. You're welcome. A little bit more for questions in case someone is thinking hard and typing question, I'll just say some more remarks on this repo. The one that I'm using for this exhibit is a little bit out of date. It was the easiest just to get up and running for this exhibit, but our best version of this is located at.
01:45:57.800 - 01:46:38.850, Speaker B: It's called Arc plonk, A-R-K plonk, and it uses the arcworks backend. If you've been doing some of the ZK hacks, you've probably already seen arcworks and know a little bit about it. I think there was a presentation on it. So, yeah, we're porting this library over to arc plonk, and I'll just give you the tab here. Rust. Zkp. Arcplunk.
01:46:38.850 - 01:46:47.220, Speaker B: The library is not complete yet. We're working on it, but it's actually a little nicer than this one that I'm using here.
01:46:50.280 - 01:46:50.770, Speaker C: Very cool.
