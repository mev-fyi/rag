00:00:00.490 - 00:00:37.206, Speaker A: All right, so this is the second edition of the Polynomial Commitments DK study Club session with Justin Drake. Today, we're going to actually continue on from what Justin had started last two weeks ago. I think, if you haven't already, it's probably good to watch that video. I think the plan for today is to go very quickly through those slides and then launch into the second part. And I think we'll leave it up again to time. So, Justin, if you wrap up quickly, then we can end an hour. If you don't, maybe we can schedule something for the new year or over the holidays, but.
00:00:37.206 - 00:00:38.520, Speaker A: Yeah, welcome again.
00:00:39.130 - 00:00:54.282, Speaker B: Okay, great. Thank you. Yeah. So I'll just quickly go through what we covered last time very quickly, and then slow down where we live. Any chance you could share the link to the slide? Yes, can you share it in the table?
00:00:54.346 - 00:00:58.942, Speaker A: Sure, I'll do that. Thank you. It's the same slides as last time, right?
00:00:59.076 - 00:02:07.378, Speaker B: Yes. Great. Okay, so polynomial commitments, they're like this really cool cryptographic primitive, super useful in the context of universal Snox, which is kind of the new hot way of building Snox. Oops. So, basically, I started with giving some context, including historical context, on when these polynomial commitments were first defined. The first construction was by Catiano in 2010, and then we saw kind of bulletproofs and fry kind of give polynomial commitments, but kind of hidden. And then we have this IOP and polynomial commitments kind of framework, and we've had lots of innovation on the IoP side of things, and now we have this kind of explosion, partly because we better understand, I think, kind of the separation of concerns between the cryptography, the polynomial commitment, and the IOP.
00:02:07.378 - 00:03:18.586, Speaker B: So I think this is a really nice framework to innovate on in the future. So I kind of briefly gave the definition of polynomial oracle and the polynomial commitment scheme. And the idea of an oracle is kind of an abstraction which kind of tries to encode a polynomial, which could be extremely large, with lots of coefficients, into just a single object, which can be queried. So you can make a query which will be asking for the evaluation of that polynomial at a point. And then we have the equivalent commitment scheme on the cryptography side of things, and then we have some formal definitions for cryptographers and by cryptographers, and then this is kind of the framework that was presented. So we have kind of three buckets. We have kind of the computer science bucket, where there's a lot of compiling going on.
00:03:18.586 - 00:03:55.910, Speaker B: You have code, and then you compile it into programs, ram programs, or circuits, and then you encode these into polynomials and things like that. And then you have this information theory land, where you have the interactive oracle proof as kind of the meat of your proof scheme. And then you have the polynomial commitment scheme here. And then at the end, when you mix all this, you get a universal snark, which is very nice. And. Yeah, like different flavors of soundness. We go from perfect soundness to statistical soundness, and then computational soundness.
00:03:55.910 - 00:05:13.050, Speaker B: And then kind of, if we look at the different oracle types that you can instantiate, which are alternatives to polynomial oracles, you can think of set oracles or vector oracles or low degree test oracles, or even inner product oracles. And kind of, there's this hierarchy because they have different kind of power, and they're kind of related to each other locally and kind of historically. The way we've built snarks is with this inner product argument, which is kind of very costly because you either have to pay on the verifier time, so you lose succinctness, for example, in the case of bulletproofs, or you have to pay with a trusted setup, which is not universal. And so we've kind of dialed it down to just this point commitment scheme, which kind of reduces the expressiveness of the PCP, but is still powerful enough to do everything we want to do. That's kind of great news here. So the canvas sweet spot is the commitment here. And then I kind of gave an overview of.
00:05:13.050 - 00:06:16.434, Speaker B: Okay, what are these schemes in practice? So, we have kind of four flavors, one which is basically based on Fry, which is hash functions. And here, the way that you commit is you evaluate your polynomial over a domain, and then you take the Merkel route of these evaluations, and that's your commitment. You have pairing groups where you have this kind of powers of tau. So you encode a secret, and then basically, you evaluate your polynomial at that secret point in the exponent. And then you have groups of unknown order, where here you do something very similar, except that there's no secret. And then we have the final flavor. My slides will load, which is based on the discrete log, which is also very similar to the previous two.
00:06:16.434 - 00:06:56.126, Speaker B: And these three are kind of very algebraic, whereas this one is not so algebraic. I mean, it does use algebra a little bit because it's based on the roots of unity. So you can do clever tricks, but it's very much an information theoretic construction, this one. And then we kind of saw kind of the benefits and the downsides. And basically, there's a very large trade off space that you can pick from. So you can have transparency, you can have post quantum, you can have an onbound setup, all these things, very different constructions. And even within the groups of unknown order, we kind of have these two flavors.
00:06:56.126 - 00:07:56.790, Speaker B: We have the RSA group and the class group. And then in addition to kind of comparing setups in terms of their qualitative properties, we can also look at them in terms of their quantitatively, like what are their performance? And there's also kind of a wider trade off space. And this is kind of where we left it last time. And one of the comments that was made here was that this is a little unfair because we're basically hiding the security parameter in various places. And so it means that we're not really comparing kind of apples to oranges. So for example, on the proverb time and the verified time, these are different operations. These are multi exponentiations inside the group, and these are hash operations.
00:07:56.790 - 00:09:14.970, Speaker B: So I just wanted to give a little bit of flavor regarding this security parameter. And so what I'm going to do is basically give you the breakdown for the commitment. I mentioned that basically all these commitment polynomial commitment schemes have a constant size commitment. But actually in practice, the commitment has a different size. For example, in Fry, your commitment will be just one hash function output, whereas for all the other ones, basically your commitment is going to be one group element, and these groups are going to have different sizes. And basically for the algebraic constructions, the size of the group is going to contain these relatively large powers of lambda. So you have lambda to power three, lambda to power three and lambda to power two.
00:09:14.970 - 00:10:28.950, Speaker B: And, you know, relative, they're large relative to, for example, the hash function here. And the reason is that you have these kind of sub exponential attacks here. So they're kind of algebraic attacks which are sieve based. So you have the general number field sieve algorithm, for example, that is used. And this is kind of interesting to see that there is some correlation between the asymptotes of the attacks and kind of the actual size of your group elements. So, for example, if you look at RSA groups and class groups, they're both groups of unknown order, but there's kind of different attacks that are known here. And so this will mean that for RSA, if you're targeting 128 bits of security, you actually have a larger group size, 3000 bits versus a class group, which is going to be only 1800 bits.
00:10:28.950 - 00:11:51.070, Speaker B: And these discrepancies between the various commitment sizes will also reflect in the proof size. So basically you have a factor of, let's say ten difference between a shot 56 output and a 3000 bit RSA group element. And this is something that I'm sweeping under the rug when you consider here and here the proof size. So you're going to have something similar. And this table here also kind of swoops under the hug rug a bunch of detail regarding the verifier time and the prover time. So, for example, here the pairing will have a bunch of lambda terms, which in practice will actually mean that you're comparable to fry, for example, which has a log d squared. One way to simplify and compare these is just to set kind of log d to be equal to lambda.
00:11:51.070 - 00:12:38.400, Speaker B: And then you have a way to compare them kind of apple to apple on a bit level operation standpoint. Okay, so that was just kind of an addendum to the previous talk, just to give a little bit of flavor as to what's going on with the lambda terms here. And what I like to talk about now is kind of actual mechanics of how the schemes work. So I've told you how the commitment works, but I haven't told you how you actually do the openings. So any questions so far.
00:12:44.050 - 00:13:01.190, Speaker A: Seems good. Also, just to note anyone who's kind of new to this, if you do have a question kind of like and don't want to interrupt, you can also put the note, you can put it in the notes we're sharing, sort of. There's a chat here and we can collect those for the next time. Justin takes pause.
00:13:03.610 - 00:14:08.730, Speaker B: Okay, great. So let's look into how these schemes actually work. And you'll notice that three of these schemes, the fry, dark and bulletproof, they have these lock terms. And this is not a coincidence. And the reason is that the way they're built is you have these logged number of rounds of interaction between the prover and the verifier, and every round of interaction will have kind of a constant size overhead, at least constant size overhead in the proof size. What I'll do actually, first is before giving you the full polynomial commitment scheme, I'll give you something slightly weaker, which is the low degree test for all the flavors. So you have your prover, you have your verifier, and they're talking to each other in these three schemes.
00:14:08.730 - 00:15:15.410, Speaker B: And there's a log number of rounds. So these, the degree of your polynomial, you have log d round, and in every round, the verifier is just sending a random challenge. So it's a public coin protocol which can be made non interactive with fiat chamir and basically, the way that the low degree test works is that you start with the commitment which the prover already sent, and then there's some sort of reduction step. So to go from f of zero to f of one, you basically reduce. And this reduction step is seeded by the randomness that was given by the verifier. And so the point of this reduction step is that you go from a polynomial of degree d to one of degree d over two. So you keep having and having and having, and after log d steps, you end up with a constant polynomial.
00:15:15.410 - 00:16:23.270, Speaker B: And kind of constant polynomials are very easy to work with. And the verifier can check that everything is consistent at this lower level. So you have this constant polynomial check, and then the verifier will kind of work itself all the way back. So if this is constant, if f, sub log d, is constant, then the one just before is of degree two, or actually maybe one, and then two, and then four and then eight and 16. And so that way you get your low degree test. You can prove that while the verifier can be convinced that indeed this polynomial f of zero was small, it turns out that this framework can also be used with pairings. So all four flavors of commitment schemes that we know can be made to fall into this framework.
00:16:23.270 - 00:17:13.490, Speaker B: It's just that for pairings, we have this extra power that means that we don't even need to do all this interaction. We can just do it in one go. So you'll get even shorter proofs of pairings. But if you want to, you can go through this framework. So what I need to tell you now, so, you already know how the commitments work, and you have this framework. So all I need to tell you, really, is this reduce function. What is the reduce function, and kind of, what are the consistency checks? And it turns out that there's only so many ways that you can take a polynomial of degree d and split it and kind of reduce it into a polynomial of half the degree.
00:17:13.490 - 00:18:38.800, Speaker B: And so you kind of have this even OD decomposition, which is kind of the Fourier decomposition. So you take the even powers, you take the OD powers, and then each of even and OD are individually half the degree. Or you can think of the left right decomposition, where you take kind of the coefficients for the small monomials and then those for the large monomials, half and half. And then you also get kind of two polynomials of degree n over two. And so, basically, each scheme uses a slightly different variance on the decomposition. So, for example, Fry will use the even and od decomposition. I mean, it turns out you can also use the left and right decomposition, but you get something slightly less efficient because you can't use the same kind of Fourier like algebraic tricks to basically shrink by half the domain every time.
00:18:38.800 - 00:19:13.962, Speaker B: But, yeah, that's the technicality. So they use even or not. And then you have groups of unknown order, which kind of use left and right. But it turns out here that you could also use even and Od, and the scheme would work just as fine. Actually, just to back up here, the very standard way of combining things in cryptography is just to take a random linear combination. And so this is what we're doing here. R is the randomness.
00:19:13.962 - 00:20:16.670, Speaker B: And so we're taking a random linear combination of these two things. So the r is going to be the randomness from the verify. And then in bulletproofs, we do something kind of like the groups of unknown order. So we split in left and right, but we do something, two things that are a little different. So one is that instead of taking just this unbalanced linear combination, where basically you have a factor of one and a factor of r here, you have this more kind of balanced linear combination, where you take r here and r inverse. So there's kind of this nice symmetry. And in addition to doing a decomposition at the level of the coefficients of your polynomial, you're also going to do a random linear combination, a similar one, for the basis elements with which you do the commitment.
00:20:16.670 - 00:20:55.594, Speaker B: So it's kind of a slightly trickier scheme because it's just a slightly more complicated reduction. But if you want kind of the universal reduction, that would work for all schemes, kind of optimally, it would be r times even. So r times even plus r inverse times r. Right. So basically, it turns out that bulletproofs, you don't need to use left and right. You could also use even and odd. And in these two schemes, it's perfectly fine.
00:20:55.594 - 00:21:39.180, Speaker B: It's, it's balanced. But it turns out that the way that these papers were written were not in the context of this more general framework. But it turns out that there's this one kind of universal reduction function you can use, and it would work for all the schemes, which is kind of nice. Any questions so far? So, we have this framework where we have commit, reduce, commit, reduce every time. The commitment is of a function that reduces in size exponentially. You have a log number of rounds, and then you have these consistency checks, which I haven't gone into yet.
00:21:40.430 - 00:21:51.822, Speaker A: I have a question about the coefficients and basis can you maybe explain a little bit more what those two things are? Because I think you just sort of threw them in. But I'm not familiar with that.
00:21:51.956 - 00:22:46.642, Speaker B: Yes. Okay, so that actually goes back to an old slide, this one here. So, the way that you do the commitment in bulletproofs is you have the coefficients of your polynomial a zero up to ad, and you need to do it relative to a basis. So basically, you have these random elements, g zero to gd, and they're independent from the discrete lock perspective, there's no discrete lock relationships between them. And so you can think of them as being basis vectors in this vector space. And so your commitment will be just this one point in the vector space. And it's kind of very similar to this.
00:22:46.642 - 00:23:38.270, Speaker B: So you can think of q to the I times g as being kind of your basis element. And you can also think of the same thing here for the pairing. So we call it s to the power I times g one g one being the generator of your big g one group as also being the basis element. So, yeah, these three schemes are very similar to each other. And end, it turns out that the way you do the reduction, you don't have to mess around with the basis elements here. So you just keep using s to the power I g one and you keep on using q to the power I g. But for bulletproofs, you need to be cleverer.
00:23:38.270 - 00:24:37.446, Speaker B: And the reason is that when you work with a discrete log group, it's just less powerful, it's less flexible. So you have to do a bit more gymnastics. It. Okay, so now I need to kind of go into the consistency checks. And this is where the schemes kind of become slightly different. So, in the case of fry, this is going to be your consistency check. And it kind of looks a little bit strange, but basically, if you take the two to the z and you divide it out everywhere, I just put two to the z on this side.
00:24:37.446 - 00:26:00.270, Speaker B: Then it turns out that this part is going to be the even part and this part is going to be the OD part. So you have this evaluation point, z, which was kind of chosen at the very end of the protocol. That's a random evaluation point by the verifier. And it turns out that you can kind of check consistency of f I of Z-F-I of minus z, and f I plus one of z squared. So if we kind of go back to this even OD decomposition, if you feed z here on the left hand side, you're going to get z squared here and z squared here and if you feed minus z, while the minus sign is going to go away, so you still get z squared here and z squared here, what changes is that you get a minus sign here. So you have kind of two linear equations with two unknowns, and so you can kind of solve it. And it turns out that you can basically solve for the even part and for the OD part separately.
00:26:00.270 - 00:26:51.470, Speaker B: And this will be the even part, this will be the OD part, and this will be your z squared. And for every opening that you have here, you have a corresponding Merkel path. So if I go back to this picture here, every commitment here is a single Merkel route. So you commit to merkel route. Commit, commit, commit. And these commitments are going to be commitments to evaluations over domain. And so you can just explain why the square, where does the square go? So the square comes from here.
00:26:51.470 - 00:28:09.250, Speaker B: So basically, even the relationship between f, even part of f, and od, part of f, you have this square part here. What creates this? Sorry, what? What creates this? What creates this? Okay, so let's say that you have a polynomial and you want to take the even coefficients, and you want to take the OD coefficients. Now, if you just take the even coefficients and the OD ones, then these polynomials are still going to be of the same degree as d. So basically, if your polynomial is going to be, let's say, x to the power four plus x squared, and you take the even part, you're still going to have x to the power four plus x squared, you haven't reduced the degree. So what you need to do is you actually need to consider the left part as a polynomial in x squared. So as a polynomial in x squared, you're going to have x squared. Squared plus x squared.
00:28:09.250 - 00:28:57.560, Speaker B: And then basically, if you want to consider it as a polynomial in a new variable, y, you're going to basically have a y squared plus y. Does that make sense? The reason you get the square is because once you do the decomposition into even and odd parts, you can kind of collapse all the squares into a new variable, y, where y is equal to x squared. Cool. And then once you collapse, do this collapse, this collapse, you basically have a polynomial of degree. Half the size. Yeah. So if you just work through an example, it'll become very clear.
00:28:57.560 - 00:29:36.838, Speaker B: And in order to do the collapse on the OD part, you have to do a little bit more work. Basically, you need to extract an x coefficient because you have all odd powers in the monomial. So you extract one x, and so you're left with all evens. And so there you can again kind of collapse the even ones. Into a new variable, y. Where y is equal to x squared. Okay, one more question about the next slide, actually.
00:29:36.838 - 00:29:55.340, Speaker B: Is the consistency check. Is it the same for fry versus deep fry. Or is it different for deep fry, do you know? Yeah. So, deep fry, from what I understand. The new idea is that this point, z. Is outside of the domain. It's kind of outside of the box.
00:29:55.340 - 00:30:18.690, Speaker B: And so here in Fry, you choose z to be within your evaluation domain. And what that means is that you can have a direct opening. Proof. You have this Merkel route. And you have the evaluations. And you can just open them by providing the Merkel path. Once you evaluate outside of the domain.
00:30:18.690 - 00:30:47.530, Speaker B: It kind of gives you more flexibility as a verifier. Because now, as a verifier, you can check anywhere in the whole field. But now you're putting more burden on the prover. To convince you that the opening is the correct one. And actually don't know exactly how deepry works. Does anyone on the call know how a verifier would authenticate. The claimed openings.
00:30:47.530 - 00:30:52.010, Speaker B: Outside of the box. In the context of deepry?
00:31:00.280 - 00:31:02.790, Speaker A: So what I do know is that good to know?
00:31:05.880 - 00:31:13.160, Speaker B: Yeah. I mean, it's on my to do list to read deep Fry and understand it. But what I do know is that you get a better soundness.
00:31:16.780 - 00:31:50.980, Speaker C: So this is like a half guess. But I think one way to do it is they think of the polynomial. Like f minus f of Z divided by X minus Z. And now if you have the evaluation of f on a limited domain. That doesn't include z. You also have the evaluation of this thing, right. On a limited domain.
00:31:50.980 - 00:32:17.500, Speaker C: And you're sort of using it like a polynomial commitment scheme. And now your evaluations of f on a limited domain. Give you evaluations on the limited domain of this thing. And you can use that to prove what f of z is. That's my half guess from things I heard in talks.
00:32:18.160 - 00:32:32.588, Speaker B: Yeah. So I think that makes a lot of sense. Because this trick that you just talked about. Where you consider f of x minus f of z. Divided by x minus z. This is something I will talk about next. And it's how you kind of take fry.
00:32:32.588 - 00:32:58.330, Speaker B: And make it into a polynomial commitment scheme. And this was kind of noticed by the matter labs people. And they kind of noticed it after reading the deep fry paper. Because the deep fry paper kind of uses this exact same lemma. But in a different context. And I guess this different context is evaluations outside of the box. Okay, that makes a lot of sense.
00:32:58.330 - 00:33:38.868, Speaker B: So, if you're slightly more clever, you don't have to pick z, which is within your domain. You have this trick, which I will talk about in a few slides. Thank you. So, just to recap, this consistency check is basically encodes this equation here. There's nothing other than this equation. And the purple bits will be part of the proof. So there'll be proof elements.
00:33:38.868 - 00:34:18.608, Speaker B: So you have a log number of these because you have log number of rounds. But the reason why you have this two factors of log in Fry is because each one of these elements also comes with a Merkel path itself of log size. Okay, so now let me go to docs. So, if you recall the commitment scheme. I'll just say it again for docs. So you have this group of unknown order. And then you pick one generator.
00:34:18.608 - 00:35:10.736, Speaker B: And then you take this generator times f of q, where q is a very large integer. So, basically, you take your polynomial, you encode it into an integer. And then you commit to the integer in the group of unknown order. And this equation here, basically is the exact kind of analog of this equation here. Or maybe. Yeah, this one. So, basically, you're going to have this sum.
00:35:10.736 - 00:35:56.020, Speaker B: And you're going to have the multiplication by a shift. And so, multiplying by q in your commitment is the same as multiplying by x in the polynomial land, in the uncommitted land. And it turns out that if you take the sum of the commitments, you get the commitments of the sum. So you have this nice linear property, linear homomorphism. And so you can just directly work with the commitments. And it's all simple. And the ri here is going to be the random linear combination.
00:35:56.020 - 00:36:33.424, Speaker B: So again, just to recap, you have a polynomial f, which has a certain degree. You break it up into two smaller coefficients, even and odd. You take a random linear combination here. And then you do the consistency check. And here you have this nice kind of superpower of the commitment scheme for darks, which is. It's kind of fully homomorphic in the sense that in addition to being additively homomorphic, you have this shift homomorphism. So you can multiply by x.
00:36:33.424 - 00:36:44.050, Speaker B: And multiplication by x is the same as multiplication by q in commitment land. Does that make sense?
00:36:44.660 - 00:36:51.860, Speaker A: There's a little question in the chat if you want to take a look from Ivan, and also Ivan, if you want to say it.
00:36:52.010 - 00:37:28.850, Speaker B: Yeah, sure. So I might be mistaken here, but f I plus one should be fi after the reduction, right? So shouldn't this be in the reverse order in the groups of unknown order. Yes, it does look like I messed up my I's and I plus ones. Yes. All right, thank you. Yeah. And Giorgio is asking is that the order should be unknown and big.
00:37:28.850 - 00:38:52.250, Speaker B: So you kind of have two kind of big enough things here. So first is you have the group of a known order in the basic kind of construction of your group, and that just needs to be big enough and kind of just difficult to compute the group of a known order. But there's another big enough that comes in, which is this q. And it turns out that in order to do, to apply the homomorphism, so to apply the addition homomorphism and to apply the shift homomorphism, you need to make sure that the encodings of your polynomials don't overflow. So you have, if you consider kind of polynomials when you manipulate with them, the coefficient of every monomial is independent and will not interact with the other ones. But when you work with integer encodings of polynomials, then it becomes different because integers, when you multiply them, you have, you have carries. So in order to avoid the carries, you just need to pick a large q.
00:38:52.250 - 00:40:00.080, Speaker B: And so what you're going to do in docs is when you reach the end here and you do the constant polynomial check, you're going to do an extra check. Not only are you going to check it's constant, but you're also going to check that it's a small constant. And if you have a small constant here, that means you have a polynomial here with small enough coefficients. And you go up and up and up every time the coefficient can go larger and larger and larger. But you have kind of a very controlled bound on how large these coefficients can be. And they're basically small enough that you can do apply all the homomorphisms that you need for the consistency checks. And kind of the final one, which falls under this framework of a log number of rounds, is kind of bulletproofs.
00:40:00.080 - 00:41:14.570, Speaker B: And here what you do is that you check that the commitment of f I and f I plus one are consistent. And then you have these extra terms here. And these extra terms kind of correspond to kind of cross products. Remember, you have basis elements and then you have the coefficients and it turns out that you kind of need to cancel off these cross products for things to work out nicely. So you have these residual terms. To me, bulletproofs is kind of the most confusing of all the schemes, like the fry and dark, even though this looks a bit dry, actually it's very, very natural here. There's a little bit of cleverness involved to come up with this, but, yeah, it works.
00:41:14.570 - 00:42:00.312, Speaker B: Okay, so now, I've kind of told you the low degree test, but the low degree is not sufficient. You actually want a polynomial commitment scheme. And so, in addition to knowing that something is a polynomial, it's kind of low degree. You also want to know what is the precise opening of f at z. So you want to know this f of z. And as was mentioned, basically, you have this trick for fry where it's kind of the quotient trick, and it's used in various places. It's kind of a neat trick to keep in mind.
00:42:00.312 - 00:42:45.536, Speaker B: So, if, basically, f of z will be the valuation of f at z. If and only if f of x minus f of z is divisible by x minus z. So if you plug in z in both the left and right hand side, you get f of z minus f of z. So that's zero. And here you also get zero. And so you have this quotient here. And so, generally, the trick involves the prover kind of calculating this quotient and then convincing that this is low degree.
00:42:45.536 - 00:43:06.088, Speaker B: And so this is exactly what fry does. So instead of showing that f is low degree. Well, first it shows that f is low degree. Okay, fine. And now we want to know the opening. And so we want to open outside of the box. We want to open outside of the domain.
00:43:06.088 - 00:43:57.884, Speaker B: And so how do we do that? Well, we just show that this is low degree, because this is low degree. If and only if f of z is the correct opening. There's a little technicality that you need to be within the unique decoding radius, but that's all there is. So, basically, fry very easily kind of elevates into a polynomial commitment scheme. And this is what was noticed by the matter labs team. And then it turns out that this exact same trick is used in the cattle scheme. In this cattle scheme, you can forget all the gymnastics that you have to do with the proof and the verifier talking to each other.
00:43:57.884 - 00:44:34.970, Speaker B: You can just do a single kind of direct proof. And what you do is that you basically check this equation in the exponent. Basically, we're using pairings. So the commitment of f minus f of. So the verifier is already given the commitment of f. The prover is gonna commit, is going to compute q and commit to that. And this equation holding is basically if and only if.
00:44:34.970 - 00:44:57.984, Speaker B: This, times this is equal to this, times this. So you can kind of think of pairings as products. So it multiplies the left hand side with the right hand side. So what is the left hand side here is going to be f minus f of z. That's good. And then what is the right hand side? This is just the generator. G two.
00:44:57.984 - 00:45:24.104, Speaker B: So there's a little one which is hidden. So it's this times one, which is just. This is it equal to q, which is this times the right hand side. And the right hand side is going to be s minus z. S is going to be your secret point in the powers of tau. And so the s here will correspond to the x. Right.
00:45:24.104 - 00:46:15.316, Speaker B: So this pairing check kind of immediately checks this. And so immediately proves that the valuation of f at z is f of d. Does this make sense? So, I mean, one. One reasonable. Sorry. Yeah, I have a question. Could you return to the previous slide? Could you repeat, where is the even and OD side in fry and why? Okay, so I wish I had the kind of whiteboard, but this would be the even part, and this would be the Od part, or vice versa.
00:46:15.316 - 00:46:40.080, Speaker B: But I'm relatively sure this is the even. And this is the od part. Okay, so let's have a look. So let's look at f of f of z plus f of minus z. What is that equal to? Wow. So we're going to take this equation and imagine that you duplicate it. So you have f of z here, and then you have f of minus z.
00:46:40.080 - 00:47:04.232, Speaker B: And basically, this even part here is going to be exactly the same. Right. Because z and minus z squared, they're the same thing. Right. So this part and this part, this imaginary part below are going to be the same. Okay, now let's look at the right part here. The Od part here is going to be of z.
00:47:04.232 - 00:47:37.216, Speaker B: And minus z is also going to be the same, because you have this squared here. But here you don't have a square. Right? So really what you're going to have is you're going to have a minus. This part here, right? So you have f of z and f of minus z. What if you add the two? You add these two. So you have this line here and the imaginary new line here with a minus instead of a plus. What if you add these two? Well, the plus and the minus are going to cancel, right? This od part is going to disappear.
00:47:37.216 - 00:48:10.028, Speaker B: And so you're going to get two times the even part. Right? So basically, f of x, f of z plus f of minus z is going to be equal to two times the even part of f evaluated at z squared. Thanks. I get that. That's that basically. Does that make sense? Yeah. You also multiply by r.
00:48:10.028 - 00:48:32.484, Speaker B: The Od part, right? Yes. That is the randomness in the linear. So it's here. Yeah. So far you don't have it. So far, there's no randomness involved here. It comes in here when you want to do the reduction, because basically you have two polynomials, each of degree half.
00:48:32.484 - 00:49:06.930, Speaker B: And if you keep on kind of taking one polynomial and splitting it into two, then you kind of have an exponential blow up in the number of polynomials. So what you want to do is you want to simultaneously keep the number of polynomials that you have constrained, and in this case, we have one polynomial at every step. But you also want the degree to fall down. And the way you achieve that is you do the decompose and then reduce with a random linear combination. All right. Thank you. Yeah.
00:49:06.930 - 00:49:12.450, Speaker B: So this is why I call it the decompose reduce kind of framework, I guess.
00:49:14.740 - 00:49:19.364, Speaker A: Actually, just so you know, we have ten minutes till the hours up.
00:49:19.562 - 00:49:20.310, Speaker B: Okay.
00:49:21.320 - 00:49:23.828, Speaker A: I don't know if there's any planning, but.
00:49:23.914 - 00:49:27.120, Speaker B: Cool, right? So, I mean, I think we can finish part three.
00:49:27.290 - 00:49:28.010, Speaker A: Good.
00:49:28.700 - 00:50:21.000, Speaker B: There'll be kind of an optional part four, which people might be interested in. Okay. Yeah. So this is how you do the openings for Fry and for Catt. And you use this really cool trick. And I guess one natural question would be, okay, why does that trick not work for docs? Why can't I get a single direct proof like pairings? Because after all kind of groups of unknown order, they have all these nice kind of homomorphic properties. Why can't I use them? And basically, the problem is that you need Q to have small coefficients.
00:50:21.000 - 00:51:07.892, Speaker B: And it turns out that there's kind of two problems here that arise. One is, if you do find such a q, well, you still need to prove that it has small coefficients. And the only way that I know of is, basically that is efficient, is to do this log number of rounds. So you'd still have to pay. But actually, there's another problem, which is that if you take the quotient of. If you take this part divided by X minus z, what you'll get is a polynomial with random looking coefficients. And so with very high probability, these coefficients won't even be small enough.
00:51:07.892 - 00:51:34.800, Speaker B: They won't be small. They'll just be random. Some of them will be large. And so you actually have a completeness problem. It's not just a soundness problem, but a completeness in the sense that you can't even prove this thing because Q is an object that you can't work with, you can't apply your homomorphisms to. Yeah, so that's why you don't have this trick. Apply for docs.
00:51:34.800 - 00:52:57.370, Speaker B: Okay, so now I need to tell you about how do you do the actual openings for the other two schemes. For docs and bulletproofs we kind of have the low degree test, but we want to go a bit further than that. And the way that it works for docs is that it's basically in line with the logarithmic number of rounds. So in addition to communicating commitments, you're also going to communicate evaluation points. And in the exact same way that you have a consistency check on the commitments, you can have the same similar consistency check on the evaluation points. So basically, let's say that I'm interested in f evaluated at z, the prover, in addition to giving me the commitments of the even part and the commitments to the OD part, he will also give me the even part evaluated at z and the OD part evaluated at z. And I can just check, is f of z equal to the even part at z plus the randomness times the OD part at z.
00:52:57.370 - 00:53:53.628, Speaker B: Yeah, so actually there's f of I plus one here, and then f of I on the right hand side or vice versa. So it's kind of an inline thing that you do very similar here. And for bulletproofs, it turns out that the scheme can be naturally thought of as an inner product argument. And so what you do is that you're going to take the inner product of f, the coefficients of f, and the powers of z. This should say powers of z, not powers of x. And if you think about it, so the powers of z will be z to the power zero, z to the power one all the way up to z to the power d. The coefficients of a will be a zero all the way up to ad.
00:53:53.628 - 00:54:33.720, Speaker B: And if you take the inner product of that, that's just the same thing as f evaluated at x, f evaluated at z. That's why the inner product argument is kind of strictly more powerful than a ponymore commitment scheme, because it is just a very special case where the right hand side turns out to be the powers of the point that you want to evaluate at. So that's how you would do the opening if you were to use a bulletproof based perennial commitment.
00:54:35.020 - 00:54:45.470, Speaker C: And in bulletproof, is it like the same efficiency to open as a polynomial and a general inner product.
00:54:46.080 - 00:56:06.212, Speaker B: So it turns out that as far as I can tell, the prover time is exactly the same, is basically the same for commitments and for openings. And that's why in my table I kind of talk here about just prover time. But actually there's two prover times, right? There's the time to commit and the time to open. And the reason they're the same, in the context of these logarithmic arguments, is that here you commit to f, and then you commit something which is half of x. So that's going to take half the amount of work, and then you commit to something a quarter of the amount of work. And so if you add up all the work to do all these commitments, you get basically twice the amount of work, all the work to do. The commitments below here are equivalent to the amount of work to do, the original commitments at the very beginning here in every scheme, because you're reusing the commitments internally, and these commitments become smaller and smaller, and their total work is the same as this.
00:56:06.212 - 00:57:31.520, Speaker B: So the very top one is what you'd call the master commit, and then all the other ones would be the openings, but it turns out they're kind of equivalent. And there was kind of something that I discovered only a few days ago, or I guess a few weeks ago, the same thing happens for Cate. It's kind of obvious that for cate, the commitment and the opening have the same cost if you work in the monomial basis. But it's not obvious if you have something in the Lagrange basis. And the reason is that your powers of tau is given to you in the monomial basis. And so what you do traditionally is you take your polynomial in the Lagrange basis, you do an FFT to go in the monomial basis, and then you do the multi exponentiation. But it turns out that this trick, which it seems like no one is doing it, but it's kind of a cool trick where you can pre compute your srs in the Lagrange basis.
00:57:31.520 - 00:58:04.760, Speaker B: So you take the monomial basis, and then you do this completely trustless and transparent transformation to transform it into the Lagrange basis. And then when you want to do an opening or a commitment, you can just reuse this pre computed basically new srs. And you don't have to bother with the ffTs.
00:58:06.000 - 00:58:20.544, Speaker C: You're saying that I give you the point to open, and you can compute this f of x minus f of z divided by x minus z. You can compute it without any f.
00:58:20.582 - 00:58:43.876, Speaker B: Of t. Yes, that's correct. Yes. So this is really a very cool trick. And actually there's another trick which is in part four, which I guess I could tell. So basically, you have a polynomial in the Lagrange basis. Okay, now you want to compute this quotient, polynomial also in the Lagrange basis.
00:58:43.876 - 00:59:36.680, Speaker B: Well, that's very easy to do in linear time without any fft, because you take what you have, you remove f of z everywhere, and then you divide by x minus z everywhere. And you can do this in linear time. And now that you have the quotient and you have the pre computed srs, now you can also do the commitments, as in the opening, the commitment of the quotient, I. E. The opening also in linear time. There is one caveat, which is like, how do you compute the evaluation itself in linear time? And this is where another trick comes in, which is a really beautiful trick, which it seems not many people in this space know, which is this one. It's called the baricentric Lagrange interpolation.
00:59:36.680 - 01:00:18.792, Speaker B: So, let's kind of unpack this equation a little bit. So you have your roots of unity, which are the wi, so w to the zero all the way to w to the n or w to the t, and then you have f in the Lagrange basis. And what that means is you have the evaluations of f at your roots of unity. So this is your roots of unity. You already have f evaluated at these roots of unity. So they're kind of already on your hard drive, and you can just read them off because you have them. And then there's this equation which says that f of z, I'm missing f of z.
01:00:18.792 - 01:01:05.856, Speaker B: F of z is equal to this thing. And this thing can just be computed in linear time, because here you have a sum which is linear n. So you just go through every element here. You just read from your memory or your ram or whatever, and then you divide by this, which take constant time. And this is just the constant that you have to compute once for the whole sum. So basically, in summary, it turns out that you can do kind of optimal commitments and openings with Cate, just in linear time, regardless of your basis. So if you're in the monomial basis, it's trivial.
01:01:05.856 - 01:01:40.000, Speaker B: If you're in the Lagrange basis, you need the pre computation and you need this formula. But it turns out that you can pick any arbitrary basis. You can go like really creative and pick some exotic basis, and you have equivalent formulas here, the formula is not as nice. There's a few extra terms, because you don't have the nice algebraic properties of the rules of unity, but you can still do it. And the precomputation trick of the srs can also be done for any arbitrary basis.
01:01:45.830 - 01:01:51.480, Speaker A: So, where are we at right now in the presentation? Have we finished section three?
01:01:51.930 - 01:01:52.680, Speaker B: Yes.
01:01:53.050 - 01:01:53.654, Speaker A: Great.
01:01:53.772 - 01:02:54.262, Speaker B: Yes. Cool. Actually, no, there's two more slides, which are, well, one more slide, which is basically open problems, like, you know, like, can you come up with new constructions? For example, you know, we, we've covered large parts of cryptography with hash functions, pairings, you know, elliptic curves and groups of unknown order. And so there's, there's natural questions like, can you use the other things? And one of the prime constructions that we have is latices. So, can you come with up a new scheme with lattices? Another interesting question is, can you come up with more groups of unknown order? So we have RSA groups, class groups. Can you come up with something new? And actually, Dan Bonet is trying to make hyperedliptic curves work. And so, as I understand these hyperliptic curves, they are groups of unknown order.
01:02:54.262 - 01:03:42.330, Speaker B: It's really difficult to compute the order of the group, but they have a problem, which is that you can compute the order of some elements in the group, and it turns out that you need a bit more than the group of known order. You need the so called adaptive root assumption to hold. And this doesn't hold if you know even the order of one group element. Yeah. And then there's all sorts of kind of. It would be interesting to find new polynomial commitment schemes which apply to very specific use cases. So, for example, if you have a sparse polynomial, it's of degree d, but most of the terms are zero.
01:03:42.330 - 01:04:12.340, Speaker B: For example, can you find ways to commit and open this polynomial in time? Significantly less than B? Actually, this last problem about FFT minimization techniques, when I wrote my slides, that was one of the open problems. But I guess, at least in the context of cate, this has been solved, which is kind of very nice.
01:04:14.550 - 01:04:17.314, Speaker A: What solved that? Was it the.
01:04:17.512 - 01:04:51.840, Speaker B: Yeah. So, two tricks. So, actually, this baricentric trick was co discovered, co reinvented by Vitalik and Dan Bonet. So Vitalik had this intuition saying, oh, I think I can do interpolation in linear time. And he came up with this recursive kind of program. He wrote python code, and he tested it, and then he showed the code to Dan, and Dan was like, oh, I can find a really nice closed formulas for this. And then I discovered that actually, this formula was known, like, 30 years ago.
01:04:53.250 - 01:04:54.000, Speaker A: Nice.
01:04:54.850 - 01:05:48.146, Speaker B: And then there's the other trick, which is the SRS pre computation trick. Which I just came up with. And as far as I can tell, no one is using, of course. And the reason why I was going down this road is that I've been thinking for several weeks now, for almost a month, I've been trying to build snarks for which the prover does not require any FFT whatsoever. And if you can crack that, then you can get basically optimal snarks, where both the proof size verifier time and proof of time are optimal. And when you remove the FFT, you also really allow for very large circuits. So, for example, trillion gate kind of circuits are not out of the question.
01:05:48.146 - 01:06:25.260, Speaker B: And the reason is that the rest of the work that needs to be done, which is these multi experimentations, they can be massively parallelized, and they don't require much memory, and they can be accelerated with custom hardware. But the ffts are kind of very awkward to deal with because you need a lot of memory and you need quite a bit of communication between the various parts. So there's a few moving parts which make it much more difficult to scale than the rest, than the other operations. Cool.
01:06:26.290 - 01:06:40.530, Speaker A: So, I think we're going to wrap it up. We've gone a little bit over time, but, yeah, I guess. Thanks so much for putting this together again and running us and walking us through it. Do we need another session?
01:06:41.830 - 01:07:14.654, Speaker B: So, the final part is what I call polynomial gadgets. And basically, the way that you build these iops is really, you don't reuse the low level polynomial commitment scheme. You have this new layer of abstraction, which I call polynomial gadgets, which are slightly more powerful. And these are the basic building blocks, the lego blocks with which you build your iop. And so I would present the gadgets if there's time. So it is.
01:07:14.852 - 01:07:25.900, Speaker A: Yeah, that sounds good. So, I think we'll talk about maybe doing a third. And then in the meantime, thanks for joining everybody. I think we're going to wrap it up now.
