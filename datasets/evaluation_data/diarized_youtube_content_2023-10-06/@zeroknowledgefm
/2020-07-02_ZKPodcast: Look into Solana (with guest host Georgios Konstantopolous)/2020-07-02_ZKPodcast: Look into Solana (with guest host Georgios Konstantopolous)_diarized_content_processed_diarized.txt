00:00:07.770 - 00:01:05.310, Speaker A: Welcome to Zero Knowledge, a podcast where we talk about the latest in zero knowledge research and the decentralized web. The show is hosted by me, Anna and me Frederick. This week, myself and guest host Giorgios Constantopoulos chat with Anatoly Yakovenko from the Solana network. We talk about their system and their innovations that allow them to achieve high performance without sharding. But before we start, I want to let you know about the CLR matching happening on Gitcoin right now. If you want to support the show, help fund our production, or just help us bring out these episodes every week, it would be a really great time to do so. The way that the CLR matching works on Gitcoin is that every time you make a donation to the Zero Knowledge podcast grant, this amount will be partially matched by a sponsor's donation.
00:01:05.310 - 00:01:33.414, Speaker A: The more small donations, the higher the matching. So basically, even a small amount goes a long way. Anyway, I hope you'll consider supporting. I want to say a big thank you to all of the folks who have already supported. We've seen a few come in in the past week, and it's been really exciting to see that. So now here's our episode on Solana. So with Polkadot being launched, like, as we speak, Frederick is super busy.
00:01:33.414 - 00:01:44.458, Speaker A: We decided that we would invite for this period of time some friends on the show to help me co host. So today we have Giorgios, who's going to be helping me to co host this episode. Hi, Giorgios.
00:01:44.554 - 00:01:45.518, Speaker B: Happy to be here.
00:01:45.604 - 00:02:07.634, Speaker A: Giorgios has been on the show before, once, I think, but mentioned many times. And Giorgios is an independent consultant in the space. Now, today's episode is all about Solana, which is a layer one project. They're based in SF, and they're recently on main Beta, which we're going to find out a little bit about. So welcome to the show, Anatoli.
00:02:07.682 - 00:02:10.150, Speaker C: Oh, thank you so much. Super happy to be here.
00:02:10.300 - 00:02:16.966, Speaker A: You're the co founder of Salana. Why are we saying mainnet Beta? Is it not live?
00:02:17.148 - 00:02:39.674, Speaker C: It's live. It is actually the full featured Mainet. Why are we calling it Beta? And we have, like, over. There's over, I think, 150 validators altogether. It's like, as decentralized and live as cosmos. We do have four boot nodes that add up to over 33% on the network. So that's partly why we're calling it beta.
00:02:39.674 - 00:03:08.534, Speaker C: And the reason why we have those boot nodes is because it's new code. This is 200,000 lines of rust written from scratch in the last two years while drinking a lot of coffee, like at 02:00 a.m. It's ridiculous for anybody to assume that this is going to be as stable of a system as bitcoin that's been around for ten years. So to me, the beta tag stays on for a year. If we can make it a year without any catastrophes, then we can take it off.
00:03:08.732 - 00:03:16.598, Speaker A: I think it would be really great to find out a little bit about you. What is your story? Where were you coming from? How did you get into Solana?
00:03:16.694 - 00:03:50.406, Speaker C: Okay, so I can start from the very beginning. Born in the USSR, right? So, grew up in my childhood, eight years in the Soviet Union, ended up in the States, right? Moving as soon as the Soviet Union fell apart, my parents got out of there. Kind of grew up in Chicago, went to urbanish champagne, met some friends who I started a voiceover IP company with. And this was back in 99 to 2003. We were trying to build like a.
00:03:50.428 - 00:03:52.566, Speaker A: Vonage some early stuff.
00:03:52.668 - 00:04:24.706, Speaker C: This was before Google, Grand Central, before avonage. We were like, let's put these telephony cards into these Linux boxes, and we'll learn Linux, and we'll learn how to programmatically control phones. And freaking was like a thing. In high school, I loved the hackers movies. I subscribed to 2600. I tried the stupid phone trick where you record the tones. It didn't work at the time.
00:04:24.888 - 00:04:27.710, Speaker A: I remember that. Man, I got to rewatch that movie.
00:04:27.790 - 00:05:01.270, Speaker C: But yeah, it got me into computers and got me into building Linux, and that got me to start to figure out how to code and see. And that was really kind of the start of how I got into computers, basically. Qualcomm hired me in the spot in 2003, and I was like, lucky because this was post.com crash. I was like, holy shit, I got a job. I'll be able to pay off my loan, which at the time seemed insanely huge. But right now, looking at student loans, I'm like, wow, I paid nothing.
00:05:01.270 - 00:05:11.294, Speaker C: I was like $70 a month. Imagine that. Imagine having a student loan from like a top five computer science engineering school of $70 a month.
00:05:11.492 - 00:05:22.260, Speaker A: That's wild, right? Yeah, I mean, I'm from Canada and tuition is different there. Also, I live in Germany where tuition is free, but I've been following what's happening.
00:05:23.670 - 00:05:59.338, Speaker C: So Qualcomm spend most of my career working on like, I just kind of got into that mind space where I have software which is code and language and mathematical, right? And then I have hardware which has very fixed instructions and opcodes and pipelines. And I'm like, how do I stuff the most amount of math into this Rube Goldberg machine that's always kind of broken? And working at Qualcomm was really interesting because at that time, when I started. You guys remember 2003 cell phones?
00:05:59.434 - 00:06:02.238, Speaker A: Yeah, the clunky Nokias, kind of.
00:06:02.324 - 00:06:25.890, Speaker C: No, this was a little after. Yeah, these were flip phones, like the razors before razors, but this was like the start of the flip phones. They're like, oh, we can make them smaller. Clamshell design. But these were 400 bit arm chips, so they were dog flow. Everything was written in C. We actually built this operating system called Brew.
00:06:25.890 - 00:07:00.766, Speaker C: I was one of the core kernel engineers, one of the first folks working on it. And this was hand rolled C with C plus plus compatible virtual tables that you wrote out by hand. And the reason why we didn't use C plus plus was because the compilers were too crappy to generate C plus plus for this 16 bit arm architecture. We're like, the code is too big and the instruction cache can't handle these big object sizes. This is stuff that nobody needed to care about now. Nobody cares about this stuff. The only people that care about stuff like this are people building for solidity.
00:07:00.766 - 00:07:55.682, Speaker C: So just a little swipe at EVM, where you have to optimize every loaded store. This was like my life working on cell phones. The cool thing though, that every year there was like an architecture revolution in cell phones. So I learned a ton and basically also learned just working there for so long, protocols, operating systems, and especially wireless, kind of like how wireless protocols work and where they get their kind of throughput, like their optimizations. So that's my story in 2017. I was at Dropbox at the time, working on compression with a really fun, awesome group. I had the perfect job, and I left the perfect job.
00:07:55.816 - 00:08:05.270, Speaker A: You left the perfect job to become a founder in crypto, of all things. So tell me a little bit about what Solana is.
00:08:05.420 - 00:08:49.634, Speaker C: A big part of what it is, I think, actually comes from the team. So myself, operating systems geek, I worked on Brew, Android and Linux, and every permutation of those things. Greg, who's the CTO? He was actually one of my friends back in those trying to build a vonage days in the UIUC. He's a compilers expert. He basically worked on languages and compilers the whole time he was at Qualcomm. And our buddy Steven, who was a GPU lead, started at intel, but also ended up with Qualcomm. He is like the kind of main go to for any kind of optimization.
00:08:49.634 - 00:10:14.506, Speaker C: So Solana is a layer one blockchain smart contracts platform. But it's really like, if you want to know anything about it, everything that we're doing is optimizing the hell out of every possible thing that a blockchain would do at the hardware level. So we're horizontally scaling everything we can by using hardware, basically. Is there any operation that's a bottlenock that if we add more network cards, we get more packets per second? If we get more SSDs, can we do more reads and writes? More state reads and writes? If we add more cores, can we execute more transactions? So the whole idea is that the stupid cryptokitties example that everyone brings up when they talk about scaling, what should have happened is that the people that were seeing the spike should have just doubled the number of cores in every one of these validators or miners. And there wouldn't have been this, like, the customers wouldn't have seen an outage, right? This would never happen in a web company that's just like, holy shit, we're hitting like a million monthly active users. We didn't expect they would just double the number of cores in AWS, right? And to me, this is like a fundamental problem. If you can't do that, then you haven't solved scaling.
00:10:14.506 - 00:10:24.958, Speaker C: If you can't throw hardware at it, you haven't solved scaling. So everything we're doing, and if you want to know anything about Solana, is that this is a hardware based scaling, layered one.
00:10:25.064 - 00:10:27.042, Speaker A: What is the origin story of Solana?
00:10:27.106 - 00:11:44.800, Speaker C: This was like a weird experience on a very strange level. I had two coffees and a beer, and I was talking to my budy about proof of work and why it sucks. And I was up till four in the morning and I had this eureka moment. It felt like, I don't want to say religious, but like an acid trip or something, right? It was just like this massive light bulb went off in my head that I can use shot to 56 and use it recursively and generate a data structure that has, like, you can measure time with it. And why, to me, that was such a big kind of punch to the head was that, I don't know, Giorgios, you probably know this. There's like this italian guy that wrote about there's like five universes and one which, there are reversible functions and one that reversible functions exist, but we can't prove that they exist, right? Irreversible functions exist, but we can't prove that they exist in a universe that irreversible functions cannot exist, and we can prove that they cannot exist. And what he's talking about is there's no mathematical construct for the era of time.
00:11:44.800 - 00:12:20.274, Speaker C: There's nothing in math that represents time, right? There's. Math represents every kind of other physical property, but there's nothing that represents time moving. And take this thing that generates data purely at a math. From that data alone, you can have objective measurement of time. That was just like, holy shit, this is the era of time. And I couldn't even figure out what to search for. There was nobody working on VDFs that openly at the time, and I didn't even know the term.
00:12:20.402 - 00:12:22.090, Speaker A: Yeah, that's. That was going to be my question.
00:12:22.160 - 00:12:51.054, Speaker C: Right. So I searched a lot. The only thing I saw was, like, RSA time locks, which were fairly similar, and nobody was using them in crypto for anything interesting. And why I was so obsessed with the time component was that all these years at Qualcomm, the wireless protocol just kind of get hammered into my head. And the first wireless protocol anyone's ever built is like, imagine two radio towers. They talk to each other at the same time or the same frequency. They get noise.
00:12:51.054 - 00:13:17.450, Speaker C: So the first thing that people did was like, let's give everybody a clock and alternate by time. And that's called time division, multiple access. And this is how you scale the number of participants per second, which is same problem as block producers per second, right. If we can stagger the block producers by time, there's no collisions. Everybody gets to take their turn, and we get as many block producers as possible. So that was like, okay, I can do this whole thing.
00:13:17.600 - 00:13:28.286, Speaker A: But the VDF research was actually happening potentially in parallel to what you were thinking of. Did you at some point realize that it was happening and that it had a name?
00:13:28.388 - 00:13:59.480, Speaker C: Yeah. So I quit my job and started writing the white paper and talking to a bunch of people. Basically, this was an easy time to kind of get into crypto because it was blowing up in this very obvious way. Like, bitcoin was doubling every two weeks or something crazy. And I ended up running into Zaki. And Zaki talked to me and he was like, okay, you have something that isn't bullshit, and it's called the verifiable delay function. You should go look at all these.
00:13:59.480 - 00:15:20.814, Speaker C: Like, the interesting part was that I looked at all these papers, and at the time, they were way over my head. I'm an operating systems engineer, right? Like, throwing myself into class groups and RSA accumulators and all this other stuff, it was hard, right? This was like a lot of learning. But the cool thing was that the thing that, the stupid chateau 56 loop that I designed, after talking to some folks like Dan Bonet, he was like, what you have actually works really well, so don't change it. The stuff we're working on right now is iterating so fast that it's just going to be outdated by the time you guys implement anything, right? So I got the blessing from some of really important people and I was like, okay, we'll just go build on this. And that was good enough. And it turned out to be a fairly good choice because we quickly realized that the silicon that people ship, like intel, AMD, it is basically the same everywhere. And we have almost no drift between shout to 56 running on an intel chip or AMD chip or any other architecture like PSMC, intel.
00:15:20.814 - 00:16:06.318, Speaker C: They're all basically at the latest and greatest material science as possible. And they can't really squeeze out and beat each other by a large like that ended up being a fairly good kind of bet because the really amazing work that folks that are building VDFs like Justin Drake and all those guys, they're building something that is not yet optimized down to this gate pipeline latency level that shot to 56s that intel ships. Here's the fastest shot to 56 our engineers could build, and it's going to take you a big pile of work to make it any faster.
00:16:06.494 - 00:16:24.546, Speaker A: We actually did an entire episode on VDFs with Bono, who is one of the co authors, I think with some of Dan Bonet's students. So that's actually an episode I'm going to add in the show notes for you to better understand what he's talking about. But it sounds like your VDF is very different. Do you call it a VDF?
00:16:24.738 - 00:17:04.254, Speaker C: Danbone did say that I could call it a VDF, but it is an embarrassing implementation. It's an embarrassingly stupid way to do it. But it's so simple that an operating system in engineer can make it secure. Right? Like I don't have to understand that much about cryptography. It's a recursive shot to 56, which means because it's recursive and chapter 56 is pre image resistant, there's no way to parallelize that process. That's kind of obvious to me at least. Right? And the way we speed it up is a very dumb way, is we just checkpoint the data as it's produced and then we verify the samples in parallel.
00:17:04.254 - 00:17:32.158, Speaker C: And because us as people that work at Qualcomm and are very familiar with hardware and SIMD lane, single instruction, multiple data lanes and GPUs. We can take any one of these kind of scaling solutions like that are chip level scaling solutions and use that to speed this up. So your phone right now has I think, 1000 simd lanes in this GPU. So you can verify a second in a millisecond, and that's good enough, but it does cost data.
00:17:32.324 - 00:17:46.270, Speaker B: Could you perhaps expand on how the verification is made faster than the evaluation? Because my understanding is that, yes, you do a sequential computation where you just keep hashing.
00:17:46.350 - 00:18:07.782, Speaker C: It's exactly the same amount of CPU time. It's just the verification is parallelized, so it takes less time to verify than real time. Right. Because what we care about is real time, that when this thing transmits a block, whoever receives it in real time can verify it faster. And the block producer has the force delay, right.
00:18:07.836 - 00:18:19.994, Speaker B: So what I was thinking is that as a validator, in order to know who the latest block producer is, I need to have the latest state of the hash chain, is that correct?
00:18:20.112 - 00:18:50.646, Speaker C: This gets into everything else that you have to build on top to build the blockchain. Right. This is like the simplest part, this whole VDF thing. The key part to this is this is a way for us to track time before consensus. Yes, before there's been any consensus, we have this lamport clock, this logical clock in the network. And the logical clock part of it isn't the hashes itself, it's the count actually. Like the count is what's important.
00:18:50.646 - 00:20:08.254, Speaker C: Like a block producer shows me that they reach count 1 billion, and the next one shows me that they reach count like 1,200,000,000 or something like that, right? The amount that they're adding to this data is the interesting part. That's the logical clock moving forward. That's the era of time, the state we leverage as a mechanism to kind of keep track of transactions. And when they reference a particular hash value, that's our kind of mechanism to make sure the transaction was signed after that thing was generated, things like that. So, to answer your question, how we do actual elections or scheduling is a very dumb way. Again, we do the simplest thing. First is based on some prior state, we generate a leader schedule, which just basically assigns some stake weighted, deterministic, seeded round robin fashion, of which validator, based on their stake weight, is assigned to which slot, and that leader schedule gets turned on and plus one epoch, right? So based on epoch one ends, right, and epoch three gets the result of the state calculated by the end of epoch one.
00:20:08.254 - 00:20:16.354, Speaker C: Which means that this machine halts if we can't find a final state within an epoch. Right. If we can't finalize a block within.
00:20:16.392 - 00:20:26.822, Speaker A: An app, is it based on anything that already existed? Is it taking other models? Is it taking an existing blockchain and sort of adjusting it?
00:20:26.876 - 00:20:58.510, Speaker C: Yeah. So I personally believe that every proof of stake implementation is some form of PbFT. We based our thing on PbFT, although we kind of did a lot of modifications. And Eman was like cream and tear that there's no proofs, but like hell it works, which is fine. The bytecode is using Berkeley packet filter, which is not wasm. It's a bytecode used by network switches. So it's designed for kind of high performance, low latency stuff.
00:20:58.510 - 00:21:41.170, Speaker C: It's got interesting properties like there's no stacks, there's only stack frames. So you can take this bytecode and execute it like a kernel on a GPU. So single instruction, multiple data. So again, in that like cryptokitties example, right, we have a single contract that hitting a spike because it's a single contract, it's a single kernel. Most of the transactions are going to take the same branches. We can run this kernel on a GPU card which has, let's say one execution thread, but 80 lanes per thread, right? And then like 80 execution threads total. So that's how you scale stuff on GPUs.
00:21:41.170 - 00:21:52.958, Speaker C: So we do like tons of stupid tricks like that, which end up reducing the cost of the execution, right. And therefore reducing the cost of the blockchain and things like that.
00:21:53.044 - 00:21:58.754, Speaker A: Is it based a little bit on the Ethereum model though? Do you have a VM? Do you have a statement? Is it set up that way?
00:21:58.792 - 00:22:51.746, Speaker C: We don't have a VM. The EVM is very academic. So the way we've designed it is more like if you look at the DMA, direct memory access to hardware and Linux and this concept of scatter gather, where it's a memory that people need to understand what a volumen machine is, right? You have basically memory, right? And that memory is just bits, right? It's bits. And those bits are grouped into cells like pages or bytes or however you want to do it, words. And they have addresses. And these things are flipped by a separate part of the machine, the CPU, right? And this thing is like getting instructions from the memory. And then those instructions say go load and store stuff, right? And you take this model like at an academic level, and this is what EVM kind of tries to do, right.
00:22:51.746 - 00:23:53.990, Speaker C: There's specific load instructions, there's store instructions, and they say, why don't we have 256 bits for everything? Hardware like at the actual implementation level is much more complicated because now you have caches and l one caches that are in the L two caches and l three caches and points of unification that are weird and prefetches and all this other stuff. So what operating systems do is they force the programmer to tell them ahead of time everything that they want to do. So when a programmer wants to load a bunch of textures and run a bunch of transformations over those textures, they actually have to tell this thing ahead of time. Here's a bunch of resources I need you to go fetch from disk. Here's a bunch of stuff I need you to load into these regions of memory, and these are the kernels and things that I'm going to run over this memory. And all this is known ahead of time. And then this hardware is programmed and then it executes.
00:23:54.330 - 00:24:05.290, Speaker B: I mean, you don't need a VM because you're leveraging the bytecode from the Berkeley packet filter as your runtime is, my understanding. Is that fair to say?
00:24:05.360 - 00:24:30.258, Speaker C: No. Berkeley packet filter is just a bytecode. Right. This is what llvm poops out. And that part we picked because it's been around in the industry since the, it's safe to run in ring zero. But the important part is that we don't have individual load and store instructions. All we have is an entry point into executable object, an ALF position independent code.
00:24:30.258 - 00:24:35.442, Speaker C: Right. And that entry point says, this is the memory I'm going to read. This is the memory I'm going to write.
00:24:35.576 - 00:24:45.222, Speaker B: So you're saying that you're not operating on a peradvis per instruction level to do loads, but you're just loading full pages of data.
00:24:45.356 - 00:24:46.040, Speaker C: Exactly.
00:24:46.490 - 00:24:49.094, Speaker B: And you're kind of amortizing the cost.
00:24:49.292 - 00:25:00.502, Speaker C: Exactly. And because each transaction tells us ahead of time all the regions is going to read and all the regions is going to write. That means we can pipeline and overlap.
00:25:00.566 - 00:25:02.880, Speaker B: And you can pre allocate also.
00:25:03.250 - 00:25:24.530, Speaker C: Exactly. We can do everything. Right. This is how hardware gets built. Right and shipped. And the EVM design is very academic and it's going to take, I think, a lot of engineering effort to take this academic approach and underneath it, implement what we have. You can do it.
00:25:24.530 - 00:25:26.802, Speaker C: It's just going to take a big pile of work.
00:25:26.936 - 00:25:57.946, Speaker B: A question on this. So the performance bottleneck that comes for using the ABM, it's not entirely. Of course, the AVM has some downsides on the performance, but also it has to do with cross contract calls, because when you call multiple contracts, you effectively have to take a mutex on all the shared state of these contracts. So how would it work on Solana? If you want to do a cross contract call, doesn't the bottleneck get introduced?
00:25:58.058 - 00:26:12.750, Speaker C: The transaction specifies all the memory is going to read and all the memory is going to write ahead of time, so there is no mutex to take. The transaction itself is the mutex. Right. I always mix up these two database terms, but I believe it's called isolation.
00:26:12.830 - 00:26:18.818, Speaker B: So what if two transactions are sent and they both want to interfere with the smart contract?
00:26:18.914 - 00:26:34.186, Speaker C: The contract is position independent code. It has no state. The transaction picks some state that it wants to mutate or read, and that's the state that's specified ahead of time by the transaction itself. The contract itself is pure code.
00:26:34.288 - 00:26:42.894, Speaker B: It's just instructions, which is different to how Ethereum smart contracts are because Ethereum smart contracts do have state because the.
00:26:42.932 - 00:26:57.986, Speaker C: Code mixes in globals, right? And the first rule that you learn as a programmer is never use globals for anything, at least as a seed developer, right? You're like, no, no globals, right?
00:26:58.088 - 00:27:31.022, Speaker B: If we can get back to your point about the consensus of the system, you said that you use some form of modified PbFT, which I recall reading about. However, in your documentation, you have a big section on fork generation. While to the best of my understanding, PbFT and all BFT algorithms do not have the notion of forks. It's just you have the latest state and that's it. And then you keep moving. Is there a quick explanation on what's the difference?
00:27:31.156 - 00:28:32.550, Speaker C: Yeah. So basically, what are the two techniques to make things fast, right? One of them is we did, that's obvious, is you use a global clock, right? If you're doing a bunch of protocols, if you have a global clock, you get read consistency, right? I sent you a bunch of messages, I sent Anna a bunch of messages, and you guys order them exact same way because you have the same timestamps, right? So that amortizes that communication. The other go to optimization is like basically using a window, Nagel's protocol sliding window, right? You receive a bunch of data and you don't respond until you don't respond on every message, right? You receive a bunch of it, you batch the processing, and then you respond on the entire slot. And effectively, what we did is we still have to do PBFT because we have to come to some conclusion on what the root block is, and all we're doing is allowing the network to slide that window out of how many states have been produced.
00:28:32.650 - 00:28:45.890, Speaker B: And the number of states may define, may introduce some branching because you may build on unconfirmed state.
00:28:46.040 - 00:28:56.710, Speaker C: Right, exactly. That's it. Right. Those are the only two go to things. None of these are like computer science, like field metal things. They're like stuff that engineers do all the time.
00:28:56.780 - 00:29:06.038, Speaker B: So is it correct to say that you're able to get very quick blocks by kind of optimistically building on top of unconfirmed blocks?
00:29:06.134 - 00:29:09.158, Speaker C: Exactly, yeah. Everything is optimistic.
00:29:09.254 - 00:29:25.166, Speaker B: How does this relate to. I'm just thinking about speculative execution attacks in this context. So is there any way that I can kind of force one processor to go the one way and another the other way and introduce some consensus error?
00:29:25.358 - 00:30:03.934, Speaker C: Yeah, not an error. It's just that this is kind of my intuition and it's outside of my expertise to prove this kind of stuff. But I think Zaki called it asynchronous safety. Right. Like you're producing state and the safety is actually following the head pointer of the chain. And me as an attacker, I can actually prevent my data from being propagated and I can observe what the chain is doing, and I could potentially then transmit my data at the right moment and effectively force all that computation to be discarded. Yes.
00:30:03.972 - 00:30:06.990, Speaker B: That's an issue with all synchronous protocols.
00:30:08.210 - 00:30:57.214, Speaker C: Because we have this window, the attacker now has a larger set of data to play with. Right. They have larger set of states they can manipulate, potentially more things. So I would assume that there's bugs there, or not bugs, but exploits that could allow the attacker to extract more value, potentially allow them to generate more rewards than anyone else in the network that's not doing this attack. Or do a denial of service cause the fork to drop blocks unnecessarily, things like that. How much of that is a problem is unclear, because if that stuff is observable, the attacker is in the proof of stake system, especially like a deterministic one like ours. The validators are well known ahead of time.
00:30:57.214 - 00:31:19.590, Speaker C: The block producers are well known ahead of time. You end up in a situation where you know that, hey, chorus one is explicitly dropping blocks of every other validator and is like trying to do a censorship attack. Why? Right. I think we need a lot more product market fit in the space for us to worry about that.
00:31:19.660 - 00:31:20.520, Speaker B: Got it.
00:31:22.250 - 00:31:57.198, Speaker A: That's interesting though, that one of the protections here is the reputation of the validators and the fact that you do know them at the moment. I wonder if we look at the example of mining, it also started out very friendly and everybody knew each other and could kind of predict how they would act. And eventually, I mean, I guess it's still known, but it became much more aggressive, much more competitive. Would there not be a time where one of them gets strong enough or there is some sort of backroom coalition and they're not that friendly?
00:31:57.294 - 00:32:50.482, Speaker C: Possibly, but that could be after product market fit. I think there's like a fundamental difference between, I think, proof of work chains, especially one that bitcoin is trying to be, which is like no weak subjectivity whatsoever. This is like proof of work to the max from a proof of stake chain that's trying to be, we're the fastest possible censorship resistance state transition engine you could think of. There are two separate things, right. I think the proof of work use case, I can see it being a store of value, right. There's some really interesting properties about bitcoin and how the mining works and how security is aggregated and things like that. These properties are not there in any proof of stake system that I've seen, right.
00:32:50.482 - 00:33:13.306, Speaker C: And I can't claim that we are going to be a store of value because I don't see those property, those emerging properties out of this. Unless maybe we get to a point where we have so much censorship resistance that, I don't know, you have to corrupt like a million parties to actually break that. Right. That's like a long ways away. Right.
00:33:13.408 - 00:33:30.670, Speaker A: I wonder. So I like this sort of using the term product market fit for whether it becomes a store of value or. This is interesting, I haven't actually heard that in that context. So how are you thinking about finding product market fit and where do you see yourself going?
00:33:30.820 - 00:34:21.790, Speaker C: Yeah, the bet we're making is that the number of companies that have enough users to where they care about transaction fees and the slowness of Ethereum and all this other stuff. There's enough of these companies already and enough that that number is going to keep doubling every year because just the space is going to grow and they're going to pick the chain that will give them the best web experience. And that means lowest latency, cheapest transactions. Because what we're competing like, I think fundamentally isn't like Ethereum, it's Google Ads, which is the value of that transfer, right. Of a Google Ad is 0.2 cents and it comes at you in 200 milliseconds. So we need to be like 100 times cheaper than a Google Ad.
00:34:21.790 - 00:35:02.670, Speaker C: For a web company to consider replacing ads with cryptocurrencies and creating social network effects from those cryptocurrency usage to where they're getting the same kind of revenue as if they would have had ads. But now instead, it's just currency based. Right? And then we are the platform where this stuff runs. So we have to be much, much cheaper than an ad for them to consider replacing ads. Right? So to me, this is what I'm competing with in my mind is how do we get rid of this parasitic form of money which is sucking your attention, right. With a more fun form of money, which is Dodge. Like, Dodge is an awesome idea, right? A meme coin.
00:35:03.250 - 00:35:05.466, Speaker A: Oh, you mean like doge.
00:35:05.658 - 00:35:31.910, Speaker C: Doge. I call it Dodge. Yeah, doge. Doge. Well, I'm born in the USSR, so I pronounce everything incorrectly, but yeah, stuff like that. Stuff that's fun, right? The Internet is supposed to be a bunch of fun things. For that stuff to work, it needs to be super cheap, like basically free to the user where the company can subsidize it.
00:35:31.910 - 00:35:45.878, Speaker C: And this is what the goal we're trying to make. And I don't think it's unreasonable because the cost of the hardware is ridiculously cheap. $50 buys you a terabyte of egress. Terabyte is 4 billion transactions.
00:35:45.974 - 00:36:15.960, Speaker B: Is the goal for Solana to have only validators or block producers to run the nodes, and then consumers of the software that gets written on it just interface with Solana via these buffed up nodes? Or do you see users running their own non block producing nodes so that they can query transactions locally or verify the state? So far.
00:36:17.690 - 00:36:42.060, Speaker C: If you have censorship resistance, right, then you can just basically trust your light client because you trust that this group of keys signed the right thing. Because your group of keys that you're using for your light client, right, is based on the censorship resistant network. So with the light client, where do you draw the line?
00:36:43.250 - 00:37:24.822, Speaker B: It's not about just about the censorship resistance or the liveness of the system. It's also about the safety in the sense that no validator did not commit a transaction which gives them a million tokens out of nowhere or something like that. In bitcoin, for example, miners may censor transactions, but if 51% of the miners or whatever try to produce a block that gives them a lot of money out of nowhere, that block will not propagate because full nodes will reject that block because it has an invalid transaction, while if there are no rule.
00:37:24.886 - 00:38:14.314, Speaker C: Yeah, so we have slashing, right? Anytime you do something that the rest of the network doesn't do, you get slashed. In that sense, we're no worse than cosmos in terms of kind of like safety guarantees or economic. The economic game theory about the security of the state is based on slashing, effectively. So the interesting thing is that we're hugely fans of weak subjectivity, which is like, I think a funny way to, like checkpointing. Yeah. Which is a funny way to just use the old security paradigm, which is tofu trust and first use, right? You observe something, you trust it in first use, and then you verify that it's still the same thing that you trusted. So tofu, right.
00:38:14.314 - 00:39:10.102, Speaker C: If you're like a user, you connect to the network the first time, you trust it at whatever state it is, and then you continue trust it based on that state. And based on that one, is that any kind of state sharding techniques that ethereum is working on? Awesome, folks, those can be applied to temporal sharding, which is like, I run my validator today, and then I run it a month from now. I get all the proofs about data availability and verification and economic guarantees a month from now. And I'm like, okay, it's consistent with what I observed a month before, right? And I can continue. So you can get the same guarantees there, but for now, what's implemented is just basically tofu. You connect to the network. As long as you stay connected, you can continuously verify that the same set of validators are validating the exact same state.
00:39:10.256 - 00:39:10.960, Speaker B: Okay.
00:39:11.490 - 00:39:28.114, Speaker C: There's fairly easy ways to catch up, because main bottlenecks are like the cryptographic operations, right? So those are trivially parallelizable because you can farm those out on a big cluster of GPUs and run all the signature checks independent of each other.
00:39:28.232 - 00:39:45.190, Speaker B: I guess. We've been talking about quite low level stuff all this time. And the podcast is called the ZK podcast. So I think a question I have around this is, do you see zero knowledge proofs leveraged in Solana to make running a node more manageable?
00:39:46.490 - 00:39:50.460, Speaker C: It's so. Or, like, it would be awesome.
00:39:52.270 - 00:40:00.406, Speaker B: I'm thinking of something similar to Cello's light client protocol called Plumo, which I believe also was recently on the podcast.
00:40:00.598 - 00:40:02.410, Speaker A: Oh, no, we had it at the summit.
00:40:02.490 - 00:40:13.602, Speaker B: On the summit. On the summit, right. So I was thinking that, is there any room for further optimizations utilizing zero troops? Is anybody in your team looking at that?
00:40:13.656 - 00:41:08.290, Speaker C: Perhaps we're not. 99% of the work in this is data availability, right? If we're working on core, it's like working on the network protocols and optimizing them and trying to get this data faster across this global Internet, that some nodes are behind the chinese firewall, right? There's like a big pile of work just there. So this is really what we're working on right now. I think there's a lot of interesting things, right? There's like stateless VMs where you just use RSA accumulators, right, and things like that. What I need to observe is a very small chunk of the state, and then everything else not only contains the memory needs to read, but it can also contain a proof that that memory actually belongs part of the state and stuff like that. There's a lot of optimizations that are super interesting. We haven't had a need for them yet.
00:41:08.290 - 00:42:09.970, Speaker C: I think fundamentally, what I'm imagining these things are for isn't like a messaging platform. I think the goal of these things is actually to accumulate as much data as they can and make it atomically accessible. And this is why I think philosophically sharding will fail, because it's not atomically accessible state. And it means that if you have atomically accessible state, right, like I have one gen and Norma state machine, let's say it's a petabyte of data. This isn't just regular data anymore, right? It's data that is globally agreed upon, and those bytes have value, and we can atomically do transactions on them. And we have this enormous price discovery engine that's constantly, effectively removing the entropy in the state, right? It's like optimizing these things down to the lowest possible price difference between any of these states. So this is like a very complicated engine of price discovery.
00:42:09.970 - 00:42:35.690, Speaker C: And when you shard it or when you split it over, you lose that, and then all you have is just a messaging system. And to me, that's not interesting. To me, what's interesting is how do we get the world's most relevant information that has anything to do with anything of value into a single giant bucket, and have the best possible price for any of these bytes?
00:42:35.850 - 00:42:40.880, Speaker A: But do you think then, does this also categorize sharding systems just as a different use case?
00:42:42.210 - 00:42:42.906, Speaker C: Possibly.
00:42:43.018 - 00:42:49.970, Speaker A: Product market fit idea? Is it just that they'd be used for different things or different types of projects?
00:42:50.390 - 00:42:59.318, Speaker C: But what if they're slower and more expensive? Add worse engines of price discovery. What is the use case?
00:42:59.404 - 00:43:57.774, Speaker B: I think the use case here is decentralization. Keeping their node running costs very low, because as we said previously in the call, as you increase throughput, clearly the hardware requirements increase. Sorry, I might sound like a broken record at this point, but I just want to clearly communicate my point in the sense that sharding allows you to scale while keeping verification costs low. And that's also in the same sense as what layer two is supposed to do. But while in your case, indeed you're able to scale because adding more GPUs or disk space is cheap, you're relying on this assumption that the cost of acquiring more computer storage grows sublinearly with the demand.
00:43:57.922 - 00:43:58.620, Speaker C: Yeah.
00:44:02.270 - 00:44:13.626, Speaker B: I'm not exactly sure convinced that this is a sound assumption. Or perhaps, how do I say, what do you do if this assumption breaks.
00:44:13.738 - 00:44:18.430, Speaker C: I just wait two years, because then it's going to be because hardware is going to get twice as cheap.
00:44:20.850 - 00:44:25.602, Speaker A: By the way, though, the idea that sharding will fail. This is the first time I've heard this said.
00:44:25.656 - 00:44:28.014, Speaker C: I have a whole podcast called no sharding.
00:44:28.062 - 00:44:35.686, Speaker A: Oh, yeah, that's true. This is your mission, to basically say this is the information you're bringing to.
00:44:35.708 - 00:44:57.542, Speaker C: The conversation to get away from the subjective bullshit. The whole purpose of decentralization, the only function of it that's objectively measurable, is censorship resistance. In my opinion, yes. If I have censorship resistance, I can build whatever decentralization features you want on top of it as many readable nodes.
00:44:57.606 - 00:45:00.478, Speaker B: And regulatory arbitrage, exactly like all you.
00:45:00.484 - 00:45:35.442, Speaker C: Need is censorship resistance. So imagine I have two shards of 200 nodes each, right? Finance is in one, Coinbase is another. It takes 67 nodes. Let's say they're all equally staked. Any 67 nodes and any other two shards can control the flow of funds between these two massive financial institutions. And the reason why I sharded is because I can't grow my committee above 200. Because we have this limitations based on software protocol, or hardware, or whatever you want to call it, or philosophy.
00:45:35.442 - 00:46:29.526, Speaker C: Now, if instead, I took the same cost of hardware and just had twice as more powerful machines, right, that committee can now be 400, and now it's 137 machines that control the flow between Coinbase and finance. And that's more censorship resistant. So if the thing that we're trying to build is censorship resistance, then what we need to be maximizing is the number of nodes, the minimum number of nodes that it takes to get to 33%. I don't even know if this is possible, because proof of stake sucks. It centralizes. Just like people are bad, they go to exchanges, they pull their money in the exchange validator. So I'm not 100% sure that if this is even possible, but I'm pretty sure that sharding is not going to get us there.
00:46:29.526 - 00:46:37.846, Speaker C: The stuff that's not possible, I think, is the human behaviors around actually participating in this networks and the governance of it.
00:46:37.948 - 00:47:06.978, Speaker B: But again, the issue is that in this situation, where you have one big chunk of data that all validators vote on, that the communication cost, as you just keep adding more nodes in this context, also grows, while again, in the context of sharding, each shard is independent of each other, and they just checkpoint their state. And you only have the overheads due to the cross chain transactions, which that's what you are trying to minimize.
00:47:07.074 - 00:47:40.810, Speaker C: Sure. But no, it's not about cross chain transactions. I'm trying to increase censorship resistance. This is the only point of decentralization with the setup, whether you have a single committee and you throw hardware at it. I can take dollars, right? I can take fiat. I can take the work that we do as humans, and I convert it into censorship resistance. Sharding doesn't achieve censorship resistance, because when you shard your groups, you're also splitting your set, right? That can decide where stuff flows.
00:47:40.810 - 00:47:45.554, Speaker C: And to me, this is like the foundational problem with it.
00:47:45.592 - 00:47:52.420, Speaker A: But what about the beacon chain or the central body that's supposed to actually fix for that?
00:47:53.350 - 00:48:01.190, Speaker C: Okay, I am finance. I'm in shard one. Coinbase is in shard two, right? I don't give a shit about the beacon chain.
00:48:01.610 - 00:48:03.574, Speaker A: You just act within there.
00:48:03.772 - 00:48:27.918, Speaker C: There's some hedge fund that bribed 67 nodes to control the flow between these two massive trading financial institutions and is now gaming the system and earning a return based on, like, that's going to happen. If it's possible, right? If it's possible for some fund to bribe the flow of funds between these two organizations, they'll do it because there's enough money on the line.
00:48:28.004 - 00:48:31.978, Speaker A: But would you only need to bribe sort of the controllers of the one shard?
00:48:32.074 - 00:48:42.574, Speaker C: Giorgio, tell me I'm wrong, but when you split your committee, right, any one of those committees is the minimum of any one of those committees can decide the flow of funds.
00:48:42.702 - 00:49:03.462, Speaker B: The idea here is that you make an initial assumption that some percentage of the total pool of the validators is honest, and then you try to distribute these validators to a shard. And the whole idea is that the way that you distribute these validators to the shard will not break the assumption.
00:49:03.526 - 00:49:06.460, Speaker A: Because it would be random in a way, or something.
00:49:07.870 - 00:49:27.794, Speaker B: And this is called the static adversary model, which assumes that an attacker that gets assigned to a shard cannot adaptively corrupt another shard. And this would indeed allow them to cause bad things to happen if they could.
00:49:27.912 - 00:49:30.066, Speaker C: And maybe that's a fine assumption, and.
00:49:30.088 - 00:49:41.078, Speaker B: Maybe that's a fine assumption. But other people, for example, the near people, are not convinced by it. And that's why we have multiple different approaches to solving the problem.
00:49:41.164 - 00:51:06.670, Speaker C: And that's fine, right? Because at the end of the day, all of us are trying to get to the next level where we slay the vampire squid, right? We're not even talking about finance and coinbase at this point. It's like the other guys, the Goldman Sachs, like everybody. That's like siphoning 20% of the world's GDP into bullshit, just moving numbers around in computers and a lot of people. So from my perspective, it's fine, people should try all this stuff because it's even possible that because you can allow more people to participate in these networks because it may be cheaper, right? That may be more important than censorship resistance. That is like, I think, a point that I don't want to miss, right? Because a big part of these networks isn't really the actual tech, right? It's actually the community that's built around some philosophy, the values that they hold, right? If they believe that this is decentralized, even if it's wrong, right. If enough people believe it, then there's enough social pressure to where this kind of corruption is very hard to achieve. In reality, it may be theoretically possible, but it may be so devastating to the community that no token holder that's sufficient enough would actually do it.
00:51:06.670 - 00:51:51.594, Speaker C: And this, I think, is missed. Or when people publish attacks in bitcoin, I think they also missed that part. Binance had like 40 million of their bitcoin stolen. But the suggestion was that why don't they just publish a private key or actually issue a transaction with a high enough fee to unroll the chain? Because 40 million is like, I think four days, or at the time, there was four days worth of blocks. So they could literally write a transaction with their old private key, right? Four days ago on a block. That's, I don't know how many blocks it is, but a few hundred blocks, let's say that says, hey, I'm going to pay the miner absurd amount of money to go create a parallel fork and unroll four days worth of bitcoin.
00:51:51.722 - 00:51:56.066, Speaker B: But I guess that's a feature, not a bug, right? The Binance Reorg story.
00:51:56.168 - 00:52:24.490, Speaker C: But binance wouldn't do it because they hold so much bitcoin that the threat of a four day reorg would devastate the price and the confidence that people have into the chain itself. And I doubt the miners would even do it if binance tried. Yeah, if we're going to get a little bit more meta, then the tech doesn't even matter. What matters is, can you make the people that participate in the network believe that this is decentralized and censorship resistant.
00:52:25.470 - 00:52:34.910, Speaker A: And have their incentives so aligned that it would be such a mistake for them to. They would lose more by screwing it up than they would gain.
00:52:36.530 - 00:52:52.510, Speaker B: Maybe we can again get a bit back on the privacy story, because we are, after all, as we said in the. The private. What is the private story about Solana? Have you thought about it? Have your users asked for it?
00:52:52.660 - 00:52:54.902, Speaker C: We thought about it, and that's about it.
00:52:54.996 - 00:52:57.922, Speaker A: I think you're so not into privacy.
00:52:57.986 - 00:53:43.054, Speaker C: No, it's a super hard problem. Your knowledge stuff is not an area of expertise. It is something that is an absurdly rapid research. So for me, as an engineer, when I look at this stuff, and I see, when I ask the experts, what should we actually even look at? And what they point me is a paper that's been published two weeks ago, I'm like, this is still too influx, right? Yeah. Because two weeks from now, you're going to point me to a different paper. And honestly, the only thing that I've heard that's worthwhile, if we had to some, I don't know, big enterprise bank says we want privacy. We'll use Solana, and we'll give you a lot of money if you give us privacy.
00:53:43.054 - 00:53:44.830, Speaker C: What we would use would be sapling.
00:53:44.910 - 00:53:48.418, Speaker A: Like Zcash, because it's something a little bit more proven.
00:53:48.514 - 00:53:57.222, Speaker C: It's been around for so long that I feel like the bugs have been mostly weeded out. Zcash was around for, like, four years now, right?
00:53:57.276 - 00:53:58.150, Speaker A: Yeah, ish.
00:53:58.230 - 00:54:07.818, Speaker C: So I feel like there's 50% chance that there's not going to be a catastrophic bug in sapling in the next four years.
00:54:07.984 - 00:54:16.154, Speaker A: Although saplings earlier sapling only came out implemented last year, I think. Because that's an update.
00:54:16.202 - 00:54:16.414, Speaker C: Right?
00:54:16.452 - 00:54:22.702, Speaker A: Like, Sprout was their first one that did have a bug, and then sapling or bug, there was like a missed thing.
00:54:22.836 - 00:54:38.206, Speaker B: Sorry. So on a know, again, a bit lower level on this subject. So sapling is built on the Utxo model. While my understanding is that Solana is account based, you can map one to the other one, I'm not sure it would be as simple.
00:54:38.328 - 00:55:02.014, Speaker C: Well, okay. So the way we kind of think of our runtime is we can actually support more than one virtual machine or more than one interpreter of the state. Because, again, contracts don't have global memory. Right. They're just bytecode. So you would run sapling as a VM inside Solana. And the stuff that you're executing in this zero knowledge environment, that's what the customer wants.
00:55:02.132 - 00:55:09.918, Speaker B: Oh, I see. So it would be inside an app, inside Solana, that within the app, everything.
00:55:10.004 - 00:55:13.566, Speaker C: Is zero knowledge and nobody cares who pays for the gas.
00:55:13.678 - 00:55:15.054, Speaker B: Kind of like a shielded pool.
00:55:15.102 - 00:55:15.682, Speaker C: Yep. Yeah.
00:55:15.736 - 00:55:16.146, Speaker B: Okay.
00:55:16.248 - 00:55:23.294, Speaker A: Do you have any plans to integrate Solana with any other? Like, are you thinking of bridging?
00:55:23.422 - 00:55:39.506, Speaker C: Yeah. Terra is actually building something, which is pretty. But like, IBC is interesting. All this stuff. Again, my experience engineer from Qualcomm, it's very boring. I'm like, this is too. You.
00:55:39.506 - 00:55:42.220, Speaker C: How do you trust, like, millions of dollars to those.
00:55:42.750 - 00:55:45.894, Speaker B: Are there any plans for bitcoin on Solana?
00:55:46.022 - 00:55:49.180, Speaker C: Like TBTC? Yeah, that kind of thing.
00:55:51.170 - 00:55:59.242, Speaker B: Pick your flavor of mechanism. If, you know, a big customer told you, we want bitcoin on Solana.
00:55:59.386 - 00:56:00.730, Speaker C: Is it bitcoin?
00:56:00.890 - 00:56:01.502, Speaker A: Yeah.
00:56:01.636 - 00:56:06.526, Speaker C: Is it actually is. Is wrapped bitcoin. Bitcoin?
00:56:06.718 - 00:56:18.870, Speaker B: Oh, well, I'm not sure. From attack, I think. Depends on who you ask. If you ask the IRS if BTC and WBTC are like kind assets, maybe. I do not know.
00:56:18.940 - 00:56:41.740, Speaker C: But the whole point of why would you even want BTC, right, is because you want this settlement into something that has the security properties of bitcoin, or you're just trading. If it's just for trading, then, yeah. We can give you an oracle price feed at 400 milliseconds and give you BTC exposure. Right.
00:56:44.530 - 00:56:52.494, Speaker B: So you would do it like a synthetic. So if somebody wanted to have access to bitcoin, you would do it like a synthetic? That makes sense to me.
00:56:52.612 - 00:57:02.642, Speaker C: One of our validators, stakefish, they're like part of the f two mining pool. They could just issue bitcoin. It's not like people wouldn't trust them. Right.
00:57:02.776 - 00:57:17.574, Speaker B: To your point, though, I think you made an important distinction in the sense that when people are using bitcoin, they care about the settlement assurances that the thing that you just transacted in, it's very unlikely to get reverted, and it's very likely that your transaction will go through.
00:57:17.692 - 00:57:49.634, Speaker C: It's not that a transaction. No, it's that me as a custodian, I am receiving the bitcoin and it's now settled with me. It's not that you and I exchange in some centralized exchange for bitcoin. I mean, that is where almost all of it trades, right? But to me, the fundamental value of bitcoin is that somebody across the world can transfer a billion dollars or arbitrary amount of dollars and they can settle immediately. And that settlement is guaranteed. Right, with this immensive amount of electricity. Right.
00:57:49.634 - 00:58:00.290, Speaker C: That part, I don't know what TBTC or Wrap BTC doesn't expose that. Right. The settlement risk is hidden from the user.
00:58:00.370 - 00:58:26.026, Speaker B: The security model changes with additional assumptions being baked in. Well, I guess in the case of a TBTC or a bitcoin on Ethereum kind of thing, you're switching your security model from whatever electricity is being consumed in bitcoin to whatever electricity is consumed in ethereum, plus some additional mechanisms introduced in each system, whether that is the fraud proofs and whatnot.
00:58:26.138 - 00:58:32.190, Speaker C: Do you believe that proof of state networks will have the same level of security? No.
00:58:32.260 - 00:58:34.466, Speaker B: I'm a proof of work guy on that end.
00:58:34.568 - 00:58:35.460, Speaker C: Me too.
00:58:37.430 - 00:58:41.730, Speaker A: And yet you have a proof of stake network, don't you? It's a lot of proof.
00:58:42.150 - 00:59:01.320, Speaker C: So security is only one parameter. Right. I honestly believe that proof of work is web scale security. Right. Bitcoin itself has managed a way to capture security at a global level. It is more secure than probably the golden Fort Knox. Right.
00:59:01.320 - 00:59:02.920, Speaker C: Cool.
00:59:03.530 - 00:59:07.058, Speaker A: So thank you for coming on the show, Anatoli.
00:59:07.154 - 00:59:10.030, Speaker C: Thank you, Anatoli, thank you so much. This was super fun.
00:59:10.140 - 00:59:12.506, Speaker A: And thanks, Giorgios, for guest hosting.
00:59:12.618 - 00:59:13.662, Speaker B: Thank you for having me.
00:59:13.716 - 00:59:15.530, Speaker A: And to our listeners, thanks for listening.
