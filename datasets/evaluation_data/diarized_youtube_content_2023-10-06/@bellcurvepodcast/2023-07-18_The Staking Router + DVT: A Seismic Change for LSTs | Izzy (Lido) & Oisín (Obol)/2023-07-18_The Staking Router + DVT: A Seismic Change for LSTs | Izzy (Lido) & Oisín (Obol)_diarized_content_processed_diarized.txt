00:00:00.090 - 00:00:28.866, Speaker A: Alright everyone, welcome back to another episode of Bell Curve. Before we jump in, quick disclaimer. The views expressed by my co hosts today are their personal views and they do not represent the views of any organization with which the co hosts are associated with. Nothing in the episode is construed or relied upon as financial, technical, tax, legal, or other advice. You know the deal. Now let's jump into the episode. Hey everyone, welcome back to another episode of Bell Curve.
00:00:28.866 - 00:01:00.986, Speaker A: We have a very special episode lined up for you today. So we're going to be talking to Izzy who is a contributor to Lido Dao and Master of Validators. What an interesting title. We're also going to be talking to Asheen who is one of the co founders at Oval Network. They, along with other folks like SSV, are pushing forth this idea of DVT which stands for Distributed Validator Technology. Super interesting idea. Similarly to the Hasu episode, what we're going to start with is kind of from a high level about why self and solo stakers are so critical to ethereum, the credible neutrality and decentralization of the network.
00:01:00.986 - 00:01:52.174, Speaker A: And then we're going to be talking about how DVT actually works in practice, how it enables more solo and self staking and what that could potentially look like in the future. We got a very interesting timeline from him, so stay tuned for that. I thought the staking router, I'm going to shout out Izzy here, I thought he gave one of the most intricate detailed descriptions of how the staking router is going to work in practice that I've heard anywhere else. And we really got into the nitty gritty of what some of these modules might look like and the idea of an internal fee market within that staking router, which I found fascinating. All that to say, I'm going to do a super clickbaity thing and tell you you got to stick around to the end of this episode. We talked about the potential of a restaking module within the staking router and that was I'm still sort of thinking about the consequences of that part of the discussion, so definitely make sure you stick around for that. All right guys, that's enough for me.
00:01:52.174 - 00:02:11.400, Speaker A: Talking into your ears here. Let's get onto the show. All right, everyone, welcome back to another episode of Bell Curve. I'm joined as always by my fearless co host, Miles O'Neill. Today we are lucky to be joined by Osheen who is the co founder at Obel. And today I'm also joined by Izzy who is a contributor to the Dao and Master of Validators at the Lido Dao. So guys, welcome to the show.
00:02:12.330 - 00:02:13.720, Speaker B: Thanks for having us.
00:02:15.290 - 00:02:51.074, Speaker A: Super excited about this, guys. And really want to get into, I think the meat of this podcast. We're going to be focused. Know we've been very focused so far this season on this idea of decentralization as offense and want to specifically dig into this concept of DVT and how that's going to integrate with the staking router. But before I think we even get into that nitty gritty, it would make sense to start a little bit at a higher level. And I would like to just ask the open question to the both of you. Solo staking is something that's seen, I think, as being very critically important to Ethereum and the decentralization of the network and just kind of a high level question.
00:02:51.074 - 00:03:06.520, Speaker A: I'd love to get a state of the Union as it is. On the state of solo staking. How important is it? What are some of the design decisions that Ethereum is making? So just want to communicate to listeners what is the state of solo staking today and how are we encouraging that moving forward?
00:03:08.410 - 00:04:00.722, Speaker C: Yeah, I can have a go at that. I think the state of solo staking at the moment is in a pretty good place, or at least in my perspective. It's starting to turn up the words and start to kind of see a lot of growth. I think over the two plus years that the beacon chain has been running, at the very beginning it was very solo stake dominated. There was lots of true believers that were in early from Genesis. And then over the kind of the course of a year or two, a lot of the growth in staking has been more in the big protocolized staking, the liquid staking protocols, particularly when there was no withdrawals and stuff, it was kind of more safe to lean towards a liquid representation of stake. But now that we have the merge behind us and distributed validators are starting to grow, we're starting to see lots of liquid staking protocols in particular really double down on getting more and more solo stakers involved.
00:04:00.722 - 00:04:19.680, Speaker C: And that's probably where I see things at the moment. Is there's a renewed interest in getting more solo stakers involved and getting a lot more client diversity in there than we currently at. So I maybe want to toss it over to Izzy and see how he sees it from Lydia's perspective. But I'm relatively optimistic at the moment. I think it's the busiest. It's been a number of years.
00:04:20.850 - 00:05:35.942, Speaker B: Yeah, I think Ashin makes a good point that we kind of started off with all of the protocol diehards are like the people that really believed in this effort who also had the technical acumen and that's kind of like the Genesis validator set which consisted majorly of solo stakers. And then we kind of saw a transition as proof of work. Ethereum and proof of stake ethereum were running in parallel where, let's say larger organizations, or at least more professional staking organizations started to get their presence on the network largely because of the technical acumen. And so it's much easier to start, for example, validating on a new network if you already know more or less how to validate. Whereas for solo stakers it's kind of like a natural ceiling of how many people are technically able and willing to take certain amount of technical risk involved with regards to running Validators and also putting that much Ethereum at stake. And I think there's like a couple of events that have happened since the genesis of the Beacon Chain back in December of 2020 that have really made it a lot easier for solo stakers to participate. So two of those are obviously two of the large Ethereum upgrades that have happened.
00:05:35.942 - 00:06:22.438, Speaker B: So the merge itself and also the introduction of withdrawals that have meaningfully reduced the technical barriers and risk surface area that solo stakers would need to kind of be wary of. There are also more minor upgrades throughout this. So, for example, we started with Ox withdrawal credentials. So like the so called BLS withdrawal keys and now Validators are spun up with Ox one smart contract withdrawal credentials that are a lot easier to reason about and a lot easier to secure and things like that. So I think that has also made solo staking easier. And then of course, we've seen a lot of maturity in terms of the tooling and client software that exists around running Validators. Arguably in the beginning, client diversity wasn't so great because there was obviously a few clients that were more mature than others.
00:06:22.438 - 00:07:30.810, Speaker B: But since then, we've seen not only new clients enter the space, but also the really amazing evolution and growth of a lot of the other clients. So there's at least four or five clients that are almost at parity, I would say, both in terms of feature set and in terms of how easy they are to use. But we also have tooling that rests on top of or to the side of these clients. It might be like docker installations that you can use or front ends that allow you to kind of set up your Validator however you want to set it up with things like Avato or Dapnode. So all of these kinds of things coming together are culminating in what I would say is kind of like a reintroduction of a new set of solo stakers to the protocol. So people that have been learning about Ethereum and proof of stake over the last two and a half years, but also now that have much more tooling and knowledge at their disposal to be able to feel safe enough and competent enough to dip their toes in. So we see that liquid staking protocols or staking protocols in general are also meeting this kind of like, renewed demand for solo participation, which I think is vital to a network like Ethereum by adding new ways for people to participate.
00:07:32.510 - 00:08:32.960, Speaker A: Yeah, I think that's really well said. And Izzy, I think you touched on a critical component which is just reducing the complexity overall for solo stakers. And maybe we can get into almost a setup of what a solo staker requires today. But I would love to also get a sense of Sriram, who Miles and I are going to talk to on the next episode, who's obviously the founder at Eigen Layer talks about market forces. Actually, you might have DApps in the future that want and will actually pay a premium for a block space that's more decentralized and perhaps run by solo stakers. I think the idea here being that what I'm trying to get at a little bit is sort of the core reason why Ethereum wants solo stakers which I'd always thought of as you want this to be a robust network both and robust in some senses mean it's geographically distributed and it's not easily shut down against some sort of government intervention. I mean, how do you guys think about the sort of need overall for Ethereum to be decentralized in the key role of solo stakers? Is that about right or are there other components that I'm missing there?
00:08:34.130 - 00:08:43.998, Speaker B: So maybe I can take a first stab at this and Ashin can follow up. There was actually a recent podcast that Justin Drake was on where he had a take on this that was perhaps a little bit spicy.
00:08:44.094 - 00:08:45.074, Speaker C: I listened to this.
00:08:45.112 - 00:08:47.522, Speaker A: Yeah, this is yeah, so he basically.
00:08:47.576 - 00:10:22.660, Speaker B: Said the two most important things for or I mean, paraphrasing, or at least the way that I understood it, just to make sure that he doesn't get mad at me. The two most important things for a truly decentralized and robust blockchain are censorship resistance and credible neutrality. And decentralization is basically the most surefire way to get those things right, but it's not absolutely necessary if you somehow find a way to get those in another way. To a certain extent, I think that's true. But I think the fact that decentralization is so important and baked into the ethos of a lot of these kind of initiatives and projects that we're doing right to kind of revitalize financial infrastructure means that decentralization always needs to exist, at least in the form of the common person, being able to participate in the network as a buffer against credible neutrality or censorship, resistance, at some point being capturable or attackable. You kind of always need this security mechanism, if you want to call it that, or perhaps in the parlance of American constitutionalism, like a Second Amendment right for a militia of people that are always going to be there to uphold the values of this kind of digital sovereign. And I think it also really ties into this idea of what we're trying to do with crypto right, the idea that everybody can participate by helping disintermediate power and the best way to do that is by having as many people involved as possible.
00:10:22.660 - 00:11:33.290, Speaker B: I know that might sound a little bit contradictory coming from somebody that participates in the Lido Dao, but what we're trying to like actually, Mike, I think this might have been a conversation that we're having a couple of weeks ago. But there's kind of like this idea like there is in, you know, where there's this quote that life finds a way, like money finds a way. So what all of these systems are trying to do is ensure that there's, like, risk management and principled ways to, let's say, curb the most pernicious effects of the financialization of things while empowering people to participate in these ecosystems, to secure these ecosystems, and to have a voice and power in everybody's collective future. And so I think to that extent that's why Solo staking is vitally important and why networks like Ethereum that have to a large extent bar some hiccups, let's say, or technical design decisions that have been built around the idea that anybody should be able to run a node and that many, many people should run a node is why they're so successful and have so much cultural mind share within blockchain ecosystems.
00:11:34.110 - 00:12:10.902, Speaker A: Izzy, you just reminded me that I even secured your permission. I love that quote, so much money finds a way, that that was going to be the title of this season and then I just forgot. So I'm glad we at least worked it into this episode because I like that idea. So much of money finding a way. And I wanted to start this conversation by just you did a great job just highlighting why solo staking is so important. And I think one of the big hiccups to that, or roadblocks, if you will, is there's a limited number of people who are competent out there to run the sort of hardware requirements. And that's what I want to segue into DBT, which is short for Distributed Validator Technology.
00:12:10.902 - 00:12:35.470, Speaker A: If you hadn't heard about that before, my guess is that you're going to continue to hear about that quite a bit in the coming months and years. And Sheen, you and Colin at Obel and other platforms like SSV have been pioneering this idea. So can you kind of just give us the broad strokes of what Distributed Validator technology is and how it actually accomplishes that problem or that solution of encouraging more solo staking?
00:12:36.130 - 00:13:25.118, Speaker C: Yes, happy to. So Distributed Validators are the idea of running an Ethereum or any other type of blockchain validator on more than one machine instead of them just being like one single server. What's nice about this is you can then introduce fault tolerance where if one of your machine goes down, you'll still be online. This is something we don't have in the Web three and proof of stake space as of yet, to my knowledge, the vast majority of proof of stake chains. Your Validator is one private key that signs messages and different chains punishes downtime more and less severely. But Ethereum is one that will punish you if you're down for even kind of six minutes. So what Distributed Validators do most validators and most servers, machines die all of the time.
00:13:25.118 - 00:14:10.406, Speaker C: Don't expect your server to stay on and not need an update and never need touching for years at a time. And if you're a solo staker, you're basically on call 24 7365. And if you want to have uptime like the professionals, you need to have redundant hardware usually ready to go if a drive dies or you want to be on call, being able to be woken up in the middle of the night if you go offline. None of these are super scalable for a one man solo operator or something like it. So when you have Distributed Validators and you're now working in groups, if your node goes offline, no big deal. The other three will keep it up and keep it online. This is useful just generally for having better uptime.
00:14:10.406 - 00:14:58.890, Speaker C: The example I can give is I've been running as a Solo staker and as partly as an Enterprise Validator. Since Genesis. The enterprise nodes I've deployed have 99.9% uptime for the last two plus years, whereas the solo node is at about 97% because machines die and I'm not always around. But with DVS, the goal is that you can get up towards three nines and more of uptime and a group of home stakers should be as effective as your professional load operators and that's kind of what DVS aim to achieve. And it's the goal is that Solo Staking, you more or less never go down so long as you have a group of people working with you or Squad Staking is often being kind of picked up as the naming for it. Yeah, I think that's kind of short description of Distributed Validators.
00:15:02.190 - 00:15:58.110, Speaker A: That'S really excellent. I want to actually get a little bit more in the weeds with you here and one of the questions that I think Miles and I want to drive towards here is just exactly not even in the current state, but the future state of how low could we get the hardware requirements implementing something like DVT? I mean, is there a future here where solo stakers could be running some of this stuff basically on their cell phone? But before we get there, I actually want to just make this a little bit more concrete for what Obel, the protocol looks like. And I know there are sort of four main components here. So you've kind of got the DVT launchpad and user interface, you've got a new custom middleware client called Karon, I believe I'm pronouncing that correctly. Obel managers and then Obel testnet. So could you kind of just walk us through the core components of the protocol and if I missed anything? Yeah, I just want to give listeners a little bit more of a concrete sense of what this is going to look like in practice.
00:15:58.770 - 00:16:59.534, Speaker C: Yeah, absolutely. So I'll start up at the high level with the user interface or what we call the Distributed Validator Launch pad. So this came out of a problem where you want to work with other counterparties. But you need to trust them or at least you need to figure out what you're going to do together. You're going to make a proposal, what are we going to validate, how many keys are we going to validate on what chain? Is this going to point out a certain particular withdrawal address? Is it in Lido or otherwise? So we originally kind of did all of this on the command line. You would kind of prepare this, look at like long bytes of strings and stuff, but it's really difficult to kind of be confident that you're setting up what you expect to set up with other counterparties if you're just looking at kind of output on a terminal. So we developed the Distributed Validator launchpad, which is a spiritual successor to the Ethereum launchpad that was put in place by the Ethereum Foundation and Consensus and others at Genesis.
00:16:59.534 - 00:17:35.230, Speaker C: And the idea behind it is it's meant to be user interface to walk you through the process of setting up a Distributed Validator and to use it, you log on, one person makes the proposal. So they might say, you know, Mike, Miles, Izzy and myself, we're all going to run a Distributed Validator together. I put in all of your Ethereum addresses. I say we're going to make ten Validators and it's going to be on main net and we're going to send them all to this Nosa safe address. And then I send you all the link. You guys can eyeball it, you can click View on Ether Scan. You make sure I'm not trying to send this to my ledger or trying to kind of bait and switch you guys with what we're supposed to be setting up.
00:17:35.230 - 00:18:21.274, Speaker C: And once everyone is kind of approved and signed off that they are happy with the terms and conditions, you then move over to the command line and to Caron, which is the software client. Caron is a middleware. It sits in between your Consensus client and your Validator. So it more or less is just an extra piece of software that you put on your Node, kind of like how mevboost you kind of add in. And one of the jobs it does is it will facilitate the key creation for your Validator. Meaning what's special about a Distributed Validator is there is in the happy path or the way we encourage people to use it, is there's no one human with access to the private key. Instead, the four of us would all come together, we'd run a bit of software and it would spit out, I call it four one thirds of a private key.
00:18:21.274 - 00:18:56.742, Speaker C: It's not quite exactly how it works, but each of us have a piece of the private key and we need three of us to put together the full private key that even day one has. You in a much safer place when it comes to getting compromised or of someone like stealing your private key and getting you slashed. And at this point you're now looking at like you have a deposit data, you want to activate that Validator. Maybe this Validator is working as part of a liquid staking protocol. Maybe it's not. Maybe it's just a custom one. The Obel manager smart contracts I alluded to these are splitter contracts for the most part.
00:18:56.742 - 00:19:46.434, Speaker C: We have a few different varieties in the works that we're hoping to kind of show more of the world soon enough. But this is the idea of do we guys want to split this 25% each way? Are we all contributing different amounts? Maybe we don't want an equal split. Maybe we're doing this on behalf of somebody else so that somebody else gets 90% and we're splitting ten. That's kind of where the manager contracts come into play. And then the test nets is tooling related to letting people run this and ensure that this is going to work for them, get them to kind of learn how to mine DVS and work with one another so that they can do this in production. Well, I will say that this is a tour of kind of where we're at currently with know obal as a protocol. One of the things that we didn't mention here more broadly is in the kind of earlier versions of Distributed Validators, you need to trust your counterparties.
00:19:46.434 - 00:20:24.082, Speaker C: You need to kind of work together. If Miles goes offline, you have to kind of hit him up on Telegram, be like, hey man, your node's offline, can you go sort that out please? But if you leave it offline, there's nothing a whole lot we can do to kind of force anything to change. But in the future we're working on succinct proofs of who is online and who is offline. And that eventually it's like, okay, if Miles is off for a week, he gets penalized and we get all the rewards. So we're not as upset if he's offline. But to do that in an ungamable manner that you can't kind of cheat is quite difficult. So that's why we kind of referred to that as our Distributed Validator protocol or kind of our future version where we get this.
00:20:24.082 - 00:20:29.320, Speaker C: It's all in cryptography. There's no human kind of social layer consensus going on.
00:20:29.770 - 00:21:34.780, Speaker D: I think that's a great overview of kind of where we're at today. And I think about the barriers to, I guess, making self staking more prevalent. I think about, one, the hardware requirements, two, the actual capital requirements, right? And you've already made great progress in bringing that down from 32 E. And then three, just maybe the natural incentives for folks that are not naturally passionate about this technology and decentralization of the network. But just starting with those first two, if I could ask you to look forward into the future a few years. I'm very curious to get a sense of how low we could get the hardware requirements as well as the capital requirements. Is this something that one day we could run on our phone? Like the Salana phone or some version of that? Or if I have one ETH, at some point will I be able to maybe mint an LST from the staking router? So, love to ask you project forward a few years.
00:21:36.830 - 00:22:06.902, Speaker C: 100%. I don't even think it's a few years more likely. So the first one of running on a phone, I think you make a great point there. This is something that Distributed Validators I think is very beneficial for there's been lots of people that have worked on building clients to run on very low resource hardware. But you don't want to cut it too fine, you might say. Or at the very extremes, particularly with mobile, your Internet is connecting disconnecting. You're not going to be on 24 7365.
00:22:06.902 - 00:23:06.274, Speaker C: So we think running low powered validator nodes is much more suited to running a Distributed Validator than to run something like full and solo, particularly if you're a solo staker trying to run this all yourself, you're like, I want fault tolerance, but I don't really want to give it to someone else. It's like, cool, I run them in my house, my friend's house, my phone, and some other thing. That's kind of how you kind of have lots of fault tolerance. And then on the capital requirement side, I think that is also something that can be mitigated by distributed validators because you don't have to have the same amount of bond if somebody isn't like God mode over this validator. And have total control if they're just kind of one piece. And you assume that even if they were offline malicious, worst case scenario, they're not harming the validator too broadly. So long as there's not a few of them that are offline, it really reduces the risk profile of actually bringing them in and delegating some other stake.
00:23:06.274 - 00:23:32.510, Speaker C: And I maybe want to throw it over to Izzy on that regard because collateral requirements for staking is kind of a big testy subject when it comes to delegated staking. And should validators be collateralized? Lido famously is not. But they have plans both for collateralization and also for how solo stakers can get involved with and without collateralization. So maybe you want to see how he thinks about DVS and its impact on hardware and collateral.
00:23:33.010 - 00:24:47.734, Speaker B: Yeah, absolutely. So maybe even going back to one of the original questions, which is what is the presence of solo stakers on the network right now? What does that mean for the network? What are the barriers for solo staking existing? Because that ties into the whole question around bonding requirements. So apart from the technical complexity and the operational complexity which is much reduced, and also the technical surface risk, which is also much reduced due to the recent ethereum upgrades, we contend with the issue of financial approachability of the enterprise of running a validator. Right? So obviously if you were solar staking, you needed to not only have 32 ETH in order to be able to run the validator, but you needed to have it in the beginning. So it's really hard to borrow it and then run a validator. And so what a lot of liquid staking protocols basically did was allowed people to participate in the grander, let's say, collective enterprise. Not in the sense of a business sense, but like a community effort of securing the ethereum network without necessarily having that minimum 32 E obviously distributed, validators like Obel is working on or even like SSV is working on.
00:24:47.734 - 00:26:06.702, Speaker B: Allows users to do this by putting their money together. So you split the 32 ETH into multiple smaller pieces of a deposit depending on how many people participate in the cluster. And then therefore you can have collective participation of one full node via more participants as opposed to one person needing all of that capital. And then you also have permissionless. Liquid staking models like Rocket Pool, which also has been very successful in terms of adding Net new node operators to the network. And I think they should be lauded for that, where basically you had a capital collateral or bond requirement where a node operator, or at least a capital allocator associated with his node operator puts up a certain amount of money, and then the remainder, let's say, of the balance on that validator can come in from injections, from stakers. So normal people who would participate in LSTs in general when we started with and by we, I mean like Ethereum staking as a whole with permissionless bonded validators, this bonding requirement had to be very high, right? Rocket Pool started with 16 E and now we've seen that liquid staking protocols have started to reduce this bonding requirement based on a number of different factors.
00:26:06.702 - 00:27:01.250, Speaker B: So one is obviously the maturation of protocols themselves, but also the underlying network layer. The second thing is that there are upgrades in the works that will potentially make it so that if a node operator disappears or is unrespondent or loses access to their validator keys, for some reason the network will be able to issue a command to this validator via the execution layer. Which means that liquid staking protocols can do this via smart contracts to eject these validators, right? And so we call this like triggerable exits. And this is in an EIP that will hopefully come sometime next year, a couple of hard forks perhaps after Dankoon, or maybe even they meet at hard Fork after Dankoon. That in concert with a lot of other things. For example, that historically we actually haven't seen that many slashings and slashings. It's very difficult for them to ramp up to the amount of slashing.
00:27:01.250 - 00:28:14.294, Speaker B: Not a full 16th, but up to 16 ETH on a validator has basically led protocols to do like a risk adjusted implementation of what is the sizing that bond should be. So obviously the main. Thing that reducing bonds means is that you lower the barriers to entry for interested participants, right? It also means that there's a corollary that the people that have already put up the bonds can basically just multiply the amount of validators that they have using the existing capital by rolling it forward. But that's also not necessarily negative. Providing that these people are actually independent node operators and smaller organizations and distributed validators adds an even more kind of interesting aspect to this. If we get to a point where risk analysis, for example, after we have EIP 7002, which is the triggerable exits functionality that we were discussing, allows you to exit validators and this happens, let's say, within a year and everybody collectively believes that this will happen within a year. You can say that on a worst case scenario, if a liquid staking protocol were to launch with a very small amount of bond, like, let's say four ETH or less, and that this upgrade happens within a year, realistically.
00:28:14.294 - 00:29:19.054, Speaker B: Speaking, it's very unlikely that such a large amount of node operators will go offline that that amount of money or loss that they cause to the protocol will be so substantial as to not take that risk to lower bond that much and DVS allow you to lower it even more. So if you say that you have four ETH bond for a validator you could split it across four participants, for example, like in a three or four model like Oshin was saying. Or you can split it even more into like a five or seven model. But you can maybe even say that due to the fact that it's so much more difficult to get four people, let's say, or three people because that's the consensus threshold to align on doing something nefarious. We lower it even more because this idea of multiparty agreement needing for something to happen, it's a lot easier to make people agree for things that are mutually beneficial and a lot more difficult for them to agree on something that's like malicious or nefarious. So that even unlocks another potential in further reducing bonds and further increasing, let's say, the breadth of economic actors I. E.
00:29:19.054 - 00:29:24.190, Speaker B: Solar stakers that would be able to, from a financial perspective and interested, participate.
00:29:24.930 - 00:30:32.310, Speaker D: I think that's a great overview and can see the direction we're going here. But I think I have kind of one more question before we move on to the staking router. And that is once we get these barriers as low as possible both on the hardware, the capital, then it becomes a question of how do you productize or package this in a way with a value prop that resonates with a larger population of users maybe beyond this kind of crypto native and very passionate folks about Ethereum, right? I'm just kind of thinking out loud here, but is there ways that you could make an app better if you were running a DV on your phone or something else like that or we've talked a lot about the principal agents problems associated with LSTs. You could argue that you mitigate a lot of these problems by minting your own LSTs as part of a permissionless set. Just curious to hear early thoughts on how you plan to really frame this value prop to a larger set of users.
00:30:33.770 - 00:31:22.242, Speaker C: Yeah, I think I'll start with that one. And maybe the first thing I would say is that the value prop to different staker entities is quite different. Not everyone kind of sees the same pros and cons in DVS, we've talked a lot about the solo staker. So for those guys, it's high up time for my own node, if I'm so lucky, is that I have a full Validator or a route into the liquid staking pools if they don't have either the collateral to either have a full one or to have the bond for some of the other protocols. So that's kind of the solo stakers benefit. The benefit to the liquid staking protocols I'll touch on maybe lightly and maybe Izzy can expand on. But for those protocols, it's about derisking the stake that they give to their operators.
00:31:22.242 - 00:32:13.362, Speaker C: Right now they're making a huge amount of trust in the operator that they allocate millions of dollars of stake to, and this currently forces them to pick the reputable big brand name, nowhere going nowhere type of node operator. But they'd much rather push the kind of extremes of that and go down the kind of to the more solo staker. But it's kind of hard to take that risk if they have total control. And then the one we haven't talked about is maybe the more centralized operator, which does exist and is a nontrivial part of the stake in Ethereum. For these people, Distributed Validators is more on the cost side and the operational side is what they really gain. Having cheap software based fault tolerance really can impact their operations. Particularly in how they've deployed.
00:32:13.362 - 00:32:58.542, Speaker C: A lot of decentralized providers because there's no safe way to run a backup have not moved towards more bare metal or on premise. Machines and instead kind of stay closer to the cloud side, because if a machine dies in the cloud, you can kind of press a few commands and detach the disk where the private key is and be like, cool, I'm back in charge of that private key. Whereas if it's on a real machine and a real server somewhere and that thing dies, you can't talk to it, you can't get to it, you need to plug it out at the wall. So most of them haven't gone that way, but with Distributed Validators, they can move to more on prem. They can kind of come out of the cloud, they can keep all of their keys with separate people. They don't have one human that has kind of the keys to the kingdom. And even when they're upgrading versions.
00:32:58.542 - 00:33:15.402, Speaker C: They don't have to take their software offline, and they can do general maintenance. The one I always talk about is you're not pinging your DevOps in the middle of the night to be like, hey, you've an outage. You need to get to it within an hour. For our SLA, it's like, oh, we've lost one. Fix it in the morning. Okay, we've lost two. Hit the alarm, get out of bed.
00:33:15.402 - 00:33:31.502, Speaker C: But for the most part, yeah, they can save hardware, they can save risk, and they can save operational expenses with DVS, and that's generally the case with most others as well. But, yeah, there's no one benefit for everyone. I guess each kind of type have a different kind of pro and con to it.
00:33:31.556 - 00:34:06.346, Speaker B: That's a really interesting point you brought up to the end in terms of, like, we're going to have to seriously reconsider what kind of alerts wake us up in the morning, because if we're running a fleet of distributed Validators and our alerts are still like, the Validator has been down for 1520 minutes, that's, like, catastrophic. It means there's at least two people out of four that aren't doing their job, or three people out of seven or something like that. So we're going to really need to change how we do our monitoring and alerting. But on the other hand, it also means that we'll have to wake up a lot less than we do right now.
00:34:06.448 - 00:34:07.740, Speaker C: Yeah, that's the hope.
00:34:08.990 - 00:34:51.506, Speaker A: Nice. That's a good goal. As a lover of sleep, I am pro that idea. So I want to transition here into talking a little bit and just sort of set the stage for why we're talking about self staking and DV technology within the context of the staking router and Lido. And one key theme of the season, I think hasu put it best in the last episode is this idea of decentralization as offense, especially for protocols like Lido or other liquid staking providers. Because Lido does something that's so core and close to the metal of ethereum that in order for it to continue to grow its market share and continue offering great services to folks that want to stake, we want to make sure that it's as decentralized as possible. And there are two key ideas.
00:34:51.506 - 00:35:18.190, Speaker A: I think we really want to dig into the staking router a little bit here, but I also want to, Izzy, just poke at you for sort of a high level on dual governance between Lido and and Steve as well. Hey, everyone. We've got a great episode here, but before we do, I just wanted to give a quick shout out to Permissionless. This is the biggest and best conference in all of DFI. It's the one that we do with Bankless, who's a great partner for us. Last year we had almost 7000 people there in West Palm Beach. We are moving this year to Austin, Texas, from September 11 through the 13th.
00:35:18.190 - 00:35:45.034, Speaker A: And if you are a listener of Bell Curve any of these last five seasons, this conference is basically custom made for you. We're going to be talking about liquid Staking, the theme of this season. We've got a bunch of great panels on mev. If you listen to the app chain thesis, we've got a bunch of Cosmos folks out there in full force. We're talking about the converging architecture of Salana, the roll up space in ETH and Cosmos. So I would love to see all of you there and to reward you for being such great listeners to Bell Curve. You get a special 30% off code.
00:35:45.034 - 00:36:01.742, Speaker A: It's Bell Curve 30 that'll get you 30% off tickets. Click the link in the show notes and then head over to the permissionless site and make sure that you get your ticket today. Again. That is Bell curve 30. Click the link in the show notes. But let's get into the nitty gritty of the staking router. And for context, this actually went live.
00:36:01.742 - 00:36:59.714, Speaker A: So Lido V two, you would know the exact date, but I think it went live a couple of months ago and that introduced withdrawals and this idea of the staking router. And actually before we even get into what that is and how that we've been sort of teasing how that applies to DVT and having different modules. I actually want to get a sense of how things worked before so we get a sense of what the original state for something like Lido was and then how the staking router is going to be a massive improvement there. And I've actually got for you here, you can bear with me while I share my screen, a helpful diagram that maybe we can walk through. And for folks who aren't listening, I can try to describe a little bit about what this is, but it's a diagram of some of the core processes of Lido. And so Izzy, if you wouldn't mind just sort of walking us through this diagram so we could understand how it used to work within Lido and then we can go through the shift that has taken place with the introduction of the staking router and just explain what that is.
00:36:59.912 - 00:37:47.794, Speaker B: Sure. So let me try to read through this diagram. Now, basically the original version of the Lido on Ethereum protocol was what we call now the curated operator set. It consists of a node operator registry that is made up of a list of curated I e. Permissioned by the Lido Dao, meaning that they are proposed to the Dao and the Dao ratifies the acceptance of these node operators joining as users. Basically that can use the protocol, but on the validator side. And this node operator registry creates or the participants in the registry rather create validators undeposited to.
00:37:47.794 - 00:38:29.694, Speaker B: So just the validator keys themselves and the deposit slips. They submit these to the on chain software, the protocol through the node operator registry. And this creates a buffer, let's say, of validators that are waiting for deposits. When a user goes to the lido smart contracts, either via a front end like lido DeFi, or just by interacting with the contract directly. They submit ETH to the lido on ethereum smart contract. And this ETH goes to a buffer, basically. And whenever there is enough ETH in the buffer for it to constitute a certain increment, let's say, of a whole number of validators I e.
00:38:29.694 - 00:39:20.218, Speaker B: Like if you have eight ETH and you can't fund a validator with that, you wait until there's multiples of 32. It gets deposited to the deposit contract on the execution layer side. Meaning like the Ethereum 2.0 as it used to be called deposit contract, where all of the staked ETH goes. And actually it's this really interesting accounting thing with Ethereum where all of the staked ETH is just sitting there on a contract on the execution layer and basically gets deposited at the same time or mapped to a certain validator key that has been input into this buffer ahead of time by node operators and node operator registry. So there is a staking allocation mechanism that happens behind the scenes on chain. Like none of this is off chain, whereby the node operator with the least amount of currently active keys is always prioritized for a new stake.
00:39:20.218 - 00:40:13.306, Speaker B: What this means is that operators that joined the protocol earlier. So, for example, like when the protocol started back in December of 2020 or January 21, if say that they're all running 1000 validators each right now and there's a new cohort of node operators that join the new node operators get all of the new ETH provided that they have submitted validators to the buffer until they catch up with the previous cohort. And then it's like a round robin fashion. This was a design decision that was purposely made to basically distribute staked ETH as fairly as possible across the entire node operator set. So we find ourselves now on the 13 July 2023, like two and a half years later, the curator registry basically consists of 29 operators. Right now, technically it's 30. But the two merged in a business or fiscal sense sometime after they joined as node operators.
00:40:13.306 - 00:40:58.106, Speaker B: And so the protocol treats them as one. And there is a really good distribution of stake across the set. So no one node operator has vastly more amounts of stake being operated through them as a result of Lido stake deep demand than other node operators, unless they've specifically, for example, chosen not to add more validators. And there might be business decisions behind that or scaling decisions behind that, et cetera. And just one thing to note is that after ETH was submitted to the Lido contract and before it actually gets deposited, there's two kinds of off chain things that happen. One is a deposit security module and Oracles. These are off chain mechanisms that end up having on chain impacts.
00:40:58.106 - 00:42:05.758, Speaker B: So the deposit security module is basically the bot that determines when to deposit new ETH to these validators. It looks at a couple of things like what's the current gas price? So if gas is in the hundreds, maybe it won't do deposits until gas goes down a bit. And it also double checks the integrity, let's say, and the correctness of the data that has been submitted by node operators so that something incorrect isn't done. And the Oracles are responsible for reading, obviously the balances of validators on the beacon chain and this is how the whole rebase mechanism for SDE works. But from a staking perspective, they're also responsible for making sure that the contract has updated information with regards to how many validators each node operator is running in terms of how many are active. So this was V One and the kind of large improvement that I would say that happened year, which is one of the two main components of Lido v Two, which was the upgrade to the Lido and theorem protocol, I think on the 15 May. So One was obviously adding the support for withdrawals and the second one was the framework for the functionality of the staking router.
00:42:05.758 - 00:43:11.526, Speaker B: So the staking router itself is kind of like an architectural decision about how the Lido protocol should work. It took what was previously happening in Lido v one, which is this curated node operator registry and it encapsulated it into the first of. Eventually, I hope, many modules that might be built by Lido contributors might be built. By third parties. It might be built by some conjunction of the two or collaboration of the two and basically transforms Lido from a very specific thing into a thing that allows you to create plugins that attach to the main set of lido smart contracts and then therefore allows lido to work more like a marketplace of stake allocation rather than a very specific thing. So when a user goes and eventually, let's say, deposits ETH into Lido as it is right now, the staking router will make allocation decisions for which modules the stake should go to. It'll do this at least in its first iteration, once there are new modules.
00:43:11.526 - 00:43:40.850, Speaker B: Right now there is only one module. It's the current curated operator registry. It'll do it in a very similar fashion to how it allocates stake between node operators within a module. So let's say that there's a new module that's created tomorrow. This module is running using distributed validators and allows solo stakers to participate. This module will have a total maximum allocation threshold, which is in terms of the total amount of stake that flows within the Lido protocol. So let's say 1% of stake within Lido.
00:43:40.850 - 00:44:59.078, Speaker B: This threshold will be configurable and manageable by the dow itself. And so if the new module is created tomorrow and it has 0% of total Lido stake in it, and its maximum threshold is 1%, all of the new stake that gets added to Lido basically will flow to this new module until it reaches that 1%. Once it reaches that 1%, the curated operator registry, which is like the mainstay module, will act as a bucket and pick up any additional stake until the dow either adds new modules or decides to change what the maximum threshold for the second module is. And this process will repeat as more modules are added and can even change it depending on sorry if you add modules that work slightly differently, but maybe we can get to that a bit later. So the main change is that V two brings the staking router and the staking router is a framework for designing validator sets that can attach to the LiDAR protocol. So instead of saying we will have one vault that works this way, one vault that works with permissioned node operators that do not post bonds, one vault that works with non permissioned operators with a strict bond model. We create the staking router which allows you to design your own vaults.
00:44:59.078 - 00:45:19.300, Speaker B: And then these vaults can be attached to the LiDAR protocol based on the Dao decision. So the Dao would need to do like a security assessment, a risk assessment, and probably some sort of like smart contract audit to figure out if it's safe. It would then assess the economics and the tokenomics of these proposed modules and then once they're added, it can manage what the stake allocation to these modules is.
00:45:20.470 - 00:46:15.170, Speaker A: That's Izzy super helpful question or explanation there and I'm going to get into a whole bunch of different questions. There one other way I think it was helpful for me to think about this and if you're a little bit more financially minded or comfortable looking at a balance sheet, we're actually looking at a very high level, illustrative sort of version of a balance sheet of Steeth both before and after the staking router gets implemented. This shout out to Adrian from Steakhouse Financial. This comes from a blog post that he wrote, but it sort of shows you this idea of the assets and liabilities of Steeth pre staking router. You see that the asset is all this one big green block which is staked ether in the curated validator set or the whitelisted validator set that LiDAR used to have. The liabilities are Steeth because that can be withdrawn. And then there's a surplus, which I guess is kind of like the insurance fund or maybe the treasury or something like that, but there's some surplus in the case of a slashing event or something like that.
00:46:15.170 - 00:47:23.878, Speaker A: Now we sort of seen a transition when Lido V two has actually happened. And with the staking router, instead of the assets of Steeth being one big green block, you can see there'll be multiple different blocks of different types of validator sets. So you could have professional node operators, maybe that's the whitelisted set that we have today. But the first question that I have for you guys is if you had to take a poke or a guess at some of the modules that we could expect, right? One is DVT and this is finally how we're going to connect this idea of the staking router and DVT because that feels like an obvious module. There also could be, I know in the blog post on HackMD that LiDAR wrote, they suggest there could be a community set as well. Dows feel like another obvious fit, perhaps even like a more institutional set of Validators, something like that. So I'd love know Izzy, I know this is know very new and there's probably some stuff that you might not be able to mention as your position as master of, but like could you kind of give us a sense of the diversity of what some of these modules might actually end up looking like in practice?
00:47:23.974 - 00:48:30.302, Speaker B: Yeah, that's a really great question. All of this is still to a certain extent up in the air because the way that it'll work from a governance perspective is that there basically needs to be a module design presented to the Dow. It'll kind of look like a technical architecture design record probably with accompanying technical specs and maybe even like a business case. And then the Dao decides whether these modules get added or not. But the really kind of important and the linchpin here is that although the diagram we're looking at kind of separates like you said, like the assets column into different node operator classes. So professionals, small groups, dows, solo stakers and things like that, modules themselves are more like channels for node operators as opposed to segments of node operators. What do I mean by that? So if for example there are modules that use Distributed Validator infrastructure and I think there will be a lot within the next twelve to 18 months and by a lot I mean three to four which is quite a lot if you consider that one module right now houses all of this.
00:48:30.302 - 00:49:39.558, Speaker B: Like hundreds of thousands of stickies operators that are both professionals or solo stakers or like Dows that want to run Validators as a community will be able to participate in these modules potentially. Or the module might say that actually this is a module purposefully built for, like you said, high class integrade level operators that all operate using sock, two S and ISO certifications and are fully insured and stuff like that. So the modules will be able to come up with different design decisions for which node operators can participate in those modules and how. And that's also something that's really important for third parties that might be looking to build the modules because they can either add their own kind of spin on them or they might want to integrate them into other networks somehow or empower the users or the operators in some other ways. But it also allows for this cross pollination of operators between the different sets. So you might have an operator just like any professional node operator that is in the current curated operator set. And then they might also want to run distributed validators, another module that's going to be possible.
00:49:39.558 - 00:49:47.830, Speaker B: And the staking router will basically be responsible for making kind of like this risk adjusted allocation between these different modules.
00:49:48.650 - 00:50:46.250, Speaker D: I think that's a key point there that I'd like to double click on and that you could see participants of the curated set also participating in the DVT and maybe more solo staker sets. And I just want to get at kind of like the benefits of that. And I think about the risks for the Steeth holders of opening up the validator set to solo stakers is that you take on more risk that your Steeth could be slashed, right, because you have less institutional operators of these nodes. But is there a way to derisk this expansion by perhaps like clustering a node operator, like chorus one with a bunch of solo stakers? And that way as long as the institutional node operator is running, then it can kind of protect, I guess, the other know, solo stakers in a cluster.
00:50:46.670 - 00:51:20.774, Speaker B: That's a really great question. And so there's a very large segue here, obviously, to the kind of stuff that Oshin is working on in oval. So I'll let him take that part. But I hope the professional node operators don't go and pretend that they're solo stakers. I'll get very mad if they do. And I'll know because it's my job to know. But the idea is that, yes, you should be able to use different combinations of different operator types or classes to minimize either the risk that a certain cluster and the validators that they run poses to the protocol or the impact of something going wrong, right.
00:51:20.774 - 00:52:27.260, Speaker B: And you can choose to either minimize both or one or the other and then find different ways to do that. And it's up to the designs of each module to figure out how to do that. And it can range from anything from bonding requirements that might be very basic and linear if you're talking about a completely permissionless solo module because that's the safest way to make something like that work to more exotic things where if it's like semi permissioned or you have operators that are not necessarily KYC themselves but prove that their identity to the extent that it's provable that they're not also another operator. Right. Not necessarily like it's Joe in his basement in Montana, but that it's not chorus one or something like that and come up with exotic nonlinear bonding requirements. Or maybe say that if you can prove that you provide on chain insurance for all of the validators that you're on, then you don't need a bond. But that insurance is, and this is why we're talking about DVS, you cluster them together in DV clusters of different kinds of makeups and I'll let Ashin talk about that because all of those can offer different technically risk adjusted ways to approach this problem.
00:52:28.030 - 00:53:17.530, Speaker C: Yeah, so I'll pick up there, starting with the challenge of the totally permissionless one and the bonded one. And as Izzy alluded to is like, it's very hard to figure out if someone is actually two people and they're not actually just civil attacking your network. So if you're trying to only let max one operator into the solar cluster, you don't want them getting ten applications in. You can do things like bonds, but as you said, you end up looking at the data, you're trying to do statistical analysis, be like they regularly have outages the same times as these other people, they might be related, but it's very hard to kind of prove that. You have to kind of get quite subjective about it. So the idea of starting with totally permissionless, we don't know who you are, so long as you put up a bond, it's probably fine. It's definitely going to know a bit on the more extreme end.
00:53:17.530 - 00:54:28.758, Speaker C: And we've been working with the Lido team on and off for more than a year, I think, at this point, and kind of getting towards that direction. And one of the things we've looked at is what you've alluded to of mixing solo stakers with the professional node operator set. And I think we can talk about this, but just over the last few months we've finished up some testing with about 40 node operators, only about twelve of them being from the professional like curated set, the other 30 or so being mostly solo stakers. And we put them into groups with half curated, half solo stakers. And the nice thing about doing half and half is if you need two thirds of them to agree to anything, you have quite a lot of comfort that at least some of the curated side agreed to it, or the curated side also don't have total control anymore. They need at least some solo stakers to get involved and agree to things. So that when we talk about some of the earlier modules, when you kind of go from, we want DVS, but maybe we're not putting an algorithm totally in charge, it's let's have one with a mixture of curated and uncreated.
00:54:28.758 - 00:54:42.550, Speaker C: We'll give them a certain amount of allocation, let's see how that does. And then you go fully solo staker, and then you go fully solo staker and it's bonded and permissionless and we don't know anything about you, and you can kind of work your way out. That kind of decentralization curve.
00:54:44.250 - 00:55:33.318, Speaker D: I think that's really interesting. And I also want to kind of poke at some of the different sorts of, I guess, the flexibility you have within each of these modules. And I think about also on the fee level, something like a protocol like Maker, which is very closely intertwined with Lido and a. Big distribution channel for Lido. Could you see something like a rev share that is done through customized fees or something like a kickback for maybe protocols that are power users or other distribution channels that also want control of their own module and they also want to see some of the economics of the steep that they are bringing in to the overall validator set.
00:55:33.484 - 00:57:11.382, Speaker B: Yeah, that's definitely one of the things that was purposefully kind of included in the broad nature of the way that the modules are designed. So even the total fee itself, like the top line fee, which is 10%, isn't set in stone technically, although practically, personally at least, I think it makes sense. It's neither too high nor too low and I don't think it makes sense messing with that right now in the immediate future, but down the road as protocol economics change and obviously, or hopefully, I guess, and this is not a financial advice, but the value of ETH also probably changes. The 10% fee might make sense to change it to something else. But within that, the share itself between what goes to the Dow, if any, what goes to node operators and what goes to potential third parties that are also included in terms of making sure that this module runs well or, like you said, is a distribution channel. There's a lot of different ways to approach this and it can range from, for example, them getting a percentage of that fee split all the way to something like secondary collateral, right? So there's a lot of permissionless staking protocols that I don't know if they offer the option, but so most right now require a secondary collateral to ETH. In my personal opinion, I think ETH, or at least the staking token itself, like the LST st ETH in Leto's case is like the prime collateral and probably shouldn't be substituted for something like this.
00:57:11.382 - 00:58:33.070, Speaker B: But from a complementary perspective, it might make sense in certain cases for certain modules to add collateral, and for example, node operators that post collateral in the secondary token might be eligible for something like increased reward share. Or it's akin to staking that token, whether it's a utility token or a governance token or something like that, and allowing module designers and creators to play with those kinds of financial aspects. I think A allows the modules to kind of be seen as like business cases on their own so that we can find sustainable models for how do you create validator sets. But eventually, and this is to the point that you brought up, that hasu mentioned in the previous cast the idea of developing a fee market for validator sets and I think fee is perhaps constrained, but like a more general market. So you can say that this module offers this, this and this and that might be like fully KYC operators or operators that run all of their validators in green data centers that are not the cloud or something like that. And then eventually either the protocol or users themselves. Although technically right now it is the protocol because for the moment we believe that an opinionated proxy for user decisions here ends up working a little bit better.
00:58:33.070 - 00:59:21.070, Speaker B: If you look at delegated proof of stake in general, it ends up very top heavy. So we're trying to ameliorate some of those kind of side effects by distributing stake in as equitable a manner and in terms of prioritizing certain things for robustness that you mentioned earlier, miles like geographic distribution, jurisdictional diversity and obviously software and infrastructure variance as well. And eventually if you allow these modules to compete against each other but perhaps within certain bounds or limits so that you don't drive fees or rates or rewards rate as the most important thing because then you will end up with corner cutting and stuff like that. I think we'll be able to find that the design space for creating your own modules allows more sustainable mechanisms to be created.
00:59:23.730 - 01:00:16.158, Speaker A: I have a couple of questions for you on that, which is that idea that hasu introduced as sort of a local fee market for Lido, similar to how Ethereum has, I think a very elegant fee market, frankly, is very interesting to me. I want to get a sense of what are the market forces that are going to shape fee markets within Lido and what is governance ultimately going to be responsible for. Because I kind of see these two conflicting forces here where there's sort of a free market rate between people who want access to a specific set of validators. And this could be sort of the shriram idea of maybe there's a premium on the block space of more decentralized validator sets using DVT like Obel or SSV. But then on the other hand, I could also see governance trying to have its own I'm trying to get a sense of what is going to be determined by the free market right. Versus what does Lido governance going to be responsible for. So can you give me a sense of how you see that playing out?
01:00:16.244 - 01:01:19.734, Speaker B: Yeah, to a certain extent the free market is going to determine that and I e that is Lido governance because anybody can go by LDO and have a hand in or have a say rather in determining that outcome. There is no decision on this obviously, because the Dow hasn't voted on it. I think, at least for my personal opinion, what's happening right now is that there's an exploratory phase. We being like the current set of contributors that are working on this. We're trying to determine what ends up with the right balance of sustainability and robustness, but at the same time also being appeasing to capital efficiency, right. You don't want to go too much in one direction where you sacrifice capital efficiency to the extent that you have ceilings on the total amount of supply that you can meet because that demand will end up going elsewhere and that those other parties might not have the same considerations or principles that you do. And on the other hand, you also don't want to go too much in the other direction and make everything about competing on price.
01:01:19.734 - 01:02:32.560, Speaker B: So I think the protocol's job here, at least from my perspective, and this is what I believe in espouse, is that it needs to set up the right, let's say, risk adjusted safeguards for what a sustainable, validator set looks like in the future. And that hinges upon the things that we talked about in terms of what makes a decentralized, validator set and how do you maintain credible neutrality and censorship resistance in the network. And if the Dow eventually sells on a protocol where users can choose specifically like, which validators their stake goes to, it needs a balancing mechanism to prevent that, let's say, possible lopsided decision making from affecting the overall protocol. So if you make it so that anybody can choose where their stake goes to as opposed to how it is right now, it needs to be able to countermand the additional risk for doing that. So if at some point somebody comes with gazillions of dollars, right, and says, I only want the stake to go to this one node operator, that might put the rest of the protocols, like fungible tokens, at risk. So the protocol itself should have safeguards that rebalance the stake in a way that makes that as difficult as possible to happen.
01:02:32.930 - 01:03:42.126, Speaker D: I think that makes sense. And it touches on a theme that we've been talking about first, that decentralization is offense with liquid staking protocols, but also governance minimization is offense. And I think you're touching on a tricky balance there. Because if you really were to put this decision in the hands of the user or maybe put this decision on the supply side where say these modules bid with the lowest possible fee that they would take to get the deposits to win the deposits natural market forces just move this towards most of that stake going to the largest operators. And so yeah, I guess is it fair to say that while there's some, whether it's MEB with PBS, you can build that into the protocol and basically ossify it completely and the parts of even the Lido contracts you will be able to ossify at some point. But is it fair to say that maybe monitoring the allocation of deposits is something that will never be able to be completely ossified or left to the free market or the user's choices?
01:03:42.238 - 01:04:30.862, Speaker B: Yeah, I mean, never is a big word. I think that eventually there's research being done on this and it's public. So I'm not saying anything like a specific set of contributors have been working on this with another mine research team. We discuss it with node operators almost every day and with other parties in the ecosystem. Like obel and even with other staking protocols, right? The difficulty here is trying to come up with a semi autonomous mechanism, right, that you don't want to keep touching this and you don't want to having to grease the wheels every couple of months, that runs well enough. And that at the same time is not, let's say, exploitable under bad weather scenarios. And the latter part is what's really, really difficult, especially with permissionless systems.
01:04:30.862 - 01:06:02.794, Speaker B: So stuff like DV is amazing and at some point, if distributed, validators end up being as battle tested as we hope and we think that they will be. It wouldn't be surprising to me if most validators are run in a distributed fashion, but you can't just say because that validator is distributed that I'm contending with end parties now instead of one, right? Like four or three. The permissionless nature of what we're trying to build eventually means that it's very possible that these four parties may be one and that it will be very difficult for a protocol, at least from a purely automated on chain perspective, to discern that it is one and not four or three or two. And so to a certain extent, I think there may be kind of like bumper guards that you have in like a bowling alley that might need to write the ball every now and then. But the question is making sure that those are put far enough away that they only have to come into play very seldomly and not in a way that can endanger the protocol. And one of the key ways of doing that, and we're kind of going back to the question that you mentioned earlier is like this idea of dual governance. So dual governance is all about how do you address this principal agent problem where the protocol that is basically responsible for making certain decisions may or not make or is governed by actors that may not have exactly the same interest or at some point diverging interests with the holders of the liquid staking token.
01:06:02.794 - 01:07:13.910, Speaker B: So for example, in this specific case, like LDO holders versus St ETH holders. So the idea of dual governance is a allowing for St ETH holders to have a veto in the case that these incentives meaningfully diverge, but also a way to, let's say, meter the possible worst case scenarios that can happen in a protocol, right? So it's not only important for people to be able to veto if something bad might happen or something that don't agree with might happen, but also to delay whatever needs to happen long enough so that the people don't agree with it can exit gracefully versus leading to a kind of situation where everybody's running for the exits, even if they might not need to. That ends up having unintended consequences down the line. Because we're talking about DeFi here, right? There's no accounting for how people use the tokens that are theirs in a completely permissionless system. And so you want to create systems that make it clear that if something bad might happen or something that they don't agree with, they have adequate recourse basically to say goodbye and do so in a way that doesn't put anything at risk.
01:07:14.250 - 01:08:30.734, Speaker A: Yeah, we could go down this whole rabbit hole of dual governance. I have a couple more questions for you on sort of the ossification of the protocol. And I want to actually lead into a restaking question because we're going to have Shri Ram on next week. But one metaphor that has been Miles and I have been playing around with a lot this season is actually like the App Store. And if you look at the development of the App Store, it actually kind of started out as this thing that wanted to be a little bit more permissionless. But ultimately what Apple decided, Juan, is they wanted to give their users a better experience and they started having higher barriers to entry and requirements for developers because the ultimate sort of market factor or market determinant for them was wanting to provide a good experience. And Izzy, that's the question that I want to ask you and open it up to Asheen as well, is just try to get a sense of how permissionless can we get here? Because at a certain point, if we just let anyone become a validator, right, if the staking router was to become so permissionless that anyone could kind of do anything, then you could risk some adverse consequence for, let's say, Steve Holders, right? If you had irresponsible validators who risk getting slashed and then the way that the pool is currently structured, everyone would sort of get punished for that.
01:08:30.734 - 01:08:38.370, Speaker A: So that's my sort of question for you is how permissionless can we ultimately get here and where does the line sort of exist?
01:08:39.910 - 01:09:51.194, Speaker B: So ultimately, and we're talking about like, I don't know, four to six year horizon from now, I think we can get quite permissionless until then, there's a lot of work that needs to be done. Some of the core problems to contend with in the space are obviously like civil attacks, which we mentioned earlier, the idea of how do you fight these when they happen or at least minimize their impact when they do because that's the most likely scenario that they'll happen. And then you just want to make sure that the protocol is robust enough to withstand them when they do. And the second aspect is, and this kind of goes to the whole idea of building on chain identity and not in the KYC sense, but in the sense that anybody can participate to the extent that we can make sure that they're not also somebody else. So there's a lot of data that you would need to bring either from on chain or aside the chain on chain. So, for example, we talk about things that happen on chain, right? But the gossip network itself is not on chain. So validators, are not aware of the physical location of other machines that they're talking to.
01:09:51.194 - 01:10:33.430, Speaker B: Those things, to a certain extent will always need some sort of proxy for meat space. And right now that proxy is governance. Eventually, when there are mechanisms to bring enough of that data on chain so that these protocols can reason about it in a sufficient manner, it doesn't need to be foolproof, but it does need to cover like 90% of cases. Then we will be able to reach systems that are either semi autonomous or almost fully autonomous with very few safeguards or let's say, configuration parameters being tuned by something less like a governance mechanism, I would say, and more like caretakers.
01:10:34.170 - 01:10:37.746, Speaker A: Got it. Sorry, Rasheen.
01:10:37.938 - 01:11:34.220, Speaker C: Well, the one thing I was going to add to that, which is I think permissionless is quite achievable, but I think the one that I'm more skeptical of is market forces driving risk parameters and stuff, and being like, oh, yeah. We can just look at redemptions and we can look at people's what they want and assume that there's kind of some natural place to put risk and stuff. I think what a lot of times is very hard to appreciate is how Black Swan staking is in that when something bad happens, it'll be nonlinearly, worse than anything that's come before. There's not going to be some data of, oh yeah, they started to decline in performance, so automatically we reduced their risk and their tolerance. It's probably going to be everything was fine until it was unbelievably not fine. So you can still do permissionless, but I don't think there'll be like a market price of finding a clearing price that is the price of risk, or at least not an accurate one, I think.
01:11:34.590 - 01:12:51.294, Speaker A: Yeah, that's such a good point. And maybe we can end on this and it'll be a lead into our next interview is the possibility of a big theme that we're exploring this season is sort of the intersection in between something like liquid staking and restaking. And frankly, the first thing that sort of comes to mind that is at the same time very interesting, but I could also see it being very problematic is something like a restaking module within the staking router on Lido. And I just have a lot of questions about how that would mechanically work. For instance, if the middleware that we stakers end up running get tokens that are denominated and not ETH, I mean, how would, frankly, something even like that work? But then also the bigger question of you can imagine right now it's the bear market and everyone's being very frankly, rational and concerned with things like safety, but we know that doesn't happen in bull markets as well. And you can imagine this negative reinforcing cycle in between, let's say a Lido clone and an Eigen layer clone that aren't as responsible and acting in as good faith, where they just layer up on a whole bunch of different risks that they end up taking. They drag the weighted APY up and people end up going into sort of much less safe conditions.
01:12:51.294 - 01:13:05.880, Speaker A: So maybe if we could take both of those at each time, I would love to get your sense of if a restaking module would be possible and how would that work if their awards were denominated in something other than ETH. And then maybe we could talk about that last point.
01:13:06.970 - 01:13:32.746, Speaker C: I think the thing I'd like to hear from Izzy is how a liquid Staking token might stay fungible while people want to start doing different things. I know, I was listening to the Hasu podcast yesterday and he was like very insistent. There will only be one derivative, it'll be like uniform and everyone's going to use it. But how does that work if there is demand for different modules and some people want the kind of risk averse one, some people want the Rest taking one, how do you blend risk profiles?
01:13:32.778 - 01:14:21.534, Speaker B: I guess so there's a bunch of different schools of thought on this and I would say that there is no consensus right now. The way that I approach this is that if you want things like that, it's better to build them on top of SDE rather than to build them alongside SDE. And like Eigen layer basically does this already. Well, it allows you to take your SD or your Rath and deposit into a contract. Not much is happening to it yet, but eventually the idea is that it will. So to a certain extent, the exposure that St E at least exogenously might have to something like restaking is unbounded. However, it's probably going to be more appealing if there is some sort of core level integration, let's say between the two.
01:14:21.572 - 01:14:21.822, Speaker A: Right?
01:14:21.876 - 01:16:04.354, Speaker B: It might give you better API, it might give you more granular control over the validators which you need in order to have a good feedback loop over liens of collateral. Otherwise things might get really out of hand if you go really high up the restaking stack, and that in itself might take the form of a Staking router module and then you basically, at least in the current way that Staking router works, rely on the Dow to put appropriate risk safeguards around that. Right? So one is the thresholds regarding how much stake might be allocated to that module and two is what the expectations would be for that specific module with regards to risk mitigation or risk management options. So it might say that for every Validator that is spun up in this module, x amount of whatever you want to denominate that in, it might be in SC ETH rewards, it might be in ETH, or it might be in these kind of like secondary tokens that end up being accrued. Like you said, Mike gets sent towards paying for insurance for these validators, something like that. And so I think that also is the most reasonable way to address the other question that you have is sorry that you had ashin, which is how do you not mess with fungibility like fungibility for me is the most important thing for St e fungibility and liquidity and they kind of go hand in hand. So if you eventually add modules that either allow you to take more risk than other modules do or, for example, specifically stake with specific operators in order to have certain qualities of validators, I think they end up doing, the same thing.
01:16:04.354 - 01:17:11.782, Speaker B: Which is? That it can, if not managed properly, end up creating different classes of overall risk, which, if they get too big contra the rest of the stake, could threaten the entire protocol. The protocol's job and governance's job is to not let that happen. So you have to put the appropriate safeguards in place from a metering perspective and from a configuration perspective. And then you also have to make sure that the actual business logic within these modules appropriately meters the possible negative effects of something going wrong. So it might even be something like these need to be full insured or they need to post collateral or it's opt in, but only a certain amount of stake can opt into this and if you don't get in, you don't get in. So there's a lot of kind of accusations that are made in terms of oh, if this Staking protocol is too big, they're going to mess with block proposal slots and they're going to construct mev blocks vertically and all of this stuff. And I've actually never seen any proposal in Lido for this to happen either on governance or amongst other people that I've talked to.
01:17:11.782 - 01:17:55.010, Speaker B: So there's a lot of things that can happen. There's a lot of things that could have happened in terms of attacks of ethereum in the past, but you ended up with them not materializing because of, let's say, the culture and the quality of people that are associated with this network on average and at large. So if we make sure that the culture and the ethos is there to not engage in the most degen, let's say, kind of practices and inducing rewards, but follow a path of reasonable sustainability but at the same time balancing for capital efficiency, I think that there's a path forward there that doesn't end up endangering the wider protocol.
01:17:57.830 - 01:18:39.874, Speaker A: I want to make sure I'm not focusing on this too much. And also just for the audience, I want to highlight this idea of being fungible. So the reason why this is so important, eigen layer actually addresses this in their white paper, where they explicitly say that we have no plans to issue a token because positions in Eigen layer, you're going to be running. Different types of middleware with different risk and return profiles associated with so you can't make it it's not fungible like it is with Steeth and ETH. That said, there actually is a precedent in finance for creating liquid fungible positions for less fungible. Like actually bond ETFs would be a really good example of that. So we do know that there is at least some pretty large precedent actually in TradFi for it.
01:18:39.874 - 01:19:34.158, Speaker A: And the question that I have for you is I've pulled up here again, Adrian's, sort of Steeth assets and liabilities, their balance sheet. And if this was a bank looking over at the right, instead of on the assets part of their balance sheet, instead of professional note operators, small groups, Dows, it would be Treasuries would be a group of assets that you'd have. Commercial real estate would be a group of assets that you have, small business loans, whatever it is, they'd sort of divide up their different assets and Steeth would be replaced by deposits. My understanding of the mechanism for how Lido pays out its rewards is there's an oracle checks the balance of ETH that's in the total validator set. And then Steeth gets minted. And Steeth will act like it gets rebased to holders. But then it also gets new Steeth shares, which essentially act as claims on the pool get minted to both the treasury and node operators.
01:19:34.158 - 01:20:12.110, Speaker A: So my question just from a mechanical standpoint and what the pool of assets would look like, is if you had these validators that were opting into additional hardware and receiving rewards, and Oracle would check that, but you still wouldn't be capable of minting anything other than Steve. So ultimately the balance, the pool of different things would encompass other tokens in there, right. And you would still just get paid out with Steeth. And suddenly Steeth would represent a claim on not just ETH in a pool, but mostly ETH plus some other tokens. Or would you shift the way that do you see what I'm saying?
01:20:12.260 - 01:20:16.980, Speaker B: Yeah, I understand the question. So there's no answer to this.
01:20:17.430 - 01:20:18.146, Speaker A: Got it.
01:20:18.248 - 01:21:04.350, Speaker B: In terms of like, there's nothing that has been set in stone, these are obviously like things that we're thinking about. My intuition here is that, first of all, the call out that you're making is correct. In the current curated operator registry, node operators are paid daily, basically, or receive their rewards fees daily. And it is pro rata based on how many validators they run as a total number of compared to the total number of validators run by the protocol. So node operators do not make more or less based on their daily performance. However, with staking router, this can be determined at the module level. So if some modules want to pay operators that are performing better more than they pay operators that don't perform as well, they can.
01:21:04.350 - 01:22:24.986, Speaker B: But it's up to the module to figure out the logic there. Now, that logic might require certain upgrades to the main Oracle, or it might work with a satellite Oracle. It's up to the module to figure out how to do that on a grander scale. Like when we're talking about this four to six years down the line, there are a lot of different things that I think need to be considered in terms of how you were talking about the total risk of a balance sheet, which might be treasuries or commodities or something like that. And it's not only about rewards adjusted returns, but also in terms of what is the contribution to the robustness of the protocol of all of these different modules. So a solo staking module which might make a little bit less money in terms of rewards rate than like a module run by fully professional stakers although with things like DVS that might not even be the case anymore might contribute more to the robustness of the protocol than the equivalent amount of validators run by professionals would be. So to a certain extent, the protocol should value that higher on a per validator basis than something that's from the curated operator registry and these kinds of, like, you can call it risk adjusted calculations, right? But it's really about the sustainability of the protocol are going to be core in terms of figuring out what the right allocations are.
01:22:24.986 - 01:23:21.766, Speaker B: So when we talk about risk restaking, the same kind of logic applies there. If the rewards that these validators are getting might be denoted in something other than ETH, there's like a couple of ways to approach it. One is that they actually don't relate to the minting of steth at all, and it ignores those rewards and they are represented in another way. So it might be another token, or it might be something that you can claim by holding st e, but not actually rebased in the full value of the token, right? So you can assume that their value is zero in terms of what's the contraposition on the liability side for the protocol. Or you can do other things like instantly sell them for ETH and restake it, and then that works. I think different modules will take different approaches to this, and then we'll figure that out. So that's the other cool thing about the module based approach.
01:23:21.766 - 01:23:31.040, Speaker B: Maybe you don't sell them for ETH, maybe you sell them for a token of a project that wants to do this, and then that is a way for that token to accrue some value.
01:23:33.010 - 01:24:06.438, Speaker A: Got it. Fascinating questions to ponder, and thanks for I know a lot of this stuff is frankly still being written and discussed, but it's always interesting to consider, and we'll have to poke a little bit of that at Sriram next episode. Izzy and Asheen. Guys, you've been so generous with your time. Thank you so much. This has given Miles and I a ton to think about. If folks want to find out more about you or follow you or either reach out, maybe Asheen or interested in Validating or Izzy sorry, want to find out more about Lido.
01:24:06.454 - 01:24:06.538, Speaker D: I.
01:24:06.544 - 01:24:09.100, Speaker A: Think folks know about it pretty well. What's the best way to follow you?
01:24:09.470 - 01:24:33.780, Speaker C: Yeah, I'll start there. So I think you can find us on Twitter at Oball Network and the website is Obal Tech. Obol Tech. And yeah, the thing I would encourage people to do, particularly if they are a home staker or someone considering staking is to give a try on the launch pad, try and run something on know, set up a squad with your friends, give it a try because it sounds like Lido might have some opening for node operators in the foreseeable future.
01:24:35.430 - 01:24:59.930, Speaker B: Yeah, actually there's an opening for some node operators right now. Today is actually the last day for people who are interested to apply for the Wave Five onboarding. So this is for the curated operator module. You can find more information about that on the Lido forum. So research lidofi. There's already over 100 applications, so our team is going to be insanely busy. Once we get back from ETCC.
01:24:59.930 - 01:25:36.306, Speaker B: We will be at ECC. There will be a Lido booth. If you're in Paris, come find us. I'll try to make sure that I get you some merch. If you say that I'd listen to you on the bell curve you can find us, of course, on Twitter at Lido Finance and myself at Isdrsp. Isidorospasadas that's my name? And in general, this is a really exciting time for me. All of the stuff that I've been thinking about for the last couple of years in terms of how do we create sustainable, valid data sets, is like the culmination of that is the Staking router and the first couple of modules that are being developed.
01:25:36.306 - 01:26:16.580, Speaker B: I'm super excited to be exploring the solution space here with teams like Obel and others. And I think the next twelve to 18 months are really going to be pivotal in terms of showing not only how important it is to build systems that allow for inclusivity, but also the idea that if you create design spaces for something. You encourage other people to help you find solutions as opposed to trying to come up with everything on your think. Like, honestly, both light of et and especially what Obel is doing by creating middleware and not necessarily like a prescriptive way to do things, is examples of the same thing.
01:26:17.430 - 01:26:51.166, Speaker A: Guys, this was just such a great episode. Thank you so much, Miles. And I keep saying we got to make these shorter, but what could we have cut out here? This was just such great conversation, so we really appreciate both your time and we'll have to do it again soon. All right, Miles. What a great episode that was. I knew we were biting off quite a bit with DVT plus Staking router, and we didn't even get to dual governance, which is, I think, again, the mark of a good episode. We're being very ambitious with the planning sheets for each one of these shows, but man, there's just so much to dig into there.
01:26:51.166 - 01:26:55.810, Speaker A: I thought that was one of my favorite episodes of yeah couldn't agree more.
01:26:55.880 - 01:27:13.320, Speaker D: And came out of it extremely excited about the potential of this staking router and have a very good idea of how this intersects with DVT and the importance of solo staking and how Lido is really trying to align itself with what is beneficial for ethereum the.
01:27:14.570 - 01:28:12.618, Speaker A: Yeah, let's kick off from, you know, starting off with solo staking DVT. DVT is not something that I had necessarily heard of that much even six, nine months ago. I feel like increasingly very smart people are working on solving this problem, whether that's Oval or SSV or and one. I think part of the reason we didn't really talk about this, why solo or self stakers are so important, is if you look at the scaling architecture of roll ups in L two S, I sort of have this kind of pet thesis that these roll ups are going to be using a centralized sequencer for a long time, if not indefinitely. At the very least. It's a much thornier problem than folks initially gave it credit for. So I think the importance of a very credibly neutral, decentralized base layer at Ethereum is probably even more important today than it has been in the past.
01:28:12.618 - 01:28:25.470, Speaker A: So technologies and mechanisms and creating a sort of incentive base for solo staking through DVT, I just want to underscore how important that is in the current Zeitgeist moment.
01:28:25.620 - 01:28:44.450, Speaker D: Yeah, no, that's a great point. I didn't actually think about it from the sense of the makeup of these roll ups is reinforcing the importance of having a good chunk of the Validator set be these independent solo stakers that are very hard to censor from any sort of government or another organization.
01:28:44.950 - 01:29:22.074, Speaker A: Yeah. And it was encouraging in talking to Asheen. I think the part of that conversation that stood out to me, I won't go back and talk about the mechanism for how it works today. But when we asked him about tomorrow and timeline, and I think the question that you and I were really interested in driving towards here was, what is the timeline for reducing the hardware requirements here to something where you could imagine being a solo validator or using just your cell phone? And he said it might not even take years. So that to me was a very impressive or a very interesting kind of takeaway. It got me excited.
01:29:22.202 - 01:30:42.326, Speaker D: No. 100% because I think the current requirements associated with this, even once you lower the capital requirements from 32 E to say, four E, there's still kind of a ceiling of in terms of the amount of people in the world that are one, motivated to run a DV Validator have the technical chops to do so. And I think lowering those barriers is extremely important to actually opening this up to a broader population. And yeah, I was surprised. I was thinking it would be in the order of five years before we could get something like this to just run on your phone in the background right, or just even have very flexible options here. And then once you lower these hardware and capital requirements as much as you possibly can, it just becomes a question of, okay, how do you frame running a DV validator in a way that is attractive to folks who may not have been interested in staking in the beginning or may not be crypto native? And I think there's still some open questions there, but at the very least, this addresses all of the problems for folks that are already interested in staking and interested in pushing forward the decentralization of the network.
01:30:42.438 - 01:31:18.066, Speaker A: Yeah, I agree with that. We talked about segwaying into the staking router. I mean, even before that, we've talked a lot about alignment. And the closer you are to core protocol or the sort of base layer infrastructure of ethereum, the more aligned you want to be with ethereum. So I think it makes a lot of sense for not only Lido, but Rocket pool is probably the most extreme example of this, being extremely aligned with sentiment of ethereum. There's actually a interestingly I'm not sure if you caught there was a vote, I think, in Rocket Pool governance as to whether or not to self limit. It was very similar to the Lido vote that happened in May of last year.
01:31:18.066 - 01:31:52.814, Speaker A: Very different answer. It was kind of an overwhelming consensus that we should self limit. The caveat that I will say there is that in the immortal words of 50 Cent, it's hard to hate it from outside the club. You can't even get in. It's very different saying you're going to self limit with 7% of stake versus 30, or something like that. But it's indicative of this alignment sort of issue and I think Lido is doing my sense is that Lido is a team of true believers in a lot of this stuff. But I want to get into the discussion that we had around the staking router was, I think, extremely interesting.
01:31:52.814 - 01:32:04.450, Speaker A: I would say I kind of came away from this conversation very actively excited about it, and I've got a bunch of different questions I could ask here. But yeah, what were some of your high level thoughts from that part of the conversation?
01:32:04.890 - 01:33:17.562, Speaker D: Yeah, so I think in general, the staking router is allowing Lido to serve more different sets of users beyond just this initial kind of core user persona of somebody that wants to delegate the hardware responsibilities and delegate running these validators to a curated set of professional node operators. So it opens up a customer segment of these solo stakers and DVT stakers, but it also opens up customer segments or at least improves the relationship with other customer segments in distribution channels like Dows themselves or even maybe a subset of the curated set that is just highly institutional and compliant. Right. And I think it's really fascinating just how much flexibility that they have kind of left and design space that they have left to the community to create these modules. Things like custom fee parameters or custom allocation parameters. Right. You have the ability to just mirror how the curated set is, how deposits are allocated across the curated set.
01:33:17.562 - 01:33:32.494, Speaker D: Or you could do something very interesting like sending more deposits to the highest performing validators or fees that kick back to a protocol or something like that. And so it almost reminds me a little bit of Univ Four in that.
01:33:32.692 - 01:33:36.320, Speaker A: I was going to say the same thing opening up. I was going to say the same thing.
01:33:36.870 - 01:34:07.180, Speaker D: Right. And I think it opens up a path for Lido to improve its alignment with the community, get everybody comfortable with its growth and its market share, essentially, while also in a more greedy greedy is not the right way, but it's in their financial incentives to do this right, because it's becoming a better product. So, yeah, I'll stop there and would love to hear your reactions as well.
01:34:08.030 - 01:34:54.922, Speaker A: Yeah, I'm really glad that you brought up V Four. I saw a direct comparison to that, and I think there's sort of this converging set of architectures. We actually talked about this, and it kind of has to do with minimizing the surface area of your product and minimizing governance. And this was a key question we actually asked in season two of this show, which is there's sort of a more vertically integrated approach where you do a whole bunch more things and governance has access to all of that. Or you can choose to be a thinner layer, so to speak, and you minimize the amount that you do, but you also minimize the amount that you govern. And that makes it more solid to build a foundation on. And I think you're starting to see, I think Uniswap V Four was a great example of that.
01:34:54.922 - 01:35:10.110, Speaker A: I think Lido is clearly heading in that direction with the staking router. I think we're talking to Shriram next week. I think he'll probably say something very similar when it comes to eigen layer. So you're starting to see these two sets of designs play out.
01:35:10.260 - 01:36:01.162, Speaker D: I would say I'd really call it the difference with Univ Four versus Lido V Two. And the staking router is that with Uniswap, users have the ability to choose which version of univ Four they want, which version with XYZ hooks and characteristics they want to use. Right. And Lido could do the same thing. Lido could say, all right, let's just let the users decide which module they want to deposit their stake to. Right? And I think the difference there is that with liquid staking protocols, again, there needs to be a level of control, one, to protect the users, and two, to basically align with the health of the network because you could see a situation where the choice gov minimization is great. Right, that's the goal.
01:36:01.162 - 01:37:02.740, Speaker D: But if you let that choice sit with the users completely, then I think it's fair to say that based off of what we see in delegated proof of stake systems, most users would prefer to delegate their stake to the most safe, largest validators. Right? And so this kind of push pull tension where you want to minimize the surface area of governance as much as possible, but you also need to basically monitor the allocation of deposits in a way to make sure that you're also driving forward the health of ethereum, the network itself. Right, because the staking router would be kind of I guess the impact of the staking router would be mitigated if less than 1% of deposits were actually going towards these permissionless validators, right, that wouldn't really actually decentralize lido or wouldn't help lido decentralize in the way that they want to. So this level of curation, I think that's interesting.
01:37:04.150 - 01:38:28.862, Speaker A: I think it's an unsaid assumption that many would like to be true, but I could see it going in the exact opposite direction that left to market forces, many protocols would prefer to be validated by professional services as opposed to self or solo stakers. Now, I think DVT has the to your point about maybe a professional validator like chorus one runs a part of that DVT cluster and then there are multiple different self stakers that sort of become a part of that group. I think that could mitigate that. But I do think as it stands right now, without DVT, I do think most protocols would prefer to be validated by professionals who will wake up not only you don't have to worry about them waking up in the middle of the night, they'll have teams that just do that. So I could easily see market forces going the other way and putting a discount on solo stakers as opposed to a premium. So I think that's probably maybe not a popular thing to say, but I think there's definitely the possibility that that ends up happening and then you will have to then it will be interesting to see the impact that lido governance has through allocation to different modules of stakers versus just straight up market forces. And it could be an issue for them if market forces actually end up prioritizing a more professional curated set of validators as opposed to the solo guys.
01:38:28.862 - 01:38:46.630, Speaker A: But one question that I had for you Miles, is you made a really great point with Maker frankly being a potential distribution channel for something like Lido. Can you explain a little bit and unpack that a bit more because I think that's an under discussed point.
01:38:46.700 - 01:40:04.362, Speaker D: Yeah, absolutely. So I think the general idea is that folks could deposit ETH as collateral to Maker and Maker in their balance sheet. Management would deposit that into lido to earn yield on it. Essentially and let's just say that Maker or another protocol is so large of a distribution channel that it accounts for say like five to ten to 20% of all circulating steep out there. At that point, that protocol might be motivated to open up a competing liquid staking offering so that they can capture all of the yield and they don't need to share any of the yield with Lido anymore, given that it could be a substantial amount of revenue. But what the staking router enables is to basically share some of this revenue or basically customize the fees to kick back some of the revenue to the distribution channel. And this isn't that different than some of the growth hacks that Lido has used in the past with basically rewarding distribution channels like ledger or other whitelisted wallets or on ramps with some sort of LDO rewards.
01:40:04.362 - 01:40:41.900, Speaker D: But this is a much cleaner way to do it, right? And so you could basically mitigate this risk of competitors like we've already seen with Fracs, right, who say, we've got all this ETH sitting around, why don't we just create our own liquid staking protocol to internalize that yield? And I think that this offers a really elegant solution to sort of let these relationships grow together. And yeah, that was just one immediate thing that came to mind. Who knows how prevalent that will actually end up being with the growth of the staking router. But it's definitely interesting.
01:40:42.670 - 01:41:39.658, Speaker A: It's an interesting question because what it ultimately made me think know, if you look at that diagram of the asset side of Steeth and you kind of see five or six different modules, maybe there'll be some more than that in the future. There is another future where there are actually thousands of different modules and ultimately a lot of those end up being individualized modules for say, Maker or Aave or something like that, which would be frankly pretty interesting. And I think that would be an alternate way of playing out. The other solution that you could do is vertical integration. And that's sort of the approach that FRAX is taking. And frankly, you're starting to see vertical integration within some of the major DeFi protocols. Like Maker, Curve and Ave are all converging on very similar business models and they have decided to go in the opposite route of something like Hasu would advocate, which is to just stay in your lane, minimize governance, minimize your managerial surface area.
01:41:39.658 - 01:41:51.150, Speaker A: They're doing the opposite. They're vertically integrating and they're maximizing trying to take on a whole bunch of market complexity and risk. I am not smart enough to know how that's going to play out, but it's interesting to see very different approaches being tried.
01:41:51.300 - 01:42:16.920, Speaker D: Yeah, couldn't agree more. And I believe just going back to that Maker example, they could say Maker really cares about the decentralization of the L1, right? They could also specify that the validators in their module will all be DVD or solo stakers. They can basically curate who the validators are, what kind of validators they are, what the fee preferences are. Yeah, so the flexibility adds a lot of possibilities here.
01:42:20.510 - 01:43:17.606, Speaker A: It would also be interesting, frankly, I'm just not creative enough to ask some of these questions. But I do remember you might be surprised about what these protocols end up caring about. So I don't know if you remember there were a slate of proposals for the PSM a little while ago to invest some of that USDC and there were these sort of competing proposals from Coinbase and CoinShares and the idea was to reinvest and earn higher yield on that USDC. And they ultimately ended up going with Coinbase, even though the amount of yield that was offered was lower. And the reason was is because they could withdraw very quickly. Remember, the function of the PSM is to be a stability mechanism. In know, there's some de pegging event, you're supposed to be able to defend the peg, whereas the coin shares proposal, even though it was actually a more traditional risk management framework and they were going to reinvest in bonds, you actually had to move from crypto into TradFi.
01:43:17.606 - 01:43:33.194, Speaker A: So the withdrawal time took too long to the fact that it was punitive. So there might be these weird parameters that are outside. Just how decentralized is the validator set and what are the fees? There will be other technical reasons that end up determining these modules.
01:43:33.322 - 01:43:50.920, Speaker D: Yeah, no, I think it will be very interesting to see exactly what modules in the same way as we're very interested to see what people do with univ four. There's a lot to look forward to in terms of what people can do with the Staking router and the modules that they create.
01:43:51.610 - 01:45:13.860, Speaker A: Yeah, I want to give listeners I want to end this by talking about the restaking module, which I thought was frankly super interesting and I wish we had more time at the end of our discussion to dig into that. We initially planned this episode to be at least 25% focused on dual governance and we only briefly mentioned it. And I want to at least give listeners an overview of how that's going to work and why that exists. I know Izzy talked about it a little bit, but it's this idea of you can imagine if, let's say in a future state where Lido ends up winning, 70% of Validators are within lido, then you could see the co opting of ETH's sort of governance and how Staking works by a very small set of lido holders. And there could be a principal agent problem existing between the wants of stakers and the holders of lido, the token. So what the solution was to give Steeth holders actually the ability to veto and the mechanics are pretty interesting. There's basically a contract where after a certain number of amount of Steeth is deposited into this sort of escrow contract that there is a voting period and you can vote down any of the like a Lido proposal or something like that.
01:45:13.860 - 01:45:54.254, Speaker A: The other thing to mention is that Lido is not purely an Ethereum protocol. It's expanded at one point it was on Terra Luna, there is Lido on Solana. So it's kind of a supranational dow entity as opposed to a national entity like Ethereum, to use an imperfect analogy. So again, it's just like if you want to be a huge nerd and go down the rabbit hole, we can link something in the show notes, which is the proposal that Sam cozen wrote that you can take a look at. But I did just want to say that was another key part of decentralizing the governance that we just didn't have time to get into in the show.
01:45:54.292 - 01:46:53.710, Speaker D: That's like directly addressing the principal agent problem, right? Correct. And I'm trying to pick, like, an analogy here, but in the absence of there being actual top down regulation and people monitoring, like, you would see in the traditional financial system, you need to give the holders or the users of these assets, the depositors, some ability to veto things that are truly, truly malicious and very frankly, like, unrealistic, I think. But still, that level of comfort that it is possible is what is the big growth unlock that gets the rest of the community comfortable. And so I do think that the introduction of the Staking router and specifically permissionless sets or solo staker sets along with coupled with the dual governance is really the core of this response that Lido is putting forward to these concerns and the self limiting debate.
01:46:54.450 - 01:47:27.980, Speaker A: Yeah. So we'll link the actual forum discussion in the show notes and if anyone wants to nerd out about that, that's probably the best place to do it. Although it has been a little while since that was proposed, so there might be some updates in terms of how Sam or the Lido team is thinking about it, I'm not 100% sure. I wanted to end by discussing the restaking module, the possibility of a restaking module. That was, I thought, one of the most interesting parts of the whole discussion. I'd be curious, Miles, like what did you find yourself wondering or thinking about when we were asking about know?
01:47:28.750 - 01:48:48.562, Speaker D: I think this is probably the edge of right at the cusp of what becomes something that could get voted down because it's too complex by the Lido Dow community. Because I guess my initial sense is this kind of fundamentally changes the product. You could look at that way or you could look at it in the way of, well, we are managing risk already across all these different validator sets and this is just a little bit more risk than those other validator sets, but nothing we can't manage. Right. But it does introduce complexity in sort of the way that you would claim steath or non native staking rewards and things like that and so my sense is that it's something that is possible but I think it's a lower priority as it relates to lido's core objectives for the next couple of years. I got the sense that they would almost prefer to delegate the choice of whether or not Steeth holders want to restake to the users themselves. And that's possible right now since Steeth has been onboarded as an LST for Eigen layer.
01:48:48.562 - 01:49:26.850, Speaker D: And I'm sure there will be Eigen layer competitors that will accept Steeth since the largest liquid staking capital base. And so, yeah, I came out with the impression that, yes, it's possible, but it's not something that is a super, I don't know, high priority. But we will see, right? As you mentioned, this is bear market and folks have care a lot more about risk management at this point and a lot less about yield. And so we'll see how much that changes over time. And I think at the very least it'll be a very spicy discussion if and when it's introduced.
01:49:27.350 - 01:49:59.980, Speaker A: Yeah, I tend to agree with you on that. I think I do fall out on probably the hasu perspective here which is that vertical integration if it ever gets explored should be much later in the protocol's lifecycle. I think right now they should try to minimize risk and complexity and I think this would be definitely taking that on. That said I view something like this even if it's not lido and Eigen layer as probably.
01:50:02.030 - 01:50:02.502, Speaker D: Agree.
01:50:02.576 - 01:51:01.406, Speaker A: I think there's a rich history of we've talked know the example would be bond ETFs. So that is actually any readers of Matt Levine out there, he's one of the kings outside of, you know, he has this trope, which is people are worried about bond market liquidity. And it's actually very similar to this, which is there's one bond ETF, but it actually represents many thousands of individual bonds, each one of which has their own idiosyncratic risk profile, but it trades as one liquid thing. And it's actually worked kind of well. Another example would be this is going to sound dramatic, but it's a very similar example of, like, CDOs or Clos during the housing crisis, where you actually had one sort of liquid instrument, but it was a bunch of mortgages that ended up getting packaged in there. And there was an idea that if you diversified enough, it wasn't really that risky. And you can see how after a period of time, people might not worry about that.
01:51:01.406 - 01:51:36.246, Speaker A: It's been working for a couple of years we haven't had a problem yet and I think the most likely outcome here is that it's not Eigen layer and lido that end up integrating like this but some other it's highly unlikely to me that Eigen layer is going to be the only restaking protocol that exists out there. There'll probably be other flavors of it launched maybe one that won't be so concerned with the fungibility of positions and the denomination of the rewards and I think that's probably what ends.
01:51:36.278 - 01:52:15.910, Speaker D: Yeah, I guess my point is I think Lido, of all LST providers that exist today or come along tomorrow, is probably the least motivated to do this given their current market position. And really they're trying to reduce complexity, they're trying to grow alignment with the L1. And introducing something like this, I could see it much more realistically coming from a competitor that is trying to capture market share from a much lower position. Right, and in doing so by offering outsized yield compared to the safer incumbents like Lido.
01:52:17.930 - 01:53:21.182, Speaker A: I completely agree with that Sedment, but you can see how these things shift and change over time, right? I mean, even within crypto, maybe a more relevant example would be one of the big advantages that people used to tout was synchronous composability or atomic composability and now it's like Async's probably good enough. And there is this idea of you being at the mercy of your stupidest competitor. I'm not sure if you've heard that, but kind of an example of that might be competition that played out in between Celsius and BlockFi. I'll let you assume who was the stupider competitor and who is at the mercy of who, but I think you sort of see my point is that it's just very hard to resist these things in the moment. I think that's the point that I'm trying to make. What do I know? I mean, people who work at Lido or contribute to the Dow have been thinking about these issues for so much longer than I have. But I would just say it's probably a good thing to not get involved with even as time might be.
01:53:21.316 - 01:53:22.302, Speaker D: I think that's fair.
01:53:22.356 - 01:54:13.354, Speaker A: Remember in the first episode of season three, sonny introduced this idea, the idea that being an app chain and permissionless was an oxymoron. I've thought about that quite a bit since he introduced that idea, and I've been thinking about it more and more, and we talked about the App Store analogy here, but I do think that this is something that protocols that are permissionless. There's probably some logical limit to how permissionless you really want to be while providing a good product for your end users. I think that's a very interesting tension to explore. And for something like a staking pool like Lido, I'd imagine that there's some amount of monitoring of validators or node operations that you always have to do. We talked about. We didn't bring it up on this episode.
01:54:13.354 - 01:54:57.546, Speaker A: But for instance, it could make sense if there's some internal fee market that gets developed within Lido, it's enabled by the Staking module that some group or module lowers their fees to zero and they don't monetize that way, attract a whole bunch of stake and then they sell colocation services to their validator set. Or there could just be much more. Simple sort of off chain contracts that you end up colluding with one another. So I'd be very curious about I think this will get played out in time. And I'm not sure this is all just speculation, but I would imagine there's some amount of either monitoring or due diligence or something that these staking pool operators like Lido have to do on.
01:54:57.568 - 01:56:17.798, Speaker D: Their yeah, I think the staking router will actually increase the need for more robust. You know, I know they have a close relationship with a project called Rated, and Rated is basically a node operator monitoring service, looking at things like uptime and performance, et cetera, et cetera. But then apart from just generally monitoring the operational performance of these nodes, there's also the subjective decision of, okay, what is the healthiest, I guess, way to allocate deposits across all these modules. Right. And to your point, if you just left it to the free market, probably the allocations would be heavier towards the larger, safer operators. And so yeah, I think that that's going to be as much as Lido would like to minimize the surface area of human involvement and decision making in the protocol, I think this is one area where actually if you were to completely minimize it, then it would be not healthy for the protocol. Whereas if you were to ossify something like the withdrawal contract, people would be happy with that.
01:56:17.798 - 01:56:43.070, Speaker D: And so, yeah, I think that this is likely points to the reason that something like PBS can be enshrined into the protocol, restaking could even be enshrined into the L1, but something like liquid staking could never be enshrined into the L1 based off of the principles of ethereum. There always needs to be somebody monitoring the deposits and the operational performance of these nodes.
01:56:43.490 - 01:57:27.020, Speaker A: Yeah, I would point folks, for this sort of trade off in between something where human capital doesn't need to be involved as much. Is there's an old multi coin post? I think the title of it is Dows manage risk. And it highlighted a key difference between maker and uniswap. They were talking about it within the context of how susceptible a protocol might be to forking. And the idea being, if you're a maker, one of your core competencies is that you underwrite a whole bunch of risk, right? Like you're managing the asset and liability side of a balance sheet. You're less susceptible to forking. But I think it applies to the rest of these Dows as well.
01:57:27.020 - 01:57:30.460, Speaker A: It's something to consider.
01:57:32.370 - 01:57:32.878, Speaker B: A little bit.
01:57:32.884 - 01:58:04.200, Speaker D: More looking more like makers operations, right, where they're looking over their balance sheet and kind of making sure that it's healthy. Right. I think that you can look at something like that as analogous with monitoring all these the health of the modules and how allocations are split between them and yeah, absolutely, that makes a lot harder to fork. But still there needs to be some human involvement there and it's just about minimizing that as much as possible.
01:58:04.570 - 01:58:45.666, Speaker A: Yeah. All right, Miles, this was a fun one. Next episode is going to be great as well. So we're going to be diving a little bit more into what we just touched on at the end of last episode. We're going to be talking to Sriram Kanan of Eigen Layer, and we're going to be delving into kind of just getting an overview, frankly, of Eigen Layer, the protocol. I think we are conceptualizing the episode as if Eigen Layer is also a two sided marketplace in between stakers and sort of middleware operators that want access to that stake. Then what are some of the drivers on the demand and supply side? And then what is this intersection going to be with big liquid staking protocols and protocols? Eigen Layer.
01:58:45.666 - 01:58:51.220, Speaker A: So I think that's going to be a great episode as well. All right, budy. I think we can wrap it there. This was a fun one.
