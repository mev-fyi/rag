00:00:00.170 - 00:00:22.366, Speaker A: Alright, everyone, welcome back to another episode of Bell Curve. Before we jump in, quick disclaimer. The views expressed by my co host today are their personal views and they do not represent the views of any organization with which the co hosts are associated with. Nothing in the episode is construed or relied upon as financial, technical, tax, legal, or other advice. You know the deal.
00:00:22.468 - 00:00:48.890, Speaker B: Now let's jump into the episode. This isn't so much a choice that we have to direct value and somewhat like play God with the value flows in the industry, but rather, if they do not receive this value back, they will never be profitable. They will never be sustainable and we will default back to an extremely centralized set of actors that offer price discovery through an opaque RFQ system.
00:00:48.960 - 00:01:28.802, Speaker A: All right, everyone, we will be back to the program in just a moment. But before we do, I want to share something that Blockworks has been cooking up for these last couple of months. March of this coming year in London, blockworks is hosting Das London, the largest institutionally focused conference in all of Crypto. Goldman, JPMorgan, Zero, 72, all in one room, so you can know what the big money is doing. So click the link at the bottom of this episode, it'll take you right over to the homepage and use Bell 20 for 20% off and I will see you in sunny London town in March. This episode is brought to you by Maverick Protocol, a suite of liquidity tools built around an innovative AMM. Maverick helps token projects dow, Treasuries, LPs, or basically anyone in DeFi shape their liquidity with efficiency and flexibility.
00:01:28.802 - 00:01:37.814, Speaker A: How, you might ask? Stick around and you're going to be hearing about them more later. Now on with the show. Hey, everyone. Welcome back to another episode of Bell Curve. Very pumped for this episode.
00:01:37.862 - 00:01:38.122, Speaker C: Today.
00:01:38.176 - 00:02:15.078, Speaker A: Dan and I are going to be interviewing Anna from Calswap and Ludwig from Sorella labs. We're going to be talking about batch auctions as a mechanism, which is obviously part of a build on from last episode where we described the whole family of order flow auctions. We zero in on batch auctions in this episode and explore how they can mitigate some of that mev leakage that we've been talking about this entire season. Really pumped for this one. There were a lot of gems, so hope you all enjoy. All right, everyone, welcome back to another episode of Bell Curve. I am joined by my co host, Dan Robinson, and today we've got Anna of Kaoswap and Ludwig from welcome to the welcome to the program.
00:02:15.244 - 00:02:17.014, Speaker D: Thank you so much for having us.
00:02:17.132 - 00:02:18.520, Speaker B: Thank you for having us.
00:02:19.450 - 00:02:38.478, Speaker A: This is going to be a ton of fun. So before we sort of get into this episode where we're going to be talking about batch auctions and sort of relegating mev from the base layer protocol to applications which I know both of you have a lot of thoughts about you. Just kind of give us a quick 32nd intro to yourself and the protocols that you're working on.
00:02:38.564 - 00:02:55.966, Speaker B: Hey, I'm Ludwig. So working at Sorella Labs. We're building angstrom. It's our kind of response to the problem of LVR loss versus rebalancing the fact that LPs are not really sustainable today, so we're trying to make them positive in expectation.
00:02:56.078 - 00:03:29.294, Speaker D: Cool. And I'm anna and I joined the blockchain space in 2017. Back then I joined Gnosis. So as part of Gnosis, I early on started already looking into Dex designs and specifically into batch auctions. And then after working on a few different deck designs that were only semi successful, we finally landed on what is now Cowswap. And yeah, Cowswap is a decentralized exchange that's based on frequent batch auctions. It's an intent based system we launched in May 2021.
00:03:29.294 - 00:04:08.474, Speaker D: So I believe we were one of the earliest applications that applied intents in such a wide range. And on carsops, users sign transactions where they give price of the sign, the relevant information, such as the buy token, the sell token, the expiration time, and the limit price. And then the orders gets collected in an order book and is then picked up by a competition of decentralized parties that we refer to as the servers. And they compete for the order flow by maximizing the value that is given back to the users. That basically ensure that's what Carson is doing.
00:04:08.672 - 00:04:47.190, Speaker A: That's super helpful. And I think where we want to spend the bulk of our time for this interview is actually talking about solving mev at the application. And I know one of the themes that Dan and I have been poking at this entire season is sort of this win win situation for both swappers and LPs that we can solve by mitigating or stemming some of the mev that leaks out from the application. So even just talking about designing mev aware sort of mechanisms and preventing that leakage is, I think, where we want to spend a lot of time. But just for listeners. The last episode that Dan and I did was on order flow auctions. And that whole design space and batch auctions are obviously a part of that family.
00:04:47.190 - 00:04:57.434, Speaker A: So before we get too far into the weeds, could one of you maybe Anna, could I just pick on you or could you just describe what an FBA is and maybe some of the primitives that come along with it?
00:04:57.552 - 00:05:49.894, Speaker D: For sure, and maybe I can even give a bit of a broader context that might be interesting. And if I get too long, just interrupt me. That's great. So frequent batch auctions is essentially a concept that has already been studied within tradefi for quite some time. And it's essentially offering a solution that is providing a solution to a problem that exists in one of the more widely known and adopted financial markets that are using continuous limit order books. And the issue behind limit order books is that they treat time as continuous. So in other words, there's usually a serial processing of orders and this allows in a way riskless arbitrage profits even in an environment where information is symmetric.
00:05:49.894 - 00:06:46.382, Speaker D: So even if everyone has exactly the same information by ordering them sequentially you have the incentive to be faster than everyone else in order to extract value. And to illustrate this better, there's this famous example of how at one point there was a cable built between the Chicago Stock Exchange and the New York Stock Exchange just in order to get information milliseconds faster. So not even like the blink of an eye significantly faster. And why was this built, even though it cost millions of dollars is because it was worthwhile. Because having access to the information just a little bit faster enabled one party to do front running. Like basically having this information faster and then take profit from this by front running. Other traders and these are basically riskless arbitrage profits and they are not supposed to exist in a well functioning market.
00:06:46.382 - 00:08:31.920, Speaker D: They harm liquidity and also they include never ending arms raise for speed. And a market design solution to this problem is essentially if the batch auctions is if you put time into units or discrete time and basically process requests to trade in a batch by using those auctions and this would have two benefits, right? One is that you have discrete time which reduces the economic relevance of these tiny speed advantages that you have and then secondly, it also by applying this auction it changes the nature of the competition because now instead of competing against speed you are now competing for price. This is essentially like the problem that was existing in tradefire is existing in tradefight, which is known as high frequency trading HFT. And I think a very similar problem exists on blockchain today with mev. And I would argue that the problem on blockchain is even magnitudes higher because since you have the public mem pool, where all transactions basically linger for up to 15 seconds until the next block is validated, you have advanced players that can now take advantage of, like, normal retail players. And most people, obviously, who listen to this are very familiar with MAV. So by reordering the transactions and by simply paying higher gas to front run transactions one can essentially extract significant financial gains and I would say for like there's many cases where a single front running or sandwiching attack gains profits of a few thousand dollars.
00:08:31.920 - 00:10:05.690, Speaker D: And on ethereum and this is where I think the argument gets very interesting is on Ethereum you already do not have continuous time, right? You have block time so every twelve to 15 seconds and your block is validated. So why would you apply this continuous time model in a non continuous environment? And therefore to me it seems like very straightforward or intuitive that you would basically apply batch auctions to blockchain which mitigates these negative externalities of mev. So essentially, I'm sure we dive a little bit deeper into this later. But mev arises because if you have this sequential execution of transactions, right, you have like different prices for one asset in one block, and then they are arbitrarily ordered or like someone else is purposefully ordering them in a specific way to maximally extract slippage. And in a batch auction, essentially what you do, you batch all those different assets, like all the assets you ensure that they have that are cleared with one single unique clearing price. And by doing this, you have not this slippage tolerance anymore that you can extract based on reordering the transactions in a certain way. And yeah, if you look into this data specifically, I think there's like many blocks where we have seen that a single asset, such as something highly liquid as USDC had already a price spread of USDC East, they had a price spread of 1% in a single block.
00:10:05.690 - 00:10:10.910, Speaker D: And that's, of course, not very efficient. And so batch auctions removed this inefficiency.
00:10:11.330 - 00:10:18.082, Speaker C: Thanks, Anna. Yeah, I think that's a great description of batch auction. I think a lot of us are also a fan of the British paper.
00:10:18.136 - 00:10:19.518, Speaker A: On batch auctions that I think you're.
00:10:19.534 - 00:10:59.306, Speaker C: Referring to there, and continuous versus discrete time. One theme we've talked about on this show is the difference between mev and Dexmev. So one is emitted by swappers with transactions in the mem pool, and that can be captured as slippage or by sandwich attacks by someone. I can see how a batch auction, an off chain batch auction, like cowswap can prevent that or mitigate that by executing all trades in a batch at the same price. And so you can't sandwich somebody, you basically have to trade at the same price as them. What about the other kind of mev that's emitted by passive liquidity providers on chain and that's often referred to as lever.
00:10:59.418 - 00:12:10.614, Speaker D: So thinking about, I guess, for example, liquidations, how does a liquidation occur? Like, maybe somebody sees that the price Oracle is changing and wants to directly back run this transaction and then take advantage of it. And if you look at some of the popular liquidation protocols, such as AVOC compound dYdX, they usually come with a fixed discount. So essentially, let's say that at a 5% discount, your collateral is being sold. And now if you would plug this into a batch auction, the order flow before it gets publicly liquidated, you would say, okay, immediately. Once the price point is hit, like basically you are under collateralized, immediately the volume is being sold via a batch auction. Then without providing this 5% price discount, you could save within the first X amount of minutes. Maybe you give access to a private order flow for auction that is maximizing for giving users the value, rather than just saying, oh, the cut is taken pretty far.
00:12:10.614 - 00:12:11.720, Speaker D: That 5%.
00:12:12.090 - 00:13:28.046, Speaker B: I was initially more so thinking about the problem of stat ARB and specifically sextex. So it's the idea that there's some exogenous price discovery mechanism that is quicker and that has deeper books where the bulk of the volume actually occurs. And here I'm specifically referring to Binance, where in effect, compared to the mev that originates from users, where some user doesn't communicate a strict slippage tolerance, and then there's some direct ordering and extraction through like a sandwich attack. What you actually have as a worse problem is the extraction of the stale quotes of LPs and it really occurs. And you can think about it fundamentally from the principle of VLPs lacking that information and lacking the ability to change their stale quotes. So Binance will have a price change and VLPs don't have time to react. And given this, they're sophisticated actors, specifically vertically integrated builders that are going to see that these LPs are offering the wrong price and are going to bid up in the block auction to be able to be the first.
00:13:28.046 - 00:13:38.050, Speaker B: To touch the pool. The first to effectively extract that value from those LPs and buy at this wrong price and then execute the second leg of the arbitrage on the off chain venue.
00:13:38.550 - 00:13:44.718, Speaker C: So how would you mitigate that? So how do you avoid that value being captured by proposers?
00:13:44.894 - 00:14:34.398, Speaker B: Well, I think fundamentally speaking, this value is already being recaptured very efficiently today. It's just not by LPs. We're effectively paying the wrong party. And what I mean by this is that if we look at the proposer builder separation infrastructure today, you have a builder that effectively bids in the block auction to the proposer and in a competitive builder setting, they're bidding most of that value back to the proposer. If we're able to shift this and remove that proposer's monopolistic ability to decide who gets to execute the arbitrage, you can then have an auction that you run specifically to pay the LP the most to get the right to execute that arbitrage. Because at the end of the day, they're the ones that are effectively incurring this loss. They're the ones that are suffering that economic loss.
00:14:34.398 - 00:15:02.780, Speaker B: So at least what we're kind of building with Angstrom is this idea of bringing back this auction and effectively directing the value of the auction directly to the party that is losing the money directly to the LP. So how you do this, simply put, is that you run a very similar auction to the block auction, but instead it is purely for the right to execute the arbitrage against the LP. And so the LP gets the bulk of that.
00:15:04.830 - 00:16:03.360, Speaker A: Think that's a I want to dig into the construction that you have on that on Angstrom Ludwig because I think it's really interesting. But I think it's an important point to underscore for the audience because when people tend to talk about mev, they talk about it as almost springing from ethereum because that's where the largest and sort of most robust ecosystem for mev exists today. But I think you could make the argument that a lot of mev actually originates from the applications themselves and based on the mechanism design of those applications. So I would be curious, maybe just an open question to the both of you before we get into the weeds of it. How do you think about from a high level designing mechanisms that are mev aware, right, with the ultimate goal being to redirect that mev that's getting extracted mostly by proposers at the block space auction today and redirecting that back to LPs. Kind of a high level question. How do you think about when you're designing these mechanisms? About keeping mev in mind?
00:16:03.830 - 00:17:06.994, Speaker B: Generally speaking, I'd say you have to be cognizant that at any point in time your application might provide a state where it is purely profitable to execute on it because of some exogenous kind of movement. May that be in price or in some information that your application does not have. Because smart contracts, they're not that smart really. They exist in isolation on the blockchain. So without providing them information, they will just follow the rule set that is effectively written in the smart contract. So how I think about it is a being able to provide that information through some mechanism, and b removing the proposer's control over that application, removing the builders and proposers ability to extract value without that application effectively signing off on it. Because if you have some very smart mechanism but at the end of the day, the proposer can sequence a transaction that executes against your state and extracts that value, you're going to have to pay them to effectively do it in a way that kind of recaptures that value.
00:17:06.994 - 00:17:29.610, Speaker B: And that rent is being paid. But that is an unnecessary rent to pay. So the most important thing that I'd say is you have to design the application whereby you are cognizant of the fact that some mev opportunity is going to be created. But when it is created, the application itself has to effectively give the right to someone. You have to be in control of who actually gets to extract that value.
00:17:29.760 - 00:18:44.530, Speaker D: Yeah, and I think Ludwig touched on a very important point, which is like, as soon as you basically have some form of mev surface, usually in the current design space, most of the times it's just being totally at the advantage of a validator right. Like ultimately you follow the mev chain and most of it ends up going to the validator. From that perspective, I agree it's really difficult to design in a way that no mev is exposed in the first place. And you can argue that even in a batch auction, to some extent there's value that solvers are dealing around with. But then I think it becomes important that the way you design the competition is in the way you set up the rules of the competition, in my view, should be set up such that they benefit the user. And of course, this is a philosophical question and not everybody might have the same opinion on this, of like where should the value of mev actually go to but me personally? And working for Kausup, it's very clear that my opinion is a I think it is creating inefficiencies the more mev you're exposing. Because ultimately, if you expose it, you need to somehow redistribute it.
00:18:44.530 - 00:19:48.390, Speaker D: And that also means that you are giving up block space. Right. At least you need to incorporate an additional transaction of where this mev is later on given to so it is making the market less efficient. And then secondly, from a more philosophical standpoint, I would argue that if the user is creating the mev opportunity in the first place by being basically vulnerable to external people, extracting value from them, then my view is that that mev actually belongs to them because they exposed it in the first place and it's taken away from them. And by creating a market framework where you say okay, maybe there is some exposure possible to create the rules of the competition in a way where you say okay, only those people gain the order flow who actually give the value back to where it belongs and I would argue to the user are winning the competition. This is a way of how you can design an application in a way that is more user friendly.
00:19:49.070 - 00:20:54.990, Speaker B: Yeah, I somewhat add on to the idea that it's a philosophical question. I think it's more so an existential question for Blockchain in the sense that this value from mev is most of the time extractive someone is losing. And we see this with LPs today. This isn't so much a choice that we have to direct value and somewhat like play God with the value flows in the industry, but rather, if they do not receive this value back, they will never be profitable. They will never be sustainable and we will default back to an extremely centralized set of actors that offer price discovery through an opaque RFQ system. And I don't think that's a desirable thing. We have to make the decentralized applications sustainable by redistributing that underlying value because otherwise the participants that are providing liquidity that are providing this extremely necessary service will cease to provide liquidity because they just won't have any capital left because they're going to be continuously arbitraged and continuously at a loss.
00:20:55.570 - 00:21:20.002, Speaker C: You mentioned some of the professional participants in the system as well as the centralized parties that might actually take it over if we don't design better mechanisms. There are a lot of these kind of professionalized participants in Ethereum today. I know in Cowswap has solvers and some of Angstrom's design. There's a role for professional market makers there.
00:21:20.056 - 00:21:51.614, Speaker A: All right everyone, we will be back to the program in just a moment. But before we do, I want to share something that Blockworks has been cooking up for these last couple of months. March of this coming year in London, blockworks is hosting Das London, the largest institutionally focused conference in all of Crypto. We are gathering 1200 of the world's largest asset managers. So think TradFi, macro funds, crypto, native funds, big allocators and financial institutions. So banks, payment processors, et cetera, all in one spot. It's very rare to get the likes of Goldman, JPMorgan Zero 72, whatever all in one room so you can know what the big money is doing.
00:21:51.614 - 00:22:23.754, Speaker A: We're diving into the themes that they care about. So we're talking about the intersection of Macro and crypto where we are in the cycle, real world assets. So everything from stablecoins to onchain treasuries to tokenized assets, it's going to be a blast. But the other reason you really want to go is London baby, center of the world. At one point you got pub culture, you got fish and chips, great beer, it's going to be a blast. So because you're such great listeners to Bell Curve there's a code, bell 20 that's going to get you 20% off. So click the link at the bottom of this episode, it'll take you right over to the homepage, you'll see all of our speakers and use Bell 20 for 20% off.
00:22:23.754 - 00:22:29.398, Speaker A: Ticket prices are going up soon. Make sure you go use that code. I will see you in sunny London town in March.
00:22:29.494 - 00:22:41.360, Speaker C: I'm curious I guess Anna, how do you see the role of the solvers in cowswap and who are those solvers and what powers do they have?
00:22:41.730 - 00:24:08.822, Speaker D: Who are the solvers? I think it's actually a variety of different actors. We do have a few individuals who we initially I think just nerd snipe to getting into the competition. Then after time I think our competition got a bit more sophisticated by more market makers entering the competition who of course by adding their own private liquidity have a structural advantage. And then we also have protocols such as urine who essentially just want to facilitate specific use cases that are maybe not in the interest of the broad variety of servers because those trades happen not frequently enough and are not providing enough profit opportunities but are still for protocols specifically interesting to be executed on. It's interesting because of course I think all protocols that have these external, whether they're called servers, fillers or resolvers are dependent on them because you ultimately want to ensure that you provide a good experience to your users. Meaning that there is competition. The less competition there is, the more risky it is that actually at one point there is no profitable execution or no good execution for the user.
00:24:08.822 - 00:25:07.950, Speaker D: The risk is somewhat limited because of course the user signs their order with the limit price. So the limit price is guaranteed pair smart contract guarantee. But the slippage tolerance is something that could theoretically be at risk if, let's say, there is suddenly no server competition, only one server executes, they have no incentive to provide a better price than anyone else because they're only the only one competing on this. And then theoretically, they would be able to extract this lipid. So from that perspective, I think all protocols that are working on this decentralized competition really require a good set of actors that are competing with each other. But I think in a way you can even compare that to block builders and searcher environment where essentially you in a similar way, are relying on a strong, decentralized competition.
00:25:08.770 - 00:26:22.230, Speaker A: So maybe just to double down on this question a little bit, because I feel like one of the common components of these more mev aware designs is you're relying on this sort of third party network of everyone's got a different name, right, if solvers or fillers or searchers or whatever. And I feel like maybe intentionally or not, even when I went into doing mev season and this season, the sort of idea is that searchers are these little, like, at home one person shops writing code and stuff like that. And I feel like more and more that is not the case. And it's a smaller group of entities that you might think that are now what people refer to as like integrated searcher builders, which are basically hedge funds. And my sort of understanding, if I had to really guess at it, would be that it's mostly the same entities that are either fillers on uniswap backs or solvers on cowswap or whatever. Any granularity that you can give for listeners about how similar are these entities across all of these different platforms? Is it mostly these sort of hedge fund entities? And then how do you see the future of that market shaping out? Because if this is the direction that we're going in terms of app design, I feel like it's important to have a competitive market. I don't know if it needs to be thousands of independent searchers, but how do you see the future of this sort of market of sophisticated entities?
00:26:22.650 - 00:27:17.266, Speaker B: In terms of the people that I've seen on the solver side, on the builder side, and on the searcher side, solvers are like, as Anna mentioned, quite a diverse group of people. At least they were in the beginning. You have these small teams from these very random backgrounds that were working in the beginning and a lot of them are now no longer able to operate. And so now you only really just have the best of that initial diverse group and then the hedge funds that kind of opted in. But realistically it all converges towards statarp, it all converges towards market makers that are just more efficient. And so I think I'm somewhat frustrated at this idea that we have all of these different roles. Like as you kind of mentioned, they're all converging towards the same person because a builder is a better solver and a builder is also a better provider of liquidity on any underlying liquidity source to Arbitrage and AMM.
00:27:17.266 - 00:28:01.202, Speaker B: So they're also a better searcher. And if we don't design applications where there's also this sense of we can operate kind of collaboratively, there is some force in us all optimizing for a problem and not having a single winner that takes all, you're always going to have that. And so without kind of applications that design that with that in mind, the competition will die down to a select few hedge funds and entities that are running very sophisticated builder infrastructure and are able to build on block N on latest state because they're aware of the block template, so they're cognizant of all of the state. And so even for a simple routing problem, for a solver, for example, they're better equipped.
00:28:01.266 - 00:28:05.958, Speaker C: So should we try to prevent this from happening or should we just lean into it or embrace it?
00:28:06.044 - 00:29:09.302, Speaker B: We should aim to maintain extremely permissionless competition, so anybody that wants to opt into the market can operate in terms of limiting that somewhat force of centralization. It's extremely difficult because these entities have economics of scale, they have greater efficiency. So it's very hard to design applications with that in mind, especially when you have privacy that enters because then there's an advantage for private order flow. So I think we should push against it, but we should do so not by forcing it and introducing unnecessary efficiency, but just simply coming up with better application design where that privacy and that inherent hyper competitiveness where you have a winner takes all, isn't like an existential property of the system. If you can have some winner of a very sophisticated actor but still have others that contribute to the solution and that are effectively pushing the solution to a more efficient end result, I see that as like a positive outcome, literally.
00:29:09.366 - 00:29:37.910, Speaker C: I want to get back to what we were talking about before about the two different kinds of mev and decentralized exchange are the dominant kinds. One that's suffered by traders by having their trade sandwiched or suffering slippage, and one that's suffered by passive liquidity providers. So my understanding Angstrom tries to solve this problem by separating the auction into two pieces. Do you want to talk about how that works and get more into the details of how Angstrom solves this?
00:29:38.060 - 00:30:42.570, Speaker B: Yeah, absolutely. So Angstrom at its core is looking at the current kind of supply chain and saying, well, we've already found a really efficient way of recapturing this value, so let's re implement that at the LP level. And how you do this is you have to a remove kind of random execution on state where anyone can execute. You have to gate it and we do so by gating it behind effectively a verification of consensus. So you have this network that is dedicated to our application, which we call the guard. Network which is run and can be run by anyone that effectively provides stake and you're effectively peering around all of these transaction and orders and then structuring them within a bundle within this single transaction that the proposer or the builder can't effectively break up. So you're removing their power to sequence and order transactions and you can break up the bundle into two components the top of bundle where the underlying arbitrage is executed and like the rest of the bundle where user orders are effectively batched at a uniform clearing price.
00:30:42.570 - 00:31:46.222, Speaker B: And how that works and why this is actually useful is that the top of bundle is the arbitrager that is incentivized to effectively arbitrage back to true price but they're competing against all of these other searchers to effectively bid and provide the biggest kind of kickback to the LP. So only the searcher that effectively contributes the most value back to the underlying LP gets to execute that arbitrage. After this for the rest of the bundle, you have a frequent batch auction that provides this uniform point of clearing price. And how you arrive to that is you look at all of the user orders and conceive of them as limit orders and you can construct and find the point of intersection between the supply and the demand. So the supply as in the asks, so sell orders and the demand as in the buys. And as you find that point, you can effectively arrive to that single fair price. But you add in the constraint whereby LPs are effectively only providing liquidity at that final price.
00:31:46.222 - 00:32:03.460, Speaker B: And what that means is that they're no longer offering that stale quote because all of these market makers and user orders are effectively providing that information whereby the LP is now cognizant of the fair price where users and market makers are willing to trade at and they only offer liquidity at that point.
00:32:04.710 - 00:32:32.990, Speaker C: Both Angstrom and the designers for Angstrom and Cowswap have batch auctions in them and batch auctions limit in some ways what you can do with an individual trade. I'm curious Anna, how Calswap thinks about the composability and whether pushing users into this bash auction model limits what they're able to do with their assets on trade on chain or whether you have new solutions to get around that.
00:32:33.140 - 00:33:11.866, Speaker D: Do you have an example in what way it limits that? Because, for example, cowswap also launched pre and post order interaction, right, that we refer to as cowhole, which also makes it now very composable in the sense that you're now able to come up with any pre order interaction. Pre swap and any post swap that you want to do whether it's like we solve my whatever maker CDP now I want to swap it and tap into Arbo for example like all those things or I want to swap and then trade cross chain all those are possible. I don't know if that's what you.
00:33:11.888 - 00:33:18.620, Speaker C: Were hinting at yeah, I'm asking about I think about programmatic orders or that's what you refer to. So how does that work?
00:33:19.950 - 00:34:00.434, Speaker D: So it's two separate things. One is essentially just allowing that you add any type of on chain interaction before and after a normal swap. This is what we refer to as cow hooks. And then with programmatic orders, this is actually something that we literally just went live with this week and it's actually pretty cool. It's leveraging ESC 1271. So it's now allowing any smart contract based system to essentially place any type of programmatable order in the future. This can be used to generate multiple discrete orders, basically endless.
00:34:00.434 - 00:34:37.960, Speaker D: Even if you want to create 100 different TV orders, for example, you can do it with a single signature. If you want to update those by, for example, saying, I want to change the date, I don't want this to be executed every day, but instead every week or I want to change the slippage, you can adjust this for all hundred orders within a single signature. And yeah, you can assess a proposed order against a set of different conditions. So you can really come up with anything. You can come up with stop loss with DCA orders. Or to come back to the topic of today of essentially, like, the issue of liquidations. Think about.
00:34:37.960 - 00:35:34.898, Speaker D: You have your assets wherever, and then you want to protect it because you're getting close to being liquidated. And you're thinking, okay, I actually don't want to continuously monitor my position, but instead, I want to automatize an order that says once I am closely to be liquidated, I actually want to buy more of my backed assets in order to secure my position. You can automatize this entirely with Kauswap today, or you can even think the opposite if, for example, you have your backing in Ether. Ether is increasing in price as it is right now. So essentially you would be able to loan more against the east that you have already deposited. You can automatize this and automatically via take more loans out automatically without individually having to monitor your portfolio.
00:35:35.074 - 00:35:41.926, Speaker A: I've got a question. How long does it take about just time wise to run a batch auction for either cowswap or angstrom?
00:35:42.118 - 00:36:16.150, Speaker B: For us, so we do it every block. But there are optimizations that can be done in terms of gas efficiency because for us, we're able effectively to compress down all user interactions within uniswap to a single actual uniswap trade, which makes it far more efficient. So if there are very few orders, you can effectively prolong the duration of the auction to collect more and then gain that gas efficiency. But in times of effectively high swap count, like high volatility, you can have one every block.
00:36:17.290 - 00:36:36.170, Speaker D: For us, it's also 12 seconds, but with the caveat that we are not yet synchronized to block time. Instead, we start an auction whenever an order comes in. But this is something that we probably want to work on beginning of next year, that we have sync time with block time. So we have one batch per auction.
00:36:36.750 - 00:37:11.078, Speaker A: That's super helpful. I guess what I'm trying to get at is here I'm trying to understand if there's a different swapper. Like the profile of a swapper for an FBA, like a cowswap or angstrom is different than uniswap per se. For instance, I would guess especially during the depths of the bear market when there's not a lot of organic on chain trading activity, there's a good amount of sextex ARBs that are making up the majority of certain uni pools. Is it as effective to do that through an FBA type? Like, I would guess it's more organic order flow that's moving through there. Right?
00:37:11.244 - 00:37:43.582, Speaker D: Yeah. Also for us, we basically never compare uniswap v one two three volume with our volume. But we only look specifically at dex aggregator volume in comparison because all this arbitrage volume you would never have on a batch auction. Right. Not even on a dex aggregator. The numbers vary. But I think there's estimates that arbitrage volume on uniswap is between 50 to even 80% or so.
00:37:43.716 - 00:38:18.442, Speaker B: Yeah, from the last estimates that I saw was about 80% for us. So we still have the arbitrage volume for the sextex because it definitely provides this gain in efficiency, especially in a competitive setting. Right. You have the best of the best that are competing to provide what is effectively the best oracle on price because their P L is dependent on it. So we still have that portion of volume that is going to occur once we launch. But we don't have the sandwich volume because by construction you can't sandwich a batch where you have a uniform clearing press.
00:38:18.496 - 00:39:06.006, Speaker A: Hey, everyone wanted to take a quick second to shout out this season's partner Maverick Protocol. Now, many of you probably know Maverick as an innovative AMM, which they are, but in reality they're a lot more than that as well. Maverick is a suite of tools for DeFi users and builders that allows them to put liquidity where it will get the most work done. Since Maverick launched in March, they have been gobbling up market share. And at the time of this recording, which is the end of September on a trailing seven day volume basis, maverick is now a top three decks by volume and they support over 50% of the volume on the L two ZK sync era chain. Maverick enables LPs and token pairs to process higher volume with limited TVL, which allows them to support some of the highest levels of capital efficiency for LSTs like wrap steep. Another very cool feature is something called Maverick boosted positions.
00:39:06.006 - 00:39:32.546, Speaker A: So that allows protocols looking to bootstrap their token liquidity to target the shape of liquidity of any token pair with surgical precision. Maverick is backed by some of the leading institutions in crypto founders Fund, Pantera, Coinbase Ventures, Binance labs. They are all backing Maverick in their vision to revolutionize the next generation of DeFi DApps and helping them build their liquidity in all market conditions. Click the link at the bottom of this episode. Let them know that I sent you.
00:39:32.568 - 00:39:45.910, Speaker C: Thanks guys. One thing we talk about a lot of the show is whether there's a role in the long term for passive on chain liquidity providers like I've been provided by AMMS. Anna, do you think in the long run we will have a lot of passive on chain liquidity?
00:39:47.210 - 00:41:14.686, Speaker D: I think it might change in the sense that I think for the very liquid token pairs USDC, ETH Wrap, bitcoin essentially you have a lot of very efficient market maker liquidity and then, okay, maybe you have some very efficient models on curve for stable coins. But I would assume that for long tail tokens you will always require on chain liquidity. Market makers typically have a portfolio of maybe the top 20 assets. And then even in a fragment like even if you look at a batch auction model that is able to do ring trading, such as Kauswab was able to do it in order to really leverage multi dimensional order book liquidity, you still will always have certain assets that will be hanging around in an order book. If there's no onchain liquidity for quite some time. And I think what will probably disappear is maybe AMMS that we know from today because we know that LP provisioning is not capital efficient and it's not profitable in most cases. But I do think we might come up with designs, with AMM designs that become more promising and more profitable for liquidity providers and for example, cowsop.
00:41:14.686 - 00:42:38.014, Speaker D: This is just like a side project that we did a little bit of research on, so it's not really our main focus. But we looked into cow AMMS and this is essentially looking at the opportunity of AMMS where essentially you would only solve trades in batches and at a uniform clearing price. So essentially you would collect orders, a multiple of orders and see if rather than sequentially being traded, if they are all traded, what would be the final price state of the AMM? And that state is the one that is being given to all orders in this batch. And we looked at it mathematically, we calculated it and this is why this is actually more profitable for IP providers even if you remove then the entire protocol fee, because obviously you also need to set some sort of incentive why you would be trading against such an AMM. And for liquidity providers this would be a more profitable way. And of course, if they could choose against providing liquidity on a normal AMM versus on a cow AMM, if they go purely by if you forget about all the other factors such as popularity, knowing uniswap more, being first. Mover advantage and so on.
00:42:38.014 - 00:42:48.386, Speaker D: If you ignore all those factors and just look at it from pure economic profitability, it would be a logic move for LP providers to shift their liquidity over.
00:42:48.568 - 00:43:57.362, Speaker B: I'd say that they're going to be able to do that very soon and on uniswap, because we're building on uniswap before, but effectively, like the design that you've kind of described there is very similar to what we're building. But I think we found that there is an improvement to be made on that design which we implemented, which is to say that that design by definition is leaving money on the table. And why I say this is because in the effectively function maximizing AMM, in the AMM where we have these batches and everyone is trading at the same price, what occurs is you, at least theoretically, are converging towards true price. Because as someone makes a trade, they are pushing the price naturally. And then that is the new effective price of the AMM. And some sophisticated actor might realize, oh, well, there's still money to be taken from that and they'll again push and then they'll progressively converge towards that true price. But where we kind of flipped that design is that process is inefficient in terms of arriving to that price because at the very end there is some delta that will always occur due to transactional fees.
00:43:57.362 - 00:45:02.154, Speaker B: So gas and then just swap fees and you're necessarily going to lose that. So where we kind of change that model is instead have an initial party that is responsible for arbitraging back to that effective true price. And if that effective true price is incorrect, let's have the rest of the bundle through market maker limit orders and user orders contribute to bringing that back to the effective price without having that additional cost. But in terms of kind of liquidity provision and passive liquidity going back to your question I think that passive liquidity will still exist but not as we know it and understand it today. All of the solutions that have kind of been designed for providing efficient liquidity on chain in a decentralized sense thinking of Iraqis and various other LP management protocols are plagued by this problem of LVR. They're plagued by the fact of life that they're continuously getting arbitrage. This is the problem that we initially worked on.
00:45:02.154 - 00:46:23.830, Speaker B: And why we kind of shifted towards Angstrom is because we realized that regardless of our knowledge and cognizant of toxic flow, regardless of how much information we have on Price, if we know that an arbitrage is going to occur, we cannot bid the value in the block auction to the builder for them to actually shift and execute a transaction that rebalances our position. Because to do that, we have to outcompete the arbitrager, and the arbitrageur is paying the builder from their profits and their profits equate to the LP's loss. So in effect, what you actually result to is as an LP you'd have to bid just as much as you would have lost and doing so is strictly unprofitable. Or you can effectively do a lot of work and do your best to be cognizant of toxic flow and move right before, but still have to pay that loss because otherwise the builder doesn't pick you or you just don't do anything and you effectively get arbitraged. So what we kind of realize is that if you remove that loss, there is quite a bit of incentive to provide liquidity in a very efficient, automated way. And so passive LPs as we know them will probably go through these third party services in hopefully decentralized applications and be able to provide their liquidity and have it managed for them, and then it'll be very efficient so long as you remove that inherent loss that occurs every time there's some toxic flow.
00:46:25.290 - 00:46:47.470, Speaker A: I've got a sort of follow up question for you here, Ludwig, and actually just wanted to maybe change TAC for a second. So I think we might not have made clear here that Angstrom actually is going to be a hook on Uniswap V Four. So I would actually love to get your thoughts here on what was behind the decision to launch as a hook on Univ Four as opposed to launching your own standalone decks.
00:46:48.130 - 00:47:54.046, Speaker B: Yeah. So for us, building an AMM and implementing that smart contract not very easy, right? It's already significantly difficult. It takes a lot of work and you have to audit it, which is extremely expensive, and these AMMS are holding a lot of capital and then there's more. So the network effect that Uniswap has, the sheer dominance in terms of people's habits of trading, when I want to trade on chain and I think, oh, where am I going to trade? Often I'll just intuitively think, okay, let's go to Uniswap. They've spent a lot of resources in the marketing end of the game to push that image, and I think that they've somewhat won on that end. So for us to say, okay, let's build this entire new design, let's have a more efficient system, and on top of that, let's compete at the marketing level, just seems like an inefficient way of going to market. And I think that that is why so many hooks are very happy to tap into the Uniswap network effects, because LPs clearly trust Uniswap.
00:47:54.046 - 00:48:18.758, Speaker B: They trust the smart contracts, they trust the quality of the code. So if you're able to innovate in finding a more efficient way of clearing trades and a more efficient way of providing value to LPs, and you base yourself on the base Uniswap smart contract, you can still be just as profitable. You don't have to leak value to Uniswap itself, but at the same time, you don't have to spend so much time working on all of those other things that seem quite difficult and very expensive.
00:48:18.934 - 00:48:58.758, Speaker A: Okay, one follow up question to you on that. So basically, to summarize here, the advantage is sort of there's the Uniswap brand, and you have a bunch of flow, right? So trading flow that's coming maybe through the Uniswap front end or wherever. So am I basically summarizing that correctly, that that's sort of the advantage. And then, I guess sort of a technical question, actually to both you and Dan maybe, is how does the routing sort of work there? What's the decision that the Uniswap front end or whatever makes in order to route to, let's say, Ludwig's hook, as opposed know other hook XYZ? And then, Ludwig, do you think about your competition as other hooks? Sorry, I've got a lot of questions for you on this.
00:48:58.844 - 00:48:59.238, Speaker B: Obviously.
00:48:59.324 - 00:49:00.774, Speaker A: I'm just curious how it all works.
00:49:00.892 - 00:49:35.634, Speaker B: In terms of routing. That's kind of an unsolved problem at the moment. And Dan, correct me if I'm wrong, but hooks add a fair bit of complexity in routing. And so I think that's where the shift in Uniswap X compared to an on chain router makes a lot of sense. So you kind of depend on that filler role to find the most efficient path to a given trade. So most of that, I think, is going to be pushed off chain. And this idea of on chain routing being efficient, I think is somewhat of an idea that passed because you're operating on state N minus one, you're operating on the previous state.
00:49:35.634 - 00:49:59.270, Speaker B: And as the block is then created, there's a ton of orders that might hit that state, and then you kind of do not have the route that you would have hoped for. In terms of competition generally, I see, like, competitions, other hooks, other people that are at the base layer trying to find more efficient design and better ways of making LP sustainable.
00:49:59.430 - 00:50:06.160, Speaker C: Anna. Why not build cow AMM as a unisoft v four hook? Can we convince you to do that?
00:50:07.010 - 00:50:29.522, Speaker D: It's just not really our focus. Right. We were just thinking about how can we make AMMS more efficient? Could be an idea to play around with, but yeah, definitely. I think we will stand away from building Cow arms as a standalone project because just competing with Uniswap on liquidity would probably be a nightmare.
00:50:29.586 - 00:50:45.238, Speaker A: It would be very difficult, I guess, just to get the other side of the coin here. Is there any downside to building on a hook? Obviously the upside is huge there, but not to put you on the spot there, Ludwig, but do you see any negatives for the negative trade offs of the hook?
00:50:45.414 - 00:51:24.370, Speaker B: There's the obvious concern that you're less gas efficient because at a technical level, you're operating within the design of Uniswap before, and you could be operating with a design that is applied to you. Right. We aren't bothered at all by that because, as I mentioned, we're going to arrive to swaps that are substantially cheaper than Uniswap V Four itself because of our batching, because of our ability to compute everything off chain and then compress everything down. So if you're operating just a standalone hook and doing all that logic on chain, it can get a bit more expensive. But I don't see it as that big of a problem, honestly.
00:51:24.530 - 00:51:46.986, Speaker C: There are also some just to be fanatic, there are also some gas efficiency benefits of V Four. Right. When you have multi hop trades on uniswap V Four, because of the singleton design, it's actually cheaper. Some of the costs end up actually getting amortized across the multi hop swap. So there's some benefits, particularly if you're anticipating it being arbed against other uniswap V Four pools.
00:51:47.178 - 00:51:53.418, Speaker B: I'd say so, yes, there are some benefits, but I think architecturally, it doesn't.
00:51:53.434 - 00:51:55.086, Speaker C: Pose limits that could increase the gas.
00:51:55.118 - 00:52:07.926, Speaker B: Absolutely, yeah. I think those benefits are somewhat marginal compared to the cost of operating purity in their framework. But if you're operating within the ecosystem of hooks, you're definitely better off just having a hook yourself.
00:52:08.028 - 00:52:46.962, Speaker A: All right, so maybe closing question for the two of you. So one of the questions that we've been asking ourselves this whole season is, do you ever see a future where we can move price discovery on chain outside of the long tail of sort of crypto native assets for maybe like, the majors, like a Bitcoin or ETH? I just heard you say something that was like that's just not ever really going to happen. So I'd be curious. Do you ever see a world where we can move price discovery off of the finances of the world and onto the DEXes? Or is that kind of a pipe dream? And if it is possible, what are the things that we need to improve to get there?
00:52:47.096 - 00:53:53.650, Speaker B: There's a big zero to one that has to happen. I don't want to say impossible, because being bearish on technology in the long run makes you look like an absolute fool. But in the near future, I don't see it happening. Because at the end of the day, looking at Solana, for example, and a lot of people that operate in DeFi and solana will tell you that they have these super efficient order books and transactions are so cheap and everything's great, and you can effectively have similar efficiency to centralized exchanges. I don't think that's true, and I think it goes back to how transactions actually end up on chain. As soon as you have a builder let's apply this theoretical framework where transactions don't cost anything. As soon as you have a builder that has a limited capacity of transactions that they have to kind of post on chain and they get to decide what gets included, you immediately arrive to the problem where if you have, let's say, a limit order book which would be necessary for on chain price discovery, they can censor those transactions.
00:53:53.650 - 00:54:33.730, Speaker B: They can choose which transactions get included and which don't. And so if you have a stale quote and a market maker is trying to cancel their limit order. Well, the builder naturally is going to say well no, that transaction looks great to me. I'd love to execute and be your counterparty there. And so if we don't solve that problem, regardless of how cheap the transactions actually are, you are still facing that loss, because you don't have the ability to cancel your orders. You don't have the ability to shift your limit orders as quickly and as efficiently, because there's that rent that has to be paid, because another party will be able to extract value and you'd have to pay them more than the value that they'd be able to extract.
00:54:34.070 - 00:55:38.450, Speaker D: That's an interesting concern to think about but in general I am more bullish. I think ultimately that's the end game, right? That is the goal for us where we work on this is that we do believe that at some point, and I'm not saying in the next year or the next two years, but that ultimately price discovery is taking place on blockchain and that the majority of trading volume is taking place on chain is the goal. Right? This is why we are essentially this is the future that we are all building towards and you see more and more assets being created on chain and you see more central banks looking into building on top of different blockchains. So I do think that again, not like in the next two, three years, but that in the future I do believe that price discovery will take place and I'm sure if you think about those problems long enough we will also come up with solutions for those.
00:55:38.620 - 00:56:21.270, Speaker B: I think those problems are very hard. Those problems are extremely difficult because you'd have to find a collaborative way to arrive to cancellations. You'd have to find a way to limit the ability of the end proposer and reduce their financial incentive to zero to censor anything which is an extremely difficult problem that we're currently very much played with. We first a have to fix scalability and ensure the throughput necessary which is also very difficult. And then you also enter the other problem which is decentralization creates latency between nodes. You have an idea of a mempool or you have private order flow. Both create their own complexities.
00:56:21.270 - 00:56:38.380, Speaker B: I'm not one to say this is never going to happen. I just think that for a significant amount of time this will not occur on chain and we are going to need zero to ones in the fundamental technology that we're building upon to be able to do this because there is no smart mechanism designed today that will make that possible.
00:56:39.230 - 00:57:12.760, Speaker D: I mean, you could probably even think this is going to go in a very deep discussion, but think about ways of if you could apply penalties for if a cancellation has been expressed or if the wish for cancellation has been expressed. And the rule then basically says if the transaction is still being executed despite that expression that the party, the executing party is being punished in whatever way, I don't know. I'm sure if you play around enough with game theory, there's some ways of.
00:57:13.210 - 00:57:30.650, Speaker B: Yeah, if you do that, you'd have to have full data availability. So those transactions have to be posted somewhere, and everyone has to hear them, everyone has to see them to be able to slash. So to slash a censorship, you need full data availability because if you don't, nobody knows that that transaction was seen by the ultimate proposer.
00:57:33.010 - 00:58:02.358, Speaker D: It doesn't necessarily have to be full data availability as long as you have some proofs to verify the transaction, right. You could still have privacy and at the same time incorporate some sort of proof system that would validate whether an order has been canceled or not. I don't know. It's getting complex, and I'm not saying that there's a solution right now, but I think if we think hard enough and long enough about the problem, there might be some ways of circumventing it.
00:58:02.444 - 00:58:05.526, Speaker B: No, we'll get there. We'll definitely get there. But it's going to take a lot.
00:58:05.548 - 00:58:17.658, Speaker A: Of work on that note of optimism. I think we can leave it. Anna and Ludwig, this has been a ton of fun and a really great episode. If folks want to find out more about you, follow you or the projects that you're working on, what's the best.
00:58:17.664 - 00:58:28.206, Speaker D: Way to do that for us on Cowswap? Basically at Cowswap is our Twitter account. And personally, Anna M. As George, also on Twitter, is the best way for.
00:58:28.228 - 00:58:39.410, Speaker B: Us at Thriller Labs on Twitter, we're going to be releasing some really cool mev data very soon and our docs. So I'm excited about that. And personally at Vanbeethoven ETH on Twitter.
00:58:39.910 - 00:58:44.210, Speaker A: Excellent. All right, team, this was a really fun one. Thanks so much for joining.
00:58:44.790 - 00:58:45.620, Speaker D: Thank you.
00:58:47.510 - 00:58:54.806, Speaker A: All right. What a great episode. That was a real thinker, a lot of eye opening moments for me in that one.
00:58:54.828 - 00:59:03.574, Speaker C: That was yeah, yeah, me too. And they got pretty deep into it, I think, also recognize a lot of the caveats, maybe, to some of the concerns about these kind of solutions.
00:59:03.702 - 00:59:40.726, Speaker A: I think so too. Yeah, I think so too. So let me ask you this question that I sort of posed to both Ludwig and Anna, and I'd love to get your response here. So you could look at mev as either originating from the base layer of ethereum and the design there, and a lot of the mev mitigation solutions to date have been sort of focused on the ETH layer. They're moving sort of the priority gas auctions off chain in the form of mev boost or what have you. The other lens that you could look at it is through with mev actually originating at the application layer, through the mechanism design of protocols like Uniswap. How do you ultimately view that?
00:59:40.908 - 00:59:45.606, Speaker C: Yeah, so I think there's certainly some kinds of so as.
00:59:45.628 - 00:59:45.814, Speaker A: For?
00:59:45.852 - 01:00:59.946, Speaker C: Where does mev originate? I think ultimately everyone designing protocols on top of Ethereum is trying to build their applications or their protocols in a mevaware way, or they have to just because of what the base layer provides. But I think on the flip side, when people are thinking of the base layer, they really should be thinking maybe a lot more and I think they have started to think a lot more about what kinds of applications it makes possible and what affordances you provide to people who are building applications on top. So I remember a few years ago, nobody who was working on Ethereum before 2020, basically almost nobody who was working on the Ethereum based layer even thought about MAV as a topic. When the Flash Boys 2.0 paper came out by Phil Dion and others, I think that kind of made it. Oh, this is an interesting academic concept that you have. This and then we sort of saw the entire all of Ethereum get flipped on its head when Flashbots launched Bevgeth, and you realized, oh, man, this is actually much more important at the base layer than we'd realized.
01:00:59.946 - 01:01:29.274, Speaker C: I think ultimately what we're seeing right now is a consequence of mev unaware protocol design. And we were all just all fairly naive, I think, before, and not really thinking about how mev would practically arise. We're seeing the consequences of that at the protocol layer and that's requiring applications to be a lot smarter in how they design Mavaware applications. My hope would be that fixes at the protocol layer could actually change some of at least the decisions that applications now have to make.
01:01:29.392 - 01:02:00.446, Speaker A: Yeah, I tend to agree with that. I do wonder I'm interested now that I feel like mev has really dominated the popular discourse to such a degree that basically everyone's been nerd sniped by it at this point. And I do think this next generation of builders, when they're building their applications they do have mev is not just like a small side consideration. You could clearly hear both from Anna and Ludwig, that's at the forefront of their ludwig is a mev native builder.
01:02:00.478 - 01:02:23.754, Speaker C: Basically, they didn't really get into it, but before, they had done some work on liquidity providing on uniswap. And so they've been research on that. And so he's very aware, I think, of what it's like to be a liquidity provider and to actually be on the other side of this. And I think generally these builders who grew up in a Mevaware environment just have a different perspective on it than for people who started with Ethereum sort of thinking it was just like, oh, bitcoin plus code.
01:02:23.872 - 01:02:40.142, Speaker A: That's super interesting. Not that you can probably comment directly on sort of paradigm strategy here, but do you feel like there is a really big difference between some of the younger, more Mev native sort of builders? And if so, what is the biggest difference between this gen sort of the Ludwigs or the ones that came before.
01:02:40.276 - 01:03:38.638, Speaker C: Well, I think the boomers like me try to be as up to date and stay aware as I say that it makes me sound even older and more out of. Yeah, I mean, generally working with people like Frankie, who yeah, kind of really lives and breathes this think I think Phil and Charlie were ahead of the game on it, but sort of have always believed this in their to. It's about really the attitude is about thinking, okay, what's the worst case that could possibly happen? You could hear that talking to both Anna and Ludwig here is when they talk about any solution, they'll go, and here's what it could lead to. Here's the ultimate possible equilibrium for that's. I think it's important that people actually still actually can do that. And I think that's a way in which the world has really changed is that I think we we start to recognize, okay, what's the end state of this mechanism? What's the actual equilibrium rather than this very fragile non equilibrium. I think often that things start off in when they initially launch.
01:03:38.638 - 01:03:39.346, Speaker C: Yeah.
01:03:39.448 - 01:04:11.622, Speaker A: So how would you define maybe not that the toolkit is the wrong word, but the different set of strategies, or how some of these more mevaware applications look like today, as opposed to their counterparts from even two or three years ago. One commonality seems to be, I feel like, the Cowswap Intense model that I feel like they did a pretty good job of leading, where they're relying on this off chain network of builder solvers. But what are some of the common core design decisions that you see being replicated across this next gen of sort of AMM or Dex designers?
01:04:11.766 - 01:04:12.410, Speaker B: Yes.
01:04:12.560 - 01:04:59.014, Speaker C: So a few things I think intents versus transactions is one where often not every mev solution. I think Flashbots has done an impressive job building mev around the transaction model, but ultimately I think Intense gives you a lot more flexibility potentially to work with mev mitigations. And so I think that's a tool you see used a lot. I think things that move that are slower than a single block are often used for certain kinds of map mitigation. You see that with Cowswap's longer batch auctions. And that's partly beneficial because you end up individual blocks have a single proposer right now in ethereum. And that can be a big constraint for mev mitigations because you have basically this unlimited power by this individual potentially profit motivated entity to censor.
01:04:59.014 - 01:06:06.882, Speaker C: So protocols that take more than one block to resolve is one solution. But on the flip side, another kind is protocols that that take find finality in less than one block. So if you look at what most L two sequencers are doing with faster sequencing, faster block times 2 seconds or less, that can actually mitigate a lot of mev because slower block times means you have less loss versus rebalancing for example, and less opportunities to reorder transactions because there's fewer transactions to reorder. So I think those are some of the things people can play with in these toolkits. And one final one that I think you're starting to see, but the tech is still kind of getting more developed is a little more privacy in the mempool and protocols that allow the encrypted transactions threshold encryption for being. Able to finalize the set of transactions before builders or proposals are able to see them and generally being able trying to play with privacy, trusted execution environments as well. And Flashbots has done some research work on this, things that mean that the builder doesn't necessarily or the proposer doesn't have access to all the information about the block before they build it.
01:06:06.882 - 01:06:14.698, Speaker C: I think that's another tool that I think is just important for avoiding some of those informational disadvantages that Ludwig was talking about on the pod.
01:06:14.894 - 01:07:15.414, Speaker A: Yeah, I think it was interesting to hear. Also, we didn't get super into the weeds about the guard network in Angstrom's design, but I thought that was a very cool thing, which is basically sort of this sidecar that execution clients can run to basically run their own auction, where it separates top of block mev opportunities from rest of blocks. And that all just happens upstream of where the block auction actually happens. Do you think that's credits to the team at Sorella Labs for, I think, coming up with a really innovative design? Do you expect to see sort of more of that where you can almost imagine this stream of mev flowing right in one direction? Maybe some of it originates up here or whatever, but most of it got extracted at the block space layer. I feel like you're starting to see there's just sort of more vanilla order flow options and then some of these really innovative designs like Angstrom has, whereby you're redirecting that flow back to LPs or Swappers or whoever it is. Do you expect to see more of that?
01:07:15.532 - 01:08:06.838, Speaker C: Yeah, I love their design and this general category, it's sort of an interesting middle ground between what would you consider a whole full L2 with a sequencer which has just like reaching the sort of pre consensus on an entire complex blockchain state versus something very relatively simpler, like an order flow auction. This is an order by order auction, and this is something that cowswap, I think, as well. It's something somewhere in between. It's this protocol that has rules. It operates on more than one transaction, and it has more complex validation logic, but it's not necessarily a full L2. It's really just for coming to consensus on these particular ordering transactions. And one nice thing there is that, unlike an L2, you may not need to prove this really complex state in order to interoperate with L One state or to exit from this quickly.
01:08:06.838 - 01:08:32.458, Speaker C: And because it's so much simpler, you could basically do all their execution on chain. But you could imagine one that uses Ek potentially to actually have that be more efficient, but it's still a lot easier to prove than a full L two. So I think playing in that design space where you have these kind of like side chain, somewhat limited, trusted, separate consensus processes for things like ordering or censorship resistance, I think is really powerful.
01:08:32.634 - 01:09:39.560, Speaker A: Yeah, I tend to agree with you. I would be curious because I'm sure you just thought more about the Dex model than I have. But one thing that I wish we could have maybe poked at a little bit more with Anna even, is just sort of the profile of the average swapper of something like cowswap as opposed to uniswap or something like that. My high level understanding of this kind of comes from payment for order flow, where the higher percentage of the transactions that are passing through your platform or organic kind of more derogatory, kind of dumb transactions, the more valuable that ultimately ends up being. I'd be curious, how do you think about the trade offs, maybe even from a uniswap perspective of taking on some of that more toxic order flow, but having higher overall volumes versus someone like Cowswap who has very little toxic order flow and then actually some of the liquid. Like, I was going to ask her about this deal with balancer, where actually balancer gives them better execution prices because they know they're not going to get run over by the toxic order flow. How do you kind of think about those two different business models, so to speak?
01:09:40.090 - 01:10:29.670, Speaker C: Yeah, so I think ultimately the value index is true, that it comes from non toxic order flow. You would expect it to I think it's maybe a little bit subtler than but so in general, I think you want as much of that as possible. One thing I didn't push back, maybe I could have on what Anna was saying was for pairs like ETH USDC, are we going to the off chain fillers have all this inventory, so what we might as well fill like really large trades on cowswap. They're filling an on chain inventory. Generally, I think you still need that on chain inventory, but there's just so much more of it. And I think it's possible that just being exposed to toxic order flow, if you've got short enough block times, if you've got the right fee level, actually isn't as much of a disadvantage.
01:10:31.530 - 01:10:31.846, Speaker D: If.
01:10:31.868 - 01:11:27.314, Speaker C: You'Re still, again going to ultimately get this large flow that ultimately off chain failures just aren't willing to take. So I'm not sure if that's as directly answering your question, but I think it's really important to have this base level passive liquidity on chain. I still really very much believe this, that this massive pool of ETH USDC, I'm not sure it has to be as deep a liquidity pool as it is right now, but having it on chain is just really valuable and makes DEXes a lot more usable than they would be if you had to depend on off chain liquidity for large order flow. So I think ultimately my point is things like cowswap, yes, they can try to filter out some of the toxic flow, but ultimately they're still going to the same place. There's still ultimately a lot of order flow there is getting filled on this from on chain liquidity. And so we're never going to actually get rid of passive liquidity.
01:11:27.362 - 01:12:10.918, Speaker A: I think. Yeah, I tend to agree with you on that. Maybe like last closing question for you, because I realized actually, first of all, if you want an advertisement for hooks in the future should get ludwig back on, because even talking I caught up with him for a little. Bit before this episode, and hearing from him describe the benefits of launching a hook as opposed to his own standalone decks definitely made me think pretty heavily. But can you give us some sort of insight into, I don't know, almost like the ranges of hooks that are being explored today or what the general interest has been? Because I'm realizing I'm kind of like, wow, uniswap, like V Four is coming. It was interesting to this is the first time I've really spent time talking to someone who's building on it. It got me honestly feeling pretty excited about it.
01:12:10.918 - 01:12:17.960, Speaker A: And I was like, I should probably be paying more attention to this. So, I mean, can you give us any sort of sense of what that ecosystem is looking like today?
01:12:18.410 - 01:12:53.486, Speaker C: I have to admit, I was hoping and we were trying to design Unisoft before to be something that a lot of projects could build on. But in the past, I think some great projects have built on top of Uniswap. But in general, in the past, whenever we've put out something, uniswap has put out something and said like, oh, let's hope other people build something on top of this. It takes a long time and you get very limited amount, I think, of stuff actually being built on top of it. The Oracle is one example where it took a while. V Two launched an oracle. A year later, the v three oracle came out.
01:12:53.486 - 01:13:25.280, Speaker C: I think during that time, very few projects even made use of the V Two Oracle because it was kind of hard to integrate and you just would have had to go to some trouble to do it. And same with V Three, I think it turns out it takes a lot of effort to get people to build on top of something. And so I thought, okay, maybe some people will build this, but we'll see like a couple of projects and it'll be at least it's going to be an option for people to do. But maybe we'll have to go out and push and find people to build on top of it. But actually just the response has been tremendous. I think there's so many people actually want to build on it. There's been tons of inbound interest.
01:13:25.280 - 01:14:10.410, Speaker C: I think a lot of people who would have built DEXes but felt intimidated, as Nana was saying about by uniswap's perceived first mover advantage, but have had a cool idea that first thing they wanted to build is a Dex. A lot of people I think a big part of it is just the message of feeling, okay, now I've been invited to basically build this built to try out my Dex design on this. And rather than feeling like I have to go compete with this thing, I'm actually going to be doing it on top of this platform. I think it's just had a profoundly different effect, I think, in terms of getting new people to build top of it from anything else that unisoft's done before. And I think it's really cool. It made me really excited about building it as a platform. And I think we're going to see some really exciting things.
01:14:10.410 - 01:14:59.622, Speaker C: I think lever reduction is probably the main interesting, the most interesting thing we see built on top of it. But people have come up with new ideas for Oracles built on top of folks. People have come up with new designs for sandwich prevention and these other kinds of mev prevention that I think make use of really clever algorithms in there. People are doing it for using it for other things. I think some of the more out there ideas you can build, like a lending protocol or something on top of it, right. You can try to even beyond just what you would think of as a traditional, you know, I think we'll see which ones really work. But I've just been trying to be as encouraging as possible for people who are building on top of it because yeah, I think the same thing Ludwig was saying, that it's great to sort of be able to build this and take advantage of some of those net liquidity network effects and brand advantage.
01:14:59.766 - 01:15:24.640, Speaker A: Yeah. All right, last question for you here. And this was actually something that you asked Ann Ludwig when we were talking about the builder searcher market and solver searcher entities being a relatively small number of pretty sophisticated actors. You're like, do you think that this is something that we should fight against or try to prevent? So that's my question to you. Do you think that this is something that we should try to prevent and strive against, or is there just some natural economic forces and we can just kind of let this happen?
01:15:24.970 - 01:16:04.906, Speaker C: I think it's important to keep it competitive. I think it's important to reduce barriers to entry. I think it's important to reduce gains from scale and sort of the extent to which there's natural monopolies in the space. I don't necessarily think it's important to if you've designed a competitive game, some people are going to be better at competing at that game. Some entities are going to be better at competing at the game than others, and they will tend to win. I think if it's still a competitive market, I think that's fine. And ultimately, that kind of thing you'd expect, if they were to disappear, someone else comes in and takes their place, if the only way that they're outcompeting everyone is just by being better at it, by being more competitive.
01:16:04.906 - 01:16:38.210, Speaker C: So I come down somewhere in the middle of it. I do think there are ways in which that can be very dangerous, and I think it's good that there are forces pushing back against that. And in Crypto, I think we always want to have forces pushing it back against anything that seems to risk a natural monopoly that could threaten the censorship or decentralization or anything censorship, resistance, or decentralization of the space. But ultimately, to me, it's most important to just design the game to be as competitive as possible, rather than trying to hobble, for example, the ability of very competitive parties to be able to compete.
01:16:38.370 - 01:16:49.500, Speaker A: Yeah, I tend to agree with you. All right, Dan, this is a really fun episode. Next week, we're going to be talking about crosschain with Hart Lambert, which is going to be a really fun one. So I will see you next week.
