00:00:00.250 - 00:00:17.738, Speaker A: There's still going to be many layers of a stack, but I just think ZK is going to be like the base glue for everything. And basically, yes, like having truths communicated everywhere. And hopefully that leads to better security like normal hacks, and leads to like, oh, there's not roll ups that are just multi sigs with like no fraud proofs and stuff. It's just like everything, hopefully.
00:00:17.834 - 00:00:19.760, Speaker B: Which roll up? Which roll up are you talking about?
00:00:23.170 - 00:00:24.670, Speaker A: We don't name names.
00:00:25.130 - 00:00:48.838, Speaker B: Hey everyone. This episode is brought to you by, say, the blazing fast parallelized blockchain which is unlocking salana like performance for the vast ocean of ETH devs out there. Now you're going to be hearing all about, say, and their new v two upgrade. But if you take away one thing, the EVM is here to stay. There are some problems with it which we're going to get into later in the episode. But say, and especially their v two upgrade is helping solve that. So thank you very much, say, for making this episode possible.
00:00:48.838 - 00:01:19.782, Speaker B: Hey everyone, this episode is brought to you by Uniswap, delivering the best on chain trading experience in crypto, period, bar none. One thing I want to call out is the uniswap extension, so say goodbye. These days of these annoying sort of pop up wallet extensions, you lose your place trading, you have to open it back up. Uniswap now has a nice, sleek sidebar that persists no matter where you are on the web. It's much easier to use if you click the link at the bottom of this episode. You can join the waiting list and I'll see what I can do to get you moved up that waiting list. But definitely go click the link, check it out.
00:01:19.782 - 00:01:58.074, Speaker B: All right, everyone, welcome back to another episode of Bell Curve. Before we jump in, quick disclaimer, the views expressed by my co host today are their personal views, and they do not represent the views of any organization with which the co hosts are associated with. Nothing in the episode is construed or relied upon as financial, technical, tax, legal, or other advice. You know the deal. Now let's jump into the episode. Hey everyone, welcome back to another episode of Bell Curve. Today Hart and I are going to be interviewing Uma Roy of Succinct, and we're going to be doing the long awaited, much talked about already on this season zero knowledge proofs episode.
00:01:58.074 - 00:02:29.878, Speaker B: Now, this is a really interesting episode because as many of our guests have already brought up this season, ZK has massive implications for crosschain, Interop. And so we get into all the details about how that's going to work with Uma and unpack the implications. And for those of you who pretend like you know what ZK actually is, but you don't, aka 95% of you, we spend a good amount of time in the beginning of the episode just reviewing what ZK is and talking about the ecosystem, which is super, super interesting. So really excited to get into this one with Uma and Hart. Thanks for listening, guys. All right, everyone, welcome back to another episode of Bell Curve. Today, Hart and I are joined by Uma.
00:02:29.878 - 00:02:32.074, Speaker B: I'm succinct. Uma, welcome to the show.
00:02:32.272 - 00:02:34.186, Speaker A: Hello. Thanks for having me.
00:02:34.368 - 00:03:03.622, Speaker B: Yeah, we're really excited for this one. Hart and I have said, I think at the end of every single episode so far this season, we need to do a ZK episode. And there is no one better to do that with than you. So I'm going to start with Uma. I'm going to give you the caveat here that we don't treat me basically as you would a five year old who knows very little about zero knowledge. That'll probably be about the right level. Actually, as a funny story, flying back from, I told Hart this.
00:03:03.622 - 00:03:29.774, Speaker B: So I was flying back from ETh Denver, and I was like, I'm going to do some research for this episode with Uma. And I started off by reading ZK sync, actually put out a five year old picture book description of what ZK tech is. And some guy was reading me, saw me reading that and was like, hey, are you guys you trying to learn about ZK tech? And he had led the investment into risk Zero. And I talked with him about ZK for the rest of the podcast, but still pretty embarrassing out myself.
00:03:29.972 - 00:03:40.770, Speaker C: A very smart ZK guy, you know, Uma, I won't dox him, but it was just very funny. He's sitting beside Mike watching his picture book and being like, hey, let me actually tell you about ZK stuff. That was his flight.
00:03:41.270 - 00:03:42.750, Speaker A: That's so funny.
00:03:42.910 - 00:03:50.626, Speaker B: Yeah. So with that, uma, can you just give us, at a high level, just explain to us, what are ZK proofs.
00:03:50.658 - 00:03:52.120, Speaker C: If you had to describe them?
00:03:53.130 - 00:04:51.450, Speaker A: Yeah, so I guess at the highest level, a zero knowledge proof does two things. The first one that I'll start with is like, I have some computation or some function, and I want to prove to you that when I run that function on a certain input, it results in a certain output. So as an example, it could be like when I do this hash 500,000 times on this input, it results in this output. It could also be like a state transition function, where when I take a state route and a bunch of transactions, when I run those transactions, it results in a new state route. So I want to prove to you some computation, and you want to verify it without having to redo the computation, because the computation can be expensive. A zero knowledge proof lets you basically prove to someone else this function, when run on this input, results in this output. And then the key part is it's succinct, which means that when you verify the proof, it takes much less time than it would to actually do the original computation.
00:04:52.350 - 00:04:52.954, Speaker B: Got it?
00:04:52.992 - 00:05:13.760, Speaker C: So on that Uma, is it like accurate? One thing that somebody told me that I think makes sense, but I want you to verify this is to think of it like a compression technique in a sense, where I'm compressing this giant computation into something I can run in linear time, and that's the succinctist property. So first of all, do you agree with that?
00:05:14.470 - 00:05:18.658, Speaker A: Yeah, except it's like constant time, but yes. So even better.
00:05:18.824 - 00:05:55.598, Speaker C: Okay. Yes, so I have a constant time proof of this. Incredible. Like it could be an incredibly complex computation, it doesn't really matter how big the computation is, and I can compress it into something that is easy to verify. And that's the succinct property, which is the name of your company, succinct labs. But also, why don't you talk about the zero knowledge bit too? Because I think when we're talking about ZK for interop, I don't think we care about the zero knowledge part. I think we care about the succinct property, but we might as well mention it because we're on the pod, right?
00:05:55.764 - 00:06:35.180, Speaker A: Yeah, that's exactly right. So the other thing I was going to mention is you have the succinctness property, and then the other thing you can do is you can prove that I have some function, when applied to some inputs, results in an output, but the inputs can be hidden to the final person. So as an example of that, you can say like I know the password or the key to something. I'm not going to tell you what the key is, so I'm going to hide that as part of the inputs, but then it results in some output that the verifier can understand. And so that is like the zero knowledge property is the verifier doesn't need to know what all the inputs are.
00:06:36.430 - 00:06:49.200, Speaker B: And Uma, could you just briefly explain the concept of circuits in there as well, and what some of the computation is? Honestly, not even super relevant for what we're going to talk about here, but just for the audience, I'm curious how it works.
00:06:50.210 - 00:07:34.242, Speaker A: Yeah, so people talk about this concept of circuits in zero knowledge proofs. Basically, if I want to take. Well, okay, historically, if I wanted to write a computation in a way that I can generate a zero knowledge proof of it, I'd have to write the computation as a circuit. A circuit is just like a fixed computational graph of your logic, and it's like a concept that exists even outside of ZK. In ee, there's like, circuits and things like that. But basically, to generate a zero knowledge proof, you need to format your computation in this very structured way. And that's kind of why you have all these DSLs and SDKs and all this specialized stuff for generating zero knowledge proofs.
00:07:34.242 - 00:07:42.960, Speaker A: And so a circuit is just like a fixed computational graph of your computation. That is an easy form to generate a zero knowledge proof of, if that makes sense.
00:07:44.610 - 00:08:21.402, Speaker C: Yeah, I think this is where the circuit stuff is when we start getting into the moon math. But I think at a high level for this pod, Mike, there's this way that historically, uma, tell me this is correct. There's this way. Historically, you need to describe the logic of your function in order to be able to generate the ZK proof. And actually, maybe that can then lead into kind of the frontier or state of the art of a lot of the ZK stuff is abstracting away that circuit generation or making it so that the average developer doesn't need to build circuits anymore. Right. I think you chanted death to circuits.
00:08:21.402 - 00:08:23.294, Speaker C: I think you did that on a panel with me.
00:08:23.412 - 00:08:59.186, Speaker A: I did, yeah. So, historically, basically, if I have some function, say I could describe the function, I could be like, mike, I want to prove that when I take some number and I hash it 50,000 times, it results in this answer. For me to actually generate a zero knowledge proof of that function, I can't just write it in normal code. I can't just use a normal out of the box hashing library. What I'd have to do is I'd have to write it as this circuit in some sort of zero knowledge proof library in order to be able to generate the zero notch proof. And that'd be really annoying. If you look at circuit libraries today, it's really annoying.
00:08:59.186 - 00:09:39.720, Speaker A: You have to basically rewrite your computation in this fixed graph. It's very bad, basically, as a developer experience, and it takes a lot of time, and it's very intensive. And then with the frontier of today, and especially one of the things we're building at succinct is this open source ZkVM called SP one. But what that lets you do is instead of having to write all this hand rolled circuit stuff, you can just write your normal computation as you would with normal rust code, and then you can generate a proof just based off the normal rust code. So that's kind of like the frontier of where zero knowledge proofs is heading is like, instead of having people do this really specialized moon math stuff, you just write normal code and any programmer can use it.
00:09:40.730 - 00:10:45.194, Speaker B: So where I want to start to transition here, Uman, I think sort of a powerful effect of zero knowledge proving, as opposed to the way that computing has worked historically in crypto, is where the computation is taking place. So historically, the way a transaction gets submitted to kind of this public blockchain gets gosped around in this mempool, a bunch of nodes within the ethereum network sort of take those up, execute them, package them into a block, but everything happens within this one virtual machine. And one of the side effects of what we're describing here and what you just mentioned with what SP one allows you to do is it allows you have many different machines or sort of computing environments. A lot of computation gets done off chain, and then the proof is submitted on chain. And something that that allows you to do is to knit together these very different execution environments, which has historically not been super easy to do. And that's where I think we want to drill into sort of the interoperability piece. But before we even get there, I would actually just be curious to understand there's a lot of new infrastructure when it comes to this zero knowledge space.
00:10:45.194 - 00:10:58.720, Speaker B: There are approving networks, there's new types of applications that leverage zkps. So can you just kind of give us a sense of what does the landscape sort of look like? What are the different buckets that exist in this new ZK world?
00:11:00.290 - 00:11:31.686, Speaker A: Yeah, so there's kind of like the app layer that uses zero knowledge proofs. And I actually think there's many different types of applications and infrastructure that can use zkps. So I think, for example, for bridging all bridges can use zero knowledge proofs. And maybe we'll dive more into that later, considering this is focused on interop. I think all roll ups, I think people are probably really familiar with ZK roll ups. I think all roll ups can use zero knowledge proofs as like a validity mechanism. Then there's other zero knowledge apps that are more focused on the privacy aspects.
00:11:31.686 - 00:12:23.254, Speaker A: I think tornado catch is a good example. But then there's other ones, for example like identity applications, where you kind of prove like hey, I belong to this set of people, but I'm not going to tell you exactly who I am. And that's just like a blockchain application, so not in the infrastructure category. So there's all these consumers of zero knowledge proofs who use them for validity, security or privacy or whatever. Then there's the actual proof systems that basically people use to express their function that they care about proving. So there's various different proof systems, and then there's also like zkvms where instead of having to write all your stuff custom, in a proof system, you can just write normal code and then have it proven in a particular ZKVM. Then there's the proofing networks or infrastructure to actually generate the proofs.
00:12:23.254 - 00:13:29.646, Speaker A: So it's kind of like you're an application, you write your computation that you want proven in a zero knowledge proof in a proof system or in a ZKVM, and then you actually have people generating the proofs. And so that's kind of where proving networks come into play, is they connect the people who are consuming the proofs with the people generating the proofs and the people who are generating the proofs. Right now it can look like companies like us, we just run some cloud proofers on AWS, but it can also in the future look like hardware teams who are creating specialized hardware for Zion odge proofs that can really accelerate the proof generation time. And then they can basically be the suppliers of proofs. And then the marketplace layer or network layer is basically, I see the shelling point where these proof consumers and applications request proofs. And then you have the proof fulfillers who are generating the proofs, who are like the hardware teams or people like us with our cloud infrastructure actually generating the proofs and fulfilling them. So that's kind of like top to bottom the layers of stack, if that makes sense.
00:13:29.748 - 00:13:39.118, Speaker C: So to summarize this, so your applications, two use cases, privacy use cases, and I'm going to call it like compression or succinctness use cases.
00:13:39.134 - 00:13:42.046, Speaker A: Like scaling use cases, scaling use cases.
00:13:42.238 - 00:14:31.502, Speaker C: And again, going back to even how you're explaining ZK, for me personally, trying to understand how the zero knowledge works for the privacy part, that's what makes my head hurt. It's still hard for me to understand how you can prove something without telling budy about it, right? But anyways, there's the privacy use cases, which for our pod and for the multichain kind of endgame we're not going to talk too much about. And then there's the scaling or proving a complex computation succinct property that we are going to talk about. Okay, so those are the applications and then your proving system. When you talk about a VM, is it right to like, if I make an analogy for our audience, it's like are you using Linux or Windows or like the operating system you're picking, right. And here you're specifically talking about the math, or the math framework that's being used for the Ck proof.
00:14:31.566 - 00:14:33.378, Speaker A: Yeah, yes, exactly.
00:14:33.544 - 00:14:55.180, Speaker C: Okay. And then once you've picked that, you've got to actually do the work to generate the proof and that you're calling that proof generation. And then there's this idea of prover networks where effectively we're going to have different provers in essence competing, kind of like a marketplace to see who will generate that proof. And then you have your proof and then you can go and use it.
00:14:55.950 - 00:14:58.060, Speaker A: Yeah, that sounds about right.
00:14:58.510 - 00:14:59.260, Speaker C: Cool.
00:14:59.870 - 00:15:28.280, Speaker B: So I have a question about the prover networks in general. And what I ultimately want to understand is the cost of generating of proving today and where that. I understand that that's trending down. But as specific as we could sort of get about that, who are the types of entities that are running these proving networks? Obviously succinct is running one. Are there going to be eventually like almost chorus one or figment like entities that sort of spin these things up? Is it actually going to be like chains themselves? Like who ultimately runs these sorts of things?
00:15:28.810 - 00:16:13.086, Speaker A: Well, I think it's not like we are running the proving network. I think it's like we are designing a proving network and protocol and then there will be entities that participate in it. So for example, it's kind of like, you wouldn't say like Celestia runs, you know, like Celestia Network's a thing. And then there's node operators that participate in that network and then there's users that participate in the network and post blobs. So it's very similar. There will be like succinct network that's like a decentralized protocol. And then on one side you have node operators who are running provers and yeah, I think like figment or course one or traditional node operators, maybe they could get into ZK proving and start running provers on the network and they will get paid for generating proofs.
00:16:13.086 - 00:16:32.950, Speaker A: And then on the other side you have applications and users submitting proof requests to that network and being like, hey, I want this proof. So those are kind of like the two participants in the marketplace. It's like the applications who want proofs and then the people who are actually generating the proofs got it. And they could include traditional node operators for sure.
00:16:33.020 - 00:17:10.926, Speaker B: Hey, everyone wanted to give a big shout out to today's title sponsor, Sei. Now I want to talk to you guys a little bit about why I think Sei is cool from a design standpoint, a big problem that it solves for e devs out there and then some cool stuff that say, has coming up. The reason I like Sei from an architecture perspective is, again, it's a very fast blockchain, parallelization, all of that stuff. But Sei has essentially been custom building block space, which is for consumer apps and dexes. Now they have some very cool features which enable that. So twin turbo consensus, optimistic parallelization, SADB, all of this stuff allows you to reduce the time to finality, make for very, very fast transactions. So if you're building a consumer app or a Dex, this is basically the blockchain for you.
00:17:10.926 - 00:17:36.080, Speaker B: If you've been building in the eVM, you love the EVM, but there are some restrictions about it that don't support your app. So maybe you can't do fast enough transactions or it's not parallelized. Whatever it is, you can now take all that stuff that you built. You don't have to start from scratch, and you can build it on say, now recently they've launched v two, but also public devnet. So the way that you can follow that and keep up to date is going follow, say, network on Twitter. All right, thanks, guys. What are the economics around proving? What does that look like today?
00:17:37.490 - 00:18:20.460, Speaker A: Yeah, so I think there's some ballpark numbers. I think right now, for example, we can just take a few different applications. So one popular one is bridging. It's the same do we've written a ZK tendermint like client where basically we verify that in a tendermint consensus protocol, enough validators have signed off. And that lets us do like trust minimized bridging between networks who use tendermint, like Celestia or DuiDX or all these other people to Ethereum or something like that. And so the proof generation costs for one of those proofs is actually really cheap. I think it's like a couple cents, if that.
00:18:20.460 - 00:18:28.190, Speaker A: And so it's very inexpensive today just in terms of raw compute costs to generate a proof.
00:18:29.170 - 00:18:54.614, Speaker C: So wait, uma, that's cheap enough that I want to push and understand that so this is like, to be clear, this is generating a ZK proof of a tenderbent consensus. And you're saying that the computational costs of that are like couple cents, which is awesome for context to kind of share with the audience how quickly this is developing. Like a year ago, what do you think the computation costs would have been on this?
00:18:54.732 - 00:19:20.960, Speaker A: Or. Yeah, like a year ago, to be honest, is kind of impossible. The circuits and all that kind of stuff didn't exist. And then, yeah, we were like one of the first teams to kind of put all of that together and get it working end to end. I think maybe a year or a year and a half ago, people thought it's impossible. They would thought like, oh, the proof generation would take like hours or something like that. I think our proof generation time takes like five minutes.
00:19:20.960 - 00:19:30.738, Speaker A: It is on a pretty big machine, but with cloud costs, the machine probably costs, know, a couple bucks an hour. And so the costs are very like, you know, a couple.
00:19:30.904 - 00:19:50.742, Speaker C: So. And actually we should go into this too, as Mike's pushing on the economics, which I really want to hear you describe more as well. It's proving time is directly related to computation costs, basically like how long you've got to run this beefy machine for and how much that costs. So your two cent proving time here is like five minutes on a beefy machine in AWS right now.
00:19:50.876 - 00:20:26.646, Speaker A: Yeah, that's how we compute costs. It takes five minutes to generate your proof. The machine's pricing is like $2 an hour. You just do that math. Other examples people might care about is for a blockchain, for example, proving a state transition function of, for example, a ZkevM. Recently with SP one, we released some numbers that showed that the proving cost per transaction. And again, this is like the same methodology where you take the proving time on a beefy cpu and you just say the cpu is a couple of bucks an hour.
00:20:26.646 - 00:21:01.230, Speaker A: And so you do that math amortizes to around one to two cent per transaction. So the proving cost to prove the validity of a transaction and its state transition from the previous state route to the next state route comes out to maybe for an average block around like one to $0.02, which is already super good because for normal transactions, the Ethereum data availability costs are like at least ten to. So yeah, the proving overhead today already is pretty cheap for transactions. And yeah, only going down.
00:21:01.380 - 00:21:26.230, Speaker C: Can we though, just to be complete here, once you generate the proof, you also have to validate or verify the proof, at least on a block. If we're using it in a blockchain context, we have to verify the proof. This very much depends on what chain you're on, but what does it roughly cost in dollars or maybe gas units to verify these proofs on different chains?
00:21:27.050 - 00:21:42.558, Speaker A: Yeah, the gas costs are pretty negligible on all chains except for Ethereum. But on Ethereum mainet, it's like 250k gas, which. Yeah, I mean, it depends on the Eth price and the gas price and things like that.
00:21:42.724 - 00:21:48.400, Speaker C: But now it's like more than 100, $200, I think something like that.
00:21:49.890 - 00:22:05.800, Speaker A: I don't know. I think gas might have been a lot recently. That sounds kind of high, but I think back when I was looking at it was maybe like $20. Yeah, maybe recently it's gone up because eth went up, obviously. And then the gas price has also gone up.
00:22:07.770 - 00:22:16.338, Speaker C: In terms of on a layer two, is there like a lot of call data or other costs associated with validating one of these proofs?
00:22:16.514 - 00:22:41.914, Speaker A: The call data is actually pretty negligible on a layer two. That probably costs. Back when we were verifying proofs, it was maybe like $20 to my best guess. Don't quote me on those numbers, but I think it was like in that range where you just pay for call data. Obviously, computation on a layer two is basically free. So, yeah, it wasn't too bad. But I think there's this kind of myth.
00:22:41.914 - 00:23:10.722, Speaker A: I think for ZK that, oh, it's so expensive, it's going to cost so much money. And I think historically that has been true. But just like every year, things are getting orders of magnitude better and the cost curves are coming down so much. And so I think even today, with the numbers we have, it's one cent or two cents a transaction. That's already awesome. That's already something that's very feasible right now for tendermint. Also, it's like a couple of pennies per tenderament proof.
00:23:10.722 - 00:23:20.700, Speaker A: So I think there's this kind of big myth about Zika that it's too expensive, and I think that's just not even true right now. And I think there needs to be education done on that.
00:23:21.310 - 00:23:28.590, Speaker B: And Uma, isn't aggregation a part of this story as well? Can't you aggregate some of these proofs together and then that ends up making it cheaper?
00:23:29.170 - 00:23:47.666, Speaker A: Oh, yeah. 100%. Yeah. So as Hart mentioned, if you want to verify your proof on Ethereum today, it's pretty expensive. But what's awesome about ZK is you can verify a ZK proof in a ZK proof. So we call that recursion. And basically all ZK systems use that.
00:23:47.666 - 00:24:27.922, Speaker A: Like roll ups use that a lot. Even we use that for our bridges and stuff like that. And with recursion, you can just compress an arbitrary amount of computation into one proof. And so for what a lot of layer two roll ups do is there'll maybe one proof per block, then they aggregate all those proofs across a chain of blocks. So maybe like every 3 hours, they'll aggregate all the proofs they've generated across those 3 hours into one proof, and then they'll verify that proof on chain. So you only pay that amortized like 250K gas every 3 hours or something. And in general, you can actually aggregate arbitrarily many computations into one.
00:24:27.922 - 00:24:38.260, Speaker A: And so I think in the end, the amortized cost for aggregating a proof on Ethereum should basically go to zero because everyone will be aggregating all their proofs together.
00:24:39.270 - 00:24:44.758, Speaker C: Yes, that makes sense. And I just double checked. So 250K gas is like $70 right now.
00:24:44.844 - 00:24:48.200, Speaker A: Oh, right now. Oh, wow. What is the gwe right now?
00:24:48.810 - 00:25:01.222, Speaker C: It's like 80 gwe Eth price has gone up. But $70 amortized over many transactions is completely reasonable, right?
00:25:01.296 - 00:25:02.154, Speaker A: Yeah, totally.
00:25:02.282 - 00:25:56.890, Speaker C: And this is the way across works too, right? We batch together all of our intent, verification, and amortize it over hundreds or thousands of transactions and makes it cheap. So I'm a big believer in that concept. I did kind of think it would be worthwhile, though, going into some of the limitations of ZK here from an interop perspective too, because I think, I have come to believe that this is incredibly powerful and transformational technology. But I also think at times, people think they don't always push on the limitations of what it allows us. So I was thinking through some examples that I was hoping I could walk you through, and you could tell me where I'm wrong or help me make sense of things. So, proof of work blockchain. Proof of work blockchain is basically one giant computation.
00:25:56.890 - 00:26:30.700, Speaker C: And so at any point in time, I could do a ZK proof of that whole computation up to the current state route of that blockchain. Right. But the problem would be I could also then mine one additional block that I mine, that I control and do bad things in. And as long as I can generate one additional block, I could make a ZK proof on that one additional block. Right. And then my ZK proof would be valid, but it would be for a bad state. Yeah.
00:26:31.150 - 00:27:21.910, Speaker A: Yeah. So, for example, if you wanted to make a bitcoin to Ethereum like client using ZK, you have to be kind of careful because as hart is saying, I can just mine my own fork of the chain. That's malicious. And then what happens is when I verify that on Ethereum, sure, it is the longest chain by amount of work, but it's not really the valid chain because no one else is updating that light client. So if you have just a longest chain based light client on Ethereum for something like bitcoin, there's like a strong liveness assumption that that light client is getting pinged, often by many different people with the latest blocks and the latest amount of work, so that you can't just have this malicious fork.
00:27:22.070 - 00:27:43.790, Speaker C: Or you could do something where you prove the ZK route for the current state route, but then look back ten blocks or 15 blocks and be like, okay, here's where I actually want to say, here's the data I'm trying to say is true because the cost to generate ten or 15 blocks right now would be so astronomically high, you could put like an economic guarantee around that or do things like that, right?
00:27:43.860 - 00:28:32.318, Speaker A: Yeah. So I think in general, for most bitcoin like coins, they do that where it's like you do longest chain and then you walk back a bunch. So you could definitely do that. Another great thing you could do is you could prove, basically you prove the state transition validity with the ZK proof. So not only do you just prove like, hey, here's like a header with the most work attached to it, you also prove that, oh, all the headers are connected with a valid state transition of the utxos with valid signatures and stuff like that. And then that makes your guarantees significantly better. Obviously, it still might not be the canonical chain, but at least then you can't just be like, oh, my balance on bitcoin is like 1 million, because there would never be a valid transaction that would make your balance like an arbitrary number.
00:28:32.318 - 00:28:37.570, Speaker A: So you can still do the double spend attack, but you can't just set your balance to something arbitrary.
00:28:37.910 - 00:29:27.970, Speaker C: Okay, yeah, so basically what you're saying is if we're proving computation, which we're doing with proof of work blockchains, because you can theoretically compute the blockchain, you got to do some work to put guarantees around how difficult it would be to do that computation ish, right? Okay, now, proof of stake blockchains like Ethereum, as I understand it, right, you can do this ZK proof on the current state of Ethereum or on the state transition of Ethereum. But Ethereum finality comes from this sync committee concept. Actually, I don't want to keep talking. Why don't you talk about what are the limitations around how I can transport the current state of Ethereum in a ZK proof to other chains?
00:29:28.630 - 00:30:34.390, Speaker A: Yeah, so Ethereum's consensus mechanism finalizes after two epochs, which are like six minutes each. So that's why Ethereum has this twelve minute finality delay. So even with zero knowledge proofs, you're never going to increase Ethereum's finality. So there's always going to be room for things like across, where if I want my money on the other side from Ethereum faster than twelve minutes, I'm going to need someone like across to basically eat the finality risk and then provide me money on the other side. And that actually seems like a great use case for something like across that Zika doesn't solve because there's no computation you could prove that Ethereum has finalized before twelve minutes, because it just hasn't. So that's like one limitation of zero knowledge proofs, I think. The other thing is Ethereum right now has Ethereum's consensus protocol has basically like around a million or something BLS signatures or, sorry, BLS public keys that kind of get aggregated into one BLS signature to prove across the finality of the chain.
00:30:34.390 - 00:32:01.650, Speaker A: This is because they have this 32 eth staking thing where you can only deposit 32 e, and each 32 ETH is associated with a public key. In practice, there's actually only maybe 10,000 or 13,000 logically different validator entities, and they're working on this max validator balance that will increase the 32 e a lot, so it reduces load on the network. But right now, if you want to prove that two thirds of the entire Ethereum validator set have signed off on a particular block, which is needed for finality, it's like a really large computation because you're having to basically prove that these two thirds of 1 million public keys have signed off on something. So that's very hard to do in a zero knowledge proof because zero knowledge proofs already have a lot of overhead. And then on top of that, you're proving this really expensive computation. So Ethereum came up with this protocol called the sync committee, where basically they choose 512 random validators that's like a subset of their million validator public key list, and then those 512 validators sign off on the headers and basically you can verify the sync committee's signature instead of having to verify from the entire validator set. Now the sync committee, statistically speaking, because it's randomly chosen, is very likely to have the same portion of honest validators as Ethereum itself.
00:32:01.650 - 00:32:37.760, Speaker A: So the probability the sync committee is honest is very high because we're already making the assumption that two thirds of the Ethereum validators are honest. But obviously it doesn't have the exact same guarantees as Ethereum because it's like this probabilistic sampling thing, if that makes sense. So right now, in a zero knowledge proof for us, for our ethereum proofs of consensus, we always verify the sync committee, not like full ethereum consensus, because the overhead of that computation is just very high. So that's kind of another limitation, is like the sync committee's properties are just not the same as full Ethereum consensus. And so yeah, some people think that's kind of bad.
00:32:38.930 - 00:33:15.434, Speaker C: Okay, but basically you're saying that the efficient way to do a ZK proof on the state of Ethereum today is to make a proof that these 512 sync community validators signed off and finalized a block. And that's after the twelve minute epoch has passed. And then I can make a ZK proof around that. That ZK proof cost me a couple of cents to make and I can export it anywhere. Yes. Okay. And there might be other things that I think we've gone deep enough, and this is actually super fascinating, but how the sync committee gets picked and how, you know who the sink committee is.
00:33:15.434 - 00:33:48.520, Speaker C: But let's leave that aside for now. So there are some limitations there around this. And then just for the audience too, if I want to make a ZK proof on an optimistic roll up. So on arbitrum or optimism technically, because they don't have finality for seven days. If I'm going to be purist about it, I can't really make that ZK proof of the state of optimism or arbitrum for that seven day period. Is that right? Is that kind of inaccurate? Or how would you think through that?
00:33:49.690 - 00:34:22.000, Speaker A: I mean, you can make a zero knowledge proof of optimism and arbitrum state transition function. I mean, my real, real maybe spicy take is just like optimism. Arbitram are going to become ZK roll ups before everyone thinks they will be. I just think the tech is ready, it's pretty cheap. And why wouldn't you have the best tech? Why wouldn't you have the future of tech securing your roll up? So I think they will become zero knowledge proofs in a much shorter amount of time than people think. But I'm obviously very biased. So today you could make a zero knowledge, proof of optimism, or arbitrum state transition function.
00:34:22.000 - 00:34:50.120, Speaker A: So that is something you could do if you coded their state transition function and took their call data from Ethereum and were like, hey, when do you have all this call data and you do this stuff? It results in this new state transition. But if you don't do that fully in an optimistic system. Yeah, you can't have finality for like seven days just because you need that challenge period, basically.
00:34:51.310 - 00:35:00.342, Speaker C: And then there's like, Lagrange is one team that's working on a thing to get more finality from optimistic roll ups and stuff like that too.
00:35:00.496 - 00:35:28.374, Speaker A: Yeah, they're working on this basically economic security solution where you have the Eigen layer node operators who are economically bonded, like sign off with economic security on optimism and arbitram state routes, and then they get slashed later. So that's like one intermediate option. But I think the end game of all of that is just like zero knowledge stuff. Then you don't even have to worry about operators, you don't have to worry about any of that stuff. You just have a proof. Like a proof is simple.
00:35:28.572 - 00:36:11.700, Speaker C: Okay? Yeah, but basically, so to wrap this up, right, if you have a chain with finality, unless you're doing some other mechanism, your cleanest way is you make the ZK proof after you've achieved finality, right? So ZK proof after twelve minutes for Ethereum, or after some optimistic time, seven days for the optimistic roll ups. That's like the cleanest way to make these proofs. And if you have a proof of work blockchain, your ZK roll up probably needs to be walked back from the head of the chain to have some economic guarantees around it being not manipulated. You kind of agree with that?
00:36:12.790 - 00:36:43.946, Speaker A: Yeah. So I think it's important to talk about for different chains, like what is finality? So for example, in bitcoin, there's never any true finality. It's all probabilistic finality. So yeah, for bitcoin you just have to prove like, hey, here's the longest chain, and if I'm walking back a certain amount, it just gives you more and more economic finality. For Ethereum, you do have finality finality with economic security. You just know like, oh, these many people get slashed if this block gets reverted. For a ZK roll up on first.
00:36:43.946 - 00:37:29.290, Speaker A: For a ZK roll up on Ethereum to be finalized, first, Ethereum has to finalize, and then on top of that, you have to prove like a ZK proof of the roll up state transition function on top of Ethereum. So like on top of all the call data posted to Ethereum and then for an optimistic roll up, an optimistic roll up for any roll up on Ethereum, it's actually finalized once all the data is on Ethereum and finalized, but the state route might not be available on Ethereum for a while because of either this optimistic challenge process or the ZK proof verification doesn't happen every twelve minutes, it happens every 3 hours just because posting approved theorem is expensive. But the roll up itself is still finalized, if that makes sense.
00:37:29.440 - 00:37:32.714, Speaker C: Yes, I love this. Okay, keep going.
00:37:32.752 - 00:37:35.740, Speaker A: Sorry, or sorry, this is really deep into.
00:37:37.650 - 00:38:17.660, Speaker C: But the point here I want to make and this is where I think we're going to lose the listeners and I don't want to go any further. Right, but the utility of ZK tech is related at a pretty fundamental level to blockchain architectures, I think from interrupt perspective. And although it is clearly the future, in my opinion, and clearly the way these networks should be connected, there is some latency concerns. So basically we get maximum security, but we probably have some latency concerns because many of these chains don't achieve finality particularly quickly, right?
00:38:19.150 - 00:38:58.598, Speaker A: I think basically at the highest level, Xeronol check can make you achieve finality faster. Because if Ethereum roll ups actually do have finality fairly quickly, it's like as soon as Ethereum is finalized and all the DA is available, they are finalized. It's just like you can't bridge out of them and you can't interoperate them because it's hard to prove that finality to someone else. But then with a zero notch proof you can prove that finality very quickly because the computation is already true after the twelve minutes. Zero notch proofs are useful for just finalizing things faster and having a validity proof of that as fast as possible.
00:38:58.684 - 00:39:32.354, Speaker B: Hey everyone, this episode is brought to you by uniswap delivering the best on chain trading experience, period, bar none in all of crypto. Here's how I would divide the protocol today. There's a web part, a mobile part, and an extension part. You can say goodbye to the pop up wallet extension, which is not great from a UX perspective. Now what they've delivered is this nice clean sidebar where you can just very easily track swaps, sign transactions, send or receive crypto anywhere. Just a huge UX improvement on the web part of it, huge improvements. Here you can buy and sell 700 plus tokens at your price on your terms, and they've got limit orders which are powered by Uniswap X.
00:39:32.354 - 00:39:52.662, Speaker B: And there's gasless. They've also got real time charts, transaction logs, pool data, project information, all this cool stuff that just really improves your UX trading on the web. And again, all this is powered by the smartest protocol in terms of Uniswap or Uniswap X. Click the link at the bottom. Uniswap extension is in alpha right now, and we'll see what you can do about getting you moved up that list. Thank you very much. Uniswap.
00:39:52.662 - 00:40:49.962, Speaker B: I want to get a sense of how you see ZK playing a role within Interop. And we've been talking to so far this season, actually, Chris goes of inoma sort of painted this world right, where you kind of have, within the inoma framework, there are these heterogeneous environments, sort of call them computing environments. And one of the big light bulb moments for him that he introduced to us very early on in the season is that the thing that changed and makes interop work within the enoma framework and sort of blockchains more generally is ZK proofs. And I guess probably just because it's been in the zeitgeist a little bit like there's kind of the polygon, sort of ag layer type thing as well, right, where you've got their CDK, their chain developer kit, where you can spin up multiple different chains. And the way that they're going to talk to one another is there's kind of two chains that want to talk. Proofs are generated, they're aggregated at this layer and then posted down to Ethereum. And that all sounds very good.
00:40:49.962 - 00:41:04.530, Speaker B: I think there might be some limitations to that. And Hart was getting at it a little bit with the latency discussion. But Uma, when you broadly kind of think about ZK proofs, how do you think about that within the context of interop? What does that bring out or enable?
00:41:05.990 - 00:41:53.762, Speaker A: So, yeah, I think for interop today, for example, first of all, I think let's talk about optimistic roll ups. I think it's very hard to scale interop with optimistic roll ups, even for people, like across, where the roll ups actually reach finality very quickly because they'll post all their data, and once the data is on Ethereum, it works. But then these relayers, in order to get paid back, the withdrawal window from these roll ups to transfer assets is like seven days. And so hearts relayers on a cross, or whoever's doing the relaying on a cross, has to wait seven days for their capital to be withdrawn. And that's just very inefficient. And so I think with zero knowledge proofs, you can just have much quicker finality because you don't have to wait for this challenge period someday thing. You can just have a proof posted within a couple of hours.
00:41:53.762 - 00:42:01.730, Speaker A: And so it just really improves capital efficiency for interop. And I think that's really exciting because it's just unlocking so much more capital.
00:42:02.630 - 00:42:29.370, Speaker C: I'll add one other thing there too, uma. Like, if there ever is a fault proof, or like, fault proofs in the optimistic system are just very messy to deal with. And the ZK stuff just is way, way cleaner because like you said, it's a proof. So I'll further add that dealing with fault proofs on optimistic systems just seems like a whole messy can of worms that nobody really wants to deal with. But keep going. I interrupted you.
00:42:29.440 - 00:43:16.054, Speaker A: Yeah. Their plan for like, oh, if there's ever a fraud, it's like, oh, well, it's just something much cleaner to just have. Like I always say, dk is like truth, and it's just like, it's very much cleaner because you're just like, I proved this function happened. So, yeah, I think capital efficiency is really nice. I also think, for example, for across today, all these, we're rapidly in this world where there's like thousands and thousands of roll ups, right? Like, you can go to roll up, go to conduit or caldera or whatever, and I could even spin up a roll up in two minutes. And so for all these different roll ups, it's really hard for someone across to support all of them because you have to run all their nodes and that actually costs a lot in AWS. And yeah, you have to run all this infrastructure.
00:43:16.054 - 00:43:46.930, Speaker A: And so that fundamentally doesn't scale super, super well. And with a zero knowledge proof, it's like maybe one day across doesn't even have to run a node for a different chain. It can just verify a proof that this happened and then be like, okay, let's do the relaying or stuff like that. And so I think that's also pretty powerful. So I think those are the exciting applications of ZK and interop. Also, even moving out of roll ups, we're still in a world where there's like a ton of l ones. And for l one to l one bridging.
00:43:46.930 - 00:44:48.670, Speaker A: Basically today, the status quo is like you have some multi sick of like five people, and then you have someone deposits money on one side. The multi sick says, yes, okay, the money was deposited and then they unlock tokens on the other side, or they mint tokens on the other side. And that's obviously very risky because these multi six can get hacked, because you just have like five people, you just hack into an AWS account or two and suddenly the keys get compromised. And if instead on the other side, you're verifying a zero knowledge proof that the original validators of the source chain actually signed off on a current state of the world, it's just much more secure because that's very hard to forge. Replacing multisigs and making cross cell one bridging way more secure is very exciting. And then transitioning all roll ups to zero ZK roll ups and unlocking capital efficiency and more seamless, smooth interop and better scalability across thousands of roll ups is also really exciting.
00:44:48.850 - 00:45:49.910, Speaker B: Okay, Uma, I'd love to get your opinion on the overall demand for how useful synchronous versus async composability ultimately is, because that's one of the limitations here at least my understanding of roll up to roll up interop that gets facilitated by ZK, that's async, right? That's not happening at the exact same time. And maybe to challenge a little bit how useful that is, is one of the reasons why people are extremely excited about, or at least at one point we're really excited about that interoperability. There's an MEV aspect to this, right? Where you could do some sort of the simplest form, like an arbitrage between two different dexes. And in this world you could maybe place a trade where there are multiple legs. You submit a proof which corresponds to each leg that gets aggregated into this one layer and then posted. And then that's very simple. The problem, as we've talked about a little bit, is like the latency there, because again, ethereum moves in twelve second block times and five minutes, there kind of limits that specific use case.
00:45:49.910 - 00:46:00.590, Speaker B: So I guess, what are your thoughts on the limitations that the latency in this world has on different sort of cross chain use cases?
00:46:02.130 - 00:47:16.120, Speaker A: Yeah, so cross chain MeV is an interesting topic because I feel like a lot of people are excited about it. But I think at the highest level, you could argue the real cross chain MeV is the roll ups and finance or centralized exchanges, because those people are always going to have a faster block time than anything decentralized. So one could argue, well, is there even going to be roll up a, cross, roll up b, arp, when the true leg is really roll up a to binance, roll up b, to binance. And then maybe that's not super ideal, but that does feel like that's going to probably be the most important volume, at least for a long time. So there's a question of does crushing Mev even really matter? So even if it's async, the second thing is, I guess if you have synchronous interop across two different chains, fundamentally, I think the proposer, like, the person who is deciding what that block is, has to be the same entity. The chains can be separate, but the proposer at that point has to be the same entity. And so that can happen with ZK, without ZK, whatever, if that makes sense.
00:47:16.120 - 00:47:25.610, Speaker A: Yeah, there. I think the proposer sequencer is like the most important person, not the validity mechanism. If that makes sense.
00:47:25.760 - 00:48:27.978, Speaker C: It does make sense, and I think it's a nuanced and interesting and tricky point, but I think where I'm aligning and Mike, we still have more episodes in the season to try to hash this out. But if you want synchronous cross chain things, the block proposer gotta be proposing that same block on both those chains. And in a sense, it almost looks like they're the same chain, but not. And maybe there's sort of like some weird nuance here where they're separate chains most of the time, but every so often, when they need the synchronous thing, there's like a block that is proposed by the same entity and it kind of becomes the same chain for that moment anyways. I think that does make sense, Yuma. But it's definitely a nuanced point. And yeah, regardless what you're saying here is to just reiterate the stuff that's really, like, in my brain, and it's like ZK becomes this glue to connect disparate systems with, quote unquote truths.
00:48:27.978 - 00:48:59.210, Speaker C: Because you literally just have a math proof of what the truth is. Your limiting factor here is that in order to create that proof of truth, you need that chain to achieve its true truth, which is AK finality, right? Which kind of makes total sense. If it's at some intermediate state where it's not fully, fully proven on that chain, then you can't quite do the proof of truth. Yeah, I don't know. That just seems really powerful.
00:49:00.190 - 00:49:01.340, Speaker A: Yeah, I agree.
00:49:01.870 - 00:49:14.240, Speaker B: So, Uma, if this is already really cheap, the latency doesn't ultimately end up mattering that much. Why aren't we aggregating all of, like, why doesn't it feel like all these chains are basically one? Why isn't everyone doing this?
00:49:15.090 - 00:49:17.710, Speaker A: Know, I asked myself that question too, Mike.
00:49:19.250 - 00:49:22.080, Speaker B: That's not visionaries like you, Uma. That's the.
00:49:23.410 - 00:49:57.242, Speaker A: I think. Okay, a couple of things. So I think historically to use ZK, you'd have to have this team with really built out ZK expertise. Like you'd have to be a ZK wizard, you'd have to have a whole cryptography team, you'd have to build your own proof system, you'd hand roll your proof system. It sucked. We did that for all our bridges and it's just not very sustainable. I think there's been this recent wave of ZKVMs where basically it allows any team to just express their computation in rust or normal code and then get that proven for a cheap enough cost.
00:49:57.242 - 00:50:41.000, Speaker A: And that's what things like SP one kind of enable is. Like you can get all the performance you ever wanted, but you can still write it in normal code. And that combination of amazing developer experience plus amazing performance, that combination is, I think, what's been kind of missing until this point. And now with things like SP one, I feel like it's kind of at the turning point where it's like the devex is good enough and the cost is cheap enough where now everyone can go adopt it. And that only came out like a few weeks ago. So I'm very optimistic that hopefully this is kind of like the turning point for ZK, where suddenly even teams that have pursued an optimistic approach for a long time are like, oh wow, now the tech is good enough and now the tech is cheap enough. Let's go do it.
00:50:41.610 - 00:50:52.720, Speaker C: Yeah, uma, I guess just to push on this. This is so recent, right? Like SP one you guys announced, but is it main net or what do you call it, Devnet right now too?
00:50:54.210 - 00:51:11.278, Speaker A: Right now it's just still, Alpha mode is not intended for production use. That's like our highest priority as a team and what we're working on very actively towards. So hopefully that comes soon, we hope in a few months. That is also a bottleneck.
00:51:11.374 - 00:51:43.760, Speaker C: Yeah, well, okay, but we're of the moment, you guys have just put this out. Risk zero, sort of in the same space, is getting, coming out soon, very soon too. I think there's another project gevulot that's like prover network. All this stuff is like, not, this is the point. Why I think the next six months to a year are going to be super impactful because these tools are really, truly going to be live and usable with a good experience.
00:51:44.290 - 00:52:04.734, Speaker A: Yes, exactly. No, yeah, to answer your question, Mike, I mean, I was kind of joking before I think a lot of this stuff is really new. So historically, ZK has been very hard to use and its performance is bad. Then a lot of people fix the performance, but it's still really hard to use. And then now with things like SP one, it's like you get the best of both worlds. But SP one obviously is very recent. I think it's like a month old at this point.
00:52:04.734 - 00:52:09.800, Speaker A: So now it's kind of like, okay, now people can start thinking seriously about.
00:52:10.330 - 00:52:15.350, Speaker C: So this is exactly where we want to talk to you. It's the frontier. You are the frontier.
00:52:17.290 - 00:52:18.422, Speaker A: That's too nice.
00:52:18.556 - 00:53:00.494, Speaker B: Can you help us think through what are some of the second order implications of if ZK tech were widely adopted, how would that change blockchain architecture and sort of where values? Here's one thing I was thinking about. So one of the layers that network effects accrue to is at the execution layer, we see massive returns to the EVM. Now we're seeing it with the SVM. And maybe this move ecosystem ends up being valuable as well. But one of the, at least when I think through how computing changes with ZK is suddenly it doesn't really matter what execution environment you're using. And most of that computation happens off chain. And then these sort of proofs, there's like some new kind of aggregation layer that ultimately ends up happening.
00:53:00.494 - 00:53:16.826, Speaker B: And then a lot of blockchains as we think about them are just kind of like bulletin boards. And that's like where I understand the sort of celestia vision going, the Ethereum vision. I mean, does that make execution environments slightly less valuable? Just help us think through some of these second order implications if this were.
00:53:16.848 - 00:53:18.330, Speaker C: To be widely adopted.
00:53:18.750 - 00:54:00.570, Speaker A: Yeah, it's interesting. I feel like if ZK proofs become widely adopted, it adds like a new layer to the modular stack. So there's data availability and consensus, which is like one part of the modular stack. There's settlement, which is kind of like what asset you're using. Where's all the bridging happening? So, for example, right now, I think a lot of people use Ethereum as a settlement layer. So that's another part of the modular stack. There will be like proving, which becomes a part of the modular stack, which is like, how are you actually getting your validity proofs to settle to the settlement layer? And then there's actually the user facing stuff which is like what network are they using? And things like that in terms of value accrual.
00:54:00.570 - 00:54:56.220, Speaker A: I obviously think proving will be very important and it'll be more of like a volume thing where it's like, oh, you just pay one cent or maybe even a fraction of a cent for a proof. But there's so many. Every user interaction with the chain will use a proof. And so that will be like proving will be like a volume based thing where it maybe ends up making a lot of money, but it's just because there's so many proofs being generated. I think obviously the economics of settlement look pretty different because it's like, oh, eth is money and so there's monetary premium and then maybe with DA, I don't know what the value of coral story looks like there, but that's just like another part of the for. I guess it's hard for me to say, but I think the TLDR for me is like there will be another really important piece of the modular stack. In addition to DA and settlement and execution, there's also like, now there will be.
00:55:00.030 - 00:56:14.642, Speaker C: That is another question I have in my mind for you, uma, too, where it's like, how many of these proofs do you think there are? And I think there's going to be this evolution here too. So if a proof takes five minutes to generate, right now, in my mind, it doesn't make sense for there to be like 10 million proofs being generated a day from at least an interop perspective, because you're going to batch your messages and send them every ten minutes or something like that. But I still think the ZK component here is incredibly valuable because you are sending truth like every ten minutes. And sidebar this fits into our across thesis very well because across batches together, repayments every hour or whatever. But this sort of runs counter to, I think, the thesis of a lot of other messaging frameworks, axilar, wormhole layer zero. I think they're all kind of built around this idea that there's going to be tens of millions of messages getting sent between blockchains. And I'm kind of curious, but that are not necessarily ZK, right? And I'm kind of curious where your mind goes.
00:56:14.642 - 00:56:36.730, Speaker C: Where if ZK takes five minutes to generate now and it's going to take 30 seconds later, but we're not down to 200 millisecond proof times or 20 millisecond proof times for a while, I think. How many ZK messages do you think or ZK proofs are getting generated in a year from now? And how does that influence blockchain, interop and architecture?
00:56:40.670 - 00:57:17.590, Speaker A: I think, for example, for a ZK roll up, it's kind of like there's not a proof generated for individual transaction, but there's like a proof generated for the entire block. And maybe actually there's proofs generated for every block that get aggregated together. So in the end, it's kind of like every transaction does trigger a proof. It might just be everything's batched together. And so I think with messaging, it could be pretty similar where it's like, maybe not literally every single individual message triggers a proof. But you have a proof that, oh, ethereum came to finality on this batch of blocks. And then, oh, I have like 15 messages that were contained in that range.
00:57:17.590 - 00:58:03.830, Speaker A: I think obviously there will always be non ZK solutions for things that, where it's like, maybe the finality doesn't matter as much or the security doesn't matter as much. And so, yeah, I don't necessarily see like, oh, everything has to be ZK all at once. I just think more and more things will transition towards that, starting with the highest value use cases. So, for example, right now, I feel like roll ups being ZK roll ups makes a ton of sense because they have a ton of TVL on the line. And I'm sure if you're a roll up team, it's really stressful to operate one. And adding ZK security adds your peace of mind, if that makes sense. And then I think as it gets cheaper and cheaper, more and more people will just start using it because it's like, why not? So that's kind of how I see it evolving.
00:58:04.330 - 00:58:57.960, Speaker C: Okay, so this is actually kind of interesting. So if you're doing a ZK proof of each block, and then every, I don't know, 100 blocks, you're doing a ZK proof of those 100 blocks recursively and then posting that to Ethereum, let's say. Right? And if your block time is 2 seconds, I just did some math. That means you're making 43,000 plus a little bit overhead, you're making like 45,000 ZK proofs a day. And then in a world of 1000 roll ups, you're making millions of ZK proofs a day here. Okay, cool. So this is like good business then, for people doing ZK things where if we have a world of 1000 roll ups or 10,000 roll ups, all ZK based, they're doing a shit ton of ZK proofs on the interop side.
00:58:57.960 - 00:59:23.680, Speaker C: Maybe that's true. Maybe that's not true though, because if you aren't able to make those ZK proofs immediately anyways, because of finality concerns, like you have to wait for chain finality, then probably does make sense to batch them together. And so maybe, like you said, you don't have nearly as many ZK proofs between blockchains, but they're just really useful. I don't know.
00:59:24.290 - 00:59:27.140, Speaker A: Yeah, I can imagine that happening. Yeah.
00:59:27.830 - 00:59:32.398, Speaker C: Interesting. Okay, I'm learning things I should ask Uma.
00:59:32.414 - 00:59:49.000, Speaker B: This is probably a really basic question. AsK right at the end of the podcast. But in l one to l one communication that makes use of a ZK like line or something like that, do you still need a general message passing protocol or transport layer like layer zero to deliver that, or no?
00:59:50.090 - 01:00:31.302, Speaker A: Yeah, layer zero actually is like this. And I think a lot of other protocols, like hyperlane and stuff, are like modular. So basically they just provide you smart contracts and kind of like this framework for sending messages. But then the security mechanism of how the messages are delivered can be abstracted away. So you can use layer Zero's multi sig, you can use ZK proofs, you can use ZK proofs, you can use someone else's ZK proofs, you can use some other system. You still need the messaging framework, because you still need the solidity side of things to keep track of the messages, you still need the relayers, all that kind of stuff, and then the security mechanisms abstracted out and can be either ZK or non ZK. So I think, yes, you still need layer zero.
01:00:31.302 - 01:00:42.970, Speaker A: We would, for example, work with a team like layer zero. We work with wormhole to help provide them their ZK security. And so we could also work with a team like layer Zero to help add a ZK oracle to their protocol.
01:00:43.550 - 01:00:44.300, Speaker C: Awesome.
01:00:45.710 - 01:01:31.354, Speaker B: Uma. In conclusion, one sort of big question we've been asking guests from this season know as crypto grows and there are many different chains and many roll ups that exist, obviously the space is getting more complicated. Sometimes these solutions, like the best solutions, end up taking a really long time to get to market. Other solutions that maybe accept some less positive trade offs or cut corners kind of end up happening. And we kind of have to have these very hard conversations with ourselves about trade offs. And something Hart and I have been questioning is like, what ultimately would be a good end state for crypto, and this sort of on like a ten to 15 year time horizon. If we accomplish something here, as this industry in this future, what would be a successful outcome from your perspective, like.
01:01:31.392 - 01:01:33.798, Speaker A: With regards to ZK or more broadly?
01:01:33.894 - 01:01:39.210, Speaker B: More broadly, but theoretically, maybe you spent a lot of time with ZK, so I'm assuming that's part of the vision.
01:01:40.030 - 01:02:00.386, Speaker A: Yeah, I think at the ZK level, I just think ZK is simple because it's just such a fundamentally simple. It's not simple to implement. Our team knows that. But once you have, it's just such a simple primitive. It's like I have some function. I proved you the function is like this value with this input. Okay, now you're good to go.
01:02:00.386 - 01:02:25.914, Speaker A: And I like to tell people ZK is like truth and the truth is simple. And so I just see ZK being integrated into everywhere. I think every bridge will be a ZK bridge. I think every roll up will be a ZK roll up. I think basically all blockchain infrastructure will run on ZK rails and it'll all be seamlessly interoperable. It'll be all verifiable. There's none of this complicated optimistic stuff.
01:02:25.914 - 01:02:43.546, Speaker A: There's none of this complicated multisig stuff. It's just like verifiable compute being used everywhere. That's kind of like the end game vision. I see for all this blockchain infrastructure stuff is like ZK is like the glue that holds everything together. And that's actually why it's like a great fit for an interop episode. Because interop is all about glue. And I think ZK is kind of like the ultimate glue.
01:02:43.546 - 01:02:56.898, Speaker A: That's not to say there won't be more layers. Like for example, I think there will always still be room for across because sometimes you do need to eat. Finality risk. Yeah. Woohoo. I do think there's obviously going to.
01:02:56.904 - 01:02:58.290, Speaker B: Be good buddy wallets.
01:02:59.770 - 01:03:31.774, Speaker A: There will always be room for wallets who provide a really seamless user experience and integrate with across or stuff like that. There's always going to be room for aggregator. There's still going to be many layers of a stack, but I just think ZK is going to be like the base glue for everything. And basically yes, having truth communicated everywhere. Yeah. So that's kind of the end game of how I see the space evolving and how I see things talking to each other. And hopefully that leads to better security like normal hacks, and leads to like, oh, there's not roll ups that are just multi sigs with no fraud proofs and stuff.
01:03:31.774 - 01:03:33.140, Speaker A: It's just like everything.
01:03:34.070 - 01:03:35.620, Speaker B: Which roll up are you talking about?
01:03:39.030 - 01:03:40.094, Speaker A: We don't name names.
01:03:40.142 - 01:03:53.350, Speaker C: We don't name think. Okay, I think Mike wants to. It's going to wrap this episode, but actually um, I want to ask you one other question. What's your most exciting use case for ZK outside of blockchains. I think I have a guess what you're going to say, but I'm really curious.
01:03:54.570 - 01:03:56.600, Speaker B: I actually have an answer for this too.
01:03:58.730 - 01:04:02.300, Speaker C: Mike and I have our bet. Mike and I have our bet. Let's see what you say.
01:04:02.990 - 01:04:41.240, Speaker A: I think some of the attested image editing stuff is really exciting and interesting. Basically you have some attested sensor, but none of the images you see on Facebook and stuff are like the raw sensor data because that's way too big. And basically you can just prove like this image was taken from a camera and then modified with these transformations and JPEG compressed and cropped and whatever. And then this is like a real image, not from mid journey. I think that's already super exciting. Actually, some people at a Stanford hackathon tried out a tested image editing with SP one, and it was really exciting and interesting. So I'm very excited for that kind of stuff, especially as we get more and more crazy, like image generation, video generation, whatever.
01:04:42.090 - 01:05:05.350, Speaker C: That was pretty much what I was going to say. Something in AI space, which is like, it's like this idea of verifiable compute. So even verifying that an AI ran account computation is interesting too. But the attested imaging stuff, having all our iPhones with like a chip in them that is going to attest that our photos are real is pretty cool. Mike, what was your guess?
01:05:05.520 - 01:05:58.346, Speaker B: I had such a lame. I mean, mine is much too optimistic. I don't think it's ever going to happen. But I was listening to this podcast, shout out Dimitri Kofinas, about basically how the modern surveillance state got created. And we don't need to go all the way into it, but basically to drastically dumb down what ended up happening was the US government just created essentially a list of everyone they're like, in order to make sure that people aren't doing bad things, we just need a whole big list of people. I don't know how this would actually translate into something like ZK tech, but I just remember thinking it would be really nice if we could actually get the desirable government outcome that the government wants, which is like, hey, are you doing horrible kitty porn things online without just aggregating all of that in one place? Because that's always been my big problem with the crypto anarchists. Governments actually do do a lot of shit, stuff that we really want them to do, and they do have hard jobs.
01:05:58.346 - 01:06:07.780, Speaker B: So if we could give them this primitive, this tech building block, maybe actually that could lead to the outcomes that we all want with the privacy that's extremely important.
01:06:08.710 - 01:06:18.062, Speaker C: Mike, you're kind of like this upstanding tack. Good taxpayer cypherpunk. It's like a great combination. I kind of love, appreciate.
01:06:18.126 - 01:06:34.540, Speaker B: I'm definitely not, like, a burn it all down kind of person. I'm actually pretty stoked. I like living in America for the most part. I'd love to not burn it down, but I don't love what's going on now. So I appreciate that heart. That's very nice. Uma, if people want to find out more about you succinct, what's the best way to do that?
01:06:35.230 - 01:06:45.886, Speaker A: Yeah, you can follow us on Twitter, like, at succinct labs. And, yeah, we post a bunch of stuff of what we're up to there. Yeah, that's probably the best way.
01:06:45.988 - 01:06:49.614, Speaker B: Okay, cool. This was really fun. Uma, thanks for joining us.
01:06:49.732 - 01:06:50.158, Speaker C: Thanks.
01:06:50.244 - 01:06:54.542, Speaker A: Yeah, this was super fun. All right, good to see you guys. All right, bye.
01:06:54.686 - 01:07:05.300, Speaker B: All right, Hart, that was a great episode. Almost made a ton of sense. We should just obviously be leaning pretty hard into ZK proofs. What do you think?
01:07:05.910 - 01:07:40.480, Speaker C: Yeah, makes a ton of sense. I mean, I think Uma is know, full disclosure. I've gotten to know her from basically day one of her starting succinct, her company, and she's just, like, clear thinker, great energy. It's hard to think that that same person is, like, a super nerdy math geek that can do, like, the crazy moon math in her sleep. And her co founder John's pretty brilliant, too. So just amazing people. And I think those are the types of folks, and there's a lot of them in the ZK space that are really taking this tech and making it a thing.
01:07:40.480 - 01:08:05.314, Speaker C: Three years ago, the idea of this existing was an impossibility. People just thought that was no way. And this industry is wild. So, yeah, the ZK stuff seems like it's a really important part of the stack. And what I really enjoyed about Uma's conversation is she was also very real about the limitations of it, like, where it works and where it doesn't. And I thought that was super compelling.
01:08:05.442 - 01:08:48.566, Speaker B: And one thing that we didn't really get into, but Uma originally succinct was really focused on the bridging sort of application. And then they. I don't know if pivoted is the right word, but sort of expanded into this new release, this VM, the ZKVm that they just launched. SP, one, I believe is the name. And you can hear her on other podcasts, sort of talk about going through the pain of developing a ZK application it was kind of beyond the scope of this podcast, but how difficult it was to just debug circuits and stuff like that. And that was actually part of the reason why she decided to build out this more general purpose infrastructure. So I think that was one of the bigger takeaways for me, is that it actually just used to be extremely hard to build these applications out.
01:08:48.566 - 01:08:59.290, Speaker B: But now with succinct, and I understand risk zero does something pretty similar, I think it's just going to get a lot easier. And there might be this massive cambrian explosion.
01:09:00.830 - 01:09:41.222, Speaker C: Yeah, absolutely. It's funny, I can speak because across we were working with succinct on their old product, their bridge product, to actually bridge messages from Ethereum to other l ones, like BSC. And it is pretty interesting, because basically, I think Uma's story here is they were hand rolling and hand tuning all of these circuits for specific use cases. And it was almost like they were becoming like consultants or like service providers that would just build this circuit for a specific use case. And they're like, this sucks. This isn't scalable, this takes forever. And then they figured out how to abstract it away, which is amazing and makes so much sense.
01:09:41.222 - 01:10:33.878, Speaker C: And yeah, they're not the only ones in the space doing it, so we should give a lot of props to all the other great teams in the ZK space that are building cool stuff, but they're an example of a team that was painstakingly tuning their own circuits or writing their own circuits, and they're like, let's abstract this away. And I don't know enough to know exactly how well it works, but from what I've been told and from what I can observe, it works really well. And so that's pretty cool. The other stat that shocked me, actually, was this like two cent kind of proof generation cost. And this is in a specific context, this is like a tendermint consensus proof. But that was way cheaper than I was expecting. I thought that was pretty cool, too.
01:10:33.964 - 01:11:39.610, Speaker B: Yeah, the cost aspect of it is coming down. The latency aspect of it is coming down. You know, where my mind ultimately went was I was drifting back to that first episode that we did with Chris and trying to think about these different environments and how they ultimately end up getting connected. And one thing I've been sort of thinking about is like, I'm starting to question this whole modular versus monolithic debate and how much it really makes sense. You know what I'm saying? The way that I'm sort of starting to look at it is which computing actions are coupled tightly. And I think once we solve a lot of the frictions that exist in these ecosystems, it's ultimately going to be like where there's an economic incentive to tightly couple certain interactions. And I sort of feel like where I'm heading now towards this ultimate sort of end state is that we're going to basically break all these blockchains apart into this super modular sort of framework, but then we're going to get the recoupling of certain interactions where it makes sense to actually couple them.
01:11:39.610 - 01:12:05.358, Speaker B: And then I kind of just think it sounds like ZK is going to connect everything else. Maybe there's not actually an enormous amount of demand to trade Ethereum on Jupiter, right? But for that, we'll have zero knowledge proofs and that'll be fine. And then it's like a different system will support trading Solana on Jupiter. So that's where I'm kind of landing on this, ultimately.
01:12:05.534 - 01:12:37.790, Speaker C: Yeah. Mike, I like all your questions. Know, I'll call them business models, like value capture. Like, where is value going to be captured here? And increasingly to me, it's coming across like a lot of these systems, there isn't really a moat. It is actually like there's a service that's going to be provided in a highly competitive market at what is going to be extremely low cost. And the value capture is just volume. It's like doing a lot of it really cheaply.
01:12:37.790 - 01:13:16.680, Speaker C: And I honestly think that this is, first of all, not a bad thing for a consumer. And when you think about blockchains being permissionless, decentralized organisms, it's almost the way it should be, too. So, Uma, we didn't push her on this, but when she was talking about ZK and how you make money here, it's just like going to generate a shit ton of ZK proofs, going to charge not much more than the cost of them, but a little bit. And it's going to be a volume game. Just kind of like, AWs is mostly a volume game. There's like some profit margin, but it's not huge.
01:13:17.130 - 01:14:18.938, Speaker B: Yeah, well, I think Sam said this so well in the second episode, it's like all these people are asking about business models and value capture, and that's such a luxury for me because I just want it to work. And I think that's one of the, by the way, people ask all the time, like, why are all these podcasts so technical? It's because the smartest people are still figuring out how to make it work, and they're going to figure out the business model and value capture stuff later on. But I do think one thing I wish we had poked a little bit more on is just where value is going to end up getting created in these proving networks. And there is a lot of interesting stuff going on in different layers of aggregations within ZK. And I think, correct me if I'm wrong, but I'm pretty sure that the sort of plan from the polygon perspective in their AG layer is to kind of have these economics economies of scale right in the aggregation layer. Once you get a whole bunch of these things together, you kind of batch them up and then submit them down to main chain eth. And ultimately that ends up being pretty sticky because if you were submitting this outside of that aggregation layer, it's much more expensive.
01:14:18.938 - 01:14:28.500, Speaker B: So there are these kind of interesting games that I can see people starting to play, but maybe it's just too early and we just got to see how that all shakes out.
01:14:28.950 - 01:14:51.930, Speaker C: Yeah, I think it's pretty clear to me if I just gut reaction. Think about Uma's succinct Labs sp one business model. They're building a marketplace for provers. Provers are going to be hyper competitive. This is going to talk about a lot of the other things we talk about in our space. It's like the HFT space in finance. So hyper competitive.
01:14:51.930 - 01:15:36.200, Speaker C: Maybe there's not a ton of provers because they're super specialized. Uma hinted at like maybe some of these provers build their own hardware, like literally to generate these Ek proofs cheaper. Maybe that happens. But you have a really competitive marketplace of provers that are just there to offer this proving service at the lowest cost. And succinct business model here is they're the marketplace for it. And that marketplace can't be usurious or can't charge too high rent because another marketplace will pop up that's cheaper, but they can charge something for being the central point of where supply and demand meet. And I think that makes sense.
01:15:37.710 - 01:15:45.002, Speaker B: I do too. Yeah, we love a good marketplace model and you know, we started do that's a popular model.
01:15:45.056 - 01:15:45.820, Speaker C: Do people?
01:15:48.830 - 01:16:20.806, Speaker B: All right, here's one interesting place that we could have gone as well that we sort of jokingly did at the end. But I do think zero knowledge is one of those areas. And I think Vitalik has actually said that zero knowledge as a technology is comparable to blockchain as a technology in terms of what it like. That's the scale that we're operating on here. And it does feel like there's so much that's been said about how much can you bridge what's really happening in a real world context into a blockchain. ZK proofs aren't really interesting part of that story. Right.
01:16:20.806 - 01:16:47.710, Speaker B: We've called this the Oracle problem. How do we relay this stuff that's happening in the real world? And one of the things that it seems like ZK proofs do really well is they take these extremely heterogeneous sort of computing environments and it spits out this output which you can put onto a blockchain. And that actually, like ZK kind of could be the missing link in between getting some of this tradfi stuff that we've been talking about forever on crypto rails.
01:16:49.250 - 01:17:06.758, Speaker C: Yeah, yeah, I think it's hugely powerful. I agree. But emphasizing what you like, this is a technology is as big as blockchains. That's a big statement, and we should probably fact check it before we say Vitalik said that, but I think he did, or he said something like that.
01:17:06.924 - 01:17:09.160, Speaker B: David told me that Vitalik said that last.
01:17:14.570 - 01:18:01.094, Speaker C: We'll fact check this at some point. But before we say, I'm willing to go so far to think that Vitalik definitely thinks ZK is a big deal, and we all should, because I think it is one of those transformational texts that there is a lot. And again, what's so interesting even about the conversation with Uma, the whole thing was about the succinctness property of ZK. We really didn't even get into the privacy bit too. And like, there's a whole other unlock of what the privacy stuff gives us as well. That I think is, frankly, that's where I've spent less of my own brain power, thinking through what the implications of that are. But it's like one technology that gives you two major unlocks at the same time.
01:18:01.094 - 01:18:02.630, Speaker C: And that's pretty wild.
01:18:03.130 - 01:18:39.586, Speaker B: I agree. And I've actually decided for the intro to the show, when I do it, I'm going to walk you through. But getting ready for the show I actually did on Wikipedia, there are a couple of very plain world sort of examples of how the ZK and the privacy model work. And the privacy stuff. I mean, again, going back to our conversation with Chris, that was super important to him. He actually viewed that as a fundamental issue with how blockchains are designed for all these different reasons. It's just very suboptimal to have your transactions revealed, either because people have a right to privacy and people don't want to be exposed, or because some bot can come in and grab your transaction and then sandwich you.
01:18:39.586 - 01:18:52.450, Speaker B: Right. Like, there are all these great reasons why privacy should actually be at the core of the design. And, yeah, we didn't really get into that, but I agree, Hart. This was a really fun one. My will. Yeah, I will see you same time next week. Cheers.
01:18:52.530 - 01:18:53.666, Speaker C: See you next week, Mike.
01:18:53.778 - 01:19:20.490, Speaker B: Hey, everyone. Want to give a final shout out to this episode's title sponsor? Say now, there are a whole bunch of really exciting reasons to be building on, say, v two outside of just parallelization. I want you to head over to say IO to looking into building on their public Devnet again. Click the link at the bottom of this episode and head over to say IO. Start building something. Today's.
