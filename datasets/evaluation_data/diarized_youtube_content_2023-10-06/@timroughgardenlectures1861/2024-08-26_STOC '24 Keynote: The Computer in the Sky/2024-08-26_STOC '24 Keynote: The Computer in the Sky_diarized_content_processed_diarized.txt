00:00:00.240 - 00:00:32.905, Speaker A: And thanks more generally to the Theory Fest organizing committee for the invitation to come speak. I've been going to stock Foxes for a long time. I think my first one was Fox 98, which was back in Palo Alto. I've been to tons of them since. This is definitely the community that I sort of came up with as a graduate student, and it's still one that I'm, as you'll see, sort of deeply loyal to. So really an honor to be here to speak to all of you. A couple things first, so as Anupam mentioned, I am these days an indication of union at Columbia, head of research at Andreessen Horowitz, which is a venture capital firm.
00:00:32.905 - 00:01:28.457, Speaker A: Rest assured, I'm here in my purely academic capacity and God forbid nothing I say should be construed as financial advice. I also want to acknowledge the very untimely passing of Luca Trebasson, who was taken from us far, far too early last week. So I recruited Luca to the Stanford CS faculty back in 2010. So as a result I had the privilege of being his colleague and write down the hall for four years. Those of you that know his work know that we lost a spectacular intellect. Those of you that knew him personally know that we lost a kind, funny and generous soul. So I want to talk today about sort of a computing functionality technology that I think would be really cool to have, and a couple of the many scientific and engineering challenges that come up when you try to actually realize that technology.
00:01:28.457 - 00:02:25.783, Speaker A: For an analogy, you could imagine 60 years ago having a vision of a technology that enabled any two people on the globe to communicate with each other near instantaneously, and you could view today's Internet as some kind of approximate realization of that goal. And if we view the Internet as a sort of neutral global infrastructure for communication, here by neutral meaning there's no one person or nation state that owns or operates it. Really what I'm after is the analog for computation, so global neutral infrastructure for general purpose computation. So fundamentally the functionality I want is just that of a general purpose computer. A computer that in effect has its own operating system, its own processor, its own memory, its own stuff state different computer than the ones we're used to. Unlike a typical computer where somebody owns it, like an individual or a corporation, someone has root access. I want a computer that has none of those properties.
00:02:25.783 - 00:03:01.785, Speaker A: Basically, there's no owner, no operator. In some sense this computer is just running in the sky for all of us to watch, kind of running as a public good doing computations. I want it to be sort of impossible almost to shut down this computer, as hard as it would be to shut down the Internet. So we should have full confidence that this computer will carry out its computations correctly for a very long time. And I want it to be open access. Anyone should be able to use it. And here by use, I mean you should be able to install your own computer programs which run in this computer's operating system or interact with other programs that have been installed on there before.
00:03:01.785 - 00:03:54.593, Speaker A: So a little more concretely, where is this computer exactly? Right. Is it like a supercomputer buried underneath the ocean floor? No, it's not a physical computer at all. Tank's going to be a virtual machine. There are physical computers involved, ideally hundreds of them or thousands of them, scattered across the globe, communicating with each other over the Internet. Right? Each with its own hardware, operating system, et cetera. But they're all going to be running software and common software, a blockchain protocol via which they will coordinate on a simulation of a virtual machine. That really is fundamentally the point of a blockchain protocol, to enable a number of physical machines to carry out coordinated simulation of a virtual machine.
00:03:54.593 - 00:04:56.875, Speaker A: Okay, so why would you ever want to do this? Or I should say, so you sort of think of this as a protocol in the spirit of the protocols for communication that we know and love, smtp, HTTP, et cetera. Notice that because this is a protocol for computation and not communication, it is going to be a stateful protocol. It needs to have memory to remember the state of this virtual machine. Okay, so why would you bother to do this? Why would you start with hundreds or thousands of computers and just get one at the end? Well, this virtual machine on the right has some pretty different properties than the physical machines. Again, we talked about turning a computer off, that's sort of trivial for the physical machines on the left, you just flip the power switch, which the owner of that computer is perfectly positioned to do. Okay, but this computer on the right, this virtual machine, if you have a well designed consensus protocol, turning this machine off is potentially as hard as shutting off the Internet. Okay? Much more, much more robust.
00:04:56.875 - 00:06:06.503, Speaker A: All right, so that's what I mean by this computer in the sky, right? So why would you want one? Right? There's a lot of answers to that. Honestly, if we really did have sort of powerful, robust, global, shared computing infrastructure, I think there's a lot of things we could think about doing with it. Let me just give you a sort of narrower answer, but still illustrates the point, which again, if you remember nothing else, remember this, that blockchain protocols, Web3, whatever you want to call it. It's really not fundamentally about cryptocurrencies, about prices, it's not about finance. It's really about a much more profound technological advance that in particular gives us ownership over things that we buy or create in the digital realm in a way that's stronger than what we've ever had before. So let me elaborate, let me add to the picture public key cryptography, specifically secure digital signature schemes. Because let me think about sort of a signature scheme as allowing an end user to own in a meaningful sense a bit string, like 512 bits representing a public key.
00:06:06.503 - 00:06:45.975, Speaker A: And here by own I mean someone who knows the corresponding private key. Because I claim there's a at least loose analogy between the cryptographic guarantees offered by a signature scheme and the kinds of property rights that we're accustomed to in the physical world. Like think about real estate, like imagine you go and buy a house, right? So that affords you various property rights. One is called the right to use. You want to live in your house, no one can stop you. A little bit like how if you own a public key and you don't have a corresponding private key, you can produce valid signatures. No one can stop you.
00:06:45.975 - 00:07:41.165, Speaker A: Another property right through your house would, would be the right to exclude. No one else can live in that house without your permission. A little bit like how with the secure digital signature scheme, no one other than the owner is capable of producing valid signatures. So this computer in the sky, remember its general purpose, certainly capable of sort of verifying digital signatures. So let's couple these two ideas in a kind of trivial way and allow users to own not just 512 bit strings, not just public keys, but arbitrary digital data that lives on this computer in the sky. And conceptually trivial, a program, maybe it's the computer's operating system, maybe it's a sort of user installed program. It can just create pairs corresponding of a public key pk, an arbitrary digital data X.
00:07:41.165 - 00:08:20.297, Speaker A: We have a notion of ownership for public keys, knowledge of the corresponding private key. And so now we get ownership notion for arbitrary data just through inheritance. So you were the owner of X if you own pk, meaning you're capable of producing a signature that checks out with respect to pk. Okay, so at this point maybe you're saying like, all right, I get it. So I guess there's a sense in which you can sort of own digital stuff like on this kind of global computer. But like why is that? Why is that interesting, right? And There's a bunch of answers I can give here, but just sort of high level. A lot of the answers kind of have the form Owning data on this computer in the sky.
00:08:20.297 - 00:09:27.591, Speaker A: It's certainly going to be interesting if it lets you do things you could not otherwise do. Maybe it lets you do things on the computer in the sky you could not otherwise do, like interact with programs there in ways you couldn't otherwise. But for owning this data X, maybe you can exchange data X that you own for data Y that someone else has owned that you would rather have. Or maybe it allows you to do things you couldn't otherwise do in the real world. It could be something as trivial as, like the representation of a membership card that gets you into cool parties, a ticket, you know, into a concert, a pass to get into a gated discord channel to interact with your favorite musician. Or it can be some sort of representation of reputation because the computer in the sky attests to your ownership of something maybe others know to Take for example, content you create treated as more trustworthy than content created by someone who does not have that analogous ownership. In any case, the high level point here is just that once you have the right mental model for this technology, it starts becoming very easy to think of things you might do with it.
00:09:27.591 - 00:10:12.545, Speaker A: There's been nice applications coming out over the last few years, and I fully expect many more over the next five, ten years or so. But again, what I really want to focus on is the scientific and engineering challenges that come up when you try to build this build the computer in the sky. And I've done interdisciplinary work most of my career, specifically between computer science and economics, But I've never seen the synthesis of disciplines required to make this technology work to its full potential. The different parts of computer science it draws on have historically had nothing to do with each other. Draws on different parts of economics that have had nothing to do with each other historically. Different disciplines entirely. It's really, I gotta say, scientifically super, super fascinating trying to figure out how to build this thing.
00:10:12.545 - 00:10:35.345, Speaker A: I have limited time. I can't tell you about much of this. I'll focus on two things. First, I'll talk about some of my work on the mechanism design side. And then in the back half of the talk, I'll talk about some work being done by my research group at a16z crypto bringing proof systems to practice. Before I jump into the mechanism design part, I'm just. Quick comment.
00:10:35.345 - 00:11:12.029, Speaker A: Your first thought might be, look, we're basically like adding extra functionality to the Internet and the Web, we're adding sort of, if nothing else, this kind of notion of stronger ownership. Like there's already, even without blockchain protocols, there's already a ton of interesting mechanism and market design applications stuff for us to do. Right. So of course there's going to be anyways, right? I don't know. It's like what auctions Google and Facebook use to sell online advertising, how should Uber and Lyft do their congestion price, et cetera, et cetera. We're used to that. But let me point out all those examples are at the application layer of the Internet, stuff that operates on top, taking the functionality of the Internet for granted.
00:11:12.029 - 00:11:45.929, Speaker A: Right. If you're talking about the Internet infrastructure, right. So like 1970s, the decision to use BGP routing, the 1980s, the decision to use TCP congestion control. Right. As far as I know, no one in our community had a seat at the table when those design decisions were being made. And so what I want to highlight here is in fact an application of mechanism design that's at the infrastructure layer of blockchain protocols in web 3. Really sort of work done by our community which actually shapes how these protocols are going to work, which is something I don't think we've historically had in the evolution of the Internet.
00:11:45.929 - 00:12:24.375, Speaker A: So that's, I think, very cool. Obviously, there will be many more applications for mechanism and market design at the application layer, but this infrastructure layer application, I think is really pretty novel. All right, so we don't need to know that much about how a typical blockchain protocol works for this talk. Fundamentally, it's again, keeping track of the sort of evolution of this virtual machine. So fundamentally, a blockchain protocol needs to keep track of a running sequence of transactions. I want you to think about a transaction as a little snippet of assembly language code that gets run in that virtual machine. Okay.
00:12:24.375 - 00:12:57.945, Speaker A: And kind of all that matters for us right now is that, okay, transactions are added to this running log in batches known as blocks. And blocks can only be so big and they're only added every so often. So I'm going to use the Ethereum blockchain protocol as a running example. It'll become obvious why as I go along. That's the second biggest blockchain protocol behind Bitcoin, and it produces a block every 12 seconds. And a typical block might have 200 transactions. So we're talking about something like 15 transactions per second getting processed in the Ethereum virtual machine.
00:12:57.945 - 00:13:41.765, Speaker A: And for popular blockchain protocols and Ethereum certainly qualifies the demand for processing in that virtual machine is much, much, much larger than supply. Many more than 15 transactions per second would like to have the privilege of being executed in that virtual machine. So in this event, blockchain protocol has no choice but to make decisions about who gets in and who gets out. And if you want to treat this sort of, you know, as kind of neutral computing infrastructure, it makes sense to somehow include the most valuable transactions. And so that's what I want to focus on. That component of a blockchain protocol is something known as a transaction fee mechanism. That's what we're going to talk about for the next 15 minutes or so.
00:13:41.765 - 00:14:29.811, Speaker A: So what might a transaction fee mechanism look like? So let me tell you about a very natural first dab. This is how it's worked in Bitcoin for the entire 15 years of its existence. This is how it worked in Ethereum until a little less than three years ago, something called the first price auction. So you can't expect the protocol to magically know which transactions are valuable. So an obvious idea is to have users just say so a user will just include a bid along with this transaction stating how much it's willing to pay should that transaction actually be chosen for execution. Okay, who chooses transactions for executions? Well, in Bitcoin, Ethereum and many other protocols, basically one of the nodes running the protocol chooses is chosen at random. It's a little more complicated if you go to like proof of work, proof of stake, that's where this comes in.
00:14:29.811 - 00:15:13.749, Speaker A: Doesn't matter for us. One of the nodes running the protocol is chosen at random and they unilaterally get to decide which transactions go in the next block. Okay, go in the next batch and then they collect the bids for any transactions that they include. Okay, so users submit transactions with bids. Should the transaction be included, that payment is transferred to the node that chose to include that transaction rather than others. Now if you watched even sort of one lecture on auction theory, you know that first price auctions, while natural and sort of pretty prevalent in practice, they have for sure one big flaw, which is the cognitive burden as a user trying to figure out how to bid is non trivial. You should never bid your actual value.
00:15:13.749 - 00:15:50.667, Speaker A: You should always shade your value. How much should you shade your value? That depends on how much competition you have. So you have to reason about who else is bidding in the auction with you and how much they value it to figure out what the optimal bid should be. And that's actually a pretty non trivial calculation for most of us. In most Cases, We've known that forever, okay? And it doesn't get any easier just because we're working in a blockchain context. In fact, maybe it even gets worse, right? This is a snapshot of the most popular wallet for the Ethereum protocol. So wallet is the software which is run client side, which kind of, as a user helps you sort of submit an appropriate transaction to the protocol.
00:15:50.667 - 00:16:36.095, Speaker A: This is what MetaMask as well looked like in 2020. And as you can see, like, they have smart people working at MetaMask, they have algorithms which are trying to figure out what would be an appropriate thing to bid in this first price auction, given what's been happening lately. And they give you some options. But again, it's hard to figure out how to bid first price auctions, sometimes they recommend a price that's too high and you overpay. More annoyingly, sometimes they recommend a price which is too low, in which case your transaction doesn't get included at all, even though you would have been willing to pay the going price. And then what's worse, the way Ethereum sort of works is that transaction would sometimes hang for like 30 minutes and block you from doing anything else on the blockchain protocol. It was really kind of disastrous user experience back in 2020 with the first price auction.
00:16:36.095 - 00:17:32.207, Speaker A: So given that, the obvious question to ask is, can we have a smarter, more sophisticated, better for users transaction fee mechanism? Most extreme, the holy grail here would be sort of the same kind of properties we get from a second price auction or a victory auction, which I hope most of you have seen before. That's an auction which is truthful in the sense that it's always a dominant strategy. It's always optimal for you to just figure out the maximum willingness to pay and report that to the mechanism. So one question would be, can we have a better transaction mechanism which is truthful in the same sense of a second price option? So I hope this seems like a really basic question, right? So demand exceeds supply in a blockchain protocol. Gotta decide who gets in, who gets out, gotta figure out who pays what, hopefully. That seems like a very basic mechanism design problem. It turned out at the time I did the work I had described, which was in 2020, it was also a very sort of timely and practical question.
00:17:32.207 - 00:18:19.137, Speaker A: Okay, how did I know it was a timely and practical question? Well, honestly, it was like the practitioners kind of just told me so. So let me tell you a little bit about the backstory here. So I think it was around like 2017, a time at which, you know, I was super focused on my research and I was just kind of focused on writing research papers. I mean, I still am, but it was kind of even more back then. And I started getting these like phone calls I'd never really gotten before. I guess 2017 was also long enough ago that I answered my phone with positive probability, which is probably no longer true. But in any case, back then I would sometimes answer it and it would often be some kind of young 20 something entrepreneur with like a four person startup.
00:18:19.137 - 00:19:13.495, Speaker A: And they would say very excitedly, they'd be like, we need exactly your skill set for our sort of new crypto project. We need a computer scientist and with a PhD in computer science, but who also deeply understands mechanism design. And for me at that time, this was sort of coming out of the blue, out of the field, right? It's true there was a lot of demand for computer scientists who know mechanism design from like big tech like Google, Facebook, etc. But not a lot in my experience, at least from the startup world. And like, I wasn't really paying attention to the crypto world at all at that point. And honestly, like, I was pretty skeptical, right? Some kid just like calls me up, I'm like, how would you know what my skill set is? And how would you know if my skill set is like helpful for your project basically, right? And usually how do they even know who I am usually? It just so happened they stumbled on my YouTube lectures forever the game theory. In fact, usually the only reason they'd ever heard of the phrase mechanism design was because it's in lecture two of the 20 lectures in algorithmic game theory.
00:19:13.495 - 00:19:46.753, Speaker A: So they would sort of call me up and you know, honestly in 2017, I kind of ignored them all. I didn't really take it very seriously. I, you know, part, I just had a lot of other stuff I was working on that was exciting. In part, I wasn't convinced the work was going to be sort of interesting or deep enough to be worth it. But it did kind of plant a seed, honestly, kind of in two ways. Like one is just, I don't know, when you're a theoretician like us, it's just not every day that there's this kind of like organic demand for your exact skill set coming from somewhere else. And so part of it was like, well, maybe the universe is like trying to tell you something.
00:19:46.753 - 00:20:27.135, Speaker A: You should try to pay a little bit of attention. The other thing that sort of occurred to me was like, look like, you know, the biggest theme throughout my entire career has been this kind of focus on computer science applications that benefit from non trivial economic or game theoretic reasoning. And one thing that's been fun about sort of having that consistent focus is it's naturally led me on this journey from like application domain to application domain, network routing and design, you know, online advertising options, spectrum options, et cetera. And so it sort of occurred to me maybe like blockchain protocols and their applications could be like a really interesting application domain for that exact interface. Right. And I now know it definitely, definitely is. But probably the first time that occurred to me was at this time in 2017.
00:20:27.135 - 00:20:49.285, Speaker A: A few of you saw that. Excellent. So fast forwarding a little bit. This is now early 2020, when members of the Ethereum community reached out to me. It wasn't a phone call, it was an email, so actually responded to it. Okay. Okay.
00:20:49.285 - 00:21:17.241, Speaker A: So members of the Ethereum community reached out to me with exactly the problem I just mentioned. Right. So they had been running this first price auction up to that point until early 2020. And in fact there was a really quite polarizing debate happening in the Ethereum community at that point. Okay. In 2020, because it turned out there was a new proposal for a more sophisticated transaction fee mechanism. Some people loved it, some people hated it, lots of people just didn't understand it at all.
00:21:17.241 - 00:21:43.013, Speaker A: And they wanted my help assessing whether or not it was a good idea to switch to this new transaction fee mechanism. Okay. Which I'll tell you about either on the next slide or shortly thereafter. Okay. And the Ethereum community is sort of very much in the spirit of sort of strong open source. So they would really do a great job of when something is polarizing, sort of trying to come to a community consensus. Right.
00:21:43.013 - 00:22:35.845, Speaker A: That's part of why they sort of reached out to me. They also did this sort of survey of various stakeholders in the ecosystem. Again, this was done in 2020, Tim, bygone, being kind of a key member of the Ethereum community. And they said, Look, EIB 1559, which is the new transaction fee mechanism, they're thinking of switching to thumbs up, thumbs down if you don't like it, what can we do to make you more comfortable with maybe making this big switch? And the respondents said a few things, but the thing I want to highlight is this underlying part, which is the lack of formal specification or analysis of exactly what this mechanism has. And I want to emphasize that none of the respondents in the survey are the kind of people that write research papers. These are people that, you know, were miners. This was Back in the proof of work days in Ethereum, or they sort of ran wallet software like Metamask that we talked about.
00:22:35.845 - 00:23:26.763, Speaker A: They're all people building things in the Ethereum trenches, right? And there it is, like in writing, there was really a visceral hunger for exactly the kind of rigorous research that people in this room do. And this is definitely one of my biggest reasons, motivations for spending a chunk of my life in 2020 working up this project. All right, so that's kind of the setup. You know, think about the problem, like it's totally reasonable to kind of say, like, surely this is like, well solved, like whatever you have, like finite box space. Some stuff gets in, some stuff gets out. You gotta figure out how to price people like, whatever, right? Open up a textbook, copy, paste an appropriate solution. And I think that's a fine sort of initial instinct, right? But as often happens, once you sort of delve into the idiosyncrasies of the application, you realize there's other parts of the problem that you hadn't been modeling previously.
00:23:26.763 - 00:23:56.837, Speaker A: So I actually wrote a blog post, 8 Reasons why Mechanism Design for Blockchains is Hard. 8 Things that Are sort of constraints on what you do that are not there in more traditional applications of mechanism design. Transaction Mechanism design illustrates many of these eight. Not all of them, but many of them as we'll see. So let me spell those out on this slide. I'm going to have a list of challenges. The first challenge I've labeled challenge number zero because there's nothing special about blockchain.
00:23:56.837 - 00:24:34.263, Speaker A: It's just the problem you always have when you're designing a mechanism, when you're doing mechanism design, the problem is you don't know what people want. Like, you don't know how much people value their transactions. You wish you knew people's preferences because then you could try to make a decision that was good for everybody. So you try to elicit information about their preferences and then make a good decision. And what you're always worried about is that people are going to try to game your mechanism, that people might misreport what they actually want, because it's going to trick you into choosing an outcome which is actually better for them than it would have been had they just reported honestly. Okay? So that's always the challenge in mechanism design. And we have language saying mechanisms that do not have this property that are not gammable in this way.
00:24:34.263 - 00:25:27.727, Speaker A: Okay? Sometimes they're called truthful mechanisms, or probably usually call them basic mechanisms, dominant strategy incentives compatible. But again, that's just the same thing as a second price option, same thing as a Vickery option to the VCG mechanism. So the next couple of challenges are unusual compared to traditional mechanism design. Remember at the beginning when I said, how do these things work? I said, how is the next block added to the running log? One node running the protocol is chosen uniformly at random and gets to have unilateral control over that block. Okay? So in other words, you have no idea who it is who's actually going to be carrying out your mechanism, who's choosing who gets in and who gets out. So, you know, as a theoretician, you can kind of sit in your armchair and like write out this description of this really cool mechanism. Oh, it'd be really cool if running the protocol sort of included these and excluded these and et cetera.
00:25:27.727 - 00:26:21.551, Speaker A: But they can do whatever they want, okay? And that's not normal compared to like a spectrum auction where the government is running some auction that was agreed upon long. So when you design your mechanism, you have to take into account not just the incentives of the user, but also the party responsible for carrying out that mechanism. Okay? This node chosen at random to decide upon the next block. So that's going to be part of this MMIC constraint, which I'll talk about. The other thing that this randomly selected node or block producer can do is it's fully capable of just, you know, inserting its own transactions if that would, for example, will increase the revenue that it makes and just sort of looking ahead a little bit. This is exactly why second price auctions become so problematic in a blockchain context. If I get a bunch of transactions with bids like 10, 8 and 6, I can just make up a transaction with a bid of like 10 minus Epsilon to make sure that I get basically the revenue of a first price auction rather than a second price auction.
00:26:21.551 - 00:27:05.603, Speaker A: So that will also be part of the second set of incentive compatibility constraints. This MMIC definition, finally, and this is one of the sort of challenges I highlighted sort of in that blog post, was collusion is sort of much more of a concern usually in a blockchain setting than in traditional applications of mechanism design. You know, one reason for that is just it's easier to sort of coordinate through the smart contract functionality that blockchain protocols offer. Another reason is just that, you know, you kind of just don't. You can't lean on the rule of law as much as you can in a traditional mechanism design setting, right? Like in a government spectrum auction, basically all the participants, you just make them sign a contract and say, don't collude. And if you collude, you're going to get busted and there's going to be punishment. And there's not really sort of a great way to implement that idea in a permissionless blockchain context.
00:27:05.603 - 00:27:51.091, Speaker A: So there's this much stronger incentive to try to design mechanisms which just automatically within the protocol, discourage collusion. And that was a key design decision, as we'll see behind EIP 1559. And so in my work, what I did is so again, the DSIC constraint is old. The MMIC and OCA proof notions are new, new to this work, necessary to capture the idiosyncrasies of the blockchain application. And then the theoretical question is, could you have a transaction fee mechanism that satisfies all three incentive compatibility guarantees simultaneously? All right, so I have a couple slides with more details on the two new incentive compatibility notions. I'm not going to really read through these. I think both of these are kind of exactly what any of you would write down, given what I've already told you in English.
00:27:51.091 - 00:28:29.297, Speaker A: So mmic, again, that just says, you know, you can write down instructions for what this randomly selected block producer is supposed to include or exclude, but it's going to do whatever it wants. So you better write down an allocation rule, you better write down a mechanism that's in its best interest, that's actually revenue maximizing for the block producer. Moreover, you should disincentivize the injection of any fake transactions. And as we mentioned, the whole reason you're never going to see a second price auction or a VIC RE or VCG mechanism in this talk is because of this. It's very easy to manipulate through these injected transactions by the block producer. The other new incentive compatibility notion is OCA proofness. This is the one about collusion.
00:28:29.297 - 00:29:15.839, Speaker A: So collusion specifically between the block producer, the person selecting transactions, and the users. And so this just says there should not be an incentive to collect, so the group will not be better off to collude in. And again, the main thing I want you to take away from this is I again want you to notice that there's a format that you'd be tempted to copy and paste from a textbook that you can't exactly because of this constraint. And that's the idea of using a reserve price or a minimum bid. So to see the problem, imagine you tried to say, hey, let's use a mechanism where there's a reserve price of $10. Anyone who bids less than $10 is automatically ineligible. Now Consider a situation, right, where you've got a whole bunch of users, all of them value their transactions at like seven bucks, right? And then you've got this block producer responsible for choosing who gets in and who gets out.
00:29:15.839 - 00:30:02.187, Speaker A: If the block producer does what they're supposed to do, they're supposed to say, sorry, everybody, nobody's above the reserve price. I'm just going to publish an empty block and make no revenue notice. Everybody would be better off if instead the block producer said, hey, everybody, just between you and me, make sure you bid $10 to the blockchain protocol so that you're eligible for inclusion and I'll refund you 250 off chain through a side name. So using reserve prices, at least in the most naive way, you cannot do because of this collusion resilience requirement. All right? So that gives us three dimensions along which to assess a tfm, dsic, mmic, OC approved these three notions of incentive compatibility. We'd like all of them. So now let's actually turn to the challenger in the Ethereum TFM battle, right? The Incumbent is that first price auction.
00:30:02.187 - 00:30:42.385, Speaker A: The challenger is this newer proposal, EIT1559. Not my proposal. It was proposed by Vitalik Buterin, who's also the lead founder of the original Ethereum Protocol. I would say three big ideas here. Idea one is to actually use a reserve price. And you're like, wait a minute, I just said you couldn't do that because you'd be sort of susceptible to collusion. Okay? I said you'd be susceptible collusion if you do the obvious thing with the revenue generated from the reserve price and you pass that on to the producer of that block, it turns out if you redirect that revenue elsewhere, and in particular, what this mechanism does is just literally burns that revenue, removes those coins from circulation.
00:30:42.385 - 00:31:20.131, Speaker A: Then in fact, as we'll see, you can have collusion resilience. So that's idea number one, A reserve price. Transactions that bid below this or ineligible revenue generated by this reserve price, this base fee, are burned. So the next question is, where does the reserve price come from? And the idea here is pretty simple. You're going to use local search. So if the reserve price currently looks too high, you're going to lower it, and vice versa. How would you know? How would you know if the current reserve price is too high or too low? Well, the second change to the Ethereum protocol was to rather than just have a hard cap on how big a block could be of 200 transactions, you made it A soft capacity.
00:31:20.131 - 00:31:56.391, Speaker A: So you said, let's have a target of 200 transactions per block, but we're willing to go up to 400 if we really need to. And so now you can use the size of previous blocks as your sort of local on chain signal about whether to raise or lower the reserve price. The last block was too big, 300 transactions, then you should raise it. If the last block was too small, like 100 transactions, then you should lower it. So it's just constantly doing local search every 12 seconds as these options get getting run over and over again. Finally, there's kind of like an emergency backup first price auction. And so the way that's implemented is users or users always have to pay the base fee, the reserve price.
00:31:56.391 - 00:32:32.789, Speaker A: No way getting around that. Optionally, they can pay some extra above and on top of the base fee, which then gets transferred to the block producer like bids did back in the first price auction. Okay. And so the idea here is if for some reason bidders need to compete this bidding mechanism, this bidding is the mechanism by which they would do that. So we've got our criteria, our three notions of incentive compatibility, the one old one, the two new ones, we've got a formal description of our challenger. So now I can ask how well does it do? Okay, well, how well does the incumbent do? The first price option? Okay, definitely not truthful. Never bid your true value in the first price option.
00:32:32.789 - 00:32:59.497, Speaker A: You always want to shade your bid. So it's never truthful, never desig, not hard to see. It satisfies the other two properties, part of the reason that it's done well so far. Meanwhile, you can prove that the EIP59 mechanism. So first of all, honestly, this is kind of a lot more satisfying because it's just really tailored to the blockchain setting. First price auctions, you could make up that mechanism for any number of applications that have nothing to do with blockchains. You would never come up with the IP 1559, unless you were thinking about blockchain protocols.
00:32:59.497 - 00:33:44.219, Speaker A: You're exploiting the fact that one of these is run every 12 seconds, so you're getting constant feedback about what the reserve price should be. You're also using the fact that the blockchain has a native cryptocurrency eth, which is what allows you to credibly burn the revenue generated by the base fee in exchange for taking advantage of these kind of unique aspects of the blockchain setting. Indeed, you get a Pareto improvement over the incumbent. So EFE 1559, it still has those same MMIC and OCA proof guarantees, but now it has DESICC almost all the time in a formal sense. So there's a particular edge case where EIP 1559 is not desicc. An edge case where the reserve price for whatever reason is just way too low. Then it basically gracefully reverts back to a first price auction.
00:33:44.219 - 00:34:05.605, Speaker A: But outside of that edge case, it is indeed a truthful mechanism. Compare that to the first price auction, which is literally never truthful at all. So that's a sense in which it's a strict improvement in incentive compatibility. Obviously you'd like to get rid of that. Usually it turns out it's sort of a new result joined with Elaine Shi and Hao Chung shows that you can't. Okay, so this will appear in EC next month. So you really.
00:34:05.605 - 00:34:44.467, Speaker A: So in some sense EIP1559 is optimally incentive compatible. So it's a very strong sort of theoretical justification for potentially adopting that mechanism. I want to keep this talk mostly focused on the theory, but I do just want to sort of quickly say, and we'll see another example of this at the end of the talk, the pipeline from theory to practice is kind of as short as I've ever seen it in this area right now. So it's pretty wild to be working at it, actually. So EIP1559 was indeed adopted. Right. So my report came out in December 2020 and Ethereum did a sort of upgrade or hard Fork in August 2021, and EIP1559 has been happily running ever since.
00:34:44.467 - 00:35:10.361, Speaker A: Okay. Obviously I don't know the counterfactual. I don't know what would have happened if I hadn't run my report, but certainly many members of the community have said that my report was important for getting support for the change kind of over the line. And certainly it was the standard reference once there was a. When there was a lot of media attention coming out around the time of that. Of that switch. There's even like some cute little dashboards which are sort of tracking how much ETH has been burned over the time because of the switch to EIP 1559.
00:35:10.361 - 00:36:01.363, Speaker A: People get kind of into sort of following that. So in the remainder of the talk, I want to say a little bit about a different ingredient that goes into making the computer in the sky the best version of itself. And it's something that really touches on something very core to the theoretical computer science community, namely proof systems. So I'm going to talk a Little bit about that. This is not my work, this is not my expertise, but it is work that's done in the lab that I direct at a 16Z crypto. So we mentioned this in the previous part that the, the computer in the sky, or at least the version put out by the Ethereum protocol, is slow, 15 transactions per second. And for this technology to really realize the full vision of what it can achieve, that number needs to get a lot bigger, right? Certainly two orders of magnitude for sure.
00:36:01.363 - 00:36:49.251, Speaker A: Ideally three or even more orders of magnitude bigger. That's where we need to get to. So what's the plan? Okay, how are we going to get there? How are you going to get much more powerful computer in the sky? Well, a lot of people, a lot of smart people have been working very hard on this for several years and there's a bunch of competing approaches. So I'm now going to sort of walk you through kind of a decision tree, okay. Of different approaches to how you would scale up this computer in the sky. We'll eventually end up at a leaf of the decision tree where proof systems are kind of both necessary and sufficient for it to actually be a viable scaling strategy. But I do want you to know, as we walk through this decision tree, this journey is actually going to give you a pretty accurate map of exactly where the blockchain infrastructure industry is right now.
00:36:49.251 - 00:37:13.951, Speaker A: Like every branch of this tree, there are like multiple projects with dozens or hundreds of really smart people trying to get it to work and tens or hundreds of millions of dollars of funding to help them do it. Okay. So there's really kind of just a big competition about all of these different ways. I have no idea which way is going to win. Honestly. I kind of, at this point it just feels like I have a front row seat at this very evenly matched kind of sporting event. And it's actually extremely entertaining, to be honest.
00:37:13.951 - 00:37:52.595, Speaker A: It's very, very fun. So top note of the decision tree, how are you going to make this computer in the sky faster? In some sense, the most direct way of doing it is like, well, let's just kind of improve its hardware and its software. In effect. Remember I showed you that sort of five by five grid of the physical machines sort of running this blockchain protocol. You can say, well, first of all, make sure those physical machines are really, really powerful, like big beefy servers with really, really good Internet connections. And secondly, do some really, really cool, cutting edge computer science to make the consensus protocols as powerful as possible. So that's approach number one.
00:37:52.595 - 00:38:19.789, Speaker A: And actually many of the more recent generations of blockchain protocols from the last, say three or so years are taking approach. Solana is probably the best known one, but there's also some other very promising ones. So that's one idea. The other branch of this decision tree is like, no, no, no. And this is the, this is the route Ethereum has taken. Let's go ahead and embrace the idea that we have this very slow computer in the sky and let's scale it up through a hierarchy. So let's add layers.
00:38:19.789 - 00:39:11.191, Speaker A: Let's supplement the slow computer in the sky with fast computers that are sort of on the, the periphery. So that's what I'm showing you here with these spokes around the sort of basic Ethereum computer in the sky. So in this context, these spokes, these are sometimes called L2s and then the original Ethereum protocol, the slow one, is called the layer one. Or sometimes these are called rollups because basically you're sort of rolling up a whole bunch of computation you would like to do on this computer, doing them, shipping them off, and doing them instead over here. Okay, so rather than. So the sort of general model here would be that users would not interact directly with layer one, like with the Ethereum protocol, they would interact with one of these L2s, and it's only the L2s that would interact with the layer ones. Okay, so this is the branch of the decision tree we're going to go down next.
00:39:11.191 - 00:40:01.371, Speaker A: So I'm talking about different ways you can try to do this, scaling through the addition of sort of supplemental off chain. So notice these folks. I've drawn these as laptops without a cloud. So these may or may not be blockchain protocols in their own right. For simplicity, think of this as actually just being like a big server owned by a single company, at which point, you know, and that's where users by default are submitting transactions, right? At that point you're kind of saying like, well, what was the point of this whole endeavor anyway? Like I thought the whole reason we built this is so that like no one could turn it off. And it was like very reliable, you know, and it had no owner. Now you're telling me users are just going to interact with these sort of corporate owned single machines anyways, like, why are we doing this? And the vision is to try to get the best of both worlds, right? That 99% of the time, or 99.9%
00:40:01.371 - 00:41:00.507, Speaker A: of the time, right, a user is just happily interacting with sort of the big BP server that's doing Everything that's supposed to be doing so it's the exact same user experience you'd have today with any sort of typical sort of web application. On the other hand, the layer one will serve as a safety net, okay, as a sort of fallback mechanism should anything go wrong with this L2 machine, which again is just a simple machine. So this company could go out of business, it could just sort of crash and go offline, who knows. So the idea is the layer one will serve as sort of the safety net for the kind of safe operation of all these spokes at the edges. Okay, now, so what's the interaction look like between the L2 and the layer one? So the first idea is just the L2s are going to just periodically notify the layer one of their state. Okay, just to be clear, this is the same computer this guy before has its own operating system, its own processor, its own memory address space. Same thing for all of these.
00:41:00.507 - 00:41:29.021, Speaker A: All of these also have their own state, their own minimum state. So each of them will periodically just notify the layer one. We'll say, hey, here's the current state of this machine, all right? It's not going to actually send that, that would be too big. It'll send a cryptographic commitment to its current state. So something like the root of a vertical tree. So all these L2s will be periodically submitted in cryptographic commitments to their current state to the layer one. And that also that already gives you some non trivial functionality.
00:41:29.021 - 00:42:17.803, Speaker A: So that allows you to be robust to crash faults to these. Layer two's kind of just going offline, like the company going out of business. Why? Well, a commitment to the last state of the L2 is sort of living here, ideally forever on the layer one. And so what that means is that if there's anybody that knows the history of transaction processing on this L2 that went down, they're in a position to reconstruct the last state of that L2, from which you know, you can fire up some other beefy server to resume operations where you left off. So because you have a commitment enshrined on the layer one, there's no way to fabricate a bogus states of that L2. But the only one you could possibly come up with would be the correct one, the one that's consistent with the commitment. You might ask who's going to know the transaction history of the layer 2.
00:42:17.803 - 00:43:11.027, Speaker A: Actually today it's sort of an extreme solution which is the most sort of popular roll ups. Literally just post a description of the transactions they process to the layer one, so they survive on the computer in the sky for everybody to see. So literally anybody can reconstruct the layer 2 state just from what everybody knows from the computer in the sky. In the future, you can imagine third parties providing the service instead. Okay, all right, so that's just to tolerate live mistake if a L2 goes away, as long as someone knows the history of its processing and can resume where it started. Okay, so that's again using the layer one as a safety net. But what about sort of safety failures? What if you actually have a malicious layer 2 node that's online but posts a commitment to a state which is not the actual state of that L2? And this is not hypothetical, right? I mean, these L2s in general are going to be hosting digital assets which are potentially very valuable.
00:43:11.027 - 00:44:20.865, Speaker A: So there's actually a very strong incentive for one of these L2s, potentially the post a commitment to a memory, to a state in which the operator of that L2 just so happened to wind up with all the assets of all of the users of that L2. So what's really important is that you can actually deal with safety failures that if one of these L2's posts an incorrect commitment, then in fact you should be able to detect it, correct it, etc. So we have another branch here in the decision tree, different ways to sort of deal with these safety violations. The one which is more common right now, if you've ever heard about optimistic roll ups, this would be that category. The one which is more common now is sort of a very quick and dirty approach, which is just kind of like a detect and then fix it after the fact. Because notice that again, under these sort of same assumptions, as long as someone's aware of the transactions that have been processed, anyone can catch this malicious L2 when they post sort of an incorrect commitment. If someone else also knows the state of that L2, they can notice that the commitment is not the correct one, and then you somehow takes pains to fix things after the fact? I don't want to get into details of how that's done.
00:44:20.865 - 00:45:24.237, Speaker A: Pretty much everybody agrees that if you put performance considerations aside, what would be much, much, much better would be to instead deal with this proactively. Rather than allow safety violations so bogus commitments and then fix it after the fact, why not just cryptographically prevent them from ever being posted in the first place? So ideally now each of these L2s would post along with its commitment to its current state, along with that, a proof that that Commitment was computed correctly that really it does represent the actual current state of the layer 2, as opposed to some validated model. The idea is that then this proof posted by the layer 2 to the layer 1, that would then be verified by the computer in the sky that would be verified by the layer one. Okay, the layer one could then just ignore anything posted to it that's not accompanied by a correct. Now, this didn't make any sense at all. Remember this layer one, this is supposed to be a really slow computer. These are supposed to be fast computers.
00:45:24.237 - 00:46:07.889, Speaker A: Not only that, but there might be hundreds of assassin computers all periodically put commitments to the layer one, right? Think of each of these as every minute or every hour sending something to this computer in the sky. So verifying better be very lightweight for the layer one. Okay? That better be much, much, much faster than it would have been to actually do that computation originally, right? Otherwise this makes no sense. We're trying to offload computation from layer 1 to layer 2. Verification better be way, way, way cheaper than actually doing the execution. And now at this point of the talk, I hope it starts, you can start to see why this is in the wheelhouse of proof systems and actually very specifically in the wheelhouse of snarks. So the S in snark stands for succinct.
00:46:07.889 - 00:46:31.123, Speaker A: That is exactly this property. That verification of the proof should be much, much, much faster than redoing the computation from scratch. The N in snark stands for non interactive. Right? And that is what we have really. We want the L2 to just post once and for all approval layer one. The layer one checks it and you're one and done, and you proceed. Okay? So just to be super concrete, this is the comp.
00:46:31.123 - 00:47:01.861, Speaker A: So for this vision to work, this is the computation that we need a snark for. Okay? So to describe this, I need to tell you sort of what's the witness. And I need to tell you the computation that gets done by the proverb given the witness. Again, just to remind you here, the verifier in this context is the layer one, which is slow. The prover in this context is the L2, which is fast and has more knowledge. So specifically, the witness here is supposed to be what the layer 2 knows. It knows the last memory state to which it committed.
00:47:01.861 - 00:47:43.185, Speaker A: It knows the transactions that are processed. The computation is just to process those transactions and compute the new commitment. Okay? So the corresponding worker root given the new state after the processing all volumes transaction. So this is what, this is the computation we need to start for and let me point out, you should think of this as really general purpose computation. Like if all of the transactions were just like payments from some Alice to some Bob, you could even talk about sort of specialized snarks. But for a turn, complete blockchain protocol like Ethereum, right? The fact that there's arbitrary transactions in here being executed, this is really arbitrary code. So you really need a snark specific, specifically a practice snark for general orbital computation.
00:47:43.185 - 00:48:23.335, Speaker A: And it's really not an exaggeration to say that this approach to blockchain scaling scaling Ethereum is viable if and only if sufficiently practical snarks for this computation exist. A lot of people right now really, really, really care about scaling Ethereum. So a lot of people really, really care about whether sufficiently practical snarks for this computation ignorance exist. What is sufficiently practical? Things you might care about and care about sort of the work of the verifier or L1. You care about the work of the Prover or the L2. Because remember, this is not a metaphor. This is not a Merlin, this is not a wizard, this L2, it's a faster machine than the computer in the sky, but it's a physical machine.
00:48:23.335 - 00:48:59.063, Speaker A: There's only so much work we can ask it to do. Okay, so for 30 years we've known that snarks for general computations exist in principle. And this again, honestly, this is a statement that if you weren't trained in theoretical computer science, I think this would seem like totally nuts. This would seem like impossible, right? It's one though, if it's some like specific computation, right? It's like long division, you know, like maybe there's some way to have a proof which is much quicker to verify when you're doing it from scratch. Like this is arbitrary computation. I'll tell you nothing about it other than it's represented with a circuit of a given size. Can you really compress anything? Yes, you really can compress anything.
00:48:59.063 - 00:49:40.065, Speaker A: So Fagan, Killian and McCalley showed him to do that in principle. So the highlight basically is almost kind of follows lack of option PCP theorem. So the idea of what the proof looks like, so the prover sends a cryptographic commitment for probabilistically checkable proof of the computation that it's supposed to do. Now, normally to check a pcp, right, you ask a sequence of queries. Here you want something non interactive. So at the end you do something known as defeated Shamir heuristic, which takes a public coin interactive protocol and turns it into a non interactive protocol. Basically Rather than relying on a verifier to supply random challenges, you just rely on a cryptographic hash function to supply those challenges as a function of the previous messages of the proverb.
00:49:40.065 - 00:50:26.447, Speaker A: That is weird. Okay, so that's, that's the idea of fill in and macabre. And so for 30 years the question has been, can you make this practical? There's been a tremendous amount of work. Probably 20% of the people in this room have done work that have been tuned into this 30 year long line. And really just circa kind of now the answer starting to be yes, literally circa kind of now. Actually I was on a call with no earlier this morning and he literally said if snarks can be made practical, anything can be made practical. Good.
00:50:26.447 - 00:51:06.129, Speaker A: So really what you need. So lately the bottleneck has really been prover time. So you need a prover time that is not just polynomial time, but really linear time and you really care about the constants to give you a sense. If you had like a constant overhead of 1000, so doing a computation plus proving it correct was a thousand times slower than doing the actual computation, that would be really good actually if you had an overhead of 1,000, overhead of a million, not useless, but not that good. Overhead of a billion, basically useless. And that's the constant in the linear time. So there's been a bunch of breakthroughs by many different groups over the last couple years that have been making great progress.
00:51:06.129 - 00:51:36.973, Speaker A: I'm going to just focus on one of them. Okay. It's the one that, you know, full disclosure is the one that came out of my research team. But it's also, I think, you know, one of the ones with the most momentum, one of the most promising approaches. And obviously I happen to do this very quickly. But one insight is that a lot of the concrete inefficiency of the Figa Macaulay, this is reflecting my approximation happens background. So Killian McCall approach is that in some sense you redundantly remove interaction.
00:51:36.973 - 00:52:15.871, Speaker A: At the end of the day you need a non interactive proof. And that's why at the end of the day they use the fusion of heuristic to not do the sequence of queries of PCP. But if you remember the origins of PCPs themselves, they in some sense are also meant to be removing interaction with some like prover which you've then since gotten rid of. The PCP is almost kind of just enumerating what a prover would have answered to for every possible challenge of verifier might have asked it. So that can also be thought of as an interaction removal set. And also they're kind of like different in character, right? Because like that TCP removal of interaction, there is this enumerative character, like you're thinking about like everything you might have asked and answering them all. Whereas speech, it's not like that.
00:52:15.871 - 00:53:09.961, Speaker A: Right. It's literally you only bother to answer the challenges. You have to given the output of the cryptographic hash function on the various sets of Kruger messenger. So the idea then is, given that we're going to use fiat Shamir at the end anyways, why not just really lean into interaction and sort of just maximize the power of interaction until the very end, until the final fiat Schmier step. So we want to ask ourselves, what would be a tool that really showcases the power of interaction? So I think I tackled that question. So I've been around a while, not as long as some of the people in the room, granted, for sort of like 25 plus years. And so I've seen a lot of different scientific communities, both in computer science and outside, kind of evolving parallel over that time.
00:53:09.961 - 00:53:54.455, Speaker A: And honestly, some have done much better than others. Okay. And it's interesting to think about the reasons why. One is obviously just the quality of the work, and obviously the stockbox community scores as high as you can score on that metric. But other things matter too. Another thing I think this community is really good at is sort of celebrating the past, recognizing that the great work that we're doing right now builds on stands on the shoulders of several generations of brilliant theoretical scientists that came before us. So I think in general, we do a good job of telling the stories, encouraging our students to go back to the original sources, learn the motivations for all the definitions that we now know and love, understand how we got from there to here.
00:53:54.455 - 00:54:59.397, Speaker A: And because of that, sort of counting on this fact that I think we're very strong in the lower category, theoretical science, that when I say what would be a tool that showcases the power of interaction? Actually, a lot of you will think about the same thing that I'm thinking about, which is the sequence of ideas in 1989 and 1990 that led to a bunch of huge complexity theory breakthroughs, culminating in IP equals p space. In particular, I want to highlight a key technical tool in this whole line of arguments, which is the sum check protocol. So the sum check protocol, for those of you that remember it. So that's basically a way to do discrete integration of a multivariate polynomial. The bad news about the subject protocol is that it needs many rounds. It provably Needs many rounds, one round for each variable in the multivariate polynomial. The good news is that at least if the polynomial is low in each degree, the prover does not have to send much to the verifier in each round.
00:54:59.397 - 00:56:01.435, Speaker A: So it's very sort of communication efficient. So it's long very communication efficient in each round overall. And so indeed the approach towards snarks which has come out of my group just very recently, this is exactly the philosophy to lean into the sum check protocol in some sense maximizing the power of interaction all the way up until the final fiat shamir step. In effect, given that we're sort of going to be doing some check, they're going to be doing some check and then passing the fiat shamir. The fact that there's so little communication from the prover to the verifier and each round of the sumcheck protocol winds up translating at the end of the day to a non interactive proof where the prover does not have to cryptographically commit to very many things. As it turns out, in the modern approaches to snark design, a lot of the prover work actually comes from the number of cryptographic commitments that they have to make. So if you have an approach which substantially reduces the number of field elements you have to cryptographic command into really good chance, that's when we translate into a commensurate speed up in the proof of time.
00:56:01.435 - 00:56:39.657, Speaker A: So that's the key idea of time's new proof system, which came out in two companion papers, Lasso and Jolts. These both appeared in Eurocrypts and they just were presented literally two months ago. So Justin Thaler, so he's full time on the research team at a16z crypto. Srinath said he's a researcher in Microsoft research assistant professor at CMU. Arzu Arun just finished his PhD at NYU and was our intern at a 16z crypto two years ago, which is when a lot of these ideas sort of got finalized. Okay, so this is 112 pages of hard theory. Right? The kind of hard theory that people in this room do.
00:56:39.657 - 00:57:14.471, Speaker A: It's a little unusual in that it's hard theory where the exact goal is to try to say that instructions are going to give you things more practical than we have before. Right. And it's hard to really convincingly make that case unless you build it. So someone would have to build this 112 pages of art theater. And so we actually did that. Okay, so in addition to the research team at a16z crypto, we have an amazing engineering team. And two of the engineers like Michael Zhu and Sam Langstale spent about six months turning these proof systems into something, into something weird, into something you can use.
00:57:14.471 - 00:57:38.955, Speaker A: It's open source. This is the GitHub repo over here. It has been benchmarked and compared to other state of the art, and it is at worst competitive with the other leading approaches and at best superior. Again, I do want to say this is not the unique breakthrough that's happened recently. There's a bunch of different projects sort of leapfrogging each other. So basically in a very heated race. Again, it just feels like having a front row at a very front, sporty event.
00:57:38.955 - 00:58:19.191, Speaker A: But this is real. Okay? So this really hasn't been built and it really is the original hope that really leading into interaction the entire time, maximizing new construction actually really does lead to significantly faster proverb times and therefore gets us closer to that full vision of a verifiable cost. Again, sort of surprising how much relatively mainstream media attention sort of get sort of working in this area. True to form, the media attention is often like very inaccurate. I don't know why, but when people think about snarks, they always want to throw in the acronym zk whether there's zero knowledge or not. Right. My focus here has really been on the S, the succinctness.
00:58:19.191 - 00:59:02.891, Speaker A: But people just love the acronyms zk. Everyone who's stop box trained is fighting what I fear is a losing battle trying to get people to use these terms correctly. But we're trauma. We're really out there trauma. So I'm almost done. You know, you look at this slide, I hope the people in this room, I really think you should feel a sense of pride identifying as a theoretical scientist, as a member of the stock Fox community. Like, this is very unusual, right? This split screen where you have on one hand, like extremely scientifically, mathematically deep theory and almost kind of in real time.
00:59:02.891 - 00:59:24.821, Speaker A: Right. That's simultaneously fundamentally shaping the trajectory of what might really be the next generation of the Internet. It gives us just much stronger property rights for digital assets than we've ever had before. This is very unusual. Okay. And I don't think this is an accident. I think there's sort of unique features of the stock Fox community that makes this possible.
00:59:24.821 - 01:00:23.435, Speaker A: One, honestly, is just intellectual horsepower. Like, I don't know of other rooms I would walk into that would have the degree of intellectual horsepower as this one, but it's more than that. So I talked about lore earlier, but, you know, sort of broader than that is really, I'd say culture. I'd say TCs. You know, for all the amazing sort of definitions, theorems, proof, techniques that have come out of this community, I think what one of the reasons it has remained so healthy and vibrant for my entire career has been the culture. For example, I think on the one hand, we hold ourselves to almost impossible standards of science, just extremely, extremely ambitious, while at the same time, I say this from experience and some of the came up in this community, very nurturing and sort of mentoring of the younger generations. And I see, as I always do at this conference, a tremendous number of students in the audience who I'm sure, as always, are being well taken care of by this community.
01:00:23.435 - 01:01:04.525, Speaker A: And so, speaking of TCS culture. So just to wrap up, like everyone in the room, right, I'm sure you were ecstatic to hear, like I was, that this year's Nobel Laureate is Avi Victorson. Avi obviously more than deserves the award purely for all of his incredible scientific contributions, no doubt about that. But in my mind, sort of almost or equally as inf important has been the contributions to TCS culture. I can't think of anyone that has had a more positive and lasting effect on TCS culture and the health of this community than Avi. So I just wanted to wrap up by saying, Avi, congratulations on the Turing world lecture. Very well deserved.
01:01:04.525 - 01:01:12.025, Speaker A: Everybody. Enjoy the week. I've seen a lot of scientific communities and none of them are quite like this one. Thanks very much.
