00:00:00.490 - 00:00:27.558, Speaker A: Hi everyone, and welcome to this video that accompanies section 20.2 of the book algorithms illuminated, part four. This is a section about greedy heuristic algorithms for the maximum coverage problem. So imagine you've been put in charge of assembling a team. Maybe it's a team to complete a project at work. Maybe you just want to assemble a good team for your upcoming fantasy sports league for this next season. In any case, you have a budget can only hire a limited number of people.
00:00:27.558 - 00:01:02.878, Speaker A: Meanwhile, the people you could hire have various skills. Maybe they correspond to programming languages in the work example or what positions they can play on the field. In the fantasy sports example, you'd like to hire a diverse team with as many different skills as possible. Whom should you pick? Well, this is exactly an example of the maximum coverage problem. Let's now define it formally. So in the maximum coverage problem, the input comprises m subsets. We're going to denote them by t one up to TM of some ground set u, as well as a budget, which is a positive integer k.
00:01:02.878 - 00:01:42.982, Speaker A: For example, in our team hiring example, you could imagine capital U as being all the sets of skills that someone could have or all of the possible positions on the field. Meanwhile, you're going to have one subset for each person you could hire, where that person is represented by the subset of the skills that they possess. K, meanwhile, is your budget. That's how many people you're allowed to hire. Now what's the objective? The objective is to hire k people so that you have as much different skills represented as possible. Or in set theoretic terminology, you want a collection of k of these subsets so that the union of those subsets covers as many elements of capital U as possible. That's something known as the coverage of a collection of subsets.
00:01:42.982 - 00:02:16.274, Speaker A: So for example, for concreteness, you could imagine you're given an array of corresponding to the subsets and another array corresponding to the elements of capital U. And then you could have pointers going back and forth accordingly. So pointers from each subset to the elements it contains and then back pointers from each element to the subsets that contain it. So just to make sure that the problem is crystal clear, let's move on to a quick quiz. All right, so on the right part of the slide, I've shown you a possible input of the maximum coverage problem. So there were 16 elements. So all the little black circles, those the, those are the elements.
00:02:16.274 - 00:02:56.620, Speaker A: And you see there's a four by four grid of them. So 16 in total, and then the six different subsets, each with a different color. Now the final part of the input is a budget, the value of k. So let's say I'm going to let you pick four out of the six subsets. So the question then is of all ways of picking, choosing four out of those six subsets, what's the maximum possible coverage you can attain? So the correct answer is the third one. There's no way to cover all 16 elements using four subsets, but you can cover 15 of them actually in a couple of different ways. So you definitely want to pick the sets t four and T six.
00:02:56.620 - 00:03:26.256, Speaker A: They don't overlap at all, and together they already cover ten elements. Then you want to pick T two. For sure there's a couple of redundant elements, but it does cover four new ones, so that gives you 14 total. And now it doesn't matter, you can either pick T three or you can pick T five. Either one is going to get you one additional element for a total of 15. One thing to notice is that you never want to bother picking the subset T one. It's a big set and has six elements in it, but those elements are kind of already basically going to be covered by other subsets.
00:03:26.256 - 00:04:13.652, Speaker A: So T one, while large, is somewhat redundant, so that's why it doesn't show up in any of the optimal solutions. And in general, the reason maximum coverage problems are tricky is because of the overlaps between subsets. For example, some skills may be common, meaning that they're covered by many subsets, others may be rare, covered only by a few. So an ideal subset is large with few redundant elements, basically a team member blessed with many unique skills. So maximum coverage problems show up all the time, and not only in these team hiring applications we've been mentioning thus far. So for example, imagine you wanted to choose locations for KNU firehouses in a city in order to maximize the number of residents that live within 1 mile of some firehouse. This is exactly a maximum coverage problem.
00:04:13.652 - 00:05:08.072, Speaker A: The groundset elements correspond to the residents and each subset corresponds to a possible firehouse location with the elements of that subset the residents who live within 1 mile of that location. For a more complex example, imagine you want to get a bunch of people to show up at an event, something like a concert. You need to start setting up for the event and only have time to convince K of your friends to come. However, whichever friends you successfully recruit, they can then bring along their friends, and their friends will bring along friends of friends, and so on. So we can visualize this problem using a directed graph. The vertices of this graph are going to correspond to people, and we'll have an edge directed from one person V to another person W if W would automatically follow v to the event if V attends. That is, W is some friend of V that will come along with V if V decides to come.
00:05:08.072 - 00:05:42.900, Speaker A: So for example, consider this directed graph in blue with eight vertices. So for example, suppose you recruited vertex one. Well, then two would follow one to the event and then once two comes, three and five would follow as well. So if you recruit one, you're going to get this whole subset of four different vertices showing up at the event. Now, another interesting case is vertex six. You'll see that six has no incoming arcs. That means there is no one else capable of convincing six to come.
00:05:42.900 - 00:06:26.124, Speaker A: However, if six decides to come, then many will follow. In particular, eight will follow followed by five, and seven followed by three. So if you decide to recruit both one and six, you actually wind up getting seven of these eight people to show up at the event. And maximizing event attendance like this is exactly a maximum coverage problem. The ground set elements correspond to people or equivalently to vertices. Of this graph, there is one subset per person indicating who ultimately would follow that person to the event graph. Theoretically, the subset corresponds to the vertices reachable by a directed path from that vertex.
00:06:26.124 - 00:07:13.176, Speaker A: So for example, from this vertex one, you see that its blob contains all of the vertices that are reachable from one by directed paths. Similarly, the light blue blob that has all the vertices reachable from six via a directed graph. The overall attendance due to the recruitment of a set of k people is then exactly the coverage achieved by the corresponding k subsets. All right, so I hope this convinces you that the maximum coverage problem is a basic problem. It would be actually really nice to have an off the shelf subroutine that could just solve this problem for you really quickly. Unfortunately, and we'll see this in the fourth part of the video playlist, the maximum coverage problem is an NP hard problem, so we're going to have to compromise on something. And again, this is the part of the playlist where we're compromising on correctness.
00:07:13.176 - 00:08:01.324, Speaker A: So we're going to have a general purpose and fast algorithm, but it won't be always optimal. We'd love it if it was close to optimal, the same way our two scheduling greedy algorithms were in the previous section. That would be sort of the best case scenario, given that the problem is NP hard. Now, once again, just like with so many problems, greedy is a very natural place to start with the maximum coverage problem. Ultimately, we're responsible for outputting k of the subsets. So why not just build up our solution? Iteratively one set at a time, making a myopic decision with each of those k steps. Now, if k equals one, if you're only allowed to pick one subset, this is not a hard problem, right? If you can only pick one subset, the one you want to pick is the biggest one, because the coverage is just going to be the size of whatever set you happen to pick.
00:08:01.324 - 00:09:02.054, Speaker A: Already. When k equals two, things get a little interesting. So suppose you're allowed to pick two subsets and you've already committed to picking the biggest one, which one do you want to pick next? You could pick the second biggest one, but as we saw in the example, overlaps really matter. So what really matters is how many new elements does this next subset cover by? How much does it increase the coverage? So the most sensible greedy criterion would be to always choose the subset which covers the maximum number of new elements that were not covered by previous subsets. So that's a famous greedy algorithm for the maximum coverage problem. We're just going to call it greedy coverage for short. So the pseudocode of the algorithm is quite simple.
00:09:02.054 - 00:09:34.618, Speaker A: There's just going to be a simple for loop with K iterations, one for each of the subsets we need to pick, we initialize a set capital K. These are the indices of our chosen sets. Initially, it's empty. In each of the K iterations we pick one subset. Which subset do we choose? We just greedily increase coverage. So in the Jth iteration we pick the subset that maximizes the number of newly covered elements, meaning elements not already covered by the first J minus one subsets that we've chosen. I'm using the notation f sub cove here, which we introduced a couple of slides ago.
00:09:34.618 - 00:10:16.906, Speaker A: And again, that's just the size of the union of all of the subsets with indices in capital K. Also notice that in this argmax, this is the step which is sort of searching over all the subsets for the one that increases the coverage the most. This argmax is never going to be attained by a subset that's already in capital K, because that would increase the coverage by zero. So if it's clearer to you, think of this argmax as only being over the subsets we haven't already chosen. So as usual with greedy algorithms, the running time analysis is not so interesting, at least not for a straightforward implementation of the algorithm. So what would that look like? Well, you have each of these K iterations of the main loop. How are you going to implement one of the loop iterations? Well, you have to evaluate this argmax, so you have to take the best over the m subsets.
00:10:16.906 - 00:11:03.956, Speaker A: So you could just do exhaustive search over the m possibilities for the subset. For each of those subsets, you have to compute the increase in coverage that selecting that subset would give you. And you can do that just by visiting the elements in that subset one by one and checking which ones have already been covered and which ones have not already been covered. So that straightforward implementation. You'd sort of have a factor of K from the number of loop iterations, a factor of m from doing the exhaustive search over the m choices of the subset. And then also evaluating the coverage increase would take time proportional to the size of that subset. So this greedy algorithm may not be as blazingly fast as the ones we saw for our scheduling application, but still, it's certainly a polynomial time algorithm.
00:11:03.956 - 00:11:58.636, Speaker A: So the real question then is, as usual with heuristic algorithms, how good is it? How good is the solution output by this greedy coverage algorithm compared to what you could achieve in a perfect world using, for example, exhaustive search? Well, it's very easy to come up with a simple example where the greedy coverage algorithm doesn't do the right thing, where it potentially outputs a suboptimal solution. So, for example, on the upper right of this slide, there's an example with just four elements and three subsets. Think of the parameter k as being equal to two. So if I let you choose two of the subsets, it's clear you can cover all four elements if you make the right choice, if you choose t one and t two. But our greedy algorithm right, we don't know what it's going to do in the first iteration, at least assuming it break ties arbitrarily. So in the first iteration, it might well choose these sets t three. And at that point, there's nothing it can do once it's chosen t three.
00:11:58.636 - 00:12:32.980, Speaker A: Doesn't matter if it chooses t one or t two. In the second iteration, it's going to wind up covering only three out of the four elements. And if it bothers you that we're sort of assuming worst case tiebreaking by the greedy algorithm, rest assured that a more complicated version of this example shows the exact same thing, even with best case tiebreaking by the greedy algorithm. So once again, this is not a surprise. We are fully prepared for there to be examples where the greedy coverage algorithm does not cover as many elements as you could in a perfect world. Remember I told you this was an NP hard problem. We've seen that the greedy coverage algorithm runs in polynomial time.
00:12:32.980 - 00:13:08.112, Speaker A: So unless we plan on refuting the P note equal to NP conjecture, there must be examples where it does not output an optimal solution. And this is one of them. But as usual, the concern once you see a simple example like this is you're like, well, what's up? In more complicated examples, maybe things can get a lot worse, and at least if the parameter k is a little bit bigger than two, they can get somewhat worse. So let's see that in this quiz. So let's look at a more complicated example that has 81 elements. They're arranged into a nine by nine grid, and then there are also five subsets. The budget here is going to be three.
00:13:08.112 - 00:13:48.778, Speaker A: So you're allowed to choose three of the five subsets, and the questions are the usual ones. So first of all, in the best case scenario, what's the maximum coverage that can be achieved by any three of the five subsets? Secondly, what is the coverage that will be achieved by the greedy coverage algorithm? And I'm looking for an answer under arbitrary tiebreaking. All right? So the correct answer is the second one. Answer b. It's pretty easy to see that there is a way to choose three subsets that cover all 81 elements. Just cover t one, t two, and t three, the red, light blue and magenta subsets. And that's one thing the greedy algorithm might wind up doing.
00:13:48.778 - 00:14:25.094, Speaker A: But on the other hand, with arbitrary tiebreaking, for all we know, in the first iteration of the greedy algorithm, it's going to pick the set t four. That covers 27 new elements, just like t one, t two, and t three do. So t four is fair game, and the greedy algorithm might pick it. And then if it does pick the set t four, now it has to decide what to do next. So t five, the brown set, that would cover 18 elements, none of which were covered by t four. So that would be 18 new elements covered by t five. On the other hand, if you look at t one, t two, and t three, nine of their 27 elements are already covered by t four.
00:14:25.094 - 00:14:58.110, Speaker A: So picking any of them would only cover 18 new elements. So again, it's a four way tie. All of them are fair game for all we know. Greedy algorithm the greedy algorithm will pick t five, and if it does that now with its last set, it can pick either t one, t two, or t three. It doesn't matter which one it picks. And the overall coverage is 27 from t four plus 18 from t five, bringing the total to 45, plus another twelve from whichever of t one, t two, or t three it picks for a total of 57. So we've now seen two different examples.
00:14:58.110 - 00:15:39.258, Speaker A: One where two subsets could be chosen and the greedy algorithm might achieve only a coverage of three when four would be possible. So in other words, it might obtain only 75% of the maximum possible coverage, and then with k equal three. In that last quiz, we saw an example where it only gets a 57 over 81 fraction of the maximum possible coverage, which is roughly 70.4%. So hopefully the two examples we've seen so far, the one with four elements and the one with 81 elements, hopefully they kind of have a similar flavor. Like you see, there might be some sort of pattern emerging and you can imagine trying to extend those two examples for all values, all positive integers, k. And in fact, you can do that. I encourage you to work it out as an exercise.
00:15:39.258 - 00:16:12.490, Speaker A: For example, you can arrange a bunch of elements in a grid where the side length is k raised to the k minus one power. So that generalizes our two by two and nine by nine grid so far. So k to the k minus one power on either side, and you're going to wind up with two k minus one subsets. So we had three when k equals two, we had five when k equals three. And in general, you're going to have two k minus one subsets. And so if you generalize that example for all positive integers k, here is what you learn about the greedy coverage algorithm and the quality of its solution. So that's right.
00:16:12.490 - 00:16:50.774, Speaker A: In general, for every positive integer k, the generalization of the two examples we've seen so far shows that the greedy coverage algorithm might output a solution whose coverage is only a wait for it, one minus quantity, one minus one over k raised to the k power times the maximum possible. That's a little bit of a mouthful, but let's just sanity check this formula for a second. Let's plug in k equal two. What do we get? Well, then one minus one over k would be equal to half, that's being squared. So that's three quarters. That's getting subtracted from one, so that's three quarters. So that verifies that this does generalize what we saw in the k equals two.
00:16:50.774 - 00:17:18.942, Speaker A: Case. If we plug in k equals three, one minus one over k is two thirds. Now we're cubing that. That gives us an eight over 27. And then we subtract that from one, giving us 19 over 27, which is exactly the same ratio as the 57 over 81 that we saw in the quiz. So this formula does indeed extrapolate the k equal two and k equal three cases that we've already seen. And if you work out the generalization of those two examples, this is what you will find is true.
00:17:18.942 - 00:17:51.366, Speaker A: So, okay, but so how should you interpret this? We know this is 75% when k equals two, roughly 70.4% when k equals three. But what is this doing for other k? So whenever you have a crazy, univariate expression like this one, and you want to understand what's going on with it, you should just immediately plot it. So I've taken the liberty of doing that for you. That's the graph you see here on the right. So the solid line, that's just the interpolation of this expression, one minus quantity, one minus one over k to the k extended to all of the positive reals. And you look at that curve and you sort of immediately see what's going on.
00:17:51.366 - 00:18:29.958, Speaker A: You're like, okay, it's decreasing. We sort of expected that the bigger k is the smaller sort of the fraction you're getting from the greedy coverage algorithm. On the other hand, what's sort of nice is it's not going all the way down to zero? There's an asymptote, and if you sort of zoom in, you see that the asymptotes at roughly 63.2%. So no matter how big we make this particular family of examples, we never are able to show that the greedy coverage algorithm gets less than 63.2% of the maximum possible coverage. So some of you may be wondering, like, what is up with this? Why is this thing asymptotating at 63.2%? And what's going on is that as k grows large, one over k is growing small.
00:18:29.958 - 00:18:56.014, Speaker A: And in general, for small X, the two quantities, one minus X and E to the minus X are very similar. And I encourage you to just plot that and see it yourself. One minus X is going to be the straight line, and then E to the minus X is going to be this curve, which kisses that straight line exactly at zero. So if you're close to zero, they're very close to each other. So that means one minus one over K is behaving like E to the minus one over K. We're here. E is 2.7,
00:18:56.014 - 00:19:22.434, Speaker A: 118, dot, dot, dot. So Euler's number. So one minus one over K is behaving like E to the minus one over K. So we're taking that to the KH power. So we get a one over E, and then one minus one over E is well approximated by zero point 62. So that would be formally how you'd prove that this expression, no matter how big you take K, it's never going to get less than 63.2%. Now, the greedy coverage algorithm, it's so simple, it's so natural.
00:19:22.434 - 00:20:08.790, Speaker A: It's maybe like the first algorithm you'd write down as a heuristic for the maximum coverage problem. And so, given its simplicity, you're probably wondering like, what is this weird irrational number one minus one over E doing in proximity to the supernatural algorithm? And you might be wondering, well, maybe it's an artifact of this kind of family of examples that we've cooked up that we're discussing on this slide. What we're going to see next is that actually the total opposite is true. We're going to prove an approximate correctness guarantee for the greedy coverage algorithm, which shows that this is the worst family of examples you could possibly have for every possible value of K. In other words, the one minus one over E is not at all an artifact of our particular examples. It's fundamental to the approximate correctness of this supernatural greedy algorithm.
