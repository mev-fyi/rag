00:00:00.330 - 00:00:30.486, Speaker A: You. Welcome back everyone, we're in the home stretch. So last couple 364 assignments, exercise sets have normally been due on Wednesday but this last one's due on Friday. I posted it late. So get the exercise set number nine in by Friday at noon and exactly a week after that is when the take home final is due. So that's Friday of finals week again by noon. So let me just jog your memory kind of where we were a week and a half ago when we left off.
00:00:30.486 - 00:01:36.998, Speaker A: So we're in the middle of part three of the course. In part three we're asking questions like do players reach an equilibrium of a game? If so, does it happen quickly by which learning processes might play converge to an equilibrium of a game? And last couple of weeks we focused mostly on positive results. And positive results are really cool because they give credibility to the predictive power of an equilibrium concept. An equilibrium concept, if you want to interpret it as a prediction of how a game is actually going to be played then you sort of feel much more confident in that prediction if you have learning processes that converge quickly to those kinds of outcomes. This week we're going to focus more on negative results and negative results are also important because they cast doubt, they should show limitations on the general predictive power of an equilibrium concept. So let me remind you, we now actually have a really kind of nice suite of positive results about computing equilibrium different kinds of equilibria via various learning processes in different contexts. So last week was we talked about regret and so the first thing we proved is that with no assumptions on the game whatsoever.
00:01:36.998 - 00:02:18.614, Speaker A: So in completely general games, no regret dynamics. So this is where each player uses a no external regret algorithm like multiplicative weights. So if every player uses a no regret algorithm day after day to choose their mixed strategies, we show that the time average history of play converges to the set of coarse correlated equilibria. So the biggest set in our equilibrium hierarchy, so no external regret gets you to course correlated equilibria with no assumptions at all about the games. So that's really nice. The most recent lecture we asked can we get that same satisfying tractability even when we zoom in and we're more stringent about the kinds of equilibrium we want? We want correlated equilibria, not coarse correlated equilibria. The answer was yes.
00:02:18.614 - 00:03:05.814, Speaker A: We had to work a little bit harder. That's where we had this black box reduction that took as input no external regret algorithms like multiple weights and outputs, a no swap regret algorithm. That was where we used the Markov chain trick in the analysis. So we did show that there exist no swap regret algorithms and as a consequence the time averaged history of play of no swap regret dynamics converges to correlated equilibria again quickly in a small number of iterations. So there's also a question and we've talked about some and we'll talk about it more this week. Okay, so we have completely sweeping results for our outer two sets. What about the inner two sets? What about just regular old Nash Equilibria, both in the mixed version where players can randomize, and in the pure version where players can play deterministically? So here's what we know so far from what we've already done.
00:03:05.814 - 00:03:46.590, Speaker A: The very last thing we did before we took a break was we studied mixed Nash Equilibria in a special class of games. So not in general games, but in games where first of all there's only two players and second of all they were zero sum. So the payoff of the row player was the negative of the payoff of a column player. It's an interesting class of games, but it's obviously not general, it's obviously special. And we show that again, if both players use no external regret algorithms like multiplicative weights, then play converges to this set of mixed Nash Equilibria. I've also asked you to show on the homework that if you prefer, you can also formulate any of these outer three problems as linear programs. Course correlated equilibria, correlated equilibria, zero sum two player game mixed Nash Equilibria.
00:03:46.590 - 00:04:28.786, Speaker A: So that's a different proof of polynomiality, not by learning processes, rather by linear programming. And then the fourth one, which was now a while ago. So this was when we were talking about best response dynamics and we were talking about potential games and specifically routing games. And we proved a result which said in a routing game where all players share the same source and the same sync, if players use epsilon best response dynamics. So as long as some player can improve by one plus epsilon factor, you pick some player and you let them switch. Then under this single source single sync assumption, epsilon best response dynamics converges to a pure equilibrium or an epsilon approximate pure Nash equilibrium in polynomial time. So that was for this smallest set.
00:04:28.786 - 00:06:01.674, Speaker A: Again, not for general games, but for an interesting class, routing games with a single source and a single sync. So that's what we covered in the two weeks before the break, lots of positive results of when you can compute different equilibrium concepts in different classes of games. So that's where we are. Any questions about that before I talk about what we're going to do next? Okay, so as usual we have some positive results, but ideally we'd like even more. So for example, we might want to compute mixed strategy Nash Equilibria beyond just two player zero sum games or pure Nash Equilibria beyond just single source single sync routing games. So how about more like say, mixed equilibria in general two player games or pure equilibria or at least epsilon approximate pure equilibria in general routing or congestion games, meaning where different players can have different sources and syncs. That's what I mean by general so nobody knows of a learning process that converges in polynomial time to approximate equilibria in either of these settings.
00:06:01.674 - 00:07:18.198, Speaker A: There are no positive results for either of these two cases known like the ones I've showed you for all of the other examples. So that motivates the question, are we just lacking imagination? Have we just been stupid and there's one out there to be found? Or is there actually some kind of obstacle? Is it possible that actually there's no learning dynamics or maybe even no polynomial time algorithm for computing these? So what I want to talk about this week, this final week, is how you can make sense of questions like that. So how do we prove limitations on what is possible, which is obviously an important complementary endeavor to the positive results of the last couple of weeks. So we're familiar with this sort of issue. When you first study algorithms and you learn all these cool polynomial time algorithms like Dijkstra's algorithm and Kruskal's algorithm, you learn that minimum spanning tree is efficiently solvable, but then somehow no one ever teaches you a polynomial time traveling salesman problem algorithm, and eventually it becomes clear that nobody knows of one. And then you have to wonder if there is one or not. And we don't know the answer to that question to this day.
00:07:18.198 - 00:08:06.166, Speaker A: But at least we have MP completeness, which gives us an explanation for why we haven't found a polynomial time traveling salesman problem algorithm. So what we want is the same thing here. We want some analog of MP completeness tailored for equilibrium computation, explaining why we don't have positive results in cases like this. Mixed equilibrium for bi matrix games, pure equilibria for general Congestion games. So need analogs of MP completeness for equilibrium computation. So that's what we're going to do next. I want to start.
00:08:06.166 - 00:09:04.714, Speaker A: So the plan is I'm going to discuss this today, pure equilibria, that's technically a little bit easier to start with. And then on Wednesday we'll discuss mixed equilibria in bi matrix games. So most of this lecture is going to be what probably is going to seem like a digression on local search problems. So I want to talk about the complexity of local search. I mean, just looking ahead a little bit, the connection is going to be you can think of best response dynamics as local search looking for a pure Nash equilibrium or specifically for a local minima of the Rosenthal potential function. Anyways, the point is, this theory was developed quite a while ago, back in the mid eighty s, and the authors, Johnson, Papa, Dimitri Yanukakis, they did not have equilibrium computation in mind, they had local search problems in mind. But it turns out, and this became clear more like ten years ago, that this is exactly the relevant complexity theory to make sense of this question, is there a polynomial time algorithm or learning dynamics for pure equilibrium and general Congestion games? The answer seems to be no.
00:09:04.714 - 00:10:02.590, Speaker A: It's going to suggest the answer is no. And this complexity theory will explain in what sense? So again, I'm going to connect this back to routing games in pure equilibrium at the end of the lecture. But for now, it's just going to be a discussion of complexity of local search. All right? So let me just say a little bit about local search. Hopefully you've all seen it, at least in passing somewhere. A canonical example of a local search problem would be max cut. So in the max cut problem, the input is an unweighted sorry, undirected graphs have weights non negative, and the feasible solutions are just the cuts.
00:10:02.590 - 00:11:16.882, Speaker A: Okay? So some of the vertices S are on one side of the cut and the other vertices S bar on the right side of the cut. Both S and S bar should be non empty. So in general, in a cut you have some vertices, you have the other vertices. Some of the edges are going to have both endpoints in S, some of the edges will have both endpoints in S bar, and then they will be crossing edges, edges with one endpoint in each side. And the goal in the max cut problem is to maximize the total weight, the sum of the weights of the crossing edges. So unlike the minimum cut problem, this is an NP hard problem. I mean, you could try flipping all the edge weights and computing a Min cut, but actually all the Min cut algorithms you learned at a polynomial time don't work for negative edge weights.
00:11:16.882 - 00:12:05.090, Speaker A: So they're really different problems. Max cut is MP heart. So one heuristic you can use for max cut, among other problems, is local search. And people this really is a heuristic that people use quite a bit for MP heart problems, including for max cut. And so the heuristic is you start with some arbitrary solution, like maybe the first half the vertices on the left, the other half on the right, just to get going. So you start with an arbitrary feasible solution and then as long as there's a local move and I'll explain what that means in a second, as long as there's an improving local move, a local move that makes the solution better. You just do it's.
00:12:05.090 - 00:13:31.360, Speaker A: And so a local move so this is context dependent, what's a local move? But in max cut, the obvious notion of a local move is you take one vertex, say, on the left side, and you move it to the other side, always making sure both sides are not empty. Okay? So you just switch one vertex to S bar it so that's a local move and a local move is improving. If go figure, if the sum of the weights that edges crossing the cut becomes bigger because that's what we want. So when you make a local move, how does the objective change exactly? Well, when you move it, say, from the left to the right, from S to S bar. There are some edges that didn't used to get cut. Now they do some edges that used to and then vice versa. So think of V as moving from S to S bar.
00:13:31.360 - 00:14:27.714, Speaker A: So if V is getting moved, if U is an S and V is now an S bar, this is an edge which is now getting cut that didn't used to get cut. Okay? So if we sum this over edges still on the left, these are the newly cut edges. And then if we look at the vertices on the right, v is now joining all of those vertices. So those are edges that used to get cut and now they're not. So these are the newly uncut edges. And so if that difference is strictly positive, then moving the vertex increases the weight of the crossing edges. So that's an improving local move.
00:14:27.714 - 00:15:51.920, Speaker A: Local search just says as long as you can make a move like this, make one and local search then terminates with a cut where there are no improving local moves. And that's a locally optimal solution. All right, so one thing which maybe is obvious, but let's just be clear about, just because you found a solution which is locally optimal that you can't improve with a local move does not mean in general that you found a solution which is globally optimal. So maybe by moving a bunch of vertices at once you could get a strictly better cut. So let me just give you a concrete example. So note, local opt does not imply global opt. So just consider a clique on four vertices, edge weights one through six.
00:15:51.920 - 00:16:36.860, Speaker A: So let's examine this cut here where the left nodes are S and the right nodes are S bar. So the total weight of the crossing edges here is going to be 15, right? One plus five, plus six, plus three. I claim this is a locally optimal solution. That means any local move only makes it worse. So a local move here is going to give me a cut with one vertex on one side and three vertices on the other side. So the cut edges are just going to be the three edges adjacent to a single vertex. So the possibilities are this cut which would have weight eleven, this cut which would have weight twelve, this cut which would have weight eleven, and this cut which would have weight eight.
00:16:36.860 - 00:17:30.982, Speaker A: So those are all worse than 15. So this is indeed a locally optimal solution, but it is not a globally optimal solution. That would be putting the two top edges, top vertices in one side and the bottom vertices in the other. Then we cut the edges four, five, six and two for a total of 17. Okay, so a local search is nothing but a heuristic. It's an often very useful heuristic, but it is a heuristic. And as such, you might expect that computing merely a locally optimal solution to a problem like max cut could be easier.
00:17:30.982 - 00:18:13.980, Speaker A: It's certainly at least as easy as computing a globally optimal solution, because global optimal means local optimal. And once you see this gap, you think, well, maybe it's actually strictly easier to merely compute a locally optimal solution. And that generally is true. It so let me give you a very concrete example of that. So think about max cut. But let's say, rather than having all of these different weights, so it's some arbitrary graph, not necessarily cleak, it's some arbitrary graph. And all the edge weights are, let's say, one.
00:18:13.980 - 00:19:14.910, Speaker A: So now the max cut just strives to maximize the number of cut edges. So one fact which I hope you find easy to believe is that the problem is still empty hard in this case. If you really wanted to find the best solution, it's an empty hard problem. But finding a locally optimal solution is not hard if all the weights are one. In fact, I claim if you just run local search, this algorithm here, I claim this can't run very long. And remember, when local search terminates, it certainly terminates at a locally optimal solution. Claim that it terminates in at most the number of edges iterations.
00:19:14.910 - 00:19:59.226, Speaker A: So do you see why that's true? It so in an iteration of local search and you make a move, what can you say about the objective function value goes down. Good. And if all the weights are one, what's the minimum amount by which it will go down? Yeah, it goes up. Excuse me, what's the minimum amount by which it will go up? It'll go up by one for sure because everything's integral. What's the minimum possible objective function value? Zero. What's the maximum possible objective function value? If it's a Bipartite, you can cut all the edges. So the maximum number of iterations because you improve by one every time.
00:19:59.226 - 00:20:34.486, Speaker A: And the maximum is the number of edges. Is the number of edges. So local search is just going to reach a locally optimal solution blazingly fast with unit weights. So it's definitely easier than finding a globally optimal solution. And just again, sort of to give a forward pointer as far as equilibrium computation, it's the local search problem that we really are going to care about, not the globally optimal problem that we're going to care about. So it's really local search we need to understand. All right, so for max cut, at least, at least for this special case where all the weights are units, computing a locally optimal solution is easy.
00:20:34.486 - 00:21:51.280, Speaker A: How do you do it? Just use local search. So here's a fact. If we drop the condition that the weights are unit, if they can be arbitrary integers, so that this maximum cut value could be very large, exponentially large, say, in the size of the graph, then nobody knows how to compute a locally optimal solution, a locally maximum cut in polynomial time. No one knows how to do that's. It I want to emphasize the claim here is stronger than just asserting nobody knows if a local search terminates in polynomial time. In fact, as we'll discuss later, well, it's known that local search does not terminate in polynomial time for weighted max cut. What I'm saying here is suppose I even allow you to compute a locally optimal solution any way you want.
00:21:51.280 - 00:22:43.570, Speaker A: I don't care if you use local search, okay? Use the cleverest polynomial time algorithm you can come up with. Even with the complete range of polytime algorithms, nobody knows how to compute a locally optimal solution of max cut instances with general weights. So the plan for the rest of the lecture is I want to tell you sort of the more classic complexity theory, which formalizes the negative result here the idea that the reason we don't have such an algorithm is because we suspect no algorithm exists. It's complete for the appropriate complexity class. And then I'll conclude by making the connection back to pure equilibria of routing and Congestion games. But so the next step is, all right, we're stuck on weighted max cut. We don't know how to compute a locally optimal solution.
00:22:43.570 - 00:23:26.510, Speaker A: Can we somehow prove that? It's not that we're being stupid, it's that there doesn't exist any polynomial time algorithm for finding a locally optimal solution. All right, so what would that look like, just as a theorem statement saying that there doesn't exist a polynomial time algorithm? Well, the strongest statement, obviously, would just be what I just said. The unconditional result. The strongest negative result you might prove is that you just say among all polynomial time algorithms, none of them work. So an unconditional impossibility result. Now, we don't really have any results of that form. So in particular, if you could prove there was no polynomial time algorithm for finding a locally optimal max cut that would separate P from NP.
00:23:26.510 - 00:24:12.182, Speaker A: So that's a little ambitious for today, at least. So we're not going to try to prove unconditionally that there's no polynomial time algorithm for computing a locally optimal max cut instance. So the next best thing then would be, well, we know about MP completeness. We agree that MP completeness is in some sense the standard means that you argue that a problem in MP is intractable and seems to require brute force search or something very close to it. And it turns out MP completeness is also too strong a goal for local search problems like max cut. There's a sense in which computing locally optimal solutions really is easier than NP hard problems. And that'll make precise on Wednesday.
00:24:12.182 - 00:25:38.914, Speaker A: Okay? But for now, I just want to point out we're not going to prove that there's no polynomial time algorithm that's too strong. We're not going to prove that it's NP complete that's also too strong. So instead, we're going to develop an analog of NP completeness tailored for local search algorithms as a way of arguing that we don't think a polynomial time local search algorithm for max cut exists. So here's going to be the plan I'm going to write down right now the English interpretation of the theorems we're sort of shooting for. We're going to formulate and prove theorems that, say, computing a locally optimal solution of max cut is as hard as any other local search problem we're shooting to formalize and prove. Again, the intent here is to have an analogy or a sort of analogous version of MP completeness tailored for local search problems. One way to interpret MP completeness is that that's a problem which is as hard as any other problem for which you can efficiently verify solutions, efficiently recognize a solution.
00:25:38.914 - 00:26:31.194, Speaker A: So this is what we're going to prove instead as hard as any other local search problem. So if max cut is sort of as hard as completely abstract, completely general local search, there's a sense in which we then don't expect there to be algorithms significantly better than local search itself. Again, remember, for these problems I want a locally optimal solution. I don't care how you do it. If you use local search to find one, great. If you use some other clever algorithm to find me a locally optimal solution, I'm just as happy. Okay, so if we can formalize and prove this idea, an interpretation is that we don't expect to improve much over local search in exactly the same sense as for an NP complete problem.
00:26:31.194 - 00:27:36.720, Speaker A: We don't expect to improve much over brute force search. It's also going to imply once we formalize this theorem, that local search takes exponential time. So again, as far as the analogy with NP and NP completeness, you can think of problems in NP as problems where you can efficiently recognize a solution, the problems which are solvable by brute force search. NP completeness says, well, you can't do much better than brute force search. We're going to do the problems that are solvable by local search, which is in some sense a little bit more structured, a little bit more guided than brute force search. And we're going to try to prove the hardest problems of that type where you can't beat local search, which is exponential time. Okay? Yeah.
00:27:36.720 - 00:28:05.430, Speaker A: By any other local you mean like any other problem? Basically, yeah, that's the intuition and the formal definition is coming up. Okay, so formalizing this. We have two things to formalize. First is as hard as this is just going to be a reduction. So most of you should have strong intuition on what this is going to look like. What we really need to be a little bit more careful about is what is a generic local search problem. Okay, so that's the next definition.
00:28:05.430 - 00:28:39.822, Speaker A: Okay. And so again, keeping the analogy with the class NP in mind. So NP you can think of as the class of problems that are generically solvable by brute force search. So I don't know what definition of NP you've seen. There's a bunch of them, but one of them is they're problems that are characterized by an efficient verifier. So if I show you an alleged solution, you can quickly check whether or not it is one like a Hamiltonian cycle in a graph. If I show you one, you're like, yeah, it's a Hamiltonian cycle.
00:28:39.822 - 00:29:52.482, Speaker A: If I try to fool you, you can easily check that it's not a Hamiltonian cycle. So NP defined via efficient verifier, that's just sort of asking what is the minimal ingredients that I need to solve a problem in exponential time using brute force search? I need a verifier to tell me that when I try a particular solution, whether it's a solution or not. So here to define very generically, what is a local search problem? We need to ask what are the minimal ingredients that we need for the local search algorithm to be well defined? Starting from an arbitrary solution, keep moving to better and better solutions until at some point you get stuck and there's no improving moves. So what are the middle ingredients we need for that algorithm to be well defined? Okay, so rather than one polynomial time algorithm, an efficient verifier here, I'm going to use three. So a generic local search problem has three polynomial time algorithms. The first one, given an instance like max cut, like a graph, just tells you where to get started. It gives you an initial feasible solution.
00:29:52.482 - 00:31:09.330, Speaker A: So again, it's just for max cut, maybe it's the first half of the vertices on the left, the second half of the vertices on the right. So some canonical way of getting started that's important. For running local search, you need to have an initial feasible solution. Now, in local search, you're trying to optimize something like max cut. So we want an algorithm which tells us how well we're doing that for a feasible solution, computes for us in polynomial time how the objective function value of that feasible solution. And thirdly, if we're at a solution which is not locally optimal, we need to know where to go next. So this third algorithm is just going to be like an oracle which says, oh, this isn't locally optimal, and here you should move to this neighboring solution.
00:31:09.330 - 00:32:16.706, Speaker A: So given a feasible solution, either reports you're done, or if not, produce a better neighboring solution. Let me just say better solution. Okay. Yeah, there has finite interesting. Yeah. So by virtue of being polynomial time, it's sort of built in. So an instance has some length, it has some L bits.
00:32:16.706 - 00:32:47.294, Speaker A: And so each of these algorithms, by being polynomial time and L, the input length, they can only operate on strings of length, some polynomial and L. So that's going to correspond. It basically says everything you're operating on are bit strings of some length, polynomial and L. So it's an exponential upper bound on what you might ever see. So that is a consequence of this polynomial time constraint. You could be operating on real numbers. That's not how it's specified.
00:32:47.294 - 00:33:09.206, Speaker A: So the input specified just as bits. Yeah. Good. So given one through three, this is enough to run the local search algorithm. You invoke algorithm one to get started. You iteratively invoke algorithm number three to generate new solutions. You can use algorithm number two just to verify that you're making progress with algorithm number three.
00:33:09.206 - 00:33:45.486, Speaker A: And at some point, you'll run out of there's a finite search space that's sort of implicit in the polynomial time definition, and eventually you'll run out. So you'll halt necessarily at a locally optimal solution. So in some sense, again, so why do we make this definition? We have this very concrete problem. We care about max cut, and we want to say it's hard. And so what we're trying to do is say it's as hard as any conceivable local search problem. So this is our attempt at any conceivable local search problem exactly. In the same way that if you have a concrete problem like traveling salesman problem, and you want to prove it's hard, the more things you can prove, it's at least as hard as the stronger the evidence of intractability.
00:33:45.486 - 00:34:37.590, Speaker A: So we're making this as abstract as possible to have the strongest possible intractability instance for the problem, evidence for the problem. We actually care about max cut for now and then pure equilibria later. And so then the goal given this is just compute a locally optimal solution. Again, an algorithm which is correct would be just run local search. That's guaranteed to terminate a locally optimal solution, but for the purposes of the computational problem, compute a locally optimal solution by any means. If you have some other clever algorithm which can just zoom right to a locally optimal solution, I'm totally happy to receive it. So that's the complexity class pls defined by Johnson, popner, you and Yannikakis.
00:34:37.590 - 00:35:29.750, Speaker A: And again, the intent was to make it as big as possible so that any complete problems for this class are as hard as possible in some sense. All right, questions about the definition of Pls. Yeah. So the neighborhood is sort of implicit here. So there isn't an explicit neighborhood in the description. I mean, in some sense, the neighborhood is just the range of that third algorithm. So I have an operatori defined a neighborhood.
00:35:29.750 - 00:36:05.940, Speaker A: Okay. Sort of. I mean, the the definition of the neighborhood is just totally implicit in what that third algorithm happens to do. So if it declares some solution locally optimal, then it tells you something about the neighborhood. Right. So just to connect this back to max cut, so in the first algorithm, again, this just returns any old cut, doesn't matter which second algorithm. This is clear.
00:36:05.940 - 00:36:30.060, Speaker A: You just evaluate the weight of the crossing edges. Clearly polynomial time. In the special case of max cut, the natural way to implement this. Third algorithm is simply to try all of the linear number of local moves and see if any of them improving. And if one is improving, then return an arbitrary such move. If none are, you report locally optimal. So in the concrete case of max cut, that's what this algorithm looks like.
00:36:30.060 - 00:37:27.206, Speaker A: But for a generic local search problem, this is all we ask of it. So computing a locally optimal solution of a max cut instance is certainly a member of Pls, and any local search problem you've probably come across in your career is probably a member of this Pls class. Okay, so good. So Pls is our definition of any other local search problem, and that's the harder definition. And now let's just dot the I's and cross the T's for as hard ATS. So this will be familiar from NP completeness. So we want a notion of a Pls complete problem problem as hard as anything else in Pls.
00:37:27.206 - 00:38:04.166, Speaker A: So we need a notion of reduction. And so a reduction from one Pls problem to another. Well, so we just follow our nose with this definition. We know the point of this definition is to say if we can solve l two in polynomial time, then we can solve l one in polynomial time. L one reduces to l two, so we can transfer tractability or intractability in the opposite directions. So if you think about that, okay, so suppose I gave you as a black box, an algorithm for l two. You want to solve l one.
00:38:04.166 - 00:38:43.660, Speaker A: So what you want to do is first of all, you want to say, okay, well, I need an instance of l two to invoke my black box. So I need some polynomial time algorithm that takes an input to l one, makes it an input to l two so I can feed it into the black box, and then the black box gives me an output. So it gives me a locally optimal solution for that instance of l two, which is not what I care about. I care about my old problem. So I need a second algorithm that maps that locally optimal solution to the l two problem. Back to one. For the l one problem that's it so called the only time algorithm A that.
