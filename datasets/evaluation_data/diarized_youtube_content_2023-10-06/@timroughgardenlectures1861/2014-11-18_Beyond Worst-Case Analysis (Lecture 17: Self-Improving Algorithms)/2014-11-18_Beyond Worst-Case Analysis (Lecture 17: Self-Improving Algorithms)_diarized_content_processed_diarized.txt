00:00:00.330 - 00:00:26.070, Speaker A: Plan for today is something called self improving algorithms. So let me tell you what those are about. So here's going to be the assumptions that we make today. The first one you're starting to be a little bit familiar with, which is that there'll be some randomness in the inputs, so the inputs will be drawn a distribution.
00:00:27.930 - 00:00:28.342, Speaker B: Okay?
00:00:28.396 - 00:01:11.422, Speaker A: So the last four lectures, both of them were talking about empirical performance of hashing and also three lectures on smooth analysis. There again, we were talking about data inputs that had some randomness in it. And we're talking about the expected performance of algorithms. Today is going to be sort of a little bit different in focus in that we're really going to kind of use this assumption in a stronger way. And in particular, we'll be considering algorithms that are designed or even try to learn the distribution over inputs. So contrast that with, say, smooth analysis, say, of the simplex method or of local search, where really a distribution on inputs wasn't fundamental to the algorithm making sense. The distribution just served as a sort of plausible narrative for why these algorithms work so well in practice.
00:01:11.422 - 00:01:56.080, Speaker A: It really wasn't kind of a fundamental assumption about why we were using those algorithms. Okay, so in contrast to, say, average case analysis, we're going to be thinking of this distribution exists. So we believe in that nature is really giving us inputs drawn from some distribution, but we don't know what it is at the beginning. That'll be the assumption. So that's different than average case analysis, where you assume you know the distribution and then you tail your algorithm to the distribution. It's also different than we talked about, say, planted cliques or planted partitions. There again, it wasn't sort of a uniform distribution, it was this sort of interesting distribution, but we sort of assumed it up front and then designed algorithms tailored to it.
00:01:56.080 - 00:02:56.050, Speaker A: And now the third point is what's really going to characterize the next three or so lectures? And this is going to be new. So this is something that hasn't been true for anything we've talked about so far, which is that we'll actually observe multiple inputs drawn IID from this same distribution. So observe multiple IID inputs. Okay? So concretely, we'll be talking about sorting for today. So it means you're just given one array, then another array, then another array, all drawn IID from some distribution over arrays. So we haven't really seen this before at all. And observing multiple inputs from the same distribution gives us a new opportunity in our algorithm design, which is now we actually have an opportunity to, even though we don't know the distribution up front, we can learn it, or at least perhaps learn relative statistics about the distribution and use that in our algorithm.
00:02:56.050 - 00:04:18.430, Speaker A: Okay, so that's a new thing, and we'll be talking about that for the next few lectures. So opportunity to learn. All right. And it's easy to imagine not all practical applications will fall under this umbrella, but it's easy to imagine some that do, right? So if you're just sort of watching the search queries come into your search engine every hour, every day or whatever, you can then batch those and think of each of those as some kind of distribution drawn from who searches for a particular query term. So it's not hard to think of cases where you might actually want to have algorithms that indeed adapt to a distribution over input and you have enough data, enough independent samples of data that you actually can adapt over time. All right, so what do we want then, if this is the setup? So what we aspire toward is an algorithm which, well, while initially does not know the input distribution quickly evolves to a point where it knows enough about the input distribution that its performance becomes just as good as if we had known that distribution over inputs in advance. So an algorithm that quickly converges.
00:04:20.290 - 00:04:20.606, Speaker B: By.
00:04:20.628 - 00:04:33.970, Speaker A: This I mean, doesn't need too many IID samples from the distribution, doesn't need too many inputs, and then what should it converge to? Well, the best thing you could do is, well, if you knew the distribution up front, you could run some optimal algorithm for it. So that's what you'd like to be.
00:04:34.040 - 00:04:34.660, Speaker B: Okay.
00:04:38.730 - 00:04:51.994, Speaker A: So to optimal algorithm for the opera unknown distribution. All right, so you can think of it as sort of an algorithm that tunes itself to the data.
00:04:52.112 - 00:04:52.780, Speaker B: Okay?
00:04:55.150 - 00:05:28.546, Speaker A: All right. So that's at a high level what self improving algorithms are. And so we're going to talk about today, about sorting. Okay, so obvious example, main example. So this was indeed the first case study of self improving algorithms that was ever done. So this was back in six. Since this time there have been follow up works dealing with other problems, like especially in computational geometry.
00:05:28.546 - 00:06:35.660, Speaker A: So 2D maxima the same problem we studied back in lecture number two convex holes, delano triangles, stuff like that. Okay, so what's the sorting model? So throughout the lecture, the array length is going to be just some fixed number n, so fixed n number of elements. Now, each of these array entries you can think of as a real number or rational number, and it's drawn from some distribution. So the ith element of input Xi is drawn from some distribution di. And again, we don't know what this is priori. And so again, maybe think of this as just a distribution over a subset of rational numbers in some range. And the di's, we're going to assume they're independent, and I'll comment on that in a SEC.
00:06:35.660 - 00:06:39.930, Speaker A: And also it's important that they're not necessarily identical.
00:06:44.530 - 00:06:45.440, Speaker B: All right.
00:06:48.290 - 00:07:24.250, Speaker A: So let me comment on both of these assumptions briefly. So why do we assume that the di's are independent? It's not really because we believe in real data. The different array entries are going to be independent. Maybe there's some lucky applications, that's going to be true. But if you think about it, that's actually probably not the common case. The reason is because positive results are only known for self improving algorithms under this assumption, okay? And on the homework you'll explore some negative results, some lower bounds without independence. That said, it seems like there should be some room in between the lower bound on your homework and the positive results in class today.
00:07:24.250 - 00:07:45.242, Speaker A: But that's an open research question to get analogous positive results under some relaxed notion of independence, right? So this really is the state of the art, even though you really would like more general positive results. So that's why we're going to assume independence. The identical case, I mean, that's a special case of what we're talking about. So in some sense we will cover the identical case, but that's uninteresting.
00:07:45.386 - 00:07:45.658, Speaker B: Okay?
00:07:45.684 - 00:08:14.780, Speaker A: So if all the array entries are drawn identically from the same distribution, then the relative ordering of the N elements is equally likely to be any permutation if you think about it. And this will become clear as we proceed. There's no opportunities to beat just your basic N log n bound. When the permutation over the elements is uniform at random, we will handle the identical case. But don't think of that as the canonical case. That's not where the results for today are. Interesting.
00:08:14.780 - 00:08:26.958, Speaker A: All right, so even though the numbers are, the Ray entries are real numbers, we're going to be working in the comparison model.
00:08:27.044 - 00:08:27.294, Speaker B: Okay?
00:08:27.332 - 00:09:22.298, Speaker A: So we're going to assume that we only can access these real numbers by comparing any pair of them. So given D one through DN, these independent distributions over the XIs, I'm going to use pi of D to denote the induced permutation, the induced distribution over permutations. Induced distribution over permutations of one through N. So in other words, suppose I give you some array with arbitrary real numbers. Seven, four, negative, one half, blah, blah, blah, blah. Now just imagine replacing each of those numbers with its rank. So the number which is the fifth smallest number, just imagine that's a five and similarly for all N elements.
00:09:22.298 - 00:09:27.438, Speaker A: So that just turns any array into a permutation. I'm going to assume for the whole lecture that there are no ties.
00:09:27.534 - 00:09:27.842, Speaker B: Okay?
00:09:27.896 - 00:10:03.680, Speaker A: You can still prove things with ties. It's just sort of uninteresting details. All right, so any array you can think of as a permutation of one through N, pi of D denotes the induced permutation, the induced distribution over them. And again, we're thinking about the comparison model. So we aspire toward correct sorting algorithms whose expected number of comparisons is as small as possible. And here the expectation is actually over the input when we say expected number of comparisons. All right, so that's the model.
00:10:03.680 - 00:10:49.018, Speaker A: That's our sorting model. Now what are we sort of hoping? So to get results that have any meaning? What are we hoping is true? Well, we're hoping two things are true. So first of all, we know a lot about sorting, even in the worst case, right? So sorting is not really one of those problems that exposes the problems with worst case analysis. We have all merge sort n login pretty good, actually. Right? So for this to be an interesting question, we have to hope that there are interesting conditions under which we can do even better than n log n. That's sort of the only interesting positive result here would be something we couldn't get in the worst case. So hope number one is that sometimes we can get little o of n log n.
00:10:49.018 - 00:11:24.534, Speaker A: I mentioned, and I'll mention this again later, that if it's the uniform distribution over permutations, you can't beat n log n. So if it's like a fully random permutation, n log n is still a lower bound. So the hope would be then that it's for some other kinds of distributions over permutations, you can beat n log n. Like maybe there's not too much randomness, it's not too close to uniform. Okay, so for now, I'll make this precise in a second. So it can get better than n log if the di's have low entropy. And I'll give you a definition of that in a second.
00:11:24.534 - 00:11:51.070, Speaker A: It's the standard definition. And in some sense, it's obvious that if you use a restrictive enough definition of low entropy, surely this is true, right? Like, imagine I told you, either the array was totally sorted or totally reverse sorted. With one comparison, you can figure out which of the two it was. All right? More generally, it's easy to see that if you only have K things in your support with K comparisons, you can figure out which of it is actually you could do much better than that too.
00:11:51.140 - 00:11:51.518, Speaker B: Okay?
00:11:51.604 - 00:12:10.360, Speaker A: But it's sort of clear that at the extreme point, if it's not very random, then you need much fewer than n log n comparisons to sort correctly. So that's the first hope. But the second hope is that the conditions are not so stringent that they preclude kind of any interesting application.
00:12:10.730 - 00:12:11.640, Speaker B: All right?
00:12:12.010 - 00:13:05.782, Speaker A: So the second thing you got to hope is that real data, maybe not all the time, but at least sometimes in some interesting cases, has low entropy. So if both of those two things are true, then there's definitely sort of a meaningful positive result that we might hope to prove, okay? We might hope to prove that in certain kinds of real sorting data sets. First of all, you can beat n log n. And second of all, let's give an algorithm that does it, even though we don't know the distribution up front. I want to point out there's sort of an amusing counterpoint between what I wrote here and what we've been talking about for the past four lectures, both in our three smooth analysis lectures and in the hashing lecture. Totally random data was too easy. So like think about hashing, right? If the data is totally random, you don't even need a universal hash function.
00:13:05.782 - 00:13:35.726, Speaker A: Like really trivial hash functions do fine, but we said, oh, that's unrealistic, right? Let's make our lives harder by sort of not assuming that the data is fully random, but it can't be worst case. So we need to assume a lower bound on the randomness in the data. Same thing with smooth analysis. We couldn't have a worst case input, so we had to assume a lower bound on the amount of randomness in the input. Ironically, or interestingly, with sorting fully random data is actually the hard case. That's like as hard as it gets. It's as bad as worst case.
00:13:35.726 - 00:14:16.010, Speaker A: So instead we have to have an upper bound on the amount of randomness in the data to hope to have positive results. Okay? Now I guess a quick comment on number two. So would real data ever have low entropy? Well, it's certainly not always going to have low entropy. Sometimes it is going to be pretty random data and you may as well use merge sort or something, or Quicksort, something like that. But definitely there are applications where when you hear about this, when you get warned about sort of quick sort pivoting rules, where the common case is maybe not sorted, but an almost sorted array, you had something sorted, a few random things got inserted, now you need to resort it. And if sort of that kind of application, then actually if you're just having inputs that are close to sorted, that's going to be a small support distribution. It is going to have pretty low entropy.
00:14:16.010 - 00:14:19.258, Speaker A: So you can imagine stories where this is relevant.
00:14:19.354 - 00:14:19.998, Speaker B: Okay.
00:14:20.164 - 00:15:15.090, Speaker A: Though, again, it's not clear to me, it's not clear to me that I don't think this is the common case in some sense, but you can certainly imagine stories where that's true. Okay, so what do I mean by entropy? Well, I just mean the standard notion of Shannon entropy that I'm not going to know if you've seen that before, awesome. But I'm not going to assume that you've seen it before. So suppose you have a distribution on a finite set. Let's say the finite set is capital X. The distribution is D. The entropy which I'll denote by H of D is what you do is you sum over the little X's.
00:15:15.090 - 00:15:30.054, Speaker A: So if I just did this sum, I'd get one, right? Because it's a distribution. So for the entropy, I additionally have these log base two of one over the probability terms.
00:15:30.252 - 00:15:31.000, Speaker B: Okay?
00:15:32.090 - 00:15:44.294, Speaker A: So think of all the P's as being strictly between zero and one. Or if you like, I mean, generally what you do is if PX is zero, you just interpret this as zero, which is what it is in the limit as P goes to zero.
00:15:44.492 - 00:15:45.240, Speaker B: Okay?
00:15:46.890 - 00:16:12.660, Speaker A: So if you've never seen this before, let's get a quick feel for it. Although I should say I mean, perhaps happily, we're actually not going to need to work with this particular definition. It's simple enough. And on the homework, I'll ask you to play with it a little bit, but we actually don't really need to play around much with this formula for this particular lecture. Okay, but let's get some feel for it anyways. So I claim that if you have a distribution and the support size.
00:16:14.470 - 00:16:14.786, Speaker B: Is.
00:16:14.808 - 00:17:11.606, Speaker A: At most two to the H for some H, say a positive integer h, then the entropy is at most H. Okay? So first imagine that if I told you that the distribution was uniform over a support of size two to the H, okay? So it had probability one over two to the H on each of these two to the H things. Well, then after disregarding all the zeros, this would always be one over two to the H. This would be log base two of one over two to the H, also known as log two to the H, also known as H to the PX is sum to one. And then I have this h okay. So if it's uniform distribution over support of this size, then this is exactly H. So as an exercise in the homework, I'll ask you to prove that's the worst case for the entropy, that's the biggest the entropy can be.
00:17:11.606 - 00:17:37.146, Speaker A: If your support size is H, it's two to the H. So the entropy is always at most H if your support size is at most two to the H. So that's one simple fact. In general, the bigger the entropy, the more the randomness is in your distribution with the uniform distribution over everything in the finite set maximizing entropy over all possible distributions.
00:17:37.258 - 00:17:37.920, Speaker B: Okay?
00:17:39.250 - 00:18:42.574, Speaker A: All right. So, okay, so let me as usual, let's talk about the line in the sand, right? So what's sort of the best even let's just suppose we knew the distribution in advance. What's sort of the best sorting algorithm we could possibly hope for. Like when can we beat N log N and by how much? I guess guess just to quickly connect that last comment to sorting and to our model, our sorting model. So imagine each di. So imagine each array entry could only take on one of two possible values. So possibly a different pair of values for the different XIs.
00:18:42.574 - 00:19:28.978, Speaker A: But suppose each Xi was either sort of some high number or some low number. Well, then there's only two to the N, different arrays, which you could possibly see, right? So the support of the distribution pi of D is going to be at most N. So I specified a distribution with at most two to the N possible outputs. So the entropy is at most N. That's something to keep in mind, by contrast. So that's small, relatively speaking, for a distribution over arrays. Because if, on the other hand, you just took the uniform distribution over all possible N factorial permutations then you'd get log of N factorial, which is basically N log N.
00:19:28.978 - 00:20:39.370, Speaker A: Okay, so those are some sorting specific examples. Any questions clear so far? Okay, so let's try to connect the entropy of distributions back to what we care about, which is we're trying to design sorting algorithms with a minimum possible number of expected comparisons. The claim is that the best we could hope for in this model of self improving algorithms is an algorithm so that no matter what these DS are, remember, those are unknown. We want the algorithm to evolve to be optimal with respect to the DS, whatever they may be. We want the running time to converge. I claim to O of N plus the entropy in the induced distribution on permutations. So I claim this should be our holy grail.
00:20:39.370 - 00:21:16.190, Speaker A: Now this is easy enough. We have to write down the output. Okay, so there has to be an N there, no matter what. In a second, I will justify why we're stuck with this as a lower bound, and we can't hope to beat the entropy of the permutation distribution no matter how smart we are with our algorithm design. So before that, let me just say there's other goals. There's other sort of second order goals you might want to optimize in addition to running time. But I'll sort of keep those pretty much largely under the rug as we do the development.
00:21:16.190 - 00:21:53.860, Speaker A: So one thing you might care about is space. So in particular one of these algorithms, you sort of expect to do some kind of learning of the distribution D. So it's probably measuring certain statistics of the distribution. So that's where sort of space comes into play. You don't want to cheat by sort of measuring an exponential number of different things. The second thing you might want to care about is sample complexity, namely the number of sort of inputs you have to see before your running time starts converging to this best possible bound. Okay, so I'll give a report card at the end of the lecture, but the main thing we're just going to focus on is let's get an algorithm which converges to this running time.
00:21:53.860 - 00:23:10.950, Speaker A: Okay, so now let me now explain why not only is N a fundamental barrier, but so is this entropy term. So this follows by sort of a famous fact and a simple but very cool observation connecting sorting algorithms to encoding schemes. So first let me mention Shannon's theorem, this down here. So this theorem of Shannon is what's going to allow us to pretty much totally ignore this formula, this whole lecture. We're basically never going to have to think about this again. So what it says is that for all distributions d on whatever finite set you care about, capital X, you can equally well characterize the entropy, not just algebraically, but combinatorially. Okay, so this number H of D is almost exactly the same, meaning up to plus one, almost exactly the same as the minimum.
00:23:10.950 - 00:24:31.760, Speaker A: Let me write this down and then I'll explain what I mean. The minimum possible expected over the distribution D encoding length of X. Okay, so here's what I mean. Okay, so X, maybe X is a set of 20 elements or something like this, right? There's a lot of ways you can encode 20 things in binary. There's the obvious thing where it's like 0100, et cetera, but as soon as you start imposing frequencies on these different 20 elements and saying some are much more likely than others, all of a sudden it's tempting to be creative with your codes, right, and to give the more frequent elements shorter codes, right? So that's what this says. So it says you've got this finite set, capital X, there's the obvious way to, there's the obvious way to encode it. But given a distribution D, instead, think about the binary encoding that makes the average encoding length, meaning I'll pick a random element of capital X according to capital D and I'll look at the length under the code of that element of X and let's minimize that expectation, okay? Get the expected encoding length as small as possible.
00:24:31.760 - 00:24:52.520, Speaker A: So is it clear what I mean by that? So again, encoding is just a mapping from capital X to binary strings. Length is just the number of zeros and ones and the expectation is just over D. So among all binary codes, just make that expected length as small as possible. And theorem says that the answer or that optimization problem is essentially exactly the same as this number.
00:24:52.970 - 00:24:53.478, Speaker B: Okay?
00:24:53.564 - 00:24:57.030, Speaker A: And this is the version of entropy we're going to be using for the lecture.
00:24:58.570 - 00:25:00.134, Speaker B: How does Shannon prove that?
00:25:00.252 - 00:25:10.460, Speaker A: So the proof is it's all elementary. It's going to take us too far afield to do it in lecture, but I'll include some aspects of it on the homework just so that you're not too scared of it.
00:25:13.390 - 00:25:14.140, Speaker B: Okay.
00:25:16.290 - 00:26:03.478, Speaker A: So we'll be using it though. All right, so now how does me a couple more comments, so in particular, just again so that you're not scared of this, hopefully some of you have seen Huffman codes. So for example, when I have time, I fit that into undergraduate algorithms. So Huffman codes already achieve the upper bound in this theorem. So Huffman codes are good enough to get basically the entropy bound and then the lower bound is some basically pretty simple combinatorial arguments and good. So let's apply this to our sorting problem because notice this is about encoding schemes. We don't care about our encoding scheme.
00:26:03.478 - 00:26:52.090, Speaker A: Lectures were long ago, we're talking about sorting algorithms. Oh, but actually sorting algorithms give encoding schemes if you think about it. So remember, we're working in the comparison model. So what can a comparison algorithm do just for simplicity? Let's think deterministic, but it really doesn't matter. Okay, well, the first thing it does makes a comparison, maybe the third element of the array to the 12th element of the array, whatever, and there's two possibilities. Either the first element is less than the second one, or it's bigger than it. And for a comparison based algorithm, the only thing I can do with each of those branches is do another comparison.
00:26:52.090 - 00:28:12.630, Speaker A: So again, now if this is less than it's going to pick possibly adaptively, some other pair of elements, compare them, and on the right branch it might pick a different pair of elements and compare them and so on. So this is what comparison based sorting algorithms look like. They look like trees, binary trees. And eventually the algorithm has to stop and say what the output is, okay? And we're assuming the algorithm is always correct, so it's going to output the correct permutation that the input was. So we have leaves and every leaf we can label with a single permutation, namely the output of this sorting algorithm on that particular termination of its execution. And every permutation has to appear at least once, okay, because each of the N factorial inputs might show up and the algorithm has to correctly sort every single one of them. So in the leaves we have single permutations, one permutation per leaf, just the output of the algorithm.
00:28:12.630 - 00:28:53.886, Speaker A: How can we, from a correct sorting algorithm extract a binary encoding? We just label the left children with zeros and the right children with ones. You tell me a permutation, I look at a leaf that's labeled with that permutation and I read off the binary encoding going back to the root, okay? So this ascribes to every single permutation, some binary string, moreover, the number of entries in that binary string is exactly the number of comparisons that this algorithm used to figure out that this was indeed the output.
00:28:54.078 - 00:28:54.770, Speaker B: Okay?
00:28:54.920 - 00:29:06.550, Speaker A: So again, root leaf paths correspond to permutations possible inputs. We can view those as encodings, and the encoding length is exactly path by path, the number of comparisons used.
00:29:06.620 - 00:29:06.854, Speaker B: Okay?
00:29:06.892 - 00:30:24.800, Speaker A: So no matter what the distribution is, the expected root path length under that distribution is exactly the same I e. The expected length of the encoding is exactly the same as the expected number of comparisons. So Shannon's theorem, what does it tell us? It tells us the expected encoding length of any binary code, in particular one arising from a correct sorting algorithm, is lower bounded by the entropy. So that means that the expected number of comparisons, it's exactly the same as the expected encoding length is equally well, lower bounded by the entropy of the distribution. That's why when I go back to the best case scenario, not only do we have to expect an N in the running time just for output considerations, but for these entropy considerations, we also have to expect the entropy of pi of D to be a barrier. No way we can do better.
00:30:25.170 - 00:30:25.920, Speaker B: Okay.
00:30:28.050 - 00:30:50.870, Speaker A: And so this fulfills a promise I said earlier also, which is I said if the permutation is uniform at random, then you can't beat the n log n bound. So the interesting case is low entropy. And you see that here, right? So if it was uniform over all n factorial permutations, then the entropy would just be log of that which is n log n. And we just argued that north sorting algorithm can beat that.
00:30:50.940 - 00:30:51.318, Speaker B: Okay?
00:30:51.404 - 00:31:13.340, Speaker A: So that's why uniform permutation is not the interesting case. You may as well just use a worst case optimal sorting algorithm. All right, any questions about all of that? So this was the part of the lecture where I tell you, I draw the line in the sand. I say, this is what we really want. The rest of the lecture, I give it to you. Any questions?
00:31:15.950 - 00:31:16.900, Speaker B: All it.
00:31:28.890 - 00:31:43.900, Speaker A: All right. So let's move on to the algorithm. The algorithm really is quite natural and it's based on bucket sort.
00:31:47.630 - 00:31:48.380, Speaker B: Okay?
00:31:49.950 - 00:32:33.110, Speaker A: So bucket sort, you may have seen it's usually described when you have data, say like that are real numbers in the interval zero one. So this is not bucket sort normally is not a comparison sorting algorithm. It's a classic example of a non comparison sorting algorithm, which does better than comparison sorting when you know some stuff about your data. Like for example, if it's uniform from the interval zero one, how does it work in that case where you just say, oh, well, let's just have n buckets or so with the ith bucket. Basically, what are you going to do? We're going to do a single pass through the input array and we're going to look at what's the I for which this stuff is in between I over n and I plus one over n. And then we just dump it in a bucket. We put all of the elements in that interval in this one bucket.
00:32:33.110 - 00:32:40.554, Speaker A: Then we go through the buckets one at a time. We hope that each one is very sparsely populated. We do some simple sort in each of the buckets and then we just concatenate them all together.
00:32:40.672 - 00:32:41.290, Speaker B: Okay?
00:32:41.440 - 00:33:18.050, Speaker A: And this is super fast in certain contexts. So often, for example, to put something in a bucket, you just look at the higher order bits and that'll just immediately tell you which index of some array to go to, to put it in. So that's the normal bucket sort that you'd learn in undergraduate algorithms. So how do we use it here? So first of all, what are the buckets? Right? We're not assuming that the data is in the interval zero one. We're assuming nothing about the data. It's just some arbitrary comparison type model. Even if we knew what the bucket should be, there's a question of how do we figure out quickly which element goes in which bucket.
00:33:18.050 - 00:33:23.660, Speaker A: Now we can't do something like just look at the high order bits and index directly into some array, right? We're working on the comparison model.
00:33:25.710 - 00:33:26.074, Speaker B: And.
00:33:26.112 - 00:33:50.346, Speaker A: In particular, let me point. Out that the obvious thing you might be thinking about, which is suppose you had n buckets and then you basically want to just to be clear. So remember, we get a whole bunch of inputs, ID inputs from the same distribution. So in general what the algorithm is going to do is it's going to look at a bunch of instances, learn some stuff about the distribution, like learn what the bucket should be and then it's going to try to apply that knowledge to future instances, future arrays.
00:33:50.458 - 00:33:51.070, Speaker B: Okay?
00:33:51.220 - 00:34:20.922, Speaker A: So what we're going to do and with natural is to use some batch of training instances to learn what these buckets should be. But even then there's going to be this challenge where faced with a new instance, how do you figure out where each of those n elements, which buckets they belong to? Now, the first thing you might think would be, and this is just again to kind of help you understand the challenges we face in getting to this bound. You might say, well dude, how hard is it to just put one element in one of n buckets that are sorted? Binary search. Okay, but how long does that take.
00:34:21.056 - 00:34:22.090, Speaker B: Login to your test?
00:34:22.160 - 00:34:44.722, Speaker A: Binary search is log n. There's n things we got to put in buckets. Okay, so that's n log n. And this whole exercise is only interesting when this is better than n log n. So the whole point here is you want to think about H as being less than n log n. So we might be shooting for a linear time bound here. Again, go back to that case where each Xi was drawn from one of two different options.
00:34:44.856 - 00:34:45.346, Speaker B: Okay?
00:34:45.448 - 00:35:10.540, Speaker A: So we know that the permutation, we know that the most two to the n permutation. So that we know that the entropy is at most n. Then we're shooting for a linear time algorithm. So our algorithm better run in linear time. If it's ever the case that each Xi is drawn independently from only two choices, for example. So it's just not going to be okay to do binary search independently on these n different things. Okay, so this is to foreshadow some of the issues we have to address to get this kind of result.
00:35:10.540 - 00:36:15.626, Speaker A: Okay, good. So like I said, your first array shows up, you don't know anything, right? So you're not really hoping to beat n log n for the very first array you ever see. Okay, so we're just going to play it conservative, use merge sort, but also start keeping track of some statistics of what we're seeing that we can hope to exploit in future instances. So specifically and basically the goal of this training phase is to figure out what the bucket should be. All right, so for the first log n inputs, okay, so there's some constant here, like five, don't worry about it. So lambda is like log. So for the first log instances we just use merge sort say to sort the array, but then also sort of in the background.
00:36:15.626 - 00:36:25.934, Speaker A: What we're going to do for these first login arrays is we're actually going to maintain a master list of the union of all of the elements we've seen in all of these instances.
00:36:26.062 - 00:36:26.354, Speaker B: Okay?
00:36:26.392 - 00:36:27.854, Speaker A: And we're going to keep them sorted.
00:36:27.982 - 00:36:28.482, Speaker B: All right?
00:36:28.536 - 00:36:39.350, Speaker A: So we sort the first array, we get the second array, we sort that, return the output, but then we also keep a copy and merge that back in with a sorted version of the first instance. We keep doing that log N times.
00:36:39.500 - 00:36:40.102, Speaker B: Okay?
00:36:40.236 - 00:36:45.242, Speaker A: So that's login instances each an array of length n. So there's going to be n log n elements we're keeping around.
00:36:45.376 - 00:36:46.060, Speaker B: Okay?
00:36:50.430 - 00:37:03.950, Speaker A: So merge all lambda I e log n sorted lists into one which is theta of n log n elements.
00:37:06.230 - 00:37:06.980, Speaker B: Okay?
00:37:09.910 - 00:37:42.140, Speaker A: All right, so don't forget Shannon's theorem. Entropy is the same as the minimum expected encoding length of any binary encoding. We'll use that multiple times. Okay, so why do we have this master list? Well, we have this master list to identify buckets. So if the buckets were going to have any property, what would you want them to have? Well, we want them all to kind of expect to get roughly the same number. So we want stuff to be spread out as evenly as possible over the buckets that we choose. So we're going to do that in the obvious way.
00:37:42.140 - 00:38:18.950, Speaker A: So we just say let V. So we have these n log n elements, n buckets. Feels like sort of the right number shooting for like a constant number of elements in each bucket in the future. So we're just going to pick every lambda element in this sorted list. Okay, so we've got a bunch of stuff. So here's the first lambda in sorted order. Second lambda in sorted order.
00:38:18.950 - 00:38:39.370, Speaker A: We pick that guy, we pick that guy, and so on. So capital V has cardinality n, so there's N numbers and they're sorted from smallest to largest and these naturally split the real line into n plus one buckets.
00:38:39.790 - 00:38:40.540, Speaker B: Yeah.
00:38:41.310 - 00:39:55.826, Speaker A: Okay, so this is the first resolving the first challenge in I think like maybe the most natural way you could ask where does this log n come from? Like most log NS, you see, it comes from a turnoff bound, we'll talk about that, but modulo that I think a very natural way to figure out what the buckets are. Okay, so good. So the first lemma says that the buckets work, they do what we want, they spread future instances out here's exactly what they do. So with high probability, and of course we have to say with high probability because you might have gotten unlucky and just gotten these lambda instances that all have super small elements for no good reason. But of course, it's very likely that you'll get a well spread out set of elements in your first login instances in the sense that for all buckets I if you look at a fresh sample from the distribution over arrays and you look at how these fresh X one through X NS distribute into these N plus one buckets. The expected size of each bucket is constant.
00:39:55.938 - 00:39:56.310, Speaker B: Okay?
00:39:56.380 - 00:40:00.482, Speaker A: Actually, it's even better. The expected size squared of each bucket is constant.
00:40:00.626 - 00:40:01.320, Speaker B: Okay?
00:40:02.090 - 00:40:11.290, Speaker A: And we'll see in a second why that's useful, but it's certainly stronger than just saying the size is constant.
00:40:12.990 - 00:40:13.546, Speaker B: Okay?
00:40:13.648 - 00:41:09.450, Speaker A: So at most 20, say, some reasonable constant. This basically says the plan worked, that basically if we just take log training instances and define bucket boundaries in the obvious way, then future instances will be distributed quite uniformly. The expectation here, so that with high probability, is over the lambda instances that define capital B V. The expectation is over a lambda plus one instance, a fresh new instance, and how it distributes amongst capital V. All right, so what's the point? So here's what's cool. So given that this is true, we're sort of a lot of the way there as far as defining a good self improving algorithm.
00:41:09.530 - 00:41:10.014, Speaker B: Okay?
00:41:10.132 - 00:41:38.918, Speaker A: There's definitely one challenge we've talked about that we haven't solved yet. But modulo one challenge, we're done. So we have these training phases, and that's fine. We're just doing N log N runtime, and we're not really trying to do better for this first prefix of login instances. Then we define the set of bucket boundaries, capital V, and with high probability, this is true. So what does that mean? Okay, that means consider now a new instance that shows up modulo the challenge of distributing X one through X N into the correct buckets. That'll be the rest of the lecture talking about how we do that.
00:41:38.918 - 00:42:07.454, Speaker A: But if we can solve that problem, then we can finish the algorithm in linear time. Forget about this entropy term, just O of N time. Why? What's the algorithm? Okay, well, again, so we've defined capital V. We're conditioning on the fact that they're good in this sense, high probability. All right? So now a fresh instance shows up, a fresh array. We somehow distribute X one through X N into the buckets. Now we just use, say, insertion sort on each bucket separately, whatever.
00:42:07.572 - 00:42:07.950, Speaker B: Okay?
00:42:08.020 - 00:42:31.398, Speaker A: Quadratic time sorting algorithm, say, how long does insertion sort take on the ith bucket? Constant time. Right? Because the expected size squared of each bucket is of one. So by linearity of expectations summing up over the buckets, all of that sorting bucket by bucket takes only linear time. And then, of course, concatenating those sorted lists together takes always linear time with probability one.
00:42:31.564 - 00:42:32.038, Speaker B: Okay?
00:42:32.124 - 00:42:55.360, Speaker A: So this is what we do. We define the bucket boundaries in the first login we somehow distribute to be determined. But given that insertion sort in each one concatenate, and we're done. And so that's really the algorithm modulo the subroutine of distributing into the buckets. I haven't told you about. So is everyone super solid on everything so far? This is kind of all of the relatively easy stuff. And then there's just one interesting piece.
00:42:55.360 - 00:43:52.670, Speaker A: Okay, so what's the point after so basically done except for buckets? Done except for bucket assignments. That's the point. All right, there's the question, the proof. So the proof here is Churnoff bounds, and it's actually a pretty nice, pretty nice application of Churnoff bounds. So I'll sort of spell that out for you in bite sized pieces on the homework number nine. Intuitively, why would you sort of expect this to be true? Well, again, so remember, the Churnoff bound, it gives you probability bounds of the form, like e to the minus stuff. And in the stuff is the expectation.
00:43:52.670 - 00:44:42.260, Speaker A: It's often mu when you go and read about it. So the reason we've taken these sort of log n instances is to make sure that that mu is logarithmic, right? So E to the minus log means inverse polynomial. The point being is log n is where you start getting really nice concentration for surenoff bounds when you're just sort of counting what fraction of times something happens. So the fact that we have high concentration around expectations says, well, we really expect a typical instance to just look sort of exactly the same as the distribution as the union of our first log n instances, but just obviously sort of scaled down, having only a one over log factor as many elements. Okay, so you can make that precise. It's slightly tricky, but again, once it's broken down, it's pretty straightforward. So we're going to take this on faith for the rest of lecture, okay? The Buckets work in this sense.
00:44:44.310 - 00:44:44.706, Speaker B: All right?
00:44:44.728 - 00:45:01.460, Speaker A: So any questions before we talk about how to do the bucket assignments? And again, the whole point here is we need to do better than binary search, right? So we really need to compete with that entropy lower bound, which in general will be smaller than n log.
00:45:07.870 - 00:45:08.620, Speaker B: Okay?
00:45:12.990 - 00:45:24.122, Speaker A: To do the bucket classification. So for a while, let's develop the approach as a thought experiment.
00:45:24.186 - 00:45:24.366, Speaker B: Okay?
00:45:24.388 - 00:45:40.158, Speaker A: So this is not going to be realizable, but this will help us think about what is realizable. So suppose we actually did know what the di's were. So di is the distribution from which Xi is drawn.
00:45:40.254 - 00:45:40.658, Speaker B: Okay?
00:45:40.744 - 00:46:00.140, Speaker A: But I'm not going to allow you to just design some new sorting algorithm from scratch. I force you to use my bucket boundaries. I force you to use my bucket sorting algorithm, but I do allow you to know what the di's are. Now, of course, we don't know the di's. The whole point of this is we don't know the di's. So I'll remove this at the end of lecture. But for a while, let's suppose we did.
00:46:00.140 - 00:46:11.322, Speaker A: What would we do? How would we do the assignments from XIs to buckets? Well, let's think about them separately.
00:46:11.466 - 00:46:11.774, Speaker B: Right?
00:46:11.812 - 00:46:18.954, Speaker A: So we have V capital V. We have our bucket boundaries, we have our xi. It's supposed to go to its rightful bucket.
00:46:19.002 - 00:46:19.166, Speaker B: Okay?
00:46:19.188 - 00:47:03.150, Speaker A: So there are these two bucket boundaries that bracket it, that flank it. We have to figure out which ones they are. But we know Xi is distribution. So the sensible thing to do would just be like, well, let's just use the optimal search tree. So let's just literally do the sequence of comparisons which minimizes the expected number of comparisons needed to figure out which bucket Xi goes in. We know di what's stopping us, okay? So if we knew di, we could build the optimal search tree ti for each Xi.
00:47:04.770 - 00:47:05.520, Speaker B: Okay?
00:47:06.770 - 00:47:48.710, Speaker A: And again, because we're in the comparison model, there's just not that many ways conceptually that you can figure out where xi belongs. All you can do so all that's relevant is xi's relative position to the elements of capital V to the bucket boundaries. That's all we care about. We just want to find the maximum bucket boundary less than it, and the smallest bucket boundary bigger than it. So the only thing that's useful to do is to compare Xi to the elements of capital V, compare Xi to the bucket boundaries, okay? And so again, just like we wrote down a tree for sorting algorithms, we could write down a tree for one of these search trees. At the root, you compare Xi to some bucket boundary. It's either bigger than it or it's less than it, and you branch.
00:47:48.710 - 00:47:58.718, Speaker A: Then you compare Xi to something else, to some other bucket boundary, and you branch and so on. Again, remember, the bucket boundaries themselves are already sorted, right? We have this sorted list, capital V. We know what's up with them.
00:47:58.804 - 00:47:59.440, Speaker B: Okay?
00:48:00.050 - 00:48:30.394, Speaker A: So there's only a finite number of trees in the world. One of them minimizes the expected number of comparisons necessary to identify excise bucket. That's the one we should use, okay? And in fact, things are even maybe a little less crazy than it sounds. Oh, yeah. So just to give you some intuition, so if this was like a perfectly balanced binary tree, then it's going to be something like binary search. So that's sort of not the interesting case for us. The interesting case is when you can do much, much better than binary search.
00:48:30.394 - 00:48:51.758, Speaker A: So for example, suppose you knew in di, so you know the bucket boundaries, you know di, maybe you know that Xi is super likely to be in the 17th bucket, like half the time. Well, then the very first thing you should do is you should check maybe xi is both bigger than the left endpoint of bucket 17 and less than the right endpoint. And if it is, you're done. You stop in two comparisons.
00:48:51.854 - 00:48:52.258, Speaker B: Okay?
00:48:52.344 - 00:49:18.566, Speaker A: So that's the kind of thing you're doing that's smart with these search trees. You're sort of checking for common cases with very few comparisons. All right? Now not only does such a search tree exist, an optimal search tree exist because there's only finitely many. But actually, you can compute these things in polynomial time. I don't know if you've ever seen this. Sometimes I teach this in undergrad algorithms as a dynamic programming exercise.
00:49:18.678 - 00:49:19.340, Speaker B: Okay.
00:49:20.350 - 00:49:40.258, Speaker A: And this is something I'll put on the homework. So if I give you di and I give you the bucket boundaries, you can actually figure out the optimal search tree. And again here, optimal means minimum expected number of comparisons where the expectation is with respect to di. You can compute that tree in cubic time.
00:49:40.424 - 00:49:49.540, Speaker B: Okay. Yeah.
00:49:50.250 - 00:49:52.760, Speaker C: Why isn't each bucket of the same size?
00:49:56.010 - 00:49:57.400, Speaker A: What do you mean by size?
00:49:58.090 - 00:50:10.182, Speaker C: Or has the expected I thought we set the buckets in such a way that every element out of di has the same probability.
00:50:10.326 - 00:50:42.180, Speaker A: No, that's not what it is. What it is is that if you look over all n of the elements so from each bucket's perspective, it's going to see roughly the same number of the n elements. But it could be the bucket number one is very likely to get x one, x three, or an x five, whereas bucket number two is very likely to get x ten, x 15, and x 23. Okay, so here we're thinking from the perspective of the buckets. How many people do I see here? We've switched. We're thinking of the perspective from an element. Which buckets am I going to wind up in? So it's like two sides of a bipytype graph, if you like.
00:50:42.180 - 00:50:44.194, Speaker A: So this is saying one side of.
00:50:44.312 - 00:50:46.310, Speaker C: X I and the same for x.
00:50:46.460 - 00:50:58.230, Speaker A: For x j. Yeah, they could be completely different. I mean, again, keep in mind, again, in particular, imagine this thing where each xi takes on only two different values, and these could be totally different pairs of values for the different XIs, right? Yeah.
00:50:58.380 - 00:50:59.080, Speaker B: Okay.
00:51:01.390 - 00:51:08.240, Speaker A: In general. So this whole search tree thing we're going to do totally separately for each xi. The different XIs really have sort of nothing to do with each other.
00:51:11.250 - 00:51:11.998, Speaker B: Okay?
00:51:12.164 - 00:51:24.686, Speaker A: So in fact, you can even sharpen this. So Knuth came up with a really nice trick where you can actually bring this down to n squared as well, but the relatively straightforward dynamic programming algorithm gives you an n cubed.
00:51:24.798 - 00:51:25.460, Speaker B: Okay?
00:51:25.910 - 00:52:15.060, Speaker A: So it's not crazy fast, but it's polynomial time, so good. It okay. So if you knew the di's, that's clearly what you'd do if you had enough time. But now there's this sort of a nontrivial question here, which is, is even this good enough to compete with the lower bound of n plus the entropy of the distribution of repermutations? And even doing these search trees optimally, there's no guarantee that this is good enough. Why not? Because that n plus entropy term is a lower bound on any sorting algorithm of your wildest imaginations. So what I've done here is I've fixed this sort of outer scaffolding. I'm saying I'm insisting that you use bucket sort.
00:52:15.060 - 00:52:44.902, Speaker A: And I'm insisting that you kind of use these end bucket boundaries and then only within this inner subroutine of bucket classification am I allowing you to fully optimize other distribution. So who's to say you haven't fundamentally lost on the number of comparisons needed by being stuck in this bucket sorting algorithmic framework? Okay, so the next question I want to answer, what I want to prove to you is actually there's no loss of generality doing bucket sorting. If you could have the luxury of using optimal search trees for the buckets, actually you'd meet the entropy bound.
00:52:45.046 - 00:52:45.402, Speaker B: Okay?
00:52:45.456 - 00:52:57.790, Speaker A: And that's actually kind of maybe the most interesting part of the whole lecture, that part. So there's still going to be the issue of sort of getting rid of this assumption that we know the DS because we don't. But this is really the key point coming up.
00:52:57.860 - 00:52:58.480, Speaker B: Okay?
00:53:01.010 - 00:54:21.020, Speaker A: All right, so here's how we're going to phrase that statement that we really lose nothing other than constant factors by insisting on this bucket sort approach. So let Bi be the distribution over buckets, over excise bucket throughout this whole discussion. Now I'm thinking of capital V, the bucket boundaries, they're fixed, okay? So the training phase has happened. I'm conditioning on this event that the bucket boundaries are good, okay? So those are now fixed. So the randomness is only over a future instance, all right? So only over the randomness in Xi, in other words. So now suppose we do do this optimally. How many comparisons do we have to make in order to assign all N of the elements, all of X one through X N, to their rightful buckets? Well, by Shannon's theorem, now we're using it in the opposite direction that we were using it before.
00:54:21.020 - 00:54:42.100, Speaker A: So before we just said that any encoding length is lower bounded by the entropy. Now we're saying, oh, given the entropy, we can actually exhibit an encoding that matches that bound. So by Shannon's theorem, expected number.
00:54:45.510 - 00:54:45.874, Speaker B: Of.
00:54:45.912 - 00:55:12.054, Speaker A: Comparisons to classify Xi is like the entropy in Bi. So remember when I talked about Shannon's theorem, it was just with respect to an abstract finite set, capital X. The first time we used the theorem, capital X was all permutations. That was a big set. So it was like N factorial things. Okay, now we're using it again, but the set has changed.
00:55:12.102 - 00:55:12.362, Speaker B: Okay.
00:55:12.416 - 00:55:35.330, Speaker A: Capital X. Now before we were doing sorting, now we're just doing searching for a particular element, xi. So capital X is just the N plus one different buckets. Okay, but whatever chance theorem still holds, no problem. Okay, so then here's the key lemma, and this one I will prove.
00:55:38.310 - 00:55:38.674, Speaker B: Which.
00:55:38.712 - 00:56:11.530, Speaker A: Is that was probably one over V. So again, we're thinking of V. The bucket boundaries is just totally fixed. We're going to compare on the one hand, the number of comparisons that we're going to need to use in this bucket sort framework. So that's going to be for each element I the entropy of the distribution over its bucket. And we have to do that for each of X, one through XN. So this is what we pay in our algorithm if we use optimal search trees.
00:56:11.530 - 00:56:24.930, Speaker A: And we need an upper bound on this. So we want to say that we're not losing anything by just using this bucket sorting approach. So we want to say is big O, and in here we're just going to use the upper bound. That's our target.
00:56:29.590 - 00:56:30.340, Speaker B: Okay?
00:56:31.270 - 00:57:03.520, Speaker A: So remember, this is what we said was our line in the sand all along, okay? We were really shooting for an algorithm that had this running time. We argued that we couldn't do better than this running time no matter what. Anyways, okay, so this is what we're going to pay if we have the luxury of using optimal binary search trees. And it is indeed at most a constant factor times what we were shooting for. Anyways, okay, everything clear. I've actually sort of cheated slightly right here. I don't know if anyone noticed little sleight of hand.
00:57:03.520 - 00:57:52.794, Speaker A: So the little sleight of hand I did is if you go back to Shannon's theorem, it says so what does it do? So it promises you can actually match the entropy bound well, so it promises you can match the entropy bound with some binary encoding scheme. Now, when we were using the forward direction of Shannon's theorem, we noticed that, oh, a special case of an encoding scheme is those induced by sorting algorithms, those induced by binary search trees. And given that we were doing a lower bound, that was fine for us. So the entropy, lower bounds, the expected length of any binary encoding scheme, in particular one arising from a correct sorting algorithm. But here there's an issue that we're trying to do the reverse direction, right? So the Shannon's theorem I wrote down only promised you some encoding scheme that meets the entropy, not necessarily an encoding scheme realizable by a search tree.
00:57:52.922 - 00:57:53.600, Speaker B: Okay?
00:57:54.130 - 00:58:03.090, Speaker A: But fortunately, since the 70s, people have known that actually you can even get away with using an encoding scheme induced by a search tree. Okay, so this is actually true.
00:58:03.240 - 00:58:04.002, Speaker B: All right?
00:58:04.136 - 00:58:25.530, Speaker A: And if I have the energy, maybe I'll put that as a problem on the homework as well. So just wanted to be honest about that little cheat. So the point is, we really can realize this up to an o of one term using optimal search trees. So this really does characterize the best case performance using the optimal tis of the bucket sort approach.
00:58:27.070 - 00:58:27.466, Speaker B: All right?
00:58:27.488 - 00:59:22.550, Speaker A: So any questions before I prove this to you? And the proof of this is really very clever, I have to say. It's not that hard, but it's very clever. So what do we want? We want an upper bound. We want an upper bound on this. That's the point of this. We want an upper bound on this. Of that, you know, the forward direction of Shannon's theorem says, well, if you just want an upper bound, just exhibit any encoding scheme.
00:59:22.550 - 01:00:36.334, Speaker A: So the entropy equals the best encoding scheme, so any encoding scheme gives you an upper bound. So that's going to be our high level approach. So we'll upper bound left hand side by suitable binary encoding of everybody's buckets, okay? So of the bucket of x one, blah, blah, bucket of X n, trying to give you a binary encoding of all N of these things, all right? And the expected length of it is going to be this. All right, so what's the encoding? I'll now use Shannon's theorem in the opposite direction and say, well, first, okay, so we're given x one through x n, and I have to tell you what the encoding is of these. So given X one through x n, the first thing I do is I encode their relative ordering in binary. How do I do that? I do that in the optimal way, whatever it is, given the distributions d one through DN.
01:00:36.462 - 01:00:37.140, Speaker B: Okay?
01:00:38.390 - 01:00:58.038, Speaker A: So optimally encode the relative ordering of the x eyes. In other words, I encode the permutation that x one through x n induce.
01:00:58.214 - 01:00:58.940, Speaker B: Okay?
01:01:06.610 - 01:02:16.030, Speaker A: So the expected length where the expectation is over the choice of x one through x n, what's that expected length going to be? It's something which is on the board. This. So about Shannon's theorem, there exists up to a plus one, there exists a way to encode the permutation so that on average, with respect to the di's, this is the length. Okay, so expected length, entropy of the induced distribution over permutations. Okay, so that's the first thing. All right, so again, what are we encoding? What's it a function from? It's a function from x one through x n to the identities of the bucket of x one through the bucket of X n. So the first part of this encoding is just what's the relative ordering? So now that I've read this part of the encoding that lets me sort x one through X n, so now I have a sorted version of them without loss of generality.
01:02:16.110 - 01:02:16.740, Speaker B: Okay?
01:02:17.590 - 01:03:06.260, Speaker A: So how do I use this sorted version of x one through x n to figure out sort of in a single shot what everybody's buckets are? Well, let me ask you to take 30 seconds and think about the following, okay? Think about we have two sorted lists now, okay, so we have a sorted version of X one through x n, and we also have the bucket boundaries, capital V, that's another sorted list. Okay, so convince yourself over the next minute that you can read off the identities of the buckets of all of x one through x n with a simple merger of these two sorted lists. So you take the sorted version of x one through X n, you take capital V, which is already sorted from low to high you merge them.
01:03:08.550 - 01:03:08.914, Speaker B: And.
01:03:08.952 - 01:03:12.260, Speaker A: At the end of that you figured out which bucket everybody's in.
01:03:16.010 - 01:03:16.786, Speaker B: Agreed?
01:03:16.898 - 01:03:23.818, Speaker A: So it's just like the combined step and merge sort, right, where you have these two lists and you're looking at both of them and you say whichever one is smaller, you copy over first.
01:03:23.984 - 01:03:24.700, Speaker B: Okay?
01:03:25.150 - 01:03:48.850, Speaker A: So if you're one of these XIs, when you reach the head of the list, maybe some bucket boundaries get copied over before you, but then whatever that last bucket boundary that you're compared to, you lose and you get copied over before that bucket boundary, that's your right endpoint, right? So the last bucket boundary to which you are compared in this merger is exactly the right endpoint of your bucket.
01:03:49.510 - 01:03:50.222, Speaker B: Okay?
01:03:50.376 - 01:05:30.726, Speaker A: So how many comparisons do you use when you merge two sorted lists, both of length o of n, o of N, right? Just for each copy, it's one comparison. So given the XIs in sorted order, all you need are an extra N comparisons to encode what happens in this merger of these two lists. Ergo, to encode the bucket identities of x one through x N. Okay? So you first encode the relative ordering. From that you can extract the sorted ordering and then you're only O of N away with probability one from describing everybody's buckets. So encode O of NRD XIs with capital. And now again, just to put it together, so suppose you told me, so there's some x one through x n in the background, okay? And I don't know what it is, but suppose you wrote down bits that allowed me to reconstruct the relative ordering and then you wrote down some more bits that allowed me to simulate this merger of the sorted version of the XIs with the bucket boundaries, right? That's all I need to know to deduce what all of these things are.
01:05:30.828 - 01:05:31.142, Speaker B: Okay?
01:05:31.196 - 01:05:38.326, Speaker A: So in that sense, this is a valid encoding. These bits plus these bits are enough to encode this for every x one.
01:05:38.348 - 01:05:39.960, Speaker B: Through x N. Okay?
01:05:43.230 - 01:05:58.330, Speaker A: So from one plus two can compute, I e can uniquely reconstruct all of the bucket of the XIs.
01:05:59.090 - 01:05:59.840, Speaker B: Okay?
01:06:00.290 - 01:06:33.580, Speaker A: And again, the expected encoding length here is this entropy. Forget about expected. With probability one, the encoding length here is N. Okay? So overall the expected length is N plus the entropy of the induced distribution of repermutations. And then of course the optimal encoding, which is the one that characterizes the entropy of what we're trying to encode, can only be better.
01:06:34.510 - 01:06:34.874, Speaker B: Okay?
01:06:34.912 - 01:06:36.410, Speaker A: So this is an upper bound.
01:06:41.390 - 01:06:42.140, Speaker B: Okay?
01:06:49.640 - 01:07:33.820, Speaker A: I did one other sort of sleight of hand here, which is what I gave you is I gave you an encoding of the sort of joint distribution of the buckets of x one through x n, whereas what I wrote here was just a sum of the marginals so sum of the entropies of each of the marginal distributions. But the x size are independent. Okay? So sort of. An easier to prove in standard fact is that if you have a product distribution so independent random variables, then their entropies just add, okay? So in fact this is the same. In other words, this equals the entropy of b one through BN because the XIs are independent.
01:07:33.900 - 01:07:34.192, Speaker B: Okay?
01:07:34.246 - 01:08:44.580, Speaker A: If they weren't independent then altogether they would have strictly less information, right? So you would get an overestimate in general if they were not independent. So just think about like x one and then x two. So think about like a fair coin flip and then the flip of that fair coin flip. So each marginal would give you one bit, so that would give you two bits in the sum, but there'd only be one bit overall in the joint distribution. So what I really gave you was an encoding for this, but that's good enough for this by independence. So any questions about that? If you're not used to these sort of information theoretic proofs, it might look a little mind boggling first time around, but they're pretty cool questions. All right, so where do we stand? What we've proven is that if we actually had the luxury of knowing the di's in advance and we had the luxury of using optimal search trees to classify people into their buckets, that would be good enough, which again is not obvious, but that would be good enough.
01:08:44.580 - 01:10:08.668, Speaker A: So what remains is to understand how to basically simulate this argument without losing anything, even though we don't know the di's up front and therefore are not in a position to compute optimal search trees. So unknown di's. So here's sort of the key insight. So suppose you had one of these search trees which in general in the interesting case are very unbalanced, okay? So balanced search trees are kind of very boring when you're talking about sort of optimal average case things, right? So you're only beating the log n bound if you have some stuff which is super shallow frequent stuff where you're using many fewer than login comparisons. Now see, the observation is suppose instead I just sort of like chopped this tree off in the middle, okay? Suppose I just sort of got rid of all of the lower layers. Now you'd have a justified complaint which says, well, but now it's not even like correct, right? So now what happens? You do some comparisons and if it's sort of this great, you know, what bucket you're going to but what if you just like fall off, right? There's still ambiguity. You don't know which of the buckets it belongs to.
01:10:08.668 - 01:10:25.860, Speaker A: So I say fine, I'll give you a safety net, I'm going to chop off the tree. And if you fall off, you just resort to basic binary search. Now suppose I make this level epsilon log n, where epsilon is a constant zero one, something like that.
01:10:26.010 - 01:10:26.710, Speaker B: Okay?
01:10:27.720 - 01:11:02.400, Speaker A: The observation is that only the first epsilon login levels matter. This is of the optimal binary search trees. Ti why? Well, suppose you chop off the bottom and resort to binary search. What does it cost you? Well, now there were these cases where you would follow the tree, but if you fell off the bottom, you knew even in the old days you were paying at least epsilon login comparisons. Now you have to have this binary search tree. You're paying log n comparisons. So you've got a one over epsilon blow up in your search time.
01:11:02.400 - 01:11:31.572, Speaker A: So if epsilon is zero one, you have a ten x blow up in your search time. All right? So what, you took these cases where you already were losing log and you're still losing log, so you lose nothing. So it's some kind of like space time trade off. Because notice the old search trees in general had length have space n. But here, if you only have epsilon log n levels and it's a binary search tree, the largest number of nodes you could have would be two to the epsilon log n, also known as n to the epsilon. Okay, so these are small trees, but they give you kind of all of the bang for your buck.
01:11:31.716 - 01:11:32.410, Speaker B: Okay?
01:11:34.140 - 01:12:16.216, Speaker A: So I'll just kind of hand, you know, we're running low on time, so I'm just going to sort of hand wave how you do this, although everything I'll say actually, you know, can be made precise and a little bit of it will show up on the homework. So again, it's not like we can compute this either, right? So so what did we just argue? So now we said, suppose I told you the di's, I let you compute the optimal search trees, the tis, but then I chopped them off, you'd still be fine, and you're like, wow, but I don't know the di's, I can't give you the tis, it's still not helpful. But there's sort of a lesson you can take away from this, which is that only sort of end of the epsilon things actually matter.
01:12:16.398 - 01:12:16.936, Speaker B: Okay?
01:12:17.038 - 01:13:00.740, Speaker A: That's all you need to actually sort of match the entropy bound. So now we're going to do something actually kind of much more straightforward, which is so what you do is you see, you keep some more statistics. So for all xi, you keep track. Again, this is over some more training phases. So before we had log n training inputs. Now we're going to have a little bit more we're going to have end of the epsilon training inputs. So keep track of xi's most frequent buckets, just empirically, and by frequent I mean probability or frequency, at least one over N to the epsilon.
01:13:01.800 - 01:13:02.550, Speaker B: Okay?
01:13:08.620 - 01:13:21.924, Speaker A: So a different way of saying this is you're going to add roughly N to the epsilon times login extra training phases.
01:13:22.052 - 01:13:22.308, Speaker B: Okay?
01:13:22.334 - 01:13:48.710, Speaker A: So before we had login, we were using that to construct the capital v list. Okay, so this is more, but it's not crazy, it's end of the epsilon where epsilon is as small as we want, end of the epsilon times, log n. And now for each xi, so we have these buckets, right? We have these n buckets. And so now what I do is I look at how many of those buckets does xi show up in in the training phases, at least log n times.
01:13:50.440 - 01:13:51.092, Speaker B: Okay?
01:13:51.226 - 01:14:30.768, Speaker A: So there's n to the epsilon times log training phases and there's n buckets. So in a typical bucket, it's never going to show up. In a typical bucket, it's going to show up well some number of times, okay? So basically what you do is you look at this many training phases and you look at which buckets it shows up in the most frequently, and then you build a search tree only on those most frequent buckets. And that's a simulation. So you're trying to identify the end of the epsilon most frequent buckets. And that's meant to be a proxy for the end of the epsilon nodes that would show up in ti in the first epsilon login levels. So that's the way you actually implement this.
01:14:30.768 - 01:15:04.636, Speaker A: You use this to say, oh, morally, end of the epsilon element should be enough, but I don't want to actually compute this tree. So let me just do the obvious thing and see which buckets does xi show up in the most often. And turns out that works well. Any questions about that? And this works for arbitrary distributions, say it again.
01:15:04.738 - 01:15:08.060, Speaker C: The XIs could be very strangely distributed.
01:15:08.800 - 01:15:44.852, Speaker A: Yeah, they're independent, but that's all you need to assume. Yeah, it's sort of funny. So if xi is uniform over the buckets, then on the one hand the algorithm is doing something very stupid because it's just like totally random. So notice, I mean, the number of training phases is still far fewer than the number of buckets, okay? So if xi is uniform over the buckets, it's going to be in each of these once and you're just going to get a random sample of the buckets and you're going to build this sort of useless search tree. But hand, there is no good search tree if xi is uniform over the buckets. On the other hand, suppose xi actually showed up in this one bucket with 10% probability. Then if you do this many training phases, you're going to notice.
01:15:44.852 - 01:16:28.308, Speaker A: So you're going to be like, whoa, that's a bucket we need to remember for xi, okay? So more generally, as long as the probability is at least one over n to the epsilon, that's when you're going to be expecting. So say in some bucket the probability is one over end of the epsilon or higher, then the expected number of times xi will go in that bucket in the training phases is log n, which is exactly when churnoff bounds kick in. So the point is any probability of a given xi in a given bucket being bounded below by one over n to the epsilon. That's exactly the frequencies that you're in a position to estimate accurately using churnoff bounds. So that's what the algorithm does. It just says, let's just look for any buckets. If there are any where we see a lot, we're only going to bother to build the search tree on that subset of buckets.
01:16:28.308 - 01:16:36.810, Speaker A: Morally, that should behave just like the upper epsilon login levels of ti, and it's actually not that hard to prove that that's true. Yeah.
01:16:39.200 - 01:16:50.000, Speaker C: Somehow continue the training if we realize or merge this training phase, I guess it's kind of difficult because building up the tree is quite expensive.
01:16:50.340 - 01:17:44.896, Speaker A: Yeah, I mean, there's different ways you could think about this is relevant to the final scorecard and this is the last thing I'm going to say for the lecture, but just there's been a lot of pieces and I know I've given it to you sort of one piece at a time, so let's just make sure we're clear where we are. So first of all, how many training instances do you need or how many samples do you need to learn stuff? So again, epsilon is just some constant, okay, that's under our control. You just have to suffer a one over epsilon blow up in the search time. So we have n to the epsilon times log n samples. We have a logarithmic number of samples to discover the bucket boundaries because we only needed log to have that lemma one, which is the bucket boundaries do their job of spreading things equally but then to get good estimates. Of the frequent buckets for each xi. That's when we needed to boost this up to end of the epsilon times log n.
01:17:44.896 - 01:18:23.592, Speaker A: Okay, now you can ask the question, should I think of those as disjoint sets of samples or should I use the same samples? For both? Doesn't really matter. I mean, for the analysis it's simpler to keep them separate, but it's not going to be a big deal, I don't think, in actually the implementation. So as far as the space, well, for building up the v list, we just needed to remember log n instances. So that was n log n space when we were constructing the bucket boundaries. But then again, we are in this implementation keeping track of potentially a search tree on end of the epsilon buckets for each of the n elements.
01:18:23.736 - 01:18:24.092, Speaker B: Okay?
01:18:24.146 - 01:19:10.300, Speaker A: So the space is going to be n to the one plus epsilon, n to the epsilon for the tree corresponding to xi. So there's n log n runtime for the training inputs because we're just running merge short before we know what we're doing. And then the really cool thing is there's n plus the entropy of the induced distribution on permutations post training inputs. So after this many inputs.
01:19:15.600 - 01:19:17.404, Speaker B: Okay, so.
01:19:17.442 - 01:19:28.764, Speaker A: It'S n log n for a while for end of the epsilon, and then it drops to whatever it has to be. There's sort of this one extra thing where it's not quite clear where to account for it, which is you do have to build those search trees.
01:19:28.892 - 01:19:29.568, Speaker B: Okay?
01:19:29.734 - 01:20:14.704, Speaker A: So in between the training phase and the post training phase to transition from n log n to n plus entropy, you have to build those search trees and that's going to be n to the one plus three epsilon time. So this is using that. So remember, each search tree is on end of the epsilon elements. And I said that there was a cubic algorithm using dynamic programming. You'll do that on homework. If we wanted to use the clever knuth version, we could get this down to one plus two epsilon to build search trees. So this is somehow like in an interlude in between the training phase and the post training phase, some extra work that has to be done.
01:20:14.822 - 01:20:15.344, Speaker B: Okay?
01:20:15.462 - 01:20:56.670, Speaker A: And again, just to conclude, just to make sure you keep track of the algorithm, because the algorithm is actually very simple and nice at the end of the day, right? So you just compute the bucket boundaries in the obvious way log instances, and you take one every log of the union of all the lists you've ever seen so far, then that determines the buckets for each xi. For the rest of the training phases, you just keep track of the frequency with which xi lands in each bucket. You remember the top end of the epsilon. You use that dynamic programming algorithm to compute the search tree on those end of the epsilon buckets for each of the elements. And then in the steady state, you just use those search trees to classify people into buckets insertion sort in each bucket, concatenate the result. So that's where all these come from. Questions?
01:21:00.000 - 01:21:00.412, Speaker B: Yeah.
01:21:00.466 - 01:22:12.820, Speaker A: Andy, does anyone ever do this with adjusting binary trees like a slate tree or something? By this you mean could you use a slate tree? Instead of using the cubic algorithm to build the perfect search trees, could you use a slate tree or something? Yeah, let's see. So in this context, it's a good question and that would also maybe help if the distribution changed over time or something, right? What's a little weird here. So you're absolutely right that when I talked about there is the static optimality property of splay trees, which says that it will eventually get to the point where it's as good in expectation as the best fixed tree if data happens to becoming ID from some distribution. So that's relevant. Here the thing about this optimization, I'm a little unclear on it. I'd have to think about how it plays out. So here like the membership, you're using this separate sort of estimation subroutine to figure out who even gets to go in the tree in the first place.
01:22:12.820 - 01:22:29.818, Speaker A: And it certainly seems plausible you could somehow blend splay tree machinery with that sort of selection of who's in the splay tree. But one would have to think about exactly how it works. The splay trees are so flexible, it seems very plausible it could be done. It's a good comment. Yeah.
01:22:29.984 - 01:22:43.390, Speaker C: We're not in the comparative model then we could basically use a very simplified version of this to get a better bucket sort or a bucket sort. We don't have a prior on the distribution.
01:22:45.970 - 01:23:05.302, Speaker A: So what do you want to assume when you say you're not in the comparison model? You have to say what you are assuming. So like, are elements in zero one or I mean, if elements are sort of arbitrarily spread out on the real line, it becomes a little less clear what to say. If they're in some bounded range, then one could certainly I mean, for example.
01:23:05.356 - 01:23:11.830, Speaker C: If we could directly put them index them into the buckets.
01:23:13.690 - 01:23:30.300, Speaker A: Yeah, if it's some very heavy tail distribution, it's not clear. Yeah. Once you start classic bucket sort is meant for sort of bounded range data. And so I agree, for bounded range data, you'd sort of expect to do something better.
01:23:30.750 - 01:23:33.550, Speaker C: We could learn some of the distribution.
01:23:34.370 - 01:24:03.814, Speaker A: Yeah, this is one comment I want to make. One thing that's really nice about this result is it's not explicitly learning the distribution. Like if you think about the distribution, the distribution is over permutations. So there's like N factorial free parameters in this distribution potentially. And so you'd need N factorial space, you need N factorial samples to really write out the distribution in the naive way. And you look at sort of what this thing's doing? Right? The space is like, okay, it's super linear, but not by much. The sample complexity is really good.
01:24:03.814 - 01:24:48.390, Speaker A: I mean, this is sublinear sample complexity. This is heavily using the fact that the di's are independent. I mean, so you really can't get this report card for general distributions, but for the independent case, it's a huge, huge win over the obvious learn the distribution approach. Now, if it's a bounded distribution, maybe there aren't that many things that are relevant that you need to learn. But one thing to keep in mind is one reason we're in the comparison world is because we need lower bounds. So there's an analogy here with the instance optimality discussion long ago where if you want to prove you're as good as anything else, which is what we're doing here, because we have this information theoretic lower bound on what anything could do. So to prove an optimal result that's so strong, you need to be working on a problem in a model where you can prove lower bounds.
01:24:48.390 - 01:25:07.800, Speaker A: So that's one of the sort of features of the comparison model is you know what the yardstick is. So you can really show nothing that's not comparison based could possibly do better. The lower bound is going to totally break down if you go outside the comparison model. So then you have to say, what are you competing with? Probably you're just competing with N at that point.
01:25:10.690 - 01:25:11.198, Speaker B: Okay.
01:25:11.284 - 01:25:11.770, Speaker A: See you Wednesday.
