00:00:00.330 - 00:00:45.542, Speaker A: This lecture, we're going to start speaking about smooth analysis in earnest. So let me just sort of remind you the salient points of smooth analysis that I told you about last time. So this is an example of a hybrid model where you're trying to take aspects of average case analysis, aspects of worst case analysis, and hopefully get the best of both worlds. From average case you're hoping to be able to make really strong and meaningful performance guarantees and, and from the worst case you're looking for things which are robust guarantees, so not overly tailored to some specific distribution. And smooth analysis is one particular hybrid model. We'll be looking at some others and the way we described it is that there's an adversary. So this is the worst case aspect, picks some initial input.
00:00:45.542 - 00:01:12.782, Speaker A: Okay, so you have some algorithm you're going to use and an adversary picks intuitively the worst input for the algorithm that you're using. But then nature sort of saves the day. So nature adds a perturbation. So add some randomness to the input. So it's sort of worst case over the starting point and then average case over this ideally small perturbation that nature adds.
00:01:12.846 - 00:01:13.362, Speaker B: Okay?
00:01:13.496 - 00:01:58.366, Speaker A: So as the perturbation gets smaller and smaller and smaller, we're becoming worst case analysis. As it gets bigger and bigger and bigger, we're basically doing average case analysis so the size of the perturbation interpolates between those two models. So when is smooth analysis generally useful? So it's generally useful when the worst cases for an algorithm seem really brittle, like they're going to be destroyed by a perturbation. Now, these bad examples are all really complicated, so you won't actually see one in lecture, it would take too much time. But the smooth analysis examples we'll be doing, they do have that feature. So the Clay mint EQ for simplex, if you sort of see it, you see it's sort of a nice edge example. Also for local search, which we'll talk about today, there are bad examples in theory, but they're very brittle.
00:01:58.366 - 00:02:29.450, Speaker A: It's just clear that they would be fragile and break down under a perturbation. Now, of the various cost measures for algorithms we're looking at, it seems like running time is usually the measure which has these very non robust pathological inputs. So that's where we see the killer applications for the most part. Still to this day, the original application is the most famous one, the simplex method for linear programming. We talked about that a bit on Wednesday from 30,000ft. That's all I'm going to say about it in class. The references on the website have more if you want to delve in more deeply.
00:02:29.450 - 00:02:40.880, Speaker A: But today I want to give you a self contained account of another genre of problems that are right in the wheelhouse of smooth analysis, namely local search.
00:02:44.290 - 00:02:45.040, Speaker B: Okay?
00:02:46.450 - 00:03:21.370, Speaker A: And one feature of discussing local search a little bit is like while smooth analysis has had some really nice successes in this space. In my opinion, there's still some really juicy open questions which just someone has got to solve. So I'll talk a little about a little bit about those at the end of the lecture, just to remind you. So local search at a high level. So what you do is you have an optimization problem. You maintain a feasible solution. You have a notion of local moves that are permitted at each step and you keep iteratively locally, improving the solution until you can't do so anymore and then it halts at some locally optimal solution.
00:03:21.370 - 00:04:09.530, Speaker A: So that's local search in general. I'll give you a specific example in a second. So why is local search crying out for a smooth analysis? Well, so it turns out local search algorithms often do suffer, at least in theory, from pathological inputs. So they're often exponential worst case, by which I mean from a worst case starting point, you might have to do an exponential number of improving moves before you stop at a locally optimal solution. On the other hand, you run these in practice and you never ever see these exponential examples. Okay, so it depends on the problem. Some but think quadratic or even a little bit better running time typically observed in practice.
00:04:09.530 - 00:05:07.006, Speaker A: Okay, so this combination is the first thing that makes you wonder about smooth analysis. And then if you actually inspected the bad examples and saw how brittle they were, you'd really start thinking you want to do a smooth analysis. Okay, so those are sort of the clues that's the right analysis model to be using to reason about local search algorithms and their convergence time. Okay, so that's local search just totally generically. We're going to do a case study for a fundamental problem that hopefully you all have heard of at some point in your life, the traveling salesman problem, or Tsp. And we'll be looking at a special case of the traveling salesman problem. Where the okay, where the city? So the story is right, so you have a traveling salesperson and they have a bunch of stops they need to make.
00:05:07.006 - 00:05:58.110, Speaker A: It doesn't matter what order they make them in, they just have to go to each place exactly once. And so the places we're going to assume the salesperson has to go are points in the plane. And moreover, we're going to, for simplicity, be using the L one metric to measure distance between two different points in the plane and a lot of that's sort of for simplicity to keep the lecture simple. So that's what we'll be thinking about. More general results are certainly possible. So input endpoints in the plane actually, let's even say in the unit square, which is almost the same thing. So we're given endpoints in the unit square and the goal is to compute a tor.
00:05:58.110 - 00:07:01.874, Speaker A: So a path that closes back on itself that visits each point exactly once. So really an ordering of the endpoints that minimizes the total distance traveled. So remember, each Pi, this is just an X coordinate and Y coordinate, and then distance between consecutive stops, we're going to use the L one norm, same L one norm that we talked about when we were doing compressive sensing, right? So just the sum of the absolute values of the in this case, only two coordinates, also known as Manhattan distance in this context sometimes because sort of Manhattan like the number of blocks, you have to go total going both east west and north south. Okay? And so similar results are possible for other norms. This is mostly for pedagogical reasons. I'm just going to stick with the L one norm. Okay, so you probably know that the traveling salesman problem in general is NP complete or NP hard.
00:07:01.874 - 00:07:36.474, Speaker A: Even this special case is NP hard. So if you don't want to work really hard and solve it exactly, then instead one considers heuristics. There is what's called a PTAs, or a polynomial time approximation scheme for this special case. So for any epsilon greater than zero, there is a polynomial time algorithm that guarantees you a tour within one plus epsilon times the minimum possible. However, it's very slow. It's really just kind of a theoretically interesting algorithm. So that's not how people solve it in practice.
00:07:36.474 - 00:07:54.670, Speaker A: If people don't solve it exactly, then instead they use heuristics. Local search for 50 years has been one of the main Heuristics used to generate approximate solutions to Tsp instances. Okay, so that's what we want to talk about today. All right, so this will be the problem for the whole lecture.
00:07:54.750 - 00:07:57.060, Speaker B: Clear? Okay.
00:07:57.750 - 00:08:29.774, Speaker A: Now, when you talk about local search, so abstractly, in each iteration, you make a local move, which is improving. So local search is only defined with respect to some notion of what are the local moves permitted. So you have to define the neighborhood of each feasible solution. So there's many ways to do that, including for the Tsp problem. But we're going to look at a very simple and well known and old way of doing this, called the two op neighborhood. And so this is nice. So you maintain a tour at every step.
00:08:29.774 - 00:09:18.894, Speaker A: You start with some arbitrary tour. Like, initially, you just go through the points in order of name, say. And in a generic iteration, if you look at two edges in the current tour, so maybe at some point, you go straight from U to V and straight from X to Y. And the rest of the tour is whatever it is. What you're allowed to do is you're allowed to cut these two links and rewire. Now, if you think about it, there's a couple of ways to rewire this. But if you rewire U to X and V to Y, then you've taken your one cycle and turned it into two cycles, right? So you don't want that.
00:09:18.894 - 00:09:53.418, Speaker A: That's not a tour. So that leaves the interesting thing to do is to connect V to X and U to Yor, by definition stays exactly the same. Okay, so that's a two up move. You have your current tour, it has n edges. You rip out two of them. There's a unique way to rewire them that gives you another distinct tour from what you started with. That's a local move.
00:09:53.418 - 00:10:30.178, Speaker A: Okay, so you have flexibility because you can choose these two edges however you want from the current tour to break it up. But that's the set of local moves available. All right? And so then the algorithm that we're going to be interested in is just generic local search under this neighborhood with the two up neighborhood. So you just execute, so you never do anything stupid. That makes things worse. Okay, so if you sort of in your mind think about doing this rewiring and you notice it takes you longer, you never do that if there's many different swaps that make you better. We're just thinking that you pick one arbitrarily.
00:10:30.178 - 00:10:45.930, Speaker A: Okay, so if there's some improving move, then you do it. So execute improving local moves until you can't anymore. So till none remain.
00:10:47.070 - 00:10:47.820, Speaker B: Okay.
00:10:49.710 - 00:11:19.734, Speaker A: So by definition the length of the tour will be strictly decreasing with the number of iterations. If you think about it, that also means that this must eventually terminate. There's only an n factorial number of different ways, actually less than that. By symmetry that you could go through all of the points. That's a finite number. So eventually you'll run through all of the cases and because it's strictly decreasing, you can't cycle. All right, now n factorial is a lot.
00:11:19.734 - 00:11:32.838, Speaker A: Of course you'd hope that you'd find a locally optimal solution much faster than that. Oh, I should also note that a given iteration, it's not hard to implement in polynomial time.
00:11:32.924 - 00:11:33.234, Speaker B: Okay?
00:11:33.292 - 00:12:17.046, Speaker A: So for example, n cubed is super easy because there's only kind of n squared different edges to try ripping out. And then in linear time, you can compute the result of the new tor and you just take the best one. Okay, so there's only polynomial many options. So it's easy to execute one iteration in polynomial time. Really the question is how many iterations are required and n factorial is not such a great upper bound. Okay, all right, so what I'm going to talk about today is drawn from a paper of Englert, Roglin, and Vaking, and there's a bunch of results in this paper. We're just going to focus on one of them.
00:12:17.046 - 00:12:46.874, Speaker A: Let me actually state two of them for context. So the first thing engraved at all prove is that indeed, even in the special case of endpoints in the plane with the L one metric, and even for this particular two up neighborhood, local search can in the worst case require an exponential number of iterations before halting at a locally minimal, locally minimum tour. Okay, so exponential and worst case, this we're not going to prove.
00:12:46.922 - 00:12:47.134, Speaker B: Okay?
00:12:47.172 - 00:12:50.734, Speaker A: This is sort of a very like I said, this is a very nice edge construction.
00:12:50.782 - 00:12:50.994, Speaker B: Okay?
00:12:51.032 - 00:12:59.830, Speaker A: So it's a tour de force. It's very impressive when you read it, but I'm not sure if we went over it. I'm not sure what I'd teach you other than the fact that this is true. Knowing that this is true is nice.
00:12:59.900 - 00:13:00.182, Speaker B: Okay.
00:13:00.236 - 00:13:03.782, Speaker A: But going through the proof again, if you're interested, just check out the paper.
00:13:03.836 - 00:13:04.438, Speaker B: Okay.
00:13:04.604 - 00:13:23.070, Speaker A: It's an impressive construction. What I do want to talk about is the positive result, which is that on the other hand, nature really does save the day. Perturbation really does save the day, which is that local search under the true Op neighborhood has polynomial smooth complexity.
00:13:24.290 - 00:13:25.040, Speaker B: Okay?
00:13:26.530 - 00:13:45.422, Speaker A: So even if you start from one of these exponential worst case examples, you perturb it slightly. Local search runs fast. So recall from Wednesday that what it means to have polynomial smooth complexity is your running time is polynomial not just in the input size n, but also in one over sigma, where sigma is the parameter controlling the size of the perturbation.
00:13:45.486 - 00:13:45.714, Speaker B: Okay?
00:13:45.752 - 00:14:01.738, Speaker A: So like, for a Gaussian, that was like the standard deviation, all right? Of course, there needs to be some dependence on the size of the perturbation because as the perturbation goes to zero, we know the running time might go up to exponential. Okay, so polynomial and one over the perturbation size is what we're looking for.
00:14:01.824 - 00:14:02.106, Speaker B: All right?
00:14:02.128 - 00:14:06.010, Speaker A: So that's what I want to prove to you in this lecture.
00:14:06.990 - 00:14:08.250, Speaker B: Is it optimal?
00:14:08.670 - 00:14:09.402, Speaker A: In what sense?
00:14:09.456 - 00:14:11.040, Speaker C: Does it give you the right answer?
00:14:12.210 - 00:14:40.886, Speaker A: Certainly not. Certainly not. It's empty hard. So that would be surprising. So, yeah, I'll discuss a little bit more about the let me just say it now, actually. So there's a couple of disconnects between theory and the practice of local search. One is with respect to the running time, and that's the one that we're addressing in this lecture, and that's the one that smooth analysis is appropriate for.
00:14:40.886 - 00:15:20.402, Speaker A: That disconnect is between just the number of iterations observed when you run it, the difference between exponential in the worst case and sub quadratic in practice. And so we're not going to get sub quadratic, but we'll at least get polynomial. And so we get a lot closer to explaining what the real performance of local search is. The other disconnect is real, but I think it's a little less embarrassing for a couple of reasons. But it's also harder to close. It's also harder to know how to actually close that gap, which is the extent to which the local minima, which is where a local search algorithm is going to stop. If it gets lucky and reaches a globally minimum tour, of course it will stop because there's no way to improve it.
00:15:20.402 - 00:15:59.754, Speaker A: But certainly, and maybe I'll put an example on the homework, you can have tours which are locally optimal, say, for the two up neighborhood, meaning any one of these swaps will only make you worse off. But if you could somehow do a much more complicated switch around of the points, you would get a better tour. Okay, so simplex is very unusual, and that simplex for linear programming is sort of doing local optimization each time. But because everything's linear and convex, it turns out the only local minima are the global minima. For discrete optimization problems, especially NP hard ones, but even most ones, it's not the case that local minima generally coincide with global minima. They're almost always different. There's usually many more local minima than there are global minima.
00:15:59.754 - 00:16:02.490, Speaker A: So you might get stuck at something which is suboptimal.
00:16:02.570 - 00:16:02.958, Speaker B: Okay?
00:16:03.044 - 00:16:45.230, Speaker A: Of course, for an empty hard problem and a simple algorithm, we're not surprised, right? We kind of always expect that if we don't work very hard for a heuristic for an empty hard problem. So you can ask the question like we've asked at other points in this class. Can you say anything about how close to an optimal solution is the locally optimal solution that you reach? Depends on the problem. For some problems, you can say regionally interesting things. For the traveling salesman problem, in theory you don't do so well. So the local minima in the worst case can be very far from global minima, even in quite simple Tsp instances, depends on the assumptions. So under the assumptions for today, you can prove some kinds of bounds.
00:16:45.230 - 00:17:26.470, Speaker A: So you can prove some constant bound, sufficiently big bound, where the constant also depends on the perturbation size. But the constant bound, again, is very far from what you preserve in practice. So maybe this would prove a factor of five and in practice it would normally be 10% or something like this. So it's not that you can't prove anything, but there's a big gap between what's true in the worst case and what's observed for the empirical performance of local minima of local search algorithms. Now, here's a couple of reasons why. Okay, so there's two reasons I think, why it's a little less embarrassing, this gap between theory and practice. One is the gap isn't quite so ridiculous.
00:17:26.470 - 00:18:13.546, Speaker A: It's still a big gap, but it's not as big as this. Two to the N versus less than N squared gap. The second reason is that if you talk about empirical performance of local search, when you talk about a number of iterations, there'll be people who do local search for a living and go to the grave without ever seeing this happen once in their lifetime. I mean, this is really, really rare. But if you run local search algorithms enough, you'll definitely see some cases where they terminate at a really stupid locally optimal solution. I mean, it's not super common, but let's say 1% of the time for concreteness, you're going to see really crappy outputs. So it can happen, right? So the theory is talking about something which does sort of exist even in the real world.
00:18:13.546 - 00:19:00.806, Speaker A: Now, the main reason it's so hard to close the gap between theory and the practice, okay? So anyone who runs local search, what you do is you don't just run it once and go to the bank, right? You run it like these things take, they often take like 10 seconds to run. So run it 100 times with random starting points and take the best one, and then you're usually going to be reasonably fine. Okay, so the question then is sort of how to turn that into a theorem. Like, you want to say the best of 100 runs of local search with random starting points is pretty good. And that's a little intimidating, actually, because there can be a huge number of local minima, there can be an exponential number of them. It's very hard to understand sort of what the structure is. If you take a uniform distribution over starting points, no one has any understanding of what the distribution looks like over the local minima you might reach.
00:19:00.806 - 00:19:35.774, Speaker A: So to prove a theorem which somehow says on average over the local minima, whatever that means, then you're doing close to optimal. Don't get me wrong, if anyone formulated and proved such a theorem, it would be fantastic, but it's challenging. It would be a great, great result. Okay, so the gauntlet sort of been thrown there. So the two good news is the gap isn't as big as far as solution quality. Plus I think we should feel it doesn't really feel like we're just lacking imagination in our analysis frameworks. It really kind of feels like, oh, it does well in practice for sort of complicated, messy reasons.
00:19:35.774 - 00:20:01.342, Speaker A: And it doesn't surprise me that it's eluded all of our theoretical frameworks so far. That's kind of my opinion. Okay, so end of digression, rest of the lecture, running time only. We're going to punt on solution quality. At least. Let's get good running time bounds. Okay, so actually what I want to do first is I want to reformulate the perturbation model a little bit in a nice way.
00:20:01.342 - 00:20:18.770, Speaker A: So this is going to be actually a very nice notion of smooth inputs. So previously we were talking about it like an adversary goes first and then nature perturbs it. We're going to actually smoosh those two steps into one.
00:20:18.920 - 00:20:19.620, Speaker B: Okay?
00:20:21.030 - 00:21:40.818, Speaker A: So the adversary is not going to pick a single input. We're going to force the adversary to pick distributions and then there's not going to be any nature. Okay, now you should ask, well, but what prevents the adversary from picking a distribution which is a point mass on a worst case input? Well, what's going to prevent it is we don't let it. We're going to insist that the distributions can't be too spiky. They have to be sufficiently diffuse so adversary picks not too spiky density functions on the unit square with the understanding that the point pi is going to be drawn from fi different points being drawn independently. So on unit square. And what do I mean by not too spiky? So what I mean is that if you look at the density of any of these densities at any point in the square, this is bounded above by one over sigma, okay? So it's an upper bound on the density, sigma is a parameter, so sigma is measuring the size of the perturbation.
00:21:40.818 - 00:21:48.494, Speaker A: So as sigma goes to zero, this upper bound becomes trivial, which allows the adversary to just deterministically. Pick a point.
00:21:48.612 - 00:21:49.280, Speaker B: Okay.
00:21:51.250 - 00:22:02.754, Speaker A: So as sigma goes the other direction, we approach average case analysis intuitively. And if you think about it, okay, so sigma is not going to go any lower than it's not going to get any bigger excuse me, than one.
00:22:02.952 - 00:22:03.700, Speaker B: Okay?
00:22:04.150 - 00:22:51.854, Speaker A: The only way this could be one is if fi is the uniform distribution on the square, okay? More generally, what this does, and this is maybe a way I encourage you to think about this, is this forces the support of a density fi to have area just in the usual le bag measure sense at least sigma, okay? And this will hold with equality if and only if this density is if and only if the distribution is uniform over its support. So if you're uniform on a support of area sigma, then fi is equal to one over sigma everywhere.
00:22:51.982 - 00:22:52.658, Speaker B: Okay?
00:22:52.824 - 00:22:56.462, Speaker A: So the smaller the sigma, the smaller the area you can concentrate the distribution.
00:22:56.526 - 00:23:01.190, Speaker C: On conforms to our other definition of smooth anomalies, right?
00:23:01.340 - 00:23:36.378, Speaker A: Well, so let's see, at the moment I'm proposing this as a different model or as another definition. What's nice about this, what we talked about last time, so for the linear programming results, it was really about Gaussians. So a drawback of that result, a drawback of the linear programming results that we have is we only know how to prove them for Gaussians. And then it's clear what sigma should be. It's the standard deviation of your distribution. So that naturally motivates the question, can we handle lots of distributions perturbations?
00:23:36.474 - 00:23:36.686, Speaker B: Right?
00:23:36.708 - 00:24:35.198, Speaker A: So why this obsession with Gaussians? Yes, if you had to pick just one, that'd be a good one. But we'd like to say any reasonable perturbation works. Okay, and so then you have to have some way of parameterizing arbitrary reasonable distributions. And what does every distribution have? Pretty much every reasonable distribution has a density, so why not some parameter on the density and densities? So sort of saying they're always at most something is a very natural way to say they're not too spiky, they're not too close to a worst case input. Certainly if you want, you could take as a special case fi to be a point plus a gaussian truncated, so it stays on the square. That's certainly one instantiation of a distribution fi, but it could also be just uniform on some region that has area at least sigma. So in that sense, again, because of the sort of the truncation.
00:24:35.198 - 00:25:09.200, Speaker A: It's not identical, but I encourage you to think of the previous case as a special case of this, even though strictly speaking, it's not quite true. Other questions about the perturbation model. So our goal is to show that no matter what distributions over points the adversary picks conforming to this bound that are not too spiky, local search is fast, or again, fast is going to mean polynomial expected number of iterations in n and in one over sigma. Questions?
00:25:11.570 - 00:25:17.822, Speaker C: All right, andy, it's an area one square, right?
00:25:17.876 - 00:25:19.266, Speaker A: So if you had sigma to be.
00:25:19.288 - 00:25:27.060, Speaker C: Like eight, so that stuff couldn't be, you had to limit everything by one eight, wouldn't that mean that you have to spread out over an area of eight which is bigger than the square.
00:25:29.670 - 00:26:08.034, Speaker A: Exceed one? Let's see. Yeah. So sigma, you should think of sigma here. Let's see, do I have something flipped one over sigma as this goes to zero, sigma equal one eight, I think is what you're thinking of. Okay. I guess minimum of this, right? It needs to integrate to one, right? It's a density, right?
00:26:08.152 - 00:26:08.820, Speaker B: Sure.
00:26:09.270 - 00:26:11.746, Speaker A: So did you say sigma equal eight or sigma equal one eight?
00:26:11.848 - 00:26:16.534, Speaker C: I said sigma equals eight because then that probability be less than one 8th, right?
00:26:16.652 - 00:26:29.114, Speaker A: Yeah. So there's no density on the unit square, which is everywhere. At most one 8th, I guess would be my response. Yeah. So sigma is never going to go above one.
00:26:29.312 - 00:26:29.674, Speaker B: Okay.
00:26:29.712 - 00:26:38.730, Speaker A: So sigma is going to be between zero and one and we want to be poly in one over sigma.
00:26:39.230 - 00:26:39.980, Speaker B: Yeah.
00:26:40.370 - 00:26:49.502, Speaker A: Other questions? All right, so the proof plan is very clean.
00:26:49.636 - 00:26:49.982, Speaker B: Okay?
00:26:50.036 - 00:27:59.106, Speaker A: So the rest of this lecture is devoted to a proof of the second part of this theorem. And for the proof plan, it's going to fulfill a promise that I made to you long ago when we were talking about parameterized analysis and its various motivations. So we're going to bust out parameterized analysis again. So what we're going to do is we're going to, just for any input, we're going to have a parameterized upper bound on the number of iterations of local search in terms of some condition number, in terms of some parameter. That's actually going to be easy. So params of a condition number. And then the second step is we're going to say that only then in the second step do we use the fact that we're doing smooth analysis and we say, well, because there's this perturbation, the value of the condition number has to be fairly reasonable.
00:27:59.106 - 00:28:39.422, Speaker A: It's not going to be some super nasty version of the condition number. So smooth instance, we're going to show it implies that the parameter we identified in part one is not too bad. Okay, so that's the plan. And so part one, usually for smooth analyses, is fairly easy. We talked about this at the end of Wednesday. So in terms of the angle of the polygon after we project the high dimensional polytope onto the plane. What we said was that what Spielman and Tang really prove is that every angle is bounded away from pi and so that bound away from pi can be thought of the condition number.
00:28:39.422 - 00:28:56.500, Speaker A: Given that it was very easy to have this iteration bound of two pi over that delta. We have something similar here. Okay, so unlike some of our work in the first couple of weeks, this is going to usually be easy sometimes understanding in these perturbed instances what's the distribution of this condition number, that can be.
