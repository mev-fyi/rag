00:00:00.090 - 00:00:44.490, Speaker A: What I want to do today is I want to close the loop, okay? So when I put courses together, I sort of think of them almost as like a playlist. And a really good playlist is the one you start want to playing from the beginning, right when you end it. So we've been talking about these unknown ID distributions over inputs and algorithms that are good for them. I want to connect that to our very first topic, which is instance optimality. Okay, let me remind you about instance optimality. So this means you're basically optimal at least up to a constant factor, input by input. So it's the strongest possible, least controversial notion of optimality.
00:00:44.490 - 00:01:11.060, Speaker A: So whatever your cost measure, algorithm A is alpha instance optimal if for every algorithm B and every input Z, you're within an alpha factor of the cost of B on Z.
00:01:15.030 - 00:01:15.780, Speaker B: Okay?
00:01:17.270 - 00:01:53.570, Speaker A: So provided alpha is reasonably small, this is a super, super strong guarantee, really the best you could hope for. So strong in fact, that often these algorithms don't exist. We talked about at least for values of alpha that are interesting. We mentioned a couple of relaxations, like throwing in the alpha. And then the other relaxation we mentioned, which is the one I'm really going to focus on today, is you can say, well, maybe our algorithm A isn't as. Good as every single other algorithm on every single input. But maybe it's as good as every other reasonable algorithm or every other natural algorithm b on every single input.
00:01:53.570 - 00:02:58.246, Speaker A: So the relaxation is restrict B, the competing algorithm to a subclass script C of natural algorithms. Okay, so that's an idea. But then of course the question and in fact, really, we sort of did this when we talked about 2D maximas. This is way back in the very first week of the class, we observed that this definition is problematic because this one so you're given a point set, you're trying to compute the maxima, the Pareto optimal points, and for a given input Z, there exists an algorithm which is basically memorized the input of that input Z. And the way we got around that is we kind of said, well, those algorithms are unreasonable. Let's only look at algorithms which don't memorize the input in the sense that their running time, or at least our running time analysis, is independent of the order in which the points are presented. So effectively, for 2D maxima, we define script C as the order oblivious algorithms B.
00:02:58.246 - 00:04:30.766, Speaker A: So that's what we were doing back then. Now, in general, for a general problem, there's this question of what should C be? So what other algorithms B should we be competing with? So what's a good, well motivated choice for the set C? And clearly what we're looking for is we're looking for sort of a sweet spot. So on the one hand, C should be small enough that we can prove something interesting, so that we can actually prove an instance automatic result for some algorithm, which is nice. On the other hand, subject to that, we want it to be as large as possible, as rich as possible, encompassing everything we might plausibly implement. So by doing the analysis, we're hoping to kind of say, well, among everything we could imagine actually implementing, here is the best one, or here's a few ones that are kind of as good as it could get within that class. And so what I want to point out and sort of the main conceptual point of this lecture is that all of our discussions about sort of unknown underlying distributions motivates a way to define the USET script C and so unlike the previous three or four lectures where we were literally assuming that there was a distribution over the inputs and we proceeded to analyze algorithms according to that assumption here, we're going to think right now, temporarily, about distributions over inputs only as a thought experiment, only to define the set C. So in this lecture I know I'm getting ahead of ourselves, but just to clarify this lecture, we will never actually do average case analysis.
00:04:30.766 - 00:05:31.250, Speaker A: We will never actually take expectations over an input. We're only going to look at distributions over inputs to define the set C. Then once we have this set of competitor algorithms, we're going to look for an instance optimality guarantee in this sense. All right, so how do we do this? Well, again, so just temporarily, suppose there was a distribution over inputs, okay, and maybe we don't know what it is, maybe it's unknown, but like in many lectures we've seen, we have some set of distributions which might be governing the input. So set of distributions over inputs. And again, in sort of a different context, we've seen many examples of what these sets might look like. So in smooth analysis, this would be, for example, point masses, slightly perturbed or sufficiently diffuse distributions, like when we were talking about local search, when we were talking about hashing, this would be all distributions that have enough randomness in the sense that we defined in that lecture.
00:05:31.250 - 00:06:21.762, Speaker A: So in self improving algorithms, when we talked about sorting, there we were assuming that the different entries of the array were independent. So we were thinking about all distributions subject to being independent across different array elements. And when we talked about pricing algorithms a couple of lectures ago, we had this regularity notion, for example, saying that this revenue curve was concave, but other than that, we made no assumptions. So all of those are examples of families of distributions, capital T. And again, the point is often it's plausible that you know something about the distributions on a course level, about the inputs, but you don't really want to assume something like a parametric form that they're gaussian or that they're uniform or something like that. So all of these are examples of what these kinds of families of distributions look like. All right, so in previous examples, we were thinking of inputs as literally being drawn from that distribution.
00:06:21.762 - 00:07:16.646, Speaker A: But now let's just use a set of distributions to define a set of algorithms. So once you've fixed a set of distributions, define the corresponding set of algorithms c sub D, as all of the algorithms that arise as optimal algorithms for these sets of distributions. So this is a family of distributions. This is going to be a family of algorithms, which are the algorithms? Well, you just range over the distributions in your head. You ask, okay, well, what if inputs were drawn from some distribution capital F? What would I do? There'd be some algorithm A, which is the best of all algorithms, in expectation over that distribution F. You put that in this set, then you move on to the next distribution. In script D, you ask the same question, you get a different algorithm, you put that algorithm in this set, and so on.
00:07:16.646 - 00:07:50.722, Speaker A: Okay, so you just iterate through all these distributions, look at all the corresponding optimal algorithms, and that's your set. So A optimal for some distribution F in D. And just to clarify, so what do I mean by optimal? Well, once you've fixed a distribution, remember, then there's an unequivocal notion of what it means to be optimal. You just forget the expected cost of the algorithm on the distribution. So every algorithm has this one number attached to it. It's expected cost. Some algorithm has the smallest expected cost.
00:07:50.722 - 00:08:05.734, Speaker A: That's the optimum. Okay, so minimizing the expectation with respect to the distribution F of the cost of A over a random input z drawn from the distribution F is a question.
00:08:05.852 - 00:08:23.600, Speaker C: How does this prevent an algorithm which checks if the input is crazy and memorizes some input for which you wouldn't consider in your distribution and is optimal on that crazy input, but is also optimal on D?
00:08:25.570 - 00:09:23.810, Speaker A: Well, so it might be hard to simultaneously be optimal on a crazy input and be optimal for D, right? If you memorize an input, what do you do? Basically the first thing you have to do is you have to check that the input actually is. So we insist that you're correct on every input generally. So if you've guessed an input to try to be fast, the first thing you have to do is check if that's it, right? And then so you waste some time checking, and then if that's never the case, so imagine it's a distribution where that's a very unlikely input, you just always spend this time checking for it, which is a waste of time, and then you have to default to some algorithm anyways. You'd be much better off just using that algorithm. So there's this inherent trade off that's always the issue, right, with algorithms is if you're going to be faster on some inputs, you're generally going to be faster on others or slower on others, right? If you can be faster on every single input. Just do it. So really think about the pareto frontier of algorithms, where it's all about trade offs between different inputs, and then a distribution lets you sort of reason about how to make those trade offs, which is just using the expectation.
00:09:23.810 - 00:10:09.680, Speaker A: All right, so is this clear? This is an important point. Okay? So the question was, how do you come up with a family of algorithms that you might want to compete with? And so what we're doing is we're saying, well, let's reduce it to a family of distributions over inputs that you think might describe the world, and why don't you just try to compete with every algorithm that arises as an optimal one for one of those distributions, okay? So it reduces choosing script C to choosing script D. Okay? Okay. And so here's the formal connection between an unknown underlying distribution and instance optimality. And it's very simple, it's almost trivial, but it's sort of one of the key points of the lecture. So let me just do it.
00:10:11.490 - 00:10:11.806, Speaker B: So.
00:10:11.828 - 00:10:15.650, Speaker A: It'S not lost in anyone. So suppose you have an algorithm.
00:10:17.430 - 00:10:17.794, Speaker B: And.
00:10:17.832 - 00:11:31.718, Speaker A: It'S instance optimal with respect to a set of algorithms like this. So script C subscript D with some constant alpha. Then for all distributions in your set, algorithm A is almost as good as the best for this distribution capital F. Meaning if I look at the expected cost of algorithm A on a random input drawn from F, this is a most alpha times the expected cost of the optimal algorithm for F, again evaluated on a random input drawn from F. Okay. And the point I want to emphasize here is that on the left hand side, this algorithm A, this is independent of distribution F, okay? So this is like when we had results where basically you knew the set of distributions. Like, so, for example, you knew it was ID or you knew it was regular or whatever, and the algorithm could depend on that information about the set of distributions, but it couldn't be tailored to the particular distribution.
00:11:31.718 - 00:12:11.174, Speaker A: By contrast, what we're competing with over here, ops of F, this is tailored to F. Okay? So another way to think about this guarantee and so when I first introduced the notion of hybrid models interpolations between worst and average case analysis, this is exactly the kind of inequality we had on the board. So one way to think about this is compare two different scenarios. One, I tell you the distribution over inputs, then you get to pick whatever algorithm you want. Of course you'll pick the optimal algorithm. Or in the much more unfair world, I don't tell you the distribution. I force you to first pick the algorithm, and then I show you the distribution constrained only by the fact of the distribution being in script D.
00:12:11.174 - 00:12:27.962, Speaker A: So if there's an instance optimal algorithm, then that says you actually lose only a factor of alpha in these two worlds. Clearly, you're only worse. Right. If you have to choose the algorithm without knowing the distribution, but there's some sort of way to hedge your bets. If there's an instance optimal algorithm, you're simultaneously near optimal no matter what f is.
00:12:28.016 - 00:12:28.426, Speaker B: Okay?
00:12:28.528 - 00:12:49.714, Speaker A: So it's sort of robustly near optimal as long as you believe that the distribution comes from script d. Okay, so this is the claim. So if your alpha instance optimal with respect to this class of algorithms, then your alpha approximate on every expectation from script d question in the back.
00:12:49.832 - 00:13:07.622, Speaker C: So while it doesn't actually affect the result that you just proved, this script c seems to be very different from our usual notion of what's a reasonable algorithm because you could imagine plenty of reasonable algorithms that aren't optimal for any particular distribution but are just generally good.
00:13:07.756 - 00:13:44.500, Speaker A: I would say it depends on whether you believe it depends on how strongly you believe that reality is roughly modeled by a distribution. So I mean if, if you believe more or less that inputs are coming from some distribution you just don't know what it is, then if you want to use a suboptimal algorithm, fine. I'm also competing with that just by transitivity. So you're right that if you really want to use some totally other measure of how good an algorithm is doing, like say, worst case. So if you wanted to optimize the worst case performance, you definitely might well use an algorithm outside of this class.
00:13:45.850 - 00:13:51.240, Speaker C: So are the algorithms that we learned in 161 and 261, do they have this property?
00:13:55.850 - 00:14:22.318, Speaker A: That's a good question. So usually if you're worst case optimal, you're also optimal with respect to some distribution. Sure. So like take merge sort, right, runs an n log n time. Okay. On the other end, if the array inputs are scrambled uniformly, there's a lower bound of n log n on the expected number of comparisons of any sorting algorithm. So you may as well use merge sort.
00:14:22.318 - 00:15:04.730, Speaker A: It is optimal for the uniform distribution over inputs. So in that sense it's going to be thrown into c sub d, as will zillions of other sorting algorithms because merge sort is not going to be optimal for other kinds of distributions. I mean, partly what I'm doing here is it depends on alpha too, right? But so, I mean, if you're willing to sort of allow me small constant factors, then you may as well throw a merge sort into the set. Merge sort basically will get thrown into the set plus a lot of other stuff. So that's not always true. There are cases where optimizing the worst case gives you a fundamentally different algorithm than if you optimize with respect to a natural distribution. But for a lot of fundamental problems, this is only going to be kind of a stronger guarantee.
00:15:04.730 - 00:15:32.040, Speaker A: So it's instance optimality, right? It's in the spirit of what we're trying to do at the beginning where if all we wanted to do was be optimal in the worst case, you kind of learn how to do that in 161. And we want to kind of have a more input by input kind of guarantee. Well, not just on worst case inputs, but every input we want to do as well as we can. So here we're saying, okay, not as well as we can with any algorithm, but as well as any algorithm, which at least that's expected optimal in some description of reality. Is that the question?
00:15:34.410 - 00:15:35.160, Speaker B: Okay.
00:15:37.850 - 00:16:18.550, Speaker A: All right, so proof so the proof is straightforward, but again, it's sort of important enough. Let me just spell it out. Basically, instance optimality is an input by input guarantee, and this is just an on average guarantee. So if you have an approximation factor, input by input, you also have it on average no matter what distribution you're averaging with respect to right. So let me just actually leave it at that. Okay, so instance optimality, point wise, only stronger than an expectation guarantee.
00:16:20.330 - 00:16:21.080, Speaker B: Okay?
00:16:24.250 - 00:17:15.986, Speaker A: So formally, if you really wanted to write this out, you'd say, okay, well, fix your favorite distribution capital F. Look at the optimal algorithm for that distribution F. By the definition of our set of algorithms, opt F belongs to our set. This is defined as all of the optimal algorithms for different distributions. By the definition of instance optimality, a is at least as good up to an alpha factor input by input as opt F. So if you average over all the inputs, you're also at least within an alpha factor. Okay, so it's really just chaining the definitions together how we define C sub D, and what's the definition of instance optimality? Okay, so the point here, what I'm trying to say here is that now we're going to okay, so the connection is that thinking about unknown distributions naturally motivates a set of algorithms for which you might want to prove instance optimality.
00:17:15.986 - 00:17:41.360, Speaker A: And proving instance optimality is only stronger. Okay? So again, if you think about the statement a is instance optimal for every algorithm in some set, there's no distribution in this statement. It's an input by input guarantee with respect to a set of algorithms. So we had a thought experiment about distributions just to define who we want to compete with, and never again will we reference a distribution over inputs, okay?
00:17:44.530 - 00:18:01.670, Speaker B: All right, here's.
00:18:02.650 - 00:18:52.594, Speaker A: Any questions about that? So that's kind of the main conceptual point of this lecture. But now I want to show you an example of why this is useful in a concrete problem. Okay? So the concrete problem I want to talk about is online decision making, very, very well studied problem. So here's the setup. So there's a set of actions, capital A, okay? And the problem is interesting, even if A only has two options like buy stock or sell stock today, okay? So then here's how it works. So there's going to be T, capital T days. Think of capital T as known for lecture, although that assumption can be removed for simplicity.
00:18:52.594 - 00:20:00.330, Speaker A: Think of capital T as known. So what happens on each day is you have to pick an action. By you, I mean, your algorithm can be randomized if you want. So you pick an action at on day T and then an adversary after you've chosen an action, or at least chosen a distribution over actions, an adversary picks a cost vector indicating the cost incurred by each action. And costs are going to be real numbers between zero and one, okay? And our cost measure. So if we give an algorithm a so an algorithm a just says as a function of what's happened so far, what action or what distribution of reactions do you pick at the next time step? An input is just a sequence of capital T cost vectors. And so the cost of an algorithm on a given input is just defined as the total cost or total expected cost if it's randomized.
00:20:04.050 - 00:20:04.800, Speaker B: Okay?
00:20:06.210 - 00:21:08.020, Speaker A: So we'd like this to be small, if at all possible. Now you see this description and you should say, doesn't this seem kind of unfair? Right, because sort of we're forced to go first and pick an action, and then the adversary gets to sort of decide how expensive the different actions are after it sees what we did or at least whatever distribution we're going to use. So there are some obvious and possibility results just because of the asymmetry of information between the algorithm and the adversary. So, for example, it's totally hopeless to try to compete with the best sequence of actions you possibly could have taken in hindsight after the capital T days, right? So for starter, like, imagine your algorithm was deterministic for starters, right? So the adversaries could just look at your algorithm and be like, okay, what are you going to do? So let's say there's two actions, right? One and two. The algorithm is going to be like, oh, you're going to do action two on the first day. All right, I'll make the cost of action two one. I'll make the cost of action 10.
00:21:08.020 - 00:21:35.722, Speaker A: Now I'll simulate your algorithm, given that that happened. I'm going to look at what you do on day two. Maybe then you try action one. Okay, well, then I'll make one cost one and action two cost zero. Okay, if it's randomized, well, you still have to play one of the two actions with at least 50% probability. So I'll just give that one cost one and the other one cost zero, okay? Because every day I gave an action cost zero. In hindsight, there's a sequence of actions with total cause zero.
00:21:35.722 - 00:21:42.790, Speaker A: Okay, but you're going to pay capital T, or at least in expectation capital T over two for a randomized algorithm.
00:21:42.870 - 00:21:43.402, Speaker B: Okay?
00:21:43.536 - 00:22:05.170, Speaker A: So that's kind of trivial. That's not interesting. And this is actually, if you think about it, this is basically a negative result, for instance optimality. Also, okay, if you think of instances as cost vectors, whatever algorithm you pick, there's going to be some input on which your total cost is high. Yet some other algorithm which memorizes the optimal sequence of actions has caused zero.
00:22:05.320 - 00:22:05.934, Speaker B: Okay?
00:22:06.072 - 00:23:23.950, Speaker A: So there's no hope for an instance optimality result without any further assumptions or restrictions. And it turns out this paradigm, this idea of sort of defining a restricted class of algorithms and doing it in this principled way of thinking about hypothetically, if there was a distribution, what would you do? This turns out to be a sort of perfect way to make progress about reasoning about online decision making. So this approach, this is sort of a modern way of thinking about it. This is sort of only mostly the past five years that it's been described this way, but it regenerates kind of the way people have always thought about these online decision making problems over the past half century or so. Okay? All right, so Note can't compete with the best action sequence in hindsight, so let's do the distributional thought experiment. Okay? So again, not because we're going to assume that inputs are drawn from a distribution, but just to have a principled way of defining the algorithms we want to compete with script C, subscript D. Let's think for a second about distributions.
00:23:23.950 - 00:24:56.234, Speaker A: What if it was the case that there was a distribution capital F on cost vectors? So one sample from capital F is a cost vector. So if there's two actions, it's going to be a pair of numbers. And imagine it's IID, okay, so every single day we have a fresh draw of a cost vector from capital F. So suppose that were true, and suppose we knew what capital F was, think for a second about what would you do? How would you make this decision at each time step? Very simple answer. If you think about it, it sort of makes the problem degenerate, actually, because you could say, okay, well, what if I picked action one? What would happen? Well, I guess there's some expected cost of action one according to distribution capital F, so that's what I get in expectation. And you could just think about that same computation for every available action. And there's no reason not to just pick the one with the smallest expected cost.
00:24:56.352 - 00:24:57.020, Speaker B: Okay?
00:24:57.470 - 00:25:43.002, Speaker A: So that's what you do on day number one. Day number two, given what happened on day number one, who cares what happened on day number one? It's ID, right? It's the exact same question, exact same answer, right? If action seven was optimal day one, it's going to be optimal day two. Okay, so optimal algorithm is to play the argument of the expectation, where here C is drawn from F. So just every day you pick the same action, the action which minimizes expectation. And because capital F is not varying with T, little T, neither is what you should do.
00:25:43.136 - 00:25:43.626, Speaker B: Okay?
00:25:43.728 - 00:25:54.702, Speaker A: So the optimal thing is to play the same action day after day after day. Now of course, as you vary f, the action which is the best one to play every day will change.
00:25:54.836 - 00:25:55.278, Speaker B: Okay?
00:25:55.364 - 00:26:50.398, Speaker A: So now think about, remember this thought experiment works. You posit a set of distributions and then you go distribution to distribution and you say, what would I do? And then you look at the entire set of algorithms that you get. So we just observed is that if you just specify one of these distributions f, assuming only that each T is a new fresh sample ID from this f, then you're going to play a fixed action. So with no other assumptions, f can be arbitrarily weird. You can have correlations between the costs of different actions doesn't matter ranging over all capital F's in the world, the only algorithms you're ever going to find are the ones that play the same action every day. Okay, so in other words, so thus if script D equals all F, and again it should be IID. So we do need to assume that each day is exactly like the previous day.
00:26:50.484 - 00:26:50.782, Speaker B: Okay?
00:26:50.836 - 00:27:12.870, Speaker A: You could generalize it, but today I only want to talk about the case where each day t, there's a fresh draw from the same distribution capital F. If that's your set d, then your induced set of optimal algorithms is just the fixed meaning time invariance action sequences.
00:27:16.010 - 00:27:16.760, Speaker B: Okay?
00:27:19.450 - 00:27:49.742, Speaker A: So it's a very simple result of this thought experiment. We range overall distributions. We see what are the optimal algorithms. It's just the things that always play the same thing. So like if there's ten actions, then there's ten possible optimal algorithms for one of these distributions. Okay, so what would it mean to be instance optimal with respect to this set of algorithms? That would mean, say, for some factor alpha, that would mean that we have an algorithm A. So that input by input.
00:27:49.742 - 00:28:47.310, Speaker A: And remember, an input is a sequence of capital t cost vectors. So we'd have an algorithm that no matter what the sequence of cost vectors is, our total cost is at most, or at least up to a factor at most the best total cost achieved by any of those cardinality of a optimal algorithms in C sub D. Okay? So again, if there's like ten actions, there's ten algorithms here. Being instance optimal means that no matter what the cost vector sequence is, our total cost is essentially as good as that incurred by each of these ten algorithms on that same input, okay? Input by input, cost vector sequence by cost vector sequence, we do at least as well as every one of these algorithms. So that's what instance optimality means for that set. Everyone with me? Okay, so it turns out what I just said is essentially exactly a very classical notion of a no regret algorithm.
00:28:47.470 - 00:28:47.890, Speaker B: Okay?
00:28:47.960 - 00:29:32.686, Speaker A: So I'll tell you more about that in a second. Those of you who've taken 364 from me have seen this before. But it turns out this is one way to think about and one way to sort of motivate and automatically generate the notion of no regret algorithms. Okay? And so the main technical result of this lecture is a proof that there exists essentially instance optimal algorithms in this sense that there actually do exist online algorithms which cost vector sequence by cost vector sequence have total cost, no more, up to a small error of the best of those algorithms in C sub D. Okay. So any questions about kind of the setup?
00:29:32.878 - 00:29:41.190, Speaker C: Yeah, so just to clarify, we should think about this adversary as sort of picking the seas in advance or something.
00:29:41.260 - 00:30:28.498, Speaker A: So it's a good question. So there's a potentially important and subtle distinction between so called oblivious adversaries and so called adaptive adversaries. And so whether an oblivious adversary would be one that observes your algorithm, I mean, it's only relevant for randomized algorithms. So for deterministic algorithms it doesn't matter because I can basically adapt up front because I can just simulate your algorithm and it doesn't matter. So for a randomized algorithm, it starts mattering, which is can an adversary actually observe your coin flips on day ten and use that information to pick a cost vector on day eleven? And it would seem, I mean, in general adaptive adversaries are more powerful. It turns out the algorithm we're going to stare at, we're going to prove guarantees about it doesn't matter. So I want you to think of it as an oblivious adversary.
00:30:28.498 - 00:30:34.882, Speaker A: But the algorithm has special properties that make it also the same guarantee holds for an adaptive adversary.
00:30:35.026 - 00:30:43.494, Speaker C: So I guess I just don't see how the adversary just can't pick the costs to just punish you good regardless.
00:30:43.542 - 00:31:10.606, Speaker A: Of what you so the adversary absolutely can. So the question then is why is that not a barrier to instance optimality? Basically everybody else is going to have exactly. If all your competitors suck as well, there's nothing else you can do. Then it pumps this kind of as we've discussed, we talk a lot about sort of approximation guarantees. They're not that meaningful if opt sucks. But on the other hand, what else you're going to do is when you're proving a theorem right, you're not going to do better than opt. But it's true.
00:31:10.606 - 00:31:16.086, Speaker A: Like for it to be meaningful, you also want it to be the case that in the problem that you care about opt, be happy with opt.
00:31:16.188 - 00:31:16.694, Speaker B: Okay.
00:31:16.812 - 00:31:21.560, Speaker A: But it's not an obstacle to theorem itself. Good question. Other questions.
00:31:23.290 - 00:31:31.066, Speaker C: It seems like trivially. If you try all t options and get punished every time, then that means that every one of the algorithms that.
00:31:31.168 - 00:31:43.370, Speaker A: You'Re competing against was punished, right? That's the hope. Right. So what your worry is when you try to simulate one algorithm, basically you're just getting all the timing wrong.
00:31:43.520 - 00:31:44.106, Speaker B: Okay. And.
00:31:44.128 - 00:32:23.498, Speaker A: Actually some of the lower bounds allows you to think about on. The homework shows that this can happen, say if you use a deterministic, like a round robin strategy would not work. For example, if you just try to round robin simulate, we'll talk more about this. Okay? There's a lot of truth to that intuition, which is if you just do a sufficiently smart simulation of all of the options, you should do fine because the only way you wouldn't do fine is if all of them were bad. There's definitely a grain of truth in that intuition, but really making it work technically requires some cleverness, which is why I still have it's a good thing I still have 35 minutes of the lecture left, although it is at this point, at this point you can get optimal bounds with just insanely clever short derivations. It's kind of amazing.
00:32:23.584 - 00:32:24.026, Speaker B: Okay?
00:32:24.128 - 00:32:51.010, Speaker A: But you definitely need to be a little bit clever. Okay, so I want to race this. So everyone, I hope, remembers the setup capital T is the time horizon. You pick an action or distribution over actions first. Then the adversary picks the cost vector. You want to minimize the sum of the costs, and the benchmark is the best sum of the costs achieved in hindsight by an algorithm that plays exactly the same thing every single day. Those are the algorithms in script C.
00:32:51.010 - 00:33:23.360, Speaker A: Script D? Okay. So here's the theorem that we're going to prove. And this is a theorem which has been sort of proved and reproved and rediscovered many times in many different decades in many different communities. So there exists a randomized algorithm, and it is crucial that it's randomized. So I'm going to be a little wishy washy with a statement for the moment. It's roughly instance optimal. I'll tell you what I mean in a second.
00:33:23.360 - 00:34:11.062, Speaker A: It's roughly instance optimal with respect to this set of algorithms. All of the algorithms that play the same thing every single day with alpha roughly equal to one. I'll also say what I mean by that in a second, okay? But that's the main point. So the main point is a very strong instance optimality result, okay? So in particular, remember our key fact, instance optimality is even better than being is even stronger than being simultaneously optimal with respect to every distribution. So in particular, this one randomized algorithm is essentially optimal in any situation where you're so lucky as to have cost vectors being drawn, IID from some distribution. That's a special case of this guarantee. It's stronger because again, this guarantee references no distribution.
00:34:11.062 - 00:34:57.430, Speaker A: But if you instantiate this algorithm when there is a distribution, then you get a good guarantee, but you also get some kind of interesting guarantee even if there's no distribution because it's instance optimal. Okay, so what do these squiggles mean? So the way we've been talking about instance optimality is we have this alpha, this multiplicative factor, okay? And it turns out for the way I've set up this problem. It's smarter to think about additive error as opposed to multiplicative error. So that's what we're going to talk about for today. So we're going to have additive loss that's big o of square root of capital t, log n, capital T. Here is the time horizon. Little N is going to always denote the number of actions.
00:34:57.430 - 00:35:26.530, Speaker A: Okay, but again, even for two, this is interesting, but it's holds for any N now. Okay, so additive loss, this means the expected cost of our randomized algorithm might be worse than the best of the algorithms that always play the same thing every single day. You see that that has to happen. If you imagine capital T was just one sort of, the only thing you can do is basically pick an action uniformly at random and you might be wrong.
00:35:26.600 - 00:35:26.786, Speaker B: Okay.
00:35:26.808 - 00:36:11.374, Speaker A: And there was some algorithm that played the best action on that one day. So there's going to be some additive loss if you think about it. Right, but so it's bounded. So the extent to which our total cost will be larger than the best of the fixed actions is at most this amount root t, times square root log n. Now you look at this and you should think, okay, well help me interpret this like is this good, is this bad or what? And in particular the larger the time horizon, this is growing. Usually in these problems you think of N as being fixed, maybe big, but you think of N as just being fixed. And then you're interested in the scaling as capital T grows as you play the game for a longer and longer and you'd sort of hope to do better, right? Sort of the bigger capital T is your algorithm should get smarter as it has more information about the past.
00:36:11.374 - 00:36:34.140, Speaker A: Somehow here it's getting worse. But so the way you should think about this is in terms of the per time step loss, okay? So on average over all of the plays of the game, how much worse are you doing than your best competitor? So then you divide this by capital T and it looks much more reasonable. Okay, so square root of log n over square root t per time step.
00:36:36.750 - 00:36:37.500, Speaker B: Okay.
00:36:38.990 - 00:37:08.310, Speaker A: And so in particular, if you, say, would like to have a guarantee, which says, well, okay, I'm going to be a little bit worse maybe than the best fixed action in hindsight, but I just want to be like, 1% worse or zero? One worse per day. Then you just need to take that's going to hold as long as your time horizon is sufficiently large. Okay, so for epsilon you need log n over epsilon squared, which is actually quite fast convergence.
00:37:09.290 - 00:37:10.040, Speaker B: Okay.
00:37:12.170 - 00:37:13.014, Speaker A: You know?
00:37:13.132 - 00:37:13.800, Speaker B: Yeah.
00:37:15.770 - 00:37:37.774, Speaker A: So that's the right way to think about this. So this is, this is called a no regret algorithm and the formal definition of a no regret algorithm is one where the average per time step regret is going to zero as T goes to infinity for fixed N. Okay? So because we have a T in the denominator here, this is going to zero.
00:37:37.892 - 00:37:38.286, Speaker B: All right?
00:37:38.308 - 00:37:45.626, Speaker A: So the longer you play the game, pretty much you're converging to it's as if you knew the smartest fixed action to play in hindsight up front.
00:37:45.748 - 00:37:46.420, Speaker B: Okay?
00:37:48.070 - 00:38:07.486, Speaker A: And also, and I'll ask you to explore this on homework number ten, this is the best possible result you can have by any online algorithm, okay? So both simultaneously as a function of N and as a function of capital T, there is no online algorithm, no matter how clever, which has regrets smaller than this. So that's another sense in which we should sort of appreciate this guarantee.
00:38:07.618 - 00:38:08.300, Speaker B: Okay?
00:38:09.230 - 00:38:44.300, Speaker A: All right, so that's the formal statement. So that's what I'm going to prove. So everyone clear on the statement? Good. So these are the interpretations of the result, but this is probably the easier way to think about it. Okay, so just sum up all of the costs so that's scaling probably with capital T, and then the extent to which our total cost is going to be bigger than the best competitor is at most root T times root log N. Now, there's actually a few algorithms which achieve this guarantee. Probably the best known one is called, among other things, multiplicative weights.
00:38:44.300 - 00:39:13.304, Speaker A: That's the one that I've taught in 364, so I'm not going to teach it today. But if you're curious, you can check out, for example, my lecture notes from my class last fall. That's sort of very natural algorithm. Basically what you do is you keep track of the past performance of all of the actions. So say you're on day 100 and there's like ten actions. A natural thing to do is say, well, let's look at the previous 99 days, see how well each of these ten actions did by how well. I mean, look at the sum of the costs you would have incurred if you'd always played action one.
00:39:13.304 - 00:39:22.396, Speaker A: The previous 99 days. Same thing for action two, same thing for action three. And you should be biased toward the actions which have been doing better, meaning have lower total cost.
00:39:22.498 - 00:39:22.716, Speaker B: Okay?
00:39:22.738 - 00:39:54.184, Speaker A: So you're going to make a randomized decision, at least in the multiple weights algorithm, they make a randomized decision and it's more probable that you'll pick actions that have lower cost in hindsight. So that's message one, you should pick probabilities, in some sense proportional, or at least according to past performances. The other thing which you need to get this optimal bound and which multiplicative weights does, is you adapt fairly aggressively. So even if an action had been doing very well, if it starts performing poorly, you very quickly move mass off of that action and onto others.
00:39:54.302 - 00:39:54.536, Speaker B: Okay?
00:39:54.558 - 00:40:03.020, Speaker A: So those are sort of the two messages for multiple weights. Use past performance to choose your probabilities and also aggressively punish poorly performing actions.
00:40:04.160 - 00:40:18.076, Speaker C: I don't understand how if we're dealing with a truly adversarial opponent, that using the past performance using the past would at all influence the future, using the.
00:40:18.098 - 00:40:19.852, Speaker A: Past would at all influence the future.
00:40:19.986 - 00:40:26.172, Speaker C: Right. Are we actually assuming there's some distribution over this?
00:40:26.226 - 00:41:07.010, Speaker A: Definitely not. Again, the name of the game is just the only way the adversary can make you perform poorly in expectation is to make all of the competitors perform poorly as well. So don't forget the you. Okay, so I'm going to actually teach you a different algorithm which has this guarantee, which is also very nice due to Kali and vampala. I'm going to do this second one partly just for the sake of variety, but also partly because there's a nice smooth analysis one can do of this second algorithm. So that's another connection which I'll mention on homework ten. Okay, so there's a nice connection between this other algorithm I'm going to show you and the other concepts we've seen in the class.
00:41:07.860 - 00:41:10.610, Speaker C: Do we define like E and B without.
00:41:14.020 - 00:41:49.196, Speaker A: Yeah, there's no adversary in this definition. So you just pause of the distributions. So the adversary comes up. I mean the adversary is sort of implicit in the definition of instance optimal. Right? So this is just like a class of algorithms, do with that what you will, but then all of a sudden when you want to start saying, oh, input by input, I want to be as good as every single algorithm. Now all of a sudden, the way we usually think about proving those kinds of guarantees is there's some adversary who's trying to prevent us from proving instance optimality by virtue of exhibiting this really nasty input where our algorithm sucks but someone else's is really good. Yeah.
00:41:49.196 - 00:42:31.256, Speaker A: So it's really instance optimality that lends motivates the adversarial way of thinking. But remember, instance optimality is just parameterized by any set of algorithms. But the distributional thought experiment is just to have a well motivated choice of which algorithms should you think about amongst. Otherwise it's sort of carte blanche and it's hard to know where to start. Good. All right, so before I show you the algorithm that does work, let me show you an algorithm that doesn't work, but it's very natural. So this is follow the leader and this again is just sort of the simplest way you'd try to use past performance to decide what to do next.
00:42:31.358 - 00:42:32.010, Speaker B: Okay?
00:42:34.800 - 00:43:07.636, Speaker A: So on each time step so on day one you just do something arbitrary. It doesn't matter. Okay, but at every subsequent day, what action do you choose? Well, you just look at each of the actions, see which ones has the smallest cumulative cost up to this point and do that's clear, very natural thing.
00:43:07.658 - 00:43:08.390, Speaker B: To try.
00:43:10.300 - 00:43:47.152, Speaker A: And you know it's. So on homework number ten, turns out this is bad. In the worst case, actually, any deterministic algorithm is going to be bad in the worst case. And by worst case, I mean it's not instance optimal, okay? I mean, it does not have small regret. So it has regret growing linearly in capital T, at least for a worst case sequence of cost vectors. On the other hand, it is good on smooth instances. Okay, what do I mean by smooth instances? I mean the adversary has to pick a sequence of cost vectors up front and then all of them get perturbed in some way.
00:43:47.286 - 00:43:47.632, Speaker B: Okay?
00:43:47.686 - 00:43:53.616, Speaker A: And so if all of the cost vectors, even if the adversary starts, gets to pick the initial ones, if they're perturbed, then follows the leader's.
00:43:53.648 - 00:43:54.084, Speaker B: Fine.
00:43:54.202 - 00:44:12.190, Speaker A: Okay, and that sort of follows from the analysis I'll give you for a different algorithm. Okay, so good and smooth instances. Well, it's sort of a homework. So kind of want you to think about it.
00:44:14.320 - 00:44:16.024, Speaker C: Worst case over distribution?
00:44:16.152 - 00:44:49.296, Speaker A: No, there's no distribution over cost over cost vector sequences. Actually. Any deterministic algorithm can be bad. So that's sort of a hint. It's not just this, it's really any deterministic algorithm. I mean, in some sense you can also, I mean, we're gonna look, we're gonna analyze a modified version of this algorithm. So you might want to, as we analyze the randomized version, think about where that proof would break if it wasn't randomized, if we were just doing it for this algorithm.
00:44:49.296 - 00:44:49.910, Speaker A: Instead.
00:44:52.360 - 00:44:58.132, Speaker C: When you say added loss over here, this is compared with the best of the quote unquote natural algorithm.
00:44:58.196 - 00:45:41.960, Speaker A: Exactly. So the claim is the following for every single sequence of cost vectors, it will be the case that the expected cost of this randomized algorithm is at most on that cos vector sequence, is at most the minimum cost of any algorithm in C sub D on that CAS vector sequence up to this error term. And that's true for every single cos vector sequence. So all of the randomization we're going to be talking about is only internal it's in the usual sense you're used to from your other algorithms classes. It's only internal coin flips made by our algorithm. There's no coin flips in the data, no randomness in the data. We really want to guarantee it's cost vector sequence by cost vector sequence.
00:45:41.960 - 00:46:20.420, Speaker A: All right? So instead we're going to look at a preterred version of follow the leader. And here's how it works. So basically we're going to do follow the leader, but we're just going to sort of initialize it randomly. And that's good enough, actually. Okay, so as a preprocessing step, before we even look at the first cost vector, we award to each action a random bonus.
00:46:21.000 - 00:46:21.750, Speaker B: Okay?
00:46:25.800 - 00:47:15.120, Speaker A: And lots of distributions for the random bonus would work. But the analysis is particularly simple if we look at a geometric random variable with parameter epsilon. So what I mean is for action number one, you do the following experiment. You start flipping coins and the coin has a probability epsilon. We'll choose epsilon later think of epsilon as small though it has an epsilon probability of coming up heads, so you keep flipping until you get aheads and then you just count how many coin flips you needed to get aheads. So if there's a probability of epsilon of getting aheads, what's like the expectation of this random variable one over epsilon? Obviously, the smaller the probability of heads, the more coin flips it takes to get one and it's one over epsilon for geometric. And we do that exact same experiment independently for each of the actions.
00:47:15.120 - 00:47:59.680, Speaker A: So each action gets its own random bonus. So number of coin flips to get heads, where the probability of heads is epsilon. And now we just do follow the leader with these random bonuses. Okay, so now we just set our action to be the one that minimizes the same thing past performance, minus the number of coin flips it took to get hits.
00:48:01.400 - 00:48:02.148, Speaker B: Okay?
00:48:02.314 - 00:49:05.380, Speaker A: So everybody starts with some like negative total cost and it's random for different people and then you just accumulate the real costs from then on. So it's a single perturbation at the very beginning and that's it does the algorithm clear. Okay, so the claim is that algorithm satisfies this theorem, okay? And the proof, the analysis is very clever. So analysis. So again, remember, we're shooting for an input by input guarantee. So fix an input arbitrarily, okay? So fix c, one c, two c, capital T. As I said, think about an oblivious adversary that has to fix the sequence up front, although actually, if you think about it, just so one can realize that the guarantee extends even for an adaptive adversary.
00:49:05.380 - 00:50:01.130, Speaker A: Okay, but think about an oblivious adversary. So the adversary sees our algorithm in effect and devises the worst cost vector sequence it can think of. So for the analysis only, we're going to define a fictitious algorithm, an algorithm we could never implement, okay, but it's going to be useful for the proof nonetheless. So define fictitious algorithm A by. So what it does is it does the same thing as perturbed follow the leader. Sorry, this is wrong. So perturbed follow the leader at the time of day t, the information it has is the first t minus one cos vectors, right? So this is what I meant to write, s less than t.
00:50:01.130 - 00:50:31.220, Speaker A: So it uses all the information that it has, which of course is only from the past, to make its decision. Okay, this algorithm capital A, it's going to also make use on day T of the cost vector at day T. Now, of course, no online algorithm in our model actually knows the cost vector at time T when it makes the decision at time T. But for the proof, let's think about this fictitious algorithm, which could sort of look ahead one step.
00:50:31.370 - 00:50:32.070, Speaker B: Okay?
00:50:33.960 - 00:50:38.150, Speaker A: All right, so it sets at.
00:50:43.480 - 00:50:43.796, Speaker B: To.
00:50:43.818 - 00:50:54.460, Speaker A: Be argmen of sum over all s up to and including GT implausibly, and again, with the random bonuses.
00:50:55.760 - 00:50:56.510, Speaker B: Okay?
00:50:58.560 - 00:51:04.610, Speaker A: And so again, for emphasis, we could not implement this algorithm for proof purposes only.
00:51:05.780 - 00:51:06.530, Speaker B: Okay?
00:51:09.540 - 00:52:03.520, Speaker A: All right, why did I do this? Well, this turns out to be a really useful intermediate object to argue about. So let me tell you the three claims which together imply the theorem that I'm erasing. So three claims, first of all with probability one, the regret of this fictitious algorithm a. And what do I mean by regret? I mean the total cost of a. Okay, so the sum over little t of CT at so cost of a, minus cost of best fixed action. That's what I mean by regret. The claim is that this is at most the biggest of the perturbations.
00:52:03.520 - 00:52:08.120, Speaker A: Remember, x of a is the number of coin flips until I get heads.
00:52:08.220 - 00:52:08.870, Speaker B: Okay?
00:52:10.440 - 00:52:22.436, Speaker A: So we have this parameter epsilon, which we haven't specified yet, but as epsilon gets small and small and smaller, so the likelihood of a heads is smaller and smaller, the sizes of these x of a's is going to be getting bigger and bigger.
00:52:22.548 - 00:52:23.210, Speaker B: Okay?
00:52:23.660 - 00:52:56.640, Speaker A: So the right hand side here is getting bigger as epsilon gets small. Okay, notice actually that if all the x sub a's were zero. So if you think about this, so this was probably one and the randomization is over. The only thing that's random is these perturbations, right? So the claim is that this is true even if all the x of a's are zero. Okay, so an x of a is zero perturbed follow the leader is exactly the same as follow the leader.
00:52:57.060 - 00:52:57.820, Speaker B: Okay?
00:52:57.990 - 00:53:19.320, Speaker A: So a special case of what we're proving here is that if there are no perturbations, then the regret of algorithm A-I-E follow the leader, where you additionally are thinking about the cost vector t, you're sort of cheating and adding t that's at most the cost of the best solution.
00:53:21.840 - 00:53:22.590, Speaker B: Okay?
00:53:25.280 - 00:54:26.008, Speaker A: All right, so that's the first claim. Second claim is that, yeah, we're cheating, so what do we do? So what's the difference from perturbed follow the leader pftl and a? The difference is that we added the current day's cost vector into the computation. That's the only difference between the two algorithms. Okay, so we're know we're looking ahead into the future by a day, but the claim is that cheating doesn't actually matter much for the cost computation. Okay, so the claim is that the expected regret of an algorithm that we can implement perturbed follow the leader is at most the regret of this algorithm that we can't implement a. Okay, it's not quite as good, but it's only going to be off by epsilon times capital t. Okay? So epsilon sort of difference per time step, right? Now, in step one, the smaller epsilon was, the bigger the x of a's, so the bigger our loss.
00:54:26.008 - 00:54:30.496, Speaker A: But in step two, as we bring epsilon down, actually the less loss that we're doing.
00:54:30.598 - 00:54:31.296, Speaker B: Okay?
00:54:31.478 - 00:54:59.400, Speaker A: So for step one, we want epsilon to be big. For step two, we want epsilon to be small. And then the final claim, which is quite simple, is that it's very easy to understand how big this is going to be as a function of epsilon. So in expectation, the biggest of the perturbations is going to be logarithmic in the number of actions divided by epsilon.
00:54:59.820 - 00:55:00.376, Speaker B: Okay?
00:55:00.478 - 00:55:46.200, Speaker A: So remember, the expected value of one of these, as we said, is one over epsilon. So we have a bunch of things. Their expectations of one are epsilon, we have n of them. The biggest of those n is going to be like log n bigger. Okay, log n over epsilon. So let me just point out that these three claims imply the theorem. So upshot, if we chain these together, the expected regrets of perturbed follow the leader.
00:55:46.200 - 00:56:38.324, Speaker A: So we chain all three of these together is at most right? So this regrets at most that and then we pick up an additional epsilon t and that's just that. So this is going to be at most epsilon t plus o of log n over epsilon. Epsilon remembers a parameter of our choosing. So our follow up, the perturb leader is just parameterized by epsilon. So we should set it to balance these terms. And so this is just o of root t, log n with the judicious choice of epsilon equal to log n over square root of log n over t. Okay, so this is why assuming claims one, two and three, we get the desired theorem.
00:56:38.324 - 00:56:40.996, Speaker A: This was exactly the loss bound claimed in theorem.
00:56:41.188 - 00:56:41.930, Speaker B: Okay.
00:56:45.720 - 00:57:12.690, Speaker A: Questions about that. So I owe you the proofs of these, but other than that, any questions? Okay, so let me address the claims in reverse order. This is straightforward, okay? So like we said, it's the expectation of one thing is one over epsilon. You have n of them. The max is a login factor bigger. You've seen a lot of statements like that. I'm going to put this on as an exercise on homework number ten.
00:57:12.690 - 00:57:39.380, Speaker A: Literally, you just compute the probability that the geometric random variable is sufficiently large and take a union bound and you're pretty much done. Okay, so that's easy. Claims one and two are not especially difficult, but they're very clever. They're very clever proofs. Okay, so let me prove two and then prove one's.
00:57:40.060 - 00:57:40.504, Speaker B: All right?
00:57:40.542 - 00:57:59.740, Speaker A: Proof of two. All right, so we want to show that the regret, so let's just keep in mind what the difference between these two algorithms are.
00:57:59.810 - 00:58:00.284, Speaker B: Okay?
00:58:00.402 - 00:58:37.192, Speaker A: So in both of these algorithms, they preprocess the same way with these random bonuses, these x subays, and then at every time step they pick the action, which minimizes the cumulative cost so far plus the random bonus. The only difference being that a uses day t's cost vector in its computation. So it uses the first t cost vectors p perturbed follow the leader is a real algorithm. So it only uses the first t minus one cost vectors. But we want to say the expected difference in regret is at most epsilon T. So we're going to prove actually that the difference in the expected costs of the two algorithms is at most epsilon T. So they have almost the same cost.
00:58:37.192 - 00:58:48.156, Speaker A: So therefore they have almost the same regret. Actually, we're going to prove something even stronger than that. We're going to prove on almost every single day the two algorithms do exactly the same thing.
00:58:48.338 - 00:58:48.876, Speaker B: Okay?
00:58:48.978 - 00:59:07.148, Speaker A: So we're going to prove that for every single day T with probability, the probability that they do something different is at most epsilon. Remember, the costs I've assumed are between zero and one. So if you do something different, it at most introduces a cost of one difference in one in your costs.
00:59:07.244 - 00:59:07.504, Speaker B: Okay?
00:59:07.542 - 00:59:40.296, Speaker A: So if in only an epsilon fraction of the days do you do things different, that translates to a difference in only epsilon T in your cost, expected cost. So that's what I'm going to prove. Okay, so for every T everyday T, the probability that they do something different from each other is the most epsilon. All right, so fixed at. Now here's another sort of trivial but very nice observation.
00:59:40.408 - 00:59:41.070, Speaker B: Okay?
00:59:41.760 - 01:00:31.224, Speaker A: What can you tell me? So, again, we want to prove on a given day T, these two algorithms are going to do very likely exactly the same thing. What can you tell me about the action that Pftl plays on day T versus the action that our fictitious algorithm plays the previous day? On day T minus one, it's the same. Okay, so on day t, per turd follow the leader, they're using the same random bonuses or the same distribution of a random bonuses pftl, what does it do? It uses all the information. It has the first T minus one cost vectors and it picks the best one. Okay, the day before A already knew that information, right? And it used all that information to make a decision.
01:00:31.352 - 01:00:32.030, Speaker B: Okay?
01:00:32.560 - 01:01:32.620, Speaker A: All right, so the probability that these two actions do something different on day T is the same as the probability that algorithm A does something different on consecutive days. Okay, so now what we want to prove is this, this fictitious algorithm on the right, if we look at an arbitrary pair of consecutive days, t minus one and T, we want to argue that with probability at least one minus epsilon, it picks the same action both days. That's what claim two has been reduced to. Okay, so why is that true? Let me tell you a sufficient condition for that. Okay, so suppose we're at day T minus one because we're thinking about the fictitious algorithm. We're curious about whether it switches actions between day T minus one day T. Suppose on day T minus one there's a big difference between the best action and the second best action.
01:01:32.620 - 01:02:17.756, Speaker A: By big difference, I mean the cumulative cost so far, plus the random bonuses is strictly bigger than one difference. Okay, let's say the best action is action number seven. Its score, its random bonus plus cumulative cost is 124 and the next best one is like 121.5. Okay, well then we're definitely picking action seven tomorrow. Costs are between zero and one, remember? Okay, so the gap in the cumulative cost between two actions can only narrow by one in a given day. In the worst case, action seven had cost one, the second best action had cost zero, and the cumulative cost gets closer together by one. But if a day T minus one, it was already outcompeting the other one by strictly bigger than one, then we're definitely going to play it the next day as well.
01:02:17.756 - 01:02:47.860, Speaker A: Okay, so where do we stand now? What we need to prove, we need to prove that on a given day, or it's sufficient to prove that if we look at a given day, the first best action is much better than the second best action with probably at least one minus epsilon. Okay, so let me prove that to you. So fix a day t. So we're going to use the principle of deferred decisions to do this. Okay, so define.
01:02:52.520 - 01:02:53.280, Speaker B: Whoop.
01:02:53.440 - 01:02:57.984, Speaker A: So let C sub A be the cumulative costs, not counting the random bonuses.
01:02:58.112 - 01:03:00.990, Speaker B: Okay, so it.
01:03:11.840 - 01:03:13.328, Speaker A: Yeah, it feels like.
01:03:13.334 - 01:03:22.130, Speaker C: You'Re making the analysis by assuming the difference between the best cost function and the second best cost function independent each day.
01:03:22.580 - 01:03:42.410, Speaker A: No. So ultimately there'll be a linear of expectations application. So I don't care about independent, I don't need independence across days. So all I'm asserting is pick your favorite day t or your favorite consecutive pair days t minus one and t. I just want to say it doesn't matter what T is, it's just going to be the case. Probably at least one minus epsilon. You play the same thing both days.
01:03:42.410 - 01:04:08.912, Speaker A: Okay, so then by our previous equivalence, that says that pftl and A play the same thing with probably at least one minus epsilon i. They play different things with probability at most epsilon. So that means in a given day t, the expected difference in cost is at most epsilon. And so now I just sum over the days and that's just a linearity of expectation computation. So you're absolutely correct in the objection that different days are not independent of each other, but it just doesn't matter.
01:04:09.046 - 01:04:10.770, Speaker B: Yeah, good question.
01:04:11.380 - 01:04:36.808, Speaker A: All right, so just to remind you, what are we doing? We're trying to say we're considering the fictitious algorithm. Actually, at this point it doesn't matter, but it's the fictitious algorithm. We fixed a day t. There's some best action, there's some second best action. We want to say that the difference between their cumulative cost plus random bonuses at this given time is very likely to be bigger than one. Now I want to use the principle of deferred decisions, okay? So that means. I want to sort of flip the random coins only as I need them.
01:04:36.808 - 01:05:13.584, Speaker A: And so let's start with no random coins at all being flipped, okay? So it's almost like think of the bonuses as zero for the moment. So that gives us these capital C sub A's. All right? So the picture. So say there's like four actions, okay? So there's action one. So here C sub A is going up. Okay, I guess this should be indexed by t also to the given day T. Okay? So these are whatever they are, okay? So remember the adversaries fix some cost function sequence.
01:05:13.584 - 01:05:51.932, Speaker A: So there just are these numbers at day t, whatever they are. And we haven't flipped the coins for the random bonuses yet, okay? Now if we were just playing normal follow the leader, we'd choose this one. We choose the smallest. Now in perturbed follow the leader, we may not choose this one because all of these are going to get sort of shifted down by random amounts. And we're going to pick the one which after the random subtraction is the minimum. So here's what we're going to do. We're going to flip the coins lazily only as needed to figure out which action perturbed follow the leader is actually going to take at DT.
01:05:52.076 - 01:05:52.864, Speaker B: Okay?
01:05:53.062 - 01:06:19.816, Speaker A: Here's what we do. We start with the worst action, all right? And we start trying to figure out what this guy's random bonus is by flipping these coins, okay? Remember, we flip a coin. If we get tails, we subtract one and then we repeat the whole experiment. It's memoryless, okay? If it's heads, then we stop. So we flip a coin for the worst action first. If it's heads, then we can definitely exclude this action from further consideration. Everybody's only going down.
01:06:19.816 - 01:07:18.190, Speaker A: So if this guy stops bigger than the other ones, it's definitely not going to be picked as the minimum this day, okay? So we start flipping coins, okay, but maybe we keep getting tails. So we keep getting better and better lower bounds on the random bonus of this person and all of a sudden it's in a tie with the next best action. So now what we're going to start doing is we're going to start flipping coins for both of these each. So it's almost like a runoff. And these are going to get subtracted, okay? So we flip coins. This is how we're determining the bonuses. As soon as we get heads for something, we can discard it, okay? So it's kind of this race, right? We're having this coin flipping race where some people have these where they have different starting positions depending on the C sub base, okay? Now at some point we'll have figured out which one is the best one, all right? So like, imagine it comes down to a race between action two and action four.
01:07:18.190 - 01:07:41.270, Speaker A: So these guys, the head was flipped and they were still really bad. And so we're going back and forth between actions two and four. We got a tails, we got a tails. Then we get another tails. We get another tails. We keep going back and forth and at some point we're going to get a heads on one of them, right? Say this one. Okay, so this guy's done.
01:07:41.270 - 01:08:19.136, Speaker A: We actually know the random bonus plus cumulative cost of this action four. Action two. So we've determined that this will be the smallest, but we haven't actually determined what its random bonus is. We've only determined a lower bound on its random bonus. Okay, so to fully sample action two's random bonus, we have to keep flipping coins, right? And what if we get a tails on the next one? If we get a tails on the next one, this guy's actually going to wind up not just less than action four, he's already less than action four. One more tails. This person's strictly more than one less than action four.
01:08:19.136 - 01:08:36.288, Speaker A: Okay, all we need is one more tails. And that happens with probably one minus epsilon, okay? So that's how you do it. So first you think of the random bonuses as zero. You just sort them by the cumulative costs at day t without the random bonuses, flip coins, lazily to figure out which guy's the winner, the smallest, and then keep flipping.
01:08:36.384 - 01:08:36.788, Speaker B: Okay?
01:08:36.874 - 01:08:43.320, Speaker A: And the only bad case is if you get aheads immediately after determining which action happens to be the minimum.
01:08:45.740 - 01:08:46.650, Speaker B: All right?
01:08:48.060 - 01:09:38.548, Speaker A: So done by principle of deferred decisions plus linearity of expectation. So that's claim two. So that's one of the super, that's super clever two claims. All right, so claim two. Claim one is a quite simple computation. So first assume so I'm claiming this no matter what the random bonuses are, okay? So this is really where the genius of the definition of the fictitious algorithm comes up. So we're saying that the regret what did we want? We cared about the regret of follow the perturbed leader.
01:09:38.548 - 01:10:13.780, Speaker A: We had no idea how to analyze that. We did understand by the last claim how to at least tether the regret of perturbed follow the leader to this other fictitious algorithm A. Turns out algorithm A is something whose regret we can get a handle on directly, okay? And that was really the point of the definition of A. So in what sense can we get a handle on it? We want to say it's regret is at most the biggest perturbation. Let me show you the let me just sort of clarify things. Let's assume all the X A's are zero, in which case algorithm A is just follow the leader with this one extra cost vector, okay? It's not perturbed. So in that case, I need to show you that the regret's at most zero.
01:10:13.930 - 01:10:14.484, Speaker B: Okay?
01:10:14.602 - 01:11:09.408, Speaker A: So follow the leader with omniscience for one extra day is as good as the best fixed action in hindsight. So suppose the X A is equals zero, then all right, so let X a star be best fixed and let a one up to at be a's actions. Okay, so this is our competitor, the cost of a star day after day after day, and then our algorithm algorithm capital A incurred these costs. So let's begin with our competitor. So we want to say our cost is at least as good as this if we're doing follow the leader with the one extra piece of information each time step. So we want a lower bound.
01:11:09.504 - 01:11:10.150, Speaker B: Okay?
01:11:11.340 - 01:11:53.450, Speaker A: All right, so what can you tell me about the relationship between A star, the best fixed action in hindsight, and at when the perturbations all happen to be zero? So let me just remind you what the in perturb follow the leader. How did we decide things? We took the argmen with the random bonuses, which we're currently assuming is zero. Okay, so ignore these plus sum up over all the days, including today.
01:11:56.220 - 01:11:56.680, Speaker B: Of.
01:11:56.750 - 01:12:17.810, Speaker A: The so this was the decision rule that perturbed follow the leader used. Okay, so look at all the days up to today so it's fictitious. And pick the best one cumulative cost up to today. So what can you tell me about the relationship between a star and a capital T? Assuming all the X A's are zero.
01:12:21.280 - 01:12:25.788, Speaker C: At gets to choose a different action each time.
01:12:25.874 - 01:12:34.800, Speaker A: Oh, so you're jumping ahead. That's pretty much the whole proof, actually. Yeah, but let's just focus on the very last one, a capital T. Yeah, okay. Yeah, no, but you're right.
01:12:34.870 - 01:12:35.612, Speaker C: Then they're equal.
01:12:35.676 - 01:12:40.160, Speaker A: Exactly. So a capital T actually equals a star when the X's are all zero.
01:12:40.310 - 01:12:40.816, Speaker B: Okay.
01:12:40.918 - 01:13:11.500, Speaker A: A star by definition is the best action in hindsight. If you stare at this and take little T equal capital T, you realize this is also just the best action in hindsight. Okay, but let me just write it this way as an inequality, even though it's an equality. So basically, a SuperScript capital T is chosen so that this is true. It's chosen to optimize this so it can only be better. So now let me just peel off continuing this derivation. Let me peel off the last term.
01:13:11.500 - 01:13:45.260, Speaker A: So this is just rewriting the left hand, the right hand side, whoops minus one. Okay, so what's going to happen if instead of if I substitute a cap T here with a capital T minus one, it can only get smaller. So if you look at the decision rule of the algorithm at every time step, it's picking the thing that minimizes the cumulative cost up to that point.
01:13:45.410 - 01:13:46.012, Speaker B: Okay?
01:13:46.146 - 01:14:22.372, Speaker A: So if I replace this capital T with a T minus one, this only gets smaller. So that's by the choice of at minus one, okay, a T minus one was chosen to make that sum as small as possible. So it's no bigger than if a capital T was there. Now I do the same thing. I just peel off that T minus one th term. So that leaves me with a sum over the first capital T minus two days. I substitute in A to the SuperScript T minus two.
01:14:22.372 - 01:14:25.624, Speaker A: That's only better by the choice of a T minus two and so on.
01:14:25.742 - 01:14:26.504, Speaker B: Okay?
01:14:26.702 - 01:15:13.184, Speaker A: So at the end of the day, what you get, you just get the sum of all these peeled off terms, which is just sum over the days of CT of at, also known as the cost incurred by the Ptfl algorithm. Sorry, Pftl algorithm in the special case where the x's are zero. Okay, actually, it turns out, and because I'm out of time, I won't do this for you. But it'll be very easy for you to do yourself if with arbitrary x's, all that happens is, in every inequality in this derivation, you lose a little bit. Because in general now, it's not the case that this is at least as good as this. Maybe the algorithm got misled by the perturbations. And remember, we expect some error in this step because of the perturbations.
01:15:13.184 - 01:15:43.020, Speaker A: Maybe it's the case that you didn't choose the best thing. You didn't choose a star, because a star got a small random bonus and your a capital T got a big random bonus. So there's going to be some error here, and it's going to be just the difference in the bonuses between a star and a capital T. That's all that matters. So you get one extra difference of two bonuses. You get a difference of two bonuses in every single inequality. You add up all of those loss terms, you get a very elegant telescoping sum of differences.
01:15:43.020 - 01:15:58.100, Speaker A: And at the end of the day, this exact derivation holds modulo one difference between two bonuses. That's the only difference, okay? And so the difference between two bonuses, they're both non negative, so that's bounded above by the biggest bonus of them all. And that's what we need to prove.
01:15:58.600 - 01:15:59.350, Speaker B: Okay?
01:16:00.520 - 01:16:40.350, Speaker A: So that concludes the proof of theorem. All right, so sometimes I do a top ten list at the end of the class, but I'm already over time, so I think I'll just post a top ten list on the website in the future. You've learned a ton of concepts in the course, and I know it's sometimes hard to keep track of the overarching narrative and all of the very connections, for example, between IID distributions, instant optimality, and all this other stuff, but out of time. So I would be remiss if I didn't thank Rishi, who I think you'll agree is a totally killer ta. So thanks to Rishi, thanks to all of you. It's been a lot of fun.
