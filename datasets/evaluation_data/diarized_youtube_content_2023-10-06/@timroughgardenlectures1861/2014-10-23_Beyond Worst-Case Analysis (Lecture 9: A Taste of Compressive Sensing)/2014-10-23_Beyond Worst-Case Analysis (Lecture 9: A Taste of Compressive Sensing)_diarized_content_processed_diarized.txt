00:00:00.090 - 00:00:43.094, Speaker A: Today's lecture is going to be a continuation of this theme of exact recovery. So I'm just curious, how many of you have heard of the Buzword compressed or compressive compressive sensing? Some of you? Okay, it's somehow like it's actually very CS esque, but sort of comes from different fields. So not all computers. Some computer scientists don't actually hear about it. So I'm glad after this lecture, all of you will know about it. So, you know, just to remind you what the theme was, we're sort of in the middle of studying problems that are generally NP hard, but we're asking about additional conditions on the input under which a heuristic, which normally would not solve it optimally because it's a polynomial time heuristic and an NP hard problem. But under these extra conditions, actually this heuristic does recover an optimal solution.
00:00:43.094 - 00:01:24.690, Speaker A: So two lectures ago, we were talking about stable k median instances, and we looked at the single link plus plus algorithm, which basically optimizes only over a subset of the possible solution. So it can do it in polynomial time, but we showed that if k median instances are stable, then actually the optimal solution is guaranteed to be in that restricted part of the space, and it's actually going to recover the optimal solution. Then on Wednesday, we studied cut problems. St cut was our warm up and then Min multi way cut. And we looked at a linear programming relaxation, which again, generally is not going to be optimal. But if it's a stable instance, then linear programming does give you the optimal solution. So those were cases, right? So this lecture is going to be another one in that same theme.
00:01:24.690 - 00:01:47.470, Speaker A: And this is about solving linear systems. So consider the problem. You're given A, you're given B you want to solve for X. A are just real values that can be anything. So it's just your typical linear system. But imagine A isn't square, but rather imagine there are fewer rows than columns.
00:01:47.810 - 00:01:48.222, Speaker B: Okay?
00:01:48.276 - 00:02:29.066, Speaker A: So there's your A, there's your X, and there's your B, your right hand side. So how many solutions, feasible solutions, does a linear system like this have? There's actually sort of two answers to that question. It could be one of two things. Either what or what? Well, it could be none, right? It could just be you have a couple of contradictory constraints. What if you do have one solution? Is it going to be unique? No, there's actually going to be an infinite number of feasible solutions, right, because it's an underdetermined system.
00:02:29.168 - 00:02:29.980, Speaker B: All right?
00:02:34.370 - 00:02:56.982, Speaker A: So if M equals N and it's full rank, if it's invertible, then there's a unique solution, namely a inverse B. But in general, if you have fewer rows, fewer constraints than degrees of freedom in x, you have an infinite number of solutions whenever you have one. One sort of heuristic way to think about this is it's almost like as computer scientists, you can almost think of it like each row of this system gives you one bit of information about x.
00:02:57.036 - 00:02:57.254, Speaker B: Okay?
00:02:57.292 - 00:03:15.110, Speaker A: Not really a bit because these are real numbers, but it gives you like it pins down sort of one of the degrees of freedom, right? So if x is in RN, then there are N degrees of freedom. So to know it uniquely, you need to pin down all N degrees of freedom. So you need n constraints. So if you pin down only M of them, there's still the subspace of rank n minus m of feasible solutions.
00:03:15.190 - 00:03:15.820, Speaker B: Okay?
00:03:16.190 - 00:03:41.614, Speaker A: So it's an underdetermined linear system. Shortly I'll tell you why one might want to care about these things, but let me just sort of say what's going to be the nature of the computational problem. Basically, amongst this infinite number of feasible solutions, we're going to be interested in singling out the one which is somehow the most, the best or the most meaningful. And in compressive sensing, the definition of meaningfulness is sparsity.
00:03:41.742 - 00:03:42.420, Speaker B: Okay?
00:03:43.590 - 00:04:30.580, Speaker A: So we call a vector x k sparse if all of its entries are zeros except for most k of them. So most k nonzeros. So I'm going to use the notation SUPP of x. This denotes the support by definition that's the set of coordinates on which x is nonzero. So the number of nonzeros is at most k. Okay? And for concrete values, maybe think of k if x has linked n, maybe think of k as like root n, maybe even log n. So those would be we don't want to think of as constant, think of as bigger than constant but much smaller than n.
00:04:30.580 - 00:04:44.950, Speaker A: All right? So the perspective of this lecture, or really of this topic, is that sparse solutions are more meaningful.
00:04:45.610 - 00:04:46.360, Speaker B: Okay?
00:04:50.090 - 00:05:10.554, Speaker A: And there are many applications where this seems like a quite justified assumption. I mean, a couple of caveats. So in applications, there's usually kind of two twists on sparsity that you need to make it really practical. The first one is like, okay, so maybe the vector isn't like zero in N minus k coordinates, but it's really small in N minus k coordinates.
00:05:10.602 - 00:05:10.814, Speaker B: Okay?
00:05:10.852 - 00:05:43.446, Speaker A: So sort of most of its mass is concentrated on just k coordinates. So that's the first kind of relaxation you want to do. The second thing is a lot of real applications you're sparse not in sort of the standard basis, but in some other basis. So like the Fourier and wavelength bases are popular ones. But the point is, everything I'm going to tell you about today in this lecture, both the results and the way the results are obtained also extend really quite elegantly to those extensions, to having approximate notions of sparsity or sparsity in bases other than the standard one. So for the lecture, let's just keep it simple. I'm only going to talk about exact sparsity, and we're all going to be thinking about sparsity meaning the standard basis.
00:05:43.446 - 00:05:48.118, Speaker A: So there's literally just in the X that we're looking for, there's knocks and the rest are zeros.
00:05:48.214 - 00:05:48.860, Speaker B: Okay?
00:05:50.290 - 00:06:26.600, Speaker A: All right, so then the computational problem we're thinking about again has this exact recovery flavor where there's some ground truth or some planted solution out there. We want to know when can we get it back? So recover some ground truth, K spar solution. Okay, so we're given A, we're given B. Let's think of it as we know K. We're going to assume that there's some K spar solution, and the question is, can we get it back?
00:06:30.830 - 00:06:31.580, Speaker B: Okay.
00:06:33.390 - 00:07:31.898, Speaker A: So that's going to be the kind of problem that we're thinking about. All right, so why think about this? So let me tell you about sort of the compressive sensing narrative. So here it's sort of a shift in how we're usually thinking about it, right? Normally we think of sort of the input as like coming down from the skies and we just have to deal with it. We're just given this A and B. We've talked about some things like what are some conditions on A and B where hopefully if nature is providing this input, maybe it has extra structure and we can do some exact recovery. The compressive sensing narrative, actually, you're usually responsible for actually designing the A and B. So if you studied error correcting codes, it's sort of in the spirit of error correcting codes, where if you're going to have a linear code, maybe that defines an encoding and you want to somehow choose a matrix A to make your life easy later, like it's easy to decode, for example.
00:07:31.898 - 00:07:55.538, Speaker A: So it's sort of similar here. And so the way one normally thinks of it in compressive sensing is you think of there is some vector X. So forget about A and B for a second. There just is an X, okay, some ground truth, a signal they would call it. And that could be something like some image you're trying to reconstruct or something like that. And then you make measurements. You somehow try to access this ground truth X.
00:07:55.538 - 00:08:12.234, Speaker A: And the way one thinks about it is one thinks about a row of this linear system as a quote unquote linear measurement of X. So think about it this way. Each row of this matrix, right, so this is an N vector, and this entry over here is just the dot product of this row with X.
00:08:12.352 - 00:08:12.682, Speaker B: Okay?
00:08:12.736 - 00:09:04.390, Speaker A: So each entry of B is just some encoded version of X. It's X dot product, inner product with some other vector. So in that sense, each of these can be thought of as a linear measurement of X. So the more values of B you get, the more linear measurements you take. It seems like the more information you're getting at about this unknown X that you'd like to recover. And so then what people think about is, okay, how can we actually design measurements? How can we actually design these linear systems so that from A and B, we can actually recover what X is. Now, we already talked about how sort of the obvious approach would be, well, why not just take n linearly independent measurements? So why not just find an N by n matrix A, which is invertible, and then if you're also given the right hand side, you just multiply both sides by a minus one and you recover x, so that would be n rows.
00:09:04.390 - 00:09:20.794, Speaker A: Okay, n measurements. So the whole deal in compressive sensing says, well, what if we assume more about the structure of the unknown signal x, and in exchange perhaps we can get away with fewer measurements, that is, fewer rows.
00:09:20.922 - 00:09:21.600, Speaker B: Okay?
00:09:22.130 - 00:10:01.510, Speaker A: So by contrast, if you think just about k sparse vectors, now intuitively there's not as many degrees of freedom, there's not as much unknown about what X could be. So the hope would be that many fewer measurements suffice. Okay, so that's sort of the idea. So what are some applications? Well, so like, one buzword you can look up, which was sort of popular early in the field, was the single pixel camera. So I'm not going to say much about this, I'll just sort of tell you the idea. It's widely thought and sort of empirically validated that images that people take in the real world are in some sense sparse.
00:10:01.590 - 00:10:01.930, Speaker B: Okay?
00:10:02.000 - 00:10:17.742, Speaker A: Not sparse in sort of the obvious basis, but there'll be some other well studied basis, like a wavelet basis under which real images are sparse or approximately sparse. I mean, indeed, if you think about sort of signals which sort of don't have any sparse representation, people usually think of those as like random noise.
00:10:17.806 - 00:10:18.082, Speaker B: Okay?
00:10:18.136 - 00:11:25.346, Speaker A: So real images have sparse representations in some sense. And so somehow the sort of obvious way to sort of take advantage of that, that you might first think about is like, okay, you get your camera, you pack in as many pixels as you can, so you just get as much information as possible when you take the actual photo. And then later you run some compression algorithm on it and it squeezes it down, okay? And probably you can squeeze it down to something much smaller than what the camera took without really losing anything as far as being able to reconstruct it later. So the idea here is, well, wouldn't it actually kind of be a lot smarter if at the end of the day we're just going to have this compressed representation? Why don't we just actually have the camera just like actually take that compressed representation right away? I mean, the image it wants to take is already low dimensional. It was already sparse. So why not just take a small amount of information now for later reconstruction? So in that context, you want to think about X as sort of the sparse representation of the image being taken, and then you can think of these linear measurements as just things like random dot products of the light intensity at each of the pixels in the camera, okay? So you're literally just remember this is one number. This is a whole bunch of numbers, like the light intensity of all the different pixels.
00:11:25.346 - 00:11:43.934, Speaker A: And you can think of if you choose this to be a random row, then this is going to be a random inner product between your image and some dot product. And so then what this would say is actually, rather than just storing in the naive way, the light intensity of every pixel, you just store all these random linear combinations and you can get the image back later.
00:11:44.052 - 00:11:44.720, Speaker B: Okay.
00:11:45.250 - 00:12:00.158, Speaker A: Another application is in MRI. So magnetic resonance imaging. So this is where you're trying to take a scan, like, of someone's brain or some other part of their body. And so there the number of measurements literally corresponds to the length of time of this scan.
00:12:00.254 - 00:12:00.802, Speaker B: Okay?
00:12:00.936 - 00:12:11.590, Speaker A: And when you get an MRI, you've got to stay really still. Depending on what you're taking a scan of, you might even have to not breathe. So it's pretty clear that less measurements, meaning less of a scan time, can be a pretty big win.
00:12:11.660 - 00:12:11.894, Speaker B: Okay?
00:12:11.932 - 00:12:34.462, Speaker A: So that's why people care about this kind of stuff. Okay, so that leads to the questions, how small can a linear system be? So that at least in principle, so that at least in principle, you have enough information to recover a K sparse vector. But then also, of course, we don't just want to know that there's some unique sparse reconstruction. We also want a fast algorithm that finds it.
00:12:34.516 - 00:12:34.686, Speaker B: Okay?
00:12:34.708 - 00:12:50.420, Speaker A: So we want to answer both of those questions. All right? So for this to be doable again, let's sort of think about the lines in the sand. So we need some assumptions about what the matrix A is.
00:12:56.470 - 00:12:57.380, Speaker B: All right?
00:12:58.070 - 00:13:20.934, Speaker A: So the first thing we've already sort of touched on, which is that A, we want it to be nontrivially small. So we want it to have fewer than n rows, but there's going to be some limit on how few rows we can get at. So, like, imagine A had just one row. Imagine like, K was ten. So there are these various sparse vectors that X might be, and we have no idea which one. We just know it's some ten sparse vector. And imagine it's like an A with one row.
00:13:20.934 - 00:13:23.920, Speaker A: So we take one linear measurement, we get back like seven.
00:13:24.690 - 00:13:25.294, Speaker B: Right?
00:13:25.412 - 00:13:40.946, Speaker A: Like this is clearly not enough information to reconstruct which of the K sparse vectors the real world is. Okay, so the first naive thought might be, okay, well, we said we kind of needed one row to pin down each degree of freedom that X might have.
00:13:41.048 - 00:13:41.506, Speaker B: Okay?
00:13:41.608 - 00:14:22.394, Speaker A: And the first thought might be, oh, as a K sparse vector, that's kind of like K degrees of freedom. It's actually a little more complicated than that, right? Because think even just about like zero one vectors that are k sparse. So remember x has length n, okay? So the number of zero one k sparse vectors, that's the number of ways to choose the k coordinates of the support out of n. So like n choose k, so that's something like n to the k such k sparse vectors. So just thinking sort of heuristically, if we think of each row of the matrix, maybe it's giving us like one bit of information. Again, that doesn't really make sense, but just, heuristically, maybe we get a bit of information from each row of the matrix. Then to kind of pin down uniquely which of these n choose k possible sparse vectors it is.
00:14:22.394 - 00:14:31.554, Speaker A: We're going to need log of n choose k rows, which is basically like k log n. Okay, Jeremy, but if.
00:14:31.592 - 00:14:46.194, Speaker C: X were zero one, couldn't you pick like primes or something on one? So consider the single row that I suspect that if x is zero one, you can do something with a single row.
00:14:46.322 - 00:15:07.742, Speaker A: Okay, I guess the point is think about supports. Suppose it's an unknown support, okay, right. And it turns out, so if you think about it for a second, you realize that actually if you knew the support of x, then you could recover it, right? Because you just force the other coordinates to be zero and then you'd have a normal linear system and you'd resolve for the unique solution x. So it kind of boils down to knowing what the support is and there's n choose k choices for the support.
00:15:07.876 - 00:15:16.682, Speaker C: You can find it if you don't have any bound on like if x is zero one and you don't have any bound on the size of the entries in a I think with a single row.
00:15:16.746 - 00:15:58.062, Speaker A: Okay, I'm just trying to motivate sort of there's going to be this k log n I'm trying to motivate or comes from. And the point is there's n choose k choices of a sport. And sort of the best case seems like you get a bit of information for each row. Okay, so e G, maybe we're going to hope for something like m better be at least k log n. Actually it can be slightly smaller than this, but I'm just going to stick with this for lecture. Okay, so M has to be sufficiently big, but again we want it to be way less than n, okay? So like k log n, we'd be pretty happy with if we're thinking of k as being root n or log n or something like that, this would be way less than n. And again, I hope you have the intuition that like k we don't think is enough.
00:15:58.062 - 00:16:22.150, Speaker A: There should be some dependence on n because of this freedom of which of the n coordinates are actually in the support. Okay, but so that's also not enough. Even if you take M to be something like this, there's still going to be some a's certain matrices a for which this is not going to work, even if it's a full rank, full row rank. Like, think of it as A, as like super sparse.
00:16:22.650 - 00:16:23.302, Speaker B: Okay?
00:16:23.436 - 00:16:29.734, Speaker A: So maybe all I do is I pick M different columns, and I stick a single one in each.
00:16:29.932 - 00:16:30.774, Speaker B: Okay?
00:16:30.972 - 00:16:43.018, Speaker A: So now if you hit this very sparse matrix with this very sparse vector X, b is going to be like mostly zeros, pretty much no matter which sparse vector you started from. So again, you wind up not getting information about the sparse vector X.
00:16:43.104 - 00:16:43.598, Speaker B: All right?
00:16:43.684 - 00:16:46.362, Speaker A: So at the very least, A better not be super sparse.
00:16:46.506 - 00:16:47.006, Speaker B: Okay?
00:16:47.108 - 00:17:23.930, Speaker A: So we're also not thinking we're going to prove that this is solvable for all A. So those are some lines in the sand. And so the questions we're going to ask is how big does M have to be and what extra conditions on A do we need before first of all, again, at least in principle, there's a unique case, sparse vector X that solves this problem. And then secondly, we have an algorithm that will find it for us. So any questions about that, about the definition of the problem or why it's interesting?
00:17:24.000 - 00:17:26.102, Speaker B: Yes. Why is it login?
00:17:26.246 - 00:18:19.980, Speaker A: What's the intuition behind the log again, so the vague intuition was this is just sort of log of N choose K. So these are the number of different supports of size K you could have in an N vector. And so somehow this is some kind of like if you think of it in terms of entropy or information, the sort of amount of uncertainty in what X could be seems to be sort of lower bounded by this quantity. And if you sort of believe that each row of A is only going to give you a constant amount of information, then you would need this many rows. Again, that's heuristic, there is a real proof of it. There is a real proof of lower bounds, but I'm intentionally giving you a cartoonish version of the intuition. Other questions?
00:18:23.650 - 00:18:24.400, Speaker B: Okay.
00:18:26.130 - 00:18:30.542, Speaker A: So let me actually tell you the algorithm before I tell you about the conditions on A.
00:18:30.596 - 00:18:30.766, Speaker B: Okay?
00:18:30.788 - 00:18:58.694, Speaker A: So we're going to be able to do this without actually many more assumptions than these obvious ones or these intuitive ones. And the algorithm is going to be in the same spirit as last lecture, in that we're going to look at a linear relaxation of the problem and we're going to show that actually the exact solution under some extra conditions of that linear program is exactly the case bars vector that we want.
00:18:58.812 - 00:18:59.238, Speaker B: Okay?
00:18:59.324 - 00:19:20.590, Speaker A: So again, it's just we're going to solve a linear program and then that's just going to be returned to us on a silver platter, which is great. In fact, this whole field basically was born because the problems, it started from the applications and people tried this heuristic, and then they observed, they got exact solutions back when they weren't expecting it at all. And then they asked, okay, what's a theory that would explain that? And that's really the origin story for compressive sensing.
00:19:22.130 - 00:19:22.880, Speaker B: Okay.
00:19:25.010 - 00:20:10.938, Speaker A: So the algorithm, we're going to solve a linear program, and clearly some of the constraints are going to be this, right? So we're given A, we're given B, we're looking for A, we, you know, we have some parameter k. We're looking for a case bar solution. So in particular, it better be a solution. So trying to directly say, minimize the number of non zeros of a feasible solution, that's an NP hard problem, it turns out. Okay, it has a discrete flavor, right? So that's not shocking. If you just say, amongst all solutions to this, find the one with the minimum number of nonzeros, we can't do that. That's sometimes called l zero minimization.
00:20:10.938 - 00:20:21.566, Speaker A: We're going to do heuristic, which is called L one minimization. And again, the motivation is this is something we can formulate as a linear program. And as we've discussed, linear programs can be solved efficiently, both theoretically and efficiently.
00:20:21.678 - 00:20:22.386, Speaker B: Okay?
00:20:22.568 - 00:20:57.406, Speaker A: So here's the exact encoding. So this is going to correspond to the L one norm of the vector x. So remember, the L one norm is the sum of the absolute values of the coordinates, and we already saw on Wednesday how to encode absolute values as linear inequality. So we're going to do that again here. So Yi is at least xi, and Yi is also at least minus xi. So for I equal one up to.
00:20:57.428 - 00:20:58.000, Speaker B: K.
00:21:01.090 - 00:21:49.790, Speaker A: So this is just a direct formulation of saying, amongst all feasible solutions to this linear system, compute the one that has the smallest L one norm, the smallest sum of absolute values of coordinates. That's exactly what this problem says. This is not the problem we care about or that we set out to solve, and there's no reason why one should expect this to work. Okay, you really, I think, would first expect if you solve this, what you'd get back is a solution to this linear system which probably has a nonzero value in every coordinate. And it's just like super small in most of the coordinates, right? Like if you have an epsilon in a coordinate, you're only penalized epsilon in this objective function. But for the objective function we care about, number of nonzeros, you're penalized one, you're penalized for the whole coordinate the same as if it's epsilon over it's 100. So at least that's what I would have expected this heuristic to do.
00:21:49.940 - 00:21:56.180, Speaker B: Yeah. So we're just trying to minimize the first k coordinates of x.
00:21:56.790 - 00:22:08.078, Speaker A: No? So the real problem is minimize the number of non zero coordinates, right? Yeah, sure. What is this problem? Is that the question in this formulation.
00:22:08.174 - 00:22:12.030, Speaker B: By writing I as one to k? Or are we just trying to minimize one to N?
00:22:12.120 - 00:22:36.400, Speaker A: Oh, this should be N. I'm sorry, my mistake. That was a typo. Thanks. Yeah, I got it right here some for Michael and yeah, okay. So there's really no reason to expect this to people. I doubt people would have thought to prove this theorem unless they actually just had the empirical evidence slapping them in the face.
00:22:38.530 - 00:22:39.280, Speaker B: Okay.
00:22:42.870 - 00:23:08.854, Speaker A: So here's the main result of this lecture, which is if we take M to be K log n, which seems sort of like a back of the envelope calculation to be roughly about the best we could hope for, and we let A be random. So random here can mean a few different things.
00:23:08.972 - 00:23:09.640, Speaker B: Okay?
00:23:10.650 - 00:23:25.882, Speaker A: So one thing would just be populate each entry with a plus or minus 150 50, everything IID. Another thing would be you just populate each one with a random Gaussian. So a normal distribution zero one. Obviously this gives you a dense matrix. We know you need a dense matrix.
00:23:25.946 - 00:23:26.318, Speaker B: Okay.
00:23:26.404 - 00:24:19.086, Speaker A: But it turns out lots of sort of ID ways of populating the matrix a work. The end result is not super sensitive to the definition of a random matrix. Then with high probability over the choice of A, if they're random Gaussians, you might get super unlucky. Maybe all the entries are zero, but probably not, right. So with high probability over the choice of A, it's the case that for all case sparse vectors x. So no matter what the ground truth is with a X hat equal to B. So again, we're sort of thinking in our minds like X hat existed already and then we're choosing A and that induces B according to A x hat B.
00:24:19.188 - 00:24:19.838, Speaker B: Okay?
00:24:20.004 - 00:24:31.790, Speaker A: So for all casepars x hat, that's a solution to the system. AB x hat is the unique solution to this LP.
00:24:33.650 - 00:24:49.180, Speaker B: Okay, it.
00:25:01.290 - 00:25:41.302, Speaker A: All right. So the plan of how we're going to go about this is sort of similar in spirit to how we analyze the single link plus plus algorithm. So you might recall a week ago, what we did is we said, okay, well, we have this algorithm, clearly it's not always going to work. Let's first identify some sufficient conditions on an instance under which this algorithm will be correct. And then let's argue that in that context, it was a stability property. Let's argue why the stability property implies that these sufficient conditions are met. So for this, the work we're going to do in lecture is we're going to identify sufficient conditions on A, on the matrix, the constraint matrix, so that we get what we want, so that the LP solves for the exact solution, then it's going to be the case.
00:25:41.302 - 00:25:46.498, Speaker A: And we're not going to prove this part, that a random matrix A does satisfy these sufficient conditions.
00:25:46.594 - 00:25:47.240, Speaker B: Okay?
00:25:48.570 - 00:26:03.050, Speaker A: All right, so for which A's is this going to work? All right, so let's do I need to do some preliminaries, okay? It's kind of geometric preliminaries about different norms.
00:26:06.050 - 00:26:06.800, Speaker B: Okay.
00:26:09.970 - 00:27:12.802, Speaker A: So it's clear we care about the L one norm, right? Because that's what this linear program is about. It's not going to be totally obvious right now why we care about the L two norm, but we're going to need to compare them. So just to make sure we're all on the same page, as we discussed, l one norm sum of absolute values, l two norm is the square root of the sum of the squares. So that's something all of you should have seen as far as one thing that's helpful to maybe keep in mind based in the next couple of things I'm going to say is the sort of geometry of these norms. You often visualize the geometry of norm in terms of the unit ball. So you ask fix a norm, what is the set of points that have a unit norm? What does that look like? So for the L two norm, that's familiar to all of you, that's just the circle and it so let me ask you. So the Unit ball for l One.
00:27:12.802 - 00:27:28.080, Speaker A: So the points with L one norm, that's either contained inside this ball I'm drawing, or alternatively, maybe it's entirely outside. So which of those two possibilities is it inside?
