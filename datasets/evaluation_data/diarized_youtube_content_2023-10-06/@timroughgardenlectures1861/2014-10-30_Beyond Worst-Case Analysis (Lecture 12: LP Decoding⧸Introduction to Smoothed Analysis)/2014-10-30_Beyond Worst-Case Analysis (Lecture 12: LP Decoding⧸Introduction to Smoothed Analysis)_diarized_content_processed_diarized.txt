00:00:00.170 - 00:00:34.802, Speaker A: We're going to finish up the discussion, the proof of LP decoding from last lecture. And then we're not going to do any hard technical stuff, but I'll give you sort of the high level goals of smooth analysis. That's what we'll do in the second half of the lecture. So let me help you page back in where we left off last time. The good news is, almost everything we did Monday, we can just sort of accept as true and then sort of just proceed from there, which is what were we doing? So we were considering to coding. So we talked about these families of codes, which you can define using a graph, a Bipartite graph. So on the left hand side, you have variables, on the right hand side, you have these parity checks.
00:00:34.802 - 00:01:12.578, Speaker A: So each parity check just consists of a subset of the variables and is asking that an even number of that subset of variables is equal to one. And the code words are exactly those vectors that satisfy every single one of those parity checks. And so we proved last time on Monday information theoretically, that these codes have good distance if you use a Bipartite graph that has expansion. So that was this short proof we did that, used the fact that many parity checks have a unique neighbor if you have a bunch of corruptions. And then we shifted attention to focusing. Okay, how can we do it computationally efficiently? We don't just want to know that it's possible in principle to decode. We want a polynomial time algorithm to do it.
00:01:12.578 - 00:01:43.786, Speaker A: And like several previous lectures we're studying in particular, when does linear programming work? What does it mean by work? Well, we write down the linear relaxation for this NP hard problem. Okay, so we started with an integer program, which was exactly the problem of finding the nearest code word to a given message, z. We looked at a linear relaxation. Sometimes it's going to be fractional because the problem is NP hard. But we're asking for sufficient conditions under which we solve the linear program in polynomial time. And it gives us back on a silver platter the nearest code word. Okay, so extra conditions under which we get the exact solution.
00:01:43.786 - 00:02:18.386, Speaker A: And we made a lot of progress on Monday. So specifically, we identified a sufficient condition under which the unique optimal solution to this linear program is indeed what we want is indeed the nearest code word to the corrupted code word that we were given. So let me just remind you sort of what that condition was. Okay? So here's theorem we're trying to prove. So all of this is exactly the same terminology and notation as Monday. So this is all in your notes or on the video from last time. So we're thinking about a Bipartitect graph that satisfies three conditions.
00:02:18.386 - 00:02:41.738, Speaker A: The first two conditions basically just says it's bounded degree. So that's the low density part of the low density parity check. In particular, every node on the left hand side has degree D. And think of D as maybe like twelve. So that's the number of parity checks in which each variable participates. The right hand side I e the number of variables in a parity check that's also constant, maybe twice as big, 25, something like that. The third condition is the expansion condition.
00:02:41.738 - 00:03:17.450, Speaker A: So I've written that here so that you can remember. So this is what says that on the left hand side, if on the left hand side of the bipartite graph, you take a constant fraction of the vertices S, so most delta times N, where delta is some constant, then the number of distinct neighbors of S and of course it's bipartisan. So the neighbors are on the right hand side. The number of distinct neighbors of nodes of S is almost as big as it could possibly be. Every node on the left hand side has degree D. So the most number of labels neighbors you could possibly have is D times capital S. You have 75% of that at least.
00:03:17.450 - 00:03:24.894, Speaker A: And that's true for every sufficiently small set s all the way up to a constant fraction of the nodes, all the way up to delta N nodes. So that's condition three.
00:03:25.012 - 00:03:25.630, Speaker B: Okay?
00:03:25.780 - 00:04:05.062, Speaker A: So we're trying to prove that whenever you have a code defined by such a bipartite graph, then this linear program and you suffered sufficiently few errors. So last time I wrote delta naught times n, I'm just taking delta naught to be delta over two. So if you suffered at most delta N over two errors, then the LP is exact. And we proved that if there exists feasible edge weights for the bipartite graph, and I'll tell you what that means, but we proved if there exists feasible edge weights, then we get the conclusion we want, then we get exactness of the LP. So we reduced the proof of the theorem to the proof under the same hypotheses of the existence of a certain family of edge weights.
00:04:05.126 - 00:04:05.450, Speaker B: Okay?
00:04:05.520 - 00:04:50.770, Speaker A: So that's what we accomplished on Monday. So the remaining action item, the remaining thing to do is to show that under these hypotheses, we really can exhibit weights that satisfy these conditions, okay? And so that's what we're going to do for the next 20 or 30 minutes or so, right? So the proof of that, that's what we did at the end of last lecture, that was just this weak duality argument which is sort of, kind of very algebraic, but you just follow your nose and it works out in some sense. Really, you should think of it. The reason these things are defined, the way they're defined is, is exactly so that proof at the end of last lecture works. I mean, really the way you think about is you start with the proof that you want and then you reverse engineer the condition. And this is just the condition that pops out, it's a special case of weak duality. All right, so what is a feasible edge weight? This is what we actually got to have to exhibit this lecture.
00:04:50.770 - 00:05:31.622, Speaker A: So we have this graph. So we're going to assign a weight to every single edge of the graph. That's wij could be positive or negative. And the first two conditions basically say it gives us a budget on the total edge weight that can be incident to a node on the left hand side. So if we think about some node on the left hand side and its various neighbors. All right, so one other thing to remember also on the homework, and we mentioned this Monday, by a shifting argument to prove the theorem, we can focus on the case where the code word we want to recover is all zeros, which is definitely a member of the code. So in our mind we think of that the sender sent the code word zero.
00:05:31.622 - 00:05:51.354, Speaker A: There were at most this many errors. So we get a vector with at most this many ones. Everything else is zero. And we want to solve this LP and have it solved to the all zeros solution. Okay, so capital I denotes the coordinates with no corruption. So these denote the zeros, j denotes the ones. So there aren't that many of these.
00:05:51.354 - 00:06:04.602, Speaker A: This is where we got a bit flip. Okay, feasible edge weights. That means that for all of the uncorrupted coordinates, you get a budget of one on the sum of these edge weights. But for the corrupted coordinates, these have to on average be negative.
00:06:04.666 - 00:06:04.846, Speaker B: Okay?
00:06:04.868 - 00:06:35.862, Speaker A: So the sum of these edge weights has to be less than minus one. So those are the first two conditions. Then we also had this condition on pairs, which says on the other hand, if you think about it from the right hand side of the graph and so you look at any two variables that participate in a parity check j, then the sum of their edge weight should be non negative. Okay, so we actually needed this for all even cardinality sets. But then we observed at the end of last lecture, if you have this for pairs, you have it for all even cardinality sets, just automatically. Okay, so that's where we were. So we want to prove that under these conditions these always exist.
00:06:35.862 - 00:07:16.114, Speaker A: And the proof is I'm just going to show you the weights. So any questions about what we're doing or why? That's the missing lima. All right then let's do it. So proof. All right, so J is the errors, remember, and there aren't too many of them and that's obviously going to have to be used somewhere. So here's the first step. So we're going to exhibit these weights and obviously we're going to have to make sure that both A and B hold and somehow right on the borderline, we're going to have to have still a third argument.
00:07:16.114 - 00:08:04.354, Speaker A: So the first step is we're almost going to have to take a closure operation over the corrupted coordinates. So we're going to supplement the corrupted coordinates j by extra coordinates which weren't themselves corrupted, but somehow they're almost like corrupted by osmosis, by other corruptions that they share a lot of parity checks with. Okay, so precisely, I'm going to define a set k of coordinates. So, again, coordinates I-E-A bunch of vertices on the left hand side. So these are coordinates which are not themselves corrupted, so they belong to i, but at least 50% of their parity checks do have a corrupted variable.
00:08:04.482 - 00:08:05.160, Speaker B: Okay.
00:08:07.130 - 00:09:03.930, Speaker A: So in other words, have neighbors in j. All right, so the picture you want to have in mind is so here are all the flipped coordinates. J and they have some neighbors, and you always want to think of sort of the parity checks that these variables appear in are polluted in some sense. So they've got some things going wrong. And K so Vertex and K is somebody for whom a lot of their parity checks contain some other corrupted variable. All right, so that's the definition, and you'll sort of see why we use this definition in a second. But the first thing I want to prove is that actually, even if we throw in k, it doesn't blow up the sort of size of coordinates we have to argue about by very much.
00:09:03.930 - 00:09:22.958, Speaker A: So k is sort of not much bigger than j. So the claim is that if we look at j and k together, so the corrupted coordinates plus these kind of corrupted by relationship coordinates, it's still at most delta n. Delta is the same delta as in the expansion condition.
00:09:23.054 - 00:09:23.700, Speaker B: Okay?
00:09:25.110 - 00:10:27.322, Speaker A: So that's the first thing I want to prove. So this will be the first, but not the last time that we use the expansion condition. So proof of claim. So suppose not we're going to proceed by contradiction, and the contradiction will be eventually to the expansion condition that g allegedly satisfies. So suppose in fact, this is bigger than delta n. I want to choose a subset that's exactly size delta n. The reason I want it that size is because I want the expansion condition to actually apply to this subset, and this only goes up to sets of size delta n.
00:10:27.322 - 00:11:18.562, Speaker A: Okay, so choose a bunch of coordinates. So, again, we're dealing with left hand side vertices here. So that on the one hand, basically what we do is we take the corrupted coordinates j, and we just supplement them by these corrupted by one hop coordinates up until the size of delta n. Okay? So j union k, and the size of s is exactly delta times n. Okay? So in this picture over here, I'm taking all of j and part of k. All right? So I want to show that this condition is violated, and that's my contradiction. So what does this condition say? Talks about the neighbors of s.
00:11:18.562 - 00:11:24.238, Speaker A: Okay, so basically I'm going to want to say that actually, if J union k is too big, then here's a set that doesn't expand.
00:11:24.334 - 00:11:24.818, Speaker B: All right?
00:11:24.904 - 00:11:51.690, Speaker A: So to talk about it not expanding, I need to count its neighbors. I need to say the number of distinct neighbors is not too big. So let's try to figure out how many neighbors does S have. Okay, and intuitively, if you look about the definition of k, k is defined as basically saying it already has a lot of redundant neighbors with the people already in J. So that's sort of where the contradiction is going to come from. There's just too many neighbors in common, given that this thing is an expander. But to really prove that, we need a short calculation.
00:11:51.690 - 00:12:31.138, Speaker A: So to count the neighbors of S, let's first count the neighbors of just the corrupted coordinates. And then let's count the extra neighbors, distinct neighbors. That the vertices that we picked from K. So the vertices in K intersect S contribute to the set. Okay, so this is going to be the neighbors of, let's see, S intersect K. So this is just the vertices of S that we haven't already counted in J. But again, we just want to count distinct neighbors.
00:12:31.138 - 00:12:34.680, Speaker A: So we're going to subtract back out the ones we already counted that the neighbors of J.
00:12:35.390 - 00:12:36.140, Speaker B: Okay.
00:12:38.830 - 00:13:06.626, Speaker A: So again, we're just breaking up the neighbors, babes, by those who are a neighbor of somebody in J and those who aren't a neighbor of somebody in J. So now again, we want to say this is not too big, so we want an upper bound. So to upper bound this first thing, we're just going to use it as deregular on the left hand side. Okay, so this contributes to most D times the number of corrupted coordinates. And of course, this we have control over. This is the number of errors, and we're assuming there's not too many. Okay, so that's fine.
00:13:06.626 - 00:13:45.854, Speaker A: We have an upper bound on that. And then the second step, we're just going to have an upper bound by the definition of k, the fact that k already has a lot of their neighbors already covered by J. Okay, so particular for each vertex, each coordinate of K, at least half of its neighbors are redundant with those we've already counted. So they're each contributing at most D over two to the number of neighbors, given that we've already counted J. So plus D over two times the cardinality of S intersects k. So any questions about that step? So again, this per vertex is trivial. This.
00:13:45.854 - 00:14:30.640, Speaker A: The pervertex contribution is by the definition of k. All right, so we know J can't be too big. We're assuming that there's at most delta n over two errors. The set overall has size delta n, so whatever is remaining is in here. So this is at least delta n over two. And for the purposes of an upper bound, since these are contributing less than this, the worst case for our upper bound is that basically this and this have the same, exactly the same size, delta N over two. That's as big as this could get, as if this is as large as possible and this is as small as possible.
00:14:30.640 - 00:15:23.342, Speaker A: So what if it were actually the case that this was delta N over two and this was delta N over two? Then it would just give us D delta N. So basically, you do this computation, the dust settles, and you get that this is at most three quarters D delta N. Okay? Delta N over two, delta N over two. You add these, you get 3D over two. So two times two of the denominator gives you the four. Okay? And now we're done because delta N is just equal to the size of S. So we've just counted up the neighbors and proved that it's strictly less than 75% of D times the size of S.
00:15:23.342 - 00:15:29.842, Speaker A: And that's the reason I left this expansion condition up here. So you can see that's an immediate contradiction. That's exactly what this asserts can't happen.
00:15:29.976 - 00:15:30.660, Speaker B: Okay.
00:15:35.990 - 00:16:18.510, Speaker A: It is not. I'm sort of toying with the idea of putting on the homework, asking you what is the minimum value for which everything in these proofs work? So it's a value which is not too far from the minimum possible for which the proof works, and subject to that, keeps the numbers nice throughout lecture. I'm looking out for you guys. All right, so that's sort of a preliminary step that we need. Okay. When we want to verify the feasibility condition, it's convenient to not just have a dichotomy between corrupted and uncorrupted coordinates, but in fact have a trichotomy, which includes this third case coordinates which are sort of not themselves corrupted, but share lots of stuff with these. Share lots of parity checks with corrupted variables.
00:16:18.590 - 00:16:19.026, Speaker B: Okay.
00:16:19.128 - 00:16:31.060, Speaker A: But it's not a big deal because this isn't too big. Okay, so good. So now I can just tell you the weights. Um.
00:16:32.790 - 00:16:48.906, Speaker C: The set k actually has an intuitive, intuitive meaning too, because if you were doing a local search method to try to reconstruct the code word right, these are the variables where they would say, hey, I should be flipped, right? Because half of my parity checks say.
00:16:48.928 - 00:17:17.090, Speaker A: I should be flipped, potentially. I mean, it sort of depends on exactly how many. It depends on the parity of the number of flipped variables that you share the parity check with. So that would be true if you were sort of the only one who was flipped or if an even number of other people were flipped. But absolutely. Somehow you'd think that if lots of your parity checks are shared with corrupted variables, you'd sort of expect most of your parity checks to be messed up intuitively. Really proving that is not obvious, but that's definitely good intuition.
00:17:17.090 - 00:17:55.280, Speaker A: Yeah. You'd sort of expect it to, right? So certainly if most of your parity checks, certainly the converse, if most of your parity checks do not have corrupted variables, then you're going to look locally quite good, for sure. And it's good intuition to think about the converse as just holding, even though that direction requires a proof and uses the expansion condition. Okay, so let me just define for you the weights, okay? And this is pretty slick. So this is actually not the first proof. This was sort of a slightly simpler version of the original proof by Feldman and Mall that came along a few years later. And yeah, it's slick, so let's just go through it.
00:17:55.280 - 00:18:13.874, Speaker A: All right, so here's the plan, right? So these are the three conditions we need. And the one which looks to me intuitively, I kind of have the least feel for is the third one. So what we're going to do is we're going to define the weight. So the third condition just obviously holds.
00:18:13.922 - 00:18:14.134, Speaker B: Okay?
00:18:14.172 - 00:18:48.770, Speaker A: That won't be the problem. And then we'll have to check that A and B hold. Okay? This third constraint is talking about the right hand side. Correspondingly, we'll define the weights J by J independently. So in some sense each parity check J will specify what the weights are for all of its edges with the variables that it includes. So here's how it works. So each parity check picks a favorite, favorite variable.
00:18:48.770 - 00:19:47.380, Speaker A: So I'm going to use the notation V of J for the variable chosen by J, it's got to be one of the variables in it. And there's going to be a nontrivial constraint, which is that if you're a corrupted coordinate or if you're one of these friends of corrupted coordinates in K, you have to be chosen by lots and lots of parity checks. That's going to be a requirement. So chosen at least three quarters D times. Now, of course, you can only be chosen by parity checks in which you participate, and you only participate in D parity checks, okay? So there's no way you're chosen by more than D people. And the insistence is that at least, at least for these coordinates in J and K, you better be chosen close to the maximum number of times. 75% of your parity checks better choose you.
00:19:47.380 - 00:20:17.126, Speaker A: Now, if you think about it, it's not obvious you can do that because a given parity check has lots of variables and actually maybe even a given parity check has lots of corrupted variables. I can only pick one of them, okay? And that could totally happen. So remember, we might have 1% of the things being errors, right? And a parity check has a constant number of variables, like 20. So maybe there's a million coordinates, 10,000 are corrupt, and a given parity check has 20 variables. For all we know, all 20 are corrupted.
00:20:17.238 - 00:20:17.562, Speaker B: Okay?
00:20:17.616 - 00:20:27.822, Speaker A: So it has to pick only one, and those other 19 will not be chosen by this parity check. So the hope of course, is then that well, then hopefully there's most of those variables, other parity checks can choose them.
00:20:27.956 - 00:20:28.446, Speaker B: Okay?
00:20:28.548 - 00:21:04.266, Speaker A: Now that wouldn't work if somehow those 19 variables showed up in all of these other parity checks together. But now, intuitively, if it's got this expander condition and everything's kind of totally scrambled all over the place, maybe that doesn't happen. Okay, that'd be the hope. Okay, that's true, and actually, it's not that hard to prove, it turns out, especially if you know Hall's theorem. So who knows Hall's theorem? Raise your hand, you should know Hall's theorem. Okay, so Hall's theorem, who knows max Flowman cut? Okay, so Hall's theorem is a corollary of max Flowman cut. Okay, actually quite easy, corollary it would be a great like 261 problem set question to just deduce Hall's theorem from max flow min cut.
00:21:04.266 - 00:21:41.062, Speaker A: But here's what it's about. But Hall's theorem came first. Actually, it was from the so it's about matchings. So let's just talk about perfect matchings for a second. So suppose you had a bipartite graph and you're wondering whether or not it has a perfect matching, okay? So to convince you that it does have a perfect matching, really easy, I'll show you the matching. So if you like, matching is in NP, but it's actually quite easy to put in co NP using Hall's theorem. I mean, we know it's in polynomial time, but forget about that for a second.
00:21:41.062 - 00:22:13.678, Speaker A: So certainly one way I could convince you that there isn't a perfect matching is if I showed you a constricting set. So suppose I showed you ten variables on the left hand side so that the number of distinct neighbors of these ten nodes was a set of only eight nodes. You should then be convinced that there will not be a perfect matching of this graph. In a perfect matching, any set of k variables has k mates. They're distinct. So if you don't have ten distinct neighbors of this set of ten variables, no way can you have a perfect matching.
00:22:13.774 - 00:22:14.370, Speaker B: Okay?
00:22:14.520 - 00:22:40.742, Speaker A: So it's clear that a necessary condition for a perfect matching is that every single subset of k vertices on the left hand side has at least k distinct neighbors on the right hand side. If you don't have that property, you certainly don't have a perfect matching. Hall's theorem asserts the converse. If it is the case that every single subset of k vertices on the left hand side has at least k distinct neighbors, then there does in fact exist a perfect matching.
00:22:40.886 - 00:22:41.626, Speaker B: Okay?
00:22:41.808 - 00:22:44.922, Speaker A: And so this again, you can deduce it just from max flow min cut.
00:22:45.056 - 00:22:45.466, Speaker B: All right?
00:22:45.488 - 00:23:41.840, Speaker A: So the flows give you the matchings and the min cuts. If you don't have a perfect matching, then the min cut of the graph will basically exhibit for you one of these constricted sets. So this I'll put on the homework. I'll just let you take Hall's theorem as a black box though again, you basically already know how to prove it. But using Hall's theorem, it is then quite straightforward to show that because the graph is an expander, which of course talks exactly about the number of distinct neighbors that any subset of the left hand side has, because the graph is an expander and the number of distinct neighbors is at least zero 75 D times the size of the set. You can find sort of a union of zero 75 times D matchings, which is the same thing as basically saying each node on the left hand side is chosen three quarters times D times. Okay, so it's not obvious, but if you think about Hall's theorem a little bit and you use the fact it's expander, it's just true.
00:23:41.840 - 00:23:55.700, Speaker A: Any questions about that? For the rest of the proof I'm just going to assume that we have such a okay, but again here again we are using the expansion property of the graph. Questions?
00:23:57.590 - 00:23:58.340, Speaker B: Okay.
00:24:00.650 - 00:24:41.218, Speaker A: All right, so every parity check picks a favorite variable and the constraint is that any corrupted variable or friend of corrupted variables, anybody in J and K is picked lots of times. Okay, so maybe think of D as like twelve, everybody gets picked nine times, everybody in J or K. All right, so what are the weights? So, homework, this is possible. All right, so here's how we define the weights. We define them differently depending on if the chosen variable is corrupted or not.
00:24:41.384 - 00:24:42.100, Speaker B: Okay?
00:24:42.710 - 00:25:45.010, Speaker A: So if the chosen variable is corrupted, which sort of makes sense if you think about it, right? Because I mean C is going to be we're just going to define these so that C holds, but we also are eventually going to have to worry about A and B. And it's really that the corrupted variables impose a pretty nasty restriction on these weights. So we basically have to have lots of negative weights next to a corrupted coordinate, but at the same time we have to have these non negative pairs. Okay? So that's why we're going to treat them differently. So for a chosen variable that is corrupted, we're going to have the corresponding edge have a negative value. So we set the weight of the edge between the favorite variable and this parity check to be equal to minus two over d minus epsilon, where epsilon is don't worry about epsilon, that's just like a tiebreaking epsilon. So epsilon is an arbitrarily small constant minus two over the degree d of the left hand side nodes.
00:25:45.090 - 00:25:45.382, Speaker B: Okay?
00:25:45.436 - 00:26:25.140, Speaker A: So the two is probably a little mysterious right now. The two is some slack that we need the reciprocal and D. We actually mentioned this very briefly last time when I tried to develop your intuition about the weights. So let me go through that discussion again so intuitively, because existence of the weights implies exactness of this linear program. We're sort of thinking that, we're thinking that the exhibiting feasible weights should get harder and harder as there are more and more errors as j is bigger and bigger. And remember if j was empty, so if you literally had no errors at all, then this is a trivial problem because you just set all the W's to be equal to zero.
00:26:25.750 - 00:26:26.500, Speaker B: Okay?
00:26:27.190 - 00:27:22.994, Speaker A: And then we also argue that, well, and at least it's a vaguely robust argument is that if j is still super small, if j is so small that no parity check has more than one corrupted variable, then it's also really easy to exhibit feasible weights. Basically just for each, basically what you do is just next to each corrupted variable, you just give all the edges the weight like minus one over d, so it has to have a total of minus one. You just spread it equally among the D adjacent edges. So that's minus one over D next to corrupted variables. And then to make sure you have this pairs condition next to uncorrupted variables, you just have a one over D on all of the D edges. And as long as no single parity check has two corrupted variables, then this condition is going to be satisfied. Of course that's not good enough for what we're trying to prove because again we might have a parity check which is entirely corrupted variables, right? So we're doing some more clever version of the argument, but we actually have seen this one over D before.
00:27:22.994 - 00:27:42.214, Speaker A: That's basically saying we're trying to get a negative contribute, we're not trying to get the minus one from just one edge, that would be crazy. We're trying to get this minus one contribution to the edges incident to a corrupted variable like roughly equally from the incident edges. Okay, so we can't have it exactly equally, but we can have it up to a factor two roughly equally from the incident vertices.
00:27:42.342 - 00:27:43.130, Speaker B: All right?
00:27:43.280 - 00:28:16.018, Speaker A: So that's why you shouldn't be too shocked to see this reciprocal and D show up. Okay? But if we want C to be satisfied, it's actually pretty obvious what the rest of the edges incident to j better have their weights defined as. So remember condition C says that any pair of edges with the same right hand side vertex have to have some of weights non negative. I just slapped down a negative weight so that lower bounds the weight of everything else incident to j to the negative of that to two over d minus epsilon.
00:28:16.194 - 00:28:16.920, Speaker B: Okay?
00:28:20.490 - 00:28:28.010, Speaker A: So set the other W I j's equal to two over d minus epsilon.
00:28:29.470 - 00:28:32.060, Speaker B: Okay? Clear?
00:28:47.670 - 00:29:23.850, Speaker A: Okay, so there means what to do for parity checks whose favorite vertex is not corrupted. So if the chosen vertex does not belong to j, then we just set Wij to be equal to zero for all incident edges. Okay, I want you to observe. So a subtle point is these friends of corrupted vertices, vertices in K also are in that second case.
00:29:24.000 - 00:29:24.698, Speaker B: Okay?
00:29:24.864 - 00:29:32.494, Speaker A: So if I'm a parity check J and I choose a variable which is not corrupted, even if it's a friend of corrupted variables, I still have everything be zero.
00:29:32.692 - 00:29:33.054, Speaker B: Okay?
00:29:33.092 - 00:29:44.770, Speaker A: So it's sort of important. So, again, these vertices and K are kind of walking a fine line between being treated as corrupted sometimes and uncorrupted as others. So those are the weights.
00:29:45.830 - 00:29:46.466, Speaker B: Is that clear?
00:29:46.488 - 00:29:55.750, Speaker A: Is the definition clear? Is it also obvious that condition C holds the third condition of the feasible weights?
00:30:00.170 - 00:30:05.594, Speaker C: Why can't you have all corrupted bits going to one parity check or like, 20 corrupted bits going one parity check?
00:30:05.632 - 00:30:08.186, Speaker A: You absolutely can. You absolutely can.
00:30:08.368 - 00:30:08.762, Speaker B: Okay.
00:30:08.816 - 00:30:09.130, Speaker A: Yeah.
00:30:09.200 - 00:30:11.382, Speaker C: Why is condition C satisfied?
00:30:11.446 - 00:31:06.560, Speaker A: Condition C is satisfied because we force each parity check to pick a single favorite variable, and the negative weight only gets to go between the parity check and its favorite variable. So if you have a parity check with 20 variables, all of which are corrupted, one of those corrupted variables for this parity check will pick up a negative. The rest will actually pick up a positive, which is kind of crazy because those corrupted variables have to have they only have a budget of minus one. So we're making backward progress as far as getting it down to minus one. But that's kind of punting the problem down a little bit, right? So eventually we have to verify B. And you should definitely be wondering, why does B hold even A? The factor of two can even make you nervous about property A, actually. But C, which was the one that maybe seemed the hardest to control, that's the one we just kind of said, well, let's make sure we do C and then suffer the consequences later.
00:31:06.560 - 00:31:13.232, Speaker A: Okay, everyone ready to verify A and B?
00:31:13.366 - 00:31:14.048, Speaker B: Yeah.
00:31:14.214 - 00:31:34.010, Speaker A: All right, let's do it. Three cases. So let's start with the corrupted case, which is sort of the one we're kind of worried about a little bit because the budget is so stringent. Minus one. All right, so in this case.
00:31:36.700 - 00:31:37.064, Speaker B: So.
00:31:37.102 - 00:32:02.124, Speaker A: This is what we need to compute. So we define the weights from the right hand side of the graph. But now to verify A and B, we have to think about things from the left hand side of the graph. So we fixed I, and we're thinking about all of its outgoing edges. So we have a budget of minus one here. So a question for you. So notice that the weights take on only three distinct values, okay? Zero, positive or negative.
00:32:02.124 - 00:32:26.260, Speaker A: And there's only one positive value. There's only one negative value. How many of those three values are candidates for a wij when J is a corrupted coordinate? So if I don't tell you anything else other than I, j is a node in the graph and I was corrupted. Anything else? Could it be zero?
00:32:26.630 - 00:32:27.506, Speaker C: No, it's only four.
00:32:27.528 - 00:32:30.082, Speaker B: If it's not elements of J, why.
00:32:30.136 - 00:32:54.502, Speaker A: Could it not be zero? So remember, just because you're corrupted doesn't mean that every parity tech chose you. It means that like nine out of twelve chose you, but there's also three that maybe didn't choose you. And if you weren't chosen, maybe someone who wasn't corrupted was chosen instead, in which case you're going to be zero, actually. So this could be any of the three values.
00:32:54.646 - 00:32:55.340, Speaker B: Okay?
00:32:56.510 - 00:33:12.490, Speaker A: Now what we do have going for us is that by virtue of being a corrupted coordinate in a parity check j, for which I is the favorite chosen variable v of J, then by definition, that edge weight is negative.
00:33:12.650 - 00:33:13.166, Speaker B: Okay?
00:33:13.268 - 00:33:23.358, Speaker A: So it's the first thing going for us if we're chosen that edge weight is negative. The second thing we have going for us is that we're chosen by most of our parity checks that's this condition.
00:33:23.534 - 00:33:24.466, Speaker B: All right?
00:33:24.648 - 00:33:57.440, Speaker A: So the worst case as for our upper bound, is that we're chosen the minimum number of times exactly three we pick up negatives, and that for the other one fourth D. Part of the time we actually get a positive contribution. We actually are like two over D from the remaining 25%. So that's the worst case that's as big as this sum could be. So this is less than three quarters D times minus or sorry, most.
00:33:59.170 - 00:33:59.486, Speaker B: Two.
00:33:59.508 - 00:34:12.398, Speaker A: Over D minus epsilon. So this is the contribution from parity checks j, for whom we're the favorite variable, plus D over four epsilon.
00:34:12.494 - 00:34:12.802, Speaker B: Okay?
00:34:12.856 - 00:34:20.994, Speaker A: And this is the worst case where we have a full 25% of the people who didn't choose us and they actually chose a different corrupted variable. So we actually got stuck with a positive value for those parity checks.
00:34:21.122 - 00:34:21.800, Speaker B: Okay?
00:34:23.530 - 00:34:37.980, Speaker A: All right, so also known as minus D over two epsilon, which is less than minus one.
00:34:38.990 - 00:34:39.740, Speaker B: Okay.
00:34:43.250 - 00:35:16.418, Speaker A: So that's case one. Any questions about that? All right, we've dealt with B. That's true, exactly right. So we get a big check mark here, big check mark there. Unfortunately, we still have two cases because we're going to have to treat these friends of corrupted variables vertices of K separately from vertices of I. Intuitively, we have control on the sum of the weights for those two different vertices, for two different types of vertices, for two different reasons.
00:35:16.594 - 00:35:22.250, Speaker C: Put the K in the same case as J, because we said that J and K are both chosen at least three, four, three times.
00:35:22.320 - 00:36:04.086, Speaker A: So let's go through the case of K. Let's see what's different. So, case two, consider a variable. Okay, so we're trying to prove A, okay, for all the remaining vertices, vertices of I, including those in K, we're trying to prove that the sum of the weights of the incident edges is less than one. All right, so consider someone in K. So let me ask you the same question I asked you before. If all I tell you is that we have an edge I J, and I tell you that the coordinate I belongs to k.
00:36:04.086 - 00:36:43.840, Speaker A: What are the possible values that this weight might have before? Could have been any of the three. Is it still any of the three can't be negative. Exactly right, can't be negative. Because if you're the chosen variable, if you're the chosen variable and you're not corrupted, and remember, if you're in k, you're not corrupted, you're just a friend of the corrupted variables. If you're the chosen variable and you're not corrupted, you're zero. So is everybody else incident to that same parity check. Okay, so that's why we can't inherit the argument for the corrupted nodes is because we don't have any of these negative contributions.
00:36:43.840 - 00:37:09.880, Speaker A: None. No negative contributions at all. Might have some zeros, might have some positive contributions. Good news is our budget is now one. Okay, but still, if we got a positive contribution two over D for all D of our parity checks, we'd be toast. That would give us two. On the other hand, why do we have control? So what do we want? We want a lot of zero contributions and not many positive contributions.
00:37:09.880 - 00:37:57.480, Speaker A: How do we have control over that? Why do we know there's many more zeros than there are positive ones? In this case it's just by the definition of k, which I guess I've erased. Okay, so k are those vertices for whom they share lots of parity checks with corrupted variables. Okay, so oh, no, that's not why. Excuse me. That's wrong. The reason we have control in this case is because our covering condition also applies to vertices of k. That's the key point at case two.
00:37:57.480 - 00:38:36.196, Speaker A: All right, so remember this is the key point when we had parity checks choose their favorite variables, we insisted that not merely the corrupted variables get chosen over and over and over again, but also these friends of corrupted variables get chosen over and over and over again. And remember, if you're chosen, the worst case is that you're a zero no matter who you are, okay? So the only way you're ever going to get a positive contribution is if somebody else was chosen and it was corrupted, okay? So if you're chosen, you're never going to get a positive number. You'll get either negative if you're corrupted or zero if you're not corrupted. So the fact that this vertex of k is chosen over and over and over again tells us that a lot of those wigas must be zero.
00:38:36.378 - 00:38:36.868, Speaker B: Okay?
00:38:36.954 - 00:38:55.740, Speaker A: Not positive, that's the point. Okay, so in particular, so this is bounded above by what's the worst case? The worst case is that it's chosen only 75% of the time by 75% of the parity checks, and that's a zero plus in the remaining 25%, it picks up the maximum possible contribution.
00:38:57.520 - 00:38:58.270, Speaker B: Okay.
00:39:01.840 - 00:39:31.856, Speaker A: And so this is just equal to d over d minus epsilon one half, which is less than one if I choose epsilon, but choose epsilon sufficiently small. Okay, so that's why we're fine in case two. All right, so case three. So case three is for vertices that are not only not corrupted, but have kept a safe distance from the corrupted variables.
00:39:31.968 - 00:39:32.630, Speaker B: Okay?
00:39:33.080 - 00:39:54.188, Speaker A: And now this is what I got confused and this is what I was saying for case two. So the reason we have control here, so again, for exactly the same reason, the contributions to this i, they're either going to be zero or they're going to be positive. We want to say there aren't too many positives, there have to be enough zeros. And here we're going to use the fact that, well, if you're not in K, that means most of your parity checks are totally clean.
00:39:54.354 - 00:39:54.780, Speaker B: Okay?
00:39:54.850 - 00:40:08.550, Speaker A: So most of your parity checks, half your parity, strictly more than half your parity checks, there doesn't even exist a corrupted variable to be chosen by that parity check. So it had to choose something uncorrupted. So you're definitely going to get a zero, okay? That's why you get lots of zeros in this case.
00:40:09.960 - 00:40:10.790, Speaker B: All right?
00:40:17.400 - 00:41:07.590, Speaker A: So formally sum of the weight contributions of all the parity checks you're part of is going to be a most. So if you looked at how I defined it, if 50% of your parity checks were not clean, then I put you in K, all right? So if you're not in K, you actually have strictly more than 50% of your parity checks have no corrupted variable. So that lets me get something bounded away from one half. Here again, the worst case is that you're in the fewest possible zeros and you have the maximum possible number of ones. Every one contributes two over D minus epsilon. And this, if you choose epsilon sufficiently small, you can check that at most one.
00:41:08.300 - 00:41:09.050, Speaker B: Okay.
00:41:21.920 - 00:42:02.850, Speaker A: So this is like D minus one over two, and that's like two over D, right? This can be strictly less than one. So that closes the whole loop, right? So that says using this expansion condition, we can always exhibit these feasible weights. And by the duality argument we had at the end of last lecture, that implies integrality of that linear program, which we know has to be the optimal coding. So that completes the proof. Any questions about that? That's LP decoding. All right, well in that case, we're finally done with this exact recovery. Stuff.
00:42:07.700 - 00:42:08.208, Speaker B: It.
00:42:08.294 - 00:42:41.256, Speaker A: And so we're going to move on to a different part of the course, which you already got an appetizer of when we talked about planted and semi random graph models. So the remainder of the course is going to be devoted to analysis frameworks which draw on aspects of average case or distributional analysis and also aspects of worst case analysis.
00:42:41.368 - 00:42:42.030, Speaker B: Okay?
00:42:42.660 - 00:43:41.372, Speaker A: So I often call this robust distributional analysis and there'll be many different interesting flavors of models that fall into this category. Okay, so today I just want to sort of tell you what are some of the high level features and motivations of these kinds of models and then also tell you a little bit more details about probably the most well studied such model smooths analysis. All right? So at a high level, the goal here is to try to define a way of analyzing algorithms where you get the best of both worlds, the average case analysis world and the worst case analysis world. So what's really good about average case analysis is you often get sort of very meaningful performance guarantees and you get very sharp predictions. And what's good about the worst case world is you get very robust performance guarantees. So if you can prove anything positive, it holds in a vast array of settings. So you'd like to be able to both prove something very strong, but also have it apply to many settings simultaneously.
00:43:41.372 - 00:45:04.084, Speaker A: Okay, so we're trying to get a sweet spot where we can have both of those at the same time. So I haven't used this notation in a while, but let me just remind you we were talking about this early on in the course. So in general, if you have a cost measure on an algorithm, which again, this could be running time, this could be the solution quality, this could be page faults, whatever, I use this notation for the cost of an algorithm a on an input z. So as we discussed, worst case means you look at the worst case overall input z, maybe parameterized by the input size, something like that. So just to be clear what I mean by average case, so one thing I want to say is average case is only defined with respect to a distribution over inputs, okay? So worst case, there's no distribution if you like, there's no real kind of model of any inputs being more or less likely than others. Average case does not make sense unless you specify a distribution D over the input z. And then the most standard thing you do is you look at the expected value on a random input drawn from the distribution D of the cost of an algorithm on a random input z.
00:45:04.084 - 00:45:35.860, Speaker A: Okay, you could certainly look at some statistic other than the expectation also if you wanted to. But for the discussion, I'm just going to focus on the expectation. But the point is, if you have a distribution, then you can talk about average case analysis. You could imagine trying to design an algorithm that makes this as small as possible. Okay, again, I want to emphasize that the algorithm that minimizes this quantity will in general be different for different distributions D. Okay? So again, you vary D. You vary, for example, the relative order of different algorithms for the expectation.
00:45:37.160 - 00:45:37.910, Speaker B: Okay?
00:45:41.180 - 00:46:27.680, Speaker A: Right. So I'm not going to talk much about pure average case analysis in this class. It will show up sometimes just as sort of a benchmark against which we compare our own solutions. I do want to just take a couple of minutes to point out that there are situations, I mean, average case analysis gets sort of a bad rap, I'd say, in the kind of theory of algorithms literature, but there's certainly situations where it's exactly what you want. And I even sort of think as time goes on, there's more and more situations where it's exactly what you want. So this is exactly the right analysis framework when, first of all, you have a very good understanding of distribution over inputs for the problem you're trying to solve. And secondly, you have the luxury of really being able to design a customized algorithm for this situation.
00:46:27.850 - 00:46:28.570, Speaker B: Okay?
00:46:28.940 - 00:46:38.900, Speaker A: So understanding the distribution, that generally boils down to, first of all, having reams of past data, and second of all, an assumption about low volatility.
00:46:39.060 - 00:46:39.576, Speaker B: Okay?
00:46:39.678 - 00:47:47.532, Speaker A: So like the stock market, we have reams of data, but I'm not sure there's anybody who feels that confident about their understanding of the distribution over what the stock market is going to do. But there are contexts, lots of contexts, where you sort of, sort of know the distribution tomorrow is going to be like yesterday, or at least the distribution tomorrow will be like it was on a Thursday last week, something like that. Okay? And if you think about it, in the age of big data, I think there's more and more cases where this actually happens. A lot of the big companies where some of you might go on to work after graduation, you might well be tasked with coming up with some optimization problem for which you have millions or billions of data points with low volatility. And in that case, by all means design the algorithm which minimizes the expected cost or some other statistic with respect to this well understood distribution. In this class, we're going to be motivated more by situations where there's still some uncertainty. So maybe you believe there really is a distribution and you're just not so sure about what it is just because maybe your data isn't very rich, or maybe just it's changing so fast you're just not confident about optimizing overly with respect to a distribution.
00:47:47.532 - 00:48:19.290, Speaker A: D. Or it may be that we're more interested in understanding the performance of a general purpose algorithm, like, say, the simplex method for linear programming, rather than coming up with some new algorithm specifically tailored to some distribution. Those are both sort of kind of domains we're thinking about. So there's good things about average case analysis. The main issues are, I'd say uncertainty about D. So maybe D exists and you don't know.
00:48:21.690 - 00:48:22.440, Speaker B: It.
00:48:25.050 - 00:49:27.514, Speaker A: And there's also an issue with overfitting, which is even if you know D today and then you optimize, you get every last cent out of your algorithm with respect to today's distribution. It could be you lose some robustness. It could be that if things are different next week because you've tailored your algorithm so carefully to how things are today, it doesn't respond very well to how things are next week. So those are two reasons why you might want to add some robustness to just the high level idea of minimizing this expectation. Okay, so how would one do that? So most of the remaining models I'm going to talk about can be cast as a hybrid model again, I mean hybrid between worst and average case analysis here of the following form. So let's see, be a set of distributions over inputs. So I realize this is a little abstract at the moment.
00:49:27.514 - 00:50:01.698, Speaker A: I'll talk about how to interpret the set C in a second. The way to think about it is you believe there really is a distribution over inputs. You're not sure what it is. You suspect it lies somewhere in this set C, but maybe C includes a bunch of Gaussians, a bunch of uniform distributions, various parameters for those distributions, et cetera. Okay, and you'd rather not have to, you don't understand enough about the application to commit to something as specific as like a parametric form for the distribution over inputs, but you like an algorithm that would work well, kind of no matter what reality is. So you believe reality is well described by distribution. You're not sure which.
00:50:01.698 - 00:50:41.026, Speaker A: Can you have something which simultaneously works well across all of these possible realities? So rather than taking a worst case over inputs, we're going to take a worst case over distributions. So I'm going to hedge my bet over which of these distributions actually governs the data. And then for a given distribution of our inputs, I want the expected cost, or again some other statistic if you prefer to be as small as possible.
00:50:41.208 - 00:50:41.940, Speaker B: Okay.
00:50:47.190 - 00:51:56.860, Speaker A: So a number of the things we're going to do next can be interpreted as instantiations of this idea. For the moment, I want you to think about this set C of distributions as exporting, a knob that you can turn to interpolate between the average case world and the worst case world. If the set C is a singleton, right? So if the set C has just one distribution in it, then the max is vacuous, and so it just reduces to average case analysis. Okay, so one thing in C is average case analysis if C is super big. So if in particular it includes every possible point mass on a given input, then for the point masses, this disappears and this just becomes some fixed input and you're taking the max over all possible inputs and then you recover the worst case model. Okay, so the smaller C is, and the more randomness there is in the various points of C, the closer you are to average case analysis, the richer C is and the more deterministic these distributions can be, the closer you are to worst case analysis. And we're looking for a sweet spot in between the two.
00:51:56.860 - 00:52:55.878, Speaker A: This is genius. Okay, so why would you do this? Or why do people do this? So like with a lot of the other high level ideas we've talked about, there's a number of motivations and they don't all apply in all situations. For each of these motivations, it sometimes applies. So the first thing which sort of works really well for in all of the applications is you avoid pathological inputs. And in particular in smooth analysis, this is really kind of the number one reason people study it. So we talked at length about the perils of worst case analysis, especially for example in online paging. And we talked about how basically you get overly pessimistic performance predictions and you get meaningless or even incorrect in some sense, comparisons between different algorithms because the performance is governed by these potentially very unrealistic pathological inputs.
00:52:55.878 - 00:53:49.120, Speaker A: So having a distribution allows you to avoid those. If the pathological inputs are sort of very sparse and you have any kind of diffuse distribution, they're not going to play an important role in your algorithm design or analysis. So a reason which may or may not be the goal is to model, quote unquote real data, but that's still uncertain. Okay, so maybe you have in mind that data is somehow sufficiently random, and we'll have an application in hashing in a few lectures where this is very explicit and you don't want to actually commit to some very precise definition of what's being inserted into your hash table, but you at least know that there's sufficient randomness in the data. You believe that about real world data. So it's sort of a nice way to articulate qualitative properties that real data has without getting too specific.
00:53:50.290 - 00:53:51.040, Speaker B: Okay.
00:53:54.290 - 00:54:47.098, Speaker A: So sort of a consequence of this first point is there's plenty of situations where there are algorithms for which neither worst case analysis nor average case analysis advocates for them strongly. Yet empirically they seem to be really good general purpose solutions. And so this analysis framework seems to capture some of the reasons why that's true. It's not good in the worst case, it's not the literal best algorithm for any one distribution, but it almost always works really well. And again, simplex method for linear programming being a canonical example. And the fourth one, which we won't see much of for a couple lectures or for a couple of weeks. But just like with parameterizations, when we first started talking about Parameterizations, the goal was merely to analyze existing algorithms like the LRU algorithm for paging.
00:54:47.098 - 00:55:44.580, Speaker A: But then at some point, once we had some novel parameterizations, we just couldn't resist trying to design some new algorithms that did even better with respect to these parameters, like maximum weight independence sets. We had this very cool algorithm where the variant of the greedy algorithm where you pick sort of more nodes than before and then compute a maximum independence in the bipartite graph and that we were driven to that with a novel parameterization. So in the same way, whenever you have a new way to analyze algorithms, it's natural to ask, does it naturally guide you to new algorithmic ideas that seem useful? But the next several lectures, this really won't be the point, really. For the next several lectures, just like with parameterized analysis, I want to start with applications where it's used only in the analysis. Then we'll talk a little bit about how it might guide design. Good. All right, so that's kind of very high level.
00:55:44.580 - 00:56:09.020, Speaker A: Let me try to help make this a little more concrete for you. So first of all, let me point out that we've already seen something which can sort of be thought of as a special case of this. It's sort of a slightly degenerate version because of the symmetry involved. But you can sort of think of planet graph models in this way.
00:56:09.550 - 00:56:10.298, Speaker B: Okay.
00:56:10.464 - 00:56:42.194, Speaker A: Think, for example, about planted clique where somebody picks K nodes to plan a clique. And then you fill in the rest of the edges with probably 50% each. That's like having one distribution D for each choice of where you put that initial KC leak. Once you decide on the kleak. Now you just have this distribution over graphs. And there, in effect, our cost measure was just zero or one, depending on if the algorithm was correct or incorrect. And we just wanted to be correct with probability close to one, no matter where the clique was planted.
00:56:42.194 - 00:57:26.850, Speaker A: That is for every single distribution D in that class. Okay, so that's one example, but really a full blown example is smooth analysis. And really, of all the alternatives to worst case analysis that have been proposed, this one I think is the most well known and most extensively studied. That says there's still a lot of stuff we don't know about it, but more work on this probably than any of the other models we're talking about in this class. And I'll talk about this probably for the next three lectures or so, different applications of it. So how does it work? Well, it's a nice idea. It's one of these hybrid models.
00:57:26.850 - 00:58:07.322, Speaker A: So an adversary picks an input. So if you like a linear program, possibly a linear program on which the Simplex method runs in exponential time. So pick some worst case input, but then nature slightly perturbs the input. Okay, so perturbation is going to mean different things in different cases. You could think of it as like adding a very small gaussian or some other kind of random variable that's not too big relative to the magnitude of the values in the input.
00:58:07.466 - 00:58:08.160, Speaker B: Okay.
00:58:10.850 - 00:58:37.934, Speaker A: All right, so there's a couple of nice things. So really the main motivation for smooth analysis is this first point, okay? So for algorithms that just like Simplex, which have this very sparse set of very bad inputs, it's going to allow us to get rid of them in effect. But it also has a nice interpretation as far as modeling real data. Whenever you have a computational problem where the data is being supplied by some kind of measurement, then it's natural to think of as what you really see is some measured value of ground truth.
00:58:38.002 - 00:58:38.234, Speaker B: Okay?
00:58:38.272 - 00:59:00.610, Speaker A: So it's some noisy version. Maybe the noise is even very small. But if nothing else, there were like rounding errors in the floating point arithmetic. And it's not clear that you see the exact ground truth, you see some slightly noisy version of it. And so this says that if ever you have input that has this small noise in it, that actually is already enough to justify the fast performance of certain algorithms.
00:59:02.470 - 00:59:02.834, Speaker B: Okay?
00:59:02.872 - 00:59:23.394, Speaker A: So let me also just compare it briefly to semirandom models. So semirandom models don't quite fit in here because the order is some sense reversed. So in a semi random model, nature goes first, it picks like a planet clique instance, and then an adversary gets to go second. And if it wants, it can, for example, remove edges that aren't part of the clique.
00:59:23.522 - 00:59:23.814, Speaker B: Okay?
00:59:23.852 - 01:01:01.158, Speaker A: So that's like a reversal of these order. All right, so they're both totally reasonable, nice models, but I don't know how to unify those two because of the difference in orders. Okay, so what is the goal in smooth analysis? Um, I guess, let me first say, you know, when, when should you, for what kinds of problems should you consider using smooth analysis? So when does it work? When useful? Um, so the main thing is that the bad inputs so whatever cost model you're using and whatever computational problem you're thinking about, and whatever algorithm you're thinking about, the bad inputs for that algorithm should be very fragile, okay? In the sense that they really seem very delicate, they're usually extremely hard to construct. The clay minty cube example, the bad example for simplex took decades after the invention of the simplex method. And you just look at it, I'm not going to go actually prove it in class, but if you go read about the clay minty cube, you look at it, you're like, that's a real knife edge example. It's just kind of obvious, right? And all of the killer apps of food analysis really are for problems where it's just clear that the pathological examples are these knife edge examples, okay? So for that exact same reason, usually it's about running time analysis. So, again, remember, this is an abstract cost measure, okay? In our different applications, this has meant many, many different things.
01:01:01.158 - 01:01:04.698, Speaker A: Sometimes it means running time for smooth analysis, it's pretty much always going to.
01:01:04.704 - 01:01:06.300, Speaker B: Mean running time, okay?
01:01:07.390 - 01:02:03.594, Speaker A: So usually for runtime analysis and the reason is that for other cost measures, the bad examples actually aren't that fragile and you can't really get rid of them with perturbations. Like if you think about the greedy algorithm for maximum weight independent set or something like that, if you perturb the vertex weights by epsilon, it's really not going to fundamentally change the performance of that algorithm. That heuristic, okay, so there's a. Bad input for the original one, a slightly perturbed version is going to be an almost bad an input for the same algorithm. Okay, so it just turns out that you seem to only have this fragility of inputs for specific cost measures. So in some sense, smooth analysis isn't always an interesting framework to apply, but for the types of problems that are in its crosshairs, it does really well. All right, so the goal then.
01:02:03.594 - 01:02:42.658, Speaker A: So now just think about Runtime. The question is, when can you have algorithms? When can you prove that an algorithm has polynomial smoothed complexity? So this is going to be the supremum, right? So the adversary picks the input first. So we take a max or a soup over input z, and then we take an expectation over perturbations, which I'm going to note by R of sigma for now, sigma here is the size of the perturbation. So you could think of this if it's a Gaussian, you can think of this as the standard deviation of the Gaussian.
01:02:42.754 - 01:02:43.014, Speaker B: Okay?
01:02:43.052 - 01:03:48.090, Speaker A: So if sigma is small, you're adding a small perturbation. If sigma is big, you're adding a big perturbation, a lot of noise. And then in here you're looking at the cost, which we're going to think about running time of A on z plus the perturbation. So you want to prove that this quantity is polynomial in the input size n and in one over sigma. Okay, so again, with smooth analysis, usually we'll have one exception, but usually it's more like you're more having the perspective of we know this algorithm works well, let's explain why. So the algorithm is fixed and you're just trying to understand its performance. Okay, so we have an algorithm, a like simplex, and we know Simplex's exponential time in the worst case, which means we know that as the perturbation approaches just being deterministically zero, this running time has to blow up.
01:03:48.240 - 01:03:48.554, Speaker B: Okay?
01:03:48.592 - 01:04:01.470, Speaker A: So we know we need some dependence on the size of the perturbation sigma, because if sigma is zero, it is exponential time. So it has to be polynomial n and some function of sigma. And it turns out the right thing to try to be polynomial n is one over sigma.
01:04:01.630 - 01:04:02.340, Speaker B: Okay?
01:04:04.390 - 01:04:25.798, Speaker A: All right. And so I hope it's clear I wrote these next to each other for a reason. Clearly this is an instance of a hybrid model. So here for each worst case input, so for each choice of z, you have a separate distribution D. The corresponding distribution is just the slightly fuzzy version of inputs around the center point z.
01:04:25.964 - 01:04:26.450, Speaker B: Okay?
01:04:26.540 - 01:04:33.050, Speaker A: So again, that's your class. Class is like slightly fuzzed out point masses and you want to do as well as possible on every single one.
01:04:33.200 - 01:04:33.900, Speaker B: Okay?
01:04:34.830 - 01:04:35.980, Speaker A: All right, good.
01:04:40.930 - 01:04:41.760, Speaker B: All right.
01:04:44.210 - 01:05:11.400, Speaker A: So I want to conclude the lecture with a discussion of sort of the origin story and still kind of the main the biggest killer app of smooth analysis, which is to the simplex method. And so really smooth analysis was invented for this purpose. So this is by Spielman and Tang in one.
01:05:14.190 - 01:05:14.940, Speaker B: Okay.
01:05:19.790 - 01:05:36.426, Speaker A: It and I'm just going to talk about what they did at a high level. So what is their result? So what do they prove? Okay, so they prove simplex. Despite having worst case exponential running time in this sense, it does have polynomial smooth complexity.
01:05:36.538 - 01:05:37.200, Speaker B: Okay.
01:05:46.150 - 01:06:49.522, Speaker A: So they were thinking about linear programs. So think like minimize c, transpose x subject to ax equal B, something like this. So the formal theorem statement says consider an arbitrary linear program. So you get to pick CA and B arbitrarily for some of the most of the applications we'll talk about are not that sensitive to the perturbation distribution. This actually does seem, or at least no one knows how to prove it without a very specific perturbation distribution. The good news is the perturbation distribution is Gaussians, which is sort of the best justified distribution if you're only going to use one. So their perturbation model, so their notion of R, of Sigma, they just add, IID Gaussians to every single entry of the constraint matrix A, to every single entry of the right hand side B, with standard deviation sigma and they prove that a particular variant of the simplex method, which I'll explain in a second, indeed, in this sense, has running time polynomial and n and one over sigma.
01:06:49.522 - 01:07:14.702, Speaker A: It's a really big polynomial. So in that sense, it's not a super accurate description of the empirical performance, which is usually thought to be roughly linear. In n, it's a much, much bigger polynomial than linear, but it is polynomial, which is nice. Okay, so any questions about kind of the statement? Is that a question? Yeah, so I don't know whether does.
01:07:14.756 - 01:07:23.998, Speaker B: This hold for uniform distribution with max bound? I mean, that seems like the other construction that might possibly work, sort of.
01:07:24.004 - 01:07:45.910, Speaker A: Black box most of theorems that we'll talk about. Yes. As far as I know, there is not a proof for this one. Yeah, I think everyone believes that's true. It's just sort of the analytical hurdles are very high. Yeah. Already the original proof of this was dozens and dozens of pages for Gaussians.
01:07:45.910 - 01:08:17.090, Speaker A: Okay. So needless to say, we're not going to be covering it in lecture in any detail, but I wanted to say a little bit about it because it is kind of the calling card for smooth analysis. All right, let me tell you a little bit about it. So first recall what the simplex method is. Geometrically, right? So geometrically, I've drawn this cartoon a number of times already. But so linear programs, you're maximizing a linear function over an intersection of half spaces. So in two space, it's just a polygon.
01:08:17.090 - 01:08:47.770, Speaker A: And again, remember, a linear function is just like a direction. So you're trying to find that vertex if you're trying to maximize that direction. Okay, now the simplex method, geometrically is very intuitive. You just walk along edges of the feasible region. So you have it in higher dimensions. It's called a polytope, but you still have these edges. So you just walk corner to corner along an edge, all right? And you always walk along some direction where the objective function gets better.
01:08:47.770 - 01:09:59.118, Speaker A: So it's almost like a local search algorithm, except by linearity. It turns out all the local maxima are also global maxima, okay? So you just keep getting better and better and better and then eventually you stop when no edge makes you even better than where you are now and you're optimal. So correctness isn't really an issue with simplex, it's more how fast does it happen? To be clear, you look in two dimensions and you get a little bit fooled, right, because you're like, how hard could this be? In the worst case, I walk around this polygon, big deal, right? And in two dimensions, if you have like m half spaces, you're going to have like m sides to this polygon, so big deal, right? But now think about like going to higher dimensions, okay? And just think about something really simple. Think about just like a hypercube in N dimensions, right? Now all of a sudden there's two to the N vertices, okay? So if you literally visited every single vertex of your polytope, you'd be in trouble. Then the running time really would be exponential in N. And actually the Clay minty example is sort of a Tweaked version of a hypercube. So they actually show that there's basically two of the N vertices and simplex actually can visit every single vertex.
01:09:59.118 - 01:10:05.846, Speaker A: So not only the number of vertices can grow exponentially in the dimensions, but in fact in pathological examples, simplex visits them.
01:10:05.868 - 01:10:07.080, Speaker B: All, right?
01:10:08.010 - 01:10:57.590, Speaker A: Okay, anyways, geometrically that's what simplex is doing. But like with any kind of local search style algorithm, this algorithm is kind of underdefined, right? Because when you're at one of these vertices and you have these like 100 different edges, maybe 73 of them are going to make you worse. So clearly you're not going to take any of those. But then there are these 27, all of which would lead to an improvement in your objective function. Now, again, in 2D, you don't get an appreciation for this, right? It's kind of like there aren't that many ways you can go, but in high dimensions, there can be lots different directions you can go. And you have to decide, if there's 27 improving edges, which one do you follow? So the choice of which edge to follow out of many improving ones is known in this context as a pivot rule. So you have different flavors of the simplex method depending on how you instantiate the pivot rule.
01:10:57.590 - 01:11:39.220, Speaker A: So the result of spielman and tang is for one particular choice of a pivot rule. Again, as far as I know, there is no smooth complexity result known for any other choice of a Pivot rule, not because people think it's false. Just again, it has resisted attempts of analysis. This is sort of hard enough. Okay, so let me tell you a little bit about the Pivot rule. The Pivot rule is called the shadow Pivot rule, which I have to say, from a practical perspective, if you only had an analysis of one Pivot rule, you probably wouldn't choose this one. But it's cool, it's cool enough.
01:11:39.220 - 01:12:03.046, Speaker A: So here's what you do. So we observe that simplex is pretty easy in two dimensions, right? So the idea is just to project your high dimensional polytope down into two dimensions, all right? So you have this high dimensional like an N space. And imagine you sort of like hang a light source from above it and then you look at the shadow that it casts on the ground.
01:12:03.238 - 01:12:03.834, Speaker B: Okay?
01:12:03.952 - 01:12:49.002, Speaker A: So that's a picture that looks something like that, okay, which I'll pass around. So when you hang this light bulb behind the polytope and you look at its shadow, some of the vertices will still show up in the shadow, but some will disappear, some will get swallowed up into the interior. Okay, so there's no new corners in the shadow. Every corner in the shadow is a corner in the original polytope, but you will lose some of the corners of the original polytope. Okay, so the idea then is like, okay, well, why not just project it down into the plane and then run simplex in the plane? Because simplex in the plane we talked about, if you only have m half spaces, you're going to have m edges. So you're just going to zip through there in linear time. Okay, well, the problem is this doesn't quite work.
01:12:49.002 - 01:13:18.942, Speaker A: The reason this doesn't quite work is it's true that if the constraints are originally specified in the plane, you're only going to have one side per constraint, but if you have just a small number of constraints in high dimensions like the hypercube. And then you hang the light bulb behind it from the wrong angle, you'll actually see an exponential number of corners down in the plane. So basically you'll still have most of the exponentially many corners in high dimensional space. You'll still have that in the projection.
01:13:19.086 - 01:13:19.586, Speaker B: Okay?
01:13:19.688 - 01:13:58.666, Speaker A: So just because you reduced it to two dimensions doesn't mean you suddenly get a free lunch and can just zip around. I mean, it will be linear time in the number of facets down in the plane, but that could be exponentially large. So technically then, here's sort of the hard theorem that Spielman and Tang prove. They prove that the expected number of vertices, or equivalently in two dimensions, number of number of sides of the shadow is polynomial in n and one over sigma.
01:13:58.858 - 01:13:59.600, Speaker B: Okay?
01:14:01.490 - 01:15:23.450, Speaker A: And so actually the way they prove this and the way a lot of these smooth analyses work actually is sort of a nice instantiation of something I told you a long time ago we were talking about parameterizing algorithms. One of the many reasons I gave you of why you might want to parameterize the running time of an algorithm is it sort of suggests an approach to explaining good empirical performance, right? So first you say, oh, well, for certain values of this parameter, this algorithm runs fast. And now real world data in some sense has nice values for this parameter, okay? And this works exactly in this way. So if you're in two dimensions and you have a polygon and you're just trying to figure out how many facets there are, suppose I told you something about the angles between two consecutive sides, okay? So that angle is going to be less than pi. And as you traverse this polygon in total, it's going to be like a two pi rotation, okay? So if I told you that each one of these angles was bounded away from pi, so it was a less than pi, but it was even like at most pi minus delta. Then after two pi over delta turns, you'd have made the full circle.
01:15:24.030 - 01:15:24.874, Speaker B: Okay?
01:15:25.072 - 01:15:58.950, Speaker A: So this minimum angle between one of these consecutive sides and pi can serve as a sort of condition number, and it's straightforward to analyze the number of iterations in terms of that gap. So what they actually prove is they say consider the biggest delta such that all angles of the shadow are at most pi minus delta.
01:16:00.090 - 01:16:00.840, Speaker B: Okay?
01:16:01.770 - 01:16:14.570, Speaker A: Then what they actually show is that the expected value of pi is now bounded below by something that's at least inverse polynomial in n and one over sigma.
01:16:15.950 - 01:16:16.650, Speaker B: Okay?
01:16:16.800 - 01:16:24.390, Speaker A: So the number of turns you'll take walking around the polytope is most two pi over this. So that's the most polynomial in n and one over sigma.
01:16:24.550 - 01:16:25.258, Speaker B: Okay?
01:16:25.424 - 01:16:52.240, Speaker A: So that's sort of the gist. And we'll see this same template reoccur for other applications, which are both pretty cool, but also that I can actually teach you fully in lecture where it'll have the same spirit. We'll say, okay, the running time of, say, local search is bounded by a polynomial on these various parameters. And as long as there's a small perturbation, these parameters themselves are polynomially bounded, and that'll give us the final result. Okay, so we'll start that in earnest on Monday. See you then.
