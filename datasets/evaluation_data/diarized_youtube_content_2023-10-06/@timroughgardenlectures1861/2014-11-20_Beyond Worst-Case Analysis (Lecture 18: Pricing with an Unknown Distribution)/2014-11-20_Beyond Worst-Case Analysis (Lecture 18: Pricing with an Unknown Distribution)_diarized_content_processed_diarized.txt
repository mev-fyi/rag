00:00:00.410 - 00:00:00.814, Speaker A: Hello.
00:00:00.932 - 00:00:42.518, Speaker B: All right, so the plan for this lecture is to continue the theme that we introduced, last lecture with self improving algorithms. That theme being we're going to sort of make a stronger distributional assumption than we had been in smooth analysis and when talking about hashing. So we'll really think of inputs as being drawn IID from some distribution, and we'll allow algorithms to react, even evolve. That's sort of even what we want. We want them to evolve so that they're as good as possible with respect to that distribution, seeing some number of IID samples. So last time we talked about sorting, and so today I want to talk about mostly just a very simple pricing problem, but then also a little bit at the end about auctions. But the goal is going to be very similar.
00:00:42.518 - 00:01:27.734, Speaker B: Given IID samples from an unknown distribution, you want an algorithm or an auction that converges quickly to how well you could have done had you known that distribution over inputs up front. Okay, so here's a pricing problem. It's about the simplest one you could imagine, but it still turns out to be pretty interesting. So you want to sell something because you've got a good or you've got a service, and there's a potential buyer that you've identified. You don't know what the buyer is willing to pay for that good or service. So there's a buyer with an unknown willingness to pay or value V. You don't know what V is.
00:01:27.734 - 00:02:04.900, Speaker B: If you knew what V was, the problem would be easy. So what you want to do is you want to set a price P, and this is going to be a take it or leave it offer. The buyer can either take it and pay you P or not take it and walk away. So if the price is bigger than what they're willing to pay, the buyer is going to walk away. There'll be no sale, and you make zero revenue. On the other hand, if the price is right, then there's a sale at price P. Okay? So the seller makes P bucks in that case.
00:02:04.900 - 00:02:44.660, Speaker B: And we want to reason about how the seller might want to set this price P. There's this intuitive trade off. The higher you set the price, if it turns out there's a sale, then the happier you are, the more revenue you get. But in some vague sense, it seems less likely there'll be a sale the higher you set the price. There's no distribution on the board yet, so it's not clear what I mean by less likely. But that's sort of the intuition. So that's what's pushing in either direction and setting the price higher or lower and without any further assumptions, frankly, given that V is just some totally unknown number, it's not even clear how to sort of reason about this, how to prove theorems about which P's are good and which P's are bad.
00:02:44.660 - 00:02:56.950, Speaker B: So we're going to like last lecture. Think of V, the willingness to pay as coming from a distribution.
00:02:57.370 - 00:02:58.120, Speaker A: Okay?
00:03:02.890 - 00:04:15.630, Speaker B: So distribution capital F. So F here denotes the cumulative distribution function. So remember, that means capital F of X denotes the probability that the random variable is at most x, that's capital F of X. And so for the moment, as a thought experiment, this won't be our main model, but for the moment, as a thought experiments, imagine the seller actually knew the distribution capital F. Okay? So to be clear, the value v, the realization remains unknown, but the distribution capital F from which that value is drawn for the moment we think of as known. Well, then it's a pretty easy problem to think about, actually. So just think of that if you just sort of set some price p, how much money would you make? Well, whenever there's a sale, you make P bucks, and then that's just gets multiplied times the probability that there's a sale, which is exactly when the value V is at least p.
00:04:15.630 - 00:04:20.634, Speaker B: So it's exactly the complementary event of this one. So it's one minus capital F evaluated.
00:04:20.682 - 00:04:21.440, Speaker A: At P.
00:04:24.450 - 00:04:28.734, Speaker B: So this is an expression we're going to see many times with different notation throughout the lecture.
00:04:28.782 - 00:04:28.946, Speaker A: Okay?
00:04:28.968 - 00:04:38.438, Speaker B: So this is always important to remember for this single buyer problem, there's two terms, the money you make on a sale, the probability of a sale. You want this to be high, you want this to be high, but they trade off against each other.
00:04:38.524 - 00:04:39.160, Speaker A: Okay.
00:04:41.850 - 00:05:11.440, Speaker B: Oops, sorry about that. Revenue per sale, fraction of the time you get a sale. So if you were the seller, you just said P to maximize this kind of a no brainer. So just to wake you up, why don't you work through an example over the next 30 or 60 seconds or whatever. So suppose the distribution was uniform on zero one.
00:05:11.890 - 00:05:12.698, Speaker A: Okay?
00:05:12.884 - 00:05:50.192, Speaker B: So I E, capital F of X is just the identity function on zero one. Why don't you think about what you would set P to in that case? Yeah, that's a good thing to set it to. So how would you solve this problem? Well, you just instantiate this. I told you what capital F is. It's the identity function. So the expected revenue function becomes P minus P squared. Okay, so that's a concave function.
00:05:50.192 - 00:06:09.060, Speaker B: The second derivative is negative, the second derivative is minus two. So all you have to do is check the so called first order condition. All you have to do is set the derivative to zero and solve, and you're done. Okay, so you just set P minus P squared equal to zero and solve. You get P equal one half. So what's the expected revenue? The maximum possible expected revenue?
00:06:09.500 - 00:06:10.548, Speaker A: One quarter.
00:06:10.724 - 00:06:45.990, Speaker B: Quarter, right. So you're going to sell half the time, and when you sell, it'll be a 0.5. So I want to wake up your calculus brain cells, which have probably been long dormant. So let me give you another example. What if it's the exponential distribution? So one minus e to the minus X on the real line. So this you might not be able to do in your head. Is it one? Yes, it's one.
00:06:45.990 - 00:06:57.344, Speaker B: Yeah. So a little terminology. So the best P is often called the monopoly price because it's sort of like you're the only you sort of have monopoly on this bidder.
00:06:57.392 - 00:06:57.556, Speaker A: Okay?
00:06:57.578 - 00:07:23.490, Speaker B: So given that you have no competition, this is how you should extract maximum revenue. So we'll sometimes call it the monopoly price. I'll sometimes use the notation P star. So for uniform, P star was one half. For the exponential distribution, p star is one. What is the expected revenue at the monopoly price? One over e. One over e.
00:07:23.490 - 00:07:24.956, Speaker B: That's.
00:07:24.988 - 00:07:25.570, Speaker A: Right.
00:07:29.380 - 00:07:48.204, Speaker B: Because the selling probability at the price one is exactly one over e. And you make a dollar whenever you sell. Okay, good. But the high level point here is if you know the distribution, this is sort of a solved problem. You just calculate some derivatives or fire up mathematica, whatever you want to do, and you just set P appropriately.
00:07:48.272 - 00:07:48.792, Speaker A: Okay.
00:07:48.926 - 00:08:03.980, Speaker B: And you get reasonable answers. So that's not the model. This is more kind of the Holy Grail to which we want to aspire, okay. Because actually, as the algorithm designer, our lives are going to make our lives a little harder. Namely, F is going to be unknown.
00:08:05.280 - 00:08:05.980, Speaker A: Okay.
00:08:06.130 - 00:08:41.252, Speaker B: So it's still a little different than it was at the beginning because we are going to assume that such an F exists. Okay. And we're going to have the opportunity to observe multiple samples and therefore learn something about capital F. So we still have some nontrivial assumption there. So just like last lecture, we saw a number of sorting inputs before we had to try to converge to what would be the information theoretic sorting algorithm. Here we're going to have M again, training inputs or samples. And these are just IID draws.
00:08:41.252 - 00:09:28.200, Speaker B: Let's call them V one through VM from this underlying distribution, capital F. So I repeat, you do not know capital F. You have partial information about capital F in the form of these MIID samples. So we have sort of a parameterized, we have sort of a measure of how much information we have about the distribution in the form of the number of IID samples that you've been given. So what's the goal? Well, the goal is the same thing it was last lecture so quickly. Meaning after hopefully not too many samples, we want to be able to solve the problem I e define a price so that our performance I e our expected revenue is just as good as it would have been had we known capital F upfront. That is, we want to do as well as the monopoly price in this formula.
00:09:28.200 - 00:09:48.350, Speaker B: So goal, having seen V one through VM as a function of those samples, we pick P and we want it to be that our expected revenue on the real distribution, which is this formula, is basically as high as it can get. That is, it's basically the same as the expected revenue of the monopoly price.
00:09:50.400 - 00:09:51.150, Speaker A: Okay.
00:09:53.140 - 00:09:57.360, Speaker B: So something like within a one minus epsilon factor is what we'd love to have.
00:09:57.510 - 00:09:58.210, Speaker A: Okay.
00:10:04.530 - 00:10:46.714, Speaker B: So in a lot of pricing problems and auction problems, there's actually kind of a very nice interpretation of what these training inputs are, these sort of samples from the distribution, which is just your data. And in fact, definitely some of the big data companies do roughly exactly this. So imagine you're ebay, or imagine you have a search engine and you're running sort of auctions on keywords. You can just look at what people bid on these keywords or on these products in the past. If you assume some kind of stationarity, which, depending on the application, may or may not be true, but if you assume some kind of stationarity, those are effectively giving you these ID samples. So, again, you'd like to extract from your past bid data. How should you sort of tune things like the reserve prices of your auctions tomorrow? So it's actually a very natural interpretation.
00:10:46.714 - 00:10:50.334, Speaker B: A lot of modern applications. Anyways, this is what we want.
00:10:50.532 - 00:10:51.280, Speaker A: Okay.
00:10:52.130 - 00:11:23.632, Speaker B: Any questions about that? Yeah, Andy, good question. That's exactly what's up next. All right, so that's what we like to do. But there's kind of a problem, which is what Andy's question was alluding to, which is the way I've phrased it. We can't do this.
00:11:23.766 - 00:11:24.112, Speaker A: Okay.
00:11:24.166 - 00:11:31.792, Speaker B: This is too much to hope for in the following sense. So there's really two questions we want to ask.
00:11:31.846 - 00:11:32.016, Speaker A: Okay?
00:11:32.038 - 00:11:58.560, Speaker B: So first is the design question, which is what's the smartest way to use your data? So what's the smartest I e the best function from samples to prices? But then also we might want to study what's sometimes called the quote unquote sample complexity, which is how much data do we need to harvest in order to achieve a certain kind of performance? So if we want to get within 90% of optimal, how much past data do we need to use? The issue is that in some sense, the sample complexity is infinite.
00:11:58.660 - 00:11:58.924, Speaker A: Okay.
00:11:58.962 - 00:12:17.360, Speaker B: What do I mean? I mean, for all fixed number of samples, m. Remember, little M is the number of samples we get. There exists a distribution, capital F, such that this and by this I mean the goal over there is impossible.
00:12:20.580 - 00:12:21.184, Speaker A: Okay?
00:12:21.302 - 00:12:32.580, Speaker B: And it's not a deep thing. It's easy to explain. The issue is if you have a very, very small chance of winning the lottery.
00:12:32.920 - 00:12:33.670, Speaker A: Okay?
00:12:34.120 - 00:13:02.876, Speaker B: So consider a distribution where the probability that V is big. Let's say capital M is small. So let's say one over little M squared. So little M is the number of samples. Capital M is just some big number, and then otherwise it's zero. Okay? Where M is big, let's say like at least little M cubed.
00:13:03.068 - 00:13:03.810, Speaker A: Okay?
00:13:05.940 - 00:13:12.608, Speaker B: All right, so first what if we knew the distribution? If we knew the distribution, then we know what capital M is.
00:13:12.694 - 00:13:13.330, Speaker A: Okay?
00:13:13.780 - 00:13:38.980, Speaker B: In general, it's kind of a no brainer to figure out what to set the price to, right? So the value is zero. Who cares? We don't want to deal with them. And then if they only have one possible positive value, we should just set the price of that value and make them pay. Everything are willing to pay. So clearly the optimal price is just to set it equal to capital M. Okay, so opt. So P star equals capital M, and opt is capital M over little M squared.
00:13:38.980 - 00:14:05.990, Speaker B: And so like for capital M is little M cubed, then this is little M, which is big, potentially. The point is this is as big as we want. We just pick capital M big. On the other hand, we get these little M samples, what are we going to see? Zeros. Nothing else. With high probability, we'd have to be very lucky to see even one. If we see a capital M, great, we're home free, right? But that's almost never going to happen.
00:14:05.990 - 00:14:25.528, Speaker B: So with high probability, all vis are zero. So we're then responsible for hallucinating up some price, basically just positing some wild guess for what this unknown capital M might be.
00:14:25.614 - 00:14:26.200, Speaker A: Okay?
00:14:26.350 - 00:15:10.516, Speaker B: And almost all the time we're either going to guess too high and then we'll never make any money at all, or we're going to guess way too low. Okay, so maybe we guess M over two or M over 100 or who knows what. Okay, so we're going to be way off. So we'll make a little bit of money, but nowhere as close to what we could, certainly not one minus epsilon times what good had we known it up front? Okay, so why do I present this not because it's interesting, but just because it says we need to at least make some modest assumption about what this distribution capital F is. And the good news is many different modest assumptions suffice. For interesting results, I'm going to show you two of the many ones that suffice in this lecture. So one of them that suffices is we're just going to look at bounded valuations.
00:15:10.628 - 00:15:11.290, Speaker A: Okay?
00:15:13.340 - 00:16:01.672, Speaker B: So to fix this issue, mild restriction on F. And the first restriction I'm going to impose, we'll look at an incomparable one later, is we're going to assume that the support of F lies between one and capital H, where capital H is some parameter that we know. Okay, so we know upper bounds on the support one and H. We know nothing else at all about capital F. Capital F can be arbitrarily weird otherwise. Okay, good. So now we can actually answer this question.
00:16:01.672 - 00:16:30.450, Speaker B: We can understand this goal. And again, to reiterate, the goal really has two parts. So first of all, given samples, what's a smart way to use them. What's just a good function from samples to prices. And then second of all, if you had a particular revenue target, like a one minus epsilon approximation, what value of M is necessary and sufficient? So how big does M have to be before you can get it? Okay, all right, so we can answer both those questions just in this model. Any questions?
00:16:32.100 - 00:16:56.776, Speaker A: Yeah, probably not in auctions, like Ebay, but in super high valued auctions, if they're collecting the data, it also becomes a game. Right. So if the agents know or can figure out that we are going to use the data, they might play, like when play to play with the data.
00:16:56.958 - 00:17:18.544, Speaker B: Submit false values such that we that's right. In the applications I mentioned, it seems like hard to pull that off. I'm not so sure that actually Google keyword advertisers are deliberately underbidding so that their subsequent reserve prices are higher. They could try, but it seems a little hard to pull off.
00:17:18.582 - 00:17:20.316, Speaker A: That happens in World of Warcraft auctions.
00:17:20.428 - 00:17:21.024, Speaker B: In which one?
00:17:21.062 - 00:17:22.396, Speaker A: In World of Warcraft auctions.
00:17:22.428 - 00:17:29.716, Speaker B: Oh, yeah, makes sense. Yeah. No, so there's no question there is an incentive. Are you saying that people who say.
00:17:29.738 - 00:17:34.832, Speaker A: World of Work are smarter than people who are buying ads on Google, less money is safe?
00:17:34.896 - 00:17:41.688, Speaker B: Yeah. So agreed. So there's potentially incentive issues if you're harboring from the same people you're later selling from.
00:17:41.774 - 00:17:42.340, Speaker A: Agreed.
00:17:42.420 - 00:18:27.120, Speaker B: So we're going to ignore those in this lecture. Okay. All right, so I need to do a little bit of technical development to explain how this is going to work. Ultimately, our pricing algorithm is going to be very natural, but it's especially now, I mean, to see how it's natural, we need to think about it in just the right way. Okay, so let me sort of tell you how you should think about learning. So the high level plan is, remember, so we talked about this with the sorting application on Monday, which was a really naive and impractical way to approach that sorting problem would be to try to learn all N factorial probabilities of all the different possible permutations you might see, but that would take N factorial samples at least. It would take N factorial space to kind of remember stuff, all those statistics.
00:18:27.120 - 00:18:50.556, Speaker B: And it's not what we did. We just learned relevant statistics, a small number that were sufficient to get a sorting algorithm, which worked well. So that was great. That was a huge win. So it's the same thing here. We don't really want to learn capital F in sort of excessive gory detail. We just want to learn enough relevant statistics about capital F that we're off to the races with figuring out basically the right price.
00:18:50.556 - 00:19:24.696, Speaker B: Okay, so that's what I'm going to develop. Now I'm going to sort of develop your intuition for what are these relevant statistics about capital F to get a good solution to the pricing problem. Okay, so I need to tell you about prices and the corresponding quantiles. So we're going to go back and forth between two different views of the same thing. Basically the same thing it says, are two different parameterizations of all the possible prices that you might use.
00:19:24.798 - 00:19:25.450, Speaker A: Okay?
00:19:25.980 - 00:20:11.408, Speaker B: So for the moment, let's just draw some pictures about some distributions. Assume we knew a distribution capital F. So in general, CDF functions sort of look like this. Remember, it's at a given point at a given P, the fraction of the mass which is less than P. Okay? So this is going up with P, and it starts at zero, and then in this case at H, it would terminate at one because it's a probability distribution. So you can think of F as mapping numbers, in this case, between one and H to numbers between zero and one. Now, if you think about it, what we really care about for the revenue computation is not so much F of P, but what we care about is one minus F of P.
00:20:11.408 - 00:20:45.380, Speaker B: That's a little more natural, the probability of a sale. That's like the good case, right? So being below P is the bad case. Being above P is the good case. So let me actually just kind of do a quick flip of this. So I'm going to replace this by one minus F of P. I'm also going to call it Q, okay? And so now instead of this going from southwest to northeast, this goes from northwest to southeast.
00:20:45.720 - 00:20:46.470, Speaker A: Okay?
00:20:47.400 - 00:20:56.680, Speaker B: So if you set a price to be zero, then the selling probability is one. If you set the price to be more than capital H, then the selling probability drops to zero.
00:20:56.830 - 00:20:57.288, Speaker A: Okay?
00:20:57.374 - 00:21:02.424, Speaker B: So and in general, as the price gets higher, you're making more money, but you're selling less frequently.
00:21:02.552 - 00:21:03.276, Speaker A: Okay?
00:21:03.458 - 00:21:27.284, Speaker B: All right, so this is what I mean by quantile, okay? And if I ever use the notation the quantile of a price P, I just mean one minus F of P. Okay? So Q of P just says start from P, go up to this curve and go over to the left and see what the number is between zero and one.
00:21:27.402 - 00:21:28.070, Speaker A: Okay?
00:21:31.320 - 00:21:45.624, Speaker B: We want to be able to go backward, too. So we also define given a quantile. So given a number between zero and one, what's the corresponding price? Okay, so if we start on the left, then we move to the curve, and then we drop down and we see what we get.
00:21:45.742 - 00:21:46.410, Speaker A: Okay?
00:21:47.100 - 00:21:53.516, Speaker B: And so the formula for this is just the inverse of the CDF evaluated at one minus Q.
00:21:53.698 - 00:21:54.430, Speaker A: Okay?
00:21:55.440 - 00:22:28.740, Speaker B: And notice that we can write the revenue. So we were writing it as P times one minus F of P. So the dollars per sale times the probability of a sale. Alternatively, notice this is just Q here, okay? So it's also P times Q of P. Okay, good. So the next claim will explain what are the relevant statistics. So I need this stuff to have a vocabulary for the relevant statistics.
00:22:28.740 - 00:22:45.660, Speaker B: And so let me tell you what we wish we knew, okay? We don't know them because at the beginning, we know nothing. So really we wish to know capital F. But let me show you sort of a sweet spot, which are just a few statistics about capital F, which if we at least knew those, we'd be fine, okay? And it involves these quantiles. That's why I told you about the quantiles.
00:22:46.640 - 00:22:47.052, Speaker A: Okay?
00:22:47.106 - 00:23:17.850, Speaker B: So claim one. So define q zero to be one over h. So capital H, remember, is our known upper bound on the valuations. And now we just look at we look at powers of one plus epsilon times one over h. Okay? So q one is just q zero times one plus epsilon. Q two is just q one times one plus epsilon and so on up to QS, which is at one.
00:23:18.460 - 00:23:19.210, Speaker A: Okay?
00:23:19.740 - 00:24:22.280, Speaker B: So power is one plus epsilon between one over h and one. And so how many is this? S is log to the base one plus epsilon of h, which is basically your favorite base of log of H over epsilon. Okay, so it's log h over epsilon, different statistics, the quantiles. So here's the claim, which is if you basically the claim says if you knew the prices at all of these quantiles, you'd be done. So the best of the P of QIS. So remember, p of Q just says, okay, if you tell me you're selling, like, with probability 10%, what price does that correspond to? That sells with 10%. So the best of the PiS and what do I mean by best? I mean maximizes expected revenue.
00:24:22.280 - 00:24:45.380, Speaker B: And remember, in this terminology, it's p times q is the expected revenue. So the sale probability times whatever price that's at. And so the claim is this yields a one over one plus epsilon approximation of OT.
00:24:47.320 - 00:24:48.070, Speaker A: Okay?
00:24:50.360 - 00:25:09.096, Speaker B: So what I'm saying is, even if you knew the distribution here's, like a very lossy compressed version of it, which is still good enough for revenue maximization, okay, throw out capital F, except retain only the prices, which make the selling probability equal to these log h over epsilon numbers.
00:25:09.198 - 00:25:09.848, Speaker A: Okay?
00:25:10.014 - 00:25:25.168, Speaker B: So if H is like 100, it's just like 1%, and then maybe it's 2%, 3.5%, 5%, 7%. So just keep track of that set of prices. Then just do brute force search over how good each of those prices are. And the claim is one of them is almost as good as the best one, as almost as good as the monopoly price.
00:25:25.334 - 00:25:26.050, Speaker A: Okay?
00:25:30.740 - 00:25:52.090, Speaker B: All right, so this is pretty simple to prove. So let me prove this for you. So just to be clear where we are right, in our model, we don't know these, right? We don't know the P of QIS because we know nothing. But this is just sort of an intermediate step. So the natural next thing to do is learn the P of QIS. Let me show you that. If we had them exactly, we'd be done.
00:25:52.090 - 00:26:00.510, Speaker B: All right, so proof of claim one.
00:26:15.300 - 00:26:16.130, Speaker A: All right.
00:26:18.500 - 00:26:55.630, Speaker B: So there's some monopoly price. We wish we were using this, but we don't know what it is. So P star be the monopoly price, and Q star is whatever the sale price is the monopoly price. Okay, so here's a little observation. I claim the Q Star. So the selling probability at the monopoly price can't be too small. It's got to be at least one over capital H.
00:26:55.630 - 00:27:37.516, Speaker B: Let's take a little time to see why that's true. So we're using here that F has support in one H. So here's the observation. What if we just set a price of one? How much money would we make? One. Because we'll always sell, the valuation is at least one. That's where the support is. So we can definitely make a buck.
00:27:37.516 - 00:28:04.788, Speaker B: All right, so suppose we tried to sell at a price that was so high that the selling probability was less than one over H. What's the max amount of money we get from a sale? H h. So our expected revenue is going to be less than one over H times H-I-E. Less than one. And we know that can't be optimal. So bound evaluations means we can have this. It's a weak lower bound, but whatever, some bounded away from zero lower bound on the selling probability.
00:28:04.788 - 00:28:13.800, Speaker B: That's why over in the claim, I didn't bother to state any quantiles less than one over H because we know we don't need them kind of operatori.
00:28:13.880 - 00:28:14.670, Speaker A: All right.
00:28:15.840 - 00:28:53.850, Speaker B: Okay, good. So let me show you now that one of these QIS has to be good almost as a monopoly price, and it's sort of like the obvious one, right? So you just kind of want to use okay, maybe we don't have the quantile corresponding to maybe we don't have Q star in our set, but we have a nearest neighbor to Q star in our set. So let's just look at that specifically. Let's look at one that's slightly below Q star. So the QL be the biggest Qi that is at most Q star.
00:28:55.740 - 00:28:56.490, Speaker A: Okay?
00:28:57.340 - 00:29:00.044, Speaker B: And there's got to be one, because Q star is at least one over.
00:29:00.082 - 00:29:00.670, Speaker A: H.
00:29:03.120 - 00:29:35.056, Speaker B: So what do we know? What can we say about QL versus Q star? All right, so I guess by definition, QL is at most as big. How much smaller could it be than Q star? What factor can't be that much smaller than Q star? One plus epsilon.
00:29:35.088 - 00:29:35.188, Speaker A: Right.
00:29:35.194 - 00:29:37.364, Speaker B: Because we have all powers of one plus epsilon in there.
00:29:37.482 - 00:29:38.150, Speaker A: Right.
00:29:43.260 - 00:30:12.100, Speaker B: And also, what can we say about the relationship between the selling price at QL versus the selling price at Q star, namely the monopoly price. No, I mean the prices. One of those is bigger than the other.
00:30:13.830 - 00:30:16.020, Speaker A: Yeah. Why isn't it the same relationship?
00:30:17.110 - 00:30:21.320, Speaker B: Because it's a different variable, one's a quantile and one's a price.
00:30:23.930 - 00:30:25.560, Speaker A: We set the price too.
00:30:26.250 - 00:31:18.550, Speaker B: So QL is a number between zero and one, right? So QL is something like 10%, and Q star is going to be something like 11%. So which of those corresponds to a higher price? QL, the lower selling probability, means the higher price. They're going in opposite directions. So to sell more often, you got to drop the price. To raise the price, you got to drop the probability of sale. So this goes this way because QL is at most Q star. So what does that mean? Well, that just means that if we look at then the revenue that QL obtains and so what's the revenue corresponding to the price of QL? Well, it's just the sale probability, which is QL times the price that leads to a selling probability of QL.
00:31:18.550 - 00:32:00.520, Speaker B: What do we know here? We know this is one over one plus epsilon times Q star times the monopoly price. This is also known as three letters. Good. So I've just exhibited for you one of the prices in our set, which gets a one minus epsilon approximation. Obviously, picking the best one can only be better. Okay, so the best Qi only better. So that's perfect claim one.
00:32:00.520 - 00:32:22.200, Speaker B: Any questions about that? So this says that this particular set of log H over epsilon numbers is sufficient to be off to the races, sufficient to get one minus epsilon approximate revenue. Now, that's only going to be useful if we can quickly figure out what these are or at least close estimates, but at least it sort of gives us a target to shoot for.
00:32:23.130 - 00:32:26.800, Speaker A: So it questions.
00:32:36.070 - 00:32:49.590, Speaker B: All right. So the idea then is just to learn the learn the idea, learn the P of QIS.
00:32:51.850 - 00:32:52.262, Speaker A: Okay.
00:32:52.316 - 00:33:12.750, Speaker B: Ultimately that's what we needed to know, right? So if we knew the I mean, the QIS are not dependent on the distribution. The QIS are just the powers of one plus epsilon between one of H and one. It's the P of QIS. We didn't know. We didn't know which particular price winds up giving you a target selling probability, because we don't know capital F. But if we can learn these, we just pick the best and we know we're done. So let's learn them.
00:33:12.820 - 00:33:13.006, Speaker A: Now.
00:33:13.028 - 00:33:50.634, Speaker B: Of course we're not going to learn them exactly. We're getting these samples. So we're going to have sort of like noisy estimates of these things at best. So let's first observe, just to sort of to encourage ourselves, let's observe that this proof of this claim one is actually very robust to sort of various approximations, okay? And that's going to start making us very optimistic we should be able to pull this off. Okay, so just some observations. So what did we really need for this proof to work? So there's two relaxations which we can accommodate pretty much effortlessly. So first of all, it wasn't so important that we.
00:33:50.634 - 00:34:30.906, Speaker B: Had exactly this set of QIS. If we just have a rich enough set of QIS and then we pick the best one, everything's fine. What did we really need for this proof? We just needed to make sure that whatever Q star was and Q star could be anything between zero and one. Well, not between zero and one, between one over H and one. And no matter what it was, we had defined a Q value in our set that was less than Q star, but at most a one plus epsilon factor less. So any rich enough set with that property is good enough for the proof. So note sufficient to have any set of QIS such that for all T.
00:34:30.906 - 00:34:43.900, Speaker B: So for all possible values that Q star might take on, there exists I such that Qi is between T over one plus epsilon and T.
00:34:45.790 - 00:34:46.540, Speaker A: Okay?
00:34:47.330 - 00:34:55.310, Speaker B: So again, whatever Q star is, we can find something just a tiny bit less than it. So that's all we needed. So that's without change to the proof. Agreed.
00:34:56.390 - 00:34:57.234, Speaker A: Okay.
00:34:57.432 - 00:36:08.840, Speaker B: So now here's something that's slightly more complicated, but barely. So that gives us more flexibility for what these QIS are, which is nice. The other thing is, like we said, we can't really expect to know these exactly with no error, right? So suppose we only learned all of the P of QIS approximately, and actually it's really the revenues. Well, it's sort of the same thing, but let me just state it this way. So if each revenue calculation only accurate, let me phrase this differently. So ultimately we're of estimates. So if our estimates, sorry, p hat of Qi, all we know is that they're within one plus minus epsilon of what they should be.
00:36:08.840 - 00:36:33.988, Speaker B: So a picture will understand what's happening here. Okay, so we're looking at two candidate quantiles and the corresponding prices.
00:36:34.084 - 00:36:34.730, Speaker A: Okay?
00:36:35.740 - 00:37:19.728, Speaker B: So in the old days, when we knew exactly what the price corresponding to every sale probability was, what did we do? We just said, oh, well, let's compare the revenue we get from this price. Okay, so this is the selling probability. This is the revenue per sale. So this is the expected revenue. And then we have some other one P of QJ, QJ, and whichever of these was bigger that's the one we take, or whatever was the biggest of these was the one that we eventually picked. That was in the version of claim one where we knew these things exactly correctly. So if we have some error, if we only know the P's up to one plus or minus epsilon, then these quantities can shift by one plus or minus epsilon.
00:37:19.728 - 00:37:59.808, Speaker B: So they can go up to a one plus epsilon factor or down they can get multiplied down by one minus epsilon. So what can happen is you can have some inversions, okay? So it could be in reality, for the true price corresponding to this quantile and this quantile, they're in this order, but with our estimates, our p hat. So our estimate p hat of the price corresponding to selling property QJ that might be too low by up to a minus one minus epsilon factor, where correspondingly, we might unfortunately overestimate the price. That would give us a selling probability of qi. So that might get multiplied times one plus epsilon.
00:37:59.984 - 00:38:00.660, Speaker A: Okay?
00:38:00.810 - 00:38:18.420, Speaker B: So some of our revenue estimates might shrink, but by hypothesis only by one minus epsilon, some of them might grow only by one plus epsilon. So we might make a mistake. We might choose something whose real revenue for the real distribution capital f, is actually lower than something else. But the worst possible error is by one minus epsilon over one plus epsilon.
00:38:18.500 - 00:38:18.744, Speaker A: Okay?
00:38:18.782 - 00:38:54.420, Speaker B: If you get the simultaneous crossing of these two things. So if our estimates of the p of the prices corresponding to the sale probabilities are within one plus epsilon, then the only thing that happens, and this is in this final step here, actually, okay? So we might make a bad choice when we pick the best one, the approximation factor degrades by another one minus epsilon over one plus epsilon factor.
00:38:56.680 - 00:38:57.430, Speaker A: Okay?
00:38:58.760 - 00:39:17.180, Speaker B: And that's the picture. That's the proof. So that leaves us with a sort of new version of claim one, where the approximation factor instead of one minus epsilon is now one minus epsilon squared over one plus epsilon. But if we just reset epsilon to be epsilon over four, it's fine, okay? That whole thing is bounded above by one plus epsilon, so it's no big deal.
00:39:17.330 - 00:39:18.110, Speaker A: All right?
00:39:18.560 - 00:39:32.896, Speaker B: So again, all of this was to say that we had claim one that was sort of a daydream where we had exact computations of the price corresponding selling probabilities. Now we realize all we need to do is get estimates of them. Surely that seems possible, and it is.
00:39:32.998 - 00:39:33.650, Speaker A: Okay?
00:39:34.500 - 00:39:41.510, Speaker B: Okay, so any questions about that? So now I want to tell you what is the algorithm which actually satisfies the hypotheses of this relaxed version of claim one?
00:39:41.880 - 00:39:43.990, Speaker A: Yeah. Okay.
00:39:44.840 - 00:40:30.096, Speaker B: All right, so, algorithm, it's very simple. So you get your samples v one up to VM. Now, we don't know what any of these cues are, okay? We know the value of these things, right? We know that the person was willing to pay $20. We just have no idea. If we set a price of $20, what would be the percentage of time we got a sale? We have no idea. So we do the obvious thing. We estimate using our samples, right? So if we think about a price of $20 and exactly seven out of 20 of the things are at least $20, we imagine as if 35% of the time in reality, we'll get a sale.
00:40:30.096 - 00:41:19.510, Speaker B: So it's the obvious empirical estimate. So we set Q hat of VI to be equal to the number of the samples, with VJ at least VI over m the number of samples, okay? So these are all multiples of one over M. All of these empirical quantile estimates, all these Q hats, they're multiples of one over M. For the minimum sample, this is going to be one. For the maximum sample, this is going to be one over M, for example. Yeah. Okay.
00:41:19.510 - 00:41:25.460, Speaker B: And then what our algorithm does is it simply returns.
00:41:25.960 - 00:41:26.276, Speaker A: Right?
00:41:26.298 - 00:41:59.760, Speaker B: So it's given the samples, it's responsible for reporting a price. So it just is going to say use as a price the sample which maximizes the expected revenue I think you would get, and the expected revenue I think you would get is just with respect to these selling probabilities, the Q hats. Okay, so return VI that maximizes VI times Q hats of VI.
00:42:01.700 - 00:42:02.448, Speaker A: Okay.
00:42:02.614 - 00:42:28.760, Speaker B: So the value of that sample, the value of VI times the fraction of the samples that are at least VI or above. So one possibility is you set it to be equal to the maximum of all the vis, but then it gets scaled by one over M. You can set it to the minimum of the vis, but then you'd only make that much per sale. Or probably you want to set it somewhere in the middle. So that's the whole algorithm.
00:42:29.840 - 00:42:30.590, Speaker A: Okay.
00:42:32.240 - 00:42:33.900, Speaker B: Any questions about the algorithm?
00:42:34.240 - 00:42:36.620, Speaker A: The algorithm is kind of the first one you think of.
00:42:36.690 - 00:42:37.068, Speaker B: Yes.
00:42:37.154 - 00:42:41.364, Speaker A: Basically says just assume that your input exactly represents the distribution.
00:42:41.432 - 00:43:12.040, Speaker B: Yes, exactly right. Which is nice, I think. Yeah, it's true. The challenge is more in the analysis showing that the obvious thing works. It's not so much some kind of unusual creativity with how to do it. So I agree that okay, so to justify this, we need a claim two. But claim two basically just says, turn off bounds, do what you want in this case, but let me spell it out for you.
00:43:12.040 - 00:43:53.984, Speaker B: I'm going to spell out the statement. I'm not going to spell out the proof. You will spell out the proof. So claim two, and here is where we'll see the sample complexity. So suppose M M is the number of samples is at least C times H over epsilon squared, log H over epsilon squared. And so, as usual on these kind of statements here, c is a sufficiently large constant, though it really doesn't have to be very big, something like four H capital H. Remember that's the upper bound on the valuations epsilon.
00:43:53.984 - 00:44:34.088, Speaker B: This is shooting for a one minus epsilon approximation. Suppose we have this many samples at least then with high probability over the samples. The following are true. So I'm going to write down two statements which basically are the Hypotheses of the relaxed version of claim one, which means we're done. Okay, so the point of claim two says that algorithm works, but I have to sort of glue it with our sufficient condition for working, which was our claim one. Okay, so what were the hypotheses in claim one? Or what are the relaxations? So the first thing we better have is we need to have a rich enough set of quantiles.
00:44:34.204 - 00:44:34.870, Speaker A: Okay?
00:44:36.200 - 00:44:56.180, Speaker B: So that's one thing we needed for the proof of claim one to go through. So the first statement is just that. So for all T between one over h and one, indeed, there exists a sample I whose quantile and notice these are the Q's. These are not the Q hats.
00:44:56.260 - 00:44:56.504, Speaker A: Okay?
00:44:56.542 - 00:45:15.170, Speaker B: Because claim one was all about the Q's, right? So you needed basically good coverage of the real quantiles of the distribution. So, again, we don't know what these are, but the analysis just says that we have them. So there exists I such the Q of VI lies in the target range, okay? T over one plus epsilon t.
00:45:17.140 - 00:45:17.456, Speaker A: So.
00:45:17.478 - 00:46:11.948, Speaker B: With this many samples, we have a rich set of quantiles available. Again, we don't know which VI, we can't compute Q of VI, but having sampled all these things, we know we have a very rich set of them anyways, which is all we needed for the performance guarantee. Okay, second thing, for all of the relevant samples. So for every VI who's corresponding yeah, no, this is right. Okay, so for all of the samples that aren't absurdly big so whose selling probabilities aren't too small, indeed, we get a good approximation of the Q hats.
00:46:12.044 - 00:46:12.690, Speaker A: Okay?
00:46:14.760 - 00:46:31.924, Speaker B: So Q hat of VI is in one plus epsilon times Q of VI. And of course, given that at a given price, you have a very good estimate of the sale probability, that also means you have a very good estimate of the expected revenue at that price VI.
00:46:32.052 - 00:46:32.730, Speaker A: Okay?
00:46:33.180 - 00:46:55.036, Speaker B: So and hence, we think the price VI would generate revenue VI times our estimate of the probability of sale, which is our Q hats. And because our Q hats are almost correct, this is in the real revenue one plus minus epsilon.
00:46:55.148 - 00:46:55.810, Speaker A: Okay?
00:46:57.300 - 00:47:52.576, Speaker B: And this was actually let me actually do this. So really, in claim one, what we needed was that our revenue estimates were basically accurate. So sorry, there's sort of a typo here. So the figure is correct, but I forgot the cues in this statement. So there's a hat here. Okay, so this figure was what said as long as we get one plus epsilon estimate of the other revenue estimates, we only lose this small factor on the approximation with respect to claim one. And the second statement of claim two is saying, yes, indeed, we do get the desired accuracy on the revenue estimates of each of the potential selling prices we might use.
00:47:52.758 - 00:47:53.490, Speaker A: Okay.
00:47:56.840 - 00:47:59.430, Speaker B: So is the statement of the claim clear?
00:48:03.000 - 00:48:10.072, Speaker A: I'm not entirely clear why in the first part, we're interested in the true hues as opposed to the Q half.
00:48:10.206 - 00:48:46.610, Speaker B: This is what claim one needs. So when we thought about claim one, so claim one basically said, here's some sufficient statistics to guarantee that one of the available prices will actually work. There's various ways to do this. This just turns out to be, as far as I can tell, the cleanest way to do the derivation. So I could reprove claim one with Q hats. Instead, when we took claim one and we relaxed it to estimates, the way I set it up, the only thing that's estimated in claim one is the revenues. And so you can see that very clearly here.
00:48:46.610 - 00:49:23.684, Speaker B: But the point is just this is true also, which sort of makes sense. Right. So you take all of these samples, you expect them to be sort of uniformly distributed throughout the quantile space, and we have much more than h over epsilon. So h over epsilon is kind of what you'd need to expect one every kind of epsilon in the quantile space. So we have a lot more than that. So you'd sort of sort of expect that you should have plenty to sort of get the coverage that claim one demanded. Again, there'd be a version with the Q hats, but claim one was easiest to prove with the queues.
00:49:23.684 - 00:49:36.220, Speaker B: And it's actually very easy to establish this statement as well, just from the assumptions we made about that, that it's bounded. Yeah. Right. And you can see the support bound is showing up in the sample complexity.
00:49:36.300 - 00:49:36.652, Speaker A: Cool.
00:49:36.726 - 00:50:15.410, Speaker B: Yeah, thanks. So, I mean, there's sort of two things. One is kind of like morally, why should this all work? And the explanation for that is pretty simple, which is just claim one tells you you only need sort of log h over epsilon statistics. It doesn't seem like it should be that hard to learn a good estimate of any one of those statistics. You'd sort of expect claim one would still be true if you had really good estimates and so then you should be done. And all that intuition is correct, but at the same time, I wanted to give you some version which was actually mathematically fully correct. That said, I'm not going to bore you with turn off bound derivations, that kind of thing, but at least the statements of everything I put on the board really does imply what we want.
00:50:15.410 - 00:51:00.220, Speaker B: So any other questions about that? So the proof here, there's no surprises in the proof. So I'll put this on homework five. So this is just churn off bounds, okay. And they're both pretty easy. One is especially straightforward and two is pretty straightforward. And so that's why the learning algorithm works. So corollary our algorithm gets a one minus epsilon approximation with this sample complexity.
00:51:04.900 - 00:51:05.360, Speaker A: Okay?
00:51:05.430 - 00:51:14.876, Speaker B: So the sample complexity depends roughly linear on the bound, h on the support and quadratically on the reciprocal of the error tolerance.
00:51:14.988 - 00:51:15.650, Speaker A: Okay?
00:51:16.740 - 00:51:46.200, Speaker B: Now, you should. You know, like Raphael said, it's kind of the obvious thing. And so you'd be natural to wonder, maybe a less obvious thing could somehow do better. Not so much better than one minus epsilon, but better in terms of how much data is necessary to get to one minus epsilon. It turns out the answer is no. So it's known that actually this is tight, not just for this algorithm, but for any algorithm up to this log factor. So no matter how smart you are with the data, you really need h over epsilon squared samples to get a one minus epsilon approximation.
00:51:46.200 - 00:52:47.296, Speaker B: Okay, how do you see that? Sort of the slickest way I know how to do it is an information theoretic argument. So basically you take two distributions, and you show that if you cannot distinguish between them, so basically, you're promised there's distribution d one, there's distribution d two. I promise you that the m samples will be either all from d one or they'll all be from d two, and I'll make them far enough apart so that you really have no choice but to distinguish between which of those two cases it is in order to get one minus epsilon approximation of revenue. And then you keep track of the distance between the two distributions, or rather the samples generated by the two distributions using the notion of statistical distance, which we talked about. When did we talk about that? Four weeks ago or so. Oh, good. Yeah.
00:52:47.296 - 00:53:12.650, Speaker B: So with the hashing, we had a notion of approximately uniform. So there we wanted upper bounds of statistical distance, saying things were almost the same. Here in the proof you're going to have lower bounds in the statistical distance, basically saying that until the number of samples becomes really large, the statistical distance is large. And therefore, sorry, it takes a lot of samples before you can get the statistical distance to be sufficiently large so that you have a chance of differentiating which ones the sample comes from. So that's how you do it.
00:53:14.140 - 00:53:14.890, Speaker A: Good.
00:53:15.660 - 00:54:04.890, Speaker B: All right. So any other questions about this stuff? All right, then. Let me switch gears a little bit, and instead of asking this question so remember I said there were two intertwined goals. One is, how do you use samples in a smart way? So we now know this is a smart way to use it if you have sufficiently many samples. The other was the sample complexity to get one minus epsilon. So now I want to look, I want to switch the regime where you have very little data, where you know almost nothing about the distribution. So getting a one minus epsilon approximation is not really realistic, but you still want to use your limited data in the best way possible, and you want to know what kind of approximation factor can you get.
00:54:04.890 - 00:54:19.660, Speaker B: And so the other thing I want to tell you about today is a kind of surprising fact, which is that even if m equals one, you can get a nontrivial approximation factor.
00:54:20.660 - 00:54:21.024, Speaker A: Okay?
00:54:21.062 - 00:54:29.040, Speaker B: So this really drives home the point that you can start getting good approximation guarantees for these kinds of problems without learning almost anything about the distribution.
00:54:29.380 - 00:54:30.130, Speaker A: Okay.
00:54:33.460 - 00:54:59.870, Speaker B: So positive results when M equals one. So I'll give you one sample and you have to pick a price. It's pretty much all you could do, right? Yeah. So that's actually what we're going to prove it for. You just regurgitate the sample as the price. All right, so the proof is actually I can give you a proof by picture, but I have to explain what the picture means for a little bit. So let me tell you what the picture is going to be.
00:54:59.870 - 00:55:43.000, Speaker B: So I need to tell you about revenue curves. And so again, we're going to be using these quantile notions. So before the quantile was on the y axis, now I'm going to put it on the X axis. So remember, this is the probability of a sale. So low Q means a high selling price. High Q means you're selling at a low price. So what's the y axis? The y axis is just going to be the revenue you get when you set a price that sells with probability Q.
00:55:43.150 - 00:55:43.464, Speaker A: Okay?
00:55:43.502 - 00:55:54.680, Speaker B: So I'm going to call that R of Q. So that's just defined as the probability of a sale times the money you make on a sale.
00:55:54.760 - 00:55:55.148, Speaker A: Okay?
00:55:55.234 - 00:56:25.750, Speaker B: And again, remember, this thing is just equal to F minus one of one over Q. So it's just the price which would net you a probability of sale of exactly Q. So for example, it for the uniform distribution. Well, so then capital F is the identity function. So F inverse is also the identity function.
00:56:26.460 - 00:56:27.160, Speaker A: Okay?
00:56:27.310 - 00:56:50.780, Speaker B: So q times p of q, it just becomes q times one minus q. So in other words, it's like exactly the curve I just drew there. Okay, so that's what the revenue curve looks like for the uniform distribution. So if it's the exponential distribution.
00:56:53.600 - 00:56:53.916, Speaker A: Well.
00:56:53.938 - 00:57:09.270, Speaker B: So now F has an exponential. So you might expect F inverse to have a log. So let me spare you the calculation and just tell you the answer. So here R of Q is going to be equal to Q times the natural log of one over Q.
00:57:09.720 - 00:57:10.470, Speaker A: Okay?
00:57:11.320 - 00:57:39.276, Speaker B: But we see something we saw before, right? So you might recall that when we solved for the monopoly price of the exponential distribution, the monopoly price was one. The selling probability at that monopoly price was one over e. So the best price corresponded to Q being one over e. And when Q is one over e, this is also one over e. So it's one over e times one. So that the just the highest. The revenue at that sell price of one over e is also one over e, which we knew.
00:57:39.378 - 00:57:40.030, Speaker A: Okay?
00:57:40.400 - 00:57:55.990, Speaker B: So graphically what the revenue curve looks like for the exponential distribution. It's not symmetric around one half anymore. It's a little bit kind of lopsided with more mass to the left.
00:57:57.080 - 00:57:57.830, Speaker A: Okay.
00:57:58.280 - 00:58:20.620, Speaker B: It's not quite that flat. And so the height, right. So the biggest revenue we know occurs when Q is one over e. And we also know that the revenue at that point is one over e, whereas for the uniform distribution, we know the best Q is one half and we know that the revenue at that point is a quarter.
00:58:22.000 - 00:58:22.796, Speaker A: Okay?
00:58:22.978 - 00:59:06.612, Speaker B: So those are two revenue curves with two distributions. One thing you'll notice about both is these are both concave functions. So that's going to be the second distributional assumption we're going to make today. So we're going to replace the bounded valuation assumption with an orthogonal assumption. So now the distribution can have an unbounded support. Like the exponential distribution is totally fine, but we're going to assume that the revenue curve is concave. That is not true for every distribution in the world, but it's true for the majority of the distributions you'd find in the back of an Intro to Statistics textbook.
00:59:06.612 - 00:59:43.620, Speaker B: So it covers a lot of cases to assume. And again, remember, we need some kind of distributional assumption to prove anything. So this is going to be the assumption for this result. So F is such that R is concave. Sometimes these are called regular distributions, although you won't need to know that. Although maybe the takeaway lesson for that is whenever you prove a theorem you really like, but it doesn't cover all cases and you need to make some assumptions. Call the objects that satisfy your assumptions regular.
00:59:43.620 - 01:00:16.832, Speaker B: Very nice idea, right? I guess you could also just call the ones that don't satisfy the assumptions pathological or something like that, but you get the idea. Okay, good. Right. So here's the key claim. So let me state this and then I'll explain why it implies what we want. So this is kind of an amazing claim. It's going to sound very weird, but it's also going to sound interesting.
01:00:16.832 - 01:00:43.064, Speaker B: I hope. So suppose you'd love to use the monopoly price. You know, that's the optimal thing. Suppose instead of using an optimal price, you do something very strange. You pick a random sample from the distribution F, and you use that as a price instead. Now we've already talked about the case with the motivation of having M equal one. This kind of seems like the first thing to try, right? Just regurgitate the sample as the price.
01:00:43.064 - 01:01:08.624, Speaker B: So we're just trying to say, how well does that do? Okay, so using a random sample as a price rather than using the monopoly price, and the key claim is that that gets a factor one half approximation. So formally draw a price randomly from the distribution capital F. So remember what F is. So F is where the bidder's valuation is being drawn from, right? But now we're sort of using it to sort of make up a price.
01:01:08.742 - 01:01:09.410, Speaker A: Okay?
01:01:09.940 - 01:02:10.660, Speaker B: And so if we look at the expected revenue that we're going to make, where the expectation here is with respect to the random price and then also with respect to this sort of bidder who shows up with some random valuation v. This is at least one half times p star, one minus F of p stare corollary. So this is for a regular distribution, I should say. So this is under this assumption. So corollary we can get a one half approximation even when m equals one. Okay, so just reuse the sample as the price. That's an immediate corollary.
01:02:10.660 - 01:02:57.860, Speaker B: Agreed? No, I've dropped that assumption. So I've dropped to the bounded valuation assumption. I've replaced it with the concave revenue curve assumption, and those are orthogonal, those are incomparable assumptions. So in particular, this holds for the exponential distribution or like a log Gaussian distribution. All right, so is this statement clear before I prove it? And it's clear why we care about this, why this is what we want. Okay, all right, well, then let me tell you the proof, and the proof is by picture. All right? So let's look at these revenue curves.
01:02:57.860 - 01:03:41.280, Speaker B: Look at the monopoly price. So we're looking at a point which in the X axis, the value is q star. So the sell probability. So I'm looking at the very tippy top of the revenue curve. Okay, so the highest amount of revenue you make, okay? So the X axis, that's the quantile. So that's Q star, the selling probability at the monopoly price, and then the y axis is the revenue at q star. Okay, so what can you tell me about the area of this rectangle.
01:03:45.800 - 01:03:46.116, Speaker A: That.
01:03:46.138 - 01:03:56.740, Speaker B: Goes through the very top of the revenue curve? So what's the width? Yeah, what's the height? The height is r of Q star.
01:03:56.890 - 01:03:57.204, Speaker A: Right?
01:03:57.242 - 01:04:04.180, Speaker B: So the expected revenue you make when you use the monopoly price, also known as a three letter word, opt.
01:04:04.250 - 01:04:04.916, Speaker A: Right?
01:04:05.098 - 01:04:07.750, Speaker B: So the area of the rectangle is opt.
01:04:08.240 - 01:04:08.990, Speaker A: Okay.
01:04:23.120 - 01:05:08.892, Speaker B: All right, so now here's the one part which is kind of neat or tricky, depending on how you want to think about it. So let's look at this expression on the left hand side. Let's do a change of variable. So p, let's think of P not just as a price, but let's think of it as the inverse image of some target selling price.
01:05:09.026 - 01:05:09.710, Speaker A: Okay?
01:05:10.480 - 01:05:37.080, Speaker B: So here's another way to think about it, right? So what are we doing here? We're drawing P according to capital F. Here's another way to draw P from F. I first draw Q uniformly from zero one. Okay, that's a sale probability. Then I take one minus Q. Okay, that's now a failure probability, a probability that evaluates at most some p, but it's also drawn uniformly from zero one.
01:05:37.230 - 01:05:37.930, Speaker A: Okay?
01:05:38.940 - 01:05:41.880, Speaker B: Now I take F Inverse.
01:05:42.940 - 01:05:43.304, Speaker A: Right.
01:05:43.342 - 01:06:20.816, Speaker B: So I map this number between zero and one to some number on the real line according to f inverse. That's a random sample from F. Okay? So another way to think about it is suppose I want to randomly sample the height of a human being. Obviously, I don't want to pick a random number between 4ft and 8ft right? It's a very non uniform distribution. So if I was directly picking on the numeric scale, it would be a complicated distribution. On the other hand, I could just pick a random person in the world and look at their height. That's going to be a randomly sampled height value from the height distribution.
01:06:20.816 - 01:06:49.884, Speaker B: So that's all I'm doing here. Okay, so rather than sampling V from F, I sample Q from uniform zero one. I look at one minus Q. That's again, uniform zero one and then I apply F inverse. That gives me a sample of P according to capital F. All right? So that change of variable where I instead sample Q. So if this is confusing, I hope you at least recognize that.
01:06:49.884 - 01:07:21.084, Speaker B: Look, there's a bijection between the P's and the Q's. There's some distribution on the P's. So surely there's some corresponding distribution on the Q's. So maybe what's a little mysterious is why is it uniform? But they're quantiles, so it's really by definition that they're uniform. So the Q equal zero five, that's exactly where half the distribution is below it and so on. Okay, so quantiles are just by definition uniform in zero one. Okay, so instead we pick Q uniformly from zero one, but then we just look at the exact same expression.
01:07:21.084 - 01:07:48.000, Speaker B: Okay, so before we were picking P uniform from F, we were looking at the revenue. Now we pick Q uniform from zero one and we look at the corresponding revenue. So Q times P of Q. So these are just different ways to write revenue. So that's exactly the same number. The left hand side is exactly that number. Okay, let me actually rewrite this still further.
01:07:48.000 - 01:08:24.550, Speaker B: This is just the revenue expected at the price with selling probability Q. So let me actually write this as R of Q. Okay, so what does that correspond to? Pictorially? So expectations are just integrals, right? And the uniform distribution is just sort of Le Bag measure or your standard Riemann integral. So by taking the expectation over Q from zero to one of R of Q, all I'm doing is calculating the area under the revenue curve.
01:08:25.130 - 01:08:30.870, Speaker A: Okay, rectangle.
01:08:37.710 - 01:09:35.426, Speaker B: So the quantity we care about the expected revenue of a random reserve price drawn from F is the same thing as the pink area, the area under the curve. So the third geometric shape I want you to think about the third and final one is a triangle. And it's a triangle that has endpoints 0010 and the top of the curve. So q star, r of Q star. In these two cases, the blue triangle is a subset of the pink curve. That's true whenever this curve is concave for a concave curve, all the chords lie below the graph.
01:09:35.558 - 01:09:36.240, Speaker A: Okay?
01:09:43.090 - 01:10:02.120, Speaker B: So by concavity area under the curve is lower bounded by the area of the triangle. Now the triangle has exactly the same width and the same height as the rectangle, but it's a. Triangle. So it has exactly half the area.
01:10:02.650 - 01:10:03.400, Speaker A: Okay.
01:10:16.030 - 01:10:41.310, Speaker B: And so we're done. That's why this is at least half of the optimal revenue. The optimal revenue was exactly the rectangle. This should be half here. Sorry. Thank you. Area of the triangle is exactly half the area of the rectangle, and the rectangle was the optimum.
01:10:41.310 - 01:10:46.230, Speaker B: Any questions about that?
01:10:49.320 - 01:10:53.060, Speaker A: Just wondering how this generalized to like, when we have two samples.
01:10:54.380 - 01:11:07.050, Speaker B: Yeah, it is a good question. When you have two samples, it's not even clear how to set the price. No, it's not just optimizer with a 30. What does that mean?
01:11:07.900 - 01:11:18.024, Speaker A: Pick the price. One way to interpret this result with one sample is just assume that the one sample is your entire distribution. Right? Yeah, kind of think of this algorithm the same as the other algorithm.
01:11:18.072 - 01:11:18.540, Speaker B: That's right.
01:11:18.610 - 01:11:19.756, Speaker A: So with two or three whatever, you.
01:11:19.778 - 01:11:24.450, Speaker B: Can just assume that's your entire that's an algorithm. It's not clear how well it works.
01:11:25.060 - 01:11:26.624, Speaker A: Do you need to prove any bounds on that?
01:11:26.662 - 01:11:51.432, Speaker B: It's not known how to prove a bound better than 0.5 for any of those algorithms. It's not to say it can't be done, but it's not clear. And it's not also just clear that it's the optimal thing to do. If you really want to sort of get the best possible couple constant. Maybe it is, but that seems hard to develop intuition about that. Even with two samples, we don't really know the best way to use them.
01:11:51.432 - 01:12:04.740, Speaker B: So it's a good question. Any other questions? Well, this seems like a good place to leave you before the holidays. So have a great break and I'll see you a week from Monday.
