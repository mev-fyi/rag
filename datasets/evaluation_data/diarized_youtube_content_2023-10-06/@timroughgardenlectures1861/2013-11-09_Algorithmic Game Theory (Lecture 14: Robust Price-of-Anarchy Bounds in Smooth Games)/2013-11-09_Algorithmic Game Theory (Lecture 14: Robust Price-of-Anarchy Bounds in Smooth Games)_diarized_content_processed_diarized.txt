00:00:00.170 - 00:00:54.954, Speaker A: Okay, so this is a, this is a fun lecture to give actually, because the theory I'm going to tell you about today has its genesis in this very class in CS 364 A. So the story was, is five years ago I was in the same sort of segment of the class teaching a bunch of price of energy analyses. And really just for pedagogical reasons, I was trying to figure out sort of, you know, the optimal way to present all of these proofs to make it transparent as possible. And as I was doing that, it became clear that there really was for many of the most well known price of energy bounds, there was a generic recipe for deriving them. And in fact, that generic recipe can even be precisely formulated as a definition, which is a definition you'll see today. And then what was cool is this definition actually has consequences, implications, and we'll use it today to prove price of anarchy bounds on course correlated equilibria. So those are the biggest set of equilibria that we talked about on Monday.
00:00:54.954 - 00:01:47.226, Speaker A: So the hardest, worst case upper bounds to prove, and those are the ones we'll be deriving from this definition that came out of the class about five years ago. All right, I'll give you this definition about halfway through class or so the intent of the definition, again is to unify a number of pricey vanarchy analyses. So before I do that, I want to give you one more important example. So it's an application domain that's already interesting in its own right, but for day it will also serve as another key special case of the general definition. And so before doing the second price of anarchy analysis, let me just tell you what are the similarities you should be looking for compared to the one we saw last week, which in the one last week was the 2.5 for the price of anarchy in atomic selfish routing games with affine cost functions. So it's not important you remember the calculations, the details from last week's proof.
00:01:47.226 - 00:02:20.126, Speaker A: But let me just sort of remind you what the high level structure of the proof last Wednesday was. So we had an arbitrary selfish routing game, affine cost functions. We started from an arbitrary equilibrium and arbitrary optimal solution. And the very first thing we did is we said, how are we going to ever use the hypothesis that we're working with the Nash equilibrium? We said, well, Nash equilibrium means deviations only make you worse. So it seems like a good idea. We can generate upper bounds on individual player costs by considering hypothetical deviations. So which hypothetical deviation should we use? Well, sort of the obvious thing to try was the optimal solution.
00:02:20.126 - 00:02:58.690, Speaker A: You say player I, how come you're not doing what you're supposed to be doing in the optimal solution? And that gave us an inequality. So step one was invoke the pure Nash equilibrium hypothesis to derive inequalities of the following form. I'll write it in the notation of cost minimization games that I introduced on Monday. So for a given player, I so here S is the equilibrium. If it deviates, then it only gets worse. And in particular, if S star is the optimal solution. If I deviates unilaterally to what we wish we were doing in the optimal solution, it would only get worse.
00:02:58.690 - 00:03:26.026, Speaker A: And one important point which I'll mention several times today is turns out if you look back at that proof last week, this was the only time we ever used the hypothesis that S was an ash equilibrium. In effect, after we generated these k inequalities one per player, we threw it out and never used it again. That's going to be an important property of that proof. So that was the first thing we did. So second step, there's going to be four total second step, we just summed these inequalities over all of the players.
00:03:26.138 - 00:03:26.800, Speaker B: Okay?
00:03:27.490 - 00:04:08.810, Speaker A: And what was really cool about that is we had an upper bound on the individual equilibrium player cost and we sum over the players. The left hand side was exactly the equilibrium cost and in a price of energy analysis, that's exactly what you're trying to upper bound. So the left hand side after summing was exactly what we wanted it to be. Now the right hand side was weird, okay? So we did the sum and we got this sort of like entangled version of the optimal and equilibrium flows. So in the notation last week, what we observed was we had like a term with the product of f star, e and fe and we had no semantics for that. We just didn't care about it. So step three, which was kind of the hardest one, was relating the entangled term that we didn't care about to the only quantities that we actually do care about, the equilibrium and optimal costs.
00:04:08.810 - 00:05:00.490, Speaker A: So in step three, whoops, we related the entangled term, which was the right hand side after summing the two quantities we cared about the equilibrium cost and the optimal cost. Precisely. What we proved is that this entangled quantity is bounded above by five thirds times the optimal solution plus one third times the equilibrium cost. That was the precise statement. And then once we'd done that, we just solved for the price of anarchy. Okay, so we had equilibrium cost both on the left and the right. We had a one third times it on the right.
00:05:00.490 - 00:05:23.230, Speaker A: We subtracted that. We multiplied through by three halves and that's what gave us that the equilibrium cost was almost 2.5 times the optimal cost. Okay, so again, not important. You remember the details, but this is the high level structure. I'm going to show you in a different application domain how we can apply exactly the same structure to get another tight price of energy bound. And then we'll zoom out and ask what's the sort of generalization of these arguments.
00:05:23.230 - 00:06:15.830, Speaker A: And then we'll turn to applications of that generalization to robust price of energy bounds. So that's the plan. So here's the next application domain. So this is a type of location game where players are trying to figure out how to position themselves in a network. So the one I want to talk about today, there's a set f of possible locations where players can locate themselves. You can think of these as like it could be anything from where you're deploying a web server cache to where you want to build the next artisan chocolate store, as you like. There's potential customers, consumers, whatever it's called, the markets.
00:06:15.830 - 00:06:56.020, Speaker A: And in each market there's some value that they have for receiving whatever service all of these players are trying to sell. And these values are just known. It's not like in mechanism design. Everybody knows what the VJs are. So you know how much money you're going to make from somebody if you can sell them artisanal chocolates. Now there's also a cost. So for every location L and market J, there's some cost of providing the service or the product to J from L.
00:06:56.020 - 00:07:51.940, Speaker A: So in the simplest case, this could just be the distance, but in general, it's just some kind of cost. It could, for example, relate to how well your technology fits theirs. So the players, each player is just responsible for picking one location. So one element from the set, fayer I has some restricted set of permissible locations f sub i, if you want. Just think of the FIS as being f. Think of that. Any player can locate anywhere if they want, okay? But more generally, you can have a permissible locations f sub i.
00:07:51.940 - 00:08:02.690, Speaker A: All right, so in defining a game, what do I need to tell you? I need to tell you the players, told you the players I need to tell you their strategies, told you their strategies. So now I have to tell you their payoffs.
00:08:02.770 - 00:08:02.966, Speaker B: Okay?
00:08:02.988 - 00:08:36.290, Speaker A: So let me do that using an example, and then I'll be clear what the payoffs are in general. So we have the possible locations over here and the possible markets here. Let's say we have two markets, each with value three. Let's say there's three possible locations. And let's say for there's just two players. So player one can either go on the top or the middle. Those are its two strategies.
00:08:36.290 - 00:09:02.890, Speaker A: And player two can go in the middle or the bottom. That's the two strategies. So there's only four possible outcomes of the game. Two strategies for each of the two players. So let me tell you in this example how the payoffs are going to work. Basically, what's going to happen is once these players locate, they're going to engage in price competition to try to sell to these markets. I have to tell you the C's too, I now realize.
00:09:02.890 - 00:09:32.526, Speaker A: So let's say from the top to the top or the bottom to the bottom. It costs one. So these are the CLJS and from the middle it costs two to either market. And in fact, let's say it's infeasible to serve the bottom market from the top location or vice versa. All right, so let me just talk through this. So let me just talk through two possible outcomes to tell you how it's going to work. So suppose player number one chooses the top location.
00:09:32.526 - 00:09:59.914, Speaker A: Player number two chooses the bottom location. So it's symmetric. I'll just talk through player one situation. So player one, if it locates a top, it's unable to serve the second market. That's infeasible, but it can serve the top market at a cost of one. And moreover, it has no competition in the top market. Given that the player number two has located on the bottom, number one is the only firm that can actually sell to that market.
00:09:59.914 - 00:10:29.062, Speaker A: So it's just going to price at the highest price it can get away with. Now the most the market to be willing to pay is three. So the assumption is going to be that given that there's no competition, firm one sets a price of three, extracts all of the value from the first market. It costs one to serve it. So its payoff will be three. The price that it charges minus one, the cost that it pays for a payoff of two, same thing down here, exact same reasoning. So suppose instead that player number one.
00:10:29.062 - 00:10:59.120, Speaker A: So that's also going to be a Nash equilibrium in the sense that a unilateral deviation from that could only make your payoff worse. So to see why so imagine that the top player relocated to the middle. Okay, so a couple of things change now. So if you look at the second market, it used to be that the second player had no competition in the second market. It was the only one capable of supplying it. That's no longer true with player one here. It could also supply the second market, although it would cost it two.
00:10:59.120 - 00:11:34.458, Speaker A: So the way we're going to think about it is while the second player would love to continue to charge a price of three just like before, now it can't get away with it if it charged three, it could be profitably undercut by the first player. We're going to assume a player can serve any number of markets, so there's no capacity constraints on the players. So if it charges a price of three, this guy could charge 2.5 and steal away the consumer base. So price competition. Given that these two players are at these locations, well, this guy can now get away with a price of two or two minus epsilon, something like that. So it'll price just low enough that it becomes there's no incentive for this guy to enter the market.
00:11:34.458 - 00:12:06.726, Speaker A: Okay, so this guy will just take the full second market. The value is three, but it's only going to get a revenue of two and then minus its cost will be a payoff of one. Now in the middle node, it's still the case that player one has market one uncontested and so it's still going to charge a price of three. But now it pays a cost of two. Its production, its transportation cost has gone up. So both of the players'costs have dropped, payoffs have dropped to one in this case. So let me write down the general payoffs and then, and then I'll see if it makes sense.
00:12:06.726 - 00:13:00.360, Speaker A: So in general it so I'm going to continue to use this sort of S vector notation. This is just a choice of location for each player. And always in this situation, the payoffs, we're just going to handle the different markets separately. So the path of a player is its payoff from all of the markets. So what's the path from a marketing? Well, basically to get any path at all from a market, first of all, you better be located closer to that market than anybody else, okay? That means you're the unique person who can set a price low enough to get the consumers in that market. Second of all, you better be close enough that you can cover your cost with their value. So it's going to be zero.
00:13:00.360 - 00:14:02.662, Speaker A: Let's say I picks location L. So if the cost to observing J from L is more than VJ, then forget it. There's no way to have a profitable transaction. So zero if CLJ is at least FJ, or if there's some other picked location which is even closer to J, then they're going to be able to just kick you out with price competition or L not closest chosen location to J. On the other hand, if you are the closest, then you're the one in the position to actually extract the value from that market and you're going to charge the highest price you can get away with. The highest price you can get away with. Well, that's governed by who's second closest.
00:14:02.662 - 00:15:07.646, Speaker A: Remember, that's what we had here. So the price that this person could charge to this market was bounded by two because that was the next closest location or the minimum of that and the maximum this person is willing to pay the value. So this is going to be, I'll say DJ two. So this is the price you're going to charge. This is the cost that you still have to pay. And the definition of this is just the minimum you can't charge more that they're willing to pay and then you also can't charge higher than the point at which the next closest person could undercut you. So in effect, what we're saying is that as a player, the revenue that you can capture your payoff, that is, is going to be your sort of competitive advantage with respect to a given market.
00:15:07.646 - 00:15:41.800, Speaker A: So a different way of thinking about is if your cost is better because your technology is better then your payoff is going to be the extent to which your technology is better than everyone else's and your serving cost is less. Okay, so we think of players as picking these locations. Then people just charge the highest prices they can get away with and this winds up being their payoffs. If you want you can sort of derive these payoffs from first principles. You can set this up as a three stage game. You can study what are called subgame Perfect Nash Equilibria, which are appropriate for games with multiple stages. We're not going to talk about them in class, but it is covered in the AGT book if you're interested.
00:15:41.800 - 00:16:18.526, Speaker A: One other thing I'll say is we've sort of flipped maximizing versus minimizing compared to routing routing. We wanted to minimize costs here. We want to maximize payoffs. Needless to say, that's just a cosmetic difference. All right, so what do we want? So we're going to study surplus maximization. And in this context surplus is just the value provided. So the sum of all the VJs of the markets who are served by somebody minus the costs.
00:16:18.526 - 00:16:46.778, Speaker A: So minus the CLJS, everybody who gets served. So let me just write that this way. So I'm going to use the notation capital V to denote the surplus, which is a function of the chosen locations. And again we're just going to add it up market by market. And I'll write it this way, where DJ is the same thing as DJ two, except it's about the closest location, not the second closest location.
00:16:46.874 - 00:16:47.520, Speaker B: Okay.
00:16:53.330 - 00:17:30.006, Speaker A: So this is the minimum of VJ. So if the closest person is further than VJ away from you, you don't get served at all. So it's just going to be zero or the closest location to J. So for example, up here in the first outcome we looked at with one player on top, one player at the bottom, both markets are served. So that's a six minus a cost of two. So that'd be surplus four if the player moves from top to middle. Again, both markets are served.
00:17:30.006 - 00:18:27.342, Speaker A: But now one of the transportation cost has gone up to two, so the surplus would drop from four to three. Okay, so that's the description of the model. All right, so when we talk about the price of energy in games like this, we're just asking what fraction of the maximum possible surplus are Nash Equilibria guaranteed to achieve? Okay, so that's what theorem is going to be about. So any questions about the model? The description of the model exactly. Yeah, so I didn't write it down, but this is closest location actually chosen by a player and similarly here second closest location actually chosen by a player and same thing here. So if you want the unchosen ones, just think conceptually, deleting them and then do these computations. This of course is a function of S.
00:18:27.342 - 00:18:34.160, Speaker A: So this depends on which locations were chosen. I didn't write that notation. But keep in mind that S determines what these numbers are.
00:18:40.170 - 00:18:41.670, Speaker B: Possible. And secondly.
00:18:43.630 - 00:18:49.710, Speaker A: It is allowed. And what was the second question? Does it give what no, it is allowed.
00:18:50.370 - 00:18:52.800, Speaker B: Does that give the circles two or zero?
00:18:56.050 - 00:19:21.206, Speaker A: Just think about what would happen in real life. So suppose there were sort of two, suppose there were literally two stores selling the same product at the same place, right, with nothing to differentiate them. So the price competition would just drive the price down to whatever the cost of service was. Yeah. So you wouldn't expect this in natural equilibrium but it is allowed. So I mean basically both markets are going to be served at a price of two. So the surplus will be fine.
00:19:21.206 - 00:19:56.270, Speaker A: So remember surplus just says how efficiently? So first of all, how many people are actually being served and secondly how cheaply is it being done? That's the surplus. Then there's this other question of how is that surplus split between the consumers and the sellers. So how much of it is revenue and how much of it is consumer surplus. And so the consumers would love it, right, if people colocate because then the prices are going to be low and the surplus is going to go to them. But we're thinking more of sort of the firms as being the strategic players and so they are actually in general going to want to try to segment the market in equilibrium.
00:19:59.010 - 00:20:00.400, Speaker B: Why is it that.
00:20:16.650 - 00:20:57.730, Speaker A: Again surplus is just about how efficiently the allocation happens and then but the payoffs are in terms of revenue. And so if you look at the global system, arguably what you care about is just you want technology to go to as many people who want it as possible, as cheaply as possible. But then of course the firms care about how much of that surplus goes into their pockets. So there's the usual friction between those two. Okay, so here's theorem I'm going to prove. This is an old theorem of veta. So in every location game, every Nash equilibrium, and I'm not going to prove it but there can be multiple equilibrium in these games.
00:20:57.730 - 00:21:34.260, Speaker A: But every pure strategy Nash equilibrium captures at least 50% of the surplus. So obviously in general they might capture more. But this just says in every single game, every single equilibrium, the worst you'll ever going to see is 50%. This turns out to be tight. On the next exercise set it asks you to provide an example showing that one half is the optimal result. So the way I want to structure this argument is I want to first talk about three properties that these games have. And in fact these three properties are the only ones required to actually establish this bound of 0.5.
00:21:34.260 - 00:22:10.720, Speaker A: So key properties. So for the first one will seem very intuitive given everything we've said about auctions, which is just that the sum of the players payoffs. I e. Their revenue is bounded above by the surplus. Say it again. I meant to comment on that. So this is a maximization game.
00:22:10.720 - 00:22:40.022, Speaker A: You have to make some choice about which way to define the price of energy for maximization games. So we're defining it in exactly the same way as before. Objective of equilibrium over objective of optimum. So this is exactly what we wrote for Cost minimization games. For Cost minimization this was the least one. For payoff maximization, this is the most one but again good means close to one. That's always true.
00:22:40.022 - 00:23:16.974, Speaker A: Max or Min, you will see in the literature some people prefer to always have these numbers, at least one. So they'll just sort of flip the numerator denominator. So we're going to prove that this is at least a half. That is again surplus of equilibrium is at least 50% of the best possible. Okay. So as I was saying so again I want to state three properties and then given I'll prove them but they're all quick proofs and then I'll use those three to derive the bound. So the first property look at the total payoffs of all of the players.
00:23:16.974 - 00:24:00.302, Speaker A: So that is their collective net revenue. That's at most the surplus. Okay, again we saw this over and over again in auctions the revenue you extract from people can't be more than the surplus that you generate. I'm not going to write proof actually. So this is just because let's compare payoffs with the objective. So the payoff, this is sort of the cost that you incur individually and also society. And then what you as a seller take home is either whichever is smaller, the second closest chosen location to J or the maximum they're willing to pay.
00:24:00.356 - 00:24:00.538, Speaker B: Okay?
00:24:00.564 - 00:24:56.050, Speaker A: So this is always going to be at most VJ. So the sum of the pi I's is always at most v. All right? So that's the real easy one can't extract more than the surplus as revenue. So secondly we have an interesting property of these games which is there's a nice way to think about just exactly how much revenue one of these sellers does take home in their pocket. So the amount of money that you make in this game is exactly the extra surplus that your location provides to the system. So your payoff is exactly the extra surplus over what it would be if you weren't there if I just deleted your location. So this requires a proof that's just really a one liner.
00:24:56.050 - 00:25:31.950, Speaker A: Let's just sort of think this through. So the right hand side here. So imagine you have K minus one players and they pick their locations. Player K shows up with its new location. Okay. And we put that into the system. So how does the surplus change? So what's the surplus? Well it just decouples over the markets and the surplus of a given market is the difference between its value and the distance to its nearest chosen location.
00:25:31.950 - 00:26:29.454, Speaker A: So for a given J, there's two possibilities. Either this new case location is its new closest location or it's not. If it's not the new closest location, there's no change in this market. It's the same as before. If this isn't the new closest location, then the jump in Surplus is exactly the extent to which this is closer than the previous one. So the jump in Surplus, when you add a new location is you sum over the markets and you just say, well, what was the distance of the old closest location and what's the distance of the new one? Okay. And if you think about it, what do you take home in your pocket as one of these firms, one of these sellers? Well, again, you look at you sum over the markets and what you take home is exactly your competitive advantage, the extent to which you are closer to it than the second closest competitor.
00:26:29.454 - 00:27:22.898, Speaker A: So it's just exactly the same as the payoff. Property two, property three is that the welfare function? V is what's called submodular. I think many of you have probably encountered sub modularity on the homeworks, but I'll certainly remind you what it is. Oh, one aside is so this property too is a pretty nice property. It shares some of the spirit of VCG. If you recall, the way we define payments in VCG was internalizing, externalities. In some sense, the rebate that you get should be exactly the surplus that you added to the system.
00:27:22.898 - 00:27:59.694, Speaker A: In fact, this property lets you prove that these location games are potential games in the sense of Monday's lecture where the surplus provides a potential function. Okay. All right. Submodularity. So submodularity is a form of diminishing returns. So what you want to think about is you want to think about a small set of locations, then a bigger set of locations. And then the case you should keep in mind is where we're thinking about a location which is not in either of those, although actually it doesn't matter.
00:27:59.694 - 00:28:22.280, Speaker A: But this is the case that's the most relevant. And we want to say what is the added value of L to T one, and what is the added value of L to T two? And so diminishing returns just says T two. Being that you have more stuff already, the added value should be less to the bigger set, T two, than to the smaller set, T one. So what this means is that.
00:28:24.970 - 00:28:25.334, Speaker B: For.
00:28:25.372 - 00:29:29.434, Speaker A: All locations, L and sets of locations, t one and T two, again, if you look at the value added of L to the bigger set, that should be bounded above by the value that L adds to the smaller set. Okay, so this is the definition of submodular. So that's what I mean when I say this. Now, why is it true in location games? Well, it's true basically, just if we kind of recall our discussion of Surplus and we recall this equation, star so remember back in Star, we were reasoning about how the surplus changes when we added a new location. So that's very relevant here. We're thinking about adding a new location to a bigger set or a smaller set. And we argued that the jump in surplus is exactly summed over the markets, the extent to which the distance to the closest location has gone down.
00:29:29.434 - 00:30:41.150, Speaker A: Okay? And market by market, that's true. So if I just throw sort of extra locations into the system, the distance between the closest one to a market is only going down. So the value you add can only be dropping as there's more and more other people who might be the previous closest location to the market. So that is if you inspect Star, which expresses surplus increase as you add one new location, then the bigger the set t that you're adding something to, the smaller the previous d sub J's. Okay? So the closer the previous closest location would be. So that's why the net increase in surplus from adding a new location is only going down as the set that you're adding them to is going up. So those are the three properties.
00:30:41.150 - 00:31:03.750, Speaker A: So given this, we're going to prove that in these location games, or really in any game that satisfies these three properties, of which location games are one of in fact, many interesting examples, as long as these three properties hold nash equilibria capture half of the surplus. Any questions before we do that, before we prove the one half bound.
00:31:16.130 - 00:31:26.446, Speaker B: That fit with the minimum of d sub j?
00:31:26.548 - 00:32:10.254, Speaker A: And right so in the example in the first document we talked about, where there's one person on top, one person on bottom, d sub j of two was plus infinity, there was literally like, I mean, I didn't draw those edges. So basically the other person wasn't even the market. So it was the minimum of three and then plus infinity. So that's why it was a three. Okay, so here's how we derive Vettis theorem from properties one, two, and three. And this is the point at which we're going to mimic the four steps that we did, proving the 2.5 for selfish routing and the price of anarchy there.
00:32:10.254 - 00:32:29.202, Speaker A: So again, we're going to use the national equilibrium hypothesis first. Once and only once. We'll never use it again. For each player we'll ask, how come you're not doing what you're supposed to be doing? In the optimal solution? And in this case, it's payoff maximization. So we'll get a lower bound on the equilibrium payoff of each player. Then we'll sum, then we'll have an entangled term. Then we'll have to apply.
00:32:29.202 - 00:32:56.810, Speaker A: This is where the nontrivial work is. So properties one through three are mostly relevant for disentangling the entangled term, and then we'll solve for the price of energy. So we're going to do that again right here. So consider an arbitrary location game. Consider a pure strategy. Nash equilibria s. And an optimal solution, s star, optimal meaning maximizes surplus.
00:32:56.810 - 00:33:31.398, Speaker A: So step one, use the Nash equivalent hypothesis. So what does that mean? That means deviations only make people worse, only decrease the payoff. In particular if someone tries to unilaterally deviate to their strategy in S star their payoff only drops. So in equilibrium they're doing at least as well as how well they'd do if they switched to its optimal location in the optimal and everyone else stayed the same.
00:33:31.564 - 00:33:32.280, Speaker B: Okay.
00:33:34.890 - 00:34:37.866, Speaker A: For all turns out for the rest of the proof we'll never again use the assumption that S is a Nash equilibrium. This is the only time we ever use it. Second step is we sum. And again the reason we do that is because then the left hand side is something we care about is the equilibrium objective function value or almost in this case. So if we sum over all I, we get that the sum of players payoffs is at least this entangled thing we don't care about. So that's clear I hope. Now this left hand side is also not what our theorem is about.
00:34:37.866 - 00:35:09.590, Speaker A: Our theorem is about the surplus of an equilibrium and this is just the sort of net revenue to the sellers at the equilibrium. But remember, property one of these games says that you can't extract more than the surplus. So you can certainly use the sum of sellers revenues as a lower bound on surplus. And this is what we're trying to prove something about. We're trying to prove that the surplus of the equilibrium is pretty good. It's at least something else, at least some fraction of optimal.
00:35:12.730 - 00:35:20.060, Speaker B: Ah, where are we? There we are.
00:35:32.050 - 00:36:01.714, Speaker A: Okay, so that's step two. Now step three is the, is the work generally, which is how do we take a quantity we don't care about. It's this weird mixed up version of the equilibrium and the optimal solution and relate it back to the only two things we care about in this case, the surplus of the equilibrium and the optimal solution. So what we're going to prove is the following. And again we want lower bounds, lower bounds on equilibrium surplus. So we're going to take the entangled.
00:36:01.762 - 00:36:02.470, Speaker B: Term.
00:36:05.550 - 00:37:10.350, Speaker A: And lower bound it by the optimal surplus. Okay, that's a little bit too good to be true, right? Because remember, a chain of inequality starts with the equilibrium surplus and it's not going to be the case that every equilibrium is fully efficient. So minus, so it's at least the optimal surplus minus something and we're going to prove it's minus the equilibrium welfare. So it's at least the optimal welfare minus the equilibrium welfare. So step three is proving this claim. All right, so the first thing we have going for us is that we have a way to take seller payoffs and relate them to surplus in the system. So we have this kind of VCG like looking property two which says that the payoff enjoyed by one of these sellers is exactly the surplus its location is adding to the system.
00:37:10.500 - 00:37:10.814, Speaker B: Okay?
00:37:10.852 - 00:38:04.970, Speaker A: So that lets us write instead of pi's, let's just write these. So that seems like progress. So by two, the left hand side. So this is just the surplus in the entangled outcome minus the surplus if I wasn't there at all. Okay, so this is by property two. Now you look at this and you have a little bit of hope because it's sort of this summation of differences and it doesn't seem like the terms are changing very much for the different indices. So this sort of has the flavor of a telescoping sum.
00:38:04.970 - 00:38:56.010, Speaker A: Okay? Now, it's not a telescoping sum. If you look at I and I plus one, it's not the case that you get a cancellation. But the next and basically final step is to massage this so that it is a telescoping sum. And that's where we use sub modularity. Why might submodularity be useful? Well, what is this? This basically says what is the added value of the location chosen by I in the optimal solution? What is the added value of S star I given that you already have S minus I? So modularity says diminishing returns, which is we can lower bound this difference by the added value of adding S star I to any bigger set, any set bigger than S minus i. Okay, that's what some modularity says. And once you have that idea and you stare at this a little bit and you have the goal of making this telescope, you can figure out what's sort of the right set of players to add to S minus I to get the thing to telescope.
00:38:56.010 - 00:39:32.600, Speaker A: Turns out the right set to add is you want to add si back in. So I's equilibrium location plus to get it to telescope, you want to add in the optimal locations of the first I minus one players. Let me write that down. So by three, meaning sub modularity, if we look at this difference. So the added value of S star I given S minus I that's lower bounded by the added value of S star I in any bigger set in particular.
00:39:35.770 - 00:39:36.520, Speaker B: If.
00:39:38.850 - 00:40:17.290, Speaker A: I'm going to add in the optimal locations of I through I minus one, and I'm going to look at the equilibrium locations of all k players. So this is a set of K plus I locations minus again, it's S star I's location. I'm thinking about its added value. So it should be absent over here. So this only goes up to I minus one. And again, I'll have all of S. So that's by submodularity and again, really just by construction, I've chosen this maneuver.
00:40:17.290 - 00:41:14.918, Speaker A: So that the difference of welfare terms telescopes. So now if you look at the right hand side here and you compare I and I plus one, you'll notice there is a cancellation. Okay? So this term will cancel with the minus term with I plus one. So that means once you plug in this lower bound into the sum, the sum telescopes, we're left only with a positive contribution from the very first summand and its first term and a negative contribution from the second term of the last summand. That is. So the biggest of all the sets is just the two K locations, the equilibrium locations S and the optimal locations S star superimposed. This is what you get from the first summand.
00:41:14.918 - 00:41:48.114, Speaker A: And at the very end, let me have it reversed minus V of S bar equilibrium. Sorry. Yeah, so, so you have the minus term from the first sum and that's this. And then you have the plus term from the final summand. That's this. That's what you get after the, after the smoke clears, that's what's left. If you think about it, surplus is definitely monotone.
00:41:48.114 - 00:42:36.390, Speaker A: You add more locations. The closest locations to places are only getting closer, so the surplus is only going up. So I can lower bound this by just the surplus and the optimal solution minus the surplus in equilibrium as claimed. So that was the hard work. So that was the disentanglement step for location games, you can lower bound the entangled term by the difference between the optimal and equilibrium welfares. So step four, the template is now after you've done the disentangling, you just solve for the price of anarchy. And so, putting together steps one through three, we have on the left hand side the equilibrium welfare.
00:42:36.390 - 00:43:40.140, Speaker A: We have on the right hand side the difference between the optimal and the equilibrium welfare. So if you rearrange, you just get that price of anarchy, which again I'm defining as the fraction of the optimal surplus achieved by the equilibrium is at least one half. And that's the proof of Vettis theorem. Any questions about that? So, again, one point of this is just here's, another relevant application domain where it turns out it's analytically tractable to understand how close to optimal equilibrium are. And especially given that you're looking at both worst case games and worst case equilibria, it's quite pleasing that it can't be arbitrarily far away from optimal. Even for worst equilibria, it's 50%. And of course in general it will be better.
00:43:40.140 - 00:44:37.114, Speaker A: So that's point number one of these location games and that's the reason they were originally studied long ago. But the sort of higher order narrative is that they furnish a second, very nice example, the first being atomic selfish routing. So a second very nice example of what are called smooth games. And the definition of a smooth game is meant to articulate exactly what one of these four step price of anarchy proofs looks like. So this is going to be the precise formulation of what this generic recipe for proving price of anarchy bounds is. So I'm going to give you this definition and we'll observe that these are two special cases of the definition and then we'll move on to applications. So nice properties of smooth games.
00:44:37.114 - 00:44:39.040, Speaker A: Okay, so questions before that.
00:44:41.570 - 00:44:42.320, Speaker B: Okay.
00:44:44.370 - 00:44:46.240, Speaker A: All right, so.
00:45:01.050 - 00:45:01.800, Speaker B: It.
00:45:13.690 - 00:45:26.010, Speaker A: Give you separate definitions for cost minimization games like selfish routing and payoff maximization games like that location game. But they're pretty much obvious analogs of each other. So let's start with cost minimization games.
00:45:26.130 - 00:45:26.800, Speaker B: It.
00:45:37.170 - 00:46:42.462, Speaker A: So there's two parameters, lambda and mu. These corresponds to the two coefficients in these disentanglement arguments that we've been looking at. So they're going to correspond to five thirds and one third in the disentanglement argument from last week. They're going to correspond to one and one in the disentanglement argument I just showed you here. So lambda, mu, smooth, and for the cost minimization case, we need mu less than one if the following property holds basically if you can disentangle. So if the entangled version of an outcome s star and an outcome S, again, what do I really mean by entangled? I mean a vision starting with the k vector, the strategy profile S, and then think about all the single component changes you could make, okay? You could go to its ith component and you could change it into some other one s star i. So starting with the starting point of S, there's k different vectors you can get by changing a single component.
00:46:42.462 - 00:47:13.614, Speaker A: These are those k other outcomes that in some sense are at hamming distance one. And we just look at the cost of the deviator, the person who changed and sum them up. This is the entangled term that we kept seeing. Again, we don't care about this per se, we just want to relate this to what we do care about. So lambda mu smooth just means you can relate this back to the cost of S star with the coefficient of lambda plus mu times the cost of.
00:47:13.652 - 00:47:14.240, Speaker B: S.
00:47:17.090 - 00:47:46.700, Speaker A: And here if you're wondering what this cost operator is, this is just any objective function which is bounded above by the sum of players payoffs. So where cost of an outcome is an objective function, satisfying this inequality with selfish routing that's held with equality. Those were our various expressions of the total travel time. Oh, I almost forgot the most important thing, which is this should hold.
00:47:48.750 - 00:47:49.162, Speaker B: For.
00:47:49.216 - 00:48:30.710, Speaker A: All outcomes s and S star. Okay, we may have in mind as a particular instantiation of this inequality, we may be thinking or guided by the case when S is an equilibrium and S star is an optimal solution. But that is not what the definition says. The definition says for every pair of outcomes, no matter what they are, you should be able to disentangle. So for example, in a selfish routing network, any two pairs of traffic patterns arbitrarily crazy traffic patterns. This inequality should hold not just under the assumption that one is an equilibrium or the other is an optimal solution. So that's what it means for a game to be smooth.
00:48:30.710 - 00:49:43.182, Speaker A: And then for payoff maximization there's an analogy. So now the entangled terms are in terms of payoffs. Now because you're maximizing you want a lower boundary and again because you're maximizing you're going to subtract off an error term rather than add an error term. And again this should have the property like in the location games where the objective function you care about can be bounded below by the sum of player payoffs. And again this is for all SNS star. So that is the definition of a smooth game. So given any pair of outcomes s and S star if you form this entangled version where you take S as a starting point look at the k ways to massage it in one component toward S star.
00:49:43.182 - 00:50:20.220, Speaker A: Look at the cost of that massaged outcome and you sum those up. You can bound those against a linear combination of the two outcome costs s and S star. That's what it means. We've seen two examples. It won't necessarily be obvious to you but you should go back to your notes and verify this. If you go back to our proof one week ago that the price of anarchy in atomic selfish routing networks with affine cost functions is five halves. What we actually proved in step three in the disentangling we actually proved that every such game is smooth with the parameters five thirds and one third.
00:50:20.220 - 00:50:56.866, Speaker A: And the reason that's what we actually proved is that in step three while we had in mind that S was an equilibrium and S star was an optimal solution we never used that fact. In the disentanglement argument that was all just algebra. Remember we had this sort of y times z plus one as amongst this five thirds y squared blah blah blah blah. That was just about numbers, it wasn't about equilibria. So we proved the disentanglement for all pairs not just for equilibrium. Similarly in the proof I just gave you it was only in step one we used that S was an equilibrium and in the disentanglement step that was just algebra. Submodulularity telescoping sums had nothing to do with equilibrium.
00:50:56.866 - 00:52:03.346, Speaker A: So we actually verified these conditions in both of those examples. All right so that's fine. So this is just all I've all you should be convinced of now is I've given you sort of a common language to talk simultaneously about multiple price of anarchy proofs which is nice but it's not necessarily clear that it's good for anything. So in the rest of the lecture I want to explain some of the things that it's good for. I want to tell you about when you actually wind up verifying these smoothness conditions automatic and interesting consequences that those games enjoy. So let me begin by connecting things to the main topic of Monday's lecture which was our hierarchy of Equilibria pure strategy Nash Equilibria mixed strategy Nash equilibria correlated Equilibria and coarse correlated equilibrium. The theorem I'm about to state is about the biggest set coarse correlated equilibrium.
00:52:03.346 - 00:52:25.520, Speaker A: So let me remind you of the definition and it plus mu.
00:52:26.050 - 00:52:27.550, Speaker B: Why is there a minus.
00:52:29.250 - 00:52:29.678, Speaker A: In the.
00:52:29.684 - 00:52:32.742, Speaker B: Payoff lambda of v minus mu v?
00:52:32.876 - 00:52:33.382, Speaker A: That's right.
00:52:33.436 - 00:52:36.070, Speaker B: And cost is equals lambda cos plus mu plus.
00:52:36.140 - 00:53:17.406, Speaker A: That's right. The reason is just, I mean it's just because of max versus min, right? So like imagine if lambda was one, which it actually was in the location game example. Okay, so imagine lambda was one and mu was zero. Okay? That would imply something incredible which is that equilibrium are fully optimal. Okay? So the mu term lets you say something weaker and says, okay, well we're not going to claim that the equilibrium is the most one times an optimal solution. We're going to claim that it's the most one times the optimal solution plus or minus an appropriate error term. Now if it's something that you want to be small then the error term is something you'd add that would weaken your statement.
00:53:17.406 - 00:53:25.880, Speaker A: So that's the reason for the plus mu times cost. If you're trying to lower bound the cost of an equilibrium and you want to have a weaker statement which is actually true, then you want to subtract something off.
00:53:26.330 - 00:53:29.926, Speaker B: Second thing is that the condition on the mu less than once are the.
00:53:29.948 - 00:54:23.106, Speaker A: Same for a perfect no, it's not actually. But again I wouldn't necessarily worry about it's, not for interesting reasons. I mean you'll, you'll see exactly the mu less than one will become clear in a SEC for the cost minimization case. So think of these as despite some minor syntactic differences, think of them as basically just being exactly the same thing from minimization maximization. So when you work it out, this is just the appropriate version for the two cases. Okay, so we had this hierarchy of equilibrium concepts and so maybe just remind you of the motivation. So again, pure strategy nash equilibria may not exist, including in games that we care about like selvish routing with weighted players.
00:54:23.106 - 00:54:52.978, Speaker A: Mixed Nash equilibria always exist but are hard to compute. Okay, so that motivated looking at even more permissive equilibrium concepts. Correlated equilibrium were easy. You can find one in polynomial time. There are distributed learning algorithms that drive joint play to that set. Coarse correlated equilibrium are even easier. In a couple of lectures I'll actually show you the very lightweight learning algorithms that drive joint play into this set.
00:54:52.978 - 00:55:41.646, Speaker A: That's one of the topics coming up soon. All right. But so the point is course correlated equilibria being so big and so permissive are a pretty plausible, relatively prediction of what might be happening in a game. A lot of outcomes are course correlated equilibrium. So it's just sort of a very relatively weak prediction about what's actually happening. So what's the definition? So a distribution sigma over the outcomes of a game. So over the product space a one times, a two times decay as usual, you think about all potential deviators.
00:55:41.646 - 00:56:28.350, Speaker A: You think about all potential deviations si prime. Here AI is the strategy space for player I. And what you compare is you say well, suppose I always played as recommended by sigma. So if we envision sort of drawing an outcome at random from sigma so then I's cost is a random variable, depends on the chosen outcome. But we can look at its expected cost, it just plays according to sigma's draw. And we compare that to how well I would do if instead it deviated unilaterally and unconditionally whereas everybody else continued to follow s. So that would be the expected value where I switches to si prime.
00:56:28.350 - 00:57:13.386, Speaker A: This is deterministic, this is random, this is being chosen from the distribution sigma. This is a fixed pure strategy deviation. And if that inequality holds for the expected cost of every player, for every deviation by a player, then it's called the coarse correlated equilibrium. And again, generalizing all of the previous equilibrium concepts that we saw. And again the point that we concluded with on Monday is that because the price of anarchy takes the worst case over all equilibria, as you widen the set of equilibria, the good news is that you're more likely to have existence, in fact you always have it. As soon as you're at mixed strategy national equilibria, you're more likely to be tractable and indeed you're already tractable at correlated equilibria. But the price of anarchy only gets worse because there's only more equilibria you're trying to bound.
00:57:13.386 - 00:58:08.160, Speaker A: So the strongest type of price of anarchy arguments abounds with respect to this picture would be for the biggest set for the course correlated equilibria. And so what's cool is if a game is smooth then even though in your mind perhaps you merely thought you were proving a bound on the price of ante of pure Nash equilibria. When we were talking about routing games, we never talked about randomization, we just sort of proved with our bare hands this 2.5 in this location game, we didn't even talk about mixed strategy equilibrium, we just talked about pure strategy natural equilibrium. We proved a zero five. We inadvertently, it turns out, played prove that exact same price of energy bound for all of these by verifying the smoothness condition. So, formally, here's what's true.
00:58:08.160 - 00:59:11.730, Speaker A: I'll just state the cost minimization version, but the other one's analogous. So in every land and mu smooth cost minimization game. And again mu should be less than one. The price of anarchy is the most lambda over one minus mu. Okay? So just to help you relate to this formula. So in selfish routing, we verified the smoothness condition with lambda equals five thirds and mu equal one third. And so therefore lambd over one minus mu is five point is five halves.
00:59:11.730 - 01:00:30.800, Speaker A: Okay? So the way to think about it is not so much lambda over one minus mu per se. The way to think about it is we proved a bound in our mind just for pure equilibria. But because we proved it in a particular way following this canonical recipe that exact same number whatever it is we get it for all of the sets, all four of the sets it holds all the way out to course correlated equilibrium. So in this sense it's a type of price of anarchy bound that extends automatically from pure equilibria all the way out to course correlated equilibrium. So for payoff maximization the only thing that changes is mu no longer needs to be at most one and it's now going to be a number less than one lambda over one plus mu and so again for the location games that we just discussed lambda was one mu was one and that's the one half. So again the point is we thought we were proving a one half for pure equilibria we actually got it for all of these equilibrium questions about what the theorem is saying. So I'll give you a proof but frankly the proof the proof once you sort of think it might be true the proof writes itself as you'll see.
01:00:30.800 - 01:01:06.554, Speaker A: So again, in particular, this tells us two examples with good price of energy bounds even for course correlated equilibria, which again means even say for agents who are distributively trying to play this game using very simple learning strategies. Says, even though they may never converge to a Nash equilibrium, because they're going to be in this big CCE set, these bounds of 2.5 or one half still apply. And these are not the only two examples they're the only two I'm actually going to show you in lecture but there's many other examples known at this.
01:01:06.592 - 01:01:07.180, Speaker B: Point.
01:01:11.310 - 01:02:26.062, Speaker A: So proof and again we can just really just follow our nose here so what are we trying to argue about? We're trying to say that coarse correlated equilibria are near optimal. Okay, so consider I'll prove the cost minimization version. So consider an arbitrary lambda mu smooth cost minimization game. Consider an arbitrary course correlated equilibrium. And what we want is we want an upper bound on the cost of this course correlated equilibrium. Now, this thing's randomized, so when I say cost, I really mean expected cost and expected where we just look at the distribution over outcomes that sigma is doing, okay? So think about remember like the traffic light where sigma picked 50 50 between two outcomes so they may have different costs we're just averaging over all the things that sigma might pick all right? So again we can just think about all right what do we got going for us, what are our hypotheses? Right? And we don't have many because now things have gotten pretty abstract.
01:02:26.126 - 01:02:26.418, Speaker B: Kind of.
01:02:26.424 - 01:02:52.010, Speaker A: All we know is that this is a coarse correlated equilibrium. So we've got this to work with, and we know the game is smooth, so I guess I've erased it. But we have the disentanglement inequality to work with as well. All right, so nothing's entangled yet, so it seems sensible to start here. So that's about individual player incentives. Okay, so this is about joint costs. But really we want to talk about individual player costs.
01:02:52.010 - 01:03:42.890, Speaker A: But remember we're assuming that the joint cost is bounded above by the individual player costs. And again this helped with equality for selfish routing. But in general, as long as it's player costs or an upper bound we're fine. So this is starting to look sort of like what we want. But the equilibrium condition doesn't apply to the joint cost, it applies to an individual cost. So we'll just use linearity of expectation to take the sum out here. And now the CCE condition is looking pretty relevant.
01:03:42.890 - 01:04:42.958, Speaker A: So this is the expected cost of player I in this course correlated equilibrium. By definition, in a course correlated equilibrium, if you make any unconditional deviation you only get worse. Okay, and again, which deviation should you try? Well again the optimal solution provides one to try. So we could think about as a thought experiment player I, just deterministically picking its strategy s star I in the optimal solution by the CCE condition its cost would only go up. So I'm really applying this condition once for each player where S star I is playing the role of S prime I in the definition are starting to look entangled. So now Smoothness is starting to look like maybe it's the appropriate tool. Now.
01:04:42.958 - 01:05:31.280, Speaker A: Smoothness. I guess I've erased it. Let me put it back up there. So Smoothness says this so Smoonis talks about an entangled version meaning the sum of players costs and these entangled outcomes. So that's not quite what we have here. Here it's just sort of about a single player and its experience in a particular entangled outcome. But again, linear of expectations lets us focus instead on the expected value of the entangled term that we want.
01:05:31.280 - 01:05:36.190, Speaker A: So let's just move the sum back inside the expectation.
01:05:51.050 - 01:05:51.800, Speaker B: It.
01:05:59.850 - 01:06:52.840, Speaker A: Now we're in business, right? This is exactly the entangled term that Smoothness allows us to relate back to things that we care about, the cost of S and the cost of S star. Now notice, it's very fortunate that we can disentangle any outcomes, any outcome, S, not just Nash Equilibria, because if you look inside here, what do we know about this outcome S? We know very little. Okay, so sigma is some distribution over outcomes, okay? You're doing this with 3% probability, this other outcome with 17% probability, et cetera, et cetera, et cetera. There's no guarantee that these things are Nash Equilibria. These could be anything. The only thing we know is an equilibrium condition on average over the outcomes. We do not know that any individual outcome that we're averaging over is an equilibrium itself.
01:06:52.840 - 01:07:41.974, Speaker A: So the fact that we've proved this not just for Equilibria but for all outcomes, s gets used exactly in this part of the proof. S is random but I don't care what it is, doesn't matter what it is. I can always relate this back to a linear combination of S and S star. So, by smoothness, we get that expected value of this convex combination. Now, S star, this is just the optimal solution of the game. This is deterministic. So there's no randomization over S star.
01:07:41.974 - 01:08:27.650, Speaker A: The game is fixed, the optimal solution is fixed. So, this is just lambda times the optimal cost. Now, this is what's random, this is what's being chosen from sigma. So, we can take the mu out times the expected value of this random outcome. So, now, where did we start? We started with the expected cost under this course, quote equilibrium sigma. And we've said that that is bounded above by lambda times the optimal cost plus mu, where mu is less than one times the same thing. We started with the expected cost at equilibrium.
01:08:27.650 - 01:09:02.340, Speaker A: So now we just solve for the price of energy. We subtract this from both sides. It gives us a one minus mu on the left hand side and we divide through. And that gives us that the expected value of the coarse correlated equilibrium is the most lambda over one minus mu times the cost of the optimal solution. So that is why in smooth games you automatically get bounds for torque square late equilibrium. Questions.
01:09:05.430 - 01:09:07.700, Speaker B: What is the cost of S? If not.
01:09:10.630 - 01:10:09.206, Speaker A: It can be an arbitrary upper bound. So, like in the location game, whereas a payoff maximization game, we observed that the sum of the player payoffs was not necessarily equal to the surplus. It could be less. So, it's whatever you want it to be, but as long as the inequality holds, then the proof is correct. Other questions so, there's a bunch of other things, properties that smooth games have automatically. I won't have time to tell you about many, but let me mention one other one that's simple, which is to an orthogonal set that looks like that, which is approximate pure strategy nash equilibria. So, I'll tell you exactly what I mean by that.
01:10:09.206 - 01:11:18.460, Speaker A: There's multiple ways to define approximate Nash equilibria, but the upshot is that in smooth games, as you relax the equilibrium condition, the price of anarchy, as you'd expect degrades. In particular, if stuff is arbitrarily weird, arbitrarily non equilibrium, there's nothing you can prove about it. But as you relax the equilibrium condition, the price of anarchy bound degrades gracefully in smooth games. So precisely it again, I'll just state it for cost minimization, but there's an analog for payoff maximization, so S and this is now just a deterministic outcome. Although you could of course have analogs for all the other equilibrium concepts if you prefer. So, S is an epsilon approximate pure strategy in ash equilibrium. If for all players I and offer all deviations, well, maybe there's a deviation that makes you better off, but not by much, okay? Just by the one plus epsilon factor, that's what it means.
01:11:18.460 - 01:11:31.570, Speaker A: So, the cost of. Equilibrium. Maybe it can be strictly bigger than something with a deviation, but only by one plus epsilon.
01:11:34.310 - 01:11:35.060, Speaker B: Okay.
01:11:37.350 - 01:12:42.118, Speaker A: So that's one natural definition of an approximate equilibrium. And following this exact same sort of derivation, in fact, an easier version, because there's no randomization in this version. So if a game is Landon mu smooth and S is an epsilon approximate pure Nash equilibrium, then the cost of this approximate equilibrium divided by the cost of the optimal solution. Well, we're not quite going to get our lambda over one minus mu bound. That'd be a little bit too good to be true. Because we're allowing some violation in equilibrium. We expect the price of energy to jump up beyond lambda over one minus mu.
01:12:42.118 - 01:13:25.442, Speaker A: But we just pick up a one plus epsilon on the lambda and also a one plus epsilon on the mu. And this is meaningful, assuming that this denominator is positive. So that's going to be epsilon is at most one over mu minus one. And again, once it occurs to you the definition might be useful, proofs like this just sort of write themselves. That's what we're going to ask you to do on the problem set that goes out on the exercise set that goes out today. That's it for smooth games. Have a good weekend.
01:13:25.442 - 01:13:25.890, Speaker A: See you Monday.
