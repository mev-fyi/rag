00:00:05.560 - 00:00:45.338, Speaker A: Okay, it looks like we're live. Hi, everyone. I have no idea how to host this type of thing, so I'll just introduce the participants. We have Rati Galashvili from aptos. We're waiting on Sascha Spiegelman from aptos, who are both co authors of the Block STM paper, and we will dive into what that means in a bit. And we have here me, I work at Starcore, and we have Noah, who is a team lead who led the effort on implementing a variant of block STM for StarkNet V0132. So today we'll mostly be focusing on the topic of parallel execution, both in 0.1,
00:00:45.338 - 00:00:58.117, Speaker A: 3.2 and in theory. I think that's a reasonable introduction. Probably wouldn't get kicked out. Do you know if Sasha is coming soon? Should we wait for him?
00:00:58.261 - 00:01:03.157, Speaker B: I think Sasha should be here in a matter of like a minute or two. That's all right.
00:01:03.341 - 00:01:35.535, Speaker A: Okay, cool. So I think, first of all, this is weird because we don't know if we're alive. I've never done this before, so I hope this is working right. So I think the first thing I want to do is just to give a very, very quick intro about concurrency and differentiate between optimistic and deterministic concurrency. And then we can ask you guys what block STM is. So our context is blockchain. We have an ordered list of transactions that is ordered according to some fee or according to some value.
00:01:35.535 - 00:02:01.597, Speaker A: We want to execute this list as fast as possible. And we have two approaches. One of them is given no additional information. We try and run the transactions concurrently and hope that it works. Sometimes it will fail if there are shared storage dependencies. And then we need to try again, and trying again in an intelligent way is the realm of optimistic concurrency. Algorithms for concurrent execution.
00:02:01.597 - 00:02:37.674, Speaker A: And then you have a different variant which assumes that you have more information. So if you have a list of transactions and you have more knowledge about their state dependencies, then sometimes you just know that you can compute them concurrently, and then you don't need to retry ever. So we will only be focusing on optimistic concurrency. So we assume that you have no extra information given a priori. You just have a list of transactions that you want to execute. That's the focus for two reasons. One is that's the algorithm we're talking about, and it's also what's implemented in version 0.1
00:02:37.674 - 00:02:45.065, Speaker A: 3.2. And that brings us to the question, what is block stm? So what's block stm? Rati?
00:02:47.125 - 00:03:39.509, Speaker B: Hi. First of all, yes, happy to be here. Yeah, I was. Regarding this particular decision or design principles of whether we wanted it to use any hints, I think our thinking process in the beginning was that, I mean, there are parallel execution engines even for blockchains, where presumably it may not be so complex to provide or like the framework, like can help you somehow, or you can do static analysis or. I've even seen people argue that they prefer specifying things because they know what happens. But I do think that for this there are better ways. Like, it just feels like me saying if I go and program something in assembly, I feel a lot more confident because I know the instructions that are executed.
00:03:39.509 - 00:04:24.633, Speaker B: Right. But not to take away from it. We just didn't want to make extra assumptions that we didn't feel confident about that we would know this, for example, the right set. But then the other side of the story is that if you want to run arbitrary smart contracts and they get complicated and we want atomicity, you can't avoid conditionals, I suppose. How can we predict what is going to be the workload? And you want to have an algorithm that puts you in the most flexible position as the space evolves. And this was actually one of the biggest reasons because if you think about it, there was this thing that we informally called secondary indexing. It's a really bad name.
00:04:24.633 - 00:05:15.017, Speaker B: But essentially anytime when you make a decision based on the previous read that you did, you are already in uncharted territory. It's hard, hard to know like estimate exactly what's going to happen. You might end up overestimating. Or for example, the easiest example is now it's like, you may like, I think we support on chain randomness now, but imagine you have random access to randomness, right? And you have like 10 buckets and you're going to access one of them at random and the next contract is going to access one of them at random. We would have, I started engine would kind of have to say that I might access all of them, right? Because otherwise you're risking that. Like you could think about like if you are okay with transaction aborting sometimes and so on. Like I don't want to dive too much into that.
00:05:15.017 - 00:05:45.747, Speaker B: But suppose you don't want to. Like you want your transaction to succeed. You have to say like sort of the worst case of what might happen. And then these worst cases can have intersections, right? Like every. Our CTO actually said, like this is some sort of sharing. And it's actually pretty neat because a lot of the intuitions from like normal concurrency algorithms and so on we see sort of manifest themselves in different ways in a blockchain setting. So in this setting, we would like, the transactions would have to be scheduled, like sequentially.
00:05:45.747 - 00:06:10.205, Speaker B: Right. By any sort of deterministic static scheduler. Because it would see that, well, you might access this, I might access this. I have to be careful. And with block stm, you would literally find out exactly what the conflict, if the real conflict happened, you would have some slowdown, but with sort of one like 90% of the chance, you will not have a conflict. Right. So I think that was our reasoning.
00:06:10.205 - 00:06:18.465, Speaker B: I think you asked me, what is block stm? And then I just answered the concurrency related question. But maybe I'll let Sasha continue.
00:06:19.125 - 00:06:35.447, Speaker A: So, hi, Sasha, first of all, and just to loop you in, we introduced by differentiating between optimistic and deterministic concurrency. And then indeed I asked Rati, what is block stm? But he answered a different question. So maybe you want to answer this one.
00:06:35.631 - 00:06:36.023, Speaker B: Yeah.
00:06:36.079 - 00:06:54.649, Speaker C: Asking such an open is a mistake, Ilya. Okay, you need to be more specific. Yeah. Block stm. Block STM is a power execution engine designed for blockchains to leverage dynamic parallelism. I think that's the short answer. You want a long answer, you ask Vati.
00:06:54.649 - 00:06:55.325, Speaker C: Yeah.
00:06:58.625 - 00:07:03.885, Speaker A: I think we want a slightly longer answer. For example, what do you mean by dynamic?
00:07:05.265 - 00:07:06.521, Speaker C: Okay, Vati, you want to explain this?
00:07:06.553 - 00:07:20.841, Speaker B: Yeah, I can. Sorry. Yes, I can take a step back there, in fact, before jumping in. So the idea is that we have. Yeah, this setting has been explored in sort of many cases. Right. There is software, transactional memories.
00:07:20.841 - 00:07:50.225, Speaker B: This is where the STM comes from in the name. But there is also databases and deterministic databases and et cetera, et cetera. And while writing the paper, we had a background on STMs, but we discovered that everybody has their own terminology. Everybody does things slightly differently. Sometimes they invent the same things, call it different names as usual. Again, not to get lost into these details and naming things. The idea is you have transactions, you want to execute them.
00:07:50.225 - 00:08:29.901, Speaker B: There are different models, but at the end of the day, we have to have a serializable schedule. Moreover, block STM executes them according to some fixed order. As long as it's deterministic, it will just do its best to give you the result. That's the same as if you executed transactions one by one in some fixed order, not deterministic order. That would be like every time you execute, there would be an order, it would be the same, but it would not be the same as you knew ahead of time. In block SDM we know ahead of time essentially the order we can have some other heuristic to decide like what's a good order. But the order is fixed once you execute the transactions.
00:08:29.901 - 00:09:25.347, Speaker B: Like the name of the game is just get to the like generate me the state that corresponds to executing this transaction sequentially one after another and do it as soon as possible. So it's actually also related to like thread level speculation techniques like a little bit what happens in the processors, right, where the try to do things like out of order, but at the end of the day you should still be consistent with the order. So you are trying to essentially just extract as much parallelism that the workload actually allows you, right? So that is kind of what block SDM strives to do. And the dynamic versus static, like in other words, like a bunch of the database papers. For example, assume that you can overestimate, overestimate the write set. At least I think that's the typical thing. Now they used to do the reads, but now there are techniques that you don't need.
00:09:25.347 - 00:10:01.873, Speaker B: The reads, you can be flexible, but the writes they assume most of the time. And it makes perfect sense because these are like storage procedures. They are not like you can analyze them much better. You sort of have more context about how they're running, where they're running. I think for blockchains, as far as I understand like the vision for like how smart contracts should evolve, it should be like arbitrary like purpose and potentially complicated, potentially complicated control flows. Turing complete like programming languages. So this is why we didn't want to assume that.
00:10:01.873 - 00:10:45.005, Speaker B: I mean almost everything can be made Turing complete. But just trying to sort of give an intuition why. Again, why we didn't want to assume that we had any extra knowledge than just a set of transactions. One more note is that if we do have to have the extra knowledge, it is actually easy to incorporate it. For example, when we were building the paper, when we were building the original prototypes, we had access to static analyzer for move. And once we would get the hints from it, the execution would become significantly faster. But we just would need to incorporate this into our data structures that we know that this transaction is going to write here.
00:10:45.005 - 00:11:31.065, Speaker B: However, at the time to run the pipeline of the static analyzer and block SDM was slower than just running block SDM and let it figure out this stuff on the fly. But this may not be true all the time, right? There might be other techniques and I think the truth will be somewhere in the middle where the most optimal system, where any information you can get for chip that is not going to be worst case and introduce false call flake check. It would be great to integrate and it could also help you help like whoever is using the protocol to order the block better, for example, such that it's more applicable to parallelization. And then block STM comes and just tries to figure out on the fly and do the best job possible.
00:11:32.005 - 00:12:04.063, Speaker C: Yeah, Just to add to it quickly, I think for me a big difference between static and dynamic parallelism is how you handle the conflicts. And in static parallelism you kind of assume that you know everything in advance. And if you had a way to know perfectly everything in advance, then yeah, then you could do like probably the best you could do. But. But like Lattice said, that's possible. That's very hard to know. And usually you have, you have overestimation and if you have overestimation, then it's that then yeah, that then, then you usually.
00:12:04.063 - 00:12:36.565, Speaker C: It's better to have a dynamic parallelism because then you can on the fly understand the conflicts, you don't need to assume the worst and then you only consider the real conflicts and not the overestimation conflicts. And then you end up with aborting much less transactions and better overall performance. I think this was what he was mentioning. If the smart contract becoming complex and the flow control is complex. A lot of if statements. So if you have an if statement, you cannot possibly know which branch you're going to take. You need to overestimate everything and in this case everything conflicts.
00:12:36.565 - 00:12:44.077, Speaker C: In block STM dynamic parallelism you see on the fly which branch you took and then you know that whether it's a conflict or not with the other transactions.
00:12:44.141 - 00:13:16.089, Speaker B: So it doesn't mean that this cannot be optimized. Right. Like this is what I was trying to say. Like there is ways where we could try to optimize the model. It doesn't need to be completely blind, but it can fare pretty well even in that case. Right. But it is much harder to extend a static execution engine to do speculation and deal with the unknown than it is to extend something that doesn't require that to perform faster when you have more information.
00:13:16.089 - 00:13:28.285, Speaker B: Right. Like so that was one of our design principles, just to be as flexible as possible. Does that make sense? We are also new to this.
00:13:29.265 - 00:13:49.083, Speaker A: No, it makes perfect sense. We didn't actually think. Yes, we didn't think how much overhead it would not overhead how much worse it would make the parallelism if you overestimate the dependencies so drastically. So it's interesting to Know what you had in mind?
00:13:49.179 - 00:13:58.775, Speaker C: Look at the extreme. I can overestimate everything. Right. I can lock the entire right set and everything is going to conflict. And you don't have parallelism. Right. So this is the extreme.
00:13:59.635 - 00:14:13.763, Speaker B: Yeah, I think this was sort of the. On chain exchanges, like things like that was like a very good example where, you know, you have potentially a lot of accounts and you don't know like which ones are going to settle, for example.
00:14:13.819 - 00:14:50.755, Speaker A: Right. I think the principle is clear. We have a relatively concrete question. So in starknet, the state is account based and the fee Token is an ERC20 token. And that means that all of the transactions, when they pay fee, they go through a shared storage slot, which is the fee of the sequencer that builds the blocks. And without addressing this case specifically, you wouldn't be able to parallelize anything. So first question is, do you have this type of problem in aptos or is the state organized differently?
00:14:52.105 - 00:14:54.325, Speaker C: I think it's the best question you could have asked.
00:14:58.145 - 00:15:06.405, Speaker B: We were writing introduction about this two days ago, the whole night. Right. So yes, it's a good time for this. Sasha, you go first. I think.
00:15:06.905 - 00:15:31.777, Speaker C: Yeah, we had the same problem. Basically we wanted to track total gas supply and every transaction burn gas. So in order to track gas supply, you need to go and access the shared counter and remodified it. And what we came up with is an extension to block stm. This wasn't in the original Block STM paper, but I don't know if you saw this work. We call it aggregators. We advertise aggregators.
00:15:31.777 - 00:15:43.241, Speaker C: The paper is called Data Lane. So this is basically a way. Yeah, Data Lane. Yeah, it's your name. So don't make fun of me. It's Delta Lane. Sorry, Delta Lane.
00:15:43.241 - 00:16:05.001, Speaker C: Yeah, because it's Deltas. So the general idea, just to give some intuition is that a lot of many times you want to. Like for example, a global counter, like you said, right. The operation is you don't care about the. It's read modified, but you don't care about the read. The only reason you care about the read is because you know you need to know what to write back. Right.
00:16:05.001 - 00:16:40.397, Speaker C: You don't do otherwise. You don't do anything with the read value. So what you can do is that you can defer the writes. You can, you can. Instead of like the simple way to think about is that is just instead of read, modify, write, you're just going to do write, you're gonna do and you're gonna do. You're gonna record Somewhere you're gonna add five to this to this number and some of another transaction will recall they want to add 10 to this transaction to this, to this, to this counter and then they don't conflict via execution. In the execution at the end you are gonna go and materialize and just sum everything together into the correct, correct value of the counter.
00:16:40.397 - 00:16:59.205, Speaker C: But during the execution there was no conflict because the read is unnecessary. So there is no read write conflict. You avoid the read write conflict. But yeah, so this concept is actually much more general and we have like much more use cases. And if you're interested, I think. Yeah, Mati will be happy to explain.
00:17:01.025 - 00:17:08.285, Speaker A: Before, before you go on. So. No, does this sound familiar? How do you. How do we resolve this problem in. In 0.132?
00:17:08.745 - 00:17:25.054, Speaker D: Yeah, basically I think it's the same solution as you suggested. We defer the writes to the commit phase and we only write to the balance of the sequencer at the commit phase and see if the transaction is still valid.
00:17:25.474 - 00:17:41.745, Speaker B: Yeah, yeah, let me. Yeah, that's, that's. Yeah, exactly. That's like a natural. And exactly the solution that we took as well. Right. But there is actually some interesting things about that that we can share that you know, we still keep discovering.
00:17:41.745 - 00:18:18.447, Speaker B: So originally we developed this when we were preparing for the mainnet launch and we had this extremely, extremely great intern that now works with us at Aptos Labs. And so. And we did that. Exactly. And like a very important property there that probably you also are using is that we don't expect these counters to overflow or underflow. The reason we don't expect them to underflow is because you cannot burn something that hasn't been minted before. And the reason it cannot overflow is because you have sufficiently wide integer or something.
00:18:18.447 - 00:18:40.839, Speaker B: Right. Because this is the total number of tokens. It's not like just some arbitrary value. And yeah, and with that if you just defer the computation, it just works. However, I mean you do have to be sort of careful there. Right. Like everything in this kind of systems because like transactions can kind of succeed, especially like, and then they can be aborted.
00:18:40.839 - 00:19:42.411, Speaker B: So it's really important to do this at commit time. But then we like we were trying to write a paper about this, then we extended the system and I'm going to mention something about this in a few words. But then we discovered that like two months before George started his like his internship, there was a paper that actually suggested this in databases. But our background is not databases and we don't follow every single Paper that get published there, but the moment we got appointed to it, and it's even a thesis of someone at Harvard, one of the optimizations that they propose is very similar, like deferring the read, modify, write stuff. But then recently we have a more powerful version, like an evolution of this system, which might be interesting for you too, since you have a similar solution for the counters, which is basically for counters. It would allow you to tolerate, let's say, overflows and underflows as well. And that would matter.
00:19:42.411 - 00:20:24.415, Speaker B: Let's say if you have limited supply of tickets and you want to mint them in parallel, but you don't want to mint more than some number. If you only discover this after commit, it would be hard to deal with. You can re execute the block, maybe. But essentially what it does is branch predict what's going to happen and then try to like fill it in. And it's sort of incorporated in block SDM such that you can, like most of the time, you'll be correct. The transaction can do everything. There is an object that basically wraps all this logic that will need to be deferred with the corresponding constraints, right? And then the constraints are evaluated during the validation time.
00:20:24.415 - 00:21:12.115, Speaker B: But it's also more general than that, right? I think we just implemented it for the counters for now, because that was the most important. But it doesn't even require commutativity. It doesn't require anything. You just are deferring some computation. And you can, for example, I really like this example. You have a checking account and you want to put half of it on a saving account, but you don't want to perform a read. This can also be a deferred computation because as long as you have a language to express what's happening, you can sort of clone this object and then you can divide both of them, one by ceiling division, one by floor division, so that you do not create or remove tokens.
00:21:12.115 - 00:21:45.085, Speaker B: Right? And then there will be no read because it will sort of be a deferred split. Deferred division. Deferred division, right. And the reason this is kind of interesting is because it, it is clearly beyond postponing a read modify, write operation. Even that paper only did counters. But theoretically read, modify, write operation can be anything like read and do an arbitrary function and then do the modification. But I think you can do even more.
00:21:45.085 - 00:21:57.395, Speaker B: You can split things, you can produce multiple things. We haven't implemented this in full generality yet, but we are pretty excited about this technology. Was this too long?
00:21:58.975 - 00:22:04.435, Speaker C: I think one thing you forgot to Mention is the rolling commit thing. I don't know if you guys.
00:22:06.535 - 00:22:21.275, Speaker B: How did you solve the. Did you. Do you commit the whole block at once, like in the original block STM paper? Or have you adopted some sort of like rolling commit mechanism?
00:22:22.855 - 00:22:24.155, Speaker D: You're asking me?
00:22:25.335 - 00:22:26.155, Speaker B: Yeah.
00:22:26.455 - 00:22:26.855, Speaker C: Okay.
00:22:26.895 - 00:22:27.515, Speaker D: Okay.
00:22:27.855 - 00:22:28.303, Speaker B: Yeah.
00:22:28.359 - 00:22:32.471, Speaker D: We commit each chunk one by one. Yeah.
00:22:32.583 - 00:22:34.479, Speaker A: So what is a chunk?
00:22:34.647 - 00:22:48.573, Speaker D: A chunk is set of some set of transactions that we process separately. And when we finish each chunk, we commit it and going to the other chunk to the next chunk.
00:22:48.709 - 00:22:57.845, Speaker B: Makes sense. Yeah, that's smart. I think that sort of sidesteps the issue. Right. It's a trade off. Sasha, you want to say about rolling commit?
00:22:57.925 - 00:23:35.477, Speaker C: Yeah, very quickly. So the thing that we originally then block stm, we committed the entire block together and we didn't see a reason not to do it. And it was like to save overhead, we said we only need to check the commitment like predicate at the end. So why do the overhead and check it all the time? But then with the aggregators, we needed to know that transactions are committed on the fly to be more efficient. So we introduced what we call rolling commit and now we have a way to commit them one by one. So yeah, it's not in the original paper and I don't think we ever even published it, but we are trying.
00:23:35.501 - 00:23:39.299, Speaker B: To write a blog post about it for a better part of year and a half.
00:23:39.437 - 00:23:43.835, Speaker C: Yeah, yeah, yeah, for sure. We have a blog post draft for like one year at least.
00:23:44.695 - 00:23:47.423, Speaker A: This will never see the light of day, that's for sure.
00:23:47.599 - 00:23:49.127, Speaker B: You put it in the paper though.
00:23:49.191 - 00:23:53.191, Speaker A: It will never see the light of day. But just to.
00:23:53.263 - 00:24:26.285, Speaker B: Sorry, just to say very quickly, basically all it does is like we added some constraints based on sort of the validate. Like because in the original block STM all the validations are the same. You don't know if like the validation was displayed, patched like a long time ago or it's like very recent. So it's just basically a mechanism that identifies which validation need to happen for you to be ready to be committed. If, let's say it's a recursive property if the previous thing is committed. So then threads can do a little bit of. We don't even see the overhead in the experiments.
00:24:26.285 - 00:25:00.965, Speaker B: It managed to hide it pretty well. Basically they just check this invariant and you have an index which keeps increasing that says what's committed. And you would not have that issue. It's just about the granularity. But the interesting thing is that we discovered we needed this not just for the aggregators. For example, sometimes if a block is consuming, there are other cases where you may want to keep some rolling statistics of the block executed so far. You might decide to cut it early and defer the rest of the transactions to the next block or something.
00:25:00.965 - 00:25:26.157, Speaker B: Such features would be impossible if you just though like if you have only the end of the book. But I think just splitting it into chunks is a really elegant solution. It just maybe for a high number of threads it would be a bit tricky to allocate, like to have the right size of the transaction to not chunk, to not worry about the scheduling extremities. Right. But yeah, otherwise it sounds really good.
00:25:26.221 - 00:25:40.885, Speaker D: Yes. And let me be more specific. We have two commit mechanism, one for a transaction and one for a chunk. So we also commit after each transaction, but also we have some other commit phase which is per chunk.
00:25:41.045 - 00:25:43.145, Speaker B: I see, Makes sense.
00:25:43.965 - 00:25:56.705, Speaker A: Okay. That led us beautifully to the next question, which is what parameters and configs do you guys use in production? So, Noah, what are our parameters for chunk size and for number of threads?
00:25:58.455 - 00:26:18.823, Speaker D: We did some experiments and we discovered that chunk size of 100 transactions and the number of threads equals the number of logical cores divided by two is the best configuration. The configuration that leads to the best tps.
00:26:18.959 - 00:26:20.487, Speaker A: For people who don't know what is.
00:26:20.511 - 00:26:49.615, Speaker D: A logical core, it's the number of cores. Not the real, not the physical course, but the number of, I guess theoretical course. Yeah, non physical course that is identified with some machine. And I think that sum ups my knowledge regarding this. Yeah.
00:26:50.595 - 00:26:57.335, Speaker A: What about you guys? Have you found different numbers work better in practice? You don't use chunks. Right. So what works for you?
00:27:00.115 - 00:27:28.785, Speaker B: Should I go for. I'll go first. That will force me to be more concise because. Yeah, so this is really interesting basically because very recently we had like a block that was small and we discovered that we did not take that into considerations. So we had to introduce another not parameter. But basically if you allocated a lot of threads and saw like a tiny, tiny block, you probably need less workers. Right.
00:27:28.785 - 00:27:51.665, Speaker B: But other than that, we discovered the scalability properties that we kind of showed in the paper. I think the graph kind of stays the same even though numbers change. Like the, like, for example, like whether the VM is slow, whether the contracts are heavy. These all affect the stuff. Right. But we also do the same. We.
00:27:51.665 - 00:28:49.705, Speaker B: I don't think it ever makes sense to allocate more than the physical cores. And because most of the, Most of the CPUs now come with hyper threading, we like the best is to avoid hyper threading. So you at least should divide by two and then for example, if the block is small, for example, that's one corner case. We actually even had some ideas about hyper threading optimizations, but this is so like far down the pipeline to try to explore that. But the other thing that we have to mention is that when we were again, when we were originally building block sdm, the idea was that we again we want to be flexible. We want to make sure that it's like a lot of blockchains back then were just sequential. So we were trying to make sure that if you have 4 threads, 8 threads, 16 threads, this scales actually really well.
00:28:49.705 - 00:29:38.465, Speaker B: And also because we didn't want to assume that you would need to have X amount of cores like it's usual, we took a machine at the time, it was 32 cores on a socket because we also didn't optimize for cross socket. That's another interesting thing. So we showed that it kept increasing the throughput, but we didn't really optimize for this high thread counts. And in fact some of the things that are kind of part of the algorithm where they have this virtual scheduling where they all go and pick up their next thing, so it introduces contention. So as you keep increasing the number of cores, at some point it stops scaling. If you go to 40 threads, if you go to 60 threads. We are internally working on it.
00:29:38.465 - 00:30:18.899, Speaker B: Of course this is one of the obvious directions where we want to improve. But that's another consideration I would keep in mind because you want to make sure that you are getting the best bang out of the back when you're adding more and more cores. Because all the cores can be used everywhere in the system. Making the right trade off there is important. But we've generally found, I think in the production configuration is now 32 cores. So we still believe that we get benefits from having like running with 32 cores. But we are aware that at this point it's not performing as well as it wasn't very optimized for this case.
00:30:18.899 - 00:30:30.695, Speaker B: And we are working on sort of extracting more there. I think up until 8.16 it does pretty well as far as not having that much slack goes. Sorry, Sasha, maybe.
00:30:30.815 - 00:30:33.271, Speaker C: No, you just, you mean, you mean scalability, right?
00:30:33.343 - 00:31:04.961, Speaker B: Yeah, yeah, just scalability. Exactly, yes. Also as you have more threads like this because just Amdahl's law, right? It doesn't even need to be like sequential bottleneck. If you have certain parts that cannot be parallelized, well, they start Becoming like bigger and bigger bottleneck. Right. So you do have to do certain sort of different types of optimizations there. And yeah, so the currently deployed block SDM has maybe some early versions of that which we use.
00:31:04.961 - 00:31:19.285, Speaker B: Right. But the original algorithm is not very well designed for like super high number of threads. Like if you would take a machine with like 100 cores and run it, it would be better to run it with 30 cores, probably.
00:31:20.955 - 00:31:22.935, Speaker A: Sasha, any additional input?
00:31:24.195 - 00:32:02.589, Speaker C: I think. I don't know if it's directly related to the question, but one of the design goals that we have in mind is that we never want it to be slower than the sequential execution. Like say someone gives us like either adversarial workload or just, you know, sequential workload. We didn't want block STM to perform much worse than sequential. So we always wanted to keep it like in a reasonable, reasonable difference between sequential and then improve when the workload. The goal is basically we wanted to extract the inherent parallelism of the workload. If the workload is like fully parallelizable, we want it to scale like fully.
00:32:02.589 - 00:32:11.005, Speaker C: If the workload is fully sequential, we didn't want to have like more than 10%, 10 to put 20% overhead. I think this was one of the design goals.
00:32:11.085 - 00:32:28.565, Speaker A: So I'll ask a naive question. When you implement parallel execution, you're adding extra logic, which is extra computation. At what point do the optimizations bring more harm than good? At what point is this overhead? Does it cause you to be slower?
00:32:30.465 - 00:33:00.855, Speaker C: Yeah, it's an interesting question because I mean so many STM system in the past as well, right. And we didn't adopt any of them. And one of the reason that we found out is that, sorry, I'm going to go a little bit longer than for the question, but some context here. So the reason is that many of these block STM this like academic works, they don't execute real transactions. They execute very simple transaction. They move some stuff between data structures and then what happened?
00:33:01.435 - 00:33:16.199, Speaker B: My pet peeve. Yeah. Like we do point like STM benchmarks do pointer chasing like in the hand optimized like data structures. Right. Like this is not. If you were to do that just use the concurrent data structure. Right.
00:33:16.199 - 00:33:17.795, Speaker B: Like why are we doing this?
00:33:19.895 - 00:33:59.955, Speaker C: The point is that. The point is that the validation cost and the execution cost are at the same magnitude and in this case different algorithms works. But in our case, because the transaction like takes much more time to execute than to validate. This is basically was. We had to come up with something new, like other stuff didn't work. And what we found out, surprisingly, I think like we talked about in all the talks that we give, is that this predefined order that we follow actually was very, very helpful because in reducing the overhead. Because like intuitively, threads don't need to fight on which one goes next.
00:33:59.955 - 00:34:22.775, Speaker C: The order is already determined. They don't need to solve like some sort of consensus in order to decide which one goes. Like they know which one goes first and now they need just to respect the order. And you can save a lot of overhead just by know that my transaction goes before transaction. So ATI needs to wait and instead of us keep aboliting and having this live lock issues.
00:34:24.235 - 00:35:01.435, Speaker A: So I want to go into a bit more detail about that, but with less detail from you guys. So if I just ask for the intuition intuitively, what does the order contribute? So just to give the very naive intuition, if there is no imposition on order and all you get is a set of transactions, then it seems, it would seem to someone like me that the scheduling becomes simpler because as soon as the task succeeds, there is never a reason to throw it out because something preceded it needs to be thrown out. So how does the order actually help?
00:35:02.025 - 00:36:01.759, Speaker C: I will try one way and then maybe if it doesn't work with the intuition, we can try another way. But there is this serializability and learnizability definitions. In concurrency, you execute your tasks in parallel and then the correctness criteria says that there is a sequential execution which corresponds to what happened in parallel and your execution, it has to respect the real time order and it has to respect the outputs. So if you don't have the predefined order, basically there is a huge set of possible of this serialization you can order the transaction in many different ways and this needs to be determined on the fly. So in some way, intuitively, the threads a lot of overhead goes into determine which specific order to take where. In our case we predefined the order, so all this overhead goes away. We don't need threads don't need to fight in order to determine which initialization.
00:36:01.927 - 00:36:05.391, Speaker B: Yeah, yeah, I think that's. Can I add something?
00:36:05.543 - 00:36:09.515, Speaker C: Yeah, I said at the beginning that might either help or not help at all.
00:36:10.455 - 00:36:55.043, Speaker B: Basically you said this much better than I would, but like one small detail might be missing to sort of connect the dots, right? Ilya? It's the same flexibility that hypothetically can help you find a better schedule, right? Like yeah, they're deciding on the fly, but they're combinatorially there are so Many more options. Like, it should be easier to find the right one. But the issue is that they are finding the right one. Every pair of them is literally fighting to go first. And you can abort each other and whatever. And in all this, as far as I know, the previous work in STMs, typically the biggest issue is cascading aborts. You read something, it's wrong, then you change.
00:36:55.043 - 00:37:20.779, Speaker B: Now you have to re execute. Now someone read what you wrote, they have to re execute. Once this happens, like, everything goes down the drain, like really, really, really fast. So like, you have to make sure that like cut down cascading aborts as much as possible. And what. And the order really helps there because you literally know like, which, like when two transactions see each other, we know. I have, let's say Sasha's transaction is before mine.
00:37:20.779 - 00:37:41.615, Speaker B: I know I have to respect. What if his transaction writes something that my transaction need to read? I have to sort of wait like, so we can build on top of the order things like the estimates and dependencies where, you know, like every time you abort, you use your latest write set. So your like your previous write set as estimates and then you wait for it.
00:37:41.735 - 00:38:02.723, Speaker A: So if I understand in a nutshell, you're saying that the predefined order sort of bounds the computation that you're wasting on things like abort. So you have a richer. You have a richer solution set, but it might be more difficult to reach even all of them, strangely. Okay, cool. In my opinion, this is counterintuitive and I think.
00:38:02.779 - 00:38:46.023, Speaker B: Sorry, one more sentence is that. And I think again, much like the static one, I think that, you know, eventual super smart system would sort of have both of these things. We just think that the obstruction of solving this, and I think the database is actually the literature there migrated towards this design as well that was like highlighted in this bone paper that we extended. Where basically, basically the ordering is important. That might give you bigger solution space. But you can separate this, right? You can have separate algorithms that try to optimize the order or try to reorder in very special scenarios. But not this like wild west of, you know, like, we don't know anything.
00:38:46.023 - 00:38:50.435, Speaker B: We're just going to figure out like one serializable order. Right? If that makes sense.
00:38:51.695 - 00:38:59.345, Speaker A: I think it does. If it's okay with you guys, I want to move on to some questions that Noah had. So the stage is yours.
00:38:59.385 - 00:39:00.085, Speaker B: Noah.
00:39:00.385 - 00:40:00.365, Speaker D: Hi. Okay, so the first question is you defined the method in the paper using pseudocode called next task, which prioritize a Validation task with lower index over execution task with the higher index. And in the paper you start by trying to validate a transaction to validate. And if it's not ready to be validated, then you leave the method, the next task and query it again. On the other end, I saw that on the aptos repo, if you try to validate, if it's not ready to be validated, then you execute the first transaction that try to execute the first transaction that's supposed to be executed. It may be a very specific question, but do you have some intuition to why.
00:40:01.585 - 00:40:48.929, Speaker B: So this is a case, this is a case where sort of like nice abstract, once you start deploying this and running in all sorts of blocks, you see so many corner cases that sort of the high level design goals that we had for the next task, that kind of paper optimized to make it clear. Then you discover like for example, I don't know if this is the one that you're talking about, but there was this case where we did not like we had this first. Let me explain the very high level design principle here again, and it's very good. I think the order of questions is great, right? Because we mentioned cascading aborts. The reason why we prioritize. Our goal is to prioritize the lowest task available. No matter, we can relax it sometimes.
00:40:48.929 - 00:42:06.197, Speaker B: Like for example, if you did, if you executed something and we know we have to validate it and all the validation tasks are like we don't like, there are cases where we try to avoid synchronization and the thread just worker just continues with the next task itself generated. But other than this, we always try to take the minimum thing that we see at the moment, right? And the goal is exactly that. Because if some lower transactions are not validated and I'm doing a higher execution, even though it seems like, oh, just much more valuable work, I am so much at risk now that if that validation fails, I have to abort longer and longer things, right? So this is the intuition why we always. And now that we have commit tasks, we prioritize those over everything, right? Because that's the highest priority. And then now that was like the design goal, right? But now you go try to implement this, you discover it might also be because of sort of the implementation details, even in the pseudo code. So one thing, for example, was that we would dispatch like bazillion validation tasks because if something was never executed, our implementation would go over that index and sort of. And then.
00:42:06.197 - 00:42:46.397, Speaker B: But this is not an issue because you might, because it's concurrent, right? So some later transaction might have finished and might be ready for validation. But then because the first executions right now don't have any hints when the thing that you skipped over finishes, it will also dispatch everybody for validation. Because that's the point. Anytime something changes that might have effect down the order. We are very aggressive because we assume validations are cheaper and we know that cascading aborts is what's going to kill us eventually if we don't take measures. Over there we added, it was quadratically growing the number of validations. It doesn't matter how much cheaper it is.
00:42:46.397 - 00:43:21.259, Speaker B: It became a bottleneck some blocks. And I think one of our engineers discovered this. Right? And then we fixed the logic in Next task to sort of be immune to this kind of behavior. And there are multiple other things that we did, but basically the way it would happen, we tried to document it. I hope there are comments in the code. If they're not, someone can open an issue. But I think our implementation of Next task currently has a few things like this, right? Where there was a specific case where we discovered the previous logic was not perfect.
00:43:21.259 - 00:43:34.055, Speaker B: We thought about it and we modified something slightly. Right. I'm not sure I would be able to remember all of them by heart to this one specific one I did remember. But later we can take it offline as well.
00:43:35.715 - 00:43:42.477, Speaker A: There's another challenge of the same sort now. So there's a mysterious number, right? Noah?
00:43:42.661 - 00:43:44.125, Speaker D: Yeah. What?
00:43:44.245 - 00:43:45.277, Speaker A: The incarnation?
00:43:45.421 - 00:43:57.105, Speaker D: Yeah, yeah. Can you please. You used the incarnation number in the paper. So can you please describe what is its main goal, its main purpose?
00:43:59.165 - 00:44:27.999, Speaker B: So that basically the incarnation number. Maybe. Maybe Sasha can explain a bit better. But like technically all it tries to say, definition is just. It just tells you like we have this invariant, which now we are doing some corner case for this too. But like execution of a transaction is never concurrent with itself. Different workers might execute transaction three, but they will all be, you know, like apart from each other.
00:44:27.999 - 00:44:55.785, Speaker B: They will never intersect, like in terms of concurrency. And then those is what is numbered by the incarnation. So the first time you execute, you'll have incarnation zero. Then if there is a validation failure and the transaction needs to re execute, it will have incarnation one, et cetera, et cetera. Now that in itself, like, okay, fair enough. But then one place where we use it pretty. An important place where we use it is basically validation.
00:44:55.785 - 00:45:31.407, Speaker B: So for a lot of stuff, we compare it based on incarnation, not based on the value. What does it mean if a transaction executed and wrote something and then re executed and wrote to the same slot. But I've read the previous thing, I know it's from a previous incarnation, so I will assume that it's changed now. This is a trade off. This is very clearly a trade off. The reason we did it this way was because first of all, there are all sorts of things that can happen in concurrent systems like ABA and so on. And we were focused on the bigger problem.
00:45:31.407 - 00:46:14.825, Speaker B: We didn't want to worry about it. But more importantly, it was because we wanted to make sure. Yeah, we wanted to make sure that our validation time doesn't depend on how big the data is because it's important for the validation to be fast. So if you just compare numbers, even if sometimes it was the same value and you could have avoided this is the trade off that we made. Now we are pretty convinced that this is not the best. The best thing should be something in the middle. I think the algorithm should be aware that if I can compare this with equality, let me compare this with equality.
00:46:14.825 - 00:46:22.317, Speaker B: But let's say if it's bigger than one cache line, let me compare the incarnation numbers or other similar things like.
00:46:22.341 - 00:46:35.337, Speaker D: This and then you get both words. So if the value stays the same, you don't invalidate it. And if it's too hard to compare, then you use the incarnation number.
00:46:35.481 - 00:46:43.633, Speaker B: Exactly, exactly. And then for counters for things like there might be like technical details, but that's it in essence, yes.
00:46:43.769 - 00:47:00.261, Speaker C: I think also if I remember correctly, we by default assume that ABA is a problem and then at some point we realize that ABA is not a problem in this case. So you know that by default the way to deal with ABA is version numbers. So we just thought, yeah, but maybe.
00:47:00.293 - 00:47:02.589, Speaker B: We should explain what ABA is, right?
00:47:02.637 - 00:47:03.621, Speaker A: Yes, yes.
00:47:03.733 - 00:47:56.945, Speaker B: So ABA is this behavior where you are in the concurrent world, you observe something, you read the value and the reason it's called ABA is we assume, we call the value that you read A, right? And then you come back later and you check it and it's again A and you're like, oh yeah, great, this has not changed. But what if in the middle someone changed the value to B and then changed it again to A, Like B doesn't necessarily matter. But what means is that you might assume a stronger invariant that like nothing has happened because you observed the same value twice, but in reality it might have changed and this has caused like numerous, numerous, numerous bugs and like everything, right? In probably like in the industry. But also I'm aware in like the algorithms, right, there were concurrent Algorithms written by experts. And then it had an ABA issue. Right.
00:47:57.245 - 00:48:03.345, Speaker A: Is there maybe a textbook example of a case where you are tempted to assume a stronger invariant?
00:48:03.925 - 00:48:54.073, Speaker B: I think Wikipedia even has some example. Right. I don't know if I can, like, I can just cook up something on the fly, but the idea is just that, you know, like typically it's even something like you're validating and you're saying I succeeded, but then you cannot find the linearizable schedule because the other instruction was actually competing with you. Even compare and swap has this issue because you just compare the value. I think locked load store conditional that some architectures had was actually designed to avoid it somehow. But in any case, this is a known issue and we really didn't want to deal with it when we were writing the original paper. But then we had to do the aggregators, in fact, and we had validation for the aggregators and we were like, we're not going to put version numbers here.
00:48:54.073 - 00:49:28.755, Speaker B: It's just an integer. Can we just compare this integer? We went back and redid all the proofs and confirmed we had an intuition that it should be like this. But we went through the proofs as carefully as we could and we convinced ourselves that ADA is not an issue for block stm. So essentially it is. It should be completely safe, as far as I know, to compare the values. But yeah, but it's sort of this trade off, right. If you have like five megabytes, you're probably better off checking the incarnation number, right?
00:49:30.295 - 00:49:37.195, Speaker D: Yeah, that's what we do actually. We don't use the incarnation number and I just wanted to see if it's.
00:49:37.635 - 00:49:54.131, Speaker B: It might be helpful to just track it for other purposes as well. Sometimes in the scheduler, all this code, you can assert some invariants. This stuff, it's really useful. But I agree this main purpose you can just do by value equality.
00:49:54.283 - 00:50:01.363, Speaker D: Yeah. Maybe to see if the last validation ended before the commit or something in the wave that you use.
00:50:01.459 - 00:50:42.425, Speaker B: Yeah, for our waves we do need it, I think. But yeah, I think in general we try. In fact, like some of the recent work that we are doing before we do some more changes to block STM is like try to push as much information into the system as possible, like for profiling, for debugging. Because honestly it surprised us many times. Like we think that we know algorithm, you know, we are sort of like. We were like for such a long time, we wrote the paper, we thought about all the cases, we ran so many experiments and then every once in a while you just Run it. And you totally do not understand what's going on.
00:50:42.425 - 00:50:55.165, Speaker B: Because it's a complex algorithm, it takes a life of its own sometimes. Then it's really useful to have as much information available as possible, even if it's just for debugging purposes.
00:50:56.505 - 00:50:59.485, Speaker C: There was always a workload that can surprise you.
00:51:02.625 - 00:51:33.065, Speaker A: Cool. I think I have a final question and we need to finish at least on our end. You are welcome to stay and speak to each other and have many people listen, but we won't be there. So we had sequential execution before and out of the entire task of building blocks. Execution is only part of it. There are other tasks that cannot be parallelized as easily. So I want to ask Noah and you guys what actual percent of the sequencer's time is.
00:51:33.065 - 00:51:49.155, Speaker A: Is parallelizable in principle and what percent is tasks that cannot be parallelized, like computing commitments, et cetera. So do you have any idea about these numbers for our case, Noah? Even if it's a suspicion.
00:51:52.975 - 00:52:11.739, Speaker D: Not. I'm not sure actually the commit phase is non parallelizable. And also currently we do not parallelize the reading from the state. But I don't know if I can give you a concrete number.
00:52:11.827 - 00:52:14.015, Speaker A: Yeah, how about you guys?
00:52:15.315 - 00:52:19.375, Speaker B: So indirectly. Or maybe Sasha knows some better?
00:52:20.355 - 00:52:21.575, Speaker C: No, I don't know.
00:52:21.875 - 00:52:52.103, Speaker B: What I can say is that we. Because I have an idea. Maybe, but I don't want to say something that's imprecise, but I can say something that I think is relevant. What we found is that increasing the number of cores was important enough for us end to end. Right. So the reason we run with 32 cores is because it was a bottleneck enough that we increased the number of cores. Even though we know that in terms of like how much scalability we get.
00:52:52.103 - 00:53:21.285, Speaker B: Like let's say how. What's the maximum possible you can get 32 and then what you're getting versus if you run with 16 threads or 8 threads you're like much more close to optimal than with 32. Unless until we make more improvements. Right, but based on the end to end system performance, it was still relevant for us to use 32 cores there because it was significant enough portion of the whole system. One thing I wanted to mention before you go was that about that.
00:53:22.145 - 00:53:42.287, Speaker C: Yeah, about that. I think it is relevant because remember the stuff that we recently did that we limited the number of threads to do some sort of work. It basically this, right? Because there's not enough parallelism there. So we might as well just limit the number of threads that do. Like for example the commit phase, right? The only commit stuff.
00:53:42.431 - 00:54:28.429, Speaker B: Yes, yeah, yeah, exactly One thing I think we will probably soon, sort of, once we deploy these changes, we'll make some sort of public service announcement. I think it's not like super critical in terms of like what is really the probability of this to happen, because it also depends on the computer scheduler. But in block STM we have this property, right? Like, other than the like synchronization overhead, there should always be a critical path of like workers doing the most important work. Like execute zero, validate zero, execute one, validate one. Like take things next, next, next task maybe. But that's the reason why we are competitive and we sort of can prove that the version of block SDM is competitive with the sequential execution. And we have experiments to show that.
00:54:28.429 - 00:55:38.697, Speaker B: And that was really, really important to us. And that is still really important to us, right? Because it's a worst case guarantee, like competitive guarantee. The same way one can say like why do we need 3F +1 in consensus? So now the, what we discovered is our implementation does not the block STM implementation, when it finds a dependency in the paper, it just kills the execution and then it restarts once sort of it's ready. And that actually satisfies this constraint because. But there is a corner case where what if like transaction two finished and transaction three is suspended? If you implement this with slip weighting, right? But it has already read some incorrect value, right? So it's destined to fail validation. Moreover, because it read the speculative value, you don't have a guarantee that its execution time will be the same as the correct execution time. It can take two times longer and then you will abort it and continue, right? So let's say if all of them are two times longer and if all of them happen in the worst case order, you would be three times worse than the sequential execution.
00:55:38.697 - 00:56:17.115, Speaker B: We discovered this recently. We were really annoyed because we like our worst case competitive analysis. So we have some changes in the pipeline now that we'll have to simplify maybe. But it basically makes sure that whoever committed transaction two can kind of ensure the transaction three to make sure that at the end of the day we still maintain our competitive like critical path to sequential execution. Happy to talk about it more, but as I said, probably in a matter of weeks this will not be like the rolling commit blog post, like we'll announce it and maybe it's also relevant for you as well.
00:56:19.615 - 00:56:43.385, Speaker A: Thank you very much. I think this is a good time to wrap up. So thank you very much. Rati and Sasha. And for people who are watching at home, starknet is a bit behind Aptos and Parallelism. But the release on mainnet is in a week from now, so you're welcome to explore both Aptos and starknet, and I think we'll end there.
00:56:43.465 - 00:56:49.809, Speaker B: So thank you very much. Best of luck with deploying this, like, for sure. We're, like, we're really working.
00:56:49.897 - 00:56:54.445, Speaker A: It's working on testnet, thanks to Noah, so I think we're set.
00:56:56.105 - 00:56:58.681, Speaker C: And thank you for this space. It was very fun for us.
00:56:58.833 - 00:57:02.165, Speaker A: It was a pleasure for us, too. Bye, everyone.
00:57:02.985 - 00:57:03.321, Speaker B: Bye.
