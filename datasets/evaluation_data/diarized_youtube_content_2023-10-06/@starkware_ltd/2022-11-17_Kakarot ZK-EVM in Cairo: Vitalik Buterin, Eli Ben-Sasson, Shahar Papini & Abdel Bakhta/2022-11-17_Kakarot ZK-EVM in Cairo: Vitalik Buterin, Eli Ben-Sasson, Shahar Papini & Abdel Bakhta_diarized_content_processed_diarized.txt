00:00:00.410 - 00:00:00.990, Speaker A: Yay.
00:00:01.490 - 00:00:04.720, Speaker B: Hi, how are you?
00:00:05.970 - 00:00:15.120, Speaker C: No good. Just happy that this finally worked. The last two times someone invited me to a space that took like 15 minutes to figure this out.
00:00:16.370 - 00:00:34.760, Speaker B: Yeah. Always technical issues with Twitter spaces. It's not very highly available and scalable, but yeah, anyway. Okay. Yeah, I guess we can start. Thank you all for joining. I'm very happy to animate this space.
00:00:34.760 - 00:01:09.300, Speaker B: Yeah, we have on the same space the creator of Ethereum, the creator of stocks, and the creator of Cairo. So this is very exciting. First, I will ask you a simple question about naming and branding. Should we say ZK rollups or validity rollups? It's a quite hot topic and we started with the term ZK rollups, but actually we should probably say more validity roll ups. Do you think it's important to have clear names? Do you think it's too late to change that?
00:01:12.230 - 00:01:18.840, Speaker C: I'm happy to use whatever name that you want to use here. So if we want to call them validity roll ups, then sure.
00:01:19.610 - 00:01:21.480, Speaker B: Okay, great.
00:01:22.490 - 00:01:40.970, Speaker D: For the purposes of. I always say this, I just have a problem with. The mathematician in me has a problem with calling ZK things that aren't. It's like saying ten is a prime number to mean that it's a really good number. It's a really good number. I love ten. I can work with it.
00:01:41.120 - 00:01:50.314, Speaker B: Yeah. Can you explain what is your concern about that and what is the risk of calling those with the wrong term?
00:01:50.362 - 00:02:30.262, Speaker D: It's something I face regularly is that if you call it ZK ZK, it's very much related to privacy. So the next question is, okay, explain what ZK is and then explain how it helps to the matter at hand. And then first of all, you have to say, well, it's not really ZK. And ZK is about privacy. So I get detracted and I just prefer to get straight to the point, which is this is about scaling and so on and so forth. One way to avoid it is not to call it something that it isn't. By the way, Aztec, which is an amazing team, they are doing a ZK roll up in the sense of zero knowledge and privacy.
00:02:30.262 - 00:02:41.710, Speaker D: So I think it's also unfair to them. They are a ZK roll up. Maybe. Probably. It's the only ZK roll up I'm aware of, so I don't want to take their thunder.
00:02:42.130 - 00:03:23.040, Speaker B: Good point. Okay, so yeah, today we are going to talk about proving systems and specifically about execution engines and the different types of execution engine. So we'll talk about ZkeVM execution engines, but also non ZKVM. Because obviously, as you all know, Starquare is working on non ZKE EVM execution engines. And we will talk about the reasons about that. And also we will talk about a new community project, which is called Kakarot, which is Zke EVM written in Cairo. So we will talk about.
00:03:23.040 - 00:03:48.950, Speaker B: So, like, the first thing I wanted to cover is Vitalik, in your article about the different types of ZKE EVM, you mentioned that Ethereum was not initially designed to be ZK friendly. So can you expand on that? And can you basically tell us a bit more about the parts that are not friendly efficient to prove on Ethereum?
00:03:50.010 - 00:04:46.518, Speaker C: Sure. So when Ethereum started the whole concept of ZK Synarx and that whole space was just not on almost anyone's minds in the crypto space. Right. Actually, I remember visiting hi fi in Israel and interviewing Ellie because I was doing bitcoin magazine then this was like literally two months before the Ethereum project started. And that's where he explained some things about Synarx and I think tried to explain a bit about the precursor to starks at the time. And at the time, when I heard these things, they seemed amazing. But this seemed like this still extremely speculative, very early stage moon mass that would take potentially decades to get to the point where it's actually something that we can use.
00:04:46.518 - 00:06:05.358, Speaker C: Right. And so at the time, it just felt like this faraway future thing. And so the EVM was not really designed with any kind of friendliness toward snarks or starks or zero knowledge, anything in mind. And as a result, there are just a lot of components in it that are basically just accidentally very ZK unfriendly. So I guess just to kind of very briefly go into what it means for something to be ZK friendly and ZK unfriendly. Right? So Synarx and starks, they work over a type of math, usually that's called finite field arithmetic, where generally done through addition and multiplication, modulo sum, prime number, right? So it's basically quad arithmetic, right? So if your modulus is eleven, then four plus five is nine, five plus five is ten, but then five plus six wraps around to zero again. And modular arithmetic is really nice for cryptography because it lets you do adding, multiplying and lots of operations.
00:06:05.358 - 00:07:37.834, Speaker C: But whatever results you get, the result is still going to be a small number that's in the same size, right? Which turns out to be really important, especially for these kinds of applications. If you can express the computations that you want to make through prime field modular arithmetic directly, then you can generally make things that are pretty friendly and efficient to these constructions. But the problem is that the existing cryptography that Ethereum relies on doesn't really work that way at all. Right? So hash functions, for example, like things like shot 256 and shot three catch. These rely on a lot of bit operations, and they rely on bit operations that are very specifically chosen to be very friendly toward what existing cpus are capable of doing quickly, right? So cpus make a lot of assumptions about stuff being 32 bits or 64 bits, and doing bit operations that do the same thing to many bits in parallel. And you have ands, ors and shifts. And these are all things that are totally not naturally translatable into modular arithmetic, right? In order to translate them into modular arithmetic, you have to get pretty close to just decomposing everything into zeros and ones and logic gates.
00:07:37.834 - 00:08:49.874, Speaker C: And even though you have these big field elements that are all 256 bits big, you basically only use one of those 256 bits. And so there's just like a lot of this inefficiency that in practice makes the EVM much more expensive to prove than a VM designed with. The idea of proving in mind could be designed to be easy to prove. That was aside from things like hashes. And then there's other things like the way that signature schemes, there's the Merkel Patricia tree, there's which specific moduluses are used, right? Because it can't be modular arithmetic in general. You have to kind of pick a specific one and stick to it. And there's just all of these different considerations because of which the EVM doesn't really line up to what starks and starks require.
00:08:49.874 - 00:09:38.366, Speaker C: And if it did, then things would obviously be much more efficient, because it doesn't. You just have to use a lot of complicated tricks and incur a lot of overhead and just pay a lot of costs and pay a lot of complexity in order to be able to ZK prove EVM execution. But of course, the fascinating thing is that we've had multiple teams go ahead and actually do the really hard work and do it. And the result is that you get these EVM provers that can make proofs of EVM execution. And so if you do that, then you add a little bit more work. Basically, it could be used to prove at least the execution side. So not the consensus side and the proof of stakes that you choose, but just the execution side.
00:09:38.366 - 00:11:13.006, Speaker C: The transactions of existing Ethereum blocks. Today, I don't think anyone's like 100% at that level yet. But people are getting closer and closer. The numbers that I've seen so far are still far from being fast enough for most things, right? Like potentially it might take 12 hours to prove an Ethereum block or something close to that. That actually can still be useful for roll ups if you come up with a roll up or you do some clever thing where you add proofs after the fact instead of proving in real time. But we're still quite far away from being able to prove in real time, which is something that would be needed if we want to say, use these synapses or starks to make the blocks in the existing Ethereum layer, one chain provable. So people not have to, or even people running clients would not have to actually run the execution themselves, right? Yeah, basically just a bunch of different choices in terms of mostly the cryptography that the EVM uses, but also a couple of other things, like the fact that memory is based on bytes, and that you grab 32 bytes at a time, and you can grab 32 bytes from anywhere, including from kind of the middle of a 32 byte range.
00:11:13.006 - 00:11:31.450, Speaker C: And just all of these decisions that really increase the number of what are called constraints, basically the number of sort of the ZK proof systems equivalent of computational steps that have to be made to actually prove execution.
00:11:32.110 - 00:12:10.020, Speaker B: Okay, thank you for this explanation. And basically, those parts that are not decay friendly are very important in the different types of DKVM, because those are the parts where some project are doing some trade off, and they decide to switch from something that is not decay friendly to something that is decay friendly. But we will talk about those later. Then the next question is for Eli, and you already gave some part of the explanation. Vitalik, what was the reason, initially when you started to not go through a ZkeDM idea? Initially, Ellie, back in the time after the creation of.
00:12:13.450 - 00:13:35.710, Speaker D: So first of all, I just want to say that I still remember very fondly sitting the meeting that, Vitalik, that you mentioned that indeed we had. What I mostly remember is that we had humus at the university. I was at the time at Technion, and we're chatting about a whole bunch of things that it's so amazing to see that several years later, you actually went and created this amazing thing that is ethereum with it. I remember, like discussing the bounded, halting problem and of course, also proof systems. But anyway, that was a long time ago. So when we started dabbling with generating validity proofs for the purposes of scaling, it was a very daunting prospect, because there's a lot of complexity that goes on into generating these proofs and complexity in two ways. One is complexity in taking a computation and turning it into a system of constraints that captures the computation.
00:13:35.710 - 00:14:36.230, Speaker D: And here maybe for the listeners, I think a good analogy is you probably all saw those pictures of Foneuman and other early advents of computers who were basically looking at these switchboards or whatever it was, vacuum tubes or something, and connecting them by these huge systems of wires and so on. So I think it's something like that. You for the first time try to take any sort of computation and put it in this model. In our case, it was generating a proof for it. And we started with relatively simple computations and we wired things by hand. In our world, it means writing polynomial constraints for these things. But that's the analog.
00:14:36.230 - 00:16:05.662, Speaker D: And that was what we felt comfortable with. And then the next stage was that some very brilliant folks, Shahav and Leo and others, said, we have something that is maybe just a little bit more complex, or actually perhaps even simpler, but it's a very minimalistic set of wires or polynomials that nevertheless captures something like a cpu. And because we still were worried about the complexity of how efficient generating proofs might be, we said this is something relatively simple, but it's good enough to start writing programs and writing computations in this framework, which is Cairo. And that's how we got to Cairo. So it really came out of this realization, this very conservative approach of trying to take one step at a time. At the time, we didn't think necessarily that we'll have starknet or open it up to the rest of the world. This was just an attempt to make things easier and make the development more secure and more scalable.
00:16:05.662 - 00:16:39.382, Speaker D: And that's really how we ended up with Cairo. So we didn't have this aspiration to take all of Ethereum existing solidity code or things like that. It was just, let's make it easier for ourselves to build systems like the ones for DyDX or Surrey, or immutable. Let's just make it simpler for us to add functionality. So that's literally how we came across Cairo.
00:16:39.526 - 00:17:32.510, Speaker B: Okay, very cool to expand more on that. Usually when we talk about proving systems, we say, okay, there is a prover and there is a verifier, and the prover wants to prove a statement to the verifier. But I think it's very abstract for people to know how you go from a concrete statement like, I want to transfer X tokens to something that is provable. And char, I would love to have your perspective from someone who was writing those polynomial constraints manually back in the time, and like to expand into detail what Ellie was saying, what led to the creation of Cairo and the different steps that led to the creation of Cairo.
00:17:33.970 - 00:18:28.750, Speaker A: Sure thing. Well, at the beginning, like Ellie said, we had the Stark protocol. And the way you describe a statement, the Stark protocol is using polynomial constraints, basically just equations that describe some relations. For example, if I want to do the Fibonacci, I would say something like z equals x plus y and do that repeated number of steps. Now, for Fibonacci, it's very easy to make complex statements. It's kind of hard to manually design these polynomial constraints and do it efficiently. At the beginning, the first product we actually did something interesting for was Starc, which was basically supporting transfers, deposit and settlements.
00:18:28.750 - 00:18:38.786, Speaker A: And we designed an air. And it was nice. We had even a framework to help us design airs, which made things a little better.
00:18:38.968 - 00:18:43.540, Speaker D: But I think no one knows what an error is. Maybe you want to.
00:18:44.150 - 00:19:59.820, Speaker A: No one knows what an area is. Yeah, the area is just the set of polynomial constraints should have mentioned that set of variables we have and the polynomial constraints on top of it. So we had a framework to help us build this in a bit more abstract way, to describe variables and relations between them. But it was very hard to make changes and make complex things, and even optimizations that would have been possible in regular programs. It was very hard to express it in polynomials, so we knew we needed something better for this. And one option was maybe writing some kind of a bit higher level language that will compile to polynomials, which is something that was happening around. There were other languages that they did for snarks, but at the end we decided against it.
00:19:59.820 - 00:21:33.002, Speaker A: If we wanted to do equations, we had our framework, which was good, but we wanted to go towards something we regarded as the Holy Grail. If we could have something for general computation, like a CPU. Now, if we consider the manual design of polynomials as designing chips, designing circuits, then the whole gray would be like cpus, a way to have some reduced instruction set and have the specific polynomial, just one polynomial set verifies it. And once we do, we'd have a bunch of benefits. We could write in really high level languages, but without the drawbacks of compiler to polynomials, just that. If any of you ever programmed in one of those languages, you can't do recursion, you can't do arbitrary loops, only just like a specific number of steps, and it's not true and complete. However, if you are doing the holy grail approach, having this kind of cpu, then you could do everything of this.
00:21:33.002 - 00:22:55.010, Speaker A: You could have real high level languages you could compile from maybe EVM, for example, or X 86, or maybe something else. And so we spent a lot of time to research and all kinds of designs, and we did have a specific goal to make it as efficient as possible, because we knew the benefit and maybe the biggest advantage of starks or technology was that it is very scalable and fast to prove, and we wanted to double down on that. So we wanted to be fast and easy to prove. So we made a lot of decisions, for example, using immutable memory, using basic field elements, and efficient ways to communicate with what we call built ins. But to add some additional capabilities on top of the basic air and those made the pronomo set that verifies the vm that you called Cairo. Very efficient. So when we came out with Cairo, the first users of this was us, was Starquare.
00:22:55.010 - 00:23:49.350, Speaker A: In the beginning, we didn't really think about making it public for other people. We didn't even think about Starknet. It was just for us to use internally to help us write better applications. And it did, it was a real game changer. When we rewrote Starkx using Cairo, it was a lot easier, and we did a bunch of very cool optimization. Even though you would think that it would be less efficient to write on top of this, because when going from Asics, from a circuit to CPU, there is an overhead. ASICs are obviously more efficient, but in this case, moving to a ZK CPU allowed us to add a bunch of optimizations that were very difficult, maybe even not possible in the old approach.
00:23:49.350 - 00:24:28.580, Speaker A: So we even got additional scaling and benefits when we wrote our applications on top of Cairo. And at the beginning, Cairo was very assembly like, which is good and bad. It allows you to do some very low level things if you want to, efficient. But even though it's a lot better than Radi polynomials, it can be a lot harder to do other stuff. I guess I'll talk about it a bit later. But this was generally how Cairo was created and why.
00:24:29.030 - 00:25:04.210, Speaker B: Okay, very cool. Yeah, I guess we can start to talk more about the different type of ZKE EVM. So maybe, Vitalik, you can give a brief overview of the different types and the different trade offs and. Yeah, we can talk about more stuff later. Vitalik, are you with us? Okay.
00:25:07.920 - 00:25:09.852, Speaker C: Yeah, I'm here. Yes.
00:25:09.986 - 00:25:19.600, Speaker B: Okay. Yeah, I was asking basically, can you give a brief overview of the different types of ZKE EVM?
00:25:21.620 - 00:27:05.410, Speaker C: Sure. I think there's different ways to look at the different kinds of ZK evms that are being made, but the categorization that I made in my post is to basically look at the trade off between being more similar to the existing EVM versus making more modifications generally in order to be more ZK friendly, right? So the four types that I had, one is type one, which is basically a perfect replication of the Ethereum execution layer, even to the point where you'd be able to feed Ethereum blocks into it and it would verify them. So this is like maximum compatibility, but at the same time maximum challenge and maximum overhead, because you have to deal with not just stuff inside the virtual machine, but also hashes and DSA signatures and a whole bunch of things. Then there is type two, and the type two is a fully faithful ZKe EVM, but not Zk ethereum. So the difference is that parts of Ethereum execution that I would consider to be not part of the virtual machine. So particularly things like how the state tree is hashed and how transactions are hashed, and possibly how transaction signatures work would not be included in that. And so you'd be able to swap those out and replace them with something much more Zk friendly to make it easier to implement and reduce constraint count.
00:27:05.410 - 00:28:19.044, Speaker C: But at the same time, type two still preserves full EVM compatibility, right? So any EVM unit tests would still run the exact same way, and any application that works on one would run on the other, unless it depends on some kind of weird history introspection, which merkel proofs that are rooted in some historical block hash or something. And there are definitely a couple of apps that do that, and those would break under anything except for a type one zKvm. A type three ZkvM is willing to break even more things, so it's willing to accept a small level of incompatibility. So it could have different hash functions to generate addresses. Maybe you could not include particular opcodes or particular pre compiles that are too difficult to implement. If it makes changes that are incompatible in order to reduce the complexity for developers or reduced constraint codes, then it becomes type three. And then type four isn't even a Zk EVM.
00:28:19.044 - 00:29:51.636, Speaker C: It's like what I called a Zk solidity, right? Or a Zk, some other environment that compiles down to EVM, but where EVM can't compile to it, right? So it's kind of strictly above the EVM and sort of the dag of what can compile to what. So type four zkvms can be even faster and both faster to execute it with a low constraint count and easier to implement. Right, and matterlabs is taking a type four approach, and they are further ahead of the type one, two and three teams so far by a pretty significant margin. But at the same time it comes with the sacrifice, which is that there's just even more tooling that doesn't work, and a lot of things that would work with an EVM end up not working with that kind of approach. Right. So if you do a ZK solidity, then the next question is, can it support Viper or other high level languages? And I believe Matterlabs's type four ZKVM, for example, can support Viper because there is now a compiler that goes through some assembly, which happens to be one that solidity supports. But it gets complicated, right? Yeah, that's a trade off.
00:29:51.636 - 00:30:08.270, Speaker C: Basically the lower the types go, the more it's easier for kind of end developers and people building things on top of it. And the higher the types go, then on the other end the easier it is to build and run the thing itself.
00:30:08.880 - 00:31:18.400, Speaker B: Yeah, right. To give another example of type four ZKEVM, we can mention rap from nethermine team, which is building basically a transpiler from solidity to Cairo. But as you said, Vitalik is very specific because it can only go from solidity and not Viper, for example. But still, I think this variety of different types is good because it can fit for different needs for different projects. And depending on what is important for your project, you can choose between all those different types. Like if you want maximal efficiency, you don't have the same need in terms of tooling, compatibility and whatnot, I think definitely that this variety of choice is very important. So to give a few words about Cairo and the fact that it is a very powerful tool to build some programs that are computationally verifiable.
00:31:18.400 - 00:32:32.170, Speaker B: The fact that we started this community project, which is called Kakarot, and the idea is to build an entire EVM fully written in Cairo. And we started something like three weeks ago, and we already implemented 109 opcodes on the 142 opcodes that exist on the EVM. And we can do that just because of the power of Cairo and the fact that you can build arbitrary and general proposed computation. So I think this is a very good showcase of what you can build with Cairo, because if you are able to build even complex stuff like Nivm in Cairo, you can build almost everything, and Cairo is not even in its final form. So char, maybe you can give a brief overview of the upcoming changes, and especially Cairo 10 and Sierra, and the fact that we will be able to prove invalid transactions, which is a big thing. Can you explain more about the upcoming changes?
00:32:33.500 - 00:33:09.092, Speaker A: Yes, of course. Like I just mentioned, Cairo at the beginning or even currently is very close to low level instructions. That's what we had at the beginning and we want to change that. We want to make Caro a lot easier for developers to use. That is the main reason of coming up with Caro 1.0. For example, currently Cargo doesn't have for loops you need to use recursion. Car 1.0
00:33:09.092 - 00:33:40.988, Speaker A: will also need to work through recursion because that's how the virtual machine works. But users shouldn't see this and they won't. They will just have loops and it will compile down to whatever. Also, Cairo 1.0 will solve all the variable issues that exist with Cairo. It will be safe, it will have a lot of nice high level features and good start library. It will have good package manager.
00:33:40.988 - 00:35:00.564, Speaker A: So basically also safe arithmetic by default and very easy to use other arithmetic types, not just field element, but you could use 256s, basically everything that will help the developer have a nice time, make it a fun language to write it. That is the main focus of car 1.0. We are at it for like three, four months right now and expected to finish this in a few months. And the second major thing about this release is Sierra, which is a way to enable proving filled transactions. The other issue today with Starknet, which is our layer two that uses Caro as the basic machine. You can't prove filled transactions because filled transactions in Cairo VM mean you can't prove this. If you do assert one equals two, you can't make a proof of it, which is actually a feature.
00:35:00.564 - 00:35:58.488, Speaker A: This is what makes this zero knowledge system sound. However, it means that if your program has this assert, you can't prove it. And it means it's hard to collect fees for virtual transactions and means it's very hard to do anti censorship, maybe forced transactions. And so Sierra comes to solve it. It's an intermediate representation, something that is more low level than the high level language, but it's a bit higher level than VM itself. And it will be safe in the sense that any program written in Sierra bytecode, anything to make representation, cannot fail. You can't write anything that fails, and this reverted transactions will actually be transformed into programs that return false.
00:35:58.488 - 00:36:27.860, Speaker A: Instead of failing, Cara 1.0 will compile down to Sierra. Then you will take the Sierra bytecode and deploy it onto Starcret. And in the last phase, Starcret will translate it from Sierra down to Cairo VM to make it also more efficient. So yeah, these are the main changes that are coming.
00:36:28.010 - 00:36:38.730, Speaker B: Okay, very cool. So are you saying me that we should have waited Chiro 10 before starting to write the EVM? It would have been much more easy, right?
00:36:40.300 - 00:37:03.760, Speaker A: Yeah. Yes and no. It will be easier, but the work done right now won't go to waste. We plan to have very good backward compatibility and migration tools to help you move from today's car zero to car 1.0. So you shouldn't be afraid.
00:37:04.260 - 00:38:07.440, Speaker B: Okay, awesome. Yeah, let's talk about Kakarot and the architecture on what type of ZkeDM it is. So for the moment we design it as a smart contract on Starknet. And this is very funny actually. It's a kind of inception, because in a smart contract on Starknet you can run a full EVM and run a bytecode program from Ethereum. So we aim for bytecode equivalents, but for the moment we can consider it something like more type three, because very likely we will do some changes to the hash functions and maybe also to the precompiles. And I really like Vitalik in the post, like you mentioned, that it can be an incremental process, basically, where a project can start with a type three and then do some modification and become more and more compatible.
00:38:07.440 - 00:38:19.400, Speaker B: Can you explain more about your vision about that? And basically how a project could start with a lower type and then do some modification to be more compatible with Ethereum tooling?
00:38:20.300 - 00:40:46.036, Speaker C: Yeah, it's definitely a good point there. I think in general, all of the ZKe vms that I'm following, they want to get somewhere, right? Like software developers, they don't want to spend two or four years of working on a thing and feel like they have nothing to show for it, all the way right up until the end, where they have some amazing thing because they could just burn out before they even get there. Right? It's fair, it makes a lot of sense, and it even makes a lot of business sense for these organizations to be able to hit the ground running with something early. And so the path that they often end up taking is start off building a ZKVM, whether it's for a validity roll up or for some other thing that they're doing. But at the beginning, if they find any parts of the existing EVM that are too hard, you just skip over them, right? So a lot of them, they skip over pre compiles like the BN 128 pairing pre compile in particular, because doing pairings inside of a circuit is just devilishly hard. Potentially some hash functions replace a SHA 256 with Poseidon or some other real fairly simple thing and just a long list of simplifications and probably just like some differences in behavior that are kind of even more accidental than anything else, right? And then over time, once that code base solidifies, then at the same time they start kind of polishing the edges and working on the discrepancies between their ZKVM and the existing EVM and just fixing them one by one, right? And so I think the thing that makes sense to do there is to first make sure you have full EVM compatibility. Because once you get EVM compatibility, there's like a lot of developer tooling and a lot of other nice things that you can unlock.
00:40:46.036 - 00:43:29.470, Speaker C: And then after that start also moving into replacing the tree structure and the block hashing and state hashing and all of that, and start moving toward so first I'm going to move toward type two and then move toward type one. And one interesting thing that you could do is once you do that, you could also add a like you could basically make EVM compatibility be a switch that you can toggle, right? Because if you want EVM compatibility because you're actually verifying Ethereum blocks, or you just really need total infrastructure compatibility or whatever, then fine, you just turn on, that's what you need to do that. But then if you want to use that same role ZKVM for some validity roll up, where it's not as important to kind of technically be possible to just directly push your roll up blocks into geth and have them validate or whatever, then if you instead turn that function off and you use, let's say efficient or stark friendly hash functions instead of regular hash functions, then that lets you save a huge amount on constraints and on proving time, which just makes your entire project significantly faster. Like you might be able to verify ethereum blocks in 4 hours instead of twelve or potentially even less or whatever, right? One other interesting gray area, by the way, is gas costs, right? So gas costs in the existing EVM are set based on the time approximately that it takes to just execute that particular operation on an average piece of hardware. And there's obviously many different kinds of hardware, and sometimes different things are faster on different pieces of hardware, but just like taking an average, but because Synarx are such a fundamentally different environment, the kinds of things that are fast and slow are just going to be wildly different, right? So like hash functions, as I mentioned, they're one great example, right? Like shy 256 and catch hag. Those are very fast on a computer, but they're very slow inside of proof. But on the other hand, like sload for example, just like grabbing storage, that's going to be expensive on a regular machine because you have to do I o.
00:43:29.470 - 00:44:08.472, Speaker C: But it's potentially going to be fast inside of a proof. Right. So the other thing that you could do is you could make a toggle switch for basically are you kind of faithfully using Ethereum's gas costs or are you going to plug in your own gas costs instead? I think I might have even used the word type 2.5 for a compatible vm that changes gas costs around. But ultimately I expect even the consensus layer EVM itself to changes gas costs quite a bit in response to all of this stuff. So it is a pretty temporary thing.
00:44:08.526 - 00:44:08.744, Speaker D: Right.
00:44:08.782 - 00:44:32.450, Speaker C: But in the meantime, yeah, you start kind of being very different. So then you get toward more and more compatibility. And then I think even after that start could even have some options for both compatibility and incompatibility if you value speed in particular cases and you just choose depending on the application.
00:44:33.940 - 00:44:47.830, Speaker B: Yeah. Okay. Are you saying that we can even see some dynamic zkvms that could choose the type of compatibility based on some configuration? Something like.
00:44:48.600 - 00:44:49.830, Speaker C: Yeah, exactly.
00:44:50.280 - 00:44:58.970, Speaker B: Okay, very interesting. Okay. Yeah, I have an important question for you Vitalik. Have you started to learn Cairo or not?
00:45:03.020 - 00:45:11.180, Speaker C: I have not done Cairo coding yet, but I have read through the tutorial and quite a bit of the documentation.
00:45:11.520 - 00:45:18.232, Speaker B: Okay, very cool. Once you are ready, I'm waiting for your pull request on Kakar.
00:45:18.296 - 00:45:20.350, Speaker C: Perfect, I will keep that in.
00:45:20.660 - 00:45:57.790, Speaker B: Yeah, cool. I would like to talk about also layer three because I think there are maybe some synergies between the different types of ZKE EVM and layer threes. I think we can even imagine a world with different layer threes that are different types of zkevms that settle on the same layer two. So maybe elique you can give a brief overview of what are layer threes and what is the technology that enables that. Like, you talk about fractal scaling, erection, stuff like that.
00:45:58.880 - 00:46:42.730, Speaker D: Yeah. A lot of the traditional world like web two, looks with admiration at blockchains and smart contracts and Ethereum and wants to have something similar for using it for composability, for easy onboarding of users, for really going towards web3. And this actually was a trend that started from the very early days of Ethereum. But part of the problem has to do with. How do you exactly have the right.
00:46:45.200 - 00:46:59.570, Speaker E: I would say that it's probably a good indication the fact if Twitter spaces crashes, that's probably indicative of some form of success. Well, not of Twitter, but of the space.
00:47:00.420 - 00:47:10.020, Speaker B: Yeah, definitely. Okay, Vitalik is back again. Okay, let's go back to layer threes and recursion.
00:47:10.360 - 00:47:42.232, Speaker E: Yeah. So first of all, I apologize to everyone who's now joining in. I think we had like around 1300 people and now probably a lot of them are trying to join in. So I'll repeat the question that I was asked before I was so rudely shut down by Twitter, by the way. I'll just say that I think that of course, that's one of the differences between something like Ethereum and Twitter.
00:47:42.296 - 00:47:42.572, Speaker D: Right?
00:47:42.626 - 00:48:42.684, Speaker E: One does crash with more usage and the other doesn't. So the question was, what about layer threes and what about fractal scaling and things like that? So I'll repeat what I started saying from the very early days of Ethereum and when it became successful, a lot of organizations said, we want this, we want something like this. We want, in fact the real thing. We want Ethereum within our organizations, or we want to be able to connect with it. And this didn't pan out. At the time, people spoke a lot about permissioned chains and there was, I think like hyperledger and other attempts to give organizations a permissioned blockchain. And this thing didn't succeed for two reasons.
00:48:42.684 - 00:50:01.636, Speaker E: One is that you didn't quite get good scale, but the other more important thing was that you lost all of the security of Ethereum. Once you move to a permission chain, you maybe have more control over it, but you lose all of the security benefits and the decentralization benefits of Ethereum. Now what is really unique about validity proofs is that you can start building layers that give you more scale, they give you more control, but they do not compromise at all on two things, on the security and decentralization of Ethereum. And they do not compromise on connectivity and composability with Ethereum. So this is really a very important thing. Let me explain it again. So if tomorrow some organization would like to say, we would like to take something that our organization is running and put it on a blockchain at very big scale, and we want it to have all the good properties of something like Ethereum and to be well connected with it and still maintain some level of control over it, that we can decide how it progresses.
00:50:01.636 - 00:50:54.372, Speaker E: Maybe we need it for privacy purposes. We need to keep privacy of our customers and things like that. Well, with a layer three you can run your system off chain and then submit proofs of integrity. Integrity in the sense of CS Lewis, which means knowing that the right thing was done even when layer one wasn't watching. So you can have all of the scale of the exponential scale that you can get from something like Starknet or from a Starkx system and you can have all of the security that Ethereum offers you and you can have full control and better privacy. So that's what is really amazing and transformative behind layer threes.
00:50:54.516 - 00:50:58.204, Speaker D: And I and we start group think.
00:50:58.242 - 00:51:09.680, Speaker E: That this will see a lot of layer threes that further expand the scale of Ethereum and make it reach basically global scale and more daily usage.
00:51:11.300 - 00:51:52.860, Speaker B: Yeah, totally. I definitely agree with that. And I think layer three can become a real game changer in the sense that they are really best of all worlds. Like you can have full control, you can have your ad specific chain and the full sovereignty on the layer three. And you can benefit from the scaling of Stark net l two and then you benefit also from the security and decentralization of Ethereum layer one. So this is very powerful tool to have full control and also enable very important features like privacy. So Vitali, can you give us a brief overview of how you see layer threes and the future of layer threes in the Ethereum landscape?
00:51:53.920 - 00:54:22.076, Speaker F: Yeah, the concept of layer threes is a tricky one, right, because I think they have a lot of promise and they make a lot of sense in a lot of contexts. But I think it's also important to kind of not overstate it and especially not pretend that you can just kind of do infinite scaling by adding layers on top of layers. And this is literally not trading off anything, right, because layers higher than two do trade off a lot, right? They trade off one really important thing, which is you no longer get unconditional data availability, right? Like if someone is inside of a ZK roll up or someone is inside a validity roll up, then even if the validity roll up operator just suddenly disappears and never talks to you again, you could still withdraw and the system, we could still continue and your assets are still fine, right? But if you're inside of a layer three and someone disappears, then that could potentially stop you from getting your assets out. Now importantly, what they can't do is they can't force invalid states to get confirmed, right? Which also means that they can't steal your money, right? If the operator of earlier three disappears, they freeze your money, but they can't steal your money. It is a bit of a difference and trade off that I think is not universally the best idea for all applications like a billion dollar defi, even if it moves into layer two, as I expect it would not move into layer threes, but at the same time, it's just incredibly obviously the correct choice for many kinds of applications, right? Actually, it's probably better to start off by talking about validiums, right? So validiums are this architecture where basically you have data availability off chain and you still do starks on chain. And the layer three for scalability construction that people are generally favoring is like a validium on top of a roll up, right? And validiums are powerful for enterprise applications because in an enterprise context you do have some amount of trust. And if someone maintaining it just disappears completely, then that's something that's going to be visible to people.
00:54:22.076 - 00:56:48.668, Speaker F: And it's not like they're actually going to be able to freeze people's assets forever or anything like that. Historically, a lot of enterprise applications have been looking into permission chains for scalability purposes, right? But permission chains have just not been successful at all over the last five years, and they all end up running into the same problem, which is that they very easily get the first five or ten users because they can even make the idea that you get to be part of the governance if you participate as a selling point. But once you get to that point, once you try to expand even further, it becomes a liability, right? And anyone who's not an early insider would actually really prefer a credibly neutral base layer. And combining that with the needs or public blockchains kind of legitimacy and function as a really powerful base layer with the scalability of basically something really close to what you get out of a centralized system like vidium, gets you the best of both worlds out of those, right? Or most of the best of both worlds, right? And what this kind of validium on top of a roll up on top of the base chain construction does is it just makes the validium significantly cheaper, right? So instead of just being able to publish new blocks once every few hours or once every day, because there's just this constant gas overhead to publishing a block, you can potentially publish them much more quickly because everything just the roll up also scales the part of the thing that is verifying proof submissions. And actually on the proof submission side, I think if we do the right engineering, then we could potentially get to a world ten years from now where you only really need one proof, right? And that one proof is like a kind of master aggregate proof for the entire block. It's a proof of all the proofs that happened in that block. And so all the proofs get combined together.
00:56:48.668 - 00:57:35.576, Speaker F: But every application still has to choose this question of data availability. And if a data availability goes on chain, then they have this unconditional security. And then if data goes off chain that they have this kind of more still very strong but more conditional security, but with the benefit of just having a much cheaper experience. And some applications go one way and some applications go another way. And I think it's excellent that that entire trade off space is being opened. But in general, I think these kinds of architectures are the place that the scaling space is going to have to end up in. And it's moving toward that direction already to some extent.
00:57:35.576 - 00:57:57.456, Speaker F: Just actually doing all of this properly and making sure that things like roll ups or validiums on top of roll ups can be done in the way that's kind of the best and most efficient and not clumsy. That's just an engineering challenge. And it's great that there's all these.
00:57:57.478 - 00:57:59.090, Speaker C: Teams that are working on that.
00:57:59.940 - 00:58:56.820, Speaker B: Okay, very cool. Talking about data availability, there is this famous EIP for eight four four that is very important for all layer two, and that will drastically improve the efficiency in terms of cost for the data availability. And a few weeks ago you basically published a post basically about a potential alternative to KZG that is currently used in 4844. Can you explain basically all the trade offs around that? And maybe we can talk later about the impact regarding data availability for layer twos and what are the different guarantees that will change after the CIP, et cetera.
00:58:58.600 - 00:58:59.108, Speaker C: Sure.
00:58:59.194 - 01:00:03.290, Speaker F: So 4844, what it does is it basically adds about two megabytes, a block of what's called blob space. So basically space that's just there to contain pure data that does not get executed over and is not indirectly accessible to the EVM. And the purpose of this is to provide data space for roll ups. And the reason why roll ups need this blob space is because roll ups, in order for it to be possible for everyone to agree on what the state of a roll up is, you do need to have perfect agreements on which of the roll up blocks that people tried to submit were actually fully published and made available so that people could scan through them and sync them and update their state and which ones were not made available. And this getting consensus on this question of which pieces of data actually are available and which pieces are not, that basically is what the.
01:00:05.680 - 01:00:06.396, Speaker C: That'S still a.
01:00:06.418 - 01:01:08.430, Speaker F: Significant amount of work, right? Because in a non sharded kind of existing blockchain context, that still requires every other network to download the data, though they only have to do a very tidy amount of computation on it. And then in the further future, there's this concept of replacing all that with data availability sampling. But for now, basically the goal is that you have this data space, and roll ups could use this data space, and there's some clever math that's being done and hold on to that data. But then EIP 44 four is called proto dig sharding because it's an early stage. It's like a stepping stone stage toward the thing that comes later, which is called full dig sharding, which actually includes these kind of fancier ideas like data availability sampling, which tries to actually verify that data is available in such a way where every node in the network only actually needs to download and check a fairly small amount of.
01:01:09.840 - 01:01:44.750, Speaker B: Okay, cool. Maybe, Ellie, you can expand on what is maybe a trusted setup and why some commitments require that and some don't require that, and what are the impact of that? Maybe also because they are the basis of different provincial system like Starc and stocks, and those are the reasons why your system is transparent or not. Even though that the distinction between snark and stocks is not as binary as it was. But yeah. Can we talk a bit more about that?
01:01:46.000 - 01:03:01.300, Speaker D: Yes. So the question is, what is the magic of proofs of these interactive proof systems? The magic is that there's this very weak object, the verifier, that does very, very minimal work. And yet even with almost no trust assumptions, I mean, in the case of a trusted setup, we'll see that there is some trust assumptions. This very weak object can enforce and ascertain the integrity of a much stronger and possibly malicious computer, where integrity, again, means knowing that the right thing was done even when no one is watching. Now, a good analogy for a trusted setup is this very nice fable, the empty pot. In the empty pot story, there's an emperor of China that needs to select an heir to the throne because the emperor has no children. So he tells all children to come and receive a seed.
01:03:01.300 - 01:04:03.800, Speaker D: The seed will soon be the parameters in the trusted setup. So all the kids of the empire received a seed of a flower. They are instructed to put it in a pot, and on the first day of spring, to come with their pots, presumably with flowers, and the one with the most beautiful flower will be selected. So all the kids go, and on the first day of spring, they come back with amazing flowers. There's one kid only, and he is the hero of the story titled the empty pot, who comes back with an empty pot. And the end of the story is that he is the one chosen to be the heir, because unbeknownst to the kids, all the seeds were cooked and barren, and he was the only one who operated with integrity, which means doing the right thing when no one is watching. So what happened here? The emperor had some secret.
01:04:03.800 - 01:04:49.220, Speaker D: The secret was that the seeds were barren and he dispersed it in a bunch of keys. The keys are these seeds given to everyone. And only those who operated with integrity would be able to prove that he operated with integrity. So in a trusted setup setting in a system, there's this secret, this secret that is somehow baked into a bunch of numbers. Those are the keys. And the point is that if you watch the emperor or somehow know what is going into this secret, then you can break the system. That's how a lot of the preprocessing or trusted setup or toxic waste based snarks operate.
01:04:49.220 - 01:05:33.428, Speaker D: They have one very cool advantage, which is that often the proofs that emerge from this are very short, even under half a kilobyte. That's what, for instance, we have in zcash. But there are many disadvantages. Most pertinent to our discussion is that the scale they achieve is, for a variety of reasons, not as good as in these other systems. So the other systems have fewer trust assumptions into them. And starks are an example or a prominent example. And there you have to rely on a higher form of magic in order to complete the task at hand, which is to verify that the right thing was done even when no one is watching.
01:05:33.428 - 01:06:31.716, Speaker D: So with a stark, it's like within magic tricks, card tricks are sort of the highest form of magic, because there are only two hands and a deck of cards, right? And your proficiency with using cards and so on. And starks are like the analog of that. You have no setups in advance, there are no tricks up your sleeve. It's really just math and randomness that come to your help. And with Starks, what happens is that by some very cool math, there's this ability to basically poll, as in polling an election poll, the integrity of a computation, even when the prover can be malicious. And moreover, even when the prover knows that there are no secrets baked into the process. And one downside of that is that the proofs turn out to be longer.
01:06:31.716 - 01:06:55.580, Speaker D: Instead of under 1 kb, they turn out to be somewhere between ten and 100 that range. But there are many, many advantages. You need no trusted setup. You have very great scale. The proving time and even verification time are lower. And just in terms of security, they're just more secure. They rely on more proven math and less assumptions.
01:06:55.580 - 01:07:04.060, Speaker D: So I hope that gives some flavor for the listeners for the difference between preprocessing trust and set up snarks and starks.
01:07:04.220 - 01:07:41.340, Speaker B: Yeah, very cool. Yeah. Like regarding KDG in AP 4844, part of the problem to identify a potential alternative was the trust in arithmetic hash functions, which are relatively new and they are not yet battle tested, et cetera. I'm curious to have your opinion, both you and both you, Ellie and Vitalik, about those new kind of hash functions and the trust we can have on those functions.
01:07:43.520 - 01:07:49.120, Speaker D: I can take first the swing at it. So, with the symmetric cryptography.
01:07:51.620 - 01:07:52.144, Speaker E: I think.
01:07:52.182 - 01:09:23.148, Speaker D: The way that the world reaches a level of trust in constructions is based on, first of all time, having things out there, a lot of people looking at them and having experts vet them and try to attack them and so on and so forth. So the more time passes, the more you can be sure, sorry. The more that you can trust them, and the more they're used, the more trustworthy they are. Unfortunately, to best of my knowledge, there's no better way. So looking at those metrics, certainly something like Posedon Poseidon is not as empirically vetted and used as something like Sha two or sha three. But it's out there already for now something like three years, and already dozens, probably, maybe even a few hundred of security experts have looked at it, tried to kick the tires, and so far it seems to be holding. So I'm quoting various crypto experts like Professor Anne Kanto and Dmitry Hovratovich and Christian and Leopin and others who say, we think this is, we kick the tires, and this looks pretty safe, but I'll be much more comfortable in 20 years and even more comfortable in 200 years.
01:09:23.148 - 01:09:47.700, Speaker D: So let's talk. Then again, Vitalik, what do you say? It I'm curious to hear what Vitalik thinks, but I know that Vitalik is also a proponent of elongating life. So one side benefit, if we all live 200 more years so we can reconvene. But Vitalik, what do you think about the safety of these hash functions?
01:09:48.760 - 01:10:41.544, Speaker F: Yeah, it's a good question. The existing hash functions that we tend to use today, like the sha two hundred and fifty six s of the world, right. They've survived a couple of decades now, and it seems like they've generally survived quite well, like collision resistance, got broken for MD five about a decade ago, and for Shaw one finally, like years ago. But even those were done in a much weaker way than some of the newer versions. And then a pre image resistance has not been broken yet. But the fancier arithmetic friendly hash functions, as I mentioned, Poseidon. And there's reinforced concrete, and there's a couple of other ones.
01:10:41.544 - 01:11:35.320, Speaker F: They're just a very different and kind of younger and more untested family. And there are risks, right? So the main risk is that the fact that they have simple mathematical representations increases the risk that there's some kind of algebraic attack against them. And there have been efforts at analyzing them against specific types of algebraic attacks and Gerber basis attacks and all of these issues and trying to improve things there. And that ended up kind of feeding into the constructions. I guess the unfortunate answer there is just like waiting until people trust these newer hash auctions is just going to be a waiting game, right? And we just have to wait for a while and make sure that it doesn't get broken in the meantime.
01:11:37.180 - 01:12:01.070, Speaker B: Okay, cool. Talking also about optimization of the proving system, can you give your opinion about using small primes instead of big primes? Yeah, I'm curious to hear your opinion. And maybe also, Shah, if you started to look at those prime for Starknet, maybe.
01:12:02.640 - 01:12:03.052, Speaker C: Yeah.
01:12:03.106 - 01:12:52.048, Speaker F: But by the way, I should mention, I do needs to go. It's just getting extremely late here. I'll answer this question. So, basically, one of the interesting things with Starks in particular, right, as opposed to Starks, is that starks give you a lot more freedom to choose which field you're doing your math over. Because traditional snarks are anything that is based off of KZG and even halo. Like all of these, they're elliptic curve based. And so the prime field that they use has to agree with what elliptic curves are providing, which means 256 bits, right? And with starks, you can do 256 bit, but you could also go lower, you could go 64 bit.
01:12:52.048 - 01:13:35.500, Speaker F: And there are a couple of things that have to be done differently. Right. When you do the fry, which is the kind of polynomial commitment part of the ingredient that goes into a stark, you can't have just one kind of regular challenge value at every step. You have to either have multiple, or do some extension field stuff, or do a bit of extra things. But in general, you can do it, and it works. And it has some important benefits. Probably the biggest ones are that they're just faster to compute.
01:13:35.500 - 01:15:30.800, Speaker F: I think it's faster to do math over these 64 bit fields, even if you accept the fact that you're going to need four times more of them to deal with the same amounts of data, but then you start getting more efficiencies, because it turns out that a lot of the time, you don't have to increase the number of constraints by a factor of four to compensate, right? Because in any realistic arithmetic circuit, you are going to have a lot of wires that just have very small constants, like zeros, ones, indices that get incremented numbers in the range of zero to two to 16. Just lots of numbers that you know are small. And so those are going to take up one value regardless, right? They're going to take up one value if it's a 256 bit field, and they're going to take up one value if it's a 64 bit field. And so for those, you actually do want the fields to be as small as possible into the efficiency gains from using a 64 bit field instead of 256 bit. Like, in principle, I guess it's anywhere from a factor of four to a factor of 16 improvements, probably closer to 16 because multiplication is close to n squared at those levels. And then there's really nice specific fields. Right? I know some people are big fans of the Goldilocks prime, which is like two to the 64 minus two to the 32 plus one, which is really cool because it lets you do basically all of the arithmetic just using bitshift operation, or not bitshift, but even 32 bit operations, which is really powerful in general, the greater flexibility that starks offer because they're not tied to some elliptic curve thing, just opens the door to all of these really cool optimizations, and it's something that's really worth considering.
01:15:32.020 - 01:15:47.336, Speaker B: Okay, thank you very much for this answer. And, yeah, feel free to go. I know it's very late for you, and thank you very much for your presence. We will conclude soon the space and let some people ask some questions, but feel free to.
01:15:47.358 - 01:15:48.212, Speaker D: Thanks, Vitalik.
01:15:48.276 - 01:15:49.368, Speaker C: Yeah, thank you.
01:15:49.534 - 01:15:57.112, Speaker B: It was very cool to have you and looking forward to do more spaces with you. Thank you very much. Absolutely. Thank you. Thanks, Vitalik.
01:15:57.256 - 01:16:07.068, Speaker D: I want to hear Shahar's thoughts on small fields. I've been trying so hard to convince him, and he's know, not budging. Right, Shah?
01:16:07.164 - 01:16:26.880, Speaker B: Yeah. Shah, what do you think about using small fields? We can't hear you. Yeah. Okay. I can jump to another topic.
01:16:26.960 - 01:16:52.300, Speaker D: I can say Shahar is strongly opposed to small fields, and we argue about it constantly. And I'm very happy he can't answer now because I'm basically saying out a bunch of lies. I don't think he really opposes small fields, so. Yeah, he's the reason we don't have any small fields. Everyone's trying to convince him and he's against it, I hope. Yeah. He still can't speak.
01:16:52.300 - 01:16:53.664, Speaker D: Oh, he's not connected yet.
01:16:53.702 - 01:16:53.856, Speaker C: Good.
01:16:53.878 - 01:16:56.048, Speaker D: So he can't hear us. He joined again.
01:16:56.134 - 01:16:57.984, Speaker B: Maybe. Charles, you can try again.
01:16:58.182 - 01:17:03.280, Speaker D: Yeah, I was saying how you're the reason we don't have any small fields, how you're opposed.
01:17:03.700 - 01:17:09.590, Speaker A: You're lying. You're lying to your teeth about me. Yes, I know. Can you hear me though?
01:17:10.680 - 01:17:12.228, Speaker B: Yes, we can. Okay, great.
01:17:12.314 - 01:17:51.490, Speaker A: Yeah, I think small fields, small primes is a very cool idea. I strongly believe that in the future it's inevitable that we will have to move to something like that. But it's probably in the future. Right now this efficiency is not a bottleneck, it's not close. We have a lot of other stuff to do before, including the car 1.0, including decentralization, a lot of things that should come before this. But I think that Caro 1.0
01:17:51.490 - 01:18:08.410, Speaker A: is designed with this idea in the back of our mind that we might one day shift to another prime and we want to be able to support this. But like I said, it will probably be in the future, not right.
01:18:11.180 - 01:19:03.960, Speaker B: Okay, very cool. Yeah. To conclude, I will talk a bit more about the community aspect of Kakarot and then we can take some questions from the audience. So, as I said, kakarot is a community driven project and we are looking for contributors to participate to this experiment to build ZKVM in Cairo. So if you are a builder, feel free to jump on the GitHub and take some issues. We need some people to write in Cairo. We need also some people that have front end development skills because we want also to build some tooling to make it easy to use it in a real world context.
01:19:03.960 - 01:20:10.846, Speaker B: Likely we will need also to build a backend like the idea ultimately would be like you could imagine, to add a new RPC node on your metamask or any other Ethereum wallet. And this backend will do some adaptation between an actual ethereum transaction and Kakarot so that you can have maximal compatibility. So we need also some rust developers for that. So yes, the message is really, if you want to jump in and participate to this community driven Zkavm, feel free to join us. We welcome any kind of contributions. And yeah, I think we can take some questions from the audience. Okay, yeah, I added you as a speaker if you want to ask your question.
01:20:10.948 - 01:20:12.880, Speaker G: Okay. Hi, can you hear me?
01:20:13.570 - 01:20:14.382, Speaker B: Yes, we can.
01:20:14.436 - 01:20:38.760, Speaker G: Okay. I just had a small question, because I heard about you shilling the small prime field, and I'm curious because for me, the smaller the prime field, I mean, in terms of number of pits inside it, the less the security you have. If you try to brute force a private key, for example.
01:20:39.850 - 01:20:56.430, Speaker B: No, it's not the same because you can do multiple rounds, actually. It's just that you will, for example, doing four rounds on small prime is much efficient and this is the same security level. I don't know if it's clear. Maybe shah want to explain more precisely.
01:20:58.210 - 01:21:12.850, Speaker A: If you need operations over larger fields, like in a specific place for security, you can use extension fields over your base field and simulate a big field. Or you can do multiple sampling.
01:21:15.110 - 01:21:15.522, Speaker B: Instead.
01:21:15.576 - 01:21:28.002, Speaker G: Of having a 256 bit prime or around it, you have four rounds of a 64 byte prime, and it's cheaper and faster.
01:21:28.146 - 01:22:12.690, Speaker D: Yeah, I can say it's even more something that will sound even more surprising. You could work with a 30 bit field, or with a 20 bit field, or even with, I don't know, 15 bit field, and you could still have 128 bit security, or 256 bit security as long as you have a commitment scheme. So you would need, let's say, to use maybe a digest size that is 256 bit long in your Merkel hash for the various commitments. But those have nothing to do with, basically with the security that you get from the field. And then you can actually use very small fields again, 16 bits, 32 bits.
01:22:13.030 - 01:22:16.550, Speaker G: Okay. Surprising. Exciting also. Okay.
01:22:16.620 - 01:22:43.870, Speaker D: Yeah, exactly. As I said, starks have this higher form of magic that is a little bit like a magic just done with your cards and hands. The analog here. There are no number theoretic assumptions, there are nothing about elliptic curves or large primes. It's basically just a lot of pretty deep algebra and randomness.
01:22:44.530 - 01:22:48.290, Speaker G: Okay, thank you for your answers.
01:22:49.270 - 01:22:55.300, Speaker B: We have two more questions from the audience. So maybe VK block you want to start and ask your question?
01:22:56.390 - 01:23:10.040, Speaker A: Right. Thank you for the opportunity. So my quick question here, you mentioned about the Kakarot project. So is it under Cairo or is it like a separate project? Can you share a link, maybe in a tweet or in the space?
01:23:10.990 - 01:23:35.090, Speaker B: Yeah, we can definitely share a link. So it is a community project. So everyone feel free to contribute if you want. So we can definitely send the link of the GitHub repository. Everything is open source. It is fully open source. And yeah, actually we already linked in the Twitter announcement of this space.
01:23:35.090 - 01:23:47.650, Speaker B: But yeah, we can send the link later and feel free to jump if you want to contribute. Monkey, you want to ask your question?
01:23:47.800 - 01:24:08.060, Speaker H: Hey, I got a question. I'm wondering if anyone has any predictions of when we're going to have a ZkevM or when we can zke eth. And what would be the proving time for that kind of thing?
01:24:08.430 - 01:24:26.620, Speaker B: Okay, can you clarify? Do you talk about all the KVM in the landscape or specifically about Kakarot? Like, you mean polygon, scroll, decay, sync? Are you talking about that?
01:24:27.650 - 01:24:39.760, Speaker H: If we could ZK east itself I'm guessing we'll have to get it down. Like proof time. Get it less than 12 seconds or something like that.
01:24:41.350 - 01:24:47.058, Speaker B: You mean using the ZK proof to scale the l one itself?
01:24:47.224 - 01:24:53.860, Speaker H: Exactly. Yeah. Not exactly scales, but. So that I could prove to you that this blocked me.
01:24:56.090 - 01:25:00.920, Speaker B: I don't know. Honestly, I don't know. Shah or Ellie, if you have any.
01:25:02.410 - 01:25:06.410, Speaker H: Just like any predictions or intuitions on the topic.
01:25:09.870 - 01:25:46.502, Speaker A: If any project which is type one full equivalent you can probably just start proving bunch of past blocks, et cetera and just consume it and get big proof for this. It doesn't really matter the amount of time it takes. If you want to prove it retroactively you could just distribute it over a lot of computers. Maybe everyone can take a chunk so it doesn't really matter. I think the amount of time it takes more like the cost.
01:25:46.636 - 01:25:50.710, Speaker H: You don't have to do them one by one. Can you do like blocks?
01:25:52.250 - 01:26:10.150, Speaker A: Right. It's just state transitions. Right. You can say the transition from block zero to block 100 is valid. This is one task. Another the transition from block 100 with block 200 to distribute this if you want.
01:26:10.240 - 01:26:40.950, Speaker B: Yeah. For example, there is a project built in with Cairo which is called zero sync. And basically the idea of zero sync is to prove the entire bitcoin chain using stock proofs like basically. And using recursion to prove recursively the entire bitcoin chain. So this will be similar to what you are saying, but for ethereum. So yeah, this is feasible, but it's hard to do some prediction about the efficiency.
01:26:42.090 - 01:26:42.840, Speaker A: Yeah.
01:26:44.650 - 01:26:45.170, Speaker C: Great.
01:26:45.260 - 01:26:59.200, Speaker B: Okay. Any other question before we conclude this space? I can take one more question from the audience if someone wants to ask something. Okay, maybe not.
01:26:59.890 - 01:27:01.182, Speaker C: I can have a question.
01:27:01.316 - 01:27:03.120, Speaker B: Yeah, go ahead.
01:27:03.570 - 01:27:15.090, Speaker G: Maybe I need it during this life because I join later. But what about this 9000 stuff about Kakarot? Is it the number of transactions?
01:27:18.630 - 01:27:21.170, Speaker A: Obviously not a Dragon Ball Z fan.
01:27:21.910 - 01:27:41.450, Speaker B: Actually he is. Actually, I think he is. Yeah, I think he is. It's over 9000. Is a famous meme about dragon Ball. So it means nothing for the moment in the context of kakarot, it's not the throughput.
01:27:43.790 - 01:27:48.574, Speaker G: I thought it was 9000 TX per second or something like that.
01:27:48.692 - 01:28:16.342, Speaker B: No, it will be cool, but no. Okay, maybe there is one more question. Or maybe it's a bot. Let's see. Are you a bot or a genuine user? Let's see. Okay. It does not work anyway, so I think we can conclude now.
01:28:16.342 - 01:28:22.920, Speaker B: So thank you very much. There is a question from one of our teammates, which is Tom?
01:28:24.410 - 01:28:26.780, Speaker A: Is Tom a bot or is he real?
01:28:27.710 - 01:28:31.350, Speaker B: Good question. Tom, are you a bot?
01:28:31.430 - 01:28:32.010, Speaker C: No.
01:28:32.160 - 01:28:33.402, Speaker D: Sometimes I wonder.
01:28:33.536 - 01:28:35.322, Speaker C: I know I'm not a bot, but.
01:28:35.376 - 01:28:45.840, Speaker A: Abdul, I wanted to ask, do you know if Kakarot will reach the fourth stage of the super cyan or not?
01:28:47.250 - 01:29:04.680, Speaker B: Oh, good question. Actually, I think we can go up to type two. So this will be Super Saiyan three, I guess to have full type one, I cannot do that alone. I will need char. If we have char, I think we can reach any level.
01:29:05.450 - 01:29:10.470, Speaker A: Okay. If we get type two, I'll help you get to type one. Okay.
01:29:10.540 - 01:29:15.910, Speaker B: This is a good motivation to get to type two, actually. Okay, challenge accepted.
01:29:16.750 - 01:29:18.540, Speaker A: Thank you very much for your answer.
01:29:20.430 - 01:29:21.082, Speaker B: Thanks.
01:29:21.216 - 01:29:21.610, Speaker C: Okay.
01:29:21.680 - 01:29:42.366, Speaker B: Thank you very much. It was a very cool space and yeah, I really enjoyed it. We will do more spaces like that in the coming weeks. It was very cool. So, yeah, thanks a lot. And the space is recorded, so there will be the link for people that missed it. And yeah, really enjoyed it.
01:29:42.366 - 01:29:47.280, Speaker B: So thank you very much, Eli shah. It was very cool. Thanks.
01:29:48.930 - 01:29:49.726, Speaker A: Thank you.
01:29:49.828 - 01:29:50.510, Speaker B: Bye.
01:29:51.410 - 01:30:02.020, Speaker D: Thank you, Abdel. And thanks so much for spearheading this Kakawa project. I think it's amazing. And of course, thanks to everyone who's participating in it.
