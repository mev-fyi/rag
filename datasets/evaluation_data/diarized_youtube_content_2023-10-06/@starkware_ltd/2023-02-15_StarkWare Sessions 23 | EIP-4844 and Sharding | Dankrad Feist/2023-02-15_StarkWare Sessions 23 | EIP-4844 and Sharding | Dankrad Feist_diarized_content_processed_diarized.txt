00:00:00.330 - 00:01:00.350, Speaker A: You. Hello, my name is Dankard Feist. I'm a researcher at the Ethereum foundation, and I'm going to give a talk today about EIP 4844 and the road to sharding. Does this work? Okay, so as an outline of today's talk, I'm going to to start talking a little bit about blockchain scaling. So that's clear why we're solving this important data availability problem. I'm going to talk about roll ups, which are going to make use of data availability, and then I'm going to talk about how we're going to develop Ethereum's data availability layer. So we're going to start with the data availability roadmap, going to talk about EIP 4844, and then finally how EIP 4844 can naturally be turned into a scalable data availability layer.
00:01:00.350 - 00:01:47.920, Speaker A: Okay, so let's talk about blockchain scaling. The famous blockchain scalability trilemma formulated by Vitalik is that it is difficult to design a blockchain that provides the three properties of scalability, security and decentralization. It's not a claim that this is impossible. And in fact, actually now we know how to design a blockchain with all three of these properties. It's just that a technology that was available in around 2015 was not there. Like we didn't know how to do it back then. But now all the basic building blocks are there and it's a matter of actually implementing it.
00:01:47.920 - 00:03:09.690, Speaker A: Okay, so one important thing here we have to talk about is decentralization. So why is it so difficult to fulfill this scalability trilemma? Because to us, decentralization means that you can run a node at home, like that, you are able to participate in this network, and you are the one who actually has control over, in particular, what security means. That you cannot be fooled, for example, into accepting some chain that is invalid or misses some other important properties of a blockchain. And this puts some very important constraints on us, because this little node that you have at home has a limited capacity for execution, and it also has a very limited bandwidth, like you can't typically download gigabytes per second on your at home Internet connection. We have to reduce this to something that is very easily manageable for at least most of the people in this world. If we look at the blockchain stack nowadays, we see basically four different layers, execution, settlement, data availability and consensus. And two of these require scaling.
00:03:09.690 - 00:04:30.028, Speaker A: That's because settlement and consensus aren't ultimately properties that depend necessarily on the number of users that a blockchain uses like you can settle everything like with very few very big transactions, and consensus ultimately can just come to consensus about very small amounts of data, say like a merkel root of the history. So the two properties that really require scaling are execution and data availability. And for execution, basically about four or five years ago, we found a very elegant solution that comes in the form of roll ups, where we separate the execution from what the actual full nodes that you have at home have to do by adding some form of proof to the transactions that are either fraud proofs or validity proofs in order to make sure that execution was done correctly by someone else. And data availability is another part that needs scaling. And we have found a solution for that that is scalable in terms of data availability, sampling. Okay, so let's talk about the data availability problem, because it's quite subtle. I want to have some clear terms here so that we all understand what it means.
00:04:30.028 - 00:05:51.210, Speaker A: So what data availability means is that no participant of the network and no participant includes a supermajority of the full nodes, or the consensus has the ability to withhold data, right? So it's a very strong condition we don't want. Even if all the Ethereum validators today try to convince you that some data is available that is not, they should not be able to withhold it. And so current blockchains like Ethereum today or bitcoin today, they solve this using a very simple mechanism, which is all the full nodes, including your little node at home, they download all the data. And so it's impossible to withhold the data because a block is simply not valid if it comes without the data, like you have no chance of fooling a full node into accepting an unavailable block. But the big question is, how do we make this scalable? Scalable means that the work required should be less than that of downloading the full blocks. So we need the work to not be o of n, but to be, say, a constant or a logarithmic amount of work. So importantly.
00:05:51.210 - 00:06:17.830, Speaker A: So data availability. I think, like Mustafa said this earlier in his talk quite nicely as well. He said it should really be data publishing. And I think that's a very good way to think of it. It's the assurance that data was not withheld or the assurance that data was published. That's how we use this term. In particular, what we don't mean by data availability is data storage or continued availability.
00:06:17.830 - 00:07:03.462, Speaker A: And this is actually not because these properties are not important to many people. These are very important. They are like, why don't you care? Why don't you want to make sure that it's available forever? It is because once you have this first form of data availability, the others are actually not that difficult to solve. So the first one is a very, very hard problem because we want it even in the face of a malicious supermajority who are trying to fool you. Whereas the other two we can actually solve using simple incentives, like if you have made sure that the data is available in the first place. If you just pay enough people to keep it, then the probability that one of them will have it is very, very high. The problem arises when the first step hasn't happened.
00:07:03.462 - 00:07:40.882, Speaker A: So that's why we're so focused on the first step. And I think a nice analogy I have for this is always think of data availability a bit like a public notice board. We have this tradition in some places. For example, if you have make a planning application or something changes, right? Then you put the notice up on a notice board to basically guarantee that everyone who wants to see it has a chance to see it. That's what data availability does, it gives you the chance to see it. Okay, let's talk very quickly about roll ups. I'm not going to go very deeply because it's almost what this conference is about in a way.
00:07:40.882 - 00:08:24.718, Speaker A: So I assume most of us roughly know what it is. But basically what roll ups do is they basically use the blocks that we have already, for example, on ethereum, and they just add their transactions as data. And so from the consensus point of view, like from the base chain, it's just data that comes with proofs. And there are basically two flavors. They are the optimistic roll ups that rely on fraud proofs in order to achieve validity. And they are ZK roll ups which use validity proofs, bit of a misnomer, because typically they don't actually require zero knowledge. And they actually both depend on this data being available in a very important way.
00:08:24.718 - 00:09:14.492, Speaker A: Because in order to construct fraud proofs, you need the data. Otherwise someone can just simply post an invalid blog, never publish the data, and you're like, I have a suspicion, but unfortunately I can't prove it. And so that's why it's very important to have the data for optimistic rollups, for ZK rollups, it's slightly more subtle, but basically in order to access your account, you need to know what's in your account. And that's why you need to have this update to your account. You need to be able to know what happened to your account in order to get full security in a ZK rollup. Okay, so let's talk about Ethereum's data availability roadmap. So what we have today in Ethereum is we have data availability.
00:09:14.492 - 00:10:01.264, Speaker A: It's just very expensive. So you can get data availability by just putting your data into the call data of a normal transaction. And that guarantees, as we said before, like all nodes download this data. And so that guarantees data availability on the Ethereum network. And what we are basically hoping to roll out this year is an EAP called EAP 4844, which is also named Proto Dank Sharding. And what it does is it adds to our blocks an amount of blob data. And that's basically data that only considers data like it doesn't even get to the execution engine.
00:10:01.264 - 00:10:47.756, Speaker A: And more details on that I will give later. But basically it is an extension that makes data a bit cheaper. But what it is not, it is not scalable yet. So it does not have this property that full nodes need to do less than a linear amount of work. They still need to download all this data. We will probably over the next years, extend this to make it more efficient. But the big upgrade that ultimately puts scalable data availability into Ethereum will be full sharding, which basically can still use these blobs that we're going to introduce now, but will do data availability sampling on them, which will finally make it scalable.
00:10:47.756 - 00:11:33.226, Speaker A: And the timelines are my personal estimates. Unfortunately, since Ethereum is quite decentralized, no way of actually guaranteeing when anything gets done. But I think roughly two years to solve the sort of networking challenges that are currently still there, because this is quite a new thing is, I think, realistic. Okay, so let's talk about EIP 4844. So this protodank sharding is an extension of Ethereum that adds data blobs to the protocol. The blobs will be priced independently from the execution. So there's a new type of gas.
00:11:33.226 - 00:12:49.090, Speaker A: And the blobs, like the data inside the blobs, is not required to compute the state updates of Ethereum, and it will only be stored for a short period. As we said before, we want to provide this data publishing, and we don't want to guarantee that the Ethereum network itself stores this forever. And this construction is particularly designed with future upgrades in mind. So what we use are KCC commitments, and these are not really required for what we're doing with 4844 to just add these data blobs, but they are designed so that it will be very easy in the future to add full erasure coding and data availability sampling, which is really cool because it actually means future upgrades can be done without any further consensus changes to Ethereum. So in principle, once this EIP is implemented, which hopefully will happen this year, we can do all the further upgrades by just doing the networking that is required. And the roll ups in particular will also not have to upgrade again to benefit. So roll ups should only have to make a transition once now from using, they are currently using call data to provide data availability and they will have to use these blobs.
00:12:49.090 - 00:13:39.500, Speaker A: And that's quite a major upgrade, let's be honest. Like using a new type of cryptographic commitments and all the engineering around that. But the cool thing is then in the future they should not have to upgrade again. So they will get this full scalable data availability layer, which will probably much cheaper. And even if we in the future upgrade the commitment, we have put in some design choices, so that is likely that we can support almost everything without requiring any further upgrades. Cool. So basically, what does this look like when it's on the network? So inside the blocks in the future, we will just have the KCG commitments to these blobs and completely separate on the network.
00:13:39.500 - 00:14:34.050, Speaker A: Somewhere we will transmit the blob data. And that's quite cool, because you can already make some cool improvements to the network if you don't always have to transmit blocks as one huge chunk of data. And what we're going to add to Ethereum is this new type of transaction, which are called blob transactions. And so the transaction itself will include a number of versioned hashes. They're basically hashes of KCG commitments of blobs. And the reason why we add this version hash is so that we can upgrade the commitment scheme in the future. And it's possible that at least if there are no, like, if we can use a similar type of field and so on, then we can actually make all these upgrades without having to change smart contracts at all if they've been designed well.
00:14:34.050 - 00:15:34.280, Speaker A: And so each version hash links to a KZG commitment, and that KZG commitment commits to an amount of blob data. And basically what Ethereum gives to the transaction is the guarantee, hey, here is the hash. You only have access to this version hash, but I guarantee you if this is included in an Ethereum block, then the data for this was available. And the second thing we're going to add is a new pre compile. So we'll add something some way in which smart contracts can actually use this blob data. And the way it works is basically it takes as an input a version hash a point, z and y. These are field elements, the KZG commitment and a proof, which is a KZG proof.
00:15:34.280 - 00:16:46.596, Speaker A: And this point evaluation pre compile verifies that f of z equals y, where f is basically the function committed to by the KZG commitment. Why are we adding this pre compiler and how do you use it? Okay, so here are basically the two major examples. So for optimistic roll ups in practice, right now, almost everyone I know who's designing optimistic roll ups for efficiency reason are using multiround fraud proofs. What this is basically it's a game between a prover and a challenger that guarantees that if the proverb was correct, they can always win this game. And if on the other hand, someone has found fraud and they challenge the prover on that, they should always be able to win this game based on that fraud. And basically it's this game that they play on chain, and ultimately it reduces to some disagreement about the data. And so they will say, we disagree about this point.
00:16:46.596 - 00:17:52.920, Speaker A: And then the final round of this game can just be we use this pre compile to prove what the data was at this point. So you will evaluate the blob in its range. Like this function will be on one of the points of data that you have committed to. Now it's different for validity rollups, and it's like a little very cool trick. Basically it's based on this Schwartz sipple Lemmer that says if you evaluate two polynomials at a completely random point and it's the same, and you're in a large enough field, then it is very, very likely that the two polynomials are the same. And so basically what validity rollups can do is they can use this pre compiled prove the equivalence to an internal commitment scheme by evaluating both their internal commitment and the KCG commitment at a random point via like a fiat Jamia protocol. Or they can also just compute the complete KCT commitment inside their proof.
00:17:52.920 - 00:18:55.612, Speaker A: Cool. So how does this extend into full sharding? So the ultimate goal of Ethereum is to have a scalable, decentralized and secure data availability layer. And what this requires is a technique called data availability sampling. So I want to quickly go into that. So what data availability sampling means is that you take your data, you chunk it into like little chunks, and basically the client, which is your node, they just choose a few random of these chunks and request them from the network. So the problem if you do this naively is if I just hide one piece of data, then it's very unlikely if I do a small number of samples, that you're going to find it using just random samples. So the trick is we have to first erasure code the data.
00:18:55.612 - 00:20:00.210, Speaker A: So if we erasure code the data, we do that, for example, using a read Solomon code, which means we interpolate a polynomial through the data, and then we evaluate that polynomial at more points. And what that means is now that I only need some amount. For example, in this example, it's half of the data samples in order to reconstruct all the data. That's because if I have four points to start with here, and I interpolate a polynomial, I will always get a polynomial of degree, at most three. And then if I evaluate that at four more points, what I get is eight points, and any four of these will give me the same polynomial. And so that's really cool, because now our sampling only has to ensure that 50% of the data is available, rather than 100%, which we can never get through sampling. So this is really cool.
00:20:00.210 - 00:21:01.110, Speaker A: But the problem is that we now have a new requirement, which is that we have to ensure the correctness of the code. And this is where the KCD commitments come in. That's why we're using them, because they always commit to a polynomial. What we do is we commit to data as KCG roots, and we just make the samples evaluations of this polynomial. And we always supply a proof, and that means we always guarantee that all the samples we get are in this polynomial, so they're automatically a correct read solmon erasure code. And what we'll do is basically we have a number of commitments, and we supply another number of commitments, which are the extension commitments. And what we get is a square.
00:21:01.110 - 00:21:51.344, Speaker A: This doesn't work, which means that our data is basically extended in two dimensions into a two dimensional polynomial. And what that allows us is that we can efficiently sample, but we can also efficiently reconstruct. Since each row and each column is itself a polynomial, it can be reconstructed locally. So it's a natural extension of 4844. So all the extended commitments can be computed from the blob commitments, so no consensus changes necessary. And the guarantee is as long as 75% of the samples are available, all data can always be reconstructed. And it can be locally reconstructed, so by nodes which only observes rows or columns.
00:21:51.344 - 00:22:23.264, Speaker A: And we never need any super full node that observes the whole square. And. Yeah, so this is data availability sampling on this. So basically you will get random samples on the square. Each full node will check 75 of these and that ensures that the probability of any not unavailable block passing is something like two to the -30 and the bandwidth required is really low. So it's amazing. 75 times half a kilobyte per 16 seconds would be about 2.5
00:22:23.264 - 00:23:16.350, Speaker A: kb/second so it's much lower than Ethereum is now and that achieves like a huge amount of data availability. So this is how we extend EIP 4844 into full charting, which is like right now it will be tens of kilobytes per second and we'll get to over 1 question is like, well, I mean sounds like most of the things are ready like what needs to happen. So the difficulty is actually like the sampling is a difficult networking problem in order to construct a decentralized networking working layer that can provide these samples. And this is currently a matter of very active research like what is the best networking design for this that is both robust and decentralized? Thank you.
