00:00:03.950 - 00:01:09.960, Speaker A: Okay, so I'm here to present our work about reinforcement learning for selfish mining work by me, Ital, who comes from the blockchain community, and Aviv Tamar, who studies reinforcement learning. This is the I'm going to talk about my previous paper and my current and research also. So the motivation is to analyze the incentives of participants in a complex protocol such as a blockchain. We want to study selfish mining, which is some form of behavior a miner can take in order to gain more rewards than their fair share. This causes unpredictable behavior of miners and can lead to less security. And the main problem is that it is possible the previous work showed a specific strategy which manages to achieve more rewards. Okay, so our goal is not only to ask whether it is possible or not, but to ask when it is possible.
00:01:09.960 - 00:01:53.860, Speaker A: And what ALMCr did in their work was to show a specific strategy which manages to achieve more. So that ensures that selfish mining is possible, but doesn't answer the question of when it is possible. So in order to do so, we want to find the best strategy. And if we know the best strategy, we can see whether or not it is selfish. And we can do this for different minor sizes and see for what parameters of the model selfish mining is possible. And this can be done by modeling the problem as an MDP and then solving it. But the main issue is that the MDP has a nonlinear utility function.
00:01:53.860 - 00:03:21.620, Speaker A: In order to overcome this, a previous work by superior steamental manages to design an iterative method where in each stage you solve an MDP, and this method converges to the solution of the original MDP with the non linear utility function. Another work which continued this line was a work by square called squarel by Huet al, and they managed to use deep reinforcement learning in conjunction with the previous method by superior in order to recreate the results for bitcoin. And they did this with deep reinforcement learning, which is a method which is supposed to be more memory efficient. But it does mean that there's no bound for the error of the method. Our goal is to generalize this and get a more general and efficient method of finding the best strategy. And this will be based on my previous paper, efficient MDP for selfish mining in blockchains, which gives an alternative method for Pearson's method in order to overcome the nonlinear utility function. So just to get a quick idea of what the paper talks about, I'll just give the picture now and then we'll fill in the details later.
00:03:21.620 - 00:04:23.540, Speaker A: So this paper talks about blockchains like bitcoin, Ethereum and we want to analyze the selfish mining certain behavior, which I'll explain later what it means exactly. And we want to analyze this by using MDP's Markov decision processes, which is a tool for stochastic sequential decision making. And our method is called PTO probabilistic termination optimization. And the main thing is that it is faster than state of the art when comparing it to superior's method, where it manages to achieve the same results but ten times faster. So this can be a stepping stone into combining it with deep reinforcement learning, like square lead, but with this method, and it should converge faster. The other part, I'll talk about DPArL briefly. This is in an intersection of many fields, computer science, neuroscience, and more.
00:04:23.540 - 00:05:06.930, Speaker A: And the main idea is to solve an MDP approximately, but it can do so for mdps with a very large state size, which can be untractable. And essentially, it can solve mdps, which cannot be solved using exact methods. It's based on deep learning, which utilizes neural networks for function approximation. But again, as I've said before, this gives no guarantee for the results. We can give any error bounds, but usually it just works. Okay, here's a glimpse of the results I'm going to talk about later. But you can see, we used our method, we managed.
00:05:06.930 - 00:06:02.706, Speaker A: Let's just focus on the top graph. You can see how the revenue of a specific policy given by the algorithm as the algorithm runs for more iterations, and it usually reaches the dashed line at the top, which is the optimal solution, which was calculated using previous methods. I'll show some more results later, but these are preliminary results not present in the paper, but using deep reinforcement learning, which we continue to work on after the paper. Okay, so that was the big picture. Let's fill in the details now and talk exactly about selfish mining. We'll discuss Markov decision processes, explain how to model a blockchain as a mark of decision process, and explain the issue of the nonlinear utility function. Then I'm going to talk about our method of overcoming the nonlinear utility function.
00:06:02.706 - 00:06:58.566, Speaker A: And if we'll have time, we'll talk more about deep reinforcement error and how to combine this with our method and show some more results. Okay, so let's start with selfish mining. Essentially, this is some form of strategy which a miner can choose by deviating from the protocol and gaining more rewards, more than her fair share in bitcoin. Usually, if a miner has 30% of the network, 30% of the hash power, she can gain 30% of the rewards. But sometimes selfish mining is possible, and it allows the miner to gain more. So a simple example would be that a miner keeps two blocks in secret, block C and D, and she tries to mine both blocks, one after the other, without revealing block c. So no one else can mine on block c.
00:06:58.566 - 00:07:46.260, Speaker A: And if she manages to do so, she can reveal both blocks at once. And other miners would have to drop block b and follow block d. The intuitive id is to risk previous works. So the miner mined block c and then didn't publish it, and tried to mine block d afterwards. So there was a chance it could have failed. But if it does succeed, it manages to waste block b, which is, it can waste other people's work. And overall, this decreases the effective computational power of the network, and this can increase the share of the rewards of the miner, even for this hurts the network in total, the miner herself can get a larger share.
00:07:46.260 - 00:08:30.290, Speaker A: Okay, so mark of decision processes. So this is a form of an, I can see the lines. So this is some form of an iterative process where there's an agent and an environment, the agent chooses some form of action. The action then affects the environment. So the environment is captured by some state s from state space. And then once an action is chosen, the environment transitions stochastically to the new state, and this distribution depends only on the action and the previous state. So this is what makes the process markovian.
00:08:30.290 - 00:09:45.062, Speaker A: Then, after the transition occurs, the agent observes the new reward, sorry, the new state of the environment, and receives some reward from the environment. And the object is for the agent to maximize some form of utility function and choose the policy, which is a function from the state space to the action space, which means that for every state, this determines for every state what the agent chooses to do. And we can have different objective functions, such as an average reward, which is the average of all rewards up to time t when you take t to the limit. Or another criterion is the stochastic shortest path, which assumes there is some terminal state, and we want to maximize the sum of rewards until termination. For solving mdps, there are standard methods in the literature for reinforcement learning. There are exact methods which are based on dynamic programming, such as value iteration, policy iteration, and q learning. I'm not going to talk about them in detail, but they are guaranteed to converge.
00:09:45.062 - 00:10:30.940, Speaker A: And when they converge, they can provide arbitrary procedures. The longer you run them, the better results you get. Other types of methods are approximate methods, and as I've said before, these are usually classified into three categories, value based methods, policy gradient, and model based RL. These are not guaranteed to converge, as I've said before, and there's no error bound for the results. But these can be used when the state space of the MDP is too large and we can't use the exact methods. In our work, we focused on a specific class of methods, deep q networks, which are value based. It's a value based method, and we're going to talk about that later.
00:10:30.940 - 00:11:22.860, Speaker A: Now, how do you model a blockchain as an MDP? So we assume we have some rational miner who wants to maximize her reward per unit of time because of the difficulty adjustment mechanism in proof of work blockchains. So maybe I should say that first, our work only applies for proof of work currently. And we assume there is a difficulty adjustment scheme, which means that the time between blocks is variable. Sorry. Which tries to make sure the time between blocks is constant. But this means that when you, when. Sorry.
00:11:22.860 - 00:12:13.740, Speaker A: When the miner chooses some strategy, she can affect the total computation power in the network by reducing, by wasting work. So this would affect the difficulty adjustment mechanism. And overall, this means that miner wants to maximize her share of the rewards. For example, if you take bitcoin, for example, you can get some sort of function like this, where r is the number of blocks of the miner and the denominator can be the total number of blocks. So we can think of a function like this to have. The numerator is the rewards the miner gets, and denominator takes the time into account. So as I've said for bitcoin, this can symbolize the number of blocks of the miner divided by the total number of blocks in the system.
00:12:13.740 - 00:13:26.622, Speaker A: Okay, so let's talk briefly about previous work by Natal and how they overcame this. First of all, we call such an MDP with a utility function like this, an ARR MDP, or an average reward ratio MDP. So they designed an iterative method where you solve the MDP and every step you transform the utility function into another utility function, which is determined by some parameter all. And when you perform a binary search on this parameter, the solution converges to the solution in the original MDP, which is also an error MDP. You can see they modeled it as r one to be the number of blocks of the miner, and r two the number of blocks of everybody else. So we get the exactly same function, but they transform it into a linear function, and then they can solve an MDP in each stage. For an MDP with a linear utility function, okay, they use this for miners of different sizes and different parameter models.
00:13:26.622 - 00:14:16.820, Speaker A: And they found the security threshold of bitcoin, which is a minimum minor size required for performing selfish mining. So as long as all miners are below that threshold, you get that the system is secure from selfish mining. But if there is some minor, which is larger than that, you can have problems. Okay, now let's talk about our alternative method, probabilistic termination optimization. The intuition behind this is to overcome the nonlinear utility function in a different way. So this is the utility function, and we want to make that linear. So an intuitive approach would say, why not just stop after h blocks? If we stop, when we reach h blocks, the denominator will be equal to h, and this will be a linear function.
00:14:16.820 - 00:15:01.730, Speaker A: But the problem with this is that in order to stop after h blocks, we need to keep some form of count of how many blocks we've had so far. And this increases the state space. And in order to overcome this, we just say, why not end the process after age blocks in expectation, and we will call age the expected horizon. We won't have any need to count blocks. And I'll explain exactly how we do this. So, we take an ARR MDP, we transform it into a probabilistic termination MDP by introducing a new terminal state and giving each transition some probability to terminate. Okay, so this is the probability given.
00:15:01.730 - 00:15:49.914, Speaker A: The idea is that each unit of dt is equivalent to some cointos, which with probability one over age gives a chance to terminate. So in expectation, we'll have age blocks. This gives us an MDP, which is a stochastic short spot, and this is its utility function. And the main result from our work is that the two utility functions for a given policy are equivalent up to one over age times a constant. So, since we choose age, this can give us arbitrary precision. The larger age is, the better precision we have. And we used our method to recreate the results from superior stain, and we managed to do this ten times faster.
00:15:49.914 - 00:16:50.770, Speaker A: Here you can see the orange line, which gives the results of both ours and the state of the art. And these are inseparable because we had three digits of precision. We also did this for Ethereum, where squirrel tried to get results, and we managed to surpass them because their method is not optimal. And we have the orange line, they have the black line with triangles. Okay, now let's have a few words about deep reinforcement learning. So, there's a huge class of algorithms, as I've said before, focused on the basic one, deep Q networks. And this is a major paper in 2015, brought this idea into the front, and they managed to train, using this algorithm to train some agent to play Atari games and achieve superhuman performance.
00:16:50.770 - 00:17:20.354, Speaker A: And we wanted to use that as well. These are the results. By squared. You can see that they're suboptimal because they wanted to be efficient enough so the algorithm would finish running. But they used the Pierce method, so they didn't get a good approximation. So you can see they have the blue and triangles, and you can see there is some difference. So the results are not optimal in comparison.
00:17:20.354 - 00:18:03.770, Speaker A: These are our results. These are preliminary results. But you can see for different models, such as alpha, which is the minor size, and gamma, the rushing factor, which I don't have time to talk about, estimated revenue is what our results gain. And true optimal is the result from the previous methods. You can see we have three digits of precision, and this is the example of some runs which managed to converge. But the problem is that the larger our states space is, the more time it takes until convergence. And this is a major problem which we are currently trying to overcome.
00:18:03.770 - 00:18:53.420, Speaker A: So, in conclusion, we iterate a process which goes like this. We model a problem, a blockchain, as an MDP, an ARR MDP. We transform it into a PTMDP. We can solve that by a PTMDP is a stochastic shortest path MDP. We can do this either by the exact methods, when it's possible, or by using deep reinforcement learning, and we get a good approximation, which for exact methods, it's guaranteed. For deep reinforcement learning, we would have to do some form of empirical analysis to show that it's still a good approximation. And if we do the four steps over and over for different minor sizes, we can find the security threshold of a protocol and see when it's secure or not.
00:18:53.420 - 00:20:00.988, Speaker A: I see. I don't have time for this, but if it interests anybody, these are further ideas for research. And main one, it's what we're currently working on. So, you very much protocol or a database protocol? Yeah. So the question is if it's important whether it's a DAG based protocol or not. What was the first one? Okay, so whether it's a block based protocol or a DAG based protocol. So, in general, if you model a DAG based protocol as an MDP, which has the utility function which I described, it can be used.
00:20:00.988 - 00:20:14.620, Speaker A: But that's an issue in itself of how you model a DAG based protocol as an MDP. Anything else? Thank you.
