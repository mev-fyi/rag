00:00:03.550 - 00:00:53.070, Speaker A: So I'm Bob and I'm here to talk about airscript. Airscript is a DSL for describing air constraints. And in a world of where we have now a bunch of different Zk virtual machines, the question might be why do we need this DSL, but we still need to describe air constraints for those virtual machines somehow. So this talk about is how do we do that? The first thing I want to go through is kind of overview, what are air constraints? Very briefly, to set the context. So when we talk about air constraints, it's basically we need to take a computation and convert it into this set of algebraic statements. Or specifically we want to describe a computation as a set of boundary degree polynomials. The step of transforming the computation into this set of boundary degree polynomials is called arithmetization.
00:00:53.070 - 00:01:37.722, Speaker A: And what we get at the end is this algebraic intermediate representation, or air for short, the way this fits into a star proof generation process. And some of you may already know a bunch of this, but basically we started an execution trace. I'm not going to go into detail of how you build this execution trace, but for every computation you can build it. Then you do something called a low degree extension. Then you evaluate constraints over this low degree extension, you combine them into a single polynomial, then you prove that this polynomial has kind of a specific degree. And then air constraints come into in this middle portion where you actually need to evaluate this set of functions over this extended execution trace. And it makes sense to group air constraints in different categories.
00:01:37.722 - 00:02:25.760, Speaker A: For example, one of those categories is boundary constraints. They're usually useful to specify inputs and outputs for a computation. And these boundary constraints can basically assert that the specific cell in a trace has a specific value. Another one would be validity constraints, which is basically imposing some validity conditions on a specific row or on all values in a given row. So for example, you would have a function that takes all variables from a given row and output zero if the constraints are valid. And then the third type could be transition constraints, which work with two or more consecutive steps of the computation. In this case, for example, for two steps, you take two consecutive rows from the trace, and if the function that you evaluate them over, if the constraints evaluate the function, should evaluate to zero.
00:02:25.760 - 00:03:03.706, Speaker A: So this is basically just a way to group all of these constraints eventually get into like rational constraints. But this is a useful way to think about how constraints are structured. And to give a very quick example, usually we use Fibonacci sequence as a very basic example in all these things. And let's say we describe Fibonacci sequence using two columns in a trace. If you think about how Fibonacci sequence works here, it's just like zigzags from column a to b. So every row in this trace represents two terms of a Fibonacci sequence. In this case, we start with one and one, and then one plus one equals two, then one plus two equals three, two plus three equals five.
00:03:03.706 - 00:03:39.650, Speaker A: So you can see how it progresses across a trace. So the constraints we need to describe this computation. In this case, we compute this Fibonacci sequence up to 64th term or whatever. So first we need to assert boundary constraints, the start of the sequence, and then the final value that we get. And then we have a couple of very simple transition constraints that define the next value in column a is basically a sum of columns from the previous row, columns a and b from the previous row, that's one of them, and then column b. Next value is column b from the previous row, plus value of column a in the current row. So that's a very simple way to describe constraints.
00:03:39.650 - 00:04:36.050, Speaker A: So how do we write error constraints? And there are different ways to do it. The most straightforward one is you just use a general purpose programming language like rust ce solidity, and in that case you basically take your two dimensional matrix, that is an execution trace as an input, and then you have to write this mathematical equations to describe them. The other approach is some domain specific languages, so I listed a few. I know there are more out there, but for example, pill or airscript, specifically that we're going to talk about is a domain specific language that helps you with describing these constraints. Now, let's say we chose a general purpose language, and the question is, why do you want a DSL versus a general purpose language? So what is the advantage of just using a general purpose language is it's very fast to get started. First of all, you don't need to learn a new language, but also assuming the language doesn't exist, you also don't need to develop a new language. So that's like the biggest benefit of starting with a general purpose language.
00:04:36.050 - 00:05:05.294, Speaker A: But the downside is that it becomes very difficult to maintain or write and maintain the constraint system, especially if the constraint system is complex. If you're doing something super simple, you might get away with a general purpose language. But if you're doing something that is fairly complicated, that becomes a problem. And the problem for a couple of reasons. First, poor readability. So if you're using a general purpose language, there is a bunch of boilerplate code that you need to do. There is always a tension between readability and optimization.
00:05:05.294 - 00:05:42.022, Speaker A: So the more optimized you make your code, the less readable it becomes. And then the more readable you want to make it, the less performant it becomes. And then general purpose languages don't provide good abstractions for you to capture complex relations between or describe complex constraints. So it's very, very difficult. The other kind of negative aspect is poor auditability, meaning that let's say we want to have multiple targets for our constraints. We want to evaluate them on a regular cpu, or maybe in webassembly, or maybe in solidity. We need to maintain multiple descriptions in every one of those languages.
00:05:42.022 - 00:06:16.146, Speaker A: So you have to have multiple redundant descriptions of error constraints. And then whoever adds this needs to understand rust. They may need to understand solidity, and that becomes a problem in itself. It's difficult to find people who understand those constraints to begin with, let alone also know rust and also know c and solidity. So that becomes a problem. And then if you describe something in just c or know, it's very difficult to formally verify that. So here's where airscript comes in, or any other GSL, but in this specific case, airscript.
00:06:16.146 - 00:06:53.138, Speaker A: The goals that we have for Airscript and the reason why we started doing this, because currently I'm working on maiden VM, which is ZKVM, we describe constraints in rust and we started out this way, and now we're switching to using DSL. And one of the goals that we want to achieve is readability. So it should be really easy and read and audit the constraints. They should be as close to their mathematical expressions as possible. The second one is we want airscript to be powerful enough to be able to describe complex zero knowledge virtual machines. After all, that's what we need it for. So it needs to have all the features that would allow us to do that.
00:06:53.138 - 00:07:33.774, Speaker A: And the last one, we want to have the modularity where you have decoupled kind of like front ends and back ends, and you can introduce new backends for the same representation. So we have this intermediate representation that could be very useful. And to touch on the last point, so we have the source script, which as I mentioned, that's where you start with airscript. Then you have this intermediate representation, and then the idea is that you can then go to different backends from this intermediate representation. So in our case, we use Winterfell as a stark prover, but we also want to do recursive proof. So we want to be able to generate constraint evaluation logic in Miten assembly. We also want to have an on chain verifier, and therefore we need something that will easily translate the constraints into solidity.
00:07:33.774 - 00:08:02.138, Speaker A: And then this structure allows you to do a number of interesting things. So first of all, you focus on different things in different places. So the point of the source script is to be as easily readable as possible. Then you can do optimizations at the IR level. You don't need to optimize things at the source level because this is not the point. The point is to keep the constraint description simple and readable. At the IR level you can optimize, remove common sub expressions and do all kinds of interesting things.
00:08:02.138 - 00:08:44.882, Speaker A: But then you also can do some backend specific optimizations for specific target architectures, for evaluating on a cpu, or like inside the midnvm itself, or for a blockchain where you care about gas and all of that stuff. So we soon will be releasing soon, meaning like in a week or so. Airscript V two, V one is out. It has a bunch of features in it that are already implemented. I'm actually not going to go through this list. One of the back ends we support right now is Winterfell, and we have a bunch of features planned for upcoming releases, more functionality, more backends and so forth. But I'm actually going to go through some of these already implemented features in the following slides.
00:08:44.882 - 00:09:18.530, Speaker A: For now, I'll say with V two we'll get to about like 95% of what we need to describe midnvm. In Airscript. There is like one or two things that's still missing that we still need to get to, and we'll get shortly after V two. But let's go through kind of the syntax of air script, and I'll give also some simple examples. So airscript consists of four sections. The first section is trace declaration, where we specify the shape of the trace, like how many segments a trace has, how many columns are in a trace, and how we can refer to them. Then we need to define set of public inputs for the computation.
00:09:18.530 - 00:09:52.954, Speaker A: We need to define boundary constraints, both initial and terminal, and we mean this constraints against the first and the last row of the execution trace. And then lastly we need to specify this integrity constraints, which we basically use a general term to specify both validity and transition constraints. So let's go through this one by one. So let's say this is what we call trace declaration. When you declare a trace, you specify the shape of the trace. So like the trace columns, there is also priority columns and what we call random values. So in trace columns, we have different segments.
00:09:52.954 - 00:10:28.086, Speaker A: And this is a concept where it's not just kind of original description of error, but this is error where you can have also another stage from verifier randomness. So like you build a part of the trace, you commit to that trace, send the commitment to the verifier, the verifier sends you randomness, and then you can build another part of the trace. So that's the idea between, let's say main and auxiliary trace. Right now we allow for only two of these trace segments. In the future we'll allow more. And then within each trace segment you can bind the column to a specific name. So I can say that in the main trace, the first column is going to be referred to as a, and the second column is going to be referred to as b.
00:10:28.086 - 00:11:12.726, Speaker A: And there's going to be a set of four columns that I can refer to as a group of columns. C, for example, we have these periodic columns that are very convenient to, they're almost free for the verifier to evaluate. So you can specify these values that happen in this cycles and you can usually use them for selector columns. And in a hash function like rescue Poseidon, you can use them for round constants and things like that. And then you can specify how many random values, like after you've committed to the main trace, how many random values do you get back? Then you can use this. In this case, for example, we have two random values that can be used to build that jewelry trace. And you can specify any number and you can also bind them to names if you want.
00:11:12.726 - 00:11:38.826, Speaker A: The next thing is specifying public inputs. You can specify any number of public inputs, and each public input can be a fixed size array. So this is fairly simple. One thing that I mentioned that we need for mitonvm, we need actually variable sized arrays. So we will be describing variable sized arrays for public inputs as well. Then we go to boundary constraints, and this is fairly simple as well. So here you can specify the first and the last value in each column.
00:11:38.826 - 00:12:21.178, Speaker A: So n keyword basically says this is a constraint. So first one, we want to have a constraint that the first value in column a is zero, and first value in column b is one. This is the first and the last modifiers are used on columns to specify the first and the last value. And then we can use a combination of constants or references to public inputs to specify what the value of the column should be. So in the last constraint here, I specify that the last value in column b should be equal to the first value from the public input named term value. And the last part is integrity constraints. And integrity constraints, as I mentioned, is a combination of validity and transition constraints.
00:12:21.178 - 00:13:00.314, Speaker A: Here again, where nth keyword are used to specify a constraint, the prime symbol is used to specify the next value on the next row in a computation. So where I have this a prime is equal x plus one. It means the next value in column a is equals to value of x plus one, and x is defined as a plus b. So we support all the common operations. Addition, multiplication, exponentiation using a constant you can reference all trace columns. You can also reference random values. So in this case, for example, column p comes from the auxiliary trace, which is built after the first part of the trace is committed to.
00:13:00.314 - 00:13:43.080, Speaker A: So we can use a random value. So this dollar sign rand basically says take a first random value saying but the verifier. And then we have ability to define intermediate variables for common sub expressions. You can define x and then use x multiple times, and we can use simple list comprehension. For example, the second to last constraint says like c is actually referring to four columns, and we want to enforce that values in those columns are binary. We don't want to repeat the same constraint four times. So we can just use this python like syntax saying that for every column and c we want to impose this constraint where raising the value in a column to power two minus itself should be zero.
00:13:43.080 - 00:14:09.374, Speaker A: So this is fairly simple. And just to give a very simple example of the same Fibonacci sequence as I laid out in the beginning of the talk, this is the full airscript file that defines the Fibonacci sequence. It basically says we have a single. There is no kind of like randomness from the verifier that we need. It's a very simple computation. We only need two columns in the main trace. There is only one public input that specifies the result.
00:14:09.374 - 00:15:13.438, Speaker A: We specify what the boundary conditions are for this computation, and we also specify the transition constraints that you need to describe the computation. And this is basically all you need to describe the Fibonacci sequence. Obviously for ZKVM, constraints would be much more complicated than that, but we have to start somewhere, so that's basically it. Thank you. Happy to take any questions or talk later as well. Yes, do I consider the tools to automate the scripting process? By scripting process, what do you mean? For example, could be interesting, we haven't thought about this yet. So basically what you are saying is you take a program and try to convert the program automatically into a description in air script.
00:15:13.438 - 00:15:35.406, Speaker A: We haven't thought about this yet, but could be done, I suppose. I think our primary goal is to describe the virtual machine itself. So it's like the computation is already defined and we just need to describe it in airscript. But for something that is more dynamic that could be useful. But yeah, we haven't really kind of put any effort into that yet. Yes.
00:15:35.588 - 00:15:47.068, Speaker B: How much work is it to write an air script for a brain fox virtual machine? And by the way, there's a lot of essential there.
00:15:47.234 - 00:16:05.228, Speaker A: I don't know how much work it would be. I'm assuming it's not going to be too much work. So I think once airscript is ready for maiden, it's probably not going to take more than two or three weeks to describe midnight constraints in airscript. So brainfuck, I think is not as complicated. So it might be a few days. Yes.
00:16:05.414 - 00:16:30.270, Speaker B: In terms of the compiler pipeline that you showed earlier, when you go to the IR specific optimization, those all have to be in sync for any of the net data translation. For those to be compatible to the optimizations, you might apply the back end specific optimizations. Would those also have to be synchronized in order for the groups to work across one another, if that makes sense?
00:16:31.360 - 00:17:02.180, Speaker A: I think so. I mean, you would want to output the same constraints. You're right, you would want to make sure that the way constraints are defined so the optimizations don't change the constraints themselves. So the output still would be the same. It's just a more efficient way of computing the same. So maybe you would rearrange something, or maybe because of the specific layout of a given architecture, it's better to put them close in memory or change the sequence. But the optimizations do not change the output of this, and this is very important.
00:17:02.180 - 00:18:00.906, Speaker A: So you can always, if you evaluate something produced on Winterfell backend, like at a random point, let's say you evaluate these constraints, they should give you the same exact result as the constraints evaluated, the same random point for MIDN backend, solidity backend, or any other ones. So the result should not be affected by that. I don't know if that affects actually the optimizations that you can do, because optimization is not about rewriting. It's more kind of like how do you compute this more efficiently? Yes, so airscript does not define how you compute the trace. So the trace generation is a separate piece of code that is not covered by airscript. So in our system we have a rust code that generates the execution trace and then airscript would come in later when you need to do constraint evaluation. So those still need to be separate.
00:18:00.906 - 00:18:43.194, Speaker A: I mean, we could try to think about how to integrate trace generation into airscript as well, but I think it's too big of a task and we want to start with just the constraints. Does that answer the question or. Yeah, okay, it's not csv, because when I say backend here, for example, Winterfell backend, Winterfell backend assumes that you have to implement a certain interface in rust, so it actually outputs rust code. That assumes that something has already generated this execution trace and it's at a specific point in memory. So there is an assumption that a given back end imposes on how this matrix is going to come. So it's not csv files. That would be probably not quite efficient.
00:18:43.194 - 00:19:34.164, Speaker A: So you already want to have that in memory and laid out in specific forms by the time the air script gets to it, but the back end specific generator will know how it needs to be done for a specific back end. Yes. So we do plan to have. So right now, as I said, we only apply to two consecutive steps, but IR actually is more general even now. So it's the syntax that the front end only allows you to access two rows, but Ir actually allows you to have bigger frames. So like in some cases we call them evaluation frames, where you can specify a frame that is not two consecutive rows, but let's say 16 consecutive rows or four consecutive rows or something like that. At the IR level, it exists at the front end and doesn't exist.
00:19:34.164 - 00:20:07.090, Speaker A: And also Winterfell back end doesn't work with more than two rows, so we haven't implemented that part in the code gen either. Oh, I see what you mean. So you want to have the constraint. So that's an interesting thing, because right now we don't have that support. So in a way we handle this type of constraints in Winterfell is by introducing selector columns that let's say are one and zero on, let's say one on one row and zero on another row. It does raise the degree. It could be an interesting extension to this.
00:20:07.090 - 00:20:53.610, Speaker A: We don't have a need for this yet, because we try to minimize the number of vanishing polynomials. So in this case you only need three vanishing polynomials to describe the whole system. Like one vanishing polynomial against the first row, the second one against the last row, and the third one for transition constraints, basically. But yeah, if you are willing to add more vanishing polynomials, then you can have more kind of complicated constraints that happen on, not on every row, but every other row. Something like that, correct? Yeah. At least in the current implementation. Any other questions? All right, thank you very much.
00:20:53.610 - 00:20:55.190, Speaker A: Thank.
