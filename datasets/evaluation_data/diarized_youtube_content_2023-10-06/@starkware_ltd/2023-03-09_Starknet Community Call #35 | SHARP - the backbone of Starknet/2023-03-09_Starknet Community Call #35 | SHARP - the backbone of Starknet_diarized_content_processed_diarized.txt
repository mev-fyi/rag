00:00:00.170 - 00:00:19.838, Speaker A: YouTube should be good waiting now. We should be live. Very nice. So for people joining us on YouTube, welcome. This is the 35th community call. I'm with Kinerette, and today we're talking about sharp improving. So I propose we wait for.
00:00:19.838 - 00:01:00.798, Speaker A: It's 244. Let's wait for a couple of minutes and. And then we can start. So I'm going to go on Twitter and retweet the stream to make everybody aware that we are here so that they can hear what we're talking about, and then we can go. There it is. All right, you know what? Let's go. You ready?
00:01:00.964 - 00:01:01.390, Speaker B: Yes.
00:01:01.460 - 00:01:14.130, Speaker A: Cool. Okay. So thank you for joining us today, Kinerette. Well, let's start by getting a short introduction about you. What is it you're doing at stockware? Sorry?
00:01:14.280 - 00:01:29.420, Speaker B: Yeah, so I work for Starkware for more than four years. Actually almost four and a half. I'm part of the engineering team, and currently I'm the team lead of one of the teams in the engineering, which is in charge of the sharp system.
00:01:30.350 - 00:01:44.000, Speaker A: Wow. So, for those of you who are not aware, with sharp, we're going to get a presentation about it more broadly. First question is, how happy were you when you found the acronym sharp? Because it's pretty.
00:01:45.010 - 00:02:08.870, Speaker B: Okay. It used to be called differently gps stands for generic proving services. But we thought, okay, GPS is something that's already there. And then actually, someone from the engineering to fit, she offered Sharp, which stands for shirt prover, and I think it's a brilliant name. So, yeah, I'm very happy.
00:02:09.020 - 00:02:16.850, Speaker A: Pretty cool. Okay. I know you have a presentation. Do you want to start presenting now and tell us everything about sharp?
00:02:16.930 - 00:02:17.560, Speaker B: Sure.
00:02:18.010 - 00:02:23.106, Speaker A: Wonderful. Okay. So can I add your screen to the stream?
00:02:23.218 - 00:02:24.200, Speaker B: Yes, please.
00:02:25.130 - 00:02:25.880, Speaker A: Nice.
00:02:26.490 - 00:02:28.726, Speaker B: Okay, but now.
00:02:28.908 - 00:02:34.174, Speaker A: Wait, it's the second one.
00:02:34.212 - 00:02:44.320, Speaker B: I think that's the second one. Okay. There you go. Okay, wait, but now I don't see you again. Okay.
00:02:47.510 - 00:03:04.470, Speaker A: There'S a little bit of my screen now. Yes, we can see the presentation. Perfect. Thank you. Before you start, I just want to mention, for people watching us, feel free to ask questions on YouTube. In the comments, we can see them. I see Giwok, Alfred, nicola, saying hello.
00:03:04.470 - 00:03:13.354, Speaker A: And enhida. Don't hesitate. Ask questions. It's way funnier when people ask stuff. And you can also ping us on Twitter. All right. Sorry.
00:03:13.354 - 00:03:14.780, Speaker A: Kinerett. The floor is.
00:03:16.910 - 00:04:00.566, Speaker B: Would like. So I would like to tell you about Sharp. What is the sharp system? Why is it so cool? I will be then talking about our users and how they interact with us and introduce some of the challenges that we are facing. Pretty interesting challenges. Okay, so let's dive in. So what is sharp? So in order to understand what is sharp, first let me just make sure we are on the same page. So we have the prover and we have the verifier, and we have a Kyo program which outputs some output.
00:04:00.566 - 00:04:52.954, Speaker B: And the statement of the prover is that the prover knows an input, a private input, such that the Cairo program runs successfully on this input and output this output. So this is the basic statement that the prover proves to the verifier. And the Cairo program can be of any Cairo program actually. So we take that ability to prove computation and we used it in sharp. So as I said before, sharp stands for shared prover. We get many, many Cairo jobs from different application, different Cairo jobs with different logics. When I say Cairo jobs, I mean Cairo program actually, and we aggregate them in a way.
00:04:52.954 - 00:05:06.430, Speaker B: I will explain a bit more later. And we run the prover on these jobs, and then the prover outputs a proof which is being sent to the solidity verifier on chain.
00:05:07.410 - 00:05:10.734, Speaker A: I have a question for you here, if I may interrupt you.
00:05:10.852 - 00:05:11.520, Speaker B: Sure.
00:05:13.570 - 00:05:36.470, Speaker A: Why do we need a common service for this? Why is it better to aggregate? So if I understand correctly, sharp gets a bunch of people's Cairo program, executes them and generate a proof that is common to all these programs, and then sends this to Ethereum. Why didn't we need to aggregate Cairo jobs instead of just running one proof per job and sending it to Ethereum?
00:05:36.970 - 00:06:13.566, Speaker B: Great. So actually before sharp access, different stockxes used their own prover, and indeed they had like a single batch of transactions going to the proof, but it was much more expensive because stark protocol gives us the exponential acceleration. And then if you aggregate more jobs to the proof, each single transaction cost much less. Okay, we can have is the better, it's cheaper.
00:06:13.758 - 00:06:15.140, Speaker A: Okay, makes sense.
00:06:15.830 - 00:06:36.730, Speaker B: And then you have also like if you are a small user and you have small computation, you can benefit from the fact that you join other jobs and you can actually gain benefit from using the proverb because otherwise it wouldn't make sense for you to use it by yourself if you have small amount of computation.
00:06:37.630 - 00:06:50.330, Speaker A: Okay, I see what you mean. It's kind of like taking public transportation versus using your own car. It's cheaper. There's a downside because there's fixed times and everything, but it's much cheaper.
00:06:50.410 - 00:07:29.254, Speaker B: Exactly. It's not a special taxi. Okay, so that's in general like the high level flow. We get Cairo jobs, which again, the input, which is the Cairo program, with some metadata, we run the prover and we send the proof on chain to the solidity verifier. So with a small demonstration, so we have the Cairo jobs, it's being sent to the prover that does his magic, and then the result is being sent to the solidity verifier. A few sentences of how we do it. So actually we have a Cairo program which is called the bootloader.
00:07:29.254 - 00:08:07.430, Speaker B: And the bootloader can upload many different programs with their output, their input and output. And actually what we do is that we run the proverb on this bootloader program which uploaded the different various Cairo programs and outputs the program hash of each of the original program with their outputs. So in that way the prover and the verifier knows the bootloader code. But we can upload different programs and different logic into this mechanism.
00:08:08.410 - 00:08:12.630, Speaker A: And these programs are all totally unrelated, right. They don't share any states.
00:08:12.700 - 00:08:13.282, Speaker B: Exactly.
00:08:13.436 - 00:08:15.510, Speaker A: You ran a program correctly.
00:08:15.590 - 00:08:45.826, Speaker B: Exactly. They can be Dex's program, Starknet OS program. They can be stuff that were sent from, I don't know, a playground, the Cairo playground, if people at least knows it. And like whatever, Fibonacci, sudoku. Okay, great. So we have the Cairo jobs, which aggregates and go into the bootloader that is being sent to the prover. And the prover proves the computation of the bootloader and then sent to the solidity verifier.
00:08:45.826 - 00:09:33.700, Speaker B: Okay, so a few words about the solidity verifier. So the solidity verifier is a stack verifier, and it verifies the Cairo program. And then it puts something which we call fact on chain, on a storage on chain. This fact is a unique id of the program and its output. So when the verifier puts a fact of a program on chain, it means that it verifies its computation. So this is the way the users can check if the verifier approved the proof of their program.
00:09:34.970 - 00:10:02.460, Speaker A: Okay, so here we're already going out from the pure stock protocol, right? Because this is an unimplementation of the stock protocol using Ethereum, right, where we're trying to maintain some common state and have a proof, like a blockchain, proof that a stark proof was verified and that the output was correct. Right. It's a way for the system to output data to Ethereum basically is that.
00:10:03.150 - 00:10:59.710, Speaker B: Will actually, in a few slides I will go into how we close the loop, like how users actually send the input for sharp and what sharp does and then eventually how they. But that's like in a few slides. Okay, so actually I presented a linear flow, right? We get Cairo jobs, they are being sent to the prover, the approver outputs approved and sends on chain. But I think probably a lot of people that hear us, they know that we use recursion. And I didn't present a recursion flow, right? So the flow I presented was more or less the flow we used until last summer. But now since the previous version of sharp, we are using recursion. So we actually get Cairo jobs, and then we run the proverb on them.
00:10:59.710 - 00:12:30.870, Speaker B: But then instead of sending them to the solidity verifier, we send them to the Cairo verifier, which is a stock verifier written in Cairo, the trans off chain. And Cairo verifier is a Cairo program that verifies proofs. So we can send it to sharp again, because actually it's a Cairo program. So we can run the proverb on the Cairo verifier program to get another proof and another proof, until at some point the last proof there is some trigger, I will explain it a bit later, that goes to solidity eventually. So just to give high level view of that, so to the left you can see the original jobs that we got from outside, from the users. And instead of sending them directly to the solidity verifier, after proving them, we compute the proof, and then we send the proof to the Cairo verifier that verifies them. That, and then for two Cairo verifier programs, we output what we call a recursive proof, which is a proof that the Cairo verifier verifies the original job, the proof of the original jobs.
00:12:30.870 - 00:12:42.480, Speaker B: And then we keep doing it more and more. The funnel is two to one. It's not like we could do it other way. It's just a decision we took. We can change it.
00:12:43.250 - 00:12:49.300, Speaker A: Why did you choose to do two to one? Is just because you thought, oh, let's start with two to one. Or is there a reason?
00:12:50.710 - 00:13:18.186, Speaker B: If you add more, it takes more time to prove it. The numbers summed up to having two to one. But it's not like we took some design decision, like for example, that each original job first gets his own proof and doesn't go with other jobs. And I mean, we are thinking of it, this is the original implementation, but we might change it later.
00:13:18.288 - 00:13:20.546, Speaker A: Understood. It's not set in stone.
00:13:20.678 - 00:13:53.750, Speaker B: No, definitely not. And eventually, when actually the trigger is that the oldest job in this entire tree waits for too long. I mean, again, configurable. We trigger a proof for the solidity verifier, and then we eventually send the proof of this entire tree to the solidity verifier. So eventually we have a tree and not some linear stream of jobs.
00:13:56.730 - 00:14:21.680, Speaker A: What I find really interesting here is that we're clearly not in the realm of research, where we're trying to maximize however many work we can fit in a proof. It's really a life system constraint. And at the end of the day, it's not just capacity that matters, it's time to proof, right? Time to mainnet. So the trigger here is time, not the amount of work inside of it. It's really interesting.
00:14:22.370 - 00:14:39.414, Speaker B: Yeah, I think there are many things we would like to optimize on, right? Latency is one, cost on chain is another one. I mean, there are many actually. That leads me to the next slide, like wirecalgen. I mean, it looks a bit like wasteful, right?
00:14:39.452 - 00:14:41.960, Speaker A: Because it looks longer. It does.
00:14:43.690 - 00:15:20.914, Speaker B: Also, I can tell you it's much more complicated to debug it. It's much easier to have a linear stream of jobs. So it's a big question. I mean, it looks really too complicated. And on each job you get another job, right? The recursive job. So why do we do it? Okay, so there are really surprising answers for that. First of all, you can see that we can parallelize the leaves, right? Because we used to wait for a few jobs to get into sharp and then start proving them.
00:15:20.914 - 00:15:59.070, Speaker B: So jobs were accumulated and waited in the queue until we got large enough input for a proof. And now when a job arrives, we start working on it and we parallelize the work. It gives us much lower cloud costs because each job is much shorter. The proofs are shorter because the input is shorter, so the machines are much cheaper. Like you don't need a huge amount of memory to support the trace of these jobs.
00:15:59.730 - 00:16:42.080, Speaker A: So I have two questions here, if I can. So there was question in the audience, actually a lot more than usually. So first one was Feltruig prime, who is asking? It was earlier in the presentation, he said, wouldn't it be easier to split the proof and then connect everything into one proof to make it faster in the end? Which is basically what you're describing now with recursion. What we didn't address is right now all of this is done by a single entity, which is stackware. Can you imagine a future where the different leaves are proven by different actors and then aggregated by different actors at every step. If there were a protocol for actors to share their proof, would it be possible to do that?
00:16:42.610 - 00:16:50.026, Speaker B: Yeah, I think we might go get into this when we have decentralized Starknet.
00:16:50.138 - 00:17:05.470, Speaker A: Wonderful. And then there's other questions around cost. So one question is which kind of hardware is needed to spin up approver? That's one question. And the other one is, isn't it expensive to make stark proofs? These are bro's question.
00:17:05.640 - 00:17:55.998, Speaker B: Okay, so it is much cheaper in recursive architecture. As I started to say, each job is much smaller now. We used to have. Before we have. Before we have had recursive architecture, we had like really long proofs which took 8 hours, 10 hours. The machines are much more expensive because the off chain cost was much more expensive because we needed larger memory to support it. Now we have smaller chunks, so machines are cheaper.
00:17:55.998 - 00:18:23.294, Speaker B: We are able to squeeze in much more chiropeps in each proof. So on chain is much cheaper, like we had users operating on the previous version before ecargen. And the same stream that they send us on a single day became much cheaper on chain because we moved sharp to be recalcine. That's like super cool.
00:18:23.412 - 00:19:02.138, Speaker A: This is great. But I have a question regarding the hardware and the cloud instances you were mentioning. So you said that now we're able, instead of using one big instance for 8 hours, you're saying we can use a lot of smaller ones for a shorter amount of time. I would think, but maybe I'm wrong. I'm not a cloud expert, that if you use a bigger machine, the marginal cost of extra resources is lower. In other words, if I need a machine with one gig of ram, or four machines with one gig of ram are more expensive than one machine with four gig of ram. Is it the case or is it.
00:19:02.144 - 00:19:25.410, Speaker B: The opposite four gig? It's true, I'm not sure. But for two terra there are machine which are really rare, you don't have a lot from them. We needed really heavy hardware to support such a large computation and now we need less than, I think less than half a terabyte, like much smaller.
00:19:26.150 - 00:19:37.494, Speaker A: What you mentioned yesterday, which was really interesting, was also that you can rely on less reliable cloud instances and that brings down the cost of loss. Can you explain that also please?
00:19:37.532 - 00:20:18.340, Speaker B: Yes, of course. So before recalcit shop, we used to have long proofs, long as 10 hours. So we needed machines that will be there for us from the beginning of the computation until the end. Because computing a proof for 8 hours, 9 hours, and then the machine falls down. That's like horrible. It's really, latency wise, cost wise, it's really expensive for many reasons. But now each chunk actually, in the new version of sharp, which we just deployed two days ago, recursive pools takes less than 10 minutes.
00:20:18.340 - 00:20:42.070, Speaker B: So the recursive step, yeah, that's like super cool. So we can use something which is called preemption. Like sometimes the machines fall down and start again, but then we lost only a few minutes of computation. I mean, for us, we can recompute it, it's no big deal. But the machines are much, much cheaper.
00:20:44.730 - 00:21:01.934, Speaker A: So we can use less reliable cloud instances which have lower slas. But it brings down the cost also, because basically you're making the protocol less resilient. More resilient because you can use smaller machines and that are less resilient. This is beautiful.
00:21:02.132 - 00:21:15.830, Speaker B: Yeah, that's like a magic for me, I think recursion is usually, you don't see the combination of smaller latency, lower costs, and cheaper.
00:21:17.530 - 00:21:18.962, Speaker A: And more resiliency.
00:21:19.106 - 00:21:53.810, Speaker B: Yeah, it's too good to be true, but yeah, that's the magic. And something else that I would like to mention here, which is not related to efficiency of the computation, is that we can have logic that we support only in the Cairo verifier. Because if we get from the user's input that at first step is being sent to the Cairo verifier, you might not need to do many changes to the solidity in order to support it, because eventually the solidity gets the recursive proof.
00:21:57.000 - 00:21:59.780, Speaker A: Wait, can you go over this again?
00:21:59.930 - 00:22:36.560, Speaker B: Yes, please. What I'm saying is that you can have much, I mean, if you want the users to send you different kind of operations, different kind of stuff that you need to support in the prover. Eventually the first step is sending it to the Cairo verifier. The Cairo verifier, not the solidity verifier. So you don't need to insert, I mean, the solidity verifier has the hash of the Cairo verifier. So if you do any change to the Cairo verifier, you need to update the solidity in that sense. But you don't need to add more logic.
00:22:36.560 - 00:23:02.680, Speaker B: You can support stuff. I can give you an example. For example, we did an optimization in the commitment part in the Merkel tree. We put for some of the layers. Instead of committing with Blake, we committed with Peterson, hush. Which was much more cheaper for the Cairo verifier, but we didn't do it in the solidity verifier. For the solidity.
00:23:02.680 - 00:23:25.280, Speaker B: We didn't want that change so that you can add different features or different changes to the off chain part. And you don't need to touch up to the fact that you need to update the hash of the Cairo verifier. You don't need to touch the solidity verifier.
00:23:25.700 - 00:23:33.650, Speaker A: Okay. It makes sense. That's really interesting. It allows for more innovation without having to go through ethereum, actually.
00:23:33.960 - 00:23:35.588, Speaker B: Yes. Which is super cool.
00:23:35.674 - 00:23:36.452, Speaker A: It is?
00:23:36.586 - 00:24:26.044, Speaker B: Yes. Okay, so that's for the benefits of recausion. I think probably there are more, but that's what I. Okay, so I would like to talk a bit about the shard backend architecture and how the data go through the different services of the pipeline. So as I said, we start with Cairo jobs which get into the gateway and then let's keep about job creator is some kind of service that make sure we don't have duplications and. Never mind, let's skip it for now. Then the first important step is the validator.
00:24:26.044 - 00:25:15.276, Speaker B: So the validator is the service that runs a validation on each of the jobs that we get. It does Kyora it anka run and checks some resources. Checks. It checks that this specific job can fit in the approver machines actually, because we have some limitations on the machine and some other checks. And if the job is not valid, it's tagged this as invalid and the job doesn't go to the approver. For sharp, this is a valid flow, like to get a job and to tag it as non valid, that's valid. For starknet, it will be a big headache because it means like real, but for sharp that's fine.
00:25:15.276 - 00:26:27.300, Speaker B: I mean, we make sure that the input, that we don't get a corrupted input inside approver. Then if the job is valid, it goes to the scheduler. So the scheduler is an online algorithm that gets the jobs and creates what we called trains. So train is the object that aggregates a few jobs and goes eventually to the prover for the prover to be improved. So if it's a leaf, if it's like a job that came from the user, we put it on a train on its own, as I showed before in the recursion tree, and it goes to the verifier, to the prover by itself. And if this is a recursive job, we pair two and we send two to the approver. The third option is to have the proof for solidity, and then it triggers like the approver for solidity with its own parameters.
00:26:27.300 - 00:27:17.370, Speaker B: And it goes to solidity. And as I said before, this is based, the trigger is time based. So when we have an old enough job, we can send it on chain. We will send the entire tree on chain. Then we go to the Cairo runner, which is a service that runs cairo for the prover needs, and then the prover which compute the proofs. After that, the proofs goes to the dispatcher. So either if this is a recursive step, so the dispatcher creates the cairo job with the verifier program and goes against the solidity to the validator, and the flow goes on to the scheduler and so on.
00:27:17.370 - 00:27:36.450, Speaker B: And if this is like the final proof that goes on chain, it's being sent to the solidity verifier. And as I mentioned before, for each job, the solidity verifier writes the fact on chain for each original job.
00:27:39.700 - 00:27:47.428, Speaker A: Makes sense. Okay, so it's the loop that processes jobs into the system.
00:27:47.594 - 00:28:18.540, Speaker B: Yes. Okay, two cool points about shop. First, which is pretty, it's not like trivial. I think that shop is stateless. Each job is on its own. If the two jobs are like the job, for example, stagnate Os, we can prove them in one order or another order. It doesn't really matter.
00:28:18.540 - 00:28:46.488, Speaker B: Like each job is on its own, it doesn't relate it to what happens in the world eventually when stagnet will use it. It needs the job on its order in order to do the state update. But for sharp, it's stateless. I mean, we do have the facts on chain, which is like a storage on chain, but other than that, I.
00:28:46.534 - 00:29:19.488, Speaker A: Think it's something that is really counterintuitive for a lot of people. But you could prove block 100 after block 50. It doesn't really matter from the point of your proving, given that sharp is stateless. In reality, when you're approving a Starknet Cairo program, you're giving it hey, here's the state at the beginning, we process the blog, and here's the state at the end. And you just prove that this was correct. Right. So in a way sharp is stateless, but the Cairo jobs inside of sharp basically bring their own state, execute it and outputs them again.
00:29:19.574 - 00:29:31.284, Speaker B: Exactly. But it's like if we start from that state, we have the valid transactions that leads us to that state.
00:29:31.482 - 00:29:32.230, Speaker A: Yeah.
00:29:33.640 - 00:29:58.588, Speaker B: And second point is that because of the cryptographic, because of how stark protocol works, soundness is based on the verifier only. You don't need to read the prover in order to be convinced that the verifier is honest. You can count on the verifier if.
00:29:58.594 - 00:30:20.544, Speaker A: You see it, which is also a really interesting thing. In general, in the blockchain world, you need things to be open source, you need to see how things are computed. But in that case, not really if the proof is valid. It doesn't matter if it was computed by me with a pen and paper or with a big machine.
00:30:20.672 - 00:30:21.350, Speaker B: Exactly.
00:30:21.960 - 00:31:34.008, Speaker A: Which also introduces back, I don't know, it's interesting to ponder. There are some questions, do you mind if we go over see, so techac is asking, can we estimate Sharp's energy consumption of generating starknet proofs and compare it to the Ethereum proof of stake consumption? So I think there's two things here. The first thing is that yes, you can probably estimate sharp energy consumption and you can probably compare it to Ethereum's Pos consumption, but you're really comparing two different things. Ethereum's pos consumption relates to security here, Sharp's energy consumption relates to capacity. So you could decide depending on how you really, you're not paying for the same thing, so they're not really comparable. But that being said, you can probably estimate the consumption of sharp. And indeed, when you prove jobs, the fact of proving there's an overhead in terms of computation.
00:31:34.008 - 00:32:11.588, Speaker A: But think about it this way. On Ethereum, if you sync a node today, you need to resync the whole chain. If you're one of the thousands nodes who recompute the whole chain, this has a cost in terms of computation, right? Whereas with a proof system, all of this job is done once and then everybody can verify it forever. So in the long run, probably proof system are the most efficient there is for an equal level of accountability by end user. I don't know if you have input on that kinder at or no, I agree with you.
00:32:11.674 - 00:32:15.290, Speaker B: I mean that would be interesting to measure it, but I agree with you.
00:32:15.900 - 00:32:50.448, Speaker A: Then get boot is saying, if I really need something to be sent to l one, is there a way or will there be a way to force it to publish it to l one? So this is a good question. I think it's out of scope for today. This is more related to the consensus. I think it's specifically referring to Starknet. So I think it's more related to the consensus of starknet. Ultimately, if you really need a fact on chain that is not related to Starknet, that is you need to prove a Cairo program. Eventually the prover or a prover will be open source.
00:32:50.448 - 00:33:32.240, Speaker A: Then you'll be able to generate your own proof and verify it on Ethereum, so you'll be able to do it yourself. And the verification might cost you a little bit on Ethereum actually might cost a bunch of money because it's fairly expensive, but you'll be able to do it yourself. So depending on your constraint, you'll be able to do it yourself. Then. Geowok is asking, in terms of decentralization of starknet, isn't it too expensive for normal users to run sharp, even though sharp can run on cheaper machine than before? Later he asks, in terms of running their node.
00:33:35.320 - 00:34:22.160, Speaker B: I think that sharp is not like a must. I mean, sharp is one prover service. I think that now when there are discussions about decentralization of Starknet, there are many models using stark proofs in order to get the protocol operating properly. I'm not sure it will be like sharp as is. For now, this is like the service for the current needs, but it's not clear how we are going. It's not like we are taking this as a black box and plugged this in as like the provers for the decentralized protocol. It will probably won't be like this.
00:34:22.310 - 00:35:19.430, Speaker A: Yeah, we're not doing a research call. This is production. This is something that is running, has been running for a few years. This is really interesting, but yeah, and I think in terms of running your own node, I think that users are not expected to generate proofs, provers will. And as we discussed before, you might be able to split a job into various smaller proofs. So there is a way for smaller players to generate proofs and benefit from this. There's a way to decentralize extensively, proving at first glance here, but we'll do another call about this, I think, later in the future because I know there's a lot of reflections around that, but in terms of running their node, no, it actually makes it easier to run your node, like to a certain extent, if you're an ethereum node today, you're a starknet node because you're verifying all the state transitions of Starknet, which is kind of beautiful when you think about it.
00:35:19.430 - 00:35:38.104, Speaker A: And then guide boot is saying, oh, I think I'll need this to watch. I need to watch this multiple times to get my head around it. Well, don't I? State, ask questions. We're happy to answer them. All right. And we've asked all the questions, so you can go on.
00:35:38.222 - 00:36:22.516, Speaker B: Okay, so let's talk about the users. So currently our users, like the main users that use sharp is our Starkx, and Starknet we have some external users that use the Cairo playground. Some of the audience used the Cairo playground. And if you didn't, you're most welcome to do it. And then you can send approves to sharp on Garly. But for Mainnet, that's the users we have. Let's look at how Starknet interact with sharp.
00:36:22.516 - 00:37:05.936, Speaker B: So to the left you have Starknet. It's not like a full architecture of Starknet. This is not the focus of the talk, but just to understand what are the main building blocks and how the information goes through the pipeline. So we have transactions that goes to the sequencer and the sequencer computes the blocks. And currently at this point, each block is going to eventually be a Cairo job. Again, it's not something that, it's like a decision we have now, but it might change in the future. And so each block is going to what we call the sharp ambassador.
00:37:05.936 - 00:38:09.248, Speaker B: So what is sharp ambassador? This is the service that's in charge of interacting with sharp. So it gets the block, it computes the job and sends it to sharp for proving services. And then it checks its status like it asks again and again. The backend, what's the status of the job, so it can monitor if it's valid or invalid and if it's being proved and so on. And eventually if the back end finishes, proves it and finish processing the job, the ambassador goes to the verifier and check the fact. Do you remember that we talked about this unique id so the ambassador can compute it? Of course, it's just the hash of the program that it knows and the output that it also knows it's public. So it checks the proof, the fact of the proof.
00:38:09.248 - 00:39:09.870, Speaker B: And if the fact is there, it means that the verifier approved the computation and then it can go through the dispatcher and the blockchain writer. Never mind. That's like the building blocks that create the transaction that goes on chain eventually to the stacknet core contract. So the stacknet core contract wants to update the state, right? And this is exactly like what the proving services gives us. Instead of verifying its transaction on chain, the verifier gives us approval that you can do the state update because the verifier saw a valid proof for this state update with the Cairo job it verified. And so the stagnant contract check that the fact is there and then change the state.
00:39:14.100 - 00:39:15.090, Speaker A: Makes sense.
00:39:15.540 - 00:39:41.492, Speaker B: Okay. And we can have, as we mentioned before, we can have many different users sending jobs to the same sharp, like the same shared proverb, right? So we have Starknet and we have starkex of different clients. Like here I mentioned, it's a, it's an old slide, but now they're called rhinofi.
00:39:41.556 - 00:39:42.360, Speaker A: Rhino.
00:39:43.660 - 00:39:56.590, Speaker B: And as I mentioned, we have the Cairo playground. And many different users can send different kind of jobs with different kind of logic to the same sharp that goes on the same prover, which is super cool.
00:39:58.160 - 00:40:20.884, Speaker A: Interesting. So basically, the way this came to be is there was Starkx, and you built a whole backend to aggregate Starkx proofs. And then Starknet was just another box on top of that proving service, which allowed us to basically be fast to be on Testnet and Mainet. This is really cool.
00:40:21.082 - 00:40:23.030, Speaker B: Yeah, Sharp was there before.
00:40:24.360 - 00:40:33.128, Speaker A: And then each of these services has its own backend that then pulls the facts and updates its state according to that.
00:40:33.294 - 00:40:53.330, Speaker B: Exactly. Okay, so I wanted to mention some of our challenges. It's really fun to work on sharp. I can say we really enjoy it. There are many changes, quite new, right?
00:40:54.020 - 00:41:00.240, Speaker A: How do you explain to people, even people who are doing in informatics like that? I'm working on a proverb.
00:41:00.820 - 00:41:48.050, Speaker B: Yeah, that's like cool. Coming to work for Staglo, I was amazed that the actual work with finite fields and polynomials, and who would imagine, when I did a math bachelor, I was like, it looks really not related to anything that you actually do something practical with. That's great. So we are talking a lot about optimizing prover. There are many different optimization paths that we can take. Each of them, most of them are like a big project. It's not something that you do in a week or a month even.
00:41:48.050 - 00:42:28.024, Speaker B: So we can play with hash functions. Some of them are more efficient for the kylo verifier, some of them are more efficient for the prover, some of them for the solidity. I mean, there are many changes that we can do with that. As I mentioned before, lately we moved from committing with the Merkel tree, which used Blake. Now part of the layers of the Merkel are using Peterson. And maybe later we will switch to Poseidon. We are talking about our field, which is a big field now, 252 bits.
00:42:28.024 - 00:43:18.620, Speaker B: And there are some ideas to move, at least in the recursive steps of the proof, to smaller fields, which have better, which we compute more efficiently. And there are many parameters for the start protocol that we can change the fry parameter block factor. I mean, I won't get into this. This is a whole different topic. There are also, of course, optimization in the Cairo code that can be done to make the Cairo verifier faster. And hence the recursive prover faster. Because if the recursive proverb gets a shorter program, it computes the proof faster.
00:43:18.620 - 00:43:39.750, Speaker B: As we mentioned before, even without touching anything in the code, just by picking the right machines, you can have a faster computation and a cheaper computation and many many more. We can talk about it for hours.
00:43:40.920 - 00:44:04.732, Speaker A: This is really interesting. Yeah, there are questions on YouTube. Can I quote them? Wonderful. So Gaet boot is asking which state corresponds to what in terms of pending accepted on L two. Accepted on l one, basically. And this is a question for Stocknet. Right.
00:44:04.732 - 00:44:39.510, Speaker A: Accepted on l two means that your transaction was included in a block, but it hasn't been proved yet. Once your block has been proved and sent on l one, your transaction moves to accepted on l one. Then get boot is also asking as Starkx and Starknet are different networks, but both need to be settled on l one. Is there some kind of priority queue involved or is everyone at the same level? I think you into that earlier. It's basically there's a timeout when a proof has been waiting for too long to be delivered to l one, it is sent and verified there. Is that correct?
00:44:41.100 - 00:45:02.284, Speaker B: At this point there is no what one can call quality of service. Like all the jobs are equal and the oldest job in the tree triggers the entire tree to go on chain. So we don't even know if this is like the algorithm doesn't know if it's Starknet or Starkx or whatever, just.
00:45:02.322 - 00:45:08.770, Speaker A: If it's been waiting for too long, we process it. Pretty cool. All right, that's it for questions.
00:45:09.380 - 00:46:02.224, Speaker B: So one more challenge, actually. Now there is a project we are working in our team and in other teams as well, is what we call dynamic layout. So the layout is like a layout which defines the different resources that the Cairo needs, that the prover needs. And currently in production we have like a final set of layouts which we precomputed and we have different Cairo verifiers that operates with these different layout, but we want to make it dynamic, it will be much more efficient. So for each proof we dynamically compute what resources it needs and how we use them in the prover. So that's like a big project, really interesting one.
00:46:02.342 - 00:46:48.140, Speaker A: What kind of resources does the layout? Okay, my understanding of the layout, and I'm sorry if this is a bit simple, but my understanding of the layout is basically when you execute a Cairo program which built ins, it has access to and in which quantity. So when you're running a Cairo job on a specific layout, it can use Pedro send ashes up to a certain limit. And once it hits that thing, it can't compute this anymore. And so I see it in my head like an embedded circuit which has access to a set amount of resources. And if you need something that is just above the board, you have to go with a bigger board. Exactly. And dynamic layout.
00:46:48.140 - 00:46:52.944, Speaker A: Okay. And dynamic layout allows you to scale resources depending on your need.
00:46:53.062 - 00:47:03.620, Speaker B: Exactly. Which is super great, because as you said, sometimes due to a single resource, you need to double your computation in order to have more instances.
00:47:04.680 - 00:47:07.716, Speaker A: For example, the bigger layout needs more computation.
00:47:07.908 - 00:47:09.352, Speaker B: Yeah, of course.
00:47:09.406 - 00:47:12.276, Speaker A: Is it for verification or for computation?
00:47:12.388 - 00:47:16.970, Speaker B: For computation, for prover. The prover need to like.
00:47:17.660 - 00:47:29.692, Speaker A: Okay, so making a rough analogy, if we take embedded circuits, you go with a bigger board, you can do the same thing, but it will cost you more in terms of electricity because you need to power the whole board, is that correct?
00:47:29.826 - 00:47:52.550, Speaker B: Yes. But if you can modify the layout when you get the data and see how much peterson you need, how much rangex do you need, and so on, you can make it like, as we said before, like the taxi, the special taxi for your specific need. So it's better than having five different lines and you can go on one of them.
00:47:53.080 - 00:48:01.892, Speaker A: Yeah, makes sense. Cool. But does this mean that you have to run the job first in order to know how you're going to parameterize the layout?
00:48:02.036 - 00:48:03.608, Speaker B: But that's true anyway, because.
00:48:03.694 - 00:48:07.912, Speaker A: Yeah, that's what you do at the validator earlier in the scheme. Nice.
00:48:08.046 - 00:48:10.430, Speaker B: Exactly. Yeah, that's cool.
00:48:10.880 - 00:48:21.432, Speaker A: It sounds really nice, but it sounds like far out. It's hard to wrap my mind around it and how you. This sounds fascinating.
00:48:21.576 - 00:48:38.660, Speaker B: This is more intuitive than having a finite set of layouts that were predefined in advance. But it's complicated to code it. So that's why I think first step, we did something not dynamic and now we are moving to this dynamic setup.
00:48:39.720 - 00:48:40.470, Speaker A: Nice.
00:48:41.480 - 00:49:49.316, Speaker B: Some other challenges which are really interesting as well, is the scheduling algorithm. So we get like online streams of jobs and we would like to schedule them to the proverb as efficient as we can. So right now, before I mentioned the scheduling algorithm, that for a job that comes from the user, we just put it on a train and send it to the proof and then we pair two to one. But we could do many other things and the way we build the trees, we can change. We're thinking a lot about the scheduling algorithm. For now, we go with a pretty simple one, but when we get more and more traffic, it will be more and more like a problem to make it as efficient as we can. And some other interesting challenges that we have is like how to sharp is like an infrastructure system because many systems are operating on sharp.
00:49:49.316 - 00:50:12.000, Speaker B: And as I mentioned like two days ago we upgraded sharp and it's a big challenge to do it in a way that won't harm many end users. So eventually we have many starkx users and all the starknet users and we don't want to have a downtime for sharp. And all this deployment and upgrading, it's not trivial. It's really not trivial.
00:50:12.660 - 00:50:20.500, Speaker A: Yeah, it sounds like there will be a lot of contingencies if you need to upgrade sharp, if you need to upgrade the verifier and all this stuff.
00:50:20.650 - 00:50:56.944, Speaker B: Yeah, Sharp is operating for a bit more than a year and a half on mainnet and this was the fourth version of Sharp. So we don't upload a new version, we don't upgrade every month or mean we do many heavy checks and we mirror input from Mainet to Gerley for a while to make sure the new version working properly. And it's a long process to upgrade the system.
00:50:57.062 - 00:51:01.248, Speaker A: It must be quite stressful also, like there's no safety net, right?
00:51:01.334 - 00:51:06.000, Speaker B: Yeah, it is stressful, but we are working hard.
00:51:06.150 - 00:51:07.436, Speaker A: Yeah, living on the edge.
00:51:07.468 - 00:51:08.530, Speaker B: That's really interesting.
00:51:09.460 - 00:51:41.180, Speaker A: Gadboot is also asking, so the transition to Cairo one had no impact on sharp or did you have to add something to endle the case where it can fail? I think I can answer that one to you whether like Cairo zero compiles to Cairo assembly which is executed on Cairo. Cairo one compiles to Sierra, which compiles to Cairo assembly. It's the same cpu, it's just a new intermediate representation on top of the same cpu. So to you I don't think it changed anything.
00:51:41.330 - 00:51:46.688, Speaker B: We didn't touch. No, you're correct.
00:51:46.854 - 00:51:53.664, Speaker A: Okay. To a certain extent your team doesn't. Do you guys use Cairo a lot or not that much?
00:51:53.782 - 00:52:30.136, Speaker B: Okay, so when we implemented the recursive program, we wrote the bootloader like shaka Samoa. I think he was here a few months ago talking with Shakar Hopini. So yeah, sometimes we write Cairo, we modify the Cairo verifier. Sometimes when you change approver you must change the verifier most of the time. So when we touch the proof we also touch the verifier. We don't write and also we generate test Cairo inputs for our system. But we don't write other than the Cairo verifier.
00:52:30.136 - 00:52:45.750, Speaker B: We don't really write Cairo. But in the team. We write c for approver. We write solidity, sometimes Cairo and python. So we have like a wide range of languages and technologies that we.
00:52:46.120 - 00:52:51.636, Speaker A: Nice. Cool. No more questions on YouTube and no.
00:52:51.658 - 00:52:53.030, Speaker B: More slides as well.
00:52:53.480 - 00:53:13.624, Speaker A: Wonderful. Looks like we reached the end of the presentation. I'm going to make one last check, but no, it looks like it's good. Kineret, again, thank you for your time today. This was really interesting. And I think you shed a lot of lights on a part of the system that everybody uses, but nobody really knows how it works. So this is really helpful.
00:53:13.624 - 00:53:36.610, Speaker A: I also want to mention that a while back you did a really good tutorial on Stark and the Stark protocol. It's called Stark 101 along with Itai. And if our viewers are interested in learning more about the Stark protocol and how to code it, you can code your own proverb there. You should absolutely check it out. It's really cool. And hopefully one day we get Stark 102.
00:53:38.340 - 00:53:46.850, Speaker B: Maybe just to mention about Stark 101. So it was way before Cairo. So it's not like Cairo oriented. It's star.
00:53:47.740 - 00:53:54.730, Speaker A: Yeah, but still, it's good. I think doing the same thing with Cairo would be a bit dense, but it would be interesting.
00:53:55.340 - 00:53:56.376, Speaker B: I agree.
00:53:56.558 - 00:54:05.850, Speaker A: All right. Well, again, Kinerat, thank you for your time. And to all of our viewers, thank you for being with us. And until next time.
00:54:06.460 - 00:54:08.340, Speaker B: Yeah, bye.
