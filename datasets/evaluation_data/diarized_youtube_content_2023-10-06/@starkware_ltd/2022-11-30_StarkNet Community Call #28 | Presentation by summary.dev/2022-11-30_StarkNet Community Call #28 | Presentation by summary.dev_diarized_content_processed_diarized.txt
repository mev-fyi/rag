00:00:31.940 - 00:01:50.440, Speaker A: We are live on YouTube, but we'll give people like two or three minutes to join. Yep, I see that YouTube on my phone is working fine. Give people another minute to start and then to join and then we'll start. Let me just confirm with one of the viewers that everything's running smoothly.
00:02:11.430 - 00:02:12.034, Speaker B: Great.
00:02:12.152 - 00:02:25.880, Speaker A: We've got confirmation that the YouTube is working and it's a minute past the official start time, so I think we can get going. Welcome to the 29th, I think.
00:02:28.190 - 00:02:28.554, Speaker C: 20.
00:02:28.592 - 00:03:32.320, Speaker A: Eigth Starknet community call today is Tuesday, November 29, 2022. A lot has happened in ecosystem since our last call, including the open sourcing of Kara 1.0. Today we have two items on the agenda. The first is Oleg from the summary dev team will present what he's up to in the Starknet ecosystem, and then after that, Ariel Alterin, one of our Starknet product managers, will give a bit of an update on the Starknet roadmap, what's been happening, and what the next few months will look like. Spoiler alert. There are very exciting updates coming through, but we'll leave that for then. So without further ado, let me turn to Oleg and Oleg, perhaps before we go to the demo, let's just introduce yourself and tell us a bit about yourself.
00:03:33.410 - 00:04:31.466, Speaker C: Thank you. I'm Oleg Olek Abdrashotov. I'm the founder of summary dev and the developer of Stacknet Indexer. Our goal at summary is to build tools for web three developers to be a one stop shop where developers can get blockchain data, analyze it, and also have other tools. Right? So we started with Startnet Indexer. We think that indexing is extremely important. Without a proper indexer, both developers and users of blockchain applications are blind because all the data is available, but it's available in a blockchain format and you need to get it out and present to the users in the proper way.
00:04:31.466 - 00:06:10.510, Speaker C: The proper way, meaning that the data should be live, should be decoded, it should be indexed so that you can consume the data with analytical tools and with your automation tools. So you're probably familiar with other types of indexers. With the Stacknet indexer, we took a little bit of a different approach where we get all the data from blockchain, we transform it right there on the fly, meaning that we decode it and we save already decoded data in a relational database. And once data is available in the database, then you can put all other, the whole plethora of tools on top of it. We give access to Starknet data via a GraphQL console and also with a SQL console. So you can run GraphQL and SQL queries against data which is actually live and already decoded. So as soon as our pathfinder node gets the data from start net chain, then we decode it and within a few seconds it's available in the database and it's available to you to analyze in GraphQL console.
00:06:10.510 - 00:06:20.260, Speaker C: So without further ado, I can start the demo if my screen is available.
00:06:22.070 - 00:06:28.134, Speaker A: We do see your screen. Let me double check. The YouTube sees your screen. Yeah, the YouTube sees your screen. You're good to go.
00:06:28.172 - 00:07:28.246, Speaker C: Okay, super. Okay, so it's a developer tool. So it looks perhaps a little low on design, but some of you who are familiar with GraphQL may recognize this GraphQL ide. So this ide lets you form graphql queries and run queries against decoded data. So on the left you see a schema explorer where you see all the available graphQl nodes. And these nodes follow Starknet entities, right? So obviously you see blocks and you can query blocks. You see then inside blocks you see transactions.
00:07:28.246 - 00:08:49.950, Speaker C: And each transaction has a list of events. And of course events have a list of parameters that we call event arguments. Similarly transaction has a function name that was invoked by the client and also a collection of input parameters, again also decoded. So a blockchain analyst or a developer would come to a graphQL console, would explore the schema. On the left we'll see entities that he or she needs to look at and will be able to form a workflows will be able to form a graphql query just in this explorer tree window. Right, I'm going to go through a few example queries to show what you can get out of the indexer and how you can form the queries. There are much more examples in the documentation site, so I will follow documentation, just the main queries from the doc.
00:08:49.950 - 00:10:01.014, Speaker C: So when you open the indexer, it loads a simple query that simply gets the latest block in the three chains that we index, which is quarterly mainnet and Testnet two. Now obviously this information has little value, so most of the value is in quitting your own data or data of a contract that you're interested in. So we can start with let me load the sample quiddies. I will simply paste them from text editor. And once the queries load in the middle, I mean you can close the Explorer. If you're familiar with GraphQL, you can write the code by hand, but I'm just going to run these queries one by one. So let's say you're interested in events, in particular events emitted by the contract that you wrote, or a particular contract.
00:10:01.014 - 00:10:49.798, Speaker C: So you can put together your query by selecting a where close on the left and explorer, or you can write it by hand, right? And it's pretty self explanatory. You're quitting an event on a test. In testnet, your workload specifies that you're looking for some mint event. Could be any name that you know, transfer, approve and so on. Any name of your smart contract. And you put in the address of your smart contract, right? And then obviously you don't want to get all of the events, so you put some reasonable limit. Let's say you want to get three mint events.
00:10:49.798 - 00:12:12.002, Speaker C: And then it gets more interesting because you also want to get the actual payload of the events, right? So we call those payload entries, arguments. And these arguments will be name, type, value, and also a value if it is a u into 56 or a felt converted into decimal. Now where graphql shines is in combining queries together. So within the same query, let's say you want to get the entire block and we simply add this statement where we want to get a block 100,000, right? So I'm going to run this query. This is the previous. So this query gets you three latest mint events and also the entire block, 100,000. I put them together to show you that the data that you get from the indexer is already decoded, right? So whenever you see a number and this number can be converted into decimal, you see that it is a decimal, right? So when we get an event from Pathfinder, then we decode it or unpack it right away.
00:12:12.002 - 00:13:52.674, Speaker C: And we save already decoded values, right? So if an event has three fields, sender amount zero and amount one, then that's what we save and that's what you get in your query. The row block, of course has this, has all the call data encoded in hex. So going forward, obviously you can look for a particular argument, right? And you can enhance your workloads, right? You don't want to get all of your mint events, but for example, you want to get events of your smart contract where a particular field, in this case, let's take amount one, crosses some threshold, right? So this field amount one is greater than ten, for example, right? Again, this is just an example. So running this query will. There you go, I will get you all of the mint events whose particular argument amount one is less than ten. And you can put together queries and combine them together and in one shot you can get all the data that your application requires, right? And this application can be a web app, it could be a script, it could be some back end automation. So that's what you get.
00:13:52.674 - 00:15:08.346, Speaker C: You get mint events whose amount one is small, right? Some of the data in events and in transaction, in function invocations, some of the data is packed in JSOn. So let's say, let's look for that particular field, let's look at this query and it returns you values for a particular tuple, right? In order to further decode this tuple, you can get just a part of it from JSON payload. So that's how you get to the second part of it, right? And that's the ABI for this event. And you see that it's a tuple, x and y. So that's how you can get the second part of your tuple. So by now you just, are you.
00:15:08.368 - 00:15:15.600, Speaker A: Saying that the ABI is stored as a raw string inside the.
00:15:22.550 - 00:16:34.162, Speaker C: Everything that we get from the node we also save as raw. That's what people call now not ETL but ELT process, extract, load and transfer. So the ABI that we got from finder will also save it. So it's available to the query. So with this graphql queries, you can filter on any contract, any event, any function name, or even any argument of an event or of a function. And you can build together rather rich queries and then consume this results with your application. But what if you want to aggregate those events? Let's say what if you want to get a sum of all the fields, right? What if you want to get an average of a particular field? Now instead of running this code in your client, you can run an aggregation query.
00:16:34.162 - 00:18:06.470, Speaker C: In aggregation query, the workload is a little bit different. You actually put an aggregate function in your query. So this query is looking for an argument with name amount zero of an event called mint of a particular smart contract. And then this aggregate gets you the sum of all the amount zero and also average min and max, right? So we believe that this functionality covers a lot of analytics use cases where you want to quickly get a sum of a particular field or an average and so on, but not quite. And I'll show how we progress from there. So what we've seen so far, synchronous calls to, to get, get data, right? And let's say you want to get some live data, you can query for your latest events, right? So this query simply calls for the last three events that it knows orders by id, which is auto incremented. So that's how you get your last three events.
00:18:06.470 - 00:18:52.002, Speaker C: What if you want to get results asynchronously? So that's where graphQl subscription comes into picture. With subscription, you can turn any query into an asynchronous call. So we take the same query that is looking for the last three events and we replace the query statement with subscription and see what happens. Um, there you go. This is the subscription. And you see that the, the client started, it opened a websocket connection and it will start getting new events whenever they arrive. So you see this change on the screen.
00:18:52.002 - 00:19:31.086, Speaker C: Whenever a new event comes in, you get the last three events. So again, the query can be as complex as you need. You can put thresholds, work clauses, and you can get a very particular slice of data updated live on your screen. Now, it may be useful for analysis, but where you want to consume this subscription or asynchronous live data is by your application. So everything that you see on the.
00:19:31.108 - 00:19:31.680, Speaker A: Screen.
00:19:34.310 - 00:20:37.330, Speaker C: Synchronous calls queries can be just put together against our HTTP endpoint. So this call calls a get request. I think it's a post request against our graphql endpoint where we put the same query. So if you're building backend or automation tools or web application, you can train your queries in the Explorer, get the results that you need, save the query, and then use this query programmatically from your application. Similarly with subscription, with subscription, we give you an example of how to build a subscriber client. It's a GraphQL subscription over Websocket client. And here I'm calling the same subscription for the last three events, and you see that the client is getting last events.
00:20:37.330 - 00:21:01.020, Speaker C: It will close after three, but obviously you can modify the code and get any data that you need and run continuously. So we think this is an important feature for building automation tools and getting live data either for your web application or for your back end. So.
00:21:04.510 - 00:21:34.630, Speaker A: Moving further, perhaps if they're asking a simple question on that, on Ethereum, on uniswap, often when you click trade and then the web app comes and says price is updated since we last quoted you the prices, I assume what's happening on that back end is that the indexer is showing them the latest parameters for the pool. So that could be one use case for asynchronous events.
00:21:37.210 - 00:22:18.360, Speaker C: Right? It could be, again, the technology behind it, it's not rocket science, it's some engineering, right? But what it does it simply pulls the database. Either pulls the database or it itself gets a trigger event. And whenever you get your event from Pathfinder, then it gets into the database and then it gets over to you via a websocket, right? So it could be any event or any piece of data right here for demo, I just show the latest event that we get with its id and name.
00:22:19.130 - 00:22:23.270, Speaker A: Do we have a comment in the chat? If you can increase your font?
00:22:24.890 - 00:22:25.640, Speaker C: Sure.
00:22:29.290 - 00:22:30.940, Speaker A: Great. If that's too big.
00:22:32.030 - 00:23:38.186, Speaker C: All right. Okay, so moving forward we'll be getting to more and more complex use cases. And at some point graphql, particular queries in this generic graphql schema hit a certain, hit a certain wall. I mean, there's only as much as you can do with GraphQL. So I'm going to shut down the subscription and I'm going to rerun this query. Now, this query gets you all the arguments for that sample event, mint and so on. Now what else can you do? You can sum these values, right? You can average these values, but you cannot really build any other data set that requires grouping over dates.
00:23:38.186 - 00:24:32.890, Speaker C: I'll give you an example. What if you want to calculate a change in LTV, in TLB and total value locked over dates? For that you need to sum a particular field in your events by date, right? So it's not that obvious to do it in GraphQL. So for that we give a SQL editor, a SQL editor at this stage is rather primitive and it's there mainly to give you another view into the data and mainly to create graphql queries out of it. So I'll give you this example, perhaps.
00:24:32.970 - 00:24:43.620, Speaker A: Can I just understand the motivation here? With GraphQL you can just do like a cumulative sum over all of history, but you can't necessarily say give me the last seven days.
00:24:44.010 - 00:24:45.800, Speaker B: Okay, correct.
00:24:47.130 - 00:25:52.118, Speaker C: And not only that, what if you have some ad hoc complex SQl query that groups over, then sums over, then more and more. So you cannot really do it with GraphQL. So what we ended up doing for ourselves is to write a SQl query. I'm going to paste this new query that runs a SQL of any complexity that you want. You run this query, it should give you account of transactions per date starting with November. And then once you're satisfied with the results, you can actually create a graphql out of it. And the way you do it is to click on create view, give it a name so it returned.
00:25:52.118 - 00:26:01.914, Speaker C: Those are the number of transactions in November. Now what I can do, I can give this a name something like TX.
00:26:01.962 - 00:26:05.086, Speaker B: Count and create a graphql out of.
00:26:05.108 - 00:26:09.230, Speaker C: It so it will show up in the explorer.
00:26:12.070 - 00:26:17.602, Speaker A: Let me, let me find it real.
00:26:17.656 - 00:26:36.290, Speaker C: Quick as a user node, right? So right here you see user TX count back to back to large font. It's hard to navigate.
00:26:36.370 - 00:26:37.034, Speaker B: Here you go.
00:26:37.072 - 00:27:47.310, Speaker C: So the query that you just created, tx count becomes a graphQl node, user tx count because it's a user query and you select the same fields as you put in your SQL query and run this for you. Hold on. You select count and date and there you go, you run it and you get the results in GraphQL. So the workflow we see is that an analyst or a developer comes to a SQL console, crafts a SQL query. Once you get the results that you need, you turn it into a graphql query and then you can run this query from your client in your web app or your automation tool. And of course you can turn it into a subscription as well. Again, this UI is still under development.
00:27:47.310 - 00:28:55.346, Speaker C: It's rather simple at the moment, but it gives you all the tools to put together synchronous and asynchronous channels to get your data. So I simply changed query to a subscription and I will start getting updates on the right or in the client. So that concludes my rather short demo. There are much more examples in the documentation, a few words on our approach and the architecture. Like I mentioned already, it's an ETL and ELT process that pulls for the latest blocks from Pathfinder nodes that we have right there. During the transformation process, it decodes the data and saves everything in raw format and in decoded format. It sounds rather simple, but of course there are details like handling proxy contracts.
00:28:55.346 - 00:30:09.840, Speaker C: For example, most of the events and function calls right now go through proxies. So whenever possible we figure out the implementation contract and we use the implementation contract's EDI to decode the data. So by now some 99% or even more percent of data that we have is decoded and decoded properly. So I'll be happy to answer questions, but before then, I should say that this is the beginning. We thank startware for your support. We will continue developing Startnet indexer, but what you will see very soon is a similar approach applied to Ethereum, and we'll start indexing EVM chains in the same manner. So we'll give both SQL and GraphQL access to Ethereum data, which of course presents much more challenges because it's three or four orders of magnitudes, three or four orders of magnitude more data.
00:30:09.840 - 00:31:01.760, Speaker C: Now another vector of our development is presenting this data. One thing is to consume this data by your client or visually, in GraphQL or in SQL. There are wonderful bi or business intelligence tools that let you connect to your database and on the spot, convert it into beautiful graphs, pie charts and so on. So expect this very soon, both for starknet data and for Ethereum data as well. So again, thank you very much for the opportunity, and I'll be very happy to answer questions to show something on the console as well.
00:31:02.290 - 00:31:33.110, Speaker A: Thank you, Alec. That's the easiest interview I've ever done because your flow made perfect sense. I didn't need to interrupt those questions. Thank you. There's one question in the chat about if users can generate a URL to make a query or if it's just the client that allows queries. I assume that the person is asking how can they get access to this client to ask these queries?
00:31:34.570 - 00:32:18.870, Speaker C: Well, the console is available. It's open to the public at the moment. You don't need any login, it's at summary Devconsol. You can get to it from the main page. There's also a link to our GitHub where you will find a simple client for subscriptions. You can put together your own subscriber with either Apollo client or with any other websocket client. But here we put together this simple client that's maybe a dozen lines of code that you can start, connect to the URL and throw your query and start getting updates.
00:32:20.810 - 00:32:34.940, Speaker A: Great, thank you. A few small questions and then to Ariel. Any other resources? I guess you have this web page. Where else should people, can you direct people, should they join the telegram group, the social media to follow you on?
00:32:35.470 - 00:32:59.262, Speaker C: Yes, please. There's a telegram group, there's a documentation site with examples. This of course will be much improved, but for now, the documentation is a few pages on everything that I just covered.
00:32:59.326 - 00:32:59.794, Speaker B: Right?
00:32:59.912 - 00:33:05.320, Speaker C: Queries, your SQL access, subscriptions and so on.
00:33:07.770 - 00:33:38.266, Speaker A: Great. Okay, the last question I will ask is what use cases do you think this is relevant for, specifically in the Starconnect ecosystem, which devs do you think should be reaching out to you? Defi apps, NFT? Any specific use cases that you can think of where we should be connecting you with these teams?
00:33:38.378 - 00:33:39.040, Speaker B: Well.
00:33:41.990 - 00:34:35.200, Speaker C: It may not sound modest, but developers of all applications that need data from Startnet, I mean, it doesn't matter if you're developing a Dex contract or DeFi application or NFT, you still need to get data out of blockchain. You can write your own indexer and that's what people have been doing it. That's what people do for other blockchains as well, and consume your own events. But at some point, you will need to start looking at your competitors, at other smart contracts, other applications, and you also need the whole length of your own data saved somewhere in the relational database. And we've had relational database databases for decades, and I think they're still a wonderful tool to explore your data.
00:34:36.850 - 00:35:15.582, Speaker A: I think that's actually the right answer. I was sort of leading towards that answer. The only thing I would add, I think with Starconnect there are a lot of teams who do on chain gaming. In other blockchains, often the game is on a centralized server and the assets are minted on chain. But with Starknet, there are a few teams trying to do actual on chain gaming. And I think specifically in that vertical, there's a strong use case for having an indexer that can keep track of the state of the game in basically real time. So users can have a really good game experience, right.
00:35:15.716 - 00:35:45.000, Speaker C: And not only through the tools of the game developer, but individual participant gamer can come to this console, find a particular event that explains his achievement or the game that he's playing, and he can independently verify results without having to run his own Pathfinder node or without decoding all the data that is relevant to him.
00:35:47.450 - 00:35:48.950, Speaker A: Great, thank you, Alex.
00:35:50.350 - 00:36:01.760, Speaker B: Sorry, I'll just throw in there that currently this is the only tool that allows event registration or subscription, as far as I know, so take note of it and use it.
00:36:04.050 - 00:36:05.040, Speaker C: Thank you.
00:36:06.050 - 00:36:30.920, Speaker A: And anybody in the community who for some reason can't reach Alec or his team, you can reach out to me or Ariel or anybody in the sacred team and we'll make a connection. Yeah, I can ask you a lot more questions, but I think we'll end here. Alec, thank you for this presentation. I'm really excited. I have had calls where people ask about indexes on Starknet and I'm very happy to pass them on to you.
00:36:31.610 - 00:36:32.310, Speaker C: Thank you.
00:36:32.380 - 00:37:03.300, Speaker A: Great. We'll now transition to Ariel for the second half of the. So Ariel is a Starknet product manager working at Starquare. And yeah, as the community knows, I think that we just actually upgraded Starknet to zero point 10.2 today. Sorry if I'm bringing some spoilers, there's been a lot going on in the ecosystem. So, Ariel, just give an update what's been happening and what we can look forward to.
00:37:03.300 - 00:37:05.380, Speaker A: Ariel, over to you.
00:37:06.230 - 00:38:01.170, Speaker B: Hey, everyone. So on the community call today, I'll talk about the Starnet roadmap, or more specifically, the performance roadmap. What we plan to do and what has already been done to increase the TPS of stagnet. And feel free to ask questions. Okay, so today we've released version autumn two to Mainet, like Leon said, and this version introduces sequencer parallelization or parallel execution into the sequencer, which I'll discuss in more detail shortly before that. I'll just say that there are a lot of stuff going on, and there's a transition to Cairo 1.0 that is planned for early q one.
00:38:01.170 - 00:39:07.338, Speaker B: But today I'll try to focus on the performance front. Okay, so in the less recent months and in the coming period, we're focusing on improving TPS and performance in general. And maybe this is a bit surprising, but the current bottleneck in the system is basically a common component which is the sequencer, the component which is responsible on collecting and executing transaction and generating blocks. So maybe you would imagine that in a Zk roll up, the provers who are, I don't know, the more sophisticated component that they're the bottleneck. But actually today our provers can handle much more traffic than they get. Today the same proofs handles both Starknet and Stargix applications. So on the proving side, we're in a much better place and can do a lot more.
00:39:07.338 - 00:40:26.930, Speaker B: But actually the more, let's say, standard component is the one who's susceptible to improvements. And before I dive into exactly what we did and what we still plan to do, I'll mention that roll ups, validity roll ups in particular, are in a much better place than l ones in this regard. Because naively you could ask why not arbitrarily improve l one sequencers? Because this is a component that appears in every blockchain. So the idea of why in a roll up you would ideally reach a much higher tps than what you're used to, is that if you were to do it on a regular l one, the load on full nodes becomes too big. Let's say we increase the block size in terms of gas, then syncing from genesis becomes much more problematic. Even keeping track with the system becomes much more problematic. Since every full node to trustlessly say what's your balance? It needs to reexecute all the transactions.
00:40:26.930 - 00:41:12.398, Speaker B: But in the roll up case, where the full node may just verify proofs and does not have to redo all the execution, then there is no, let's say, theoretical obstacle on improving, significantly improving the sequence servers. Just engineering barriers which are nontrivial. But there are no theoretical obstacles. Because in an l one, if you increase tpS, let's say by three orders of magnitude, then you effectively centralize the system. Because now only two full nodes are able to keep up and reliably say what's your balance? Okay, maybe I'll share my screen for a bit.
00:41:12.484 - 00:41:33.410, Speaker A: Go for it. Do you have permissions? Yeah, just reemphasizing what you're saying. I think it's so powerful. Basically you're saying with the exact same hardware requirements that you have on a traditional layer one, you can still have higher scale in a validity roll up like starknet. And the nodes are still the same requirements. That's really powerful.
00:41:33.570 - 00:42:11.726, Speaker B: Exactly. Even less requirements if you really have a node that only verifies proofs like Pathfinder is doing now. Because that's easier work. Okay, so that's the post we've published today, or welcome to read it. I'll talk now a bit about the component we already put into stagnet, which is sequence operalization. And then I'll also discuss what else is planned. I think the easiest way to do it is to focus on this picture.
00:42:11.726 - 00:42:48.682, Speaker B: So let's say we want to generate a block with the transaction one through nine. So that's the block we want to create. And what we did so far before ten two was execute them sequentially. So, transaction 1234, et cetera, et cetera. And this is important because some transactions may be dependent on others. For example, imagine that transaction one transfers some tokens to my account. And transaction five makes use of those tokens to do some other operation.
00:42:48.682 - 00:43:14.840, Speaker B: Say swap them for a third one. Then obviously transaction five won't be successful. Before transaction one, which passed me the tokens, I must execute them in order. But this is not necessarily true everywhere. For example, I can run transaction one and two and three in parallel. Because they are not coupled in this way. They don't have the relationship with the transaction one and transaction five.
00:43:14.840 - 00:43:57.762, Speaker B: So that's the idea behind what we. And the algorithm developed by Aptos block STM, which is called optimistic parallelization. So that's the idea. And it basically says, okay, I don't know the relationship between the transactions in advance, but I'm just going to try to execute them in parallel and see what happens. And if there were no sort of collisions or dependencies, then I'm happy. And if there were, then I'll have to do some extra work and reexecute some transactions. So in this example I can, let's say execute transaction one four, five in parallel.
00:43:57.762 - 00:44:55.506, Speaker B: Okay? And after I was done. I can see that. Let's say for the purpose of this example that transaction five executed before transaction one, then I can see, okay, in fact, they're dependent, and I should be sure to re execute transaction five because apparently there was a dependency and it's important to first transfer the tokens in transaction one and only then swap them in transaction five. So I'm optimistic. I'm trying to run everything in the best parallel with the maximum parallelization that I can utilize all my cores. But if I end up seeing, okay, something did not run in order, then I have to redo a portion of the work. That's the gist of the idea, and that's already taking place in this version of Starknet on testnets and now mainnet.
00:44:55.698 - 00:45:08.746, Speaker A: Ariel, a few high level questions on that. Let's say there are nine transactions. How do you choose to do like three streams of parallelization and not nine streams of parallelization? Or do you start with nine and.
00:45:08.768 - 00:45:54.700, Speaker B: Then, okay, so it depends on the hardware available to you and the size of the block. So according to whatever USD calls you have to say, okay, there's going to be, I'm going to start with 100 transactions in the block. And then if you have, let's say five cpu cores, then you can just start putting your five calls to work on the first five transactions, okay. And then you start getting results you're seeing, okay, transaction one executed successfully. Transaction five also executed successfully, but you see that it attempted to read some value that is affected by transaction one. So that's where you say, okay, I was a bit too optimistic. I'll have to rerun transaction five now.
00:45:54.700 - 00:45:56.826, Speaker B: Okay, good.
00:45:56.928 - 00:46:08.110, Speaker A: And then the last question is, how do you know the transaction five failed because it was dependent on transaction one, as opposed to it's failing because it's an invalid transaction, like you just try to change assets.
00:46:09.970 - 00:46:33.800, Speaker B: The answer, which is slightly technical, is that you keep for each transaction the set of storage cells that it read from and wrote to. And dependency is formally defined by transaction reading a cell that another transaction wrote to. So in the concrete example, transaction five read a balance that was updated by transaction one.
00:46:34.330 - 00:46:41.990, Speaker A: Yeah, good answer. So basically there's maybe a slightly more overhead per transaction with knowing what these dependencies are, but it's worth it in order to enable federalization.
00:46:42.070 - 00:47:00.110, Speaker B: But technically the sequencer should probably maintain those differences anyway because they affect the final state commitment and other things. But there's a slight overhead for every transaction because of communication between different processes, et cetera, et cetera.
00:47:02.150 - 00:47:07.300, Speaker A: Great. Do you have rough numbers for what an improvement this has been so far.
00:47:08.630 - 00:47:41.614, Speaker B: I think that we've seen on Testnet a factor of two for now. Obviously the improvement depends on the nature of the transactions. You can send a lot of transactions that are inherently sequential, so you'll see a lot of variance there. But there are still some optimizations that are planned for the optimistic parallelization mechanism in o ten three. So yeah, that's the initial number.
00:47:41.732 - 00:47:58.834, Speaker A: I also assume that as mainnet matures, as more apps deploy on these networks, those testnet or Mainnet, you'd actually have more benefits from prioritization, meaning if it's just two or three big apps right now, there's probably more dependent transactions than you would have in there.
00:47:58.872 - 00:49:09.846, Speaker B: Exactly. If you have more contracts and more variety in general, then you'll gain more from parallelization. Okay, so other plans we have on the performance front is an exciting endeavor that is being done by Lambda class right now, which is an external team or company. So they basically rewrote the Cairo VM implementation in rust internally. We notice that perhaps expectedly, most of the work the sequencer is doing is executing Cairo code, which is to be expected because you have to run contracts written in Cairo. And the current implementation of the Cairo VM is pythonic and could be improved a lot. So what they did over at Lambda class is essentially re implement the Cairo VM from scratch.
00:49:09.846 - 00:49:29.840, Speaker B: And basically every day they keep adding more optimization on top of it. And on starknet, we plan to transition to this new vm as soon as everything's ready, and it's basically as reliable as the current one.
00:49:30.690 - 00:50:04.170, Speaker A: I want to just take a step back here to explain this to the audience, because the word Cairo, it took me a while to fully understand this. It means a lot of things. There's a virtual machine, there's the operating system, and there's the actual programming language. So the focus on this rust implementation is specifically for the virtual machines. From the developer's perspective, writing kyro smart contracts, there's no changes in terms of this. The changes happen later on in the translation of the code to bytecode.
00:50:05.230 - 00:50:52.202, Speaker B: Exactly. So the VM implementation does not affect developers. It's only important for the sequencer who actually has to run the code. And obviously, if most of the work the sequencer is doing is running Cairo and you've improved the Cairo VM by, I don't know, 20 x, then the sequencer improvement on block generation times will be almost 20 x. Depends on what exactly is the portion it's working on. Cairo execution. So probably that's the major factor that will affect performance in the near future at least.
00:50:52.202 - 00:51:17.330, Speaker B: And another thing that's worth mentioning, that it can be even better with the current sequencer parallelization. So there's only more to gain. If the Chiro VM is basically running faster, we can gain more from parallelization. We can spot dependencies sooner, we can do a lot of stuff sooner and end up with lower latencies.
00:51:19.270 - 00:51:48.566, Speaker A: And the third step, which that's mind blowing, I'm actually learning stuff just listening to you. You're basically saying that, let's say, naively speaking, I'm throwing out numbers. Sequence of parallelization was a two x improvement. Cairo VM moving to rust is a ten x improvement. You're saying it's not just a 20 x improvement, it could actually be more than 20 x, because having the rust implementation of VM might make the parallelization even more efficient.
00:51:48.678 - 00:52:04.962, Speaker B: So it won't make the parallelization more efficient, it would make only the Cairo VM more efficient. But having a faster Kyro VM can obviously affect the parallelization process as well. For example, you spot dependencies sooner and that sort of stuff.
00:52:05.016 - 00:52:05.860, Speaker A: Very cool.
00:52:08.150 - 00:52:48.746, Speaker B: And the final step, which is a major change that was started recently by two independent teams, is the implementation of the sequencer in rust. So not just the Cairo VM component, but the entire sequencer, the entire logic that takes a bunch of transactions decides, okay, that's going to be the next block, starts creating it, and then computes whatever the block hash and sends it to the network. This entire logic is being now rewritten in rust, and we again hope that this will add a lot on top of the two previous steps.
00:52:48.858 - 00:52:50.880, Speaker A: What language is that currently written in?
00:52:51.350 - 00:54:31.098, Speaker B: Currently, both the VM and the sequencer are implemented in Python, and I'll just mention that if we'll include the new VM in its current implementation in the pythonic sequencer, there will be some overhead, possibly significant, due to the communication between the pythonic and rust processes, while when the sequencer will be written in rust again, that's another factor you can gain from. You won't have to go through this additional communication layer, because right now, again, while the sequencer is pythonic, if the VM is in rust, then there's some overhead in sending messages back and forth. So that's the three big things that are planned for the sequencer. The first one is basically already there, up to some further improvements that we're going to add in ten three regarding the Cairo VM. So the version that can communicate with Python is essentially almost done, but it's TBD when exactly it will be incorporated into our sequencer. Before being incorporated in our sequencer, it can be incorporated in many developer tools, for example, the Devnet. So it will be much easier to replace the VM in the Devnet than it is in Starknet.
00:54:31.098 - 00:55:28.850, Speaker B: And then many of the complaints regarding long testing times, the situation there will be improved significantly. So that's planned for the near future, and eventually also for Starknet. Whether it will be already in the pythonic sequencer or in the new rust one, that's still TBD. But these are the next steps. And obviously, performance aside, there's another vector for Starknet, which is the transition to Cairo 1.0, which will be a much more rust like and higher level language than the current Cairo. It will also guarantee DOS protection in Stocknet, which is a prerequisite for decentralization.
00:55:28.850 - 00:55:36.838, Speaker B: And we can talk a lot more about that. But I kind of wanted to stay focused on performance, but feel free to ask questions there.
00:55:37.004 - 00:56:09.738, Speaker A: Well, I think just with Cairo 1.0 it's worth mentioning, and I'll put the link in the chat. It went open source last week. Now perhaps, Ariel, you can explain that this open source version that people see on GitHub isn't yet ready to write dark net contracts, but it's definitely worth mentioning that trying to look at what other updates are worth sharing about. Kari 1.0 yeah, it can open pull requests. Yeah, go ahead, Aria.
00:56:09.834 - 00:56:55.310, Speaker B: Yeah, I just wanted to elaborate a bit on that. So we've open sourced Kyo one, and you can see the new compiler, repo, and you can actually already compile and run basic Chiro 1.0 programs. It still does not have the smart contract part of the language capabilities, for example the syntax for storage variables. This is planned for the coming weeks, and we plan to incorporate Chiro one into Starknet in the next major version, which is eleven. And by incorporating it into Stocknet, I mean being able to write, declare and deploy Cairo 1.0 contracts.
00:56:57.330 - 00:57:12.820, Speaker A: And then just for the community to know, once that is supporting Starknet, there'll be a migration period called the Regenesis, where you'll be able to upgrade your existing Cairo 0.9 contract to Cairo 1.0.
00:57:14.170 - 00:57:25.218, Speaker B: Exactly. There will be a period where you can essentially replace your implementation with a new Cairo one. One, yeah.
00:57:25.384 - 00:58:42.346, Speaker A: So on the performance side, what Ariel said, the three big introductions there, parallelization of executional transactions, Cairo VM in Rust, and sequencer written in rust, and then on the decentralization path, I guess I would call it Cairo 1.0 will enable reverted or failed transactions, which will then enable the regenerates of the blockchain, which are all prerequisites for ultimate decentralization. Perhaps I'll just spend 30 seconds explaining that for the benefit of community members who don't understand this and Ari, I'll just add on anything I missed. But basically, the previous versions of Cairo were basically built as internal tools for decentralized operators of like Starkx and Starknet. And basically, if you submitted a transaction which didn't have enough gas or failed, there was actually no way for a decentralized sequencer to know that ahead of time, and then be rewarded for trying to run the transaction and then the transaction failing. Like on Ethereum, if you do a reverted or failed transaction, the validator still collects the transaction fees. So this upgrade to Kyrie 1.0
00:58:42.346 - 00:59:01.010, Speaker A: that now enables failed or reverted transactions is like a very critical step towards decentralization. So it can enable that validators can run these transactions without needing to know that they will actually be valid.
00:59:03.030 - 01:00:00.770, Speaker B: Yes. So maybe I'll just add that the big motivation in Cairo 1.0 and the intermediate representation which it compiles to, which is called Sierra, in addition to being a high level rust like language, is to be able to overcome the fact that we can't prove failed transactions. So let's say at one point your transaction asserts that your balance is greater than something. For Ethereum, that's not a problem, because the validators will say okay, valence not big enough, transaction fails, and other validators will sign on it if it was true. But for our roll up, it's something that the prover verifies he does not know to continue. If this assertion does not hold, he can either prove, or if the statement it false, not prove.
01:00:00.770 - 01:00:32.302, Speaker B: So the big idea in order to overcome this and have failed transactions inside the block and have them in a proven manner, was to make sure that all contracts compile to Cairo assembly, which does never fail. It never uses a self instruction. For example, it can never get to a point where the proverb says okay, in this particular execution, I just can't prove it. So that was the big motivation also.
01:00:32.356 - 01:01:04.406, Speaker A: For people to appreciate if the prover fails, it doesn't just fail. For that transaction, the entire batch fails, and you need to redo the entire batch without that invalid transaction. So it's a big step. Somebody asked in the comments, cairo 1.0 will be written in Cairo. The language is called Pyro 1.0 not to be confused, I think it'll be similar to Rust syntactically, but just that at a higher level, it looks more like python or rust rather than the current Cairo.
01:01:04.406 - 01:01:09.660, Speaker A: That looks more like assembly or like something much more at least.
01:01:10.190 - 01:01:34.674, Speaker B: But the idea that the low level Cairo assembly remains unchanged, so the virtual machine remains unchanged. So, for example, the work for implementing the virtual machine in rust is not affected by the transition for Cairo one. It's not like we're going to have to redo this effort or still gaining from it.
01:01:34.872 - 01:01:46.710, Speaker A: Yeah, that's a really good point to emphasize. Yeah, because the virtual machine basically takes the bytecode once it's been compiled down to bytecode, but that bytecode can be compiled from Kyrie 1.0, kyo 0.9, et cetera.
01:01:48.970 - 01:01:49.670, Speaker B: Cool.
01:01:49.820 - 01:02:22.210, Speaker A: I am really excited. I also think another thing with Kara 1.0 that it's been communicated, but it's underemphasized, is how much of the pleasure the new syntax will be. So there'll be for loops and while loops and integer data types, if I'm not mistaken. So you don't need to know what a field element is, or how to use field elements, or the safe math of using a field element and then having some sort of buffer overflow, all that will be solved.
01:02:23.030 - 01:02:38.710, Speaker B: I'm told that people who like rust are very excited by stuff like panics and ownership, so we're going to have those stuff if that excites you. But for the more regular people like myself, then I find loops more exciting.
01:02:39.850 - 01:03:28.830, Speaker A: Yeah, loops are great. I never want to complain for loops. I have to relearn all of my recursion when I learned the old Cairo. I do want to recommend to teams who are considering exploring Starknet to not wait for Cairo 1.0. Still tackle things with the current Cairo because you'll learn more about the tooling in the ecosystem and just how stark networks in general, and then just upgrade and go along as more and more things are released. Okay, great. Let me just double check if there are any questions from the audience that doesn't look like there are.
01:03:28.830 - 01:04:12.750, Speaker A: If anybody does have questions, you can dm me on Twitter. You can email ariel. We'll get to the right person in Starquare or in the community to answer your questions. I would like to thank Oleg for presenting summary dev indexes are a critical tool for all blockchains. As Oleg presented, if you're building on Starknet or considering that and you're worried about your indexer needs, we can definitely introduce you to the summary dev team, and also thank you to Ariel for that Starknet roadmap update. Things are really excited. I'm really excited for the future.
01:04:13.680 - 01:04:14.236, Speaker B: Great.
01:04:14.338 - 01:04:18.540, Speaker A: I think we'll end there. If I don't see any questions in the next few seconds.
01:04:20.160 - 01:04:20.668, Speaker B: No?
01:04:20.754 - 01:04:29.890, Speaker A: Okay. Ending the stream. We've ended the stream. Thanks.
