00:00:02.790 - 00:01:10.862, Speaker A: You know, I'm not sure if kind of closing remarks is the best title for this presentation. It kind of feels a bit grand. And really all I wanted to do is just talk about fraud proofs and data availability proofs, but here you go, fraud proofs and data availability proofs. So I guess to kind of set the stage for this technique. The idea here is that if you look at blockchain protocols like bitcoin, ethereum, and specifically full nodes in those protocols as they exist today, there are some protocol guarantees that get broken if, say more than 50% or 33% or 23.21% or whatever number of nodes stop following the protocol. So you might have 51% attacks, you might have finality failures and so forth, but there are a lot of properties that you still keep even if arbitrary 51% attacks or like arbitrary dishonesty of nodes is possible.
00:01:10.862 - 00:02:28.790, Speaker A: Right? One example of this property is the property that all the blocks in whatever chain you're going to accept as the main chain are going to be valid. And what this implies is that also people can't steal your money, people can't give themselves money, the coin supply can't get inflated, applications can't get randomly screwed around with, and a whole bunch of other things, right? Another guarantee you have is that the data in the chain that you accept is available. So this basically means that whatever chain you accept as the leading chain, well, if you want to download any particular piece of it, you will be able to. And this seems trivial in kind of non scalable blockchain land, but it's actually very nontrivial for some reasons that I'll go into. And what this guarantee basically gives you is it gives you the ability to first of all be able to ensure that the blockchain things are correct. And second, to be able to do things like generating new blocks, like creating new transactions, and just in general like figuring out what the hell is going on on the chain. So to get directly into things, let's start talking about fraud proofs.
00:02:28.790 - 00:03:44.782, Speaker A: So the setting here is basically, suppose a malicious miner, malicious proof of stake validator or whatever publishes a block, and we're going to assume all the block data is present. So there's some magic oracle that verifies that all of the data in the block, the full body of the block, has been published to the open Internet, and at least one honest node has it. But some of the block data may be invalid. And so what we're going to do here is we're going to design the state transition function. So the function that basically takes an existing state, processes all the transactions, outputs a new state in such a way that any specific invalidity, any specific error in the block can be proven to be an error to a light client by basically just providing a small subset of the, I mean, who here is familiar with how Merkel branches? So, okay, you know how Merkel branches work. And so basically, if you have a transaction, the transactions process them incorrectly, then all you need to do is just provide a few leaves that touch the transaction, that touch the accounts affected by the transaction, and you can prove that something invalid happened. Once someone's made a fraud proof, they can broadcast it.
00:03:44.782 - 00:04:29.142, Speaker A: And any client, upon receiving a fraud proof, can immediately verify that there's something wrong with the block. So with this technique, what you can do is you can basically create a system where clients can download all of the data to verify availability. But after they've downloaded all of the data, they don't need to process and verify all of the data themselves. Instead, every client might just randomly choose to process and verify some random 0.1% of the data. And if any client discovers that something somewhere is wrong, they can broadcast a fraud proof. And once you've broadcasted a fraud proof, that fraud proof can get traversed through the network.
00:04:29.142 - 00:05:06.662, Speaker A: Other clients can verify the fraud proof and they can verify something is wrong. And if you've downloaded the data and no one has sent you a fraud proof for a while, then this is evidence that the chain is correct. Now, under what assumptions does this break? Number one, it breaks if the number of honest nodes in the network is just extremely low. So in this case it's like under 1000. And so there's just data that no one ends up checking over. And the second condition under which it breaks is if network latency is extremely high. But notice that this does not break even if 90% of nodes on the network are malicious.
00:05:06.662 - 00:05:59.910, Speaker A: So we've really kind of cut security assumptions for verifying validity of data down. So here what we've done is we've basically reduced the amount of computation you need to do to verify that some data is valid. But what we haven't done is reduce the amount of data that you need to download. So data unavailability, right. So this is the big problem with probably the hardest problem in blockchain scaling. And the problem basically is suppose that a malicious miner or validator publishes a block, and instead of publishing the entire block with some mistakes in it, what the attacker is going to do is the attacker will publish a block where parts of the block are just not published. So the attacker will publish most of the Merkel tree, but not all of the Merkel tree.
00:05:59.910 - 00:06:40.280, Speaker A: And the parts that aren't published, well, they could be valid, they could be invalid. But both cases are attacks. So if we're going to think about kind of proving data availability, it also kind of mentally helps to separate out the two cases. So we can think about a model where we don't really care about any notion of validity. And if we do care about validity, well, we can kind of separately have a fraud proof mechanism for it. Let's just talk about a chain where all the chain does is it holds a bunch of data, and a valid block is a block that has data. The data could be anything, but the data has to have been published, it has to have been published to the network and be available.
00:06:40.280 - 00:07:56.320, Speaker A: And the kind of more, slightly more formal way to think about this is that for every chunk of data, at least one honest node has that chunk. And so if you want to download any particular chunk, then you can ask the network for it and get it if you want to. So basically, the problem is, how does a client verify availability without downloading all of the data themselves? Now, you cannot use fraud proofs for this, right? You can't just publish a message that says, hey guys, I think chunk zero is unavailable because as soon as you publish that claim, well, what a clever attacker can do is they can just not publish that chunk until they see your claim, and then they just immediately publish the chunk and they're like, hey, this guy's an idiot. Why is he claiming this data is unavailable? Here you go. And then the attacker could just keep doing this again and again. And basically eventually either people stop trusting these claims and the scheme breaks, or the participants get dossed and they have to download the entire data. So you can't solve data availability the same way that you can detect faults in data where all of the data is available.
00:07:56.320 - 00:08:31.306, Speaker A: So easy solution one, check everything for OFC data. This obviously takes OFC bandwidth. It's okay for existing blockchains, but there is plenty of contexts in which it's not okay. Context one, higher capacity chains. So that's Ethereum today, ethereum after the Istanbul hard fork. That'll reduce the call data gas cost from 68 to 16. The Craig Wright chain and sharded Ethereum, with its 1024 chains also plans very high.
00:08:31.306 - 00:09:52.038, Speaker A: Right now in the grand scheme of things, a few megabytes a second isn't very high. Like it's still within bounds for at least some home users in at least some context to be able to download everything sometimes, but it's definitely too high for most people to reliably download. So that's case one, case two, and this is a case which is relevant to existing non scalable blockchains, including pretty much everything that exists now. Quick verification of history, right? So suppose that you're syncing a chain for the first time, or you've been offline for a long time and you want to just get caught up with the history and you want to verify all of this stuff. So you want to establish this guarantee that the data is available and that the data is correct in sublinear time. So in sublinear computation complexity and in sublinear data complexity, now, right now, what you have is you have light clients, and light clients can find the correct head in very sublinear time, but only if you accept this trust assumption that says that the majority of the miners or the majority of the validators are good. Let us try to remove the assumption that the majority is good.
00:09:52.038 - 00:11:01.180, Speaker A: Can we still keep the guarantee? And it turns out that if you're willing to accept these assumptions about either network latency and at least some minimal number of honest nodes existing, yes, you can. So, light client protocol for verifying data availability. So suppose you have a block, and we're going to just take that block and we're going to Merkel hash it. And think of there being data at the bottom. Could be a few megabytes, could be higher, and it just gets Merkel hashed and you have a Merkel route. So one simple protocol for probabilistically checking availability is I, as a light client, am going to privately select 80 positions and I'm just going to ask the network for each one, and I'm only going to accept the block as available when all 80 of my queries get answered by someone. If only 70% of the queries get answered, then I'm just going to sit there, pretend the block is not there, and if an hour later there are the last ten queries get answered, then I'm going to accept the block an hour later.
00:11:01.180 - 00:11:55.580, Speaker A: So this is kind of similar to what happens already, where you only accept a block after you've downloaded the whole thing. But here, instead of trying to download the whole thing, we're just going to focus on a particular 80 positions. So the idea here is that this is not a good way to verify that the entire data is there, because there could be just one chunk that's missing, and if there's only one chunk that's missing. There's no way any of these clients are going to catch it. But what it does prove is it proves with a very high probability that more than 50% of the data is available right now. Notice that an attacker can trick specific clients. So what an attacker could do is when an attacker could, by default, publish nothing, and then just wait for these queries to come in and only publish specific chunks of data responding to the queries, and then stop at 49%.
00:11:55.580 - 00:13:08.686, Speaker A: But basically, if the attacker tries to do this, first of all, there is a bound on the number of clients the attacker can corrupt or trick, because eventually 80 chunks multiplied by a bunch of clients is just going to add up to half the data. And also, you can make it even harder to trick specific clients, basically by having all the clients make their queries with onion routing, and so you don't know which queries to respond to in order to kind of trick one specific client. So this is the, like, client protocol, right? Basically, just like check 80 random positions, and you're probabilistically verifying that at least half the data is there. Problem? Of course. What if the one position that the attacker has not published is an invalid transaction that gives themselves 91 bajillion coins? So, erasure coding. What we're going to do is we're going to low degree extend d, and we're going to low degree extend it to a larger d. So now any 50% of d suffices to, or any 50% of d prime suffices to reconstruct all of d prime.
00:13:08.686 - 00:14:35.598, Speaker A: And so verifying 50% availability of d prime suffices to verify 100% availability of d prime. Do people get this? Okay, so this is erasure coding, right? Now, there's one problem here, which is, what if the D prime is constructed incorrectly? So what if it's not a low degree extension? What if the data in some places is just like, totally random? What if at position five, the merkel tree doesn't even go down that many levels? What if one of the chunks has 34 bytes instead of 32? What if there's crazy stuff happening and r prime actually isn't a kind of faithful low degree extension? Or D Prime actually isn't a faithful low degree extension of D? So, by the way, r is numerical root of d, and r prime is numerical root of d prime. So you might think, well, no problem if r prime is constructed correctly. But if it's constructed incorrectly, what do we do? So one approach is to just bring back fraud proofs, and we'll make a naive fraud proof. And the naive fraud proof is pretty simple. Basically, if you as a full node, or at least as a node that is full with respect to that particular block. So you actually try to download more than 50% or all of D.
00:14:35.598 - 00:15:43.182, Speaker A: If you've only managed to download somewhere over 50% of it, but not the whole thing, then you can use that 50% to reconstruct all the data. And after you've reconstructed all the data, you can basically just make a big huge fraud proof that just shows, hey, if you take this data and reconstruct it and make a Merkel root of the reconstructed version, the Merkel root is not going to be the same as the Merkel root of the original data, right? So basically we're just making a big huge fraud proof containing all the data you have and verifying it just requires running this Entire computation, extending the data and proving that it's inconsistent. So this works. But the problem is this makes a big fraud proof. So in the case of Ethereum 2.0 crosslinks, for example, in the worst case, 1d prime can have a size of a gigabyte, and so you really don't want gigabyte long fraud proofs. So what do we do? So the solution in this paper from last year by myself and MUSTAfA Al Basam basically said we're going to have two Dimensional erasure codes.
00:15:43.182 - 00:16:33.906, Speaker A: So what we're going to do is instead of just extending d to d prime along one axis, we're going to make d be this kind of Little Square on the left, and then we're going to interpret D as being a Bivariate Polynomial. And we're going to low degree extend it this way, and then we're going to low extend and degree extend it that way. We're going to low degree extend it this way and degree extending this way, and low degree extending this way gives the same answer. And so we have this square. So then a light client, to check data availability is going to make basically 80 random queries into this structure. But the light client is also going to download the Merkel roots of every row and the merkle roots of every column, right? So a light client has to download O square root of n data. So in the case of like gigabytes, this is going to end up being about a few hundred kilobytes.
00:16:33.906 - 00:17:36.282, Speaker A: So a fraud proof, instead of including the entire data, just has to prove fraud in one row, or prove fraud in one column, or prove inconsistency between one position and one row and one position in one column. That's it, right? Because ultimately, if the data is broken, then what that means is that some row is broken or some column is broken. And so you just choose the specific row or column that's broken, and you prove it. So what this means is that the amount of data that a light client has to download goes up a bit, because a light client has to download these row roots and column roots. But the size of a fraud proof goes all the way down to just proving fraud of one row, right? So now fraud proofs go down to being under a megabyte, and we have a scheme that theoretically works. There are possible techniques for improving this with Starks. So I'll start with the scheme at the bottom.
00:17:36.282 - 00:18:43.840, Speaker A: The scheme at the bottom basically says, we're going to use fry. So we're going to use the ingredient in Starks. And what we're going to do is we're going to use this to basically give the same properties as the two degree scheme, the two dimensional scheme, but on top of simple one dimensional codes. Right? So, basically, if you have a D prime where that D prime has a bug in it, instead of providing the entire D prime, we're just going to do a bit of clever math, and we're going to basically use fry, use one of these kind of f of x minus minus y over big x minus x arguments that you might have heard about all of this morning. And we're going to prove that there exists some Output, which is the correct value of extending the parts of d prime that you have to some position. And, look, it doesn't match the thing that's in the hash tree. So, basically, you can just use Fry to prove an inconsistency, instead of proving an inconsistency by providing the entire data.
00:18:43.840 - 00:19:42.754, Speaker A: Another thing you can do, which is simpler conceptually and better, but it takes more work. And this may end up requiring ASICs to run effectively, is to just directly use a stark to verify correctness of the Merkel root. Right? So, basically, what we're going to prove is we're going to just take all of the hashes in the Merkel tree, line them up along a polynomial, and it'll be like p evaluated at powers of Omega and P at Omega to the two n all the way up to p at Omega to the four n minus one. We're just going to place the leaves there. And then we're going to basically make an argument to prove that these particular pieces of data represent a degree n or less than n polynomial. So it means it's an actual low degree extension. And then we're going to just verify every other leaf or node in the Merkel tree by just showing p of x is the hash of p of x squared and p of x squared times w.
00:19:42.754 - 00:20:20.382, Speaker A: So every parent is the hash of its two children. You can't shove this in a constraint directly because h is high degree. You need to break it up a bit. And then you prove that, you check that the root actually is r. Right? So the nice thing about this holy grail approach is it basically says, well now we don't even need fraud proofs. And if you don't need fraud proofs, then to check availability you actually don't even need a network latency assumption. All you need is basically this assumption that says well, there is at least some minimal number of honest clients somewhere in the network and that's it.
00:20:20.382 - 00:21:12.750, Speaker A: And that number is not 51%, it's not 5%, it's just a constant. So two possible ways to improve it. So what do we use this for? Right, so the first use case, as I mentioned, verifying existing chains. So this is a fully kind of layer two technique that could be used to improve the security properties of existing light clients. And basically what you do is you just connect your node, you receive a piece of data provided by someone that claims to be a Merkel root of the chain. And for antidos reasons, you could attach this to proof of work. If there's a chain you've already authenticated, you could also have the provider have a deposit in a smart contract so that deposit could be taken away if there's a fraud proof.
00:21:12.750 - 00:22:14.638, Speaker A: And then on this route you perform the data availability challenges, you wait for fraud proofs, and if the data availability challenges pass and you see no fraud proofs, then you accept the chain as valid. So you could theoretically come up with this scheme, just make it be this layer two thing. And just by having light clients do this, you can improve the security of syncing of existing blockchains. And by basically creating this mechanism that says, well, if there is a 51% attack and a light client syncs a chain, then the network basically will kind of collaboratively detect the fraud, pass the fraud around, and light clients by default are not going to accept this chain. So it makes 51% attacks kind of considerably less powerful. Use case two scalable chains, right? So basically in Ethereum 2.0, it's a sharded chain, and so there's way more data than most people can download individually.
00:22:14.638 - 00:23:45.690, Speaker A: And so you would just use fraud proofs and data availability proofs as this kind of additional check so that you can basically get security guarantees on data, even if whatever particular subcommittee was responsible for signing off on that data turns out to have been corrupted. Now use case layer three is some layer two, right? So basically, if you have some Dex plasma chain thing, and your Dex plasma chain thing requires users to verify data availability so that they can make challenges, which pretty much every plasma thing requires, then you can use this to check for data availability instead of users downloading the data themselves or relying on some trusted watchtower scheme. Yeah. So conclusion, data availability verification and fraud proofs, two nice ingredients. You can combine them together, you can quickly verify large amounts of data and computation indirectly. Right? So you can, given these security assumptions that are kind of much lighter than the security assumption of there not being a 51% attack, you can achieve this guarantee that says the data is available with cryptographically high probability and that the chain is or every piece of data is valid. Because if it wasn't valid, someone would have broadcasted a fraud proof.
00:23:45.690 - 00:24:18.760, Speaker A: And in all of these cases, succinct, zero knowledge proof, select starks can make things better and in many cases reduce the need for one of the security assumptions, the network latency one, so that you don't need to worry about network latency at all. And this is great. Thank you. Are there any questions? Yeah, no.
00:24:21.420 - 00:24:42.060, Speaker B: Should I go? So you said that you can use this type of data availability proof for plasma and for watchtowers. I think I disagree with this, because in plasma, if the data availability, if data has become unavailable and the state transition happens, the damage has already been done. So there's no advantage to you detecting.
00:24:42.220 - 00:25:06.196, Speaker A: So the idea would be that whenever you receive a plasma, so in a kind of normal client running plasma, what you would do is you would try to download the data, and if you fail to download the data, then you would exit. Right? So the scheme here would be when you see a new plasma block, you would run a data availability check. And if the data availability check fails.
00:25:06.308 - 00:25:09.752, Speaker B: Then instead of having download the full block to check that the data is available.
00:25:09.806 - 00:25:20.440, Speaker A: Yeah, okay. Exactly. Excuse.
00:25:22.080 - 00:26:37.696, Speaker C: So I'm not sure if it's a question, but I kept thinking through your presentation that when you say the data in terms of data availability, I assume that you mean the block data with the transactions containing it. Because what it makes me think about. So there's an assumption that somebody could watch the chain or watch something, and they can basically, in a sublinear time, figure out the fraud proofs, but I would state that it's not even possible in the systems like Ethereum, or I would even say bitcoin. The problem is the state, of course, because even if you know the composition of the blocks and the transactions, you would not be able to verify it unless you have the state which precedes this transaction. And in order to compute the state, you have to run everything from the beginning or do some other tricks like download it from the state first. And that's why, for example, when people designed the fast sync in Ethereum, they were thinking, oh, we're going to just fast sync to the snapshot and then probabilistically verify everything that happened before. But of course that didn't work because the state became so large that even probabilistic verification wasn't practical.
00:26:37.696 - 00:26:58.520, Speaker C: So what I'm saying is that the fraud proofs are something which probably will be very expensive to produce, and I'm basically challenging the assumption that there will be a lot of people actually doing fraud proofs at all. So I would say this is the very weak assumption.
00:26:59.020 - 00:28:31.130, Speaker A: So in an Ethereum context, this scheme, working on top of Ethereum as is, does rely on basically at least one honest node, having run an archive node, which also has a bit of extra software attached. Given that we're looking at raising gas limits, and then two more years happens and it goes down to the case where there are zero people running archive nodes, then yes, there is going to come a point where basically if people forget historical data too much, then they won't be able to create fraud proofs and they won't even be able to create these Merkel routes to verify data availability. So yeah, there is definitely a kind of implicit assumption that people keep storing data. And if this is broken, then yes, the scheme also fails. If people do have the data, then if you are an archive node, then producing the fraud proofs becomes easier. Also, another thing that you could do is you could have a scheme where basically for every block, basically a stateless client system, where for every block you can generate the block plus the transactions plus the witness, and you could potentially have every node agree to store 0.1% of witnesses or something similar.
00:28:31.130 - 00:29:08.850, Speaker A: So there's two ways to go about this. One is kind of what's looking like the current e two approach, which is basically kind of establishing very strict state size control over the entire system, and so not really having much state. So you can do fraud proofs with just the block and a tiny bit of extra data on. The second approach is to kind of do a bit of stuff on top to basically ensure that some nodes actually do have these witnesses. And potentially you could even just stick witnesses inside of this data, too. Thank you so much.
