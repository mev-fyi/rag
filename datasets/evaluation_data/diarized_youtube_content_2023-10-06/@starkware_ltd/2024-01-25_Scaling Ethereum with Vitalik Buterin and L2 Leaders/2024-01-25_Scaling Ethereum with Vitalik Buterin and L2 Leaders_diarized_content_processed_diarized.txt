00:00:00.250 - 00:00:00.800, Speaker A: You.
00:00:02.850 - 00:01:18.920, Speaker B: Okay? Yeah. So hello everyone, and hello to all of our wonderful l two crew and anyone who is currently watching this. It's good to see the greater Ethereum verse come together again. As I yes, we've done this before in person at Dev Connect and we're doing this one online. So looking forward to seeing these kinds of amazing cross l two chats become a regular thing. We'll start off with just a fairly simple question to any team that wants to answer it for themselves, which is just from a technical perspective, whether it's decentralization or functionality or whatever else. What milestones are many of you particularly excited about your project reaching in 2024? Anyone start?
00:01:21.370 - 00:01:23.206, Speaker A: I can start if you want.
00:01:23.388 - 00:01:24.120, Speaker B: Perfect.
00:01:27.130 - 00:02:07.122, Speaker A: Or main battle here for decentralization is forced transactions. It's a little bit disgusting in my side that we have this designet and it could be enabled from the first day and it's still not enabled. It's mainly for security, many, much for security reasons. I mean, the problem of forced transactions. So forced transactions can be anything that can be a forced transaction. So this opens very much the vector space. But the good news is that what we are going to do in the next version is we are going to enable that, but we are going to limit the force transactions just to withdraw transactions.
00:02:07.122 - 00:02:55.670, Speaker A: I mean this is just not the perfect thing, but this reduce a lot the vector space for that. So this allows us to enable that and with that, so force transactions. And here is maybe a philosophical discussion. I mean, it's not a perfect thing because I think the goal of these decentralized systems is not to build, it's not only censorship resistance, I would say like more the concept of universal systems systems that anybody can use it and with force transactions, actually nobody can steal your funds, you can always recover that. But you effectively, what you can do is you have the right to kick somebody out of the network. And this is not really what I like the best. So I think it's better that a fully centralized system.
00:02:55.670 - 00:03:13.340, Speaker A: But yeah, this is the main step that we are fighting for and let's see if we can enable that as soon as possible. Want to promise nothing at this point, but yeah, it's as soon as possible. It's just a matter of getting confidence on that.
00:03:16.590 - 00:03:18.380, Speaker C: Okay, I'm happy to go next.
00:03:18.750 - 00:03:19.500, Speaker B: Sure.
00:03:20.110 - 00:04:30.322, Speaker C: What I'm really excited about this year is that this is going to be the year where DZksync decentralizes. And I believe that decentralization of l two s is a really big topic where we still have a lot of centralized components with centralized sequencers, centralized provers, security councils are not in the perfect shape and upgradability. All of those things, we really need to polish them to bring the promise of actually scaling Ethereum with all of its values in the l two space. And what I'm really excited in the ZK syncs decentralization roadmap is that we'll have many chains powered by ZK sync. We call them hyperchains. They should be connected in the hyperbridging network, which I believe eventually is something that we will all converge in the l two space, that the bridgeability between all l two s have to be completely seamless and trustless, and we just want to pioneer that with the Zk technology. And I think that ZK is really a crucial component there.
00:04:30.322 - 00:05:11.950, Speaker C: And I'm really excited that we can accomplish that with decentralizing, not at the level of a single sequencer, and then doing something like shared sequencing for multiple chains. That will mean that all of them are using the same providers and kind of like getting the power together, but actually truly decentralizing. That many different teams can run many different versions of consensus of different protocols to aggregate proofs, et cetera. And all of them still inherit full security and trustless properties of ethereum and censorship resistance down from layer one. So this is the design space that we're super excited.
00:05:12.630 - 00:05:15.860, Speaker B: Got it. Anyone want to go next?
00:05:18.070 - 00:06:06.530, Speaker D: I can go next, sure. I think for us, and for me personally, what excites me the most is the steps that we're taking towards removing the multi sig or like just having delayed upgradability without having any protocol capacity of instant upgradability. And we just announced the first step towards it. We announced an SGX based multi prover that is going to go live on the main net soon, and I'm not sure if we're going to be able to actually remove the multi sig this year. It's still quite risky, but we're taking steps towards it. And the multiproover is a good first step. And a side effect of actually having a multiproover is that we can easily decentralize, and that's also going to happen.
00:06:06.530 - 00:06:17.080, Speaker D: We're planning for this to happen later this year, so we're working towards decentralization, but it's going to be a side effect of the fact that we're trying to improve the security.
00:06:18.890 - 00:06:49.360, Speaker B: Got it. Maybe two people who have not gone yet. Just a question in a completely different direction. Right. So I think we're excited about L2 is because L2s scale Ethereum and scaling. Ethereum reduces fees and enables other applications. So what are some particular applications that you guys are excited about becoming possible that haven't been possible on Ethereum so far?
00:06:52.210 - 00:06:54.834, Speaker E: Yeah. Happy to maybe jump in.
00:06:54.872 - 00:06:55.460, Speaker B: Sure.
00:06:56.870 - 00:08:27.390, Speaker E: Yeah, I think maybe I can even loop this back into the previous question and get two in one. I think that kind of, one of the things I think that we're seeing a bunch of is sort of a lot of innovation in blockchain gaming, which has sort of historically been an area that's been quite bottlenecked on capacity, on functionality, in terms of kind of actually being able to have real experiences, and the combination of kind of a number of things that we're working on and excited about in the next year, one of which is sort of arbitram orbit chains, and the proliferation of essentially kind of Ethereum dap chains and kind of the concept of sort of dedicated capacity, certainly 4844. And also sort of other interesting exploration in Alta in order to lower costs enough in order to sort of actually have these things be feasible and then work we're doing on arbitram stylus to enable wasm execution and kind of get computational requirements down and allow kind of game developers to kind of come in and use languages that they're more used to and kind of depend on libraries that might not already exist in Ethereum because they're not that useful for kind of existing applications, and sort of all of those things coming together to form sort of a perfect mix of what's needed to enable new functionality.
00:08:29.730 - 00:08:31.374, Speaker B: Amazing. Yeah, go ahead.
00:08:31.412 - 00:09:17.738, Speaker F: Yeah, I think one of the things that we're trying to kind of enable is privacy, so that you can easily have a kind of privacy preserving smart contract and don't have to expose your balances and all that. So that's one of the things. And more broadly than privacy is just like off chain transactions, meaning where you can prove using ZKP that the transaction was executed correctly, and then just send the ZKP to the network so that the network can verify it. And that enables. Well, this is needed for privacy because you need to prove local state transitions for privacy purposes, but also may enable scalability where you can execute some very complex code on the client side, prove it, and then send it to the network. And then the network just needs to verify a short proof that it worked correctly.
00:09:17.914 - 00:09:47.080, Speaker B: Yeah, I'm actually Bob, and I think since a lot of people listening to this probably know about polygon as basically a set of EZM clone chains, but you're working on some pretty different stuff around privacy. So maybe just introduce what is the thing that you're working on, what kinds of technology it's being used, how it fits into the Polygon universe in general.
00:09:48.090 - 00:10:38.534, Speaker F: Yeah. So polygonmiden is one of the kind of components of polygon overall. I think we announced this aggregation layer yesterday and this is something that will be one of the chains that plugs into this aggregation layer. But the nice thing about the aggregation layer is that it doesn't have to be all EDM chains. It could be different chains that use different execution environments and have different properties. So one of the things that we're trying to solve with polygon mining specifically is we're using a different state model from Ethereum specifically to enable privacy as one of the things, but also make final transaction execution more naturally workable. Enable this off chain transaction execution as I mentioned, and a bunch of other things and the way we do it know, as I mentioned, but changing how the state works.
00:10:38.534 - 00:11:18.470, Speaker F: So we're doing a hybrid between UTxo and account based state model. So you kind of have the benefits of both accounts and Utxos where to make transactions, for example to transfer assets between two accounts. You actually have two transactions where one transaction creates a Utxo and another transaction consumes a UTxo. And that allows you to decouple kind of like updates to one account from updates to another account. And that makes a local state transition locally approvable. So this is like changing the state model is one of the bigger things. And that enables, as I mentioned, both privacy and other interesting things.
00:11:18.470 - 00:11:35.020, Speaker F: And with this aggregation layer we will be able to prove execution of polygonmiden, put it to the aggregation layer. Then this will get rolled up with EVM based chains together in a single proof. And that proof ZK proof will go to Ethereum. So it's pretty exciting concept of how this all can come together.
00:11:37.230 - 00:11:48.000, Speaker B: Amazing. Thank you Ellie. I feel like we haven't heard from you yet. What's exciting in Starquare land and what applications do we want to see?
00:11:49.730 - 00:12:57.490, Speaker G: Yeah, so I think the thing that I'm most eagerly awaiting is the good ux. That would come from applications building on Starknet that are using the native account abstraction that is common to all Starknet addresses. Which would mean that users will get a much better ux that will feel. Well, it already feels very web two like and familiar. Those who install things like the Braavos or argent wallets already feel this. So the main thing I think will be that the UX will look much nicer for users on Starknet, which will enable a lot more adoption by others who are not right now crypto savvy. And this will be coupled with the enhanced performance in terms of throughput and low costs.
00:12:57.490 - 00:13:29.390, Speaker G: Also, thanks a lot to 48 four four, which is soon coming. And congrats on yet another delivery that is going to impact us all in a positive way. So the main thing I would say is applications making cool new use of account abstraction and better Ux enabling totally new things. Now, what these things are going to be, I don't know, but I'm eagerly awaiting influence, Nakubo and the other very cool applications who are at the vanguard of this movement.
00:13:30.770 - 00:13:35.680, Speaker B: Yeah. Nicholas, what's your take on both of these?
00:13:36.130 - 00:14:17.758, Speaker F: Yeah, so for ZK rollabs, there's still the compression that we're going to add. And basically it's a fight for cost and transaction prices, and it's not a fight that is over. So compression allows to divide the price by five. Usually that's pretty good just for cold data, 48 four is incredibly important. I think it's not going to be the end of a story. I think this data is going to be used very quickly, so we will need an extended number of blobs or more solutions, something like in a year or two. So I think we should anticipate this and once we will get there, which is basically very low transaction prices, a lot of use cases will open.
00:14:17.758 - 00:14:45.990, Speaker F: I totally agree on the contestion and on UX, I think it's absolutely key. And UX is actually linked to security as well. Sometimes in order to save, you either go too fast, sometimes you could actually, I've seen some patterns, like you authorize access and you do your transaction and then you remove the access. A lot of things are linked to the transaction prices and would be quite important in terms of UX and security, in my opinion.
00:14:46.410 - 00:15:12.270, Speaker B: Got it. Thank you. Yeah. So let's move on to another theme that I think is really important, which is customization versus standardization. Right. So I think the various L2s here have a lot of things in common. In not all, but most cases, the EVM, in many forms of cryptography, a connection to Ethereum, potentially other things.
00:15:12.270 - 00:15:43.100, Speaker B: But at the same time, all of you guys are exploring different ways to extend the functionality that you're providing to your users. And so how do you see the balance between extension and standardization being what are some things that are particularly important to standardize, and what are some areas where you're personally excited about your project? Uniquely innovating on.
00:15:49.480 - 00:16:15.150, Speaker F: I can start on this. Oh, ok. The way we see it, we would like to be as standardized as possible. That's as much as we can. So we would really like the EVM to continue to evolve, not to be stuck in the logic where okay, actually everybody diverges in different way, and if we don't have to, we will follow the protocol exactly as it is.
00:16:18.480 - 00:17:42.596, Speaker C: I agree with this, but I also think that it's our job to innovate and try things out that will then later be implementable for Ethereum itself and for all L2s, because it's too risky to do all of the experiments on layer one because of the massive responsibility of the basis chain. We are, on the other hand are much more free to try things out, and if they don't work, then they don't work right? So one thing ZK sync has done was native account abstraction. I think we are the only EvM l two that has native account abstraction, and it really leads to much better ux, where you can pay in different tokens. In the end, we will convert all of these fees into eth automatically, programmatically, so that we can pay for the forecast for the block space on Ethereum, but the end user should not be bothered with that. And it also enables things like multicol. And this leads to much better ux, to much better security, because all of a sudden you can construct protocols that make it very visible, very clear to the users what are going to be the impacts of their action. Like when I'm signing a transaction, I should not be worried that this transaction can steal all of my funds from the wallet.
00:17:42.596 - 00:18:18.310, Speaker C: And the only way to do it is to go away from the infinite approvals and things like this and actually create strong invariance, which can be enforced by account obstruction, which has to be implemented natively. So that's one example. And we'll see more examples where I think we should innovate. And then before we innovate, we should obviously coordinate. And we have the really nice coordination channels, we have the roll up improvement proposal group, we have the chats. I think we should be using them really heavily, but still be courageous to innovate, try new things.
00:18:19.880 - 00:18:20.870, Speaker E: Got it.
00:18:22.600 - 00:19:07.750, Speaker D: I think for us, our goal is first and foremost to maximize compatibility with Ethereum. So we want to be as compatible with EVM as possible and with Ethereum code base in general. And then from there, there's nothing stopping us from adding features on top of it that Ethereum or EVM doesn't have. So as long as we support the standard features that EVM has, we can extend them and add features that EVM doesn't have. But for now, we're focusing on compatibility with EVM and Ethereum. And then once we achieve that, we're going to move on to experimentation and adding things that EVM doesn't have or possibly isn't considering. Ethereum isn't considering adding for the foreseeable future.
00:19:08.200 - 00:19:23.160, Speaker B: Yeah. One specific thing to zoom in on. The state tree hash function. Right. So for the Zk roll ups here, what are you guys using? Like, are you cloning vl one or do you have some Zk friendly thing at the moment?
00:19:23.310 - 00:19:24.868, Speaker D: We use Poseidon.
00:19:25.044 - 00:19:29.156, Speaker B: Poseidon, okay. And linear Mimsi.
00:19:29.348 - 00:19:31.880, Speaker F: Mimsi, mimsi with a complicated logic.
00:19:35.420 - 00:19:38.520, Speaker A: We are using Poseidon too. With gold.
00:19:39.380 - 00:19:40.080, Speaker B: Okay.
00:19:40.230 - 00:19:41.600, Speaker F: We're using rescue.
00:19:42.420 - 00:20:30.560, Speaker B: Okay, cool. Yeah, so I think one interesting question here is that the ethereum l one is currently actively planning to move to vertical trees because we care about reducing witness sizes. And I think one question here that I wonder if it's worth thinking through is do we think it's the goal for both l one and every l two to eventually converge to one particular sonarc friendly state tree design? Or might you see legitimate reasons for different systems to diverge on this? In the long term?
00:20:32.820 - 00:21:08.748, Speaker A: I think it would be good to have the same tree across. I mean, this will simplify a lot of things on that. I'm not sure if this should be Patricia try on. I mean, maybe Berkeley trees is an option there. Here it's important to mention that the proving systems, they are evolving a lot. And things that were doing something that was not Poseidon or some hash friendly function at some point was like impossible or difficult to think about or hard to do. But every time we see that more things are possible.
00:21:08.748 - 00:21:49.960, Speaker A: So thinking currently, for example, thinking in a Virgo trees, that's using a good prime field, even. It's a big one, but it's a good prime field. It doesn't look like a bad option. I mean, there are options. And I think here that the evolution of the zero knowledge can change a little bit on what's the best thing at this point, I would say that most of the projects that choose something that to build, I mean, it's something that works. But maybe now we can start thinking about being a little bit more flexible here and unifying the tree. The state tree.
00:21:50.460 - 00:21:50.776, Speaker B: Yeah.
00:21:50.798 - 00:23:11.280, Speaker G: I want to say on this that we will always be using the most efficient proving technology that there is, especially on the scaling side and everything points towards the very small fields. So to that know as we do with the KZG blobs, we can use them and we adopt. But to the extent that standards will also support a number of fields and especially small fields, I think that would be far better from the point of use of scaling. Just because the big bottleneck is the prover and the prover, you get much more efficiency over small fields. So be it vertical tree or something else like going with a 381 bit field or even a 256 bit field will be much less efficient mathematically and engineering than using the small field. Now we'll work with, as we were working with KPG, we'll work with anything that is a standard, but the standard could be open to things that also support small fields. You'll just get very much better efficiency by two to three orders of magnitude.
00:23:12.180 - 00:24:16.330, Speaker B: Yeah, so with these ZK constructions in general, I mean, I remember back in the good old days when I was visiting Ule up in Haifa and you were explaining a lot of this stuff to me for the first time. There were basically a couple of ZK protocols out there. But now we have all of these distinctions of like is it KZG, is it IPA based, is it stark, are we using Halo? Are the stark's Goldilocks field, are they binary field? Are we doing some lookup singularity thing? So do you guys see this space stabilizing anytime soon? Do you think five years from now at least the sane engineers among us will be able to look each other in the eye and agree that we all agree that this thing is best? Or do you think we'll continue having ten different dueling constructions for a long time?
00:24:17.340 - 00:25:35.280, Speaker G: I think we'll have a lot of dueling constructions, but I do think that along certain parameters, especially if you look at numeric things, stuff will dominate. So it's going to be very hard to get a single proof with smaller bite size than a gross 16, right? So maybe it could go down by factor two or something, but it's very close to optimal. I think that in terms of proving time, some construction that uses either univariate or very small variate fields over, sorry, polynomials over very small field will probably have fastest proving time. Let's see what would be a good analogy. If you think about sorting algorithms, right? There is no end to there will always be more sorting algorithms, but there is a small number of very popular ones. None of them is the very best along every dimension, but there is a very small number of them, and they're well known trade offs. So I think, I don't know if in five years, maybe in 20 years will be something like that.
00:25:35.280 - 00:25:50.484, Speaker G: Where, yeah, there's always ongoing research about all kinds of problems, but along several important parameters, there'll be sort of consensus as to the tools at hand that work best.
00:25:50.682 - 00:25:53.664, Speaker B: Okay, any other. Yeah, go ahead, Bob.
00:25:53.792 - 00:26:43.700, Speaker F: Yeah, I think probably within five years is the right time frame, but not because people will stop innovating, but because I think making frequent changes or significant changes will matter less. I think we're almost on the verge where proving is fast enough or good enough for most purposes, where it doesn't make sense to change it every year or every two years. I think from a cost perspective, we're basically there where, at least in the blockchain context, the proving costs are almost negligible compared to all other costs. Like from latency standpoint, we still have some ways to go and things like that, but I think within five years, we'll get to the point where people will say, okay, this proving system is good enough. I don't need to change it next year, because it doesn't really make a material difference to the overall system design. Obviously, people will keep innovating and maybe we'll change things more slowly. But I do think we're kind of almost on the Verge where the proving cost and speed doesn't matter anymore.
00:26:44.200 - 00:27:21.660, Speaker C: I agree with Paul, and I just want to add that I believe it's really important to standardize on the vertical tree design that is compatible with ZK efficiency and that we can all then embrace and make it a lot easier to build type one ZK vms that we can like. Then you can use all of the l two projects as basically light clients for main one for layer one. And yes, on the efficiency side, we are at Ziki sync. We are using shuttle f six. We're not using Poseidon. We are not using Mansi. We're not using any algebraic hash functions.
00:27:21.660 - 00:27:36.810, Speaker C: Because even though shutter five six is slightly more expensive, we're currently the lowest cost roll of the cheapest ZkVM on Ethereum. So it doesn't really matter because the proving is not good.
00:27:38.060 - 00:28:13.830, Speaker B: Okay, so I think just zooming out a bit from some of the math, just for the readers who are a bit overwhelmed by the weeds here, it sounds like lots of amazing tech is still rapidly ongoing, but we value standardization. But it'll happen over time. As the years pass by and as we flip over more cards and discover what technologies are waiting, we'll start to converge on being able to standardize more and more.
00:28:15.000 - 00:28:18.070, Speaker G: I haven't heard anything from Kelvin, and I'd like to hear.
00:28:21.080 - 00:28:31.210, Speaker H: Yes, the optimism. Zk. No, I'm kidding. I don't know. I got to be fed a question to answer.
00:28:35.340 - 00:29:00.160, Speaker B: Actually. How about we'll go into probably one of your favorite topics and something that's, I think, probably of interest to lots of others in the l two ecosystem as well, which is, how do you see l two's role in funding common infrastructure across the ethereum ecosystem?
00:29:01.960 - 00:29:28.140, Speaker H: Oh, boy. I would say. I think we have to. This is a personal opinion, but I think this is one of the big things that's missing from l one. Something that it's hard to implement on l one. Obviously, it's a very challenging problem. That doesn't, in my opinion, doesn't mean it's not missing.
00:29:28.140 - 00:29:39.970, Speaker H: And ultimately, I think that it is sort of the l two responsibility to fund these things, because nobody else is right. That's sort of the fundamental problem.
00:29:44.980 - 00:29:48.450, Speaker B: Anyone else wants to give their thoughts on that.
00:29:49.140 - 00:29:52.210, Speaker F: I just agree with what Kevin said.
00:29:52.660 - 00:29:53.410, Speaker B: Okay.
00:29:55.460 - 00:30:42.580, Speaker E: Definitely agree. Also, I think it's interesting in that it's a very hard problem, and I think that trying to figure out sort of experimentation and sort of resistance to capture is really hard. It's very cool watching what the optimism collective has been doing there, just because I know there's iteration in that. It's not an easy one to crack, but it's one that's extremely high value. And in general, kind of like, I think, at least from my perspective, the thing that's sort of most valuable now is sort of exploring a heterogeneous set of kind of different possible mechanisms, just because how hard it is to actually know how these things will play out and evolve in practice.
00:30:44.120 - 00:31:21.920, Speaker H: Yeah, I think maybe even the mental model on my end is that it's not just necessary, it's in a lot of ways, fundamental to what we're trying to achieve. Right. We talk a lot about the technology, but if we just build this technology where the only people that can fund it are people with an enormous amount of capital, then we've sort of just created the same system again. Right. If we can't fund our own infrastructure in a communal manner, then how can we really run communal infrastructure? So, yeah, I see it as fundamental to the goal.
00:31:22.820 - 00:32:02.190, Speaker B: Okay, maybe a different kind of question, but still in a similar spirit. Right. So this is the l two piece panel, and ideally, we want to go a step beyond the sort of cold piece of not having Twitter wars with each other. And also have a warm piece of lots of amazing collaborations. So maybe anyone interested, maybe just share a concrete story of what is a place on which you or your team have collaborated with someone else on this call and gotten a really good outcome out of that.
00:32:03.920 - 00:32:14.530, Speaker G: I'm not going to share details yet, but we have a very cool collaboration with Polygon Zero. And I'll say no more.
00:32:15.060 - 00:32:15.810, Speaker B: Okay.
00:32:17.540 - 00:32:46.708, Speaker A: I can say some public things. For example, we are collaborating, for example, with Nier. We are building Ziggy wasm together. I mean, they're using all their imitation system and approving system that we have, and here we are collaborating together. And especially all the proving systems and the language and all the. Is a good example. I mean, when we're building the CKVM, building CKVM is mainly building tooling and building the tooling for building the CKVM.
00:32:46.708 - 00:33:33.530, Speaker A: So all this tooling can be used for many other projects and many other things. And what we are trying in Polygon is try to open and try to open this tooling so that the community can use, improve, get better, even out, even. It's in your own interest, but in interest of the community to having this pooling as extended as possible. And I think this is the spirit of the collaboration. I mean, you are taking a lot from the community, you are giving a lot to the community, or you are giving everything to the community. And this is the way to really progress and really go fast and innovate and getting results and getting adoption faster on this.
00:33:36.620 - 00:34:09.460, Speaker E: There was a great effort a number of months ago that was led by Joav Weiss and pulled in, I think, a good number of the l two teams to enable this interfaith send raw transaction conditional, which is sort of a key enabler of a good 43 37 experience on l two s, which was awesome and definitely an area that benefited a lot from standardization in that we want to have all the wallets work on all the chains. And I think that it was a really valuable movement.
00:34:12.680 - 00:34:57.892, Speaker G: I wonder if we could invite Abdel on the show because collaboration stories, like probably 99% of the collaboration stories of Starknet with other cool projects originated in his mind or in discussions he had. So I don't know if he can come on and share a few cool collaborative stories. I don't know if he's. Yeah, I know he's lurking in the mean one of. One of the areas of. Yeah, Abdel, can you hear us? Can you come online and share some stories? There we go.
00:34:57.946 - 00:34:58.550, Speaker D: Okay.
00:34:59.880 - 00:35:12.890, Speaker I: I was not too far. Yeah, briefly. Hi. Hello, everyone. Yeah, first of all. Do you hear me?
00:35:13.980 - 00:35:15.130, Speaker F: Yes, we do.
00:35:15.660 - 00:36:05.732, Speaker I: Okay, so yeah, first of all, I really like to collaborate with everyone. So I welcome everyone here to reach out to me to do some kind of collaboration. I wish to do more collaboration with other tools, but in general we do a lot of collaboration also with other DA layer like Celestia. We also did some work with near, but it's not directly for the public stocknet, but more for app chains and layer threes to be able to support multiple DA layers and so on. By the way, I want to shoot out optimism because we use proxyd in production. So proxy is a tool from optimism to do load balancing on RPC requests and we use it in production and it helps us to improve the liveness of the network. So this is also a way of collaborating using other people, tooling, et cetera.
00:36:05.732 - 00:36:35.888, Speaker I: And we want to contribute to it, contribute back and eventually discuss about making it a standalone product and why not adding compatibility with non EVM L tools like Starknet and so on. So yeah, generally speaking, I love to collaborate with everyone and I wish we could do that more because I agree it's very important. As Nicola said, standardization is also very important. And even if stocknet is not even compatible, there are many other aspects other than the execution engine that we can standardize and collaborate.
00:36:36.064 - 00:37:25.700, Speaker B: Yeah, perfect. Yeah. One other, I think, specific standardization topic that's worth kind of digging in just a little bit. Account abstraction, right? So it's becoming a bigger and bigger topic. We're starting to see more and more of these AA wallets, and especially as we're seeing different native AA implementations, we're starting to see different vms become available in some cases. How are you guys thinking through making it friendly to account abstraction while it's being able to deploy on all of the L2s, including yours simultaneously.
00:37:28.760 - 00:37:37.130, Speaker A: As a fully compatible ZKBM, we are all in 4367, we are growing together here.
00:37:38.700 - 00:37:40.010, Speaker D: Same for us.
00:37:41.980 - 00:37:50.876, Speaker B: I'd be interested to hear from someone who is not in the sort of uncompromisingly 100% EVM camp for us.
00:37:50.898 - 00:38:10.480, Speaker C: As I said, we support native account obstruction, but we follow the conventions of 43 37. So it's compatible, but just can do more because you can natively execute code. Like for example metamask wallets can enjoy count obstruction, not only the wallets that are smart.
00:38:12.980 - 00:38:16.790, Speaker G: Yeah, I would answer, but Abdela is going to do such a better job.
00:38:18.280 - 00:39:09.780, Speaker I: So a few examples. So on the gaming vertical that is very strong on Starknet we have some builders who are using session keys because of course, if you have an on chain game, you don't want to sign each individual transactions. So they implemented some session key mechanism where you can use some session keys for the duration of the game. But of course it's limited to a specific context, like it can interact only with a specific contract, only with a specific selector and so on. That's one example. Another example is of course paymaster and we will extend the native account subtraction to also have non subtraction that will enable to send multiple transactions in parallel without having to execute them sequentially. Another aspect we start to explore is social finance wallets and social recovery wallets that can be very powerful with native account abstraction.
00:39:09.780 - 00:39:26.640, Speaker I: Yeah, and a very cool example is cartridge. They implemented a wallet that you can an account that you can control by unlocking your face id on your iPhone. So it's totally seedless. You don't have any seed phrase, so you just unlock your face id and you interact with your smart contract.
00:39:26.820 - 00:39:31.340, Speaker B: Oh, interesting. I mean, in that case, where are the underlying keys stored.
00:39:33.120 - 00:39:42.210, Speaker I: In the secure element of the phone, basically. Yeah. It's using the curve on the iPhone. The r one logic in Cairo. Exactly.
00:39:43.460 - 00:39:44.656, Speaker B: Okay, amazing.
00:39:44.838 - 00:39:58.244, Speaker C: We have a similar example on ZK sync with clay where they use the pass keys for basically just like sending a link which anyone can click on and on board in one click just using your face id.
00:39:58.282 - 00:40:00.790, Speaker I: And you also have paymaster examples on the.
00:40:03.640 - 00:41:41.830, Speaker B: Yeah, this is lovely. I feel like we should be having some account abstraction wallet peace calls too at some point soon. And I know Blockto did a good job of doing an event at ECC, and I hope we're going to start seeing more and more of those. One of the kind of l two Twitter war topics that we've seen over the past couple of months is people arguing what isn't isn't a real roll up or what isn't isn't a L2, particularly with an eye to data availability strategies. And I feel like I've tried to kind of keep the peace on this because my view was that different strategies just make sense for different applications, and $10 million DFI does not need the exact same quality of data availability as some relatively tiny game. But from, I guess, each of your perspectives, I know actually many people here are increasingly offering both a roll up and some kind of offering with an on chain data availability strategy or off chain data. So how do you guys see the distinction between different data availability approaches, what kinds of applications each ones are appropriate to, and how you expect that to evolve in your ecosystem in the future.
00:41:43.080 - 00:42:21.312, Speaker F: So when it comes to data availability, I'm always more comfortable when it goes to basically ethereum, because it makes the security analysis much simpler. There's a single dependency, you don't have multiple dependencies with links between the dependencies and so on. So I think it's always simpler. I totally agree as well that there are some use cases where it's absolutely acceptable to have lower security requirements. And I think there's another category which is actually we can accept to lose the data. The famous example is everything related to social network. We're not going to put on chain all the pictures of all the world.
00:42:21.312 - 00:43:04.200, Speaker F: It's just not possible. But it's kind of interesting, because here it's data that you can accept to lose if something happens, okay, it's lost. We've got the occurrence, we've got some stuff on chain. And this separation between, there's a part on chain that is, we're sure that we're not going to lose it, and another part that we can accept to lose, it's fine. It's an intermediary step between a validium where, okay, actually very likely we'll not lose it, but you have just a more complicated security analysis to do. So there are a little bit those three categories, in my opinion. I think people tend to think of this as two polar opposites, like you have on chain and off chain.
00:43:04.200 - 00:43:47.612, Speaker F: But I think there are a lot of interesting models where things could be in between as well. Like you don't have to put all the data on chain, or some things could go on one data, like l one. And some things could go to some other DA layer. For example, a good example of this, like in polygon mining, for example, we have this distinction between accounts and notes. And you could say accounts are on l one, but notes could go somewhere else. And what this means is whatever is in your account is secured at the same level as Ethereum, but whatever is in flight has a slightly different security guarantee. So an inflight transaction may have a lower security guarantee, but usually don't transfer your entire account worth at the same time.
00:43:47.612 - 00:44:20.410, Speaker F: So that's just one interesting way. I know there's other designs, like volition, where you can have accounts live. Sometimes users can move in the same account from one data availability model to another. I think adamantium that starkware guys published at some point where it could automatically switch from one model to another. So I think there is actually a very interesting space in between those polar opposites where you have some data sometimes in one place and another times in another place, and the user decides where the data goes.
00:44:22.300 - 00:45:24.316, Speaker C: I agree. We will all converge around these hybrid models where every chain will not be strictly a roll up or a validium, every ZK chain, and I think everything is going to be ZK in the future. But we will all have volitions and even deeper things where users host part of their data for certain accounts. But there are clear use cases just for roll ups. Obviously you want to host most of your net worth in a roll up because you want to derive full security from Ethereum without compromises. I think that's going to be the case for not just most of the value on layer one and on L2s are held by whales, by power users who are smaller in numbers, but they have a lot of value there. You think of all the liquidity, provider arbitrage, market makers, they hold their value in something that they absolutely need to rely on.
00:45:24.316 - 00:46:08.200, Speaker C: So security is part of ux if you want. So they will certainly be on the roll up. And if you think about the enterprises and institutions and banks, most of them will clearly prefer a validium where they fully control the data. And that's just the reality. I don't like it, and I hope we will eventually build much better tools purely on chain that people own and fully control. But we will have a continuum of the systems and the banks and institutions will have their assets issued in private validiums where the privacy is provided out of the box, but it's still fully interoperable with the rest of Ethereum ecosystem.
00:46:08.880 - 00:47:23.060, Speaker D: From my perspective, I think that for general purpose protocols, it just makes sense to be a roll up, just purely because if you have a lot of apps deploying on you, et cetera, minimizing third party dependencies just makes sense because let's say. Whereas if you're an app specific chain, then I guess it depends on a specific use case. As you said, if the chain is securing billions of dollars, then I would say it's probably best if you continue functioning as a roll up. Whereas if it's like a gaming chain or something that has less value in it and is more towards geared towards storing data, or just like making certain state transitions, it just makes more sense to use a validium or an optimium in case of an optimistic roll up, because for the vast majority of that data, you don't need the full security of Ethereum. It's perfectly fine to compromise a bit. What the trade off is that you get significantly lower costs.
00:47:25.160 - 00:48:13.010, Speaker E: Completely agreed there. And I think to me it's a matter of essentially it's not going to be practical for a while at least. To have everything be a roll up and kind of fundamentally you need to compare to sort of what the next best option is. And I think that there are a lot of DA approaches coming along that certainly, like everyone has said, would not want to have be kind of the main kind of primary ecosystem roll up chains, but sort of fundamentally do enable use cases that don't exist otherwise. Just to sort of. I think there's another part of your question that we kind of went less into one, just a minor disagreement. I think you said that over the last few months there's been arguing over what's it held to, I would say over the last five years.
00:48:13.010 - 00:49:03.860, Speaker E: It's this sort of definitional problem that's plagued us for a long time. And in my mind on that front, the thing that I'd really love to continue to see more of is sort of less kind of. It's very appealing to sort of like have these sort of terminology buckets and kind of like you're an l two, not an l two. You're a roll up, not a roll up, what have you. But I think the really kind of valuable thing is sort of the one level deeper of what are the assumptions being made when I use this system? What am I trusting? And for example, sites like l two beat that sort of dig into security frameworks to me are kind of great resource there, although there's this tension, which is how much can we expect sort of end users and even application developers to actually sort of fully understand those details, which has sort of always been kind of a challenge.
00:49:07.080 - 00:49:58.420, Speaker A: I have the feeling that the data availability as a consensus, I mean, how much bandwidth can handle a consensus in that availability. This is a problem that's a relatively new problem and we don't really know what are the limits there and what are the theoretical limits there. I know that foundation for sharding and all there are some theoretical problems, but I mean it's very young. It's a very young problem. There is not many chains that has been designed just for that availability itself. But the feeling that I have, and this is just a personal opinion, is that in a few years that availability is going to be very much solved. It's not going to be free because everything, the consensus is not going to be free, but it's going to be cheap enough or very cheap.
00:49:58.420 - 00:50:17.630, Speaker A: So that the problem that maybe we are having now where we store this data availability have the feeling that maybe in four, five, six years, ten years, it's not going to be a huge problem as it may be right now. And here I would like to hear opinions on that.
00:50:24.230 - 00:51:10.450, Speaker F: I think one other thing that is interesting from data availability standpoint to me personally is ability to maybe push a lot of the data completely off chain and to the user. So like, only the commitment to the account or someone's account state goes on chain, and the actual contents of a user's wallet and things like that are stored locally by the user. This enables a lot of scalability. Obviously it has its own pros and cons and trade offs, but this is like when we think about where does the data go? We always think some other chain, but I actually think that another option is just the user stores the data locally and is responsible for the data, and then just provides either ZTP or data kind of proofs in a stateless way to the network whenever they need to execute a transaction.
00:51:15.030 - 00:52:28.460, Speaker B: Okay, maybe just to pull things into a bit of a different direction, what should the layer one do? Right. So there's been, I think, a lot of just ongoing discussion around layer one, making changes around, possibly changing some gas costs to be L2 friendly, possibly adding some pretty deep and complicated functionalities. I know Justin Drake has been a fan of doing some of these things, possibly changing in response to or kind of following behind some of the changes that are being innovated on L2s. Maybe other things. Yeah. What are some ways that you guys think that the l one should continue evolving in order to make the l two world continue to progress as smoothly as possible?
00:52:29.630 - 00:53:11.320, Speaker F: I will put two things. One is fast finality, which is not exactly a new topic, but I think it will secure much more the roll ups. It will be much better for everybody. And maybe the technology as well could be reused in the L2. All the progresses on fast finalists on the layer one can be very interesting for the L2, and on a totally different level I will put at the EVM. I think it's important linked to actually a contrast action to add some cryptography primitives. There are a lot of EIP around that that will allow much, many more signature schemes or many more complicated cryptography at a contrast action level.
00:53:14.090 - 00:54:16.720, Speaker E: I think there's a number of areas that are awesome to see. I appreciate that finality was the first call out, just because I think that's a really exciting one, the most obvious one probably to people, which kind of is exciting is expanded data capacity. And obviously we have our kind of dream final state of full dank sharding and kind of a bunch of interesting work going on in pure Das, which is sort of relatively new to discussion as sort of another intermediate state. And then I think the last area I would call out, and I think one that's sort of probably more useful to optimistic rollups than to Zk roll ups, although I think incredibly useful in general, is censorship resistance. In that right now Ethereum has relatively weak censorship resistance guarantees, and there's kind of between inclusion lists and personally I'm kind of pretty excited and interested in embedded pdfs and kind of various things which would get Ethereum to a much harder guarantee there.
00:54:20.050 - 00:54:46.120, Speaker A: I would add here ossification. I think it's important that in order to build, if you want to build really l two s that are decentralized, we need l one to be decentralized. And for that I think that l one should be ossified. I know that this is not something that happened from one day on the other. Ethereum is not finished yet, and there is still a lot of things to do before doing ossification. But something that's really important to happen at some point.
00:54:48.890 - 00:55:29.710, Speaker G: If I could request or shape from a layer one, is, I think the biggest challenge of blockchains is around the area of, let's call it broadness, or I just prefer the term broadness to decentralization because I don't like double negation. So decentralization means not centralized, centralized, bad, decentralized, good. So I prefer the term broadness. So blockchains are ultimately all about broadness, maximal broadness, so that there's a very wide basis. It's related a little bit to what Jodi spoke about censorship resistance.
00:55:29.870 - 00:55:30.580, Speaker B: And.
00:55:33.610 - 00:57:22.680, Speaker G: It'S a constant battle because there are natural forces and incentives that tend to centralization. And then what we need from the layer one of ethereum is, I think, things related to tokenomics and incentives that would just keep on broadening all the time, the base of operators and stakeholders. In a way, now it's a very hard challenge because there will always be counter forces of centralization. But if the one thing that should be best done, it would be focusing on that and then compatibility with, I mean, by the EVM being the first Turing complete machine, it already allows all of this whole family of l two s is all because of Vitalik, your vision of like a single Turing complete machine. So if the one thing I would try as much to push forward from the layer one is the broadness, and it's related to the incentives and tokenomics, I don't know how to change it in a way so that a larger part of humanity has a stake in it and it's ever more broad. And at some point maybe we solve proof of humanity so that people can actually will get closer to one individual, one vote, or have elements of that without relying on nation states. So that would be the area I'd like.
00:57:23.210 - 00:57:50.560, Speaker B: Amazing. So I saw Brian from risk Zero just joined, and I feel like Risk Zero is another one of these newer members to the family that a lot of people have probably not yet had a chance to hear about. So maybe talk about what are you guys up to and what are you guys looking forward to being up to in 2020 for?
00:57:51.010 - 00:58:31.760, Speaker C: Yeah, I mean, I can also sort of answer that last question, and I think something that would be really useful from our perspective is just to make it easier to integrate zero knowledge proofs onto the l one in any manner. So if that looks like gross 16 pre compiles or even pre compiles that support more stark like systems, that would be great. And then obviously dank sharding is going to be huge, I think, for all of us. But, yeah, what we're focused on right now is really getting our verifier onto main net so that people can use zero knowledge proofs produced by our system on ethereum proper.
00:58:34.290 - 00:58:40.900, Speaker B: Okay, any final words from any of you guys?
00:58:41.990 - 00:58:52.482, Speaker H: We need account abstraction, please. Have you tried sending transactions recently? Oh my God, it's so bad. We need to fix it, really.
00:58:52.536 - 00:59:12.266, Speaker B: I thought sending transactions is like, amazing compared to four years ago. Remember back in the bad old days when you had to send a transaction and Ep 1559 did not exist and so you had to wait potentially like five minutes or an hour for the thing to even get included? That's true. But yes, we do need to get.
00:59:12.288 - 00:59:13.258, Speaker F: Up, but you have to know how.
00:59:13.264 - 00:59:13.974, Speaker H: To use a wallet.
00:59:14.022 - 00:59:14.858, Speaker F: I think if you know how to.
00:59:14.864 - 00:59:24.910, Speaker H: Use a wallet today, it's great compared to four years ago. If you don't know how to use a wallet, it's a whole mess. There's so much complexity.
00:59:27.970 - 00:59:36.900, Speaker C: I definitely think of that as a major obstacle in terms of when will my friends start using crypto instead of making fun of me for doing it.
00:59:41.270 - 00:59:46.120, Speaker A: Next biggest test right now is I think that all of us will have a lot of work there.
00:59:48.970 - 00:59:54.620, Speaker G: Yeah. 4844, let's see it arriving. That's going to be really great.
00:59:57.870 - 01:00:20.660, Speaker D: I would like a bit of an increase in the gas limit for the l one because that will make our lives a bit easier in terms of decentralization because I have a feeling that for all of us, once we try to decentralize the costs of publishing the batches and finalizing batches on chain is going to increase for a gas increase will help us with that.
01:00:23.190 - 01:00:37.270, Speaker E: I want to see more kind of multichain l two native wallet development in general, just sort of managing the complexity of multiple networks in a way that makes it less confusing for users.
01:00:44.630 - 01:01:04.780, Speaker C: I'll just say I want to see more of this gathering in this format in broader formats, coordinating, speaking together and making peace with ltus and with this. Guys, I need to unfortunately depart now. So it was a pleasure seeing all of you.
01:01:05.150 - 01:01:18.320, Speaker B: Yeah. Okay, so we met on land, we met in the cloud. So maybe our next one is going to be in a swimming pool. Okay. Yeah. Thank you so much. Yeah.
01:01:18.320 - 01:01:23.310, Speaker B: Anyone else have anything otherwise?
01:01:24.550 - 01:01:26.430, Speaker E: Thank you for coming on and hosting.
01:01:26.590 - 01:01:27.300, Speaker C: Yeah.
01:01:29.830 - 01:01:30.626, Speaker G: Thanks.
01:01:30.808 - 01:01:33.198, Speaker B: Yeah. Thank you guys for joining.
01:01:33.374 - 01:01:34.900, Speaker G: Abdel, do you want to say.
01:01:38.230 - 01:01:59.914, Speaker I: Thank you, Vitalik, because you accepted this invitation very last minute. So Vitalik actually accepted like 2 hours ago. He did not know anything about the event. So thank you for that. And thank you very much to all the participants. I hope we can do another edition sometime later in the year and also some physical editions too. I think we can do.
01:01:59.914 - 01:02:07.260, Speaker I: Probably we will all be at etc. Or something like that. So maybe we can do something in person that can be amazing.
01:02:09.070 - 01:02:09.820, Speaker B: Amazing.
01:02:10.990 - 01:02:12.946, Speaker I: And yeah, let's make peace together.
01:02:13.088 - 01:02:22.100, Speaker B: You. Yes, let's make peace and peace out and keep peace back in. Not too far in the future.
01:02:23.110 - 01:02:24.210, Speaker I: Bye bye.
01:02:25.030 - 01:02:26.018, Speaker F: Thanks everyone.
01:02:26.184 - 01:02:28.270, Speaker A: Thank you. Bye.
