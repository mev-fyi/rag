00:00:07.760 - 00:00:32.754, Speaker A: Hello. I guess my first question would be, can you see and hear us? Please write in the comments. Yes, thank you, Patrick. Okay, so terrific. And thank you, Mark. Okay. It's always confusing with these things.
00:00:32.754 - 00:01:01.278, Speaker A: So my name is Eli Ben Sasson. I'm CEO at Starkware. And welcome, everyone, to another stark at home episode. We have here Ulrich Habok, an applied cryptographer from Polygon Labs. Shahar Papini and Andrew Milson from Starquare. And today we're going to discuss Circle Stark and Stu. So let me.
00:01:01.278 - 00:01:29.594, Speaker A: I'll give a brief introduction and then hand it over. The schedule is going to be like this. We'll start with a very brief introduction. We'll hand it over to see some performance metrics and why, at least we are very excited about what's coming. And then we'll talk first a little bit gently about like the math and innovation in there. And then, time permitting, will go deeper and deeper into the math. As much as you have patience for that, I urge you all to post questions.
00:01:29.594 - 00:02:13.084, Speaker A: You can use the. I think you can use the comments for it. So, like, here. Here's a question. And from time to time we will try to answer them. So, you know, about four years ago we came out with our first production system and it was serviced by a prover that we call stone. This prover was used to settle over a trillion dollars worth of volume on top of Ethereum and settle hundreds of millions of transactions.
00:02:13.084 - 00:03:12.382, Speaker A: And it's been working pretty nicely for the past four years or more in production. And it is open source now and it works very well. But now we're actually building a new open source prover called Stu and its development is being done open source. We can share with you the link and it's going to be much, much better and much faster. And to explain what we mean by much better and much faster, I'm going to hand it over to Andrew Milson to share some numbers. And again after that, after discussing the performance and also offering some interesting research and collaboration avenues, we're going to try and explain why is it a little bit faster? So please, Andrew, take us through it.
00:03:12.558 - 00:03:57.004, Speaker B: Awesome. Well, thanks, Ellie. And hey, everyone. So, yeah, what I'd like to speak about right now is a couple of things. I want to talk about the high level design of Stu, how we've gone about designing it so that it's fast on multiple different architectures and different kinds of hardware. And then I want to talk about the performance compared to stone. And at the end, I'd like to talk about some exciting directions or some exciting things that we can collectively build as a community with stuff.
00:03:57.004 - 00:04:23.364, Speaker B: So, yeah, let me just share my screen, actually. Okay, so can everyone see that? Can everyone see my screen?
00:04:26.444 - 00:04:30.544, Speaker A: I can answer, but, yeah, let's wait for the people from the audience.
00:04:34.404 - 00:04:38.748, Speaker C: Yeah, the audience seems to see the screen.
00:04:38.876 - 00:05:16.204, Speaker B: Okay, cool. So, yeah, so I want to start with the high level design of Stu. So our goal is to make Stu as fast as possible. To do that, what we've done is we've written the protocol in rust, and then we've made it generic over a backend. A backend is a type, but it's two things. It defines how it stores its data and how it operates on that data. Currently in STU, we have two backends.
00:05:16.204 - 00:06:02.162, Speaker B: We originally wrote the protocol and we created a cpu backend, which was kind of like our naive implementation where we wrote the backend without trying to make it optimized for any specific kind of architectures or hardware. And that was good. We already saw like a really great performance. We saw a great performance increase over stone from that alone. But then we created another backend and we created a SIMD backend. And for those who may not be familiar with SIMD, it stands for single instruction multiple data. And it's a class of instructions on the machine that instead of targeting one piece of data, target multiple.
00:06:02.162 - 00:06:52.534, Speaker B: So an example would be you have an instruction that performs an additional on two numbers. And with SIMD you can perform an addition on like 16 numbers in more or less the same amount of time. So when you utilize something like SIMD on a machine, you can get a really great performance increase. So, yeah, so we built this SIMd backend and we saw a really great performance increase on that. So, yeah, that's a little bit about the high level designer stew. It's, we wrote the protocol and then you can plug in kind of whatever back end you want, whatever hardware you want to support. And now I'd like to talk about the performance.
00:06:54.394 - 00:07:07.682, Speaker C: I'd just like to add that we do expect other backends to also be implemented for all kinds of hardwares, like GPU's, FPGA's, wherever you want to, basically.
00:07:07.818 - 00:07:46.210, Speaker B: Absolutely. So I don't have the data on how fast it takes, how fast you can prove Cairo and Stu. We're still working on that. But what I have here is just the single thread FFT performance of Stu with both of our backends compared to stone. So the, the blue line here at the bottom is stone, and all of the dots at the top here is Stu. And you're gonna have to. You're gonna have to believe me that the FFT is a big bottleneck for the prover.
00:07:46.210 - 00:08:38.444, Speaker B: Like, if you squint hard enough of the protocol, it's just a bunch of ffts and hashes and polynomial evaluation. So, by improving the performance of ffts and some of the other primitives, it directly translates to performance improvements of the entire protocol. So, this is. So I just want to say a couple other things about this. These red and pink dots at the bottom, this is the cpu backend I just talked about. And all of the, like, the green, the orange, the purple, and the. And the brown, like, these are all a SIMD backends, and was actually something that maybe the engineers will find interesting, is, like, we originally wrote a backend for AVX 512.
00:08:38.444 - 00:09:39.976, Speaker B: Specifically, we just focused on AVX 512 instructions, and then we switched it out with Rust's portable SIMD library, and we saw no degradation in performance. And what we got along with that was not only, like, a implementation for AVX 512 SIMD instructions on intel machines, but we also got, like, really good performance for AVX two, and even web assembly, and the neon instructions on, like, an M one max. And, like, the difference between, like, the M one Max performance, the native performance with neon instructions, and webassembly, there's only, like, a 30% loss when in webassembly. I thought that was quite interesting. And also, if the log scale doesn't make it clear, like, the difference between the stone FFT performance and the stuff FFT performance is, like, 200 x. So it's a real jump in performance. And this is just implementation of an FFT.
00:09:39.976 - 00:10:13.474, Speaker B: But in Stu, we haven't just optimized the implementation, we've also. We're using more optimal algorithms. So, yeah, so Shah will touch on how we're using GKR for things like lookups and mixed degree to get improved performance there. Okay, so that's the. That's a little bit about the performance. And. Yeah, the other thing I just want to say is that the backends are modular.
00:10:13.474 - 00:11:00.614, Speaker B: You can create your own backend in your own repository and plug it directly into stew. And if anyone's excited about this, I think it would just be so awesome to have. Have more backends, as kind of Shah mentioned. So, like, the backends can define how they store memory and operate on that. So anything from, like, an Nvidia GPU or some crypto specific hardware, like a fabric VPU, or even really fast implementations in the browser using web GPU can all be supported in Stu and people can create their own backends for this. And I think that's really exciting. Like we'll see more orders of magnitude performance on top of the SIMD backend.
00:11:00.614 - 00:11:18.044, Speaker B: I think if we see these backends, I encourage anyone who's interested in that to explore that. And yeah, feel free to send a message about that or anything like that. So yeah, anything you'd want to add, Eli Shah or Ulrich?
00:11:18.084 - 00:11:35.608, Speaker A: Well, do you see, do you see the question? Maybe there's a question by Patrick Grinaway that Shah already answered, but maybe you can answer it for the audience. How should we interpret the throughput of the FFT? Does this directly translate to the performance of the LDE step? Patrick?
00:11:35.656 - 00:12:24.366, Speaker B: Yeah, exactly. So the, the throughput here is like elements, like field elements per second. So in terms of Stu, this is the Mersenne field, and in terms of stone, it's the field used with Cairo. But yeah, so that's, that's kind of how looking at throughput. But for instance, let's take Cairo, for example. You have an LDE over the addresses used in Cairo, which could be stored in a Mersenne element. So you can actually, although the field sizes are different, you don't need a large field to store a lot of things that you need in like an air for a VM or something like that.
00:12:24.366 - 00:12:33.954, Speaker B: So yeah, the, yeah, I just want to mention like the throughput is for field elements, Mersenne versus the chirafil.
00:12:37.334 - 00:12:38.102, Speaker A: Okay.
00:12:38.238 - 00:13:14.384, Speaker C: Yes, I want to emphasize that the 200 x you see here is again for field elements. And it really depends on the statement you're trying to prove. Like some statements, if you go to smaller field, you don't need to add more of these fields, field elements. In some you do. So it depends on the statement you are trying to prove. But like Andrew said, usually in vms, then you get this 200 x tradeoff because you usually are not using big values in your elements.
00:13:17.104 - 00:13:39.084, Speaker D: Yeah, I have a question, shacha, because I don't have any clue of neither implementation of yours. So the storm prover that's in rust, it's C. Okay. And it uses also SIMD.
00:13:39.584 - 00:14:26.098, Speaker C: Well, it's a good question. Using SIMD did not provide the performance gains because we are using another feature of the CPU, specifically of Intel. CPU's for these multiplications. It's not vectorized, but it is using a more efficient version for the multiplication and propagate carry. It's like a CMO cad, something like that. So we're going to, for the big fields, we did implement it as performant as we could. Using vectorization did not make it more performant in that case.
00:14:26.266 - 00:14:41.030, Speaker D: Okay, I'm not a developer. I'm quite surprised. I mean, couldn't even just scale, say across columns. That means row wise, you know, you do always the same operation, even if it's.
00:14:41.062 - 00:14:54.834, Speaker C: But you don't have the same operations in vectorization that you have on single elements. There are these operations on single elements that did more things than the just vectorized versions.
00:14:59.174 - 00:15:03.014, Speaker D: Okay, thanks. Sure.
00:15:03.514 - 00:15:10.574, Speaker A: Okay, so, Andrew, are we done with the performance discussion?
00:15:11.074 - 00:15:13.706, Speaker B: Yeah, I'm happy to pass it over now.
00:15:13.890 - 00:16:17.714, Speaker A: Okay, so maybe let's move on to talking gently and lightly about some of the math we saw here, the fft that I guess some of it is the effect of moving to this smaller field. And we'll talk about that a little bit later. But there are a number of other improvements that will go into Stu. Most notably, two of them are the use of GKR and mixed degrees. So now, I asked Shachar in advance to prepare a gentle non math explanation of what is each one of these and why should we care and how will they improve performance. So let's start with GKR. And I encourage the listeners, if things are not clear, please ask questions and.
00:16:17.714 - 00:16:21.734, Speaker A: Yeah, so Shaho, what is GKR? How should we think about it?
00:16:22.314 - 00:17:19.174, Speaker C: Yeah. Okay, so first of all, starks began in a really specific way. Starks have their pretty much, there's a table of values and you have some constraints on them, and that is what you prove. That is how it started, that's how stone works, and that's fine. There are other proof systems, which are not stocks, which use other kinds of mechanism. Instead of this constraint system that is proven using stocks, they use some checks, things like, other things. And the problem was that some check needed some primitives that weren't in starks, a bit more technical.
00:17:19.174 - 00:18:23.542, Speaker C: Starks need univariate polynomials, and some checks need multilinear polynomials, that is, polynomials with a lot of variables, but of degree, one in each. And these things didn't really fit together. However, it turns out they can work together, and that is exactly what we are doing in Stu. And I spend the benefits. Some checks enable this GKR thing. It's a cool algorithm, it's cool protocol, also for proving some computations. But instead, like in stark, of taking your table of values and committing on it, using some kind of a hash, hashing it and sending it to as the proof, instead, it's using multiple rounds between the prover and the verifier to prove the some kind of arithmetic circuit.
00:18:23.542 - 00:19:23.770, Speaker C: If you have these inputs, you do some operation, some operations, operations operate version of them, we get some output. So it's trading off, instead of committing to all these intermediate layers, it's doing more rounds of pulling randomness from the verifier and sending things from the prover. And that is a trait of GKR. Sometimes it's okay to take, sometimes it's not. It seems like the best way is to combine both. We have a single error on some of the witnesses we are committing on, and some of the computations on this witness we are using GKR to prove, and we get this combination that makes our errors more efficient. Specifically, we took as an example proving the Blake hash.
00:19:23.770 - 00:20:14.544, Speaker C: Blake hash is a hardware hash, and it's fast for hardware and for cpu's, but it's not as efficient to use in stock proofs. But using both GKRs and regular stocks, we were able to cut down the cost of the proof about X two, X three, something like that. And. Yeah, and that was enabled by this way to combine some checks, GKR and stocks. And so that is one thing we have in STO. I think Andrew worked on implementing it. So maybe you can also provide some details on its concrete performance.
00:20:14.544 - 00:20:19.474, Speaker C: Okay, that is one.
00:20:20.014 - 00:20:28.834, Speaker A: How much does it save like this, moving to more rounds of interaction, but fewer use of hashes.
00:20:29.174 - 00:21:18.504, Speaker B: So what we saw is that with grand products, I think it was a two times increase in improved performance, and I think we expected it to be faster initially, but the FFT. So previously we'd done lookups with FFT and hashing, and those are so fast in StU that it, you know, maybe we were expecting like a five x or ten X, but it was really only a two x, because in stuff, FFT and hashing are just so fast. Anyway, with the log up lookup arguments, I think we saw, we see kind of more like a three or four x improvement, and these translate to big, big performance improvements overall, because something like Blake two would have thousands of lookups. Yeah.
00:21:20.244 - 00:21:30.036, Speaker A: Is there any significant. Again, Patrick. Patrick, thanks for asking excellent questions. Is there any significant effect on proofer memory usage with GKR?
00:21:30.180 - 00:22:03.838, Speaker C: Yeah, it's a good question. There is some kind of trade off you can do there. Like if you do it naively, you still need to keep everything in memory. You can do some trade off that for the intermediate values, you can keep only some of them in, in the memory at each, at each point of time. But then you pay with more rounds or a more overhead later. So if your main concern is memory, you can use this to reduce the memory. But again, it's a trade off.
00:22:04.006 - 00:22:08.344, Speaker A: But aren't they all of them like sort of o of n anyways, like.
00:22:09.124 - 00:23:03.868, Speaker C: But it depends on like, if you're bounded by the maximum memory, then you don't want to keep all of it together. Let's say you have a, you know, your weakness is of size 100, but your intermediate computations are size 400. So instead of saving 500 in memory, you could maybe reduce it to like 200 most and do it. And splitting the 400 minutes to four parts each one of size 100, you can do things like that. Okay, which has you using the hash. We are using Blake hash. As for the prover as well, we are proving the statement of Blake's, and the proof itself also uses the Blake hash for the commitment that is to allow us to do recursive proofs.
00:23:03.956 - 00:23:16.624, Speaker A: The reason we'll ask are, isn't this, I mean, isn't it a trade off where it's very efficient in terms of cpu to hash, but then very inefficient to recursively proof that you use the Blake?
00:23:17.124 - 00:24:14.924, Speaker C: It is, it is, but ok, if your main concern is to reduce the overall approval time, and you assume that you have enough things to prove, then you can always, because the verification is sub linear, you can always take computations which are so big that make the recursion insignificant anyway, even if it costs two x or three x. So it still pays off. If your intention is to reduce the overall cost of proving, you want to make the prover as fast as possible. And another point is, assuming you are doing recursion at the end, like the last proof, you can't start to do like recursive proofs that are more beneficial for the verifier to make the final proof very, very efficient so you can somehow enjoy both worlds.
00:24:16.624 - 00:24:20.698, Speaker A: Okay, there's another question now, a lot.
00:24:20.706 - 00:25:55.904, Speaker D: Of very, oops, sorry, can I run into here? Because I was trying to answer one of the questions. I hope I did it. So, where are the places where GKR is a benefit versus a classical univariate strategy? That's typically, I mean, the main benefit, as Shacha was pointing out, is, or let's say trade off you face here is on the one hand, a multi round protocol with these many interactions, but intermediate results are not committed. So no hash of intermediate states versus something, an universe structure strategy that computes all the way to commit all the intermediate states and use universe strategies to prove it. So, in particular, it's the cost of the GKR protocol itself, which uses a lot of linear extrapolation steps. That's the linear and multilinear protocol that costs a lot because it do it over large domains, but you don't commit. And in the end, it's about really, first of all, which hash function do you use? Is it something like Poseidon, which is more costly and primitive, or is it something like Blake, which is very fast and primitive, but more costly than Poseidon? In circuit things like this, versus how fast is the arithmetic steps you do in every single round of the GKR protocol?
00:25:57.204 - 00:26:09.704, Speaker A: So, there's another question here. Does the Logan, by the way, terrific questions keep them coming. Does the fact that GKR is an interactive protocol lead to a downturn in performance and memory?
00:26:14.104 - 00:26:53.920, Speaker C: The interaction is, first a verifier penalty, and the verifier needs to do a lot more hashes in its verification for each round, basically. So that's the first drawback in terms of performance memory. The performance is actually better. Assuming your computation is in the extension field. It's a bit of like a fine point. If the computations are in the base field, there are still stuff you can do. It's like at the edge of research.
00:26:53.920 - 00:27:28.364, Speaker C: We don't have any of that getting stuff. But when it's computation, the extension field, then, like Andrew mentioned, the performance is better. The memory naively isn't like, isn't it better? But you can do another trade off that makes the memory a bit better. And you again, pay more for the verifier and bit more for the proofer. So you have like two trade off space you can do with GKR, which generally gives better results.
00:27:30.164 - 00:28:35.420, Speaker A: I want to add here that I think this was mentioned before, but the use of GKR, GKR, or some checks are very useful when, and also Ulrich wrote this, when what you're verifying with them or proving with them is a very shallow and very wide circuit. So in the extreme case, if it's something like a lookup table, where basically it's like depth one, because the complexity grows with the depth of the circuit or the computation being made. So probably for general purpose sequential computation, GKR doesn't work as well as using errors, but for things like lookup arguments or log up arguments and very, very shallow circuits, they work very well. Right. Okay, let's see if this I remember, it was claimed 1.6 x speed up in the paper for some memory issues. What is the obstacle for the claim? 100 x speed up in stu.
00:28:35.420 - 00:28:39.404, Speaker A: Is that solved? I'm not sure I understand exactly the question. Maybe you guys understand.
00:28:39.564 - 00:28:46.344, Speaker C: Do you mean, like, it seems like you're mixing memory with the runtime performance?
00:28:47.164 - 00:29:06.324, Speaker A: No, no. I think the 1.6, I think, was over in the. In the white paper. I think there's a comparison of circle Stark to some other small field like baby bear. And here we're talking about stone, which is over a much larger field. So.
00:29:06.324 - 00:29:17.496, Speaker A: Yeah, exactly. Oh, sorry. So the 1.6 in the paper is compared to baby bear.
00:29:17.560 - 00:29:18.018, Speaker C: Right.
00:29:18.136 - 00:29:39.590, Speaker A: Baby bear, which is another 32 bit prime. Or 31 bit prime. Okay, so let's move on to the next improvement, which is. What do you mean by mixed degree? What? Give us some informal explanation of what's going on.
00:29:39.742 - 00:30:10.026, Speaker C: It's not something that hasn't been done before. Some people did it. I think even Polygon had some version of it. But basically it means. Remember I said in stocks, we have usually a stable of values. Well, sometimes you can't really fit your statement in, like this rectangular table of age on W. Sometimes you have like, columns of different length.
00:30:10.026 - 00:30:45.658, Speaker C: You want. For example, maybe I'm doing like a thousand hashes, but only 200 multiplications or something like that. I want things to not be of the same degree. Not want not all the columns to be of the same length. So what we can do, and what we do in Stu is basically have multiple. It's sort of like multiple tables of different heights. But we're not committing on them separately.
00:30:45.658 - 00:31:12.396, Speaker C: We're committing them together using merkel tree shenanigans and fry shenanigans by a. You know, Fry, for example, is a protocol that, based on layers, you have this big layer. And then you fold it to a smaller layer. Big layer, smaller layer. And then you fold it to a small layer. So you can imagine that if I started the layer of size, let's say 1000. And then next layer is size 500.
00:31:12.396 - 00:31:56.056, Speaker C: Then I can hear a jump and add another column that is shorter. If I had like a column size 1000 and another column size 500, I can edit later in the fry protocol or later in the moco tree. So that is a. And what mixed degree means that it gives us better. A gives us better packing of the data we want inside the commitment we have. So we need to do less hashes and not waste this commitment space. And b, the degrees of all kinds of things, like constraints and polynomials.
00:31:56.056 - 00:32:44.424, Speaker C: And fry, these degrees can be lower than they would have been before. And that actually affects the performance directly. Because for a constraint, you need to do some work for it. In Starc, this work is proportional to total degree of the constraint. So if you have a constraint that work on smaller degree polynomials and the total degree of them is smaller, then you work less for them. And yeah, this is a mixed degree, basically. Are there any questions? Is there a soundness consequence? Mixed degree? Not that I know of.
00:32:44.424 - 00:32:45.084, Speaker C: Yeah.
00:32:47.064 - 00:33:34.904, Speaker D: It'S negligible. It's negligible. You know, if you look carefully at the formula from the proximity diaps paper for batch fry, because it's always, you know, you always have, it's always about random linear combinations, either the big, the huge batch in the, in the very beginning, or the smaller batches of just odd and even functions in the folding cascade of rye. But essentially, there is an influence on, on the size of the batch. So how many functions are subject to the linear combination, on the soundness error, but it's negligible. Yeah, maybe one bit, two bits, something like this, two bits of security. So typically something which is not of concern.
00:33:35.644 - 00:34:18.274, Speaker A: Yeah, I'll say that when it comes to soundness, for mysterious reason. Well, I guess mysterious reasons. Up to the fact that, you know, that's like what just the math shows. There are certain parts that are very, very good soundness, like the linear mixing of various commitments. You pay pretty much the minimum possible that you could hope for. So if you add k things, you select randomness. Select randomness over a field of size q, then your soundness, you know, you lose something like k over q, which is very, very negligible, because often k is small, let's say less than 1000.
00:34:18.274 - 00:35:00.094, Speaker A: And q is like, by this stage, really large, like two to the 128 or maybe even larger. So you lose a very small soundness. There are things where you lose in a much worse way, which is more correlated to the rate of the polynomial or the blow up factor. And that is related to the queries. And there, it's actually, soundness isn't as good. It is the ratio between, well, either the ratio or square root of the ratio between the code length and the degree of the polynomials. And that's like a very major.
00:35:00.094 - 00:35:19.474, Speaker A: Things are not as good there. But luckily, the mixing or mixed degrees is on the part where it costs you almost nothing. As Ulrich said. There's another question Logan is asking. Are there constraints between the subtables with polynomials of different degree?
00:35:19.634 - 00:35:57.924, Speaker C: Yeah, that is also a good question. You can do them. We try to avoid them. The reason is it adds inefficiencies. Let's say you have a constraint between a column size 101 of size 500, then the total degree of the constraint would be 500, because it has this polynomial of degree 500 in it. So you would have to evaluate it on 500 points in the StArC protocol. And that means that the small column of only size 100, you have to evaluate it in a lot of points all of a sudden.
00:35:57.924 - 00:36:14.544, Speaker C: So it adds these kinds of inefficiencies. Sometimes we have to do it, but usually we try to avoid it, and we link different size tables using lookups. And this is more efficient, usually.
00:36:19.504 - 00:36:33.124, Speaker A: So did we measure circle stock performance for other fields? I mean, I know the short answer is no, because we didn't. It's not that easy to implement efficiently. You know, any sort of FFT. So. Right. The answer is no.
00:36:34.824 - 00:36:36.764, Speaker C: Yeah, we haven't.
00:36:37.944 - 00:36:52.356, Speaker A: Okay, by the way. But. But also, even like, m 127 would still be. It's like half the size of the stone prime. So, you know, you'd expect it to be a bit m 17. More interesting. All of these are resent primes.
00:36:52.356 - 00:36:53.584, Speaker A: There's so many of them.
00:36:54.364 - 00:36:55.264, Speaker D: Okay.
00:36:56.084 - 00:37:21.496, Speaker A: Okay. So let's move on to the. To a third part of circle Starks. I'll try to give an explanation of something that I actually wrote in, in a blog post that you're welcome to. I put it here in a comment. You're welcome to read it later and share with your friends. And it is.
00:37:21.496 - 00:38:32.532, Speaker A: So I attempted to explain, like, why am I excited about circle Starks and what is even going on? So let me try and run this explanation, this very informal explanation by you. So, one part that is obvious, and I think is a major part of the explanation for the 200 x improvement from moving from stone to stew is just that you get to work with smaller size numbers. The stone system works over 252 bit numbers, and now you sort of, each multiplication and addition takes many cycles. We counted about around 40 cycles to do one multiplication. And when you move to these 32 bit integers especially, well, not all 32 bit integers apparently behave the same when it comes to things like multiplication and addition. And the reason for that has to do with a binary representation of them. And in this case, Mersenne.
00:38:32.532 - 00:39:34.584, Speaker A: Primes are very special, not just because they're named after Mersenne, but because, well, think of M 31. It is the prime two to the 31 minus one. So if you're working modulo this prime, which is what you do when you work in this finite field, it means that the number two to the 31 is equal to one. So one with a lot of zeros, I guess 30 zeros or something, or 31 zeros is equivalent to one, which means that when you're doing modular operations and reductions after you did the multiplications or additions, you get something that is very efficient and involves a very small number of cyclic shifts. So not all 32 bit primes are the same. And you would like, ideally, to work with something that is like a Mersenne prime. It would be really great.
00:39:34.584 - 00:40:40.434, Speaker A: However, there's another thing that you want from the modulus that you're working with, and this has to do with the structure of groups over this finite field. So I'm assuming that if you stayed with us so long, you probably are not afraid from hearing the term FFT, the fast four year transform. And I'm guessing that a lot of you probably remember learning the FFT algorithm or reading it someplace. It's this beautiful, super efficient algorithm that basically moves from one representation of a signal to another. And when you heard about it first, you probably were given a signal transformed by evaluating it over roots of unity. And the number of roots of unity was a power of two. And the reason you want an exact power of two is because you have this beautiful recursive structure.
00:40:40.434 - 00:41:25.284, Speaker A: If you look at the number, let's say two to the ten, if you divide it by two, you get another number that is of the same form, but smaller. It's two to the nine. You divide it again by two, you get two to the eight, and you can repeat this time and again. And if you remember how the f of t works, you take a sequence of size n, and you break it into two subproblems of size n over two. And if you want to repeat this thing time and again, so you want to take something of size n, break it into two subproblems of size n over two. Well, for that to happen, you want n to be even, but you want to do this Again. So you want each one of these n over two chunks.
00:41:25.284 - 00:42:17.482, Speaker A: You want to be able to break them up into two even parts, sorry, two equal size parts of size n over four. So you want four to divide in, and then you want a to divide in. It's not hard to see that you would like a number of points, evaluation points, that is an exact power of two. And really, I think, pretty much all ffts, really fast ffts that are used in the industry for signal processing or learning or whatever it is used for, they will take signals of size two to the n. Well, it turns out that when you do our low degree extensions, you want a similar structure. You would very much like a number of points to evaluate your functions over that is an exact power of two. So you can do like things like FFT.
00:42:17.482 - 00:43:03.752, Speaker A: And it even shows up in things like fry, this sort of breaking up into two chunks of even size. The problem is that the Mersenne prime doesn't have, um, a set of points that both form a group. And the size of this group, or the number of points on it, is an exact power of two. And the reason for that is that the multiplicative group of Mersenne prime is actually two to the 31 minus two, which if you factor it, it's two times some odd number. So the you. And because of this, you won't have a group. You're very far from having a group of right FFT structure.
00:43:03.752 - 00:44:32.242, Speaker A: Okay, now this problem was already tackled previously in the context of starks in this mathematical paper about elliptic using elliptic curves. And the key idea was to say, well, we really want a group of size two to the k that is defined over the field, and we would like it to be an algebraic group. So the brilliant three co authors of the Circle Stark paper said, aha. There is a very elegant algebraic structure called the circle, which is defined by the equation x squared plus y squared minus one. So it's the good old circle that we all know from high school, elementary. And it turns out that the number, well, the points on the circle defined over the Mersenne prime, the number of points there, is a very high power of two, forgot if it's two to the 30 or two to the 31. So you have like this structure that is a group and is of the right size and is defined over this Mersenne prime that allows you to have fast arithmetic.
00:44:32.242 - 00:45:16.610, Speaker A: So if you want to read more about this or explain this to your friends, because I'm assuming that a lot of our listeners actually understand this and didn't mean my explanation, you can go and point them to this blog that I wrote about it. But now, unless you have questions about this explanation, I'll hand it over to Ulrich. So first of all, I'll ask, do you have questions about what I said? And if not, I'll hand it over to Ulrich to give a little bit more mathematical rigor into what is circle Stark? And why should we care? So Constantine is asking something in Russian, I think, but I don't. So if someone can translate it, the.
00:45:16.642 - 00:45:25.482, Speaker D: First word is super. No. Super? No rusk? No rus?
00:45:25.658 - 00:45:28.090, Speaker A: No uski. Okay. Sorry.
00:45:28.242 - 00:45:28.786, Speaker D: Yeah.
00:45:28.890 - 00:46:37.626, Speaker A: Okay. Well, if someone can translate it, we can try to answer, but do extension. Well, I guess, what exactly do you mean by do extension fields? First of all, the circle, okay, the circle, just like if you look at x squared plus y squared minus one, right? It has a bunch of solutions over the integers. I think it has four solutions, right? 0110 minus 10 and minus. Well, whatever, right? The four points, it has an infinite number of solutions over the field of rational numbers and as many, many more solutions over the reals and even, well, the same, how do you say the same cardinality but a lot more solutions over the complex numbers. What am I saying? As you move from one field, right, from the, as you move from the field of rational numbers to real numbers to complex numbers, you get more solutions. And the same thing happens of course with Circle Stark.
00:46:37.626 - 00:46:46.946, Speaker A: So as you go from Mersenne to some finite extension of Mersenne to the algebraic closure will have many, many more solutions and also a beautiful group structure.
00:46:47.050 - 00:46:47.266, Speaker B: So.
00:46:47.290 - 00:46:51.854, Speaker A: Yes, and maybe it will even come in handy one day. Yeah, good question.
00:46:52.834 - 00:47:12.726, Speaker D: Yeah, actually we just one off to the case we use. No, I mean with classical univariate starks. You know the line how many points does it have? As many points as the field is large. So that's the size of the field. Right. I'm trying to do something else in the chat here. So.
00:47:12.726 - 00:47:35.628, Speaker D: And if you take another good question. Extension, that's exactly what you were targeting. If you take it like say an extension field of degree e, that's. Wait, I'm typing it. So then you, you just have like the eth power of that field. So, so that, that, that's what used to. And with the circle, it's quite similar.
00:47:35.628 - 00:48:00.096, Speaker D: It's just that instead of f, the size of f, it's always the size of the field. Mine plus one. So we're just one off, but the same, like if we go from, so the Mersenne M 31 has two to the 31 minus one points. The circle has one point more as the field is large. That's his. Exactly. Two to the 31.
00:48:00.096 - 00:48:38.194, Speaker D: If you have a quadratic extension of it with. Yeah, with quadratically many points. In short, then the circle over that extension has the same size of the quadratic extension plus one. So essentially the same as essentially the same behavior. So it means for security, if you used to scale up to degree four, degree five extension or beyond, depending on the base field, the same blow up from small field to soundness field you face with the circus star.
00:48:39.214 - 00:49:28.968, Speaker A: So there's another question that I can try to answer. I assume you need a larger field than 31 bit for soundness it was curious how this is accomplished. So you're right that at some, well, it goes like this. For the basic arithmetization, if you take your computation and write down its trace, the only, you know, you need your field to be just as for perfect soundness, you just need your field to be just, you know, as large as the size of the computation. For later interactive steps, you're right that you need to sample randomness from a larger, from a large enough field, because part of your error, as we said before, like when you do like batching, so you pay roughly well the number of batch things. If it's k, your soundness error is going to be like k over the size of the field. So you do need a larger field at that point.
00:49:28.968 - 00:50:11.414, Speaker A: And yes, you can then move to the proper extension field. Like in this case it would be the degree four extension, you'd have 124 bits, which is probably good enough. And the nice thing is that all the steps before that where you do arithmetization and FFT, those parts you can do over the base fields and not have this extra factor. Great question, by the way, terrific questions. In the paper it states that FFT will be the same as radix two with different twiddle. Can you provide a little more details of this? I mean, butterfly diagram would look the same as radix two.
00:50:14.754 - 00:50:28.354, Speaker C: Yes, I tried to answer in the comments. Yes, it looks exactly the same network, just with different values. I also sent the link to the implementation if you want to look, but it's just relics to FFT.
00:50:30.054 - 00:51:03.954, Speaker A: So I guess the answer to this is yes, right? For Circle Stark, you need projective space and add complex number. I guess. Yes. And then, well, last one, which this is about speculation. How does two impact gas stream? I'm sure it's going to do a lot of wonders, like right now in Starknet, like gas fee is very, very low already, like whatever sub descent for. So probably would bring it down, I don't know. By factor hundred, I'm guessing.
00:51:03.954 - 00:51:39.224, Speaker A: I don't know. We're still very far from knowing the exact effect of it, but won't make things worse. This is interesting, but I, you know, we're in math. Yeah, sorry dude. I mean I feel for you, but this is, this is for math discussions. Okay, so, Ulrich, all yours. Can you walk us through some, just a little bit of like what is the math of circles talk?
00:51:40.594 - 00:53:08.188, Speaker D: Yes, so actually in a nutshell, the circle stark is a simpler construction than the ECFFT that the EC FFT or the elliptic curved stark that Ailey mentioned before. So you know, EC FFT stark, two papers, about 70, 80 pages each, a lot of algebraic geometry because elliptic curves, of course you can, everybody works with elliptic curves, even developers, you know, or applied people like me, but really deep understanding of the theory behind it. And why do they look so? At least I don't have it. So that's main benefit one is the entire construction that was done with the ECFF and the ECFF Stark, we just took it over to something that is so easy to understand. It's the circle. And moreover, because the circle has such a simple structure, you can take this even as an occasion to learn or to approach algebraic geometry if you like to. You don't have to, it's not necessary, but I learned basic notions of algebraic geometry by it and yeah, but this simply with having this simple structure, the whole, I think the paper is much more readable, but to be clear on it.
00:53:08.188 - 00:54:12.350, Speaker D: So if it comes to performance, there is no benefit. You could run m 31 elliptic curve stocks, it would be essentially the same speed versus the circus stark. But the circus dark is much easier to implement, much easier to understand, much easier to audit and things like this. So, and the reason why it's so much simpler is that the group, I mean every FFT is related to certain symmetry groups that like a rotation group, as we used in the classical FFT. And the same holds also for the circle. So if a group that doesn't, that is not that complicated as in, in the ECFFD. So let's say if I just drop some names here and it's probably going too far.
00:54:12.350 - 00:54:56.788, Speaker D: It's an affine group. Consequently the objects you encode witness data into. What's it in? Univariate stark? It's polynomials. You place your values over the trace domain and you encode it into polynomials, as we used to. Over the ECFT, you end up with rational functions, still algebraic of low degree, but rational functions, it's a bit harder to understand them. With circle again, you end up with polynomials, these polynomials, the polynomials over the circuit means there are two, but variables, that's the only difference. And actually because of x squared plus y squared is equal to one.
00:54:56.788 - 00:55:29.044, Speaker D: That's the circle identity. Because of that you can even replace every quadratic power of Y with a quadratic power in X. And if you do this repeatedly, you can boil down the power, you can kill all, all the high powers of Y, ending up just with a linear power in Y. That's already going too much into details. Let me rewind here. And let's close Pandora's box. So it's a much simpler construction than the EC fft.
00:55:29.044 - 00:55:55.794, Speaker D: We end up encoding again in polynomials. And as a consequence we can take very much directly over from the intuition we gained from the univariate Star wars starks as we know them, to the circus stark. That's in short, the big advantage of it. I think we're already running out of time.
00:55:55.834 - 00:55:59.614, Speaker A: Now, questions?
00:56:03.034 - 00:56:05.614, Speaker D: Oh, that's bad. So I talked too much.
00:56:05.954 - 00:56:06.854, Speaker A: No, no.
00:56:10.034 - 00:56:12.174, Speaker D: No questions. It's never good.
00:56:14.074 - 00:57:03.624, Speaker A: Maybe everyone understood everything, but let's see, I'll ask a question, I mean, so we now know that you can use like let's say group. Okay, there's a question, exactly the question I wanted to ask, so I'll just say it in more words, because we know that. Thank you, yao. So we know that you can use the group of the elliptic curve to do some stuff. And now we know that you can use circles which are genus zero curves. There are so many algebraic curves defined over fields. What is, you know, will we see more?
00:57:07.034 - 00:57:10.854, Speaker D: That's a good question, as.
00:57:13.714 - 00:57:15.014, Speaker A: I can try to answer.
00:57:15.674 - 00:58:13.802, Speaker D: Let me give you first my view, and you have probably the deeper answer here, really, but just from the way of applied crypto, or let's say from the implementation perspective, as I know from our team at Polygon, it's like, because I was always asking, you know, M 17 is that of use because there's many traces, which makes sense just to go up to 17, look up arguments for 16 bit range proofs. That's exactly, it seems to be like kind of sweet spot from the view, from, from the mathematical point of view. Nevertheless, on standard cpu architectures, they're all set through it throughout. No, it's. You really don't gain any performance at all here anymore. So that's one thing. So I would guess there is already the ECFD.
00:58:13.802 - 00:58:36.256, Speaker D: You know, in terms of performance, I wouldn't see an, I wouldn't see any improvement with any other type of group here. Then the classical FFD as we know it, like being log linear, to be short. But yeah, I have another answer.
00:58:36.280 - 00:59:44.818, Speaker A: I have another answer. So there's a story with it because so years before, this was before Star War, I was doing a lot of research on proofs of various sorts. And there's this, there's this, let's call it one round IOP, which is called a PCP. And there are many, many terrific questions about pcps and one of them is, has to do, you know, can you get like pcps with very good parameters and also defined over a constant size field, like, you know, whatever five bit field, even if your computation size is really large. And so we realized this was probably more than ten years ago, that there's a way it can be done, but only if you use these very exotic algebraic groups that are formed from towers of fields and stuff that I completely don't understand. And all of my co authors didn't understand either, but we knew who to ask. Like, we needed these very weird exotic structures.
00:59:44.818 - 01:00:31.580, Speaker A: We needed the groups that are transitive and over small Alphabet and like over these asymptotically good codes. So we wrote to Enig. Stichtenov was one of the leading, still one of the leading authorities on these algebraic geometry codes. And we were expecting them to answer either yes, and here's a result, or to say no, and it's probably a deep, open problem. But he wrote back to us, like, after we asked this question, he wrote back and said, oh, you know, I know how to do it, but I never bothered to write it down. And we were like, really, man? I mean, this is like really important result. You should write it down.
01:00:31.580 - 01:01:31.136, Speaker A: He said, well, you know, whatever. I didn't have the time or something. So we ended up having this paper that has an appendix by Henning Stichtenov that uses towers of algebraic well, towers of these fields and curves over them that give you something of benefit. Like it allows you to use a very, very tiny group, very tiny field, and to have very short proofs. All this to say that, you know, math always surprises you, and there's more to do. And I'm sure that more algebraic groups and structures are going to be used for, like, a lot of beautiful things. If I have to guess about one particular thing that may be of interest, like right now, we're dealing a lot with very sequential computations, and for that, all you need is like a cyclic group.
01:01:31.136 - 01:02:04.740, Speaker A: And maybe they're like between the finite field, you know, a circle, an elliptic curve, you're done. But if you want things that are, like, massively parallelizable, then maybe you want, like some other group that is much more. No, not cyclic, but has a few different generators. And then you're going to need to dig deeper into algebraic groups. Okay, we definitely went, I think, on a deep tangent. Who wants to. So each one of you, please summarize.
01:02:04.740 - 01:02:18.052, Speaker A: Why are you excited by Circle Star? Who wants to go first? We'll go backwards. So Ulrich, then Shachar, then Andrew. I already said why I'm excited.
01:02:18.108 - 01:02:50.394, Speaker D: Yes. So why excited? Because, I mean, mainly we had polygon as a lack of our understanding of the EC FFT. Back then we tried for simple approaches for the M 31. Daniel Luborov was very much pushing from early days. I joined we need to move on to smaller fields like M 31. And that this little draft is write up. We did this simpler construction.
01:02:50.394 - 01:03:25.614, Speaker D: We were very enthusiastic at the beginning. And then we discovered, okay, there are some caveats. And the caveats are like when it comes to constraint evaluation, in short, so with beside it. And the FFD is not as fast as the circle FD. So circuit fft. And working together with Shacha here and David, that was like super excited for me personally, because it knew, first of all, I could learn algebraic geometry. Second, I could tell Daniel M 31 is the way to go for us at least midterm.
01:03:25.614 - 01:03:34.294, Speaker D: And third, third one I forgot. But for sure that I forgot because I'm too excited.
01:03:39.834 - 01:03:42.774, Speaker A: Okay, my turn. Yeah.
01:03:43.794 - 01:04:25.174, Speaker C: I think circus talks to will open up so many possibilities. Possibilities with this. It's great performance and not just scaling Starknet or it's open source. So every project that ever needs to prove something will have a fast prover and will come up with these crazy ideas to how to use ZK technology for them. Open up a client side proving, which will be very efficient to just open up so many things that we can't even imagine right now. That's how I see it.
01:04:28.034 - 01:04:28.698, Speaker A: Andrew.
01:04:28.786 - 01:04:59.634, Speaker B: Yeah, I guess I'm excited because it seems that like Mersenne prime seems to be like the most optimal for cpu's, GPU's at least. And I just think that, yeah, the kind of performance we'll get will just be insane. And that's very exciting. It's also just cool stuff. Like, for me, I think a lot of the math related to it is new and it's just. It's exciting and fun to learn about. Like, there's something magical about it, I don't know.
01:04:59.634 - 01:05:05.830, Speaker B: So, yeah, it's. I'm pretty excited and yeah, yeah, I'll.
01:05:05.862 - 01:06:02.180, Speaker A: Say that I'm so happy that we're moving to small fields of any kind. I mean, when like the original ZK stark white paper was over a 64 bit field and even a binary one. And I guess I have this obsession with small fields because they're just so efficient. We can talk some other time about why it was definitely the right choice to start with a simpler, very large prime field for stone. But I'm really happy that the performance demands and basically our collective understanding of how to build really efficient start provers, allows us now to use much more efficient and smaller fields. They're just so fast and efficient. So that's why I'm really excited about it.
01:06:02.180 - 01:06:58.054, Speaker A: And also, like, the beauty of, like, this. This, you know, integration of math and engineering is just so powerful. And it's just so fucking amazing that we take these amazing, you know, high level math things and then bring it into code that is just used massively is going to be used massively by humanity. It's just mind blowing. And it's, like, such a beautiful thing for cryptography, science, and math to be done in this way. I'm just so happy to be observing it. Okay, thank you.
01:06:58.054 - 01:07:06.914, Speaker A: Thank you, everyone. Hope to see you again on another stark at home. And take care.
01:07:07.214 - 01:07:08.554, Speaker B: Awesome. Thanks, everyone.
01:07:09.174 - 01:07:10.554, Speaker D: Thanks for having me.
01:07:11.414 - 01:07:12.982, Speaker A: Thanks for joining. Bye.
