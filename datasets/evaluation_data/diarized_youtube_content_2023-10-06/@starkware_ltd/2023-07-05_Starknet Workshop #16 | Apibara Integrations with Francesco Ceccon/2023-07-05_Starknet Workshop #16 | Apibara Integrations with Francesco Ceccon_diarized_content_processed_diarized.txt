00:00:00.250 - 00:00:02.560, Speaker A: Gan, are you live on YouTube or not yet?
00:00:04.370 - 00:00:04.830, Speaker B: Yes.
00:00:04.900 - 00:00:07.374, Speaker A: Did you press go live on YouTube now?
00:00:07.412 - 00:00:08.160, Speaker B: We are.
00:00:08.850 - 00:00:25.910, Speaker A: Nice. All right, cool. I'll leave you to it. Cheers. Thank you. Thank you for the introduction. So yeah, I'm Francisco and the co founder at Apibara, and today we are going to see how to analyze unsigned data using Apibara and LGBT.
00:00:25.910 - 00:00:56.954, Speaker A: So what is Apara first? And I think it's the fastest way to build production rate indexers. And we do that by connected stacknet data with existing web two services that you really use and love. And we make it very easy to do that. Buffers. It's the faster way to build indexer. But what is indexing exactly? And it is that in a blockchain we have the blockchain state, which is for example, the balance you have or which nfts you have. And we want to update them by sending transactions to the network.
00:00:56.954 - 00:01:36.698, Speaker A: That's what we do when we click approve on Bravos or arsentex. Basically the sequencer, what it does, it takes the previous state of the blockchain at the previous block. Then it takes the transactions that the user submitted and use this information to produce the new state of the blockchain. And basically we get a startner block, which is nothing more than just the startner state. So the state at the new block. So with all the changes that happen because of the transactions, it contains the transactions on the network and the receives of these transactions. So what happened when executing the transactions? And usually we have that events, which is probably like what 90% of you are interested in.
00:01:36.698 - 00:02:17.320, Speaker A: But we also have messages between l two and l one and between l one and l two. The idea is that index is just updating off chain state based on these changes that are happening on chain. And obviously we don't want to do that only for one block. We want to do that for every block on the chain. And as the blockchain produces more blocks, we want to keep repeating that, and that's indexing. So what is Afibar? The idea behind Afibar is that we make it very easy to build indexer. We basically build first what we call direct node access, which is a very low level protocol that is very efficient and is a way to stream on chain data into your application.
00:02:17.320 - 00:03:03.814, Speaker A: And today we are not going to use that directly, but it can be used using, for example, one of the sdks we have for Python or typescript. And that's how most of our users are using Apibara now. I think the future is to use integrations which simplify a lot of the tasks that we've seen developers do over and over again by basically having these programs that do something simple like for example taking onchain data and sending to a database or to HTTP API. And basically that way it's very easy to integrate on chain data with your web, two applications. And so our slow reset stream, transform integrate. And now, because that's the three steps that it takes to actually build indexer with Apara. So with streaming we say, okay, we have a full startnet block that has a lot of data, but we are not interested in that.
00:03:03.814 - 00:03:28.080, Speaker A: We are only interested in a subset of this data. For example, today we are going to analyze ten k swaps. Swaps. And so we are going to only want that type of data. So we filter the data. So we go from a lot of data like several megabytes of data, to only a few events and changes at every block. And the next step is that we want to transform this data in a way that in between a shape that makes sense for our application.
00:03:28.080 - 00:04:19.886, Speaker A: And as we are going to see, we are going to do that using JavaScript and this runtime called Dino. And that is that we take the block with the state changes, the events, the transactions, the block headers and everything, we reshape them together in two PSG, some data that makes sense for application, for example, new rows that we want to insert into a database or maybe a payload that we send to our webhook. So we can be like a telegram bot or a distro bot very easily. And it is that the last step is integrate, where we take this data that has been produced by the transformation and we add it to another application. For example, in this case we just send it to superbase. And so each item in our list will be inserted as a row in the table, or it's very easy to switch. And we can send it to a serverless function so we can call like AWS lambda function that will exit user code and then maybe call another API, right? Advantage that, it's up to you.
00:04:19.886 - 00:05:00.886, Speaker A: I'm sure each one of you has different developers whose that they like. And the cooler thing about API bar is that you can keep using them. You don't have to learn something new, you can use what you like. And so I think it's more fun to build applications that way. And so again, stream, transform, integrate that is that we keep repeating the process hundreds or thousands of times per second. So every time there's new data being produced in the blockchain, we keep repeating these steps so our data in our database is always up to date and that's really all it takes to be the integer. And so today the goal is that we take a ten k swap, swap for the ease USDC pool, we filter and we transform it with Apibara.
00:05:00.886 - 00:05:46.554, Speaker A: We produce some new data that then we handle to superpad and the data will be inserted in a table. And it is that at the end we use this library called launching together with OpenAI to basically ask questions about this data in a very natural way that you would ask to a person and then that back the result without having to write any SQL code, which maybe you haven't used in a while. It's very easy to forget how to write SQL, so that way it's much easier. Yeah, so that's all. And now we can start coding. So the idea is that here I have a new project. You can see it's an empty project, has nothing of interest inside.
00:05:46.554 - 00:05:58.880, Speaker A: And now we want to use superbase. So we use the superbase command line tool to initialize a project. We do, yes. And now obviously we have an empty superbase project.
00:06:01.650 - 00:06:10.754, Speaker B: Could you please share what are some of the prerequisites or installations that folks would need to have in order to get started?
00:06:10.952 - 00:06:38.730, Speaker A: That's good point. So for this workshop, we need to have a superbase installed and basically the command line tool from superbase and the command line programs that we have in the API bar repository. So basically this repository, we have a series of obviously what we call like sims or integrations and they can be compiled and installed. And we are going to have binary releases later today or tomorrow.
00:06:39.310 - 00:06:40.538, Speaker B: Okay, great.
00:06:40.704 - 00:06:58.180, Speaker A: Yeah. So the idea is that now we have a superbase project, we can run superbase locally, which is very cool. So we can do superbase start. And now instead of using Superbase is also a hosted service. So we can go on superbase.com and run it over there, or we can run it locally in our machine. So everything is in our control.
00:06:58.180 - 00:07:51.906, Speaker A: And now they are launching the containers and basically launching the database and everything. So I have a complete developer environment running locally on my machine and now I get all the connections, I get a connection stream for the database, which we will use later to tell a people where to write data. And I have the Superbase studio which is basically like a complete interface for my database. Now here I just see I have a project, there's no tables because we are trying to trade that soon. So let's do that. So how do we do that in Superbase? We create a migration which is basically a way to tell the database what tables we want. So we go back to the command line, we say superbase midrate new vitro tables, superbase migration new.
00:07:51.906 - 00:08:43.810, Speaker A: Okay, so I go back to vs code. My migration tables have a SQL file that the superbase will execute to create my table. And it is that, okay, now we can create the table and have the SQL code to data. And so we are only interested in tracking the swaps. So we create that table called swaps. And we want, for our production deployment, we want to track which exchange that swap is for, right? So it could be ten k swap, it could be z swap, the name of the first token, the name of the second token, and the address of the pool. And then because we are going to build a dashboard or something like that, we want to track basically what block number the swap happened and the block time, so at what timestamps and the transaction hash of the swap.
00:08:43.810 - 00:09:21.582, Speaker A: And then we want to keep, basically track the swap data. So we will have the amount of token zero and token one that the user transferred to the pool and the amount of token zero, token one that the user transferred out of the pool. The sender address is basically the person that did that perform the swap. And the two address is the person that we received the amount of money after the swap. Usually it's the same person, but not always. And then we want to have something, basically that all these numbers will be like tricky. And we want to basically compute the price of token zero in terms of token one and the price of token one in terms of token zero.
00:09:21.582 - 00:10:00.354, Speaker A: So we want to say, what's the price of ETh in USDC or the price of USDC in ETH? And we can compute that value for each swap. And finally, the last value is something that's needed by the API bar integration, which is basically what we call a cursor, which is nothing more than the block number. And we use that to invalidate data when there's a chain reorganization. But that's a bit more advanced topic. So we can sort of ignore that for now and just say you need to have this cursor column in your table. So now we do superbase midration. Okay, and now superbase apply my midration.
00:10:00.354 - 00:10:03.790, Speaker A: And now we have a table which is empty in our database.
00:10:05.570 - 00:10:28.466, Speaker B: So Francesco, you mentioned some of the parameters that you wrote in the SQL file, right? Is there a place where people can go and look at the parameters? Like, are the name of the parameters, do they indicate somewhere? Is it there on scan? Or parameters that are sent to the contract or where do we get these names?
00:10:28.578 - 00:11:00.910, Speaker A: That's a good point. So the only parameters that required is this underscore cursor. The other parameters we can decide the name. Again, the idea of, is that you are in control of your data, right? We don't want to tell you how to ship your data because you are the expert on your own application. Right. As we see later in the Transform step we are going to take the raw plots and data and reshape it so it has exactly this shape. But if your application is doing something else, it's not swaps, it's maybe your routine like Ztland or your routine.
00:11:00.910 - 00:11:07.330, Speaker A: You can create tables that have exactly the shape that is needed for your application. I think that's very powerful.
00:11:07.670 - 00:11:10.020, Speaker B: Okay, interesting. Thank you.
00:11:10.870 - 00:11:54.658, Speaker A: Okay, so now we basically did all the setup that's required and now we can start streaming on send data. We could send data directly to the database by my opinion, right. I always need some time to debug what I'm doing. And so for that we use the webhook integration. And so on my computer I started like a small HTTP server, which is nothing more than a docker image that we simply print out the payload I'm sending to the server. So that way we can debug what we are doing in the filter and transform phase. So let me go back here.
00:11:54.658 - 00:12:26.780, Speaker A: And now I want to run the web book integration. So the webhook integration takes some arguments. The first argument is the stream URL, which is the API bar, DNA stream. In this case we are using mainnet stream. So we have mainnet data, we want to streamnet data. And on Apara we need to authenticate using the token that we generate from the interface. So if you, oops, now you have my API key, I will have to delete it later.
00:12:26.780 - 00:12:57.334, Speaker A: So if you go on API bar, you will get basically API key. You can create your indexer, create your API key. You need to copy that. So we can track basically how much data you're using to avoid as someone streams too much data and sort of ruins the system for everyone. So that way you know if your indexer is working or not. We basically set up that we tell the web to authorize that and then we can tell which block we want to start streaming data we can do. So.
00:12:57.334 - 00:13:32.430, Speaker A: For example, I'm looking at a pool on start scan and I see it's been deployed at this transaction and this transaction is a block 5000. So I want to start streaming data from that block. Okay, so it's block 5000. So I would send here so that we don't miss any data. And then I say, okay, it's a web book. I want to post the data from API bar, from onchain to my webhook running locally on my computer. And the next step is that we need to set up a filter.
00:13:32.430 - 00:14:20.666, Speaker A: So we create a new folder here where we will put all Apibara stuff. And we create Apibara, we create a filter which is a JSON file that basically tells Apibara what type of data we are interested. If you remember from our presentation, we are basically doing this step here. We go from having all on chain data, we want to filter this data down to have only the ten k swap swaps. And we use that with say header. So we say week true, we miss that. We want to receive a SG block header only if any other type of data is generated.
00:14:20.666 - 00:15:20.114, Speaker A: And then we want events. So we go to the Tenjiswap contract, we want all swap events generated by ten j swap. And so we go back to Starscan and here we say, okay, the contract address is this one, we copy it, paste it there. And now we go for swap events here, the open one, right? And the reality is that startnet doesn't understand about swap events but has the ash of the event name. And on startum is pretty nice because you can just drop it a value and that's the event key. So every event has this key is a swap event from the ten k swap pool. Okay, so now we go back to the command line here.
00:15:20.114 - 00:15:55.822, Speaker A: We change the argument to filter to rp bar filter. Okay, and then I made a mistake in my, okay, so it's not key, it's keys. There was a mistake on my part. Okay, now we getting a lot of data and it's streaming very fast, so it's pretty annoying to look at that. And now it's printing a lot. Okay, but we see the data we received, that's the raw data that API bar is sending to our integration. And we see for each event we have the transaction that generated the event.
00:15:55.822 - 00:16:28.038, Speaker A: So we have which contract sent the transaction, which method code, and basically the code data on starter, you can have multiple calls in one transaction. And here we can have all of them. We have the transaction received. So the data that the sequencer generated after accepting the transaction. So we can see, for example how much fees the user of that transaction paid. We can get the transaction hash. So we can even save that.
00:16:28.038 - 00:17:11.894, Speaker A: So it's easier to link to australian paid reviser paid. And then we see all events that were together in the transaction. And then we add the final event and we see both like the address that generated the event, which is obviously the ten days work pool, and we also see the raw data of the event, which if we go back to star scan, we can see that and we will see that this data here is the same as this data here. Obviously it's not the same event. So the data is slightly different, but the size is the same. Okay, and now we have everything. But now obviously if I don't want to insert this data in a database, it's like not in the shape I want.
00:17:11.894 - 00:17:50.690, Speaker A: It's very tricky to work with. I want to reshape this data to actually be useful and nice to use. And for that, in apar we have this concept of transformation which we have here. So now we are going to do this step where we take this data that we have now, which has this OD shape, but it's not too much data, but it's still od shape. And we want to run a piece of JavaScript code over this data to actually produce some data that is a nice shape. And so how do we do that? We create a simple JavaScript file. So we go back to Apibara, we create transform.
00:17:50.690 - 00:18:47.814, Speaker A: So the file name could be anything actually. And now let me find, so what we want to do is that is a dino runtime. So the cool thing is that we can import any JavaScript library we want. So for example, I can import these two libraries use tartnet js and VM, which is a library for EVM change. And we are going to use VM just to format basically big integer. And also we know that we are working with east and USDC, and as you maybe know, if we go back here, we see that the amount of token zero is like this huge number. But the reality is that the user didn't deposit like 200 billion eth, right? The deposits are a much lower amount.
00:18:47.814 - 00:19:37.122, Speaker A: But because on startner we cannot represent fractional numbers, so we cannot represent like 2.7. So we need to add many zeros after the fractional part to transform it into an integer. And that way we can represent very small amounts of units of, say, eth, but we represent that as an integer, not as a floating point number. So that's the number of decimal. So e says 18 decimals, USDC s six. Okay, so now we can go down to actually be like the transformation phase. So we do like, so we basically have this transform function, which is the default export of the basically JavaScript file and we receive a bus of data.
00:19:37.122 - 00:20:29.510, Speaker A: So because appy bar tries to be very efficient, it will not send you data block by block. It will send you maybe ten blocks at a time for historical data. So that way you can insert like ten blocks of data at a time into your database, which is much more efficient than going block by block. So your indexer will run faster, which means that you can presync everything much faster. I want to extract the events in that block. So now I create a new function for like under block, okay, and now we have a block which is similar to this data we have here, where you have many events and a lot of things, right? They are quite full blocks, so difficult to see. Now all we can do is that basically we know that this block will have headers and events.
00:20:29.510 - 00:21:33.626, Speaker A: So I use classic Javascript and then I do, okay, so I want to map each event which could be null, maybe want to do this map, I want the event and received not interested in the transaction. For this case I say handle event header. And now I create the under event function. And finally we can do the reshape of the data, right? And I think what's excited is that I'm just writing normal Javascript, I don't have to learn any new library. I can use exactly what I wanted. Okay, so now the next step, and the final step is to decode call data for the event. So this data, we go from all these field elements that don't really tell us anything.
00:21:33.626 - 00:22:54.660, Speaker A: We go to this nice representation of things and we go to even better, where the amounts are not like just a big integer, they're actually floating point numbers that we can understand, okay, so the way we do it is that we can say, okay, so the sender address, just the random data is the first element, right? So this one is the, it's starting this value which is the sender. Now for the other amounts it's a bit more tricky. Okay, I just pasted some code because so we say okay, we want to convert these two field elements. So I go back to bro these two field elements into a big integer and I do that using the start ns function because I can use any JavaScript library. So I say okay, so I take these two field elements and convert them to bD integer, and then I don't want to have a PD integer, I want to have like a decimal number. And so to do that I use the format unit function. So when it receives a number that looks like that, it will convert it to basically like a string, which is something like 0.2.67.
00:22:54.660 - 00:23:56.630, Speaker A: And then we use the plus before, which is a Javascript thing to convert from string to a number. And we do that for all amounts here. And finally we have the two address, which is this element here. Okay, and now that we have the numbers and we have the amount of token zero and token one that the user put inside and the amount of token zero and token one that the user got out, we can convert the price the swap traded at. And we do do that this way, right? So we say, okay, the user put basically the total amount of token one was the amount that the user put inside and got out, and the total amount of token zero was the one that user put inside or took out. And basically that it was the price the user traded at. Okay, and now we are almost finished because now we can finally return the data we want to insert in the database.
00:23:56.630 - 00:24:56.042, Speaker A: Okay, so now we go back to the SQL file, right, where we have all these columns. And you asked basically where does this name come from? Basically I had a clear in mind what type of data I wanted to put in my database and I knew that my transform step will return that, right? So you can see that I have the exchange, I have the token names and pair, and then I have this information that comes from the block. So I have the block number, block times and the transaction hash. And then I have this value that I just computed. And now basically this data and Avivar is going to basically call my Javascript function on each broadcast data is going to stream and then it's going to send this data to the webhook or to a database, to an integration we brought. So let's run it now. And of course I need to tell Apibara, okay, just use that script to transform data.
00:24:56.042 - 00:25:50.694, Speaker A: So I need to say transform equals Apibara transform JS. And hopefully I didn't make any mistake in my thing. I run it for a bit and now I go back and I see that now we go from the data was like in a very messy format to something that's very tidy, right? We have data that anyone can understand. And so for example, we see this swap here, we had like user put inside 32 USD and it dropped back 0.2 ETH and the price of ETH at that time is $1,294. Okay, now we are creating a webhook, but we said we want to analyze this data using charge CPT. And we know that chargebt is quite good at writing SQL, so why not send data to a SQL database and we can do that very easily.
00:25:50.694 - 00:26:28.390, Speaker A: By changing the integration we are going to use, we are going to use the synth postgres, which requires a bit different arguments. So we need to say how to connect to postgres. So we give it a connection string, which is what superbase tables earlier. So it should be postpress equals passpress 54322. Okay. And then we have to say table name, which is the name of the table we created earlier, which is swaps. Our table is the empty.
00:26:28.390 - 00:26:57.140, Speaker A: Now we do that and hopefully, okay, now our integration is writing data to our table. As you can see it's writing probably like a few hundred blocks per minute. And it's mostly because the servers, like servers are in the US, I'm in Europe. So it's a bit slower than what it could be, but it's pretty fast. Right. We just inserted like 43,000 events. Why? Just we were chatting, right.
00:26:57.140 - 00:27:36.590, Speaker A: As you can see, the good thing is that now we have this data and it's like in a table. And now we can analyze with SQL so we can go SQL editor. I can say, okay, how many rows do we have now? And it tells me, okay, now you have like 100,000 row. Now it's like 117,000. And this data will be Apipara will keep updating the table and when it reaches the top of the chain, it will basically keep inserting data as new blocks are produced. And if there's any chain reorganization, we'll invalidate data. So it's pretty nice.
00:27:36.590 - 00:28:18.630, Speaker A: And basically the API bar integration part is done, just streaming, inserting data, right. But now we said I don't want to write any SQL, right. Probably set count is the most I can do from SQL, right? I don't want to write anything. Let's use TPT and OpenAI to do that. Now we can basically create a new script, create a new folder, AI. And basically this script will just draw the OpenAI API to generate SQL for us. We are not going to use OpenAI actually directly.
00:28:18.630 - 00:28:46.626, Speaker A: We are going to use it through this library called Lancjain. So let's install the libraries. And to do that we have this Dino JSon file. So we could have used like node js, just. I like Dino. Okay, so here I say, okay, I want to install the Langchain package, which is this machine learning package from typescript. I want to give access to postgres.
00:28:46.626 - 00:29:09.500, Speaker A: So I install some libraries that do like postgres interactions and then I want to basically like a nice interface. So I use these two node packages to build a nice command line interface. We could build like a react front end for it, but it's too much work for a demo and it's not the important part, but it is that you can build anything you want.
00:29:11.090 - 00:29:21.120, Speaker B: Francesco, quick question. Is Langchain similar to OpenAI? Basically like it takes in a text input and tries to get.
00:29:21.570 - 00:30:00.074, Speaker A: Basically Lance is a library that wraps the OpenAI API and it makes it easier to use and there are a lot of integrations. And what we are going to use today is to agents. And the idea behind agents is that they use the Chat GPT to think about what to do, but they also have external commands to basically get more information to interact with the outside world. And we are going to see how that works soon. Okay. Okay, so I'm going to copy and paste my code and now we have everything ready. Okay, so the idea is that we have our main function.
00:30:00.074 - 00:31:00.042, Speaker A: We basically set a data source to our superbase database running locally, which has very secure credentials postgres, which is the same where we are writing data. Then we basically just copy and paste some code from the Lancet documentation. That's how to connect the database to this agent. Then we say, okay, how to connect to OpenAI. And here I had to basically set the environment to my OpenAI API key. And then we basically said the agent should be able to access a database, right? And so we use this SQL toolkit, which is a library that lance and implements that basically instructs the agent how to access data about the database. So we can get a list of tables, can get the shape of a table, and can also write and execute SQL code that then run it as the live database FNA.
00:31:00.042 - 00:31:55.150, Speaker A: We create the edge editor, which is basically the agent. Okay, so then we keep repeating a thing where we can ask anything, right. We will ask normal question and then based on the input we give to the agent, the agent will take some actions and we are going to basically print the output and also see what steps the agent took. And then we printed in a nice way so it's a bit more visual. And now we are still writing data. And I need to set up a CLI with my OpenAI API key. Want to leak my OpenAI API?
00:31:57.170 - 00:31:58.880, Speaker B: Yeah, that's a good name.
00:32:00.470 - 00:32:41.790, Speaker A: Okay, now we have here and should have my OpenAI API key project. Okay, so now we can call Dino on our program. So Dino has this concept of permissions. So for security we not allow the program to access the Internet unless I allow it to do, let's say. Okay, what was the volume in say March. So it has access to our table. It's going to call the OpenAI API, which is quite slow.
00:32:41.790 - 00:33:11.090, Speaker A: Anyone that use the API knows that it's slow and eventually will give me answer. Hopefully soon. Yeah, that's the life when working with OpenAI here is writing data. Okay, so I say the volume in March was zero. That doesn't look right. So I want to see what's happening.
00:33:11.160 - 00:33:11.780, Speaker B: Right.
00:33:13.430 - 00:33:41.340, Speaker A: So here I can see what the actions that the agent took. Okay, so the first action it took, apt list the tables that are in my database and they see the swaps table. They say, okay, and then says, I want to see the schema of this table so I can write SQL code. And I basically see the schema. They see which column. So it's very important to give good names to the column. So the agent has actually a bit more understanding of what's happening.
00:33:41.340 - 00:34:13.570, Speaker A: Okay. And I say, okay, I can query the swaps for March. But as you probably know, Op and AI was, they stopped training based on data from 2021. So it's saying March of 2021 north 2023. So we need to specify that we want may of 2023 because that's this March. And also I know that that's not how you filter based on data in SQL. So I think he made two mistakes, right.
00:34:13.570 - 00:34:59.920, Speaker A: And part of basically working with JPT and all these agents is to basically write good prompts. That's to write prompt engineering. So I'm going to ask her then the question. I say, what was the volume in March 2023? Use the date trunk function. Okay, now I tell JCPT two things. One, I want March 2023, not March 2021, just test their current year. The second is that I want them to use the date function to actually filter by the month, right? And now we get a value dilutes closer to what it is.
00:34:59.920 - 00:36:07.530, Speaker A: But there's still some issues. The reality is that he only summed the amount of token zero, so the amount of ease that people traded in reality. What I want to tell chatibility, we are not going to do it today. We just take some time to prompt, is that we also want to teach Chattvity how to convert, say, the amount of ease into USD by using, for example, the token zero price trolley that we have so that it can give us a value in USD instead of telling us just the value in eth. Okay. But I think we show that in less than like half an hour, we were able to go from zero to set up a project, keep streaming data into the database, catch up with the team we fully synced all the swaps, so I think it's like 100, I don't know, we almost synced like 1 million events while we were just writing the code for the rest and copy and paste it. So it's quite fast, right? Something you can do over and over during your day while you're developing.
00:36:07.530 - 00:36:41.474, Speaker A: And now we have a good starting point to basically be the agent that knows about on chain data. So I think that's all. And it is go. If you want to basically use appibara for your application, continue building what we started today, maybe you want to turn it into a product. You can get a free API key on apibara.com. And so we are also going to launch integration site live so that you can deploy and we run it for you. So you don't even need to run servers, right? Everything is serverless from the point of view.
00:36:41.474 - 00:36:56.920, Speaker A: And again, everything I showed you today is open source. So make sure to go on our GitHub page, put a like, see what you for contributions and also remember to follow us on Twitter. Also my account where I tweet about this data stuff and general development things.
00:36:58.830 - 00:37:36.406, Speaker B: Amazing. This was super quick and I think this has been like one of our fastest community call workshops and it was also very interesting. Francesco, thank you so much. I saw that there were about a million events through which we were able to go through get data, et cetera, and it was super fast. Basically we did it all within a couple of minutes. And that's very cool. And do you have any repositories that you could share? Like for example, of course, hiding your open AI API key.
00:37:36.406 - 00:37:46.454, Speaker B: Do you have any repositories where you can share in terms of code that people can see? How does the chat GBT, how does the superbase integration, et cetera, work?
00:37:46.572 - 00:38:07.390, Speaker A: Yeah, so I'm going to upload, we are going to have a tutorials repo where we put all these tutorials that we are going to write today in the next few days. So if you follow the API bar organization, GitHub, you will see a tutorial repo coming up very soon after this talk where you can find the code that I showed you today so that you can run it on your machine.
00:38:08.050 - 00:38:26.280, Speaker B: That's amazing. And if folks have questions on so Apibara right now, from what I understand from your initial presentation, is like open source to use like folks can, their own custom indexers, et cetera, as you mentioned. So if they have any questions, where can they reach out to?
00:38:26.730 - 00:38:45.530, Speaker A: So if it's a technical question, they can use our distort server or they can ask on like if they find parts they can open issue on GitHub, I will fix it. Or if they want to start contributing. So it's a project in brass. So it's a two way to learn brass. They can also contribute on GitHub.
00:38:46.030 - 00:39:21.000, Speaker B: Okay, that's amazing. So for those of you who are interested in contributing as an open source contributor, then you can do that using the Apibara GitHub that's mentioned over here. And yeah, for all of you who have been looking to find an intersection between analytics, blockchain and cutting edge tech like Starknet, I would definitely recommend contributing to the API Bara report. Might be a great learning experience, as well as of course working at something very new at the intersection of something very cool.
00:39:22.490 - 00:39:45.870, Speaker A: If there's any integration you want to see, there's maybe some web two project that you really like and you like as a developer too. Like for example, like superbase for example, maybe you like Firebase or other project. Just open an issue on GitHub and we can write the integration for that. So it becomes like this, right? You just run a command and you get your data in the other application without having to write much code, just a simple Javascript script.
00:39:46.850 - 00:40:08.118, Speaker B: Awesome. That's amazing. Thank you so much Francesco, thank you for your time and thank you everyone who was able to call with us today. If you have any questions, feel free to reach out. Like you can also put your questions on the YouTube livestream and we'll make sure to get you the resources, the answers that you need. Thanks everyone and have a great week ahead.
00:40:08.284 - 00:40:09.460, Speaker A: Thank you. Yeah.
