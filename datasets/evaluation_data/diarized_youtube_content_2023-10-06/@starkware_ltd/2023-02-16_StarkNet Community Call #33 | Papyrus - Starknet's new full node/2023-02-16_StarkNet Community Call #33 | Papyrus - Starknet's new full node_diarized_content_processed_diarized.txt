00:00:00.250 - 00:00:15.040, Speaker A: Stream is ready. I'm pressing go live. And we're live on YouTube also. So perfect. Let's start. So again, welcome everyone. This is Starknet's committee call for number 33.
00:00:15.040 - 00:00:45.820, Speaker A: And today we're here to talk about full nodes. So I have with me Dan. Hey, Dan, nice to see you from Starkware. And Dan is working on papyrus, which is the second or third node implementation of Starknet. How would you qualify? Like, I know there are free live, but I think papyrus is the second one that is public.
00:00:46.510 - 00:00:48.940, Speaker B: That probably would be the third.
00:00:50.110 - 00:00:57.200, Speaker A: So we have Pathfinder already, whom we talked with in a community called a while back. Is Juno public also?
00:00:58.050 - 00:01:02.990, Speaker B: Yeah, they should be. Juno is a nanomind node.
00:01:03.750 - 00:01:27.030, Speaker A: Wonderful. So Pathfinder is developed by equilibrium, Juno is developed by Nevermind, and we now have papyrus developed by Starware. So let's start. Dan, can you tell us a little bit about yourself? How long have you been at Stockware? What are projects you were involved with in the past before starting papyrus?
00:01:27.930 - 00:01:43.870, Speaker B: So I've been in stockware for about three and a half years. I started on the team that open source, part of our approval for the Ethereum foundation. We have approval for a statement of rescue hash chain.
00:01:44.610 - 00:01:47.162, Speaker A: What was it called? It was East Stark.
00:01:47.306 - 00:01:51.866, Speaker B: East Stark over and verified.
00:01:52.058 - 00:01:58.850, Speaker A: So you said you were involved in open sourcing it. You mean you didn't write it? You worked on making it open sourceable?
00:01:59.430 - 00:02:20.326, Speaker B: I was just part of the team. Part of it was new implementations. The whole rescue hash function itself, we didn't have it beforehand. So the statement and part of it was just making whatever necessary changes we need for our repository to open source.
00:02:20.518 - 00:02:24.250, Speaker A: Was it the same team who wrote the prover and the verifier?
00:02:25.790 - 00:02:33.214, Speaker B: Yeah. So did you work on the verifier? Our own proverb and verifier, and just the open source parts of it.
00:02:33.412 - 00:02:36.720, Speaker A: Did you work on the verifier? Did you work with solidity in the.
00:02:39.810 - 00:02:45.310, Speaker B: Okay, I mainly did, I would say, google parts.
00:02:46.630 - 00:02:52.194, Speaker A: Super cool. And then between this and now on Piperis, what did you work on?
00:02:52.312 - 00:03:19.900, Speaker B: I had a short term walking on some trade of time space trade off in our pooler. We wanted to run bigger traces on the same machines, so we checked. This is before we had recursion going on. So we're playing with it for a while. Then I was involved in the perpetual effort in.
00:03:22.190 - 00:03:23.130, Speaker A: Stockx.
00:03:23.650 - 00:03:34.930, Speaker B: In Stockx, yeah. So the perpetual product side. I was leading this team for a while, and now I'm leading the full node effort.
00:03:36.070 - 00:03:42.354, Speaker A: Very nice. How long have you been working on this full node effort now about six.
00:03:42.392 - 00:03:47.090, Speaker B: Months a year, give and take. Pretty amount of people at different stages.
00:03:47.250 - 00:03:49.800, Speaker A: And how many people work on papyrus now?
00:03:50.970 - 00:03:54.630, Speaker B: Currently we have four developers.
00:03:55.370 - 00:04:09.500, Speaker A: Super cool. Okay, so let's get a little bit into papyrus now. So papyrus is a full node for Starknet and it's currently still being developed. Right. It's not feature complete?
00:04:09.970 - 00:04:12.266, Speaker B: Yeah, it's very early stages.
00:04:12.378 - 00:04:15.600, Speaker A: Nice. Can you describe what it is able to do?
00:04:16.450 - 00:05:04.106, Speaker B: Currently we can sync from the fiddler gateway from Sarknet, so we treat it as a centralized trusted source. We don't do any verification, we just take everything that's granted. We support most of the get endpoints, so we can answer about question about the state blocks headers. We don't support any of the ad transactions, I guess it's county because there is only one sequencer. Supporting them would be just delegating to the gateway, to target gateway, but we don't do that. We don't have any execution support, so we cannot use call and estimate fee.
00:05:04.298 - 00:05:30.120, Speaker A: Okay, so when you say you have get functionality, basically right now papyrus allows you to explore the history of Starknet. So to get blocks, I imagine events all the RPC specification for the nodes, right. But to read the history of the blockchain, you can't yet interact with the state. You can't read the current state, is that correct?
00:05:30.490 - 00:05:36.200, Speaker B: You can read the current state, you cannot execute on the current state.
00:05:37.790 - 00:05:49.500, Speaker A: Okay, so I could technically read my balance, but I can't send a call that executes Cairo code that reads my balance, is that correct?
00:05:50.210 - 00:05:55.120, Speaker B: You can read your balance, you cannot alter your balance. For example.
00:05:55.810 - 00:06:14.174, Speaker A: Okay, really nice. So you say papyrus nodes currently query the feeder gateway. Do you use the feeder gateway get API or do you use the RPC? I don't know if the feeder gateway actually answers with RPC requests.
00:06:14.222 - 00:06:41.920, Speaker B: No, it has its own API. This is part of the reason why we moved to JSON RPC endpoints that all of the nodes have to implement. Actually at Starcore, part of the reason that we have it is because we want to move from one API to a better API. So I hope that in the near future users would stop interacting with the fitter and start interacting with whatever node they choose.
00:06:42.850 - 00:06:58.862, Speaker A: Okay, super cool. All right, so you have the ability to reconstruct history and to reconstruct the current state. So what's next? What's the sort of roadmap and timeline.
00:06:58.926 - 00:07:22.460, Speaker B: For papyrus, just for a bit more context? We're implementing it in rust Pathfinder. Also implementing in rust Juno and go. We are working together on defining the peer to peer spec right now. Actually with whoever wants to contribute for this.
00:07:23.950 - 00:07:26.250, Speaker A: Where should they go if they want to contribute?
00:07:27.790 - 00:07:43.826, Speaker B: The spec is on a public repo. I can share it later. I think we have at least few channels designated for that. You'll have to ask Arielle or Tom or someone on the product for that.
00:07:44.008 - 00:07:50.980, Speaker A: Sure seems like a big thing to be working on.
00:07:51.510 - 00:08:20.220, Speaker B: Definitely. And it also means that other than all the challenges of a distributed system now, we cannot think of the sources trusted anymore. So we have to do some verifications. And there are great question about finality. We want to support not only l two concern books, but also l one finality. When you switch to p two p and you have long history.
00:08:21.410 - 00:08:51.880, Speaker A: Interesting wait challenges. So there's a lot to unpack here. I'm curious. So you're mentioning PTP. So are you reinventing the PTP interactions in the network from scratch, or are you planning on reusing some libraries? Because if Juno is in go and papyrus is in rust, I'm guessing there are some. I mean, are there some libraries that allow you to have PTP out of the box?
00:08:53.290 - 00:09:29.700, Speaker B: There are. And at first stage we gather all the great mind that we could think of and try to decide on. Do we want to use a specific library? Do we want to design our own p two p platform, like the whole stack? And we decided currently to go with lib p two p. It has implementation several languages, including graphs and go. So this is the current approach. Still, we have to design the protocol itself. We are trying not to invent anything new first.
00:09:30.870 - 00:09:40.982, Speaker A: So when you say to define the protocol, you mean the kind of request that node are answering to, the kind of request they're making to each other, this kind of stuff.
00:09:41.036 - 00:09:44.630, Speaker B: Yeah, exactly how two nodes are going to communicate.
00:09:45.290 - 00:09:59.580, Speaker A: Okay, so liPTP will give you the tooling for machines to connect with each other, but you still have to define exactly what they're saying to each other. In these kind of protocols. Where do you draw inspiration from?
00:10:01.230 - 00:10:18.674, Speaker B: You have several implementations already, and they don't have to use p to p in order to get inspiration from them. So it can be from guests. Turbogas Aragon, all of the full node and consensus layers that you can think of.
00:10:18.792 - 00:10:26.550, Speaker A: But all these full node and consensus player, if they're from Ethereum, aren't they using the same protocol?
00:10:29.130 - 00:11:25.474, Speaker B: So there are different levels. You can think the protocol is how the pill would actually what messages they exchange. But there are other layers, interesting layers that you have to like what address, how to model it within the node, or even when you do, for example, block propagation sequence are announced, they have a new block. What then? Do you only let the node know that there is a new block, maybe with its number. Do you provide the header, do you provide some of your peers with the metadata itself? I don't know the transaction so they won't have to query and everyone else. And there would be a lot of noise during that time. How to do discovery even now, Ethereum protocols, they evolve.
00:11:25.474 - 00:11:32.746, Speaker B: You used to have disk before, now you have disk before. And there are interesting stuff that you can take from that.
00:11:32.928 - 00:11:37.020, Speaker A: So discovery is about finding new nodes in the network, is that Correct?
00:11:37.870 - 00:11:56.558, Speaker B: Yeah. When a node is spin up, it has to find its tiers. You can think of it, I guess as a bootstrapping someone that you have to know and be able to connect and through him to know other nodes.
00:11:56.654 - 00:12:35.694, Speaker A: Then he will tell you other people you can connect to. Okay, cool. Wait, so you took the example of block propagation, which is really interesting, and you mentioned also like modeling inside the node. So you mean like this is not protocol level stuff, it's on your implementation level thing, I imagine, right. Or maybe I'm not getting this correctly, but when you say modalization of this means like modularization of p two p from the node itself, like how does he register all the calls he made and the different interactions he has in.
00:12:35.732 - 00:13:35.680, Speaker B: Both within the node and outside the node? I would expect that every node that gets something pushed to him, like block propagation, he wouldn't just push it back to the network. Right? Maybe it's spam and you don't want to have congestion. So after some minor validation, I would expect him to be nine nodes would repropagate this data so it won't disappear. This can be part of the spec, but within the node, what do you do with it? Are you trying to build the longest chain you can, even with the data farming the future. For example, if I just spend a node now I'm at block 1000, I just got a block propagation message about block 100k. What do I do with it? Do I want to save it? Stuff like that.
00:13:36.630 - 00:14:54.410, Speaker A: Interesting. You mentioned also validation, because right now every block you're receiving, you know you're receiving it from a trusted source, so you just take it as granted. Can you expand a bit on that? Because it sounds like a hard problem, because, well, basically taking a step back, I think a really interesting question is what do we assume a starknet full node to be in terms of specs? Right. Like what kind of power it should have, because to a certain extent, if you're an ethereum node, you're a starknet node, because you're validating the consensus and you're validating the validity of starknet, but with a time granularity, that is every time there's a proof on ethereum, which is not ideal, but still you validate starknet. Right? And that's nice. You can do it on ethereum node. Now, if you want a full node to be able to track each new block as it comes out, do you need every block to re execute all the content of a single block or not? Does this mean that a full node should have the same kind of capacity as a sequencer?
00:14:54.830 - 00:15:10.430, Speaker B: It's a great question, actually, and it's funny because I mentioned that we don't support execution, right? But we do support state sync. So it actually means that we can sync without executing any transaction.
00:15:13.090 - 00:15:22.020, Speaker A: So you support state sync if the sequencer gives you a here's the block and here's the state at the end of the block, you can sync your state. Is that correct?
00:15:22.390 - 00:15:33.720, Speaker B: Yeah, but this is not what the sequencer gives us, right? It gives us the state diff. What were the changes between executive states? And we apply them to the state.
00:15:34.570 - 00:15:39.034, Speaker A: All right, so then validation becomes going forward.
00:15:39.232 - 00:16:14.500, Speaker B: One way that we could have validated is re executing the transaction and see that we get the same state diff. Right. Same difference between consecutive standards. Another way is, can we actually verify the state diff object itself? For example, if we would have a commitment of the state diff on the block header, then it's enough that we validated the block header to validate the state diff, and in turn means that we can sync without executing anything.
00:16:15.690 - 00:16:23.960, Speaker A: But wait, is there a gain to committing to the state diff, whereas committing just to the state, to the full state?
00:16:25.770 - 00:16:50.640, Speaker B: There are several aspects of it. One of them is data availability. We have the state of on chain leds right now. So the same sync mechanism that we do today with the sequencer, with the fiddle gateway, we can do the same with l one with Ethereum without having to speak with anyone else.
00:16:54.550 - 00:17:17.190, Speaker A: I'm not sure I'm following. Putting l one aside, which is also a really interesting topic, you're saying that when you receive a new block, you need to verify that it's valid. You don't need to re execute all the transactions to check that they're valid, because right now papyrus is designed to. Do you say papyrus or papyrus?
00:17:18.430 - 00:17:20.154, Speaker B: I'm saying papyrus, I don't know.
00:17:20.192 - 00:18:00.058, Speaker A: Actually it sounds frencher. So let's say papyrus is better. Okay, so when you're running papyrus, it's meant to track state diff. So it updates states. So the question becomes, okay, when you receive a new block, how do you know that the block is valid? And you're saying, well, you would have to check consensus to make sure that the person who is giving you the block is allowed to give you this. And then you need to make sure that the state diff is giving to you is correct. So maybe they should commit to state diffs.
00:18:00.058 - 00:18:03.880, Speaker A: But I don't follow the link with data availability here.
00:18:05.130 - 00:19:09.210, Speaker B: Yeah, I was missing several aspects of it. But we have different levels of finality, right? Whatever is on Ethereum, this is our source of tool. For example, even if we would have a problem with our l two consensus and someone would be able to test it somehow and provide a new proof on chain, that would be the source of tool. And today we have all of the state diff data on chain, which means that I can sync the state from Ethereum. That's one thing. Let's see now, a different level of finality is for example everything that is not yet state update on Ethereum, right? Because we expect the first block from the sequencer, they only live on l two. And for that we would have to validate the l two consensus mechanism.
00:19:09.210 - 00:20:24.980, Speaker B: It's still not defined, it's in the process. I don't know if you great ideas there, but there will be some mechanism for l to contend this validation. Once you've done that, so you know that the header, you have the block header is the right block header and then you can start verifying different pieces of information with respect to that header. For example, so you get a bunch of transactions from one of your peers is claiming that this is the transaction of this header. So you have to verify it. So you some commitment on the header itself and you can verify it with respect to the header doing this for state this means that you don't have to execute in order to maintain state. Still, I think that I would assume that most of the nodes, if not all of them would support execution as well, because the ability to call transactions is something of great importance for sure.
00:20:25.450 - 00:21:07.922, Speaker A: But it does seem alluring to just be able to sync with state diffs because it will allow you to track the network with a node that is simpler than with a node that is less powerful than a node that is supposed to reexecute everything. You having to reexecute everything means you basically need to run a sequencer. If you're able to use just state diffs, then you can have a lower power machine. You still need a machine that is able to hold all the state data though. But that's a different topic. Okay, fascinating stuff. I think I was really seeing p two p as well.
00:21:07.922 - 00:21:36.060, Speaker A: P two p is done on over networks. Why don't we just take something and plug it in and we're done? It sounds a bit more complex than that. Let's say it like that. Okay, cool. One thing I keep hearing about papyrus is that its database is very fast and that there's a huge performance improvement there or something. Can you give us a bit of context on this?
00:21:36.430 - 00:22:46.240, Speaker B: Yeah, sure. When you say improvement, you have to have some base level that you compare it with. So we use key value database. And the way we index data is that we just specifically for the state, for whatever the state of each contract. So we store each contract states as key value, plain key value, for example, in contrast to how we do it in other systems, in stocknet, for example, where we keep it in a Patricia Merkel tree with the data in the leaf of the tree, very similar to how it's done in Ethereum. In turn, it means that whenever we query for a value of the storage of contract, we get it in constant time. I mean, we just have to query the database once and not reverse the whole path from the tree roots to its list.
00:22:46.240 - 00:23:02.294, Speaker B: Other than that, we do some very interesting indexing of data that allows us fast quelling and traversing over data. For example, in order sure to unpack the first thing.
00:23:02.332 - 00:23:18.140, Speaker A: If I understand correctly, you're saying that the current sequencer stores data in a way that makes it easier to reconstruct the patricia tree that is committed to and that is proven to, is that correct?
00:23:19.070 - 00:23:34.258, Speaker B: Not in a way that it's easier, but coupled as part of the Patricia tree, which has great value. Right. Because you always have the commitment over the data. That's a great feature, but it's a.
00:23:34.264 - 00:23:40.740, Speaker A: Great feature as long as you need to prove it. If you need to exploit it, it's less useful. Is that correct?
00:23:41.990 - 00:23:58.174, Speaker B: That's right. And even today in Starquil, we decouple these two stages where you first build the lifts of the tree or the state lifts, and on the different part of the pipeline we calculate the patricia tree.
00:23:58.322 - 00:24:33.380, Speaker A: Interesting. So in other terms, it used to be that the state was, let's say, built around consensus and built around the chain. And right now you're saying that for papiris, the state is built around execution and the state of the actual state. Interesting. So it's more like it's more applied oriented. Interesting. And how hard is it to reconstruct the state commitment from the way it is modeled currently? Is it easy to do so, or can you still have the benefit of.
00:24:34.890 - 00:24:42.920, Speaker B: So it's easy to do so, but very inefficient because calculating the whole tree would take hours.
00:24:45.370 - 00:24:57.210, Speaker A: So how can you verify that you're sync with the network then? How can you verify? How do you make sure that your database doesn't lose sync with the rest of the network?
00:24:59.150 - 00:25:21.090, Speaker B: That's a great question. As long as I verified each part of the process, I just have trust in the final result. If I know that I applied the right state to the right state, up to bugs, I'm fine. But then again, I can have bugs in my Patricia tree implementation.
00:25:23.350 - 00:25:35.960, Speaker A: Interesting. Okay, I was going to say it's a leap of faith. No, it's basically saying, yeah, your program may have bug, but programs may have bug in general.
00:25:36.810 - 00:25:42.920, Speaker B: It's two different approaches, provable approaches to gain the same result.
00:25:44.430 - 00:26:10.580, Speaker A: And is there a disadvantage to, I don't know, committing or having some kind of like this database that you're building in papyrus will be the same for every node, right? Couldn't you just get a commitment to that specific database for Papirius node and share this within them to say, hey, this is where I am, are you there too? That makes sense.
00:26:11.110 - 00:27:03.780, Speaker B: It makes sense and it actually makes more sense in context of fast thinking, right. If I was able to persuade someone else that this is the right database for a valid database, that would be great, right? That's a question that we think of actually of how to do fast thinking. And we have several approaches. I can do a haunt all talk only on this topic, but I guess that the main question there are how can you prove that this is the right database? Assuming that in order to make this proof you have to pay some resources, how are you going to be paid for it? Why would you want to do that?
00:27:05.030 - 00:27:34.880, Speaker A: Oh, I see what you're saying. Because I was going to say, because it basically means having a consensus for papyrus node and you're saying, well, no, I could get the state of my database in Papiris and generate a proof that this is the correct database for that block. Super cool process. You said that would take hours, then you need to spend even more hours to prove it. And if you're not paid for that, is there a point?
00:27:35.810 - 00:27:40.880, Speaker B: Yeah, we have several ways to deal with it.
00:27:43.090 - 00:28:01.480, Speaker A: You could make a marketplace for proof. Like, hey, you want to sync your papyrus node in like ten days? Yeah, sure you can. Do you want to sync it in 2 hours by using a proof? Just need to pay this bond and then somebody can post a proof and get the bond if the proof is correct anyway.
00:28:02.650 - 00:28:22.554, Speaker B: No, I think we would want the system itself to. I don't want to have users that pay to get better. It's like saying, okay, if you have better hardware, you get better results.
00:28:22.682 - 00:28:23.278, Speaker A: Fair enough.
00:28:23.364 - 00:28:25.280, Speaker B: This is not the way we want to go.
00:28:27.250 - 00:29:11.966, Speaker A: The cool thing with proofs though is that you only need to do them once and at some point it kind of becomes a public good. Right? I mean, in the sense that if you prove the database at a given block, then it's proven with quotes, unquote, it's proven forever in the sense that you can always reuse it. So I don't know. I think it's interesting because there's a lot of. Proving introduces a lot this dimension of, oh, if somebody does the work once, everybody will benefit from it. I think it's something we like. It's a paradigm we can leverage a lot.
00:29:11.966 - 00:29:18.878, Speaker A: But it still remains unclear how you pay for people to do this kind of work. Does that make sense?
00:29:19.044 - 00:29:36.820, Speaker B: Yeah. And you have to do this over time perpetually, right? It's not enough to do it once. You have to do it once in a while. And we have stocks, so we can use tags for that as well.
00:29:38.310 - 00:30:06.960, Speaker A: They have to do it once in a while. But for example, if there is 2 million block, and I prove to you the database at block 1 million and I publish it somewhere, it is always going to be available for everyone, right? So yeah, obviously it's better if somebody then later does it at a million .5 and then at 2 million. But you will always have the benefit of having 1 million saved for everyone, right? People will always be able to checkpoint this.
00:30:08.850 - 00:30:10.000, Speaker B: Yeah. Okay.
00:30:12.770 - 00:30:35.190, Speaker A: So there is a question on YouTube. Charles is saying, will Paprika's node be involved into starknet consensus together with the upcoming new sequencer in the future? So I guess the question is, what's the interaction and what's the connection with Papiris node and the sequencer? How do these two intersect?
00:30:35.530 - 00:30:55.870, Speaker B: It's still being defined. Currently. At first milestone, we're going to use the Papiris storage library in the sequencer only the storage library, not as a full node and not with syncing capabilities. And this is just for faster querying for the sequencer.
00:30:56.450 - 00:31:02.538, Speaker A: So what kind of query? When people make get request to the sequencer or when people make calls?
00:31:02.634 - 00:31:56.490, Speaker B: No, now I'm talking the sequencer as only the part that actually is doing the sequencing, giving transaction to apply applying them to the state. So I think that the name of this component today is called the blockifier. It's also open source currently. And the blockifier gets the transaction from another part of the sequencer and he has to run each transaction on the state and get the state is. So in order to run a transaction you need to know what is the current state specifically it's like the endpoint of get storage at. But instead of querying the full node through its JSON RPC, the sequencer will just have the storage as a component in the source code.
00:31:56.660 - 00:32:18.438, Speaker A: Isn't it like a risk? You mentioned that basically you have to trust your processes that the database of papyrus was processed correctly. If you give access to another process, isn't there a risk of the sequencer mismanaging the database of the node?
00:32:18.534 - 00:33:07.990, Speaker B: Yeah, there is of course, but it's a different mode of operation. In this mode of operation the only source of data for the Puru storage is the sequencer. You don't sync it elsewhere. So the source of truth is the Starknet sequencer. And the Starknet sequencer has another database which it's been used today, currently. And this database is going to be the source of truth. So at the beginning of each execution context or each block, the sequencer is going to make sure that the two databases are leveled and if needed it will use the current database, I mean the source of truth, to sync the Papiru storage.
00:33:07.990 - 00:33:11.382, Speaker B: But wait, we're going to mitigate this.
00:33:11.516 - 00:34:01.754, Speaker A: But in this case, currently on Starknet, when I send a transaction and there's a block that is being built, I send a transaction, it is applied to the state, the transaction becomes pending and eventually it becomes validated on l two. So in that case I guess that the storage of the sequencer is modified before the end of the block. So now you describe Papiris as being able to get state diffs and apply state diff on block by block basis. So it would mean that at this point the sequencer would, in other words you would give the sequencer the ability to modify Papira's database. And then at the end of each block you verify that everything went correctly. Is that it? Yeah.
00:34:01.792 - 00:34:27.970, Speaker B: At the beginning of each block you verify that previous block went correctly. And then the sequencer is the owner of the Papiro storage. Again, it's just the storage library. It has no execution, no test, no driver whatsoever. The owner of the database is the sequencer. He's the only entity that writes to this database.
00:34:29.350 - 00:34:43.160, Speaker A: All right, well, in that case, is it still a papyrus node or does this mean that just the sequencer will reuse this specific components of papyrus and reuse it to maintain its own state? Exactly.
00:34:44.970 - 00:34:54.358, Speaker B: Not a node. That's just the first milestone, internal milestone that we do in order to get better efficiency for the sequencer.
00:34:54.534 - 00:35:14.020, Speaker A: Do you think it makes sense to keep. So you're saying that basically these two are going to merge. What you're describing seems more like a merge, more than two components living side by side. Do you think it's like eventually they will merge or they will really live side by side?
00:35:15.030 - 00:35:53.120, Speaker B: Eventually they will interact, but not like this. I mean, the sequencer would have the ability to query states using a papyrus node. So not like syncing the states wouldn't be done by the sequencer itself. It's done by the node. And at some point the sequencer will make new batch new blocks, write them to the storage, and give the papyrus node the ability to sync once again.
00:35:55.410 - 00:36:33.434, Speaker A: Okay, interesting. Okay, so from what I understand, papyrus is really meant to be a production node that is meant to operate a sequencer, something that handles a lot of value. Can you give us globally, are the free nodes for starknet made for different categories of users or different categories of companies or players? Or do they all target the same thing? Like who should run a papyrus node versus a Juno node versus a pathfinder node? Is this defined or not really?
00:36:33.632 - 00:36:59.300, Speaker B: It's not really defined, but naturally this is an internal stock or implementation. And we have the stock, well, sequencer in mind. In few months or years, you would have different sequencers and different nodes. And there is no prioritizing this or any other.
00:37:00.630 - 00:37:17.750, Speaker A: Okay, cool. Interesting. All right. Thank you for all these details. It's really interesting. Are there other topics you're particularly interested in when developing this node or particular topics where you think, oh, here's a cool thing, or an interesting topic?
00:37:18.590 - 00:37:24.540, Speaker B: Well, actually, for me personally, it's the first time I'm involved in an open source project.
00:37:24.910 - 00:37:25.658, Speaker A: Oh, nice.
00:37:25.744 - 00:37:40.030, Speaker B: I've never had the opportunity to do this beforehand, and it raises several challenges. I'm not used to manage projects outdoors. It turns a string. I'm learning.
00:37:40.100 - 00:37:49.700, Speaker A: Can you expand on that? Yeah, for example, what's different? I think people contribute, I think people open issues.
00:37:51.270 - 00:38:04.870, Speaker B: I don't know personally how to manage Opensoft project. Right. I don't do it over GitHub and use the issues. It's like technical stuff that I need to learn how to use. How do you manage your projectivity.
00:38:06.810 - 00:38:07.174, Speaker A: To.
00:38:07.212 - 00:38:33.790, Speaker B: Support and have them interested in it? And we are trying to define the workflow at first, how we can define good tasks for external contributors. We just a few hours ago published a few of these tasks. I think we already have volunteers for note.
00:38:35.430 - 00:39:07.050, Speaker A: Okay. So basically saying, hey, here's something you can work on as an external person is already something that is challenging. You're right, because when you work in a small team, especially in a team of four, everybody knows what is the global objective. And you talk often a lot when it's open source and anyone can contribute, you don't manipulate information the same way. Like, you have to write stuff, you have to describe the issues link to make sure that the person on the other end understand what you want them to do and then makes it correctly.
00:39:07.710 - 00:39:31.026, Speaker B: Yeah. And I don't want just to have boring tests. I want to have tests that involve design and architecture and interesting stuff that people can actually contribute. And it even make things more challenging. When I do design with my team, we all sit together and we have better communication. Right?
00:39:31.208 - 00:39:47.270, Speaker A: Yeah. Do you plan on having any kind of. So in this process of opening up papyrus, do you have plans on making, I don't know, calls or where should people go to learn more on how to get involved with papyrus?
00:39:48.110 - 00:40:43.418, Speaker B: So we try to address it in the repo itself as a contribution part. It's still not perfect and it's work in progress. At first stages, we will publish some issues on the repo and I'm not sure where Abdul, if it's using discord or telegram or I can find out about it. We're trying to listen to people from the community whenever they think that we need to do something. So to prioritize this and to offer other community members the opportunity to do that, super cool. At later stages, we would actually try to shift management open source as well and to have a very clear milestone out there in the public. We're still not there.
00:40:43.584 - 00:40:55.040, Speaker A: Do you see eventually starquare not maintaining papyrus anymore and it's being maintained by the community? Or is that not really the plan? Or does this seem too far fetched so far?
00:40:57.410 - 00:41:11.300, Speaker B: I wouldn't say far fetched, but far down the road, if you ask me. What is my goal? That would be my goal. I'm not sure we would achieve this, but this is what I strive for.
00:41:11.990 - 00:41:35.526, Speaker A: Super cool. All right, Dan, I think I have covered most of the questions I had. I'm going to check real quick on YouTube and on Twitter. If there are some questions. I don't see any. So I'm guessing we were super clear and people learned everything they wanted to learn. If you want to say, ask some questions, don't hesitate.
00:41:35.526 - 00:41:55.634, Speaker A: Send them on YouTube or on Twitter. We'll be more than happy to answer these. All right, let's wait a couple of minutes to see if other people want to ask questions. Let's end with another question. So were you at the conference software session a week ago? A week and a half ago.
00:41:55.752 - 00:41:56.178, Speaker B: Cool.
00:41:56.264 - 00:42:00.900, Speaker A: So what was something that surprised you there in a good way?
00:42:03.270 - 00:42:11.222, Speaker B: First of all, I know this venue. It's a great theater here in Israel, and I really enjoyed the venue itself.
00:42:11.356 - 00:42:12.214, Speaker A: It was beautiful.
00:42:12.332 - 00:42:52.580, Speaker B: Interesting for me, yeah. And I tried to go to as many talks and sessions as I could and tried to talk with people outside as well, at least those that are involved in full node and the state of out. There are different kind of talks from all variety of aspects. And I found it was not very homogeneous and I liked it. Different people can find interesting stuff there.
00:42:52.950 - 00:43:07.494, Speaker A: Nice. Super cool. Understood. So the diversity you enjoyed. Cool. All right, there were no more questions, so let's stop it here. Dan, thank you again for your time.
00:43:07.692 - 00:43:08.986, Speaker B: Thank you for having me.
00:43:09.088 - 00:43:47.240, Speaker A: No, please. It was really interesting. And to the people listening, when are the next community call? So next Tuesday, 21 February, we're going to get an online workshop led by community member Danilo, who is part of Kakarot, and he will walk you through how you can contribute to Kakarot. He prepared a bunch of issues for you to get started on Kakarot and basically be able to contribute in an easy way. It's going to be fun. You don't want to miss it. Then the following week, on February 20, eigth, we're going to get ashtag to present their product on the community call.
00:43:47.240 - 00:44:28.110, Speaker A: Following week, March 7, actually, March eigth. This one is going to be on a Wednesday. We're going to get Kineret from Starquare presenting sharp. So that's also going to be a real interesting call where I get to ask dumb questions to people who are way more tech savvy than I am. That's going to be fun. And we'll get also another workshop on March 14, the week after that, which will be led this time by Eroditus. I'm going to share soon a calendar where you'll be able to follow all those events and in the meanwhile, stay tuned on Twitter Discord everywhere.
00:44:28.110 - 00:44:38.470, Speaker A: All right, again, thank you to Dan for being here today. And see you soon, everyone. Bye.
