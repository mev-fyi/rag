00:00:01.200 - 00:00:32.213, Speaker A: Thanks. Okay, so I'm going to talk about the timer isn't. So how do I know how much time I have? Good. Okay, I'll just. You're stuck. I'm going to talk about this beautiful result of Ulrich Habock from Polygon Labs and David Levitt and Shahra Papini from Starquare. But before I get to it, I'm actually going to talk about a bunch of other things.
00:00:32.213 - 00:01:21.055, Speaker A: And this will probably be the least algebraic and least mathy talk you've seen so far. One of the reasons is that I've over the years veered a little bit further from math. So the less time I spend on it, the more I'll be able to fool you that I understand it in a deep enough way. And I just want to comment generally that like math is one of those things, right? There's this famous saying that, you know, you never quite understand math. You can get used to it and I really empathize with that. Even those results that I obtained in my career, I can't really say that I understand them on some very deep level. Just to give one example, you know, we talk about polynomials and we all view them as these squiggly lines usually do like three or four times.
00:01:21.055 - 00:02:12.849, Speaker A: But of course we're working usually over these finite fields, so they have no such structure. And even. And like what are these mysterious objects anyways? And why do they show up and solve the problems they solve? There's something utterly mysterious and magical about it. So I'm going to first of all remind those of you, because I'm assuming some of you are not necessarily versed in blockchains and roll ups and so on. So why, why are we even solving these problems? Or a conference done one day before, like the science of blockchain conference, you know, dealing with algebra. Then I'll say a little bit about how the circle start result came about, at least from our point of view, like why was this motivated, was motivated by questions of efficiency. And then I'll try to explain a little bit about what is circle start.
00:02:12.849 - 00:02:42.927, Speaker A: And then with two questions, I have 30 minutes. Okay, cool. I thought 25. Okay, so I'll start with like why Stark. And a little bit about scaling and how proofs go very well with blockchains. So both blockchains and proofs are related to integrity, which is beautifully. Has been beautifully defined by C.S.
00:02:42.927 - 00:03:57.707, Speaker A: lewis as doing the right thing even when no one is watching. And really blockchains offer or this, you can think of them as establishing this sort of Universal or global PC. PC in the sense of like a personal computer, some reasonably strong but not too strong computer that is extremely trusted and has just immense integrity, which is why we entrust with it. Well, as of two days ago, you can say you entrusted with it whatever, over a trillion and a half dollars. As of this morning, it's, I don't know, like half a trillion dollars but, or whatever it is, but it's still a lot of, a lot of value is entrusted with this like, you know, set of technologies, this one PC. And how is it achieved? It is achieved by a absolutely magical, you know, another thing I don't quite understand, but I got used to, or we got used to the world got used to combination of transparency, distributed computation and some game theoretic incentives that of course Satoshi Nakamoto first unleashed. Okay, so we get this like PC, this weak computer that we trust, and we would like put it to work on a lot of things.
00:03:57.707 - 00:04:28.407, Speaker A: And the question now is, how do we scale it? Because it turns out that you can't just replace this PC easily under the same principles with, let's say, a supercomputer. It won't work. It would lose integrity. Part of, of the way it works is like everyone is supposed to use their own PCs in order to track it. So that won't work. And this is where cryptographic proofs come in. And there's this prophetic line from one of the early works, a very important work that talks about scaling computation or scaling integrity.
00:04:28.407 - 00:05:22.797, Speaker A: So I'll read it. This is the work of Babai, Fortnow, Levin and Segedi, which says in this setup, if you have a single reliable PC, it can monitor the operation of a herd of supercomputers working with possibly extremely powerful but unreliable software and untested hardware. So basically, a little bit like Jimenez said, give me a point that I could lift or move the earth. If you have one reliable weak computer using some more math and cryptography, you can actually use it to put in check some immense amount of computation that you have no assumptions about its integrity. And really this is the connection of blockchains to cryptographic proofs. So I mean, there are like two very different methods that solve the same problem. So they work together really well.
00:05:22.797 - 00:06:24.625, Speaker A: And here's a very abridged history. So ZK has been around for 40 years almost. I'll mention I first heard this from Madhu when I was doing my postdoc with him. He said about Reed Solomon codes that it took about, I think, 30 years from their invention to when they were first put in use, like on CDs. And it's curious that the first general purpose usage of zero knowledge, meaning general purpose for general computation, was as a product, as an end user product was in the Zcash protocol really like 31 years later and PCP techniques then used for which also came sort of the late 90s, sorry, the early 90s, put to use as a scalability solution by Starkware. This is what we were among the first to do in 2018. Now what we've done, Starkware, we've demonstrated to the world that this particular set of technologies is actually really, really good at solving the problem of scale.
00:06:24.625 - 00:07:03.535, Speaker A: And by now it's been adopted by almost all of the scaling solutions that use these validity proofs on top of blockchain. So three Polygon teams, RISC0, Succinct, Zksync and US, they're all using starks. And the power, the force that really showed it is this. For us at least it's called this stone prover. That's been around for a while, which is how we're going to get the circle start. So we've settled a lot of transactions, saved a lot of value for customers using this technology based on cryptographic proofs. And I just want to explain how it roughly works.
00:07:03.535 - 00:07:53.167, Speaker A: So if you have your chain, which is this very trusted computer, but it's running very, very slowly, so it costs a lot to put computation on it. Well, you could maintain a state that is much greater and let's say it could also be captured just by a hash of the most latest state of accounts. And what you do is you get your transactions actually off chain, they never appear on chain, and you sequence them into blocks. And then what you do is you run these blocks, you generate proofs for them, and the proofs are very succinct in their verification time and resources that are needed. And then what you do is you send them to. You know, we have this PC that's very trusted, which is the blockchain. So what we do is we send to this PC, we send the proofs.
00:07:53.167 - 00:08:21.561, Speaker A: The proofs are much, much shorter than the amount of computation. They attest to the verifier checks that the proof is okay. And because of the math and cryptography, we know that everything really was done with integrity. So we can now update the state. And this is the way all of these systems work. So this is a brief explanation of why stark or why cryptographic proofs are important in the context of scaling blockchains. Now how did.
00:08:21.561 - 00:08:48.865, Speaker A: What is the motivation for Circle Stark, which is the stuff I'm going to tell you about in a little bit. So I told you that stone was the. Well, it stands for like Stark one. It was the first generation Prover that we used. It's already been in production for four years. So it's like one of those, you know, Boeing 747s that hauls a lot of stuff. But it, its technology is already by now four years old and its design is five to six years old.
00:08:48.865 - 00:09:38.171, Speaker A: And the biggest thing to know about it, or the one that is, I would say, if you talk about places to improve, it is constructed over a prime field that is very large. It's a 252bit prime field. And the reason for that was that if you look at the main cost that we were trying to optimize, it is the cost of verifying things on Ethereum. Ethereum is designed so that all integers are treated the same as long as they're less than 256. So you don't need to save on this important resource. And the second thing is that it also leads to simpler auxiliary cryptography. If you want to do things like elliptic curve signatures and stuff like that, it's simpler to construct them over these large prime fields.
00:09:38.171 - 00:10:31.409, Speaker A: So for these two reasons, we just started with this very large prime, even though it's not needed in the world of Starks. Another thing is that the infrastructure is by now kind of old and we know how to do things better. Which means that we said, okay, about like a year and a half ago, we said, okay, it's time to now come up with or basically build a new prover. And so much has progressed. For instance, we know how to move, just like Swastik was explaining, we know how to move beyond fields that have subgroups that are FFT friendly. There are also new arithmetization techniques that let you speed up things like log up of again of one of the authors of this. We know how to even merge together things like univariate and multivariate polynomials, the stuff that is used for some checks that Yael referred to in her talk.
00:10:31.409 - 00:11:06.331, Speaker A: So you can basically unleash like new kinds of arithmetizations. And also even just the task of taking computation and baking it into constraints. We know how to do it a lot better. And now actually the programming language that everyone loves is actually Rust, not C. So yet another reason to revisit things. Okay, so what did we want? We wanted an efficient start in terms of the proving costs and other costs and we wanted to run really fast over commodity CPUs and GPUs. And then this pretty much leads you to work with 32 bit words.
00:11:06.331 - 00:12:24.577, Speaker A: Why? Because CPUs and GPUs, that's like the Even though they can operate also on 64 bit words, they're much faster on 32 bit words and they're not that much faster on smaller things. Now one thing that another thing to notice is that integer arithmetic is supported very efficiently on both CPUs and GPUs for historical reasons, whereas binary convolution, which corresponds to binary field multiplication, is currently not as well supported on GPUs. It's very well supported on CPUs but not in GPUs. Now the talk after me is actually going to be about using binary fields, which I personally love a lot. The Irreducible team will speak about that. So curious to hear their take also on this. We decided to go among all the design choices with a 32 bit integer prime field for these reasons and indeed just jumping to the end, we already released like two weeks ago that we can now broke the world record in terms of proving capacity, proving over 500 half a million invocations of a hash, a stark friendly hash, which is 100x better than anything the world has known prior to that.
00:12:24.577 - 00:13:13.899, Speaker A: At least the best of what I'm aware of. But I just want to mention, I'm sure that this will not remain the world record and there are a lot of proof systems, most of them over small fields, that will probably will be this. This is not going to stay this way. So which 32 bit field should we take? Well, there was already one in use called Baby Bear, which is this particular prime, two to the 27 times 15 plus one. And if you look at its multiplicative group, so the size of the multiplicative group of a field is always one less than the size of the group. You get the size that is 2 to the 27 times 15, which means that there is an FFT friendly group of size 2 to the 27, which is why for instance, RISC 0 took it and worked with it and a bunch of other teams. So it's a really good prime.
00:13:13.899 - 00:14:01.435, Speaker A: But there's actually one that is even cooler, M31, which is the Mersenne Prime, 2 to the 31 minus 1. And the reason it is cooler is that it allows for very fast multiplication on commodity CPUs and hopefully also GPUs, because 2 to the 31 equals 1. So you can do all kind, you know, all kinds of like shifts and so on. And so it turns out to be roughly 30% faster multiplication. But the problem is that it does not have an FFT friendly subfield. So what do you do? So this is the motivation for circle start. We want to work with this particular field because it's like really efficient, but it doesn't have the necessary structure.
00:14:01.435 - 00:14:58.255, Speaker A: And why do we need FFTs by now I think you know, because the previous talks already mentioned that you get faster encoding, you get fry. And it's really important if you want really efficient algorithms. Okay, so which group are we going to use? So all of these groups are cyclic groups. So. Well, first of all let's realize that it cannot be the nascent, sorry, the native subgroup of M31, because this is the size of that multiplicative group 2 times 9 times. So if you think about FFT, you don't have really a very good structure here to do FFTs with. Now, as Swastik already pointed out, there was the option to use some elliptic curve to go and look for an elliptic curve over this field that will have size divisible, let's say by 2 to the 30, or maybe 2 to the 29 or something like that.
00:14:58.255 - 00:16:13.665, Speaker A: And then you would get some other group that would be more complicated, so it won't be already powers, but it will be like this evolution of points where each point is two numbers by some complex formula. And it gets even more complicated because like the kind of FFT reductions in the ECFFT paper call for a sequence of different curves. So it's a bit more messy to work with and which is how these three brilliant individuals came about to this thing to take a different curve, which is the circle, the good old circle. If it was over the reals it would actually have this picture, but it's not quite that way there. So it's all pairs of points that when you square them and add them up modulu this Mersenne prime, you get one. So for instance, one of these points is the point, well, 2 to the 15, comma 2 to the 15. And so, but, but again we sort of fool ourselves or we get used to it and we think of it as sitting on the circle and you get something that is much nicer in terms of its FFT friendliness.
00:16:13.665 - 00:17:20.617, Speaker A: And now I want to explain a little bit about what is nicer about it and also a little bit about the complications. So now we'll get a little bit more into the math of what do we mean, what Is this circle Stark actually will be mostly about the, just the structure of this group and how to work with it and what kind of functions are we, are we going to evaluate using it? Okay, any questions so far? Okay, so going back, you know, I started by saying that the use of polynomials and similar critters in the construction of proofs is utterly mysterious to me. I don't really understand, but there's a pretty long collection of properties that each one of them makes an appearance and you need all of them. And magically or mysteriously, polynomials happen to have them. They're not the only set of creatures that have them. There are a bunch of other algebraic creatures that happen to possess them. So one is their error correction properties, you know, which.
00:17:20.617 - 00:18:38.561, Speaker A: And all of these were mentioned in some form or another in all of the talks before. So they, they are the polynomials form the basis for error correcting codes. Here's another thing that wasn't mentioned, I think, but is really important for constructing statements about computation. You would like, you know, if you wrote down, let's say an array that corresponds to an evaluation of a polynomial, well, there are certain permutations that you can apply to this array and you will still get a polynomial. So for instance, for polynomials, if I have P of X, I can move to P of AX +B for fixed A and B, which means that I will sort of permute the values and it's still a polynomial. And this is a very important property because it allows us basically when we talk about computation, usually we want to talk about something that examines both a certain point in time and a point later in time. And in order to do this, it turns out that you really want your error correcting code to have this sort of property that you can move things around and still work with the same objects.
00:18:38.561 - 00:19:54.471, Speaker A: So you actually need this automorphism group which is this ability to permute things around. They also have the multiplication property, which is very important, which means that if you take the point wise multiplication of two of these error correcting codes, you will get an object that resides in some other error correcting code, but also one that has good distance. You have another thing that's very important, which we'll call zero testing or the ability to factor out certain properties. A polynomial P vanishes on a set of points H if and only if you can explain that by breaking up P into the product of two polynomials and one of them is depending only on H, and it is actually the product of all X and H for all little H and H of X minus H which we will call the vanishing polynomial. So you have this ability to explain or demand explanations in the form of factoring polynomials. And you have succinctness, which means that in certain cases, for nice Hs, for instance, when they are subgroups, you can actually evaluate this polynomial, this vanishing polynomial, very efficiently. It turns out that all of these properties are needed.
00:19:54.471 - 00:20:29.941, Speaker A: If you take one of them away, then suddenly things become much less efficient or impossible. So this is a little bit about the mystery of, like, why polynomials, you know, they have all of these useful properties. There are a bunch of others. If you talk about zk, they have this MDS property, which is also very useful, and I'm not going to mention it. So all of these very important, very beautiful properties come about in polynomials and some of their related, you know, some of their relatives. And now we're moving to a world where we're not going to have like good old, let's say, univariate polynomials. And the question is, do we have all of these properties? So the nice answer is, yes, you do have.
00:20:29.941 - 00:20:50.826, Speaker A: And that's basically what's, you know, what appears in that beautiful paper. Go and read it. I'll give a little bit of intuition about what exactly is going on there. So let's remember, let's recall the circle. The circle is the set of points xy. Oh, okay, we're going to fix our field. Our field is the Mersenne prime to the 31 minus 1.
00:20:50.826 - 00:21:36.825, Speaker A: The circle is the set of all points xy in this field such that x squared plus y squared equals 1. And it turns out that this set of points actually forms a group. And maybe the simplest way to see it, or to verify it later offline, is to notice that first of all, there's a complex extension of f31. The equation x squared plus one doesn't have a solution, so minus one does not have a root, just like over the reals, which means that you can extend the field to a degree to extension. With the reals you would get the complex numbers. With this M31, you get the degree to extension. Then you can think of it as adjoining the root of minus one.
00:21:36.825 - 00:22:42.139, Speaker A: And then it turns out that you move to a larger field the degree to extension, and within that larger field there is a multiplicative subgroup of size 2 to the 31 that maps to the unit circle over this base field. By applying something known as the norm map. There's a way to show that there is a group there, and there's a group operation that actually corresponds to also the geometric way you would add points on the real circle, and it has exactly two to the 31 points that reside that we care about. Okay, so there are some operations that you can do with these points or some definition. Every point has sort of its reflection or inverse, which is, you know, just the point here. And you can double points which, if you would draw it this way, it would just be like adding up the angles. Again, I'm reminding you that we're not working over the real.
00:22:42.139 - 00:23:27.465, Speaker A: So this is just some representation and you can project onto the X coordinate. And something that is really important, and again, mysterious, is that you have two things that you can sort of commute. You can double a point and project, or you can project and then double. So what do I mean when you double a point? When you move from the point p to 2p, notice that the X coordinate depends only on the X coordinate of the previous point. And when you project onto X, it means you take the X coordinate. So what this means is that you can first project and then double or do it the other way around. And this turns out to be extremely useful in making things more efficient.
00:23:27.465 - 00:24:23.685, Speaker A: Okay, so the group would look like, here's a group, the green points are a group. And then you could also move the group and get the red points, and that would be a cosette. Now, some cosets are really nice in that you could have them so that when you project onto the x axis, they come in pairs. So every two points are projected to the same point. And this is the set of points that we will take in order to evaluate functions and do low degree extensions and so on. So if in the FFT world we would have taken, let's say, the green points, and they would have been powers of a generator. Now we're going to actually take the red points, a coset of them with a property that each pair of them can be projected, they can be divided into pairs that project onto the x axis.
00:24:23.685 - 00:25:48.243, Speaker A: We call this a canonical coset. What are going to be the analog of polynomials? We're going to look at bivariate polynomials, but we're going to take always the modulo x squared plus the curve, which means that you can represent any such function as basically f0 of x plus y times f1 of x, where both the functions f0 and f1 have bounded degree. And those are the functions we're going to take. Now, because we're evaluating things on the curve, Bezou's theorem actually shows that as opposed to standard polynomials, the polynomial of degree One will have exactly one root here. Every polynomial that is of degree one will have exactly two roots because it will intersect the circle at two points, okay? And then there, if you move to the projective version of this, polynomials have poles at 1 plus minus I. I don't want to go into this too much. So the code that we're going to take is a generalization of the Reed Solomon code.
00:25:48.243 - 00:26:50.183, Speaker A: It will be bivariate polynomials of total degree at most N evaluated over the set of points that come on a coset of a circle on a coset of a subgroup on those red points that we saw. And then, just like Polynomial of degree 1 has two roots, a polynomial of degree N will have at most 2N roots. And that is going to be our curve. That is going to be our code. Now, there is a minor, as I said, I pointed out there, that there are like these minor things that need to be fixed. For instance, this code that we're using does not correspond exactly to what's known as a Riemann Roch space, which is the natural generalization of polynomials. There's some extra dimension of one that is a bit tricky.
00:26:50.183 - 00:27:36.635, Speaker A: I don't want to go into this. But the nice thing is that you can. Well, here the nice thing is that you can represent. Remember that every function can be written as f0 of x plus y times f1 of x, and you have two endpoints. So a different way to think of this representation is to. Well, let's say you can use these two points projected onto X to sort of break up your function into an F0 part and an F1 part and make sure that each one of them has degrees strictly less than N over 2. And that's basically the kind of code that we're working with.
00:27:36.635 - 00:28:34.909, Speaker A: It's basically combining two polynomials of degrees strictly less than N over 2 in a particular way that you can go back and forth between. And the reason this is useful is that you can just move very quickly between this bivariate polynomial to just a pair of polynomials. And you can start now applying things like fry or FFTs or various things like that onto the projection. So basically, even though the real code that we care about is, like, evaluated over these points on the circle, there is an alternative way of viewing things as just pairs of polynomials evaluated over specific points that happen to form this FFT that you can apply FFTs to and evaluate things efficiently. So that's what I wanted to talk about there. And I'll end with just two questions that I hope someone will solve one day. Correlated agreement and bounds and concretely efficient modern PCPs or Stark.
00:28:34.909 - 00:29:34.923, Speaker A: So the first one is related to the talk that Shubhangi just gave. She told you about this correlated agreement or proximity gaps paper. And there's this quadratic factor that we don't know what the right answer is. So we already proved this for agreement that is at least square root of the rate. But the question is does it hold all the way up to the rate or doesn't it hold? And the answer to this question will determine what is the soundness level of all use starts today? Are they a factor two off in terms of their security or not? That's a very interesting algebraic question. A second one which is more high level is that if you look at all of the stuff that is implemented today, in practice it pretty much relies on properties of polynomials. In algebra, a lot of these had their starting points in the early, sorry in the mid-1980s and 1990s.
00:29:34.923 - 00:30:22.005, Speaker A: But there are like other techniques that come from combinatorics, things related to parallel repetition, direct product gap amplification, asymptotically good LTCs. And the question is, what about the concrete efficiency of these? Can someone take these techniques and construct a completely different Stark or Snark or Snarg that is concretely efficient? Because they have been use the to do marvelous things within theoretical computer science. So can you de randomize these? That would probably be a way to make progress there. And I'll end with that. Thank you. Questions.
00:30:25.905 - 00:30:42.375, Speaker B: So the way you are converting like the F31 to this pair of points, the addition operation takes what's supposed to be an xort to be like four multiplications.
00:30:42.495 - 00:30:42.847, Speaker A: Right.
00:30:42.911 - 00:30:57.235, Speaker B: So why is it efficient? So the way you define your group operation on the circle, it has four multiplications of f31 yz.
00:30:57.995 - 00:31:43.291, Speaker A: Well, I think that in most cases you never need to do these group operations when you're doing your FFTs and fries and so on. Basically you have like they are a way of indexing this, these tables, these code words. So for instance. Yeah, so you never need to access them directly. It's. Well, one place where you may need to access them is when you're doing these two to one transformations, they would show up. But if you project onto the X axis, which is something that in circle start you do immediately, you now just sort of work with the x projections of those.
00:31:43.291 - 00:31:57.035, Speaker A: And it turns out that now you're basically using twiddle factors and things that are just in the base field. So you never need to use the group operation and the Computations you're actually doing.
00:31:57.775 - 00:32:16.431, Speaker B: So as a follow up, like maybe I missed. But what's the advantage of this transformation of the fields? Like, are you realizing that in the, in the proof part, in the, in the proximity which transformation? Transformation to like pair of points.
00:32:16.503 - 00:32:55.247, Speaker A: Oh, the projection. Yeah, yes, exactly. The projection means that now the twiddle factors don't need. Like how do the twiddle factors come in? I guess think about the fft. You say I take this value and the next value and there's something that's connected to the group operation that I take into account when I sort of do my fft. Right. So you would need, if you were working over the circle group, you're sort of the next value would have needed to take both the X and the Y axis into them and then you would have more multiplications.
00:32:55.247 - 00:33:11.075, Speaker A: But if you project onto the X axis and you work with a pair of polynomials of half the degree, then you're basically just manipulating twiddle factors on, just on field elements, not pairs of field elements.
00:33:13.415 - 00:33:20.355, Speaker B: But I guess my question was more around why is it better than roots of unity?
00:33:20.855 - 00:33:29.487, Speaker A: Oh, because the roots of unity just won't have an FFT structure in this particular field. They won't have size 2 to the K. So you wouldn't have nice 2 to the 1 maps.
00:33:29.671 - 00:33:30.655, Speaker B: Oh, okay.
00:33:30.815 - 00:34:20.351, Speaker A: Thanks Serino. Constructively, how to find the coset that has this property? Like if you have the circle and you need to shift it, do you know how to find the shift? Yeah. You would. One way is to, let's say start with a group that is twice as large in size, pick the first jump, make that the co set and then just have the group be like jumps of two. So spare me because I'm not a cryptographer, I have a different math background.
00:34:20.383 - 00:34:24.607, Speaker B: But what properties do you lose?
00:34:24.791 - 00:35:27.205, Speaker A: Or is any loss of kind of important properties as a result of switching to circle? I mean, it's a beautiful construct, but what do you pay? Like, what do you lose? Yeah, in terms of like properties that existed on the previous schemes. Oh, the biggest one that is fixable but gets a little bit more complicated is like this vanishing property because every even line has immediately two zeros. If you want something that just vanishes at a point, you know, you don't get that. You can't like divide by one thing that vanishes at a point. You could get something that vanishes at pairs and you could get like this. You can correct things by a ratio of like, by a rational function, but it's a little messier than with polynomials, where. And these things happen with algebraic geometry codes, where you can get pretty much similar things but now use rational functions and it gets a bit messier.
