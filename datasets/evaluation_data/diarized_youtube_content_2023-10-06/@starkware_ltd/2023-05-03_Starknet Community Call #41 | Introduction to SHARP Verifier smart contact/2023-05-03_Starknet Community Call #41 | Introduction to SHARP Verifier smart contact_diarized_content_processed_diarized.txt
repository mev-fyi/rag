00:00:00.170 - 00:00:58.378, Speaker A: So much for joining us today for the intro to sharp verifier smart contracts community call. This is our 41st community call, so we hope that you have been seeing some of the recordings or attending some of the previous community calls as well. If not, then we have prepared a very small sort of contextual presentation for you so that you understand what is happening and get getting some context to what we are going to talk about today. So today's agenda, just very quickly for context setting, we are going to talk about an overview of architecture and shard. And then large part of the call is going to be about shard verifier contracts. What are these contracts? And I have my colleagues David and Remo here. David is from the devil team and Remo is a software engineer who has been working in stockware since 2018 and he has touched a lot of solidity smart contracts.
00:00:58.378 - 00:01:32.010, Speaker A: So he's obviously the best person to explain about all of these sharp verifier contracts. Don't hesitate to ask a lot of questions. If you are viewing this live on YouTube, then feel free to ask your questions in the comments as well. But towards the end, we'll also have A-Q-A section where we will break out to answer all of the questions that you have asked. So yeah, let's go. Just to give you a quick context about what are the different components and what is the one component that we are going to highlight today. This is the stocknet architecture.
00:01:32.010 - 00:02:28.182, Speaker A: This is very brief. Each of these components have their own functionality and how they work. So just to give you an overall understanding of how starknet works, right, so at the layer two level, that is, at starknet, all of the transactions are bundled together at the sequencer. Think of this as your validator for Ethereum. L ones, which creates your block and bundles of the transactions, puts them together in a block, and then sorts of builds the blockchain itself. Now in this case, since we are going to be discussing valdity proofs, since Darknet is built on top of validity proofs, it's not just enough to bundle these transactions into a block, but the execution trace from the sequencer is sent to a prover, which is known as sharp, that is the shared prover. And the prover creates the validity proof which is then sent to Ethereum, which is the l one to verify.
00:02:28.182 - 00:03:40.590, Speaker A: Right? And the component that we are going to focus on today is the verifier itself. The verifier is a solidity smart contract deployed on ethereum which verifies the validity proof that is generated by the prover that is sharp, some of the other components in this picture. Very quickly, successful verification of a validity proof becomes a fact which is then stored in the fact registry. So anybody can basically verify a particular fact whether a certain computation has been successfully proven or not. And Starknet core is a component which is used to store the state differences. When you think of state changes, which is what is sort of a successful transaction, right? There is a state change that happens and the state difference, or the state change, the difference is stored in staffnet core, which is a component, again at the l one level on ethereum, and we have a full node which syncs the state from sequencer and the stocknet core. So in a situation where say the sequencer goes down, you can always populate the l two set of transactions again by getting the state differences from starknet core.
00:03:40.590 - 00:04:06.326, Speaker A: So the full node sort of acts as it takes the data and then syncs it. It can sync it back again. That's why you can also think of it as an escape hatch. I'm not going to go deep into each of these other components today, since we are going to deep dive into the verifier itself. But to understand the verifier, I want to also give a little bit of context in terms of the prover. That is the sharp. So sharp is the shared proverb.
00:04:06.326 - 00:04:48.258, Speaker A: It is a shared proverb because it is shared between Starkx and Starknet. Today we are going to focus on Starknet, but largely the prover functionality remains the same. So what does the prover do? The prover aggregates multiple Cairo programs from different users, each containing different logic. And then these Cairo programs are then executed together, generating a single proof. So the sharp prover actually uses something called as recursive proofs. And let's see just a brief glimpse into how these recursive proofs are generated. The first time when you get these aggregation of multiple Cairo programs, there is a single proof that is generated.
00:04:48.258 - 00:05:48.666, Speaker A: It is then sent to a stock verifier program written in Cairo, and then this stock verifier generates a new proof to confirm that the initial proofs are verified. And you can think of this recursive cycle that keeps going until a certain number of cycles are met. And then the last proof in the series is sent to the solidity verifier on Ethereum, which is what we are going to be talking about. You can read more about the Shard prover in our Starknet book, which has in depth explanation about how the recursive proofs work. How they are generated and the architecture of the prover itself. You can also watch one of our previous community calls where we have Kinaret from our prover team talking about how sharp itself works. So now that you have some context in terms of what the architecture is like and what is the sharp proverb, it is now time for us to understand the verifier in itself.
00:05:48.666 - 00:05:56.750, Speaker A: So today we are going to talk about sharp verifier smart contracts, and I would love to invite my colleague Remo to take the stage.
00:06:07.840 - 00:06:09.244, Speaker B: You're muted now.
00:06:09.442 - 00:06:23.550, Speaker A: Yeah. All right, David, do you want to quickly introduce yourself while Remo is getting set?
00:06:23.700 - 00:07:18.210, Speaker B: Yeah, as you mentioned, I'm one of the starting developer advocates. I'm based in Toronto, in Canada. And yeah, I think architecture is something area that's very fascinating for me, especially for Stargnet. I've been trying to do a lot of going deep into it, and sharp is one of the, probably the most complex component of the whole architecture and maybe even the most critical one, because that's where the validity proof is created. But the verifier is on the other side of the equation, right? The validity proof sent to Ethereum for validation. That's why we say that we inherit the security of Ethereum on a secure roll of like a starkness, because the validity proof is verified on the whole Ethereum network. We're talking about tens of thousands of validators independently verifying the same cryptographic proof.
00:07:18.210 - 00:07:30.950, Speaker B: And then the result of that verification is a store in a storage variable called the factory history. I think you have Remo online. We can hear you now. I can hear you typing.
00:07:32.890 - 00:07:34.680, Speaker C: By the way, how do you pronounce your name?
00:07:37.690 - 00:07:41.910, Speaker B: I don't. Do you see it, Nana? I don't see a slide from Remo.
00:07:42.070 - 00:07:50.300, Speaker A: No. Oh, I see them. Let me add them to the stream. Are we able to see it.
00:07:52.190 - 00:07:52.950, Speaker B: Now?
00:07:53.120 - 00:08:36.286, Speaker C: Okay, excellent. Okay, so again, I'm Remo. I'm here as a software engineer in staff for the last four and a half years. So let's start. Why doesn't it do anything? Okay, so we cover the background and just another slide with the flow of jobs from starknet sending Cairo jobs onto the system. It's validated, it's scheduled, it gets into the mid grind of sharp. It's proven, it's rated.
00:08:36.286 - 00:10:21.978, Speaker C: If it's a recursive proof, in case of sharp it is, eventually it goes down through our components called blockchain writer onto the blockchain. Hence our solidity verifier contract. Or actually it's not one contract, it's a bundle of. So when we started off writing solidity contracts for our stockx or stock exchange, at the beginning, we had a verifier contract that was monolithic and it was triggered by the main contract in the sense that the operator would call the update state function on the main contract, which provided to the main contract the state that is to be replaced with, changed, updated and approved to attest that this change is valid. And main contract would provide that proof to the verifier in different fashion to the validium committee. Those contracts would respond without process the proof and return, hence would not revert attesting that the proof would be correct and then the state would be updated on the main contract. That was the first architecture we had once we launched our prototype back in 2019.
00:10:21.978 - 00:11:42.310, Speaker C: Before it was not a main net and it didn't have real money, but that was the first attempt we had the proof of concept to the stock exchange product. Now it was very well working. The only problem that we had quickly, we found a few limitations. First limitation, we found that we hit transaction size limit. Guest at that time had only 32 transmission size of a transaction. So quite quickly, if you accumulate too many transactions into one batch quite quickly, we hit that limit of transaction size of the proof, of the update state or whatever, and also not as quickly, but immediately thereafter, we would also hit by that time 8 million gas block limit. So it was just not big enough to contain one batch of proof.
00:11:42.310 - 00:12:53.882, Speaker C: So we had to find a way to go around that. Another limitation that was not relevant back then, but would ring a bell when we talk about chop is the fact that we triggered the verification of the proof from the update state and not independently of it. And if you think of sharp now, it would not work this way because sharp has as many batches, many proofs within one big proof. So you have to have it waiting for the customers and not customer triggering the verification. But that was not a problem then. It is only a benefit of the redesign we had done later on. So what we needed to do, obviously, is to split it up, to have decoupling of parts throwing independently onto the blockchain, and then later binding the dependencies on the work that has been done and recorded through the fact registry.
00:12:53.882 - 00:14:23.378, Speaker C: So a major cornerstone of architecture is a very simple pattern we called fact registry, which as Jan already mentioned, you have a heavy lifting computation or whatever processing you need to do, and you want to do it once and to query that later or many times later, or maybe not, or maybe yes or whatever has something that would rely on that in this or that way. You don't even have to know exactly what way, but you just want to register the side effects of a fact to the work you have just done. So we decouple the work or the computation with the fact that attest for that work to have been done correctly. And the interface is we call it fact registry or IFAC registry, and we have a key of a fact that would be indicative of the work, or the proof or the whatever or transfer or anything you want to query on and you ask for is valid on that fact and it would return true or false, given the fact would be two or false. I lost my screen. I hope it's here again. Can you see my screen? I think you can.
00:14:23.378 - 00:14:50.560, Speaker C: Okay, so we came up with the fact registry pattern for the sake of splitting the verifier from the main contract, and we actually use it extensively throughout our code. It's like all the code I'm referring to, it can be seen in the GitHub, it's a little bit too small.
00:14:51.250 - 00:15:00.470, Speaker B: So a question this fuck registry is a separate smart contract from the verifier, or is just like a one contract variable, like a state variable of a smart contract?
00:15:00.890 - 00:15:02.646, Speaker C: Sorry, could you repeat that?
00:15:02.748 - 00:15:06.338, Speaker B: Is the fact registry a separate smart contract from the verifier?
00:15:06.514 - 00:16:08.380, Speaker C: It could be a separate contract, it could be a pattern within the contract itself. It doesn't really matter. The pattern indicates that you have a very trivial is valid API that you query on facts and someone else, a producer, is responsible on making the computation or the work or the transfer or whatever, and register the fact that that thing had been done. And once the fact is there, if you query it, you get the true, you know that it's okay. And if it's not there, or if it was not valid or whatever, you get the false and you know that you cannot rely on that fact to be valid. In the context of sharp verifier, we use it to split up the contacts, as we will see in a second.
00:16:09.950 - 00:16:23.920, Speaker A: Okay, Remo, I also have a question. So for those who are looking at this, say for example, querying facts, what could be a fact? Could you give an example?
00:16:25.890 - 00:17:46.154, Speaker C: For example, if we talk about stockx scenario of a fast withdrawal. Fast withdrawal is a scenario, usually in the slow withdrawal you send a request to the operator that you want to withdraw funds. The funds would be marked as going to be withdrawn, and once the proof is verified and the update state on the main contract is accepted on chain, those funds are available to be withdrawn from the main contract, but it may take a few hours or so. Depends on the parameters of the system and the latency of what is the maximum waiting time for the proof and how long it takes for the proof to burn, et cetera. So it can take quite a while. So the flow, named fast withdrawal, is a flow in which the operator has a liquidity pool that it used to send the funds upfront to the customers, and the customers send what they call a conditional transfer. What does it mean, a conditional transfer? It means if a condition is met, the condition is that the money was sent to me.
00:17:46.154 - 00:18:53.422, Speaker C: If that condition is met, then please transfer those funds to the operator. Meaning give me the advance. And if the advance is actually received, I allow you to take the money that you paid the counter sum for that advance. So how do we implement that? We have a contract called transfer registry. The operator sends the funds using the transfer registry and all the parameters of transfer, how much, who sent to who announced that against replay attack or whatever. All that is like a vector on which you write the fact on that transfer registry and the transfer registry buffers it sends over. It dispatch the funds as it received from the operator, and it registers the fact that this transfer has made through it, and the user will use that fact to use as a condition in the conditional transfer.
00:18:53.422 - 00:19:46.510, Speaker C: So we have an external contract called transfer registry. Someone is sending funds through that, and that transfer registry is a fact registry, attesting the transfer that had been done through it. And the user that is sending the conditional transfer request is basing the condition of that transfer on the funds that was sent through that transfer registry. So this is a way of decoupling of actions and registering them in effect. Of course you need to be able to formulate the fact in a way that sits well. For example, if you don't have a non so whatever, you may get into a situation of replay attack. So you need to formulate the fact that it is representing the stuff behind that fact in a good manner.
00:19:46.510 - 00:20:20.474, Speaker C: That depends on the realm of the problem, but it's quite obvious what I mean here. So in the sense of a proof, you would have to have the public input or program output or whatever way we call it. And those parameters are the fact that we, later on we see that we query on with the isvalue to see that the proof has been verified correctly. Okay?
00:20:20.672 - 00:20:22.542, Speaker A: Okay. Yeah, thank you.
00:20:22.676 - 00:21:26.900, Speaker C: Let's continue. So besides decoupling the main contract and verify, hence breaking the trigger by main contract or triggered by update state pattern that I spoke about before, we also needed to break it up. The proof was too big, too much gas, too big of transmission size, it was just too big. So we need to break it up. And we had also the need to break it up in a manner that the integrity of the entire proof would remain. But also you want at least some of the parts to be such that they are independent of the bigger picture. So you need to have a big main proof that some parts of it are torn out and become smaller proofs that are independent, but the main proof depends on them.
00:21:26.900 - 00:23:15.620, Speaker C: So we have an easier way of managing the preparation and sending and acceptance of that part, because if you have big chain of dependencies, if you break a very big proof to, I don't know, 100 or 200 transactions and some of them don't go through, then it's kind of not scalable if you have too much dependencies in this chain. So what we came up with is that we have only one dependent transaction, which is the main proof that of course is depending on all the important parts to be in place, because otherwise the proof would be invalid to begin with. But those parts that were taken out are standalone, so we can have as efficient as we can way of sending and padding, as stuffing as many of those as we can in each block, et cetera. So the white, isn't it losing my screen? I think I will have it here instead. Just a secondary. We split the verifier was called by then Dex verifier Dex from distributed exchange. We split it to three parts.
00:23:15.620 - 00:24:07.270, Speaker C: The main parts which we call Dex fileless verifier file is because the fry was outside and we broke out the main contract. And the Merkel statement verifier and the fry statement verifier. The Merkel statement verifier as it sounds, was verifying Merkel path proofs and the fry was verifying phi layers. Each time you would just send. I can't work with this screen. Each time you send a phi layer to be proven each layer at a time. No.
00:24:12.760 - 00:24:13.992, Speaker B: We lost your screen.
00:24:14.126 - 00:24:17.130, Speaker A: Yeah, goes to me too.
00:24:21.500 - 00:24:29.550, Speaker C: I think I will take it from here instead of from the other screens because this one will not.
00:24:36.800 - 00:24:52.550, Speaker A: Sure. Meanwhile, for those of you who might be completely new to the idea of starknet or verdict proofs, maybe David, you want to just say like a one liner about what is a validity proof? I know I'm putting David on spot a lot, but I know that he knows these things.
00:24:53.640 - 00:24:54.148, Speaker C: Yeah.
00:24:54.234 - 00:25:31.904, Speaker B: Validity to prove is a way to attest to the integrity of a computation. So basically that certain output is a result of executing this particular program with certain inputs which are hidden, although not completely private. So you can verify to this attestation independently in a polylogarithmic way. So that's where you get some called computational compression, because the verification is much cheaper than the re execution of the underlying computation. Yeah, that's basically it, yeah.
00:25:32.022 - 00:26:00.410, Speaker A: And validity proofs use the mathematics behind zero knowledge proofs, wherein you have a prover and a verifier. So today we are talking about the verifier in details as to how this verifier verifies this validity proof. Right now, of course, we are talking about the architecture and sort of the evolution of how the verifier itself was built. But I'm sure going forward, we have some more juicy bits about the contract itself and how it looks and what it does.
00:26:00.940 - 00:26:43.828, Speaker B: Yeah, and the validity proof is one implementation of zero knowledge proofs, which is a common misconception, is that when you have ZK rollover, that everything is private, the transactions. But in reality, we use CK proof for scaling. So it's not about privacy. It's using Starx and CK proof to guarantee this computational integrity in a way that is very cheap to verify. And that is thanks to the Cairo CPU architecture as well, which has a very different architecture compared to a traditional cpu. And Stark is one type of ZK proof that you can use to create validity proof. You can use Narx, you can use something else.
00:26:43.828 - 00:27:21.252, Speaker B: Probably plunk as well. I'm not a cryptographer, I don't know too much about the differences. And also probably it's connected to fraud proofs on optimistic roll ups. I think they might use similar technology in a different way. Right, because we try to, on an ongoing basis, submit validity proofs to ethereum for validation while on optimistic roll up. It's only whenever an external entity detects a sequencer to be cheating that they submit a fraud proof. But I think those two technologies have some things in common.
00:27:21.252 - 00:27:56.780, Speaker B: It's just the way the architecture works is very different. I think seeing the verifier is interesting because it's completely open source. You can find the code on the startgraph repositories and you see how it works. And that's the entity that dapps might actually reach out to to find if their carrot job was verified successfully. It's probably what indexers are looking at to provide that information on a more real time basis. Remo is still having issues.
00:27:56.850 - 00:27:57.756, Speaker C: Can you see my screen?
00:27:57.858 - 00:27:59.072, Speaker B: Yes, yes, we're back.
00:27:59.126 - 00:29:03.350, Speaker C: Okay, so let's continue. I took it off the main monitor because the main monitor was great, only that it was shutting off every minute. So I hope I will be more consistent. Okay, so as I said, we split it to three parts, the main part of the verifier and two small parts for the merkel and the fry. And those parts were sufficient to offload enough particles of the proof so that it would fit. Next step, we went CPU Air before we named it Cairo, the name for the general purpose Air for programmatic, not fixed, but the programmatic air was CPU Air. And the second version of Starkx in late 2020 was already using this.
00:29:03.350 - 00:30:08.120, Speaker C: And later on it became Cairo and exclusively on Sharp. Okay, so in sharp we have four contracts that are externally accessed. The GPS statement verifier, which is the verify contract of sharp, on which you send the verified proof and register, which is the main transaction of here. Hey, here is the proof, verify its correctness and register the appropriate fact. Not fact, but many facts. Because one such proof contains many jobs, many tasks, each one with its respective fact that would potentially later on be queried by the main contract of that application, whether it's Starknet or stockx, or this or that customer. Additional side contract are the fi statement contract and the Merkel statement contract.
00:30:08.120 - 00:31:15.810, Speaker C: As you see here. The new parts are the GPS maintenance verifier, which is the next generation of the DeX verifier. It is not Dex only, it is general purpose one because it's now running the CPU air and it verifies proofs of Cairo program being run on particular input. And also the memory page factor registry, which is a new contract that we introduced to write different kind of independent particle of the proof. These particles is the memory pages that are used for the data outputs of Sarcnet, or the data availability in roll up mode or volition roll up mode, or entirely roll up mode of a deployment. So this is the set of the contracts we have. These are the addresses of those in Mainnet, so we can have a quick look.
00:31:15.810 - 00:31:30.870, Speaker C: This is the main contract, you see only verify proof and register transactions on it. You may here see. Let's not get into that.
00:31:33.160 - 00:31:39.030, Speaker A: But we only see the screen with your slide, not the other one.
00:31:42.060 - 00:31:47.050, Speaker C: Can I share the entire laptop screen? Does it support it?
00:31:47.420 - 00:31:49.816, Speaker A: Yes, I think you should be able to.
00:31:49.998 - 00:31:53.950, Speaker C: I will try to do that. It'd be much more fun.
00:32:00.710 - 00:32:16.482, Speaker B: You just answered one of my questions that I was about to ask you. It's like how do you make the connection between the data availability transaction sent to layer one with the associated validity proof verified. And I realized that there's a memory page registry.
00:32:16.546 - 00:33:24.362, Speaker C: The memory page are the data. The association is a little bit tricky because the way it goes, the memory pages are factored, they are hashed in some products, and the hash and some product are recorded. And this also is a contributing factor into the general fact of the proof. So that you have this way it is not only floating there, but also in a way cemented there so that you cannot rely on different recorded data for the data availability or not having. You must have it and you must have it correct. So the main contract expects and finds the fact in the appropriate key, otherwise it would just not find it. So basically the main contract is validating, is querying the verify contract.
00:33:24.362 - 00:34:24.800, Speaker C: So the fact, it's now trying to update the state accordingly. According to so how you do that, you have a vector of parameters that are, as far as the main contract is concerned, those parameters are the states to be updated. There is the root and the order, root and state and sequence number and the batch id, something like that. So you make a big hash of those parameters and this is the fact, but it's not exactly the fact. This fact is then later on hashed with the Cairo program hash and also with the data availability fact. So that you have one big fact that bind all of these contributing factors together. So that you know that if that is valid, it means that you have the correct program in Cairo on the correct state update with the correct data availability recorded.
00:34:24.800 - 00:35:16.290, Speaker C: And then when the verifiers tell you true on that, you know that everything is valid so you can safely and soundly do the fact. So I don't see how I share just my screen. So I will share the slides and the avid reader can later on just hop to the links and see the data on iTer scan or if Tx info that is related to that, I will talk about it instead of showing it. I don't want to lose the screen too many times. Okay. These are the big contracts that we use to interact with the sharp smart contract. We send transaction to those and we query.
00:35:16.290 - 00:35:48.154, Speaker C: That is valid on the GPS table verifier. Okay. But in fact we have deployed a little more than that. Something like, I think almost 40 of those contracts. So what are the actual contracts we deploy? Let me just really breeze through the list. We have the proxy and the call proxy. I will get into that in a second.
00:35:48.154 - 00:36:49.662, Speaker C: These addresses are actually the real addresses of the current deployment of shops. Later on you can just copy paste them to Iter, scan to see the transaction, to see the code, to see the events being emitted, see just about everything related to that. So we have proxy for upgradability. We'll get into that in a second. We have the high level contract, as I said before, the GPS, the memory, the Mercury and Defi. We have another bunch of top level contracts, which is the Cairo bootloader and a few XY columns. The Cairo bootloader is the Cairo program written in first, just bunch of numbers, which is the Cairo program that is being used to validate the Cairo program of the actual statement, whether it's Starkx or stark perpetual or Starnet or whatever.
00:36:49.662 - 00:38:48.230, Speaker C: You have the bootloader which runs this program, and you need to have the code of that to cement correctly the execution. So you executed the Cairo program using a bootloader program. So the bootloader loader program, you need to have it also in order to be able to cement it correctly to the proof to get its hash, to get everything in place. So this contract doesn't do much, but it's used to be queried on please give me the co program, give me the hash of that Kyo program or whatever so that it can be put into the calculation correctly. The XY columns, various contracts are basically lookup tables with almost infinite amount of just raw data that is queried on by the evaluation functions throughout the code to calculate the Peterson hash or the elliptic curve calculation need to be done. And now we have set of seven, eight actually similar stacks of cpu, fileless verifier cpu out of domain sampling, cpu constraint polynomials, contracts times eight times of those from zero to eight with one small exception I will get into in a second. So basically those are eight different, pretty similar, but different layout cpu files verifier what is a layout? A layout is basically a set of resources built in constraints that is specific to a particular task.
00:38:48.230 - 00:39:46.550, Speaker C: We prepare a specific layout for kind of a particular evaluation of a mix that would do for the job, say for dy Dx we had a layout that would have more this or that calculation that has to do with perpetual derivatives, perpetual, et cetera. Calculation. For stacknet we introduced now the poseidon or the ketchup or whatever. So we have different needs of built in in the layout. So the different layouts simply has different parameters of such built ins and resources. The number six one with the poseidon also support the newly introduced Poseidon built in, so this is why it has more of those. It has also Poseidon related lookup tables.
00:39:46.550 - 00:41:02.042, Speaker C: Okay, so I spoke about the proxy and the quad proxy. Basically, proxy as used from other places as well, is just a way of doing updability in a manner that you have one facade proxy contract that holds the state and the address, and it uses usually as also we do, we use the fallback function to delegate the call onto the real implementation contract. So you can replace the real implementation, but you don't need to replace the state or the address using which you interact with the system. We have that also for sharp, but in a little bit different manner. For various reasons. We prefer the state not to be on the proxy contract, but to remain on sharp verifier, on the GPS statement verifier contract. So what we did, we introduced an additional contract between the proxy and the application contract between the proxy and GPS statement verifier, which we called it call proxy.
00:41:02.042 - 00:42:43.520, Speaker C: What does a call proxy do exactly as a proxy contract does, but only that it doesn't delegate call, but it simply calls. So essentially you have a fallback function that would read the implementation address exactly as it does on a proxy contract, and then it passes the call onto the implementation address it has read, but it doesn't delegate call, but simply calls. So what the trivial but important difference is that the context on which the transaction works is the context of the destination address of the statement verifier and not the context of the proxy contract. So it also allows us to be able to be more flexible with chaining the path between the proxy and the actual verifier, et cetera. It also has some limitation we need to be aware of, such that, for example, that the facts being registered are scattered over multiple verifiers. Because if we upgrade to a different verifier, the facts will now be registered on the new verifier and the previous facts were registered on the older verifier, et cetera. So there is some trade off of flexibility we gain from that, and flexibility, not flexibility we lose, but kumberness we also gain, but we need to be able to be aware of and be able to manage with.
00:42:43.520 - 00:43:30.922, Speaker C: It's just an interesting addition in the mix of this contract. Okay, next, layout. As I said, we have different layout for different, exactly application, but mixes of needs and resources. Need for the applications from zero to seven. These are the names of those layouts, just so that if it makes any interest or sense. Not really. By the way, since we have gone recursive, we have very most of the proofs that get to the solidity verifier.
00:43:30.922 - 00:44:43.560, Speaker C: Are all proofs of Cairo verifier running because you have one Cairo program, say Starknet, say stock exchange, say stock perpetual, whatever, Cairo program with a batch that has been proven, but essentially since it's recursive. So the next layer of verification is of a Cairo verifier proving a program. So essentially when it gets to the Cairo for the solidity contract of doing Cairo verification, it's just always the same program of Cairo verify, approving, Cairo verifying, et cetera. So it's all basically we are using just one of those layouts at the moment. So most of those 40, some contracts are just not being used. Maybe in future version we will not deploy all of those layouts, but only the one that are expected to be used. Hence the number six or whatever, the one for the recovery five.
00:44:43.560 - 00:44:54.726, Speaker C: That's about layouts. Anything about that before we continue the.
00:44:54.748 - 00:45:10.350, Speaker B: Way I think about layout, and maybe you can let me know if that's an appropriate analogy. I think of a circuit board and the bigger the layout, the more circuits I have for a special purpose, but it's the most expensive is to run. Is that like a valid analogy?
00:45:10.770 - 00:46:25.842, Speaker C: Yes. Well, you have a trade off because in a layout you have like a budget of how many built ins of each of particular type you can have, you can have so many range checks and so many catch ups and so many bitwise and whatnot. And if you exceed that you cannot make the proof or you need to pay more Cairo steps and it gets way bigger, et cetera. So you need to make for the Cairo job, you would have the best mix of built ins and resources. So the layout, usually you select the layout that will be appropriate for the entire. If you have a set of jobs that enter the train, the Cairo train, you need to find the layout that is on average basis suitable for the entire train. Now when it's extremely dominated by startnet, it's not really a big deal because with starknet it's really almost homogenic, the resources and built in mix.
00:46:25.842 - 00:47:18.822, Speaker C: So it's not a complicated task anymore to pick a layout and just pick the one that fits. For starnet Stacknet it would fit. Let's continue. Okay, so let's look about on the constructor parameters needed for the different contracts we have in the mix. Only two of them actually have significant or any construction arguments. All the others are just standing on with no construction arguments but the GPS statement verifier, the main contract of the sharp verifier has, and also those that are cpu files. Verifiers we said we have few of those.
00:47:18.822 - 00:48:19.930, Speaker C: We had eight of those. And each one of those eight has such a set of dependencies. The ones that are not number six that support Poseidon don't have so many. It has only up to the ECD, say up to the elliptic curve points y column, and it doesn't really matter much. But essentially you see, you are passing on all those subcontracts that we had to split up also for better engineering of it, and also because you cannot deploy a contract that would exceed 24 case, especially for the giant lookup tables, we need to split that up significantly. But this is kind of a different split up. It's not a split up of the proof, it's a split up of the contract that verifies a proof.
00:48:19.930 - 00:49:42.230, Speaker C: So here you have all those auxiliary polynomials contracts. Here you have the out of domain sampling, evaluating contract, the memory and the memory, Merkel and Fry contract that we talked about before and two parameters for security. The security bits that currently we use 96 bits and the minimum proof of work bits. What is the minimum size of proof of work bits that we put as grinding portion of the proof for the main contract we have essentially, as you can see here, the bootloader program, all the verifiers, the sub verifiers for the different layouts that we deployed before and a couple of parameters. The hash supported Cairo verifiers and the bootloader. The program hash are essentially the program hash of the Cairo verifier itself or the bootloader itself. It's needed in order to cement the fact of the verification with the program that produced the verification.
00:49:42.230 - 00:51:05.380, Speaker C: Okay. And also if you click on it later on, you can see the actual code of the constructor in GitHub. I will not do it now because it will not show, it's not shared. Okay, so the new flow or how it goes, the dispatcher of sharp as we spoke before, send all the subtransactions, the memory pages, the merger statement, the five statements. Now, because Tarknet is so dominant in the mix, you have for each shop verification, you have even up to a few hundreds of memory pages recorded and a couple of merger statements and five statements, not so many of those, but a really huge number of memory pages. Because at least until we introduce validium or volition or whatever to Starknet, the data availability of Starknet is also on Mainnet, so it has to be recorded using the memory pages. And as numbers go up on Starknet, it goes up on the memory pages recorded.
00:51:05.380 - 00:52:57.010, Speaker C: And after all those were sent to Mainnet and accepted. The shop dispatcher sent the main proof using verify proof and register and the verify proof and register as you will be able to see if you click on the links later on with beautifully shown with TX info. It just evaluates the proof and goes and also query the subcontract the memory pages and Merkel and fry to those parts to be valid as well, thereby validating those subparts. And once that has been accepted, the main contract of the application with Starknet or Stockx or whatever send the update state to update the state according to the batch being accepted on chain. Same with drawing. And here when you look later on I have links to Eroscan and also to ticks info to get into the execution of the transaction and the flow. It's quite amazing to see if you look into the ticks info of the JPS step and verify of a verified proof and register, you see the huge amount of facts being omitted, of events being emitted due to the facts being registered.
00:52:57.010 - 00:53:06.520, Speaker C: Just give you some sense of the numbers. Okay, questions?
00:53:09.770 - 00:53:25.198, Speaker B: Cool. Thank you for the presentation. I think the last thing you mentioned was the memory. I keep forgetting the name the page registry, right? That's basically those transactions. That's what we call it regularly, the data availability, right?
00:53:25.364 - 00:53:32.062, Speaker C: Yes. The data availability facts are recorded using those transactions, correct?
00:53:32.196 - 00:53:45.620, Speaker B: Because you mentioned that there are many of those transactions. And for me it brings to mind like why when we think about the cost of a layer two transaction, most of the cost comes from the data availability because all these transactions need to be sent to layer one.
00:53:46.070 - 00:55:10.960, Speaker C: At the moment it's by far the most dominant part. It's, I don't know, 90 or whatever or more than 90% of the God cost of shop are the data availability. And once we went validium, it's going to be way cheaper. Obviously validium for Starnet is a little bit more complicated because starnet being a general purpose and not one application or one operator, it's a tougher needle to thread. How you do validium in a way that is complete and sound and accepted and easy to maintain and to explain and to deploy or whatever. All those nice things are easier to do when you have your own application like this or that stock exchange and you have your own agreement with the various data availability providers. It's a little bit more complicated for startnet, but obviously since it's such a big limiting factor of cost and scalability, it's going to be, and it's going to be done, I think quite soon.
00:55:14.210 - 00:55:38.680, Speaker B: Yeah, that's why I think Stargate is going for volition to allow for people to choose if it's the roll up mode or validium for each transaction. But seeing the impact of the cost of data availability, it will be a big deal. Not only the prototype sharding, but also volition in the reduction of average cost per transaction on layer two.
00:55:40.270 - 00:55:42.934, Speaker C: It would be in order of magnitude.
00:55:43.062 - 00:55:55.694, Speaker B: Yeah. Great. Another question. You mentioned the layouts of the verifier. I think you mentioned that we have only one layout that we actually use on layer one.
00:55:55.732 - 00:56:07.680, Speaker C: Right. Not exclusively, and technically we can use all the layouts, but effectively for the recursion we use only one.
00:56:08.370 - 00:56:19.990, Speaker B: Right. And so that brings a question, because I know there's a verifier written in Cairo used by sharp for recursion. How would you compare those two verifiers, the one that we have in layer one and the one we use internally for recursion.
00:56:24.380 - 00:56:32.068, Speaker C: So you were asking how do I compare the solidity verifier versus the Cairo verifier?
00:56:32.244 - 00:56:37.740, Speaker B: Yeah, because I imagine the Cairo verifier will need to have more layouts available, more complex verification.
00:56:38.400 - 00:57:17.160, Speaker C: Yes, but it doesn't have the limitation of solidity or EVM. So if you can use, you run it on x 86 or whatever, you don't have to run it on EVM. So you're not bound to the limitation of gas cost, say, for Peterson hash or whatever. Peterson hash is way cheaper for chiral verifier than it would be for a solidity verifier. For solidity verifier, doing Peterson hash is a rather expensive operation for the Cairo verifier. It's pretty cheap.
00:57:19.100 - 00:57:25.736, Speaker B: I see. Go ahead.
00:57:25.758 - 00:57:47.760, Speaker C: Sorry. The fact that you run on a big machine with big memory and x 86 set of instructions is dramatic difference comparing with the limitation of running on EVM.
00:57:48.820 - 00:57:58.320, Speaker B: All right, well, thank you. We actually made it to the end of the 1 hour Tesla we have for today. So thank you, Rema, for the presentation about Sharp and verifier.
00:57:58.400 - 00:58:16.060, Speaker C: I will send you the link so you can post it. So if anyone wants, can also look at the most importantly, just reference to the code and to the Edoscan code or transaction that can put things into a more concrete context.
00:58:17.120 - 00:58:21.036, Speaker A: Yes, that would be great. I've shared the, yeah, I would just.
00:58:21.058 - 00:58:23.516, Speaker C: Send it to you just after that.
00:58:23.698 - 00:59:10.172, Speaker A: So for those of you who are watching this live on YouTube right now, we'll share the links on the YouTube comments as well as on Twitter where it is streaming, and then for those of you watching the recording, you have access to these. If you have any questions as well, feel free to post on our discord and we'll be happy to answer. I did not introduce myself throughout this community call. I'm Gyan, and I'm also one of the deverels at Stockware. And thank you so much, everyone, for coming to this community call. And if you can subscribe to our YouTube channel, you'll also get notifications on when the next community call is scheduled, what is the topic, et cetera. And I hope that you get to do some more research on the verifier and come back and ask us a lot of questions.
00:59:10.172 - 00:59:18.750, Speaker A: Thank you, Remo, for presenting today. We really appreciate it, and we're looking forward to sort of deep diving in the resources that you have shared with us.
00:59:20.800 - 00:59:21.788, Speaker C: Thank you.
00:59:21.954 - 00:59:22.828, Speaker A: Thank you.
00:59:22.994 - 00:59:23.640, Speaker B: Thank you. Bye.
