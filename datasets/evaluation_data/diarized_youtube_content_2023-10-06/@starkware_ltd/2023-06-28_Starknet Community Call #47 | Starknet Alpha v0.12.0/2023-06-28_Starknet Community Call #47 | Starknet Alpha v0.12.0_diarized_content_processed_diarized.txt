00:00:00.170 - 00:00:25.974, Speaker A: And we pressed. I pressed the button and we're live. So, hello everyone, and welcome to the 47 Starknet community call. I'm joined here by a bunch of people. Everyone. So we have people from Starkware, we have people from Lambda class, we have people from the Starknet foundation. And today we're going to talk about Starknet Alpha Point twelve.
00:00:25.974 - 00:01:04.560, Speaker A: More TPS, Daddy. So we're going to have a presentation about the technicals of zero point twelve. The goal would be for everyone to be familiar with it so they can make informed decisions on whether we should have zero point twelve live. Don't hesitate and ask your questions. If you're a part of the Builders council, you can ask them on telegram or in the YouTube comment section. If you're on the YouTube video, feel free to ask your question in the chat. And if you're in the audience of this call, don't hesitate and raise your hand and we'll make sure to get your question.
00:01:04.560 - 00:01:07.550, Speaker A: Evieta, the floor is yours.
00:01:08.450 - 00:01:38.600, Speaker B: Thank you, Henry. I'll share the screen with our slides for the community call. So first of all, nice to meet you. I'm Evetta. I'm from Starquare. I'm the product manager in charge of this version. So the main message for this version is of course, the impressive increase in throughput.
00:01:38.600 - 00:02:30.056, Speaker B: But we have three other additions in this version. We'll start with Zam so we won't lose all our focus. The first one is another syscall that will be introduced in this version. Change in the transaction status, and then we'll talk about the increase in the throughput and the new Cairo syntax. So the syscall aim to allow fetching all the past blocks block hash. It will allow us to fetch from within a contract blocks since the beginning of version twelve. On each environment, which is more than Ethereum.
00:02:30.056 - 00:03:30.416, Speaker B: In Ethereum, you could fetch 256 blocks to the past. And now we'll be able to fetch blocks all the way. Since the beginning, the implementation of this cisco will be through the state of Starknet. We'll keep it under address number one and we'll have a mapping between the block hash and between the block number and its block hash. We have a delay. We will be able to fetch only block that were posted at least ten blocks to the past in order for the sequencer have the time to finish all the commitment tree and computing of the new state without delaying the sequencer from creating new blocks. So of course everybody could use the Cisco for whatever aim and plans they have.
00:03:30.416 - 00:04:12.350, Speaker B: But one of the biggest features that this Cisco will allow are storage proof in storage proof? We have really nice post about it, but in general, it's the ability to prove that you have a certain state in a certain blockchain, and you could use this proof also in other chains in smooth and convenient way. That wasn't possible until now. So this is the first feature for 0.1. Next, we'll talk about all the.
00:04:13.040 - 00:04:15.532, Speaker C: Do you want to stop for questions or just go?
00:04:15.666 - 00:04:30.610, Speaker A: So I see that there are no questions online on YouTube at least I actually have one. When you say the block hash will be written on every block at address ox one, what does that mean? Does this mean that it will be written in a smart contract whose address is ox one?
00:04:32.200 - 00:04:51.880, Speaker B: It's not really a smart contract. There is no code under this address, but this is a special address related to the implementation of this specific Cisco. And under the storage of this address, we'll have a mapping between the key, which is the block number, and its hash.
00:04:52.400 - 00:05:16.896, Speaker C: Yes. One of the drawbacks in the implementation of get block hash opcode in Ethereum is that if you want to allow it, you need to have two types of. You suddenly need the historic information for the execution. Right. Which you would ideally only want the state itself. So when we make this part of.
00:05:16.918 - 00:05:17.750, Speaker B: The state.
00:05:19.640 - 00:05:29.750, Speaker C: We do not require any other information apart from the state to allow the execution of a transaction. So that's the reasoning for that.
00:05:31.660 - 00:05:39.332, Speaker A: Makes sense. Thank you. All right. And I don't see more questions either on Telegram or on YouTube, so I think we can proceed.
00:05:39.476 - 00:06:32.010, Speaker B: Great. So the next thing is actually all the depth that we have and a little bit ambiguity from the past. Today, the lifecycle of transaction is accepted when it's received, when it's received by the gateway, by the mimpool, and then it go to the sequencer. And once the sequencer include the transactions in the pending block, which we talk about the meaning of the pending block in a second. Until today, it was signed as pending transaction. Then the block was closed. The transaction got to be accepted on l two, like the block, and then proved and to be accepted on l one.
00:06:32.010 - 00:07:36.290, Speaker B: The meaning of pending is very clear when we talk about the block itself, pending block, meaning that the block isn't full yet, so the sequencer didn't close and submitted the block to Apuf. Its state can change, and the block hash will probably change until the block will be closed. Until now, transactions in the pending block also had the status of the block itself. But actually, after the sequencer included a transaction in the pending block, its finality is the same as the finality of block that was already accepted on l two. So, the actual status of the transaction is actually accepted on l two. So, since version twelve, we won't have anymore the status pending for transactions. Meaning that once the transaction was already processed by the sequencer, it will be automatically accepted on l two.
00:07:36.290 - 00:07:56.180, Speaker B: We think that it will reduce ambiguity in the future, because it wasn't that much clear for every developer and user in the community. What is the meaning of pending transactions? Any questions about this, Henry?
00:07:57.480 - 00:08:03.880, Speaker A: Nope. This is fairly clear to me, too. And I see that there are no other questions also in the chat.
00:08:04.220 - 00:08:55.130, Speaker C: Yeah, maybe I will say something on that. The importance of that, together with the improvement in the execution time and the throughput, is that the finality of transaction should reach accepted on l two much, much faster now, even if the demand on Mainet, for example. I don't know. Not a lot. The fact that the sequencer just execute the transactions all the time, step by step, and apply it accepted on l two to them, means that, from what we've seen so far, approximately, we'll see something like ten second or something around that confirmation times for transactions. So, that's the big importance, in my opinion, for this.
00:08:55.820 - 00:09:07.420, Speaker A: Absolutely. It will change the UX of Starkhand for users drastically. So, Milan actually has a question. He's saying, can reorgs affect the transaction status?
00:09:09.280 - 00:09:30.450, Speaker B: Yes, as it can today. But the probability of Reorg will affect accepted on l two transactions the same way it will affect the pending transactions. So, in that matter, there is no actual difference between the two. And this change will, I think, make it more clear.
00:09:31.220 - 00:09:41.830, Speaker A: Yeah. Do you mean reorgs? I'm not sure the question to say reorgs, but I'm guessing this is reorgs on l two.
00:09:42.200 - 00:10:03.880, Speaker C: So, Reorgs on l two cannot affect the transaction status. The only thing that can change the transaction status is reorgan l one. For example, if a transaction is dependent on a message from l one, or something like that, then this can change the transaction status. But as the vatar said, this is true for all cases.
00:10:04.040 - 00:10:21.030, Speaker A: Yeah, but if there is a reorgan l two, it can affect the status of a transaction. Imagine you want to, I don't know, do an action in a game, and then there's a reorg, and somebody did it before you. Then your transaction is not going to work anymore, right?
00:10:23.000 - 00:10:35.032, Speaker C: Yes, but this will be true once we'll have the fee market. But as we currently sequence transactions, by the time that they arrive to the sequencer, yeah.
00:10:35.086 - 00:10:38.920, Speaker A: There's very little chance that you will pick another order of transactions.
00:10:39.660 - 00:10:40.680, Speaker C: You won't.
00:10:42.300 - 00:11:05.232, Speaker A: Okay, understood. And I guess another question would be once the transaction, I mean, if the transaction is accepted on l two in a pending block, what is the likelihood for that transaction to later be rejected once the block is actually finalized? This was an issue in the past. I don't think it's an issue anymore. But is it just to make sure.
00:11:05.366 - 00:11:27.050, Speaker B: It'S not an issue anymore? And it's actually the same as block that already accepted on l two. The sequencer won't fail the transactions after it possessed it in the pending block. We won't change the prices and it shouldn't happen.
00:11:27.740 - 00:11:29.370, Speaker A: Wonderful. Thank you.
00:11:32.240 - 00:12:40.012, Speaker B: Great. So I think that we'll go forward to the main event of this version, which is the throughput improvement. We'll talk a little bit more afterward about the throughputs and the measurement we've conducted and what are the measures that we need to think about and to talk about. But before, we'll talk a little bit about what made it possible. And I'll say that after the test that we conducted with all partners in Starknet yesterday and two days ago, we are now very comfortable and very sure with the message that we sent earlier about the more than seven x improvement. But before, let's talk about what made it possible. So there are three main ingredients in the new solution.
00:12:40.012 - 00:14:14.300, Speaker B: In the new version that made the sequencer really different than the current sequencer that we have. The first one is Cairo Rs that fed and the folks from Lambda class will talk about in a second, which is Cairo Runner developed by Lambda class blockifyer that was developed in Starquare, which is a rust based starknet runner that basically replaced the current sequencer and instead treating the transactions with the pythonic sequencer. We'll do it with the rust sequencer, and we'll use the Cairo Runner in rust instead of the current Cairo runner that is written in Python. And the last thing in this version is having more efficient way for managing the local state of the sequencer. We understood that the time of updating even the local state of the sequencer was a bottleneck in the former sequencer. So we used papyrus, which is a full node developed by Starquare, and we used as storage management for the local state. Okay, so, Feder, would you like to talk about Kairares?
00:14:14.640 - 00:14:54.520, Speaker D: Yeah, sure. Can you share the screen with the slides or should I do it? Okay, perfect. So, yeah, the first thing I wanted to say is that Ras was obviously a key ingredient in what we did. But I don't think it's the only piece in the sense that Ras in general gives you a two to ten x performance gain in memory. The gain is way bigger. It uses way less memory than the Python VM or pypy. But in terms of performance in general, removing a huge cases, in most cases it's ten x maximum.
00:14:54.520 - 00:15:34.772, Speaker D: In most cases it's like five. I will share a link about this. What I should said in YouTube, and I'm saying that because I think most of the work we did was take what Stockworth did. That was amazing. And obviously always when you do a rewrite you have the possibility of understanding on improving the algorithms, on the data structures that were used on the general architecture. So from my point of view, the meme of like rust, it's a good meme. I have been in the rust community since 2014.
00:15:34.772 - 00:16:03.600, Speaker D: I love it. But I think that the biggest opportunity that we had was the time that Star wars gave us working with them, learning on, working together on the full redesign. So I think that's the biggest improvement in terms of like we could re architecture the full VM. Could you go to the next slide? Yeah. So the long story short, I did talk. What's Cairo Rs? We're changing actually the name to Kairobm. So that's even more clear.
00:16:03.600 - 00:16:54.544, Speaker D: The name in the crate in the packages of rust, it's already called Cairo RS was only the name of the GitHub repo. We didn't want to do it before because we didn't want to confuse the community. The Cairo VM until now was the Python VM. So we're going to change that. There shouldn't be any type of issues with that. We're going to talk a little bit about where this fits in the ecosystem, how we did build it, on how we could improve the architecture, performance on design, why we did, how we did it, how you can use it, some stats on some testing. Could you go to the next one? As you can read here, the full process is like we take the code from that Cairo code.
00:16:54.544 - 00:17:26.584, Speaker D: Now Cairo one gets compiled by the compiler, an amazing compiler. If you ever have the chance, take a look at that code base. It's actually from my point of view, better than the KairobM code base to learn rust. It's really, really good. The team of Starware did an amazing job. They use a lot of interesting traits and tools that generates a program. We take the program, we generate a trace on the trace is what's proven by sharp or in the future hopefully.
00:17:26.584 - 00:18:09.930, Speaker D: Also the lambdaworks proverb that we're doing. So the VM, it's sequential for now, takes the code compiled by the Cairo compiler, then the execution gets proven, the proof gets uploaded to the luan on verified by the smart contract. Could you go to the next one? Yeah. So right now the Cairo VM has two modes. It can be used as a runner only to run the transactions. On Cairo one, compile code or it can also generate proofs. That's pretty important because there is a big performance difference.
00:18:09.930 - 00:18:44.180, Speaker D: That's related to the fact that when you run it in proving mode you need to keep a track of all the addresses that are touched by your code. And that's obviously slow because every time you do something you're inserting or looking for things inside the data structure. So if I'm not wrong, please correct me, Tom, or anybody. I'm pretty sure I'm too sleepy. That's why I am asking that on the sequencer we're not running the trace generation, right? If I'm not mistaken.
00:18:46.200 - 00:18:54.790, Speaker A: No, to the best of my knowledge they're not. Yeah, Abdel is saying, fede, you should rest. Stop burning the dense floor in.
00:18:55.400 - 00:19:09.420, Speaker D: Yesterday was a funny day. Actually it's ready to this it was because we like what we do and sometimes we stay up to late. But I cannot give a few updates about new things. But yeah, this is the general flow. Is there any question about the general flow?
00:19:11.680 - 00:19:13.688, Speaker A: No, there's no question on YouTube.
00:19:13.864 - 00:20:07.484, Speaker D: This is something I want to double down a little bit on stocks. So one of the big amazing things about stocks is that in general, in comparison to snarks, they're designed more for virtual machines. So there is a separation between the proverb and the actual DM. That's very good because you could in theory nowadays if we wanted to with some engineering work, obviously, but in theoretical terms we could run the VM as WaSm, thanks to the work of Timothe on Abdel. We could run the VM standalone in a browser, run a Cairo program and send the transaction to be proven to the sharp prover. Doing that with a snark would be way more difficult because here from the design you have the VM that generates the trace and you have a prover that proves it. So those two things are separated.
00:20:07.484 - 00:21:13.218, Speaker D: And that's amazingly good because you can have a centralized server that proves things, but you don't need to trust that server because mathematics on the verifier, this makes the Kairobm something that we can put in a lot of places in the yacht on the proving done in our servers. Next slide, please. Yeah, so the first reason that it's not here of why we built it was because we like what Saguar was doing and we wanted to learn. So that's how we approached the problem. It was an engineering interesting problem we have been doing for ten years, performance analysis. We manage millions of users in different products. We saw that the code base by Kyrolang, the code base by stackware was really interesting, but we thought a reorg of the whole code would be good not only for performance, but if we wanted to ever change the code base.
00:21:13.218 - 00:21:46.566, Speaker D: If it was in brass, it would be safer. It will be easier to iterate on. That would add robot's nest. So from our point of view, performance was important. We were expecting a big performance improvement, not as big as we expect as the one we got. We will show numbers afterwards. But the two reasons were security in terms of being sure that any change wouldn't break the whole system or safety encoding terms, not in terms of money going through the VM.
00:21:46.566 - 00:22:17.830, Speaker D: The second of all was speed. So if you can. Let's go to the next slide. Yeah, so why rust? It's one of, from my point of view, our point of view, one of the best programming languages. It's very fast, it doesn't have a garbage collector, it's memory size. So you get the things that are good from C plus the things that you get from language like Python, like high level language. The community is amazing, the crates are amazing, the documentation is amazing.
00:22:17.830 - 00:23:07.000, Speaker D: As with everything, it has some issues, but in general it's a very good language. Writing vms, compilers, databases, network stacks, anything that Aki or other member of my team wants to add. Perfect, let's go to the next one. So, yeah, I remember with Federica the first few days we had a headache. We had to take the code base and start understanding the full process of the VM. Many times we thought we understood it, many times we felt that we were smart on reality kicked in and was like, no, you didn't get it. We read the papers, we read the code.
00:23:07.000 - 00:24:00.310, Speaker D: So the first few months were interesting because we didn't have the full picture on. Obviously we didn't want to bother saga, always answer any question that we had. But it's like when you get a new job and you're a junior, you don't want to go to your boss being like, even if he's open on understanding, you only want to be like, hey, how this works, how this works. So we had to take the time to understand it. This is a small flow now. I think it's a little bit more complicated even than this of how the previous VM worked. One thing I want to do, I'm not completely happy yet because we're prioritizing delivery, is that I want to write a lot of documentation, a really small book, like a companion of the Cairo land community, book that is already in the wild, that is really good.
00:24:00.310 - 00:24:31.710, Speaker D: We want to write a small, really book more technical about how the Kairobm works with all the flow, so that anybody coming from the outside can work on this. It's something we want to do, like with Aki and the team. I think we're very good at writing documentation. We want to, before the end of the year, have something pretty solid, not very big, but pretty solid explaining each part, what's done, how it's done, why it was designed that way. So this is just something to show for now of the type of documentation that we want to work in to make it even clearer to the community.
00:24:31.860 - 00:25:02.266, Speaker A: So Zogiz is saying the picture is too small for me to see. So the point is, wait a little bit and then you'll be able to read the full book and understand how the Cairo VM works. We also have gedbut who is asking a question about Rust and he's saying how concerning to you is what is happening with rust and its different forks. Like it doesn't matter.
00:25:02.448 - 00:25:04.614, Speaker D: Okay. It can be a very long topic.
00:25:04.662 - 00:25:06.566, Speaker C: You should just listen to Inyaki.
00:25:06.758 - 00:25:25.554, Speaker D: Yeah, I think it's going to be fine. It's just politics, community, it's a big language. When a family grows, you have these issues. So I don't know. I'm used to that. It's open source, I have my own opinions, but the language will be fine. It's already used by from intel, Amazon, Google.
00:25:25.554 - 00:25:34.818, Speaker D: With folksia it's already too important. There won't be any issues. It's just one small comment.
00:25:34.834 - 00:25:39.586, Speaker B: You can find this diagram in the chirs repo in the docs folder.
00:25:39.778 - 00:25:40.230, Speaker A: Nice.
00:25:40.300 - 00:26:10.974, Speaker D: Yeah. The objective of the diagram is not to explain you each part. It's more like to explain you how we started on the full process again, as Enri said, you can check in the web page in the future, the small book we're working in. Next slide please. Yeah, I would like born or Peter to talk about a little bit about hints if you want to. If not, I will do Peter or Bono. Bono.
00:26:10.974 - 00:26:12.660, Speaker D: Do you want to mention anything about.
00:26:13.590 - 00:26:56.862, Speaker E: Yeah, yeah. Hints were one of the portions of the original VM that took more time because of they are basically python code. Implementing them in rust was a difficult task, but it lead to an increase in performance, a really big increase in performance. But yeah, the problem was that since they are basically python strings, we need to match it and it was not as extensible as we would like.
00:26:56.916 - 00:27:18.950, Speaker D: But yeah, by matching we mean literally matching. Searching the string in the program and when we found the hint we were like, no, don't run this in Python, we're going to run this rascal. And we had to make sure. That's very difficult and was the most difficult thing is to make that the execution is exactly the same than the one that was beforehand.
00:27:22.170 - 00:28:03.118, Speaker E: I want to add something more. Yeah, so basically the approach we took was to develop an interface so users can plug in hint processors like you can extend the hints that the Kaido VM can execute and in that way you can, for example when running a Kaido one contract, you can plug in Kaido one hint processor and implement them in rust and forget all the python matching and just match on a rust enum and make it easier. So it's really extensible.
00:28:03.294 - 00:28:22.890, Speaker D: Yeah, nowadays it's way better. At the beginning was the search and replace and now we have like a plugin system. But it took us some time to design it and to make sure that it was useful to others. Aki, anything you want to add there or let's move to the next. Perfect. Let's move to the next slide. So yeah, that's the way you use it.
00:28:22.890 - 00:28:45.970, Speaker D: Now it's Cairo BM. We need to update that. But the link is working because of the redirects. So you clone the repo, you cargo, build it with release and you can use Cairo compile to compile a program and you run it. It's pretty simple. Peter, do you want to go through the library how it works on the. Yes, yes.
00:28:45.970 - 00:29:09.414, Speaker D: You can also use it as a library instant new virtual machine, a Cairo runner. Well in that part you also instantiate the hint processor. That is the one who is able to run all the hints that we want to explain earlier. You pass the JSON file, compile and it runs.
00:29:09.462 - 00:29:10.060, Speaker A: Okay.
00:29:11.390 - 00:29:19.022, Speaker D: Also it supports Kyro one contracts. You have to compile it to Cairo assembly and it's also able to run.
00:29:19.076 - 00:29:22.400, Speaker B: Them the entry point that you want.
00:29:24.130 - 00:29:27.978, Speaker D: Cool. Any questions, Henry? Or if not, let's move to the next.
00:29:28.084 - 00:29:31.826, Speaker A: No, it looks clear. There are no questions. Perfect.
00:29:32.008 - 00:29:54.482, Speaker D: Oh, something happened there. We're not very good at slacks. We're better at code. So the project is pretty big. We have a process where at least two members before it goes three. I think at 1.2 or two members have to review anything that gets into the vm.
00:29:54.482 - 00:30:28.094, Speaker D: We want to make sure that we get anybody from the community. Taking the effort to add a pr here means that they are doing a big effort because it's not easy. You need to understand a lot of things. So we wanted to make sure to get any pr from the outside, but we wanted to make sure there were no security issues. We wanted to make sure that nobody tried to add the bag. So we had a process of, at the beginning, three members, now two because we're working more on stagnant and rust. We have more than 2000 tests, 97% coverage.
00:30:28.094 - 00:31:13.570, Speaker D: I cannot tell you how many discussions I have with my team about testing fasting. We should go fast, that we need to make sure that things are working fine. We have more than 200 programs being run while comparing the memory, the trace generation with the Python VM in pypy on CPython, because we want to make sure that everything is exactly the same. And this is something we got from the beginning. Like, you cannot add this after doing the VM. You need to start from scratch, making sure that a no security bag is added, B, that safety is in place, that you have enough programs to compare things with the rest. We also compile this in.
00:31:13.570 - 00:32:07.774, Speaker D: Well, Debian Ubuntu are the same from my point of view, or almost the same, but we're having this compiled in Debian on Mac. Yesterday we were discussing with the DevOps team that we should support windows also by default in case we want to run this in as wasm or in Windows in the future. So you can enter the repo right now and check the numbers whenever you want. Next slide. Yeah, so also this is important. Every time we send a PR, we have some CI running things on one of our machines and we get if we did an improvement in performance or a decrease in performance because we wanted to make sure that we didn't. These things are very complex systems where one layer can add something worse to another layer.
00:32:07.774 - 00:32:21.638, Speaker D: It's not a very direct system because it has so many layers. So a small change somewhere can be a big issue somewhere else. Yeah. Next slide. Yeah, sorry, I'm being too slow. I'm too sleepy. So these are the numbers.
00:32:21.638 - 00:32:48.970, Speaker D: It's 600 times faster than the Python VM. The important thing is that nowadays we can do a lot of improvements once this goes to production, it can be way easier to iterate. Yesterday we added a pr that makes 40% things faster. With lambda works, we have too many things that we like. Performance is important, but for me the methodological part is more important. We have something in place where we can iterate on. We are sure the VM won't be a block.
00:32:48.970 - 00:32:55.294, Speaker D: So that's all. I don't know if Aki or Peter or Bono can add anything. I'm sorry for taking too much time. I'm too sleepy.
00:32:55.422 - 00:33:00.930, Speaker A: It's fine, don't worry. This is really interesting. Thank you. Bono and Yaki, Peter.
00:33:02.890 - 00:33:04.280, Speaker B: Nothing much to add.
00:33:06.250 - 00:33:07.000, Speaker D: Questions?
00:33:08.570 - 00:33:14.562, Speaker A: There are no question. There are discussions in the comments, but no specific question regarding what you're presenting.
00:33:14.706 - 00:33:33.950, Speaker D: Yeah, that's for example, that 300 to 600 increase was done only yesterday with one pr. So that's why I'm saying like performance is important, but the methodology of creating something that we can change, it's even better. I'm sure after co twelve we will be adding a lot, lot of of changes that will make the VM on the whole system more performant.
00:33:35.730 - 00:33:43.700, Speaker A: This is fascinating. Thank you. All right, evie, do you want to present the rest of the.
00:33:48.150 - 00:35:03.310, Speaker B: Did you finish? Okay, so let's talk a little bit about how to measure throughput in general and specifically in our case. So the most straightforward measure for throughput is of course TPS transactions per second. The big problem with transactions per second is of course, that it's very much dependent on the exact transaction that's being executed. So it's not really a good way to compare Apple to Apple, because in one chain, most of the transactions could be demand, require very small amount of computational resources, and in another change chain, it will require much more. But it's the most consensus way to compare. So we will talk a little bit about this as well. Another way that was suggested to measure throughput was by measure how many ERC 20 transfers we can do in per second.
00:35:03.310 - 00:36:40.792, Speaker B: This is a little bit better way to compare between different chains because the computation here is pretty much the same and it's common and basic building blocks in any other chain. A better way to measure throughput, I think, especially in Starknet, is the chiral steps per second, as the chiral steps are really almost linear in the complexity of the computation we want to do in the sequencer. So measuring the chiral steps per second is maybe the best benchmark out of all those three to measure the stagnate improvement. But of course, it's a little bit hard to compare between different chains as different chains uses different virtual machines. Another way that I want to talk about and just to mention but we are not able to measure it yet until we'll have fee market is maybe the right way to compare between chains and the throughput. And the demand for the throughput in the future will be by the fees collected in the sequencer, because that way you can actually know the actual demand for each network. And this is the same metric for all networks.
00:36:40.792 - 00:38:02.548, Speaker B: It will be possible once Starknet will have the fee market. That will allow us to determine the fee and the prices according to the actual demand or end supply of the network. So just one last comment before we'll talk about the measurement that were taken about our current bottlenecks. So as Feder mentioned, and as we talked and communicate through the way, the main issue that we wanted to tackle is the performance of the sequencer. The sequencer is the most complicated component in the network, and is also the only component in the network that is single tone, meaning we cannot have multiple copies of the sequencer. The proverbial for example, we have a lot of provers working in parallel. So even if one proverb has relatively low throughput, if we will operate a lot of provers in parallel, we could breach whatever throughput needed.
00:38:02.548 - 00:39:33.504, Speaker B: But the sequencer, since we need consensus on the history, has to be one. So our focuses, both starquare efforts and lambda class efforts, was focused on the sequencer. Another component in the network is our temporary gateway, which will be replaced by more sophisticated min pool in the future. In version 14 probably, but so far is pytonic service that is accepting the transaction, making a very scene verification, and added to the storage so the sequencer will be able to process those transactions. The sequencer is not a single tone, but yet there are few process during the gateway that are singletones. For example, in order to prevent duplication of transactions and to have only one transaction hash in the system, so you'll have direct connection between the transaction hash and its status. So this process is not a heavy process, but it has to be a single tone.
00:39:33.504 - 00:41:30.250, Speaker B: In our current implementation, and so far it wasn't a button leak at all, since it was much simpler than the sequencer. Now with the processing and progressing to our sequencer, that became much faster. In some cases the current buttonneck will be this gateway and we are working on solving it, and I believe that we'll have much better performances in the sequencer in the really near future. But in version twelve it might limit a little bit the maximum throughput that the sequencer can handle. So as for the number when we tried to measure yesterday, simple transactions just to have like ERC 20 transfer, one transfer per transaction just to have a feeling of what are the capabilities of the new sequencer. We've reached to more than 70 transactions per second in the sequencer, but the gateway is limiting us to about 40 transactions per second, which is I think more than ten x our current performances. As for ERC 20 transfers, when we try to use multiples and to avoid the gateway limitation, and maybe to try and test a little bit more realistic transactions in a block, when we use like 20 multiples in one transaction, to have 20 transfers in one transaction, we've reached to 350 transactions per second.
00:41:30.250 - 00:43:01.830, Speaker B: Or if we prefer to talk about Cairo steps in average transactions, we've reached more than 220 kyro steps per second, which is our current high record of Cairo steps per second is about 30k transactions per second. So we are talking about more than seven x improvement in the average case of the version twelve in compared to the peak and the highest record of version eleven. Another measurement that I think will be interesting for all of us is the latency, meaning the time between the sending of a transaction until you could query and have a result of accepted on l two. And we will reduce it in version twelve to about 32nd in average. And of course that in all fronts we will improve in the near future. So this is a little bit about our test from the last days that have been conducted with all of the full nodes of Starknet and of course with all the partners building the network. Emery, any questions so far?
00:43:02.940 - 00:43:25.180, Speaker A: I'm super excited about latency dropping. I can't stress that enough. I think going from tens of minutes to under a minute and to tens of seconds is very good. So I'm super excited about that. There are no specific comments or questions in the sequencer.
00:43:27.280 - 00:44:23.824, Speaker C: Yeah, just to add on what you said, henry, before, we're going to the contact right now. The main mean for the past, I would say, let's say few weeks, month, probably a bit more, is borderline unusable. And I have a ton of appreciation for everyone who's building, even in this state. And to me, I don't care about numbers, but the most exciting stuff is that it will change maintenance to be something that will be happy to deploy to and to get users to, and to do campaigns and to do cool applications. To me that's very exciting. Just to allow the community to finally unleash what's been building for quite a long time.
00:44:23.862 - 00:44:58.252, Speaker D: Now, Tom, can I add something about that? I completely agree. I think it's not only a number. At one point the number won't change a thing. The thing will be so fast that it won't be a big blocker. I think what this will enable is what you just said is that people will be happy of working on top of this and you will be able to do things that you cannot do nowadays in any blockchain, like, I don't know, machine learning. With this type of performance, there will be a lot of things that will be able to be created on top of it. So I agree.
00:44:58.252 - 00:44:59.150, Speaker D: I'm really.
00:45:03.280 - 00:45:05.692, Speaker A: Yep, there are no other questions in the.
00:45:05.746 - 00:45:07.470, Speaker B: Got a question over here.
00:45:08.500 - 00:45:19.792, Speaker D: Could you expand a little bit on why is the gateway bottlenecking the transaction throughput? Overall, I'm not sure I really followed there, sorry.
00:45:19.846 - 00:45:22.256, Speaker B: Can you come again about your question?
00:45:22.438 - 00:45:33.192, Speaker D: From what I understood, you said that the performance of the sequencer allowed us to reach something like 70 dps, but you were getting bottlenecked by the gateway, is that correct?
00:45:33.326 - 00:45:33.960, Speaker B: Yes.
00:45:34.110 - 00:45:38.760, Speaker D: Could you explain a little bit how that happens and how it can be tackled in the future?
00:45:38.830 - 00:45:39.450, Speaker A: Maybe?
00:45:40.140 - 00:46:43.720, Speaker B: Yes. So we have actually a few gateways that with load balance between them to separate the balance on all the gateways. We could have as many gateways as we want, but we have two kinds of sequential handlers in the gateway that work one by one. One of them is related to the writing of the actual transaction in the storage. Today, when one of the gateways is trying to write transaction in the storage, it locked the storage and released it only after it wrote the transactions. So this sequential process is one of the processes that limit the rates in all gateways. Because even if we have like five x more gateways, this process is still sequential.
00:46:43.720 - 00:48:11.930, Speaker B: One of the ways to solve this problem in the really meantime is in a really short term, is just to skip this log, for example. Another thing is that every gateway need to have need until version 14, our intentions are to have sequential id number for every transaction. So this is also a sequential process. And one of the way to solve it is to have each gateway have list of allowed ids that it can give to each transaction. So this process will also be able not to be a sequential and to happen in parallel. So if to summarize in one word, there are some processes in the gateway that currently are sequential, because so far it wasn't a bottleneck. And the solution in the short term is just to make them parallel, which shouldn't take much time just to add on that.
00:48:11.930 - 00:48:12.872, Speaker B: Great.
00:48:12.926 - 00:48:26.856, Speaker C: It's not that we're waiting for future versions. This is already in the works, and since it's just the gateway service, the hope is to release it as soon as possible without the need to have any upgrade.
00:48:26.968 - 00:48:27.196, Speaker D: Right.
00:48:27.218 - 00:48:35.090, Speaker C: So those are problems. We'll tackle them. I mean, people are working on it at the moment and it's not big.
00:48:36.420 - 00:48:59.944, Speaker A: A. I'm trying to find a good meme for that, but I can't. But you probably saw those videos on YouTube where you see people taking like a piece of a big dam or something and then water burst. But there are other smaller bottlenecks afterwards. We're at this stage we removed a major bottleneck and there are a few that we're going to remove in the coming weeks basically, but they're much easier than the previous one.
00:49:00.062 - 00:49:10.430, Speaker B: Exactly. And also today, the next time that Starknet will be congested, Starknet will be more in use than Ethereum itself.
00:49:12.640 - 00:49:18.610, Speaker D: I'll wait to eat up all that throughput at brick. Exactly.
00:49:19.620 - 00:49:39.910, Speaker C: We have a new challenge for you. Okay, great. So we don't have a lot of time, so maybe there's still one change and it's related to the Cairo syntax. So if we don't have more questions, maybe. Yes.
00:49:40.600 - 00:50:50.540, Speaker F: So I'll take over for a minute or two. We'll try to be brief. So the last thing with twelve is the compiler upgrade to V 2.0 and presenting the new, which introduces the new contract syntax. So we'll talk about it a bit in a minute, but before saying some details about the actual contact syntax from this version onwards we're going to support on Starknet classes submitted from different compiler versions. So basically, if there's now a compiler upgrade that includes some breaking changes, you have a period of at least six months to keep working on your contact with the old compiler and eventually declare or deploy it on Starknet, at which point it will last forever. So this is the guarantee from now onwards in regards to breaking changes in the language syntax.
00:50:50.540 - 00:50:54.300, Speaker F: Can you go over to the next slide?
00:50:56.240 - 00:50:56.652, Speaker D: Yes.
00:50:56.706 - 00:52:08.544, Speaker F: So now specifically about the new contact syntax, I only have about a minute, so I just mentioned the headlines. There's a more detailed post in the community forum and some migration guidelines in the documentation, but essentially we're separating a contract into an interface and an implementation or trait, and an implementation which allows you to make sure that your contact has exactly the desired functionality, nothing more and nothing less. There's now enforcement on view functions. So ever since Kyozio you could annotate functions that are supposed to not modify the state as view functions. But this was not actually enforced, you could just write to storage. So this new syntax enables this kind of enforcement, which should help avoid bugs eventually, as when it's very clear whether or not a function modifies the state. With a new syntax, events has been changed.
00:52:08.544 - 00:53:05.128, Speaker F: Also, up until twelve, events still looked similar to how they looked in Kyozio. They were basically defined as functions, which was weird. So now we have the event type and corresponding emit syntax. And finally there's the foundation for components. So components are not in this version, but the new syntax is what will allow us to introduce components in a non breaking way, which is a very critical feature in the language. In a few words, component is basically using the functionality defined externally in my contract, analogous to solidities. So if I want my contract to be ownable, upgradable, whatever, and is defined elsewhere, elsewhere, and I expect it to enrich my external functionality, then components is the way to do it.
00:53:05.128 - 00:53:22.030, Speaker F: You can do it now, but it looks a bit weird. There are walkarounds to do it, but with the new syntax and components, there will be a much nicer way to go about it. Any questions?
00:53:23.680 - 00:53:43.720, Speaker A: Nope, there are no questions on YouTube. If you want to know more about the new contract syntax, we did a community call about it last week. I invite you to check out the replay. I mean not URL, but the people listening. Right. I don't know if there are questions in the builders council or elsewhere. I don't see any.
00:53:43.720 - 00:53:55.348, Speaker A: Okay, so should we move forward? We reached the end of the presentation.
00:53:55.524 - 00:53:56.250, Speaker B: Yes.
00:53:57.360 - 00:53:58.110, Speaker A: Nice.
00:53:59.280 - 00:54:34.900, Speaker C: Can I say something very. Just to name the elephant in the room, this is yet another change in Cairo. Right? And the question is, okay, so will this continue to be like that? What's the plan? I mean, okay, we have some compiler support for six months, but still what's the plan? So first of all, I completely understand and it is not the intention to continue, evolve and change. The changes to the contract syntax.
00:54:38.040 - 00:54:38.356, Speaker E: In.
00:54:38.378 - 00:55:23.030, Speaker C: My opinion, were blockers for a few teams to actually start the migration to Cairo, one from Cairo zero. So we basically very much prioritize them. And currently there is no plan to break anything. And this is stable. What I want to say is that, at least from my perspective, and if I'm missing something, I would love to understand it. Zero point twelve mark the time that the network is ready. I mean the language is stable enough, the documentation exists, the tooling is in my opinion quite mature to allow developing, and the network can support the demand and throughput that this will bring.
00:55:23.030 - 00:55:38.764, Speaker C: So it's been a long time and I'm very excited about it because it's been a long time, but for me this is what this version means. So again, if I'm missing something I would love please wake me up.
00:55:38.882 - 00:55:40.190, Speaker D: But I really feel.
00:55:42.560 - 00:55:53.170, Speaker C: This is my message or what I want to try to communicate externally. So this is what I wanted to add.
00:55:54.420 - 00:56:08.710, Speaker A: Thank you. Tom, there was actually a question from beta asking when is zero point twelve to be available on Testnet? I don't think we talked timeline yet, did we?
00:56:09.240 - 00:56:47.600, Speaker B: No, we didn't talk about timeline yet. But our current plan is to release the version to integration tomorrow. So we will have the stable release tomorrow on integration. So we are planning to launch on Testnet next week. There is no an exact date yet. We'll discuss it, but the plan is to have if the vote will approve the version, we hope to have it on Mainet before ECC.
00:56:49.460 - 00:57:03.990, Speaker A: Fancy. This is going to be exciting. Cool. All right folks, I think we have reached the end of the call. We presented everything we had to present. I don't know if anyone wants to add something.
00:57:04.680 - 00:57:17.960, Speaker D: One sentence. We also released Starnet in rust. That is synonymous blockifier. So if you want to integrate it, you can do that. Right now we will be sharing more news in our blog and probably also stragnet on strategy.
00:57:18.700 - 00:57:52.390, Speaker C: Yeah, and this is super important that it comes out together with integration because as the infrastructure matures, we want to ensure that we are able to develop all the different implementations in parallel and not in a sequential way. So it's amazing that it was made available with the features of zero point twelve today. And really good job. And I mean it's amazing that the first release of it is when the version is ready. So yeah, really good job.
00:57:53.880 - 00:57:55.930, Speaker D: Thank you. Bono didn't sleep a lot.
00:57:56.780 - 00:58:15.356, Speaker E: I want to add something that it is meant to be standard runner, that every library or tool that wants to execute transactions or execute a given Kaido contract, you can use it and build.
00:58:15.458 - 00:58:16.670, Speaker F: On top of that.
00:58:23.140 - 00:58:58.044, Speaker A: All right, well folks, again, so I'm going to say it on behalf of the community. Thank you to everyone involved here for their hard work on this new version. These things are not easy to ship and there's a lot at stake and there's a lot of work to be done and the people behind the screen don't have a way to say it, so I'll say it. But we're thankful for the work everyone is doing here and it's really inspiring to see everyone coming together from all side of the world to build a future like that. So congrats on shipping. That new version will be one. Happy to break it.
00:58:58.044 - 00:59:10.700, Speaker A: I'm sorry to test it very soon. And looking forward to the next one. Thank you, everyone. And to the people at home. Until next time. Bye bye.
