00:00:03.310 - 00:00:46.720, Speaker A: All right, guys, we've been told we can start, so we'll just start. So basically, who the hell we are. We're like from no foundation. And what we're going to show now is, well, let's say it this way, it's the way to prove heavy competition to Starknet and to eth from mainstream languages without, let's say it this way, without proof generation, performance issues. Yeah, that's triggering. Our folks have been hidden after this title. So yeah, this is basically what it is.
00:00:46.720 - 00:01:47.650, Speaker A: That's like why, what for? And shit. Who did that anyway? Why do we need this? Right? Well, basically, why do we need this kind of proven rust or like CPP or anything, like proving computations in Rust and CPP or anything? Why do we need this? Well, basically, let's say it this way. Actual applications need taxes, dy dux, for example. They require scalability, and obviously this scalability requires a lot of data and computations. Well, that's true as well. And basically computations, when I'm talking about computations, I'm talking about computations, continuous computations, not like computations which can be done inside the virtual machines or like anything like this, like evms or like anything like this. But it's about continuous computations, like ML stuff or consensus proofs, or like ZKVM proving, or like anything like this, like evms, whatever.
00:01:47.650 - 00:02:18.106, Speaker A: Cairo VM, like risk, five vms, whatever, and all of that. If such. Fellas, if fellas using this, want to use this inside a protocol, it has to be proven to a protocol. So you have to be able to access the data of this, access the result of this within the protocol, within some particular problem. So basically that's why we need proof based compression. That's pretty obvious. That's exactly the same thing how Starknet works.
00:02:18.106 - 00:02:47.026, Speaker A: It basically just compresses the data, pushes it to ETh. The quality of a compressions determines the size of a data, the amount of data you can put in ETH, or like something like this. That's like obvious thing. Next thing, well, let's say it this way, it's consensus proofs. It's just an example. Okay, it's like what folks have started calling ZK bridges some time ago. It wasn't initially ZK bridges.
00:02:47.026 - 00:03:10.734, Speaker A: Ahat will not let us lie. It was initially like trustless bridges. But then folks have started calling the Ziki bridges. So, okay, it was accidentally we invented that accidentally. We didn't want that. So basically, consensus proofs are consensus proofs for ZK bridging are say this way, consuming, time consuming. This is a lot of continuous computations, and that's the first thing.
00:03:10.734 - 00:03:53.530, Speaker A: And the second thing, defining consensus proofs in Cairo or in custom language or in anything like this, often time consuming. Well, it's just huge time to market. And sometimes there is no necessary cryptography primitives, so it's easier to simply compile the code which is already implemented in the mainstream language and prove it, and that will be fine. Basically, that's what I just meant. So designing circuits is hard. Sometimes you don't have necessary primitives. That's why it makes sense to use mainstream languages.
00:03:53.530 - 00:04:43.910, Speaker A: Well, for example. Yeah, designing, for example, Solan's consensus proof took us something about eight months for manual circuit design. So that's kind of like plonkish circuit design, but anyway, and generating proofs like consensus proofs, again, it's just an example is hard as well. So I mean, some folks are trying to do that on the local machine, some folks are trying to outsource that, and some folks do things like we did. So what is the thing we brought as the solution to all this is basically we designed and brought the thing which we call ZKLvM stuff. ZKLVM compiler. What is that? Well, first of all, it's not a ZkVM, it's just a compiler.
00:04:43.910 - 00:05:21.970, Speaker A: Well, basically, just to understand the difference, ZKVM is, ZKVMs are basically, well, just a huge circuits. Just huge circuits which define some particular cpu architecture. In our case, I don't know if any from risk zero folks have came in, in here. Hey, nice to see you. Basically, in our case, we do not have a unified vm circuit or anything like this. We propose to generate these circuits from high level languages. So basically, each time, each new computation produces a new circuit which is specific to a particular set of competition, to a particular set of logic.
00:05:21.970 - 00:06:05.366, Speaker A: So that reduces the overhead, that allows to prove really huge logic, that allows to prove really a lot, let's say this way, a lot. Like as I said, for example, 4000 DDDSA signatures is doable. And that's like Salon's consensus proof. In case of our mini consensus proof, that was something about 35 billion columns of multiplications. So that's quite a lot. Well, proof generation, what the hell would you do with proof generation? Proof generation? Well, the more you prove, the harder it is, right? Yeah. So basically for all of this stuff, well, you need to either have a really badass local machine, or you need to simply outsource that to those who do that.
00:06:05.366 - 00:06:44.554, Speaker A: Professionally. So that's what we did. We kind of built a decentralized proof market thing, which allows those who generate proofs to come in to generate proofs which will be verifiable on EVM and on. Yeah, that's what it is. There will be two sections about each of these things, like Zkladm and proof marketing, like, from our team heads who are responsible for this. And, well, they'll do a little bit of a show and tell in more details about each of these things. Like.
00:06:44.554 - 00:06:45.380, Speaker A: Go on.
00:06:47.190 - 00:07:21.934, Speaker B: Yeah, hi. So I will tell about ZKLVM and about the tool chain we use. Let's start. Okay. Recently, we designed two ZK bridges, one for Solan and one for Mina. And each one of them requires to generate a huge circuit of the consensus of the consensus protocol of each of the clusters. And with each of them, we have two pretty huge problems.
00:07:21.934 - 00:08:30.100, Speaker B: One is that we need to reduce the prover time, and the other one is that we want to control the costs on the verifier side. That's basically why we decided to design the ZK snark proof system, which we call placeholder. This is basically fry based, LPC based. This is fry based ZK snark system with custom gate arithmetization. And the key features we wanted, and the key features we achieved with it, is that we have pretty flexible parameters. We can adjust them for the target we want, for the target environment, for the proverb, for the target environment, for the verifier, we can adjust it for particular prover time. Proof size verifier costs everything.
00:08:30.100 - 00:09:23.042, Speaker B: Yeah. And after we made our proof system, the second huge task for building the bridges was to build the circuit itself. And with building circuits, there are different tools, and we have checked pretty lot of them. And one of the best solutions to use is to use the ZKVM for that time. When we started, it was like that. This is basically the ZKVM pipeline, more or less, in general. But when we tried to use ZKVM, we understood that there is a problem.
00:09:23.042 - 00:10:10.690, Speaker B: Mike told it's a problem that the ZKVM circuit. This is the circuit which is put it in the proof system after the execution of ZKVM. It contains a lot of additional stuff, a lot of constraints, which represent the logic of the virtual machine. And that's why even a pretty simple logic requires to add a lot of constraints to the circuit. And that's why the size of the circuit. The size of size of the circuit is pretty big. And that's why obviously the size of the proof and the costs on the verifier side, they skyrocket.
00:10:10.690 - 00:11:49.970, Speaker B: So we understood the problem. We understood that we want to get rid of the additional cost, additional constraints in the circuit and the result circuit. And we started thinking about why not to build a system when we can transform the code of the algorithm from the high level representation in C plus plus or rust directly into the constraints on the data, which algorithm works with. So we took the usual VM pipeline and designed basically a backend for that pipeline which works with the circuits. We have chosen the LVM pipeline because that gives us ability to add new front ends if you want. We basically started with C Plus plus front end, because we write a lot in the C Plus plus, but the addition of the rust front end took for us like a month or something like that. And that's basically the, it's basically a very nice feature we have, because the cost of the extension of our compiler, they're pretty low because it is an LVM based compiler.
00:11:49.970 - 00:13:05.210, Speaker B: It installs the same way as the clunk or rusty compilers. And you can use it from your environment, you can install it in your regular environment or development environment and use it in your usual tool chain without changing anything. You can just write the C Plus plus code and compile it with the cmake configuration, make it's the same for the rest. So I will show a simple example of how the transformation of the, of the code representation looks like. Okay, yeah, so this is a C plus code, obviously, and we just call make as we usually do for the C Plus plus code. We compile it into the intermediate internal representation, which looks like that. I think it's pretty obvious how to match the code from the first slide to the second slide.
00:13:05.210 - 00:13:54.090, Speaker B: And then you call the tool which we call assigner, which also can be installed in the pipeline, and which generates the circuit representation and the plunk assignment table for the proverb. And that does what he needs to do, generates the constraints in the assignment table. Okay, I cannot show. So you see, there is the code. It contained only two operations, three operations at addition, subtraction and multiplication. And so we have only two constraints instead of like 200 or something. For the ZK wave pipeline.
00:13:54.090 - 00:14:39.462, Speaker B: That's pretty effective. And that's basically what we wanted to achieve. And we achieved that through this way of compiling circuits, because we wanted to build the ZK bridges upon the compiler. And actually not only the ZK bridges, but we started with ZK bridges because of that. We wanted to reuse the logic we wrote on the compiler, the logic we wrote for the compiler. So we designed cryptography SDK. Now we have two sdks, basically, one for C Plus, plus one for rust.
00:14:39.462 - 00:16:14.434, Speaker B: The rust one is kind of work in progress. The C is more stable, but both of them, they serve the same purpose and they contain a lot of cryptography stuff. The proof system, I mean the starks, the commitment schemes like LPC, fry, KZG and everything, the basic cryptography like hashes and cyphers, signature schemes and serialization, martialing, everything like that, and all the stuff we have in the sdks, it's pretty good optimized for the circuit back end. So you can for example, take some commitment scheme from the SDK, write your own breach, write your own proof system using this commitment scheme, because you obviously will reuse one of the existing commitment schemes. And it will be pretty good optimized for the circuit back end because you will just reuse the block components from other circuits. So this is basically how the whole pipeline looks like. You write the code on the SDK, it compiles through the front end, through the front end for this language.
00:16:14.434 - 00:17:24.160, Speaker B: Then it compiles to the IR and to the back end. I have something like two minutes, so I will show the example of writing the Merkel Tree circuit using the SDK upon the ZKLVM compiler. I think most of people who are here know about the Merkel tree, but I'll anyway say some words about it. Merkel tree is a concept which is used in the base of the fry commitment and the LPC commitment scheme which are used for ziggy snarks. And it's usually pretty huge because you can reduce the size of the proof using this concept. And we just wanted to show that you can easily compile it into the form of circuit, because if you can do it, you can do the recursive proofs, you can do the bridges. That's pretty important stuff.
00:17:24.160 - 00:18:29.810, Speaker B: So that starts with includes from the SDK, from the crypto three library, because this is a CPP example. So we use CPP SDK, the declaration of the function, which takes an array of size two power 20. These are the leaves for the Merkel tree, which will be committed into the root value. And then you write like 20 arrays for each layer of the Merkel tree. But we obviously do not have this on the slide. You just need to imagine the whole bunch phrase. Then you write the cycles to hash two nodes of data from the previous level to the node of the next layer and that's basically all you just wrote the Merkel tree.
00:18:29.810 - 00:19:02.080, Speaker B: It just required two concepts from the SDK, and it doesn't require to know anything about the circuits, about how do they work. You just use the concepts from the ready to use library and just write your stuff. This is the links for the query and for the docs. And now Elia will tell about how to use the circuits, how to generate the proof and everything. For that.
00:19:04.850 - 00:20:07.534, Speaker C: Hello everyone, I'll talk about proof market as proof generation infrastructure for digitalized projects. Mikhail explained why non interactive proofs important, how they can help to grow industry. Nikita shows a kind of easy way to generate the circuits, but the issue is that it's not enough because you still need to actually generate proofs. And proof generation is costly, high load process, especially for complex circuits. So that means that first of all, you need powerful hardware which works twenty four seven. And that would still not be enough because, well, to achieve competitive proof generation timings, you still need to bring new optimizations in software, into hardware, into algorithms. And that still not be enough because even in this case you cannot rely on centralized node.
00:20:07.534 - 00:20:49.754, Speaker C: I mean in this case because centralized node affects liveness of the system, it affects throughput of the system. And actually you need a network of proof producers. Here I've introduced proofmarket. Proofmarket is a decentralized protocol that delegates proof generation process from proofdemand applications to network of professional hardware operators. Basic workflow includes two roles, proofrequester and proof producer. Proofrequester wants to delegate proof generation process. For example, it could be ZK breach.
00:20:49.754 - 00:21:36.110, Speaker C: Application may act as a proofrequester and as I said, proof producer is a professional proof generation hardware and software operator. The process includes several steps. First of all, both sides, proofrequesters and proof producer sends requests and proposals to the market. The matching algorithm continuously checks if there is a match between them. It takes into account proof producers fees and expected proof generation time. If there is a match, then the request is assigned to proof producer and proof producer starts generate proof. Then proof is ready, it's sent to proofmarket, there should be verified.
00:21:36.110 - 00:23:05.256, Speaker C: If the proof is correct, then if the proof is correct, then payment goes to proof producer and well, proof goes to proofrequester. Developers could add new circuits to proof market to let proof producer use them to generate proofs for them. And moreover, proof market is open to new proof system. It means that if you use starks or ROF 16 implementation in your project, you still can integrate them into proofmarket and use this because basically it includes implement a verification functionality on proofmarket side let's look at developer workflow application developer workflow here. Proofmarket tool chain is a repository that contains first of all proof generator and CLI tools for communication with proofmarket. Firstly, developers should prepare a statement which is structure that contains circuit description. I mean proving and verification keys for the circuits and some extra information like short description, what the circuit accessory does, type of proof system, URL to repository, all that stuff and then it can just send just some description of this circuit to the proof market.
00:23:05.256 - 00:24:16.850, Speaker C: From now on any proof producer can prepare proofs for this and get requests for this circuit from the proofrequester side. Proofreaster mainly interested into two things to send requests for a proof and actually get the generated proof. So to send requests he just choose set the key or identifier of the circuit which interested in and pass some costage they are willing to pay and public input. From now on proof market should find a proof producer who is ready to generate the proof and well proof producer generates proof and proofrequester just wait for this and then can download it via proof tools. Verification on external protocols works in the same way as usual as any call to any smart contracts on these systems. For now, for placeholder proof, system verification is available on Starknet and Ethereum. In the future we prepare verification modules on other protocols too.
00:24:16.850 - 00:25:29.450, Speaker C: So autograph in the full picture. Using this toolchain we get a straightforward, lightweight way to prepare ZK based applications. To build ZK bridge, you just need to write its logic in C plus plus or rust, compile it and send the circuit to proofmarket. And of course that makes the whole process faster, cheaper, easier. But the most fascinating thing here that's using this tool chain, even small teams or individual developers are able to build complex application like ZK roll apps, Solana Starknet Bridge trust without any trust in it. And it means that if individual developers are able to do this, it's open a new wave of ZK based protocols applications solutions that can help to actually solve these computation and data scalability issues. That's all from my side.
00:25:29.450 - 00:26:04.270, Speaker C: Thank you. And jen, that's all. Thank you everyone. Any questions? Okay, no questions. Suppose. Okay, yeah, it's decentralized. It's decentralized.
00:26:07.090 - 00:26:07.514, Speaker A: Okay.
00:26:07.572 - 00:26:26.620, Speaker C: For now it works as centralized node. We are in the process of moving into asset decentralized protocol. So in the future there is not our links, but if you subscribe you'll find out the details on how it will be done.
00:26:28.670 - 00:26:30.300, Speaker D: How do you enforce that?
00:26:35.970 - 00:27:01.276, Speaker C: Well, verification procedures takes proof and public input and just, well, usual verification of production, proof of the producer proof, well, you just use the same public input as an input of verification algorithm for the proof. I'm not sure if I answered, let's.
00:27:01.308 - 00:27:12.712, Speaker D: Say I'm trying to verify the circuit. And what if the public input that I give is wrong? The proof will not be generated. That's why I can toss the network.
00:27:12.796 - 00:27:37.448, Speaker C: Oh, got it. Yeah. Well, in general, it's more like a question for, okay, there's two phase of here. First one is public input verification. I mean it cannot check that signatures are actually fine. Okay, it may check that signature is actually fine. But yes, the first part is input verification.
00:27:37.448 - 00:28:16.132, Speaker C: It's up to circuit developers, the one who builds the circuit, who transpile it, send it along with some rules of verification of any input. The second part, again, it's more like on circuit developer side because, well actually in most cases you can build circuit which in case, if there is at least if input goes through the basic verification checks, basic sanity checks. In this case circuit will work. Yes.
00:28:16.266 - 00:28:23.560, Speaker D: How do you match the requester and provider? Is it for a specific algorithm.
00:28:27.630 - 00:29:27.534, Speaker C: Proofrequester or proof producer? Oh, how do match, okay, yeah, the matching algorithm works as this, let's call it application. Okay, for now it's centralized. Yes, we are in the middle of match. Basically it's more like any exchange like matching algorithm for sets of bits and ask. Then you need to find, do you mean from technical side or from logical side? How much? I'm not sure that I understand. Okay, yeah, well from the technical side it's similar to typical exchanges matching of bits and asks from the, well, logical side. Again, it depends on the input from proofrequester because proofrequester could prioritize cost or proof generation time.
00:29:27.534 - 00:29:37.330, Speaker C: And well, basically if it prioritize cost, you just choose the cheapest proposal from proof generators, from proof producers.
00:29:39.430 - 00:29:46.820, Speaker D: Algorithms and other.
00:29:50.010 - 00:30:26.450, Speaker C: Matching algorithm is not related to, well, ZK proof system at all, it's just usual matching algorithm. I'm not sure. Well yeah, as I said, it's not related to ZK. I mean it just matches set of two tables, one with requests, one with proposals. Yeah, I'm not sure, am I answered? Okay, looks like that's all. Thanks everyone.
