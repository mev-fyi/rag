00:00:03.660 - 00:01:08.690, Speaker A: I'm going to invite to the stage among the biggest brain in the oil industry, the OG, the voG, the people who've been here for some of them for five years doing vk stuff. So let me just call them one by one. Brian Redford from Reef Zero, Bobin Tredbar from Polygon Maiden, Geordie Ballina from Polygon, Hermes Zach Williamson from Aztec, Alexkowski from Zike Sync and Shakar Papini from Starquare. To moderate this extremely IQ level, insane, whatever you want to call it, panel where I'm going to ask Federico Carone from Lambda class, who very courageously took on himself to take the risk. So thank you so much and have a good time.
00:01:11.860 - 00:01:41.490, Speaker B: Hey, thank you struggle for organizing this. I'm the dumbest guy in the room, so that's good. I'm going to ask the questions. So to break the ice, the first question I have, especially for people that have been building ckebms, is like, what do you believe that will happen when all the ckebms go to production? What do you think that will happen? Like, we will have many ckebms. Will they consolidate? What's your idea?
00:01:44.820 - 00:01:46.640, Speaker C: We are going to launch on March.
00:01:50.340 - 00:01:51.650, Speaker B: Anybody else?
00:01:54.360 - 00:02:23.390, Speaker D: When all come to fruition, I think there is a lot of opportunity to collaborate and do interoperability between these ZKVMS evms. If they are all scalable at the core, it shouldn't bring very hard cost to interact between all. So I see place for all of them to coexist. This is my take.
00:02:24.880 - 00:03:07.930, Speaker E: I agree. I think we will see some specialization of the vms going in different directions with different technical design. For example, we already see distinctions between Zksync and polygon scrollers building with regard to data availability. We use different data availability approaches where some projects put data inputs on the call data in Ethereum, some projects only put the state delta final state updates that give you different properties. Some transactions will be cheaper on one type of system, some will be cheaper on the other one, and there will be more of those changes which will I think it's going to be interesting how, whether or not in what direction this specialization will go.
00:03:08.400 - 00:03:16.700, Speaker B: Okay, is there any other type of specialization that you think that can happen apart from data availability like opcodes?
00:03:17.440 - 00:03:54.228, Speaker F: Yeah, I'm not sure how much new this has in evolution. There's a general principle that no two species can occupy the same ecological niche and coexist because one will outcompete and devour the other. So I think what's going to happen is either that's going to happen, or projects are going to specialize in different niches, basically provide different value propositions through specialization, which I do suspect that's probably what's going to happen and we'll see them evolve and yeah, excited to see how things take shape.
00:03:54.404 - 00:03:55.130, Speaker E: Cool.
00:03:56.540 - 00:04:10.190, Speaker B: How are you going to manage changes on the EBM? How are you going to keep track of all the changes happening on Ethereum? Do you think that's going to be difficult? There has been quite some discussion on Twitter about that.
00:04:14.320 - 00:04:51.900, Speaker C: Yeah. The thing is that at some point Ethereum is going to be locked, it's going to be fixed, it's going to be finished if you want. I think that we are still far away on this sharding needs to be implemented. We are getting to the final. So Ethereum is not final. So thinking in finalizing the roll up, I think it makes no sense until Ethereum is final. Somehow in the while you need to give some warranties that what the changes that you are doing that you are not doing in a malicious way.
00:04:51.900 - 00:05:26.390, Speaker C: And that's where you can do these time lock protections or you can have some specific mechanism for controlling these upgrades in the system in a controlled way. But it's going to be some point, I don't know, maybe in 2345 years that ethereum is going to be finalized and then we can think then in finalizing. So not finalizing the roll ups in the sense that you cannot upgrade unless you're doing some crazy thing.
00:05:27.800 - 00:06:12.820, Speaker E: I think one of the great things about roll ups and L2s in general is that you can experiment much faster than at layer one, where a lot is at stake. So you can go and try different designs and actually deviate completely from EVN in some senses. So like Kyra is innovating with a different virtual machine and different language. We're doing the same as eksync. We used LLVM compiler, which can compile from solidity, but also from anything that has LLvM frontend like rust or move. And those features will take you completely away. Like there will be some classes of applications that are not thinkable on Ethereum because the solidity lacks the exclusivity to build those applications in an efficient manner.
00:06:19.140 - 00:06:20.630, Speaker B: Do you want to add anything.
00:06:23.880 - 00:06:24.484, Speaker C: Perfect?
00:06:24.602 - 00:06:34.650, Speaker B: So this question is more targeted to Brian on Shahar. How was the process of choosing not to go through the EBM line?
00:06:37.020 - 00:07:26.356, Speaker D: Well, Cairo started as a project for us internally for stockware to accelerate the development process of our own ZK products. And it did come up to mind like maybe do X 86, ZK, maybe do EVM. But the choice was we want the best scaling thing for our products. We don't care internally for compatibility with EVM. We don't even have code for EVM for our products, so we don't need this. So the decision came from there, from our use internally. But I think it also makes sense for Starknet and for the broader community for cases that don't require solidity.
00:07:26.356 - 00:07:44.700, Speaker D: Maybe if someone is not really locked into his already existing contracts in solidity where he really wants scaling, so he's prepared to write it in a new framework. And this really suits this use case in my opinion.
00:07:47.200 - 00:07:54.000, Speaker B: Again, repeat it. How was the process of choosing note to go through the EBM route.
00:07:55.700 - 00:07:56.496, Speaker F: For us?
00:07:56.598 - 00:08:19.000, Speaker G: I think we always just had this thesis that to really expand the pool of developers in the overall ecosystem, we needed to give developers more choice in terms of what languages they were using. So that, and you know, it seemed very crowded. So it was kind of an easy decision to not jump into the fray.
00:08:19.900 - 00:08:27.980, Speaker B: You chose to use the risk architecture. Did you ever believe that it could be done with X 86 or LLBM?
00:08:28.560 - 00:09:06.040, Speaker G: I mean, LLVM is a totally reasonable abstraction point. I think we wanted to use something that actually is sort of reified in silicon. There's a lot of reasons to do that around. Sort of the availability of formal verification for RISC five, as well as variety of tooling and the ability to have binaries run on physical hardware, and then also be able to, ZK, prove the exact same set of instructions. But yeah, anything that level seems like a pretty good abstraction.
00:09:06.780 - 00:09:49.236, Speaker H: And I would say from our side, within polygon, our goal is to extend Ethereum a number of different ways. So Zkevm is a super important part of this puzzle. But also maiden is not a Zkevm, it's a different vm, very similar to Cairo in spirit, not in the actual design. There are a lot of differences. But I think the important part is we have one or several approaches to Zkevm, but they also want to provide or cater to other use cases. Like to the point earlier why other vms should exist that can provide functionality that may be difficult or maybe in some ways even impossible to provide with the, you know, within Polygon. We're trying to expand Ethereum a number of different ways.
00:09:49.236 - 00:09:54.280, Speaker H: Scalability through EDM is one, but also through different vms as another approach.
00:09:54.860 - 00:09:55.610, Speaker F: Cool.
00:09:56.060 - 00:10:27.200, Speaker B: Do you have any take regarding if fpgas or gpus will be the best thing to use on ckebms or ckbms we got the results from the c prize only a few weeks ago. Actually, I believe that matchlabs won. I don't know if it was the MSM, I don't recall correctly, but some of you are working that. Do you have any take like where that will go via fpgas or CUDA or gpus?
00:10:27.700 - 00:11:04.348, Speaker G: At least from our side, I think that gpus are probably going to dominate for at least the next three to five years and maybe for much longer than that. It kind of depends on how the use cases evolve. Currently we see about like a six to ten x improvement by using just even your desktop gpu. And we expect to get more like a 50 x once we finished parallelizing everything for CUDA. Personally, I think ZK hardware will show up more in smaller form factors for robots and stuff.
00:11:04.434 - 00:11:06.350, Speaker F: But I don't know, it's a hot take.
00:11:07.540 - 00:11:55.644, Speaker C: Yeah. Here we have some news, because in Polygon we have been investing a lot in fpgas and gpus development. Actually we have very good results from the GPU, especially for speed. So if your goal is to make proofs fast, then gpus is a good path. But we've seen, for example, if what you are targeting is price, then here gpus are not as good as we were expected. And let me explain you why. Well, first is because the limitations are not in the computational side.
00:11:55.644 - 00:12:39.576, Speaker C: So the bottleneck is not in the computational, it's more in the memory. Right now, in our current prover, it takes half a terabyte. And this is quite expensive. So adding a GPU, so you are more cost, and you cannot use other resources in an efficient way if you are transmitting prices. So gpus is not profitable. And then there is another point that we didn't take in account. And I think it's important is that in the testnet, for example, we have been running at some points like 30 provers in parallel.
00:12:39.576 - 00:13:33.550, Speaker C: So here for the price, we are using a lot, the spot instances in the cloud services, AWS mainly, but you can have another clothes. And the market for cpus is much bigger. So here you have a factor of the cost, just by the size of the market, that in gpus you cannot take advantage as good as that. So when you put all the pieces together, we end up to the conclusion that for being cost effective gpus, it's currently with the current proving technology, with the limitations and how we are doing is not worth enough if we are targeting prices, if we are targeting latency, maybe it could make sense.
00:13:37.520 - 00:15:10.152, Speaker F: I think one thing to make a distinction here is how one intends to use zero knowledge proofs, because there's two key categories. There's for scaling and privacy. So where scaling is you're trying to verify computations performed on, you're trying to verify with a rather weak computer, computations being performed on a very powerful computer or series of computers, and then there's privacy, where you care about proving computations over encrypted data. For the latter, which is something that we focus on a lot at aztec, the available hardware is much more restricted because typically those proofs of knowledge are going to be constructed by the users of whatever system you're building. Typically, if they're going to be consumers, then that's going to be wimpy hardware, phones, old laptops, et cetera, which restricts what you can use. I mean, at best you can use gpus, but we're not talking about beefy gpus in CuDA, we're talking about intel integrated gpus that are not widely supported on the web for care for privacy related applications. Cpu at the moment is really the best you can get, but certainly for scaling, I do imagine that hardware acceleration will become more and more of a valuable use case, particularly because I feel like Zprize was really the first time that the surface had been scratched in terms of optimizing these number theoretical algorithms for dedicated hardware and gpus.
00:15:10.152 - 00:15:18.770, Speaker F: It feels like there's a lot more work that can be done to squeeze performance out of these devices. But yeah, we'll see for sure.
00:15:19.780 - 00:16:00.300, Speaker E: Yeah, I think fundamentally gpus are right now, from the architecture, from our experience, they are actually capable of giving you advantage in terms of both cost and latency, but of course a lot more for latency. So our implementation is much cheaper on gpus than on the cpus, but latency is a lot smaller. And this is going to be important once we have multiple chains and we have bridges within those chains, and we will actually need really short finality for being able to utilize those bridges. For great UX fund consumer, one other.
00:16:00.370 - 00:16:36.330, Speaker H: Thing to add is that it also depends on whether you want your prover to run in available infrastructure on AWS or Azure, or you can build a custom system, because for fpgas, for example, there is a limited set of fpgas that are available in those cloud providers, and you don't even get the best ones and similar with gpus. So if you're able to build your approver on a dedicated hardware, you can take much more advantage of the specific gpus or fpgas in the cloud. It's not as clear as Georgia said. Whether it will outcompete cpus or not. For some proving systems it could. For some setups it could, but not always.
00:16:38.300 - 00:16:54.270, Speaker B: I have another question changing topics. I know that Stargwer and Stargnet are working on decentralizing the sequencer on the prover. Are you guys working on the same thing, or is there anybody of you that already did some advances on that?
00:16:57.040 - 00:17:27.960, Speaker E: I can start for us at ZK sync, it's our top priority. It's our mission to scale Ethereum or public blockchains in general, not just for the mass adoption, but fully preserving the properties that made Ethereum valuable in the first place. And this relies on decentralization to avoid central points of control, to avoid aggregation of power, censorship and all things. We are working hard on decentralizing the sequencer.
00:17:28.620 - 00:17:56.530, Speaker B: Do you believe that at the end most of those protocols will be the same between each l two, or each one will implement a different protocol? I can explain better. I mean, in the space I'm seeing that a lot of things are converging to the same type of solution. Do you think that will also happen with the sequencer on proverb decentralization? Or each provider ol two will have their own solutions?
00:17:57.110 - 00:18:26.150, Speaker E: I think with it we'll see different solutions even within a single ecosystem. Like you're building l three s on top of Starknet, we're building hyperchains on top of zksync. Those different chains might choose different decentralization properties. Some of them will be having centralized sequencers because they don't care. It's a game where validity is the only thing that matters. But some others will be broadly decentralized. They will prioritize decentralization over performance.
00:18:26.150 - 00:18:31.760, Speaker E: Some others will be somewhere in the middle spot. They will have a bigger right there.
00:18:32.850 - 00:19:12.780, Speaker F: Yeah, there's no standardization, and there won't be for some time. So we'll see. I think there are some stuff we want to provide sequencing services as a service, and other startups that are trying to provide a kind of an l two sequencing consensus layer and delegate some of the complexities around at least organizing sequences. But I think we'll see how that plays out. I think it's way too far, too early to tell. I do have something going back to your original question around focusing on decentralized sequences approvers, I think I agree with. I think probably everyone here that is very important.
00:19:12.780 - 00:20:20.702, Speaker F: I think one risk that I think the LT space runs the risk of like there's a problem that the LT space runs the risk of falling foul of which is the potential for infrastructure centralization. For example, I think pretty much all the l two s are included there in production today. Use cloud providers for their provers. And I think it's quite important that the architecture of an l two is such that your provers can be run on, if not like run on the milk commodity hardware. Individuals can build their own provers and run them without the need for cloud infrastructure providers because these networks are supposed. Well, these networks are permissionless and decentralized, which means you can't control what content is being deployed on them. Which means that until the precedent has been firmly established that ltus are networks very much like the Internet, where the content going through our fiber optic tables are not.
00:20:20.702 - 00:20:43.282, Speaker F: That's not the site of regulation. The regulation is at the site of the application layer. That precedent has not yet been established for web3 and l two s. Until it does, cloud infrastructure providers are, in my opinion, a rather large potential vulnerability, because if your entire network traffic is throwing through them, then it's very easy for it to be shut down and created like a catastrophic network failure.
00:20:43.346 - 00:21:22.470, Speaker G: Yeah, I totally agree with that, and I think that's one of the reasons at least. At risk zero. We've been really focused on trying to reduce the hardware requirements for the sort of core recursion circuit to be below four gigs of memory, to make sure that as many people as possible can participate just using their laptop or even their mobile phones in the broader network. And at least from our perspective, that's part of the reason we're starting from an unsequenced, stateless kind of place, is we want to provide more of an infrastructure layer that is in fact far more centralization. Sorry, censorship resistant.
00:21:23.370 - 00:21:52.400, Speaker B: That opens two questions that I have that are related to the things you mentioned. The first one is, did you have any particular security concerns when designing the ckbms? Is there anything different from writing traditional software? What was your experience? Did you have to learn during the process? I don't know. Is there any big difference with writing traditional software or in terms of security?
00:21:52.930 - 00:22:38.170, Speaker H: Yeah, I would say that's probably one of the bigger concerns right now, because if you ask me, let's say four or five years ago, what are the biggest issues to be solved in ZK space? And I would say performance and security. And by now I don't think performance is much of an issue anymore. I mean, there's still a lot of work to be done, but I think it's more or less we know what needs to be done in performance. On the security level side, I think it's very different from just writing regular software because you can't create test cases for missing constraints. It's very difficult. You can try to do formal verification and all of those things, but they are complicated, take time, and bugs can creep up in many different places. So you can do your best, like audit, make sure there is no bugs, make sure it runs in production for a while.
00:22:38.170 - 00:22:55.106, Speaker H: But I don't know if there's a silver bullet that exists right now that will say, okay, I have no security concerns. It's kind of like in a way and much less so like similar to cryptographic hash functions where we believe they are secure, but there is really no proof that they are secure. Kind of. Nobody has broken them yet.
00:22:55.288 - 00:23:04.260, Speaker B: Do you think that differential fasting or formal verification is more important? The two things are complementary, or what's your take on that?
00:23:05.030 - 00:23:23.580, Speaker H: There's a lot of things you can do and like horrible verification is one, but there is other things like audits and all of that. I think all of them need to be done. And more importantly, it needs to run in production because you verify one part of the system, but are you certain that other part of the system doesn't have bugs and things like that? So it's just battle testedness is one of the important parts of it.
00:23:24.750 - 00:24:21.886, Speaker F: Yeah, I'd agree on that. And also that basically under the constraining, your algorithm is the largest source of bugs and security vulnerabilities, where you think you're checking a computation correctly. You think you're checking like for example, that an EVM instruction is correctly constrained, but in reality there's a bug in the software or a bug in the algorithm design, which means that, no, that's not strictly the case, and therefore you can create fake poofs with fake inputs. It's a very common problem. As Bob said, it's extremely hard to actually rigorously formally test for. The tooling just isn't there yet. We did find differential fuzzing to be extremely useful, but in small cases, it's difficult to fuzz an entire complicated algorithm that's billions or tens of millions of constraints, but basically the equivalent of unit testing for ZK circuits, where you have a relatively small self contained algorithm doing differential fuzzing to try and find in cases where you have imperstat ought not to, for example, satisfy constraints, but your algorithm claims they do.
00:24:21.886 - 00:24:28.350, Speaker F: We found that extremely helpful and caught some very interesting bugs we would not have found through any other method.
00:24:31.990 - 00:25:32.680, Speaker D: I think security is indeed, one of the top things that need to be taken care of, especially at the blockchain world where so much money is everywhere. And if you make a problem, then all the money goes. In Stockholm there are a few places where you can really apply in a good way form of verification and things the polynomials is one place where it's a lot more possible, or it's straightforward how a form of verification of something like that could look like, because you have this set of polynomials and that's what you need to prove. You can prove statements about them. Doing this about software is a lot harder. Like complex software. In Starco we do have this form of verification for the equations and also for a lot of the library functions we have for Cairo itself.
00:25:32.680 - 00:25:51.340, Speaker D: Another important place is the verifier we have on chain solidity. That is hard to do form verification. But a lot of auditing is very important in this space. I think it's not like form verification or other methods, but auditing is important.
00:25:51.870 - 00:26:44.118, Speaker E: And I would like to add that I also fully agree. Publicity has been the biggest challenge we face with the systems, and I feel that we should be doing audits, we should be doing differential fuzzing all of the things we're investing millions of dollars into audits alone, and then the security contests, and then there will be bug bounties, ongoing, et cetera. But that's not enough. That's like investing in security by construction, whereas you need orthogonal mechanisms, because no matter how much you invest in one single mechanism, it can always fail with some remaining probability. Just like you're investing in making your ship robust. You still need to have live boats and live vests on that ship. That's like a completely orthogonal mechanism that will work even if the original fails.
00:26:44.118 - 00:27:18.600, Speaker E: So for ZK roll ups, we need something like two factor authentication, two factor protection, where you rely on zero knowledge proofs, but then in addition to that, you rely on something like decentralized consensus mechanism, so that at least two mechanisms need to fail in order for any severe event to happen. And then on top of that you can have mitigation strategies, like you monitor the re withdrawals and then you put some limits for some period of time, et cetera, et cetera. So you want to put as many layers of protection, rather than just like double downing on one layer and making it as robust as possible.
00:27:19.690 - 00:27:59.122, Speaker C: Yeah, I want to explain as so mainly in the last three months, mainly we have been working very much in the security, in securing and thinking a lot in the security, in the security. We distinguish between four possible things that can go wrong, two are critical. Two are, if you want, can be mitigated by the way that we are handling. Okay. The ones that are critical, one is of course the smart contracts. You can have some back in the smart contract here. Of course we are doing all like any other tap, just audits, bug bounties going on.
00:27:59.122 - 00:28:33.630, Speaker C: The cool thing of smart contracts is that the community is more mature in the smart contract sites. And so you never have of course the 100% warranty that the smart contracts are going to be okay. But it's not the first smart contract that's audited in the space. The other thing that can go wrong that would be really bad is what we call the complete mess. So if you are proving something that you go to something that's wrong, if the next state, for example, puts a lot of ether in my account, then that's a critical thing.
00:28:33.700 - 00:28:34.126, Speaker E: Okay.
00:28:34.228 - 00:29:25.034, Speaker C: For mitigating this here, besides the auditing and just doing normal checking, here we have an extra thing that's interesting is just we are passing all the ethereum tests because we are all ck eVM compatible. That means that we are taking all the ethereum tests and we are checking that all the theorem tests pass. So this gives us an extra confidence that the completeness is correct. Again, as a security, 100% security, you never will get it. But here I feel very comfortable in this part. The other two critical, the other two parts is a soundness mistake. So something that you are here, you can have some mitigation things in the smart contract.
00:29:25.034 - 00:29:59.530, Speaker C: For example, if you have two provers that prove that the same transactions goes to a different state, then in that case you can halt the smart contracts and maybe going to centralized and then you have to update on things like that. There is another mitigation is that you can run a trusted prover or centralized prover at least. Of course you need obligation decentralized prover that they need to prove. So the worst thing is that they don't prove. But there is a timeout. So if this proven that anybody can prove. But then if you are a trusted prover, that means that you cannot prove something that's not correct.
00:29:59.530 - 00:30:42.514, Speaker C: Because from the soundness perspective you can prove. So these are the two protections that the system mitigates with this soundness, which is probable. I would say the one that I feel less comfortable because sonnets, proving sonnets is here is where formal verifications make sense, uniqueness and so on. And here investment sale excess is as much so as much as you can on that and the last thing that can go wrong is that maybe especially this is for the forced transactions. But maybe there is some transaction that you cannot prove. That means that the proofer gets like then the roll up gets stopped on that. This is the mitigation is very easy because if you cannot prove, nobody to prove, and then there is just a timeout.
00:30:42.514 - 00:31:04.240, Speaker C: And then timeout, you just hop and then you just restart. These are the four things. So two of them are mitigated. And there the ones that I'm less comfortable, the two that are more critical and more comfortable. This is why we are confident to launch, of course, with all the precautions to mainnet very soon.
00:31:05.570 - 00:31:13.940, Speaker B: Actually, I have a question for you related to Shahar, like Sierra in some way was designed because of security or. I don't know much about Sierra, to be honest.
00:31:14.550 - 00:32:08.670, Speaker D: Yes. You haven't been to my talk, I assume. Yes, Sierra is designed for some kind of security, in a sense. The main purpose is kind of a security for starknet that transactions can't fail and halt the system or do a dos on the system. But the introduction of Sierra actually allows for a lot of safety features on the contract side. For example, users writing contracts on Starknet can use the strong linear type system that is incorporated in the semantics of starknet right now in Sierra to enforce all kinds of environments and make sure that things don't go crazy. So I think it's a lot of features that help writing secure code on smart contracts.
00:32:09.650 - 00:32:10.110, Speaker C: Perfect.
00:32:10.180 - 00:32:15.430, Speaker B: Thank you very much everyone. I hope we can talk about these things afterwards or tomorrow.
