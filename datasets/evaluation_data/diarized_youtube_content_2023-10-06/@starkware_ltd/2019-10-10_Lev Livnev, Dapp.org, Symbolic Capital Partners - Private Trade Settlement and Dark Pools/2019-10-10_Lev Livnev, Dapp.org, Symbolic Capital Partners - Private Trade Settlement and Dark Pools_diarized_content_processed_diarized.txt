00:00:05.140 - 00:00:25.748, Speaker A: One person clapping a lot. Thank you. Hi, I'm Anna Rose. I'm the co host of a podcast you may have heard of. It's called zero knowledge. I'm going to be hosting this stage for the day and helping kind of navigate the application speakers that we're going to be hearing from. So I'd like to first invite Lev Livnev from dap.org.
00:00:25.748 - 00:01:36.944, Speaker A: Lev is going to be talking about Dex designs that provide privacy as well as the evolution of dark pools. So let's give him a hand. Hello. Can everyone see this properly? Cool. I'm going to talk about actually mostly just dark pools. The title that includes private trade settlement was from a little bit before I totally decided what this talk will be about. So I think, just to summarize, the reason I think it's interesting to talk about dark pools at a zero knowledge cryptography conference is that, well, I guess the sort of application that people are most familiar with when zero knowledge proofs is like providing some form of settlement privacy.
00:01:36.944 - 00:02:42.440, Speaker A: So abstractly, settlement privacy is that if I transact with someone today and then later I transact with someone else, that I don't necessarily need to be leaking to the whole world that connection, so that people who I transacted with in the past, in the future don't need to know about those transactions. While there are actually many other types of privacy that you can get with zero knowledge proof. So I remember a while back, Ellie did a talk about starks, where, just as an example, he gave this thing where you can prove that your dna sample is not in some database without providing to the whole world your DNA sample. So you provide the zero knowledge proof that you weren't at the scene of the crime or whatever. But that doesn't mean that the whole world, to verify this, needs to know all about your DNA and maybe where else they saw your DNA or whatever. And I think this is really mind opening because it shows you that there's actually a lot of applications, and this naturally leads on to medical applications and insurance or whatever. So dark pools is another, I think, kind of a different example of how this stuff can be useful in practice.
00:02:42.440 - 00:03:56.676, Speaker A: And here it's kind of the theme is when you want to engage in some kind of trade, how you can kind of, when you're looking for a counterparty of this trade, you can avoid announcing to the whole world what exactly your future trading intentions are. And just for kind of almost very basic microeconomics reasons, this is desirable a lot of the time, is if you want to engage in some kind of transaction, you don't necessarily want to advertise to the whole world what you're going to do. Okay, now, of course, in reality, dark pools is a quite technical thing. It's got a lot to do with market microstructure and to understand exactly how they work and how they're used, you need to really look into how these markets work. And I'll try to basically, for the first part of my talk, give some background introduction to why these are useful and how they work, and also show that they're actually a huge part of the traditional markets. And for the second half of the talk, I'll present a design that I think is relatively simple for a dark pool that you can imagine, like running on a blockchain like ethereum or something. And it's also kind of very closely based on how zero cash works.
00:03:56.676 - 00:05:15.832, Speaker A: So if you're familiar with that, then it should be straightforward to follow along. So to start with, the motivation, just a basic kind of game, theoretic reason why if you're doing some kind of trading, you might want to use something like a dark pool, is because as I said before, you don't necessarily want to, when looking for a counterparty, you don't necessarily want to announce to all the potential counterparties which direction you're going to trade in. But the way it works in practice, especially in things like financial markets, is that the market is very, very competitive and there's a lot of sophisticated market participants. But also the markets are kind of a priori, they're very public. So I mean, things like centralized limit order books is basically a place where you can post your intent to buy or sell something to the whole world and then hope someone else will respond. But because the trading landscape here is so competitive, people will develop very sophisticated strategies that they will try to deduce what your future trading intentions are based on orders that they see at these venues. So, I mean, the kind of simplest explanation here is that if you want to trade a very large block of some asset, you won't be able to execute this trade all at once because there just won't be enough liquidity available on the market to satisfy this trade.
00:05:15.832 - 00:06:35.700, Speaker A: So you're going to try to split your trade up into pieces and execute it over a certain portion of time. But once you start doing that, then as soon as you start executing this trade, people in the market, if they're smart, should be you're already leaking information to them about your future intentions, and they'll actually be able to detect this. So an interesting perspective here is, for example, is what they call market ecology, which is kind of trying to explain these observable behaviors. For example, in the stock market, why do trends exist, for example? And then, of course, some people think that trends exist because of speculation or speculative bubbles or something. But there's actually kind of a market ecology explanation for trends, which is that a trend happens when there's some large trader who is trying to get into a position, executing a trade over a period of time, which will cause kind of the price of some asset to move in the same direction for that time as the supply demand is kind of systematically imbalanced over this period. And of course, people will try to take advantage of that because this new price discovery information is leaking into the market as these trades are being done. So a market participant like this would often benefit from trying to find a counterparty without leaking to the market their trading intentions.
00:06:35.700 - 00:07:11.360, Speaker A: So basically, there will be other people in the market who, once they see this, they will try to trade ahead of them, and they will try to basically follow this trend and make it harder to get a good price. So, and then the other thing you have to know is that, in case you didn't already, is, for example, in the equities markets, trading is very fragmented in the sense that there's many, many different venues where trading happens. So I guess if you're familiar only with cryptocurrency, it's kind of similar. There's actually a lot of crypto exchanges. There isn't just one, thankfully. The equities exchanges are actually much better interconnected than crypto exchanges. So in some sense, much more decentralized.
00:07:11.360 - 00:07:40.430, Speaker A: But what you have to know is that, yeah, there's basically a lot of different places you can trade the same stock. And this is in order to understand how dark pools work is crucial. So now I'll just briefly talk about market regulation. This is very important in practice for determining how market structure works in the US and in the EU. So in the US, the crucial piece of regulation is regnms. In the EU, it's mifid. And then the spirit of it is kind of broadly the same, even though the implementation is different.
00:07:40.430 - 00:08:37.944, Speaker A: So, I mean, the main points is that Reagan MS applies to US NMS stocks. So, like stocks that trade on national securities exchanges, it introduces a notion of an NBBO, a national best bid and offer. What that does is basically it says that kind of the order book on all national securities exchanges has to be considered together, and that exchanges aren't actually allowed to execute orders outside of the national best bid and offer, which provides quite a lot of consumer protection for people who are trading on these exchanges. And that's what the order protection rule does. There's also this concept of an ATS, an alternative trading system, which is something that's more kind of lightly regulated than the national securities Exchange, but they're basically prevented from really participating in the price discovery process. So these ATss have to basically trade within the national best bidding offer. And this is relevant we'll see later, because dark pools, in practice for equities markets are registered as atss.
00:08:37.944 - 00:09:13.896, Speaker A: So essentially, dark pools aren't really supposed to be a place for price discovery. They're just supposed to be a place to execute a trade at a price that's kind of being discovered somewhere else, and you're happy to execute at that price, provided you don't have too much extra impact. And then Mifid actually has some differences in the implementation. This is what happens in the EU. So MiFid makes a big emphasis on pre trade transparency. So, I mean, the purpose of all of this regulation is really to encourage price discovery to be as transparent as possible. So they want to avoid having too much stuff kind of going on where there's some huge volumes being traded in the dark where no one can see it.
00:09:13.896 - 00:09:55.588, Speaker A: So at least they want to guarantee that the price discovery process is happening in a lit market. But MiFid actually puts a lot of caps on dark pool volume, which I'll show you in a second. Yeah. So, for example. Okay, well, yeah, I mean, and then I obviously want to say that there's other subtleties about market microstructure that's kind of interesting here, but we don't really need to address for this talk. And there are also other solutions to this problem where a trader wants to execute a larger trade without leaking information. And auctions are an important thing to study here.
00:09:55.588 - 00:10:33.404, Speaker A: And auctions, I would say, have a lot of these similar advantages to dark pools because usually, well, especially auctions where you're not revealing your bid, they're in some sense, they're like a dark pool. Yeah. And then just in terms of the adoption of dark pools. So in the US, a lot of people don't know this. 35% to 50% of the daily equities volume is happening on dark pools. In the EU, this number is lower, but it's actually lower because of these regulatory caps. So mifid caps, 8% of a stock's volume can happen on dark pools, and only 4% in one venue, but in the EU, a much higher percentage is happening in things like auctions.
00:10:33.404 - 00:11:00.264, Speaker A: So almost 25% of the average stocks volume is happening at the closing auction. So actually, traders really like to use this. So a huge amount, even though the thing that you think of when you think of a stock market is continuous daily trading with the limit order book. Actually, a huge amount of the actual volume is happening in auctions. And then there's also quite a lot happening in internalizers and broker crossing networks, which are also regulated in the EU differently to dark pools. And they provide a lot of the similar properties of dark pools. And that as well.
00:11:00.264 - 00:11:49.032, Speaker A: The quotes aren't shown when I'm intending to trade something I don't have to broadcast to the market. So it's also broadly similar to a dark pool. So now what is the connection to dexes and zero knowledge proofs? Well, I think this is actually a very strong case that some kind of trustless design for dark pools, obviously, all of these in the equities markets, these dark pool designs are totally. They don't use any fancy cryptography. You're just delegating some, like Ellie was saying, you're just delegating this information to some authority and they don't publish the order book, but they know all the data, they know what trades are happening. And yeah, there's reason to believe that this isn't ideal and that the operators of these dark pools are often taking advantage of their authority. And it's kind of eroded trust in dark pools to some extent, even though they're still very popular.
00:11:49.032 - 00:12:26.310, Speaker A: So these are pretty huge fines for Barclays and Credit Suisse in the past, and there's been a lot of SEC enforcement actions around dark pools. Okay, so this is just a thing about market share. I'll share these slides at the end so you can look into this more deeply, but you can just see here that 35% in this week happened on dark pools and us equities trading, and that's the EU market share for the primary market. So this is the non dark pool share. And you can see how it's been dropping a lot since 2007. And most of this is going to dark pool like things. So basically the adoption of these is very high and is growing quickly.
00:12:26.310 - 00:12:58.800, Speaker A: And this is a more detailed breakdown. You can see how many different dark pools there are in the EU. These are some of the biggest ones, just a lot of different ones. Basically, it's a very, very fragmented market. Okay, so now I'm going to try to propose a really basic scheme that relies on zero knowledge proofs to make something like a dark pool. And I'm not going to talk about the cryptographic primitives that you need to implement this. I'm just going to say at this point, we pull out of a hat a zero knowledge proof of this statement.
00:12:58.800 - 00:13:45.808, Speaker A: And this design gives you pre trade privacy in that before the trade is executed. When you're looking to trade, you don't have to reveal to the world what you want to do, and it also gives you post trade privacy. So basically, settlement privacy, just like in zcash or something, once the trade has happened, people who weren't a party to that trade can't see which asset was being traded or what the size of it is or anything. And I'm presenting this as basically like a protocol extension to zero cash. So it really helps if you're familiar with that design. Basically, you start with a setup that looks a lot like zero cache. So I'm going to use kind of some of the zero cache navy conventions to facilitate the understanding of this notation a little bit.
00:13:45.808 - 00:14:38.800, Speaker A: So, just to remind you, in zero cache, a coin is this tuple consisting of a coin secret, a value, a serial number, a commitment, and I'm also giving them color so that way we can implement different assets, which we obviously need if we're doing an exchange where you're trading between different assets. Zero cash doesn't have this concept of color, but it's straightforward to add that on. And then this commitment is derived from the secret, the serial number and the value and the color with some cryptographic commitment. And then just the main thing to remember is that when the coin is minted, only the commitment is revealed, not the secret or the serial number. And when the coin is spent, you also reveal the serial number and it gets added to this nullifier set, which prevents the coin from being spent again. And we're going to use basically a lot of these same concepts. So this hopefully will be familiar.
00:14:38.800 - 00:15:23.020, Speaker A: And then also in zero cash, as opposed to zero coin, they also kind of add this layer of public and private encryption keys that you can actually use to make the coin that you're sending spendable by someone else. You kind of generate a new coin and you encrypt it to them, and then the serial number is actually derived from this private key as well. But otherwise the notation gets a bit heavy. I'm going to kind of pretend that we can do the same thing here. Otherwise it's just a little bit too heavy on the machinery. Okay, so our dark pool is going to have the following setup. We're going to have a maximum matching depth, so there's actually going to be a limit to how many orders a new order is matchable with.
00:15:23.020 - 00:16:05.960, Speaker A: We're going to have a set of n colors, which you can think of as the assets that you can trade. I'm going to assume that, I'm going to call them chi. Chi zero is going to be the cash asset. So I'm going to assume that all the markets are between some non cash asset and a cash asset, just to simplify the notation, basically. And I'm also going to assume that we have a price feed for every traded asset that we're going to use to execute trades. So this is basically analogous to how in rag, NMS and mifid, dark pools are just taking a price from some public exchange as their reference source and executing all the trades at that price, rather than actually doing any price discovery. And we're going to do the same, then it's going to have a fixed lot size and all trades are actually going to meet for the same size, which you might think is impractical.
00:16:05.960 - 00:16:35.350, Speaker A: But actually the point of dark pools is that you can execute larger trades. So it's pretty reasonable to set this to a pretty big size. And then if you've got like some, you know, you can execute your trade in those lots. And then if you've got some leftover, you execute the remainder in a lit exchange and it's probably not a big deal. And then we also have some technical parameter like the order reserve ratio, because the price can drift. And to make sure that the order is collateralized, you need to make sure that the thing hasn't become under collateralized. And then there will obviously be some realization, specific parameters for the cryptography or whatever, but we don't talk about that.
00:16:35.350 - 00:17:13.940, Speaker A: Okay, so I'm going to describe how you make, well, there's three operations here, make, match and kill. I'm going to explain make first. So let's say I want to trade an asset chi, and I've got a direction, identify one with the buy direction, minus one with the sell direction. So I generate an order secret cure s, and I generate a salt u, and then I make an order nullifier, which is kind of similar to the coin nullifier in zero cache. And I make an order commitment which is analogous to the coin commitment in zero cache. And the coin commitment is derived from the nullifier, the color and the trade direction. And then I generate a public private encryption key pair.
00:17:13.940 - 00:18:20.076, Speaker A: And then here I'm not choosing an encryption scheme, but we'll discuss later which ones would be appropriate. I generate one key pair like this, and then I generate a bunch of fake key pairs using some pseudo random function that's derived from this order secret and the order derived from the order secret, and every other color that I'm not actually trading, and every trading direction that I'm not actually trading. And the point of this is that I put together all of these fake keys for if there's n assets that exist in this market, I'm going to create n minus one fake keys and I'm going to have r1 key and the fake keys are actually going to be provably fake in that the public key is a pseudorandom function of these parameters. And later I'm actually going to prove that those keys are fake. And this is basically the key trick that allows you to do the zero knowledge matching. And then we create this thing called the key set, which is going to consist of basically r1 key. And that key is going to be indexed by the asset in the direction that I want to trade.
00:18:20.076 - 00:19:12.280, Speaker A: And then I'm going to have n minus one fake keys in there as well. And it's crucial. Yeah, so that this is a pseudorandum function. So I'm assuming that if this public key was derived using a pseudorandum function from those inputs, then there's no way that I have the private key corresponding to that public key. And there's this notion of key privacy, which is also discussed in zero cache, which is essential, which means that it should be impossible to tell based on the ciphertext which public key was used to create it. Okay, so continuing with how to make one of these orders, I'm going to produce a proof, I showed you how to generate all the data. Now I'm going to produce a zero knowledge proof that I know the order secret, the coin secret for some coin that exists in this zero cash like pool, which is already preexisting.
00:19:12.280 - 00:20:07.588, Speaker A: And then I'm also going to prove that if it's a buy order, then all of those kais match up. And that my order, that my coin that I'm using to back this order has the lot size in value. And if I'm selling, I have to show that the lot size scaled by the feed price, but also with this extra kind of collateralization buffer because the feed will change over time. And I also show this coin is not in the nullifier set, even though I'm not revealing the. Sorry. Yeah, I show that the coin has not been spent. And I show that all of the keys that don't correspond to the direction and color of the asset that I'm trading are dummy keys.
00:20:07.588 - 00:20:52.440, Speaker A: So they're derived using this pseudorandum function, which presumably means that there's no private key for that public key. So so far I have this operation which takes all those mostly secret parameters and produces a zero knowledge proof which concerns the coin serial number, the order Commitment, and this big key set. That's the order, that's the public part of the order. We're actually going to need one more component to this, and I'm going to show you first how the order book looks. So this is basically a list of a queue of orders. And the orders, new orders, are going to be added to the front of this. And for every order we tack on some messages.
00:20:52.440 - 00:21:56.108, Speaker A: How much time do we have for questions? Okay, I'm going to keep talking. You can tell me when to stop. Okay, cool. So this is basically showing how we have orders coming into the front of this queue. And then to each order, we're going to actually be attaching some messages as new orders are arriving, but only up to a depth d. And here, what I've written is that one of these messages is when I'm posting a new order for every order, for all the top d orders in this, in this order book, I pick the asset that I'm trading and the direction opposite to the direction that I would like to trade. And I encrypt my order secret to the public key that was attached to each one of those orders corresponding to same asset in opposite direction to how I would like to trade.
00:21:56.108 - 00:22:30.040, Speaker A: And this results in a bunch of these messages basically being stuck onto the order book. And that's why we have this maximum matching Depth D is so that we don't have to, this doesn't. So I don't have to do arbitrarily many of these. I only have to create D messages. And now this part is complete. So each order also has to contain this message set, which is a list of these D minus one encryptions of my order secret to each of the D minus one most recent orders. In each case, I choose the public key that corresponds to the order asset and opposite direction of my order.
00:22:30.040 - 00:23:09.430, Speaker A: And then I also extend the zero knowledge proof to actually prove that indeed I followed the rules, and that I'm doing this kind of proof of Encryption, that each one of these Messages was indeed this thing that I said that it was, even though we're not revealing, crucially, we're not revealing I and Delta here. So I is the color of the asset and delta is the trade direction. And also the coin nullifier is revealed. So the coin is now basically spent as a result of this operation. So now I explain how to do a match. So suppose I'm already in this queue and I'm this blue order, and there's been new orders arriving. So now I'm somewhere in the middle.
00:23:09.430 - 00:23:42.684, Speaker A: I'm going to be getting these messages every time someone is adding a new order to the queue. And most of them, I'm not going to be able to decrypt unless they happen to correspond to that key that I shared that was not a dummy key. In that case, I am able to decrypt this and my order is matchable. And then I have to do a match. So suppose a new order that's coming in is matchable. Then I can decrypt that order because I have the order secret. And then to do this match, I basically do this big zero knowledge proof that says for some order commitment, which is my original order, I know all of the inputs to that computation.
00:23:42.684 - 00:24:31.010, Speaker A: So I know the order secret, the order salt, the acid, and the direction. And then for some other order commitment, I don't need to know the order secret in the order salt, but I need to know the order nullifier, which is the thing that was encrypted to me. And I know the asset in the direction as well. I'm going to show that we're trading the same asset, and I'm going to show that we trade in the opposite direction, so we should be matchable. And then the order nullifiers that I just revealed are added to the list of order nullifiers so that those orders are matched now. And also, yeah, I need to create some new coins, add those coin commitments to the list, and those orders are filled. But I haven't actually revealed which of the orders in the order book got executed, which is very nice.
00:24:31.010 - 00:25:08.380, Speaker A: Okay, and then also you cancel these orders. Maybe I'll just skip through this really quickly. I basically prove that my order isn't matchable with any of the outstanding orders by doing another zero knowledge proof. And then basically when I do that, I mint more coins to refund myself to get out of this trade. But crucially, I can't do that if my order is matchable. Okay? So hopefully it was clear what the privacy result there was. So that basically, when we're posting an order here, we don't leak the asset or the direction that we want to trade.
00:25:08.380 - 00:25:51.130, Speaker A: And we also don't even leak which orders we're matching. So if you're trying to do some kind of metadata based analysis based on timing, it also kind of obfuscates things that when you see a match, you don't know which orders it's actually matching. In this long list of all of the orders that have ever been done on this exchange in this design, there's no time priority. So if you're concerned about that, we need to talk about some kind of like matching schedule or something. This matching depth limit means that there's some churn, so you kind of might need to replace your order after a while, once it becomes too old. And I want to hear also on the zero knowledge proof side, whether doing multiple kind of ord proofs of encryption and decryption is actually feasible with current proof systems. And maybe if there are any caveats there.
00:25:51.130 - 00:26:32.650, Speaker A: Yeah, here are some numbers. For example, if you could do this even with n equals one and d equals five, I think it would already potentially be useful. And I would like to know if people think that's feasible and what kind of encryption would be efficient here to use and what kind of zero knowledge proof system. And if you're interested in talking about the cryptography of this or the application or something, then I'd be really interested to hear from you. Here's some other work that you can check out in the slides that seems related to this, but I haven't been able to really find anything that was kind of directly comparable to this kind of design. But if you want to look at these slides, they're available at this link. And there's a lot of links in the slides to references and stuff.
00:26:32.650 - 00:26:55.654, Speaker A: Cool. So do you have a little bit of time for questions? I think we have about two minutes for questions. So I don't know if everyone heard that before. There's a mic right over here, and I believe on that side as well. So if you guys have questions, just go over to the mic. Yeah. Hey, cool talk.
00:26:55.654 - 00:27:46.674, Speaker A: Just a basic question. Is it possible to use dark pools also for price discovery, or do you always have to rely on this price feed from some way? In some way? Well, yeah. So it seems like it would be a big design challenge to extend this that would allow price discovery, because basically what makes this tractable is that you'll need to kind of worry about asset color and direction. I think I would like to hear about designs which also allow for price discovery. The reason I talked about this stuff in the beginning about most dark pools today, not doing price discovery is, it's kind of for regulatory reasons, which is that people tend to think that it's more healthy for a market to not do price discovery in the dark. Because if you imagine if everyone starts doing price discovery in the dark, imagine that we don't see any asset prices anymore. No one even knows what's going on.
00:27:46.674 - 00:28:10.666, Speaker A: And you're just like, it sounds a little bit scary. And potentially without regulation, markets would actually go that way because for every individual trader, it's not advantageous to help price discovery. So it's kind of a tragedy of the commons where if I'm trading, I don't care about price discovery, I just want to get the best price. But for society as a whole, it's good to have price discovery. So, yeah, this aspect of it is kind of interesting. I think there's time for one more. Okay, let's go.
00:28:10.666 - 00:28:54.646, Speaker A: What kind of settlement model do you see working with this kind of exchange? Do you see this like the centralized exchange which is holding the order book there with all the proofs and everything? And then how do they enforce the actual trade if they match, basically? Well, I mean, as stated, the settlement model was the same as zero cash. So you can imagine that you take zero cash and then you could just add these operations on top. And hopefully what I said wasn't too on the actual chain, you would put like. Well, in theory. So from what I said, it's possible to do that. Of course, it's probably not very scalable or something. The nice thing, and I think people often say this with zero knowledge stuff, is that because of the zero knowledge property, in some cases it makes it more censorship resistant, even if your consensus layer is more centralized.
00:28:54.646 - 00:29:26.178, Speaker A: Because how would you, at the very least, if we were running this on a one computer blockchain controlled by someone, it's pretty hard for them to censor me because they really can't see what these transactions are about. But there might be other reasons why that's not desirable. But yeah, you're absolutely right. I think it would be quite nice as a proof of concept, at least to start with. You can just run this on one machine and it's quite good. It gives you a lot of the guarantees that you want. And that's why I mentioned the role of dark pools in equities trading is because I think it would actually be pretty neat if someone made this product for equities trading.
00:29:26.178 - 00:33:50.636, Speaker A: And it doesn't even matter if it doesn't need to use blockchains, it doesn't need to use some non permissioned consensus layer. Thank you. Cool. Well, thank you so much for your talk, Lev. Very nice. So I don't know if you guys have noticed. I think we're running about like 20 minutes behind.
00:33:50.636 - 00:34:21.720, Speaker A: So the schedule is a little bit must be adjusted. I don't know if you mind moving, but if you wanted to come a little bit closer, there's some room up here. So just to those in the back if you want to come up, that would be cool. And, yeah. Next up, I want to introduce Martin Kupelmann from gnosis. He's going to be talking about a decentralized trade matching engine with batch auctions, and he's going to be making the case for fully permissionless exchanges without any operator. So welcome to the stage, Martin.
00:34:21.720 - 00:35:41.800, Speaker A: All right. Okay. Yeah, exactly. I guess it's a good continuation of the last talk because I think I want to make the point to experiment with even more radical market design changes. I see basically the potential to make market designs, fully decentralized market designs that don't, for example, require duck pools. Maybe the issue that duck pools are addressing is that you don't want to have your trade front run, so you don't want to publish your trade and then someone else being able to acting so front running. Resistance is a big topic in this talk.
00:35:41.800 - 00:36:37.350, Speaker A: And also the idea of a fully permissionless Dex. So not just a Dex where that's noncustodial but still has an operator, but dexes without an operator altogether. I will hopefully come to the next slide. Okay. It's just taking time. Right. So I will give a short overview of different dexes and a little bit discuss the different designs and kind of make the point between those that require an operator and those who don't and present the batch auction approach as a design that doesn't require an operator.
00:36:37.350 - 00:37:41.280, Speaker A: All right. Okay. So again, those are the designs we have seen on dexes that are existing. And we have on the one hand, those where orders are basically sent to an operator and the operator does some matching and eventually the settlement is then done on the chain. And on the other hand, there are also designs where there is no operator whatsoever required. And that typically involves sending the orders directly to the blockchain and also having the matching process part or executed by the blockchain or by smart contracts. And, yeah, those are examples here.
00:37:41.280 - 00:38:37.482, Speaker A: Okay. This is just always taking a little bit. So the advantages, of course, are on the permission side. So on the site with an operator that you can achieve stuff like instant finality and in some cases high, less scalability because you can, for example, place orders off chain, but you still have this operator, and facing regulatory requirements is just like one issue. But in general, I basically want to kind of show how a permissionless system could be much superior. So the core point is that fully decentralized system provides liveness guarantees. So it's basically the whole point of Ethereum.
00:38:37.482 - 00:40:15.870, Speaker A: So you want to have a system that's always available where anyone can use it, where other protocols can build on top of it. So a lot of DeFi protocols, lending protocols, require some form of exchange. And ideally they want to build on something where they have the absolute guarantee that this is available forever and that there is no operator that can at some point decide that they don't want to support their specific application. But even the bigger vision is that a decentralized system has the potential to become a global liquidity pool. So imagine you want to place your trade, or you want to trade some asset a versus an asset b. And at some point you will have the choice whether you just submit this order to one operator and then kind of just everyone else who is dealing with this operator can trade with you, or you submit it to this open global system where everyone in the world can access or kind of can give you the best rate and give you the best price. So my claim is that if such an open and global system would exist and it would be sufficiently efficient, there would be no point whatsoever to kind of submit your order just to one operator because you want it to be accessible to the whole world, this open system will win.
00:40:15.870 - 00:41:46.250, Speaker A: I mean, ironically, as you described it also how it works today, that is to some extent the case, because there are those regulatory rules that basically require from operators that they give you the best price even if the best price is available at other operators. But that is of course tied to geographical boundaries or jurisdictional boundaries, to regulatory boundaries. And I think with decentralized systems, we can build those systems and have those guarantees without regulation enforcing it, but by technology enforcing it or by technology like providing that. So yeah, I will relatively skip fast over this. At this point we can just look at where's trading volume happening and. Right, so at this point, at this point, exchanges that are permissioned or IDEX is still the most successful one. So the open or non permissioned systems are so far smaller, but they already play a significant role.
00:41:46.250 - 00:42:34.520, Speaker A: Yeah, this is also just trading volumes changing over time. But I think the only message maybe is that the race is still completely open, which market designs will get a lot of traction. Right. So I give you now an overview of those different market designs. This is, is working actually, or is someone else clicking? If I click? Okay, anyways, try to continue. Okay, perfect. Thank you.
00:42:34.520 - 00:43:20.406, Speaker A: Yeah. Right. First discussing on chain order books. So the most naive approach would probably, or is probably to post just orders on the blockchain directly and basically replicate, one to one, how most centralized, at least crypto exchanges work. So continuous double auction just replicates this one to one. The issues with this is that it's fairly expensive to use because you have to post orders on the chain and everything. That is.
00:43:20.406 - 00:44:23.350, Speaker A: Yeah, you have definitely issues with front running, but I will discuss this later. It tries to mimic the continuous double auction, but you have those racing conditions and gas auctions. So basically, as you see the transaction that is trying to trade against this exchange, as this is already available in the mempool, you will often find that then people doing arbitrage and are trying to outbid those orders with gas auctions. Same problem with the uniswap mechanism. The advantage of this mechanism is that it's fairly cheap to market make. So those that put liquidity into this, they don't have to constantly update their prices. Basically, just as people trade against them, the prices are updated algorithmically.
00:44:23.350 - 00:45:23.918, Speaker A: And Kyber is actually doing somewhat similar with a little bit more flexible price design. So there, those who provide liquidity can do this probably more efficiently, and they also aggregate different sources of liquidity. So then that is the design we first built, and now others are using it as well. Those are the dutch auctions. So here it's a process that takes a little bit of time. So if someone wants to trade token a two versus token b, they submit this order and then basically say, yeah, I want to trade a for b. And in the beginning they request a very high price, and as time passes by.
00:45:23.918 - 00:46:42.120, Speaker A: So here the x axis is time. The price continuously drops on a predefined schedule. That could be linear, or it could be one divided by x function. And while the price drops, those who want to do the opposite trade can do an order, and it always adds a price that is defined by the current time. And as there are enough orders submitted to clear the full auction, then everyone or every trade is cleared at that price. So even those that submitted their bid earlier and were willing to accept, from their perspective, a lower price, they will still then they get the final price, which is the price where all trades are clearing. So the advantages here are that this can run without any operator, is front running resistant, but takes quite a bit of time or at least in the design that we build, we choose an auction time of on average 6 hours.
00:46:42.120 - 00:47:51.754, Speaker A: Right now it's most actively used by set protocol. They are doing auctions with a volume of a million dollars or more, and they have like 20 minutes auction times. So yeah, different designs are possible, but of course the shorter the time is, the riskier it is for those who are selling, because the faster the function goes down and they are at risk that not quick enough bidders are coming in. But I want to now go onto the finally design we are proposing. And those are the batch auctions. So a batch auction works quite similar, like a normal order book, that you have a bunch of orders with limit prices, but instead of executing them immediately as they come in, you always wait for some period. So you always wait for a batch, and only if the batch, or kind of if that time is over, a batch is closed.
00:47:51.754 - 00:48:34.202, Speaker A: And then you calculate an optimal clearing price and kind of not to confuse with. So I think Eli earlier talked about batched execution. So this is something different. So batch auction, the key factor is the single clearing price. And that's not necessarily the case with batched execution of single orders. So in this batch auction, you can also introduce something we call ring trades. So such a batch does not have to have orders that just match each other exactly.
00:48:34.202 - 00:49:32.990, Speaker A: It just matters that overall kind of all orders clear together. So it just matters that overall for each asset you have the same amount of buyers and sellers. But again, you don't necessarily have to have for someone selling b, four c, you don't have to have exactly the counterparty. The advantages here are those ring trades, and again, no front running opportunities. I will skip this a little bit and go again, look into the batch auctions. So key features are front running resistant, no operator whatsoever. Therefore it's also not possible to be front run by operator, because there is no operator, there is no one with a special permission or special role.
00:49:32.990 - 00:50:28.346, Speaker A: We enable those drink trades and roll up compatible, or that dark snark scaling is possible. We'll discuss this also later. So again, the key arguments for why batch auctions. One is that decentralized systems, almost by definition, do not have such a thing as continuous time. So the smallest unit of time in a decentralized system is one block within one block. The ordering that the miner can assign with one block is totally random and totally up to the miner. So you should not, or any system should not make assumptions that the ordering within a block is actually saying kind of something about the actual time when those transactions were submitted.
00:50:28.346 - 00:51:30.414, Speaker A: So the consequence should be that all orders within one block at least should be treated equally. And that is already a strong argument for a batch auction. And in this case, the batch time would just be one block for other reasons. If you would design a blockchain that just does this, then of course it would be possible. As we are using Ethereum, and we are not the only users of Ethereum, we right now cannot do those batches as small as one block because we are not guaranteed to kind of be able to process things every block. So we will have to start with something a little bit more than a block. Currently we are talking about five minute block times on a high level, this system is trying to replace, or right now, how centralized exchanges work is they are connected and they are super fast connected.
00:51:30.414 - 00:52:43.610, Speaker A: So you have this real time price discovery because of all the arbitrage happening super fast, you have this price efficiency. And effectively, then among all exchanges, one price. What you here do is it's slower, but you have the right price finding by having this actually np hard optimization problem of how to set the clearing prices. And we are addressing this by turning this into an open competition. So once the orders are submitted, you have this open competition where solvers can submit their solution that provides the highest trader utility. And there are also arguments for normal, for kind of outside of the world of blockchain, there are strong arguments for batch auctions. Buddhist published a paper where he basically argued that after some point, doing arbitrage does not increase overall efficiency, but is only kind of a tax on those who don't do high frequency trading.
00:52:43.610 - 00:53:48.974, Speaker A: Again, we collect those orders, maximize, trying to maximize for trader, what we call trader utility. So that's basically if you request a specific amount of tokens, and if you get more than that, then this increases your utility. So in the case of just one order book, or like just one trading pair, maximizing trader utility is basically equal to maximizing trading volume. In the multitoken case, it's a little bit more complicated. So here, this is roughly the optimization problem. So we have to respect the limit orders, of course, of every trader, we want to have price coherence. So price of a and b and price of a and c should also define price of b and c.
00:53:48.974 - 00:54:26.700, Speaker A: So within the prices, there is no arbitrage whatsoever, arbitrage freeness and of course preserved value. So yeah, we cannot mint or destroy tokens. And again, we maximize then for trader utility. So this is roughly how the optimization problem looks like. We have all those possible prices, and actually we will have much more tokens than three. And we can turn this into a linear optimization problem to find the best prices. Yeah.
00:54:26.700 - 00:55:12.166, Speaker A: Last remarks on scaling. So we are building a version that's fully on chain. So that means the orders are collected on chain and we can collect an unlimited, or, like, there's no limit of how many orders the system can hold. And adding an order costs roughly 40,000 gas. And then orders can also be valid for many batches. So there could be 100,000 2000 orders in the book. But for each execution, because the execution is fully done on chain, we can actually just execute 20 to 30 orders because the solver has to settle.
00:55:12.166 - 00:56:28.430, Speaker A: Actually, all those traits earlier, we have been exploring, of course, snark and stark designs where we would just submit. I mean, there will be a talk here on the stark decks where this will be presented much more in detail, but it's a very similar concept. So on the blockchain, we would just update a hash that represents all trading accounts, and then we would submit a proof that basically all those rules here are fulfilled. Right. And maybe like just one last comment, there are of course also designs, or in our first design, the publication of the orders is or you publish the orders openly. But designs are also, of course possible where the orders are encrypted, and then it's quite similar to the stack pool design. So orders are encrypted and only once the batch is closed, you have a service that then reveals the key.
00:56:28.430 - 00:57:19.474, Speaker A: So I'm definitely not a fan of this commit reveal, because it always commit reveal of an individual trader, because it always opens up the possibility for a trader to not reveal. So here, another service would just reveal this key. And this other service could be something like an n out of m service where n parties have to, or at least the threshold has to reveal their key share to construct the full key. Then orders can be decrypted and then you can have again this competition round solvers to find the best clearing price. That's it. Thank you. So once again, if you guys have questions, there's mics here and here.
00:57:19.474 - 00:57:52.326, Speaker A: We have about five minutes, so if you have some thoughts on this proposal that Martin made. Hi, can you hear me? Great talk. I have a question on you mentioned earlier dutch auctions with Seth protocol. So isn't short time. Dutch auctions suffer from a very severe problem of censorship. Like a miner can literally wait and gets lower price. Definitely, I think, yeah.
00:57:52.326 - 00:58:54.180, Speaker A: Again, set protocol is currently using 20 minutes auctions. So the relevant number is like probably the price will start somewhere where no one wants to trade. So actually the period where prices are there, where people want to trade is pretty short. And Lev can actually maybe give more details about this. So, yeah, so probably by just already censoring three, four blocks, which would be like half a minute, that could already play a role. And the question is a little bit, if you are the miner and censor a block, you are not necessarily guaranteed to then later get the better price because you might not mind this block, unless of course you have 50, 60, 70% of the hash rate, but then you could completely screw this. Thank you.
00:58:54.180 - 00:59:49.858, Speaker A: Hi. Thank you. So could you say something about the role of the privacy of the bids in the batch auction? Because it's pretty clear how the dutch auction is quite resistant to front running without any bid privacy. So dutch auctions, usually there's no notion of a sealed bid, but in the batch auction, I'm just wondering if the same thing up with. Yeah, so definitely in the design that is game, theoretically, that has the game theoretic properties that you would actually be willing to reveal your true price. It would be required to have the bits sealed or encrypted. This design is still, I would say, significantly less, or has significantly less problems with front running because of the single price clearing, basically.
00:59:49.858 - 01:01:14.462, Speaker A: So in other designs, as a front runner, you can completely risk free make a profit by, well, getting the better price and kind of buying at the good price and then sell at the worst price. And this is not, obviously not possible in the batch auction because everything is cleared at a single clearing price. So within one batch, there's guaranteed no arbitrage possible. That still leaves the possibility for some games to be played to kind of try to, depending on the state of the order book, set prices at a specific level that will lead to a clearing price, but still for everyone, if you can influence the clearing price, everyone else will also get this clearing price who's trading on the site. So this effect is, of course, only, I mean, like, if you're the only trader or if there are just two traders in the system, then you fall back to it being an issue. But as there are more traders and there's more liquidity in the system. Cool.
01:01:14.462 - 01:02:02.506, Speaker A: Thanks a lot. Thank you very much, Martin. All right, so as mentioned, this stage is running 20 minutes behind. I heard that the other stage is sort of comparable, so we can still switch if we want. I want to invite Arthur Gervais to the stage from liquidity network. Let's give him a hand. He's actually going to be talking about a solution kind of to the problem that Martin had already outlined, focusing on how you can prevent front running using zero knowledge proofs and timelapse cryptography.
01:02:02.506 - 01:02:51.270, Speaker A: Cool. Thank you very much for the kind introduction. That's like rather a broad intro because the prior two talks, I think, summarized the topics very well. But financial exchanges are really at the heart of our economy and speed matters. And what we see in the traditional world is we have these very centralized operators here that have custody of their funds. So over the last centuries, there was no alternative actually to use to trade significant volumes, but to go over these centralized custodians on a very high level. Financial exchanges 101 is an exchange is built out of two components.
01:02:51.270 - 01:03:24.978, Speaker A: So we have a trade matching system and we have a trade settlement system. And this really is the exchange in a very simplified manner. So if you look at Ed Martin's talk previously, he explains how to do this without a central entity, which is really amazing. But here we're looking in this talk, we're going to focus still on an operator model where we have an order book. So this is an exchange architecture. Let's assume here we have two traders. So there's Michaela, she has some black coin.
01:03:24.978 - 01:04:04.386, Speaker A: There's Jackson, he has some white coin, and they would like to trade these coins. So Michaela puts in an order, I would like to sell this black coin and get a white coin and the same done by Jackson. And then the matching system can say, great, I have two matching orders. I can forward them now to the trade settlement layer. So the trade settlement here just switched the trades. You see, the trade was executed on the settlement layer. So what could possibly go wrong? Martin thankfully already mentioned very well front running resilience or front running issues.
01:04:04.386 - 01:04:33.194, Speaker A: So there are actually many different things that can go wrong in exchanges. And it's really fun. You can go to the New York Stock Exchange website. They have a compilation of all the kinds of issues. So it seems traders in the last decades have become very creative on different malficiencies. Within this talk, we will focus on front running primarily. And it's defined as when you enter into an equity trade on option or futures contract, enhanced knowledge of a block transaction.
01:04:33.194 - 01:05:18.650, Speaker A: So like a large transaction. And you're aware that basically you're entering into this trade will influence the price of the underlying security. And your only intention here is to capitalize on the trade. So you're aware your trade will change the price and you're trying to capitalize on that change that's actually forbidden by the SEC. So according to, yes, your law and traders are not allowed to act on non public information in general, which spans across insider trading, to trade ahead of customers lacking that knowledge. So it should be public knowledge. So now some might argue, well, if you do front running in a blockchain, these transactions, they are broadcasted, everyone knows about, right? So it's like transparent.
01:05:18.650 - 01:06:03.686, Speaker A: So another malficiance is wash trading. This is where you can create beautiful gardens. And here you can see we have analyzed some blockchain exchanges on chain exchanges. And you can see some addresses are trading with themselves, creating beautiful flowers. So the talk will be mostly about front running. Just to give you some example that there are many other different security issues in exchanges. In particular, if you're looking at blockchain based exchanges, the question we should ask ourselves is, so who is actually the adversary? Who is the malicious entity that we're dealing with? There is a blockchain miner, there's an adversary trader, like your other traders.
01:06:03.686 - 01:06:59.820, Speaker A: Some financial traders say, well, trading is like a war, right? You're going to war, and it's always like a jungle, like Radalia says, and you really have to fight. So your opposing traders are kind of your opponents here or the operator. So if you're not having a dutch auction model like Martin presented, then you're probably dealing with some centralized operator here who might or might not front run your trades. Why is that a problem? In reality, nowadays, most crypto exchanges are not regulated, so the regulators haven't yet caught up with regulating what is good and what is bad behavior. Moreover, it's really hard to detect bad behavior, and you can almost never prove it. So it could be that you have an exchange that offers you zero trading fees, but maybe they just capitalize on your trades by front running them constantly. So there are potentially millions of us dollars in damages every year.
01:06:59.820 - 01:07:49.690, Speaker A: And one recent study by Diane that I'll try to quantify for on chain exchanges how much front running practices there are. So, it's an excellent paper I recommend you have a look at. So, let's look at the trade matching engine first. So, trade matching engine is really just an older book where you collect bit and asks, and Michaela here, for example, wants to buy a certain asset, and Jackson wants to sell a certain asset at a certain price. So there's a certain spread here. So, because they haven't yet matched to agree on a trade, in this talk, we will be looking at two order book models. So you can either put the order book on the server side or on chain on the blockchain.
01:07:49.690 - 01:08:37.096, Speaker A: So the advantage of putting the order book on the server side is that you have a fast matching, right. There's no fees for canceled orders, there's no censorship resistance. However, trades, if you go with the onchain model, well, you do have great censorship resistance is fairly robust, right? I mean, your broadcasting is to the world. Everybody has your traits. However, you have slow matching. You need to pay blockchain fees for every order, even if they're not fulfilled. And now you're having a different set of adversaries.
01:08:37.096 - 01:09:21.864, Speaker A: You're having the miners and the adversary traders. So miners can just reorder within a block or sensor, as was discussed earlier. Or the traders can pay a higher gas price or high transaction fees in general to prioritize their transactions and try to front run you. We will discuss the trade settlement layer, but we will focus on noncustodial trade settlement layers. And there are two models that we could think of. So there's an off chain model and there's an onchain model again. So by off chain, I'm referring here to the broad set of offchain techniques that are out there whereby you can perform a transaction off the blockchain, but it's secured by the blockchain.
01:09:21.864 - 01:10:13.470, Speaker A: We published a paper recently of trying to summarize the state of the art and off chain solutions. And I mean, the field is progressing so fast, so there might be a few that have basically come up since then. So if the trade settlement is on chain, again, we have censorship resistance. It's fairly robust, but it doesn't scale and it's quite slow, which is bad in financial exchanges, as was discussed earlier, you don't want to be the slow exchange because otherwise the fast exchanges are profiting off you. There are blockchain fees for each order and minor trader fund running. If you do the settlement off chain, it's fast and scalable. There are typically no blockchain fees, no minor trader front running, but you don't have censorship resistance, and the exchange operator could front run.
01:10:13.470 - 01:10:51.912, Speaker A: So as a summary, this is kind of the design space that we are looking at within this talk. There might be further options, like the dutch options that we saw earlier. So for the purpose of our exchange tax, the trustless exchange, we settled with this architecture. So a server that maintains an order book and an off chain protocol, trade settlement protocol that I will go into details later. So what about the server exchange trade matching system? So here we can see. Well, it's good because we're quite immune to front running from miners and traders. So those bad guys are gone.
01:10:51.912 - 01:11:26.160, Speaker A: Right. However, now, well, the server can front run our orders. So we will talk about this now in the front running resilient order book. So by resilient, I mean almost immune. It can probabilistically detect front running if you're willing to take certain assumptions. It's actually robust. So the underlying idea that we had to build such a front running resilient order book is to have a commit and reveal protocol for limit orders.
01:11:26.160 - 01:12:15.940, Speaker A: Our goals was that it's difficult to dos the exchange. We want to hide the order content before the exchange commits to an order so that the exchange actually doesn't know anything about the trade information, like what coin I'm buying or what asset I'm buying, what volume or selling. But the exchange still needs to know that an order is valid before committing to an order. Sounds like a perfect use for zkps. The faster the exchange commits to a particular order, the better, because then we know that likely there was no front running. And this is where we came in with a moonwalk order. So in order to prevent front running, we're using moonwalk orders, which are composed of a zero knowledge proof, the encrypted order, and a timelock puzzle.
01:12:15.940 - 01:13:08.420, Speaker A: So why do we need the zero knowledge proof? The zero knowledge proof proves that the order is a valid order and that I'm allowed to spend this amount of assets or buy this amount of assets. The encrypted order is basically just the order contents in an encrypted form that the exchange can decrypt with the timelock puzzle. So the timelock puzzle allows the exchange to decrypt an order after time t in case that the trader doesn't reveal the appropriate key. So let's go into the protocol and see step by step how this would work. So we have here Michaela the trader. We have a trade matching off chain settlement system on the server that by default would not resist front running from the operator. We have here parent chain, a blockchain, and a smart contract.
01:13:08.420 - 01:13:52.832, Speaker A: So Michaela creates this moonwalk order with the ZKP encrypted order and a timelock puzzle. She then sends this order to the exchange. So that's an encrypted order. The exchange doesn't see the contents of the order, and the exchange has a Merkel mountain range of existing orders. So it will append this new order to the Merkel mountain range and send the commitment of this MMR back to Michaela. So now what MichaElA can do, she can measure the round trip time it took for the exchange to send this order back. So the faster the exchange came back, the less likely the exchange was actually attempting to solve the timelock puzzle.
01:13:52.832 - 01:14:33.372, Speaker A: That would allow the exchange to look into the encrypted order. If MIchaELA is nice, she will reveal the key. And once this happens, the exchange basically allows MIchaElA to resubmit a new order. So that's one DOS protection mechanism here for the exchange, whereby the exchange only allows the trader to submit a new order if the trader revealed the prior key. So what's interesting here is Michaela can repeat this. She can actually test the exchange multiple times. And you can just do this for very small orders and always measure this delta t, the time that the exchange takes to respond.
01:14:33.372 - 01:15:21.712, Speaker A: And if you know how much time it would take with the best computational hardware that you're aware, to decrypt a timelog puzzle, if you're aware of the round trip time, the network latency to the exchange, you can roughly approximate whether or you can guess whether the exchange attempted to front run your order or not. So, front running here would require the exchange to change the Merkel Martin range commitment. And this Merkel Martin range commitment is checkpointed at regular intervals to the on chain smart contract, where Michaela can prove malficians later on. So it's almost about building trust. So you have an operator. He can decrypt the order in t seconds. If it takes Michaela t seconds to receive back the receipt of the Mooch mountain range, then front running can happen.
01:15:21.712 - 01:15:59.836, Speaker A: Doesn't have to, but can. So Michaela would basically issue k fake or small orders to build up trust here. If the exchange would do anything malicious, then we can always resort to the on chain smart contract and challenge the operator. And, for example, there can be a slashing of a certain deposit or some other actions are possible. So we don't specify this. What about the trade settlement layer? So, now we looked at the order book, but once we have found two matching trades, we want to settle them. So which off chain solutions do we want to look at? There are many solutions out there.
01:15:59.836 - 01:16:33.396, Speaker A: I think the broad categories would be channel networks and commit chains. So commit chains, for example, they are fraud proof based, validity proof based. There are a lot of solutions out there. We settled with nocast because we develop nocast, and Nocast also has a zero knowledge version. So if you're looking also at real world systems in production. So there's. On the one hand, there's lightning, but, yeah, we couldn't really see how to do atomic trades or to build a decks on the lightning network here.
01:16:33.396 - 01:16:57.272, Speaker A: And there's the liquidity network, who implements and runs the nocast protocol in production since March this year. And you can see real traffic on this network. So roughly on a high level. I just want to give you a glimpse of what a commit chain is. And there are many variations of this, so it's quite cool. The design space is really large. You basically have a blockchain.
01:16:57.272 - 01:17:28.728, Speaker A: You have a central operator. The central operator commits an off chain state in typically a constant sized checkpoint or via snark, stark, et cetera. And there are a certain set of off chain transactions or swaps or tummy swaps that happen off chain. And basically here the operator would commit at regular intervals. These can be small or larger. It's really just a parameter, like the block size or the block time interval in a layer. One blockchain, depending on the design you specify here, round or eon size.
01:17:28.728 - 01:18:03.296, Speaker A: And your users might be required to come online to verify the integrity of those checkpoints. It really depends on the specific design that you're looking at. In Nokia specifically, we have. This is the key innovation. It's a Merkle interval tree where each account is represented as a balance, where we have an active and passive tree whereby a user can be offline to receive a payment or even a swap. So in the case of tax, two traders. So two traders that want to match a trade, they don't have to be online at the same time to match the trade.
01:18:03.296 - 01:18:39.148, Speaker A: That's a very important requirement. You don't want users to be required to traders to be online at the same time for their trades to match nocus. ZKP is an extension to nOcas, which adds your knowledge proofs to this checkpoint. What we're using is a recursively composed. Recursively composed proofs. So the idea here is that we could potentially enable a distributed ZK miner. So instead of having one central operator who does this big giant proof, which is computationally very expensive, we could outsource this to a set of miners.
01:18:39.148 - 01:19:38.800, Speaker A: And this would actually add to decentralization, because then the central operator would depend on the collaboration of those individual miners. Xenolodge proofs reduce a few attack vectors that would be otherwise possible. We currently evaluated the gas costs is about 500k pre EIP, one, one eight, and by a factor of five smaller post EIP. So because we are fraud proof based, the commit chain can be halted. So what could happen if the exchange does something malicious or is being dossed for a continuous amount of time? So in any case, if it doesn't render its service properly, a nocus commit chain can be halted. So this means some trades would be reverted. In the trading world, this is a bit less tricky than in the payment world, where because in the trading world you would get basically your prior assets.
01:19:38.800 - 01:20:21.680, Speaker A: In the. In the payment system. This should be mostly insured, but this can be fixed by providing collateral for instant finality. Still, the proofs are quite computationally expensive to be performed. So on the right side you can see the wallet of liquidity network that already is on the play store and App Store on Mainet, and on the left side you can see a mockup of text, but you can already try the rinkby version. It's online text liquidity network. Basically, you can send funds from the mobile wallet to the exchange near instantly and then trade on the rinkby network on the exchange.
01:20:21.680 - 01:21:17.992, Speaker A: So we really see this as being a layer where you can build up your decks or your particular wallet and enable it to be more scalable. The full details are published in those two papers, so if you have any questions, we try to be as precise as possible, and we know there are so many subtleties that it's really hard to explain them very well. But I think it's very important to have this open research and details such that anyone can build such systems or extend them and build better ones. So thank you very much. I'm happy to take questions. Yeah, just make your way over to the mic. It's working.
01:21:17.992 - 01:22:08.920, Speaker A: Thank you very much. Arthur. One question regarding the moonwalk orders. So when the user wants to prove that he has the right amount of balance and he can actually execute the trade, he proves it according to what state? How does he prove that he has enough balance? Yeah, that's an excellent question. So the front rowing resilient order book and the trade settlement layer are not that decoupled as I presented them in the slides. So in our case, we actually mix the information of the trade settlement layer with information of the exchange order book. Ideally, it would be great if even some centralized custodial exchanges could use the front running resilient order book design, but then they wouldn't be able to include probably.
01:22:08.920 - 01:23:05.176, Speaker A: I mean, probably would be trickier to include a proof that a trader has a certain balance to be executed. In our case, we mix the information of the trade settlement layer and therefore have access to this data. Thanks. Thank you. I was wondering if when a trader realizes that they might be being front run because of the responses being too slow, is that limited to just subjective knowledge, or is there some way that they could prove this, or have you thought about that kind of a thing? Right. So here we're really trying to detect, in the first instance, to detect front, the trader receives a receipt, the head of the Merkel Martin range from the exchange. And that's a commitment, that's a non reputable signature that the trader gets.
01:23:05.176 - 01:23:55.790, Speaker A: If the exchange would then commit on chain non compatible checkpoint, then there's actually cryptographic proof that there was front running. Yeah, sorry, my question was about the, because my understanding is that the front running detection is coming from the delay. Right. So let's say I'm always getting slow responses. That just means that I'm going to avoid trading there. But have you thought about how to prove it to other people that they should also avoid trading there? I mean, if you have a signature from the exchange on a certain Merkel Martin range, and the exchange does not commit to the same one later on, then you do have a proof that you can show to others that the exchange actually performed front running. Okay.
01:23:55.790 - 01:24:42.424, Speaker A: Hi, I think there's another question over here. Sorry, just one question. Why is it important to prove that the order is valid? I mean, what would be the issue if someone just submits an order and then it's kind of encrypted and it turns out that they don't have enough balance, but it's just ignored? Right. This exchange is not directly, I believe, at least, I mean, the further measurements and studies should, should show this. I don't believe it's suitable for high frequency trading, because if a trader does not reveal a key, then the exchange is forced to open up the time lock. Right. So this might take at most time t.
01:24:42.424 - 01:25:12.868, Speaker A: The good thing is, if many traders are kind of trying to dos the exchange at the same time, then the exchange can parallelize the decryption of these time lock orders. So your question was, why would we prove, or why would we try to? Basically, why it's necessary to prove that this order is valid. I mean, would it hurt to. It basically reduces the number of attack vectors for the exchange. Okay. Yeah, that's the main reason. Right.
01:25:12.868 - 01:26:02.004, Speaker A: So there might be other ways to reduce those vectors. Yeah, but xenorge proof verification is very quick, so doesn't hurt. I mean, it's more work on the client side to prove that it's actually a valid order. Hi. Do you see any other things that are required from the regulator, such as front planning, as market conduct, for example, spoofing, watch trading, publishing of Libor. All of this exists already in the trading right. And currently, by the way, from the regulator, the responsibility is from the action, not from the exchange to mitigate that, but from the one that is, for example, big banks to take action upon that.
01:26:02.004 - 01:26:43.730, Speaker A: Do you see any action that is currently being taken by the regulator or it's still in? I have not seen any action so far. I really see this kind of solution. I see it as a competitive advantage for exchanges today, for just being honest before the regulators even come in. But I haven't seen any actions, and I can only speak for Switzerland for swiss regulations. For example, if you're a non custodial exchange, then you're not even subject to AML KYC laws. Okay, thanks. All right, there's actually a little bit more time if anyone has any more questions.
01:26:43.730 - 01:27:19.530, Speaker A: No? All right, so then I guess we'll move it up. We're going to start catching up on time. Thank you. All right, so next up, I want to invite Joey Krug to the stage from Auger to speak about sort of the other side of Dexes, a very important component that's oracles. This is going to be an overview of the importance of oracles, the challenges they face, their history, and, yeah, I'm very excited to hear about this. Welcome to the stage, Joey. Thanks for having me.
01:27:19.530 - 01:28:25.040, Speaker A: Yeah, so this is the first talk that's not really too much about Dexs, although it's still a little bit about dexs. So we're going to go over what an oracle is, the history of Oracle's kind of current approaches, and what the future of the Oracle landscape looks like. So it's not the oracle of Omaha. It's the problem of how do you get real world data into the blockchain in a secure way? So how do you know that the weather in San Francisco is 72 degrees and that somebody didn't just make that up and submit it on chain? And so if you look at why are these useful? My favorite use case is prediction markets. They're betting markets on kind of future events. Other use cases are for margin calls in various decentralized trading apps, things like derivatives, things like DYDX, Makerdao, cdps. They're liquidated based on the price of ether, which, of course, comes from an oracle.
01:28:25.040 - 01:28:58.360, Speaker A: And so pretty much almost all the main defi apps use or touch oracles in some way. Even if you're not using them directly, you're probably built on top of somebody who is. So it's a pretty huge thing in the space. It's also a huge systemic risk because it's probably one of the most for most apps in the space right now. It's probably one of the most centralized areas in crypto. So the kind of history with them started with really three projects. The first one is very classic, trusted third party.
01:28:58.360 - 01:29:49.656, Speaker A: So a person named Edmund Egger in 2014 came up with this idea called reality keys. Basically, the idea is you sign a message with a private key, and then on chain, you verify the message. These face lots of problems. They can be easily bribed, hacked, you can pay them to be dishonest, you can coerce them or force them to report something. And it has kind of all the same problems any traditional trusted third party would face. The next one was also in 2014, when Vitalik published a kind of off the cuff blog post for this idea called Shellingcoin. It was really just one of those initial use cases for Ethereum, kind of like a thing with some sample code written in serpent to get people excited about what you could do with Ethereum.
01:29:49.656 - 01:30:25.800, Speaker A: And the idea for shellingcoin is basically, you ask a bunch of people a question. An example might be how many inches of rain fell in a certain location. And basically everybody submits a value, and anyone within a certain percentile range gets a reward. He called these tokens shells, hence the name shelling coin. And by default, this system has no staking mechanism. So kind of out of the box, it doesn't really have a civil prevention mechanism. And even if you add one, basically what ends up happening is the richest actors determine the truth, and successfully lying kind of cost the attacker nothing.
01:30:25.800 - 01:31:13.744, Speaker A: Because the system is reward based. There's no slashing conditions, no punishment for bad behavior. So somebody else basically published a different idea around the same time called truthcoin. And for this idea, the idea is you basically ask a bunch of people the same question, and you ask them a bunch of other questions as well. Then using principal component analysis, you basically bifurcate people into truth tellers and liars based on a big covariance matrix created from users in the events that they voted on. So you might have like a row being an individual user and the columns being specific events that they voted on and the values being what their vote was. And so by default, the system has a staking mechanism built in, and there's different branches.
01:31:13.744 - 01:31:53.840, Speaker A: And the idea is that if you have a question about, say, politics, it might go in the politics branch. If you have a question about sports, it goes in the sports branch. If you have a question about ether prices, maybe it goes in the crypto prices branch. And so on and so forth. The problem is, without branches, the system doesn't scale because everyone has to report on everything. With branches, the system essentially slices the security model by a factor of n, because each branch is its sort of own universe. And so you can envision that if there's some asset that you're staking and the asset is different for each individual branch, you're basically decreasing the security model by splitting it up across these different universes.
01:31:53.840 - 01:32:46.368, Speaker A: It has some other problems, too, like principal component analysis turns out, actually isn't the most effective way to kind of distinguish between people who are being honest or reporting with the consensus and people who aren't. Simple clustering algorithms perform better. They're cheaper and easier to understand. But the other big problem is that it's vulnerable to a bribing attack called the p plus epsilon attack. And this was published by Vitalik, I think, in 2014 or 2015. And the idea is basically that you can promise people that if they're dishonest, say, you'll lose $100 for being dishonest, you can promise them that if you force them to report something inaccurate, you'll say, I'll pay you $100. And in the case where that inaccurate vote wins, I'll just pay you a small amount extra profit.
01:32:46.368 - 01:33:24.184, Speaker A: And so your financial incentive is basically to be dishonest, because worst case scenario, you make a small profit and make up for whatever loss you would have made. Best case scenario, you get your money back plus a small profit. And so that's why it's called a p plus epsilon attack. And the only way to really combat this is to add forking to the system, where you kind of split the network into multiple universes. You can envision one universe said ten inches of rain fell, another universe said eleven inches fell. And so if you look at current approaches to the oracle problem, there's a bunch of them. They generally kind of fall in a few different buckets.
01:33:24.184 - 01:33:57.450, Speaker A: So there's things like Oracle lies and various variants of that. These are systems where you're generally trusting some m of n set of parties that kind of differ in various ways. Second one is something called TLSN. It's something that nobody actually uses, but it's interesting. So we'll go over it briefly. Trusted hardware is another category where you're basically running stuff on things like intel trusted hardware modules. The next is like shelling coin, but souped up a little bit.
01:33:57.450 - 01:34:33.372, Speaker A: Then you have something called Claros Auger uniswap. And then there's two others that we'll go over briefly at the end. One of them actually just published a paper like a week ago, which is pretty exciting, and we'll talk about that briefly as well. So subjectiveocracy is this idea. It's also published by Edmund Egger, who came up with the first kind of trusted third party oracle mechanism. And this idea is, you know, will it rain in San Francisco? And you have no explicit oracles. Instead, what happens is after the event, there's two universes.
01:34:33.372 - 01:35:06.860, Speaker A: Yes, it rained, and no, it didn't. And so you can envision, say, you had a betting market on this, where there's like yes and no outcome shares. Afterwards. People have yes shares and people have no shares. And after the event happens, his theory is people should just value the one that actually occurred more. So if it did rain, the idea is that people would be willing to pay roughly one ether or one die or whatever for the guest universe shares. The problem is it's a great idea in theory, but it would be a mess to deal with in practice.
01:35:06.860 - 01:36:13.384, Speaker A: Nobody wants to denominate future defi products in things like did it rain in San Francisco? And then you would have this long chain of events where you might have to check the history of did it rain in San Francisco? Did it rain in Houston? What was the closing price of Apple on last Tuesday? And you would have to do this to kind of figure out the chain of custody for your money. So it's kind of like a nice economist idea, but it doesn't really work in practice because nobody would actually do this in consensus terms. It lacks finality. The next one is TLS notary, which is an idea that sounds good, but it's not actually that good. So the idea is there's a system called TLS notary where in theory you can basically do like an SSL handshake with a website and form a proof that you got data from that site. The problem is that the person who created the proof can be malicious and they can provide an incorrect proof. And so it's not really very useful in practice.
01:36:13.384 - 01:37:10.132, Speaker A: Other problem with it is it uses TLS 1.1 or lower MD five and Shaw one, all of which have various pretty serious security issues with them. So it's another one of those things that might work for small amounts of money, but you probably shouldn't trust it to your millions of dollars. Tlsn, not to be confused with TLS notary is very similar, except it allows you to provide a proof to someone else and they can check it and verify it. And there's not really a way for you to maliciously modify the proof. So if a site is running a TLSN server, you can basically get an assurance that I can prove to you that the site gave me a certain result. The problem with this is it requires custom web server that basically nobody runs and that almost nobody's likely to run it.
01:37:10.132 - 01:37:54.236, Speaker A: And it only works for things with clear existing APIs that are kind of already on websites that are relatively constant and not changing. So this is the one that's cool idea, but nobody uses trusted hardware is the idea where you take something like Intel SGX and you have that prove that you ran some computation. In this case it's probably fetching a result from some website or doing something to get the oracle result. And then you submit that to a smart contract. Smart contract verifies it. Usually what people do is they have SGX basically create a private key within the trusted hardware module and then sign a message with that key. Then the smart contract can just verify the message signature.
01:37:54.236 - 01:38:42.890, Speaker A: It's a lot easier than verifying the SGX proof itself. It's kind of a clever hack. The issue with this is intel requires you to get a license or key that they sign with and they can turn it off at any point. So it kind of has the classic trusted third party problem. They can also turn it off for legal reasons if you read the terms of service, which is quite likely to happen in my view, for a lot of the things people are using DFI apps for, especially if they got any kind of sizable scale. If for instance, there was 100 million in derivatives volume on DyDx on a daily basis, and they started using Intel SGX for their oracle, I think within a year intel's lawyers would stop that. The next approach is basically a souped up version of shell and coin.
01:38:42.890 - 01:39:09.666, Speaker A: So it's basically kind of a beauty contest approach. This comes from John Maynard Keynes. He had this concept called a beauty contest. It's like an actual economics term. What it means is it's very simple. You're basically just picking what you think other people will pick. And so the example is, will it rain in San Francisco? You have people stake some collateral on yes and no, and finality is determined by whichever side has more capital staked.
01:39:09.666 - 01:40:15.470, Speaker A: In most of these systems, there's no or limited ability to challenge things afterwards. There's not forking, so there's not really a penalty to being dishonest, assuming you can kind of bulldoze or steamroll your way through getting some sort of outcome. And so the P plus Epsilon attack vulnerability also exists here due to no forking. Claros is a project, I'm not sure the current status of it, but they're using sort of beauty contest approach with specialization and trying to figure out if there's ways to create oracles that are subjective. What Auger tries to do is fix problems with the beauty contest approach by basically having an economic bond called rep, where you're posting that as collateral, and traders and market creators in anchor are basically paying for insurance that the markets will resolve correctly. And if you're a reporter, the idea is you get penalized for reporting inaccurately. And so if you report honestly, you get trading fees and any rep from dishonest or inaccurate reporters.
01:40:15.470 - 01:41:17.780, Speaker A: If you don't report, you don't get any fees. One of the main innovations of Auger is it doesn't require splitting security into end slices like truthcoin, and the way how is it switches from a model of reporting on everything to disputing. So you can envision a question like will it rain in San Francisco? You have a first report where it could just be a centralized person just throws a report on the market and says yes or no, and then there's n number of rounds of disputing where people successively stake higher and higher amounts of collateral on either side. Eventually, if it disputes a bunch and a large amount of the rep supply is locked up in a specific market, then the network can actually fork into different universes. So you can envision that there's a universe. If the market became very contentious over rain, you can envision a universe where, yes, it rained, another universe where no, it didn't. That's just a diagram, basically, of what I just said.
01:41:17.780 - 01:41:57.610, Speaker A: And so if you look at Olgar, the reason it has a separate token is the main reason is forking. That's kind of the only technical reason why it needs one is if the network forks into multiple universes. You need the ability to basically have supply exist in multiple universes. And so you can't get that assurance from forking, say, ethereum. You could, if you can convince the entire Ethereum network to fork over your dap. I think that's unlikely these days. And so the way the kind of incentives work out is if you're dishonest, the forks that you voted for should have a lower value kind of going forward in the marketplace.
01:41:57.610 - 01:42:44.350, Speaker A: The idea being that people are not going to want to use a system that doesn't reflect reality. So markets in that system would be created less frequently and the market should kind of value that set of reputation lower than a system that actually reflects reality. The other piece is with forking. It kind of forces the whole network to participate, so you don't just have a tiny subset of the network. In a system like auger, everyone's required to participate in a fork. If you don't, you actually lose your rev collateral. And so if you think about how to reason about security of oracles, the simplest kind of naive model is to assume that the value of things backed by an oracle, the value of the assets that you're staking, should be worth more than the assets you're securing.
01:42:44.350 - 01:43:18.654, Speaker A: So if you have a million dollars staked to secure some network, there should be less than that in collateral that you're securing. In practice, there's a few tax vectors that mean that it should actually be a few times higher. That's kind of a longer conversation. The auger white paper goes into details why, and there's kind of a derivation for why that's the case. But the punchline is you need to compensate oracles appropriately such that the stake is worth enough to make it worth actually using and participating. Otherwise the security model falls apart. And I would say today in the space, almost nobody's actually doing this.
01:43:18.654 - 01:44:13.054, Speaker A: Most people are just kind of hand waving the problem and having one oracle that's centralized or an MFN multisig secure their entire network. I think at some point there's going to be some attacks there where that starts to fall apart. One big open question in the oracle space is parasitism. So the idea is if you have an oracle result, say it's for the rain market, what prevents somebody else from just using that result, embedding a bunch of money on like a side market and not paying the oracle. And that's a really big tax factor that nobody really has a good solution to. There's some incentive not to be too parasitic. You don't want to kill the host in the sense that you don't want to attack a system so much that the result actually ends up being incorrect because you're basically going to lose capital yourself.
01:44:13.054 - 01:45:04.590, Speaker A: But it's still a pretty major problem that nobody's really quite cracked. But I would say the first step for oracles is right now most systems aren't secure against even known collateral. So meaning if there is $100 million in compound that oracle is probably not secure for that number. And so the first step would be making it secure for that. Maybe later on people can try to figure out ways to solve the parasite problem. And so if you look at the kind of future of oracles, the first step is to at Auger, we're actually going to publish a paper pretty soon that formalizes our oracle framework into a more generalized framework that can be used for more than just prediction markets. Another area of research is research on how do you create oracles on subjective things, things where there's not a clear right answer.
01:45:04.590 - 01:45:37.334, Speaker A: I'm not sure if that's actually possible, but it's an interesting area to look into. And then the last two are kind of new oracle systems. One is uniswap style price feeds. So this is where you take like Uniswap trading history and use that to create some sort of a price feed. Now, that's not really useful for short term things. If you're trying to get the price of an asset over an hour, I wouldn't trust Uniswap for it. But if you're trying to get the price of an asset maybe over a week or maybe even 24 hours, it's more feasible.
01:45:37.334 - 01:46:09.130, Speaker A: I think. I think the better way to do that is using a TWAP instead of a VWAP. So, time weighted average price, meaning if the price is sitting at $10 for some asset, and it sits at that for 6 hours, it's probably a pretty good sign that the asset is actually worth that. Otherwise somebody would have arbitraged it. But if you're doing it based on volume, that's a number that's very easy to manipulate. And then the second one is this thing that came out about a week ago. It's called deco.
01:46:09.130 - 01:46:58.626, Speaker A: It's a decentralized oracle, similar to sort of the TLS notary family, but it solves a lot of the main problems with them where you don't have to trust the proof author and you also don't have to run specialized software on the server. So it should work with basically any website, and you can prove to somebody that you got a result from that site and that you didn't modify it or tamper with it. And that's a paper that just came out a week ago. You can find it on archive.org. And I think that's pretty cool. It uses some sort of zero knowledge proofs to do that. Then the last is, if you look at kind of like starquare in particular, they could use their tech to create decentralized oracles for in particular on chain data.
01:46:58.626 - 01:47:34.030, Speaker A: So if you know, taking a bunch of trade data from a decentralized exchange and using that to create a proof that the VWAP or the TWAP price over a certain time window was x. And creating that very succinctly, I think is an interesting use case for oracles as well. And that's it. If there's any questions, feel free to ask. So, yeah, if there's any questions, just go over to that mic and ask there. Hi. You mentioned on the last slide about subjectivity versus objectivity.
01:47:34.030 - 01:48:43.154, Speaker A: We've seen with Auger that there does exist a gray area kind of between those where the answer is objective, but based on the nuance of the language of how the question was asked, that can create a misunderstanding. I'm curious to hear what aspects or updates of the auger platform that you feel will address that or improve the clarity. Or if not, do you think it's sufficient just with the current process and that this is something that the market will naturally evolve on its own? Yeah, it's a good question. I think it's something that people need a little guidance in terms of how to actually solve that problem. I think one way to start to address it is to add things like multiple sources for a market. So if you have a market on, say, something like a sporting match outcome, sport event outcome, it would make sense to include maybe three sources for that and take the mode of them or something like that. That's like one simple approach towards solving that problem for context.
01:48:43.154 - 01:49:37.654, Speaker A: The problem is people might make a market on, say, a soccer match and say that the result should be available on ESPN at midnight, and then ESPN doesn't post the result until 03:00 a.m. What do you do? And that's the sort of problem where if you're building a system that's relatively deterministic, you can't really just say, well, we can just wait till 03:00 a.m. Because then people who staked on something at midnight are going to lose their capital even though they reported what they actually thought the accurate result was. And so I think the answer is kind of redundancy. It also adding more clarification and wording to markets and questions to make them more specific. And then the last one is in V two of auger. From the trader perspective, people will be able to basically bet on whether they think a market is invalid ahead of time.
01:49:37.654 - 01:50:03.680, Speaker A: So from the user standpoint, you can effectively kind of by default hide markets where there's a high probability of it being invalid ahead of time. Hi. I really like your talk. Thank you. I have a couple of questions. First of all, you talked about TLS and twice at the beginning you talked about TLS, and then in the end you said something about TLS and snarks. Right.
01:50:03.680 - 01:51:07.090, Speaker A: So what is the problem with TLS? For example, if I can settle a market with a SSL signed message from CNN.com. So what would be the vulnerabilities with this kind of settlement? And can you explain? I just missed the second thing. You talked about TLS with snarks. And the last question is about SGax. So if I have SGX on the processor, where's the inputs going to come from? What do you mean by SGX? The code runs on SGX, but if I'm collecting them out of rain that went on some certain day, then I need some external sensor, right? Yeah. So I guess the first one is the SGX question. And the way that works is basically you basically write code that fetches a result from a website and you run that all in the trusted compute module.
01:51:07.090 - 01:51:54.660, Speaker A: And then within SGX you basically sign a message that you then broadcast on chain and so on chain, you can verify that according to SGX, it did an HTP request to some website, got some result a and that's what you have on chain. Then you could just send the original message. Why do you need the SGX component of it? Because if you just send the message from the site, there's no way to verify that you actually got it from that website where the SSL certificate. Yeah. So this gets to your first question about SSL. And so the reason that's the case is the way the SSL handshake works. You can't just by default out of the box, prove to somebody that you got a piece of data from a website.
01:51:54.660 - 01:52:43.330, Speaker A: There's basically a few vulnerabilities there that make it actually difficult to do that. It's really easy to fake a piece of data when you're doing that. And so the solution to that is, the final solution I mentioned to that is this thing called deco, where you're basically creating a zero knowledge proof that you got the data from a website. So you can basically prove that on your machine. You used an HTPS request, did an SSL handshake, got the result from the site, didn't modify it, and then you broadcast that thing on chain and you can prove it. That's kind of how you would actually do it in practice. But the punchline is you can't just take a regular SSL handshake and send that on chain.
01:52:43.330 - 01:53:58.838, Speaker A: Okay, thanks. Hey, thanks for the talk. As formulated in this presentation, it sounds like the Oracle problem is mostly reasoning about abstract economic costs of lying, independent of any sort of physical connection between what's happening on the chain and what the data the oracle is reasoning about. But it seems like ultimately there are also physical security assumptions. So even if you have some sort of tokenized Oracle, people hold those tokens with keys. And I'm wondering if you know of any approaches which are actually just trying to create a direct physical connection by, for example, having in the San Francisco rain question lots of tiny, tiny raspberry pis scattered around San Francisco with individual keys which are registered somewhere, constructed such that although it might, independent of questions of whether you could ever pay for that, it would be very hard, with little notice, to go to all the raspberry pis, take them over, and control the answer to the question, does that make sense? Yeah, that makes sense. I haven't heard of anybody doing that, mostly because the decentralized application space is in such an experimentation phase.
01:53:58.838 - 01:54:58.830, Speaker A: There's not like anything where people are regularly speculating large amounts of capital with a specific oracle result that's not in the virtual world. Like, the only examples today in the DeFi space where you might want to have a recurring oracle feed would be things like what's the price of ether? Or something like that. Nothing's taken off yet where you could get the result in the physical world like that. I think if there were, for instance, billions traded on weather futures on top of Ethereum, maybe that's something people would start to look into. And a second related question, since no one else has come up yet, are there any attempts to enforce, although it seems like a very hard problem, but a sort of logical structure or consistency between answers in oracle. So reasoning not only about reputations of particular reporters or whatever, on questions treated in the abstract, but on questions treated in the concrete. Like, if you said yes to this logical implication, you must also accept this one, or it's inconsistent.
01:54:58.830 - 01:56:13.526, Speaker A: People have done stuff like that in the way that they've phrased a question to an oracle. Nobody's created an oracle system that has that baked in at the protocol layer, though, in general, if you look at Auger in particular, the way people create questions is they actually specify the possible outcomes ahead of time, which can run into error if you don't think of all of them ahead of time. But it's a way to force a sort of logical consistency. But there are some where people have created logic questions like the ones you mentioned, and phrased that as part of the question to the Oracle network. Thanks thanks for your talk. I want to ask regarding the subjective approach and the usage of the ability to fork as part of the incentive model. Isn't there a risk that the more use you have of the oracle, more application using it, and then other applications relying on that, applications that eventually the cost of forking relies on other factors, not necessarily the correct answer of the oracle? Meaning like it's too expensive to fork or.
01:56:13.526 - 01:57:18.620, Speaker A: What do you mean exactly? Or there are now other factors, factors related to the applications that use the oracle, which may be okay with a wrong answer because the cost of forking for them will be much higher. Oh, yeah, I see what you're saying. So the way the kind of incentive game is structured is it's less about the application that's using it and more about whether you've staked some valuable asset on one of the outcomes. And so if you look at Auger in particular, there's actually empirical data for this. The most disputed market on Auger had very little trading volume and some of the most traded markets have almost no disputes. And so there's not like a huge correlation between what demand there is from an application and what people will take seriously from an oracle standpoint. And I think the main reason for that is if you look at Auger, the way it's structured due to these dispute games, it's more about what you as a reporter think the right answer is than what specific application is doing with that answer.
01:57:18.620 - 01:57:32.842, Speaker A: Thanks. Cool. So thank you so much for your talk. Yeah, let's give a hand. All right, so it's lunchtime now. I don't know if you guys are aware of that. I'm a little bit hungry.
01:57:32.842 - 01:58:04.220, Speaker A: We come back here to this room in I believe 1 hour. So I'll see you there. How are you feeling? Are you excited for the afternoon? Good. There's an amazing program. We're going to start with everyone in this room. I'd like to invite Avihu Levy, who's head of product for Starkware, to the stage to talk about Stark exchange. Hello, everyone.
01:58:04.220 - 01:58:42.142, Speaker A: So I'm going to start with a bold statement on how we are in stark. We are building the best crypto exchange in the world. And even a bolder statement how we build the best exchange in the world, period. So I'll start with the current state in crypto. So when you want to trade crypto today, you can choose between risk your money in a centralized exchange or have a poor trading experience in a decentralized one. So as most of us probably know, centralized exchange, they offer scale. They're inexpensive to trade, but they are custodial.
01:58:42.142 - 01:59:40.890, Speaker A: That means that you can choose the custody service that you are using. You have to handle your coins to the exchange. On decentralized exchange, on the other hand, you get a self custody trading, but the access are not scalable and they are expensive to trade for the user. Therefore, we have stock exchange, and with stock exchange, you can enjoy the best of both worlds. So a stark powered exchange is scalable, inexpensive, and self custodial. So that means that the user can enjoy all the benefits of a centralized exchange, but without handling the money to the exchange, he can actually choose whatever custodial service he wants to or even trades from his own wallet. Okay, I'll give a very brief explanation on how the basic flow stack exchange currently works.
01:59:40.890 - 02:00:22.470, Speaker A: If you want to get more details on how it works from the technical side, we have various blog posts and presentation presenting it that are available. So what's the basic flow for Stark Exchange? So, let's start with Alice. And Alice has her own chain wallet on Ethereum, and now she want to join the Stark Exchange micro universe. So she start with on ramp her funds from her on chain wallet to her off chain one. Once she did that, she can trade with everyone else that already have an off chain wallet. So for example, she can trade with Bob. This trade will be joined to many other trades, and together they form a batch of trades.
02:00:22.470 - 02:01:13.410, Speaker A: For every batch there will be later approve, generated, and be submitted on chain to be verified by our onchain verifier. So, basically, step number three and four is where the magic happens. And we use proofs to basically make the verification of a trade much cheaper or logarithmically cheaper than what it would be otherwise. Okay, so now, in the current system, as we describe it, Ellis can trade with anyone who has an off chain wallet. So, for example, if Bob has a wallet, she can do many trades with him and enjoy all the benefits of stark exchange. So she can enjoy a scalable, inexpensive system. She can do high throughput trades, a very good trading experience from her wallet.
02:01:13.410 - 02:02:01.140, Speaker A: What's not? But what happened if Alice wants to, for example, interact with Carol, and Carol doesn't have an off chain wallet? Well, the simplest case is if Alice has enough funds, say that she want to pay carol for an ice cream, and that ice cream cost 100 die. It's an expensive ice cream. So then she can simply pay from her on chain wallet. But what happened if Alice doesn't have 100 die in an onchain wallet, just an off chain one. Well, in this case, Alice will have to withdraw her funds from her off chain wallet to the onchain one. And in our system, currently, withdrawal mean that you have to wait a time that we call the proof time. Because since you have to wait for a proof to withdraw, you have to wait a couple of minutes before you see your funds from the off chain wallet on the on chain one.
02:02:01.140 - 02:02:37.090, Speaker A: And this is a barrier. This is what we call the proof time barrier. And this is something that we would like to break to create a better experience for the user and the ability to interact from each off chain wallet. Now, as some of you may notice, the tiny figure there in the center. So it's a turtle, which is also a slow turtle. And here we have a fast turtle. This is the first time that we present on stage fast withdrawals, and we use fast withdrawals in order to break the proof time barrier.
02:02:37.090 - 02:03:26.046, Speaker A: So, very simple, basic, short explanation on how fast withdrawal works, without omitting some of the details, say that Alice wants to withdraw those 100. I really fast. So she start with a transaction, a conditional transaction that she sent to the operator. She signed this transaction that gives the operator those 100 die off chain under some condition. When the operator gets that, the first thing that she does, he verifies that Alice actually has those funds on her off chain wallet. And if she does, he transfers those 100 die on chain from his own on chain wallet to hers. And then because he did that, he can later execute the off chain transaction and get those 100 die back off chain.
02:03:26.046 - 02:04:13.886, Speaker A: So, basically, now Alice, with fast withdrawals, can expect getting money from an off chain wallet to their on chain one within a time that equal or almost equal to what we call a blockchain time. A single on chain transaction and fast withdrawals already make the situation much better, but we can do even better than that. So, for example, if Alice wants to pay Carol for the ice cream, she doesn't necessarily have to go through the process of off chain wallet to her on chain wallet and then pay Carol on chain. She can actually execute a transaction directly. With a similar method, she can execute any transaction directly from her off chain wallet to Alice on chain one. And now think of Alice, of Carol. Sorry.
02:04:13.886 - 02:05:07.470, Speaker A: Not just as an ice cream seller, but actually can be, for example, uniswap or kyber or any on chain matching engine. So, actually, you get a benefit that you can, within a single transaction, interact from an off chain wallet that can trade with everything in stock exchange to an on chain one that can actually interact with on chain contracts. Now, the next thing is a little bit more surprising. We can actually do one step farther. So say that Dave works in bitcoin coffee in Prague, and he only accepts bitcoin. And Alice comes to the bitcoin coffee, but she has her off chain wallet, right? She has ether there. So within a similar mechanism, we can allow Alice to actually pay from an off chain wallet with whatever currency available to her, from the operator in a trustless way, to dave on the bitcoin blockchain.
02:05:07.470 - 02:06:13.000, Speaker A: So the whole user experience now of an off chain wallet becomes much, much better. It basically gives the user a seamless transaction experience from his off chain wallet, and therefore he can execute almost everything that would be possible from the onchain one through his off chain wallet. Okay, so I'm going to now start with a recap of what we've seen so far. So we started with stark exchange as an engine for exchanges that can provide the scalability, inexpensive trades and self custody for every exchange. Will it be a centralized or decentralized one? Now, we start with trading as the only feature that we provide there. The next thing that we added now is seamless transaction experience. So from now on, you not only enjoy the scale of stark exchange, you also enjoy the fact that your off chain wallet can basically interact almost the same as an on chain one.
02:06:13.000 - 02:07:02.930, Speaker A: So you get that, and then we can build more abilities on top of that. So the easiest and simplest one then one would expect that if we have trades actually to go to payments is not farther down, it's not too far down the road, and we can actually implement that. And this is something that I won't go into in this presentation. We cannot only do payments as in slow payments. We can actually do instant payment at the same speed that you know, from lighting, for example, and still would be trustless ones. So we have payments and we have instant payments, and then we can add on top of that all different kind, whatever you can think of that is financial tools. For example, margin trading today in many exchanges is a hot potato.
02:07:02.930 - 02:07:51.862, Speaker A: So we can add that as well. Now, if you stop here and you take a look on what we have so far. So what we have here is, I would say the state of the art of crypto exchange. And actually we get to the point where we are in pair with traditional exchanges, because the same as in traditional exchanges, you can choose your custodial service, you can do it. Also here you get scale. It's not expensive to trade. The wallet experience is pretty good and you have different financial tools, but we can actually go one step farther right, because well, we can do, as some of you may if you happen to be on the application track and you heard some of the talks there.
02:07:51.862 - 02:08:54.634, Speaker A: So actually using starks, we can also do front running prevention. Now I won't go too much into details of how this work, but just to point out that what I've been talking so far comes from proving on stuff on the settlement layer, but we can actually go also into the matching layer and for example do fair matching. And another important topic that we can definitely implement is regulated privacy. Meaning that the traders get to keep their private balance, private trade history even, or orders like whatever is possible by the regulator. And still the regulator can run over all this information. We can give him access to check whatever functionality he wants without the need to get the data that he doesn't need for that from the trader itself. So those two abilities, front running prevention and regulated privacy, are ones that are very desirable in the traditional exchanges, not just the crypto ones, but they are not currently existing, a form that you can actually trust.
02:08:54.634 - 02:09:36.454, Speaker A: And here we offer a form to solve them on top of the existing infrastructure. And therefore you make product because we solve it in a trusted way, you make a product that is actually better than all the existing ones. So with doing that, we not only provide or build the best crypto exchange, but actually the best exchange in the world. Thank you. Should we do any questions? I don't know if there are any questions here. Oh, there are. Okay, why don't you grab the mic over here if there's any others.
02:09:36.454 - 02:10:41.674, Speaker A: There's also a microphone over there. Can you talk more about the front running resistance and the regulated privacy? Yes, I'll give it like in a very high level. So before some people presented different method how to go with front running, but the really high level, I'm going to simplify some things. On front running you can think of the way that we think currently to go is that, well, if we let people publish orders, let's say they publish them on chain for the moment, and those orders are such that they have a time lock, so the operators know that at some point in the future he will be able to know what's inside. But when you post them, nobody knows. So their only priority matters might be fee ones, but everybody can just post those time locked puzles. That includes their order on chain.
02:10:41.674 - 02:11:22.082, Speaker A: And later on the operator knows after some time, let's say after 1 minute the operator can open all those orders and then execute them. So you get a late execution, but almost without hurting the speed of submitting orders. And this execution will be fair. And one way to use starks here is to, instead of showing every execution on chain, you can just commit to them anywhere that you would like to, and just prove that the way that you committed to the orders is also the way that you executed them with Starks. Thank you. Very cool. Thank you so much for the talk, Avihu.
02:11:22.082 - 02:11:59.698, Speaker A: Thank you. Next up, we have a panel all about the future of trading. So if I can ask the panelists to come up and take your seat. This panel will be moderated by Balaji Srivasad. I'm trying. Thank you. All right, so going to talk today about the future of trading.
02:11:59.698 - 02:12:27.646, Speaker A: My name is Balaji Srinivasan at Balajs on Twitter. I know seemingly like half the people in this room, which is great. Formerly CTO of Coinbase and GP at a 16 z, and I want to let the folks here introduce themselves. So you want to go first? Thank you. So, I'm Florian from fundament. Fundament has made quite the news recently because we've launched the first fully unrestricted security token on the public Ethereum network. It represents a real estate bond.
02:12:27.646 - 02:13:02.170, Speaker A: We're issuing 250,000,000 of them, and it's based on the ERC 20 standard. It's a vanilla ERC 20 token contract. And we figured out a recipe where we get full approval to do such an issuance by the german financial Market Authority, enabling us to sell this to any investor worldwide with a minimum investment of 1. And we plan to apply this recipe to many more assets like private equity, VC shipping, and all things. Always based on a fund structure. That's what we do. Fundament.
02:13:02.170 - 02:13:48.966, Speaker A: Thank you, Will. So, I'm Will. I'm the CEO of diversify, which is a decentralized exchange. Formerly I was working at Bitfinex and then launched Ethfinex as a hybrid, centralized, decentralized exchange. And we announced about a month ago that we decided it was time to close down the centralized side of ethnex and spin out and focus only on decentralized exchanges. Given where we see the position of the technology now being and what's going to come in the next year or two years, and diverse by now will be focusing on figuring out, reducing cost and scaling decentralized exchange, starting from the existing platform that we're operating and then launching also several new platforms. And Bartek.
02:13:48.966 - 02:15:05.330, Speaker A: Yeah, and I'm Bartekushevsky from Makerdao. I'm a product owner for the Oasis Dex trading protocol. I'd say it's a decentralized exchange that today I'd say that it's by far the biggest liquidity pool on Ifdai market, which powers defi. And iftodai, it's the major front end that's actually using Oesis Dex protocol. And I would consider Oisis Dex as part of a larger makerdao ecosystem alongside things like oracles and keeper network and a couple of other components which sort of are actually necessary for the core protocol to run properly. Awesome. All right, so we're going to go through just a few questions here, so why don't we just go kind of in order? So, Florian, where do you see crypto exchanges in one year and five years? What's your vision for the near and medium term future? So I think crypto exchanges will in general become more and more commonplace for a more and more mainstream audience.
02:15:05.330 - 02:16:26.730, Speaker A: Since we issue security token, it's kind of a new crypto asset, and there are currently no regulated, centralized exchanges for these assets, so effectively, they cannot be traded. So we are actually building on decentralized exchanges to fill this gap until there is a regulated venue. And the hope is really that in this time frame, which could be up to a year, decentralized exchanges will get so good that maybe we won't even need the really centralized versions of it anymore. So I would say we really see that. I think decentralized exchanges are starting, have proved the concept. It's now clear that we will have better exchanges coming out of the technology that's being built here, and actually not just for cryptocurrencies, but for settlement of all sorts of assets and the kind of developments that are happening here. And I think, as Avahu was saying, will build better exchanges and better settlement layers eventually for trading traditional assets, I think in the short term, let's say the next year, the reality is that except for niche types of arbitrage, I think most decentralized exchanges aren't going to become mainstream.
02:16:26.730 - 02:17:29.422, Speaker A: They're not able to offer the experience that you'd need to ever compete with centralized cryptocurrencies exchanges, and certainly not outside of this industry. But that will change very fast because it's now essentially a cost problem rather than a proof of technology problem. And that will mean that over the next five years, I think we will see fundamentally decentralized exchanges start to take over. And the reason for that being that they offer, or they can offer open access in a way that existing alternatives can't. So because you don't need to trust everyone who's in your network, you can have a much bigger network of participants directly accessing into an exchange and therefore ultimately better liquidity, network effect and lower costs. And so I think that will be a very long, at least a three or four year journey, but that's where we consider it will go. I have to admit that in my opinion, we as a community are not that great at making predictions.
02:17:29.422 - 02:18:25.794, Speaker A: I mean, we failed so many times before. But having said that, I can only say what I do hope for, and I do hope that every single exchange that we will be using in few years time will be non custodial. I think this is like crucial. It's beyond me why people are still using custodial exchanges, given so many of know, just continue to vanish in the Finnaire. I spent most of my time in Poland, and the polish crypto community has been shocked by the sudden disappearance of one of the biggest centralized crypto exchange in Poland with the longest history. And the owner of that exchange was actually found dead in the forest. And frankly, I have very bad feelings for the reaction of the polish regulator because of that.
02:18:25.794 - 02:19:39.420, Speaker A: I mean, they have no idea what's going on in crypto, but they would sort of try to put everyone into the same basket. Right. And we as a community should be smarter. So that's one thing. And then again, I think trading a lot of different use cases, and I think that there will be a lot of different architectures and there'll be no one winner. And this is essentially what we've witnessed with oasis, given this cambrian explosion, not of just zero knowledge proofs, but also on the Defi. In the last few months, we've been kind of pleasantly surprised that this explosion actually brought a huge volume to our exchange, in spite the fact that it's fully on chain, it's slow, it's been criticized by so many, and yet it's still somehow a preferred choice for people who just want the features that you get when you trade on.
02:19:39.420 - 02:20:08.254, Speaker A: Clearly, this is not about scalability or privacy. I mean, this is really for DeFi protocols that require composability. Right. So if you do margin trading using compound or whatnot, or DydX, this is what they use today. Well, let me actually. So you guys brought up a few points, maybe I can push on this a little bit. So a while ago, one of the theses on decentralized exchanges was that they were going to offer some form of regulatory arbitrage.
02:20:08.254 - 02:21:03.506, Speaker A: You were going to be able to list everything that you can list on a centralized exchange and, you know, just trade whatever and state couldn't do anything or whatever. And that really is kind of played out, but really hasn't fully played out because the liquidity isn't there. And so a new thesis, which, you know, two of you talk about, and Florian, you may think about as well, is that the advantage is noncustodial. And one kind of macro thesis I have on the whole space is come for the money, stay for the freedom. So people come for the upside of crypto, and then they stay because the philosophy or the programmability or something like that draws them in and keeps them. And so with respect to that, with noncustodial trading on a dex, you could keep your funds locally and then arbitrage multiple exchanges, and that actually gives you money, and then you keep it noncustodially because it actually makes you more money. But it's also something that, long term keeps your coins safe.
02:21:03.506 - 02:21:48.754, Speaker A: So that's, like, one way that it might get adopted. What do you think about something like that? I think that's absolutely the case now, certainly. So, for example, talking about Eth, two die. The reason, I think, why a lot of people would use that is not because it's decentralized at the moment. It's because if you are generating Dai from using makerdao, it's faster to trade it to Eth using that, much faster than deposits into an exchange, waiting a centralized exchange, waiting 20 minutes for it to be credited, making your trade, then withdrawing back to your wallet. And so there's already a speed advantage there, which translates effectively to a cost advantage as well, if you're worried about losing an arbitrage opportunity. And so, ultimately, I think we have to consider that, particularly in this design space.
02:21:48.754 - 02:22:49.270, Speaker A: But I think, obviously, in a lot of applications, traders are one of the most economically rational entities as close to it as you can get. And so they do look at the ultimately at things in cost sense. And so, yes, using a centralized cryptocurrency exchange, maybe even if there's a 5% chance that it's going to get hacked and lose funds or that it could be facing other sort of regulatory pressures, that currently could be a higher cost than doing everything on chain and paying lots of fees and losing opportunities, which you'd otherwise get from using a faster off chain exchange. But in the case of arbitrage, at the moment, already we're seeing the speed advantage being the primary driver. So price arbitrage rather than regulatory arbitrage, which is interesting. Go ahead, Florian. Yeah, I want to jump in here because I think the reason this regulatory arbitrage didn't play out for lack of liquidity was because there were good enough alternatives, or better alternatives, actually.
02:22:49.270 - 02:24:00.558, Speaker A: And when it comes to security token, which have been talked about for years, but never really were there for a number of reasons, but which are now starting to be there, there is actually this opportunity for the arbitrage and a lack of better alternatives because there are no regulated, centralized trading venues for these types of crypto assets. And you actually need decentralized exchanges right now to get any liquidity at all for these token. And a week ago, we've installed our token smart contract on the Ethereum blockchain. It's now there, we've started issuing some of these tokens, and people are starting to do secondary market transactions. And there's no other way than a Dex to do this. And indeed, if you want to do this arbitrage, in terms of the regulatory arbitrage, then the question is, what is a dex? Right? When is it decentralized enough to actually fall into this loophole that in Europe is really the loophole of a bulletin board, where if you're not running a centralized order book, you're essentially not an MTF, and thereby unregulated. And I think there are dexs that call themselves dexes that still have some sort of a centralized order book, and then there are dexs that are actually truly decentralized, like the Dutch X, for example.
02:24:00.558 - 02:25:15.074, Speaker A: And I think at least these types of venues will see a big popularity among security token holders that want liquidity. Well, yeah, that's, you know, Florian, one thing we were talking about off stage is sort of like an algebra of regulation. If you take C and D and you combine them together, your regulatory burden is basically the maximum of ABC and D often. So if you take a highly regulated security token, for example, and then you put it on an unregulated dex, the state still has a hook in you from a right. So how do you think about that? It's certainly true, and I think we will have to see this in the wild, how it plays out, which jurisdiction claims some form of right to intervene in this. In our case, we're regulated in Germany and in a wider context in Europe, and there are very clear rules around what is a trading venue. When do you have to be regulated? What is a security token? What are the KYC AML requirements? And we believe that if our token were to be traded on a dex in Europe that falls under this exception of the bulletin board put up by ESmA, then I think no state, at least in Europe, would intervene if then american token holders would trade on these venues.
02:25:15.074 - 02:25:45.680, Speaker A: Obviously it's a different story and we just don't know what the SEC would do. Interesting Bartek. I think you all folks should pay a lot of attention to what Florian and his colleagues has done, because I think it's kind of revolutionary in terms of the topic that we're discussing. I frankly was very skeptical. I didn't know it was even possible. But right now it seems like the token is in the wild. I think you have one.
02:25:45.680 - 02:26:56.818, Speaker A: Well, you shouldn't have said that. But now everyone knows that I have one. And now we come back to the question, what does it even mean to trade this token? I can send this token to any one of you and you will have it. You will be tainted with the security token, right? And you wouldn't even know that it's a security token just by looking at it, because it's just a pure, plain vanilla ERC 20 open Zeppelin library created token not only can be traded at Uniswap or easy stacks, any other exchange that essentially is permissionless, but it can be easily transferred among anyone, really, by ethereum network. Right? So how do you stop that? That's my question to anyone who thinks that this should be regulated. I mean, should we regulate token transfers on Ethereum? Should we regulate ether transfers on ethereum? Well, shift in like the referee grabbing a chair and jumping into the middle of WWF. So my thesis on this, for what it's worth, is that I'm rooting for you guys on security tokens.
02:26:56.818 - 02:28:19.854, Speaker A: I'm actually kind of bearish on it because redemption of that token for the actual underlying security is like a little bit of a hot potato you send to somebody. Can they actually redeem it? An alternative is actually to take what you guys have done. Bartech at Makerdao, you've piped in the Usdeth price, and from that you've been able to basically get Dai like a stablecoin. In theory, you could pipe in not the USD ETH price, but let's say the Google ETH price or the Tesla EtH price, and essentially set up like a CFD, a contract for difference like thing where you could get the benefit of any publicly traded stock without actually having the redemption issues associated with the security token, right? And I think that's like a more organic, true to crypto way, because it's hard to ban a price signal. Now, I mentioned CFD, CFDs are highly regulated in the US, but not everywhere, right? And so it feels like an approach like that, that kind of takes away the redemption risk in a clever way might be feasible. What do you guys think about that? Yeah, what you've just described, I think these are the things that we will see in the very near future once our system is more mature. I mean, in few months we're going to launch multicolatal die and we'll be hoping that the community will come out and sort of provide us with different tokens that will be used actually as a collateral for Dai.
02:28:19.854 - 02:29:15.934, Speaker A: So that's, that's, that's something that we will see next year for sure. And I do hope that there will be another cambrian explosion of different token schemes that will actually play out like you've just described. But just one extra remark, I think what you've said about billism board versus a trading venue that does the order book matching, that's kind of a divide that in my view, is kind of flawed because look at Oasis Dex. It's got order book, it's got matching engine. And yet I would argue that it's a protocol because it does not have any operator. Right. And I think that divide that Martin showed on one of the presentations, the exchanges that require an operator versus the exchanges that don't require an operator, that's a huge divide.
02:29:15.934 - 02:29:57.010, Speaker A: And that's a divide that will play out to have an important role in future discussions, because the regulators will eventually come to realize that if there's no operator, then the only thing that they can do is to sort of, I don't know, put developers to jail. And I don't think that's going to happen. Well, it's interesting you say that, because I think, I tweeted this while back, that pseudonymity is as important as decentralization. Because decentralization, you can't get the network. But if the founder is not pseudonymous, well, they kind of know who it is. Even if Facebook had released Libra, like totally open source whatever, well, they know they're in Menlo park, they know where to reach them and so on. So it may be that the higher the risk you're taking, you might want to do something pseudonymous.
02:29:57.010 - 02:31:05.666, Speaker A: Let me switch topics here. So what are the major innovations in trading that you've seen, and what things do you think are important? Will, let me bring you in since you haven't. Yeah, so I think probably the biggest thing that's happened in the last year or so, and I think which will continue to grow, is the kind of movement around leverage in the defi space. And that's something which is required for large scale speculation in the sense that it will empower growth of liquidity, but also prices being driven through decentralized exchange networks, rather than just following in the sense that I think what you're describing earlier with synthetics that would work for large shares like Google, where you're following the price, but if it's a new token that's issuing a new security for the first time, there may not be a clear price signal. And so you do need a different form of trading to happen. So I think that's the first thing. But then secondly, different scaling techniques to reduce the cost of settlement.
02:31:05.666 - 02:32:00.810, Speaker A: I think that will be the next stage. Those two things combined, I think will mean that this can really start to compete. Excellent. Florian, what do you think? Obviously my perspective is biased from what we do, but for me the biggest innovation is how for example, the Dutch X but also maker come up with exchanges that are really protocols without an operator, because this is where this divide is in the regulation. And so I think this is super innovative, at least for people that are interested in the security token. From my perspective, we going to follow both routes. To be clear, we think that first space for a protocol which is fully decentralized and very space for an operator run exchange, and obviously for the second case, ZK rollups and zero knowledge proofs providing scalability and privacy are crucial.
02:32:00.810 - 02:33:35.590, Speaker A: And I mean, the architecture that you have just seen before this panel, I'm very bullish on this. Having said that, I do hope that we're going to find a sweet spot between the privacy and actually transparency of the actual volumes traded, so that everyone can be assured that we're talking real volumes and not wash trades or some tokens that no one's ever heard of, which is we know that this is a problem with crypto trading, obviously, but also decentralized exchanges and our community here, we should be more vigilant. I'd like to really understand, for example, IDEX volumes. And I'd like to talk to someone who can provide me with the real data from IDEX, because I'm seriously, personally fed up seeing IDEX as the number one exchange. Yeah, it's funny you say that, because basically the value of being ranked as a top end coin is much higher often than the fees that you would pay to wash, trade and fake it or what have you. And I often think of coinmarketcap as sort of being like with no offense to coinmarketcap, it's a great thing and so on, but sort of like the Alta Vista digital of coin rankings, right where Alto Vista Digital, for the folks who are too young here to remember, that was a search engine that was subject to keyword stuffing. And because of that, any individual page could get up there, and then you needed a new algorithm like Google style page rank, which was robust keyword stuffing because it had backlinks that any individual coin couldn't fake.
02:33:35.590 - 02:34:05.926, Speaker A: Something like that, where you have things where you can't fake it. This gets us into the topic of sort of trustworthy exchange analytics or data. Right now, the heuristic people use is like, these exchanges are trusted and these aren't. So you just exclude all of their data and you just include all of these. And that's actually a reasonable first cut. But maybe you have ideas on something better than that. For our statistics, we actually do deep blockchain trace analysis.
02:34:05.926 - 02:34:54.494, Speaker A: We're trying to find out what are the sources of transactions. So I'm kind of assuming that if the transaction is a part of the broader defi ecosystem comes from compound Dydx and whatnot, or from trusted wallet, that's a legitimate use case because there's not just a transfer of tokens from one account to another. But it's a tough problem, right? How do you actually classify these? But this is essentially our approach. You get to a good issue there, which is the transparency versus privacy trade off, because before you trade, you want to have a glass box and know everything about it. But your particular trade, you actually don't necessarily want everybody to know about it. So, you know, different incentives in different parts. Okay, so moving to another one.
02:34:54.494 - 02:35:41.230, Speaker A: So what do you guys think of as the best jurisdiction to do crypto trading to set up an exchange? I would definitely place it in the European Union for a number of reasons. I don't think it's that smart to go offshore in the long term, especially if you want to get more and more reputable players into the space anywhere in the EU or Switzerland. So the good thing about the EU is that it is uniformly regulated by mifid two. So it doesn't really matter specifically where you do it in the EU. You would then look at the availability of how good are the legal institutions in the country. Is there a court system and everything? Right. I wouldn't go to Switzerland simply because they are not part of the EEA.
02:35:41.230 - 02:36:47.566, Speaker A: And in particular now Germany is interesting because they are pushing for security token regulation on all aspects. I think they want to be kind of a first mover there. But ultimately, it's really about the european framework that is harmonized in a huge market and provides, unlike, for example, the US, some very clear rules on how things work, which reduces legal cost by an order of ten or more. And so, yeah, I would be voting for the EU, there will. So I think it's a difficult decision to take for any exchange that's launching, especially now. But I think actually, although mifid, of course, gives you all the advantages you give there. And actually, I think in terms of primary issuance, it maybe makes sense in Europe, but there are a lot of disadvantages to trying to fit something that's different into an existing very sort of solid regulatory standard which wasn't designed for these sorts of things.
02:36:47.566 - 02:37:51.830, Speaker A: So I think somewhere like Abu Dhabi is one of the most interesting. So it's based on UK law, but they actually have made very specific provisions, for example, for self custody, managed custody of assets, and that changes the way that the treatment of the exchange is done, both for settlement and if you have a centralized operator, and there are various others, I think, which are following that model of actually designing from the ground up for cryptocurrency exchanges and take into account all these different challenges. And I think that will probably be the first landing point and then hopefully, I think maybe later on. Europe, for example, are you located in Dubai? So at the moment we are located in British Virgin islands. So offshore. I think that's, to be honest, the only way. I mean, if you genuinely want to be operating a cryptocurrency exchange, there's a huge cost associated with doing it anywhere that's sort of an onshore like UK or even Switzerland.
02:37:51.830 - 02:38:44.646, Speaker A: But in the long term, that's the most interesting direction, is places like Abu Dhabi. I guess I would say that it all depends on the use case and your own definition of cryptocurrency. Right. Because everything to us is like a token. It can represents what we think it's a currency, but it can be a security token. And of course, there's this huge gray area in between of tokens that some would say that it's a security token, some wouldn't. But normally I would definitely prefer jurisdictions that are trying to recognize that you're actually trying to change the world for the better, rather than put you into this broad basket of people doing cryptocurrency scams.
02:38:44.646 - 02:40:06.846, Speaker A: So what we are doing with our exchange, we are actually trying to make sure that indeed, our understanding of a protocol which is not operated, this is a good understanding in many jurisdictions, and I think we try to actually talk to as many jurisdictions as possible to make sure that there's nobody that will sort of challenge this point of view. And at the same time for the parts that we do operate. I think it really depends on the use case. So we will approach Americans to register ATS, for example, because we do want to trade security tokens for american customers and that's the only way you can do it in Europe. We probably might actually do it in Switzerland because Switzerland is far more advanced in terms of they thinking and they've got special provisions. Right, again for DLT trading. Whereas European Union, the MTF concept is not really designed to use for blockchain, the GDPR is not designed to be used for blockchain and so forth.
02:40:06.846 - 02:40:28.314, Speaker A: Right. So it's actually harder. And Switzerland has this advantage that they're very progressive and at the same time they located right in the center of Europe. So whatever we do in Switzerland should be actually usable for users, not just from Switzerland. Right. But in broader area. I'm surprised none of you said so.
02:40:28.314 - 02:41:23.040, Speaker A: Basically, I heard Europe, I heard Dubai, I heard quasi Europe, is that right? Yes, Switzerland also we're talking with Liechtenstein lawyers for. And like I said, I think it depends on the use case. So let me just give you an example. So if you wanted to have some sort of a pro version of margin trading where it wouldn't be fully decentralized, but we would actually borrow, we as an institution would borrow us a trade of money, then we would certainly use the jurisdictions that will make it easy for. So summary Europe, BVI, Dubai, Liechtenstein and so on. I would give a shout out probably for Singapore, which I think is a good all round jurisdiction for a lot of people. There's probably asian jurisdictions that folks might be able to educate us on.
02:41:23.040 - 02:42:23.630, Speaker A: So just last minute that we've got just quick lightning, 30 seconds. What's the next big thing in crypto trading? I guess I'm like a broken record that repeats itself. So I would say it's security token. We will see, I think a lot of tokens being issued over the next two years and people will be looking for liquidity. So whichever trading venue, be it fully decentralized or totally centralized, can make it possible for the biggest number of users to get liquidity. There will be a very, very strong contender for a, I think, super massive, super successful company in the future. Well, so I think it will be scaling via two routes, but I think the primary one that will eventually win out will be things like Starks for batch settlement, which will just reduce cost and mean that these do become practical for very high frequency trading, which then means that they can also lead price discovery.
02:42:23.630 - 02:43:04.080, Speaker A: With the multicolateral die launch, you'll be able to self leverage yourself without any counterparty. So that would be huge, we hope. And I definitely encourage every one of you to check it out because it's going to be a very different experience from using someone else's funds like you would do with compound and whatnot. And also what these guys did, it's ingenious. I'm very much looking forward to see how that's going to play out because that can actually radically change how we think about security tokens and security token trading. Great. Thank you all very much.
02:43:04.080 - 02:43:39.320, Speaker A: Very cool. So for those of you who've stayed around, welcome to the application track. Next up we have Alexey Ahovnov from Ethereum one X who's going to be talking about starks for stateless clients. The stateless client paradigm is one of the approaches to erase the pressure of ever growing available state. And he's going to explore the problem this approach solves and how stark proofs could be used in the process. So welcome to the stage. Thank you.
02:43:39.320 - 02:44:13.054, Speaker A: Thank you. Is this Mike okay? Yeah. So to start with, I would say that after this talk, at some point I will turn this into the textual version of it, because at the moment there's only pictures. And I hope you appreciate that there's not much text in there. All right, so first of all, we're going to start with the. I'm not going to go from far, let's just get straight to the business because I want to maybe leave some time for the questions. So some of you might familiar with Ethereum.
02:44:13.054 - 02:44:56.778, Speaker A: So let's imagine that we have a genesis in Ethereum, which consists of three accounts, and each of them is loaded with a certain amount of ether. You can see one of them is nine ether and another one, some others, has a fractional amount of ether. I'm going to be doing a lot of visual adjustments here to make you see better. So there's a number of them. The first one I'm going to do is that I'm going to shorten these keys. So on the left side you can see that how the complete keys look like. So essentially here we have a branch, branch node with the three branches on the top, and then you've got three this hanging keys.
02:44:56.778 - 02:45:24.394, Speaker A: So why is the colors. So some of you might know that in ethereum they use a hexadecimal representation for the keys. And each square in here represents a different number from zero to 15. So that's 116 colors. But these keys are quite long, so they will get in the way. So we're going to shorten them. So we skip part of the key.
02:45:24.394 - 02:46:02.562, Speaker A: So that's the first adjustment. So then I would demonstrate you what happens if we make a transaction which sends 1000 of the ether to a fresh account. So what you might notice is that instead of three accounts, we now have five accounts and they still connected to the same branch node. And so one of them is a miner who just earned two ether. So the sender got some deducted from its account and the receiver got some extra. So you see the new account appeared. So then we decide to send another thousand of ether to the same recipient.
02:46:02.562 - 02:46:46.994, Speaker A: You see the structure of the state does not really change, but the only thing that changes are the leaf. So you see the miner got another two ether and the receiver got some extra ether. And then obviously there will be some transaction fees, but they're quite small so you can't see them here. Right. Next we're going to do something more interesting. So on the left hand side you can see a very simplistic token contract in solidity, which doesn't have all of the functions like transfer from approve and all this kind of stuff, because I just wanted to minimize the code, but all it has is mint and transfer. So basically the minter could mint an n number of tokens to another address and the transfer, like anybody could transfer them.
02:46:46.994 - 02:47:31.838, Speaker A: So when we compile it with the solidity, with the latest solidity compiler, we see just under the kilobytes of bytecode, which is basically that big square with lots of colors, so they could appreciate how big it is. This is basically like 800 bytes or something. And then you can see that there's address of the miner appears somewhere there. So essentially what this diagram is showing is that there is this nuke account with zero ether in it which has some other bits attached to it. So apart from the nons, which is the first number, number one, and the ether balance, you also have two other things. Now you have a code attached and you also have a storage. So this account now starts to have storage.
02:47:31.838 - 02:48:16.350, Speaker A: And the storage, as you could see, starts as just one thing, but it will later grow into its own tree. So next thing we do is again we apply the visual adjustment again. So we don't want to be wasting space for these huge blobs of code. So we're going to compress them into a smaller representation to still let you know that there's a code there, but you're not really interested in what is inside. So it's just another adjustment. And then we do another adjustment. So instead of staring at this long address, which is currently essentially, this address is essentially a number, for the purpose of this discussion, it's simply a number which happens to be equal to some address of the minter.
02:48:16.350 - 02:48:40.754, Speaker A: But you just see that this is a big number, so we compress it as well. We just use the ellipses to put in the middle. So now let's now do something with this contract. We mint ten tokens. And so what happens is that you get in the storage instead of one. You now have three elements. And so the tree of the storage starts growing.
02:48:40.754 - 02:49:30.198, Speaker A: And so you can have a total supply, you might remember in the code that we had the total supply field, which is on the left. And then we have minted ten tokens, which is zero, eight in hexadecimal, and that's now another storage item. So we have three now. Now we're going to do the transfer of three tokens from one account to another, obviously from the minter, because it's the only account which has tokens at the moment. And so what you could see there is that the owner of these tokens is shown by the red arrows. You can see that, you could see that they spent some ether on the transaction fee, but they're not like in any way shown in the storage of the contract. It's because of how currently all the ERC 20 tokens work.
02:49:30.198 - 02:50:03.902, Speaker A: But you can see now that the total supply stays the same. But you now have tokens split between two different accounts. So now you can see the storage is now becoming much more interesting. So let's now do it on scale, on mass. I hope that you could still appreciate the scale. So now we forget about tokens for a moment, and we create 32 transactions that all send 1000 of ether to different addresses. We just generated the addresses randomly and just send some ether.
02:50:03.902 - 02:50:53.054, Speaker A: You could see now that we have quite a few accounts in a tree. And you see on the right hand side, you start interesting structure starts forming when you have not just the things hanging out, but you have multiple branch nodes, you have some nesting of the branching. And this is what people call Patricia Merkel tree. It's kind of starts becoming more interesting. And so here I'm going to do another visual adjustment. So I found in practice when I was preparing this visualization then that the diagrams becoming really wide and obviously for the purposes of presentation, they're just becoming so wide, it becomes impractical to see them because of this number 16, so everything is just branching so wildly. So instead I'm going to say, okay, I'm going to switch our tries to quad tries.
02:50:53.054 - 02:51:34.160, Speaker A: So instead of 16 we're going to use number four. And the reason for that is because we're still preserving this. So it's still bigger than two, which will be binary trees. And I consider binary trees as a special case. So it's still bigger than two, but it's not as big as the diagrams become like really wide. And I also noticed that when you choose the number four, you also start seeing more interesting features in your try. For example, you may notice that at some level, unfortunately I don't have a pointer, but around here, sorry.
02:51:34.160 - 02:52:20.400, Speaker A: So around here, just up where I'm standing, you see these three things, three nodes, and if you look on the top of them, there's a little blue dot over there. So this blue dot is called an extension node. What's that? Oh, the pointer, sorry. Thank you. So you see this blue dot dot, this is actually another special case in Patricia tree which is called extension node, which is basically like the way of compressing these little long stretches. So in here we don't have them because in order to get to this kind of feature we needed to have at least 1000 accounts. And so that's why this visual adjustment pays off.
02:52:20.400 - 02:52:57.290, Speaker A: So this is actually now a bit magnified. So I can show you that little thing again, this little feature. And actually there's another one here, right. And of course you can see there's a four colors only, so it's easier to work with because people kind of remember, they don't remember 16 things but they can remember four things. So now let's do another interesting thing. We're going to deploy a new token. So you see this is our old token contract with our, I think we did something with it again, but anyway, no, this is our old tokens which has the three and seven.
02:52:57.290 - 02:53:42.540, Speaker A: And now we're going to deploy the new one which is going to be here with the same code but with a much larger storage. And because we actually not just deployed it, but we also sent one token to 32 different token recipients. And that's why we see how the storage has grown out of here like it's another big tree. Okay, so what are we going to do next? Okay, so then we're going to send another. So this is where we're going to get to the interesting concept, which is I call block proof. So now imagine that we have this as a base, right? So this has happened like all these transactions and block happened. And this is where we now arrived at a certain block number.
02:53:42.540 - 02:54:10.610, Speaker A: So what is going to happen next is where we're going to start applying the stateless clients. So imagine that in our block that we are now concerned with, that we are going to propagate. There's only two things happening. There's two transactions. One of them sends 1000 of ether to one of the existing addresses. And the second transaction sends one token. And I can point out to you, so this goes from the twelve to the 13.
02:54:10.610 - 02:54:46.954, Speaker A: This is not the only places where it changes. For example, here you notice what changed is the amount here. So we sent to send extra ether to this account. So that's here. So then here what changes is that the miner earned another two ether, although it wasn't a party in any transaction. So here you could see that we sent one token and that's changed as a storage of our contract. And here, where was it? There was a non somewhere.
02:54:46.954 - 02:55:24.890, Speaker A: I can't see it. No, I think I didn't show all the changes. Okay. But you can basically see that there are some parts of this big tree that change it, but most of the tree does not change at all. Right. So the idea of the stateless client is that what if we are going to somehow only show, like we somehow compile the list of changes that have happened and somehow prove that these things that we are pointing out to you are actually belonging to this big tree. So this is exactly what I call the block proof.
02:55:24.890 - 02:56:04.774, Speaker A: So what I did here is you could see that in these things that are like little rows of quad digits. They used to be like that, but now some of them are large. And what are these things? So in this particular block with two transactions, we have touched only a few accounts. And all of them are shown with the large things. So we touched this account, that one, that one. So about five accounts here. And then we touched two storage items of this contract because we sent the token from one to another, because supply didn't change.
02:56:04.774 - 02:56:35.566, Speaker A: So we've got two changes here and we've got 12345 changes. And why is this five? Because one of them is a miner, one of them is a sender, one of them is receiver for the east transaction. But then another one is the sender for the token transaction. And the contract itself as well. Yeah. So this is the contract itself you could see. But why do we have these things included as well? So essentially you might already guessed what I've done here.
02:56:35.566 - 02:57:39.990, Speaker A: So this is the root of our try and only the things that we are kind of that changing in this block are included into our block proof everything else is kind of rolled into these hashes, right? And this is where we are. Instead of showing the entire tree, we just show the bits. The reason why we also have to carry around these things, which are seemingly unrelated is because I've discovered when I looked at the code of these operations with the Patricia Merkel tree, let's say in ago Ethereum and then in Turbogeth, is that the deletion operation in Patricia tree is a bit tricky, because when you delete things, let's say that you have a branch node with two elements here. Actually, let's take this one. So this is the branch node with two elements. If you happen to delete this one later on for some reason, then this one becomes like a non branch node and you have to collapse it. You have to merge this key with this one, and then after that, this is going to become one.
02:57:39.990 - 02:58:45.930, Speaker A: So you have to merge these two things and then make some transformation here. It turns out that if we did not present this thing together, then the next operation could have deleted that thing, and then we needed a bit more data to fetch. So essentially, inclusion of these extra things ensures that whatever operation is going to happen on this block proof, it will always be able to handle it without requesting more data. So this makes the block proof complete, meaning that any transaction which touches the data will be executed. It doesn't matter if it's inserting or deleting or just simply reading it. Now, I'm going to explain you the general idea of the stateless client. So currently, the full nodes in the serum network are supposed to have a copy of the state, like the things that we looked at before, like this large thing, right? And then by having the copy of the state, they can then execute transactions and know what the transactions are reading and what they're writing.
02:58:45.930 - 02:59:41.798, Speaker A: And then they arrive at the next version of the state and they hash it, and they compare the root of that hash to what is it in the header. And the problem of course, with that is if the state is really large, then it becomes harder and harder to keep it in such a way that you can efficiently access it. So your cache is getting full and stuff like that. So what we're going to in a stateless client idea is that instead of requiring that everybody has this state, somebody, it's enough just for one participant in the network to produce such a block proof for every block and ship it, I mean, to gossip it around the network together. With the blocks. So then everybody who receives block and the block proof will be able to execute the entire block because they have all the data available in here. This is enough to execute the entire block.
02:59:41.798 - 03:00:09.830, Speaker A: They will be able to verify that the data that you got are actually hashing to the correct root. And they will also be able to verify that after the execution, they just rehash the tree and then they will get the root in the header and that's it. So the great idea, right, okay. But there must be a catch, of course. Nothing. So before we get to the catch. So now let's go a bit into more details.
03:00:09.830 - 03:00:44.190, Speaker A: Let's say that we want to send them around what is going to be our payload for this kind of tree. So this is what I currently arrived at. So we are splitting this block proof into four parts. So obviously the keys and the values that we have. Then we have some hashes also, like a list. Then we have the codes, which I currently think we're not going to be able to do statelessly. And then we have this, which is some kind of instruction in some weird assembly language.
03:00:44.190 - 03:01:13.594, Speaker A: So I will show you pretty quickly how this assembly language works. So essentially we start with the first instruction, and it tells us to take one key and value and put it on a stack. And then we move to the next instruction. Again, we do the leaf. So this number tells us how many digits to take from the key. So then we move out the stack. Then we create a branch node with the three, which will be our, you will see.
03:01:13.594 - 03:01:44.734, Speaker A: So this is the zero three. So in our quote notation, white is the zero, blue is the three, and we keep moving. And then we basically compress it as a hash. So we hash that little subtree and hash it. So then we keep going and eventually there are more things getting added to the stack, and then we get more branches and blah, blah, blah. And so eventually you might get that. Eventually when we run out of, through all this program on the stack, we will have a root of our tree.
03:01:44.734 - 03:02:11.514, Speaker A: So this is the way we're going to verify that our and so on. And eventually you will just have one item on a stack, which will be the hash, and everything else has to be empty. Okay, now I've done the experiment. I've implemented similar things. Not exactly that thing, but very similar, which actually worked. It managed to process, I generated block proofs for all the blocks on Ethereum up until like 7.2 million.
03:02:11.514 - 03:02:49.270, Speaker A: And I measured how big they would be. And this is actually the moving average with a window of 1000 blocks, because I hope to smooth out these lines. So as you can see, that 1 mb line is about here. So you see that they're actually just getting over 1 mb on average. Individual ones could actually go much higher. But what you could also see that the biggest part of them are these hashes, the intermediate hashes, which you saw in orange. This is where I'm suggesting to use the stock proofs to be able to compress them.
03:02:49.270 - 03:03:35.330, Speaker A: So you can see the 1. We know that the stock proofs could be something on an order of, let's say, 40. Can also see that the remaining bits are the contract codes. That's why I'm saying that I suggest, in a way, we're not making them stateless yet to chop this up. And the remaining things, which are the yellow and the blue things, they are on order of, let's say, 60 kb, which is comparable to the stark proof. So in the stark, if we wanted to generate the stark proof of this computation, we just basically have these four things represented as inputs, as auxiliary input. So this one needs to be open so that you make sure that these things are all hash into the tree.
03:03:35.330 - 03:04:13.700, Speaker A: And this is basically a hidden input because we compress it away. And this is just the initial idea, because I'm not expert in Starks, but I'm inviting people to help me with that. I'm just suggesting this kind of specific. I don't think I'm going to have time for this. So I'm suggesting the specific architecture for how we can run this computation that just showed you with the stack as basically the polynomial stark construction. And I think I'm going to finish here. Thank you very much.
03:04:13.700 - 03:05:02.812, Speaker A: Are there any questions? The mics are here and here, if you have any. Go ahead. There's a microphone right there. Is that okay? I can ask one. Meanwhile. Okay, do you have the numbers? Like those numbers are without any, I would say, caching. Do you have also caching mechanism so that if you remember some witness or some block witness or block proof that you received in the past, I don't know, 1024 blocks, then maybe the total witness is smaller? Yes, I did have experiments like that.
03:05:02.812 - 03:06:16.860, Speaker A: So I experimented with number 256, which is basically, if you assume that people are storing the part of the state that has been touched in a recent 256 blocks, then that allows you, on average, again, on average, to reduce the size of the proof by the factor of three. But this is still quite unreliable, I would say. The question is about the code that you mentioned that you would like to make stateless yet. Which means that you're just planning that it will be distributed as a full code between all the nodes. So I'm basically suggesting that for the first step, we assume that all the full nodes will have the will keep the entire code of all deployed smart contracts. Okay, thanks. Do I understand correctly that you're not actually planning to execute the actual semantics of the transaction, only check that the hash was correctly calculated? No, the idea is that you will execute the semantics of the transactions, but given the block proof, you will never need to go into some kind of database for the state because everything that you need for this execution is in front of you in the block proof.
03:06:16.860 - 03:06:58.062, Speaker A: I see. And just indicatively, what is the ratio, computationally to how much you spend on verifying the transaction compared to verifying these hashes? Where I'm going with this is maybe you want to use starks to also verify the transaction semantics. If it's a problem, it's much more ambitious. I think in theory it's possible, but basically, I think at the moment, for me, it's too ambitious to do. Cool. So thank you very much for the talk. All right, so next we have James Prestwich.
03:06:58.062 - 03:07:15.362, Speaker A: I don't know if you know him. He's from summa. He's been voted best hair in Ethereum. That was a real poll. He really did win that. He's going to be talking about how zero knowledge proofs can be used for cross chain communication. This is one of my favorite topics.
03:07:15.362 - 03:07:34.634, Speaker A: So I'm very curious to hear what you have to say about. All right, thanks, Anna. Oh, the first thing you'll notice is that there might be misspellings in these slides. I don't actually spell check anything, and I made these very quickly. So cons usually has two m's when we use it this way. All right, so quick. About me.
03:07:34.634 - 03:07:51.422, Speaker A: I co founded a company called Storage about five years ago. For the last two and a half years, I've been working on Suma. We work on cross chain communication, focusing on bitcoin, ethereum and other chains. We can move on from that. Don't worry about it. We're here to talk about blockchains. Blockchains.
03:07:51.422 - 03:08:08.550, Speaker A: All right. No laughs. Okay, so this is what Satoshi Nakamoto thought a blockchain looked like. He got it wrong. All of those arrows should be pointing the other direction. Typically, we represent a block as like this prev hash and a nonce. And it's full of transactions.
03:08:08.550 - 03:08:37.570, Speaker A: There's a bunch more stuff that goes into a header. Bitcoin headers are about 80 bytes. Ethereum headers are about twice or three times that. And a chain is just a collection of blocks where each one references the previous block. So this is as opposed to Satoshi's wrong version, this is what a blockchain looks like. Okay, so we follow valid chains only. So if you say stupid things, they don't get in the chain, that entire block becomes invalid.
03:08:37.570 - 03:09:07.654, Speaker A: And we follow the heaviest chain in proof of work. In proof of stake, we use other fork choice rules. But we're going to keep it on proof of work for a minute because it's a little simpler. So if there's any conflict between two observed histories, we follow the one with more work put into it over time. And this resolves conflicts. And so we follow the heaviest valid chain and that determines what actually happened. You lose the invalid stuff like flat earth.
03:09:07.654 - 03:09:47.880, Speaker A: It's just dumb and wrong. And if Alice tries to double spend funds, we have a objective metric for which history we follow. So chains, we talk a lot about what is valid and we haven't put a lot of work into thinking about what can be valid yet. This is something that we typically figure out kind of on the fly. So if you're going to introduce rules into your consensus protocol, which I think most of us want some rules, we have to figure out whether something can be valid in consensus. And the only things that can be valid are objective facts. We often say, like publicly verifiable information.
03:09:47.880 - 03:10:32.502, Speaker A: If you want something to go on chain, it has to be something that everybody can check as part of the consensus protocol, right? So everyone has to make the same decisions to conform to the same, to reach the same consensus result. So that's why we can't have entropy. Time's fake and blockchains like time is an illusion and it's really wibbly wobbly and sometimes goes backwards for no reason. We can't have real world information. So this is what Joey was talking about earlier with oracles. That's a way to route around this limitation on public verifiability. And we actually can't spend a lot of resources on chain because everybody has to run the same thing.
03:10:32.502 - 03:11:14.420, Speaker A: It's called the verifier's dilemma. If you're checking somebody's work and they've put 1000 computer hours and you only have a desktop, you just can't do that. So every time you do something expensive on chain, or if you permit that in your chain, it limits the nodes that can be in consensus with you. So if you have expensive operations, you actually have a much smaller consensus set. If you look at EOS, they really only have 21 nodes in consensus, and everybody else is just running light clients all the time because of this problem. So what is an objective fact? Proof of work history. So all past chain state, everything we've reached consensus on before is an objective fact.
03:11:14.420 - 03:12:01.890, Speaker A: Any signed message, we say that probably ECDSA is pretty secure right now. So anything I see that's been signed by a pub key, I have an objectively verifiable criteria and can admit it into the chain and other cryptography, hash functions, zkps, whatever kind of proofs you want. So we're kind of limited in what we can do on a chain to these four things, basic categories. So what about other chains? We're here to talk about crosschain communication, really. Crosschain communication is about verifying the consensus process of a remote chain in the local chain. So let's unpack that a little bit. We already said we can't do expensive computations.
03:12:01.890 - 03:12:38.826, Speaker A: Isn't blockchains are pretty expensive? If any of you have ever tried to sync an ethereum node, it's not a simple task. It typically takes days, weeks if you want a full sync. And last time I tried to run an archival, it just wouldn't come up at all. We added blocks faster than it could process and store them. So we have this thing that's seven, eight years old now in bitcoin called SPV, which stands for special, sorry. Simple payment verification. That was from my fundraising deck.
03:12:38.826 - 03:13:17.718, Speaker A: Excuse me. So, SPV, we have these blocks, and blocks go in a chain, and we'll just make them real small, take those blocks, pack them real small. So what we actually do is, you remember our block, take all the transactions out, and now it's just about 80 bytes instead of a megabyte or 1.4 or whatever bitcoin actually is this week. And so to do that, we use what's called the SPV assumption, which is that miners check validity. All right, so let's talk about that for a second before we move on. We've already said we follow the heaviest valid chain in SPV.
03:13:17.718 - 03:13:51.602, Speaker A: We don't check validity, we can't know what is valid. That's how we get the computational savings. We have to process 80 bytes instead of 1.4 megabytes. So we're assuming that someone else is checking validity and that the chain that is heaviest will be valid. The headers alone give us enough information to check weight so we can follow just seeing 80 byte headers which chain is heaviest, but we don't necessarily know that it is the objectively correct bitcoin chain. We have a high degree of confidence, but not absolute certainty.
03:13:51.602 - 03:14:14.662, Speaker A: So it's an additional weaker security assumption than running a full node. But as we talked about earlier, you can't run a full node in another blockchain. You'd be running a full node. In your full node. The occursion is bad. Okay, so we can use the SPV assumption to make headers small enough to process on other chains. We're running a relay on Robston right now.
03:14:14.662 - 03:15:07.610, Speaker A: It runs a bitcoin header through Robston every ten minutes, give or take. We estimate it costs about $150 a month to maintain to stay up to date with bitcoin consensus at current gas rates. So that's cheap enough to do on other chains, right? Well, yes, but only for bitcoin. Unfortunately, we went through this fad four or five years ago, around the time Ethereum launched of memory, hard proof of work, which has turned into a dead end and an endless nightmare of debate that we won't go into right now. We haven't seen it produce any objective benefits. And it has this awful side effect of making headers awfully expensive to parse. For example, it takes something like 20 million gas in Ethereum to run a naive script execution, which means that you would need to fill up three or four Ethereum blocks to process a single litecoin header.
03:15:07.610 - 03:15:26.770, Speaker A: And we run litecoin headers every two, two and a half minutes, give or take. Zcash's headers are 1.5. No good reason. It's just a bad idea. So we can't actually run those things on any other chain. Practically speaking, it's just too expensive to do in the consensus process. And proof of stake.
03:15:26.770 - 03:16:54.434, Speaker A: In proof of work, we validate one hash and one hash per header, which costs like 120 gas in Ethereum is sufficient to follow bitcoin's chain history. In proof of stake, we would need to be checking messages from all validators against all past chain state, which would means we would need to store all validator balances at the time they enter the validator set and then validate signatures from each validator as they sign messages for each block. So if there's something like 20 validators signing ECDSA, you're looking at something like 400,000 gas for storage each time the set changes, and something in the neighborhood of 40,000 gas per block minimum. And again, bitcoin takes a few hundred gas per block, so it's something where validating proof of stake is orders of magnitude more expensive. And obviously weird things like ripple and stellar consensus we can't do at all because they're kind of like federation based and don't have an objective trust route. Stellar, you insert your own trust so we can't interact with those in any meaningful way. All right, so bitcoin is cheap enough to run on other chains, which is nice because bitcoin is the thing we actually all want anyway.
03:16:54.434 - 03:17:24.330, Speaker A: It's a bitcoin Ethereum and then everything else, right? Yeah. Okay, good. So how does Zkps fit into this? We've talked a little bit about public verifiability. We have another term that we like to use for this. It takes public verifiability like what is an objective fact and adds on what is an objective fact that we can practically verify on chain. And we call this observability. Right.
03:17:24.330 - 03:17:57.778, Speaker A: So we want to make our blocks light so that we can observe them from within a chain. If it's too heavy, we can't observe it. If it uses randomness, we can't observe it because it's not publicly verifiable. Okay, so we want to make our blocks light. This means something that the other chain can practically validate. And so we don't actually need to make the blocks light, we need to make the verification overhead of those blocks light. This entire talk isn't about like following bitcoin consensus, it's about what is the verification overhead of checking remote chains.
03:17:57.778 - 03:18:51.314, Speaker A: And that's where zkps come in and are really useful. We can use cryptography to solve our blockchain problems. So zkps allow us to make arbitrary statements about information, which means we could make some statement like I know of twelve litecoin headers that form a cohesive chain with work totaling x. So we can summarize in a very small, cheap to validate statement, a very large and expensive statement that would take millions of gas. And that's how we can use zkps to aid cross chain communication. We can make proofs about the remote consensus process that are efficiently verified in the local consensus process. So this opens up relays to more proof of work chains like litecoin.
03:18:51.314 - 03:19:52.650, Speaker A: If we had a good ZKP representation of scrypt and potentially proof of stake chains, but they're still probably expensive. So proof of stake has this annoying, well, kind of like we use memory, hard proof of work in so many proof of work chains. Very few chains are designed for chains use signature schemes and hash functions that are not amenable to zkps. We use, like, ECDSA or EDDSA, and those are quite expensive to do in most ZkP constructions. And we use SHA two and other bitmashing based hash constructions, which are very expensive to do in zero knowledge proofs. So the one significant exception to this is coda. Coda designed their entire consensus process and state model and every primitive selection around being efficiently provable.
03:19:52.650 - 03:20:44.410, Speaker A: And the cosmos approach to this. Shout out to zucky, who's in here somewhere, was to design tendermint to be efficiently verifiable by other tendermint chains. It's a bit more complex than that, but the goal of Cosmos was to make relaying efficient so that you didn't have to do zkps. All right, so future plans. What am I actually like building here? I still don't know. Right now we're working on bitcoin relay and bitcoin based contracts in Ethereum, Cosmos, and other chains. I would love to kind of apply this and apply zkps to other problems in cross chain communication, but it's very difficult to tell what's useful to other people and useful to engineers, and I would love to take questions or suggestions.
03:20:44.410 - 03:21:39.580, Speaker A: Does anyone have any questions? Hi. There used to be this thing called BTC relay, and I know that some people even try to write basic dapps that would allow you to trade tokens for bitcoin and stuff. So, actually, I think someone stopped pushing headers to that, like, years ago. And what would you do? What would you like to do better? What do you think the cause was that it didn't work out? Or what would you improve to make it more popular? So there's a few different answers to that. Like, I know the team that wrote BTC relay. Part of it was serpent, is people moved away from serpent very quickly. But the other part is just that BTC relay stored something like, I want to say, six or eight slots per header.
03:21:39.580 - 03:22:14.410, Speaker A: And newer relay constructions, like we've been working on and running store one and a quarter slots per header. So already there's just, like, four x overhead on gas costs for BTC relay. The other answer is that at the time, we had no evidence and no market had developed for cross chain communication at all. And I think we're still extremely early for this. So BTC relay was, like, three years earlier than very early, and I think that was its main downfall. Cool. Thanks.
03:22:14.410 - 03:22:46.690, Speaker A: Hey, cool talk. Thanks. You've heard of fly client, this protocol, right? Yeah, we're actually working on Merkel mountain range commitments for Zcash. Oh, you're doing that work? Okay, really cool. So do you think you can get better than fly client with ZKP, like stocks or whatever? Do you think you can do better than that, or is that pretty good? Potentially, fly client is pretty good. It allows us to do a logarithmic. Okay.
03:22:46.690 - 03:23:47.944, Speaker A: It allows us to do an update that can do any number of headers and is logarithmic in the number of headers. The problem is fly client requires like Fiat Shamir. And so if we want to have an on chain fly verifier, we're going to be needing to do fiat shamir on chain, which is a bit of a mess. We've looked at fly based optimistic relays, so fly client commitments would allow efficient fraud proofs, which means we could use optimistic techniques rather than direct verification. It is an additional security assumption, because you're assuming that a fraud prover can get a transaction on chain in a certain amount of time, and for a relay to be useful, that amount of time must be very low. However, fly client is still really interesting. Your original question before I went off on a tangent was, can zkps do better? The answer is that you could actually probably Zkp the verification of a fly client proof if you really wanted to.
03:23:47.944 - 03:24:27.616, Speaker A: The main question in my mind is how expensive will that prover be to run? When we zkp complex things composed of many hash functions, the prover gets awfully expensive, awfully fast, and I have no way of benchmarking this in my mind yet. Thank you. Thank you. That's a great approach. And I have two questions. The first one is, where do you want to store these small blocks? And is it, it will be on chain or it's like some other chain? Yes. So right now we're running a bitcoin relay on Robston, and we actually don't store blocks.
03:24:27.616 - 03:25:18.820, Speaker A: When I said earlier that it costs 1.25 slots per header, in Ethereum, a storage slot is 32 bytes. So we're actually storing what's a quarter of 32? Eight. We're storing 40 bytes per header instead of 80 bytes. And the way we do that is by abstracting the blockchain down to its minimal thing, which is links in the chain. So the relay stores on chain in Ethereum or in cosmos or wherever, for each header, the parent hash of that header, and for every fourth header, the height of that header. And so it's absolute minimal impact on the state of the host chain, but it's still kind of unavoidable if you want to use that relay in host chain smart contracts.
03:25:18.820 - 03:26:26.804, Speaker A: Thank you. And second thing is, what kind of parameters do you want to verify by ZKP from the big doc headers? What the most important information to verify? Typically, the most important information to verify is the proof of work on a proof of work chain or the staking process on a proof of stake chain. We're looking to observe the remote chains consensus protocol, and so practical limitations prevent that for most chains. So we'd be looking to snark the signature algorithms or the tendermint consensus or the proof of work hash function. Thank you. Are there any other questions? All right, thank you, James. So next up we have Dan Robinson from Paradigm, who's going to be talking about plasma stash improving plasma cache with starks.
03:26:26.804 - 03:26:49.504, Speaker A: And I just learned today why you call it stash makes sense. Dan will be speaking about the ways Starks can be used on and off chain to reduce the size of the history. Oh, and one thing, we're actually back on time. I don't know how that happened, but we're back on time. Okay, cool. Great. Hi, everyone.
03:26:49.504 - 03:27:26.140, Speaker A: My name is Dan Robinson. I'm a research partner at Paradigm, but I'm here representing my own views. We're a crypto asset investment firm. So I'm going to talk about plasma cash, if you're not familiar with it, plasma cash is plasma. It's an off chain scaling solution on Ethereum, and what's particularly useful about it as a scaling solution is that it doesn't require data availability of these state transitions. So it shares that in common with state channels. And I'm going to be talking about how starks actually are a really good fit for plasma cache and for fixing some of the problems with plasma cache, because there are these proofs that are part of it that I'll tell you all about.
03:27:26.140 - 03:28:12.872, Speaker A: So plasma is called a non custodial side chain. I think Patrick calls them commit chains. It's a side chain that's committed to, say, the Ethereum main chain, and you can move assets onto and off of it, but it's operated by, say, one operator. And the key thing is the operator can't steal the funds and can't prevent you from withdrawing them. And withdrawing them can be done in constant steps, even if no matter how many times the asset has been transferred. So these are the nice things about plasma. There are some alternatives in terms of scaling that have different advantages and disadvantages, and one of the major ones that's relatively sort of adjacent to plasma is roll ups.
03:28:12.872 - 03:29:27.136, Speaker A: So Stark decks, Starkpay, stark exchange, all the stuff that starkware has been working on, generally falls in the roll up category, because that's something where it's similar to plasma, except it requires that you guarantee data availability of these state transitions in some way, typically by just sort of splatting them onto the, onto the main chain in the call data. So that means that you no longer have this sort of infinite scalability, where you can have as many state transitions as you want without any kind of scaling burden on the main chain, but you can get something like a ten x to 100 x to 1000 x advantage from this. So the big disadvantage that plasma has compared to roll ups is that it supports a much more limited set of applications, specifically, really only sort of like payments in exchange. The one I'll be talking about today is just simple payments and transfers of non fungible tokens, whereas roll ups, you could run whatever kind of computation you want. Anything you can verify in the EVM can be done with that. But I'm going to be talking about plasma cache here. So, just to start, the core of plasma cache is based on this data structure called the sparse Merkel tree, or similar ones, an optimization of which I'll be talking about.
03:29:27.136 - 03:30:16.400, Speaker A: So, a sparse merkel tree is a commitment. Normal Merkel tree is just a commitment to a list of values, and a sparse Merkel tree is a commitment to a mapping to something where there's particular keys, and you can only open one key to one particular value. So here, most of this tree is empty, but the fourth key is some value v. And so it's very efficient to prove that slot four has value v, and you can only open this commitment to that one value. There's no way to prove that slot v is empty or anything else, and that's in just sort of logarithmic and the number of total number of slots. So in this case it's four. Similarly, it's very efficient to do a non inclusion proof, so you need to have these two primitives in order to support plasma cache.
03:30:16.400 - 03:31:27.060, Speaker A: There's an optimization to sparse Merkel trees that's sort of the one that's currently used, which is Merkel interval tree. And this one, it just allows this leaf here, this leaf at index one, to cover a whole range, because all these values were empty, if you recall, in the sparse Merkel tree. So we just compress them all, and it just says this is the hash starting at one, and then you only have to actually put leaves when there's a value or one that isn't identical to its adjacent value. So it's a bit of a compression. So this means now our proofs are log m in just the number of non empty slots in the Merkel tree. So that's just an efficiency improvement. Okay, so how do we use these commitments? So the structure of a plasma chain is that it's divided into a set of coins, and a coin is some asset, and that asset is tracked by some particular slot in the Merkel tree, in the sparse Merkel tree or MerKel interval tree.
03:31:27.060 - 03:31:59.552, Speaker A: And the key thing here is that the coin doesn't move. The coin stays in the SAMe place, and the owner comes to it and changes with each j transistent in plasma. So if you're familiar with on the Isle of Yap, this is some monetary history 101. On the isle of Yap, there are these rise stones, and these are used as money. And that's like, I don't know. I don't know how big. It's, it's very, it's really convenient because it's very hard to steal this as money, but of course it's very hard to transfer it to somebody.
03:31:59.552 - 03:32:43.010, Speaker A: So instead, when you transfer someone one of these, you don't actually have to move it, you don't give it to them. You just tell everybody. And maybe a public ceremony to this effect, you just say that you're transferring one that everybody already knows that you own, and now everybody knows that someone owns, the recipient owns it. So it's just like with these coins, each of these right now is owned by some particular party. But then if Dave wants to transfer it, he just signs a transaction. Or if Alice wants to transfer something, she signs a transaction that sends it to Bob, and that transaction has to go in her slot, in the slot for the coin that she's transferring. So in this case, here, it says slot four because it's slot one.
03:32:43.010 - 03:33:25.388, Speaker A: Alice just says, I'm sending this coin, and then that transaction is included at the right spot in the Merkel tree, and it actually just moves. The asset gave you a little preview of that. Transactions end up being merkelized and put into one of these sparse merkle trees, or Merkel interval trees. And so each plasma block is just a commitment to. Here are all the state transitions, here are all the things, the coins that change donors. And this is committed into the parent chain. And this is what allows plasma to prevent double spending, because once it's been committed to the parent chain, there's just no way for someone else to exit.
03:33:25.388 - 03:34:10.616, Speaker A: And I'll describe how the exit game kind of protects you now, once you've been committed to the chain. So, in order to exit, you can initiate an exit just by showing that at block five, you provide an inclusion proof that says Bob owned this as of block five. And block five is some block in the past. Now, what happens if somebody exits this from this outdated state, but Bob is actually already double spent it. Bob's already sent it to Charlie. In that case, you can just reveal this transaction here from Bob to Charlie, and that proves that this is an outdated exit, and that just cancels the exit right away. So Bob is enabled double spend, slightly more difficult to prevent, is a malicious exit.
03:34:10.616 - 03:34:47.080, Speaker A: And so the way that works is just, this can only happen if the invalid exit. This can only happen if the operator is malicious. But what happens here is just Charlie never received this coin. Bob owns this coin. Bob thinks he owns this coin, but then the operator just says, at this point, Charlie owned this coin, and Bob knows that Charlie doesn't, actually. So what Bob can do is just exit his coin, and the prior uncancelled exit wins. So whatever exit is earliest succeeds, and so Charlie's exit won't succeed because Charlie has no way of canceling Bob's exit.
03:34:47.080 - 03:35:42.092, Speaker A: This exit game basically works. I papered over a few sort of subtleties that aren't really important for what I'm talking about here. But there's this problem, right? Which is, when I get a coin, I need to know that no prior owner still owns that coin, or rather, that I need to know that I can cancel any prior owner's claim on that coin. And in order to do that, I need to receive a proof that, say, when Charlie receives this coin, we need to send Charlie a proof. The operator of the chain or the prior owner needs to show. Look, here's your transaction at this particular I received it in block, this coin in block three. It was deposited in block three, and then it wasn't spent in any block since then.
03:35:42.092 - 03:36:10.864, Speaker A: And then now you're receiving it. And so once Charlie sees all of this, Charlie knows that he owns the coin. So this history is independent of any other coin. This is only Charlie's rhinestone that's actually being tracked by this particular slot. But it is linear in the length of the history and the number of blocks in the plasma chain's history. So that can be kind of a large proof. Fortunately, it's an off chain proof.
03:36:10.864 - 03:37:16.072, Speaker A: And this is what's really cool about plasma cash, is that you have this proof, but it's only verified, needs to be verified by the user because on chain it's verified by this interactive exit game. But when Charlie receives this coin, they're not able to take advantage of this on chain interactive game because that wouldn't be scalable. So they actually just need to verify this proof off chain. And that's the problem we're going to be solving with starks because these are big proofs. So like I mentioned, it's linear in the size of the number of blocks in the history and it's also logarithmic in the depth of each of these proofs because there's a non inclusion proof or an inclusion proof, or each of these sort of merkel proofs. So if we want to support 1000 transactions per second, and I think we know that we do, and we want to say no, we're not sacrificing any latency compared to the Ethereum chain, we're just going to have five blocks every minute and 12,000 transactions block that comes out to be about 2.6 gigabyte proof of proofs per year.
03:37:16.072 - 03:37:51.060, Speaker A: And so I checked my iTunes library and 2.6 is almost exactly the size of Pirates of the Caribbean Dead man's chest, the second film in the Pirates of the Caribbean film Pirates of the Caribbean series. So it's a good way to remember it if you're familiar with my itunes library. But generally it's a pretty large file to have to send somebody 2.6gb every time you want to transfer them a coin. So remember this doesn't go on chain. We're not lunatics, but you have to send this to somebody every time you transfer them a coin.
03:37:51.060 - 03:38:32.748, Speaker A: Fortunately here what we want are efficient. I was so undecided between Tony Stark and Arya Stark and then Ellie used Tony this morning and it was like a this. We have a case here where we have these proofs and it's a pretty simple proof, but it's really large and we want it to be smaller. So this is a perfect use case for starks. And what's cool is we're not going to verify any starks on chain here. And in fact there's nothing about this construction that needs to be built into the plasma cache contracts or anything. This is all just an upgrade that you sort of just layer on top of plasma cache and it makes the proofs, can make the proofs a hell of a lot more efficient.
03:38:32.748 - 03:39:10.460, Speaker A: So looking at this example, 1000 transactions per second, five blocks a minute for one year. If you just give people the explicit Merkel proofs that's 2.6gb. With Starks, it's only around 100. Do a stark. And thanks Avihu, for helping with the benchmarks here and estimating these numbers. One downside of this is I checked, and 100 kb is actually about the size of the screenshot I took for the lame iTunes joke that I had earlier. So that's much smaller.
03:39:10.460 - 03:39:59.544, Speaker A: Now, there's a downside here, which is if you were to sort of do this on your laptop, it would take about five days in order to prove just one of these. Now that might be okay because you're proving a whole year. So you only really have to do this like at the end of every year. You go, okay, I'm going to do my stark proof for the previous year, and you run your laptop for a couple of days. This is also a very parallelizable process. So you could have this run in the cloud and figure out how to generate this quickly, or the operator could run a service that generates these proofs for users. But if you want a slightly more efficient system, you can reduce the number of blocks, the frequency at which blocks are committed to the main chain.
03:39:59.544 - 03:40:56.608, Speaker A: So you get these deeper blocks, but they're a bit fewer than approve. And so for that you'd still have an 80 megabyte merkel proof per year, which is not a trivial amount to download just in order to receive a transaction. But with Starks, you get the same space savings, but also it's a lot quicker. So one final optimization, it's a bit of a problem to have to prove the entire history of your chain, of your coin. If that goes back a couple of years, it'd be really nice if we could just checkpoint the whole chain at some point. And you can do that off chain using Starks as well. So what could happen is the operator can just generate a proof of the current state of the chain and some information about the prior transactions that you need to cancel them and just create a proof that this is the current state as of a particular point, and just provide that off chain to anybody who wants it.
03:40:56.608 - 03:41:38.904, Speaker A: This is still a compact, like 100 to 200 kilobyte proof, but once the job operators generated this for this particular point, now all the other history proofs, all the coin specific proofs, can just start at the point that was checkpointed. So this takes a lot more. The proving time here is about linear in the number of transactions over that period. So 1000 transactions per second. It turns out it takes about 10 seconds to prove every 1 second of history. So you're never going to catch up. But you could parallelize this and run this on a server, or you could just run something sort of, that lags quite a bit behind, but still would be materially reducing the amount of sizes of these proofs.
03:41:38.904 - 03:42:16.744, Speaker A: So starks do also give us, I think, potentially some interesting ways to extend plasma cache and to make more complicated state machines viable in it. But I think that's going to have to wait for further work. So is building it, because I don't build stuff anymore that much, but I do make t shirts. So if anybody builds plasma cache, plasma stash, I will make them a, you know, it's something that if you're building plasma cache georgios, I think you need to. It's just a layer on top of it. This isn't something you even have to worry about in the base layer. It's pretty cool.
03:42:16.744 - 03:43:17.688, Speaker A: The one thing is probably you should be using Patterson hashes. So if you want to tell me about stuff you're working on, if you want to ask more about this or my other work, you can reach me easiest at Dan Robinson on Twitter. All right, thanks, everyone. All right, does anyone have any questions? Thanks. Do you mind to announce about this in plasma calls that we have? Because just recently on the last one I talked about similar approach. And the huge question is, is it possible to put proof for sparse merkel tree inside the star? Did you try to do this? Talked to. I haven't tried implementing this, but I talked to Avihu about it and I think it's a relatively straightforward process, especially, I think if you use the Merkel interval tree rather than a sparse merkel tree, it gets a lot more efficient.
03:43:17.688 - 03:43:33.552, Speaker A: Thank you. I'm sorry, I haven't been on the plasma calls recently. I moved to the west coast, so we're at 07:00 a.m. It's just a pain, but I'm sure they're great. But it's a great point just to continue because that's the point that we start to talk about this. Yeah. Thank you.
03:43:33.552 - 03:44:13.436, Speaker A: Cool. I had a question regarding the. I didn't quite get the negatives of the roll ups, and so my experience is building roll ups for crypto collectibles and the way that you've done your merkel trees are very similar. I just didn't quite get the negatives of why a roll up. I didn't quite get the negatives of that. So with the roll up, you have to put all the data for the state transitions on chain, right? Yeah. So at least in theory that gives you some limit to the scaling benefits, right? Like if you couldn't do a million transactions, you probably couldn't maybe do 1000 transactions per second.
03:44:13.436 - 03:44:51.892, Speaker A: I'm not sure if you can with roll ups on Ethereum today. That said, honestly, I think there's a good argument to be made that that's enough for now. It's probably good enough just, and you get like a ten hundred x maybe scaling benefit. So I think that's a pretty good argument. So yeah, I think plasma research is very helpful. For one thing, I think it informs some stuff about how to design roll ups and to potentially make them more efficient. And because for sort of the eventual day when we want to kind of break free from the bounds of on chain datability.
03:44:51.892 - 03:45:19.932, Speaker A: Perfect. Thank you, very excited and honor to be here. And thank you starkware, for organizing this event. So as Anna said, I'm going to be talking about Janstark. This is a library that you can use to generate and verify starks. So first a bit of context. So the overarching goal of the library is to let you create starks without actually understanding all the nitty gritty details behind how starks work.
03:45:19.932 - 03:46:22.680, Speaker A: And the idea here is that you should be able to define a computation in kind of abstract and hopefully standardized way, and then the library will do the rest. It's in a way similar to how you don't really need to understand how a processor works to write python and Javascript code. And the way I really got into this is that I wanted to learn about starks. And I read the white paper, I read Vitalik's blogs on starks, and then I kind of went through his code and to internalize the concepts better, I decided to rewrite the code in JavaScript, and also wanted to do something slightly more complex, basically not just MIMC calculation, but try to create a stark that would approve a pre image or hash function or something like that. And as I was working through the code, two things really became apparent to me. The first one is that there is a lot of kind of boilerplate code where I was writing code that had nothing to do with the specifics of the computation that I was trying to create a stark for. And the second thing that kind of was apparent as well is writing constraints.
03:46:22.680 - 03:47:00.876, Speaker A: Defining computation writing transition function transition constraints was incredibly tedious and error prone process, and at least in the way I was doing, it wasn't fun to do at all. So I came up with this air script language that allowed me to define constraints in a simpler way. And in a way that could be much more easier to reason about. So the library is very young. I only started doing this maybe four months ago. It's still at the alpha stage, I would say. So don't use it in production yet, but it's fully functional.
03:47:00.876 - 03:47:53.248, Speaker A: You can play around with it, you can go and generate starks, you can prove them. And it was kind of easy for me because really I'm just standing on the shoulders of giants here, so it wasn't too difficult in that sense. And the tech stack for the library is JavaScript, so you have to have node js to make writing in JavaScript a little bit more tolerable. I'm using typescript, and because JavaScript is not really that good with kind of computationally heavy stuff, I'm using webassembly to move some of the parts of the library and make them more efficient. All right, so today I would like to cover basically these topics. First, I'm going to give like a very high level, very brief overview of stark generation process. And I know it's going to be redundant to a lot of the things that you heard yesterday and today, but I think it would be kind of useful because it will show how I thought about it and how it aligns with how I broke up the library into different components.
03:47:53.248 - 03:48:45.876, Speaker A: And then I'm going to show a specific example of airscript and very simple trivial computation, how you can define it in airscript and how you can generate a stark for it, and how you can verify it as well. Then lastly, if there is time left, we'll show some performance benchmarks and kind of talk about next or plans that I have for the library. All right, so let's start with the stark overview. So as you all know, starks are used to prove correctness of a computation, and any computation starts with a set of inputs. And then if you have inputs and you have a transition function, you can apply this function to the inputs in an iterative manner and you'll get something that is called an execution trace. And execution trace is nothing more complex than it's a two dimensional matrix. And you can think about this matrix as having columns which correspond to registers, and every row in the matrix is basically a state of that register at a given point in time.
03:48:45.876 - 03:49:18.876, Speaker A: So once you have the two dimensional matrix, the execution trace, the next thing to do is to apply transition constraints to it and you get another two dimensional matrix. And basically the way you apply the constraints is that you take. In my case, I just take the two consecutive steps in the library or in the execution trace, and I apply constraints to it. And then that kind of generates a row in the constraint evaluation matrix. And what the constraints do, they just verify that transition was done correctly. So you get this matrix and here. Now each column doesn't correspond to register anymore.
03:49:18.876 - 03:49:48.276, Speaker A: It is now a specific constraint. So if you have five constraints, you'll get five columns. If you have more or less, you'll get corresponding number of columns. So the next thing to do, you merge all of these constraints into a single composition polynomial. Now, I'm skipping over a bunch of things. There is much more kind of math and logic that goes in between the steps, but kind of at a high level, you get a composition polynomial, which is kind of second to last step, so to say. And then the last step is to prove that this polynomial is of a certain low degree.
03:49:48.276 - 03:50:26.116, Speaker A: And you use fry protocol to do that. And then you get this dark proof, which on a high level consists of two parts. There is a low degree proof, and then there is a proof of the execution trace. And now if we think about kind of these four steps in a high level, there are some things that we can notice is that the first two steps are computation specific, but the other two steps are computation agnostic. And what I mean by this is that, let's say you have two computations. One is computing hash function, and the other one is verifying correctness of a digital signature. For example, your transition function and transition constraints will be very, very different.
03:50:26.116 - 03:50:56.024, Speaker A: But the way you merge constraints, the composition logic, and also the fry protocol will be exactly the same. So that could be reused. And the way I broke it up in the library is that you have air script that defines transition function and transition constraints. And then you have a stark object that has the logic for composing the polynomial into running the fry proof. Fry protocol. All right, so let's go this kind of step by step. So the first thing you have, you start out with airscript, and I'll show an example of airscript in a few slides.
03:50:56.024 - 03:51:31.268, Speaker A: But airscript, as I mentioned, defines transition function, transition constraints. It also defines some other things, like a field in which you're going to be working with. It also defines, like constant and read only registers. And I'll talk about them later on as well. And then you compile airscript into a stark object, and that basically generates a code that you can execute that will generate your execution trace, and it will generate your constraint evaluations. And then you can use the stark object to generate stark proofs for this computation, and you can generate many proofs using the same object based on which parameters you pass in. And you pass in three types of parameters basically to the proof method.
03:51:31.268 - 03:52:03.376, Speaker A: The first one is secret inputs. Then you can pass in public inputs, and also you pass in assertions, or another name for those are boundary constraints. Basically they specify what a given register should equal to at a given step in a computation for the execution trace to be valid. And then verification works kind of in a very similar manner. You start out with air script, you compile it into a stark object. Now you provide a proof as one of the inputs. You still need to provide the public inputs and assertions, and then you can verify the proof whether if it either passes or fails.
03:52:03.376 - 03:52:32.764, Speaker A: So, pretty simple. So let's do a very quick example. Let's say this is our computation. It's a trivial computation of sorts, but basically what it does is we have a single column, a single register, and the way we define transitions is using these two equations. And the first equation basically says you take the current value, square it and add some constant to it. And the second equation says you just subtract that constant from the current value. And we decide which of these two equations to use based on whether the current step is a multiple of four or not.
03:52:32.764 - 03:53:07.752, Speaker A: So every four steps we apply the first equation, and then in the other steps we apply the second equation. And this is how the air script defining this computation looks like. It's basically just about 20 lines of code, I think, and it's very small font. So let's go through this line by line and zoom in a little bit. So the first thing that you do is you kind of define what your stock is, you give it an aim, and you also define the field over which you're going to do all the mathematical operations. In this case, it's a prime field, 32 bit modulus. At the current point in time.
03:53:07.752 - 03:53:45.204, Speaker A: The library supports only prime fields, but I wrote in such a way where it's modular. And if I get to writing the binary field implementation, it's just pluggable into the library. The interfaces are well defined. So once we have kind of the basic properties set up, the next thing to do is define what I call readonly registers. And these are the constants that are going to be used when we generate an execution trace. And when we kind of evaluate our transition constraints, they're not really mutable by the transition function. So the transition function can use these registers, but it cannot change them.
03:53:45.204 - 03:54:13.912, Speaker A: And in this case we're defining two read only registers, the k zero is going to be just those four values, one followed by three zeros repeated over the execution trace. And it's going to be used to figure out which of the two transition equations we're going to apply. And they are the p zero register. It's an input register. Basically we don't provide values here as a part of the definition of the script. The values will be provided at the time when we generate the proof or try to verify the proof. Okay, so now we have these read only registers.
03:54:13.912 - 03:54:49.728, Speaker A: The next thing to do is to define a transition function. So as I mentioned, this trivial computation has only one register. So we say it's one register and we're going to run it for 64 steps. And then one thing to note here. So if k zero was a static register, r zero is kind of a register in execution trace that holds the current value of the register. So what we're doing here is we're basically saying when the value of k zero is one, then square the current state of the register and add p zero to it. Otherwise, take the p zero and subtract it from current value of the r zero register.
03:54:49.728 - 03:55:22.644, Speaker A: So pretty straight forward. And then once we have the transition function, the next thing to do is to define transition constraints. And in this case they are pretty straightforward as well. We have one constraint. And if you look here, I think I made a mistake with exponents here, but basically it should be the out portion and the else clause should be corresponding to. Let me go to the previous step, r zero minus p zero. So here, ignore the else clause, but when clause is correct.
03:55:22.644 - 03:56:03.312, Speaker A: So when k zero is one, we kind of compute the transition function and then subtract it from n zero. And n zero in this case is also a register that refers to the execution trace, but it refers to the next step in the execution trace, not the current state. So we basically say if we compute a transition function and subtract the value that we get by computing that from the next step in the execution trace, we should get a zero. And if we get a zero, then transition is valid. Okay, so let's look at the concrete example of if we run this air script, what kind of execution trace would it generate? And here the first thing to notice. So we have our k zero, p zero and r zero registers. And the first thing is in k zero.
03:56:03.312 - 03:56:41.874, Speaker A: You see that pattern repeating over time. So you have one followed by three zeros. And then I'll explain where the values for p zero come from. At the next slide, they basically going to be provided at the time when we generate the proof and r zero, we initialize it to value three. And then the way we compute kind of the execution trace is that we apply the transition function to kind of the first row, and in this case k zero is one. So we square the value three and add one to it, so we get ten. The next step we repeat the logic, but now k zero is zero.
03:56:41.874 - 03:57:18.778, Speaker A: So we apply the second equation, which is just subtracting one from ten in this case, and we get a nine. And then if we run this for 63 64 steps, we get to this three nine four value at the bottom. So this is how the execution trace would look like. Now how do you run it from Javascript? So, pretty straightforward. The first thing you do, you create the stark object and you pass the script into it. And script in this case is just a string variable containing the same script that I showed on the previous slides. The script gets compiled into executable code, and then you can use this dark object to generate proofs and to verify proofs.
03:57:18.778 - 03:58:20.210, Speaker A: But before you do that, you need to specify some assertions and you need to provide some input values. And here the assertions are again boundary constraints, very simple. Just saying that the computation would be valid if at step zero the value of our register is three, and it would be valid if at step 63 the value would be this three nine four number. And here we also define the values for p zero, in this case just a sequence of integers, and those are going to be kind of stretched over the execution trace when we pass it to the proof method. So once we have that, we run the proof method, generates a proof, and then once we have a proof, we can verify it using just the verify method. And of course in this case the verification would pass because we just proved and verified the same exact thing. Now what this really is basically, if you think about at a high level, what this says is that if we were to take this computation defined by these two equations, and if we were to run it for 63 steps, we would get an execution trace that would basically comply with the constraints that we specified.
03:58:20.210 - 03:59:26.598, Speaker A: And of course if we, for example, before verifying, if I were to change p zero values to something else, or if I were to modify assertions, the proof would fail. All right, so this was a very trivial computation. I used airscript to write kind of starks for different types of computations, specifically MEMC and also Merkel proof, like verifying membership in the Merkel tree using rescue and Poseidon hash function. One thing I should mention that this should not be used as a comparison between hash functions. There are many different ways to fine tune those hash functions, and I wouldn't say that I kind of spent enough time to trying to figure out what's the optimal parameters for each hash function and so forth. So take it more as a directional kind of numbers. But at the high level, what this would tell you is that if you were to use the library right now, and you wanted to kind of create a stark, that would prove a membership within the Merkel tree of depth 16, for example, it would take you about 2 seconds to prove, and the proof size would be somewhere between 60 and 80.
03:59:26.598 - 04:00:08.950, Speaker A: Things that you can see here is that the numbers behave as you would expect for starks, because as an example, if you double the length of the computation, your proof time pretty much doubles, which is to be expected because it is linear. And then proof size doesn't grow nearly as much. This is the logarithmic portion, which is a nice property of starks. It just grows slightly with doubling or quadrupling computation and so forth. If you really paid attention to the proof times, you probably noticed that for 56 bit fields, the proof times are outrageously huge. And one of the reasons there is that because 128 bit fields are actually optimized for webassembly. So all the math is running in Webassembly, but 256 bit fields are running in JavaScript, which makes them almost ten x slower.
04:00:08.950 - 04:00:46.202, Speaker A: Okay, so future plans for the library. As I mentioned, the library is pretty new. I've been only working on this for a few months. There is a ton of things that can be improved, and one of the big areas of improvement is airscript itself. It has few nice constructs, but it's not yet powerful enough to define kind of everything that I would like to define with it. So one of the things, for example, I would like to have loops that would allow to create more kind of expressive statements within the language. And I've been thinking about loops for a while, and I think I have a good sense of how to implement them in an elegant way.
04:00:46.202 - 04:01:46.994, Speaker A: But more kind of like important thing I would say is a script composition, where the goal here is to create a structure where you can define airscript, let's say, for a hash function, and then you can import it into an air script that would define stark for a Merkel proof, and then you can import that into a different airscript that would define an even more complex construct. And the ultimate goal here would be to make airscript kind of powerful enough where you can write a stark prover in Airscript. So you can basically do a recursive proof of stark on airscript, and this would be kind of ultimate success if I can get to that point. Beyond this airscript itself, proof speed can be improved dramatically. I showed you the numbers there, but I optimized some things, but not nearly enough. As an example, in the Merkel proof Starks, 80% of the runtime for generating proofs is spent on evaluating constraints, which is basically just poor optimization on my part on how airscript gets converted into executable code that generates those matrixes. Now, there is a much better way to optimize that.
04:01:46.994 - 04:02:27.406, Speaker A: And if I spent some time optimizing it, the proof time would probably decrease by a factor of two or three. And then kind of the big thing with Starks in general is that they're hugely parallelizable. And right now it's all running in a single thread, and moving it to multithreaded environment would kind of give much, much more benefit to reducing the proof time. Beyond the kind of proof speed. There is a proof size, which is another important parameter, and there are a few things that I can still do on that front to improve the security of the proofs while keeping the size the same or reduce the size of the proofs. And I've listed a couple of them there. But really, in this area, I am not the expert.
04:02:27.406 - 04:03:14.242, Speaker A: So I'm kind of looking at startware guides and other people in academic community who can come up with a novel ways to reduce the proof sizes even further. So with that, it kind of covers what I want to talk about, and I'll open it up for some questions. So what do we think? Are there some questions? Give a second if there are. The mics are just over here. All right, well, if other questions arise later on, find me somewhere after the conference or during the breaks. All right, David. Oh, there you are.
04:03:14.242 - 04:03:40.550, Speaker A: Perfect. All right, so next up we have David Vorick from Sia. He's going to be talking about decentralization and cutting edge cryptography. So here it's going to be about how new cryptographic discoveries introduced new trade offs. So let's give him a hand. Thank you. So, yeah, I wanted this talk to.
04:03:40.550 - 04:04:37.526, Speaker A: Good. Okay. I wanted this talk to be a little bit more philosophical. As we start to introduce new things like starks and snarks into the ecosystem, we should take a step back and ask, should we actually be doing this? What are the implications of doing this? And so, from a super high level goal, I think we're all interested in these sorts of things for the purposes of decentralization. And so I'll be looking at a couple of things that we're doing from the lens of decentralization and as an overview. The goal of decentralized projects is to empower the individual and eliminate trust in external parties. So for bitcoin being the prime example, you can take a blockchain, verify it, and you don't have to depend on anybody else to know that everyone else in the world is seeing the exact same thing as you are on the blockchain.
04:04:37.526 - 04:05:45.330, Speaker A: And so we want to kind of ask the question, is there an external group that has power over me or can make changes to myself, my environment, my tooling, the stuff I depend on, the infrastructure against my will or without my consent? And so when we talk about decentralization, we're really trying to figure out all the different ways that people can, or groups or externalities can impact the way we operate. And of course, this ends up being a gradient. So you can have partial decentralization, or you can have full decentralization. And I want to look at the following four things from this lens. Which would be trusted setup, rolling trusted setup, cryptographic complexity, which is just like, for example, snarks are very complicated. How does this relate to decentralization? And then things like novel cryptographic assumptions such as class groups. So we're going to kick off with trusted setup.
04:05:45.330 - 04:06:46.070, Speaker A: And for the purposes of this presentation, I'm going to strictly define trusted setup as a ceremony with end participants, where so long as one participant in this ceremony is honest and faithfully follows the protocol, then the result is absolutely secure. And so I'm specifically talking about the construction where we need one person in the entire ceremony. To be honest, if we ask the question, is there an external group that can manipulate me or my experience, the answer is instantly yes. Whoever participated in the trusted setup ceremony has the power to, if they all collaborate together, they can manipulate you. And it's for this reason that it's called a trusted setup. We do have to trust the group to at least some extent. It's instantly not purely decentralized.
04:06:46.070 - 04:08:05.134, Speaker A: And specifically, I would say that trusted setup has a super counterintuitive threat model. Normally when I hear people talking about trusted setup and asking like, is the zcash trusted setup secure? Are these other trusted setups secure? They're thinking about what would have had to happen, which people would I have to not trust in order for this trusted setup to be broken. And I think this is a very bad way to think about trusted setup. What we should be asking instead is what would it take for a malicious group to work together and convince everybody that they have made a trusted setup, which everybody believes the trusted setup is secure, when in fact the trusted setup is broken. And so really, when you're thinking about trusted setup and the security model inherent to it, you should be looking for underhanded strategies and like sneaky techniques that people or an adversarial group could be employing to convince the population or convince a group that a trusted setup is good. So a couple of ideas that we can apply thinking from an underhanded sort of model is you can cherry pick individuals. If you have 20 individuals who to the general public, seem trustworthy or seem at ods with each other.
04:08:05.134 - 04:09:31.450, Speaker A: But you know, because you're backroom friends with all of them, that they are in fact willing to collaborate, you can cherry pick people. And so a group of like 20 prominent individuals is not a convincing trusted setup ceremony, because out of all the prominent individuals in the world, the malicious group can cherry pick the 20 that they know will work together to create a malicious trusted setup. And we really can't tell if that has ever happened. Trusted setups can also do things like civil attacks. So if there's a trusted setup with 10,000 participants, unless I personally know all 10,000 participants, there's some question as to how legitimate was this? How many of those 10,000 people were actually real? Or even if they were all real, how do we know that the trusted setup software was not compromised or not backdoor? Did everyone, before participating in the trusted setup, look at the software, verify the software, download it in the correct way? Do they even have the expertise to do such a thing? And this kind of is just scratching the surface. We don't really know what types of underhanded techniques for trusted setups exist. It's not something that's been super well researched, and it could be future pondering on this issue could reveal some very nasty underhanded tricks that could be employed to manipulate trusted setup.
04:09:31.450 - 04:10:15.618, Speaker A: So I think I'll go ahead and skip this slide. Actually, I will cover this. So after you have a trusted setup, your trusted setup confidence can only decrease over time. Once you've chosen to trust a group of people, you can't reverify them. If they cheated, you may never find out. As time passes, as distance grows within between the trusted setup ceremony and the present, confidence actually degrades over time. And as the groups grow, as you try and expand, your trusted setup ceremony looks increasingly fragile.
04:10:15.618 - 04:11:31.630, Speaker A: Going backwards so trusted setup also introduces these systemic issues. An ecosystem with multiple trusted setups faces an exponentially compounding degradation of security. Basically, if we really blow it up, and we say that our general stack depends on 10,000 different trusted setups, and each trusted setup we have 99.99% confidence in, that means that the ecosystem as a whole is almost certainly compromised, and the whole ecosystem is bad. If it all depends on each other and one of the trusted setups in the ecosystem is broken, you really don't want to end up in that situation. So I don't like the idea of going piecewise from trusted setup to trusted setup to trusted setup, and deciding, okay, well, we looked really closely at the zcash one. The zcash one is probably okay, because now if zcash starts to become something that the ecosystem depends on, or if that trusted setup becomes something that the ecosystem depends on, that just has systemic implications, especially if you start to do that repeatedly with five or ten or 15 or 15,000 different trusted setups.
04:11:31.630 - 04:12:28.770, Speaker A: So for all of these reasons, I think that, just like point blank, trusted setup should never be considered acceptable. You only need one broken trusted setup in your general purpose ecosystem stack to erode the security of your entire ecosystem. Especially if the trusted setup starts to get used as building blocks for higher level applications, which I think is almost inevitable once you have a primitive. People really want to build using it, people should build using it. And in the terms of trusted setup, I think that this just means the systemic risk gets out of control very quickly. And so I just underscore one more time, like a single trusted ceremony. Static trusted setup should never, ever be considered secure, and we should just throw all of them away and view them as a research body and not as a set of applications.
04:12:28.770 - 04:13:33.670, Speaker A: Thank you. Okay, so I'm going to move to the next one, which is rolling trusted setup. So I'm going to define for this presentation a rolling trusted setup as a trusted setup, where you can join at any time something like sonic. And once you've joined it, as long as you are honest or as someone else joins it, as long as one person in the entire history of the rolling trusted setup is honest and successfully participates in the protocol, the trusted setup, from that point forward, is forever secure. So this is obviously better than a static trusted setup, in that if I'm late to the party, I can convince myself that it's secure. Just by participating, we kind of move back into the model that bitcoin operates under, except that it does not give me any confidence in preexisting operations. So, for example, if we're doing something like zcash, actually, I think this is true of zcash.
04:13:33.670 - 04:14:17.750, Speaker A: If the trusted setup breaks, you can have infinite inflation. If it's a rolling trusted setup and I join after the fact, infinite inflation may have happened before I joined, and therefore the entire system can't be trusted. So you have to be really careful. And then people talk about combining something like sonic with turnstiles, which I won't get into, I would say maybe, but that's complicated. I don't like it. Generally speaking, you should not trust the rolling trusted setup until you've participated in it. And then once you've participated in it, you should treat everything that happened prior to you participating in it as completely broken.
04:14:17.750 - 04:15:13.190, Speaker A: And I think that this severely limits the utility of rolling trusted setups. So generally speaking, similar to static trusted setups, I would strongly advise against them, but I wouldn't say they're strictly no good. If you do more complicated, elaborate, like rolling trusted setup plus turnstiles, plus some other overheads, maybe it's acceptable. So I won't say it as absolutely as I said it for a static trusted setup, but I do think that we should just, trusted setup is just not a good idea. We should stay away from them. Okay, so that's probably like the harshest part of the talk. Now I'm going to be moving on to cryptographic complexity, which is, for the purposes of this presentation, I'm going to be saying that it's a measure of how likely it is that a cryptographic system contains a mistake or unnoticed security vulnerability.
04:15:13.190 - 04:16:24.890, Speaker A: So, for example, if a year from now, some researcher is looking at Starks and just goes, oh, this has been broken the entire time. Look at this mistake, and everyone kind of agrees, oh, man, starks have been broken this entire time. So we're looking at how likely is it that someone finds this mistake in Starks in the future versus how likely is it that starks, as they are today, are completely perfect, completely secure? All the proofs are ironclad, bulletproof. So especially for larger papers, it's not uncommon for proofs to be found incorrect or the threat model to be widened or some mistake in the execution of the crypto system to determine that it's been insecure the whole time and that we need to upgrade it. So I would say that this is similar to trusted setup in that it represents an opportunity for external parties to manipulate you. It's a way for someone to cause infinite inflation or to just cheat. And so we should be careful.
04:16:24.890 - 04:17:25.598, Speaker A: With really complex crypto systems. Unlike trusted setup, though, with trusted setup, as you get further away from the ceremony, your confidence goes down over time. But with complex systems, as we get further away, as they get more review, our confidence in them grows over time. And so this really complex, really what I would call initially, like, toxic ball of cryptography, over time, as we understand it better and we get more familiar with it, it can actually turn into something that we start to really depend on and can safely really depend on. And so I guess three big ways that you can chip away at the scariness of a complex crypto system is one increase review. The more people who have looked at it and failed to find a mistake, the better you feel about the system and the more likely it is that it doesn't contain a mistake. And then you can build new analytical tools, things like formal verification.
04:17:25.598 - 04:18:26.598, Speaker A: And so you can take, say, pieces of the proof or pieces of the system and just analytically produce, like, a clean computer proof that this is definitely correct. And then finally, you can get new mathematical techniques that give you confirmations that this thing that we proved one way, now we have two or three other ways to prove it. All these sorts of things build confidence over time that our complex system is correct. And so definitely unlike trusted setup, I wouldn't say you should stay away from complicated systems. I think they're worth a lot of energy, and over time, they become less scary. And so the question then becomes, how scary is something today, and when should we be comfortable jumping in and using it? So I am just going to give, oh, okay, I'm going to skip this slide. So I would say that most of the new cryptography that cryptocurrencies are considering is substantially under reviewed relative to how safe it should be.
04:18:26.598 - 04:19:32.154, Speaker A: Just if you keep an eye on the headlines and the papers that come out and you see, oh, this EC DSA construction had a break in it, or you find, as researchers break things, I would say that cryptocurrencies are taking on. We come back to the systemic question, a lot of systemic risk, because they're using a lot of building blocks at once. They're using a lot of novel techniques at once. I think the risk is very high, and that if we're going to be considering running the entire banking system on this, we need to slow down a lot on how quickly we are accepting complicated cryptography into our toolkits. And so I think most of the new cryptography that's being introduced should really only be used in contexts where a full break would be considered acceptable. And so if you're using cryptography like starks to prove that there are a finite number of coins, you don't have infinite inflation. This is a good example of something I would be very afraid of.
04:19:32.154 - 04:20:05.890, Speaker A: And I would not consider that a production ready system just because of how new the cryptography is. So I'm going to call out starks specifically as probably being just strictly too early to be considered safe for production use. And like I said, this will change over time. The further we get along. The more people who get interested in starks, the more academics that review starks and don't find a mistake, the weaker that my warning becomes. So I definitely think that this is something that. A direction that we should be moving.
04:20:05.890 - 04:20:42.682, Speaker A: And I'm, like, a big fan of Starks, but I would say it's too early to deploy them in production bulletproofs. I really waffled on. I think I'm going to actually change what I said in the slide. I think they're probably not okay. And what I wasn't considering when I made this slide is the systemic risk. And so if we take multiple constructions that are as risky as bulletproofs, we really don't want all of them in place. And I think because you use lots of constructions, Schnorr signature, Schnorr threshold, things like taproot and graftroot.
04:20:42.682 - 04:21:16.730, Speaker A: And this is at the bitcoin level. Because we have so many just new cryptographic constructions that we're starting to put into production. I do think that things as complicated as bulletproofs. I feel like it's likely that if we have ten different things as complicated as bulletproofs, the chances that one of them has a critical mistake in it are unacceptably high to run a banking system on. So, yeah, I'm going to say bulletproof is also not okay yet. Okay. And that brings me to the final phase of this talk, which is novel cryptographic assumptions.
04:21:16.730 - 04:21:59.660, Speaker A: So the example would be like, a classical example of a cryptographic assumption is the discrete logarithm problem. So I would say that novel assumptions should be seen as very scary. Mathematical assumptions often take decades or even sometimes you get these problems or conjectures proven or disproven, like 100 years later. And a lot of times the proofs that come out are very surprising. And so sometimes you get a very surprising result to a long thought, reliable conjecture. And so, for that reason, I would say cryptographic assumptions are scary. You should be really careful around them.
04:21:59.660 - 04:23:02.890, Speaker A: And so I would call out class groups specifically as something that's like, even if an assumption is many decades old, if no one's been looking at it, the age doesn't really count for anything. So you want to make sure that you're using assumptions where lots of very smart people have considered the problem for long periods of time. And you've had specialists with a decade of linear thought on trying to dissect an assumption and failing to come up with anything meaningful before you want to start depending on it. So, yeah, I would call out class groups is probably, like, even further away from than starks by a substantial amount from being acceptable. And this is me not knowing too much about class groups, for what it's worth. But as a new assumption, something that didn't even have code written around it a few years ago, I think is just, to me, it's very scary. And I think we should be avoiding things like class groups in our cryptography.
04:23:02.890 - 04:23:35.574, Speaker A: And so this follows the same thing as the complexity. Over time, we can get more comfortable with it. If we get really lucky, someone will come out with a nice, clean mathematical proof. Hey, class groups are okay. If you have a nice mathematical proof that the assumption is true, it disappears as an assumption, and then on the bad side, it might break. So I would say, just in general, cryptographic systems only gain in complexity. We only gain primitives if people are actively working on it.
04:23:35.574 - 04:24:11.300, Speaker A: This is true whether it's like starks, whether it's class groups. And so even though some of the things I said were kind of discouraging or slowing down in the presentation, I just want to emphasize that I'm really excited about everything that's coming out. I think that all the energy going towards trusted setup aside, this new cryptography is really good. And I think in production, we should be slowing down and being more conservative. But all the energy that we're spending on research is, like, really well spent. All right, I'm just going to jump to questions. I am out of time.
04:24:11.300 - 04:24:55.962, Speaker A: It's. If you have questions, it would be great if you go over to one of the mics on the side. Yeah, it's too far. I can run into the middle. I was just wondering, how would you define complexity? Because things tend to be complex at one point, and then a couple of years later, they teach them to undergrads. Yeah. So, for the presentation, I specifically defined complexity as how likely something is to contain a serious mistake.
04:24:55.962 - 04:25:39.540, Speaker A: And so this obviously has an inherent assumption of how familiar everyone is with it, how much tooling is around it. So complexity doesn't necessarily mean that you can't have a thing with 900 parts if all 900 parts are super well reviewed and analyzed and have been part of the toolkit for a long time. That's not complexity under my definition. My definition is purely a probabilistic analysis of how likely something is to have a mistake that we haven't found yet. Thank you. My question is, I guess today we learned we live in a Cambrick explosion. Hey, I'm here on the other side.
04:25:39.540 - 04:26:19.802, Speaker A: I see you. Today we learned we live in a Cambridge explosion. And I love this picture, basically. And I love competition, because this is basically the best thing what man ever invented. And still you say we have basically too much innovation and we should stall some of them in production systems, which I disagree, because in order to check all those crypto economic incentivization, we need skin in the game. We need people to fail and lose money and do whatever mistakes there are. So basically, we should encourage everybody to jump into the pool without knowing what's in the pool.
04:26:19.802 - 04:27:10.490, Speaker A: Basically, yeah. So I think of a lot of the stuff we're building as deep infrastructure. So I might make a comparison to, say, building a bridge or a skyscraper. And so if you're talking about a cambrian explosion of new bridge building techniques, and then you're essentially urging people to come cross the bridge before we know it can bear load, I think that this is very risky and irresponsible. And so I'm definitely not discouraging people from developing new bridge techniques, but we should be more confident in the math behind the bridge and the physics behind the bridge. And we should be load testing before anything of importance is using the bridge on a daily basis. And so, yeah, I think that would be my opinion.
04:27:10.490 - 04:27:41.478, Speaker A: It's not that I'm discouraging the innovation, it's that people should not use it until we've verified that it's actually not going to collapse. There's a question over there. Well, I think it's an impossible question to answer. Impossible problem. How you can trust a mathematical assumption. It will takes many years, I would say almost impossible. But we have this discussion in the context of post quantum cryptography.
04:27:41.478 - 04:28:12.034, Speaker A: So where we are going to replace a current signature algorithm. And so, of course, we are afraid because it's a basic. And so what is proposed in this context is to do ebrid techniques. So when you do a signature, you do a post quantum one plus a classical one, so you have a backup. I don't know if we can do this with Stark, but in terms of agility and trying new things. It's a good way to move, I would say. Yeah.
04:28:12.034 - 04:28:55.218, Speaker A: So I think that comes down to systemic risk. I'm personally a big fan of the pre quantum, post quantum multi sig, for example, in bitcoin. I think it would be great if bitcoin had support for lamport signatures. That way, if quantum computers come out, everything breaks. We can just institute a soft fork that says no pre quantum signatures are allowed ever. If we know that most outputs also have tap rooted, hidden in them, a lampport signature, then people can still spend their money, even though this catastrophic failure has happened. In terms of starks, you have to be really careful on what the failure mode is.
04:28:55.218 - 04:29:38.942, Speaker A: So, for example, if your failure mode is that infinite inflation becomes possible and there's no way to go back. If you have 500 transactions in your history, and any of the 500 transactions could either be non inflationary or infinitely inflationary, the whole system becomes broken. So I think it's a good idea to try and balance having a failover. If this assumption fails, we all fail over to this new way of doing things. But you have to make sure that if you're going to be doing that, the failover is actually successful and there's not some critical meltdown that occurs. So I believe we're now out of time, actually. So we are going to stay.
04:29:38.942 - 04:30:03.926, Speaker A: Anyone who's here should probably stay here. We are ahead of the other room. Okay, so we're not out of time, but I don't know if you want to keep talking. I'm glad to keep taking questions. All right. Do you have an example of an assumption that was relied upon for and then that was broken after 100 years? So I wish that I had the name of the paper. I don't know, about 100 years.
04:30:03.926 - 04:30:32.800, Speaker A: You'd probably have to go into the mathematical space. But in terms of cryptography, I know that there were. I don't have the paper on the top of my head. I believe some ECDSA schemes that were all thought to be secure and then, like, one paper came out and broke something like close to a decade of ECDSA work. So if you find me afterwards, I can pull that up for the 100 year one. I don't have an example. Off the top of my head.
04:30:32.800 - 04:31:43.842, Speaker A: I actually have a question. Do you know of any trusted setup that actually was compromised? Like any public, proper trusted setup? So it's kind of a stretch, but NIST did that whole elliptic curve, random number generation thing, right? So some of the NIST stuff, which I would consider NIST published cryptography, to be trusted was actually backdoored, and they got it through the whole standards process. The difference between that and trusted setup is that that was really brazen, because if you get caught, you can just prove it and you're like, look, this has been broken. It's always been broken. Trusted setup doesn't need to be. That is much less brazen, because if you do the same thing in a trusted setup, there's no proof that the trusted setup was compromised, or it's very difficult to come up with a proof that someone has done this. So I would say if we've had backdoors at the NIST level, we will definitely have backdoors at the trusted setup level when trusted setup reaches international scale.
04:31:43.842 - 04:32:37.746, Speaker A: And that's why stay away from trusted setup. So pardon my rudimentary understanding of the sort of underlying security here, but do trusted setups assume that the hardware on which the setup is being done is trustworthy? That's a great question. It depends on the trusted setup model. For example, one of the things you can do is you can get like say, four processors from four different manufacturers and have them do a multiparty computation between each other. So if that's part of your trusted setup now, not only do your trusted setup participants have to collude, but also the processors have to collude. In short, the answer is yes. Ultimately, if you don't trust your hardware, the hardware is almost the ultimate trusted setup.
04:32:37.746 - 04:33:37.418, Speaker A: And I think it's something that we're going to have to face more head on in the future. For example, the most popular hardware wallet ledger is closed source. Is that really a great idea? I would say no, it's not a great idea. And I do think the hardware problem is a lot more significant. What is working to our advantage is that hardware is insanely challenging. Most super advanced cpu techniques can be explained in like a 30 or 40 minutes lecture, because going from something complicated to going to a perfect gate level implementation is just a process that humanity has not figured out how to do very well at this point. So the one thing in terms of trusted hardware that's running in our advantage is that designing a backdoor to put on a circuit is super nontrivial, just because hardware does not easy to make complicated hardware, but it's nonetheless possible.
04:33:37.418 - 04:34:27.582, Speaker A: And it's something I say we should be a lot more concerned about. One of your concern with the static trusted setup is that you could have a cabal of people who kind of collaborate, and the way you should think about it is what is the probability that such a cabal can form. What if you have a static setup where it's open participation for a period of time? So let's say for a whole year, anyone can participate. And you'd use, let's say, a censorship resistant platform like bitcoin to register interest as a participant. It would still be a static setup. And would that still be not trustworthy? Yeah. So I would look at all the other underhanded techniques that could be used.
04:34:27.582 - 04:36:01.590, Speaker A: For example, what's the technical barrier to participating in the trusted setup? If you want to participate in the trusted setup with an alternate set of software, let's say I don't trust the software that's being given to me. Is there a way to do that? Are we doing things like build system integrity? So do we know that the trusted setup software was compiled with libraries that were trustworthy? So even if we have something like Gideon and deterministic builds, Gideon depends on libraries that come from Ubuntu. And especially once we get to the more international scale, we have to wonder, are these build systems trustworthy? And so I think that there is no silver bullet for trusted setup. There's no single technique that can give you confidence in a trusted setup. Purely because I think that if you really dive deep into all the underhanded strategies available, you find a lot of really scary things, and it seems like a ball of bad ideas. It seems like this argument applies to many other things, not just trusted setups, but the key that differentiates trusted setup from everything else is that we can't go back and check it later. So with something like starks, if we don't trust the build system for compiling our stark verifier later on in time, we can redo the build system, or we can use new techniques.
04:36:01.590 - 04:36:36.722, Speaker A: We can go back, we can make a new verifier, and we can say, okay, these are still equivalent. Like, we can go back, double check and reconvince ourselves continually that the system that we built does not have any issues with it, whereas with trusted setup, once the deal is done, you have no idea if there was an issue in the process later. And it's not something you can go back and double check. Thank you so much for doing this. Extended Q A. Let's give you a hand. All right, so I'd like to introduce our next speaker.
04:36:36.722 - 04:36:58.062, Speaker A: Balaji is going to speak about. The talk is called towards a pseudonymous bridge. I'm very curious to hear about this. So welcome to the stage. Thank you. All right, let's see if this works. Should I use this one? All right, great.
04:36:58.062 - 04:37:34.582, Speaker A: Let's move this one out of the way then. All right, so one thing I've been thinking about for some time is that pseudonymity is as important as decentralization. We talk about decentralization a lot. A network that is split across many different jurisdictions. It's hard for one country to shut down, for any one entity to shut down. But pseudonymity is also quite important because it means that even if the network is decentralized, you can't target the person who is creating it if they're pseudonymous. And there's other reasons for it as well.
04:37:34.582 - 04:38:22.918, Speaker A: Like if you think about the founding of the United States of America, that was done by actually the federalists or the folks who wrote the federalist papers, which are all pseudonymous. There's many important ways in which pseudonymity has allowed people to speak freely about things that otherwise would be attacked under the real name. And so what I'm going to talk about today is sort of a high level construct for how we might be able to transfer some reputation to a pseudonym. And just to motivate this a little bit with something like Zec or more generally, zero knowledge, we can now transfer wealth to a pseudonym. And the question is, can we transfer reputation to a pseudonym? And we need some definitions then, of what reputation is. So right now, one way of thinking about, can we see the cursor up there? Okay. It doesn't.
04:38:22.918 - 04:38:50.226, Speaker A: All right, fine. Nice. Okay, so right now we have basically a couple of choices. You can set up an account that has, this is your main account, and it has zero anonymity. And I'll define this in a little bit, but zero anonymity in all of your followers, all your reputation. Or you can set up a new pseudonymous account, and that has 100% anonymity, but it has none of your followers. Right.
04:38:50.226 - 04:39:33.374, Speaker A: And that's kind of the trade off. And one way of thinking about this is this is your main account, your Mark Anderson, who will be my running example here. And this is Mark's zero follower pseudonyms account, where he has to start completely from scratch. The question is, is there something in the middle where we can, with a construction, reduce your level of, have maybe fewer followers in your full thing, but have somewhat less anonymity, but yet enough that you still feel anonymous. Right? Okay, so first, let's quantify anonymity via this concept of 33 bits, right? The idea is that two to the 33rd power is about 8.6 billion, and 8.6 billion is bigger than 7.53
04:39:33.374 - 04:40:12.074, Speaker A: billion, which is the population of the world. So that means that with 33 bits of information, you can uniquely identify any person. Right? And what that does, it turns your degree of anonymity into a measurable, a quantifiable thing rather than a zero run. Rather than just being anonymous or not anonymous, you can say, how anonymous are you? Well, how many bits of information do I still need to take you to? Just exactly one person. If it's ten bits of information, then you're one of 1000 people. If it's 20 bits of information, you're one of like a million people. Just to give some stats, as I mentioned, 230 3rd is about 8.6
04:40:12.074 - 04:40:26.426, Speaker A: billion. The world population is 7.5 billion. Two numbers that will be useful to us. The Twitter maus monthly active user is about 330,000,000, and the Twitter verifieds are about 330,000. And so this is like the number of bits in that set. And it's a very simple calculation.
04:40:26.426 - 04:40:45.240, Speaker A: It's log base two of n. We define it as the anonymity, the number of bits. So 28.3 bits is how much anonymity you have. If all I know about you is you're a Twitter MAU. Okay, now one way of thinking about. So this is quantifying the anonymity side, quantifying the reputation side.
04:40:45.240 - 04:41:29.454, Speaker A: Well, we know that with buying Zec on Coinbase. Coinbase.com, you heard of it? Okay, if you buy Zec on Coinbase, you can send to a shielded address, and you can get 50k zec with a pseudonym and 50K USD in your normal bank account. So you can use the wealth you've earned under your real name to bootstrap the wealth of a pseudonym. Can we come up with a construction that's sort of similar, where we take, let's say, some of the followers over here, and bootstrap a pseudonym that has some of those followers. Is that possible? Because that's what I mean by transferring reputation to a pseudonym. So another kind of data structure or concept, which we'll need before we proceed, is just this idea of an adjacency matrix.
04:41:29.454 - 04:42:45.914, Speaker A: And you may all be familiar with this from graph theory, but if this is you and your mark, and you've got followers over here, and here's another person who's following one of your followers, this adjacency matrix is basically defining, in this case, as Mark is followed by this person and this person, not this person, by this person and this person, and then she is actually following her, and that's that edge over here. Right? So it's just a binary matrix of adjacencies. Right? Now what we want to do is we want to see, okay, given those definitions, can we trade off some anonymity for some followers? So the naive approach is you set up a new account, your little pseudonymous account over here, and you try to preserve all of the same followers that you had over here. Right? You have something where you can send over all of those followers, your entire account. And of course, the issue with this is anybody who's looking at the full adjacency matrix can see that. Oh, and by the way, these are not totally separate networks. The right way to represent it or think about it, is really something that looks more like this, where you set up a pseudonym, and then you had your entire neighborhood.
04:42:45.914 - 04:43:32.290, Speaker A: All the people who are pointing into you are now pointing at that pseudonym. And the issue with this is anybody who is, like, scanning the entire graph, the matrix of all connections can immediately see that this vector of followers and this vector of followers are the same. And so therefore, you're basically just, like, reduced to one person. It's unlikely anybody else has exactly the same pattern of followers, and therefore you have given up your anonymity. Right. Okay, so naively just transferring all your followers to the new pseudonymous account, that may not work. So what else can we do? What if we back up and we say, all right, not just think about your followers, but all the profile information more generally, there's really kind of three types of user profile data on Twitter.
04:43:32.290 - 04:44:29.758, Speaker A: There's platform specified stuff like your verification checkmark, which Twitter itself generates. There is user specified stuff which is like your name and your address, your bio, et cetera, which you are typing in. And then there's stuff that is specified by others, which is your followers. And then also every, like every RT that comes from other people, right? It is the stuff that is specified by the platform and by others that's the most reliable to maybe to do something with because it's harder to fake for an individual. And what you can do is you can take this idea of, here's your new account, your pseudonymous account. This is one of the things I was mentioning towards the end. But if you assume a crypto Twitter, that is to say, if we have something that's like, if you heard the saying, assume a spherical cow, it's from physics, right? Okay, whatever.
04:44:29.758 - 04:45:16.174, Speaker A: I thought it's funny. So if you assume a backend where you can do a construction like this, where you can do a zec, like, transfer, maybe that's like twitch or something like that. You can move that verification over here to the pseudonym. And now this thing actually is a verified checkmark over here. You've moved data in a similar way to how, with zksnarks, you move money from one zec address to another. And what does that give you? Well, what that gives you is some progress, because here, this was just under your full real name. So you were just one out of one people.
04:45:16.174 - 04:45:38.740, Speaker A: If you just set up a pseudonym with no followers, you're one out of 330,000,000 Twitter users, but you had no followers or anything. Now, when you're verified over here, you've given up some anonymity. You're one out of 330,000 verified. So you have 18.3 bits. We lost ten bits of anonymity to put a verification flag on this profile. Right.
04:45:38.740 - 04:46:34.470, Speaker A: But what did that gain us? Well, let me introduce one more new concept, which I call autofollow. Okay, so autofollow would be a very useful feature for this, let's say, future crypto Twitter. And the idea behind autofollow is you'd set a flag in your profile which says, okay, I'm going to auto follow any of these new pseudonyms that have a verified flag, or any new pseudonym that can prove it had more than 10,000 followers with some other pseudonym, or any new pseudonym that was followed by adjack, right? Something like that. Where you're auto following these notable accounts because you think that they're going to say something interesting that they wouldn't otherwise say. So in this example, we have these three people all set up to auto follow someone who's verified. And notice that this lady over here is not actually following Mark Anderson. She's just set up to auto follow somebody who's verified.
04:46:34.470 - 04:47:41.334, Speaker A: So if you have a normal pseudonym, nobody's following them. But if you have a verified know, you do that construction and you move the verification over, well, then autofollow kicks in, and boom, these three people are now following them, right? And so this is kind of interesting, because now what we can do is we can say, okay, that gave up ten bits of anonymity when we did this verification, but in return, we gained three followers for that pseudonym. Right. Now, just to plug in some random numbers here, I showed just five nodes over here, but mark has, like, 700,000 followers. So let's just assume that there were 150,000 accounts that would auto follow after this verification step. So in that example, we would go from a situation where either Mark had his full account with 700,000 followers and zero anonymity. Or he had a pseudonym that was starting from scratch and it had no followers but total anonymity.
04:47:41.334 - 04:48:15.378, Speaker A: Now he has the option of something in the middle, okay? Where he uses zero knowledge proof to move over some information to the pseudonym, namely that he is verified. That drops the degree of anonymity by ten bits. That's a reduction in anonymity from porting over the verification, but in return gains all of these people who are auto following any new verified account. Right. So that's actually pretty interesting. This is a relatively small tweak to Twitter. The auto follow concept.
04:48:15.378 - 04:49:24.762, Speaker A: Again, I'm not necessarily thinking Twitter itself would implement this, but if you could do a Twitter like thing on the blockchain, which a lot of people are working on, this would be a very useful feature. Now, you can generalize this, because while I talked about verification over here, there's lots of other digests that you can move over with zero knowledge. For example, you can have the assertion that you have more than 100,000 followers, or you're followed by adjack and other things like that. Anything that does not in and of itself reduce you to an anonymity set of just one you can potentially port over. And before you do that attestation, before you port this over, you can see how many bits of information you're giving up in terms of anonymity and how many new followers you would get with auto following. Okay, so this is kind of like a high level sketch of how we might be able to transfer reputation to a pseudonym, just like we've been transferring wealth to a pseudonym with zec. In terms of next steps as know summer serial cow assuma crypto Twitter, a lot of people are working on things like this, and this gives an interesting sketch of some of the APIs, some of the functions you'd want to call.
04:49:24.762 - 04:50:11.844, Speaker A: You'd want to be able to make these kind of zero knowledge constructions. You'd want to have autofoll, you'd want to have a few things like that. But the broad idea is that if we set this as a goal of a pseudonymous bridge to bridge between your real name's reputation and the pseudonym's reputation, I think this at least sketches some directions for that, and it can say what could go on chain for a crypto based social network that would support this kind of thing. So thank you very much. I guess I can take some questions. Sure. So thank you for the talk.
04:50:11.844 - 04:50:39.656, Speaker A: Quick question. Are you assuming in this model that you somehow could limit the number of pseudo anonyms that a person could create for themselves. Or if it can be like, if I can create 10,000 verified accounts for myself, now I have an army of kind of reputable people on the platform that is actually represented by a single individual. Yeah, great question. So I think a couple of thoughts relate to that. One is you could make it costly to create a pseudonym. It might cost you zec or money of some kind, right? So that's one solution.
04:50:39.656 - 04:51:30.176, Speaker A: The second is that it might actually be good for people to be able to have multiple pseudonyms so that, for example, one of them you say, hey, okay, I'm followed by at Jack, but the other one you say, hey, I'm followed by this geophysicist. Right? And so then you can talk to the geophysics community over here without being character assassinated or whatever, right? So I'm not against having multiple pseudonyms. I think that's reasonable. You may want to tax it to prevent abuse of the kind that you mentioned. Yeah, very cool construct. I have additional question. How do you handle modification on the attributes of the main id? Like let's say mark goes below 100K followers.
04:51:30.176 - 04:52:10.080, Speaker A: How does it goes for the other? It's a great question. And so what I've thought about is that you just have it be a one time transfer and then auto follow kicks in, and then if it's out of sync, it's out of sync. That's the simplest way of doing it. And I don't think you lose too much if you do that. Instead of, I guess, an alternative to taxing pseudonyms, I guess you could also use linkable ring signatures to limit the ability to have multiple pseudonyms. Yeah, that's a good comment. Yes, that's true.
04:52:10.080 - 04:53:34.508, Speaker A: Have you considered there's another alternative, making the full graph of followers private so that you could just fork your following to a new account, but people couldn't see that map that you were showing earlier. That might be possible, but I think that if you had a crypto Twitter, a big part of it would be allowing folks to be able to download and view all of that data, because it might be possible to keep all of your followers private somehow. But it would seem to be different from how Twitter currently works and different enough that I'd have to think about it. I'm really struggling with the auto follow part. If everyone's following everyone, doesn't that break down the signal versus noise? So it's not everybody following everybody, but you would set so part of the attraction of a platform like this is basically Twitter, except folks would set up these, like, for example, a verified account that has at least one other account associated with, that has 100,000 followers. And you'd have a threshold for how significant those anon accounts would have to be. Right? So you wouldn't just auto follow every new account that came out.
04:53:34.508 - 04:54:09.636, Speaker A: You'd say, okay, I'm going to auto follow somebody who's got 100,000 followers and is verified, but somebody else might have a higher threshold and say, I'm going to only auto follow a new pseudonymous account that has 500,000 followers is verified, is also followed by at Jack. Right, but how's their at Jack if it's anonymous? Because at Jack follows like 4000 people. No, but there is no at Jack in new Twitter where everyone's anonymous. There's no at Jack. Oh, what I mean by that is if you had a, let's. I'm using adjack as a signal for a high profile account on Twitter. As you know, you've got both named accounts and you've got pseudonymous accounts.
04:54:09.636 - 04:54:34.336, Speaker A: Right? So if a high profile named account follows your named account, that's actually something that you can porter to the pseudonym. My named account was followed by a high profile person. Therefore this pseudonym is actually valuable. It's one of 4000 accounts that adjack followed. Does that make sense? Well, so it's not new Twitter, it's hybrid Twitter with an anonymity. Yes, that's right. Basically, you might have to actually rebuild a whole piece of Twitter on there, but.
04:54:34.336 - 04:54:46.004, Speaker A: Yeah, that's right. I think I had, there's one over on the other side. Other side? Oh, yeah, go ahead. I can't see where you are. Wave or something. Okay. Yeah.
04:54:46.004 - 04:55:48.992, Speaker A: So if I followed just Mark and if I then got auto followed, or rather if Mark got auto followed by me, I would know that it was Mark, right? Yes, that's actually an important point. Basically, before you actually put out the attestation, you don't want to put out an attestation that knocks down all 28 bits of information and something that, like you're describing, if Mark only followed one person and you put out my pseudonym is followed by at mark, then you would know before you put out that asstation that would give up all your anonymity. So one of the features of this is with the auto follow thing, you know, first, how many followers you're going to get for a particular asstation. And second, with the 33 bits model, you know how much anonymity you're giving up for an attestation. You wouldn't want to give out an attestation that gives up your full light entity. One or two more questions, I'll grab the opportunity. Have you thought about refining this auto follow? Because where.
04:55:48.992 - 04:57:04.948, Speaker A: Because you could potentially prove that you follow some of your original following and make that proof, and then depending on how much of them you'd force to follow you, you'd reveal your identity more or less, because the auto following is just identical to being verified on this new crypto Twitter, right? Yeah. So the way I was thinking about autofollow is at the time that you sign up for the platform and get set up, you have a bunch of checkboxes, and it's opted into an aggressive auto follow where you're auto following these new accounts that maybe everything that's verified and has more than 10,000 followers. And then some people will mess with those defaults. But the defaults are for aggressive autofollow. And what that does is basically it says that pseudonyms that do these attestations will actually get a lot of followers. That's essentially what it's engineered to do. I'm open to other constructs that can get you there, but the basic idea is this way you can, before you do an attestation, quantify how much reputation you get, how many followers you get for a given level of decrement and anonymity.
04:57:04.948 - 04:57:40.356, Speaker A: And there's possibly other ways to do it. But that's one thought. I have a question. Go ahead, I'm over here. I was wondering. So the whole idea is based around the verified accounts, but is there an idea that the verified accounts would actually act better or more ethically or not lie? Yeah, great question. I just use verified accounts as one kind of piece of data that you could bring over, but you could have an account that wasn't verified that had 10,000 or 10,0000 followers or those followed by a particular person.
04:57:40.356 - 04:58:05.864, Speaker A: The key is just that it can't be data that that profile itself was generating about itself. Verification is something imposed by the platform. Your followers are coming from something else. So it has to be some kind of data like that. I think we got one more. Okay. My understanding, this is like a theoretical construct form, practicing the idea of today in the median and how you can transmit through reputation.
04:58:05.864 - 04:58:48.900, Speaker A: What's your incentive and what's your motivation to work on such an idea? Great question. It is sort of like the other part of what Zec lets you do. Zec lets you move money to a pseudonym in the medium to long term. I think that we want to move towards a pseudonymous economy where you have one name for earning, another name for speaking, and you use your real name, like your Social Security number, only in official context. And this is sort of like an hd wallet. You don't keep all of your balance necessarily in just one address. You split it into things, and you can have a root address that generates things downstream the same way you have your real name and you generate all these other names for different purposes.
04:58:48.900 - 04:59:18.564, Speaker A: And we kind of already do this, like, on Reddit, people will use this pseudonym or that pseudonym as it suits them because they don't necessarily want to give out their global identifier for all purposes. Right. And so the idea here is one thing that deters people from using pseudonyms in social media is that they have to rebuild their audience completely from scratch. If that was not the case, then I think you'd see a lot more interesting content out there. People could speak more freely. So that's part of the motivation. Thanks.
04:59:18.564 - 04:59:44.136, Speaker A: Thank you so much for the talk. Did I. All right, so there's a bunch of people back there. There's a lot of seats free if you want to take seats. We have a panel up next. This is a panel called L1 infrastructure, business models, and compensation. I want to invite the speakers to the stage.
04:59:44.136 - 05:00:32.290, Speaker A: The participants, Matt Wong from paradigm will be the moderator. Cool. Arthur Breitman from Tezos will be on this panel. Zachie Manyan from Cosmos and Vitalik Buterin from Ethereum. All right, folks. Hello, everyone. My name is Matt Huang.
05:00:32.290 - 05:01:01.874, Speaker A: I'm a partner at Paradigm. We're a crypto investment firm. But today I'm here to ask some simple questions. I think the panelists here probably need no introduction, but maybe we can start with each of you articulating how you think about your role in your ecosystem. We're working with decentralized systems. I think a lot of people think of you as leaders in your respective ecosystems. How do you guys think about it? Maybe.
05:01:01.874 - 05:01:39.118, Speaker A: Let's start with Zucky. I don't know. I'm probably the newest to having a decentralized ecosystem that was live. Still figuring it out. Launch was my sort of thing last year and early this year, and now it's sort of this mass chaos of things that are going on and trying to play a facilitator leader, figure out how to go to market with IBC role right now. I like the word facilitator. I think I've also used the word yenta before.
05:01:39.118 - 05:02:03.814, Speaker A: Essentially. I meet a lot of people who are interested in building into those who are billing on tezos. And sometimes they'll reach out and say, like, oh, I'm doing this, where are you thinking? And so on, so forth. And I think the most useful answer that I can give to all of this question oftentimes is like, hey, this other person is building something similar or something complementary. You should meet each other. And so I do a lot of that. I don't know.
05:02:03.814 - 05:02:31.760, Speaker A: I do research. I talk to people. I do a bunch of stuff. Thank you. So, could we get a show of hands? How many people in the room are building infrastructure on top of or around l one? And keep your hand up if you'd like a foundation grant. Okay, talk to our panelists after. So maybe to kick off with kind of a simple question.
05:02:31.760 - 05:03:21.786, Speaker A: How do you guys think about the most important or interesting infrastructure being built in your ecosystems today? What are some examples that you would highlight most important or interesting infrastructure in terms of things on Ethereum that are immediately useful to people, like the mixers, tornado to tornado cache. It's still in kind of very early beta mode, but even still, it's like something. It's useful, people can use it. And now you can finally, for the first time, conveniently just move eth to another account to pay for gas and do things from that account without linking it to the rest of your activity and gain more privacy. Arthur. Yeah. As long as you're building something on something, then it's not like layer one anymore.
05:03:21.786 - 05:04:21.922, Speaker A: So it's like the layer one infrastructure is really the core protocol. And I think one of the things that's interesting in Tezos is that through the governance model and the fact that you can do binding on chain of grades, you have the ability to have a lot of innovation at a protocol level at a pace which is typically faster than you would have otherwise. I don't think the goal is primarily the pace, but it's a side effect. So among the things which are interesting, which are being built inside the protocol, so similar to what vigili was saying, privacy preserving transactions for smart contracts. And there's some very nitty gritty stuff that I think is not very interesting to most people. You don't write whole papers about it, but making it faster to process transactions. For example, making sure that your nodes are not processing transactions several times, but only once, making sure that you can create blocks without actually executing the transactions better.
05:04:21.922 - 05:05:35.050, Speaker A: I o it's boring, but if you look at almost every single blockchain out there, all of them are I o bound. And just like having much better databases or disk access, this type of things. It's super important to have well functioning system and so it's not fascinating, but it is super important. I think a lot about what stuff that is being built, how to get it launched. I think we do have a large contributor community, but there isn't yet a well understood path to how do you write something new for Cosmos and then get it live and value on top of it and it working well. And I think that is a path that we have to figure out over the next few months and sort of how to teach all of the different entities that are building inside the ecosystem how to launch things. A lot of them have been building test nets for zones, there's a lot of activity there and then there's how do you actually adopt IBC is going to be a big challenge.
05:05:35.050 - 05:06:51.090, Speaker A: So maybe let's start with a question challenging the premise a little bit, which is if we think about two classes of motivation, I think you can think of some developers as more missionary. They're excited about doing the work, they're excited about contributing, and some developers as more mercenary. They're excited about building a business or earning something from their work. It seems like a lot of traditional open source as well as early cryptocurrencies benefited a lot from missionary work. I'm curious what you guys think of the question. Do we need to think about incentivizing blockchain development or is the missionary motivation good enough? Zachy well, a if no one is paying people to work on these things, essentially eventually everyone runs out of money and people have to eat and this is critical infrastructure and it can't be built in the people's spare time for the most part. So you do need to have a contingent of people who are working full time for periods of time on these protocols in order for them to actually survive.
05:06:51.090 - 05:08:15.994, Speaker A: Then there's the question of whether or not you can get rich building infrastructure, which is like an orthogonal problem. And I think there are opportunities to get rich building infrastructure regardless of whether or not it's necessary. So just to be clear, I mean, there are projects which demonstrates that just traditional open source software has found ways of funding itself, but it doesn't mean that it hasn't been a struggle. So a lot of people point in the success of Linux and say oh look, it's just all volunteers and maybe a few people paid here and there by IBM, but by and large it's a volunteer community but it is highly non trivial and you don't have the counter factual which is how many Linux Linuxes? Or I guess that's the plural of Linux, how many Linux will we have if there were more funding? But there's a deeper problem, which is I think you need missionaries in some sense, because it's always going to be a little more than just about the money. I mean, even in private companies, a lot of private companies are very, very focused on their mission because otherwise it's hard to get people motivated enough or interested enough in what they're building in order to build something of quality. And the classic example is how early us colonies, the one which are very religious and which are going for very ideological purposes, survived better than the other ones. Or the other ones.
05:08:15.994 - 05:09:00.054, Speaker A: Maybe they were like, you know what, this is not a good environment, let's just leave. So you have necessarily going to have a little more success with missionaries. Ideally you want to pay your missionaries. Yeah, and I'm going to echo the boring centrist viewpoint and say it's a balance. If you just have missionaries, then basically what you've got is three guys in a basement and they can build some things. But that's not enough effort to build infrastructure that's going to handle the load of and attacks from multibillion dollar actors. On the other hand, the mercenary approach that we've seen from some blockchain projects.
05:09:00.054 - 05:09:42.760, Speaker A: Cough, cough. Not this panel. I think there's definitely a lot of money being just burned there for probably very little impact because ultimately blockchain projects are maybe one third technology, two third social. And I think I'm underestimating the social there. And you can't buy social, right. You can buy 5000 people to come to your conference, you can buy dinners with certain famous people and you can buy some stuff. But it's not the path to strong and long lasting sustainability and it's also not the path to having.
05:09:42.760 - 05:10:20.206, Speaker A: Making your missionary developers happy. Yeah, I worry a little bit about if you have money to pay mercenaries, and so frequently mercenaries are very professional in what they do, you end up discouraging the missionaries and that can kind of undermine the long term viability of the project, sort of crowding out. Yeah. I would also say, don't knock the boring century's viewpoint. It's a safe viewpoint. It's also oftentimes correct. The other thing is reflecting on why it's important to have missionaries.
05:10:20.206 - 05:10:58.000, Speaker A: I think one of the main source of cost as well, there's the fact that blockchains are social. There's also the fact that anytime you're dealing with missionaries, it may be very professional, but you're going to deal with a lot of principal asian problems. And that's why I think you'll see a lot of money being burned if you're just trying to to pay your way into doing these type of things. You'll get RC selected by people who are going to try to get this money and nothing will happen out of it. So throwing money at things is a much harder problem than it sounds. It sounds like there's some consensus that it is worth incentivizing infrastructure development. Let's take Ellie's framing from this morning for folks who missed it.
05:10:58.000 - 05:12:00.874, Speaker A: Ellie had a framing of blockchain funding as sort of a government versus private sector dichotomy. How do you guys think about the role of the foundation or the government effectively funding infrastructure development versus private sector incentives? So I guess, first of all, is the government the foundation or is the government the protocol? Interpret it both ways. I guess that is true in some ways, because the foundation, given pretty much all modern cryptocurrencies, have pre mined the protocol does give a one time grant to the foundation. So it's a state owned enterprise. Yay. I think the advantage that protocol funding has is that it can provide funding that is large. The big, huge disadvantage that protocol funding has is that it has a very hard time being credibly neutral.
05:12:00.874 - 05:13:00.150, Speaker A: And whatever mechanism you have is going to get attacked, and mechanisms get attacked in lots of different ways all the time. Right. And we've seen a lot of attempts by kind of wealthy actors to influence social layers of blockchains and even things like cartels forming. Basically the challenge on the protocol side, I think, is basically figuring out a governance structure. And governance structure doesn't have to even mean something perfect, it could even mean very small components. One very simple example would be if you come up with a mechanism where if you just write a contract and a huge number of people start using them, then at some point some portion of the transaction fees just get rebated to whoever published it. That mechanism is totally credibly neutral.
05:13:00.150 - 05:13:41.906, Speaker A: But on the other hand, it's limited. So there's a spectrum. Maybe you could say a word on the approach of the ethereum foundation. It seems like the approach you've taken with the ETH two clients and some of the ZK R and D work is different from the historical approach. Yeah. So I guess, first of all, the Ethereum community has really pushed very hard on this approach of multiple implementations, multiple clients kind of separating between the specification and the implementation. And in part, this is a kind of redundancy for security strategy.
05:13:41.906 - 05:14:29.806, Speaker A: And it has actually saved us in 2016 during the DoS attacks. In part, it's also a political decentralization strategy. So to avoid one organization having too much control, basically for the first few years, what we've done is just the foundation supported both the clients, but now we're supporting most of the client developers with grants. And these are kind of independent external companies, and sometimes they get grants from other sources. And I think it's actually worked out even better than I thought it would. Yeah. So the main mechanism that's provided for in Tezos to provide for upgrade over the long run is something that's been dubbed inflation funding.
05:14:29.806 - 05:15:10.558, Speaker A: And the idea essentially is you come up with a way of improving the layer one protocol. And I should say that it is focused almost exclusively on layer one. You can do more with it, but it's designed for layer one. And so you come up with a proposal, and a proposal is not just an idea, it's like really a piece of code that's fleshed out. And you say, let's replace the state machine of the protocol from this old set machine to this new set machine. And you publish this on a website and you propose a hash on the chain, and people can vote for this protocol and accept it or reject it. And the idea is, since you're changing the state machine, one of the things you can do in changing the state machine is issue new coins and send them to your address.
05:15:10.558 - 05:16:02.274, Speaker A: And I think the benefit of this approach is that it is in some sense credibly neutral. In a sense of like you've produced a work and you propose it, and if people want it, you get your coins and then you walk home and there is no further discussion to be had about this. If people are not happy with it, they can change it. But you know what? They had a chance. And so it ends the unless discussion as opposed to funding, whether it's through a foundation or through a dow, as soon as you fund things, people are going to start trying to think, well, is this the right way to do this? And so on and so forth. There's a lot of second guessing that comes after, which is very difficult, hence the credible neutrality. The other thing when it comes to government funding versus private funding, the funny thing is that it's a debate and a question that comes a lot in libertarian literature, and it goes like this.
05:16:02.274 - 05:16:25.426, Speaker A: So when you start thinking of the provision of public goods, the reaction among libertarians is divided in two. Some people will try to pretend that there's no such thing as a public good. All you need to do is to have a toll booth everywhere. And so you'll have a private city, and then in the streets, there's going to be a private toll booth at every part of the streets and then privatize everything. And that doesn't really work. Well. There's a lot of friction.
05:16:25.426 - 05:17:03.934, Speaker A: You can't really make that work. And so the second approach is to say, no, you just need to internalize the externality. And what will happen instead is that some developer will buy an entire tract of land and have a private city, and then they can internalize all this externality and produce good things. And of course, the question immediately is like, well, what is the difference between a city government and a privately owned city? At the end of the day, one is going to charge you taxes, one is going to charge you a fee. That looks very similar. There's no competition. And there's a good answer to that by economist David Friedman, which compares capitalist trucks and communist trucks.
05:17:03.934 - 05:18:11.674, Speaker A: So you take a truck, and I tell you it's been produced in the Soviet Union, and you take another truck, and I tell you it's been produced in the US, for example. And they're both trucks, right? They both have wheels. They both try to do the same function, but, you know, one is going to work better than the other. And the general idea is that if the process that led to the creation of your trust is a competitive one, is one where people opt in voluntarily, you probably are going to produce a decent truck, whereas if it's based on what is essentially amounts to forced labor, you're not going to have a very good truck. And so the comparison between the protocol or a well and known foundation, for example, being like a government actor, has its limits, because fundamentally, the fact that it originated from something that is voluntary means that there are very different outcomes that you should expect out of this process. So I would say the approach that Cosmos has been taking is we are trying to create spaces where different ideas can be tried out. And Cosmos has a very pluralistic approach to basically everything.
05:18:11.674 - 05:19:08.160, Speaker A: That's probably what defines the project. What is not cosmos is unclear, but essentially what we are trying to do with the foundation is fill gaps where we think something needs to be done. And a rate limiting factor is funding. The foundation is able to provide funding there. There's going to be a small on chain inflationary funding mechanism that emerges after the next hub update. But it's also a little bit unclear how we're going to interact with all of the things that are sort of like cosmos, like all of the other PBFT chains that could easily support IBC that are emerging. But I would hope that eventually the importance of the foundation in the ecosystem is diminished quite rapidly and that there are primarily private incentives driving the system.
05:19:08.160 - 05:20:23.206, Speaker A: Thoughts on inflation based rewards such as the founders reward or other models? So basically, I think currently I know of no scheme that satisfies at least my own standards of kind of security and rigor for being both credibly neutral and attack resistant. Basically, there's the centralized scheme, which is like, hey guys, we love Zuko and he's wonderful, yay. And we're going to just agree to pump some more money to him. Honestly, there's a very high probability Zuko is a dead attacker. So that's fine, but it's centralized and it's limited. And on the other hand, you have schemes that do try and also those kinds of schemes, unless they're created at the very beginning before, when a community is still basically below Dunbar's number, you start getting attacks. Even Ethereum, for example, there was this kind of working group that was trying to figure out how to do issuance funding, and it kind of fairly quickly devolved into essentially people coming up with reasons why they should get more money printed for themselves.
05:20:23.206 - 05:21:15.014, Speaker A: And as we know, other people's money. OPM is addictive. Yeah, and it's hard. The second option is some protocol mechanism, but then the biggest kind of big money example of this eos, as far as I can tell, basically it's evolved into a cartel of rich people bribing each other to vote for each other. So, yeah, I'm not diplomatic on stage, man. Yeah, basically we have such kind of strong and rigorous standards for things like evaluating the security of proof of stake protocols. And we've done really cosmos, tezos, Ethereum have done zarika, really, I think a good job of that.
05:21:15.014 - 05:22:50.180, Speaker A: But decentralized governance protocols don't really seem to have kind of stood up to the same level of economic rigor, especially once you consider economic models that allow collusion between participants. I think. So just to react on the last part, I think the main thing is that you have to look at what is limiting the security of your system in general. And the problem is that the fact that the difficulties with decentralized governance and all of this, it's not a reflection of decentralized governance, it's a reflection on this entire space, because we have really nice guarantees around the cryptography, for example, and even some of the distributed system aspect but at the end of the day, all of this stuff relying on honest majority assumptions, no matter how hard you try, even if you try to do something where I know that in cosmos there's really this aspect on being able to slash a very large number of the tokens, but at the end of the day, you're still relying on some sort of coordination to restart after that and doing some hard fork and so on and so forth. So knowing that at the end of the day, you're just all like trying to not have to bounce too often on a soft mattress of subjective community assessment of how things are going. Trying to be far more rigorous than that is basically not necessarily useful. Like you want to minimize reliance on it, but you don't want to fool yourself into thinking that you have some sort of computer science guarantees that everything is going to be great.
05:22:50.180 - 05:23:45.510, Speaker A: Regarding the funding, I think so. Part of it is human nature, part of it is bike shading, because that's going to happen in any human system. I also think that there are protocols which didn't get enough funding for historical reasons, and they have an incentive to create moral norms against any type of inflation based funding or anything like that. And it's purely anti competitive behavior, which is normal. But I guess the hypocrisy is a little annoying. I would say that the governance experiment is very primary to what the cosmos hub is trying to do. We sort of structured the thing in such a way that governance was like upfront and very early on, a significant barrier for the chain becoming useful.
05:23:45.510 - 05:24:50.806, Speaker A: Now we've added this inflationary funding mechanism, which may or may not work, but we want to test what is possible. And it is certainly true that we can't predict the success of these things in any sort of rigorous fashion. And what we are trying to set is a set of evolving social norms around it, and we don't know whether or not those will work out, but we want to put that experiment like front and center and learn early what does and does not work. Maybe switching gears. When we think about sustainable business models for companies, I think it's often underrated how much business model innovation matters relative to product or engineering innovation. If you look at the traditional Internet, annual monthly subscriptions, really enabled cloud software, take rates on marketplaces, et cetera, I think it's clear that there are incentives to build a layer one protocol. You get Senoridge.
05:24:50.806 - 05:25:46.042, Speaker A: I think there's clear incentives to build an exchange or a mining pool, potentially a validator. How do you think about incentives for businesses beyond those models? I think we got a taste in 2017 of what's possible if a lot of people think a business model is interesting today. We know medium of exchange tokens are probably not sustainable. But long term, if this ecosystem is going to succeed, presumably we want long term sustainable models for many businesses to grow. So how do you guys think about that? Maybe. Let's start with Vitalik. Yeah, I think there's definitely some cases where tokens make some sense, and I don't think it's as bad as some people think.
05:25:46.042 - 05:26:44.158, Speaker A: Basically, if you are making a token, then yes, it's backed by fees, but fees and fees that you expect to happen within your own protocol. And one example of a place where it could actually make sense to do that would be basically L2 systems where you want the participants to also be people who would suffer if they act in some way that the base chain cannot detect. That would make the protocol break to add extra incentive alignment. And the reason it's not that terrible is because transaction fees aren't that low. Right? So for example, the Ethereum chain, to date, I think the transaction fees on and have been something like $250,000,000. So it's entirely possible to come up with protocols, given some assumptions about them succeeding and getting usage to a reasonable extent. Your ability to capture value out of that is not too bad.
05:26:44.158 - 05:27:29.178, Speaker A: Now, there are going to be some things where you can't make this kind of protocol and charge for it. The idea that I suggested earlier of coming up with mechanisms where if you write a contract and lots of people use it, then you get some rebate. Even something like that, as a protocol feature, is something that could be worth exploring. And maybe we can figure out more market based in protocol ways of incentivizing development models. What examples would you point to that are promising? It seems like Joey made a case for the Auger token. The MKR token seems to have. Yeah, an MKR token has definitely been successful.
05:27:29.178 - 05:28:17.414, Speaker A: There's been a non negligible fraction of the MKR that's been burned as fees already. Auger seems to be working pretty well as well. There's a difference. So the difference between some of the tokens that exist on a chain which can have their own existence, like MKR for example, or rape, and the tokens for Lego one, because we're discussing Lego one. And to be honest, I think that I don't think you can survive on fees alone, because I'm pretty optimistic about L2 solutions. And so I think at the end of the day, the only realistic way of success for layer one blockchain is to be cryptocurrencies and to be store of values and mean of exchange. So that's the first thing I am pretty convinced of.
05:28:17.414 - 05:29:27.758, Speaker A: The second thing is that I'm also convinced that more or less you're going to have maybe not one, but at least an extremely strong power law in terms of which cryptocurrency dominates. And the reason is that they are, for many, many users, close substitutes to one another. Not quite. There are many cryptocurrencies that do things better than bitcoin can do, but bitcoin has much greater network effect. And so at the end of the day, I guess it's a bit of a pessimistic on the industry, but it's a reality and it's really hard to survive if you don't have a monetary premium because otherwise what you have is you have network effect and you have computation. Computation is cheap if you're looking at the amount of computation you're actually buying by using a blockchain, if you're willing to use AWS, you can look at the cost of running something on the US, for example, as opposed to AWS, and it's just like orders of magnitude more expensive. So you can't really justify your fees like this and you can have network effects.
05:29:27.758 - 05:30:15.540, Speaker A: So for example, I think the reason MKR might have value, for example, is that they are in a dominant position for people who want to make CDP loans in Ethereum. Ethereum, and as a result of that they can extract rent. But as this thing gets competed away, it gets really hard to maintain any value. So I don't think we're going to see more and more and more layer one chains being launched. You're seeing a whole slew of them which have been primarily funded by venture capital and who primarily try to distinguish each other now by having a different consensus algorithm. It's like, oh, we have distinguish each other by having a different professor as the head. Yes, go ahead.
05:30:15.540 - 05:31:25.898, Speaker A: All I would say is one observation that I think is interesting is essentially the conventional open source model is breaking down, because the assumption was that for a long time was the business model was, oh, we'll develop an open source software, but we can operate the software better than anyone else. And that's why you would pay up a centralized company for their cloud services. But Microsoft, Google and Amazon now operate anyone's software better than the developers, like they're specialists in operating software. And so the open source model has broken down. I think right now there is almost no expertise in either deploying distributed software or operating distributed software. And both of those seems to be opportunities to capture rents. And I suspect that expertise in that space will be scarce and provide some mechanism of sort of bidding between both the protocol and the developers to attract the best people at building this stuff.
05:31:25.898 - 05:32:57.974, Speaker A: So one place for attracting rents that we haven't talked about that's smaller, but at least it's guaranteed to not go away with improvements in technology is domain names. So maybe last question, because I think we're almost out of time changing gears a bit. What's something that you guys are thinking about or excited about that you think the other panelists would disagree with you on Zucky first thinking that they would disagree with me? Bid for everything? Everything? Well, I would say probably the thing that I'm excited about is I am excited about the many chain ecosystem that I think is going to be emerging. I think that there's going to be a whole bunch of new blockchain protocols, and many of them will fail their go to markets. But even if they fail their go to markets, the software will be interesting and maybe exist in better token distributions. And so I think that's a thing I'm pretty excited about. So I'm excited about the prospect of having more complex and intricate roles for onchain governance, in particular constitutionalism, which is the idea that it's not just, oh, you vote on a protocol, the protocol gets activated, but the old protocol can reason about the new protocol and check for its properties and then accept it or reject it.
05:32:57.974 - 05:33:27.180, Speaker A: So it's basically treating protocol upgrades like you would treat a whole smart contract, homogeneous, layer one, charting. That's actually a good list of things we all. Because I don't agree with either of those things. Perfect. It sounds like we have five minutes for questions. So if you can line up at one of the two mics, we'll take some questions. And maybe if there are no questions, right off the bat, I can cold call Ellie up there.
05:33:27.180 - 05:34:10.886, Speaker A: Do you have a question for the panel, Ellie? Okay, there's a mic. Okay, I have a question. What about? I mean, governments often, what they do is they sort of do something like for infrastructure, they run an auction and then gives like limited license. I'll give an example. You say, okay, we need three wallet providers. So we're running an auction. You can submit things and then actually the governance will give you a license for the next whatever three years you can extract from the layer one, you will get some fee that will be sort of enforced in the protocol.
05:34:10.886 - 05:35:40.680, Speaker A: And now there'll be three wallet providers and go compete among yourself and so on and so forth. What about something like that for sort of building fostering infrastructure using private sectors? So one example of a kind of implicit auction that fully satisfies my admittedly tough standards for credible neutrality is direct in protocol incentives for generating starks and other kinds of succinct claims about basically validity and other properties of the blockchain protocol. Because there's a lot of benefit that you could get from having proofs that say that some block was executed correctly and some state transitions were done the right way. But these proofs are hard, and especially in the case of general purpose vms, prohibitive to generate. But if you had some schedule of in protocol rewards that you committed to, then you could incentivize an ASIC ecosystem to be created and developed. You could incentivize people to actually continue running the hardware. And this seems like something that actually could end up incentivizing quite a lot of socially productive development effort, but without needing to have mechanisms that are kind of overly subjective or could be captured in some way.
05:35:40.680 - 05:37:05.358, Speaker A: I think there's oftentimes a tendency to try to say like, oh, this will be the agreement, and so on and so forth. And at the end of the day, whether or not you give some sort of exclusivity or anything like that, at the end of the day it end up being money. And it's just like with extra steps. And maybe that's not, I don't know if the extra steps add a lot to it. The thing that I think is important in terms of funding and which goes towards credible neutrality is to try to pull on the rope, not push on the rope in a sense of like identify projects which are good, which are popular, which are well liked, and it's a lot easier to notice this thing and it's harder to fake and then provide assistance to those projects as opposed to trying to Abinicio say this is going to happen and then trying to make it happen. I think part of the challenge is that no blockchain today is truly a natural monopoly, and as a result it might be in the future, but at the moment it's not. In fact, they're all rivalrous with each other and they're all substitute goods frequently for each other.
05:37:05.358 - 05:37:52.980, Speaker A: And so the challenge that I would see with trying to grant any sort of exclusive license in that environment is it does create incentives for people to use the rival, which might be cheaper if everything is open source and you have to pay this extra fee to use starks on a cosmos chain, there'll just be another cosmos chain that doesn't charge that fee, and people will do their transactions there. And so it's a little bit challenging, at least at this stage, to try and make something like that naturally enforceable. And I think it's, well, lot of people tried with appcoins. There was a lot of appcoin model, which is we're going to write an open source smart contract and deploy it. And then you'll use our token to call it, even though you don't need to. And it's kind of like putting a toll bus in the middle of an open field. It doesn't do much.
05:37:52.980 - 05:39:26.696, Speaker A: Idea of contract? Sure. So basically the idea would be that, say I deploy a contract and this could be a software library, it could be a copy of an EC, whatever, verifier, something. Then right now, calling that contract has some gas cost, and there is reasons for that gas cost to exist. But suppose that in the case where a lot of people call the same contract, what you do is you would take some portion of the gas fee. So maybe up to 50%, maybe it's a sliding scale, goes up logarithmically, and you would basically just direct that to whoever published the contract originally. Now, there actually are already good reasons to kind of charge sublinearly for contract use, basically because if a contract ends up getting called often, then you could stick the code in a higher level cache, the Merkel witnesses could get smaller and blah, blah, blah. But basically what this would do is it would create this kind of natural moat where if you create a library that becomes popular, then you're getting funded and other people would not be able to undercut you because you're not charging a fee that's higher than anyone else.
05:39:26.696 - 05:40:13.394, Speaker A: You're not even charging a fee at all. You're just getting money that's given to you by the protocol. Yeah, it's tough to do anything with the fees because you always have to contend with the possibility that people pay the fees offline. And so if you're just monitoring the fees and then minting money in order to do that, then the contract is going to pay fees to itself. If you're trying to tie, if you're trying to have a burn that's related to the gas cost, that might work, but then people might spend the gas when there's very low demand and then do it for free. So I'm not saying it's possible to do it, but it's always very tricky when dealing with fees. In this case, the really important kind of mathematical point is that the return to the developer is or the discount is super linear.
05:40:13.394 - 05:41:16.460, Speaker A: So it starts off being zero. So basically, if you're an upstart trying to compete and you're going to say, hey, I'm going to give my rebate back to you by paying you on the plasma chain or whatever, then until you actually succeed, you don't actually have any money to give the rebate with. It's not a perfect mechanism at all, and you can totally do a loss leader around it, but it is something. It does seem very interesting to try to figure out a way for protocols to incentivize people that are proportional and super linear to some notion of the economic value they generate for the protocol. And I don't have a mechanism off the top of my head, but I think it's worth experimenting with, because if you get it right, your protocol has a market advantage if it's a monopoly, but then someone uses another protocol and then has an ordinary hunt and has a copy of that contract, then you don't have to. Sure. And you don't have either to pay the extra fee or the inflation associated with that.
05:41:16.460 - 05:41:44.310, Speaker A: But the thing is, even for a store of value, pure medium of exchange cryptocurrency, I don't think you can secure it with fees. I think you need some form of inflation to secure it. But then the strategy is that you can build a parasite cryptocurrency on top of it, which is only going to pay fee and not going to be affected by inflation. So even that much simpler part of the problem is not even solved. Cool. To be resolved on Twitter. Thank you, panelists.
05:41:44.310 - 05:42:28.560, Speaker A: Sure. Yeah, just put it there. That's fine. Hope the music started, but hold up, hold up, hold up. I don't know, this music go down, YouTube takes down. It's okay. 15 years.
05:42:28.560 - 05:43:09.590, Speaker A: Yeah, we'll see if. We'll see if it can actually make a lot more money, probably doubling more money than they would turn up. Okay, so we are getting very close to the end. Before we do end, though, we have Vitalik here who's going to give some closing remarks. But just before you start, I want to say a big thank you to starkware, to Sheeri, to putting this on, to putting on the starware sessions. Thank you so much for doing that. And I'd like to hear the closing remarks.
05:43:09.590 - 05:43:36.830, Speaker A: Where's the thing? Okay, cool. Yay. There's the thing. I don't know. I'm not sure, if kind of closing remarks is the best title for this presentation. It kind of feels a bit grand. And really all I wanted to do is just talk about fraud proofs and data availability proofs, but here you go, fraud proofs and data availability proofs.
05:43:36.830 - 05:44:54.842, Speaker A: So I guess to kind of set the stage for this technique. The idea here is that if you look at blockchain protocols like bitcoin, Ethereum, and specifically full nodes in those protocols as they exist today, there are some protocol guarantees that get broken if, say more than 50% or 33% or 23.21% or whatever number of nodes stop following the protocol. So you might have 51% attacks, you might have finality failures and so forth. But there are a lot of properties that you still keep, even if arbitrary 51% attacks or like arbitrary dishonesty of nodes is possible. Right? One example of this property is the property that all the blocks in whatever chain you're going to accept as the main chain are going to be valid. And what this implies is that also people can't steal your money, people can't give themselves money, the coin supply can't get inflated, applications can't get randomly screwed around with, and a whole bunch of other things, right? Another guarantee you have is that the data in the chain that you accept is available.
05:44:54.842 - 05:46:05.220, Speaker A: So this basically means that whatever chain you accept as the leading chain, well, if you want to download any particular piece of it, you will be able to. And this seems trivial in kind of non scalable blockchain land, but it's actually very nontrivial for some reasons that I'll go into. And what this guarantee basically gives you is it gives you the ability to first of all be able to ensure that the blockchain things are correct. And second, to be able to kind of do things like generating new blocks, like creating new transactions, and just in general like figuring out what the hell is going on on the chain. So to get directly into things, let's start talking about fraud proofs. So the setting here is basically, suppose a malicious miner, malicious proof of stake validator or whatever publishes a block, and we're going to assume all the block data is present. So there is some magic oracle that verifies that all of the data in the block, the full body of the block has been published to the open Internet, and at least one honest node has it.
05:46:05.220 - 05:46:57.762, Speaker A: But some of the block data may be invalid. And so what we're going to do here is we're going to design the state transition function. So the function that basically takes an existing state processes all the transactions, outputs a new state in such a way that any specific invalidity, any specific error in the block can be proven to be an error to a light client by basically just providing a small subset of the Merkel tree. Right? So who here is familiar with how Merkel branches? Yay. So, okay, you know how Merkel branches work. And so basically, if you have a transaction, the transactions process them incorrectly, then all you need to do is just provide a few leaves that touch the transaction, that touch the accounts affected by the transaction, and you can prove that something invalid happened. Once someone's made a fraud proof, they can broadcast it.
05:46:57.762 - 05:47:42.122, Speaker A: And any client, upon receiving a fraud proof, can immediately verify that there's something wrong with the block. So with this technique, what you can do is you can basically create a system where clients can download all of the data to verify availability. But after they've downloaded all of the data, they don't need to process and verify all of the data themselves. Instead, every client might just randomly choose to process and verify some random 0.1% of the data. And if any client discovers that something somewhere is wrong, they can broadcast a fraud proof. And once you've broadcasted a fraud proof, that fraud proof can get traversed through the network.
05:47:42.122 - 05:48:19.642, Speaker A: Other clients can verify the fraud proof, and they can verify something is wrong. And if you've downloaded the data and no one has sent you a fraud proof for a while, then this is evidence that the chain is correct. Now, under what assumptions does this break? Number one, it breaks if the number of honest nodes in the network is just extremely low. So in this case, it's like under 1000. And so there's just data that no one ends up checking over. And the second condition under which it breaks is if network latency is extremely high. But notice that this does not break even if 90% of nodes on the network are malicious.
05:48:19.642 - 05:49:08.502, Speaker A: So we've really kind of cut security assumptions for verifying validity of data down. So here what we've done is we've basically reduced the amount of computation you need to do to verify that some data is valid. But what we haven't done is reduce the amount of data that you need to download. So data unavailability. Right. So this is the big problem with probably the hardest problem in blockchain scaling. And the problem basically is suppose that a malicious miner or validator publishes a block, and instead of publishing the entire block with some mistakes in it, what the attacker is going to do is the attacker will publish a block where parts of the block are just not published.
05:49:08.502 - 05:49:45.734, Speaker A: So the attacker will publish most of the Merkel tree, but not all of the Merkel tree. And the parts that aren't published, well, they could be valid, they could be invalid. But both cases are attacks. So if we're going to think about kind of proving data availability, it also kind of mentally helps to separate out the two cases. So we can think about a model where we don't really care about any notion of validity. And if we do care about validity, well, we can kind of separately have a fraud proof mechanism for it. Let's just talk about a chain where all the chain does is it holds a bunch of data, and a valid block is a block that has data.
05:49:45.734 - 05:50:59.690, Speaker A: The data could be anything, but the data has to have been published, it has to have been published to the network and be available. And the kind of slightly more formal way to think about this is that for every chunk of data, at least one honest node has that chunk. And so if you want to download any particular chunk, then you can ask the network for it and get it if you want to. So basically, the problem is, how does a client verify availability without downloading all of the data themselves? Now, you cannot use fraud proofs for this, right? You can't just publish a message that says, hey guys, I think Chunk Zero is unavailable because as soon as you publish that claim, well, what a clever attacker can do is they can just not publish that chunk until they see your claim, and then they just immediately publish the chunk and they're like, hey, this guy's an idiot. Why is he claiming this data is unavailable? Here you go. And then the attacker could just keep doing this again and again. And basically eventually either people stop trusting these claims and the scheme breaks, or the participants get dos and they have to download the entire data.
05:50:59.690 - 05:51:33.130, Speaker A: So you can't solve data availability the same way that you can detect faults in data where all of the data is available. So easy solution one, check everything for OFC data. This obviously takes OFC bandwidth. It's okay for existing blockchains, but there is plenty of contexts in which it's not okay. Context one, higher capacity chains. So that's Ethereum today, Ethereum after the Istanbul hard fork. That'll reduce the call data gas cost from 68 to 16.
05:51:33.130 - 05:52:45.982, Speaker A: The Craig Wright chain and sharded Ethereum, with its 1024 chains also plans very high. Right now in the grand scheme of things, a few megabytes a second isn't very high. Like it's still within bounds for at least some home users in at least some context to be able to download everything sometimes, but it's definitely too high for most people to reliably download. So that's case one, case two, and this is a case which is relevant to existing non scalable blockchains, including pretty much everything that exists now. Quick verification of history, right? So suppose that you're syncing a chain for the first time, or you've been offline for a long time and you want to just get caught up with the history and you want to verify all of this stuff. So you want to establish this guarantee that the data is available and that the data is correct in sublinear time. So in sublinear computation complexity and in sublinear data complexity.
05:52:45.982 - 05:53:42.810, Speaker A: Now, right now, what you have is you have light clients, and light clients can find the correct head in very sublinear time, but only if you accept this trust assumption that says that the majority of the miners or the majority of the validators are good. Let us try to remove the assumption that the majority is good. Can we still keep the guarantee? And it turns out that if you're willing to accept these assumptions about either network latency and at least some minimal number of honest nodes existing, yes, you can. So, light client protocol for verifying data availability. So suppose you have a block, and we're going to just take that block and we're going to Merkele hash it. And think of there being data at the bottom. Could be a few megabytes, could be higher, and it just gets Merkel hashed and you have a Merkel route.
05:53:42.810 - 05:54:52.922, Speaker A: So one simple protocol for probabilistically checking availability is I, as a light client, am going to privately select 80 positions and I'm just going to ask the network for each one, and I'm only going to accept the block as available when all 80 of my queries get answered by someone. If only 70% of the queries get answered, then I'm just going to sit there, pretend the block is not there, and if an hour later there are the last ten queries get answered, then I'm going to accept the block an hour later. So this is kind of similar to what happens already, where you only accept a block after you've downloaded the whole thing. But here, instead of trying to download the whole thing, we're just going to focus on a particular 80 positions. So the idea here is that this is not a good way to verify that the entire data is there, because there could be just one chunk that's missing, and if there's only one chunk that's missing. There's no way any of these clients are going to catch it. But what it does prove is it proves with a very high probability that more than 50% of the data is available right now.
05:54:52.922 - 05:56:04.126, Speaker A: Notice that an attacker can trick specific clients. So what an attacker could do is when an attacker could, by default, publish nothing, and then just wait for these queries to come in and only publish specific chunks of data responding to the queries, and then stop at 49%. But basically, if the attacker tries to do this, first of all, there's a bound on the number of clients the attacker can corrupt or trick, because eventually 80 chunks multiplied by a bunch of clients is just going to add up to half the data. And also, you can make it even harder to trick specific clients, basically by having all the clients make their queries with onion routing, and so you don't know which queries to respond to in order to kind of trick one specific client. So this is the, like, client protocol, right? Basically, just like check 80 random positions, and you're probabilistically verifying that at least half the data is there. Problem? Of course. What if the one position that the attacker has not published is an invalid transaction that gives themselves 91 bajillion coins? So, erasure coding.
05:56:04.126 - 05:57:36.030, Speaker A: What we're going to do is we're going to low degree extend d, and we're going to low degree extend it to a larger d. So now any 50% of d suffices to, or any 50% of d prime suffices to reconstruct all of d prime. And so verifying 50% availability of d prime suffices to verify 100% availability of d prime. Do people get this? Okay, so this is erasure coding, right? Now, there's one problem here, which is, what if the d prime is constructed incorrectly? So what if it's not a low degree extension? What if the data in some places is just like, totally random? What if at position five, the Merkel tree doesn't even go down that many levels? What if one of the chunks has 34 bytes instead of 32? What if there's crazy stuff happening and r prime actually isn't a kind of faithful low degree extension? Or d prime actually isn't a faithful low degree extension of d? So, by the way, r is numerical root of d, and r prime is numerical root of d prime. So you might think, well, no problem if r prime is constructed correctly. But if it's constructed incorrectly, what do we do? So one approach is to just bring back fraud proofs, and we'll make a naive fraud proof. And the naive fraud proof is pretty simple.
05:57:36.030 - 05:58:34.146, Speaker A: Basically, if you as a full node, or at least as a node that is full with respect to that particular block. So you actually try to download more than 50% or all of d. If you've only managed to download somewhere over 50% of it, but not the whole thing, then you can use that 50% to reconstruct all the data. And after you've reconstructed all the data, you can basically just make a big huge fraud proof that just shows, hey, if you take this data and reconstruct it and make a Merkel root of the reconstructed version, the Merkel root is not going to be the same as the Merkel root of the original data, right? So basically we're just making a big huge fraud proof containing all the data you have and verifying it just requires running this entire computation, extending the data and proving that it's inconsistent. So this works. But the problem is, this makes a big fraud proof. So in the case of Ethereum 2.0
05:58:34.146 - 05:59:41.206, Speaker A: crosslinks, for example, in the worst case, 1D prime can have a size of a gigabyte, and so you really don't want gigabyte long fraud proofs. So what do we do? So the solution in this paper from last year by myself and Mustaf Al Basam basically said we're going to have two dimensional erasure codes, right? So what we're gonna do is instead of just extending d to D prime along one axis, we're going to make d be this kind of Little Square on the left, and then we're going to interpret D as being a bivariate polynomial. And we're going to low degree extend it this way, and then we're going to low extend and degree extend it that way. We're going to low degree extend it this way and degree extending this way, and low degree extending this way gives the same answer. And so we have this square. So then a light client, to check data availability is going to make basically 80 random queries into this structure. But the light client is also going to download the Merkel roots of every row and the Merkel roots of every column, right? So a light client has to download o square root of n data.
05:59:41.206 - 06:00:46.170, Speaker A: So in the case of like gigabytes, this is going to end up being about a few hundred kilobytes. So a fraud proof, instead of including the entire data, just has to prove fraud in one row, or prove fraud in one column, or prove inconsistency between one position in one row and one position in one column. That's it, right? Because ultimately, if the data is broken, then what that means is that some row is broken or some column is broken. And so you just choose the specific row or column that's broken, and you prove it. So what this means is that the amount of data that a light client has to download goes up a bit, because a light client has to download these row roots and column roots. But the size of a fraud proof goes all the way down to just proving fraud of one row, right? So now fraud proofs go down to being under a megabyte, and we have a scheme that theoretically works. There are possible techniques for improving this with Starks.
06:00:46.170 - 06:01:48.858, Speaker A: So I'll start with the scheme at the bottom. The scheme at the bottom basically says, we're going to use fry. So we're going to use the ingredient in Starks. And what we're going to do is we're going to use this to basically give the same properties as the two degree scheme, the two dimensional scheme, but on top of simple one dimensional codes. So, basically, if you have a D prime where that D prime has a bug in it, instead of providing the entire D prime, we're just going to do a bit of clever math, and we're going to basically use fry, use one of these kind of f of x minus y over big x minus x arguments that you might have heard about all of this morning. And we're going to prove that there exists some output, which is the correct value of extending the parts of D prime that you have to some position. And, look, it doesn't match the thing that's in the hash stream.
06:01:48.858 - 06:02:46.302, Speaker A: So, basically, you can just use Fry to prove an inconsistency, instead of proving an inconsistency by providing the entire data. Another thing you can do, which is simpler conceptually and better, but it takes more work. And this may end up requiring ASICs to run effectively, is to just directly use a stark to verify correctness of the Merkel root. Right? So, basically, what we're going to prove is we're going to just take all of the hashes in the Merkel tree, line them up along a polynomial, and it'll be like p evaluated at Powers of Omega and p at Omega to the two n all the way up to p at Omega to the four n minus one. We're just going to place the leaves there. And then we're going to basically make an argument to prove that these particular pieces of data represent a degree n or less than n polynomial. So it means it's an actual low degree extension.
06:02:46.302 - 06:03:23.866, Speaker A: And then we're going to just verify every other leaf node in the merkle tree by just showing p of x is the hash of p of x squared and p of x squared times w. So every parent is the hash of its two children. You can't shove this in a constraint directly because h is high degree. You need to break it up a bit. And then you prove that, you check that the root actually is r. Right? So the nice thing about this holy grail approach is it basically says, well now we don't even need fraud proofs. And if you don't need fraud proofs, then to check availability you actually don't even need a network latency assumption.
06:03:23.866 - 06:04:11.918, Speaker A: All you need is basically this assumption that says well, there is at least some minimal number of honest clients somewhere in the network and that's it. And that number is not 51%, it's not 5%, it's just a constant. So two possible ways to improve it. So what do we use this for? Right, so the first use case, as I mentioned, verifying existing chains. So this is a fully kind of L2 technique that could be used to improve the security properties of existing light clients. And basically what you do is you just connect your node, you receive a piece of data provided by someone that claims to be a Merkel root of the chain. And for antidos reasons, you could attach this to proof of work.
06:04:11.918 - 06:05:17.010, Speaker A: If there's a chain you've already authenticated, you could also have the provider have a deposit in a smart contract so that deposit could be taken away if there's a fraud proof. And then on this route you perform the data availability challenges, you wait for fraud proofs, and if the data availability challenges pass and you see no fraud proofs, then you accept the chain as valid. So you could theoretically come up with this scheme, just make it be this L2 thing. And just by having light clients do this, you can improve the security of syncing of existing blockchains. And by basically creating this mechanism that says, well if there is a 51% attack and a light client syncs a chain, then the network basically will kind of collaboratively detect the fraud pass from the fraud around. And light clients by default are not going to accept this chain. So it makes 51% attacks kind of considerably less powerful.
06:05:17.010 - 06:06:30.918, Speaker A: Use case two scalable chains, right? So basically in Ethereum 2.0, it's a sharded chain, and so there's way more data than most people can download individually. And so you would just use fraud proofs and data availability proofs as this kind of additional check so that you can basically get security guarantees on data, even if whatever particular subcommittee was responsible for signing off on that data turns out to have been corrupted. Now use case layer three is some L2, right? So basically, if you have some Dex plasma chain thing, and your Dex plasma chain thing requires users to verify data availability so that they can make challenges, which pretty much every plasma thing requires, then you can use this to check for data availability instead of users downloading the data themselves or relying on some trusted watchtower scheme. Yeah. So, conclusion, data availability verification and fraud proofs, two nice ingredients. You can combine them together, you can quickly verify large amounts of data and computation indirectly.
06:06:30.918 - 06:07:42.260, Speaker A: Right? So you can, given these security assumptions that are kind of much lighter than the security assumption of there not being a 51% attack, you can achieve this guarantee that says the data is available with cryptographically high probability and that the chain is or every piece of data is valid. Because if it wasn't valid, someone would have broadcasted a fraud proof. And in all of these cases, succinct, zero knowledge proofs like starks can make things better and in many cases reduce the need for one of the security assumptions, the network latency one, so that you don't need to worry about network latency at all. And this is great. Thank you. Are there any questions? Yeah, no. Should I go? So you said that you can use this type of data availability proof for plasma and for watchtowers.
06:07:42.260 - 06:08:23.100, Speaker A: I think I disagree with this, because in plasma, if the data availability, if data has become unavailable and the state transition happens, the damage has already been done. So there's no advantage to you detecting. So the idea would be that whenever you receive a plasma, so in a kind of normal client running plasma, what you would do is you would try to download the data, and if you fail to download the data, then you would exit. Right. So the scheme here would be when you see a new plasma block, you would run a data availability check. And if the data availability check fails, then instead of having download the full block to check that the data is available. Yeah.
06:08:23.100 - 06:09:23.016, Speaker A: Okay. Exactly. Cool. So I'm not sure if it's a question, but I kept thinking through your presentation that when you say the data in terms of data availability, I assume that you mean the block data with the transactions containing it. Because what it makes me think about. So there's an assumption that somebody could watch the chain or watch something, and they can basically, in a sublinear time, figure out the fraud proofs, but I would state that it's not even possible in the systems like Ethereum, or I would even say bitcoin. The problem is the state, of course, because even if you know the composition of the blocks and the transactions, you would not be able to verify it unless you have the state which precedes this transaction.
06:09:23.016 - 06:10:34.704, Speaker A: And in order to compute the state, you have to run everything from the beginning or do some other tricks like download it from the state first. And that's why, for example, when people designed the fast sync in Ethereum, they were thinking, oh, we're going to just fast sync to the snapshot and then probabilistically verify everything that happened before. But of course that didn't work because the state became so large that even probabilistic verification wasn't practical. So what I'm saying is that the fraud proofs are something which probably will be very expensive to produce, and I'm basically challenging the assumption that there will be a lot of people actually doing fraud proofs at all. So I would say this is the very weak assumption. So in an Ethereum context, this scheme, working on top of Ethereum as is, does rely on basically at least one honest node, having run an archive node, which also has a bit of extra software attached. Given that we're looking at raising gas limits.
06:10:34.704 - 06:11:46.940, Speaker A: And then two more years happens and it goes down to the case where there's zero people running archive nodes, then yes, there is going to come a point where basically if people forget historical data too much, then they won't be able to create fraud proofs and they won't even be able to create these Merkel routes to verify data availability. So yeah, there is definitely a kind of implicit assumption that people keep storing data. And if this is broken, then yes, the scheme also fails. If people do have the data, then if you are an archive node, then producing the fraud proofs becomes easier. Also, another thing that you could do is you could have a scheme where basically for every block, basically a stateless client system, where for every block you can generate the block plus the transactions plus the witness, and you could potentially have every node agree to store 0.1% of witnesses or something similar. So there's two ways to go about this.
06:11:46.940 - 06:12:30.664, Speaker A: One is kind of what's looking like the current ETH two approach, which is basically kind of establishing very strict state size control over the entire system, and so not really having much state. So you can do fraud proofs with just the block and a tiny bit of extra data. And the second approach is to kind of do a bit of stuff on top to basically ensure that some nodes actually do have these witnesses. And potentially you could even just stick witnesses inside of this data, too. Thank you so much. All right, so now are the actual closing remarks. Thank you for your talk.
06:12:30.664 - 06:13:12.790, Speaker A: Hi, I'm Uri Klodny, co founder CEO of Starquare. Thank you, Vitalik, for these off the cuff closing remarks. So I wanted to say thank you to a whole bunch of people. Thank you, Anna, for emceeing the day. Thank you Adi Aviv and Hadas of Faza, the production company who produced this event. Thank you to our sponsors, the Ethereum foundation, the Tezas foundation. Thank you Shield, for initiating and running a beautiful workshop, Stark 101 yesterday, and the entire starkware team behind it to make that whole thing happen.
06:13:12.790 - 06:13:39.290, Speaker A: Thank you to our many speakers who came from far and wide. Thank you all. And lastly, one huge thank you to shealy, who saw this whole thing, did a spectacular job. And anyway, we're very happy tomorrow is the israeli elections. So, in the words of Alice to Bob, vote early, vote often.
