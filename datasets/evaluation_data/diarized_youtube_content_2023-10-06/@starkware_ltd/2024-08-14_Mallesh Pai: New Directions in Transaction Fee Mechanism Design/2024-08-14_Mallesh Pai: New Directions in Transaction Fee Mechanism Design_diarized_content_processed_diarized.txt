00:00:00.960 - 00:00:38.893, Speaker A: Thank you. All right, thanks. Sorry about the long introduction. Should have been cut short. So we've, you know, we've done all our three quarters of a day on cryptography and cryptography related stuff and then we got a little bit of crypto economics and now we're just going to go purely into economics. This is, as my defense, this, I am an economist. This is joint work with Noam Nissan, who you probably know better, and also, also Nir Navi, who is a PhD student who's working at Stockware, and Max Resnick who works with me at consensys.
00:00:38.893 - 00:01:57.949, Speaker A: All right, so what's this paper about? It's, it's a really simple sort of collection of observations and I'm going to keep it really simple because you've been doing a whole day of polynomial fields. Blockchains, at least as we know them currently, are extremely resource constrained environments. Right? You can't do, you can do more on L2s than you can do on L1s, but in principle they're not comparable to like just modern, you know, PCs. Almost often, for example last night, the demand for block space can greatly exceed supply. The main purpose of a transaction fee is that in a permissionless environment, we need some way to allocate this finite supply with demand that can be highly bursty, highly stochastic. And transaction fee mechanisms attempt to balance that demand and supply. Now there's this problem then of how do I measure how much is a transaction? Using the sort of formal theory of mechanism design tells us that if you want to correctly price a transaction, you want to measure, I mean, sorry, not they didn't have transactions.
00:01:57.949 - 00:02:58.295, Speaker A: If you correctly want to measure the. If you want to correctly charge somebody for what they are using, you need to find out what they're using and in particular using that, find out the value of what they have displaced. To figure out the value of what they have displaced, you need to figure out what they're using. So there's this question there of just what resources is a given transaction using? If you're undercounting this resource, if you're undercounting how much a transaction is using, and that is a problem that some blockchains are facing. For example, Solana, that means that the transactions are paying too little relative to their load on the network and you're going to incentivize things like spam. The flip side, if you get conservative, if you overcount how much resources they're using, you'll be underutilizing the available resources. So you'll be actually pushing through less throughput than what your blockchain is mechanically capable of.
00:02:58.295 - 00:03:32.953, Speaker A: An example of what's done is Ethereum. Ethereum measures transaction sizes in something it calls gas. As many of you probably know, but we have some pure academics in the audience. Every opcode has a given fixed gas cost. It keeps track of a base fee per gas. So the part of consensus is what is the current price of gas? And any given transaction they just meter how much gas did you use, multiply it by the base feed per gas, and that's the price you have to pay. Give or take to a first approximation, that's what's going on.
00:03:32.953 - 00:04:16.857, Speaker A: Okay, now the problem with this is it's sort of compressing everything down that a transaction is using into a single dimensional measure of the size. But blockchains are really an amalgam of multiple resources. So there's compute, there's storage, there's networking. And transactions vary a lot in how much they use different resources. So for example, in the context of L2S, you have, because of L2S, Ethereum has introduced these things called blobs. Blobs are just sort of data stores that are little chunks of data that come along with the block. On Ethereum, blobs use a lot of storage and networking, but they don't use compute.
00:04:16.857 - 00:05:00.565, Speaker A: Now if you're going to meter these things in the same way, the problem is you won't know which of your constraints is binding. Are you hitting the blocks Networking constraint? Are you hitting the blocks computation constraint? Are you hitting the blocks, you know, storage constraint? And in the context of L2s, there's even more resources. So for example, oops, typo already. There might be different transactions, might correspond to different costs in terms of computation versus proving. There might be op codes that are really easy to compute but really hard to prove. And conversely, stuff that's really easy to prove but hard to compute. In particular, using a single dimensional measure therefore doesn't capture the full picture of which resources are constrained.
00:05:00.565 - 00:05:18.325, Speaker A: By the way, I'm an economist, so I'm not used to talking uninterrupted. If anyone has questions, please stop me. Let's have a picture. And these are much simpler pictures. Here's two transactions. There are two resources. We call them compute and storage.
00:05:18.325 - 00:06:10.017, Speaker A: Transaction one wants to use 75 off computer and 20 of storage. Transaction two wants to use 10 of compute and 85 of storage. Okay, these are two particular transactions. Now note that these two transactions are intensively using different resources because they're using different resources. In principle, we could squeeze both of these transactions into the same block. So if you think the total amount of both compute and storage available in a block is 100, you could in principle have a block that contains both transaction one and transaction two. The trouble, of course, with this is that if you're going to keep track of multiple dimensions of what a transaction is using, so you're going to keep track for each.
00:06:10.017 - 00:06:56.345, Speaker A: Instead of having just a single GAS measure associated with each opcode, you have a compute measure, a storage measure, a networking measure, and constraints relative to that, then your block packing algorithm is actually much more complicated. You're doing a multidimensional knapsack. Single dimensional knapsacks are at least approximate. Well, sort of. If it's a continuous knapsack, which it's close enough to in a given block, then it's easy enough. Multidimensional, not so easy. What's an easy simplification you could do for each transaction, you could just keep track of the max resource usage of that transaction, and you keep that as a proxy for the size of the transaction.
00:06:56.345 - 00:07:28.185, Speaker A: This is clearly easier because it produces feasible blocks. And the first question we want to ask and answer is, given a collection of transactions with different resource resource usage profiles, what is the loss in throughput if we use the max resource usage as a proxy for the size of a transaction? Okay, this is a really simple question to state. You can state it in the context of this example. For this. This example, for example. So here were my two things. I come up with my measure.
00:07:28.185 - 00:08:00.409, Speaker A: My measure is. Sorry, mucked it up already. The blues should be. This blue should be at 75. This blue should be equal to the green at 95 at 85. So now what's the problem with this cumulative measure? This. Sorry, not this cumulative measure, this max measure which is capturing the most used resource of each transaction.
00:08:00.409 - 00:08:30.307, Speaker A: When. If you were to just try and pack the blocks using this max measure, you'd say, well, I can't fit both of these transactions into the same block, right? This one is using 75 according to the measure. This one is using 85 according to the measure, you only have 100. So you would say, no, you can fit in only one. But I already showed you that with multidimensional. If you were to do multidimensional bin packing, you could fit in both. So that's an example of what you lose.
00:08:30.307 - 00:09:09.397, Speaker A: The question is you're losing. The question is how bad is it in general? So to answer that, we need a model and we write Down a really simple model which is we're going to say there are N types of transactions, each with a unique resource usage profile. So I let T1 through TN be a set of transaction types. There are M resources. So I'm modeling the blockchain as having M resources. And I'm going to normalize the amount of resource available in a block by one. So each resource has a amount of resource availability normalized to 1.
00:09:09.397 - 00:09:53.525, Speaker A: And I'm going to have an N times N matrix where UIJ is the amount of resource j that is used by transaction type I. Okay, the simplified GAS measure is going to be denoted U bar I. So U bar I is for any given transaction I, it's the amount of the maximum resource it uses. Ellie, practically speaking isn't like very large or infinity, because like there could be so many different. Think of them as just the set. So yes, N could be very large. Think of them as just, but you could think of them as just a set of opcodes that you have available.
00:09:53.525 - 00:10:22.385, Speaker A: That's like we have, we have a price list on. We have a price list on Ethereum, right? Of these are transactions. I should have called them like I can break it down and I can break it down by opcode. So just transaction can be a sum of transaction can be a sum. So just break it down into the. Sorry, break it down into the primitive instructions available. And currently we have a price list, right? Like on all blockchains we have a price list.
00:10:22.385 - 00:11:03.215, Speaker A: S store transfer is 21,000 gas because it sums up to doing various operations and so on. All right, so there is some finite set of transaction types or instructions. You could just think of them as primitive instructions. Those are the simplest things you can do and they're finite. All right, so U bar I is the maximum amount of any resource that transaction type I uses. And that is sort of. Is it maybe a compute intensive transaction? Is it a proving intensive transaction? Whatever? Consider any formal.
00:11:03.215 - 00:11:56.667, Speaker A: Consider any distribution over transaction types D. I'm going to call it a distribution even though think of it as how many of those transactions are appearing in a block such that the maximum measure is using at most one. This is any sort of. So what is D in this case? It is any vector of the amount of each transaction that is appearing such that the block as a whole would be feasible if it used the maximum measure. So I see this collection of transactions that I denote by D. I look at each transaction's max. I sum up how much am I using if that sums to less than 1? Because it's the max.
00:11:56.667 - 00:12:35.187, Speaker A: Every resource is being used at most one and therefore it is a feasible block. It's using at most one of any resource. The trouble is, of course. Oh, sorry. The trouble is, of course, if I am using the maximum measure and I'm getting I'm packing a block with the ma, if I'm using the maximum measure and packing it so that it's less than 1, I've produced a feasible block. But the actual usage of resources is this vector that is just D or D transpose whatever you want times the matrix U. That's the vector of resources that I'm actually using.
00:12:35.187 - 00:13:38.341, Speaker A: And the most used resource is just the maximum component of that vector. In other words, even if the max measure is full, so I've packed the block fully, I might not have packed any given resource because I've done this coarsening by using the max measure. So our question, our first question is what is the worst case loss in throughput if you use the max measure as a proxy for the size of the transaction? And we have a simple answer, which is you can reduce this to a zero sum game. In particular, consider you the set of resource usage profiles of these transactions to be a zero sum game between to be the matrix of a zero sum game between what I'll call the blockchain and an adversary. The adversary, who is the minimizer, is the role player and and has a set of strategies. What is the set of strategies? It's any D that's feasible according to the max measure, the blockchain, the maximizer chooses any weight vector W over the set of transactions. Sorry.
00:13:38.341 - 00:14:29.443, Speaker A: Over the set of resources that sum to one. The worst case loss in throughput is the value of the zero sum game. So if you tell me what are your various opcodes and what are their usage profiles. I actually have now a simple linear program that tells you how much are you using, how much are you losing? If instead of packing according to the true usage profiles by resources, you pack according to the max, that's part one. This answers the question for the worst case distribution over a single block. So the worst thing you can see in a single block is this distribution. The next question you might ask is could an adversary choose different profiles of transactions over different blocks to make us worse off on average, average throughput over a whole bunch of blocks.
00:14:29.443 - 00:15:04.753, Speaker A: And the answer is no. And it's actually, we spent a lot of time on this and then it turned out to be just a trivial application of Jensen's inequality. So that was cute. So one way to interpret our result is that if the value of this game is sufficiently high, then don't waste your time thinking about multidimensional transaction fees. Instead just use a simple single dimensional proxy, the max. But how should we do it if the game is low? Well, with large m. Like I said, doing multidimensional bin packing is hard.
00:15:04.753 - 00:15:38.981, Speaker A: One simple idea is choose some small M prime and partition the M into m prime disjoint sets and use the simple max gas measure on each of these m prime sets. Clearly does better, but it's computationally hard. We're working on simple heuristics and approximation algorithms. Hopefully I'll have more to say soon. All right, that tells us one part of this thing. The next part is then suppose you have multidimensional gas prices. Sorry, multidimensional resources.
00:15:38.981 - 00:16:21.005, Speaker A: Then how should you price gas? Well, with single dimensional we know how to price gas. For example, EIP 1559 is known to have decent properties. In practice there are possibly some issues in tuning parameters. There's a fool by randomness problem. This is a free advertisement for our talk at SBC on Friday, so please stop by if you want to hear more about that. But what if we had multidimensional gas? Well, one set of results suggests that EIP 1559 style schemes still work well resource by resource. You just do EIP 1559 and a reduction from online convex optimization tells you that you're still going to be doing sort of the right thing.
00:16:21.005 - 00:16:57.141, Speaker A: And there are other papers that suggest using other pricing mechanisms. So you outsource the pricing to a broker and if their job to set the price correctly. But I'm going to talk about a difficulty that hasn't been studied yet. So one difficulty with multiple dimensions is that it's hard to define the true underlying these true underlying resources. So let me give you an example. Ethereum now has two resources. Effectively it has regular gas or regular transactions and it has blob transactions and currently these two are priced separately.
00:16:57.141 - 00:17:22.695, Speaker A: So it's doing independent pricing on these two resources. If blob usage goes up, the price of blobs goes up. But this is what you get when you invite an economist. You get some free Latin thrown in the terrasparibus. The price of the regular transaction stays the same. So even then it means everything else staying the same. So maybe block transactions are going up because there's also Demand on the L2.
00:17:22.695 - 00:18:21.562, Speaker A: But like yesterday for example, there were a lot of people wanting to transact on the L1. There was a lot of load, gas prices went up to 360 Gwei, resource block prices stayed at one way. Now, that's because we have defined these resources as gas and blobs, but note that both of these are sharing the same inherent underlying resources. So they're both sharing the networking capacity of the validators, they're both sharing the storage load that these validators take. So if we design these, define these resources incorrectly, some resources may be underpriced. So blob gas being one way when the network is clearly struggling to handle all transactions. Last night at 360gwei, or conversely two months ago, when the price of blobs went up to 7,000 Gwei and the price of regular gas stated, you know, 15 way.
00:18:21.562 - 00:19:07.831, Speaker A: 15 way. These are examples of you're clearly not correctly allocating the whatever throughput resource, whatever throughput your validators can push, you're not correctly allocating it to the two underlying sort of sources of transactions. So I just wanted to point out that this problem has been observed before. And in economics, we call this problem one of thin markets. So when we slice a large market into underlying thin markets, you potentially achieve efficiency because each source of demand can correctly define itself into the market that it wants to address. But each individual market may be, I wrote underpriced, but I mean mispriced. So here's an example that happened.
00:19:07.831 - 00:19:37.035, Speaker A: So as you, most of you know of or are on Facebook. And Facebook originally was about just universities, so it was just university students. And when they first launched ads, they allowed, they just had a bunch of universities. You had to sign up with a dot edu address and they started allowing targeting. So maybe somebody wants to advertise a party. You're throwing a party at Harvard. You don't want to spend your money advertising to students at Northwestern, right? So you say, okay, you can target Harvard undergraduates.
00:19:37.035 - 00:20:25.599, Speaker A: And let's say the market equilibrium price of Targeting Harvard undergraduates was 50 cpm cost per thousand. So every thousand impressions you'd pay 50 cents. Okay, next what happened was Harvard Facebook started collecting more data, so it allowed, it had more insight into what demographics you fit into. So it could say, it allowed you, for example, to target by undergraduate majors. So you could say, okay, I want to target not just Harvard undergraduates, I want to target Harvard econ majors. Or you could target Harvard computer science majors, you could target Harvard English majors. But the costs of each of these underlying sort of sub targetings was they found it was incredibly low.
00:20:25.599 - 00:21:14.753, Speaker A: So people could target in equilibrium. People were targeting Harvard Econ majors for maybe 5 cents per thousand views. Now if you think about it, the union of all Harvard English majors and math majors and econ majors is just a set of Harvard undergraduates. So you could target all Harvard undergraduate students by running ads targeting each major for 1/10 of the price. And the story is basically you're splitting the demand up across into these subcategories. But when that happens, you don't apply sufficient pricing pressure to realize that there are also demand that wants the union, not just. So the pricing algorithm didn't account for this simple pricing algorithms in general cannot account for these cross market pressures.
00:21:14.753 - 00:21:56.561, Speaker A: So unless you define the resources incredibly correctly and incredibly narrowly in terms of like maybe incredibly primitive things like networking versus compute versus storage, you're not going to get. You're not, you know, something like an EIP 1559 multiplier is not going to price correctly. And that's what we're observing on L1 yesterday. So a version of this is what we saw happening on Ethereum's gas. Ethereum saw spike in gas to 360g but blob gas stay constant. Conversely in April blob gas spiked and transaction gas stayed the same. Both use the same underlying resource.
00:21:56.561 - 00:22:45.361, Speaker A: So in principle appropriate market pricing should sort of take that into account. So maybe you can push through more transactions and fewer blobs when people really want to transact on mainnet. And conversely, you can push through more blobs and fewer transactions when people really want to just post L2 data. So defining and metering this using these methods should be worthwhile. I'll stop here and take questions. So where is this gas optimization happening?
00:22:45.393 - 00:22:47.263, Speaker B: Is it happening like in the MEM.
00:22:47.319 - 00:22:54.399, Speaker A: Pool or what do you mean by how are you optimizing these transactions?
00:22:54.447 - 00:22:55.595, Speaker B: At what stage?
00:22:56.495 - 00:23:50.017, Speaker A: So currently, I mean our perspective is the transactions are not strategic, they're just coming into you. But you're trying to set up the prices so that you can do two things. One, in any given block you don't promise more than you can deliver. So you don't sometimes take like 30 megs of data when you can only do 1 meg of reliable throughput. We need to set how to price a transaction so that we are using each of our dimensions correctly. So that if you are sending me a high data transaction and Ellie is sending me a high compute transaction, I'm potentially taking in both. Because together I can if you know, I have enough resources to take care of each of you, but it's still simple enough for me to compute and Price correctly.
00:23:50.017 - 00:23:57.641, Speaker A: That's the way we're thinking about it. So it's not transaction optimization, it's sort of transaction price optimization for the blockchain.
00:23:57.753 - 00:23:58.513, Speaker B: Does that make sense?
00:23:58.569 - 00:23:59.965, Speaker A: Just understood? Yeah.
00:24:05.515 - 00:24:13.283, Speaker B: All right, maybe one comment and one question. First comment is if you want to throw a party at Harvard, invite the business majors.
00:24:13.459 - 00:24:16.011, Speaker A: Probably, yeah.
00:24:16.123 - 00:24:39.805, Speaker B: My question about this too is it sounds almost like you're suggesting changing pricing proactively or actively as opposed to having some kind of derivative market on this. Now would it a separate market for gas, et cetera. Options predictions solve some of these problems because smarter people who knew more econ would actually predict this stuff and it would all pan out in the end.
00:24:40.665 - 00:25:39.771, Speaker A: So there is a separate question of how do we run and price a block space futures market, a gas price futures market? Because that would be incredibly useful for a lot of people. I think there is this fundamental question before that which is, you know, think about the following which is what Ethereum says is its validity condition is any block has to contain less than 30 million gas and they are happy to do that. Right? But that might be if you just want to. I mean just going back, I'm going to be repeating myself. But there is this question of well, in principle yes, 30 million gas is safe. We know 30 million gas is something we can do on our Raspberry PIs in under 12 seconds. But in practice, maybe given the distribution of transactions that we're seeing, we can do more than 30 million gas because we know that some transactions are only going to constrain us on some resources like the data heavy.
00:25:39.771 - 00:26:21.265, Speaker A: Some transactions are going to be constrain us on proving. Some transactions are going to constrain us on compute. So can we come up with better like when is it that it's worthwhile to set up a multidimensional pricing scheme instead of having one measure gas and say anything is valid as long as the total block is less than 30 million gas. Can I create gas 1 gas 2 gas 3? Any transaction is valid. Any block is valid as long as it uses less than 30 million gas 1, 10 million gas through and 20 million gas 3 and how much given the set of opcodes that I have, how much can I potentially gain from that? In the worst case, that's sort of the enterprise of the paper. Okay, cool.
