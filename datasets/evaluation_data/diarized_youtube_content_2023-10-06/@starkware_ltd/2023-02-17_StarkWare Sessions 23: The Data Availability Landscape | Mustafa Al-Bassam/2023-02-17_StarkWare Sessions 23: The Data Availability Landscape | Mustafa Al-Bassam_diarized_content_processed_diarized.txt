00:00:00.390 - 00:01:00.330, Speaker A: You. Hello everyone, my name is Mustafa. I'm a co founder at Celestia, which is a modular data availability layer, one blockchain. And today I'm going to talk to you about I'm going to present an overview of the landscape for data availability solutions in 2023. But this talk isn't specifically about celestial you because I'm going to try to give a neutral overview of the data availability landscape. But that being said, obviously I'm biased, so you can take that how you will, but I will try to be neutral. So first I'm going to discuss what is data availability, a quick brief overview, and then I'm going to discuss different approaches to data availability and different trade offs in this space, and what the kind of trade offs between these different solutions are because there isn't necessarily one size fits all solution.
00:01:00.330 - 00:02:17.490, Speaker A: Different applications might want to have different security or performance trade offs. So what is data availability? So if we recall that all blockchains, including roll up chains, consists of two main components. The first component is the block header and the second component is the actual transaction data in that chain. And the block header commits to the transaction data, usually in the form of a transaction merkel route. And the question of data availability asks, if you're a node or a program that only downloads the headers, how can you actually check that the data, the transaction data that the header points to was actually published by the producers of that block. Because the problem is, if they only publish the header but they don't publish the actual data, then no one knows what the actual transactions behind that block header actually are. And that can cause various problems for roll ups chains, specifically for ZK roll ups or validity roll ups.
00:02:17.490 - 00:03:43.690, Speaker A: Data availability is important because if the sequencer doesn't publish the state different the state changes or the transactions, then users won't actually know what their balances are, they won't know what the state of the chain is. And that means that that kind of boils down to effectively an attack where the sequencer can freeze people's funds and hold them hostage, and potentially other things like bribery attacks. They can freeze people's funds unless you pay them. For example, for optimistic roll ups, it's even more important because optimistic roll ups rely on fraud proofs. And if the transaction data behind that roll up isn't available, then that means that block producers full nodes cannot generate a fraud proof if there's a malicious transaction inside the roll up block, because they don't know what the transactions are, so they can't generate fraud proofs because they can't see which transactions are bad. It's worth noting that because there's a lot of confusion about what data availability really is, data availability is not the same thing as data storage. Data storage is more about making sure that the data is stored long term.
00:03:43.690 - 00:05:07.334, Speaker A: But data availability is only about making sure that the data was even published in the first place, so that storage providers actually have the opportunity to download that data and store it. So data availability is not concerned with long term data storage, it's only concerned specifically with the question, how can we make sure the data was published just once on the Internet? So a better name for it might actually be data publication. So like for example in Ethereum EIp four eight four. Data is only guaranteed to be stored by nodes for 30 days and then it's deleted because it's only about making sure it's published so that people can generate fraud proofs or users can download the data so they know what their balances are. It's not about long term storage. Now the interesting thing is that data availability wasn't really a question before roll ups, because before roll ups, all blockchains work the same way where you have full nodes and full nodes download all the transaction data and that's how they know the data is available. Obviously, if you have to download all the data anyways, then obviously you know you have the data, it was available because you could download it, but that's obviously not scalable.
00:05:07.334 - 00:06:17.970, Speaker A: And that's why we have roll ups, because it's not scalable to have a system where every node has to download every transaction. And that's why roll ups exist, because they effectively shard execution into different roll up chains. And the main chain or the settlement layer does not have to execute all the transactions in the roll up chain. The question of data availability asks how can light clients or smart contracts check that all the data for a roll up chain or any specific application was actually published or was actually made available without actually having to download all that data yourself and check it. So I'm going to give a brief overview of different data availability mechanisms and their trade offs. And there's kind of like, I guess two buckets here on chain data availability and off chain data availability. On chain data availability is when data availability is guaranteed by the layer one network.
00:06:17.970 - 00:07:07.550, Speaker A: So if data is unavailable, then the fork choice rule of that layer one network actually just rejects the block outright. So it's like the data availability is coupled with the actual chain. The off chain data availability is opposite. So data availability is guaranteed off chain or by some third party chain or by some third party service that is not directly connected or part of the layer one's consensus mechanism. So we just talked about the most obvious data availability solution, which is full node download all data. The other on chain, as we mentioned, that's not scalable. The other on chain data availability mechanism to make on chain data availability more scalable is called data availability sampling.
00:07:07.550 - 00:08:01.460, Speaker A: I'm not going to go into the weeds of how it works. There's lots of material about it online. But the general principle is that block producers can commit to something called an erasure coded version of the block, and that basically allows light nodes to download random chunks of the block. And when they download random chunks of the block, then that effectively gives them an extremely high probability guarantee that 100% of the data is available by only downloading a very small percentage of the data. And this is used like Reed Solomon encoding, which is similar technology that's also used in stocks. This is a new technology that isn't widely implemented yet. There's various protocols including Celestia, Ethereum and Polygon avail that's currently implementing this.
00:08:01.460 - 00:09:26.350, Speaker A: And there's several current challenges that people are researching. The first one is in order for data availability sampling to be fully secure, you need to have some network, you need to have some peer to peer mechanism for light nodes, for full nodes to reconstruct the block by downloading the samples from light nodes. So if light nodes collectively, let's say you have 1000 light nodes and they collectively downloaded enough samples or enough random chunks from the block that they can collectively reconstruct the block. If the block producer was trying to be malicious and hide the data, you need to have a peer to peer protocol for that. And that's kind of quite challenging because it's a unique challenge compared to other peer to peer distribution protocols like BitTorrent or ipfs, because it's like lots of different nodes have small pieces of the data and they have to discover each other. The second challenge is proving that the erasure code was constructed correctly and there's kind of like two different ways of doing it right now. The first one is using what's called the KZG commitment scheme, which is a validity proof scheme.
00:09:26.350 - 00:10:36.050, Speaker A: You can prove that the erasure code was constructed incorrectly. But the trade off there is that the times to prove the commitment, to prove that something is inside the commitment is quite slow, what's called the KZG opening. And then the second way of doing it is fraud proofs which are faster, which makes computing the erasure code and generating proofs faster. But the trade off there is that you have a light nodes have to wait a challenge period, usually a few minutes, before they can accept that block is valid because they have to wait to see if there's going to be any fraud proofs. And the third challenge is, in my opinion, the biggest challenge to data availability sampling is light node adoption. Because data availability sampling is only completely secure if there's enough light nodes in the network such that they can collectively download enough pieces of the block such that they can collectively reconstruct that block. But right now, light node support for any chain other than bitcoin is extremely poor.
00:10:36.050 - 00:11:48.380, Speaker A: This is something that bitcoin did extremely well. Like if you can download bitcoin wallet on your mobile phone that has about 5 million installs, and it's a bitcoin like client that connects directly to the bitcoin network. But we don't really have anything like that as adopted for other chains like Ethereum or a chain after that. So a big challenge there is how do we move away from the metamask module where wallets just connected in Fiora? How can we embed light clients directly into users'wallets so that we have widespread light node adoption? And part of the challenge there is how do you actually bootstrap the network? Because if there's not enough like clients, then the scheme kind of basically, as I said, doesn't have full security. It reduces to what's called a proof of data retrievability scheme, which still gives you some security, but not as high as a proof of data availability scheme. I have three minutes left, so I'm going to kind of go to off chain section. You're ready.
00:11:48.380 - 00:12:40.940, Speaker A: In terms of off chain solutions, there's kind of like four different kind of mechanisms. The first one is one where there's no guarantee at all. And that's actually kind of not bad for some use cases. Like for example, if you have NFT and NFT data just points to some IPFS URL or URI, there is no data availability guarantee because there's no guarantee that the data behind the IPFS URI was actually ever published. But that's actually perfectly fine for nfts, for example, because obviously before you buy that NFT, you can check if the data for that NFT was actually published, otherwise the NFT would be worthless. Then there's solutions that might have a single trusted third party. So you might have like a single signature or a single party that attests to the availability of that data.
00:12:40.940 - 00:13:47.780, Speaker A: And this is okay for some schemes like plasma cache or mentium, because in those schemes, if the data is not available and the single trusted third party lies, the users actually have the power to exit to the l one. But the trade off there is that this doesn't really work for all use cases. It only mainly works for payments. It's harder to make it work for generalized smart contracts or DFI. Like plasma cash, for example, only works for payments. And that's why plasma was abandoned in favor of roll ups, because it's hard to design mass exit schemes for data unavailability for use cases other than just like payments. There's also data availability committees, and that's just basically like a data availability multi segment scheme where a set of people can guarantee the data availability of some data.
00:13:47.780 - 00:14:47.650, Speaker A: Example of that is like stark weblilliums, there's an honest majority assumption, there's a seven to ten member dacs, and obviously most of them have to actually sign the data. And then another example is arbitram, any trust, which makes a slightly different trade off, 19 of 20 of the committee members have to sign the data. So you get like a higher safety threshold. But the trade off is that you have lower liveness guarantees, because it's more likely that if just one committee member goes down, then you lose liveness. And users can actually just force transaction inclusion by submitting transactions in the l one. But if there's a mass exodus, it's not clear if the L one can handle that kind of capacity. Then finally there's lashable committees, which are like data availability committees, but with some crypto economic guarantees.
00:14:47.650 - 00:15:35.486, Speaker A: If the data availability committee lies about the data availability, then that means that they can be slashed, and that gives you some extra crypto economic guarantees. But you can't do this. You can't do this. If the data availability committee is l two, it has to be an independent l one, because you can't slash data unavailability on chain, but you can only slash them if the data availability committee is an independent chain. Quickly go through these and then I'll take questions. And an example of that is celestiums. So it's what we call ethereum l two that uses celestia for off chain data availability.
00:15:35.486 - 00:16:10.480, Speaker A: And celestiums can be slashed. Celestia can be slashed if data is unavailable by the light nodes, thanks to their availability sampling. And we can see that for the off chain data availability availability landscape, it's a wide trade off space. Obviously roll ups are the most secure because they use on chain data availability. But there's a trade off between gas costs and security level depending on what kind of trade offs your application needs. Thanks. Any questions?
00:16:16.690 - 00:16:31.090, Speaker B: Yeah, for the sort of erasure code based system of the light nodes, do the light notes need some sort of incentive to participate?
00:16:35.610 - 00:17:26.854, Speaker A: Yeah, that's a good question. So the question is about do light nodes need incentives? And that's actually related to the challenges that I brought up is about how do we have wide adoption of light nodes. So in my opinion, light nodes don't need incentives because if you look at bitcoin, for example, as I mentioned, there's a bitcoin wallet that you can download on Android that has 5 million installs. That's a bitcoin like light node. Light nodes have extremely low resource requirements. So you can embed one into a wallet without the user even noticing because their resource requirements are so low that to the user it's just a wallet. So to me the problem isn't really how do you incentivize users to run it because users will download wallets anyway.
00:17:26.854 - 00:17:40.620, Speaker A: They download metamask, they download bitcoin, so on and so forth. The question is how do you embed light node into metamask because it's a browser extension or how do you encourage people to download desktop wallets, for example?
00:17:40.990 - 00:17:53.150, Speaker B: Yeah, but I mean then you're still relying on whoever's designing this license to embed this thing in their volatility, right? Which is fine, but it's not guaranteed in any way.
00:17:53.300 - 00:18:33.126, Speaker A: Yeah, well the first thing to note is that first of all, light nodes are something that benefits the user from a security perspective. But secondly, to guarantee the security data availability scheme, you only need an honest minority. You only need a small number of light nodes for all the light nodes have security. So for example, we're talking about a few hundred light nodes. As long as you have a minimum threshold of light nodes, then that guarantees the full security of the scheme. That only needs a few hundred or 1000 light nodes. So we don't necessarily need millions of light nodes.
00:18:33.126 - 00:19:58.150, Speaker A: So in my opinion that's not really difficult to get as long as there's an option. Yeah, sorry, can you speak louder? Is a question. What do I think of protocols like Eigen layer which separate data availability into separate module? Yeah, so I guess the question is about separating consensus from execution. Sorry, separating consensus from data availability. Well actually so what Eigen layer does is they separate kind of like consensus from peer to peer networking. But I don't necessarily think you need to separate that in order to get the benefits of what Eigen layer claims, which is increased peer to peer network. The main reason why Eigen layer claims higher throughput is basically because it's not because they don't do consensus, but it's because they use kzgs, which means that they claim that the node operators don't have to download they can do sampling instead of downloading the data.
00:19:58.150 - 00:20:43.360, Speaker A: So it's more about using kzgs than separating the availability from consensus. But the main kind of trade off with Eigen layer is that they use ETH restaking, but you can't slash data unavailability on the chain, so the ETH doesn't actually contribute to the security. What they instead have is a dual staking mechanism where the eigen layer token is slashed if data is unavailable. Okay, I'm out of time, so thank you. Everyone know you.
