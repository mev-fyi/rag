00:00:03.560 - 00:00:48.025, Speaker A: Thank you, Eren, for the introduction. Right, fantastic. So, good morning everyone. So, today I'll talk about recent work Stir read Salomon proximity testing with fewer queries. It is quite related and a good follow up to Swastik's talk because it is specifically about retesting. As we'll see in a moment, this work is joint work with my wonderful colleagues Gal Arnon, Giacomo Fensi and Elon Yogev. All right, so the setting is that we are interested in constructing efficient succinct, non interactive arguments snarks.
00:00:48.025 - 00:01:36.671, Speaker A: Here, the main efficiency goal is you want a small proof or succinct proof. That means in this context that the size of the proof string sent from the prover to the verifier is say for example, much smaller than the witness for the instance that you're considering. Okay? And specifically today we focus on a particular setting and that is these cryptographic proofs constructed only using random oracle. Okay, so that means that you have some hash function, ideal hash function. In practice it would be something like Shatu 56. And so these types of snarks are kind of wonderful because in practice when you take the random oracle and instantiate it using shutter56, you get a lot of nice benefits. You get something known as a transparent setup.
00:01:36.671 - 00:02:24.035, Speaker A: This means that the only global system parameter for your proof system is just agreeing on which hash function to use. This is much simpler than agreeing on some kind of common reference string that it must be sampled by a trusted party. Using hash functions also means that you don't have slow public encryptor sitting around. This makes implementations particularly efficient. They're also plausibly post quantum secure, like many of the known constructions of snarks in the random oracle model are also proof secure in the so called quantum random oracle model. Okay, so these are many wonderful benefits. And in fact snarks in the random Oracle model, or more precisely, their instantiations, are widely deployed.
00:02:24.035 - 00:03:08.099, Speaker A: They're used across a number of companies. They were initially pioneered and deployed by starkware and now by this many other exciting efforts. So if you open up, how does a snark in the random oracle model look like? Usually they'll be built from something called an interactive Oracle proof. Swastik has already introduced these. So IOPS are an interactive analog of of the classical notion of probabilistically checkable proof. So you have this interaction where the prover sends over. You have an interaction where in every round the prover will send over some long message which the verifier can query at any location it desires, and then the verifier replies with some random message.
00:03:08.099 - 00:03:56.697, Speaker A: And this interaction goes on for some time. This is what snarks are built from. How are they built from these things is something known as the missense Spooner transformation that kind of says, okay, thank you for the iop and now I'm going to use the ideal hash function to, you know, with things like numerical tree and the feature mir transformation to assemble snark from that. Okay. And perhaps the most important efficiency measures of the underlying IOP are the proof length and the query complexity. The proof length is the total amount of information sent from the prover to the verifier, and the query complexity is the number of locations that the verifier will read into these proverbs messages. Why do this matter? Because they directly influence the argument size.
00:03:56.697 - 00:04:43.123, Speaker A: So the argument size of the corresponding snark is proportional to the query complexity and logarithmic in the proof length. So in particular, a key concern is constructing iops with small query complexity. Why? Because then that gives you ability to construct snarks with small argument size. Okay, but I also wanted to mention I'm happy to get questions during the talk, so please just raise your hand at any moment. Okay? All right, fine. So we want to construct good iops, because they give us, through the BCS transformation, good snarks. In many iops of practical interest, the main bottleneck is a subroutine already introduced by Swastika, and that is these proximity tests for the Reed Solomon code, they really are the main bottleneck.
00:04:43.123 - 00:05:31.481, Speaker A: So when you look at the query complexity of the overall iop, it is primarily dictated by the query complexity of this subroutine. The subroutine is not the entire iop, but it is the workhorse of the iop. Ok, so all of this is to say is that it motivates the study and efficient construction of proximity tests, or in this case, iops of proximity for the Reed Salomon code. The Reed Salomon code was already introduced in Swastik's talk. These are evaluations of low degree polynomials over a certain domain. In this talk, I'll primarily keep track of these three parameters. N is the domain size, D is the degree, and rho is determined by n and D and it's the rate.
00:05:31.481 - 00:05:53.091, Speaker A: Okay, so it's D over N. It's kind of, you can think about it as the inverse of the blowup. When you encode information, there is some implicit finite field which must satisfy some properties, like Swastik alluded to. And indeed, generally powers of two. But so, yeah, these are just mostly technicalities. All right, great. So, an IP of proximity.
00:05:53.091 - 00:06:20.269, Speaker A: You're given this function, you want to test whether it is indeed a code word or far from it. And the way that the prover is going to help the verifier do that is they're going to exchange messages. And however, the verifier has the power to query the proverbs messages at specific locations. Okay, great. So these are. And then you have the usual completeness and soundness conditions. If F really is indeed low degree, then the verifier should accept.
00:06:20.269 - 00:07:01.715, Speaker A: The prover should make the verifier accept with probability 1. If F is far from low degree, then the verifier should accept, with small probability. A small telechemicality here is that not only do you want the acceptance probability to be small, but you want a much stronger notion of soundness, known as round by round soundness, to be small. This is something needed for essentially feature mirror to make sense to be secure. So it's important in all of these IOP constructions not only that soundness error is small, but moreover, the round soundness error is also small. Something important to keep in mind. Great, so this is to say that, okay, we want good snarks.
00:07:01.715 - 00:07:27.775, Speaker A: Good snarks come from good Iops. Where do good Iops come from? Well, several places, but primarily they come from good Iops of proximity for the Reed Solomon code. Okay, so now we're down into this kind of very well defined, nice mathematical question. And you have already seen an example of a design of an Iop of proximity that is the Fry protocol. Okay, so today I will tell you about an improvement on the Fry protocol. Okay? And so I'll tell you now kind of what we achieve in this work. But so far, so good.
00:07:27.775 - 00:08:12.063, Speaker A: Any questions? Good. All right, so what do we get? So, as Swastik alluded to, constructions of these IUPs of proximity have a reduction flavor. You start with some problem of a certain size, and you try to kind of reduce it to roughly the same problem, maybe some smaller size. And you keep doing that until the problem has constant size. And you can just send over the polynomial and the verifier, you can just directly check, yes, it's low degree. This reduction creates a round complexity. So in this case, what we achieve in this work is precisely the same round complexity and proof length as in the Fry protocol.
00:08:12.063 - 00:08:38.767, Speaker A: So in each round of the reduction, you have the problem size. You start with problem size D. That's the degree you will need log D rounds, okay? And each time the prover will send something of size n, Then n over 2, n over 4. All of these things adapt to, say, 2n. So that's a linear proof length. What we improve upon is query complexity. Okay, so now we get this expression may look a little bit ugly, but let me parse it for you.
00:08:38.767 - 00:08:54.833, Speaker A: Okay, so there are three parameters here that appear. One is the degree D. Okay, so you want to be exponentially better than the degree. You also have the rate. Think of the rate as a constant. So let's just ignore it for now. And then, of course, you also have the security parameter.
00:08:54.833 - 00:09:24.839, Speaker A: Remember that ultimately we need to have small rumb around sum of sadder. Okay, so if you want sum of st to be 2 to minus lambda, that is going to be reflected somewhere in your query complexity. And here the query complexity is going to be lambda times log log of d plus log d. Okay? Okay, fine, whatever. Just some expression. Let me compare it against what Fry's achieves. Fry gets the product of the main terms, so it's lambda times log d rather than lambda plus log d.
00:09:24.839 - 00:10:15.655, Speaker A: Okay, so this is the asymptotic improvement and hopefully now the expression at least maybe if it doesn't make sense standalone, at least they make sense in comparison to one another. Okay, so the improvement is in the query complexity. There is this curious appearance of the square root of the rate, square root of rho. This has to do with what is known today about the least decoding properties of the Reed Salomon code for both protocols. There is a conjecture that one can consider to improve both protocols simultaneously. And if you're willing to assume some properties that are currently not proven about the Reed Solomon code, you can replace in both cases the square root of the rate with just the rate. Okay, in both places.
00:10:15.655 - 00:10:43.043, Speaker A: So both, there is a. Where is the laser here? There we go. So it appears here and here. So in both places it can go away. This one is also can be. Can be made to be improved. This comes out with basically, in general, the query complexity will depend on the distance, but the distance is set by the preamble of the protocol.
00:10:43.043 - 00:11:21.557, Speaker A: And you can set up the preamble to get an even better distance under that assumption. Okay, so this is what the what we achieve. And so today I will tell you some numbers about this protocol. And then how does this protocol look like in comparison with the Fry protocol? Okay, so that's all we're going to do in the next 15 minutes. More questions about the result itself. Okay, so we have implemented this protocol using ARC works as a backend. We implemented both Fry and Stir in the very same code base with the very same kind of underlying building blocks.
00:11:21.557 - 00:12:18.439, Speaker A: So we're comparing apples and apples and what we get is that not only does the do we have better asymptotics, but when you kind of reveal the underlying constants and you measure them, you do get fewer queries. These fewer queries after the compilation will lead to smaller argument size. It will also lead to because remember that in the compilation you kind of add Merkle trees and the feature mere transformation. So the resulting argument verifier will also be tasked with evaluating hash functions SHA256 some number of times. Okay, so because the verifier overall is more lightweight, it will be evaluating hash functions fewer times. This is great, because less verifier hash complexity means that when you use the verifier recursively, it is faster and more recursive friendly. Right, because when you express the verifier as a circuit.
00:12:18.439 - 00:12:58.325, Speaker A: Now using much fewer invocations of SHA256 means that the verifier as a circuit is also much smaller. Okay, the prover runtime is rather similar. We'll see graphs in a moment. And overall, the improvements of STR over Fry improve as the degree increases and as the rate increases. Okay, so here are some numbers with a target of 128 bits of security. Let's say, for example, we're trying to load test the Reed Salomon code with the degree 16 million and rate 1 over 4. And let's say that we are looking at not the query complexity, but the resulting argument size after compilation.
00:12:58.325 - 00:13:26.873, Speaker A: Okay, and on one hand we see what Fry achieves, which is let's say around 170 kilobytes. And stir it's around 110. Okay, you can also count the number of SHA 56 invocations. Again, after compilation, there is three and a half thousand for fry and 1.8 thousand for structure. We can look at what happens if we increase both degree and rate. For example, we can go to degree a billion and rate a half.
00:13:26.873 - 00:13:58.735, Speaker A: And you see here the improvement in argument size is two and a half and the improvement in number of hash invocations is almost threefold. Okay, these are just some data points, but you can look at graphs. For example, in the top left you can see argument size as a function of degree as degrees exponentially increases. The red line is Fry. The blue line below it is Stir. You see that not only the blue line is below the red line. But moreover that you see the different symptoms because they're growing at different speeds.
00:13:58.735 - 00:14:36.237, Speaker A: Ditto for the verifier hash complexity. In the bottom left you can see the prover time and you see that very similar, the verifier time. Something interesting happens due to some additional logic. When the degree is sufficiently small, the STIR verifier is slightly more expensive. But as the degree increases, this flips and these asymptotics kick in. Okay, so all of this to say is that the asymptotic improvement is perceived in sort of concrete experiments in a prototype implementation. Okay, great.
00:14:36.237 - 00:15:09.573, Speaker A: So this is just what I've told you is, hey, we have. There's a new logitest called stir. It has better query complexity than fry, including in practice. Now, I want to tell you about how to assemble it before I do that. Any questions? Yeah, so I think somebody's going to take your microphone first. There's a question over there. Can you give some intuition as to how the queries are fewer? Oh, yes, that is exactly what I'm going to.
00:15:09.573 - 00:15:21.149, Speaker A: The question is, why are there fewer queries? There's a reason and we're going to see it now. Okay, thank you. Yeah, good. All right, so we have 10 minutes for that. I think we can do that. So. All right.
00:15:21.149 - 00:15:51.635, Speaker A: So Swastika has already talked about this operation. In his talk it was called sketching. Here it's called folding. Whatever. It's the same operation. We're just going to review it and we're going to talk about it. So it's a randomized reduction from proximity testing to a Reed Solomon code with parameters N, d and rho to a smaller Reed Solomon code where now both the domain and degree have been reduced by the same multiplicative factor K, and therefore the rate remains the same.
00:15:51.635 - 00:16:34.419, Speaker A: And the way it works is that you have the function that you want to test. The verifier sends over a random field element and this defines a smaller function that we call it the fold of the function around alpha. It's defined over a smaller domain of size n over k. In Swastik's talk, you've seen the case of k equals 2. That is, when you consider the odd and even parts of the function, you add them up with the coefficient alpha. But this can be defined for a more general case that are powers of two to shrink the domain more than a half. This is actually important for today, the way that folding the properties of folding Swastik has already went over them.
00:16:34.419 - 00:17:03.645, Speaker A: It's local. Any point of the fold function can be kind of recomputed from a small number of points from the original function and it's distance preserving. If the function you start from is far from the bigger Reed Solomon code, then with high probability over the choice of alpha, the folded function is far from the smaller Reed Solomon code. Okay, this is again the properties of sketching that you've seen in Swastik's talk. Great, so this is just. We have the notation to talk about it. In this talk.
00:17:03.645 - 00:17:38.584, Speaker A: The Fry protocol mostly concatenates this reduction. So it says, okay, we start with a function. The verifier sends over this random field element alpha. The prover thinks of the folding and says, hey verifier, here's a function F prime. I claim this is the folding of the function. Please go ahead and check that. And the verifier can check it by comparing F prime and querying it at some number of points against what the folding of the function is, which it can compute because of the locality property.
00:17:38.584 - 00:18:31.325, Speaker A: And then you just recurse at a high level, modulo some details and some simplifications. This is the Fry protocol. An intuition here is that let's say, for example, the original function is far from the bigger Reed Solomon code and the function written down by the prover, let's say for simplicity is in the smaller Reed Solomon code. Then distance preservation will say that with high probability the folder function is far from the smaller Ritzalman code and therefore the consistency check will pass with small probability because the folder, the function and F prime are far they differ on many points. So the consistency check cannot pass with high probability. So to achieve a small round error, it means that in each round the verifier has to make t queries, where T is lambda, the security parameter divided by this expression. That depends on the rate.
00:18:31.325 - 00:19:03.105, Speaker A: Okay? And this depends of course on the distance. Right? So now in the real Fry protocol, the consistency checks are in fact correlated across rounds to save a low order factor. But doesn't really matter. The point is overall what you see here is that you have lambda times log d, because you have to repeat this reduction log d times. Okay? So that is where the query complexity of Fry comes from. Lambda per round times log d. Okay, good.
00:19:03.105 - 00:19:50.591, Speaker A: That's right. So now I'm going to tell you about what we do in str. So the starting point is the same, the verifier, the prover is still going to think about the fold around the point received from the verifier but the consistency check is going to change. What the prover is going to do is instead of sending F prime, which is claimed to simply be the folding of the function, the prover will loadily extend the folding of the function and write it down over a bigger domain. Okay, so the folding of the function is defined over a domain of size n over K. Think of k as 8 or 16 or 32, say much smaller than n over 2. But then the prover will write down what is supposed to be the loading extension of the folding of the function over a domain of size n over 2.
00:19:50.591 - 00:21:00.201, Speaker A: So we're making progress, but we also have a lot more redundancy. Okay? Okay, fine. However, now the consistency check becomes unclear because these two functions that we want to compare, allegedly they are claimed to be the same polynomial, have different domains. Okay, so what we need is a new consistency check protocol that is going to be based on an idea that we call domain shift. It's going to be a T query consistency check protocol that will compare whether indeed it is the case that G is the logic extension of the folding of the function F. Okay, now this is going to define this consistency check protocol will define a new function F prime, okay, with a certain property. If the folding of the function F is far from this kind of small Ritz Salomon code, except for small probability, this new F function F prime that we're going to define in a moment is going to be far, in fact farther than before to this other medium size ritzaloman code.
00:21:00.201 - 00:21:31.731, Speaker A: So notice now we have a new rate, okay, because we have reduced the degree, but we have not reduced correspondingly the domain. The rate has become smaller. The new rate rho prime is the previous rate times 2 divided by k. When k equals 2, as in fry, the rate is the same. But in this case you should think of K as being, I don't know, 8, 16, 32. So the rate has become smaller and the new function F prime is actually even farther than the original function. And this is making our job easier.
00:21:31.731 - 00:22:29.835, Speaker A: Okay, so I haven't said how to do this, but I want first to tell you, if we can do this, why does the query complexity reduce? Okay, then you recurse. So if you concatenate this reduction round after round, you will get a sequence of rates that are geometrically decreasing, starting with rho, then 2 over k times rho, then 2 over k squared times rho, and so on and so forth. This means that to achieve in round I, round around soundness error to the minus lambda, the Number of queries you need to make in that round is ti, where now you have the same numerator, but the denominator gets bigger and bigger because row I gets smaller and smaller. So, for example, let's look at an example where k is 16 and lambda is 100. In the first round, to achieve soundness error of 2 to the minus 100 with rate a half, you need 200 queries. In the next round, the rate has improved to 1 16. You only need 50 queries.
00:22:29.835 - 00:23:00.457, Speaker A: In the next round the rate has improved further to 128 to 1128. You'll need 29 queries, and so on and so forth. When you add up all these queries, that's the saving that you get in stir over fry. Okay, so you no longer get this lambda times log d, but when you work things out you get lambda times log log d plus log D. Okay, good. The only thing I haven't said is how do you do this? Domain shift consistency check. Okay, so now we have this kind of two functions.
00:23:00.457 - 00:23:26.235, Speaker A: One is written over a domain of size n over K, or you can imagine it as being written over a domain of size n over K and the other one is over domain of size n over two. How do we do that? Okay, so I have two minutes and I can say that. So, okay, so this is the problem. We have the folding of the original function over domain of size n over K. We have domain. We have the new function written down by the prover over domain of size n over 2. That's the function G.
00:23:26.235 - 00:24:06.595, Speaker A: And they're claimed to be the valuations of the same polynomial. Okay, and our goal is to define a new function F prime that is very far from this medium sized roots Solomon code, provided the folded function is far from the small roots Solomon code. How do we do it? So let's make a temporary assumption. And let's say that there is only a unique polynomial. There's only a single polynomial P that is close to G. Okay? Now in this case we know that the folding of the function is far from this one polynomial, because by hypothesis, the folding of the function is far from the Ritz Solomon code. So in particular, it will be far from this particular polynomial.
00:24:06.595 - 00:25:25.645, Speaker A: Now it turns out that because the folding of the function is far from P, if we pick a random point of that function with high probability, that point will disagree with P. Okay? So I claim that defining the new function F prime to be this quotient is a good choice. Why? Because one can prove that this quotiented function derived from G is very far from this medium sized code if and only if P evaluated at Z is not equal to Y. Okay, so this is some It's a fact that has been used in prior papers and we use it here because even though we don't have access to P, we have access to the folding of the function which differs from P on many places. So if we define f prime that way we obtain that with high probability it will be the case that the Y that we read in that location is not the correct value of P, in which case F prime will be very far from this medium sized code. Of course, one value of z may not be enough. We'll take a few more and we'll get our small error.
00:25:25.645 - 00:26:04.845, Speaker A: There are some because I'm quotienting actually off Beth by one in a degree. These things can be fixed with degree correction. Okay, so I'm at the end of my time. I will not talk about how you relax the assumption. I'll simply conclude I've outlined a new IOP of proximity for Dries Salomon code with an improved query complexity. I did not talk about other technicalities that are important such as degree correction. That's actually important and you want to do it in an efficient and high soundness way.
00:26:04.845 - 00:27:05.431, Speaker A: In this paper we also talk about a high soundness compiler and explain how you take any load of test such as stir and combine it with a polyomal IOP and get an IOP essentially preserving soundness of the ingredients. It turns out this is actually not so straightforward and kind of the old ways of doing these things were okay when you didn't have such high soundness kind of requirements. But you know, as the soundness of the underlying ludicrous test improves, you need to improve the compiler as well. And as a final plug, I'll mention that all of this falls under the umbrella of snarks in the random local model. And if you're interested in this, together with my wonderful co author Elon Yugev, we wrote a book that provides a accessible, comprehensive and very rigorous treatment of snarks in the random Oracle model. And the book is available for free both the PDF and the latex code at this website and you can learn about how you build snarks in the random Oracle model. Stop here.
00:27:05.431 - 00:27:46.619, Speaker A: Thank you. Okay, thank you Alain. Thank you. Question? Yeah, thanks for the talk. I wonder if you could give some intuition or an example of what can go wrong if an IOP is not round by round sound where fiat shamir will fail yes. There is a very simple example which is consider a public coin protocol where in each round the verifier sends a single bit. Okay, so round one, the verifier sends a single bit random bit.
00:27:46.619 - 00:28:16.145, Speaker A: Round two, also random bit. And now you have k rounds. At the end of the interaction, K bits have been sent. The verifier accepts if and only if all the bits are zero, ignoring everything the prover has said. The soundness error, namely the probability the verifier will accept in an interaction is 2 to the minus K. Okay, now imagine applying the fiat Shamir kind of transformation to this protocol. What you're going to do is you're first you're going to attack the first round.
00:28:16.145 - 00:28:34.185, Speaker A: First you're going to attack the first round. You kind of send a message. You see what bit you get. You don't like it, you try with a different message. Now you got the bit zero and now you continue, you kind of slowly attack each round and you make progress until the very end. So this is not secure. Why? Because this protocol has bad Rembrandt sound.
00:28:34.185 - 00:29:06.593, Speaker A: Hello. Can this be used along circle stores to do maybe a circle stir and try to mix both of them or they are not compatible. Probably one can, but you know, one has to sit down. I'm guessing one can, but there are technicalities and some probably some new ideas to be had. So it would be like a reasonable thing to try to do. Let me put it differently. I would put a first year PhD student to do that with a clean conscience.
00:29:06.593 - 00:29:15.865, Speaker A: It may not work out, but it was as reasonable bet as any. Okay, thanks Ade. Thank you.
