00:00:00.330 - 00:00:54.570, Speaker A: You. Nice to be here and talk in front of this completely packed room about deep mathematics. I'm so glad to see so many people interested here. So, the progression of the talks that we had is both that as we go on, there are fewer and fewer diagrams and more and more formulas, and also all notations change from one slide to the next. So it will be a test on how well you understand the concepts, whether you can see them with different notations as well. So thankfully, a lot of these slides we can go about quickly, because Ali Tuastek covered the topics quite well. So the question is, how convincing is a fry proof? What is the soundness? We know that if we have a polynomial of degree less than rho n evaluations of this polynomial, then the proverb can definitely convince the verifier that this is what they have when they do a fry proof.
00:00:54.570 - 00:02:00.126, Speaker A: But if the function is not such a polynomial, if it's arbitrary or random, then how likely are they to convince the verifier of this false statement? The naive attack will be just to replace the function that we have by some polynomial of the correct degree, which agrees with this function on that number of points. So on any number of points, we can interpolate a polynomial of degree less than that number of points, so it will be of the right degree. And then we just pretend that we have this polynomial instead of our original function whenever we are queried to the function. And we continue in the same way. Eventually we reach this constant after all of the steps, and the verifier will catch us if and only if they query outside of the set that we chose. So there will be probability raw for every query that they make to fall inside the chosen set. And if we make q queries, then the probability that all of them will fall inside the chosen set is rho to the power of q, and that's the probability that verify will accept this false proof.
00:02:00.126 - 00:02:51.406, Speaker A: So that's a very naive attack, and it gives us a lower bound on the soundness error for this protocol. And for example, with some numbers, if rho is one quarter and the number of queries is 64, then there will be a probability of two to the -128 that the verifier won't catch us, that you will accept this false proof, which is good. That doesn't happen. Okay, so that's basically how we analyze the soundness, and this gives us just the lower bound. And we are also interested in the upper bound, which is actually the security that we have, right? So let's analyze it again. But let's look at the final round now. So, in the final round, we have this function after all of these steps, we got some arf function after our rounds, and we want to commit to it being some low degree polynomial.
00:02:51.406 - 00:03:48.582, Speaker A: In swastik stock, this was just a constant, which is a zero degree polynomial. But actually, in Fry, it can be also, we can stop before the very bottom and have some low degree polynomial on some set, and just send this polynomial, and it will be more efficient. So, one more thing that I will do in this talk is also give some more nuances to fry that relate to how we do them in practice. So, one of them is that we don't really go down to constant polynomials. We reach some low degree polynomials, roughly the same number as the size of the queries is where it's most efficient. So, once we have this function, and we want to falsely convince the verifier that this is some low degree polynomial, the best polynomial that we can tell the verifier that this must be is the polynomial that agrees with this function on the maximum number of points. So the agreement between a function and the polynomial is just the number of points in the set that we evaluate on, that we agree on.
00:03:48.582 - 00:04:41.266, Speaker A: And we take this relative to the size of the set, so that the numbers would be between zero and one instead of large. And if the agreement between the function and the polynomial that the prover told the verifier is alpha, then the chance that every query will fall inside this final set is alpha, and the chance of accepting the whole thing is, again, alpha to the q. And since we can always interpolate polynomials on some raw endpoints, we can always choose some polynomial that has agreement, at least row. So the best alpha will be at least rho always. So that also gives us another variant of this attack, by choosing a polynomial at the final stage instead of the first stage, but it gives us, again, the same rho to the q. And again, about notations. Swastik mentioned distance.
00:04:41.266 - 00:05:24.530, Speaker A: The distance is just one minus agreement. So everything here will be in terms of agreement instead of distance, which turns out to give slightly nicer formulas, we can formulate the same thing in the terms of the language of code. So here, now you see formulas, as Vastik mentioned, we have read Solomon codes. They're just evaluations of polynomials on a given set, polynomials of degree, which is smaller than the size of a set. And what we are actually asking for is agreement with this code. So, agreement of a function with the code is just the best agreement of any possible code word of any polynomial of low degree with this function. So the best attack that once we reach the function fr.
00:05:24.530 - 00:05:51.974, Speaker A: The best thing that the proverb can do is just to choose the polynomial with the best agreement. Which is the agreement with the code. And that gives us the upper bound on the soundness of the polynomial of the algorithm. If the prover is able to convince the verifier to accept a proof with probability. Which is greater than alpha to the q. Then it's only possible if the agreement at that final stage is greater than alpha. Okay, so this gives us some lower bound.
00:05:51.974 - 00:06:25.806, Speaker A: If the verifier accepted. And he assumes that no miracles of size two to the 128 exist. Then he can be certain that it must be greater than alpha. Now let's consider fry step. And now we want to understand how this agreement changes during fry steps or doesn't change. So this is just a single application of a fry step going from f to f prime in run round. From some f I to some f I plus one.
00:06:25.806 - 00:06:58.346, Speaker A: And z was previously. B is the verifier input. F zero and f one were previously g and h. Now suppose that these two functions currently have agreement greater than alpha with. After we split this function up, we have agreement greater than alpha with this code. Which came from f having some agreement greater than alpha. More precisely, if f had agreement greater than alpha at points which of the form x and minus x.
00:06:58.346 - 00:07:25.250, Speaker A: Then both f zero and f one will have such agreement with x squared. And it's important that f zero and f one will have agreement with code words on the same set. So usually, when we agree with a certain code. There is the set of points where they agree on. And if we have different functions. Then they can agree with different codes on different sets. And here we want f zero and f one to agree with coderode c zero and c one.
00:07:25.250 - 00:07:55.862, Speaker A: On the same set of size greater than alpha times the size of d prime. So that every combination of f zero and f one will also agree with a combination of c zero and c one on the same set. If they agree on different sets. Then their combinations will agree only on the intersection of these sets. So we need these sets to be also equal. If this happens, then definitely also f prime will have agreement greater than alpha on c prime on that same set. So that's what happens when we go down eleven.
00:07:55.862 - 00:08:30.386, Speaker A: And in particular, if the agreement is just one. If these are just polynomials. Then f prime will also just be a polynomial. And that gives us the completeness of prime. But what we are really interested on is going up eleven instead of down eleven, and starting from f prime and trying to say something about f zero, f one, and f. So again, we have the same step, but now we assume that f prime has agreements greater than alpha with c prime. And what does this actually mean? F prime is not really well defined.
00:08:30.386 - 00:09:15.954, Speaker A: It depends on this verifier input z. So actually the assumption is that there is a non significant chance that this will happen. So for some amount of z's, this must happen for more than one z, for some significant amount, but much less than the entire possible choice. So some positive probability, but not very small. We'll go a bit more into it soon. If this is true, then f zero and f one will also have this correlated agreement that they agree with code words on the same sets, with agreement greater than alpha with code words in c prime. And that now implies that f also has this agreement greater than alpha with the previous code, with the Reed Solomon code of the same rate on the previous domain.
00:09:15.954 - 00:10:25.418, Speaker A: And that happens because if f zero and f one agree on some x square, then just by plugging that into that formula, that means that f will agree with the polynomial that is c zero, x squared plus x c one, x squared on points x and minus x, both of them, because they are both given from the same formula with the same values of c zero and c one, which both agree with f zero and f one in that particular x squared. Okay, and why is this good? So this is just one step, but we can just repeat this argument for every step. So if we know that the final sound necessarily was greater than alpha to the power of the number of queries, then we saw earlier that that means that the final function had to have agreement greater than alpha with the final code. And now we repeat this argument that means we can go up a level. The penultimate function had agreement greater than alpha with the penultimate code, and up and up and up until we reach the original function. So now we know the original function was not some arbitrary or random function. It was actually a function that had a significant agreement with the original code.
00:10:25.418 - 00:11:15.100, Speaker A: So it was close to being a polynomial of a correct degree. So there's some inaccuracies going on here. So I didn't say what alpha needs to be for this statement to be true, and I didn't really specify what this significant amount of z's is. So it's not true for every value of alpha, it's not true for every amount of z's. There are parameters that for some sets of parameters, this holds. So we frame this in terms of agreement gaps, or sometimes proximity gaps, just same thing with one minus. We say that this rid solomon code has this alpha epsilon agreement gap if for u zero and u one.
00:11:15.100 - 00:11:53.720, Speaker A: For any two words, u zero and u one just functions from d to f q. When we take random linear combinations of these functions, u zero plus z u one, and we measure the agreement with the code, if it has agreement greater than alpha, with probability greater than epsilon. So that means that for at least epsilon times q values of z, the agreement will be greater than alpha, then actually it will be true for every z. It can't be somewhere in the middle. It can be that this probability is one half. It is either very, very small, smaller than epsilon, or it's just one. It's true for every z.
00:11:53.720 - 00:12:48.426, Speaker A: And the theorem says that they do have this agreement gap for any alpha which is even slightly greater than the square root of raw and epsilon, which depends on this value of alpha and on n and q. So we need the field size q to be at least quadratic in the domain size in n, and then we need this gap between alpha and square root rho to somehow fall in that gap. If this eta is zero, then that thing explodes and goes to infinity. But as long as Eta is a bit farther than zero, then this is just a constant. And the constant in the big o is actually like one over 128. So there isn't some huge constant finding there, it's just that thing. So that's the proximity gaps theorem from the paper Swastik mentioned, Vciks 2020.
00:12:48.426 - 00:13:24.710, Speaker A: And as Swastik showed, there was quite a lot of work going, building up to this thing. And this is the current state of the art. Yes. So the proofs that appear in this paper rely on decoding algorithms and list decoding algorithms. Specifically the guruswami Sudan list decoding algorithm for read Solomon codes. And that's where the range comes from, the square root rhossetta. Basically, it tries to just decode this word.
00:13:24.710 - 00:13:56.110, Speaker A: And you see that if we can decode it for many values of z, then it means that just algebraically it's decodable when z is just a formal variable. I don't know if it really gives intuition to this thing. It gives intuition to why we can do this range and we can't go below it, maybe. Yes. I think if you want to give intuition, like an exponential threshold effect, and it happens often in code. Yes, very quick. Yeah.
00:13:56.110 - 00:14:32.006, Speaker A: Well, I think just basically the intuition is what we said before. If this combination, it can't just happen at random. That a random linear combination will be close to code words. It has to come from the fact that these things are actually close to code words. It will be very hard to construct some words that are not themselves close to code words, but the combinations is often close to code words. You can construct it for a very few points, maybe by hand, but you will soon get contradictions if you try to do it for many, many values. So that's why you can't get more than about n squared, such z's.
00:14:32.006 - 00:15:23.352, Speaker A: That will give you this thing together, right? And actually this gives us also the correlated agreement. Not just agreement that each of them separately has, but that they agree on the same set of points, which is what we actually need for going up a level. And that's why from the correlated agreement, we get the probability one. So let's do a numerical example of this. If a field size is about 250 bits, our block size is about 34 bits, which is actually slightly bigger than we have in our stock proofs, the rate is one over 16, which is exactly right. And the number of rounds doesn't really matter, but let's say 24. So we end up with a block size of two to the ten at the final round, but really doesn't matter.
00:15:23.352 - 00:16:30.136, Speaker A: And say we want 128 bits of security, then we can take in that theorem ETA to be two to the minus nine. Then Alpha is just slightly bigger than one quarter and we get that epsilon. This thing will be less than two to the -133 so less than this two to the -128 and this ETA with two to the nine is chosen such that ETA to the power of seven, two to the 63 will just exactly fill in that gap between n square over q and the security we are aiming for. So that's how the numbers here work out. And we take the number of queries to be just 65, one more than 64. 64 is just not enough because our alpha is slightly bigger than one quarter. And we get that alpha to the power 65 is less than two to the -129 so putting these things together, we get that if the verifier will accept with probability, which is greater than -128 bits, then that means that the proverbs original function must have had agreement greater than this alpha greater than this thing, which is more than one quarter.
00:16:30.136 - 00:17:01.214, Speaker A: Yes, EtA is just a small gap between square root row, which is the ideal, and what we can actually do. So we can't reach quite square root r just slightly bigger than the bet. It's the 0.2 here. Okay, so that's basically how the soundness analysis works. And in the short remaining time I'll try to get into some more nuances. So I didn't really talk about cheating.
00:17:01.214 - 00:17:51.294, Speaker A: I just talked about if the prover does every algebraic sketch step correctly, and then maybe cheats on the last step, then what is best thing to do. But it can cheat on every step. The prover can change some values of fi at every step after computing them instead of what they should be. From a combination of f zero plus z f, one can commit to something which is different. But this doesn't actually help the prover, because if a verifier will query that node that the proverb checked, then he will definitely catch that cheating and will reject the proof. So we shouldn't really count such nodes as part of the agreement. And if the entire verification path passes through that node, then the verification will fail.
00:17:51.294 - 00:18:53.534, Speaker A: And if we look at the node one step later, then when we verify this, we will also verify either one of its two nodes that lead to it, one of the x or minus x that leads to this x squared. If one of them had cheating in it, then with probability one half the sun will have one cheating, and this will go on. So the way that we can take this into account is instead of considering agreement, we consider a weighted agreement where the weight that each point gets for contributing to the agreement is proportional to the probability that there is no cheating there, that when we check it, we won't reject anyway because prover cheated along the way. So these weights get modified along the way. Whenever the proverb cheats, the weight becomes zero. And when we do a price step, the weights average out. And because we have now these weights, then whenever we cheat, we just don't increase agreement.
00:18:53.534 - 00:19:37.150, Speaker A: We modify the weights, and now with the new weights, we can't have any greater agreement, because even if we added more agreement points, we will have weight zero. The previous function before cheating had at least as much weighted agreement before. And the theorem that we can take the agreement and go up a level and get at least the same agreement still works even for this weighted agreement. So we start with the final weighted agreement and we pub it up. And when we reach the top, the weights are all one, because no cheating happened at the very beginning. So that's one nuance that we had to add, thanks to reviewers that caught on that some things were not. Maybe we're a bit hand wavy.
00:19:37.150 - 00:19:59.318, Speaker A: So you can see that there are a lot of nuances. The mathematics is one more nuance that we have is that we can do slightly larger fry steps. We only talked about two to one fry steps so far. But we can do them greater than that. So, for example, here is a four to one fry step. We split the function into four functions. And then we take a combination of those four functions.
00:19:59.318 - 00:20:20.866, Speaker A: And we send each x to x to the power of four. Which is like x squared squared. So we skip a step if we do it this way. And this can be much more efficient. Because on one hand, we have to reveal more values of f. In order to compute the next value of f prime. So each value of f prime depends on four values of f.
00:20:20.866 - 00:20:39.974, Speaker A: Instead of two values of f. But on the other end, we have to do much fewer steps. And the decommitments, which are very expensive. We will have to do them fewer times because of this. So there's a certain trade off. And it turns out that the best thing to do. Will be eight to one and 16 to one most of the time.
00:20:39.974 - 00:20:56.810, Speaker A: And sometimes this four to one. But two to one is almost never efficient. And the results about the agreement gaps is still correct even in this kind of method. And another thing that we can do. Is we can take some random linear combination. Of a lot of functions. And do fry on them.
00:20:56.810 - 00:21:14.606, Speaker A: And it's still true that if fry convinces us. That this has large agreement. Then all of these functions in separate. They also have large agreement. And this is correlated. And I think I will skip talking about this thing. Let me just summarize.
00:21:14.606 - 00:21:36.006, Speaker A: What is actually the soundness of fry. We know that it's impossible to prove a false computation. In less than square root rho to the power of q work. Because of these proximity gaps results. And we know that it is possible, with a nave attack. To do it in just rho to the power of q minus q work. So the answer is somewhere in between.
00:21:36.006 - 00:21:53.342, Speaker A: We don't know any way to do it better than this rho to the minus Q. And not for lack of trying. We did try to find out to find better attacks. But this is the best we can do. Many different permutations of getting to that same result. But not anything better than that. And if anyone does know one, then tell us.
00:21:53.342 - 00:22:22.162, Speaker A: And there will be a reward offered pretty soon. And we actually believe in conjecture. That this is actually the true soundness, true security of this protocol. And that is actually what we also use in practice. Which is kind of similar to how we know certain attacks against factoring. Against elliptic curves. And this is what is used for tuning those facts.
00:22:22.162 - 00:22:33.590, Speaker A: No one knows what is actual security for such things. But we believe that we can do better than what we can do. So that's it. Thanks for listening.
