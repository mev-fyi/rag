00:00:00.170 - 00:00:19.200, Speaker A: Is the thing we're testing the system itself or our own assumptions and knowledge about the system. And to some degree, I think we always have know for any meaningful system of testing, we have to encode somehow and somewhere knowledge about how the system is supposed to behave. Right.
00:00:20.050 - 00:00:30.278, Speaker B: GMGM, everyone. My name is Ugash, your host from scraping bits, and today I have a special guest, Horsefax, an elite dev in the EVM space. And it's such a pleasure to have you on.
00:00:30.364 - 00:00:32.870, Speaker A: Thanks. Yeah, I'm excited to talk with you too.
00:00:32.940 - 00:00:36.678, Speaker B: So let's start off with basically who you are and what you do.
00:00:36.764 - 00:01:01.518, Speaker A: Sure. Yeah. My pseudonymous dev identity has become horsefacts over the last couple of years, but I'd say I'm primarily an EVM smart contract programmer and developer working in and around all things EVM. Recently that's been smart contract programming solidity. Prior to that was working on an indexer project, actually, which is sort of how I got into the space in the first place.
00:01:01.604 - 00:01:07.694, Speaker B: You were like web two before crypto, or you got into basically web three first and did the index?
00:01:07.822 - 00:01:41.930, Speaker A: Yeah, absolutely. So I spent ten years in web two working as a consultant, actually, so sort of dev shop consultancy, think thoughtworks, kind of, kind of company building products, usually for companies and businesses. Oftentimes we do stuff like embed with a team in a company with an existing engineering team, or deliver a product, work like that, and then sort of made the slow transition over. Into web three over the last couple of years. Yeah.
00:01:42.000 - 00:01:43.286, Speaker B: And how did you get into web three?
00:01:43.328 - 00:02:55.318, Speaker A: The first project I worked on was an indexer project, very cool project called Vulcanize DB, which sort of started as just a project that I got through my web two job. This was around, I guess it was early 2018, late 2017 kind of time frame, that kind of era of the cycle. Yeah. And the original concept for this project was sort of like what became the graph, if you think of the graph? An indexer project we would track on chain data, monitor a contract for events and state and storage changes, and expose ETL that into postgres and expose that through a GraphqL API. Yeah, I worked on that with Rick Dudley, who is one of the most brilliant people in the space. I think he had this idea for Vulcanize, which was this indexer tool that's since become a larger project. Rick has had a long term vision of this as a system where really anyone can run their own indexer and have sort of verifiable access to verifiable provable state and build dapps on of it.
00:02:55.318 - 00:03:29.310, Speaker A: Right. I think everyone in the space has probably built their own custom indexer at some point. Right. But yeah, we got started on that project and sort of started as another project, a very interesting one with a lot of interesting technical details. That was in this sort of, this Ethereum ecosystem, which I had paid some attention to, was not deeply involved, but I knew what Ethereum was and enough to be interested in the project and get started on it. But that really became kind of a full stack introduction to the EVM and kind of the full EVM stack. Right.
00:03:29.460 - 00:03:37.486, Speaker B: Is this including like nodes as well or are you talking about full stack in terms of websites and wallets?
00:03:37.678 - 00:04:04.300, Speaker A: Yeah, more, I guess, smart contract and below, if we think of the stack that way. Your application level of the smart contract layer. Yeah. Down to running nodes and infrastructure and indexing infrastructure and so forth. Yeah. At the time we really wanted access to state changes, historical state changes within a contract, which was harder to do then I think tracing APIs, it's come a long way. Right.
00:04:04.300 - 00:04:41.206, Speaker A: We maintained like a fork of guess that would emit over a websocket state diffs on every blockchain so you could see which storage key changes happened and introspect that information. Yeah. Which was really cool, I think. Really ahead of its time in some ways. And the idea here was that this project, anyone could run a vulcanized node and run their own indexer. Ultimately, somewhat more centralized services have kind of won out in terms of access to cash application data on Ethereum. But yeah, that was my intro.
00:04:41.206 - 00:06:09.234, Speaker A: The claim to fame for that project really was that maker used that for multilateral Dai as their internal indexer. For a while we worked with Rick on that project, worked in combination with the folks at banker and spent a lot of time reading the, you know, learning about how to decode the events and data in those contracts in order to convert them and index them and store them in a useful way for people to build apps on top of. In working on that, Gethfork got to dig into the internals of nodes that have been hard and execution a little bit, got to be terrified every time there was a hard fork and we needed to rebase our custom patch on top of a guess which anyone who's done meV stuff I think is familiar with. Right. And yeah, spent some time working with the infrastructural challenges of just running an indexer and keeping up with the head of the chain. And what do you do when there are reorgs and changes to data and what do you do when events are dropped and data is missed, all that kind of stuff. I worked on that, I was still in sort of engineering management arc at the time as well, so worked on and off as kind of an active contributor or just kind of part of managing that team or just supporting that team for a while for a few years, but sort of came up for air and realized I had learned a lot about the Ethereum ecosystem and, yeah, wanted to work on that stuff more full time.
00:06:09.234 - 00:06:17.750, Speaker A: And so, yeah, since then, the last two or three years have been much more full time, focused on EVM and Ethereum.
00:06:18.250 - 00:06:42.266, Speaker B: And you got into the tooling space as well through this. But I did want to ask, how did you really get to understand basically nodes at a deep level if you were to do all of that again, try and learn it again, knowing what you know now, how would you re approach that to basically get to an accelerated point? To this point, you are now at a faster speed.
00:06:42.298 - 00:07:18.154, Speaker A: I guess it's a good question and it's tough to untangle. For me, this interplay between learning the fundamentals of the EVM through sort of practice and doing and building things and experiencing and learning those principles kind of from the ground up. Right. I think a lot of things in the design of that system we could have avoided. A lot of things we learned or mistakes we made, depending on how you look at them, just by. If we had a sort of perfect understanding of go read the yellow paper from scratch and understand how that all, how that all works for me, I can't really solidify that knowledge. Yeah.
00:07:18.154 - 00:07:23.580, Speaker A: That theory has to interplay with trying things out in practice. Right. And working in practice and building.
00:07:26.910 - 00:07:43.086, Speaker B: Did you ever build some simple kind of, I guess, tool to interact with it to just like trial and error what parts of it do, or even build just some simple kind of program to emulate what it does, but maybe not copying the code. Exactly.
00:07:43.268 - 00:08:06.214, Speaker A: Yeah, absolutely. I think when I go back and think about this project, one, I don't know if this is relevant to your question. One thing I think about a lot is the emergence of solidity tooling that's written in solidity. Right. Like DS test and forge and foundry have been huge in getting people to write unit tests in solidity. And I mean, that's been around for a long time. Right.
00:08:06.214 - 00:08:52.198, Speaker A: Depth tools was around, DS test was around. I think forge and foundry made this huge improvement in terms of accessibility because devtools had to install nics and stuff was written in haskell and stuff. And so it felt unapproachable for folks, but that's been a big step. And when I think back to this project as well, we were writing a lot of our indexer code in go, which is fine, get those in go and you can do stuff on top of that. But still, we were trying to do these transformations and grab data that was very legible from solidity from inside the EVM and perform these transformations that were somewhat difficult to do in go from sort of outside the EVM. And I think that trend is something that's been, that's been exciting for me. Right.
00:08:52.198 - 00:09:21.662, Speaker A: If I went back into this indexer again, all those transformations would be in solidity, right. Rather than go or typescript. There was even a step between there where we rewrote some of that go to a sort of typescript application layer. But this trend of like, let's just write things that interact with the EVM in solidity directly, where you have access to the contract state and whatever manipulations you want to do and you can listen to things I think is a good one. So that comes to mind. But I don't know to what degree we could have anticipated that, right. Without building the thing.
00:09:21.662 - 00:09:42.050, Speaker A: So I don't know. This is a challenge in all technical work and engineering, I think. And part of the challenge is to design systems that are, and a special challenge in anything ethereum related that is often immutable and unchangeable. Right. Is designing systems that are flexible and open to change and extension.
00:09:42.130 - 00:10:00.800, Speaker B: Yeah, I think to learn without over engineering anything like at the node level, you basically just got to read different files and go through one section of it at a time and kind of see how you can play around with it, then move on to the next. Or how they intertwine, because they all intertwine at some stage. I guess you have to learn like modularly, right?
00:10:03.490 - 00:10:04.366, Speaker A: Yeah, absolutely.
00:10:04.468 - 00:10:07.578, Speaker B: The only way you really can at a fast pace.
00:10:07.674 - 00:10:31.554, Speaker A: Yeah. I think you can work sort of abstraction layer by abstraction layer as well. Right. For example, just solidity. And I would say when I was starting in smart contracts, right. That's where I was focused and sort of staying within the boundaries of just the high level language and syntax of solidity. That's where probably most people start if they're approaching smart contract or EVM development.
00:10:31.554 - 00:10:59.934, Speaker A: Right. And then going a level deeper to understanding the execution environment of the EVM and the VM itself. Right. And looking at opcodes and writing huff maybe, and sort of dipping the layer below that level and then even dipping down to the execution layer and reading the code in the code in Geth and how that's mapping that. There's a ton to learn, I think, from just reading the source code of an execution client sort of dipping for me. I think you grow a lot. Right.
00:10:59.934 - 00:11:06.978, Speaker A: When you kind of dip one level below whatever abstraction level you're working at. Right.
00:11:07.064 - 00:11:40.800, Speaker B: Yeah, I think going down to the bare metal really builds the foundational knowledge of. Okay, what's actually happening at these higher levels. I've gone down to half currently, and I feel like solidity. I know exactly, you know, basically how to optimize everything because you know what's going on under the hood. But then I guess even with opcodes, if you want to get an even stronger understanding, you would go down to the node level and see what's happening and how everything works. Yeah, I guess that's basically the way I think. And it's kind of what I've done as well.
00:11:40.800 - 00:11:53.360, Speaker B: But now you're working on smart contracts and tools as well. You mentioned outside of this, you're working on a fuzz testing engine with Seaport. How did you get into.
00:11:55.170 - 00:12:12.950, Speaker A: Yeah, yeah, it was really an amazing opportunity, actually. So Zeroedge and Dan at OpenSea Seaport team reached out to me, I think in large part, actually, because I participated in a couple of their code arena audits. They did the big million dollar audit and they did.
00:12:13.100 - 00:12:14.086, Speaker B: Yeah, I remember that.
00:12:14.108 - 00:12:53.394, Speaker A: An audit for cpart one one or one two. Yeah, I had participated in those, and I don't think we found anything in either of them. Right. But did a pretty good QA report in those contests, and I think was known to the team for doing that. And I'd just written that article about invariant testing at the time as well. So I've been really into exploring invariant testing and foundry fuzzing and some of these sort of more advanced testing techniques lately. So, yeah, I think for folks who are out there participating in c four, this is a great example that not just findings that can come out of that sort of work.
00:12:53.394 - 00:13:03.810, Speaker A: Right. But kind of, you can establish that reputation. And it was kind of a dream opportunity to go work with zero h and cport and openc on this. But, yeah, they were working on a fuzzing system for seaport.
00:13:03.890 - 00:13:15.622, Speaker B: How does that work? Is it just for inline assembly, or are they going even lower to Hof? But why are they building like, an in house one, particularly for cport?
00:13:15.686 - 00:13:32.890, Speaker A: Yeah, sure. So, yeah, I don't know if you've. Have you read the cport code base much or certainly you've used opensea. Right. Um, yeah, sorry, cut this out.
00:13:33.740 - 00:13:36.616, Speaker B: Like you can take your time. It gets cut out. Yeah, yeah.
00:13:36.798 - 00:13:46.076, Speaker A: So I think seaport has, Seaport is on version one six now. One one five. There have been several iterations of the.
00:13:46.098 - 00:13:48.552, Speaker B: Protocol, different optimizations and whatnot.
00:13:48.616 - 00:14:03.760, Speaker A: Yeah. And like, testing in Seaport has evolved over time too. Right. They have some tests that are in forge and foundry. They have some tests that are in JavaScript. Right. There has been enough evolution of that protocol that there's a lot of supporting code for testing and so forth.
00:14:03.760 - 00:14:14.904, Speaker A: I'm not sure if you go look by lines of code, but I'm sure the vast majority of that code base is support and test code. Yeah. We were interested in, there had been a couple of recent releases, right.
00:14:14.942 - 00:14:23.412, Speaker B: Where, why did they basically develop an in house fuzzing tool specifically for them when there's all these other tools like echidna, et cetera?
00:14:23.476 - 00:15:17.230, Speaker A: This really is sort of a framework that sits on top of the foundry fuzzer. And I think the answer to that is, I'd describe the system sort of as generative testing, maybe more than fuzzing, where I sort of think of fuzzing as sort of unbounded, unconstrained fuzzing of call data or input space, right. And this system starts from a valid order, right. We have a system that constructs a randomized but valid cport order and then runs it through the engine and makes some assertions about what the outcome should be there. There's also a system that will mutate the order and do a step that we expect to revert. But, yeah, we sort of start with the assumption that the order is valid and introspect. What are all the token transfers that should happen here? What are the events we expect? Things like that.
00:15:18.960 - 00:15:26.332, Speaker B: Super tailored version, you already know what sequences can occur, so you basically just fuzz the sequences in different orders. Right?
00:15:26.386 - 00:15:44.976, Speaker A: Yeah, and we say fuzzing a lot. Right. But that's a very broad category. Right. That I think encompasses on one side of that scale generative testing, which is maybe a little bit more constrained to assumptions about your environment. Right. And maybe on the far end of that scale, sort of unconstrained call data fuzzing.
00:15:44.976 - 00:16:40.804, Speaker A: Right. Something where you just pass random data into the system and try to break invariants or try to break certain rules, things like that. Yeah. And if you think about cport, right, the cport contract is pretty big and has a bunch of subcomponents, right? Maybe you've seen like the sort of famous diagram showing all the seaport inheritance structure, right? And has all these internal components, but ultimately it's one contract and it has a pretty reasonably slim external interface. Right. I think eight or ten functions on Cport, most of them are different types of different types of order fulfillments that take slightly different types and constructions of orders, right. And then each of those takes usually a pretty massive struct with a nested struct representing the order, and then nested consideration items and offer items and a whole series of nested entities that represent the structure of that order.
00:16:40.804 - 00:17:42.888, Speaker A: And certainly for me, I like to fuzz when I'm developing any project, kind of from the ground up if possible. I've been working on some projects recently that just start with that assumption, right? Let's not even really write specific unit tests. Let's start with making every test a fuzz test and go from there. But I think if you've done that, right, as your system grows to some scale of complexity, you start to need to make a lot of assumptions in those fuzzing, right? If you're fuzzing on anything more than sort of simple input arguments, a couple of integers and an address or something, maybe like dynamic arrays are like a great example of this, right? As soon as you're fuzzing on dynamic arrays, you have to do a lot of setup and a lot of bounding and assumptions and things within your test to make that work. And you write a lot of supporting code that's just encoding all the assumptions that you're making about the input and output to your tests. Right. And so sort of starting from this approach of let's generate a valid input and go from there and then maybe mutate it in interesting ways and tweak certain assumptions and verify those, I think, yeah, that can work.
00:17:42.974 - 00:17:50.590, Speaker B: How did you go about basically mutating them in different ways? Was it completely random or you had kind of a sequence for that as well?
00:17:51.520 - 00:18:29.464, Speaker A: Yeah, there's a sequence for that. It's pretty custom. Right. I think it's the nice thing about building this all on top of the forge and foundry fuzzer, is that you can really customize it to make it bespoke to the project or use case. So yeah, there's a whole lifecycle in this fuzz engine, starting with generation, where we have a bunch of enums that represent the potential state space of a cport order. And we sort of randomly walk those enums, pick a particular order configuration, we read that state space enum and then convert that into a real test order.
00:18:29.582 - 00:18:39.868, Speaker B: So an example of an enum would be like, let's say a buy order. Then what would an order look like? Basically. Give me like an example, I guess.
00:18:39.954 - 00:18:52.784, Speaker A: Yeah, sure. So an order sort of at core usually consists of a list of offer items and a list of consideration items. Also sort of additional contextual information about.
00:18:52.822 - 00:18:57.424, Speaker B: Who'S doing the transactions, doing the transaction, fulfilling the order.
00:18:57.462 - 00:19:00.496, Speaker A: Yeah, sort of additional components there.
00:19:00.598 - 00:19:29.070, Speaker B: Yeah. Okay, so it's just like randomly sequenced. There's an enem where there's a buy, there's a sell, then whatever else. And then basically the sequence just is randomly putting in these, in different sequences, right, for x amount time, x length, and then it goes, reiterates over it in a different sequence, and if it passes, move on to a next one, or if it fails, move into another branch type thing. Or is it completely different?
00:19:30.080 - 00:20:24.700, Speaker A: Yeah, so we sort of want to work from a randomized state space which describes this is like a big struct full of enums that describe the configuration of an order. And for Cport, this is stuff like, is it a basic order or an advanced order? Is the order available now? Is it partially fulfilled? Has it been validated? Not validated, just its structure and type. Right? Like is the offer item a native token, an ERC 721 token? In ERC 20 token, does it have criteria? Does it not have criteria? Does it have a fixed amount and ascending amount, descending amount? All of these different components that can be sort of the different parameters that make up an openC or c port order. And then we walk that state and go from there, converting those to what we want to be a valid but randomized c port order.
00:20:24.770 - 00:20:29.664, Speaker B: And you can do basically the randomized. And basically, how do you form these sequences of random stuff?
00:20:29.782 - 00:21:30.224, Speaker A: Sure. So, yeah, we just use a fuzz seed, right, as the input. And actually we used Solady's lib prng pretty extensively to take that fuz seed and then generate other pseudorandom components, right? So we'll take that struct and say, okay, just pick out of the list of possible configurations for whatever this state space representation is. We pick one of them, right? Maybe a good example is like the caller, right? We just have five or six test addresses and sort of loop through those and pick one of those, one of those at random, or the token types and the length of the number of items on the order and so forth. So, yeah, then we take that kind of randomized description and then walk over all that state and convert it into a test order. So the same thing you might do in a unit test or a fuz test and doing your test set up, but sort of automated through an engine that sets that all up. So in that sense, it's kind of guided.
00:21:30.224 - 00:21:57.512, Speaker A: Right. Like we're not fuzzing a totally random caller for every component. Right. Or we're not fuzzing like a totally random token contract for everything. Right. There are places where kind of, if you think of sort of like the leaf nodes of this state space, oftentimes we'll fuzz values or fuz something there. But some of the configuration at a higher level, you can think of as kind of like a graph or a tree of the whole state configuration of a possible order.
00:21:57.566 - 00:22:16.048, Speaker B: I think it would be incredibly inefficient if it was just random fuzzing. So I think anything worth having is guided in some fashion or some way, especially like for time complexity as well. You don't just sit there for 5 minutes waiting for it to just do a whole bunch of iterations. Yeah, it's super interesting.
00:22:16.134 - 00:22:45.864, Speaker A: Yeah, absolutely. And this is like a constant trade off, I think, in kind of fuzzing and invariant testing and other testing mechanisms that introduce this element of randomness, I think. Right. Is this trade off between in fuzzing? Certainly this trade off between exploration of the space and quality of the test. And these trade offs you have to think about in terms of introducing assumptions to your system that make the tests more meaningful, but also introduce assumptions. Right. And simplify the extent to which.
00:22:45.864 - 00:23:01.808, Speaker A: To which you're testing. Right. And so you have to kind of pick that. Right balance between fuzzing the entire enormous possibility space and making assertions that are meaningful. If you have a limited number of runs, if you're working in this sort of probabilistic, sort of, sort of testing wall.
00:23:01.894 - 00:23:13.750, Speaker B: And I'm curious to how you basically fuzz door call dialogue as well. Like you just change like a specific byte, or are you changing a whole word for a parameter? I guess. How do you set all that stuff?
00:23:18.600 - 00:23:41.500, Speaker A: So, yeah, this is actually something that Dylan Keller contributed to extensively. So I can lay no claim to this, but this is a really cool system. We call these scuff tests to come up with a name for. Yes, scuffing. So you sort of like scuff the call data. Unlike the core, the core system sort of takes an order. We know it's a valid order.
00:23:41.500 - 00:24:20.616, Speaker A: And sort of, we're operating at the high think sort of application level of the contract, right. Where we're working with a cport solidity, advanced order, we run that through cport, fulfill that, and verify the expectations about that. Yeah, the scuffing engine takes, it's a separate step within the engine where there's kind of a whole lifecycle defined, where you build the order, convert it to an actual advanced order, run a success case, and verify those expectations. We then perform a mutation. Right. Where we pick at random some high level mutation to that order and change it in some way and expect a specific revert. And then.
00:24:20.616 - 00:24:29.950, Speaker A: Yeah, when the scuff engine is running, we also have a step where we will manipulate the call data in some way and expect a revert or an error there.
00:24:30.800 - 00:24:43.536, Speaker B: So you're basically changing whole words, not like specific pieces of call data, like bytes, really. And how do you determine the ranges of which to basically fuzz as well?
00:24:43.638 - 00:24:58.116, Speaker A: Yeah, we have a whole harness for doing that that's built in. I think we started with some pretty. That's a system that we're actively working on. Yeah, it's a different, it's really difficult. Yeah. And we're working on some additional mutations there. You'll see.
00:24:58.116 - 00:25:26.670, Speaker A: I think it's not even in the core merged to the core Cport repo yet, because we're still sort of actively exploring that. Yeah, that is a tough problem to pick exactly how to mutate call data and do so in a meaningful way. Dylan wrote a bunch of libraries that do that. And building on some of the stuff that's already in cport for constructing call data and encoding call data, I don't know if you've seen some of his ability work API encoding work that's used in cport. Sort of building on top of that to manipulate and change encoding of call data.
00:25:27.520 - 00:26:07.390, Speaker B: Yeah. Fussing call data is extremely hard because the ranges on the types are just insanely big, and it just introduces a lot of time complexity. If you're going to go for each, basically, I guess, increment in, let's say, a un, two, five, six, you have this gigantic number to fuzz. Right. So it's really necessary to do bound analysis, just like what pyrometer does. They obviously aren't a dynamic analysis kind of tool. But, yeah, it's extremely important to basically bring down your time complexity to as well as possible, while being as efficient, as basically as accurate as possible.
00:26:07.390 - 00:26:24.016, Speaker B: And it's really difficult, but I don't know, I guess there's like trade offs to different solutions, but, yeah, I guess. What's one way, the kind of approach you're taking now with your current one.
00:26:24.118 - 00:26:37.300, Speaker A: Yeah. In terms of managing that possibility space. Yeah, I think it's not just in call data buzzing. Well, I guess at a high level, right. Everything is called data fuzzing. Right. But yeah, I mean, if you think about fuzzing an address.
00:26:37.300 - 00:26:50.024, Speaker A: Right. Or a uni 256 or something. Right. Often that's a really enormous space to fuzz in terms of analyzing, I guess. In what specific context? In cport or in just thinking about the practice in general.
00:26:50.142 - 00:26:52.570, Speaker B: I think both. Let's do seaport then.
00:26:52.940 - 00:26:56.472, Speaker A: General, how we're thinking about how to find specific.
00:26:56.606 - 00:27:05.212, Speaker B: Yeah. Like fuzzing cold data. Why would you change something a certain way? Or what are the ranges you're going to go by and how do you determine that?
00:27:05.266 - 00:27:14.048, Speaker A: Yeah, so we have defined, if I recall, how this all works. We have defined what we call scuff directives, which are sort of specific rules that we apply.
00:27:14.214 - 00:27:24.790, Speaker B: Okay, so you have predetermined, basically rules for how big a call data mutation can be, or like a word mutation, I guess.
00:27:26.600 - 00:27:27.350, Speaker A: Yes.
00:27:28.680 - 00:27:47.032, Speaker B: I wonder how it would be done in a generalized sense. So if you're trying to basically fuzz test, just call data of a protocol, for example, and you didn't write these properties, I guess, like specific predefined properties, how would you even basically fuz call data in a normal one? In a normal, I guess, protocol?
00:27:47.096 - 00:28:37.992, Speaker A: Yeah. I don't know. I think this is a really interesting thing about testing to me. Whenever I get deep into testing any system, I get into sort of fundamental questions about knowledge, right? How do we know what we know? How do we know it becomes epistemological at some point, right? How do we know what we know about the system? How do we know we're testing what we think we're testing is the thing. We're testing the system itself or our own assumptions and knowledge about the system to some degree. I think we always have to, for any meaningful system of testing, we have to encode somehow knowledge about how somehow and somewhere knowledge about how the system is supposed to behave, right? At some point. Only a human can describe that, right? We can build tools that infer that from somewhere in increasingly sophisticated and useful ways.
00:28:37.992 - 00:29:10.516, Speaker A: Maybe being able to introspect production code and figure out what assumptions it should make and what things we can verify and what things we can test and check. But somewhere at the core of any system has to be somewhere that we encode that knowledge of how we think the system is supposed to work. So, yeah, I don't know. I guess I always wish there were magic tools, right, where you could just throw something that fuzzes all your call data or throw a magic formula. Verification tools feel this way sometimes, too, right? Run the magic SMT solver. That will tell you where all the bugs are. But I don't think that's how it works.
00:29:10.516 - 00:29:25.980, Speaker A: And I think once you get down and you think about it, it's all sort of fundamental to the way that building a system and your knowledge of those expectations works. Yeah, sorry to get kind of deep and philosophical there, but this is always where testing ends up leading me, I have to say.
00:29:26.050 - 00:30:02.710, Speaker B: Yeah, testing is incredibly well, one important, but also two very complex, the lower you go, especially when it comes to automating. Automating testing is insanely hard, and I did not think it would be this difficult when I started. But it's such a complex problem, and it'll keep you busy for, I think, honestly, months or even years, depending on which route you go and how you approach it. And if it works or not. Right. You might have to restart and do it all again. But I think a testing is like a big part of reconnaissance on the protocol or whatever code base you're going of.
00:30:02.710 - 00:31:06.990, Speaker B: Yeah, you need to know what basically interacts with each other, what influences what, and then basically get the bounds of what influences and try and twindle it down into something reasonable where you're not spending like a day waiting. And then you also got to be accurate as well in those constraints. So there's a lot of things to really think about when automating something like this. But I think if you know the protocol prior, you kind of know what's common and then what's not common and kind of like a reasonable bound. And I guess let's say you have a function with a UN 224 and then like a boolean, then an address. You already know what's kind of a match, like a boolean one or zero. But maybe the UN 24 can be a range of, it's usually going to be, let's say like four bytes, but then maybe there's an outlier that just does like the max or something.
00:31:06.990 - 00:31:26.130, Speaker B: I guess you can do some constraints for certain situations, but even just doing a fuzz where it's incrementing by one for a un, two, five, six. It's kind of like. I don't think that works there for ages, right?
00:31:26.580 - 00:31:28.556, Speaker A: Yeah. Why don't we just check all the values?
00:31:28.748 - 00:31:31.780, Speaker B: Yeah, let me just chill here for like a decade.
00:31:33.160 - 00:31:33.716, Speaker A: Yeah.
00:31:33.818 - 00:31:38.900, Speaker B: I think there's got to be constraints, right? Or like an efficient way to iterate.
00:31:39.400 - 00:32:17.056, Speaker A: Yeah, I think that's a place where kind of using every tool in the toolbox and knowing about these tools that are available is really useful to me. Right. That's a place where maybe a symbolic execution tool or some sort of more formal methods based tool would be useful in certain scenarios. Right. Okay. I'm trying to test some mathematical property, and I can use a tool that gives me a much stronger guarantee than a fuzzer if I'm trying to throw a bunch of random concrete cases at it or something. Like you said, to think about the bounds, right? Like Brock is doing with parameter to bounds check or test in that.
00:32:17.056 - 00:32:46.424, Speaker A: Know, I'm a big believer in the interplay, even during development, too, like the interplay of your tests and the design of your code. Right. I think you'll meet people who are TDD zealots who really believe deeply in the influence of testing on the design of code. And I think there's a lot of truth to that. Right. That the things that are hard in your tests or the things that you're forced to think about in your tests convey meaningful information to you about the design of your code. Right.
00:32:46.424 - 00:33:23.988, Speaker A: Bounds are a good example there, right. If you're writing a fuzz test and it's difficult to configure those bounds or you're finding that you need to constrain it in a bunch of ways, maybe that's telling you something about the checks within your code itself or even simply thinking through as you're fuzzing something in a test and thinking through those inputs to the code, being forced to kind of slow down and stop and think about where those boundaries are is something that you can carry over into the production code. Right. If you're using bound in your fuz test somewhere, you should probably be also defining a boundary in your code that you can enforce.
00:33:24.084 - 00:33:34.330, Speaker B: Yeah, exactly. Not like just doing an assumption of like, okay, this is an open parameter, basically, but we have bounds in our mind.
00:33:35.180 - 00:33:45.244, Speaker A: Yeah. But I guess I'm a big believer that when you feel pain, either in your tests or in your code, it's the universe trying to tell you something about the design. Yeah.
00:33:45.282 - 00:33:55.104, Speaker B: Testing is such an important thing, but I think it's despised by so many people. It's despised by me, definitely. Especially with half when the tooling isn't there for testing. It's just like the most horrible experience.
00:33:55.222 - 00:34:02.336, Speaker A: But, yeah, we need a test framework for huff that gives you direct stack access, for sure.
00:34:02.518 - 00:34:22.248, Speaker B: Yeah, I think foundry debug is pretty good, but it is somewhat tedious and takes a while to set up. But at least it's a tool, right? Like without that. Oh, my God. Jesus. Testing would not be fun in half. But, yeah, I think tool devs are extremely, like a massive shortage. Right.
00:34:22.248 - 00:34:25.916, Speaker B: It's so hard to find, like a good tool dev or someone that's even interested in it.
00:34:25.938 - 00:34:26.510, Speaker A: Right.
00:34:28.160 - 00:34:38.720, Speaker B: How did you even get into basically tool dev? You were on Cport's c four contest, got poached and then pilled into automated fuzzing.
00:34:39.540 - 00:35:06.132, Speaker A: Yeah. Would we call it tool Dev? I think it's a really interesting system, right. Because it's a bunch of solidity code that sits on top of the foundry fuzzer, but it's not like we're writing rust to do it. It's all solidity, right. It's all libraries and code that's built on top of the base test class within foundry. So, yeah, I mean, it's a tool, right? It's definitely a tool, right? It's bespoke code. It's all written in solidity and it's a tool.
00:35:06.132 - 00:35:41.904, Speaker A: But, yeah, I don't know if I'd describe that as a tooling dev in the same way that, say, the core folks who are building out foundry every day are really working on improving those tools so much respect to them. I think this design is pretty cool, right. And more teams could be building sort of bespoke systems on top of the tools that foundry has given us. Right. And the abstractions that foundry provides, you can build a lot on top of that. And it's been fun for me in the seaport case too, to do stuff like this that is sort of wildly inefficient that I would never do, never do in a production contract. Right.
00:35:41.904 - 00:36:25.488, Speaker A: A lot of the core of this cport code is a library called Seaport Soul, which James Emo worked on and built out, which is a bunch of libraries for interacting with cport structs in cport code and sort of like builder libraries for building up all the different structs and components and data types and entities that you need to interact with Cport. And, yeah, building this huge fuzz engine contract on top of the base test case and building all that out. It's kind of fun to have this playground that you can do interesting and experimental and kind of advanced solidity stuff in without necessarily deploying it to production or thinking about all those things that you have to think about when you're really trying to gas optimize a contract or deploy something that's going to be out there in onchain, like building a.
00:36:25.494 - 00:37:01.004, Speaker B: Fuzzing engine in itself is extremely challenging task, but it's also incredibly fun as well. And I think going down that route also brings a lot of new pathways and opportunities that you wouldn't think would come, even though it's like a very niche thing in itself. That's what I've noticed when getting into it as well, and I never had the intentions of getting into it either. It just kind of happened when I was writing half. I'm like, why can I just automate this test for me? Yeah, it's quite interesting. And how did you really get pilled into it? Did they just teach you from scratch, or were they just like, try this.
00:37:01.122 - 00:37:03.884, Speaker A: In terms of testing in general, more.
00:37:03.922 - 00:37:06.750, Speaker B: Like the fuzzing and automated analysis, I guess.
00:37:07.140 - 00:38:06.800, Speaker A: Fuzzing, yeah. I've been playing with fuzzing and generative testing for a long time. In my pre solidity career, I was very interested in testing, in generative testing, stuff like quick check, and I was really into the closure ecosystem for a time, and generative testing and enclosure had a cool. There's a cool ecosystem for that type of testing enclosure, and seeing it in depth tools and seeing it in the EVM was sort of a continuation of that, I have to say. I never used it that it was interesting to me, and the theory behind it was interesting. And the design of quick check and tools like that was really cool in the web two space, but I never really used it too extensively in my prior career. But then coming into EVM, these tools are, I think, especially useful and especially well suited in this super immutable, super adversarial environment where you have to use every testing tool at your disposal to really test the hell out of everything you're building.
00:38:06.800 - 00:38:14.964, Speaker A: And so seeing it there sort of brought over some of that interest and some of that understanding from previous work. So, yeah, I guess it's something I've always, always been, been interested in.
00:38:15.002 - 00:38:24.064, Speaker B: Yeah, it's quite a fun area. So is this how you're spending most your day right now, just doing this, or are you also doing some other stuff on the side?
00:38:24.202 - 00:39:01.764, Speaker A: Yeah, I'm working on a few things. I've been working independently since February of this year, just working across a few different projects and teams and things that are interesting. Yeah, the c port fuzzing work has mostly been wrapped up, but a couple of other things I've been working on are working with the Farcaster team right now. They're migrating the Farcaster protocol to Mainnet to have permissionless open onboarding for users of Farcaster. So I've been helping them out with that a bit and getting their contracts ready there. I've been working a little bit with the folks at party bid as well. Party Dow, we just launched party latest party app.
00:39:01.764 - 00:39:07.808, Speaker A: Yeah, just different projects and folks who I've connected with in the ecosystem.
00:39:07.904 - 00:39:14.792, Speaker B: Right. And how do you really get approached for this contract work, or do you kind of approach them?
00:39:14.926 - 00:39:34.430, Speaker A: Yeah, I think working in public has been most useful for me. Just c four has been a great place to do that. Right. To establish a security focused reputation and participating in projects there. They just launched these new profile pages, which I think are really cool.
00:39:36.400 - 00:39:55.476, Speaker B: Yeah, they're quite good because it's kind of like a promo on your name. It's like, okay, here's actual experience and what you rank against the rest, and then people come and basically poach the top people. I think it's like a terrific platform to basically get your name out there and show what you can do.
00:39:55.578 - 00:40:31.490, Speaker A: Yeah, I think it's an amazing ecosystem, and the ecosystem has been evolving rapidly. Right. Like the past year, there's been both demand side and supply side. Right. Of the number of auditors and researchers out there and the number of protocols and amount of code that's out there. I think it's interesting to observe how the sort of two sides of that supply demand curve interact and change and evolve over time. But the amount of smart contract code that is being developed is just simply going to outpace the number of eyeballs that we have in the long term, for sure.
00:40:31.940 - 00:41:07.284, Speaker B: I think long term it's just going to be some automated tools to basically do auditing. But I think it's also like a very difficult task in terms of business logic. Right. Because how do you get an understanding of what the contract is meant to be doing? But I guess in terms of post deployment, then you already know what it's not meant to be doing, which is like losing money. So it's quite easy to think of ways to do that. But I think pre deployment, it's a whole different game. Right.
00:41:07.284 - 00:41:16.350, Speaker B: I wonder if you ever gone into AI kind of like fuzzing as well. Have you ever thought of that? Or would that not be on your radar at all?
00:41:17.060 - 00:41:32.020, Speaker A: I think they're interesting places to explore there for sure. I think with any automated tool. Right. It's most useful when it's augmenting human intelligence. Right. I think we'll get there. Right? Absolutely.
00:41:32.020 - 00:41:50.616, Speaker A: Yeah. But certainly I found this was really useful in augmenting my own work writing rank. I was really skeptical at first, I have to say. Right. Solidity in particular feels so security critical. Right. The thinking like, oh, I'll let copilot autocomplete for me, seems dangerous, right.
00:41:50.616 - 00:42:06.076, Speaker A: For a long time. Yeah. Feeding something to check GPT. Right. Maybe I'm kind of a boomer, right. But I've been increasingly impressed with these tools and increasingly using them in my own workflows. Right.
00:42:06.076 - 00:42:11.456, Speaker A: To augment what I'm doing and feeling like they do a good job. Right. Yeah.
00:42:11.478 - 00:42:59.200, Speaker B: I think it's a terrific kind of thing to implement into your workflow as well. For example, if you made a program to find all the control flows in basically a smart contract prior to auditing, then you've just eliminated x amount of time of basically just reading. And now you can just focus on what you do best, which is finding bugs you don't have to just spend. I don't know how many hours it would normally take someone, but a couple of hours just to understand the contract, and then you start working on what you're actually good at. Right? Yeah, I think it's kind of like a superpower, really. Having the ability to make a tool to help you in what you want. If you can do it effectively, then it's just like, oh, wow, I've just made my life x amount better or more efficient.
00:42:59.200 - 00:43:04.416, Speaker B: I'm interested about the bot races on c four. What do you think of those?
00:43:04.518 - 00:43:17.270, Speaker A: Yeah, I haven't participated at all, but, yeah, I think the concept is interesting. How have they been performing? Actually, I haven't kept up with the meta on bot races. I know, I think they're quite basic. What have you observed in terms of.
00:43:18.120 - 00:44:10.436, Speaker B: Yeah, I think they're quite basic at the moment. Where are they only finding lows, mediums? I don't think any of them have the capabilities of finding criticals of quite complex criticals, not just valid data validation input, I mean, or like reengency or anything like that. Those are pretty easy to find, but like, the know need to think of outside context. I don't think anybody's even remotely close to finishing that, apart from maybe one person. I talked to him prior on the podcast, Lucas from pen testify. He's doing a similar thing to me, where actually he's doing kind of like the exact same thing, but AI, which is basically just building a program to only find criticals. So that's what I'm doing, and that's what he's doing as well.
00:44:10.436 - 00:44:28.250, Speaker B: But he's doing AI. But I don't think any of the bots currently have even close to cracking that. I think it's all in like a. I think it's language like rejects or somewhere like that. I don't know how to say that word, but yeah, I think that apparently. So. I think it is interesting.
00:44:28.250 - 00:44:51.810, Speaker B: It could definitely find a lot of low hanging fruit, and I think that's going to stop orders basically from. It's going to prevent a lot of normal human auditors from finding low hanging fruit because it's just so easy to automatically. So I think it's also like a good shift for forcing auditors to become better at identifying more critical things.
00:44:52.260 - 00:44:52.876, Speaker A: Yeah.
00:44:52.998 - 00:44:55.190, Speaker B: Otherwise they won't have a job then.
00:44:55.960 - 00:44:56.612, Speaker A: Right.
00:44:56.746 - 00:44:59.430, Speaker B: Yeah, I think it is quite.
00:45:00.680 - 00:45:18.350, Speaker A: Yeah, I agree. I think it's very interesting to go back and look at kind of previous findings and previous reports and the evolution of where that bar has been over time and just how rapidly that has evolved and changed even on c four over the past nine months. Right.
00:45:19.920 - 00:45:31.808, Speaker B: I think it would be like a fun little. Just to build like a little bot to find low hanging fruit. You can even do that with just like chat GBT. Actually, I think that's probably what someone would do.
00:45:31.894 - 00:45:54.404, Speaker A: I think the really interesting thing about c four is kind of embedding those incentives into the system. Right. I think of it as an iterated evolutionary game that hopefully the outcome of that game is positive. Some for everyone. Right. More secure protocols and in a healthy community of people who are auditing and auditors who are participating being well compensated for that work. But yeah, it's interesting.
00:45:54.404 - 00:46:14.508, Speaker A: I think simply setting up the incentives. Right. In the bot race, something like a bot race tool, starts pretty simple, but if the incentives are aligned over time, you can really evolve that over time. And so hopefully this is sort of the foothold for that to evolve into something that could be really useful. Yeah, we'll see.
00:46:14.674 - 00:46:23.372, Speaker B: Yeah, for sure. I think you should definitely try. You've gone into basically fuzzing, right. So why not build something that's kind of like generalized in a way.
00:46:23.426 - 00:46:50.728, Speaker A: Yeah, I've got to get back in the game. I haven't been on c four in a few months now, but, yeah, you were asking about building in public. That's one place. I think another thing is just building little projects and putting them out there into the world. And I've done a lot of, when I was sort of getting my sea legs in the ecosystem, I did a lot of global hackathons and side projects and things like that. That's been a really great path into the ecosystem. For.
00:46:50.894 - 00:47:14.210, Speaker B: Yeah, yeah, definitely. You meet a lot of people through these events as well, don't you? I think I went to one. It was Eve Tokyo, and that was like my first ever event just for anything, actually. It was fun. You meet a lot of people, and especially if you already have a presence online, you start to actually put a face to a name. Depending on what you do, you may know what to do that. But it is a great networking opportunity, right?
00:47:15.220 - 00:47:24.544, Speaker A: Yeah, it's a great networking opportunity for me. I really like to approach those as this challenge of, can you build something end to end in 48 hours?
00:47:24.662 - 00:47:36.730, Speaker B: Oh, yeah. I mean, I do it already. It's kind of like you put a little timer on in your head, like, all right, let's see if I can do this in a day. But then you start a little hackathon in your own kind of.
00:47:37.820 - 00:47:47.464, Speaker A: Yeah. And without sacrificing quality and really challenge yourself to that. It's kind of like a time trial or something like that.
00:47:47.582 - 00:47:49.820, Speaker B: How did you get into auditing as well, by the way?
00:47:49.890 - 00:47:51.870, Speaker A: I just started with C four.
00:47:52.480 - 00:47:56.460, Speaker B: You're like, I already developed. May as well just look at a code base and see how it goes.
00:47:56.530 - 00:48:33.496, Speaker A: Yeah, I was working on my first code that was going to go to production and be deployed. I hesitated to write smart contracts for a very long time. I attribute some of that, I think, to working originally with the maker team who have a very high engineering bar and engineering standard. And I still think those maker contracts are some of the best EVM code ever written. And so to me, it was very intimidating to deploy anything to Mainnet, right. If you go back and look at my history, lots of projects that were just on Testnet or never deployed or put about on polygon or something. I think the boom in nfts really helped me.
00:48:33.496 - 00:49:09.044, Speaker A: Right. Because suddenly not everything was DFI, right? Handling millions of dollars of ERC 20 tokens or whatever. There was this little corner of the ecosystem that was kind of fun. And there are a lot of interesting things you can do and build with nfts, and they're sort of more creative and artistic in some ways. And so a little bit less pressure to build something that is NASA space shuttle quality, I remember on chain svgs and things like that are just fun to work with. So a lot of my early projects were kind of little things like that.
00:49:09.242 - 00:49:21.576, Speaker B: Building like little projects is a great way to boost your reputation. Just keep on basically shipping and eventually someone's going to notice it, right. And then try and try and onboard you or contract you or even just connect with you.
00:49:21.598 - 00:49:21.928, Speaker A: Right.
00:49:22.014 - 00:49:43.260, Speaker B: And then it's a great way to share. People want to see value, and then once you can provide value, they'll provide value to you as well. I think that helped me. Yeah, I did it in a way where I just did, like, content, like articles and now podcasts. So that's kind of providing value. But these little projects are also like a terrific way. I did like a mev template and that blew up, surprisingly.
00:49:43.260 - 00:49:49.170, Speaker B: And it's the same with anything, like, just keep shipping and then people eventually start noticing. Right, because it's out.
00:49:49.540 - 00:50:15.608, Speaker A: Yeah, absolutely. My path into security was really focused because I was developing as well, and I just wanted to test my own. Right. Test my own abilities there a little bit. Right. Like calibrate myself. Can I see the bugs in my own code? Do I have the capacity to see it? Am I self aware enough to know? And fortunately, I did pretty well in auditing and was able to calibrate myself there a bit.
00:50:15.608 - 00:50:33.596, Speaker A: But honestly, I consider myself sort of a midwit auditor. I don't know, I'm number 100 or something. A little under 100 of all time on the c four leaderboard. And it's very much a power law distribution. Right. The top ten or 20 people in there are really brilliant. And the best auditors are absolutely built different than the best devs.
00:50:33.596 - 00:50:38.656, Speaker A: I think they're doing symbolic execution of the code in their brain or something like that. Exactly.
00:50:38.758 - 00:50:39.410, Speaker B: Yeah.
00:50:40.340 - 00:50:55.736, Speaker A: I think every dev should try their hand at it. Every auditor should try their hand at development. Right. It's all connected for me, and I prefer to do both. Right. Yeah, but that's how it originated for me. Yeah, I think I still consider myself more that way.
00:50:55.736 - 00:51:02.628, Speaker A: Right. I am a dev who. Dev with a focus on security more so than a security researcher who does development.
00:51:02.804 - 00:51:51.800, Speaker B: Yeah, I think it's important to establish that connection as well, because especially for developers, you need to know what kind of attack vectors there are so you can build secure code and you can't really do that. I mean, that's what happened to me when I first started. I didn't do security at all. I didn't personally get hacked, but it was something that was always dwindling in the back of my mind. Like, how do I know that this is secure? You can build tests, sure, but you only cover so much. You're not going to think about, like, okay, it's already deployed, it's established, it's initialized. Okay, what protocols can I use to hack this? Or what kind of abnormalities can occur at a certain block timestamp or with x amount of tokens from this account.
00:51:51.800 - 00:52:21.072, Speaker B: Yeah, it gets just like too complex, I think. But even just having the information of. Okay, what are common vulnerabilities? I guess like reentrancy data validation. Okay, cover those. But maybe the newbie dev doesn't know about those so they can't really think about it too often. You need to have experience, at least a little bit experience in cybersecurity, I think, for being an efficient developer. And same goes with auditors, I think.
00:52:21.072 - 00:52:35.770, Speaker B: Yeah, man, I think it's been quite good. We are running to an end now, but hopefully you've enjoyed this and I hope the audience has enjoyed listening as well. It's been a pleasure to have you them and, yeah, it's been great.
00:52:37.420 - 00:52:39.160, Speaker A: Thanks. Yeah, this was a great conversation.
00:52:39.740 - 00:52:42.650, Speaker B: Of course. All right, everybody take care.
00:52:43.660 - 00:52:44.390, Speaker A: All right, thanks, man.
