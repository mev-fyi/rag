00:00:00.490 - 00:00:26.322, Speaker A: Let's say we're talking about memory safety, right? You take c, and it's pretty obvious that the only way to have memory safe c code is the human programmer writes it correctly. And then you have Python, which assuming the bytecode interpreter is correct, and so you can do that using static analysis, but it requires what is essentially annotations using references and mutability and lifetimes from a human programmer. To have those guarantees without the performance.
00:00:26.386 - 00:01:06.526, Speaker B: Tradeoff of Python scraping bits is brought to you by the following sponsors MeV Protocol maximize your Eth staking value with me v exclusively on MeV IO and composable execute any intent on any chain coming soon to Mantis app. That's Mantisap and fastline Labs, the only mev and intent centric team that has a daily deodorant application rate of over 68% GMGM everyone, my name is Takashi, the host of scraping bits, and today I'm with special guest alpha Rush from trailer bits. How's it going, friend?
00:01:06.628 - 00:01:08.010, Speaker A: Hey, it's good to be here.
00:01:08.100 - 00:01:22.358, Speaker B: It's good to have you on. We've chatted for a bit, but we haven't really spoken at all, so it's good to finally get on call now. It's going to be in the history books for sure. I think it's for some context, just for the people that don't know who you are. Who are you and what do you do?
00:01:22.444 - 00:02:27.466, Speaker A: I got into programming probably about 2019, 2020 somewhere around there, and somehow heard about crypto and started getting really interested in it and just kind of from the more ideological or the applications, stuff like that. I thought it was interesting and realized that maybe there's a way I could actually work in this space one day. And probably about summer 2021, I started getting into solidity and went through the thing that you see common now, where people, they go through crypto zombies, they get on Twitter and start asking questions or posting stuff that they figure out and kind of through that experience, I got connected with Jocelyn at Trill bits. I was trying to fix a bug on slither or something like that, and I ended up getting interviewed for the trailbits apprenticeship. And somehow I got in without very much experience, time to just write. And so I've been there now at trill bits doing security reviews and working on different security tools like Slither for about two years now.
00:02:27.568 - 00:02:52.580, Speaker B: Yeah, cool. It's crazy how you got in so early as well, kick starting your career as a dev, especially getting in like 2020. I think I started the same time as well. It was either 2019, late 2019, early 2020, but I went straight into the entrepreneurial route, I guess jobs never really worked for me, always were like solar. It was kind of onboarding, like going into this big kind of firm, fresh out.
00:02:53.270 - 00:03:46.306, Speaker A: It kind of felt like a sink or swim thing. And I spent the apprenticeships about three months and stuff, but I wrote like Python scripts because it was useful for some work that I was doing in a previous job. And I had done a little bit of front end stuff like react and stuff like that. I just didn't have very much maturity or especially a deep understanding of how computers or virtual machines work. So I feel like during the apprenticeship I was learning very fundamental things, like bytes and bits and stuff like that, where to someone who has a computer science degree or they've been programming since the age of twelve, maybe those things were obvious, but I was learning things from first principles and also learning it on stuff that's very high priority. There's going to be millions and sometimes billions in these projects, so it felt pretty high stakes to do it correctly. What was it?
00:03:46.328 - 00:04:02.634, Speaker B: Kind of getting mentored by these people on the top of their field, especially in the crypto space by Jocelyn and these senior security researchers get it pushed around. It kind of forges you into their kind of standard really early on. So what was that kind of process like for you?
00:04:02.752 - 00:05:05.470, Speaker A: Yeah, I would say I was very fortunate because I think that's the hardest part about getting started in anything, is that you can quickly catch on and kind of in the age of the web where there's YouTube lectures or articles that are pretty. They have a lot of information density, kind of have to have someone to help you prioritize. Like, this is what you should be looking at right now, because if you don't understand this thing, when you get to the next thing, you're not going to be prepared for it. And that's sort of like what, I guess colleges are traditionally set up for. But I never finished. So trobits kind of gave me the stability I needed during the apprenticeship. And even falling just till there's countless people, especially people who aren't even on Twitter or even on the blockchain team, that they take time to answer, just like maybe questions that seem simple and they would take for granted that they know that, but they're kind enough to say, like, hey, I can set aside time to talk to this person and make sure that they really grasp this or help you solve a problem.
00:05:05.470 - 00:05:24.606, Speaker A: So whether that's like some deep theoretical thing about static analysis or you're trying to run a GitHub CI action. I mean, there's just people with wide ranges of skills and just getting to learn from people who are a lot further along and smarter than myself. I feel like that was a great privilege.
00:05:24.718 - 00:05:51.866, Speaker B: Yeah, I definitely agree. You got to find someone that's smarter than you, kind of stick around them, and then you kind of eventually get to the level just from just in nature because you're surrounded by that kind of environment. If you don't, then you're not going to really survive there, are you? And then it just kind of like ever growing. You meet people that are at the same level. Once you get up there, you keep expanding, meet their network, keep expanding. Eventually get to the top. And then once you're at the top, you start innovating and inventing stuff, which I think is the fun part.
00:05:51.968 - 00:06:09.534, Speaker A: Yeah, for sure. It definitely feels like a long road because you get to this point where you realize how much you don't understand, despite maybe I don't appreciate how much I've learned in the time, but then you start becoming the person that people are asking these questions to and you kind of realize like, okay, maybe I have, but it never ends.
00:06:09.662 - 00:06:35.306, Speaker B: Yeah, it's a never ending journey of learning. Once you do get to that top spot, there's even two paths. You just go into a new field or you try know, break ground in. I mean, that's what we're trying to do with the tooling as well right now, especially trailer bits. You just released Medusa, which is like the next version of Kidna, right. Which introduces some interesting things like parallel fuzzing. I've been thinking about that quite recently as well, but I haven't implemented it.
00:06:35.306 - 00:06:57.810, Speaker B: So I think it's kind of like a late stage thing. Basically, you just introduce new threads, right, and it's like highly efficient fuzing. Well, in terms of efficient optimization, but you still need the underlying kind of algorithms to make this fuzzer efficient. So did you play any part in Medusa at all, or are you more on the static analysis kind of side?
00:06:57.960 - 00:07:42.010, Speaker A: Yeah, with Medusa, I think it actually even been started before I worked at troll of bits and was kind of in this period where it just needed to get, or it was in this place where it just needed to go the last mile before it could be shipped. The vision has always been that it would probably replace echidna eventually. I mean, the people who work on echidna are also extremely talented. And so we kind of have this thing where and also with the foundation of HeVM. There's some interesting things that are available to echidna that Medusa is actually built using go ethereum as kind of like a library. So there's, I think, a trade off of what will be available for the same amount of engineering effort. But, yeah, Medusa's primary.
00:07:42.010 - 00:08:13.434, Speaker A: I think improvement would be just like. I think the number one thing would be that we have a lot of engineers who would love to contribute to tooling. But half scale is kind of a niche language, and it makes contributing the barrier to entry is that much higher. You don't only need to understand how a fuzzer does work and maybe possible improvements that you could contribute, you also need to understand the language. So that was one thing. By writing Medusa and go, we want it to be something that people can.
00:08:13.472 - 00:08:51.606, Speaker B: Almost just, a lot of people know go better. I mean, you basically integrate into the node as well. Then you're basically able to grab the actual state live or just use the RPC directly or even just integrate it into the node itself. But, yeah, I definitely agree. You got to make it more enticing for contributors. I also wonder why these large firms actually open source their tools instead of keeping them closed, forced for monetary gain. Maybe it's just to help the overall ecosystem, but I guess it's different perspectives, right? What do you think on that?
00:08:51.708 - 00:10:05.262, Speaker A: Yeah, I wasn't around for it, but at one point, I think trillbits was trying to do the same thing that we see a lot of people doing right now, which is they launch proprietary versions of tools, and sometimes they're even forking open source tools like Slither or something, and they're trying to monetize this through some sort of, like, software as a subscription. And there's definitely, I think, a trade off of if you can get a lot of revenue and subscribers and stuff, you have a lot more resources to build more advanced tooling and have dedicated engineering to. You know, you see places like Satora where they know an incredible number of very senior researchers, like people who are pretty foundational to the field of SMT solving, and they just have them on staff working on that sort of stuff for them. So there is some value to it. I definitely just think that at trollbits, we realized that that was not sustained, even didn't align with our values, and just that we think that security research is like a community effort. Right. And this is not just in the crypto or web3 security world.
00:10:05.262 - 00:10:58.826, Speaker A: It's like, as attacks and vulnerabilities get made public, right. Everyone is trying to add that to their kind of knowledge base. Like, we know this is something that can occur. How can we basically never have to find this bug in the wild again? Can we catch it during the development lifecycle? And that's where you introduce testing and stuff like static analysis and just trying to eliminate those bugs. And so I think that it kind of became part of the mission, just that our tools always need to be improved, but just that we would make them open to people so that anyone can, you shouldn't have to pay whatever it is per month to catch a reentrancy bug. Right. Where it's like some fundamental thing where I definitely think long term slither is fairly mature and so are some of our other tools.
00:10:58.826 - 00:11:25.702, Speaker A: And of course, as I said, software doesn't just, you can't just let it sit there. It also needs to be constantly maintained and improved. And that's something we're well aware of and always trying to figure out what's the next priority. Right. Where is tooling going? And I think that at least my personal conviction is that we will always be giving proprietary people a run for their money.
00:11:25.836 - 00:12:05.234, Speaker B: Yeah, definitely. It's like a big disincentive to kind of make your own tool by yourself or in your own company, because then you're just competing with all these open source tools. And that's like a big backlash I got, or not really massive, but it was just the only thing really mentioned when I talked about it to some friends. They're like, oh yeah, you're basically competing with all these firms. But at the end of the day, I think it comes down to what your goals are. For me, it's financial, but I think once I reach the financial stage that I'm comfortable with, then open source is fine. Be glad to work on that.
00:12:05.234 - 00:12:47.460, Speaker B: But yeah, I definitely think for firms that wouldn't necessarily care about money too much because it's kind of improving the space overall, and that is kind of a positive feedback loop in terms of them more adoption, if the space has better reputation. More people want audits. I know trailer bits is more than just audits, it's actually just improving your code, giving feedback on that. So it's just a massive feedback loop for them. But I think the small fish like open source is great if you want reputation to get people on board. I guess closed source is good if it's really next level stuff, but yeah, it all comes down to goals, I think.
00:12:47.910 - 00:13:57.826, Speaker A: No, I don't think there's anything wrong, especially as an independent researcher to rely on. If you're trying to make your rent or pay for your food every month, you can't really afford to give up your source of how you're making money. But that's kind of where you get the whole. And I still think it's an unsolved problem in academia, right? People get grants, and even then, some of them still don't open source their code. But there is a trend of people trying to open source even the tools that they build with grant funding and making it actually possible to run benchmarks and stuff and verify that these investments that ends up being the government or some other private interest. Even with the advent of public goods funding from optimism and stuff, I think there's still always going to be room just for funding and grants to develop these kind of tools. The way it basically works is that in our spare cycles, between security reviews, whoever wants to, if you're interested in fuzzing static analysis, and a lot of it's ended up being not solidity recently, like, we just released a tool for Cairo.
00:13:57.826 - 00:14:21.210, Speaker A: So the people on our team who are, their goal is to be specialist in Cairo have been building static analysis tools for that. I think that's the long term. Any language or any basically, whatever the frontier of blockchain or just security broadly, that's where trailbits wants to be. We want to be pushing the state of the art, or at least applying the state of the art to these new domains.
00:14:21.290 - 00:14:43.798, Speaker B: Yeah, the ones with ZK, fuzzing is going to be quite interesting, especially exploit generation, because you have private transactions, and in order to do fuzzing, you need to basically fork or have execution engine locally or something to run it on. So I guess an execution engine on ZK is so much more advanced than basic EVM one. So I'm pretty keen to see how this all plays out.
00:14:43.884 - 00:15:17.138, Speaker A: Yeah, there's a couple tools. I don't quite understand the intricacy, but for some reason, fuzzing is particularly difficult for circuits. It has something to do with the field, basically. Generating valid inputs is harder because it's based off of these different fields. And we have started working on stuff like our cryptography team is incredible. I don't know how many people realize it, but I mean, there's a team of like ten people who they're even putting out vulnerabilities of these academic papers. There was one called frozen heart about some.
00:15:17.138 - 00:15:36.038, Speaker A: There's a lot of stuff going on besides fluidity and EVm trobits to apply static analysis and formal, even like some of the ZK tools have delved into the range of formal verification, like using SMT solvers to make sure that your ZK circuits are non deterministic and stuff like that.
00:15:36.124 - 00:16:11.140, Speaker B: Yeah, I'm really keen to see kind of how black hats act in this kind of state as well. I think it's always interesting to see both sides of good and bad, especially when there's private transactions and even just like ZK bridges as well. That's going to be a massive field, which I think is highly competitive right now. People are trying to get a head start, but in such early days, there's no standards. And when there's no standard, you've even built your own fuzzer, like live. It was like a four hour stream, wasn't it? Something around that? And you built like a basic fuzzer in Python. What kind of made you do that?
00:16:11.750 - 00:17:31.226, Speaker A: Yeah, well, I guess being sort of self taught, I've kind of done this thing where I try to look at a tool and understand how it works or read a paper, and there's kind of this unfortunate thing where it's sort of changed even since I joined trailbits, that there's a lot of tools that I found that are open source to understand what these papers describe and stuff like. And it's something I sort of stole from George Haugts, who is notorious for going on YouTube, and he'll read an AI paper and try to implement it. And so rather than waste time trying to understand something and never really accomplish anything, I was like, okay, I'll just take this model of learn by doing and kind of see if I can get this to work. And at that point, I had the motivation would be like, I had been exposed to primarily working on slither, which is a static analysis tool, and I hadn't really worked on fuzzing a ton. And I'm interested in both. But as a constraint of time, I can only really construct to fuzzing to give feedback on. Is it working well? Is there something we could change to make it easier to use, or is there something that could increase the efficiency of finding bugs or something? So a lot of that gets into more of just design or high level ideas.
00:17:31.226 - 00:18:37.330, Speaker A: And so to understand the process concretely, and not just, like, conceptually, I wanted to go through the process of, here's how you randomly generate transactions and figure out what you should be calling. And can you break an invariant that requires. It's not just calling one function, right. You need to call multiple functions across transactions. Can't remember I don't think I ever got the part of actually having coverage guided. Yeah, that was something that remained to do, but yeah, it was a good exercise. And I think the technique of kind of taking a toy tool and trying something out is extremely valuable, because when you want to make a modification to something that's tens or hundreds of thousands of lines, like fuzzing tool, a static analysis tool, a compiler, if you don't have end to end, or at least very good understanding of the code base or the component that you're modifying, you'll spend more time trying to understand it than if you were to take a toy language or a toy tool and try to experiment with some idea.
00:18:37.480 - 00:19:02.298, Speaker B: Trial and error. Yeah, I agree. I've done it from experience of the big code base and then realizing you need to upgrade and then rebuilding it. It's not fun, but I think that's like an expensive lesson as well, in terms of time. But how did you really build it as well? Was it more of. All right, let me just build it simultaneously while thinking. Or did you plan it out, read some papers beforehand, or just go for it?
00:19:02.384 - 00:20:17.058, Speaker A: Yeah, I had read some fuzzing papers, but most of the design, I guess, was just based off of asking questions to Gustavo, who works on Echidna, like, how does this work over time? Kind of having a rough idea of, okay, this is how a smart contract fuzzer maybe should work, and trying to actually convert that to code. And I had worked with Slither and critic compile, which is any hard hat foundry projects you can. It's kind of like an abstraction layer to get all the source code and the bytecode from the solidity compiler and deploy it to a blockchain. So I had some familiarity there. But the biggest thing actually, which is kind of crazy in retrospect, when you look at tooling today, I was trying to use, I think, geth locally, and I could not for the life of me figure out how to start a local blockchain. So I ended up using Ganache, which is terribly slow. And so if I were to do today, you have something like RevM, the rust implementation of the EVM, where I think there's even python bindings to it, and you could very easily spin up something much better than what I was able to do, but it didn't exist at the time, so I was kind of left to what I had available to me.
00:20:17.144 - 00:21:11.234, Speaker B: I agree. Just having an idea, a plan of what to build, and then following it step by step, trying to modulize as much as possible, seems to be the best approach. From my experience at least, I've kind of done everything we're building simultaneously while thinking of what needs to be next. But when building a larger system like fuzzer and dynamic analysis and static analysis combined with symbolic execution, it's usually best to research beforehand and design the architecture on paper and then start implementing. Because then maybe you go down the line and you realize once you finish building it and you print out the output, you think of an edge case and it's not actually scalable and you have to rebuild it, especially since it's such a foundational kind of thing. Like these tools is the required data that needs to be processed and then build a feedback loop for this process to make it more efficient, direct it in some way.
00:21:11.352 - 00:22:07.618, Speaker A: Yeah, no, I definitely think if you haven't built any of these tools before, I wouldn't expect the first one to be like maybe if you really read through a lot of the source code of other ones and you just have some great insight that the original authors didn't. Maybe it's possible, but I would expect it to be more like a learning exercise. And you would kind of realize maybe this doesn't work, because both for using these tools and building them, I think the order is in terms of difficulty of engineering and using the tools. Fuzzing is probably the easiest. You don't have to have coverage guided fuzzing, you can do random fuzzing. And foundry has kind of shown that, right. You have sometimes a really fast, what they call, and I don't mean this derogatory, just like in terms of this is what people call it, they call it like a dumb fuzzer, like it's just random and doesn't have any sort of tricks up its sleeve to cover code that it might not be able to probabilistically.
00:22:07.618 - 00:23:54.034, Speaker A: But when you go build a static analysis tool, you can do stuff like just sort of visiting nodes in the abstract syntax tree or basically grepping the source code, but you're going to have a hard time implementing analyses that are out of the box, available in a more mature tool like Slither, where if you want to find out, does the user's input to a function like does it reach or influence some other variable and affects the amount that you get out of a token transfer or something? That's going to be more difficult without investing time. And then you have tools like formal verification, where kind of a broad umbrella of terms, but generally it means you're representing a program as ultimately some sort of logical constraint or like SMT query to a solver and you're asking it for this input. Can you find a solution? You could spend all the rest of your life probably working on those kind of tools because I think there's a lot of fundamental research to be done there still. And there's also just like a lot of tricks to get the value of it without the sacrifice and speed. For instance, you mentioned hybrid fuzzing, where you combine fuzzing and symbolic execution. And that's one of those tricks where it's like if you aren't kind of studied up on something, you might try to go build a tool that is completely symbolic and you'll realize for some really complex project that it's just going to either blow up the memory or take like hours to run. And that might not be feasible for if you have a week to do an audit and it takes a week to run, you're not going to get the return on investment that you could just using a fuzzer or a tool that combines them, for instance.
00:23:54.162 - 00:24:35.890, Speaker B: Yeah, I think for any kind of team doing a manual audit, they should build some sort of tool where you can direct the fuzzer because you have more context in the code base, and then you can kind of like compile it down into bytecode and then feed in these user provided values to make it a bit more smarter. Whereas if you're just doing like gray box, throw in the bytecode. All right, let's find a vulnerability. That's one of the most complex tasks I've ever dealt with. There's so many moving parts, so many things to account for. And you also need to have a very good understanding of just bytecode in general at that point. What are the patterns you see in vulnerabilities? And then kind of putting that in a heuristic way is quite difficult.
00:24:35.890 - 00:25:06.458, Speaker B: And some people don't even follow patterns as well. So like a pattern, for example, is seeing if there's an ERC 20 transfer or transfer from, and then what kind of relates to this, what's being used, what's a dependency? But then maybe your product doesn't even use these standards. And then how do you generalize that to the point where you know what the storage variable is? Is it relating to money? Is it like a shares kind of thing? Something like that. But it kind of explodes of complexity the more you dive into it with generalization.
00:25:06.634 - 00:26:48.734, Speaker A: Yeah, that's kind of exactly what you say, I think explains why Trollbitz has focused more on fuzzling than formal verification. Because I think there's a ton of room for innovation and automation and for automated vulnerability finding. But when you have such a variability between projects of like, at some point it requires you have to have an idea of what the code is doing and what it should be doing and what behavior is undesirable. And I guess the way to put it would be like you have an oracle problem, right? You need to be able to ask, is this a bug? You can do that with SaC analysis, you can have a heuristic like this is probably a bug based off of we're seeing this function call and just basically it's based off the syntax and you're trying to based off the meaning of what that probably does, you say that's likely a bug or with a fuzzing tool, right? You have a human who's writing a harness, and like you said, you can kind of direct it to where maybe you're not interested in fuzzing, like this really peripheral part of the system where it won't really affect anything. You might find a bug that has really low impact if you were to fuzz it, but you're fuzzing an amm and you can target the swap function and somehow get more tokens out than you put in. When you have a human who can identify like, this is what's important about the system, this is what would be really bad to happen. And we can express that, and you can write what we call like properties or invariants of the system, which are things that should always hold true, that we want this behavior to be consistent to where we can rely on this property.
00:26:48.734 - 00:27:56.726, Speaker A: We know that if you were to deposit and provide liquidity to this MM pool, you're not going to wake up tomorrow and all your funds are gone. I think the point where we're at expressing those properties and solidities is something that solidity is something that's available to developers, it's the language they're writing this code in. And they don't have to learn some specification language, which would be the next step of like when you're asking yourself what is a bug? And you're using formal verification, you typically have to write some sort of specification, and it might be a domain specific language which you need to learn. And you have to be able to express what you call axioms. For all inputs to this contract, this property holds true, which is not something that is available out of the box in solidity. Maybe could be eventually, but we're not there now. So I think it still makes sense to primarily rely on lighter weight techniques and only reach for formal verification when you have really mature code that you have a good understanding of, and you have the time, expertise and resources to invest in that.
00:27:56.726 - 00:28:35.280, Speaker A: And again, I still think back to what we were talking about earlier. Having a dependency on proprietary tools is not something that I'm personally comfortable with. That decision is something that has to be made for everyone. And I think that the ability to fork something on GitHub and clone it and run it locally, I think it's really powerful. It's unfortunate that there's not more advanced, really easy to use formal verification things, but that's kind of like if you ask any person who's really interested in that, that's what they're all focused on and trying to solve. And it's hieroglyphics to anyone who's not in that field. Still.
00:28:35.730 - 00:29:12.362, Speaker B: Yeah, there's actually not a lot of resources on it. You have the fuzzing book, for example, which is a terrific resource, but if you go on like Udemy or any of these course websites, there's no courses on building a fuzzer from scratch talking about these topics in depth. It's more about very specific things, like using a fuzzer to, I don't know, develop some software or something like that. But yeah, I've noticed there's not really a lot of content on it at all. It's more about very surface level things. But I do agree with teams needing to write invariants. But the question is how fast can you write them, right? And if they're using a domain specific language, then it's going to take.
00:29:12.362 - 00:29:50.298, Speaker B: It's just not a good trade off unless it's a very good tool. I think Satora does domain specific language thing with a specification, and they actually just get hired to write it for teams. But if there was some kind of streamlined tool, I think this is a great idea if someone wants to do it, but make something where you can kind of. If I were to do it, I would do this, I would have a really good fuzzer, and then I'll have a user input thing of. Okay, what storage slot should be what at a certain time. The problem with that is solidity compiles into a whole bunch of opcodes. That's just optimize it all.
00:29:50.298 - 00:31:09.714, Speaker B: If you run a basic ERC 20 contract, you'll see there's a lot of filler code or just inefficiencies, which is fine, but it's just like a compiler problem at that point. But yeah, it gets a bit complex to that degree. But if you have some kind of tool that can identify the storage slot in solidity is represented here in the bytecode, you can say, this shouldn't pass this value and then push it back into a fuzzer, and then you can kind of direct it like that, which I think foundry does quite well, actually. I think foundry is kind of like the standard now, but that's not even like a smart fuzzer. It's more of a given variant than random inputs. So if someone built like a tool of a smart fuzzer with tangent analysis, bound analysis, all this kind of stuff, but also having the project and then being able to just write a line of invariance and push that in, where should it be writing invariance within the code itself? Because the thing with foundry that I think is a big flaw is you're only allowed to write invariance at the end of a call, but you're not actually capable of writing invariance within a call. So you have to actually modulize the function to be able to do that, which obviously auditing team is not going to do.
00:31:09.714 - 00:31:18.422, Speaker B: Right. So if someone developed a tool where you can write invariance within a function itself, then that would be massive game changer, I think, for auditors in general.
00:31:18.556 - 00:32:31.082, Speaker A: Yeah, I think the experience of there's the thing they call the handler pattern, which I think is kind of a workaround for not persisting state or something. But Echidna and Medusa, Echidna has been around for, I don't know, five years, and it's always been based in solidity. So, I mean, being able to write invariance in solidity is not anything new. I would be all in favor of having some unified tool chain. It's just a question of who's going to fund it. You look at, even with stuff like the Rust foundation, and you have the compiler and cargo and crates IO and this stuff, there's still all this stuff which we kind of see in the Defi governance stuff, where anytime you have humans involved in making decisions or humans working on something, right, you have two questions to answer, which is like, how are we paying these people, and can we all get along and reach consensus on something? And so for trail, it's working on our tools, having the ability to freely modify them when we determine that something is going to be the best way forward. And I think there's definitely a lot of improvement to be made on user experience and stuff.
00:32:31.082 - 00:32:55.474, Speaker A: But the whole thing, I do think that there's all these things that you mentioned, like bound analysis and stuff, and there's like theoretical benefits or I think the idea was to use it in foundry as basically to help it get past certain. Like you put a constant in your code and you could detect that the value needs to be equal to that constant to pass this basically to reach this branch.
00:32:55.522 - 00:32:55.830, Speaker B: Right?
00:32:55.900 - 00:33:59.302, Speaker A: Well, that's something that's existed since Slither and Echidna have existed. We do those same things and you don't have to wait or do anything complicated to use these things. It's just a matter of like when any advanced functionality for people who aren't regularly using tools is kind of opaque. And that's a matter of improving education and improving the usage of the tools, which is like one of my highest, and I'm always looking for feedback and trying to help people. Very open to and well aware that security tools are some of the hardest things to use for developers. And if we want people to use them effectively, you have to make it as easy as possible and straightforward to do. The other thing you say about using stuff inline to kind of indicate this variable is important, or this variable adding meaning to the code that a tool could use, which is something that I think has kind of been explored with static analysis traditionally, and something we've used a little bit in slither.
00:33:59.302 - 00:35:04.734, Speaker A: Like for instance, if you know that an address is trusted, then you can annotate with a comment, and slither won't warn for reentrancy because you trust that contract not to call back and cause some sort of unwanted action that you didn't anticipate. Or if you have a variable that should only be written by specific authorized users, you have an only owner modifier. For instance, like you can annotate and say any rights to the state variable should be preceded by a check that the address is authorized. So there is work towards that. And I think again, it's a matter of surfacing that to users and coming up with a good way to. There's probably a good way to, as an auditor, when you first look at a code base, there's probably some way to very quickly annotate source code or answer questions about the code where you could kind of ramp up and make your tools more effective. I know I'm looking at something like Makerdao where it's a stablecoin system.
00:35:04.734 - 00:35:48.822, Speaker A: I should probably make sure there's always more collateral than debt or something. And you could very quickly try to identify the state variables relevant to that and then make it easy for your fuzzer before you've even fully manually reviewed the code, you're already starting to fuzz on important properties of the system like that. But getting to a point where it's fully automated, I still think that you're always going to need someone in the loop. You can always reduce the amount of reliance on humans, but sometimes it's better to ask a human than to spend years of research trying to solve a way to slightly incrementally solve something automatically where you don't get as much of benefit.
00:35:48.886 - 00:36:48.026, Speaker B: I think it depends on the ultimate goal as well. I think fully automated tool is definitely feasible for a certain goal. For example, if your goal is to find every critical bug in every single state of a contract, that's quite difficult, and it probably, I mean, depends on what you really are interested in at that point. But I think if you can kind of generate some global invariant, for example, can I take money away from this? That's usually the main one that blackouts use, and that's what you want to deter when you're live. But if you're trying to find mediums, for example, like something that could happen at a certain context, but it's highly unlikely, then maybe they're not really interested in it. And I think that automating, that is quite hard as well. So I think if you have some global invariant, you can try, and that applies to kind of everything, which in this case, smart contracts.
00:36:48.026 - 00:37:18.774, Speaker B: It's always, can you take money? Then you can actually fully automate, but it may take a while, depending on your skill and experience, whatnot, which I think is like a contrary opinion, but, I mean, you don't believe it until it happens, right? It's kind of the same with any innovation. Like, oh, humans can fly now is an aeroplane okay, that was an innovation. Yeah. I think in due time, everything can really come into play. It just depends on what's happening. But I think for specific things, if you have time constraints, it's very hard.
00:37:18.892 - 00:38:09.320, Speaker A: Yeah. I think, for instance, you can pick a bug class and develop a way to find it automatically. It's just a question of, can you do that repeatedly for every single bug that you ever want to find? Or are you going to kind of do this mixture of, like, we'll use static analysis and fuzzing for what they're good for, and then fill in the gap with human security researchers, which is kind of the model of trail of bits. And I think at the end of the day, you always have this trade off. Like, for instance, you look at languages like, let's say we're talking about memory safety, right? You take c, and it's pretty obvious that the only way to have memory safe C code is that the human programmer writes it correctly. And then you have Python, which, assuming the bytecode interpreter, all that stuff is correct. Right.
00:38:09.320 - 00:39:07.210, Speaker A: You're not going to have memory safety issues because you're not manually managing memory. But the trade off there is like, now you're really slow. And so you kind of have this happy medium of rust where using, again, static analysis, they can do this thing called borrow checking, where you can look like, am I accessing memory? Or that I don't own? Or like, for instance, are you writing to an area of memory that you don't have exclusive ownership of? And so you can do that using static analysis, but it requires what is essentially annotations, using references and mutability and lifetimes from a human programmer to have those guarantees without the performance trade off of Python. But I don't know that there will ever be. I think things could always get better. Like will there be a programming language that has memory safety and you can write it like Python, but it's as fast as rust. Does that world exist? Probably.
00:39:07.210 - 00:39:20.110, Speaker A: It's just a matter of a lot of work and research, but I don't know that you get any of these things for free. Or just like, there's no utopic world where we never write bugs again. I think we're always just going to be.
00:39:20.260 - 00:40:03.694, Speaker B: Yeah, I agree. Even with new tech and new innovations that come out, they're always going to be new bugs and new classes of them, new kind of vulnerabilities. Like with AI coming into play, there's going to be vulnerabilities. With AI, for example, you can prompt it in a certain way to get maybe some private information. It doesn't really get adhere to restricting that kind of prompt you can get around. It just depends on how you ask it and all that stuff. So even you see with new innovations like AI, you have deepfakes, which can be great or they can be bad, and then now you have problems to solve, or how do you detect a deep fake in case it's used in core or something or anything like that.
00:40:03.694 - 00:40:32.854, Speaker B: So I think it's always going to be security is just permanently there. It's just you got to keep up with innovation and just really stay on top of it and keep trying to improve. Because I think manual stuff is not scalable at all. The only way you can scale is just hiring more people. But at that point, you just need a shit ton of money and people with experience. So I think eventually you're just going to have to automate as much as possible to maybe help the manual side of it, or maybe. Yeah, it's interesting.
00:40:32.854 - 00:40:52.394, Speaker B: I'm also very interested in invariant generation. I know it's incredibly hard, but I think AI could be used to kind of understand the code base and generate invariants. And you can kind of choose, oh, yes, this is true, or no, this is not true. There's obviously a bug there then, which I think can work if you think about it.
00:40:52.512 - 00:42:06.450, Speaker A: Yeah, I'm not too deep into it, but I think that's kind of the problem you face with these verification tools, is that, and again, like looking at a tool like Sartora, right, where they rely on at some point the bottleneck of SMT solving, they can use static analysis, which can be used for invariant generation, where it's like the example they give in their documentation, something like the memory region, or, I don't know. Basically you can have, using techniques like stack analysis, you can decide that this property of the program is true, and then you can simplify or feed that information into this logical constraint that you've made, and it makes it easier to solve. There probably is a point at which you could do the same thing. Instead of using something like static analysis to find invariance, you could use AI. I think the problem is that, for instance, like a high level property, I always get my money back out of the system for something simple like that. Maybe the AI gets it right, but we don't have any guarantees about correctness, and you would probably end up needing a human to review it and facing this issue of is it subtly wrong? It's kind of like reading code. You didn't it right.
00:42:06.450 - 00:43:05.718, Speaker A: Does it do the thing that the programmer intended? Maybe, but unless you know what the programmer intended, and then for lower level stuff like reasoning about things in bytecode or machine code, where you're trying to speed up something like, could you use it? I think you would probably end up in a place where you're trading off correctness for speed. Because with static analysis, if you're doing what they call like over approximation, which basically just says that you would always favor correctness over missing a bug, then you have basically mathematical guarantees that this bug or invariant is correct, or the bug is not there, or the invariant that you've generated is correct. But with AI, again, you're sort of flipping a coin, I think, at the end of the day, and you would need a human in the loop to check that, but it probably could be used at some level to speed things up.
00:43:05.884 - 00:43:43.490, Speaker B: I don't know if anything actually tells you what is happening. Well, I guess the code tells you, but you don't know the bounds to the code or what's required. So maybe a tool where you can have a static analyzer that tells you what is actually possible. So it generates kind of ranges and what can be called before it, what can be called after it, even that might be useful just to create that kind of documentation. In a sense, it generates documentation in like code format and then the developer can go read that and say, wait, no, this is not actually meant to happen. I missed this before. I think that would be quite interesting actually.
00:43:43.490 - 00:43:44.674, Speaker B: I don't know if that.
00:43:44.872 - 00:44:34.158, Speaker A: Yeah, I think using the stuff for code understanding is probably a little bit more imminent. There's a company called source graph. They have a tool called Cody which you can access in a GitHub like view and you can open a file in the browser and ask questions about. And I've tried that some, and I think it's similar to when you're trying to understand a code base, right. Or how to use an like you frequently use the test or something. And so rather than having to do this really manual process of clicking, go to definition, go to references, you could rely on the AI and it might have inferred these relationships that you would manually discover and ask a question. And it might very quickly give you something that's workable.
00:44:34.158 - 00:44:39.218, Speaker A: Maybe you have to tweak it, but it's definitely more quick, it's faster.
00:44:39.394 - 00:45:12.670, Speaker B: Yeah, I think AI is definitely going to be a massive factor. I'm keen to see how it's played in program analysis as well. It is kind of similar in a way you think about it. AI is quite similar to fuzzing. My knowledge is limited, but from what I can tell, it's basically the same thing, fuzzing into a reward and then based off the reward you kind of change your approach until it's perfect, almost efficient, optimal. So I don't know, I think it's a great pathway to get into if you're in the dynamic analysis and routes, both kind of heuristic based. Right.
00:45:12.670 - 00:45:33.766, Speaker B: That's a path I want to take and I think it's quite interesting, especially if you're an automation maxi like myself, even though it requires a whole lot of prerequisite learning, like math, a bit of probability calculus, in the long term grand scheme of things, it's probably worth, especially in this day and age, when chat GBT is basically revolutionizing the world quite clearly. Yeah.
00:45:33.788 - 00:47:04.066, Speaker A: I don't know too much about the theory, like underpinning artificial intelligence. It's with program analysis, right? You have these things like they talk about soundness. Do you find all of the bugs that exist or something? Whenever you design some sort of technique or analysis, you can ascribe properties to it. And with AI, there's this area of research called explainability and other things like that, where it's basically like, we don't know exactly why the program is arriving at the output that it has, and we don't know that at a future date when we ask it the same thing, would it also determine the same thing? I guess it's just like a trade off of. Would you rather use a technique where you have strong theoretical and even mathematical understanding of what guarantees you have based off of the technique you're using? Or would you rather use something that is a probabilistic system, where you're relying on details like training and the internal weights, which you don't really have any way to understand, or introspect? It might be useful, but I think there's a different level of trust. Like, I would trust a program verifier that uses abstract interpretation, and there could be, like, software bugs that it's incorrect or something, but you still can reason about that versus something that behaves more like a black box, where you're asking artificial intelligence, is my program correct?
00:47:04.248 - 00:47:37.838, Speaker B: Yeah, I think it still comes down to, in my case, whenever I talk about the stuff I'm thinking about, can I extract money from this? Can I brick the system? Can I gain access or manipulate a storage slot in any way that I choose or within some bounds, I think AI can definitely do that. Anything that goes into the realm of understanding business logic, what it's intended to do, and finding criticals from that, I think that's the limit, or at least what's incredibly difficult. If someone can achieve that. They've got like a billion dollar business in my eyes, when you put it.
00:47:37.844 - 00:48:58.040, Speaker A: That way, do you think when you ask yourself, how will AI help black hats or hackers or something, will they be more effective and able to cause more damage? The answer is probably yes, at least at some timescale. Like, I don't know if that's 510 years, maybe it's next month. When you're talking about defending, it's a much harder task to not have one bug versus to find one and exploit it, which maybe that means you have to update your assumption about the capabilities of who you're up against. But at least with smart contracts and stuff, I think it's been pretty ingrained into most people that they're dealing with potentially nation state, or at least people who are willing to spend months and have very talented hackers working to exploit the systems, because it's different than like corporate espionage or whatever else we're doing. Ransomware or stuff like that, where sometimes there's like an indirect value, where it's like maybe we found some information on our competitor and there's sort of an intangible, and then there's ransomware where you get some sort of bouncy and there's like DeFi and you get potentially millions and sometimes like tens, hundreds of millions of dollars just instantly in your wallet. The motive to attack or what you're getting out of it is very clear and obvious.
00:48:58.650 - 00:49:16.762, Speaker B: Yeah, I agree. I think auditing should always be looked at a black hat perspective because that's ultimately what you're going up against. In the d five world at least. I don't have any experience in web two, but if I were to build a protocol, I would want to prevent money being stolen, money being bricked, just money. Anything money related.
00:49:16.906 - 00:50:22.210, Speaker A: Yeah, I definitely think auditors or researchers should have that mindset. But when you're actually working with their teams, coming away with a lot of critical findings is not necessarily reassuring. And then you have projects where they're extremely. If you're going to review any old t protocol and someone paid you like ten k to review it, and they don't have any experience writing solidity, you might be able to boast about it on Twitter and see, like, I found twelve critical issues, but that's a huge difference than working with someone. They're on version four of their protocol and they're like a billion dollar company, and their developers are extremely regimented about building a product that's not going to break and trying to go the extra mile and offer some level of. At that point, maybe you didn't find a bug, but you wrote a fuzz test that now you have higher assurance about that code than you previously did. But there wasn't like a material finding.
00:50:22.210 - 00:51:19.862, Speaker A: I think there's kind of two sides of the coin where it's like, you should definitely find as many. Steal all the funds and tell people about the bugs, or you should not steal the funds. I mean, you should tell them that. Tell your clients or people, responsibly disclose vulnerabilities about loss of funds. Right. There's also a very needed component, because there are also people who are writing code and they need help not just finding bugs in their code, but how are they going to write code tomorrow a little bit better. It's an imperfect process and it's like you're never going to reach, I don't think you're going to become a developer who just can't write bugs, but there definitely is a way to develop software where you're less likely to have something really bad happen than otherwise if you weren't to implement any of the sort of things that people advocate for, like testing and stuff.
00:51:19.996 - 00:51:57.874, Speaker B: Yeah, I think for a larger code base that's gone over multiple iterations, it's definitely a different kind of scope than someone that's on their first iteration startup, because obviously they've gone through all the audits before, all the battle testing, especially with a giant system, big team, reputable developers, engineers that have built a reputation for providing this very strong foundational software. Like maybe it's a primitive. Right. Finding something in that would be, I think, much more complex as well, because if they're a primitive, they're interacting with multiple protocols. Multiple protocols depend on them as well. For example, like curve. Right.
00:51:57.874 - 00:52:38.138, Speaker B: A critical in their thing is having you have the option to choose we or efer itself. And then if you choose efer, you can produce a reentrancy, whether it's read only or an actual one. And then from there you can generate exploits. But I don't know if that's really like a bug in itself, more of a feature, but it has consequences. It's quite interesting, the different kind of areas. And from the people, like I've talked to the auditors, at least the most elite ones, they always look for the functions that handle the money and go from there. I guess there's two different kind of auditing perspectives as well, is post deployment and pre deployment.
00:52:38.138 - 00:52:52.766, Speaker B: Obviously, post deployment you're just looking to steal money, but pre deployment you're just looking to kind of find the business logic bugs. You're not really thinking of game theory stuff, whereas post deployment is all game theory from what I've experienced.
00:52:52.878 - 00:53:35.070, Speaker A: Yeah. And then when you get outside of smart contracts and you're looking at the, we have all these roll up solutions like optimistic and zero knowledge popping up, and there's a lot that goes into add to that, you have the layer one nodes themselves. And sometimes in the case of Ethereum, I don't even know how many, like six, seven, eight, something like that. And probably more if you include. They're all expected to be like the execution and consensus compatible. So you now have all these combinations of clients where you have to be absolutely sure that they're going to work correctly. And so there start to be different kind of bugs that are equally devastating.
00:53:35.070 - 00:54:56.806, Speaker A: But it's not as clear or easy to find as some way to steal tokens from a smart contract, because it might require understanding a system with a lot more code and a lot more complexity. And you have a peer to peer network, you have a virtual machine, you have some sort of proof system and a way to verify those proofs, and it just becomes this endless web, and it very much becomes web two security, where sometimes something that is benign, you can send an input and it crashes a node or something, where it's like if you submit that to some company like, oh, I can crash your server, they'll fix it and roll out the fix and they'll be done the next day. But if you can crash a blockchain node and you can crash a large portion of them or all of them, there's probably ways to make money from that. And those things are sometimes not as I guess, obvious or I think people have become a little bit myopic about. We see all these really big hacks, about tens and hundreds of millions of dollars of D five protocols, but there's going to be a lot less of them that are actually in terms of the amount of code and the amount of, if the things below them are not more secure than they themselves are, you're in a world of hurt.
00:54:56.838 - 00:55:39.382, Speaker B: There's so many attack vectors, there's like wallet hacks, phishing, you got web two attacks in it as well. So it's like combining two different worlds. It's a lot that goes into it, especially once these L2s and ZK VMs come online. And then you have the bridges as well. It becomes a massive network, and especially as blockchain evolves, adds more, I guess opcodes and infrastructure changes, all that kind of stuff. It's going to become quite interesting, and I think security is going to become even more important. So it's definitely a great time to hop on board any kind of opportunity find, because you're really in the grand scheme of things quite early.
00:55:39.382 - 00:56:20.374, Speaker B: I think now we're starting to ramp up on innovation, at least from my perspective. But people really working on some really cool things and it's kind of everybody competing with each other, which I think is fair in every single stage of development. You could be super early and still feel that way. We haven't even hit solidity like version one yet. So if you really think about it really quite early and like ZK VMs only just recently became like a hot topic, that's going to be a whole new era, which I'm very excited to see. But yeah, it's been quite a good chat in my opinion, and it's great to always talk about program analysis. I hope the audience learned a fair bit, and I hope the chat was pleasurable for you as well.
00:56:20.374 - 00:56:21.960, Speaker B: For our first chat as well.
00:56:22.490 - 00:56:28.722, Speaker A: Yeah, I've enjoyed it. We'll have to cover more than just program analysis next time then. It's good talking to you. Digacity.
00:56:28.786 - 00:56:51.886, Speaker B: Yeah, likewise. Thank you so much for coming on out for us. Appreciate your time and I'm excited to see how your career flourishes. It's only been a couple years so far. Three years, right? Can't just see how it's plays out long term. But for the audience, if you want someone to come on the podcast, please dm me on Twitter at scrapingbits or email me at scrapingbits@gmail.com and I'll review whoever you send.
00:56:51.886 - 00:56:54.860, Speaker B: Otherwise, thanks for listening and hopefully catch you in another one.
