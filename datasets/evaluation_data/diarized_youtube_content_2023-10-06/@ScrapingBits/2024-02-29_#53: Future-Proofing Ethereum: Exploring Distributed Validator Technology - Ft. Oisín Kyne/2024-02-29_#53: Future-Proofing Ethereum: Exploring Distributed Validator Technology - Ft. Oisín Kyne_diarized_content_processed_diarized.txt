00:00:00.330 - 00:00:23.278, Speaker A: Back in the glory days, anybody mined independently and every now and again they got a block. It took about two years for the cpu miners to say this is never going to work. Let's make software to share our hashing and we'll divide the rewards. We see distributed validators as kind of similar, which was asterisks. Anybody could have had 32 ether back in a certain point to run and stake and have a chance. Now the only way you do it is if you team up.
00:00:23.444 - 00:00:51.420, Speaker B: Scraping Bits is brought to you the following sponsors MeV protocol maximize your eth staking value with me V exclusively on MeV IO and composable execute any intent on any chain coming soon to Mantis app. That's mantisap and Fastlane labs, the only MEV and intent centric team that has a daily deodorant application rate of over 68%.
00:00:56.350 - 00:00:57.100, Speaker A: Wow.
00:00:57.630 - 00:01:08.102, Speaker B: GMGM everyone. My name is Tagachi, the host of scraping bits, and today I'm with Osheen, the CTO and co founder at Obel Labs. Welcome GMGM.
00:01:08.186 - 00:01:09.614, Speaker A: Happy to be here, GM.
00:01:09.662 - 00:01:23.750, Speaker B: It's great having you on, man. I'm appreciative of you coming on, spending your time with me. I think it'll be very informative for anyone that is interested in validator technology, distributed validator technology, just for the people that aren't familiar with you. Who are you and what do you do?
00:01:23.900 - 00:02:00.674, Speaker A: Yeah, so my name is Oshin. My background is in software dev and kind of DevOps. I first got into crypto in 2018 full time. Kind of heard about it in the 2017 bull run, joined consensus, did about two years there in full stack, mostly in their tokenization business, working on security tokens back in 2019 and 2020 on Mainet before gas got really expensive and that didn't make sense anymore. Left consensus in about March 2020 at the bottom and set myself up self employed over that time. I had figured out the kind of energy use of proof of work and was like, oh, this wasn't great. I consider myself like interest in the environment and stuff.
00:02:00.674 - 00:02:33.430, Speaker A: I was like, how do I square this circle? And I heard about proof of stake and at the time proof of stake was going to be 1500 ether minimum and that was like half a million dollars. And it was kind of unclear how severe slashing would be. It kind of implied that you could lose it all. And I was like, if there's a half a million dollar risk that you could lose it all, bar the OD. Really wealthy early developer, no one's going to stake this. They're all going to outsource it to kind of a professional entity to kind of do it as part of their core business. And I was really concerned that staking would be very centralized.
00:02:33.430 - 00:03:09.958, Speaker A: That was 2018. Two years later, when I was self employed, I got picked up by block demon to do Ethereum two R and D. I had been kind of running one of the early prism clients on Testnet and following along with the proof of stake roadmap, particularly as it came down to 32 ether in maybe 2019, and ended up getting involved with the block demon team, helping design and ultimately build their eth two staking stack. And then they happened to be very successful at Genesis, and we ended up running quite a large amount of the network in the early days. And then I was a little concerned. I was like, wait, wasn't I just kind of criticizing this centralized entity that.
00:03:09.964 - 00:03:11.882, Speaker B: Would run quite a lot of now we're doing it.
00:03:12.016 - 00:03:52.342, Speaker A: Yeah. And I was a bit, maybe kind of confused or I wasn't really sure where I landed. And along that journey, in about the end, almost around Genesis, I got an intro to a community call for what we now know as distributed validators. And I kind of came on the call and there was a good chunk of people on it. And the idea was using a quirk of the BLS signature scheme to make what I would just call it like a multi Sig validator, or in web two speak, a highly available validator, meaning one that can run on multiple servers. And so long as some of the servers are online, your validator stays up. And at the time, having set up like block demons infrastructure, I was like, oh, this makes perfect sense.
00:03:52.342 - 00:04:18.506, Speaker A: Like everything in web two is highly available. Your database runs on multiple servers, your Nginx, your kafka, anything and everything that you run in enterprise software runs on multiple servers. So the idea that your money making validator is something that runs on a single process and falls over. Yeah, I was immediately sold. I was like, yeah, this is going to be a thing. But I was very much a DevOps at heart. And I was like, I'll wait until somebody else makes this and then I'll just run it in my stack.
00:04:18.506 - 00:04:49.378, Speaker A: And then I continued. At the time, I had been working on the tokenization side of things that was kind of my specialty in the space and was kind of mixing that with the validators. I had built kind of an NFT issuance stack kind of before it was cool to some extent in retrospect. And one of the things is somebody asked me to make an ERC 20 token for validators. And I was like, no, you shouldn't make an ERC 20, you should make an NFT because you care about which validator is and isn't slashed and you want to own a particular validator. They're not perfectly fungible. Not all validators are created equal.
00:04:49.378 - 00:05:19.290, Speaker A: So I built the stack to set up validators and NFT. I got a little blocked at the very end on the kind of smart contract side of things because this would have been pre merge. So there was no link between the EVM and the kind of beacon chain as we might have known. That was maybe where I got to. I had built a kind of prototype and then went and attempted to sell it to some people and got nowhere. At sales really is not my strong suit. I had to realize one said, oh, sorry, Oshin, there's no margin for us.
00:05:19.290 - 00:05:46.358, Speaker A: The other was my old boss of consensus and he was like, sorry, Oshin, we're just going to build this ourselves and not buy your one. And I was like, okay. So I reached out to Colin Myers, my now co founder, because he was the one that was organizing the community. Like the distributed validator community calls, this is about April 21. And I was like, hey, built this software for validators, can't sell it. And he was like, funny you should say that. I'm trying to raise money to do this distributed validator thing, but I'm not technical so no one will give me the time of day.
00:05:46.358 - 00:06:01.294, Speaker A: And then we met up, had a chat for a while. I couldn't convince him to build the easy thing. He eventually convinced me to build the hard thing. And two going on three years later, we're on main net with these distributed validators that we'll talk about for the rest of this podcast. So yeah, that's my background story.
00:06:01.412 - 00:06:27.602, Speaker B: Yeah, it seems like current day Ethereum kind of vertical stack. There is always, I guess, the focus on MeV, which is just builder, searcher, relayer, and then just nodes. And you don't really think about validators because there's so much money involved. So it's just kind of discarded in the general sense. From my experience at least, you never really hear about validators, apart from they get all the rewards from all these auctions that are going on on flashboards.
00:06:27.746 - 00:06:41.580, Speaker A: They get all the MeV. It's something I very much enjoy when someone's very into Mev. I once had an investor type come up to me and say like, hey, did you know that 90% of MeV goes to validators? And I was like, did you not.
00:06:42.670 - 00:07:07.906, Speaker B: But the thing is, I think there's so much money involved to run a massive amount of validators. But maybe you do get like the one off chance where you do hit the 500 e bribe and then it pays for a fair amount. But until then, you still have to have a large amount of capital to even compete, right, because there's just so many validators. But for the people that don't really know about validators, what are they and what are their role in the Ethereum ecosystem, for sure.
00:07:08.008 - 00:07:43.600, Speaker A: So before I talk about validators, I'll talk about proof of work, which is what I'd say most people listening to this podcast really are familiar with. But it's all about how do you decide who adds the next block to the chain? So in proof of work, they make it a guessing game. Anybody can take part. You just make a lot of guesses. The guessing game gets harder or easier depending on how fast the blocks are coming. And the problem with that is it's kind of unbounded energy spend, because people are like, I'll just throw more compute at the problem. And that's how you end up with blockchain using more energy than a large country type of situation.
00:07:43.600 - 00:08:34.106, Speaker A: So instead of that, we have proof of stake systems, and you could consider these turn based and the way they keep honesty, instead of it being kind of a public open to anyone, competition, is that you put a lot of capital on the line that is forfeit if you're caught being malicious, more or less. And if you play by the rules and always add a block when it's your turn, and never an invalid block. Well, a block that's kind of not following the rules of the chain, you'll get rewarded at kind of a small percent APR. And if you're caught cheating, your validator is punished heavily, and if you're offline, you're also punished. So this design, it's gone through quite a lot of variations. I think some people get mad at you. Cosmos was certainly one of the more famous currently around proof of stake systems, but they kind of go right back to kind of 2014 and some of the early bitcoin era.
00:08:34.106 - 00:09:07.254, Speaker A: I don't know, maybe like feather coin or something, I don't know. But this is getting. Well, this is a technical podcast, I guess I'm allowed to get into the technical weeds. The bitcoin elliptic curve is secp two five six K one, which doesn't necessarily scale a huge amount when you're trying to do proof of stake. You can normally get into the at best hundreds of validators. These you could probably consider, like the Tron and the eos kind of era of proof of stake validators. And then there was kind of different curve choices and trade offs, and Ed two, five, 5119 was an improvement.
00:09:07.254 - 00:09:40.290, Speaker A: This is what kind of cosmos runs on this maybe allows you to push it to the hundreds of validators. A validator is basically just a private key signing a block, saying, it's my turn, this is the block I'm putting in. And then in about 2019, the Ethereum team adopted BLS. BLS means two things. It means the curve, and it means the signature scheme. But what's special about it is you can aggregate these signatures without access to the private keys. It's an untrusted operation that assuming two signatures sign the same hash, you can take one signature, you can take the other signature, you can combine them together, and they check out against combined public keys.
00:09:40.290 - 00:10:21.054, Speaker A: And this was the kind of breakthrough that allowed one, or like a subset of the validators, to aggregate thousands and thousands of different validator signatures all into one that the rest of the network just has to check one signature. And then they can check off thousands of validators, or even most of them. And this is what's allowed the e two proof of stake spec to get to about. There's currently, I think, about a million validators, or maybe about 800,000 lives. There's been kind of a million created, and that's lots of thousands of increases in magnitude versus the other ones. But a validator, as I alluded to, is just a private key. In practice, there are particular entities that are running most of these private keys.
00:10:21.054 - 00:11:20.018, Speaker A: So although there's 800,000 validators, there's a few of them that are a solo staker, putting up their 32 eth and running it with a fully LNCL in their house. But a large, large amount of them are running in data centers, by enterprises, by the exchanges, by these kind of specialized staking as a service firms, by liquid staking protocols, and so on. It's a bit more mixed. And then while I'm on a roll, I'll also talk about distributed validators. But distributed validators are taking those individual private keys and splitting them like a multi sig, and let's say, for example, keeping it like a three or four setup, and then having four machines play a consensus game, come to consensus, what to sign? They all sign the same block, and then any one of them that gets the signatures can combine them and send them to the network. So this is how you get fault tolerance in distributed validators, and you have this kind of byzantine resistance. So you can have humans or different entities running the different pieces if you want, which is useful for Ethereum and for decentralization.
00:11:20.018 - 00:11:23.842, Speaker A: So that's kind of what distributed validators are beyond a normal validator.
00:11:23.906 - 00:11:30.280, Speaker B: Yeah. When you say four machines, do you mean they have to spin up four validators over just one or something different?
00:11:31.210 - 00:12:00.046, Speaker A: Yes, they're all different clients. We all have containers these days, so you can get a bit creative in what you really want to do. But in the platonic ideal case, it's four different computers in different parts of the world. Better yet, each of them have different execution layers and consensus layers. So one might have geth, one might have Besu, one might have Aragon, one might have rev on the consensus layer. You have different ones, and each of them all have a variety of different pieces of software in the stack. They all have a third of the private key, to oversimplify.
00:12:00.046 - 00:12:04.142, Speaker A: And three out of the four need to sign the message for a full signature to be produceable.
00:12:04.206 - 00:12:19.722, Speaker B: Are they all rewarded or what happens then? Because usually it's just the one validator gets rewarded and it's only one machine. So you can run on a single AWS. But if you're now splitting them all up, are you quadrupling the amount of servers needed and the rewards? What's happening with the rewards as well?
00:12:19.856 - 00:13:01.106, Speaker A: Good question. So both need to be taken into account. So I'll start with your rewards for the validator flow to a single address. And this is where we've done quite a lot of work and why I brought up the idea of tokenizing validators as nfts a long time ago, is trying to figure out how to split these rewards without trust assumptions is something that I think is very important. It's been hard to do a because for the first two years, there was no join between the execution layer and the consensus layer. So a lot of our liquid staking protocol designs come from that area and don't have any kind of communication. So they mostly rely on an oracle that is trusted that says, hey, this is what's going on the beacon chain.
00:13:01.106 - 00:13:40.402, Speaker A: Here's what our balance is. And were they to be compromised or something? They could just say, oh, yeah, there's actually a lot more. There are some rate limits in the protocol, but it is kind of a trust, kind of oracle type of setup. But now that we have the chains merged, that has been a step in the right direction. However, the way that they've been merged, you might call it like, I think they called it like the minimal viable merge. There's no API between the EVM and the kind of consensus layer. During block processing, they process withdrawals and they process what they call a skimming, which is where they kind of take the accrued ether beyond 32 eth and push it to your withdrawal address.
00:13:40.402 - 00:14:22.110, Speaker A: And both of these processes happen without execution. No kind of fallback or anything is being hit. I think they don't even think there's gas or anything. It's just like this account balance is incremented during kind of post block processing, and the next time something touches that account, the balance isn't what you would expect, and this reason. So when it comes to splitting rewards, you have to get a bit creative. So what we've recently released, and something I'm pretty happy about after many years of taking various cuts at it, is an oracle free system to divide validator, principle and rewards, as well as something that's, well, scalable, is the first thing. It's not too bad to do it at all, but to do it with low gas has been something that's taken us quite a long time.
00:14:22.110 - 00:14:58.774, Speaker A: We've now got it down to kind of two contracts, one for the withdrawal address, one for the fee recipient, which are the two sources of revenue for a validator. And it's built on splits Org's splitter contracts. They can be mutable or immutable, but the magic really is that they're pull based accounting. So whenever you call the kind of claim or the distribute method on the contracts, that's where it's kind of checking its storage, saying, what did I write down? The state was last time, one ether. What's in it now? 33 ether. Okay. In the last while 32 ether has come out, that must have been a full withdrawal.
00:14:58.774 - 00:15:16.458, Speaker A: So that goes back to the principle. Whereas another time it runs and it's on 1.5 ether. It's like cool, half an ether. This couldn't have been a slashing. This is clearly like a skimming or a block proposal or something of the sort. We'll push that to the splitter contract, which is basically a contract that allows you to split things by percentages.
00:15:16.458 - 00:16:01.886, Speaker A: So you can say you can either split principal and reward, or most people just split rewards and then one entity is putting up the principal and the entity putting up the principal is getting 90. And then the 10% is, in Lido's case, going to the DVT provider, Lido themselves, the node operators, but things like that. So that's the revenue side of things. So smart contracts do kind of play into it. They'll also get fancier after the upcoming cancun and like Deneb update, because the first piece of the puzzle will be allowing the EVM to access the state route on the beacon chain side will be possible. And then you'll be able to do Merkel proofs, like prove a validator is active, prove a validator's balance is a certain value, and things like that. So things will get a bit fancier in that regard.
00:16:01.886 - 00:16:48.782, Speaker A: And they'll also bring in, hopefully forced exits, which is the ability to exit a validator, which funny enough, right now needs the hotkey, which can be a bit tricky when it comes to who's really in charge of a validator. The withdrawal address is the beneficiary, but right now they don't have necessarily as much power as they maybe should have. And again, this was a function of the chains being split a long time ago. And then the other part of your question was the hardware side, which I also wanted to touch on, because it's something that people ask regularly enough, which is in the case where you're just running one single validator. Yes, you are now running kind of more machines if you want the fault tolerance. However, a lot of times there's different ways you can mitigate it. So one of them is on what we're starting to call like squad staking, which is partnering up with other solo stakers you know, and Trust.
00:16:48.782 - 00:17:25.446, Speaker A: It's like, hey, you already have a node synced and stuff. I'm willing to give you a quarter or like a third of my private key, effectively, and you run a piece of it. Maybe I'll run a piece of yours if you want, partner up and stuff. So there's a lot of solo stakers that already have the sunk cost of a box that are happy enough to kind of put more validators on. And then on the enterprise side, most of these enterprises are already running many machines, and because of the lack of failover, you have private keys. Well, I don't know if you're familiar with it, or maybe I'll say it just for those of your audience that aren't. The biggest way you get punished in Ethereum is getting slashed.
00:17:25.446 - 00:18:06.902, Speaker A: And I think out of the 100 slashing events in the last three years, 99 of them, if not more, come from somebody running the same private key in two locations by accident. They kind of failed over on one box, put it on another, the primary comes back, and then they sign two conflicting messages that is interpreted as an attack on the network, and that will get you a minimum of a punishment of one ether and more if you're a large validator. A lot of validators get slashed this way. So the one thing you can't do is fail over a machine that dies, or if you do, you want to be extremely sure that the original one doesn't come back. And that can be quite difficult to do unless you have physical access to a machine. You can kind of plug it out at the wall. I had a funny story.
00:18:06.902 - 00:18:46.466, Speaker A: It didn't actually happen to Ethereum nodes, it was a different chain. But there was a fire in the data center in OVH, and it wasn't great. There was no one injured, but the anti fire suppression shuts it off and stuff. And all those machines are gone for the foreseeable. But three or four months later, they know, finish the repairs on the data center and they turn the power back on. And in the wings of the data center that weren't burned down, the machines are perfectly fine and they came back online and they booted up and they started validating again. And if you were in an ethereum context, that would have gotten you slashed if you had like many months later been like, oh yeah, those machines are gone, I'm just going to go running away again.
00:18:46.466 - 00:19:40.278, Speaker A: So a lot of these enterprises over provision and have only a small amount of keys per machine, or they're doing something fancy in the cloud. So yeah, either they do bare metal, they keep a relatively small blast radius, or they do something in the cloud where if a machine dies, they can detach the disk and reattach it to another one and kind of trust software in that regard. But the cloud is extremely expensive to run servers in, as you probably know, certainly relative to bare metal. So with distributed validators, you all of a sudden have cheap software based fault tolerance instead of hardware based fault tolerance. So instead of having like an expensive machine with dual network cards and raid arrays and stuff, you can be like, no, give me a commodity bare metal machine that costs as close to nothing as I can put one piece of my stack on it. And so long as I have four working commodity servers that will have better uptime, generally speaking, than some of the cloud based systems. And the cloud based systems are particularly also struggling for performance reasons, not just kind of cost.
00:19:40.278 - 00:20:00.102, Speaker A: So yeah, at the low validator size you have one, you have ten validators. It is like an increase in hardware, but if you're an enterprise in the thousands or tens of thousands of keys, generally you can move to bare metal. And we feel pretty comfortable when we talk to our enterprises, talking about 50% to 80% reduction in operating costs versus a cloud based deployment.
00:20:00.166 - 00:20:22.386, Speaker B: Yeah. I mean, if you think about enterprises, if it's just one split into four now, you're still making the same amount, really, unless someone else comes into that squad you mentioned. But if you're a solo staker, it's actually a higher probability of you actually earning something now, because instead of having the chance of just a single entity in however many validators there are, I think it was like 80 something or 800,000.
00:20:22.488 - 00:20:26.098, Speaker A: It's still only a single one of those, though. You don't end up with four of those.
00:20:26.184 - 00:20:45.418, Speaker B: Yeah, if you're solo valido, you're one in 800,000. But if everyone adopts this distributed validated technology, then now the chances have gone up to, let's say, if their squads are four. Now you have a four in 800,000, because now you're placed into a squad where you share rewards. Right? So it's actually better.
00:20:45.584 - 00:21:22.994, Speaker A: Yeah. That's assuming each of the people in the squad is bringing a full validator into the mix. There's also the situation for people that don't have 32 ether, where they're like, hey, we all have eight ether. We can all go and make it up to 32 ether and have one validator between the four of us. And that was what I was going to kind of say is, I sometimes give the analogy of the evolution proof of work went through, which was, back in the glory days, anybody independently, and every now and again they got a block. But it took about two years for the CPU miners to say, this is never going to work. Let's make software to share our hashing, and we'll divide the rewards.
00:21:22.994 - 00:22:11.618, Speaker A: And that was the invention of mining pools, which is how all of bitcoin works these days. So we see distributed validators are kind of similar, which was asterisks. Anybody could have had 32 ether back in a certain point to run and stake and have a chance. Now, the only way you do it is if you team up four or eight, or we have ten people putting up like 3.2 ether each to run a node. And that is great that you can, and you can take part in the set, but I would say that it's not the most economically feasible to kind of continue to subdivide your awards and further and further. But that's where kind of some of the liquid staking protocols come in and some of this delegated staking and why I spend a lot of time worrying about these splitter contracts and how to make them kind of immutable and trust minimized.
00:22:11.618 - 00:22:57.246, Speaker A: What I'm hoping, and we've kind of seen a bit of so far, is that these large liquid staking protocols that want to decentralize their operator set are like, cool. You ten solo stakers, only seven out of the ten of you need to be working on online. We'll take those odds, we'll put 100 validators across your machines, and we'll all give you 1% each. And that's kind of $2,000 a year, instead of $20 a year if it was just one validator or something. So that's kind of some of the things we're doing with Lido and other people at the moment, is convincing them to decentralize their operators. Actually, I can talk about Etherfi. They also have it public, which is putting their liquid stake protocol capital on solo stakers, or just generally more diverse node operators.
00:22:57.246 - 00:23:05.030, Speaker A: Then when it's a solo thing, you have to kind of trust the enterprises, but when it's a group, it becomes safer to trust. Kind of the long tail, you might say, yeah, 100%.
00:23:05.100 - 00:23:17.110, Speaker B: There's three things. There's a question of if one person fails in a group of four, do they get slashed, or are they saved by the bell from the other three? As in like, are they slashed or are they saved?
00:23:17.270 - 00:24:30.082, Speaker A: So this touches on our delineation between our version one and our version two. Back in the original days, when this was kind of a research item, and it was first produced at like Devcon four, the vast majority of the time spent on that talk was how do we provably figure out who's being lazy and punish them so that there's not like a free rider problem, and it's quite difficult to prove a mission. And we were struggling away with it when we were kind of getting oval started, and I was kind of agonizing over trying to not introduce something that was pretty easily cheatable or gameable and stuff, because you kind of, probably as soon as you set down the rules, the rules change. And eventually I was like, oh, to hell with this, let's just make a best effort distributed valid first, and let humans sort it out on the human side for the first while of if they're not happy with their counterparties, they can just shut it down and start again. And our version two is, well, it's multi phases, but one of the phases is bringing in attribute, so you can say, hey, you were down for an hour a day, that's fine. That's within our kind of agreed upon SLA. But you've been down for a week.
00:24:30.082 - 00:24:48.678, Speaker A: Here is a proof. So you're slashed the bond. You have now posted a slash. But yeah, we kind of said there's not necessarily market demand for the crypto economic game right now, and that our logic was like, let's just make the software and make it good and let people use it, and then we can get fancy with the kind of crypto economics. Second.
00:24:48.764 - 00:24:55.930, Speaker B: Yeah, I wonder, I guess the intuitive solution would be just don't give them any money if they don't sign it.
00:24:56.000 - 00:25:01.238, Speaker A: But do you want to put admin control on the splitter contract? Do you want administration over that flow of funds?
00:25:01.334 - 00:25:12.282, Speaker B: But also there's the question, is it all random? Like, are the squads being generated with a new validator each time in a squad, or is it always the same squad?
00:25:12.426 - 00:26:24.086, Speaker A: Another good question and difference between our version one and our version two. So in our version one, you trust your counterparty, you find them on the Internet or like a telegram chat, and you say you invite somebody's address and you're like, you put in their, er, ensna or whatever, and they are the only ones that come and can sign approval to the terms, and then once they have it, can proceed to the distributed key generation and at that point you activate it with the 32 ether and you run it. So yeah, there are other people that go for the totally permissionless route, but you're back into, we need to punish people and you also have to worry about sibyl attacks and stuff. So I just was like, to help that, let's just make the software for known counterparties, no built in kind of dynamic reward and punishment. If they want to put a multi sig as like the controller of the split so that they can just kind of update the split that's on the people that deploy. You know, we as Obel weren't like going to be arbiters of performance or monitoring everybody or taking kind of anyways, control over those splitters and stuff. So we've gone the simple, I call it the simple end, and we can add in lots of those features and do some of those work in the longer run.
00:26:24.086 - 00:26:27.670, Speaker A: But the goal is firstly to give out this technology for the most part.
00:26:27.740 - 00:27:14.230, Speaker B: Yeah, 100%. I think when you think about the system that's always changing continuously and you're handing out these sections of your private key into a group, I just imagine some attack of, okay, you sent it to let's say the entity is running like 100,000, for example, which is pretty improbable maybe. But anyway, let's just say it is working. If you send to, let's say five different groups, and each five group has one of these entities and entity validators, and you're sending them different private key sections each time, eventually they're going to be able to build that private key. So I wonder, unless you send the same private key the same time, so it's not distributing different parts every single time, which makes the most sense. Why would you send different private key each time?
00:27:14.380 - 00:28:01.254, Speaker A: Well, this is around rotation. It's one of the features that everybody asks is like, hey, can we remove an underperforming operator and add a new one? I'm like working on it. Yeah, we'll have something published. But it scares me so much to leave behind key material and be like, hey, we've done a very fancy rotation of our Shamir secret sharing setup. When designing this, I was like, if I was the other side and somebody said, hey, we've a bad lever or we think they've been compromised, or we're having a dispute with them, do you want to just leave them with a piece of the key and rotate ours, or do you want to exit and recreate the validators? I'd be like, exit and recreate the validators. So I shipped our version one without, and it's something everyone really gives out to me for. They're like, we want to add and change and stuff.
00:28:01.254 - 00:28:14.310, Speaker A: And I'm like, yeah, but it's hard and scary, so we will do it. We have some research on our forum about it, but you take kind of one or two looks at some of the posts and you're like, oh, I can see why they didn't immediately start this. It's just pages and pages of math.
00:28:14.390 - 00:28:44.420, Speaker B: Yeah, I think the v one makes sense. Anything dynamical and continuously rotating obviously brings in a lot more challenges than static, and especially with trusting a validator set, which is like the premise of ethereum, where everything is actually being confirmed and building the blockchain. This is like a very vital thing. And speaking about security, what do you think are some vulnerabilities that can actually occur and how you're mitigating against them in both v one and how you're thinking in v two?
00:28:45.050 - 00:29:21.774, Speaker A: Yeah, this is actually one of maybe the most important design decisions that we made early on, and I'm grateful. I was stubborn when the EF researchers and I were kind of originally bashing out the spec one of the design goals that they had was not to make any changes to the normal spec for a validator that we will perfectly fit within the existing spec. And that was kind of one of the goals. I was like, cool, yeah, let's plug along. And then I consider myself a DevOps. I didn't really know that much. Cryptography, undergrad and cs, that's about it.
00:29:21.774 - 00:30:18.174, Speaker A: And was learning about BLS signatures and the fact that they're additive and particularly homomorphically. Additive is the fancy word, but it basically means you can add signatures for the same hash without access to the private key. Whereas most people, when they think of doing these kind of multi sigs, they're doing some sort of NPC where it's like sending around private keys and crazy things with BLS, all of the private keys be separate so long as you're signing the same object, anybody can combine them. And when I had this kind of moment, one of them said it kind of very nonchalantly and I was like, wait, we can build this software without access to the validator private keys? And they were like, yeah, but we'd need to tweak one thing about aggregations. And we said we weren't going to tweak one thing about aggregations. And I was like, I don't care. You can go and tell these big staking companies that hey, here's software that doesn't need runtime access to these keys.
00:30:18.174 - 00:30:53.626, Speaker A: It's just like a reverse proxy that intercepts HTTP calls, or you can do something that has access to the private keys. There's like a ten x difference. I'm massively afraid of supply chain attacks or basically anyone trying to compromise the developers in our team or any of the libraries we use and stuff. And if that were to happen and your software has access to the private keys, they can all get exfiltrated if this happens. Now, basically when I found this out, I was like, to hell with it. No, we're doing the middleware guys. We're not making a validator client.
00:30:53.626 - 00:31:39.334, Speaker A: And that was a bit of risk because I needed to hope that the consensus layer devs would support it and make any tweaks we needed. And I'm grateful that Danny Ryan and Ben Edgington in particular were know all of the validator clients can have a pluggable, replaceable middleware that we can just throw out and put in a new one. Or we have this new validator that's like custom and does peer to peer networking over the Internet. We don't want that yeah, the middleware sounds way less scary. So in the end, it was only kind of a small API endpoint addition. So it wasn't like a change to the spec or the behavior or anything? Well, it was slight change to the validator behavior, but it was pretty minor, mostly all of the validators supporting that now. So that means you can run caron and, well, it facilitates you creating the private keys.
00:31:39.334 - 00:32:25.658, Speaker A: We're working on having extra implementations of that. Nethermind will be helping us build something, but other than that, you put it into your own private key infrastructure, you store it in your own hashicorporfall to keep your own web three signer or your voucher Dirk, our software, all it does, it comes to consensus on what the downstream validator should sign, and it aggregates what comes back. And if we were compromised and fully malicious, and we were showing slashable messages to the downstream validator? The downstream validator has its own, what they call this anti slashing database. It basically just says, hey, I've already signed something at this slot height. Signing a second thing will get me slashed. I am not going to produce a signature. So you have a liveness failure instead of the safety failure, and that's like an order of magnitude difference.
00:32:25.658 - 00:32:54.270, Speaker A: Going offline is fine, getting slashed is not fine. So not having runtime access to the private keys is probably the big thing. You get ejected. So it takes about 27 days, and you get immediate one ether penalty. And depending on how many more people get slashed in the next two weeks, that dictates the extra penalty. And if it was just you or just 100 validators out of 800,000, you're paying about 1.2 ether in penalties.
00:32:54.270 - 00:33:09.334, Speaker A: But if something really large happened, like there was a client bug or something of the sort, and like percents of the network, at around 16% of the network getting slashed, you lose half your eth, you lose about 16 ether, and, yeah, at 33% of the network getting.
00:33:09.372 - 00:33:11.526, Speaker B: Slashed, you can't validate anymore.
00:33:11.718 - 00:33:14.810, Speaker A: Yeah, exactly. Unless. Yeah, that's a big loss for sure.
00:33:14.880 - 00:33:26.714, Speaker B: Damn. I've always been interested in validators, but I've never taken the leap to get into it. So let's say all these current validators right now on Lido, why would they switch over and add this middleware?
00:33:26.842 - 00:33:58.994, Speaker A: Yeah, it's a good question. So I'll split it into kind of two aspects, the node operators and then maybe lido themselves as like a protocol. So a node operator in the curated set might decide to run their own stack with distributed validators to reduce cost. And increase their fault tolerance. So having just four machines, most of these curated set, they have about 8000 keys each. So they have quite a large amount of stake. So they could divide it across four machines quite comfortably.
00:33:58.994 - 00:35:04.206, Speaker A: And the hope is that they'd have near perfect uptime and would be able to work a nine to five job and only fix it when a machine dies during working hours rather than paging people in the middle of the night because their machine has died and they have no backup. So that would be why an existing curated operator might use it. But what's more interesting is for Lido, the protocol, why they might be interested in it in it, and they're more interested in it for the multi operator aspect. This idea of having multiple nodes, like multiple humans, run a validator because right now gives, generally speaking, total control over a private key to a particular entity, and that they can screw up and get slashed, they can go offline. It can be very difficult to get these people to exit the validator. It's only really hacky workarounds at the moment. And because of all of that trust, they have a pretty thorough curation process where you kind of interview, you apply, they measure you, they put it out in kind of a short trial with a cap, you have to hit these kind of KPIs and so on.
00:35:04.206 - 00:35:55.758, Speaker A: And what they would love to do is reduce the risk in these operators. And recently Lido upgraded to a version two design, and it allowed them to introduce what they call modules, which is ways to kind of permission ido into different sections. And the first extra module, so like the existing curated set is module kind of one. And the first extra module they'll be adding is something called simple DVT. And this is kind of their first attempt to bring distributed validators into Lido, but in the managed sense, like we talked earlier about, you have to pick your counterparties. If they're not performing well, you just have to kind of kick them out or exit the validator. Like it's not in this self sustaining auto rebalancing kind of era yet.
00:35:55.758 - 00:36:16.206, Speaker A: So they called it simple DVT because it will be curated. Like the first module is in that there's kind of an application process. You have to go through a test, you get paired up. So that's what we've been doing for the last three months or so now. We kind of have been doing on health. We actually just basically recently finished. I'd say by the time this podcast is out, it's probably publicly finished.
00:36:16.206 - 00:37:04.014, Speaker A: And I don't know, it depends on how long the turnaround is. But the plan is to put main net validators on by kind of February or sometime during February, and they'll start with a very small percentage just to kind of get comfortable. But the idea is that we had node operators take part in this testnet, which is five x or so more larger than the existing 34 operators. And even if like only half make it through to mainnet, that's still a four x kind of increase in the node operators running in Lido, making it more decentralized. But also, these node operators don't have the same control as the existing ones do. If, for example, they wanted to, I normally give the example of like, start censoring the tether contract. Single operators can kind of just do that in a distributed validator.
00:37:04.014 - 00:37:31.390, Speaker A: You have to coordinate with all your counterparties. Lido opted to do seven node clusters, so you need to kind of get four other people to buy in on your plan to get to five seven to be able to be like, yeah, okay, this is now how we're driving this validator, and that's much more decentralized. And similarly, if you lose your keys, if three lose their keys, you could be offline or you could be in trouble. If five lose their keys, you could be slashed in this instance.
00:37:32.450 - 00:37:53.538, Speaker B: And so let's say out of the seven cluster, if only three of them fail to sign and four of them want to sign. So it's like a multi sig if it's only part like sections or slices of private keys. So it'll be free, seven of the private keys. So how are you able to actually sign?
00:37:53.704 - 00:38:18.954, Speaker A: Correct. You will not. The threshold will be five 7th. In this case, you can design multisigs with different thresholds, but because we also play a consensus game on what to sign, consensus games have pretty strict lower limits. Normally it's two thirds or two thirds plus one. So in this case, in a seven node cluster, two thirds plus one is five or three s plus one to be. Yeah.
00:38:18.992 - 00:38:34.146, Speaker B: How do the keys actually work? Is it like the validator that there's seven validators? Right, and they're all got a sliver or a slice of one private key, or they all have their own private keys that can join together.
00:38:34.328 - 00:39:22.686, Speaker A: It's closer to the former, but I may as well be a bit more accurate. Each of them have a point on a polynomial where the y intercept, where x equals zero, that's the real private key, but none of them know what that is. They just have some random point, and someone else has a random point, and you can figure out the equation of the line when you have five points in this instance. So it's not actually a private key per se. And if you gave it to someone, it's only meaningful with those other private keys that people have. It's not necessarily sliver, they are like actual private keys, but it's a private key that represents a polynomial, and that's where the kind of information is going is you need enough people to be able to interpolate the line.
00:39:22.788 - 00:39:28.338, Speaker B: Interesting. Okay, that makes a lot more sense. I thought they're all given like a few bytes of the private key.
00:39:28.424 - 00:39:34.670, Speaker A: Yeah, I say that just to make it easier, because you try and take in an equation from. Then people get scared.
00:39:34.750 - 00:39:38.082, Speaker B: Okay, it's really like the points in the elliptic curve then. And that's what's happening.
00:39:38.136 - 00:39:38.740, Speaker A: Exactly.
00:39:39.110 - 00:39:40.422, Speaker B: Okay, that makes a lot more sense.
00:39:40.476 - 00:39:45.558, Speaker A: Don't even think it's elliptic, very precise. I think it's a polynomial. But yeah, I think elliptic is whatever.
00:39:45.644 - 00:40:15.198, Speaker B: Okay, interesting. Okay, so we've talked a lot about the V one. What are the challenges that you face when doing the V two, which introduces the continuous dynamical systems? And even a thought I had when you were speaking about the rotations is maybe you can do it like a reputation based system, kind of like flashbots, not similar, but if someone gets slashed, well, they have a less probability of being entered into the next kind of groups if it does happen, than like someone that hasn't been slashed.
00:40:15.374 - 00:41:07.714, Speaker A: Yeah, for sure. So we're splitting the V two effort into phases. And we've been working with the Nethermind research team on this since, I'd say about March of 2023. And we recently started to publish some of these kind of articles or like, kind of research papers on our forum on community global tech. We'll probably repost them to eth research or something for views in a certain length of time. And the first phase is purely getting the cryptographic primitives down, so the ability to change, then we get fancier at the gamification, to kind of oversimplify it, but actually, what is it that will cause a change and stuff? So, in the first phase, the main couple of things we're doing is distributed key generations. There's kind of two classes of them.
00:41:07.714 - 00:41:54.882, Speaker A: There are what are called verifiable and publicly verifiable. A verifiable DKG is just something that if you took part in the DKG and you have one of the pieces, you can check that the protocol was followed correctly, and that everyone actually did what they expected. But if you don't have one of the private keys, you can't tell. And then a publicly verifiable, distributed key generation is the same thing, but with a proof that's a ZK proof or something that anybody can verify and say, like, okay, this DKG followed the rules. They have a valid proof at the end, so everybody probably has the proper piece. That's one of the things that something like Alido would like to see, because they don't have any of the pieces. They're just like, how do I know you guys even created a private key and didn't know? Pick one off of the ground that you found.
00:41:54.882 - 00:42:42.234, Speaker A: That's one piece around public verifiability, and the next piece is around RedKG or doing the rotation. So I've now given that clarification that everybody's really holding points on a polynomial. You can take a new polynomial with the same y intercept, and that's a totally fresh set of keys to everybody. And that's effectively what a key rotation is. You have some kind of hidden information game where those of you that are going to the new polynomial know, and those of you that are getting removed are not being told the key information. But it's like, hey, yeah, we now all have these new points on this new line. It still goes through the same y intercept, so it's still basically the same validator.
00:42:42.234 - 00:43:37.774, Speaker A: And the game is, how do you provably pick that new polynomial while leaving out the person that you're leaving out and stuff? And there's, like, liveness issues. If you can't get everybody online, can you still make it happen? Can you still say, hey, here's this point to multiply by everybody. Just multiply by that point, and you have the new keys, but then the person you're trying to leave behind can also do that. So it's a bit tricky. And then there's maybe just the loading inside is the last piece to point out, which is everyone does the new keys, but they probably do it on a dev machine, or that it's not necessarily, well, this comes back to, I don't want to have runtime access to the private keys. I don't want the runtime binary to sort this out and do it all on demand, or at least not to start. So it'd be, you load in the new key stores, and the runtime says, oh, hey, it's like almost describe as an SQL schema migration.
00:43:37.774 - 00:44:11.978, Speaker A: It's like, I was on revision two with this hash. Now I know revision three, which is going to be on this hash. You wait till everybody knows the same revision three with the same hash, and then you're like cool. From next epoch, we're running on the new one and you kind of do an in flight swap over, and they're the kind of pieces of the puzzle that all have to meet. And then we have to update the solidity as well to be like, hey, solidity, here's the new breakdown or whatever. So they're all the pieces that kind of have to meet in the middle to make the editability, just like the feature happen. And then on top of that you can kind of get into reward and punishment and when and stuff.
00:44:12.064 - 00:44:44.120, Speaker B: Yeah, the initial thought I had was, well, if you're distributing all the polynomials, then if you can do it in advance and know who the next group is, you can generate this new key or new polynomials for everyone in it. And so you can do the rotation, and then that guy's already kicked out, there's a new group, and so what's reset? Again, I don't know if that would work, but then you have to create the keys in advance, and if someone is not actually he fails to join, then maybe you have to subsidize another guy with it or create a new one. Does that sound kind of on track?
00:44:44.730 - 00:45:04.430, Speaker A: Yes, it is. I think normally to get away from the pre creation, most protocols force everybody to contribute some entropy, so you have no idea what's going to happen as a result. But if there was no entropy, you could kind of plan ahead or try and guess. You're never kicking me out because I've crunched and grinded all of the different options or something.
00:45:04.500 - 00:45:24.990, Speaker B: Yeah, because I'm thinking right now it's all pre established, these groups in order to do rotation. Well, it's still pre established right now. How is the key distribution actually done? And how do you ensure that if someone does get kicked out, what happens with the key distribution again? So there's a new player in that party that you have to redistribute?
00:45:25.150 - 00:45:48.358, Speaker A: Yes, it's a good question. So giving it to the new person is easier than somehow not letting the old person find out. So giving it to the new person is a threshold of people is enough. They can figure out the equation of their line together. They can decide on their new line together, usually by contributing entropy. So they've all randomized it together and then they have the new one. But again, there's still five of them involved.
00:45:48.358 - 00:46:30.834, Speaker A: So they can just give the new person another point on the line collaboratively. So that's how the new people get keys. However, hiding this whole process from the person you want to leave out right now, I would say it's mostly on the software side, which is like we use the lib p stack and that's all identity based. So every node has a k one key that identifies itself. So you'd be going into this process of like a change operator. You'd be like, remove this k one key, find this new k one key and give him the new pieces. So I should clarify the node, the binary has its own kind of node key and it's operating on many BLS key shares.
00:46:30.834 - 00:46:47.114, Speaker A: So what we're doing here, well, we're doing both. We're talking to a new k one keys. Like, hey, new k one key. Here's a load of BLS private keys for you. We've done a rotation and you just basically don't communicate over PTP with the old guy. You're like, no, you're on the naughty list. You're blacklisted, you can't contribute entropy.
00:46:47.114 - 00:47:13.506, Speaker A: You can't be involved in this. We're just going to pick a new one and we're going to forget stuff. So you can't see what happened just either. Like we all now have new key stores and the only problem with that is liveness. If there's a sick, like in a seven person cluster, you're kicking out one person and the 6th person is offline. Five is enough to do the rotation. But how do you give that 6th person when they're back the extra keys? It'd be great if there was a bit of a proof.
00:47:13.506 - 00:47:23.110, Speaker A: They could just see and be like, cool, I'll multiply my private keys by set proof. But again, you have to keep out the bad actors. So you can see why I didn't ship this in my first version.
00:47:23.190 - 00:47:29.206, Speaker B: Yeah, it's super complex. When does the key distribution actually happen? In the process of validating a block?
00:47:29.318 - 00:48:03.078, Speaker A: Yeah, so I was going to say key creation might be the thing. It's actually much, much before. It's when you're provisioning the validator. At the very start, you pick a private key and you register it and assign your 32 ether. They say this 32 ether is attached to this public key. You could create that key just randomly rolling dicer picking in a central location. We instead have like a distributed key generation, which is what I described, where it's, everybody contributes some randomness and that's how you kind of define the equation of the line, rather than just having the private key first and then arbitrarily picking a line and giving it to people.
00:48:03.078 - 00:48:13.738, Speaker A: So it's quite important because otherwise one person has the private key and if they get a virus or a keylogger or something, kind of game over. It's much safer to have many people have a piece of the key.
00:48:13.824 - 00:48:44.434, Speaker B: There was another thing I was thinking of. It's okay. What if this group is already selected and now the block is being proposed and they're meant to validate it? What if they just generate it all live as they're meant to be validating? I don't know if that's an option. Maybe it would be a bit way slower. Because let's think about it. If they're all going to actually validate this one block, they're already all in the group, and so they should all be connected to each other and can send or create this key together somehow. Or maybe it's just coming back to the same problem as you all need to do the thing.
00:48:44.434 - 00:48:45.458, Speaker B: To do the thing.
00:48:45.624 - 00:49:26.386, Speaker A: Well, no, it's definitely possible this would be what would be called key refresh scheme. So they vary in different manners. Normally they pick them as like 24 hours or something, but it's that. Hey, yeah, guys, we have this just protocol. We always, every 24 hours, rotate in some sort of predictable pattern. That is an even stronger guarantee against compromise, because then it's like not only do you have to compromise more than two thirds of the nodes, you have to do so before the refresh happens, assuming the refresh is secure and all of that good stuff. The only reason we haven't prioritized that is that would need pretty strong access to private keys to create new ones and stuff.
00:49:26.386 - 00:49:44.630, Speaker A: So we may go for it. But so far I've been like, let's keep the key creation offline. You do it kind of every couple of times, I could imagine. Certainly getting to full in place key refresh, but I'm not aiming for it in my first phase. Currently there's research, I think, on the forum about it.
00:49:44.700 - 00:50:03.546, Speaker B: Yeah. If someone implements this middleware and now you have to centralize, is it a centralized key creator, is my question, because a lot of things with l two s is they have centralized sequences. It's like ethereum is just kind of pushing away the scalability problem and offloading it to someone else, and then it becomes centralized and no longer decentralized. So it's like, what's happening?
00:50:03.728 - 00:50:47.238, Speaker A: Yeah, I agree. So in this case, we strongly encourage nobody to own the keys. We have the ability to just take a key and split it. But in the lido case, for example, it's all distributed key generations, meaning many people contribute and no one person has the full private key. And similarly, I do agree with your concern around sequencers and like admin keys and stuff. And this is where I sweat on the solidity side and why what we've shipped to date has been arguably a bit simple, but simple for the reason that it's immutable and pull only and distribute, and you guys use it yourselves at your own risk. And it's more of like kind of a tool than like a full thing, because you kind of defeat all this, you do all this cool distributed key generation, blah blah blah.
00:50:47.238 - 00:51:03.998, Speaker A: But if I have update access to the smart contracts and update it to my exit address, you've defeated the whole purpose. That's kind of the thing on the l two side of all the fancy complaint rounds and proofs, and z approves. But if the bridge has an admin, what's the point?
00:51:04.084 - 00:51:46.926, Speaker B: Yeah, exactly. I was actually talking about this before at an episode with Ox Porter from Z Sync, and we're talking about decentralized sequences, and I thought that would be really fun. I don't know if anybody's actually doing a decentralized sequencer, so I think anything decentralized and anyone can come up and it's distributed as well, just like anyone can do a builder or anyone can become a relayer, obviously anyone can become a validator, but this makes it so much more safer and it actually makes a lot of sense, incentivizes people to do it, because now you have a higher chance of getting a reward, but it's also securing the network. But yeah, I think it's quite interesting. What do you think about the future of validation on Ethereum and how you see it playing out?
00:51:47.108 - 00:53:03.526, Speaker A: Yeah, I certainly think validation on Ethereum is pretty up in the air right now, in particular because of things like l two s and eigen layer, and kind of doubling down on the commitments for why your stake might get slashed. I'm a little concerned, I might say I wouldn't put myself as like an eager adopter of adding kind of risk to the equation. What I get really excited about, or makes me feel good about the future of staking, ignoring the oh yeah, staking is less bad for the environment than proof of work is. I see distributed validators as basically the first way to take in people with technical ability, but not a lot of capital into staking. The reason being that until now, only something like lido could take in a big enterprise that was really well capitalized and they could be an operator, or you could go into something like rocket pool, which at this point is 16,000, $18,000, and maybe the cheapest is down around two ether and maybe five k dollars. But that's still a huge amount of money for anyone young particularly. And what I realized as we've been running testnets over the last few years and seeing kind of community engagement is there's a whole load of technical teenagers I see, I don't know myself in them to some extent, a lot.
00:53:03.526 - 00:53:43.518, Speaker A: When I was 16 I would run every piece of software I could pirate and play with them and learn about servers and stuff. And we come across lots of people that will download prism clients, they'll download els and cls. You can run them on a testnet with a standard laptop mainnet. The biggest restraint is you need like a two terabyte disk these days. But they have the technical ability to stake, but they don't have the capital, they don't have thousands of dollars to allocate. And what projects like Etherfi and Lidos, even simple DBT, is with distributed validators. You can say, that's okay, we'll put you in as one piece of this validator and it's worth the risk, and you can then give them a cut of rewards.
00:53:43.518 - 00:55:00.150, Speaker A: And this is what 8th five been doing, they're still rolling it out, but they basically did kind of an application form, took applications from solo stakers or people that wanted to solo stake around the world. They proved their credentials with testnets and stuff, and they sent everybody dap nodes and they put twelve or so validators across a group of operators like ten of them, and they started dividing the rewards. And I think in November 1 of the first payments went out and they're still only rolling it out, but I think one of the guys is a kenyan guy who is running his node from a small town and I think hooked up to Starlink and stuff and he got like a $30 payout and he was like, class, $30 is a nontrivial amount of kind of daily living expenses in that part of the world and stuff. And one of my kind of feel good things is I'm hoping I can use DVT and use these kind of oval splits and get one of these liquid staking protocols to put 100 validators on a group and pay for some kid to go to college that wasn't going to go to college, otherwise being like, yeah, I run this box and it makes me $2,000 a year. And those $2,000 a year is what allows me to go to college or something like that. That's, I don't know, the feel good side of I'm glad I don't do mev or do like a leverage d five protocol or something. And instead what I hope to do is get the long tail kind of paid and get ethereum more decentralized.
00:55:00.150 - 00:55:32.594, Speaker A: But then on the techie side, yeah, there's the editability, there's the l two s. Decentralizing sequencers, base sequencers. How validators will be very integral in l two access. We kind of even talked before we started recording about futures and block space demand for validation slots. There's definitely a lot to view on. Can a distributor validator sell to an l two sequencer its sequencing rights on promise of being slashed by Eigen layer if it doesn't do it or something? All of that neck of the woods is still to be figured out. But that's a bit more academic.
00:55:32.594 - 00:55:58.382, Speaker A: Or maybe not quite as impactful as seeing all the people we had. One guy's in our testnet, all he had access to was an Android, iPad or phone even. And he'd done all of our stuff through an SSH terminal. He had rented it, kind of got a free month kind of vps somewhere. And he'd like configured the whole cluster elcl the lot off his phone. And I was going like, this is insane. But he was like he had the technology, he had the interest.
00:55:58.382 - 00:56:03.770, Speaker A: He loved Ethereum, but he just didn't have a two grand server or whatever. $1,000 server.
00:56:03.850 - 00:56:08.834, Speaker B: Yeah, I think that's the limiting factor. It's like hosting. The server is actually hosting these.
00:56:08.952 - 00:56:31.594, Speaker A: It's the capital more than the server. Like the server, there's now Raspberry PI. You get a raspberry PI and you slap on a two terabyte SSD and you can do a whole build for like 500, $600, maybe a bit more for countries where you can't get access to tech so cheaply. But that's still a lot cheaper than the lowest liquid. Staking protocols are two and four ether and stuff. That's five grand, that's ten x.
00:56:31.632 - 00:56:38.650, Speaker B: And I was thinking more of someone running a validator on AWS server. Plus putting 32 e in the validator.
00:56:38.990 - 00:56:57.474, Speaker A: Never run a validator on AWS server. There's like basically no scale that's profitable anymore. Like even if you had 5000 keys, your operating cost is crushed. I'll tell you. This is, again, a technical podcast. So I'll mention why there's a lot of cpu intensive stuff in e. Two proof of stake, particularly this, like, BLS signature aggregation and stuff.
00:56:57.474 - 00:57:37.018, Speaker A: And virtual cpus struggle much more than physical cpus. And also disk space is often, like, networked, and you can maybe get an SSD, you can maybe be kind of close, but generally you'll struggle with disk I o, and you'll struggle with cpus struggling to keep up. So you often need these TL extra larges. You're paying like $400 a month for one machine, which is much, much more like, that's, what, $5,000 a year? And one validator makes $2,000 a year. So how many to even cover your costs? And that's one machine. No fault tolerance, no nothing. So, yeah, if anyone's thinking about running a validator, do it on bare metal.
00:57:37.018 - 00:57:43.918, Speaker A: Or better yet, just buy, like a NOC or something in hardware and put it in your house. If you have the Internet for it, it takes a lot of bandwidth, maybe.
00:57:44.004 - 00:57:52.274, Speaker B: Yeah. Interesting. Okay. You can really just get a pie like a raspberry PI, put it on that, slap it onto a socket, and then click it onto the Internet, and you're all good.
00:57:52.392 - 00:57:54.526, Speaker A: Sign up to take part in the distributed validator cluster.
00:57:54.558 - 00:58:12.598, Speaker B: Yeah, exactly. I super hope it all goes well with this. I think this is a focal point in Ethereum that hasn't got too much attention in, I guess, the dev space that isn't particularly invested in validators. As I said before we started, everyone's kind of focused on Mev or smart contracts and normal nodes, like reef, for example.
00:58:12.764 - 00:58:18.454, Speaker A: Yeah, validators are just so exclusive until now. You either had the amount to take part or you didn't talk about it.
00:58:18.492 - 00:58:40.930, Speaker B: Exactly. Yeah, I remember seeing this when it first came out with the proof of stake, and it was like, oh, my God, you need 32 e to participate. Oh, wait, I don't have that much money. And I think if ETH was like $2,000, that's like $64,000, man. I don't have that lying around to run one node, and then you just don't pay attention to it. And so it's become like this lost kind of knowledge. Unless you're in that game, I think this will open a lot of eyes.
00:58:40.930 - 00:58:51.014, Speaker B: This definitely opened a lot of ideas to me, and I'm now kind of firming into validator technology now, and I'm sure listeners are doing the same, but, man, thank you so much for jumping on.
00:58:51.132 - 00:59:16.446, Speaker A: Yeah, absolutely. I wasn't sure how level of docs or Opsec you are, but there are lots of people in your part of the world that run validators that'd be interested in extra nodes. If you ever want to consider putting a box and taking part like the simple DVT wave two testnet will be going live soon enough. I think it'll be kicking off, I don't know, in mid February something. So if you ever want to run validator a part of Lido and make a revenue stream for it and stuff, you can find a box and a machine and start practicing on test and get the hang of it.
00:59:16.468 - 00:59:21.966, Speaker B: Yeah, I guess the only bad thing with Australia is the Internet is incredibly bad and I do not rely on it at all.
00:59:22.068 - 00:59:28.894, Speaker A: Yeah, that's true. The latency is also quite difficult for Mev. The network is densest in West Europe America.
00:59:28.942 - 00:59:55.094, Speaker B: Exactly. Luckily I'm moving so but man, thank you so much Oshin for coming on. I really appreciate it and telling us about the validator world and hopefully the future of validators as well. It sounds really interesting and I think a lot of people should actually look into oval and hopefully everything goes well and widow starts using it and everyone else that has a know. But yeah, thank you so much for jumping on. I really appreciate it and hopefully we get to talk again soon in the future.
00:59:55.212 - 00:59:59.806, Speaker A: Absolutely. Thanks for having me. I love an excuse to talk nerdy about this type of stuff and not stay too high.
00:59:59.948 - 01:00:03.980, Speaker B: Course we'll definitely have you on, but until then, all the best. I'll talk to you later.
