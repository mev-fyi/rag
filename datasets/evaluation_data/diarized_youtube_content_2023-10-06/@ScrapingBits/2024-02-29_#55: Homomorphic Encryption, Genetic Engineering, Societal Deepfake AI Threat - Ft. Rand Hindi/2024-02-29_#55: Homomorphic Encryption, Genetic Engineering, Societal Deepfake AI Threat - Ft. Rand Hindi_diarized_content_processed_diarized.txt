00:00:00.170 - 00:00:18.640, Speaker A: It was always on the back of my mind that privacy was going to be an issue. And when I got into AI a few years later and then eventually into bioinformatics, which is really just AI for biology, then it became obvious you're manipulating medical data. Genetic data privacy is not even negotiable. My goal since then has been to make the Internet encrypted, end to end, one way or another.
00:00:19.170 - 00:00:59.978, Speaker B: Scraping bits is brought to you by the following sponsors. MeV protocol maximize your EF's seeking value with me V exclusively on MeV, IO and composable execute any intent on any chain. Coming soon to Mantis app. That's Mantisap and Fastlane Labs, the only MeV and intent centric team that has a daily deodorant application rate of over 68%. Gm. Gm. Everyone, my name is Digotchi, the host of scraping bits, and today I have a special guest, Rand from Zammer AI.
00:00:59.978 - 00:01:01.070, Speaker B: How's it going?
00:01:01.220 - 00:01:03.530, Speaker A: Very well. How are you, Degachi?
00:01:03.610 - 00:01:12.514, Speaker B: I'm very well as well. Just landed in London a little bit sick, but that's what comes with the winter weather. But yeah, it's great having you on.
00:01:12.712 - 00:01:17.006, Speaker A: Thank you. And I'll do my best to not to melt your brain too much. If you're sick.
00:01:17.118 - 00:01:23.878, Speaker B: We need the melting man. I want to hear about the technical stuff and get into the granular things, all right?
00:01:23.964 - 00:01:25.174, Speaker A: You're asking for it.
00:01:25.292 - 00:01:29.226, Speaker B: Just for the people that are not familiar with you. Who are you and what do you do?
00:01:29.328 - 00:02:06.018, Speaker A: My name is Ran Hindi. I'm the CEO of Zama, a company working on homomorphic encryption. My background is actually AI and biology. I started coding when I was ten years old, built a company as a teenager, then went into AI when I was 18 years old and started doing some stuff around genetics and using artificial intelligence for genetics. That was my phd, actually, and eventually got into the world of privacy machine learning. Blockchain built a company in AI that got acquired and now building Zama and investing in a lot of cool, futuristic companies.
00:02:06.184 - 00:02:09.266, Speaker B: Oh, sick. What was the last company's focus point?
00:02:09.368 - 00:02:51.618, Speaker A: So Xama specifically is a company working on something called homomorphic encryption. The idea of homomorphic encryption is that you can compute on encrypted data without having to decrypt it. So, for example, imagine I am a user of a cloud service, Chat GPT, for example, today, OpenAI, who's running Chat GPT, sees all the data I'm sending because, well, it has to process it. With homomorphic encryption, I could send an encrypted query to chatgpt. OpenAI wouldn't have the key to decrypt it, so I have no idea what I'm asking. But it can still do processing on the encrypted data itself, producing a response which is self encrypted that I can decrypt on my end. So for me as a user, nothing changes.
00:02:51.618 - 00:02:59.026, Speaker A: Right. I'm just sending data, getting a response, but now the data is encrypted when I'm sending it and during processing. So it's encrypted end to end.
00:02:59.128 - 00:03:17.514, Speaker B: Yeah. This is incredibly important. I was actually thinking of this when I was using these chat bots. It's like you ask these very specific questions and they can see all this stuff. And I think they now work with the Pentagon, if I'm not wrong. And I think they've nerfed the chat GBT very hard as well. So you have to kind of use different chat bots and you don't know what's mean.
00:03:17.514 - 00:03:21.670, Speaker B: They're really good. But the security of it, of privacy is nonexistent.
00:03:21.750 - 00:03:39.666, Speaker A: Yeah. And it's not just privacy for users, like as an individual like you and I, it's also a confidentiality of company information. When your employees are using those tools for work, corporate espionage is a real problem. Right. And so you really want to avoid that kind of stuff leaking when you don't want it to. I think it's Chad. GPT is a recent example.
00:03:39.666 - 00:04:12.414, Speaker A: If you take this DNA company in the US 23 andme, it recently went around Twitter because they went from like a 6 billion in valuation to like a couple hundred million, mostly because they got hacked. That company got hacked. And the dna of 20 million people, nearly 20 million people is going to be potentially exposed. So when you look at that and you're like, okay, I can't really trust anyone anymore because everybody gets hacked eventually. How can it protect against that? Well, the only answer is don't give them data that can get hacked. Basically means keep everything encrypted end to end. End to end.
00:04:12.414 - 00:04:16.590, Speaker A: Encryption of the entire Internet is what we should aspire for.
00:04:16.660 - 00:04:26.594, Speaker B: You wrote an article on this from HTTPs to HTTPs. How hard would that be to actually transition the Internet from HTTPs to HTTP z?
00:04:26.712 - 00:04:58.534, Speaker A: A little bit harder than when we transitioned from HTTP to HTTPs. Remember in the early days of the Internet, there was no encryption whatsoever. And eventually we started encrypting data in transit. Right. So data, when you're sending it HTTP z, HTTP z is really just a continuation of that, where you're keeping the data encrypted even during processing. So the difference is that you're not just encrypting the pipes, you're also encrypting the servers doing the computation. So it's a little bit more involved for application developers.
00:04:58.534 - 00:05:42.058, Speaker A: But that's exactly why as a company, a big part of what we do is automating the conversion from none encrypted to encrypted processing for our users and developers. So we're making that easy on purpose so that this becomes eventually just part of your standards infosec stack, and you just don't think about it anymore. You just build your application, convert it to she homomorphic encryption equivalent, and then just push it live and you're done. At least from an engineering perspective. Then there's a question of how do you push for a new Internet standard? It might take 20 years, who cares, right? But eventually it's obvious we're going to get there, because why not? What is one good reason not to keep the data encrypted end to end when you're offering someone a service, I can't think of one.
00:05:42.144 - 00:05:47.606, Speaker B: Yeah, well, there's no downsides from initial thought, and you've been in it way longer than me, so if you can't.
00:05:47.638 - 00:06:25.958, Speaker A: Think of it either, well, there is eventually no downside. But to be very honest, right now, homomorphic encryption is slower and more expensive to run, so there is like a cost to the user experience. But in some cases, like blockchain, for example, or some simple applications, that's not noticeable, there's really no reason. But we're making this technology faster. And so I would say that in two to five years, the technology will be fast enough that 80% of what you're doing in the cloud will be doable in fhe. So fhe stands for fully homomorphic encryption. So it's really just a matter of time before this becomes practical for pretty much everything.
00:06:25.958 - 00:07:01.314, Speaker A: And so when that happens, then, yes, there is no longer a downside. And then you should be asking the question to those companies, why aren't you using it? And I can only give you two answers, why company wouldn't be using it. One, either they don't care enough to actually even do it, in which case you probably shouldn't trust them with your data in the first place. Right? And secondly, it's maybe because they're doing something that they're not supposed to with the data itself. But any legitimate use of your data by a company who cares about cybersecurity and privacy will end up using homomorphic encryption just because it just makes sense.
00:07:01.432 - 00:07:27.514, Speaker B: I can imagine you get rid of this process of now it's private processing, right? But the vulnerabilities from there, it's not related to it directly. But let's say how you're training AI on it as well. What if someone manipulates the data and you can't actually see it now because it's all encrypted and let's say it's like medical documents and someone gets into the system and manipulates it, and then you're training AI on this data you can't actually see.
00:07:27.632 - 00:07:57.042, Speaker A: That's a fair point. And I'd like to answer that because I think this is something that people often tell us is, oh, but if I can't see the data, I can't fix potential issues. Well, that's not exactly true, because first of all, when we're talking about AI specifically, the amount of data that you're computing on is so huge that you're not looking at it manually anyway. You're using some automated processes to do that. You can still do that on encrypted data. Instead of removing the data, what you do, you just zero out the data, for example. You just basically make it like a no up.
00:07:57.042 - 00:08:43.202, Speaker A: It just doesn't have an impact on the model anymore. So you can still filter the data sets even if it's encrypted, by applying rules on the encrypted data itself. So that's fine when it comes to manual inspection, instead of trying to measure and track everything users do to find every single microbug. What I think we should go back to is what we have always done on our desktop software, which is not 24/7 analytics and surveillance, but rather when the user notices something doesn't work, they just send a bug report. Can you imagine if your computer was 24/7 streaming your screen to Apple or Microsoft or whoever so that they could do bug fixes? No fucking way, man. No. When something crashes, there is a pop up that says, would you like to send a bug report? Yes, I want.
00:08:43.202 - 00:08:48.114, Speaker A: Because I want this thing to get better. That should be the same thing. And everything we're using online.
00:08:48.232 - 00:08:57.222, Speaker B: I imagine it would be, man, if they were recording twenty four seven, I imagine they could definitely do that with the new Apple. What is it? The headset. Imagine that was streaming twenty four seven.
00:08:57.276 - 00:08:59.782, Speaker A: Like a ski goggle, not an oculus, I would say.
00:08:59.916 - 00:09:05.546, Speaker B: Oh, the memes have been great with that new product. What is the difference between partial and full?
00:09:05.648 - 00:09:31.026, Speaker A: So partially homomorphic encryption is when you can only do some operations on encrypted data. For example, you could add to encrypted numbers, but nothing else. Or you could maybe multiply to encrypted numbers, but nothing else. Fully homomorphic encryption, which, to be honest today is just homomorphic encryption because that's what everybody does, enables you to do anything with the data. So it's turing complete, right? There is no limitation in terms of what kind of computation you can perform on the encrypted data itself.
00:09:31.128 - 00:09:36.290, Speaker B: And what's the difference between this kind of encryption and something like Shaw free.
00:09:36.360 - 00:10:07.374, Speaker A: For example, when you look at traditional encrypt, like AES and all of these things, right? They don't allow you to compute on the encrypted data. They only allow you to encrypt it and decrypt it, and then that's it, right? So it's a way to keep data private when you're storing it and when you're sending it, but you have to decrypt it when you want to process it. With a homomorphic encryption, you're basically keeping the data encrypted even when you're doing processing on it. So it allows you to effectively do those computation on the encrypted data itself.
00:10:07.492 - 00:10:09.438, Speaker B: So you're able to basically reverse it.
00:10:09.524 - 00:10:54.890, Speaker A: You don't actually reverse it because it never gets decrypted at any point. Think of it a little bit like, imagine if I took all the numbers in mathematics and I randomize the orders of those numbers. So one becomes seven, seven becomes 1515 becomes five, and so on and so forth. Those numbers still have mathematical properties. They're still numbers, right? You can still add them, you can still multiply them together, you can still compare them, right? You just don't know what they mean. And if you have the private key, the decryption key, you can put them back in order. And so this is really how you have to think about homomorphic encryption, is you're changing the interpretation of the mathematical space you're operating in, but you're not changing the rules and properties of those spaces.
00:10:54.890 - 00:10:57.614, Speaker A: You're projecting yourself in a different universe, effectively, yeah.
00:10:57.652 - 00:11:00.830, Speaker B: So you need a key, basically, to be able to interact with this stuff?
00:11:00.980 - 00:11:06.722, Speaker A: Well, you need a key to decrypt the result and do something with it, but the key stays with the user, hopefully, yeah.
00:11:06.776 - 00:11:12.958, Speaker B: How do you manage key management, then? And like, a secure communication in a distributed environment, like blockchain.
00:11:13.054 - 00:11:36.186, Speaker A: So in the case of blockchain, it's a little bit different, because when you think about a blockchain. We're not in a single user setting, so maybe let me explain the two scenarios. Let's imagine something like Chat GPT. I'm a user, I want to use the service. I'm a single user using the service. So in this case it's very easy. As a user, what I do is I send my public key to the service provider, I send them my encrypted data to do processing on.
00:11:36.186 - 00:12:16.934, Speaker A: They do the processing, send it back to me and I decrypt it with the private key that I'm holding locally on my computer or mobile phone. So this is just like any key management on your phone or computer. You put it in an enclave, or maybe you put it like in your crypto wallet. Doesn't really matter. Or maybe it's in your browser natively, ideally something like that. However, if you're in a multi user setting, like a blockchain, or let's say you want to train an AI model collaboratively with multiple different people, well, by definition nobody has the key because you need to have multiple people being able to access the result. So here what you want to do is you want to encrypt everything under one unique public key so that all the data could be mixed and matched together, right.
00:12:16.934 - 00:12:37.600, Speaker A: You want this kind of composability on the encrypted data and you split the private key amongst the different people who should be allowed to get the result. That's something that's called threshold decryption. The idea is kind of like a multi sig. The idea is that until you have a certain number of people agreeing to do the decryption, nobody can get the result.
00:12:38.130 - 00:12:40.426, Speaker B: The multi sig for decryption.
00:12:40.618 - 00:13:02.102, Speaker A: Exactly. It's kind of multi sig for decryption. And so you're using a public key for homomorphic encryption with a multi sig for decryption of the result, so that you don't have to worry about anybody holding the private key alone. And that's exactly what you do in blockchain, right. You basically have a public key for the network, and then you just split the key amongst the validators and the smart contracts dictates who's allowed to see which value.
00:13:02.236 - 00:13:32.558, Speaker B: Interesting. This is another thing that I always think about when talking about zero knowledge and cryptography and blockchain. Like a big thing that happened was the tornado cash sanction and those developers got wrecked. But it was really weird because ZK was still developing after blockchains. Based on ZK, where you basically enable something like tornado, how do you see it all playing out with platforms that could enable unethical use cases, which also, it's a guarantee going to happen in every scenario. So what do you kind of see happening?
00:13:32.724 - 00:14:01.654, Speaker A: I don't think there is anything you can do to prevent misuse of technology when you're a technology provider. Right. Like, at some point, you just have to accept that every great technology that has advanced society has been used to do bad stuff as well. That's just the nature of it, right? Powerful things are useful for powerful stuff. One good way or one bad way. What I do think, however, is that when people think about privacy and confidentiality and these things, there are two different things, really, that they're talking about. One of them is anonymity.
00:14:01.654 - 00:14:30.382, Speaker A: I don't want people to know that I'm the one doing XYZ transaction on this blockchain. This is what's happening with tornado cash or Zcash. They're not actually hiding the data, they're hiding the transaction traceability itself. Right. They're making transactions untraceable, which is basically a way to remain anonymous. Homomorphic encryption. Fhe is different in the sense that it doesn't make you anonymous in the sense of anonymizing your transaction pattern.
00:14:30.382 - 00:15:08.938, Speaker A: It keeps the data on chain private. So let's take the example of transferring money. If you're using tornado cash or zcash or any of those existing ZK protocols, I'm sending you, let's say, $1,000, what will happen is you could still see on chain that I sent $1,000 to someone. You could still see that someone received $1,000, but you cannot link the sender and the receiver. You're basically breaking the traceability. In the case of homomorphic encryption, you keep the traceability. So you can see on the blockchain that Rand has made a transfer to the Gotchi, but you cannot see the amount being transferred or the balances of Rand and Degatchi.
00:15:08.938 - 00:15:29.762, Speaker A: Okay, it's a little bit. If you're comparing, like, Tor versus signal, Tor is a way for you to access public content anonymously by making it untraceable. Signal is a way for you to share private messages on the public network that anybody can watch. Fhe is like signal, tornado cache is like tor.
00:15:29.826 - 00:15:37.830, Speaker B: Okay, yeah. Interesting. Do you think being anonymous on the blockchain is a bad thing or just different use case?
00:15:37.980 - 00:15:58.286, Speaker A: Different use cases. I think we all have a doxed and non doxed account. Right. But why do we do it? Right? Different reasons. I don't do it for any legal purposes. I just do it because I want to be able to freely follow people and do things without giving too much information into what I'm interested in. As a CEO of a company, everybody I follow is a data point I'm giving everybody else.
00:15:58.286 - 00:15:59.454, Speaker A: And what I'm currently thinking.
00:15:59.572 - 00:16:01.118, Speaker B: Yeah, definitely.
00:16:01.284 - 00:16:24.690, Speaker A: I do think for sure that there's going to be eventually, like a KYC public official compliant blockchain, and then there's going to be like a dark web. It's inevitable. There's so much money at stake now. Most people, they'd rather just be compliant and never have to worry about it. But there are legitimate users for the dark web, just like there will be legitimate uses for the dark blockchain.
00:16:24.850 - 00:16:40.378, Speaker B: I definitely see that happening. Like a dark web tour, basically a blockchain, dark web tour kind of thing. Like a form of payment. I mean, it was already used as payment for it. It's just not really private. So it seems obvious that it's going to be the next thing, but it's just a question of when.
00:16:40.464 - 00:17:31.630, Speaker A: And beyond privacy. There are just some use cases you wouldn't do on a blockchain if everybody can see what's happening and all the data on chain. For example, if you want to use a blockchain for medical data, you're never going to put your medical data on chain for everybody to see, right? That would make no sense. Or even if you go back to finance, let's say you want to do uncollaterized lending, so credit, same as you go to your bank, you go to your bank, you want to borrow some money, your bank will look at all kinds of very personal financial data, record history, where you live, how much money you make, who you're married to. You would want to do that on a blockchain to borrow from anybody else, but you wouldn't want to put all of that personal data on chain in order to do so. Right? There's so many use or just games, man, games. If you and I want to play a game on chain, let's say poker, it's not exactly fun if we can see each other's cards, is it?
00:17:31.700 - 00:17:53.342, Speaker B: Yeah, I think this kind of technology enables all very important things to go onto the blockchain, which is kind of like what the whole point was, shift everything onto it. But, yeah, you need privacy. And this seems like the perfect kind of pathway, apart from, I guess, fully anonymous, but then I guess fully.
00:17:53.406 - 00:18:08.946, Speaker A: You want both. Ideally, you want both. You want anonymity and privacy. But if I had to choose today, I would choose privacy over anonymity, because even if you are anonymous, if your data is publicly available, there's always going to be a way to retrace you somehow.
00:18:09.058 - 00:18:11.062, Speaker B: Yeah. Especially if it's there forever.
00:18:11.206 - 00:18:12.458, Speaker A: Exactly. Right.
00:18:12.544 - 00:18:16.246, Speaker B: So what happened with all those drug dealers and users and whatnot?
00:18:16.358 - 00:18:19.126, Speaker A: You only need to make a mistake once and then you're doxed.
00:18:19.238 - 00:18:22.022, Speaker B: Yeah, exactly. It's super hard to stay anonymous.
00:18:22.166 - 00:18:24.046, Speaker A: It is, yeah, it is.
00:18:24.148 - 00:18:42.702, Speaker B: Because I think actually a lot of hackers in web3, I don't think they actually take the time to be anonymous in web two, which is majority of the effort, actually, because you could do a transaction without a VPN on, for example. And if you're sending it to an RPC, they're most likely.
00:18:42.766 - 00:18:45.250, Speaker A: Yeah. Tracking has your IP address.
00:18:45.320 - 00:18:46.420, Speaker B: Right, exactly.
00:18:47.990 - 00:18:51.602, Speaker A: Even if you go through a VPN, right, you have to make sure a VPN provider is legit.
00:18:51.666 - 00:18:54.674, Speaker B: It's like just basically web two with extra steps.
00:18:54.802 - 00:18:55.480, Speaker A: Exactly.
00:18:55.850 - 00:19:13.030, Speaker B: Even if you spun up a node, you're still connecting to an Internet service provider, and that's got your home there and all this other stuff. It's a bit hard. But, man, how did you really make the transition from biotech to privacy? It seems like a big 180.
00:19:13.110 - 00:19:45.206, Speaker A: It actually came before bio. So when I was a teenager, I was 14 years old in the social media website with a friend in high school. And it was quite popular back then in France, and a lot of people were using it. And at some point I had this older dude in school, I was 14. He was like 16. Remember at the time, there's a big difference between 14 and 16, right? He was like tall, big, older, and he was basically bullying me. And I was like, man, I obviously can't defend myself physically, right? It's just not possible.
00:19:45.206 - 00:20:21.714, Speaker A: So I have to find a different way to get him to stop and to basically kind of leave me alone. And I started looking into the private messages people were sending on my social platform. And I saw that he was actually exchanging messages with a few people and that he was talking about all kinds of secret crushes he had and stuff like that. So the next time he came to me, I confronted him with that, saying, hey, if you actually approach me ever again, everybody's going to know about XYZ secret crush that you've got. So obviously he never bothering me again, right. He just disappeared. So justice was made.
00:20:21.714 - 00:20:57.982, Speaker A: I kept on thinking, is it right for me to be able to access his data just because I'm operating the service? No matter why I did it. Right. Just because I bullied a bully doesn't make it right for me to access his data. And I kind of felt like, man, this whole Internet thing is going to be a real problem there. And so he was always on the back of my mind that privacy was going to be an issue. And when I got into AI a few years later, and then eventually into bioinformatics, which is really just AI for biology, then it became obvious you're manipulating medical data, genetic data, then privacy is not even negotiable. Right.
00:20:57.982 - 00:21:04.340, Speaker A: It's just like, you have to think about that. My goal since then has been to make the Internet encrypted, end to end, one way or another.
00:21:04.710 - 00:21:26.226, Speaker B: Interesting. Yeah. Because a big thing I always think about is cybersecurity. And let me talk about genetics as well. Genetic engineering for, let's say, childbirth, say ethical or not, but it's probably going to happen if someone hacks into that and just messes up the genetic programming, right? It's going to completely screw up. Like humans.
00:21:26.338 - 00:21:40.278, Speaker A: That's right. DNA is really just code, right? So you can read it, you can write it, you can hack it. I didn't think about that. But you definitely wouldn't want someone to hack with the DNA of your future kid. Man, I just had a kid, so that makes me shiver just thinking about it.
00:21:40.304 - 00:21:40.878, Speaker B: Yeah.
00:21:41.044 - 00:21:41.806, Speaker A: Interesting, right?
00:21:41.828 - 00:22:06.914, Speaker B: But even I reckon in the future, if you can genetically modify DNA, then you can cure diseases, right? And if someone's doing that kind of treatment and it's been hacked, the tides of everything. Imagine a whole society is doing this treatment and you're just wrecking them genetically. That would wipe out a whole civilization if they all do it.
00:22:06.952 - 00:22:17.686, Speaker A: I think people are already working on that, aren't they? There must be a lab somewhere doing some really dangerous stuff with some kind of viruses. That wouldn't be the first time, I think.
00:22:17.788 - 00:22:26.810, Speaker B: Oh, yeah. I think a big thing is like gain or function, where people are just trying to modify viruses to get more advanced.
00:22:27.230 - 00:23:17.542, Speaker A: I wish homomorphic encryption was a way to solve that, but it isn't, because, you see, just because the data is encrypted doesn't mean you cannot modify it. And so it's very tough to guarantee that someone did exactly what they were supposed to do. I mean, you can use something like zero knowledge proofs, but what is a zero knowledge proof of a biological experiment, right? I don't know what that would look like. What are ZK proofs of physical world things you do? I don't know. Maybe you put some camera and recordings, and then you create some temper proof way to verify what has been done or something. Maybe it's AI. Maybe you create an AI that supervises every experiment and automatically flags anything that goes wrong publicly or something, when we think about Xamar.
00:23:17.686 - 00:23:43.874, Speaker B: So obviously medicine is like a big part of this, of why you started it and the effects. It can have everything. How do you plan to partner with hospitals so you can actually start this on ramping and building these kind of solutions for, let's say, diseases, right? So people can actually train shit. And all this data in different hospitals so it's not gatekeeped. Do you have any plan for that.
00:23:43.912 - 00:24:11.834, Speaker A: In the pipeline right now? Most of our customers and partners are blockchain companies, because when you think about it on a blockchain, you really need homomorphic encryption if you want to do anything confidentially. In web two, the issue is that people still trust the service providers. Not good to get hacked. I think they're wrong, but they can always say, hey, don't worry, the data is secret. I'm not getting hacked on a blockchain. You can't really do that. It's public.
00:24:11.834 - 00:25:01.466, Speaker A: It's public, right? So you need to encrypt it. So the way that we are approaching that in web two is we're trying to find use cases that are just not possible without morphic encryption. For example, if you want to do cross border collaboration on medical research, it's very complicated to have data go from one country to another, even for legitimate purposes. If the data is encrypted, though, it doesn't matter as much, does it? Because, hey, you can send it to any foreign country, who cares? They cannot decrypt it without a key anyway. So it opens up really this kind of like, cross border collaboration opportunities, which I think for research specifically is extremely important. Like, imagine if you could train like, a medical AI model, not based on a single country's population, but based on the entire world's population. That would be amazing, wouldn't it?
00:25:01.488 - 00:25:22.514, Speaker B: I think it'll be insanely great for society, except I can imagine some countries just don't want to participate. Doesn't matter, though, everyone else, it's better than just having a single country. Even, like, the country is not united either, because all the different medical hospitals are like, they're all gatekeeped for their own private company or whatever it is.
00:25:22.552 - 00:25:29.286, Speaker A: But you can still do that, right? Just because data is encrypted doesn't mean you have to share it. It just gives you more options.
00:25:29.468 - 00:25:30.998, Speaker B: It does. Interesting.
00:25:31.084 - 00:26:03.610, Speaker A: It's kind of like the Internet. You could argue that some countries want to run their own local Internet and not connect to the global Internet, but it's better to connect to the global Internet and maybe limit the use locally according to whatever you want in your country, because at least you still have the option to open up if you wanted to. France did try to have its own Internet, by the way, called the we. It was before the actual World Wide Web. It was kind of a success in France, but eventually people wanted to access content globally, not just in France.
00:26:03.690 - 00:26:12.398, Speaker B: So that died off. Things very limited if you're just doing a single place. How did you actually start developing this as well? The homomorphic encryption?
00:26:12.494 - 00:26:18.598, Speaker A: It started in my previous company in 2016, when I was building my previous AI company.
00:26:18.764 - 00:26:22.070, Speaker B: We were doing similar to this, like, privacy wise.
00:26:22.810 - 00:27:07.086, Speaker A: So it was similar in the sense that we were building an AI assistant with a very strong focus on privacy and the way that we were doing it back then. So the company was called snps. The way we were doing it back then is that we were running the AI model on device directly so that the data would never leave your device. If you, for example, wanted to control your tv by voice, your tv would do the processing, and so you wouldn't have to worry about data privacy. By the way, that company got acquired by Sonos. So if you're using a Sonos speaker and using the voice control in it, that's our technology, and it's running locally, so it's private. But anyways, so I was thinking about ways where we could offer privacy and still do the processing on the server so that we could have one big model for everybody using it and we could improve it and so on and so forth.
00:27:07.086 - 00:27:31.670, Speaker A: So kind of like what everybody's doing now, right? And that's when I came across homomorphic encryption. And I remember the first time I heard about it in 2016. I was like, damn, this has to be the future. It makes so much sense, but you couldn't really do much with it back then. You could only do a few things. It was, like, very experimental. We did manage, however, to build a privacy preserving analytics engine for all of our applications.
00:27:31.670 - 00:27:38.314, Speaker A: We could basically tell how people were using our products without knowing anything about individual people, like sensitive data sets.
00:27:38.362 - 00:27:40.810, Speaker B: How are you doing that? Like medical documents and whatnot.
00:27:40.890 - 00:28:06.534, Speaker A: With homomorphic encryption, there's a whole point, right? And at the time, at know this was a use case that was possible with fhe, but AI definitely wasn't. But for me, I always kept it. On the back of my mind, I was like, man, I got to do something in that space at some point. This is important. We got to make this work. And that's when I met my current co founder, Pascal Paelle, at Zama, who's one of the inventors of homomorphic encryption. He's like, og og og guy in that space.
00:28:06.534 - 00:28:40.082, Speaker A: And him and I were friends, and so when he heard that I was selling my company, he called me. He's like, hey, I just had a breakthrough in homomorphic encryption. It might be a good time to start a company doing it. Do you want to talk about it? And I was like, I don't, you know, I'm selling my company. I'm toast. Maybe I'm just going to travel the world, like, live in Japan for six months or know. But then I thought, how often you get a chance to work on such a groundbreaking technology with the perfect co founder who just had a breakthrough that makes this thing work for the first time? I'm like, I can't pass on that.
00:28:40.082 - 00:28:46.670, Speaker A: This is too week. Just a week after I finally sold my company, we started Zama.
00:28:46.750 - 00:28:49.262, Speaker B: Oh, wow. Straight on love about it.
00:28:49.416 - 00:28:58.374, Speaker A: Yeah, I went straight for it, man. I was like, to hell with resting and sleeping. Back to work.
00:28:58.572 - 00:29:03.690, Speaker B: I think it's the best option, though, man. You got to keep the ball rolling, for real.
00:29:03.840 - 00:29:08.282, Speaker A: I wouldn't advise anyone doing that, to be honest. I think it's good to take a break from time to time.
00:29:08.336 - 00:29:13.450, Speaker B: But the opportunity, if you see the opportunity and it's in your grasp, you got to take it.
00:29:13.520 - 00:29:36.146, Speaker A: I was sitting in Pascal's office, and he was, like, explaining his breakthrough to me on the whiteboard, and I was like, oh, my. Like, this changes everything. How can you not get excited if you genuinely care about a problem and someone's literally handing you the solution, telling you, can you be my co founder in this company that I want to build to solve that problem once and for all? You can't really say no, can you?
00:29:36.248 - 00:29:40.166, Speaker B: Yeah, 100%. Especially for something that big as well.
00:29:40.348 - 00:29:40.790, Speaker A: Exactly.
00:29:40.860 - 00:29:43.270, Speaker B: How are you just going to turn that down? You're going to regret it as well.
00:29:43.340 - 00:29:44.054, Speaker A: Exactly.
00:29:44.252 - 00:29:56.474, Speaker B: Interesting. Damn. I think a lot of people, it's like the dream for startup founders to sell their company. So what was that kind of process like?
00:29:56.672 - 00:30:19.602, Speaker A: It was hard. It was the hardest thing I've ever done, and that's considering that Sonos, who acquired us, was an amazing acquirer. They were so nice, so friendly. They were not trying to trick us or trying to. So it went really well, as well as it could have gone. It's just when you're selling your company, it's such a stressful moment. You have so many stakeholders you're trying to manage.
00:30:19.602 - 00:30:41.526, Speaker A: There is the buyer, there is your investors, you have your co founders, your employees. Everybody has to be happy. How do I split the money? Who? Guess what? You have so many decisions. At the same time. The company who's buying you is going to look into every single line of everything in your company because it becomes their responsibility. So this is not like an investor is just doing some due diligence. We're talking about.
00:30:41.526 - 00:30:54.510, Speaker A: They're taking legal and fiscal responsibility by buying you. And so they want to make sure that there is nothing they're not aware of. It was exhausting, I have to be honest. It was the hardest thing I've ever done. Way harder than building the company itself.
00:30:54.660 - 00:30:57.578, Speaker B: Oh, God. I guess it's like one giant audit.
00:30:57.674 - 00:31:14.862, Speaker A: Yeah, pretty much. Pretty much. But, hey, it was worth it. Our technology ended up making it into tens of millions of products. Half of the team who got acquired four years ago stayed. Which means that they're happy. For me, as a founder, I'm very proud because I've built something that endured.
00:31:14.862 - 00:31:18.898, Speaker A: Right. It wasn't just acquired and then disappeared like it actually served a purpose.
00:31:18.994 - 00:31:36.986, Speaker B: It's a terrific thing as well. It's being used, and it's one of probably the hardest things that you could do, getting a company acquired. Since you've done that and now you're doing XaMl, what have you learned from the last one that you're now applying to the current one? Are you doing anything differently or everything different?
00:31:37.168 - 00:31:43.614, Speaker A: I was even wondering, how do people even start companies the first time? And then I realized, well, you got to start first time to do it a second time.
00:31:43.652 - 00:31:43.854, Speaker B: Right.
00:31:43.892 - 00:32:19.558, Speaker A: So the biggest takeaway is that I used to spend a lot of times optimizing things that didn't make a difference to the end game of the company, of our actual goals. I would spend a lot of time trying to think about what is the right way to compensate people. How do we do stock options? Right. How do you promote people? Right. How do you organize people? Right. Who becomes a manager? How do we do management? It's so much effort to eventually end up concluding that other people thought about this before I did, right. And that their conclusions are probably applicable to me as well, and I should just do whatever they're doing, which, in short, means look at the best technology companies and do whatever they're doing.
00:32:19.558 - 00:32:52.930, Speaker A: Don't reinvent the wheel. There is no point in doing that. So first of all, if you do that, you're removing a lot of brain, sort of like challenges and problems trying to solve. Second thing is, I would spend a lot of time on trying to prove things before we actually had people using them. In other words, I was trying to prove that we were right in theory without trying to prove that we were right in practice. And that's bad because you spend six months, whatever, nine months building a feature, you launch it, nobody gives a shit. And you just wasted nine months.
00:32:52.930 - 00:33:37.602, Speaker A: So, of course, some things do take time. For example, homomorphic encryption is not an overnight thing. Like, it takes time to do proper science, but it doesn't mean that you cannot release things as you go. So for Zama, we're like, okay, it's probably going to take us three to five years for this technology to be widely applicable, but we can still already put out products that will be maybe only usable for a few people, maybe only practical in a few use cases, but are still going to be things that people are going to use. And so that's we did from day one as Zama. Every quarter, every three months, like clockwork, there is a product update, there's something that comes out, there is an improvement, there is a new feature. And every time we look at it and we see what people use, and we continue on what works and we kill what doesn't work.
00:33:37.602 - 00:33:44.910, Speaker A: I'm very comfortable now basically just shutting down a project because it's just part of the game. Everything's an experiment.
00:33:44.990 - 00:33:49.542, Speaker B: Yeah, you got to just keep on experimenting. And that's the way, that's the best way to learn.
00:33:49.676 - 00:34:17.566, Speaker A: The third thing I really learned also is how to raise money. In the beginning, everybody has this kind of image that raising money is. I'm going to raise like $100 million, and people are just going to trust me because I'm Steve Jobs and whatever. In practice, it doesn't work like that. When you raise money, you have to understand the way that investors are thinking. You have to understand that they have their own investors that have their own thesis that it's a partnership. It's not just someone blindly giving you money thinking, I don't care what you're going to do with it.
00:34:17.566 - 00:34:35.794, Speaker A: It doesn't work like that. And when you understand that, fundraising becomes a lot less frustrating, right. It kind of becomes like sales in a way. I've got a product, you buy it. You don't get bothered or frustrated when a customer doesn't buy your product. It's just part of the process. So why would you get frustrated and bothered when an investor says, no, you shouldn't? It's part of the process.
00:34:35.794 - 00:34:54.246, Speaker A: So all of that really helps. And I think in hindsight, it's incredible that we got to where we got in my previous company, given all the mistakes that I've made as a CEO and founder. Zama, despite being orders of managers, more complicated to build, actually feels much easier for me personally.
00:34:54.358 - 00:34:56.090, Speaker B: Right. Why is that?
00:34:56.240 - 00:35:10.570, Speaker A: I don't know, man. It's just, like, already done the rodeo once. The rodeo once, right. So when someone comes to you and says, oh, this doesn't work, you're not panicking. You're like, yeah, okay. Just, hey, Spartan, the game, right. It's like every problem becomes a challenge.
00:35:10.570 - 00:35:20.798, Speaker A: It becomes exciting. You become immune to short term variations in what's happening in your company. You're more focused on long term vision.
00:35:20.894 - 00:35:37.794, Speaker B: Yeah. You mentioned you did a lot of mistakes in the last one. What were some of the major ones that you did that you wish that probably would have helped you accelerate if you didn't do them? New founders or people that haven't been acquired or anything or still going, something they can avoid?
00:35:37.922 - 00:36:11.138, Speaker A: You know, I think it's going to sound really boring, but we just didn't have good day to day management processes in place, so we were, like, a bunch of very clever people doing a bunch of really cool stuff, but it was all over the place, and as a result, we just wasted a lot of resources and mental goodwill on things that just didn't matter. Right. Instead of pushing in one common direction and vision together. So I think that was probably my biggest mistake, is not recognizing early enough that people need structure and they need leadership in order to do something significant. You cannot just put people in a room and say, hey, do whatever you want. It just doesn't work like that.
00:36:11.224 - 00:36:13.378, Speaker B: Definitely. And that was, like, the major one.
00:36:13.464 - 00:36:15.586, Speaker A: Yeah, because everything else trickled from that.
00:36:15.688 - 00:36:29.986, Speaker B: Yeah, definitely. You need the routine and the management. You can be the smartest people in the world, but if there's no order, then it's kind of like a super team, let's say the NBA. There's no structure, but they will get beaten by a team that has structure that might be worse off in general.
00:36:30.108 - 00:36:40.982, Speaker A: Exactly. And, you know, the most experienced and best people in all companies I've worked at, they know that the more senior and accomplished someone is, the more they will ask for management.
00:36:41.046 - 00:36:41.386, Speaker B: Makes sense.
00:36:41.408 - 00:36:53.950, Speaker A: Sounds counterintuitive because it's kind of a cool thing when you're like 25. You're like, oh, yeah, management is bad. Everybody's equal. Let's be flat, and everybody decides on everything. Let's just vote. Just doesn't work in a company, man. It doesn't work in a company.
00:36:53.950 - 00:37:21.690, Speaker A: Like, a company, whether you like it or not, is going to be structured with people whose job it is to lead other people, and people's job it is to actually get shit done in the best possible way. And it's fine. Being a manager doesn't make you better than being a developer. There are just different tracks that you can follow in your life. In fact, actually, at Zama, there is no difference in pay. If you're a manager with ten year experience or a developer with ten year experience, you get paid the same, because what matters isn't what you do is how good you are at what you do.
00:37:21.840 - 00:37:47.934, Speaker B: Interesting. I do want to kind of switch the conversation to something else. You've done AI for quite some time. Where do you see it all going? Obviously, this is a giant topic of conversation, and people have vastly different ideas of what's going to happen in the future and when it's going to happen. It's all speculation. We could run a stock and it could be short and long, like a crypto market, very volatile. Where do you see it all going? You've done the genetic stuff, you've done the AI.
00:37:47.934 - 00:37:57.310, Speaker B: Now you're doing piracy. It's all up in the air. And people with chat GBT that came out a couple of years ago, that really made the world think about it. It seems like a very hot topic.
00:37:57.390 - 00:38:44.722, Speaker A: Yeah, it's not new. So obviously I can't really predict what could happen with some hypothetical future technology that we haven't invented yet. I mean, we can have fun doing that, but let's stick to what we know. What we know today are various AI technologies, including large language models and transformers and these things. So my views on that haven't changed in the past 20 years, right, since I've been in AI. In fact, in 2017, the french government asked me to think about the impact of AI on society and what it would mean, and I looked at what I told them back then, recently, and I was like, let's see, five years later, actually, seven years later, what held up and what was wrong and what I said back then. I believe that today is that AI will not be more intelligent than humans, but it can help humans do more things at the same time.
00:38:44.722 - 00:38:48.818, Speaker A: It's a way to increase throughput, not depth of intelligence.
00:38:48.914 - 00:38:57.730, Speaker B: Do you think that's because the algorithms aren't actually up to that level yet. Why do you think it won't actually go beyond human intelligence?
00:38:57.810 - 00:39:25.758, Speaker A: Because I think, first of all, I think humans don't want to give away control. We want to have control. So I think we will design systems that we can control. It's just in our nature to do that. We just like doing that. But putting that aside, when you look at the way that current AI models are learning, they're learning from human data to replicate human intelligence. Because of that, they're also picking up on the limitations and biases and cultural quirks of humans.
00:39:25.758 - 00:39:56.986, Speaker A: Example, there are a few papers that show that if you include emotional cues in your prompt to an LLM, you will get better answers. What that means is AI is susceptible to emotional things, right? If you ask it, write me this essay. It'll do an okay job if you ask it. Can you please write me this essay? This is really important. If I don't give a good essay, I'm going to fail my studies and my career is going to be over. On average, that essay will be 30% better graded, 30% better.
00:39:57.088 - 00:39:57.786, Speaker B: Interesting.
00:39:57.968 - 00:40:27.214, Speaker A: If you bully it, it'll be 30% better. So you literally have to bully or beg the AI to do something good. How does that not sound human? But hold on. Some experiments, it's a little bit anecdotal, right? And it changes, but still it's there. Some experiments are showing that AI is actually lazy. There are some stuff it just doesn't want to do. There are days of the week, on weekends, for example, that it doesn't want to actually give you as good answers as during the other days.
00:40:27.214 - 00:41:01.594, Speaker A: It literally has a weekend concept, right? Which is insane, dude. If we're picking up on those human traits, how can we assume we're going to be more intelligent? No, what I think is going to happen, we're going to replicate human intelligence. I believe that. Right? At least to some extent. But we're not going to create super intelligence. But replicating human intelligence is already amazing because it means that you, Digatchi and I, Rand, can replicate our brain a thousand times, and we can do a thousand things at the same time. That's what I meant by saying that we're increasing throughput of intelligence, not depth of intelligence.
00:41:01.722 - 00:41:21.826, Speaker B: Right. Interesting. Seems like we're training off human data and not letting it learn by itself empirically. Like going out into the world and messing around. I've been kind of experimenting with it, actually, and trying to get it to do that because I do see these limitations as well. To get super intelligence, it needs to do stuff by itself. But the hardest thing, actually is the goal.
00:41:21.826 - 00:41:48.350, Speaker B: If you're trying to make it completely autonomous, what is the goal? There is no reproduction needed because it just doesn't die like a human. It doesn't need to pass on genes to the next generation to be well adapted and survive. It just permanently is alive on hardware. I guess you could say reproduction is like a good goal, and then it would find ways then to find vulnerabilities and exploit different products. So we can go onto a cloud and spread through that.
00:41:48.420 - 00:42:00.802, Speaker A: Yeah, I mean, the paperclip theory, right? There are so many layers here that are missing today for this to actually happen. Could it happen someday? Maybe. Can it happen now? With current technology, definitely not.
00:42:00.936 - 00:42:07.714, Speaker B: Yeah, I think there has to be a completely new architecture for this to be done. There's no way current ones could do that.
00:42:07.832 - 00:42:29.994, Speaker A: It might not even be worth it. So I'm a much bigger believer that the biggest risk we have right now isn't super intelligence. It's actually ready what's happening now with deep fakes. I've been saying this since 2015. In 2015, I started warning people. I was like, guys, don't worry about AI killing. You worry about AI destroying the concept of truth in the world.
00:42:29.994 - 00:42:56.690, Speaker A: Because if you can no longer trust anything you see here, if you don't know, if you're talking to your mom, to some scammer impersonating your mom on FaceTime, you just go nuts, man. You become paranoid. Like, you end up just melting the fabric of society, because it doesn't matter what people think with trustless stuff. A society is built on trust. Trust that you're not going to get killed every time you do something. AIS is really threatening that. It's really threatening that.
00:42:56.690 - 00:43:24.302, Speaker A: I read the news yesterday. There was this guy in Asia in a financial company. He ended up making a wire for $25 million to a scammer because the scammer called him on Zoom with a deep fake of his boss. So, like, the dude, the worker, had a Zoom call with his boss telling him, do this 25 million wire. So he's like, okay, well, I literally just talked to my boss, right? I saw him. I saw my boss on Zoom. I heard his voice, so I did the wire as I was told.
00:43:24.302 - 00:43:35.246, Speaker A: It was a fake, man. It was a fake. This is the real risk we have to fight against, right? Like, this is the big problem that is looming in our face. Nothing is fucking real anymore.
00:43:35.278 - 00:44:01.994, Speaker B: Now, I was actually thinking about this with let's say like TikTok or any of these social media doom scroll apps, there won't be a need for human data anymore because then you can just generate of what you already have and it try and tunes for what you're mostly scrolling into, right? I imagine it will just generate and just keep you hooked from just generation and the next generations are so fucked because of that.
00:44:02.112 - 00:44:44.790, Speaker A: Unless some of those will not give in to those kind of things, right? And those will be so much ahead. Think about what that means for creativity. If you only see content tailored to your existing tastes, it's going to reinforce your current beliefs, but it's not going to give you new things to think about, right? It's kind of like you remember when we're kids were playing with this playdoh. If I have yellow play doh and blue play doh and red play doh, I can mix up and make a bunch of colors. If only have red play doh, I can only do red stuff. So what's better in terms of diversity? Well, to have multiple colors you're looking at. And so you really don't want an AI content that's auto generated for what you like.
00:44:44.790 - 00:44:59.990, Speaker A: What you want is AI content that makes you discover things you might like that you don't know about. And that's equally possible to build, well, less dopamine inducing and less addictive. So not as good if your main business is eyeballs.
00:45:00.070 - 00:45:07.322, Speaker B: This is true. But yeah, it could be a very powerful thing if you decide to go down that route. Unfortunately, getting someone addicted.
00:45:07.466 - 00:45:22.130, Speaker A: I think people who are using AI to learn new things and expand the horizon of what they know are going to be light years ahead than people who are using AI to only look at what they already are looking at.
00:45:22.200 - 00:45:27.474, Speaker B: How do you see that playing out, though? How would you make AI help you become more creative?
00:45:27.602 - 00:45:52.362, Speaker A: Well, first of all, I would literally forbid those kind of like closed loop AI generation products, period. I've been thinking a lot about that. I see no benefit whatsoever in the short format like reels. I just don't see the purpose. There is nothing good that can come out of it. Nothing. We should just think about how different formats enable different type of usage and what that means for people.
00:45:52.362 - 00:46:15.678, Speaker A: And we should just prevent some formats from existing. Not everything is acceptable. You get a lot more. Let's just take this podcast an hour long. You learn a lot more from an hour long podcast you're listening to than you will learn from like a five second short on blockchain on TikTok because we're talking about a bunch of different things. We're making connections between different things. There is diversity in the Topics we're covering.
00:46:15.678 - 00:46:28.966, Speaker A: If you can push people back towards longer form content, that's already a very big win. And the second thing is, yeah, you should just promote diversity and content in a way that's tough, right? Because it's not as good for money. That's a problem though.
00:46:29.068 - 00:46:57.022, Speaker B: Yeah. It's not as glamorous, and doing other hard things is not. A lot of people are doing it, obviously. It's so much easier to tap into the amygdala and just go instantaneous gratification rather than waiting for the prefrontal cortex to be like, you know what? I should actually invest my time into listening to this for an hour. Well, I'm super keen to see where Zama goes, man. This was a really great episode. Very diverse, touched a lot of topics.
00:46:57.022 - 00:46:59.730, Speaker B: It's been a great chat. I really appreciate you coming on.
00:46:59.800 - 00:47:01.538, Speaker A: My pleasure. Enjoyed it as well.
00:47:01.624 - 00:47:04.674, Speaker B: And for anyone interested in Zama, where can they go?
00:47:04.792 - 00:47:16.486, Speaker A: You can follow us on X. Zama. Underscore fhe. You can follow me as well. My dms are always open. Rand Hindi. R-A-N-D-H-I-N-D-I on X.
00:47:16.486 - 00:47:31.006, Speaker A: Get in touch. Hang know Zama. Everything is open source, so we love when people just hack things around with our technology. We have a grant program, we have a bounty program. Anything you feel might be interesting to build with homomorphic encryption, we'd love to see.
00:47:31.108 - 00:47:41.706, Speaker B: Sure. And thank you so much for jumping on again and for anyone listening. Go check that out. It's very interesting technology and hopefully it changes everything. That's the dream. But until then, we'll see you next episode.
00:47:41.818 - 00:47:42.990, Speaker A: Thank you. Have a good one.
00:47:43.060 - 00:47:43.866, Speaker B: You too. Bye.
