00:00:00.160 - 00:00:26.885, Speaker A: Thank you so much for coming here for the hack talk. During the Technical AI Safety Startups hackathon, we have the pleasure of inviting Lucas here. Lucas has been in the recent YC cohort and received a bunch of funding for the startup Vector View that has worked on evaluations historically. And he's here today to also chat a bit about his experience and what it's like really building AI safety in YC in this case. So take it away Lukas, and excited to hear what you have to say.
00:00:28.115 - 00:01:28.917, Speaker B: Thank you. Yeah, like Espen said in the, in the intro I will talk about AI safety as a startup and my experiences in yc. So to start off more generally I think that AI safety is a problem and in general people pay to solve their problems. This should allow you to make a startup around this. My personal view is that like there is a non zero chance that we will or like probability that we will have some very big problems with the default option or like the default outcome of AI safety or like of the AI progress. And even though the probabilities might be very low since the stakes are so enormous, this is something we need to work on. And I think also because of this view is becoming kind of mainstream because a lot of like regulations are happening and a bunch of like high profile researchers are moving or like leaving their companies because of AI safety.
00:01:28.917 - 00:02:39.005, Speaker B: I think this is like a good time to start a company around it. But why should you solve these problems with as a startup? Why not like as a researcher or some other setup? I think like one of the main problems with AI safety as a startup or like it's not a problem but like it's that the problems are not very dire at the moment. So people assume that this is not something I can work on because no one will pay me at the moment. But this is what this exist to allow. They allow you to take big bets on things that will be problematic in the future. If you can convince them that no one will buy this now, but they will buy them for a lot of money in two or one year, then they will pay you to develop this technology before even someone pays for it. Another point here is that I think builders and entrepreneurs are very underrepresented in the AI safety community and this should make the available options of things to work on much bigger.
00:02:39.005 - 00:03:30.835, Speaker B: And if you are someone that is a good builder and could go into AI safety but you think like it's more tempting to go to a big AI lab and work there, I think one thing you should ask yourself is like what is the counterfactual impact here? If you don't go to an AI lab, then someone else will do that work and the total impact on the world might be less. But if you do a startup, that wouldn't have happened if you didn't do it. And I think the best people, they do stuff that wouldn't happen if they didn't do it. So you're motivated to do a startup with AI safety. How do you do it? Well, there's many ways. I took the path of Bicombinator yc, which is like a startup accelerator. There's many others that are basically similar.
00:03:30.835 - 00:04:17.225, Speaker B: What they all are, they are a bunch of events, they are advice, they're a bunch of money and you get a great network, you don't need any of this. But it's a great starting point. And for me, in my experience, I got there and I met some incredible, incredible people. The talent density of the cohort is unmatched in my experience. And it was very, very, very inspiring to meet so many people working on super ambitious ideas and realizing that these people are incredible. They are very smart, they're working on incredible things. But I am too and like, I could like do more ambitious things because I see that they, they are so in that way it kind of changed my life.
00:04:17.225 - 00:05:00.225, Speaker B: So yeah, YC is great, but it's not necessary. There's other paths as well. YC is famous for some very basic advice that people still, they feel like very basic and kind of dumb, but people fail on these points over and over again. So they have become very famous because they are very useful still. And this is, for example, make something that people want. It sounds super basic, but the number one problem with startups in general is that they have an idea in their head of something that they think would be cool, but it turns out that no one wants it after they have built it. And that's so yeah, make something that people want.
00:05:00.225 - 00:05:49.343, Speaker B: You should launch early and iterate if your product allows for that. But like in general, if you launch and you are not ashamed of what you're launching, then you should probably have launched earlier. Another piece of advice is do things that don't scale. It sounds kind of counterintuitive, but basically doing things that don't scale is like don't waste time on processes that will change in the future. And also by doing these processes manually, you will learn so much about the processes that when you do finally build them, you will make them much, much more robust. You should talk to your users. It sounds obvious, but People talk to their users in the beginning, but then they stop.
00:05:49.343 - 00:06:51.469, Speaker B: And you should not stop. You should continuously talk to your users and make sure that they like what you're building. It's better to have on the same note, it's better to have 100 passionate users than 1 million average users, because the 100 passionate users will give you feedback and the million users want so then they will give you feedback to build a better product which you can scale up in the end to something that is very big that a million people are passionate about. And finally, you should love your customer because hopefully you will spend a lot of time with them. You will need to put yourself in their shoes and relate to them, otherwise you will probably fail at building what they value. So those are like some famous advice that YC gives and now I want to talk to them in the context of AI safety. So in AI safety many of the problems are far in the future.
00:06:51.469 - 00:07:38.865, Speaker B: No one will pay for them at the moment. There are some, of course at the moment, like misinformation is already starting to become a problem with AI on the Internet. But most problems like GPT4 is not capable of killing us all at the moment. So in the future there's sub problems of this like big existential problem of AI safety that will be very, very important. And like I said, venture capital exists to do this. But this like one big problem with this is that you can't really follow the YC advice on like iterate on user feedback. And how do I know that I made something people like? Because like I made it and it's like one year until someone actually wants it.
00:07:38.865 - 00:08:36.675, Speaker B: So that is kind of hard. But yeah, I don't have a great answer here, but you have to be aware that that's a problem in many of the AI safety startup ideas that people have today. It's also like because this problem appear in the future, the market is very uncertain. And that means that you definitely should not do things that scale because these processes that you spend so much time building now will probably change because the market is so unnecessary. So do things that don't. Scale is even more applicable in AI safety ideas also in general in AI safety ideas. Take this with the note that this might not apply to all AI safety ideas, but in general the ones that I have seen have smaller customer pools.
00:08:36.675 - 00:09:20.005, Speaker B: So for example, at my startup we do evaluations for the big AI labs. That's a customer pool of five or maybe ten. It's not huge. This is not fine, you can tell yourself, but okay, this is fine because we have 100, or in my case, 10 passionate users, which is like better than 1 million average users. But this is not really what this advice is saying, because this advice tells you that you need to start at 100 passionate users and then you can scale that up to a million users. So this is still a problem, that the small customer pool. The small customer pool is still a problem.
00:09:20.005 - 00:09:54.703, Speaker B: But what this means is that you need to charge more. And this is another advice by YC that generally you should charge way more than you think that you should charge. And this could be very, very not comfortable to do. But in general, if you're solving someone's problem, if you charge, if you like say something that is way out of their budget, they will not say, okay, then I don't want this, and then leave. They will say, okay, I want this. So how about. And then you have the negotiation.
00:09:54.703 - 00:10:47.695, Speaker B: So I don't think you should fear charging too much or at least trying to charge too much. And lastly, love your customers. In my case, this is pretty easy because like we're working with the researcher at the AI labs and I think those are the kind of people that I like hanging around with. But in general you should like think about like, okay, this idea, who is the customer here is IT Researchers, government, enterprise, consumer. And then think about like, would I want to like spend a lot of time with these people? Would I be able to relate to them in a way that I can actually solve their problems? And this applies generally, but also with AI safety. One big part of doing a startup is finding clients. And this can be very hard, but it's maybe the first hard thing that you do.
00:10:47.695 - 00:11:15.001, Speaker B: So some ways of doing that is like warm intros, going to events or cold emailing. I think warm intros is the best one. And like to engage in like networks of other founders. The YC event, the YC network is very strong. So I can recommend applying to YC if you, if you are interested. And the AI safety community is very like altruistic. So they also generally people in these communities want to help you.
00:11:15.001 - 00:12:03.085, Speaker B: So if they know someone that might be a good client for you, then they can like interview you. So they say like, hi. They wrote an email and said like, hi, you should talk to this person, it's my friend, blah, blah, and then you can meet up the conversion of those kind of emails. Warm intros is much higher than cold emailing. For example, many AI safety startup ideas are high effort, low volume, like I said, because the customer pool is often Quite small. You have to make sure like you don't have a million potential customers. So then you have to make a high effort of converting the small amount of people that do exist.
00:12:03.085 - 00:12:46.555, Speaker B: And events are a great example. Like it takes a lot of effort and money to go to some conference or something like that. But it's worth it if you can actually start to chat with people and, and becoming friends with them because then they might buy from you. And lastly like if none of the above works, cold emailing is not fun, but it's. You have to do it. And here I would like caution you to first identify if your product is like a high volume, low conversion or a low volume high conversion type of situation. Because the emails that you write are very different in these scenarios how much like personalization you should do and stuff like this.
00:12:46.555 - 00:13:25.115, Speaker B: I would, I don't want to go through every single point and like guide you to how you how to write your emails because there's like a thousand guides out there. So read them before you go out and spam people. And it's okay to be annoying. That's the only one I want to ask like or like add it feels like when they haven't answered you and you have sent them like three emails, then okay, I should stop. But like generally I would still ping them until they tell you to stop. And when they tell you to stop then yes you should stop. But before that it feel free to continue pinging them.
00:13:25.115 - 00:14:28.829, Speaker B: So I want to end on some discussion on like doing good. Often people that have AI safety startup ideas, they come from a very altruistic perspective and want to do like impact in the world, not only make a bunch of money. And I've had some thought about this. For example, I think impact is like not only the intentions that you have but also like how big the magnitude of your, of your like projects that you do in the world. So impact is like magnitude times direction. And I think in general a lot of people have find it easier to do one of these and then they only focus on one. So they, some people might like disregard completely the direction and only do like some startup that makes a lot of money and then they disregard the direction and then they didn't have that much impact in the world.
00:14:28.829 - 00:15:22.137, Speaker B: And then there's other people who like I don't really know how to make something super high impact where the magnitude is high. But I know that this research direction for example is very pure and there's no way of doing anything bad here. And I would just ask yourself if you're taking the easy way out, like focusing on one because you find that easier. And maybe focusing on both might be something that you should do to have more impact in the world. And one maybe hot take here is that I think more people should focus on the magnitude direction. This is maybe a bit controversial, but I think Elon Musk still is having a direction that is net positive and his magnitude is enormous. So in that way he's very, very good for the world.
00:15:22.137 - 00:16:20.563, Speaker B: I also think there's like a conversation to be had if Microsoft or the Gates foundation is what has had the most positive impact to the world. Obviously the direction is much more pure and good for the Gates foundation. But the magnitude could be argued that it's so much bigger at Microsoft that actually Microsoft have been more good for the world than the Gales Foundation. I'm not claiming that this is true. I'm just like this is things like examples of things that could be true and if they are, they are an interesting thought experiment on how you should like go about doing your impact on the world. And the final one is like altruism did not bring phones to poor countries. Like people that wanted to maximize profit for, for the, for the rich people did in a way.
00:16:20.563 - 00:17:05.003, Speaker B: But then just like that trickled down. So I think like yeah, and sometimes when you do startups it can feel like I'm just like I sold out. I wanted to do AI safety and I wanted to like solve. I'm an effective altruism and I wanted to solve this like very big problem. And now I'm doing like a startup and we're making money and I've sold out. But I think you should look at people like this and say that they still had a positive impact on the world and even though their direction wasn't as clear cut positive. I think if you can have a big magnitude you have a great impact on the world still as long as your direction is at least positive.
00:17:05.003 - 00:17:38.485, Speaker B: But that is big big if there are some people that have a negative direction and then having big magnitude is just bad. So. But yeah, that's something I thought about at least. And then lastly non profit versus for profit. So startups are generally for profits and but a lot of people working in this space are non profits. And what I, from what I've seen there's like cases for both for nonprofits. I think it's way easier to stay on a good path.
00:17:38.485 - 00:18:47.101, Speaker B: You have funding from people that gave you money to do something purely good and there's no market pressures, nothing like that and it's also very or like much easier to attract altruistic talent. So people that have the interest that you have and want to like do good for the world often want to work at nonprofit. So yeah, the caution is that maybe I'm not an expert here, but I think generally it's harder to get like like a lot of funding if your project requires that. But it's not like necessarily true that your project does require that. But I think the funding might have like a much lower ceiling in the non profit case. And in the for profit case I think the potential for a big magnitude is, is much greater. It's easier to attract like non safety pilled people and they can still even though they are like not their heart maybe is not with AI safety, if you give them tasks that are aligned with AI safety, they have still contributed.
00:18:47.101 - 00:19:30.271, Speaker B: And I think if you have a lot of money you can hire a bunch of people like this who wouldn't do safety work otherwise. And with for profit you always have the backup of earn to give. If you start a startup and you have the good intentions, your direction is positive. But then the market pressures and investors and a bunch of other pressures put you off track. You should try to avoid that. But if it comes to that, at least you made a lot of money which you can give to charity. And but the caution here, like I said, like you should be honest with investors but then don't give them too much influence.
00:19:30.271 - 00:19:59.909, Speaker B: So be honest with your intentions that I am doing this partly because of positive impact to the world. And then when you have been honest with them, then they can't complain when you maybe do some choices that are not directly in the short term maximizing profit and instead doing something good for the world. So I think that was all of my talk. Thanks for listening. Nice.
00:19:59.997 - 00:20:32.935, Speaker A: Amazing. Thank you so much for the talk Lukas. And I'm sure everyone here is super inspired now to go ahead and think a little bit about how their ideas can maybe make it into the real world as well. Also interesting to hear about Ysense. There are notes in the chat as well from everything Lucas has mentioned if you want to look through some of it. Otherwise I'm curious to hear if there are any questions from our audience here today. Otherwise I might also ask the first pieces of questions.
00:20:32.935 - 00:21:00.215, Speaker A: Of course I'm curious with Vector View specifically, how do you look at this? So you mentioned this for profit case for doing big magnitude problem solving and AI safety. How have you seen that pan out with Vector View and what's the specific sorts of. Yeah. What have been the biggest challenges? What are your current challenges in terms of making sure that it has the impact you want and doesn't turn into the back out plan in that sense?
00:21:00.375 - 00:21:56.649, Speaker B: Yeah, yeah. I think we're still early case, early stage and looking to find our footing. I think in terms of what are the biggest challenges, in the beginning it was like finding clients, but as you get one foot in the door, then people start to trust you more and more. So I think if you're in the stage where, oh, I'm finding no clients, I think don't be discouraged by this because once you get the foot in the door at one place, then that will trickle down to or like there's a snowball effect there. For us now, it's just like finding great people and because we have, I guess, more demand than what we can output. But this is also quite important point that you want to have your cultural in line with your values. And this is like especially hard.
00:21:56.649 - 00:22:24.975, Speaker B: It's like finding, like I said, builders. Like, my opinion where builders are underrepresented in AI safety comes from the fact that I've been trying to find people. We're looking for engineers, but it's like there's not that many. And I guess the ones that do exist, they get poached by the big labs with a much bigger salary. So I think that's one of the problems that we have been having.
00:22:25.525 - 00:22:53.385, Speaker A: Yeah, very interesting. I think it's also a big question for anthropic. Even though they can lure you in with all the money, their last round, as far as I understand, is a culture fit round. But they don't ask specifically about safety there, but exactly. Ask about what was the most altruistic action you took and what's the sort of like. Yeah, I'm not sure exactly what the question is, but it's something along the lines that are more generalizable than just AI safety. Focus.
00:22:53.385 - 00:22:59.165, Speaker A: Phil, do you want to jump up on the stage as well to ask your question?
00:23:09.545 - 00:23:11.977, Speaker C: Sure, yeah. Thanks. Can you hear me?
00:23:12.081 - 00:23:12.725, Speaker B: Yes.
00:23:13.865 - 00:23:38.215, Speaker C: First of all, thanks a lot for all the insights and tips. I wanted to ask about one particular point, namely the two things that don't scale. 1. So maybe can you give one concrete example to make it a bit more clear? It's still not 100% clear. What are things that actually don't scale? So is this meant in terms of product or in terms of business process? Yeah, maybe you can illustrate.
00:23:38.875 - 00:24:14.055, Speaker B: Yeah, I think it's all of the above is the answer And I think, like having one example. I think so. I think, okay, so I might be wrong on the very specifics here, but it doesn't really matter. But I think the example comes from rb and in the beginning they didn't have. This might be wrong, but it doesn't matter. The point still stands. In the beginning they didn't have a process to automatically add people.
00:24:14.055 - 00:25:00.385, Speaker B: Well, let me give you a more concrete example of my friend. My friend made a chatbot before the OpenAI API was available. And the way that he did that was that he had ChatGPT or the OpenAI API was available, but not the GPT 3.5. So a worse version was available as an API, which was not good enough. So he sat at home when he got like from his app, a user asked the question, he copy pasted that into chatgpt and then took the answer and like pasted it back to the chat. That is not scalable. If he gets like a million users, then this is not going to work.
00:25:00.385 - 00:25:59.443, Speaker B: But like in before the API was available, then this worked because he only had like 10 users and he learned so much about what his user was using the app for. I mean, there could be like privacy concerns here, but like, apart from that, like, the example highlights how you can like learn a lot from what your user are doing in your app. And also he didn't have to build the entire like pipeline for. I mean, what he could have done, I guess is to have like some scraper that like inputs it automatically in the website and do stuff like this. But he knew that API is coming in like two weeks or something, so he didn't do that because he knew he had to change. And I think in like, once you start to think about this, there's like a great Paul Graham essay about this if you want to know more about it. But it's like once you get this phrase in your mind, do things that don't scale.
00:25:59.443 - 00:26:16.335, Speaker B: There's so many instances where you sit and like, oh, now I'm doing things that scale. Should I really be doing this? And then, and then you ask yourself that question. Yeah, sometimes it is worth it, but like often when you're early stage, it's not worth it. And then, yeah, and then you stop yourself.
00:26:17.075 - 00:26:39.545, Speaker C: So is it also about timing? It sounded now a bit like, you know, at some point these things will scale, but you have to do them at the time when they don't already scale, you know, because then you actually get the experience first and you really understand what's going on. And then you are in the better position to scale when everyone else is starting to scale but they don't have this steep knowledge, you know.
00:26:40.125 - 00:26:42.825, Speaker B: Yeah, that is good. It's only in the beginning.
00:26:44.805 - 00:27:03.435, Speaker A: Yeah, I don't imagine that's scalable unless you are Amazon and can just hire enough Indian like annotation labor to make your shops work there or whatever they were doing. All right, thank you so much. Phil Ashish also has a question. I've invited you to the stage too.
00:27:08.135 - 00:28:08.295, Speaker D: Yes, hi. My question is related to particularly evals. Now that we see that there's a lot of sort of demand for evals and people willing to fund to generate good evaluations even from anthropic and you are in the same space lupus. So how do you see that panning out in the future in the sense that there's multiple players? Is there be the government, the regulatory authorities? There's a need for third party regulators. There's anthropic or other big players who are at the forefront of AI safety and then there's the startups like yours and others which are particularly focused on us. How do you see that ecosystem?
00:28:10.955 - 00:29:09.895, Speaker B: Yeah, I think the labs will always want their eval. They will want to have evals internally which they can run before they give something away to auditors. So I think there will always be like a demand for evals within the labs. I think there will be some kind of third party auditing like organization because I think they will be scared that like if we don't like self regulate here they. I'm not an expert on governance so like you should take my comments on this with a grain of salt. But like I think they will think that if they are forced to be regulated then, then that regulation will be much like tougher than if we like self regulate now. And if there's like a third party auditing organization that takes that role, I think that could work.
00:29:09.895 - 00:30:08.699, Speaker B: However we have seen like for now there's not been that many success stories where people try to do that. The labs usually have their like pre release things quite tightly for themselves governments. I think we saw that OpenAI now has some like just recently started to say that they will give their models pre release to the governments. So that might be like instead of this third party auditing external org that the governments take this role. I am not smart enough to understand the pros and cons of having it as a separate organization or as a government organization. But I think something like this will happen. Yeah, okay, thank you.
00:30:08.827 - 00:30:13.735, Speaker A: Thank you. Ashish. We also have Artie, who I have also invited to the stage.
00:30:15.875 - 00:30:18.859, Speaker E: Oh yeah, thank you. Am I audible?
00:30:19.027 - 00:30:20.375, Speaker B: Yeah, we can hear.
00:30:20.715 - 00:31:03.757, Speaker E: Oh yeah. Thank you. So first of all, I have two questions. First one is regarding your product. How many clients have been like willing and like very excited to hear that you're doing this eval thing and like what value proposition do you give them? Like if you eval this then you will get, gain this sort of value or profit or something. So that's the first one. And the second one is how do you think the market is for like data annotation thing? Because now we see that there are many companies doing, doing that like scale AI also data store AI, the other stuff.
00:31:03.757 - 00:31:22.755, Speaker E: But I think when we get to like agent, like AI agents, I think the data annotation problem would be so much higher and I think it opens up like space for other companies to come in. Yeah, I think that's for me. Thank you.
00:31:23.935 - 00:32:05.957, Speaker B: Yeah. On your first point, I would say that we are in this category of low volume, high effort sales. So we haven't talked to thousands of customers and try to sell them. We have been very selective, trying to only sell to the people that are building models. And the value proposition is basically you say that you care about safety so then pay for it or not really like that. But yeah, they, they, they want to. Yeah, like they, they, they would otherwise do this in house.
00:32:05.957 - 00:33:02.547, Speaker B: So like they, they will pay money for this regardless if it's like a third party or if it's, if they do it in house because they do have significant like safety researchers in house. So yeah, that's on that front on like I'm not an expert on data labeling, the data labeling market. I, so I wouldn't comment there but I think you're right that that is like a new avenue. And I think in general like post training, post training techniques will be very valuable secrets within these labs. Or it is already. If you for example, look at like the llama 3 paper now they have like one section where they're like we have these seven capabilities that we want to improve. It's for example like math and reasoning or like multilingual or some coding and capabilities like that.
00:33:02.547 - 00:33:39.355, Speaker B: And then for each they have some like hacky way of creating synthetic data or some kind of data set to post train for that specific capability. And I think, I think like the labs like anthropic and OpenAI, they probably have something like even more sophisticated than what the Llama 3 paper was talking about. And part of that is probably Something like you said that you need to label agentic data in some way, but I wouldn't know what exactly the problem there is.
00:33:41.215 - 00:34:01.245, Speaker A: Yeah, I guess I would imagine based on recent changes in the research space as well, that we'll see companies much more doing the data annotation in house because the data quality is such an important differentiator, it seems, compared to doing this at scale, for example, which scale AI have done. And other data annotation platforms.
00:34:01.825 - 00:34:02.685, Speaker B: Yep.
00:34:04.065 - 00:34:11.605, Speaker A: Nice. We'll take one last question from Ashish and thank you so much for the good questions. Artie.
00:34:15.025 - 00:34:15.385, Speaker B: Yes.
00:34:15.425 - 00:34:16.245, Speaker D: Hi again.
00:34:18.265 - 00:34:18.841, Speaker B: Hey.
00:34:18.953 - 00:34:49.275, Speaker D: We are for this hackathon. I'm also working with my teammate on building safety valves. So I wanted to sort of ask you if you can share like the broad idea or maybe the broad idea that goes on in like that you're working on without sharing any secrets, of course. Or maybe the future of how you see the technical side of evaluation screen.
00:34:50.055 - 00:34:52.875, Speaker A: You're of course also welcome to share all your secrets.
00:34:56.535 - 00:35:32.425, Speaker B: Yeah, no, I think I'm. I assume. Are you familiar with Milter? Yeah. Yeah. So, I mean, I don't think, like, the distributional shift from what they are doing is that significant to what we are doing. So, like, if you have a good sense of what they are doing, I think that's the kind of stuff that we are doing as well. And yeah, I think it's just like I couldn't tell you anything that is more useful than, yeah, look what the military is doing.
00:35:34.805 - 00:35:35.665, Speaker D: Thank you.
00:35:36.805 - 00:36:03.555, Speaker A: Awesome. Thank you so much, Lucas, for these great thoughts. And you'll of course be able to re listen and relook at the talk online on our YouTube channel as well. But until then, hack away and make sure to do some awesome projects here and then we'll be excited to check them out and see if maybe that's the next vector view tense of you coming out of the second. All right, thank you, everyone.
