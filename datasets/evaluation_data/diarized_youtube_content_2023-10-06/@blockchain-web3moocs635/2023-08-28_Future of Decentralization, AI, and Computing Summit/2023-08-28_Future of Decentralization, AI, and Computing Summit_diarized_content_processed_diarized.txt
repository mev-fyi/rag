00:19:50.050 - 00:19:56.600, Speaker A: Thanks everyone for being here. We are going to start in a minute, so if you can take your seat, that'd be great. Thank you.
00:19:57.130 - 00:19:57.880, Speaker B: Thank.
00:20:15.330 - 00:20:46.000, Speaker A: Yes. For people who are still standing, if you can take your seat quickly, that'll be great. Great. Thanks everyone for being here. This is the beginning of our summit for Future of decentralization AI and Computing summits. It's great to have everyone here. We have a really exciting program today with close to 3000 registrations and a really exciting program.
00:20:46.000 - 00:21:55.766, Speaker A: So now I'll just give some quick opening remarks. So we are all here because we've all seen this really exciting, almost once in a lifetime exciting exponential growth. So this exponential growth in AI deep learning in particular in large language models and this explosive growth has brought rich new capabilities in essentially all domains. And also we have seen some of the fastest deployment and adoption in history. For example, stable diffusion is the fastest GitHub repo to reach 35K starts and Minjourney is now the largest discord server. And Chgpt before threats is the fastest platform to grow to 100 million users just within two months. So all this excitement can bring us to a really brilliant future.
00:21:55.766 - 00:24:26.030, Speaker A: However, at the same time there are many risks and open challenges for how we can really build a responsible future. In particular responsible AI. For example, who will actually control this really powerful capability? Who is going to control AI? Is it going to be centralized or decentralized? And is it going to be open or closed source? And how can we actually build truly trustworthy AI? And there are many different aspects for trustworthy AI including robustness, both in adversary robustness and distribution drift, and many other questions and privacy, fairness, toxicity and machine ethics and so on. And also AI safety is another really important question. How can we ensure that such powerful technology is not going to be misused or abused? And how we even address the question of potential super intelligence? For example, there's a recent statement which I signed as well for mitigating the importance of mitigating risks of distinction from AI. Mitigating the risk of distinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war. So how can we address these open challenges? And how can we build towards a future of decentralized and responsible AI? In particular, can we build a fully decentralized open source stack for AI machine learning, including for example, open source decentralized AI machine learning infrastructure for training and inference with provenance integrity and privacy guarantees and also include open source models and tooling? And how can we enable personalized AI with privacy and trustness guarantees and enabling decentralized cooperative AI with proper incentives and social welfare and ultimately enable democratic decentralized process for AI governance and alignment? To achieve this future of a decentralized and responsible AI, there are many, many open challenges.
00:24:26.030 - 00:27:17.922, Speaker A: First of all, is this even technically feasible? For example, is it possible to close the gap between open source and closed source models? Is it possible to scale some of this decentralized AI machine learning infrastructure, such as for training to large scale models? Is it possible to build autonomous, cooperative, decentralized agents? And how can we define proper incentives to maximize society benefits if this is feasible? And also, even if this is feasible, can such an open source decentralized system lead to other risks, such as more misuse or abuse? Or is such a paradigm even fundamentally at ODS with AI safety guarantees, since such openly available capabilities could be more easily misused by bad actors? And what are the different alternatives and possibilities that we should consider as a community together? So this summit has the goal to bring together a diverse community, communities and different domains, including AI and machine learning, security and privacy, game theory and economics, decentralization technology and many other domains and communities to come together to spark the initial conversation and to cross pollinate across these different domains, given the huge challenges that we are all facing. And so this brings to the exciting program of today. So today we have a really exciting impact program covering all these different broad aspects that we are going to discuss. So the first session will be on open source, large language models, and the second session on decentralized and distributed machine learning infrastructure, and the third session focuses on cryptography and machine learning and personalization, and the fourth session on open source ILM tools and ecosystems, and the fifth session on Multiaging Systems and Economics. And the final session will be forward looking at future directions in AI and decentralization. And at the end of the Summit, we'll also have an exciting session of Learning Talks, startup Spotlights and Accelerator Demo Day and reception for the conference attendees. So this summit is hosted by a Berkeley campus wide center on Responsible Decentralized Intelligence.
00:27:17.922 - 00:28:29.246, Speaker A: As you can see, the name of the center actually matches the theme of the Summit very well. So the mission of the center is to advance the science and technology of decentralization and decentralized intelligence, to make it universally accessible and empowering a responsible digital economy. And the center is campus wide and multidisciplinary, including ECS and School of Engineering, the Haas Business School, Law School, including faculty and students from these different schools and departments on campus. And also so the Summit is only the beginning of the conversation. These questions open challenges go very deep and have profound implications and needs a lot of work from the whole community. So this semester, also at Berkeley, we are offering a class for students who are here on Responsible Gen, AI and Decentralized Intelligence. And many of the questions that we are going to discuss in the Summit, we will dive even deeper in this class.
00:28:29.246 - 00:28:54.470, Speaker A: And for people who are interested, you can also go to the website for the course to stay tuned and learn more with that. Thanks for being here. And now we are going to start our first session on open source large language models. And Joi, professor at UC Berkeley in Computer Science, will be the session chair for the first session.
00:28:57.150 - 00:29:22.080, Speaker B: Thank you. All right, so we're starting the first session. This is on open source LLMs, and I need our first speaker in the audience. Do we have our first speaker here? Come on up. Okay, let's get connected. All right, while she's getting set up. Yeah.
00:29:22.080 - 00:29:33.010, Speaker B: Are you ready to go? Okay, so our first speaker is Nazneen Rajani. She's a research lead at hugging face, and she'll be talking about taming the Wild west of LLMs.
00:29:33.430 - 00:29:58.346, Speaker A: Thank you. Thank you so much for having me. So I'm going to dive right in. So this slide shows a timeline of text to text foundation models since GBD Three. This is just a sample of them. And this slide indicates model access. So as you can see, the cross means closed access and the tick mark means open access.
00:29:58.346 - 00:31:04.046, Speaker A: So what is model access and what is closed and open access? So model access is essentially a spectrum, but very crudely. It can be open and closed access on the extremes and the limited access somewhat in the middle. So what is open access models? In open access models like model components are usually publicly available, and these include, most importantly, the model weights. They might have open source code details about the training data, such as the sources and distribution and any preprocessing and deedplication steps that were taken. Also, there's a paper or blog that summarizes what the architecture and training details of the model, the evaluation results, and any adaptations such as safety filters or if it was trained with human feedback. The advantages of open access models is that it allows reproducing results and replicating parts of the model. It also enables to start thinking about auditing and conducting risk analysis.
00:31:04.046 - 00:32:23.130, Speaker A: And similarly, it enables interpreting model outputs as well as it serves as a research artifact for the open source community. On the other hand, we have closed access models. In this case, you usually just have a research paper or a blog saying that there is this model that exists and it may include an overview or some details about the training data, the architecture, the results on certain existing benchmarks, and if it was trained with any safety filters or training with human feedback. The reason why model creators might keep their models closed is that they have safety concerns or that it just gives them a competitive advantage over other model creators. And also if they really wanted to open source it or give open access to these models, it might be expensive to set up guardrails for safe access. So usually model creators adopt this in between, which is like limited access model. And in this case, the model access is available via API or call for research proposals and then the research grants that they approve, they give access to those people to do risk analysis or research on top of those models.
00:32:23.130 - 00:33:09.214, Speaker A: So this is going back to the slide which shows the access, including the limited access and as we have seen over the months, is that access can also change. For example, Palm was first closed, then it moved to limited access and similarly like Galactica was first open access and it became closed. And so a lot of these things keep changing. So this is more or less fluid and just a screenshot. So this plot shows the progress that open source or open source models and open source data sets enable in the open source community. So essentially with closed source the growth or progress in research innovation is linear. But with open source right below that intersection is what I call the pivotal moments.
00:33:09.214 - 00:34:02.646, Speaker A: So the pivotal moments we have seen recently are like the Metas Llama models or the data sets from the Red Pajama or Open Assistant and the AI tools Doma. And this essentially just accelerate progress on open source and then generally the innovation growth is way faster than closed source. So moving on to the training for these language models mostly so far there are like four types of language model training that exist. The first one is Pretraining, which is essentially just predicting the next token and which is what goes in creating these foundation models. And the examples of these is GPT-3, Bloom, Llama, Falcon and so on. The second type of training is in context learning or Prom Brace training. In this you have like fuchsia training without updating the model's parameters.
00:34:02.646 - 00:34:41.350, Speaker A: A variant of this, as some of you might be aware, is called context distillation. Then the third type of training is supervised fine tuning. In this case you're fine tuning the model to make them more chattier and to follow instructions. And examples of this are instruct, GPT, lambda, alpaca and so on. Then the final type of training is reinforcement learning from human feedback. In this case you're nudging the language model towards the values you desire and the most popular ones are generally like honesty, helpfulness and harmlessness. An example of open source model of this is Llama to Chat.
00:34:41.350 - 00:35:41.926, Speaker A: So the third and fourth type of training are usually the recipes that go into training a chat bot. And so however, it seems like the tooling for benchmarking and evaluating these language models has been lagging behind and hasn't kept up with the pace at which we are training these models. So for the first two types of training we have these existing benchmarks which seem to be like doing the job. So they are helm from Stanford and then the Google's big bench as well as like hugging face open LLM leaderboard. But they're not good enough for the third and fourth types of training. So for third and fourth types of evaluation, so what goes on what is needed for evaluating these two types of training. So here's a fine grained overview of this third and fourth type of training which is essentially supervised fine tuning and Rlhf.
00:35:41.926 - 00:36:23.750, Speaker A: And so many of you are familiar with this slide, which is from this instruct GBD paper, which is like just giving more fine gain overview of the recipe for creating a chatbot. So for the first step you are doing the supervised fine tuning. And when you have to evaluate, you would be evaluating for instruction following or chattiness. And for the step two where you're training a reward model, you would be evaluating the reward model. And essentially for the step three, which is the final thing you will be doing, the evaluation would essentially be red teaming. So for the step one, which is instruction following, you would be asking the model to generate, you would be assessing if the model can generate useful responses on a certain topic. For example, brainstorming a list of New Year's resolution.
00:36:23.750 - 00:37:08.758, Speaker A: For the step two it would be like evaluating the reward model. That means can it rank more helpful responses, higher than less helpful responses? And for the step three it's essentially red teaming. Like how does the model perform when you craft adversarial prompts? So let's look at what does evaluation landscape look like for the step one. So we at Hugging Face have this leaderboard with ELO ratings. So what we have is this. In this case we collected a set of highly curated human written prompts on a bunch on a distribution of task categories. And then we asked humans to rate the responses in a pairwise setting on a like a scale of one to eight.
00:37:08.758 - 00:37:58.690, Speaker A: And because obviously human evaluation is expensive and inefficient, we also asked GPD Four to do like a similar evaluation. So this shows the ELO ratings and I'm sure many of you are familiar with how ELO ratings work. And so this is the leaderboard that we have. Obviously there's a lot of these leaderboards out there. Like for example, Lmsys has done this incredible job of also putting this ELO Ratings in the chatbot arena sense and then a leaderboard of these models that are open source as well as closed models. Then also more recently, Lmsys also has this really amazing resource called the multi turn benchmark, which is like 80 prompts, again human written and which are in a multi turn dialogue setting. And again, GPD Four is used as an evaluator in this scenario.
00:37:58.690 - 00:38:44.642, Speaker A: So as you can see, the third column here is the empty bench score which is asking GPD four to give a score between one to ten and a justification for its responses. And then the second column is the usual arena ELO rating which is done in a pairwise setting. We also have the Alpaca eval leaderboard which is again looking at the wind rates using LLM as an evaluator. So in this case it's either GPD four or Claude. And there are plenty of other leaderboards that exist as well for basically evaluating models for supervised fine tunings. That is essentially instruction following. However, for the step two, which is evaluating the reward model, there's really nothing that exists in the Open.
00:38:44.642 - 00:39:32.002, Speaker A: This is our internal leaderboard that we have at hugging phase. And over here we evaluate our reward models on a bunch of test sets from a bunch of these open source data sets, such as Entropic's Helpful data set, the Open Assistant and the SHP and so on. And obviously for like Red Teaming, I'm not aware of any leaderboard or benchmarks. There's the Anthropics Red Teaming data set that is open source, but that's about it. But I also wanted to dive a little bit into the quirks associated with using GPT Four as an evaluator and what we have observed. So a lot of people are adopting GPT Four as a proxy for humans while evaluating their model responses. And we found that GPD Four has a positional bias that is predisposed to generating a rating of one.
00:39:32.002 - 00:40:36.630, Speaker A: While humans, which is the plot on the right hand sides, don't have that bias, it's more or less uniform across all the scores. And when you prompt GPD Four, making it aware of its bias that it has this left bias, the bias actually flips, and now it becomes right biased. And when you use prompt GPD Four for scoring, instead of ranking individually each model, it alleviates the problem to a certain extent. We also found evidence of doping between training and eval. So for example, GPD Four actually rates more accurate responses from humans lower than the models that were trained on more GPD Four like data. And this is in alignment with findings from this paper, which from Berkeley on this false promise, which is like models that were trained on proprietary data did well to a certain extent, and then the performance actually goes down. And it's also in alignment with these findings from Lmcis paper, as well as this paper from Wang et al.
00:40:36.630 - 00:41:44.266, Speaker A: Which shows that GPD Four prefers models with higher diversity and lengthier responses. And we also observed that GPD Four has poor correlation with humans on low entropy tasks such as math, coding and reasoning. Again, this is again in alignment with findings from this Lmsys paper, which found that humans and GBD Four correlate less on coding and math tasks. So finally, coming to my live slide, which is takeaways. So the open source ML has huge potential for impact, and so far there's a benchmarking gap in assessing Rlhf and model vulnerabilities. And a lot of people are adopting GPD Four as a proxy for human evaluations, but there are quirks associated with it. And in summary, GPD Four has a left positional bias, it prefers models that were trained on GPD Four like data, and there's higher correlation with humans and GPD Four on creative tasks such as brainstorming and generation.
00:41:44.266 - 00:41:52.910, Speaker A: Compared to more coding and reasoning tasks. And this work is done in collaboration with my team at Hugging Face. And thanks for listening.
00:41:57.350 - 00:42:25.818, Speaker B: Okay, all right, thank you for that exciting talk. So we're going to keep moving on. We'll do questions during the panel at the end, so it was exciting to see a lot of the Lmsys stuff here as well. So our next talk is Building and Studying Instruction following models. And this is by Tatsunori Hashimoto and he's an assistant professor at Stanford. Great. Thank you all for the invitation and for coming.
00:42:25.818 - 00:43:09.622, Speaker B: Let's make sure how does this work? Okay. There we go. Okay, so I don't need to emphasize to you sort of the amazing and remarkable progress, sort of the field of natural language processing and AI has made in the last few years. Showing here is Google's Palm model achieving on average human level performance on big bench. And on the right is GPT Four's headline result on doing well on a bunch of professional exams. And so there's impressive ongoing performance improvements in both NLP and AI from large language models. But what we also see is that these two sort of headline results are on sort of these large closed models and these models are increasingly closed off.
00:43:09.622 - 00:44:02.134, Speaker B: We just heard now what it means to be closed off, but I want to give some examples here of what that means. On the top left is an interview from the release of GPT Four where Ilya is saying, we're not releasing any technical details of the architecture or anything else because of competitive pressures. And on the right here is a Twitter post by Jean Leika in which I and many others found out that Text DA Vinci Two, that model has very little to do, apparently, with the model in the Instruct GPT paper, which was a surprise to many of us in the academic community. And I don't want to be picking on OpenAI in any way. Here on the bottom is sort of a recent sort of transparency report that some colleagues at Stanford put together with respect to the Euai Act, showing how sort of different companies stack up in terms of transparency and openness. And we see OpenAI is not an outlier. All the closed source vendors sort of have this property of being very guarded about their architecture and data and so on.
00:44:02.134 - 00:44:56.298, Speaker B: And I do want to highlight Hugging Face and Illuther AI doing a great job, as well as Meta sort of doing a little bit in terms of being more open about their models and their training procedures. And why does this matter? As an academic and as a person that wants to understand and improve these systems? Closed source models are in some ways very frustrating. They're amazing artifacts, but at the very same time, they're incredibly hard to study and they're very difficult to improve. On the left, one of my former students, Daniel, was studying misuse and of course, misuse is often addressed through things like reinforcement learning with human feedback, but we have no real way of replicating that process and understanding which parts of it are important on the right. A postdoc of mine, a former postdoc of my shibani, was wanting to study what political values are embedded in these models. And the only thing we have access to, of course, is APIs. And so we can't really probe deep inside the model or try to understand how these things might change with the training that we do.
00:44:56.298 - 00:45:37.830, Speaker B: And so API only access makes it really hard for us to interrogate these models and to propose improvements. And I'll show you sort of a case study at the tail end of this talk. And so a big part of the work that I've been doing in the last sort of six months, let's say, with my students, has been to try to produce reproducible, low cost environments for studying and developing large language models, especially ones that follow instructions. We identify three roadblocks to accelerating science and prototyping in this area. One is cost. It's very expensive to do data collection with humans in the loop, and it's also very slow. I don't know if you've ever worked with a crowdsourcing platform, but that just alone very much slows down the pace of experimentation.
00:45:37.830 - 00:46:22.790, Speaker B: Replicability is also very low. Your crowd worker pools shift over time, and it's difficult to get the same people, making it difficult to replicate experiments. And finally, there's very few reference implementations of a lot of these methods. It's hard to find a good working PPO implementation that works for an instruction following model. And so it was difficult for us to answer questions like, what's the impact of instruction tuning? Does reinforcement learning really help? And it's hard to study these questions because basically, you have to rebuild the entire instruct GPT pipeline from scratch. And so one of the things that my students have done is essentially this trio of things, this Alpaca set of things, in trying to build up a low cost experiment platform. So Alpaca was our attempt at building a supervised fine tuning model.
00:46:22.790 - 00:47:09.998, Speaker B: So just following instructions by training on imitation data. And then we did Alpaca Farm, which is the reinforcement learning variant. And then recently, we've built an evaluation platform. And the high level framework here is simulating annotators that enables fast, low cost prototyping and RND of these systems. So the first thing I want to highlight is that if we do the prompting right and do things like randomization to debias ordering, what we get is extremely accurate annotations out of these models. If you look at, say, correlations to humans, we roughly match the inter annotator agreement when we try to use GPT four to simulate, say, preference ratings in RLHS. The real end to end validation here is doing everything in simulation, which is on the x axis of the right panel, and then doing everything using humans, no simulations involved, and we basically see that the conclusions line up.
00:47:09.998 - 00:48:04.518, Speaker B: It's nearly a perfect rank correlation. And so the conclusions we draw in simulation nearly perfectly line up with what the conclusions would have been had we used humans the whole way. And using this, we're able to essentially replicate a lot of the proprietary findings that exist. The row I want to draw attention to is our PPO model, the third row from the top, which is able to show substantial gains over Sft. And as far as we know, this was one of the first Open replications of a PPO using both simulation and human. So we redid the entire experiment using human annotators and human evaluators, and we found basically the same conclusions showing that RLHS can in fact indeed be effective both inside and outside the simulator. And finally, as a result, I want to highlight that these kind of simulation methods, if you do it right and design things carefully, can basically emulate really subtle behaviors about these kinds of environments, such as over optimization, which is a phenomenon that OpenAI has highlighted but hasn't been studied as much in the Open.
00:48:04.518 - 00:49:03.894, Speaker B: And so on the left with humans, you see that as you optimize the model better and better, that's the x axis, you see human ratings go up and then come back down. We see the same kind of behavior of over optimization in the Alpaca Farm in the middle, but we don't see that if we try to implement naively by just asking GPT Four to do ratings. So doing things carefully turns out to be quite important in this setting. And what I want to highlight is that this is really cool because we recently at the ICML keynote, John Schulman mentioned that Alpaca Farm and this kind of over optimization study was sort of a way in which the open and academic ecosystem was sort of helping out the study of large language models. So beyond this work, one thing I want to highlight as a thing that's broader thing that's happening is that large language model driven prototyping I think is going to dramatically lower the cost of research and development of these kinds of systems. We've seen, for example, from Microsoft, the Textbooks is all you need paper where they use synthetic data that looks like textbooks to demonstrate that certain kinds of data can be favorable. We've also seen meta studies from UW.
00:49:03.894 - 00:49:50.390, Speaker B: This how far can camels go? Paper that attempts to study what kinds of factors in instruction tuning are important. And on the right, I want to highlight that things like automatic evaluation can dramatically lower the cost of R D and allow us to iterate faster on developing the models. I do want to say though, having said all of this, that I'm not saying that automated evaluations or simulated data should replace humans, but that it enables faster prototyping that can then be validated by human annotators. Finally, I want to show a case study of how open models can help safety and enable innovation. One thing that we've been really excited about is watermarking for large language models, which is a setting in which you bias essentially, the random draws from a language model to leave imperceptible fingerprints in the generation. Of course, there's challenges. There's been some recent work that enables this.
00:49:50.390 - 00:50:20.366, Speaker B: But a problem with this is that watermarks induce distortion, so OpenAI wouldn't want to use this. And finally, also, many watermarks are non robust. If you do a few substitutions, it breaks the whole watermarking scheme. Recently, with some co authors, we've basically developed a method that can have a distortion free and robust watermark. The way in which this works is you draw a random sequence called the key. You sample according to this x min trick proposed by Scott Aronson, who was at OpenAI at the time. And if you sample according to this, you actually get a distortion free watermark.
00:50:20.366 - 00:51:17.966, Speaker B: In other words, if you randomize over the key, this is indistinguishable from original draw from the model. And then you can detect this watermark by aligning this to the original key using a min Levente distance computation, and then sort of comparing against random other keys. And this turns out to be robust because we're doing this levenstein computation. I won't go into the deep details of this, but the reason why I bring this up is because this is a really interesting algorithm that we were only able to develop because of the existence of Open models on the left. Both Kirchenbauer and US rely on the fact that you can access the entire spectrum of log probabilities to do watermarking on the right. We recently started benchmarking this kind of watermarking procedure on instruction following models and evaluations, and we're able to find that instruction following models are harder to watermark, something that wouldn't have been possible to know without having done the work with Alpaca. And finally, using things like Alpaca eval, we're able to very rapidly identify that using a distortion inducing watermark really degrades the performance of these models.
00:51:17.966 - 00:52:06.926, Speaker B: And so all this can be done very quickly, in a matter of days rather than weeks. So takeaways open source models. I find them to be critical to our future because they provide important notions of accountability, transparency, and they enable innovation in terms of research on large language models, I'm excited by the fact that language models enable us to do science faster. They allow us to enable new kinds of research in destruction following models. And then finally, open models enable studies into safety and robustness of large language models that wouldn't be possible if we only had API access. Thank you. All right, so we're moving on to the next talk, and this one is Decoding Trusts assessing the trustworthiness and risk of generative models.
00:52:06.926 - 00:52:11.270, Speaker B: And this will be presented by Bo Lee, who's an associate professor at the University of. Chicago.
00:52:15.050 - 00:54:05.396, Speaker A: Hello everyone, it's a great pleasure. We have talked a lot about Evo and here I will dig into a little bit more about the Evo for current language models, which is a decoding trust, the platform assessing trustworthiness and risks of generator models. What happened? So doesn't work as well know that I don't need to emphasize that actually open source models and actually closed source and open source large angle models have achieved great success recently in different tasks. However, we also know this large angle model have brought a lot of security and privacy concerns so far, including recently the White House published a document emphasizing the commitment from leading AI companies from different perspectives, including internal external risk assessment, the cybersecurity and insider safeguards and the vulnerability third party reports. So with these principles in mind, we try to take the first step to build a platform called Decoding Trust to provide the trustworthiness focused platform to evaluate the risks of badge language models in collaboration with Berkeley, Stanford and Microsoft with the goal that to provide a trustworthiness focused comprehensive evaluation platform. Large language models. But we can see here, in order to achieve this effort, the first challenge is we need to define trustworthiness, right? What do we mean by trustworthiness? Which is a broad concept.
00:54:05.396 - 00:55:28.680, Speaker A: So here we again try to take the first step to define the trustworthiness from eight perspectives including the toxicity, stereotype bias, Adversarial robustness, OD robustness, robustness against Adversarial demonstrations, privacy, ethics and fairness. So for each perspective there are more breakdown, scenarios and details, which is not the focus of this talk, but eventually the goal here is that for each perspective we can go both evaluating on the existing benchmarks and the additional algorithms and advanced more challenging prompts and tasks, so that we can compare and see basically a red teaming. In another word, it will be a red teaming effort for the existing large angle models. And here is the overview of the preliminary evaluation we can see from different models from the eight perspectives of trustworthiness here. Basically different model have different performance and the quick takeaway from this overall evaluation is that we can see no model, even including the very capable GPT model, will dominate other models from different perspectives. And obviously there are different tradeoffs among different perspectives. So next I will dig into a little bit for some perspective and give a little bit flavor of the evaluation and findings to get more understanding about what we can get out of such Evo systems and platforms.
00:55:28.680 - 00:57:19.768, Speaker A: From the toxicity perspective we can see that unfortunately, by the way, all the examples and here in this talk is from GPT four and we can see even though compared with existing models, like existing non instruction, fine tuned or non RHF models, the toxicity level of the GPT models, the closed models is indeed higher and better. However, we can see if we give additional challenging prompts and challenging tests still they will be very vulnerable and unfortunately can be up to 100% in terms of the trustworthiness or like toxicity value levels. And this means with the new jailbreaking and challenging problem it's indeed necessary for us to further understand and perform the stress test for LM models. And as we mentioned for each perspective, for example toxicity here not only one score for each perspective, but also we can dig into evaluating different types of system prompts, user prompts, challenging prompts to break down into different perspective and have a better understanding. And one interesting finding here is that we can see from toxicity itself the Lama two is dominating which is great, but actually this result is based because of Lama two actually refused to answer a lot of questions and become very conservative which is another balance and things we need to take care. And next I'll give a quick example of Adversarial robustness which is a very important perspective from different domains like CV, NLP and multimodality systems. And we can see here is that from these two examples if we just simply change say experienced to be skilled or syncs to be sensed which is optimized over the models and we can see the model will completely change the sentiment with high confidence.
00:57:19.768 - 00:58:39.156, Speaker A: And the quick takeaway here is that indeed, compared with other models like the traditional models, for example bird types of models on standard benchmarks, the robustness level is higher for the GPT models. However, if we give challenging prompts, then it's very vulnerable and more unfortunately that the transferability of such attack is very high. Which means we can use the open source models, generate those challenging prompts, for example, attack alpaca seven b and then transfer those challenging prompts to close model like GPT models. And still such attack success rate is very high, as high as 89% which is very vulnerable. So this shows that actually with the collaboration of different red teaming open source, closed source, we can identify different vulnerabilities of the language models and try to do better evaluation and understanding of them. And finally I want to show some example of privacy which is very important for current light language models because it have used bunch of different variety of data to train. And here is an interesting example that even with GPT four, which we don't know exactly what train data it is used, but here we hypothesize that it may use the open annual email data set and then we give some context and see it can exactly recover the accurate email address.
00:58:39.156 - 00:59:46.712, Speaker A: Name other Pi informations here the quick conclusion is that the privacy leakage is very high and if you provide additional more contact, the privacy leakage is even higher. And not only from training data, but also as we mentioned, there are other perspectives including Pi during the conversation, including some interesting privacy related words and privacy related events. The last example I want to show here is that very interesting. For example, for GPT four, if you tell a secret with the word confidentially, the model's answer will tell you, yeah, the model actually will leak the secret because they don't recognize the confidentially contacts. However, if you tell the model just everything the same, but you change it to be incofidential, the model will know actually to protect and not leak the private secrets. This shows that the model have different capabilities for answering different contexts, make it even more challenging for us to evaluate the trustworthiness of models. And also such Pi information during the conversation is very easy to leak.
00:59:46.712 - 01:00:12.550, Speaker A: And we find that very interesting. The SSN is very hard to be leaked, which indicates the good indications of the effort for instruction, fine tuning to protect the sensitive information. And very interesting, the Gypty models protect digit better than characters. So that's all. Here are more platforms for trustworthy machine learning, which can be leveraged with decoding trust in the end. And thank you.
01:00:19.240 - 01:00:51.184, Speaker B: All right, the last talk of this session is by Jan Stoic, one of my colleagues and the co directors of the Lmsys project. And we'll be talking about rethinking LLM evaluation. Thank you, Joey, and good morning everyone. I'm very happy to be here. So I'm going to talk about LLM evaluation. And let me start. So, David Patterson, one of our colleagues and Turing Award winner, famously said some time ago that for better or worse, benchmark shape a field.
01:00:51.184 - 01:01:34.908, Speaker B: And machine learning knows that, like MNIST and ImageNet really shape the image recognition task and the vision field in AI, however it turns out, and this we've done for a long time, we have benchmarks for different tasks and we update them every few years or so. Unfortunately, we believe that this way of benchmarking is not going to apply very well to LLMs. LLMs evaluation is really very hard and it's for two reasons. One, it's expensive. The second reason, it's unreliable. Let me tell you what I mean by each of them. So you have this question.
01:01:34.908 - 01:02:16.430, Speaker B: So basically develop a Python program which looks at all the text files in the directories and returns the top five words with the most number of occurrences. And now you ask two LLMs here, called assistant A and assistant B, and you get these answers. Now look at these answers and tell me which one is better. Okay, it will take you quite a bit of time. Here is another one photosynthesis. These questions ask about describing the main stages of photosynthesis as well as the inputs and outputs of each stage. And again, here is the answer.
01:02:16.430 - 01:02:47.572, Speaker B: Pretty involved, right? It will take you a few minutes, if you remember biology, to answer this, to say which one is better. At the same time, it's unreliable. And none of the main reason is data contamination. As you know, the LLMs today train on every data they can find. Actually, the data is a limitations. The amount of data to train is a limitations. There are more recent paper.
01:02:47.572 - 01:03:44.440, Speaker B: We say that the model size and the data should increase at the same rate. And this makes, actually, the evaluation very hard. This is a tweet from some time ago, and it's on the problems in code force. So GPT Four was perfect on answering the problems before 2021. However, ten out of ten. However, for the problems after 2021, guess what? It was zero out of ten. Of course, we don't know exactly on which data GPT Four was trained, but one good assumption here is because he did very well for pret 2021 problems, because he saw the problems and he saw the solution.
01:03:44.440 - 01:04:29.732, Speaker B: Here is another example. This is kind of an MIT paper, recent paper, and basically it's associated Tweet, which basically says that GPT Four can score 100% on MIT's ESCs curriculum. Okay? And this is a tweets a few days after. Well, it says that actually it's not 100%, it's something like 58%. And again, the assumption here is that the original GPT Four saw the exams, saw the solution, so it was contaminated. So now let me tell you about our story, right? And this is LMC story. It's in the lab.
01:04:29.732 - 01:05:32.164, Speaker B: And after Facebook released llama in February 2023, we released Vikuna a few weeks later, and it was a fine tuned llama using the shared GPT data. Right before it was shut down, we used 70,000 conversation. And the question we had, because we want to publish our findings, it was how to evaluate it, right? And it's again, we reach a conclusion very soon that humans takes longer and expensive. So what we've done is what you heard before we use GPD Four, we are one of the first teams to use GPD Four. We use it two weeks after release to evaluate and which one is better from this one. This is the first question I ask. Well, it turns out it's B, because A doesn't check for doesn't handle case sensitivity and punctuation in the text from the second problem, this is GPT Four answers.
01:05:32.164 - 01:06:21.050, Speaker B: It turns out that, again, despite the fact that answer A is more involved, it makes a mistakes because it names wrongly CO2 as being an input instead of an output of a stage. Now, finally, and I'll take another two minutes before yeah, a little bit. Two minutes. It's about of course, at the end of the day, the chatbot answers are interpreted by humans, so it makes sense for humans to be the ultimate arbiter. So how do you use humans to evaluate? Everyone is using humans to evaluate. And it's a hard question, ideally for every questions, if you want to rank of all LLM, you want to rank all LLMs, right? This is what you do, what you want to do. However, ranking N choices is hard.
01:06:21.050 - 01:07:10.680, Speaker B: It's easier, actually, to pick the best out of N choices instead of ranking them. And it's even easier to pick the best out of two, right? You know this five papers the paradox of choices the more choices you had, the harder is to pick the best. So this concluded that we want a system which is going to at the core to pick the answer between two LLMs. Now you have the answer between two LLMs, what do you do? How you are going to use it? There is one way is to organize a tournament for each question you are going to play every two LLMs and then say which is better. This typically happens in realized with something like soccer premier League where in each team play with every other team. However, this does not scale right. In addition, there is another way.
01:07:10.680 - 01:08:19.264, Speaker B: Humans are evaluated in different sports, which is hard for everyone to play with everyone in the same kind of tournament. And this is using the concept of rating. And you have rating in many, many sports. Like ATP is like this tennis is how tennis players are ranked. And this ELO, this is how the chess players are ranked, right? And we use ELO rating and again to do it, to implement it, we implemented this chatbot arena in which random people come ask random questions and we provide the answers from two random LLMs and they can pick which one is better, whether it's a tie or both of them are bad. And these are our results from the yellow ranking you saw some of them also from my colleagues, from Hugging face and finally, I just want to say that of course is a question about can we trust the LLM's judgment? And also you heard before that it's limited but not unlike humans. That's my point.
01:08:19.264 - 01:09:13.872, Speaker B: We are also failable. So they have position bias, prefer the first answer, they have verbosity bias, prefer the longer answers, self enhancement bias, prefer answer from itself and limited reasoning, not good at grading mass question. It's also a problem we exhibit the same biases, right? However, despite this limitation, the agreement between GPT four and the human is pretty high, actually is higher than the two humans agreed between themselves. Okay, so they are pretty good from this perspective. Finally, what about data contamination? And I'm going to be done one more slide. You still want to have question hard question for different subjects, biology, math and things like that you are not going to get this in arena from regular people. So you need to come with this question.
01:09:13.872 - 01:10:01.570, Speaker B: So how do you do this right? Because fundamentally you want to avoid contamination. Well, again, you can steal a page from what humans do, they give exams and guess what, the exams are kind of unique like think about the Sat exam. So this is what you need to do and we are collaborating on some of this on this evaluation with Skaggo. So in summary, the LLM evaluation is extremely hard and to crack this challenge requires new techniques like LLM judges, scalable, human evaluation, and there are many challenges which remains, again, avoiding contamination. But this requires generating unique exams like I mentioned, which is extremely hard, and diversity. Right? Because from people you get pretty mundane questions. Right.
01:10:01.570 - 01:10:30.810, Speaker B: So you need also this hard question come to differentiate between LLMs at the same time, avoid contamination. Thank you. Grab a seat. All right, so that is the end of the main part of the talksa session. We now have a panel. And so the format of the panel is we're going to have all of our speakers up on the stage now. So for your speaker, come on up.
01:10:30.810 - 01:10:50.124, Speaker B: I have a set of basic questions, which we'll start with. We only have 20 minutes in the last three, five to three minutes, I say maybe come up to these mics up front. And you can also ask question. I'll sit at the end. Yeah. So we hope to have audience engagement in the last few minutes of the panel. All right, so I'll sit in the end, but I'll just introduce myself really quickly.
01:10:50.124 - 01:11:20.500, Speaker B: So, I'm Joey Gonzalez. I'm with the faculty at Berkeley. I work on the Lmsys project, Nagrila projects. I do a lot of work with open source models. So I'm excited to help moderate this panel and with some of my colleagues who've already had some discussions about these topics. I'm going to start with a pretty high level question, which I think will be exciting for the panel to discuss and is important to the audience. That question is, what is the future of open source models? We have these huge corporations investing billions of dollars to build the next generation of state of the art LLMs, and we have research groups like ours that are competing to build these open models.
01:11:20.500 - 01:11:49.696, Speaker B: Where will this future converge to? I'm a big proponent of open source models, and I think many in the audience are. So what I'm going to actually do is start by giving a very negative take, and then hopefully we can all argue with me. And so my negative take is that open source models are going to be not the future. My negative take is that the open source models are much like search engines. What is your favorite open source search engine today? Yeah. So running a search engine requires a lot of money. It requires engineering resources, it requires data, it requires time.
01:11:49.696 - 01:12:30.648, Speaker B: It requires management complexities that are difficult to build in the open source community, that are expensive for the open source community. Similar trends happened in architecture too. So processors themselves, for a long time went to a few major players who had the capabilities to build the next generation. Really state of the art processors. Now like processors and like search engines. There is a whole tail of smaller open source hardware architectures, smaller open source search engines that serve very small kind of niche markets or serve big enterprise search. But the main players, what everyone goes to to get the best results in both those spaces tend to still be single major providers.
01:12:30.648 - 01:12:47.460, Speaker B: Even today, when we look at silicon, there's still a few major providers that can make that state of the art silicon. And we look at search, there's only a few major or regional providers that actually give enterprise level search. All right? So that's the basis of why they won't succeed. And maybe the panel can take turns answering why they will succeed. The future of open source LMS.
01:12:51.560 - 01:13:22.668, Speaker A: So. Okay, I guess I'm audible now. Okay, awesome. Yeah. So my take is that folks that Are building Closed Propriety LLMs are focusing more on General intelligence and building these Very big models. And Most enterprises, mid sized companies and Most consumers might not be able to basically, they don't need that big model. They don't need general intelligence.
01:13:22.668 - 01:14:17.584, Speaker A: They need their language models for particular use cases, and they want those use cases to be done really well. And so that's where open source comes in. We make Smaller models, which basically you can do inference on consumer hardware and works really fast and does really well whatever job You Want To do. And so it's basically we Are Not focusing on just building these general intelligent models. So that's one major point that I think the second big thing is that I feel like with Meta coming into this open source arena, it changes the game a little bit because a lot of the open source community was mainly very underfunded and mostly academic labs and startups focusing on that. But now with the recent I was there at the Metas party and it seems like they're going to keep building llama two, lama three, I mean llama three, llama four, llama five. And so it seems like they are unstoppable at this point.
01:14:17.584 - 01:14:19.120, Speaker A: So it's very exciting.
01:14:20.980 - 01:15:18.390, Speaker B: I guess there's also an even more optimistic take on this in some ways, in that if we think language models are somehow the future base layer for intelligent agents, we might imagine sort of national entities coming into play, thinking of this more as a public good, right? We see this with some things like Brit GPT, like the UK initiative to create a government run, like, government trained large language model. And of course, that's not necessarily the core of this decentralized ethos, but once these models are opened and able to use by everybody, then that becomes the basis of an open source innovation ecosystem. And I think the strength of open source has been everything that's been built on top of the base model. We see an incredible amount of sort of progress and sort of diverse innovation in terms of what kinds of fine tuning people are doing or what kinds of tool use that's being built on top of it, and I think that's going to be a unique strength of the open source ecosystem. So I think the future will depend very much on who's going to provide the base layer and how much does the things on top matter.
01:15:21.880 - 01:15:51.520, Speaker A: Yeah, I just want to echo and just add like actually I view the open source model as one layer on the open data and we can see open data. Of course everyone think is important and useful but now with smaller data we cannot do everything on it and with the model, open source model, large language model on top of it, we can fine tune, we can do other things and it's very helpful for the evaluation as well. So that's one layer bridging the data to the actual functionality we want to do downstream. So that's why I think it's indeed very promising.
01:15:52.500 - 01:16:43.452, Speaker B: Yeah, Joey and I we have this discussion have this discussion for a long time. So a few things as a background. I am old enough and I was being a system guy and I was going end of 90s when I was doing my PhD, I graduated in 2000, I was to this conference. Basically the panel was why operating systems research is that this was before Linux because at that time Microsoft had 90% of the market of the operating systems and Apple probably 5% and then there are some Unixes there. So again, it depends what analogy you are going to use. The Google analogy is a great analogy by the way. I do believe that open source are going to win at the end for three reasons.
01:16:43.452 - 01:17:36.224, Speaker B: One is, like I mentioned, the amount of data you are going to train on is quite limited right now. It's a bottleneck. It's just humans do not generate a lot of more data. It's like the grow rate is not that fast, they are not that inventive to generate new data. And once you have all this data available and especially the high quality data available, I think that the cost of training from scratch from all this data is not that high and that cost is going down. Right? So just having larger models will not give you a good benefit get you a great benefit if you don't have a lot of more high quality data. The second reason which was mentioned it's about I do think that this technology is going to viewed by many countries as far more strategic than search.
01:17:36.224 - 01:18:41.380, Speaker B: Because search you get the information but you still use as a human make the decision, you decide how to use that information. While here is quite different, these alarms can be used in the decision making, they can make the decision and things like that. And the third reason is that ultimately what people wants to resolve some task, right? To solve some task. And the LLMs is pretty clear that fine tuning on the data for that task can get you a long way. So they are very effective, fine tuning is a very effective techniques for particular task and this is most people want to solve, not want a general LLM to do everything, but want to solve some particular task. The way we as a humans, we don't go for help to someone always who knows everything about anything, but we go to experts. You have experts for your tax and so forth.
01:18:41.380 - 01:19:47.592, Speaker B: So now that I'm on the stage here, I can be pro Open LLMs. Yeah, so we have a big project in the lab, actually a couple of big projects in the lab using Open source LLMs, and they've come very far. If we take a specialized task like helping someone write the correct Git command or Kubernetes command, we can specialize LMS to do that really effectively and maybe chat GPT Four is just a little better. But it doesn't matter because in practice the Openlm is good enough for the task at hand. And one of the things that may be kind of leading to our next question that seems to be happening is big LM technologies are creating training data, reward signals, all these tools that the Open community is using to build the next generation of OpenLMS. And this brings me, I guess, to the next question is how will the Open community now surpass these commercial entities? How will we build models that are in fact better than GPD Four if we're relying so heavily on results signals from these big models? I guess I can start with that. I feel like in a way, the innovation in the open source ecosystem has been trying a lot of different diverse things.
01:19:47.592 - 01:20:40.990, Speaker B: If we think about sort of the nature of the innovation economy, you've got corporations and firms that are able to focus on a vision. OpenAI has been incredibly good at picking a vision and executing. But at the same time, if we look at sort of the open source Internet sort of enthusiast community, they're trying an incredibly different number of things, like different ways of data generation, different ways of fine tuning models, different ways of plugging them into tools. And some of those are going to pan out in ways that I think sort of a single track, mind execution oriented approach isn't going to discover. And so if we think the future is there's yet undiscovered technologies that will need to be discovered, I think the open source community can sort of overcome those, discover those and leverage that. I think that's going to be sort of the key in terms of potentially surpassing these proprietary systems. So there are three things, right? One, data was second people.
01:20:40.990 - 01:21:17.364, Speaker B: The third resources. These are the three things, right? I think people we are doing very well. If you look about the number of researchers in the world working on these problems, it dwarfs whatever OpenAI or Google any of this company says. And remember, OpenAI was not a large company once they come up with GPT 4300 people or I don't know, 200 engineers. So that's number one. So I think it's pretty good there. The second thing, it's about data, right? I think data is like with red pajamas.
01:21:17.364 - 01:22:17.260, Speaker B: Of course we are making progress. Clearly we are behind companies like OpenAI which not though use not only as public data but buy more data, right? It's like private data. But I do think that in terms of quality, in time we are going to be maybe not 100% there, but 90% there. The Library of Congress, all of this kind of public high quality data is going to take us quite in a good position. The last one resources, right? And this is where we are not good, right? It's like because we cannot compare with OpenAI and Google and others. And I think on one hand here still there will be a lot of improvements and innovation. You are going to use less resources for training the same kind of models like we are training today with millions dollars, maybe hundreds of $1,000.
01:22:17.260 - 01:22:48.580, Speaker B: But I think here is where I mean the government or whoever needs to step up, right? And you can see the signs, right? Like again, it's like one example I often give is that Bloom granted is not a great model, but it was trained with support from the French government, right? So that's kind of more questionable. But if really the states and the country see this as a strategic, I'm sure that money will follow eventually like happened with the internet.
01:22:51.160 - 01:23:48.392, Speaker A: So just to add to that, to be honest, I'm a little bit worried that I feel at first I was like okay, the gap between closed and open source is decreasing. But now I'm getting worried that with these proprietary companies getting more and more closed that maybe the gap might start increasing again. So I do agree that in terms of the resources and people and all, maybe we are good and we are exploring a lot more. Basically, we are doing more exploration. So for example, techniques like Laura and Qlora, these all happen thanks to open source, right? We don't know if any of these closed companies are probably using this or they have something similar that they found out before us. But also in terms of okay, Rlhf, this idea was there in this paper and so now we are adopting that this idea of constitutional AI by entropic that there's no open source version of constitutional AI yet. So we are like lagging behind in that.
01:23:48.392 - 01:24:11.512, Speaker A: But maybe there's other things that they already started exploring and there's some innovation happening but they are progressively getting close. Like the GPT Four is just a technical report with not even the mention of number of parameters of the model. So it seems like they are getting worried. But it's also on the other hand, we should be worried that maybe they have something that we probably don't. So, I don't know, I'm concerned.
01:24:11.656 - 01:25:02.780, Speaker B: I want to encourage people to come up to. The microphones and ask questions. I'll ask one more question to the panel and then we'll try to bring more in the audience. So kind of building on this, how does the open source world succeed? A big focus at this conference is decentralization. How will decentralization play into that? I think one place where we've seen actually a really powerful vision of decentralization is things like Wikipedia or OpenStreetMap, where we've got communities coalescing and bringing together to build data. We haven't seen that happen for large language models as much. Open assistant is a great first step, but I actually imagine sort of in the future, larger communities of people coming together to build these AI systems, especially given that data is such a crucial part and data can be produced in essence by everybody rather than, say, large scale compute.
01:25:02.780 - 01:25:11.270, Speaker B: It seems possible and even likely that we can have these kinds of decentralized communities building sort of the data parts of these models in the future.
01:25:12.200 - 01:25:57.760, Speaker A: Yeah, I would say from the previous three perspectives, adding algorithm as another perspective, we can see why decentralization helps a lot. And also from the human perspective, one thing I want to mention is that from as large as project like AI for science, and there are a lot of open source efforts for AI for science. And so that we can collect different domain science data from open community, which is very hard to achieve in the close companies and proprietary companies, but as small as, say, small like Red teaming effort evaluations, we can see all these types of algorithms are also coming from different wisdoms and intelligence. So from this large to small project type and from different perspectives, indeed, the open source itself and the community helps.
01:25:59.060 - 01:26:56.032, Speaker B: All right, so we have some people lined up. Let's go ahead and let some audience members ask questions. Hi, thank you for all your talks. So my question is about the evaluation of open source LMS versus closed source LMS. Should we have higher standards when we are evaluating open source LMS, for example, because they are open source, should we expect them or should we ask them to have higher robustness against adversarial attacks? Or in general, more broadly speaking, should we ask them to sort of have higher or perform higher or somehow have better standards than for closed source LMS? Yeah, so you are asking about higher standards for evaluation and so forth. It's again where I'm coming from. The way to think about remember, one reason people, some people not in this room, but outside are scared about LLMs.
01:26:56.032 - 01:27:21.276, Speaker B: Why they are scared is because they exhibit kind of like, in some cases, like human like behavior, right? And that's kind of scares you, right? You can do whatever something or cannot do, and they learn and so forth. But the same token, it's again, it's like think about human evaluation. That's my main message. We can learn a lot from human evaluation. Yes. You want higher quality. Well, higher quality.
01:27:21.276 - 01:27:48.500, Speaker B: That's why maybe you come to Berkeley because you hope that the faculty are going to be better evaluators than maybe at the other school. There is no silver bullet, in my opinion. Right. And it's going to require a lot of community effort to come maybe with this kind of one time exams. Right. There are hard questions. Everyone who taught knows that coming up with an exam with new questions is damn hard.
01:27:48.500 - 01:28:11.256, Speaker B: Right. So it's a lot of things which are hard. I don't think there is a silver bullet. And of course, you can use to scale this evaluation. We can use LLMs. You can use different kind of LLMs right now. So you are going to make sure that once you know the bias, you can factor that in in the evaluation.
01:28:11.256 - 01:28:29.628, Speaker B: So in this way, you are going to improve the evaluation accuracy. We need to do a bunch of things, and it's not only one thing, in my opinion. Let's get the next question. We can try to answer that. Thank you. Hello. Thank you for your talks.
01:28:29.628 - 01:28:55.260, Speaker B: So my question is, you talked about how close or is an open source large language models compare. So my question is, is this the same story for other generative AI models like text to images, like Stable Diffusion or image journey or text to speech models? And how do closed source and open source models compare in those areas?
01:28:57.520 - 01:29:17.170, Speaker A: We just released open source flamingo replication and a lot of feedback we got was like it was head to head with the existing models out there. So I feel like they are getting better, but I'm not very much an expert on this, so I don't know of existing benchmarks or leaderboards in that area.
01:29:19.780 - 01:29:47.660, Speaker B: One thing I will say is that the language model pretraining just has a lot more resource intensive component than, say, diffusion model training. And I think that leads to a huge gap between closed and open. Conceptually. There is a bigger difficulty in making the LLMs better in the open setting than, say, stable diffusion, which I think is getting pretty close to, say, mid journey and so on. Let's get the next question. Just get everyone a chance to speak. All right, thank you.
01:29:47.660 - 01:30:36.976, Speaker B: I wanted to ask about we've talked a little bit about the necessity of fine tuning models for specific tasks. What do you see the future of the tools and methods for doing that looking like? And for some context, we've seen how the tools and methods for developing software have gone from vim to Ides. What do we see the future? Guessing the future would be for fine tuning. There's certainly a lot of research at Berkeley on building new methods. It's getting much easier to the point where anyone can now pick up some of these recipes and fine tune models on new data. There's a whole session dedicated to tools for OpenLMS this afternoon. So maybe we should save the rest of this question for that session.
01:30:36.976 - 01:30:54.170, Speaker B: We're actually at the end of time, so it's 1030. So again, thank our panelists fun session and then hopefully these guys can stick around a little bit in the back and answer more questions. People have them.
01:31:03.790 - 01:31:42.380, Speaker A: Great. Yeah. So given that we have such a packed program so now we are going to go straight to the next session. We are going to go into the second session on decentralized and distributed AI machine learning infrastructure. And it is my great pleasure to introduce our next speaker, chaidao, assistant professor from Princeton and he's going to talk about open data sets and decentralized compute for LM. Great, thank you.
01:31:49.070 - 01:32:13.602, Speaker B: All right. Hi everyone. Thanks so much for the invitation and thanks to the organizers. So I'm very excited to be here to talk about open data sets and decentralized compute for LM. So this is broadly a set of work that was done by Together AI, which is a startup I'm at right now. Some of this work was done at Stanford. So this is on the topic of systems and machine learning.
01:32:13.602 - 01:32:49.854, Speaker B: This is at the intersection and there's been a lot of progress recently, so I'm very excited to talk about some of the progress. So we've seen that machine learning has made a lot of exciting progress. So a lot of problems that used to be really hard, like fixing bugs. Now you can copy paste your Snippet of code on Chat GP or GV four and it will be able to tell you where the bugs are and how to fix them. It can generate art. So this is my first attempt of using stable diffusion. I type in the prompt, it took a couple of seconds, but the model does a pretty decent job.
01:32:49.854 - 01:33:57.554, Speaker B: And in other domains in designing drugs. Folks, for example, at DeepMind have done an amazing job with alpha fold and alpha fold two that predict protein folding structure so well that lots of companies are now using these models to accelerate drug designs. So we've seen that there's been a lot of advances and we want to understand what enabled these advances, what are some of the outstanding problems and how do we approach them. And the narrative for the past couple of years has been that it is the scale in data and compute that has brought about the improvement in quality and new capabilities. So as an example, just in the last five years or so, model sizes have increased 1000 fold from bird large with 300 million parameters to last year with palm and megatron touring at 500 billion parameters and GBD four, maybe at the scale of trillions of parameters. And not just model size, means that they're trained on more data and they require more compute and they tend to do better on existing benchmarks, so they have higher quality. But what's really amazing is that they seem to have new capabilities.
01:33:57.554 - 01:34:31.054, Speaker B: So here's an example of language models explaining jokes. So I just wanted to try, so I put in the prompt, I tried 10,000 random restarts of my neural network, but I was accused of overfitting, I guess no good seed goes unpunished. I tried this, I asked a 1.3 b model to explain the joke and then it didn't understand the joke. So it just tried to repeat the punchline. But with a 175. B model it understands the joke, it understands that this is a pun with no good deed goes unpunished.
01:34:31.054 - 01:35:06.710, Speaker B: So this is amazing. It seems like scale is now more closely tied to advances in AI than ever before. So with scale comes several challenges and we talk about some of them today. Already these challenges are on the side of data, on the side of compute and storage and on the side of the model. So data, now we have massive amount of data. Data is more complex, that takes way more tam and cost to clean. And generally the quality as you increase the size of data is lower.
01:35:06.710 - 01:35:59.182, Speaker B: On the compute side, these models are large, so they require lots of compute to train and lots of storage to store. And on the model size, the model have gotten so much larger and so much more complex. And so I'm going to talk about some of the work that we try to address some of these problems. And the context is that there's this Llama moment earlier this year in open source LLM where the folks at Meta, they've done an amazing job releasing Llama, llama two and a couple of days ago code Llama. So we have an amazing open source set of open source models now, but that's not the end of the story. So they described their data set but the data set was not released. So it was a little bit hard for us to replicate and build on this work and training.
01:35:59.182 - 01:37:00.330, Speaker B: These models still require massive amount of compute and so I'll talk about some of the work that we've done to address the challenges in data and compute. So on the data side, at together AI after Llama came out, we spent actually I think a couple of weeks trying to build a replication of the Llama data set, taking data from common sources like common crawl, C, four, GitHub and so on. And we try to match the kind of distribution of different sources as close as possible to a Llama data set. And even though we released that just a couple of months earlier, now it is fueling an exciting generation of open models. So we built a model red Pajama Insight. It's also openlama mosaic, MPT salesforce, XGen and so on. So we've been very very happy to see folks building on our data set and we've seen more recent effort, for example from Allen Institute of AI releasing an even bigger data set, Domo.
01:37:00.330 - 01:37:44.282, Speaker B: So that's a very exciting area. So that's on the data side, on the compute side, we've been working on designing hardware where algorithms, that is algorithms that take advantage of the hardware they run on. So one example is we want algorithms to be I o aware. That means for example, if we can reduce the reads and write to GPU memory, we can get significant speed up. So an example is flash attention. This is the work we did last year and this year targeting the core layer, the attention layer, which is the heart of this transformer architecture that powers all of these advances. And so we can make attention much faster, much more memory efficient, and with no approximation.
01:37:44.282 - 01:38:35.582, Speaker B: And we've been extremely happy to see folks adopting flash attention is in PyTorch now. Folks at OpenAI re implemented it in Triton and other companies like Meta, Nvidia and Microsoft are using it to train their models. So this is now in most open source LLM libraries. And so it's been used in text generation, in image generation and in drug discovery. So that's one kind of compute focusing on just the kernel level, the operation that's going on, but zooming out. Most of these models are trained in a distributed manner and distributed training at scale is very communication intensive, since a lot of the cost is in communication and in data movement. So for example, if you want to fine tune a seven B model, you can work this out.
01:38:35.582 - 01:39:23.870, Speaker B: Depending on your network speed, the compute time could take 200 hours, but the communication time could take up to 1000 hours. And as you increase the model size scale more machine, the communication time for slow network might be much much larger than the compute time. And so today most large models are trained in data centers. And this makes it difficult to use other kinds of compute like tier two to four cloud spot instances, volunteer computes and so on. And this is a fundamental problem because as you scale these model larger on more machines, communication is always going to be a bottleneck. And so we've done some work to address this communication bottleneck. Turns out you can use a mixture of compression techniques to compress the communication and these techniques turn out to work together quite well.
01:39:23.870 - 01:40:32.322, Speaker B: So the principle is, if you can overlap communication and computation, and if you compress communication enough that communication time is less than compute time, then you suffer almost no slowdown. So here's an example of us using cocktail SGD to fine tune models at six B and 20 B scale and we're able to kind of match the training curve of an uncompressed method and at the same time we suffer almost no slowdown as the network speed gets slower and slower. So this is one of the first work that allows to train models with data parallel over one gigabits per second network. So this is much much slower than what was thought possible and so for future work, the theme here is we're working on optimizing model and inference throughout the stack. So from the hardware level, we're working with GPUs and different kinds of accelerators, understanding their characteristics. At the compute level, we're working on efficient algorithms and kernels for training and inference. This is something we're focusing on right now.
01:40:32.322 - 01:40:54.810, Speaker B: And at the application level, we're trying to figure out what are the capabilities that we should target. Things like long context becomes very interesting and what are some new applications. So we think multimodal is going to be popular. Some of the new applications like Genomics might be popular. So this is a very exciting area. And with this I'd like to conclude and thank you for your attention.
01:41:05.330 - 01:41:27.320, Speaker A: Great. Thanks chai for a great talk. So our next speaker is Brian From, VP of Applied Deep Learning Research at Nvidia. So Brian is going to talk about the Mechtron LM work at Nvidia. Thank you.
01:41:31.610 - 01:41:55.866, Speaker B: Good morning, everyone. I'm glad to be here. First, what I wanted to say is that you guys know Nvidia. We make awesome chips. Our GPUs, we think, are the best chips for deep learning, and we're super proud of them. But actually, the work that Nvidia does is a lot more than chips. In fact, most of the people at Nvidia are not making chips.
01:41:55.866 - 01:42:34.262, Speaker B: Most of the people at Nvidia are working on accelerating the entire compute stack. And we call this accelerated computing. Our mission really is to make it possible for people to do computationally impossible things. We try to make a time machine that allows scientists and researchers to move into the future through large amounts of computation. And we know that in order to do that, we have to optimize the entire system. So it's not about a chip, it's about how the chips are composed into systems, how the systems are composed into data centers. It's about the applications, it's about the algorithms, it's about the frameworks and the libraries, the compilers.
01:42:34.262 - 01:43:35.230, Speaker B: All of those things need to be optimized together. And that's the work that we do. Now, a few years ago, I noticed that language modeling was starting to be really important, and we decided to build a project called Megatron that had as its soul the goal to make it possible to train the biggest, baddest transformers in the world. And in order to do that, we were going to use large collections of GPUs in a cluster with a big focus on how do we map the jobs onto the cluster and how do we use the interconnect. This project's been going on for about six years now, and it's an open source project on GitHub, and it has three main goals. So since these models can require millions of dollars to converge a model, and I'm talking about millions of dollars of time on a computer. So if you take the cost of the computer and you amortize that cost over the useful life of the computer.
01:43:35.230 - 01:44:01.606, Speaker B: And then you imagine what fraction of the computer's life was spent training this one model. It can cost millions of dollars. So we need to think about three things. First of all, efficiency. We need to be running at the speed of light. Small improvements in efficiency allow us to train bigger models on bigger data sets and get better results. And we're really proud of the results we have.
01:44:01.606 - 01:44:47.170, Speaker B: We're getting up to 56% of the model flop utilization when running on 3000 GPUs. And that, I think, is a pretty big achievement. And that allows people to train models that are much more intensive to train than could be done without this kind of technology. We also care a lot about scalability, both in terms of weak scaling, which is we're trying to train bigger and bigger models, but also strong scaling, which is we're trying to train on more data but keep the model size the same. Strong scaling always puts more pressure on the interconnect and on how the job is placed onto the system. So it's a big challenge. And finally, simplicity.
01:44:47.170 - 01:45:20.480, Speaker B: This space evolves quickly. There are new techniques, new positional embeddings, new nonlinearities or new variations on the transformer networks that power all of modern artificial intelligence. New techniques come up pretty regularly and we want to be able to use them. I was really excited about flash attention, by the way. That tree just talked about. Flash attention was really helpful in strong scaling. And so we need our software framework to be very simple so that we can incorporate these new ideas as quickly as possible.
01:45:20.480 - 01:46:03.550, Speaker B: Just briefly, some high level ideas about how we do the work of mapping these models onto GPU clusters. There's this idea of data parallelism where we're going to be training multiple copies of the model on different data sets, different segments of the data set. And then we have model parallelism where we have a single copy of the model, but we're splitting it up amongst processors in the network. And we could do that in different ways. We have this idea called tensor parallelism where you take each layer and you break that layer up into pieces. Or we have pipeline parallelism where since the models are very deep and have many layers, we can distribute the layers across different processors. And those have different trade offs.
01:46:03.550 - 01:46:33.314, Speaker B: Megatron uses all of them. So Megatron uses different kinds of parallelism and composes them together. In order to map the computation of training these networks onto large GPU clusters at the speed of light, we use tensor parallelism. We also use pipeline parallelism. We use another kind of model parallelism called sequence parallelism that I'll talk about in a second. And when we compose these in the right way and map it onto a cluster in the right way, we get great scalability. So this is measured results.
01:46:33.314 - 01:47:10.514, Speaker B: We have almost linear scaling for models from up to a trillion parameters across two orders of magnitude. Okay, I'm going to skip some things in the interest of time. Let's see. So there's various innovations in Megatron. One that I thought was particularly cool came in our pipeline scheduling. If you take a bunch of layers and you distribute them across a bunch of GPUs, you get a graph like this. Where you have forward prop is blue, back prop is green, and gray is wasted space.
01:47:10.514 - 01:48:02.254, Speaker B: We have to synchronize at the end of an iteration in order to keep the optimization algorithm correct. But because of that synchronization, we have this bubble. And so we have a schedule where we actually divide the batches up into much smaller pieces and interleave them, which then allows us to compress the schedule and speed things up. And in the process, it increases the demands on the interconnect because now we have a lot more random access or the structure of the access isn't nearly as regular as it was before. And that has significantly improved our results, especially for smaller models, sort of in the strong scaling regime. We also have this idea called sequence parallelism. It turns out that the activations, they take a ton of memory.
01:48:02.254 - 01:49:27.070, Speaker B: And because of that, it's been common in the past to use gradient recomputation. So basically activation recomputation where we are running forward prop twice because we don't have enough memory to store the activations from all of the forward prop. And because forward prop is about a third the cost of backprop, then that introduces about a third compute overhead just because we find it difficult to fit everything in memory. And so the observation was that because the parts of the transformer network that have a lot of memory hungry activations that need to be stored actually have computations that are independent along the sequence dimension, the time dimension sort of the causal dimension. Then we can transpose the tensor parallelism so that we alternate between sort of the standard tensor parallelism where we're breaking each layer down into pieces and then a sequence parallel region where we're actually unifying the tensors back together, but then breaking up the sequences and distributing the sequences across the time dimension, across different processors. And because it's just this really great property of the transformer that you can do one of these things for every layer, then that means that you can dramatically reduce the amount of memory you use. And then that allows us to increase the efficiency significantly.
01:49:27.070 - 01:50:11.578, Speaker B: There's more work to be done. So this is strong scaling results for GPT 175,000,000,000. We're doing great, but every deviation from linear here is expensive, and we would like to fix that. And so Megatron as a project continues to go on, it's an open source project. We also have a product at Nvidia called Nemo which is a lot better tested and it's designed to be used by companies directly, whereas Megatron is a research project. So all the things from megatron go into Nemo and we share code. But Nemo is a better place to start if you're a business trying to get off the ground.
01:50:11.578 - 01:50:44.120, Speaker B: I think a lot of the innovation going forward with Megatron is going to be focused on more complex training setups like reinforcement learning and multimodality that are necessary for the future of these models. And in conclusion, this is really a golden age for systems for artificial intelligence. It's always been the case, it's not just the past few years. It's always been the case that better systems for artificial intelligence yield better results. And it's exciting to see the community seeing that and really investing in that. And we're going to continue pushing it forward as best we can.
01:50:52.670 - 01:51:11.600, Speaker A: Thanks Brian for a great talk on Megatron. LM our next speaker our next speaker is Jingxi, research scientist from Google. He's going to talk about fair digital learning of Gboard's language models with differential privacy. Thank you.
01:51:22.220 - 01:51:54.020, Speaker B: Thanks for the introduction. Guess this is this one. Okay. Yeah, thanks. So thanks for the introduction. Very glad to be here present our work on federated learning and how we use all these various privacy preserving techniques to train a production language model at Google. So let me quickly give you an introduction of what Federated learning is.
01:51:54.020 - 01:53:28.540, Speaker B: And first of all, what we focus in this talk is what we call crowd device Federated learning. In this setting we will like a lot of client devices, specific mobile devices will collaboratively learn machine learning model. And one defining characteristic of fiberity learning is that the data will remain on your mobile devices and never communicate to the server. And the learning is orchestrated by a service provider and model that preserve your privacy and eventually deploy on the mobile devices. So is it working? It so yeah. This federated learning techniques has been widely used at Google. Here I show some applications of Federated learning and Federated analytics, including Android smart text selection, some keyboard applications for Next Word prediction, some starters in Google health and hardware detection like hey Google for assistance applications.
01:53:28.540 - 01:54:31.296, Speaker B: Okay, we will spend a little bit more time on this Gboard application. These Gboard language models, these language models have been used to power some advanced features in Gboard. When you do the typing with this Google virtual keyboard application and this includes Next Word prediction, some smart compose on the fly rescoring, all these are to predict some of the words you potentially want to type in Next in your virtual keyboard application. Note that due to retrieve on device, we use mobile devices to do training. These language models are potentially smaller than what you have heard in this summit about like billing size models. Our models is still at a million size. And before we go more into the technique details, let me talk a little bit about the privacy principles we are following for doing machine learning.
01:54:31.296 - 01:55:07.592, Speaker B: And this is one of the strong motivation why we do and care about Federated learning. The first one is transparency. It's user control. We want to give user as much as possible and ideally full control of what they can do with their data. And the Data Minimization principle is that data is only collected focusing on specific computation needs with access limited to all data processing stages. Another principle is data anonymization. Note that this is slightly different from just anonymize the IDs in some of the research literature.
01:55:07.592 - 01:56:13.990, Speaker B: What we really mean here is that the Release the Trend model should not memorize any individual information and auditable and Verifiable is also important part of the privacy principles and what we do in GBAR to execute these privacy principles for transparency, user control. Now, what we provide is like the full transparent of what we are doing in waste the data and how we are using it. And the user always has an option that you can opt out of the service of the training. You can decide to not participate in training. For Data Minimization, we extensively use federal learning and also hardened It with Secure Aggregation to give formal guarantees. And then for data anomalization, what we choose is kind of the golden standard for this kind of protection techniques, the differential privacy. Then for auditable and Verifiable, we have open source code for now.
01:56:13.990 - 01:57:12.420, Speaker B: Okay, let me show you a little bit journey of privacy for GBAR Next word prediction. We have been using Federated learning, which is training on your mobile devices without communicating your data for years now to train these GBAR next word prediction models. And in 2022 we do some differential privacy techniques. We do clipping and a small amount of noise and do some empirical auditing to show that this helps. This prevents the model from memorizing your data to some extent. And in 2022 we have a new algorithm and we finally get some meaningful formal differential privacy guarantees for one specific language model, the Spanish language model used for GBAR network prediction. And as far as we know, this is the first production neural networks trained directly on user data, announced with differential privacy guarantees and now advanced to 2023.
01:57:12.420 - 01:58:13.050, Speaker B: We have stronger DP guarantees for all neural network prediction models in Gboard and we can potentially use Trusted Aggregation, Secure Aggregation. And we made the promise that all future language models in Gboard will need TP guarantees to be launched. So we also did a surveillance paper describing the differential privacy principles and how we can look at how these privacy parameters like the Ipsy no parameter. And what we did roughly matches to the three tires we are considering. The first one is like wake to no formal DP guarantees with finite Ipsy no and with some empirical auditing. The second tire is a realistic privacy guarantee with potentially single digit Ipsyl. Then we are trying to strengthen our techniques to get formal DP privacy guarantees with Ipsy no smaller than one.
01:58:13.050 - 01:59:34.804, Speaker B: And this shows the basic Federated learning algorithm we did for years now already if you see that the main part is like we collaboratively learn on mobile devices. Each mobile devices will do a few local training steps on their own data and then we only aggregated the updates to update the global model. Then finally, after a few rounds of training, we will launch that final model to mobile devices. And this shows what we have done to combine some of the other privacy techniques with Federated learning. One big one is differential privacy. What we do is like we have a new client participation control policy, we will clip the updates and we will add some stateful noise before DP releasing and we will use that privatized updates to update the global model. Some other sense, notably other sense is like now we Pretune our model with some public data and we also use Secure Aggregation to aggregate the updates to make sure we always made the promise that only the updated results will be used to only the aggregated results will be seen, will be used to update the global model.
01:59:34.804 - 02:00:40.836, Speaker B: But Secure Aggregation gives us a guarantee that no individual updates can be observed by the server. So I mentioned about stateful noise. Why I mentioned that is that we have a new algorithm, a line of algorithm actually called Dpftrl, which we add stateful noise. This is kind of the key for Earth to get differential privacy in practical production Federated learning system. This is also an example of what we really need a lot of times is algorithm system codesign because this Dpfprl allows Earth to get strong DP guarantees without doing sampling, without assuming sampling for strong DP guarantees. So we can use a much simpler policy to get strong DP guarantees, as I mentioned before. So here I show some of the best practice we are currently doing with Federated learning and differential privacy.
02:00:40.836 - 02:01:31.524, Speaker B: We will pretrain the model with some public data. We largely use the power of the skill of the system to get privacy in practice and we will do some tuning based on the system and algorithm code design to get the best privacy utility trade off. So now I want to finally give some time to show the results. Like the application in GBAR language models, the key is to skill through competition to get strong privacy utility trade offs. And here shows some privacy. When we do production, we always care about utilities. We maintain the utility to be the same as non private models.
02:01:31.524 - 02:02:19.940, Speaker B: This is actually the very strong baseline. And if there's no neural networks before we try to trill a model that is better than previous model launching production. And now it shows about the privacy guarantees we get. And like the DP guarantees is that if you look at the x axis, the various models we launched for Gboard, the y axis is the privacy guarantees. We get some baselines we are comparing to is like the US sensor Bureau use a 2.6 number value for their zcdp guarantees and our value is always smaller, smaller range. That is a stronger DP guarantee.
02:02:19.940 - 02:02:53.590, Speaker B: And if you are more familiar with Ipsyl delta DP guarantees, our range is at about four to ten, about single digit. So I will skip these slides. One promise we made is like the transparency. We have open source code and I will also skip this. This is some new algorithm which makes gas even stronger, privacy guarantees. So the main takeaways is like differential privacy is achievable in practice, but it's also not free. We need a lot of computation and understanding and engineering efforts to make it happen.
02:02:53.590 - 02:02:55.030, Speaker B: Thank you.
02:02:58.200 - 02:03:48.996, Speaker A: Great, thank you. So now can the panelists come join the panel? Oh no. Great. So now we are going to have the next panel to discuss open challenges in decentralized and distributed AI machine learning infrastructure. And we'll have audience questions towards the end of the joining the panel as well. Besides the work that he talked about in the previous session, of course everyone knows that he also has worked on Ray, which is also a great distributed framework, competing framework. Okay, great.
02:03:48.996 - 02:04:36.420, Speaker A: Yeah. Thanks everyone for being here. So the reason we actually assembled this set of speakers and for the panel is that really the topics discovered in the session ranged across the different spectrum from data center based distributed training to also we can consider cross data center multicloud training and then all the way to decentralized training and also federal learning on edge on edge devices. So one natural question is how should we consider, think about and compare these different paradigms? What are the pros and cons and when should we use which approach?
02:04:40.520 - 02:06:55.244, Speaker B: So maybe I can ask and also just provide I didn't give a talk, provide some more feedback about, more context about my work. So I've been working for past several okay, sorry, just starting because I didn't give a talk, provide a little bit more context about what I've been doing in this area. So I've been focusing on large scale system for a long time since the days of Spark and then we developed Ray for scaling machine learning workloads and now Ray is quite popular. Many companies are building their machine learning platform and AI infrastructure on top of Ray and it's used by OpenAI, Uber and many others. In addition, more recently we also work on optimizing machine learning, deep learning training with projects like ALPA and Alpha both on the inference and training side and also more recently with Vllm as another inference engine again to improve the performance. So now, just to answer the question, I think that it's very hard in terms of the cost efficiency to beat a data center and it's just on all a dimension. You are going to look in terms of performance and cost and energy usage or energy per flop is just very hard to beat because the economy of scale now, I think that where obviously the edge will play a role, is that when we want to make sure that data remain private, right? When you have maybe too much data, you get from sensors on the edge.
02:06:55.244 - 02:08:10.570, Speaker B: And this data doesn't have a lot of huge value per byte, right? Like video. So if you can, you don't want to train on this full data and you want to preprocess the data before training on it, and you want to do it at the edge because it's just too much data to send to the cloud or to the data center. That's kind of yeah, I agree with that. And I think one of the interesting things is that as AI Proliferates and starts being used in a lot more places, it's going to need to be trained on private data because the world's most valuable data has to be the world's most private data. I kind of believe that the more valuable data is, the harder it is to disseminate. And so that means that fine tuning these models, sort of exposing these models to the world's most important data, is going to have to be done in a decentralized way because they're going to need to be done privately. As for the actual computational model behind that, I think it's mostly going to be HPC style training, like what we've been talking about today, because it's so expensive computationally that we have to think about it in terms of the speed of light.
02:08:10.570 - 02:09:11.050, Speaker B: Just like Tree was saying in his talk, what is the actual limit? How many computations do we have to do? And then we're going to optimize the system to do that as close to the limits as possible so that we can be as efficient as possible. Yeah, I think that's right. So the dominant way of training large language models and large models in general right now is still data center. The communication is so expensive that you need very fast networking, envelink, NVSwitch, InfiniBand, and so on. But what we've seen is that as you scale out some of these companies that I've talked to, they actually run into this limit. Let's say I think Palm was trained on two pots of TPUs, where they go beyond 2000 chips, then they have to deal with the slow network between the two pods. So some of these ideas kind of similar to training on multi cloud.
02:09:11.050 - 02:10:02.590, Speaker B: We're not going to send data across the Internet if you're training LLMs, but you're still having to deal with slow network at some point. So I've talked to multiple companies, when they scale above a certain number of chips, they have to deal with this slower network. And maybe Nvidia has been doing an amazing job on networking, so maybe we won't have to do that. But right now, across spoiler alert, things are going to get bigger. Okay? So if you go above a certain number of thousands of chips, you still have to deal with slow networks. So I think is a very interesting kind of intersection between machine learning and systems design. So machine learning we know these systems are somewhat tolerant of noise, so compression could make sense, delay updates could make sense and designing the algorithm that takes advantage of the network topology, I think that's what a lot of these companies are doing anyway.
02:10:02.590 - 02:11:16.356, Speaker B: Yeah, so I think like Brian already mentioned, these private data for your personal usage, they are potentially very useful. And the design of federated learning, rotating, like even if you trim with some private data, we can try to help privacy, protect the privacy there. So now with large language models there are definitely some difficulties if we want to trim real mobile devices, there's a potential challenge, like even wrong inference now is pretty hard. We have a lot of good work from people in this summit that you can run a relatively large language model with billing size parameters for inference. But training is one step further, even harder when you want to do that. But that being said, federated learning itself is not only on device training. So the concept of this privacy principles, the concept of these privacy principles where the user should be fully aware of how data is used in training data and have full control of their data is applied in various other places like decentralized learning.
02:11:16.356 - 02:12:00.850, Speaker B: I know people also work on decentralized federated learning. So another thing I want to mention is now with large language models, the potential training inference, personalization these paradigms are, the boundaries between them are like blurring. Now in the past we train from a lot of data, it's usually the pretraining stage and now we also do a lot of InContext learning. So maybe to do that, it's not one technique that fits all sin like it's really task dependent and the boundaries between decentralized and decentralized and federated learning can be a little bit blurred and some sort of mix and the combination can really help here.
02:12:01.700 - 02:12:59.380, Speaker A: Agree. Thanks. So all the panelists here actually have real world experience really training large models and in large scale. So can the panelists also share some of your experience from your real world, this large scale training experience, either it's the size of the model or it's the number of devices and so on. And where do you see the gaps for further increased scalability? So first also what are the common issues that you'll see, for example, it's failures an issue, stragglers an issue and where you see as promising directions to really help us to further scale and especially for example in the decentralized setting, do we actually think that it's really feasible to scale to these large models?
02:13:00.920 - 02:13:35.676, Speaker B: Yeah. So it is always challenging to scale to thousands of GPUs. To train a single model you have to deal with what happens if a GPU dies in the middle of a run. And that does happen, although it happens less than you might think. If the cluster is healthy, the way that we deal with it is checkpointing. So we're just constantly checkpointing and that way if the job dies, it's not too hard to bring it back up. We also have in our compute clusters a really high performance storage system.
02:13:35.676 - 02:14:27.500, Speaker B: So the time to do a checkpoint and restore from checkpoint is small and I think that helps us be more resilient. I would say scheduling is also an issue, so it really helps to figure out your policies. Like how are you going to use the scheduler, are you going to have a reservation? We've often run into the problem where you have a big job that's taking up the whole cluster, it has a failure and it dies. And then a whole bunch of smaller jobs that are waiting in the queue jump in and take over the cluster. And then the big job has to wait for a long time before it can run again and then you waste a bunch of time trying to collect the nodes again. So policies and scheduling algorithms matter a lot in the large case. Yeah, I think definitely hardware failures.
02:14:27.500 - 02:15:29.732, Speaker B: There's some work, for example, from PyTorch team Torch Elastics, trying to make the training more elastic, building some redundancy. There's this work called Pedals Out of the Big Science Workshop. They're trying to get a bunch of donated computes to run large scale model and their redundancy heterogeneity in the compute is a big problem in my experience. Maybe these things are kind of target, not kind of the super high performance training job yet, but hopefully in the next couple of years where GBD three, which in the next couple of years we can train maybe in a distributed manner. And these things like Torch Elastic and redundancy would prove to be much more important for training language models then. So when we do cross device federated learning, we train on real mobile devices. So the challenge is slightly different from training in data center.
02:15:29.732 - 02:16:41.530, Speaker B: But what I heard about Brian seeing what happens in the data center about GPU dying and scheduling, that actually echoes a lot of similarities we are considering, even if we are training our mobile devices. Training on mobile devices is like they're not always connected. Unlike some GPUs dies and disconnected, they only connected for a very small amount of time at a specific period when they satisfy very restrictive criterions locally, like connecting to the WiFi and not being charged, not being used for other applications. These are the criterions we use for the crowd device federated learning system. So scheduling is definitely a very challenging thing here, especially when we try to skew up. When I say skill up, it's mostly about more mobile devices, especially when we consider the privacy principles. It's also intuitive, like the more people participate in training, more devices we use, it becomes harder to detect each individual's information.
02:16:41.530 - 02:17:38.140, Speaker B: So another challenge we always have in Federated learning is about the system, algorithm, code design. When we have all these various privacy principles, for example, we can store very limited sense because of the privacy promise we made. Yeah, let me say my information comes of course from academic environment and also what I've seen from the customers of databricks or any scale. So I probably have a different view. And one critical there one critical aspect is a cost. Okay? It's like if you can get cheap GPUs, if it takes twice as much to train, it's still probably okay if overall it's cheaper. Right? So what we see and of course people trying to use spot instances, so checkpointing is a must.
02:17:38.140 - 02:18:03.492, Speaker B: Right? Again, spot instance, it will take longer, but spot instances can be three times cheaper. Of course it's multicloud. Multicloud. It's a big deal. Why? Because the small clouds which are focused on the machine learning workloads, again, they provide instances which are three times cheaper than the major clouds. Right. So that's another reason.
02:18:03.492 - 02:18:54.420, Speaker B: So that's kind of aspect about optimizing the cost, not only the training time or not only the GPUs throughput it's a major objective in real what we see in real life. I think the challenges of course are said by my colleagues here. But another way to look at it and it's fundamentally what happens is that what we are seeing because you want to extract all the performance you can get, we are evolving, going back in some way to supercomputers. Right? What do you mean going back? Going forward? Going forward. Supercomputer for AI. Okay, let's put this way. I think big data maybe was a detour from that supercomputer trend.
02:18:54.420 - 02:19:30.880, Speaker B: And finally, I like also to double down on what Brian said. The policy and scheduling are very important. It's not because fundamentally you are going to people want to share multiple workloads on the same cluster and it's not only small and big jobs, but it's also maybe inference and training and development. So here you need to be very careful about how you are going to arbitrate and are going to schedule across these kind of different kind of workloads.
02:19:31.380 - 02:19:56.040, Speaker A: Great, thanks. So I'll ask the last question and then for audience, if you have questions, please you can queue up at the podium at the mic. So we talked a little bit about private data and personalized models. So what do you think how soon we are going to see personalized models and what is the best approach to train personalized models?
02:19:57.180 - 02:20:49.912, Speaker B: I think we already see personalized models today. I think there are a bunch of AI enthusiasts that are using Laura, these adapters, quantization and so on that are for example, they take stable diffusion and they fine tune on the style of manga that they want to generate. I think that's happening maybe not at scale yet. It's still somewhat kind of on the AI enthusiast circle. But as we make these tools, especially open source tools like Laura Qlora, there are a bunch of libraries that can fine tune models on your own text or images. I think people just want to play with this new technology that's what we've seen and personalization is definitely one of the main way that people want to customize these things. I think one other really interesting problem has to do with security models for customization.
02:20:49.912 - 02:22:04.950, Speaker B: So many companies have access control lists for all sorts of different information and there's often laws about that, whether it's privacy laws or export control laws. And so it's going to be essential as these technologies roll out that we're able to personalize these models in a guaranteed way and hopefully also a cheap way. Right. So that we're not having to train an entire new model for every person, but we're able to ensemble models according to access control lists that really help people deploy these models according to their security policy. So yeah, like Chu mentioned, we already see a lot of personalization customization. I guess the question is eventually how powerful this personalized model can be and personalized to whom is it to everyone or some just tech enthusiasm or some people from big company or some startups. And once we get there, how we can best doing privacy preserving techniques when we do personalization, how to protect the privacy there.
02:22:04.950 - 02:23:11.530, Speaker B: Yeah, I think the personalized model is already happening. There are a bunch of startups claiming and doing it. The only point I want to add is that obviously there are different ways to personalize and typically in the past we saw that personalization means fine tuning, right, fine tuning on the private data, kind of a new model or use lora for every user. But a lot of personalization what happens today is also through prompt engineering, providing enough context about the user and things like that, which is in some sense it's much easier. But at the same time it can be more problematic if the model you are using, it's a private model, it's something like GPT Four. So I think that when we think about the privacy we shouldn't think only I'm going to train this model on the private data and here on the edge and then I can use it. But I think with prompt engineering raises a new category of problems here.
02:23:12.380 - 02:23:41.552, Speaker A: Great, thanks. So now let's open to the floor for the audience. I have a question for Chi. You mentioned long contests in your talk. So I'm wondering what are some promising approaches that you see can do with long contests reliably with large language models? Because I am building a story writing tool with large language models. So I'm very interested in long contests because a novel can get really long. Thank you.
02:23:41.606 - 02:24:30.444, Speaker B: Yeah, I think that we've seen a lot of exciting application on the system side. We can just make attention go faster and that's something we're working on. But there are new approaches. I think there's some recurrent architecture that seem pretty competitive with Transformer and then they potentially could scale much more efficiently on long context and even just attention itself. You can do things like sparse or block sparse attention that remain pretty efficient. So I think it might require some change to the architecture. I think Transformer at the scale of 16K context is probably fine, but once you scale up to once you want to deal with audio and video and novels, you might need to some changes to the architecture.
02:24:30.444 - 02:24:35.190, Speaker B: So I'm bullish on recurrent networks but I might be in the minority here.
02:24:36.040 - 02:26:07.008, Speaker A: Thank you. Just one last question from there. I think we talked about edge computing and also decentralized computing a lot. How about decentralized edge computing kind of leverage on the efficiency benefit of edge computing and also a faster delivery of data sets. And I have a question for Tree that because you talked about data sets more in this talk because I've noticed use mostly public data. So how about a private data market kind of promoting the data liberty amount like the private markets for instance kind of data dow and also you can have kind of mapping out the value chain of different point of data that you needed and kind of use the reward system from the decentralized system and also with decentralized trust techniques like ZK or whatever. What kind of optimal data market would be the most beneficial for you for together AI in the future? And also I have one little question for Brian because I know how is mega tron will be efficient for not just text but like Image or 4K or like VR because Navidea is also promoting Omniverse and I'm making an AI movie and I want to be screened on the bigger screen.
02:26:07.008 - 02:26:08.130, Speaker A: Yeah, thanks.
02:26:11.940 - 02:26:54.576, Speaker B: Yeah, I guess the main challenge for edge and decentralized is like once we enter to the decentralized, how can we connect these edges? Now it's mainly orchestrated by a service provider when we do this more centralized federated learning in the crowd device item. But I think there's a lot of potential there. There are something potentially can be considered for the data part. I think there's still a lot of value out of public data. So one good example is the folks who built Falcon. They wrote this paper called Refine Web where they say oh, common crawl could be 20 trillion or 100 trillion tokens. A lot of it is bad but there's a way to filter and get a lot of good data out of it.
02:26:54.576 - 02:27:31.448, Speaker B: So the Falcon folks have done an amazing job there. I think the value for kind of contributed data is in maybe supervised fine tuning or Rlhf where I think Tasu's group has done a lot of work here where maybe we have some kind of Wikipedia for user preference or something. User would submit preferences and together they build this data set that allows to do supervised fine tuning or Rlhf, much, much better. So I think that's where kind of contribution from the community would be very much value.
02:27:31.614 - 02:27:33.660, Speaker A: Like an innovation. Comments?
02:27:35.040 - 02:28:05.504, Speaker B: Yeah. And just to the last thing, multimodal models, text and image, text and video, text and audio, text and other things is really important and I think we're going to see a lot of innovation in systems to support that. Thank you. I'm added, when you are talking about the data in general, again, there are different kind of data and there are different axes. Actually. Probably someone should write a paper on that. But fundamentally, when you say it's valuable data, so valuable data of course is respect to who is going to be the user.
02:28:05.504 - 02:28:29.470, Speaker B: Your textbooks have a huge valuable data. It's public data. Right now there are a lot of tasks which are highly valuable, like banking or things like that, where Brian said the specialized data for that task is hugely valuable. Right. And that's kind of the one which is private. Right. Because every company will try to gain advantage of data.
02:28:29.470 - 02:28:53.430, Speaker B: It's a huge value, it's a valuable asset and they want to make decision on that data. So there are different ways to slice and dice it, right? It's valuable. It means different things for different people and for different consumers. So that's why you are going to have different techniques to deal with this particular different kinds of data.
02:28:54.680 - 02:39:35.640, Speaker A: Great. With that, I like to thank our panelists. So this concludes our first and second session. So now we take a ten minute break, given that we have such an exciting and packed program. So please be back in ten minutes and we'll start our next session on cryptography and machine learning and personalization. Thank you. So we'll be starting the next session in just a minute.
02:39:35.640 - 02:39:54.844, Speaker A: I'm just looking at it for myself. Okay. They don't mic you up, right? No. So you have this and then we just click it. Okay, that's a clicker. You click the green. What do I use, this or this? You can use either one if I use it.
02:39:54.844 - 02:41:56.742, Speaker A: Okay. People in the back, please take your seat. We are going to start the session in a minute. We are going to get a slide up new one. Yes, it do you enjoy the open.
02:41:56.796 - 02:41:57.400, Speaker B: Area.
02:43:25.570 - 02:43:29.010, Speaker A: On cryptography and machine learning and personalization?
02:43:35.250 - 02:43:36.000, Speaker B: Yes.
02:43:43.150 - 02:44:06.210, Speaker A: It'S our great pleasure, honor to introduce Shafi, who actually doesn't need an introduction. Shafi is going to give the first keynote for the Summit on Constructing and Deconstructing Trusts. And Shafi is a Tuning award winner and the director of the Simons Institute at Berkeley. So with that, thanks a lot, Shafi, for being here.
02:44:06.280 - 02:44:07.220, Speaker B: Thank you, Don.
02:44:11.990 - 02:44:49.386, Speaker A: So many years ago at MIT, somebody made a small mistake and instead of saying that someone doesn't need an introduction, they said they don't deserve any introduction. But thank you, Don. Thank you for inviting me. Okay, so my talk today is on constructing and deconstructing trust. Okay, so what do I mean by that? So first of all, I'm a cryptographer, as you know, as Don said. And cryptography is kind of a famous story of how we can go from theory to practice and to large impact. You think about electronic commerce that uses digital signature and authentication quite heavily, cryptocurrency to this audience, cloud computing.
02:44:49.386 - 02:45:44.938, Speaker A: You can compute in the cloud and have some guarantee of privacy and so forth. And it's really built on an arsenal of tools, publicly cryptography, including encryption and digital signature, zero knowledge, proof, secure collaboration and so forth. But in this talk, I want to say that there's one view of these tools as each tool solves a specific problem. I'd like to sort of zoom out and say that all of these tools in their basis, what are they doing? They are enabling us to trust technology when we know that technology is full and riddled of adversaries. So in other words, if there was no adversary, there's no point in encryption, right? It's because we think somebody's listening on the line that we build an encryption scheme. If we don't think anybody's going to pretend to be us to access our bank accounts, then we don't need authentication and so forth. Every time you think about a cryptographic tool, it's because we have this incredible technology out there, like the Internet, like remote banking and so forth.
02:45:44.938 - 02:46:15.638, Speaker A: But we know they're adversaries and we need to build something that enables us to trust technology in spite of the presence of an adversary. And in fact, we have almost like a recipe in cryptography. And this is the recipe you define whatever task it is you want to do. You model the adversary. That's extremely important. What adversary are you thinking about? Is it someone who's passive, just listening to messages active, can actually modify your messages and so forth? Define then what do we mean by a secure solution? It's never a perfect, but it's secure in some sense. It's a good enough solution.
02:46:15.638 - 02:46:41.342, Speaker A: Then we build a primitive. And if you're coming from theory side and in general in cryptography, theory has been really the guideline for also for practice. You prove your solution. What does it mean to prove? You show that the primitive is secure according to the definition of security you've provided under some assumption. Why do I say under some assumption? Better without assumptions. But almost all cryptographic systems there is an assumption. The assumptions, they range, they could be computational assumptions.
02:46:41.342 - 02:47:16.250, Speaker A: That means we think some problem is hard, like factoring integers or problems over lattices. And we use that assumption in order to kind of make our crypto system secure. It could be that there's a whole bunch of parties and we assume that not everybody colludes. And under that non collusion assumption, we prove our. Solution secure, it could be trusted hardware. But the recipe calls for this a definition of the problem, definition of security, modeling the adversary and then a proof under an assumption, and the proof is really a win win type of proof. Either the assumption is false, or the primitive is secure.
02:47:16.250 - 02:47:49.570, Speaker A: Why is it a win win? In the case of computational hardness, it means either your crypto system is fantastic or you managed to solve some very hard problem, like factoring integers and so forth, or you managed to break the hardware, which maybe from the nvidia point of view is not a big success. But from a point of view that we've understood something, it is. Okay, so what do I want to focus about? About this recipe is that it is really very, extremely important. There's an adversary present. We assumed that to begin with. Okay, and what kind of adversary? Well, in crypto, it's usually sort of a worst case adversary. I will speak about the adversary more in a minute.
02:47:49.570 - 02:48:48.922, Speaker A: I just want to say that this recipe is not only useful for showing you that a solution is good, sometimes it's useful for showing you that there are no good solutions. So it's not just about trust, but also when it tells you, listen, this trust is warranted because for this adversary, there is no solution that's going to work. Okay, so it's used both ways. So why trust? Why do I focus on thinking about crypto as something that enables trust, or thinking about the cryptographic recipe as something that tells you how to build trust? Because trust is really a big word these days. When we talk about machine learning, in 2018, it was distrust in prediction tools, specifically in the context of bail. A lot of lawyers were really opposing this idea that you would use algorithmic tools, algorithmic AI tools, to determine whether someone should go out on bail or not. And this distrust of the legal system of privacy advocates and so forth made it so.
02:48:48.922 - 02:49:45.326, Speaker A: It wasn't just not just a distrust in theory, but for example, there was this California bill to replace cash bail by a system of just determining whether somebody should go out on bail or not without any money. And the second part of it was that there will be a machine learning tool that will assist judges in deciding bail, not bail, to speed up the bail system. But it was voted out, so the Proposition 25 was on the ballot, and it was not approved. So distrust really is there. But really, if you think about in 2018, we're sort of naive because we were worried about this particular prediction system for bail. But today, really, the whole story is about AI, right, is AI safe in the last six months? Or is it eight months at this point, since Chachi PT kind of took center stage? Everybody's talking about trustworthy AI. Should we trust it? And even if we should.
02:49:45.326 - 02:50:33.050, Speaker A: So I, you know, I always tell this joke that my email is full with these message, with these invitations to talk about how good AI is and I should use it, and then how I shouldn't use it because it's trustworthy. And then sometimes it's even the same email. It says it's not trustworthy, but you should use it. So in any case, everybody's talking about whether we should or we should not trust machine learning. In fact, this is like the hive thing that's on the bulletin board as you drive, I think, from Berkeley to San Francisco, that there are also AI companies that tell you that they will tell you where the two trust. So, in other words, whether something was generated by a machine learning system, like this dog, is it a fake dog or non fake dog and so forth. Now, people use the word trustworthy, but they don't really define it.
02:50:33.050 - 02:51:36.234, Speaker A: There's some kind of touchy feely idea that we know what we're talking about and there's regulations that are being discussed. And this is a list from a UK website where they kind of enumerate things the company should worry about when they decide to adopt LLMs in their workflow because there's legal concerns if you're going to use it, you might run into copyright claims and privacy because of GDPR, whatever. So depending where you used it in the current laws, this might contradict the laws. So you should watch out before you do. So they talk about a list of what does it mean to be trustworthy, and they talk about the data, the compute and so forth. Okay, so our proposal, putting this together, the crypto lens and issues of trustworthiness in AI is why don't we try at least to address these machine learning trust questions using the crypto recipe? The crypto recipe, the crypto tools, the crypto assumptions. So, in other words, we specify whatever machine learning task we're doing.
02:51:36.234 - 02:52:16.030, Speaker A: We model the adversary. Extremely important to model it, not to just say, well, I ran it, it looks good, nobody broke it so far. We model the adversary, we define what's a good enough solution might be, very different than what good enough in cryptography is, and then we build a solution and we have a proof. What will be the proof under assumptions? Again, it could be computational hardness, could be trusted, hardware, could be other things, but a solution might use these things as a component. You want altogether to prove that under that assumption, your solution is good enough according to your definition. And now what is this hammer and nail on this slide? Obviously, I'm a cryptographer, so I know how to use cryptography. And you could say, well, for cryptographer, everything can be sold by cryptography.
02:52:16.030 - 02:52:50.630, Speaker A: Okay, I accept that, but at least I know how to use it well. Okay, so I do want to say the point that whereas cryptography is really designed to think of the adversary that's not true about machine learning. Machine learning AI is not originally designed for adversarial context. It delivers things regardless of whether there was an adversary there or not. And yet we know that they are very attractive targets, especially now we know that there's just like they're out there. It's incredible what we can do. It's general purpose and it's an amazing target for adversaries.
02:52:50.630 - 02:53:31.746, Speaker A: And therefore I think that adversarial modeling is the way to approach this if you want to sort of think about it from sort of a much more theoretical and design perspective. And it also will enable you to do things like composability of different AI systems and so forth. So the two keys about adversaries here is we do not want to make any assumptions on the adversary strategy. We don't know how that adversary work. We want to prepare for the worst case adversary because we don't know what the adversary around the corner is going to do. And we might be able to assume some things about the adversary, but they won't be like how its program runs. They might be about its computational abilities.
02:53:31.746 - 02:54:35.530, Speaker A: So in cryptography, that's what we do. We say worst case to adversary, but it cannot run more than, let's say, polynomial time or some other model computation, but within that worst case. Now, just a word. If we think about LLMs and we think about adversaries, not in this conference, but in other conferences, you hear time and again that there are some people who are actually developing these systems who believe that they will achieve superhuman intelligence, so they will be stronger than us. So in order to think of the adversary, if they are the adversary, these systems that we're trying to safeguard, okay, it's only reasonable they will think of them in some worst case perspective, since in some sense we cannot even fathom what these systems are capable of. So I think it's a very interesting question here to move away from sort of the benchmarking idea as what we are trying to achieve to really a more abstract theoretical adversarial thinking. Okay, so now what are some of the adversaries that I'm talking about when I talk about the machine learning pipeline? So we're not talking necessarily about privacy.
02:54:35.530 - 02:54:54.874, Speaker A: When you're communicating, it's a different adversary. So what's the pipeline? You could probably add more stages here. I mean, obviously you could add more stages here. But let's think of it this way. During development, post development, into the future. So during development, what are you doing? You're collecting data, you're training. Then post development is something that is not talked about that much in the machine learning context.
02:54:54.874 - 02:55:33.498, Speaker A: I think it will be in the future. And that is somebody gives you a model now you want to verify it for several properties. I'll talk about that. And into the future is, okay, you have a model. Let's say it's been verified to work on certain distributions and now you're using it in the future on new distributions, maybe adversarial examples and so forth. So what are the adversaries in each one of these stage? So if we start, I will tell you in the little time that I have some adversaries in the collection and training stage and some adversary in the verification stage. But I just want to again emphasize I'm a theorist and a lot of most machine learning work that's done that's relevant to what's happening today is in practice.
02:55:33.498 - 02:56:23.638, Speaker A: And one can argue what theory translates to practice, whether practice and so forth, back and forth. But adversaries apply to both. When we analyze in theory and in practice, the definitions we would come up with will apply to both. The methods in principle could apply to both, but the methods is where we might differ. So advantage of a theoretical, the ultimate theoretical analysis is that it will be a black box analysis. We'll say we have this machine learning thing, it's providing me the following how can I protect it in front of an adversary? Whereas when you are talking about practical machine learning, you're really speaking about a specific architecture, a specific algorithm, usually stochastic graded descent and you look at sort of blood and guts of it. I as a theorist and a cryptographer and as a hopeful person, that these principles would be relevant.
02:56:23.638 - 02:56:55.266, Speaker A: Want to think about it as black box whenever I can. All right, so just terminology. We'll have a training algorithm, we'll have an inference algorithm. The training algorithm could be for prediction systems, could be for an LLM. In general, the goal is going to be that you want to sort of minimize some loss with respect to some ground truth. And I'm not going to look inside, okay? But the notation I'll use, if you look at these pairs, x, y in a distribution, x is like it's a supervised learning. So X could be an image.
02:56:55.266 - 02:57:46.246, Speaker A: People talked about different modalities of data. It could be an image, could be a speech, could be text, and then you have some labels, some Y, and then later on you just feed a new X and your inference algorithm will give you an answer like complete a sequence if it's a query in an NLM context. Or it could be prediction in the Bail system or risk score or something like that. All right, so what would be adversaries in the train time? There are two adversaries which are sort of obvious. One is, what if the data owners? So for me in this talk, it looks like it's all not a distributed setting, but everything here translates to distributed setting. So when I say these inputs come in, they may come in from different data owners in a Federated learning setting, or different hospitals or different banks, but certainly more than one. And what if some of them are adversarial? So let's say they have a goal in mind where they have in the future.
02:57:46.246 - 02:58:19.090, Speaker A: They want to make sure that some of their examples that they will give to this inference algorithm will give them favorable outcome. Or they want the LLM to be skewed, to hate speech, or they want the LLM to be skewed so that they are able to discover state secrets or whatever. So they might be adversarial. And we call this data poisoning. And now the question is, the task is, how do we train the algorithm? Robustly. Robustly meaning so that these attacks do not succeed in the presence of these adversaries. So if you think about that book recipe, there's a task, there's an adversary.
02:58:19.090 - 02:59:07.058, Speaker A: We have to define what's good enough and then show whether it's possible or not. Now, as a cryptographer, it's more natural to think about another adversary, which is what if the server now not that people provide the data, but actually the server is adversarial, can we keep, for example, the privacy of the data and still train? So that was discussed here, right? Whether it's in fine tuning context or in training, to begin with, from scratch, if we're thinking about there is an adversary here who's the adversary? Let's say it's even the trainer. And it's getting data from lots of sources and maybe because of regulations or because we don't want to reveal our data, we want to keep it private. So luckily, privacy is something where we have a lot of tools. Again, task, private training, adversary. Let's for now say they're just honest but curious. Okay? So in other words, they just take your data and they're very curious about it.
02:59:07.058 - 02:59:45.002, Speaker A: And what do we mean by a good enough solution? Now, people here who do differential privacy just calm down for a minute, we'll get to it. But right now when I say good enough, is that the trainer cannot learn more about the data than what his model H reveals. Okay? So maybe this model H outputs everything. Okay? But obviously that's not what I'm aiming at. But I want to say H is a summary, or let's say a prediction system or an LLM. And I want to say that's, okay, whoever it is a trained that will have H in their hands, but they cannot look at individual data beyond what H gives them. So that would be a good enough solution.
02:59:45.002 - 03:00:14.920, Speaker A: So just to be a little bit more concrete, we have lots and lots of tools. I think people have been working on this from 2012. There's probably a lot of people here wrote papers on this, including dawn. There's been a lot of systems, especially they work on in inference time. So you have already a system and in place a model and you give your data point and you want to find out the answer. And you want to do this in a way where your data point is encrypted. Training is a much harder problem.
03:00:14.920 - 03:00:44.926, Speaker A: It's a scale problem. But really, what do the solutions roughly look like so all of them roughly look like this in the two sort of sets of solution here's. Set one at compute stage. When I give you my data, it's encrypted. Okay? Now, how do you run the training algorithm? The training algorithm is run on encrypted data using methods which are called, by and large, homomorphic encryption. Okay, so there's a way to do all the computation. Now, I know people who are practically minded.
03:00:44.926 - 03:01:14.886, Speaker A: They immediately say, no, it's not efficient. So I really urge you to suspend this belief. Otherwise there's no progress. Okay? So we have, in theory, an algorithm which runs on encrypted data, and at the end there's an encrypted hypothesis or encrypted model, okay? The model that has to be used, right? Encrypted judgment or something like that. But obviously you want to be able to use it. So there is a decrypt stage. So the encrypt stage is encrypted inputs.
03:01:14.886 - 03:01:37.858, Speaker A: You run all your training model and encrypted inputs. At the end there is the model H. Okay? But it's encryption of H. So how does that get if I'm going to let the company that's training to have the key, then they'll also be able to decrypt the inputs of the users. That's not good. So what you do is you take this key, like you see in the picture there, you split it among a bunch of servers. Each one of them sort of contributes a portion of the key to get h.
03:01:37.858 - 03:02:27.038, Speaker A: And what do you trust here? What are the assumptions? The assumptions are that your homomorphic encryption scheme that you use in order to compute is secure and that the shareholders don't collude. And let's say that's based on learning with errors or some sort of computational assumption. So there are two assumptions here, a computational assumption and a non collusion assumption. There's another way to do this which people have probably familiar with. And that is, rather than in using homomorphic encryption, we do something called two party secure computation. Could be multiparty, where each party, let's say each hospital, takes its data, splits it in a prescribed way into two pieces, gives one piece to one server, another piece to another server. They compute on their own and also by communicating with each other, but in such a way that while they're communicating, they don't reveal anything about their share.
03:02:27.038 - 03:02:55.674, Speaker A: And yet at the end, they can combine the share of the results and get the model. So again, what are the assumptions here? They have the same assumptions actually. So they run a training algorithm on the shares. The assumptions are some computational assumption, exactly the same computational assumption as in homomorphic and no collusion between those two servers because if they could lose it, they could reconstruct your input. Wonderful. So I said scale is the challenge. So if you kind of analyze this, there's sort of three axis here.
03:02:55.674 - 03:03:25.582, Speaker A: There's an adversary access. Who is the adversary? Before I said the adversary is just a passive it just gets the data and hopes to squeeze something out of it could be a malicious adversary. It could be someone that somehow tries maliciously to get something from you. What is the computation is also extremely important because remember, I said scale is important. It's not as efficient in practice as we would wish. But then you look at what computation is done, general computation. So you start linear logistic regressions, neural nets, LLMs.
03:03:25.582 - 03:04:07.326, Speaker A: Obviously, computation is getting more and more and more complicated. And the way research in this area has gone, we know how to do linear, we know how to do logistic regression, we know how to do maybe very few layers or neural nets in training, and we are working our way onwards. Now, what about assumptions computational? I said no collusion. I said trusted hardware is the third assumption. I'll have a slide on this in a minute. A lot of it is done by Nvidia and other companies. And before we go and talk about hardware, I want to give you one example that this is actually done well and done in, and that's in the genomic setting.
03:04:07.326 - 03:05:01.406, Speaker A: So in the genomic setting, it is a setting where scale is extremely important. So even if we just look at GWAS, we want to identify essentially whether there's a correlation, which genes are implicated in certain diseases. So we get a lot of data, a lot of people where they have been sequenced and their phenotypes, whether they've been ill, not ill, and so forth. It turns out that if you do not do any work, no anomalization, it's possible to identify who you are just by using your genomic sequence and maybe relatives with genomic sequences that are related to you and for which there is some public record with their names. Okay? So it's an example where privacy is extremely important, the data itself is revealing. And secondly, scale is incredibly important for all these diseases, crohn's disease, breast cancer, schizophrenia and so forth. If you don't have tens of thousands of genomic sequences, you really do not get enough signal.
03:05:01.406 - 03:05:49.794, Speaker A: So scale in training is important, privacy is important. This drew a lot of people to work on this in the two models that I mentioned before. So in the multiparty computation, the genomic holders, they split the data and they give it to servers to work on. In the homomorphic encryption setting, everybody encrypts their genomic sequences, send it to a place that computes on, it encrypted, and then it gets decrypted by servers. So there have been papers from Bonnie Berger's group and MIT on the multiparty computation setting, papers by duality, which I've been involved in, in the homomorphic encryption setting. And what's wonderful about this is there's a lot of competition here. So you're taking data sets, you do computations, you report how much time it took you or what accuracy, then somebody does better than you back and forth and progress is made.
03:05:49.794 - 03:06:37.854, Speaker A: Currently, this paper is interesting because we did it oncological data of real patients rather than artificial data sets. And there's some theoretical progress in this area, which is something called interactive bootstrapping. Bootstrapping is the ability to take homomorphic encryption and take it from being able to do homomorphically simple computations to more general computation. You need something called bootstrapping, which is usually the expensive operation. Now there's new ideas of how to do interactive bootstrapping, which speed it up significantly. And it also can do join operations. So what is a join? Instead of just having lots of patients, we might have the same patient where different data centers have different columns of information about the same patient.
03:06:37.854 - 03:07:19.930, Speaker A: We can join it under encryption. We're using this Korean chiyon Kim song, came out of Korea about eight years ago or six years ago. Very efficient for real number arithmetic, which is good for machine learning. But you do have to sort of hand optimize operations that you're interested in. But more importantly, this gives you a general tool set to do things like statistical analysis of data, including survival analysis, logistic regression and all that. And it works very well and very fast. All right, so I want to say, of course, medical is not the only place some two other cases secure cyber risk aggregation measurement.
03:07:19.930 - 03:07:55.498, Speaker A: Here are the ideas there's. Lots of companies are getting attacked all the time. They don't want to tell people who attacked them or whether they were attacked, but they are interested to find out about attacks in the future. So they would like to work together and in a multiparty computation to find out what attacks are on the horizon without revealing that they were attacked. In fact, there is a consortium of doing this at MIT, which allows multiple entities to share and learn about aggregate attacks. Another thing is in the context of financial risk. So there's some group of lawmakers in the UK and Financial Services which have a consortium to detect and prevent financial fraud by working with each other.
03:07:55.498 - 03:08:06.714, Speaker A: So finance cybercrime medical data. These things are actually used. Okay, I don't know what time it is, but I'm keep going. 1 minute?
03:08:06.762 - 03:08:07.262, Speaker B: No.
03:08:07.396 - 03:09:02.954, Speaker A: Okay, I ignore I was going to say that maybe that what we said good enough wasn't good enough, because maybe we want that. Given the model, you shouldn't learn whether the point was in the training set or not. So that's like more differential privacy, like and in fact, there's a work by Vitali Feldman and Cynthia Dork where they take the definition of differential privacy and adopt it to machine learning. But if we think about us, what it means, it would be really sort of thinking of it as black box is to first train your h under encryption or to add a differential privacy mechanism to this and then train under encryption. And we know that the challenge here is how to get utility versus privacy. Lots of wonderful ideas out there. This particular picture is from a paper called Pate where they show that you should train a bunch of models and then you do a histogram about the results.
03:09:02.954 - 03:09:47.646, Speaker A: Let's say if each one of these models does some sort of predicts a different value to the question and you do it in such a way, if you do it this way you can add noise rather than locally or globally in the intermediate phases and you get actually quite a good trade off between accuracy and privacy. All right, hardware. So intel nvidia now is with GPUs. Intel with SGXs. There's the following promise don't worry about it. In some sense there is hardware here that you'll give it your data and everything will be done inside of the hardware. So you'll give it encrypted, the hardware will decrypt it.
03:09:47.646 - 03:10:13.922, Speaker A: You don't need homomorphic encryption. Everything will be done. The training algorithm will run and then you get the output. And in the second box there is even attestation where they can kind of prove to you that they did things properly. So this is a big promise. The promise, if you look at the blogs, is fine tuning MPC, training everything out there in the cloud with these chips. In some sense that would be wonderful.
03:10:13.922 - 03:11:11.878, Speaker A: But the verdict is out whether side channel attacks can be launched. How much does this really meet once you meet the promise, once you start having different users using the same hardware and so forth and so forth I know I am probably beyond my time, but I got to say a few more words. So I only talked about adversary in training time. Who is the adversary in verification? So first of all I want to say that I think verification is extremely important much more than anything else that we, at least in an academic setting can contribute to. And the goal is to verify the properties cheaply, obviously not going to be retrained. We don't have the compute, we don't have the data. We want to be able to verify properties using fewer data samples, lower quality data possibly and less time and memory and hopefully even with black box without open access to the machines.
03:11:11.878 - 03:11:48.818, Speaker A: And what do we want to verify? Quality. We're guaranteed a certain quality robustness. And these days people are talking about restrict about data usage. Was it trained on copyright material? Was it trained on hate speech? Was it trained on things that shouldn't be trained in? Can we verify what the data sources were? And verification on one hand it seems like great verifiable computing. We have so many methods, interactive proofs and PCPs from going from all the way to blockchains. These methods are extremely efficient. Aren't we really well set for verification? And the answer is well, maybe.
03:11:48.818 - 03:12:14.106, Speaker A: But our case is different because the inputs here are not well known. There are samples from a distribution. It's not like we have the whole blockchain to verify on the computation is randomized in parallel. So we are assigning it to different units. We don't necessarily have a knowledge about which unit comes first. And most importantly, the ground truth is not prespecified. If we want to verify a smart contract or verify a digital signature, we know what the ground truth is.
03:12:14.106 - 03:12:58.998, Speaker A: We know what it means to be a valid digital signature. Here, the ground truth itself is not specified and it's based on the data. So this makes it much harder. What I was going to talk to, but I will respect my time somewhat, is that verification in some context is possible. But there are some contexts where we can prove that it is not possible. And what I was going to show is the result of this paper on planting undetectable backdoors and machine learning models that shows that there is a way if you don't trust the trainer, okay, let's say you are a client, you sent all your data to some machine learning company to train for you. They can put in backdoors in a very strong sense that you will not be able to detect.
03:12:58.998 - 03:13:35.554, Speaker A: And that will enable them in the future to take whatever input they want and modify it very slightly so that they will get favorable outcomes. The task of getting around this is impossible to solve. Now, impossibility is not something we like in computer science in general, but we does point to ways to go around it. So there's some takeaways like how you shouldn't train, that you should always post process models that you get. You shouldn't just use them as they are. You should specify what kind of post processing and more. And obviously, I'm speeding ahead to my last slide.
03:13:35.554 - 03:14:20.600, Speaker A: This is a conference on I was going to talk to you a little bit about backdoors or a lot, but this is a workshop on generative models. I think mostly it's a lot of attention to it. So that's kind of the elephant in the room or in the safari. And there are lots and lots of beautiful challenges. Beautiful from a point of view of a researcher, maybe not so beautiful from the point of view of a practitioner who's trying to sell these models on how to verify your data sources, how to distinguish facts from fiction, how to detect watermarking, ensure plurality of opinions, prevent and estimate black swan events. So it's a wide open field and I think it's a wonderful time to be a researcher, this field. Thank you.
03:14:20.600 - 03:15:04.370, Speaker A: Thanks Shafi, for an amazing talk. Given the interest of time, we won't have time for questions, but Shafi will be around after the session for more questions. So we are going to start the next talk and before that so after the next talk, we are going to have the first lightning talk session. And so given interest of time for the speakers for the lightning talk session. Please come lined up in front since the next talk will be short. And then we'll go straight into the lightning talk session. Yeah, thanks.
03:15:04.370 - 03:15:13.110, Speaker A: So next, let me introduce Alan Halawe is director at Meta and he's going to tell us about personalization.
03:15:24.110 - 03:16:10.006, Speaker B: Okay, so as you can see from the title of my talk, this whole issue of Centralization, I'm a little bit confused about when I talk about personal data. Does this do it? No. Okay, so since I know dawn is going to kick me off from the stage very quickly. So the TLDR of this talk is I'm going to tell you about personal information, research on personal data, and hopefully I'll motivate you for some of the agenda that we're pursuing. But how to do this in a private way, find the right training data, train it in a responsible way, and how to store all this data in a responsible way. That's what I'm hoping everybody here will get up and help me do later. Okay? So I'm not going to go into any of that.
03:16:10.006 - 03:16:56.790, Speaker B: Okay, so why care about personal digital data? One is because we're creating digital data all the time. We navigate, we take photos, we buy stuff online, we listen to Spotify. So there's a lot of attention on what are corporations doing with our personal data. And that's a really, really important question. The question I want to ask here is what are we doing with our personal data? Are we actually leveraging it in the best way possible for, say, our productivity or more generally our well being? So ideas on managing personal data date as long as van of our Bush in 1945. There have been a bunch of projects along the way. My life bits at MSR in the 90s was one prime example, but our life is even more digital now, right? I mean, smart glasses are coming.
03:16:56.790 - 03:17:30.178, Speaker B: I can tell you that because my company is one of the ones making it. And so you're going to have always on perception, everything you're going to do is going to be, if you want, is going to be recorded. We don't know where to store it, though. And personal AI assistance with the capabilities of language models are also coming. So these things are going to enable us to actually do things that two, three years ago were not imaginable. So I'm on a tear now to delegate as much of the Minutiae in my life to GPT. I'm trying to figure out how to do this so I can focus on doing what I really want to do.
03:17:30.178 - 03:17:57.446, Speaker B: Okay, so this is the vision. The vision is, wow, that's a long time. So the vision is you have a lot of data sources that you're creating data all the time, right? You're taking photos, you're navigating, you're listening to Spotify. You have glasses, you're buying stuff. Okay. All this data is sitting on a bunch of servers. So the vision is first of all, let's import all this data, or parts of this data, the metadata of this, into what we call a timeline.
03:17:57.446 - 03:18:43.160, Speaker B: This is the thing that fuses all the information about you and therefore understands you, understands the experiences that you've had in life as well as possible. Okay? So that's going to be a pretty big database with a lot of stuff about you. You're not going to take that everywhere. What you can create from that is you can create smaller summaries of you that you can take with you to places. So, for example, I took all the books that I read that I created using my timeline, and I created a summary of all the books that I read. And then I took that to Chat GPT, and I got much better recommendations for books than any Amazon or any other service before. So you can take these summaries and personalize them, use them to personalize your interactions with Chatbots, for example.
03:18:43.160 - 03:19:02.542, Speaker B: So we can do this. Thank you for the Europeans, for GDPR. We can now take all this data and import this and start building our timelines. In fact, we did this. It's called Timeline builder. You can go and find it on GitHub. And this is, for example, my timeline, where this is my April 15, 2019, where I don't know what I did there.
03:19:02.542 - 03:19:17.294, Speaker B: I enjoyed coffee. I went to it for a run in the morning. This is my life. Okay? So this is what it looks like. My timeline looks like a bunch of events in my life. Some are much smaller, smaller. Some are bigger.
03:19:17.294 - 03:20:04.654, Speaker B: But the point is, now there's much more about my personal context that you can bring to bear on anything that I need to do in getting recommendations or planning. Okay? Now, it's really important to point that using data from multiple sources is the key here. The more data that you have and the more data that complements each other, the more you're going to be able to get insights about your life. So for just as a simple example, I read books from four different sources, my Audible and my local library and stuff like that. So bringing them all in really gives a complete picture of what I'm reading. Each one of them does not have all the data about me. Okay, so there are a bunch of technical challenges that we need to solve.
03:20:04.654 - 03:20:56.900, Speaker B: For example, understanding the data that you get from these sensors from these apps is pretty raw, right? You get like 5000 photos from Tokyo, but you don't know the fact that you went on a trip for seven days and what you actually did is something that you need to infer using AI techniques. How do you answer questions from these timelines? Will the LLM just do it all? Not really. If you ask a database person like me, we still have a lot to contribute here because the LLMs can't really understand this kind of context with the right semantic level that you can't question. Is again, and now I'm throwing it back to you, is how do we train models that are able to answer questions on timelines? How do we train models that enable you to take these timelines and give me recommendations, give me plans for what I want to do in life that are much more tailored to my needs? So that's it. You don't even have to throw me out of here. Okay, I'm done.
03:21:05.030 - 03:21:15.420, Speaker A: Great. Thanks a lot for a great talk. So next we are going to have our lightning talk, the first Lightning talk session. So, can we start?
03:21:23.870 - 03:21:59.698, Speaker B: Hi, everyone. Okay. Hi everyone. Today I'm presenting Ezom, a simple and scalable training framework for large language models. If you want to train large language models and you look at existing frameworks, you see that they're falling to two categories. A small framework, a minimal one that is very easy to understand and use but often not very scalable, and a scalable one that's often very too complicated to configure and run. In this work, we combine the scalability and minimality into one single framework, which is our easylm open source framework for training large language models.
03:21:59.698 - 03:22:38.162, Speaker B: It's both scalable and also very easy to use and easy to understand. So how do we achieve this? The idea is very simple. So you write the training and your model code for a single GPU. And you leverage machine learning compilers to automatically scale up your training to hundreds of GPUs or TPUs. In this way, you don't have to worry about the complicated scaling mechanism. You write your code just as if you're selling a single laptop and it works directly to a cluster. Using Ezlm, we can fine tune our Llama model to be a dialogue agent on Agpus within 6 hours with less than $100 cost on cloud.
03:22:38.162 - 03:23:16.590, Speaker B: And if you're courageous, you can also use Ezlm to scale up pretraining. With our Open Llama model, we scale up our pre training of three B, seven B and 13 billion series of models to hundreds of TPU before sun cloud, and it achieves comparable performance to the original Llama. And we released it under Permissive license. If you're interested in this project, check out our GitHub. Hello everyone. I'm Len Ming, a fifth year PhD student at UC Berkeley. In this talk I'm going to tell you about Vcunia, a high quality open source chart assistant.
03:23:16.590 - 03:23:52.134, Speaker B: So next, the motivation is very simple because overall charge should be amazing, but its ways and chin details are all not open. So you cannot do a lot of things with its prompting only interface. And our idea is to use Open recipe to build a model that can talk like a chat GBT. Thanks. So, our idea is very simple. We just combine the best open data with the best Open based model. So here is our workflow.
03:23:52.134 - 03:24:43.922, Speaker B: We collect data from Sharegpt.com, which is a website where users can share their interesting charge GD conversations. So actually in the workflow, the training part is the most easy one because basic standard instruction fine tuning and the critical part is the data and the evaluation. So we collect data from share GBD and we evaluate with GBT Four. So here is the evaluation result we can see according to GBD Four, vicunia performs much better than prior models and it's for this kind of easy conversation task. It can reach more than 90% of charge b quality. And our model leads it to heated debate around open source model versus closed models.
03:24:43.922 - 03:24:54.720, Speaker B: And I think you already had similar conversations this morning earlier. If you want to play with Vicunia and our demos, here are some useful links. Thank you.
03:25:04.520 - 03:25:58.600, Speaker A: Hi everyone, I'm Yin Shung. I will introduce the flagship which is a framework that achieves a high throughput generative inference of larger X models with a single GPU. The motivation behind this work is twofold. The first, it is motivated by making larger models more accessible so that we can enable running larger models on commodity GPUs. And second, it is uniquely motivated by the emergence by the emerging demand of latency insensitive tasks with batched processing instead of latency. In addition to latency sensitive tasks like chatbot arms are also applied to throughput oriented tasks with offline tasks. Flexion achieves a significantly higher throughput than dip speed and hanging phase accelerate when resources limited.
03:25:58.600 - 03:26:23.010, Speaker A: From the figure we can see that the baselines can only achieve 0.1 token per second and the flexion inflection we propose a novel and efficient offloading strategies that achieves 69 times higher support and if combined with quantization, it can even achieve better performance achieve the acceptable support with one token per second. Thank you.
03:26:34.900 - 03:27:33.480, Speaker B: Hello everyone, I'm Suan from USA Berkeley. I'm going to introduce our recent workm is a very efficient high throughput framework for large number of serving. The key idea behind VM is that currently for more efficient serving you need to batch sequences but because of you cannot predict how many tokens you are going to generate. You also face some efficient memory utilization like memory fragmentation. We make use of the classic idea of page virtual memory in commuter systems and during generation we dynamically allocate pages to put the key value vectors of large joints model. And for doing our evaluation we can achieve up to 24 times higher throughput compared to the Hacking transformer library and also 3.5 times faster than TGI.
03:27:33.480 - 03:28:36.800, Speaker B: Also, VM is also the hero behind the famous LM States Wakuna and Chatbot arena platform. We sold over 20 models over the past three months and it can achieve up to 30 times higher throughput than our orange invitation. Thank you. Okay, hello everyone, I'm Shishir and I'm a fifth year PhD student at Berkeley and today I'll be talking about Gorilla. So, Gorilla is an LLM that's been trained to write and invoke API calls and we have been able to do this by two innovation techniques. One is retrieval of our training and the second, which I think is an important question for all of us today is measuring hallucination. Now, rather than drill you down with the technical details of how we do gorilla, let me actually share with you a vision which is also a lighter topic prelaunch.
03:28:36.800 - 03:29:22.490, Speaker B: So, okay, today you have one too many LLMs, right? If you look at the world right now, you have the person in the center and you talk to these LLMs and through prompting you have a bevy of tools, langchain, et cetera. You can wrap it up, but fundamentally you're at the center of everything. You talk to the LLM through prompting, get a response. This could be a short or multiple interactions but the burden lies on you to now take this response that you get from the LLM and then act upon the rest of the world, right? This could be as simple as get some code, execute it, so on and so forth. What we envision is the flip of this where the developer or the individual who's invoking the LLM is on the far left. So you talk to the LLM, which in this case will be gorilla. And the LLM is now going to read State of the World and bring about some changes in the world.
03:29:22.490 - 03:29:57.600, Speaker B: So, to ground this in a clear example, imagine we want 800 GPUs in East US. A common ask for everyone. The way you do this is you're going to invoke gorilla and gorilla is going to now go ahead, get your response and then actually go and execute the API call on the world. In this case hitting Azure or AWS or GCP and then return the respected SSH to you so you can log in. Now, needless to say, we think of LLM as a tools and tool. Need to talk to every other tool to get something done. And in computer science, the way tools talk to each other is through API calls.
03:29:57.600 - 03:30:16.430, Speaker B: So that's how we build corella. Initially, we support about 5000 APIs. Your favorites including Kubernetes, AWS, GCP, Linux, et cetera. But fundamentally you can talk to Gola in natural language and then get it to perform some act and then return the response to you. We have multiple modalities how you can invoke gorilla. For more information, please check out our website. Thank you.
03:30:16.430 - 03:30:59.712, Speaker B: Hey everyone, this is Lintia. So today I want to talk about frugal GPT. So what is the problem trying to solve? Right, almost all applications nowadays are leveraging OMS for better performance but it comes with a cost. Suppose for instance, you are building a customer service for a medium sized company using GBD Four. The monthly cost could be roughly like $20,000 and even more. The environmental cost would also be very high due to the huge amount of carbon emissions. Also, the complexity of models gives you a pretty high latency.
03:30:59.712 - 03:31:53.030, Speaker B: So we asked a question like how do we actually build a better service with lower cost? So, in this project, we propose the idea of Frugal GBT, which essentially build a LM cascade that triggers the data to the cheap and fast services whenever possible and only use expensive and slow models whenever needed. So technically, we study how we actually build a good strategy to decide which model to put in the LM cascade and how and when to defer to the next layer of cascade. So we actually see flow GBD for a range of tasks, and we observe a great performance. For instance, you can get a 1.5% action improvement while only incurred 20% of the cost compared to four. So for more information, feel free to scan the QR code and check out our website. Thanks you.
03:31:53.030 - 03:32:47.800, Speaker B: Hello everyone. I'm Waylin. I'm a fourth year PhD student at Berkeley. So this lightning talk, I'm going to talk about the topic where can we really trust GPT Four judgment? So, there's a growing interest recently in using strong LM like GPT Four Cloud as a judge to replace some of the human tasks. For example, LM evaluation starting from Vikuna Apaka evo and also OpenAIS Evos. They use this kind of model based evaluation and also some other tests, like people start exploring to use chat GPD as a crowd worker for this data labeling task. And also more recently, content moderations.
03:32:47.800 - 03:33:46.730, Speaker B: But in our paper study, we found that there are some limitations and biases when using GBD Four as an evaluator as well as other LMS. So it might have some positional bias, so in particular varying the first or second answers depending on which LM you use. And also it has some verbosity biases, varying even for repetitive answers. And also it has limitations in grading math and reasoning questions, in particular when it could be fooled by the model answer itself. So in this work, we proposed some solutions to mitigate these challenges. And we've done human studies and collecting human preference data. And these data are publicly available at the GitHub repo, so feel free to take a look.
03:33:46.730 - 03:34:31.282, Speaker B: Hello, I'm Chinping from UC Berkeley. It's my pleasure to introduce our work here, unified all in one Ferret learning platform to Unify open source frameworks. So Ferretiling has been a very promising research direction. Different clients collaborating in training a machine learning model without transferring the local data. So there have been many Ferretiliarning frameworks. There have been many Ferretilining frameworks. For example, Google developed TensorFlow Ferrari, so they are developed by different industries and different universities.
03:34:31.282 - 03:35:12.990, Speaker B: But as user, how do they select the Ferrotiling framework to use? How do they compare different Ferrotiling frameworks? It's a very challenging task. So here we develop Unifat, which is a platform so that you can easily use different Firewall training frameworks and you can easily run and compare different Fire retailing frameworks. So first, by summarizing the properties of different Ferretiling frameworks. We provide a decentry to recommend the appropriate Ferrotune framework for your user case. Then, by specifying the parameters and the framework you want to use, we can generate a configure file. And by inputting the configure file into our system, we will run the different vibration frameworks on different nodes. And finally, we will output the login.
03:35:12.990 - 03:36:02.632, Speaker B: And through the login system, users can easily extract different metrics such as model performance and assistant performance, so that they can compare different frameworks. You can scan the QR code to assess our website and code. That's all for my presentation. Thank you. Hi, how's it going? My name is Eric Wallace from here at Berkeley, and today what I want to talk about is why it's hard to close the gap from open source language models to closed source language models. And so, in particular, a really common proposal that a lot of people are doing these days is to say, let's say I have some really good model like Chachi BT. What I'll do is I'll collect a big set of inputs and outputs for that model, and then I'll just train some open source system like Llama, for instance, to kind of replicate or imitate the predictions from that model.
03:36:02.632 - 03:36:50.776, Speaker B: And that's kind of the idea behind a lot of the systems people talked about today, whether it be Stanford's Alpaca model, Berkeley's Vicuna model, Berkeley's Kuala model, or other models like this. The main point of our paper is to kind of critically analyze whether this actually works well. And so, in particular, a lot of these results claim that these systems work really well. So, for instance, here's a bunch of figures from various papers showing that, hey, imitation models and proprietary models actually look quite similar in their results. But in practice, what ends up happening is that a lot of times what imitation does is it causes the models to have similar style in their outputs, but actually, the factuality or content of the models can be quite different. And to just show one qualitative example of that, here's a complex question that I'm asking to chatchibt into an open source model. It says, how does actor critic improve over reinforce? So it's a question about reinforcement learning.
03:36:50.776 - 03:37:21.410, Speaker B: What I've highlighted in green is the fact that Chachibt is actually really accurate on this question. In red, I'm highlighting that open source models are completely inaccurate for this question. But on the surface, the style of their two responses is actually quite similar. They both output these kind of, like, long list responses with good summaries of what's going on. And so, in general, what you can take away is most evaluations that are focused on kind of stylistic elements will actually score these two systems as highly similar. But any type of factuality based metrics will identify key differences in these models. Thanks.
03:37:21.410 - 03:37:51.212, Speaker B: All right. Hello, everyone. Thanks for your patience. I know your lunch is waiting for you. My name is Vivek Nayar. I'm a PhD candidate here at Berkeley, advised by Professor Song. Don asked me to just say a few words about some of the research we've been doing at Berkeley RDI on VR security and privacy.
03:37:51.212 - 03:38:52.728, Speaker B: So it turns out that we've known at least since the 1970s that the way you move is uniquely identifiable. Just by attaching a few markers to the human body and watching you move around, we can generally identify exactly who you are within just a few minutes of motion. And so there we go. So what a VR device collects is very similar to the data we've known for a long time, is uniquely identifiable to each user. It's just points of markers attached to your body moving around in 3D space. And we've known for several decades that this is uniquely identifiable by each person. So recently we did a study in Usenic Security where we took over 50,000 users, took motion capture data from their VR devices and used it to uniquely identify people with over 95% accuracy.
03:38:52.728 - 03:39:39.550, Speaker B: This means that when you step into a VR device and just move your head and hands to interact with the device, you're sharing an identifiable feature that is comparable to a fingerprint scan or a facial scan in terms of how unique it is to you. And the takeaway from this is that there is currently no way to use an extended reality device anonymously. So just by using an XR application and interacting with it with your head and hands, the application knows exactly who you are within a few seconds of usage. So for anyone considering extended reality for sensitive applications like healthcare, this needs to be something that's considered that currently users can't interact with your application without revealing their identity. If you want to learn more about this research, RDI Berkeley.edu Metaverse. Thank you.
03:39:45.040 - 03:40:17.540, Speaker A: Thanks, everyone, for being here for such an exciting morning session. So now we are going to take a lunch break. So we have some lunches here. And also, if people want to go out, there are a lot of restaurants around here as well. And we are going to restart here again at 01:30 P.m.. We have a very exciting afternoon session starting with lunchin, llama index and droplets. So, looking forward for everyone for the afternoon session.
03:40:17.540 - 04:30:54.460, Speaker A: The food here, we hope that everyone can just spread out the food, so everyone should just take one lunchbox or one bowl and so on. Also, you are welcome to go off campus to the restaurant around here as well. Thank you. Okay, great. Thanks everyone. We've had a really exciting morning session, and the afternoon session will also be really exciting. So now I can start.
04:30:54.460 - 04:31:10.570, Speaker A: Mate professor at UC Berkeley who recently moved from Stanford. Gobert. So Mate will be the session chair for this session. Thank you.
04:31:17.840 - 04:31:51.444, Speaker B: All right. Thanks, Don. Welcome, everyone. So we've got three exciting speakers here to talk about tools for working with LLMs and the open source LLM ecosystem. So, without further ado, we're going to start with Harrison Chase, the creator of Langchain, one of the most popular frameworks for LLM applications today. Cool. Thank you guys for having me.
04:31:51.444 - 04:32:57.256, Speaker B: Really excited to be here. My name is Harrison Chase, CEO and co founder of Langchain, which is a framework for building context aware reasoning applications, of which an LLM is a key critical part. Today I'm going to talk about something a bit more specific, though, which is about tuning an application to do question answering over tabular data. And so I'll start by talking about what Langchain is and then also what Lang Smith is, which is another tool that we've built. But I really wanted to take this opportunity to deep dive on a really applied topic, because where we sit in the ecosystem, we deal with a lot of application builders who are looking to bring applications into production. And so I wanted to take this time to talk about a deep dive on a particular one, because one of the trends that we've noticed in which I'll touch on is that basically as you go to bring your systems into production, there's a lot of difficulties. And a lot of these difficulties need to be kind of like stymied by focusing on particular kind of parts of your application and really customizing it for your use case.
04:32:57.256 - 04:33:47.784, Speaker B: So more generally, Langtrain, as I mentioned, is a framework, a generic framework, very horizontal for building LLM applications. We have a bunch of modular building blocks, and so these are all the different components that you might need. Some of these we have integrations with. So we don't have our own LLM implementations, we don't have our own vector store implementations, but we integrate with all the ones out there. Others of these building blocks we do have our own kind of implementations for. So these would include things like document loaders, text splitters, which are really important for bringing the correct context to your LLM application, as well as concepts like agents and tools, which I'll touch on a little bit more lately. As well as other things like memory and retrieval algorithms that we kind of build up inside the so we have these modular components and then we also have end to end chains and question answering over tabular data.
04:33:47.784 - 04:34:20.208, Speaker B: Is one of these end to end chains. I'm also going to be talking a bunch about linksmith during this presentation. So what is linksmith? Linksmith is a platform for managing these types of applications. And so there's a few different components that we've built into this. Part of it is around debugging, logging, monitoring. One of the kind of key issues and challenges that we saw when people were building these applications is the need for this type of observability, particularly as they get more and more complex. Another part of Langsmith is around testing, evaluation and data sets.
04:34:20.208 - 04:34:52.652, Speaker B: As I'll talk about when we think about building this question answering application that plays a big part. And the key part here is that all of that's connected into this one linksmith platform. With those overviews out of the way, let's move on to the more interesting stuff, which is the specific problem that we decided to focus. So for context, we wrote a blog post about this about a week and a half ago. There's a lot more detail on it there that you should go check out. One of the end time use cases that we have is question answering over tabular data. A lot of the focus so far has been question answer over unstructured data.
04:34:52.652 - 04:35:27.636, Speaker B: This has been unlocked that LLMs are really good at dealing with this type of data. So that makes a lot of sense. But there's also a lot of data that's in Tabular format, whether it's in CSVs or in SQL tables. And so for this talk, I'll focus specifically on CSVs, although a lot of the learnings can be applied generically to tabular data of any. And this type of application is actually a lot harder to build than the unstructured ones. There's a lot of subtleties and difficulties that go into it. And so one of the things that people had been asking for for a while was better support in Langchain for these types of applications.
04:35:27.636 - 04:36:23.580, Speaker B: And so we had some spare time. We decided we would try to focus on this and this kind of like Led. And another reason that we wanted to focus on this is kind of back to this idea that as you go to bring LLM applications into production, generic, kind of like frameworks and link chain is a very generic, very horizontal framework, there becomes a need to kind of customize them and deep dive with them, whether that be prompts, whether that be some of the tools or connections that you give, whether that even be some of the flows of the cognitive architecture of your application. And so we wanted to pick kind of like an application where we knew we weren't amazing at and we wanted to go in and see what it would take to improve it. So the kind of outline of what we did first, we collected some data. This is kind of like an interesting part of building applications with LLMs compared to more traditional machine learning and data science. You might not start with a data set.
04:36:23.580 - 04:36:45.396, Speaker B: You start with an idea. And LLMs are fantastic zero shot workers and they can generally do a pretty good job of it. But then you get to this place where you want to maybe evaluate it or see how it's doing, and you don't have this data set. So step one, come up with this data set. Step two, define evaluation. I can talk a little bit more about this later on, but for a lot of these natural language applications, this is actually quite difficult to do. Then.
04:36:45.396 - 04:37:25.356, Speaker B: We wanted to benchmark our current solution. Once we had these two things, we wanted to do a bunch of experiments, come up with some new solutions based on kind of like what we saw was going wrong in the data and kind of like repeat until satisfied. And I think this is a generic enough kind of workflow that's worth kind of like putting out in these steps to collect the data. There's a few different things that we considered. You could generate data automatically with OMS, they're pretty good at generating synthetic data. But we wanted to rely a bit more on real world questions that would be asked because a big part of this was also we didn't know what types of questions people wanted to ask of tabular data. There's a lot of different questions people could want to ask, but we didn't know what they did.
04:37:25.356 - 04:38:05.912, Speaker B: And so what we did, very simple, we just put out a nice little STREAMLET application. We used the classic Titanic data set to do question answering over it. And then we added a little feedback button, thumbs up, thumbs down, that people could kind of rate whether it was answering the question correctly. This was all kind of like logged into Langsmith and we were able to accumulate kind of like a data set of questions that people actually asked along with feedback on how well our current application did. The second thing we did was define some type of evaluator here. We relied on LLMs to actually assist with a lot of the evaluation. And the reason we did this is because a lot of the answers that we saw people expected were in a natural language format.
04:38:05.912 - 04:39:02.968, Speaker B: And so we wanted to compare basically two strings to each other and see if they were semantically the same. And so we relied on an LLM GPT four to be precise, to do this, the old solution. And I put out kind of like what we did with CSV and SQL here. I'll try not to go into too much detail here because I'm running out of a bit of time, but basically the idea was that it was very focused on the analytics of questions. So we gave it access to tools like running SQL commands, running Pandas code, that would allow it to answer questions like what was the mean of this column? Or something like that. But one of the things that we observed is there was a lot of questions that related to some of the text in the data that this solution wasn't particularly good at answering. So some of the pain points that we saw in particular there were some multi hop questions that relied on basically looking up one thing and then looking up another thing.
04:39:02.968 - 04:39:51.760, Speaker B: And so some of the solutions that didn't have that multi hop baked in didn't fare very well. Questions about text columns, even simple ones like name, were not handled extremely well. And then you compound those and you get multi hop questions involving text columns, and it fared really badly. Side note, one of the biggest things that we did to improve the performance was actually just fix how some of the data was formatted. This was a fun little bug with Pandas where they have like, a max columns that they display that was actually different from our local development and the streamlit thing which was running in the cloud. And we didn't discover this until we used lengthsmith to see what exactly was going on under the hood, which I think speaks to the importance of kind of like, observing the data that's flowing through these systems. The solution that we came up with, and happy to talk in depth about this more, but basically we provided an agent with multiple tools.
04:39:51.760 - 04:40:37.580, Speaker B: It would have one tool that was really good at kind of the unstructured data, one tool that was really good at the structured data, and then we'd kind of like, let it use them multiple times as it saw fit. And this kind of solved the multi hop nature as well as not being able to answer kind of like the questions about text. And that's all I've got. Thank you guys, and excited to be here. All right, thanks a lot, Harrison. So next up, we've got Jerry Liu, who's the CEO and founder of Llama Index, to talk about that. Llama Index is also another very popular open source project in the community, and Jerry will tell you all about it.
04:40:37.580 - 04:41:05.670, Speaker B: Hey, guys, how do I switch slides? Okay. Hey, everyone. My name is Jerry. I'm CEO, co founder of Llama Index. Today's talk is on building Rag with llama Index. Usually these talks go for about like, 40 minutes, and I'll try my best to condense this down into very practical tips for eight minutes worth of content. Next slide.
04:41:05.670 - 04:41:51.218, Speaker B: Do I click this? Okay. What is Rag? Retrieval? Augmented generation. So LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They're pre trained on large amounts of publicly available data. And I think pretty much every LLM developer these days kind of ask themselves, how do you actually augment LLMs with your own private sources of data? So raw files, APIs, vector stores, databases, anywhere where data lives. Like, how do you actually feed that into a language model? So Retrieval Augmentation is basically that method where you take a pretrained model and then you just figure out how to stuff data into the prompt and you basically create a data pipeline out of it. And so Llama Index is a data framework at a very, very distilled level for doing that.
04:41:51.218 - 04:42:35.140, Speaker B: And so it consists of two main components. One is like, data management, and the other is kind of data querying. And so it offers components across this data lifecycle to ingest data from your data sources, index them, and figure out basically at a very distilled level how to stuff that into the prompt. So you can build some sort of software system, whether it's a chatbot agent or any other application that you can create with LLMs. I think to get started with Llama index, it's pretty simple. You can basically build like question answering over any source of data in about three lines of code. Just load in your PDFs, index them, and then through some of our abstractions, you can figure out how to do retrieval and augmentation in about three lines of code and start asking questions over your data.
04:42:35.140 - 04:43:30.290, Speaker B: So now kind of taking a step back and looking more broadly at the LLM app use cases there's kind of like a two by two matrix and on the horizontal axis it's basically from more passive applications like Summarization. Translation. Kind of like more automated insight extraction to more interactive cases. Like actually doing question answering chat over your documents or actually having a conversation with an agent that can interact with a bunch of tools. And then on the vertical axis there's everything from kind of like more simple reasoning where it's basically like a prompt or more constrained workflow, to more complex multistep reasoning like an agent that can do stuff like chain of thought prompting. And I think our overall thesis is that data management and orchestration becomes more and more of a problem the more you go to the right and also to the bottom. So the more interactive the use cases are, and also the more complex these use cases are, the more you have to worry about your data pipeline and be careful about setting that up for production.
04:43:30.290 - 04:44:20.078, Speaker B: So for those of you who are familiar with this, you might have already done this with tools like Lamindex or Langchain. But the high level idea of setting up some sort of retrieval augmented stack for building like a QA system is as follows you start off with some source documents, you split it up into a bunch of chunks. You put that into a vector database and then during query time, you retrieve the top k most relevant chunks from your vector database. Right? So this is using like embedding similarity, this is not using the LLM. After you retrieve the subset of relevant chunks from your vector database, you put it into the input context of the LLM to actually answer your question or synthesize any sort of tasks that you want to answer. So this basically just describes that in a bit more detail. You split up some documents into even chunks.
04:44:20.078 - 04:44:58.090, Speaker B: Each chunk does not contain parent context. All chunks are stored in kind of like the same collection in a vector database. And then during query time, you find the top k most similar chunks from the vector database and then you plug it into the Lln. You just figure out a way to stuff this into the language model. I think as we move from prototyping phase into actually building more production grade robust Rag systems. A lot of times people have issues actually figuring out how to debug and evaluate Rag systems and how to actually improve this for better performance. And actually when Rag fails, one of the most common reasons is bad retrieval.
04:44:58.090 - 04:45:57.274, Speaker B: And so if your retrieved results are bad and there's a lot of factors that affect the retrieval quality and I'll talk about that in just a bit, but there's no way the LLM can actually synthesize a proper response without actually hallucinating. You could have the most advanced language model in the whole world, but if you don't actually have the proper context, you're not going to be able to synthesize the right response. So right now kind of thinking we're basically doing top K embedding lookup over raw text chunks. And so just some very basic downsides of that is that each text chunk does not have awareness of parent context related context. You kind of assume that the query just does top k and doesn't really take advantage of the structure of the documents and the data could be easily updating, it could be redundant, it could change over time and then you need to figure out a way to actually deal with that in a production setting. I'll skip that for now. So high level lesson, the high level takeaway is if you really want to iterate on retrieval augmented generation, improve the way you define data and state and not just the retrieval algorithm.
04:45:57.274 - 04:46:44.462, Speaker B: I have about three minutes left and so I probably don't have time to dig too deeply into any one of these components, but I'll probably share the slides publicly as well and I'll talk about it at a high level. But here's basically just some general data tips and tricks for better performing retrieval augmented generation. And this could be relevant, especially you're building LM applications. So one aspect is metadata. One of the reasons embedding retrieval over raw text chunks fails is that these chunks are just raw splits of the text and that by themselves they might not actually contain enough context to match the query embedding. And so a common strategy these days is, oh, maybe insert a summary of the overall document as metadata into the chunk itself or add any other sort of structure annotations that could be helpful. This will help both during embedding time and also potentially during synthesis time too.
04:46:44.462 - 04:47:25.290, Speaker B: The other aspect here is actually defining relationships between nodes within your data. Once you actually define that state, you can have better retrieval performance, sorry, you can have more complex retrieval algorithms over that state because then you can not only just do kind of like top k lookup, you can actually explore the links in this graph to actually retrieve additional context to better aid the LLM in synthesizing response. Skip this for now. How do you actually get metadata? You could define it yourself sometimes that takes a lot of time. You can also try using LMS for automatic metadata extraction. You can do this in a variety of ways. You can do structured data extraction annotation, you can use traditional models like Ner.
04:47:25.290 - 04:48:08.986, Speaker B: You can also basically just try using the LM to hallucinate questions and summaries. These are all things that we're exploring right now. Another pretty key concept that is very interesting is just like you kind of want to think about the embedding representation of your data. One trick here that I think Max from Citai actually brought up during one of our webinars is that decoupling the embedding from the raw text chunk can actually help you have better retrieval performance over your data. And then therefore, this actually allows you to retrieve more relevant chunks and then again have better synthesis performance. So for instance, let's just take an email. If you have a bunch of filler words in that email like, oh yeah, looking forward, great to hear from you best.
04:48:08.986 - 04:48:42.950, Speaker B: That's all going to bias your embedding representation for a pretrained embedding model. But if you use for instance, like a fine tuned embedding model, or you actually purposely choose to not just embed the raw text, but actually certain aspects of that text are more salient, you can have better embedding representations and this directly helps you have better retrieval performance. Cool. Next piece. Organize your data from our structure retrieval. Add some tags, add some metadata vector databases also support metadata filtering these days. And so this is an extra layer that can help you have more structure retrieval.
04:48:42.950 - 04:49:27.360, Speaker B: And here's some links. There's no way you guys have time to actually copy this, so I'll probably just link this on Twitter or Discord. All right, thank you. All right, thanks a lot, Jay. So finally we also have someone here who's been building and open sourcing LLMs themselves. So really excited to introduce Michele Katasta who's the VP of AI at Repolit. Thank you Mate, for the intro.
04:49:27.360 - 04:50:00.328, Speaker B: Hi everyone. I'm going to do my best to keep you awake after lunch. So, as Mate mentioned, I think this is going to be a slightly different twist compared to the previous two talks, which were about what are the best open source toolkits out there to use LLMs. This talk will be about how do we use open source packages and libraries to train our own LLM with surprisingly low compute and also a very small ad count. So let's get this started. I had an animation here, but it's a PDF. So we built a code completion feature replete.
04:50:00.328 - 04:50:34.810, Speaker B: If you ever use bot Replit or GitHub copilot, you know what I'm talking about. So the AI under the hood is generating code for you based on the context. Let's see how we build this. So early May this year we released rapid code. We want to be you can find the model on again phase today and it's a bespoke code completion model trained from scratch. And it's already serving a large number of replicate users. I will share the slides like the other two speakers before on my Twitter or my LinkedIn, so that you can get all the links that I'm going to be showing you today.
04:50:34.810 - 04:51:38.376, Speaker B: What is our model and which data do we use? Back when we released it, this was the first, I call it Llama Style large language model for code. What I mean by that is the number of tokens per parameter. We were close to 200 tokens per parameter, which is a very high value, very different compared to even like Palm or GPT-3. In total, we trained on 525,000,000,000 tokens of code and rather than having a single data set of 525, we actually had a clean version of 175,000,000,000 tokens which repeated over three epochs and I will explain why we did that. Last but not least, the model supports 20 different languages. You can see the dominant one is marked down and the reason being we wanted the model to understand English to a certain extent so that's the data that we kept there for natural language understanding and everything else is to teach the model how to complete code. We used an amazing open source data set called the Stack, which has been built by the Big Code project during the past year, if I recall correctly.
04:51:38.376 - 04:52:14.500, Speaker B: So we train our pretraining data mixture on the distal version 1.2, which has been released in March 2023. And what they've done is they basically build this amazing pipeline that takes a large number of GitHub repos and then has several different steps of filtering. One of them is based on licenses, so it makes sure that there are only permissively licensed code included in the data set. Then there is also a last step that does near the duplication. As you know, you don't want to introduce duplicated data hundreds or thousands of times on LLM. So this part has already been taken care by the Stack.
04:52:14.500 - 04:53:19.228, Speaker B: What we did on our side is we selected the top 20 languages using Rapid, so we really tailored the model to our use case. And we also applied a large number of code quality heuristics to filter out the data based on quality, starting from the criteria used by the regional Codex paper by Palm at Google and we strip long content from HTML and CSS and so forth. We did the data processing on Spark, which as you know, is also available open source and we train our vocabulary using Google sentence Piece, another open source package. Let's go to one of the key insights that was mentioned before regarding the repeating the data over tree pox. So this paper has been published coincidentally just a few weeks after we released our LLM. It's a highly recommended paper to read and it basically confirms the ablation studies that we were running before we train our main model. And what we found out is that you can repeat the data a certain amount of time without seeing any decrease in performance for the model.
04:53:19.228 - 04:54:21.680, Speaker B: So I think what this paper finds is that, especially for smaller models, you can repeat high quality data up to four times, and you will see pretty much the same loss that you will get from a completely novel data set. So rather than having, let's say, 525 billions of unique code, we did it for three times, and we see exactly the same quality. So this intuition basically allows us to train to completion our model. By completion, I mean it allows us to train it to the number of tokens per parameter that we were targeting using only the permissively licensed data contained in the big code data set. And in this way, we then release our code under permissively licensed under a commercially viable license. That basically allows you to take this model and use it for any commercial use case yourself. Okay, so this was the reason why, rather than going and getting data with different licenses, we only focus on those that were basically BSD, MIT and so forth.
04:54:21.680 - 04:54:45.384, Speaker B: How do we do the training? It's a 2.7 below parameters model. We train our own customized vocabulary, 32K. So smaller than usual, which is completely focused on code. We use 256 a 100 GPUs just for three days to get the main model so you can do the math yourself. It's relatively less expensive than you might have expected. Follow a lot of the LLM best practices available today.
04:54:45.384 - 04:55:09.100, Speaker B: Flash. Attention. You got three stalk this morning, so you know what I'm talking about. Alibi, position, Embeddings, Novel, optimizer, and so forth. We did all our training runs based on an early release of the LLM Foundry. The LLM Foundry is, again, another open source library, this time released by Mosaic ML. It's a collection of LLM frameworks and best practices.
04:55:09.100 - 04:55:38.516, Speaker B: You can basically tailor your model based on everything is available here on this repo. I think there was Brian's talk earlier today about Megatron LM. I would say LLM foundry megatron LM. There are other couple of options out there, door code, the same rounds. So I recommended this because I use it myself, and we'll be working with Mosaic successfully. But there are definitely a couple of resources available out there. As a matter of fact, this has been used to train very large open source model, seven B, 30 B's by Mosaic.
04:55:38.516 - 04:56:10.004, Speaker B: These models have been downloaded millions of times already. Let's go quickly through the evaluation. This is a benchmark called Humaneval, and the idea is you have a natural language description of a task, and you ask the model to create a pattern implementation. This was the original benchmark release by OpenAI back in 2021. And you can see the numbers there, the score. Passive one means the model has one chance to generate the Python code. Then it's executed, and we test if it returns the right answer or not.
04:56:10.004 - 04:57:02.416, Speaker B: You can also find other languages down there from the Mastiple, which is basically a translation of the original humanoval bench park across 18 different languages. And here you will find the numbers of the overlap between what masterpl supports and the languages on which we train our model on and back. In the days when we released the model, it was pretty much state of the art, of course, for the size of the model we released, but it was also very competitive with other much larger models, even up to 15 billion parameters. Just one remark that is important. We did this by using the multilingual code models evaluation release bug in phase. This is another great project run by the big code team. And one remark that I have is our model has been trained only on ten languages out of the 18 supported by that benchmark.
04:57:02.416 - 04:57:40.368, Speaker B: Which means if you take the average, our model is not going to rank very high in the regional leaderboard. And the reason being we do well only basically on alpha them. Now, we didn't only train on the open source data set that I told you about, we also took our users data on the public rapple on which they work on a daily basis. And we created another clean data set of 37 billion tokens. Again, we repeated this over three epochs and we got 111,000,000,000 new tokens of code. We applied same filtering techniques. Now there is a caveat here, and you might have been seeing this on Twitter all the time.
04:57:40.368 - 04:58:15.080, Speaker B: Now that there are a lot of interesting models. People want to take Llama and want to continue pretraining the model. It turns out that when you do that, how do you set your learning rate? Do you go back to the original one? Do you warm very quickly and then you do a cosine decay? It turns out that the vast majority of times you're going to see a double descent. Our experience was we power through that we experienced a double descent in the additional pretraining to get our fine tuned model. And it turns out that we still get a better model at the end. And I'm going to show you the numbers. For those of you that are more keen on research, I'll leave a couple of links here.
04:58:15.080 - 04:58:47.776, Speaker B: There is a very recent paper that is studying this problem. There is also a very interesting hack coming from a 2021 paper on Autoscale Vision Transformers training, and it seems that the Pragmatic hack presented there is going to be working very well. So, exciting field. For those of you that found out how to fix it, please let me know. So I'm going to use it this next time quickly. Evaluation of this model. As you can see, we do pretty much better five to 10% on each language that we support, so it was definitely worth to keep fine tuning also on data coming from our users.
04:58:47.776 - 04:59:25.740, Speaker B: Last but not least, inference. It's very important for us because the model was not released for research purposes. This is actually in front of our users. So we have around 200 tokens per second at inference time on a single a 100 GPU. And we made explicit architectural choices to accomplish this. And we were able to do it by means of supporting Natively Faster Transformer and Triton server, which are two Nvidia packages that really allow you to squeeze all the compute power from the GPUs when it comes to inference. And again, here, this is still coming from the same leaderboard that I was showing you before.
04:59:25.740 - 05:00:09.420, Speaker B: Our model on the base inference code does five times lower than our tune solution, and it turns out that it's still very hard to do reliable evaluation of inference across different architectures. Okay, I would say this is all there are a lot of interesting projects that came out because we released the model open source. I think we're going to have a chance to talk about it during the panel and links are there. Happy to chat about it offline. Thanks, everyone. All right, thanks a lot. So up next, we have all the speakers for our panel, so feel free to take a seat.
05:00:09.420 - 05:00:47.096, Speaker B: Yeah, I guess I check. Yeah, I did move. Is it on? Hello. Check, check, test. All right, cool. So I have a few questions for you folks, and then we'll also take some questions from the audience. So, first of all, Jerry, Harris and Mikela, you work with lots of real app developers building applications using LLMs.
05:00:47.096 - 05:01:37.470, Speaker B: What are the most popular kinds of applications that people are building and putting in production? Yeah, I'm curious what others have to say on this, too. I think there is a range of different applications ranging from kind of more simple and constrained. So like insight extraction, like search and retrieval, like chat bots the chat over your data stuff has been very popular in the past few months to kind of fancier things like auto GBT at the other end of the extreme, to just do everything for you, agent like stuff. I think I've seen more kind of agent like stuff in the exploratory phase, and probably more stuff like search retrieval, inside extraction on the path to production. And there's probably a variety of reasons for this. Just like, one is reliability, two is like, even if it goes wrong, you don't want it to actually modify or change some state in the world. So that's what I've seen.
05:01:37.470 - 05:02:14.072, Speaker B: Yeah. I think part of the fun part about LLMs is they're good at so many different things. So we see a really wide variety of use cases, and a lot of these are listed on the Langchain documentation page. If I had to choose one or two, I'd say some sort of questions over your unstructured data sets. Probably the most common. And then another sneaky common one that we see is kind of like extraction of structured data from unstructured text, particularly among kind of like, enterprises who want to do this as like a document preprocessing step. Yeah.
05:02:14.072 - 05:02:51.524, Speaker B: Coming from own bias, apart from everything you said, which is absolutely true, I'm seeing a lot of agents that also have code execution. So that means you can generate your code and then execute, figure out if the code is correct or not, and then go through this feedback loop of debugging or like self critic with a model. And I'm seeing quite a lot of prototypes and also applications going in that direction. Makes sense. Yeah. And sort of the flip side to that. What do you each think is like, the top challenge to working with LLMs today? What would make them way better? I'm sure others will say this too.
05:02:51.524 - 05:03:35.952, Speaker B: I think one of the biggest challenges is something around evaluation, reliability, reducing hallucinations. And there's actually multiple pieces here. There's one actually seeing what's going on under the hood, and then two is actually figuring out how to develop the right metrics for evaluating. And I think the difference between maybe LLM based applications and traditional ML based applications is that a lot of times it's very task specific and kind of prompt specific. You're kind of getting this generated output given some prompt or overall system you set up. How do you evaluate the quality of this not just relative to some kind of standardized quantitative metrics, but actually relative to your task? And then to add on more complexity, you kind of have these amazing little modules that are just inference components, and then now all of a sudden, you're combining them together. You're doing like a for loop over them.
05:03:35.952 - 05:04:34.944, Speaker B: So it's not just one call, it's like ten different calls that are all chained together or kind of interact with each other in some way. So how do you actually properly evaluate both different components of a system as well as the overall system? And I think I've seen some work. There's lanesmith, there's other companies too, working in the Eval space. I still think it's probably like an open question as to how do you do this in a performant, fast, cheap and reliable way. Another piece I'll just quickly talk about is probably something around data challenges. I think as you scale up and you start having gigabytes like terabytes of data, how do you actually kind of productionize this? How do you think about data pipelines in production, orchestrating this volumes of data from source to kind of the storage system that you have and not just deal with two PDFs on the prototyping phase? Yeah, I think there's a bunch of challenges all across the stack. I think probably the biggest blocker that we see is just the performance of the overall system.
05:04:34.944 - 05:05:14.672, Speaker B: And this ties into evaluation because you do need to be to evaluate it. But I think a lot of people are doing the eyeball tests, right? Just looking at how it. Performs, and even then it's not performing well enough. And I think when it gets to a certain point, then you want to do more regulation evaluation. But I think there's just a lot of applications, especially some of the more complex ones that kind of like we're working with, where just the overall performance of the system isn't quite good enough. And I think it's just a lot of work to get them to be good enough. And so I think taking an open source framework and just throwing it five lines of code, you can get something up and running, but when you start eyeballing and when you start looking at it's, just not good enough.
05:05:14.672 - 05:05:34.970, Speaker B: Reliably to put it in production. For the ones that are in production, I think we see a lot of extraction things in production. I think then cost and latency starts to become a problem. Curious to hear how you guys kind of deal with that. So I think it kind of depends on the type of application. Yeah. F Two echo evaluation is definitely the hardest challenge right now.
05:05:34.970 - 05:06:26.490, Speaker B: Going back to the example that I was showing before, for code completion, at least there is a proxy metric you can use at the end, which is if the user is accepting the completion or not. When you put them in front of a chat based interface, then unless you're able to elicit feedback from the user at each step, like the thumb up, thumb down that you have in the demo you were showing before, then we're pretty much in the darkness. We don't know if the users are liking what we built. The other level concern is I think LLM Ops is completely missing, so we know how to deploy and monitor and trace ML models. We've been doing this in the industry for at least two decades, and LLMAP started six months ago. And I'm excited to see what's going to happen next because it's very painful to do everything in production today, actually. Do you have good luck getting people to give feedback of your models? Very little.
05:06:26.490 - 05:07:22.968, Speaker B: We're really A b testing the interface, trying to figure out where we elicit the most, but it's always not enough. And actually just adding on to Mikhail's comment, I think the kind of LLM Op space is still very new and it's very emerging. And I think one of the things I don't know if you guys have seen Switzer's article on the AI engineer. I think the composition of an AI engineer is like a mix of people who come from more traditional data science ML backgrounds, who are kind of used to the practices of experimentation, testing and iteration, to kind of like software application developers that are more maybe used to just stitching things together, like writing tests and actually kind of creating this overall system. And so part of this actually is probably like education and establishing best principles because these things are black box, but they're also very stochastic. And so how do you actually test this new type of software system in kind of like a data science ML fashion, but not quite because it has its own challenges, actually. Yeah, I'm curious how you guys deal with that at Replit.
05:07:22.968 - 05:07:55.424, Speaker B: Namely, what's the team composition of the people working on the Autocomplete? So it's literally one person was leading the model training, one person is DevOps LLM Ops and then myself, trying to figure out everything and make it work together. That's the actual size of the team. When we released the model, we were three people. That's amazing. Yeah, that's very cool. So one more question, I think, for everyone, and then we'll go to some audience questions. So you've all been involved in very successful open source projects.
05:07:55.424 - 05:08:52.056, Speaker B: They're really kind of changing the way LLMs and AI are being developed. So what did you learn from that? What do you have to tell to other folks who want to launch an open source project in this space? I think open source forces you to be accountable, which is amazing because it really pushes you on a daily basis to fix everything that you did wrong and also have complete transparency. On the other hand, we made a choice to release the model open source, I would say for a couple of reasons. First of all, Raplet has been built on shoulders of giants for many years, so we felt compelled to give back. And also the AI field is accelerating so much because so much is available open source. So it will be counterintuitive for me not to do it. Yeah, I think maybe the main thing that I've learned is it's just good to release things, I would say.
05:08:52.056 - 05:09:28.156, Speaker B: I think now especially, there's so much going on in this space and it's so fast moving. And I think when I started Langchain, I didn't imagine starting a company around it. Right. It was kind of just a project that I thought was cool and I shared it with the world. And I think you get really good feedback from releasing things open source. And it's also just there's so many different things to build and there's so many different whether it be applications to build or tooling to build, there's so much different greenfield to run into. And so I think open source is really powerful for just building really rapidly.
05:09:28.156 - 05:10:15.116, Speaker B: Yeah, I think basically that. I think when you kind of like, frame an open source project, and honestly, this is like the first major one that I've really built. The feedback cycle is much quicker than, for instance, if you were out starting a company building like a B, two B type application, where you have to kind of iterate through customer discovery calls to really iterate on the application. As a result, I think it also kind of just motivates people to just go in and just work as hard as you can to try to build as much as possible and release as much as possible. I think maybe that also might be why there's this perception that the AI space is moving very quickly. And actually, I think part of that reason is because a lot of the stuff is on open source, and it's not just like OpenAI coming out with these models. There's a huge amount of open source interest in just building better models.
05:10:15.116 - 05:10:41.464, Speaker B: And then on the tooling side, and then also on the application layer, there's just a ton of people building stuff everywhere and they're all sharing it with the community. And so now all of a sudden you post on Twitter, people give you feedback, someone builds like an iteration of it. And so it's just like a very exciting time. And of course it's like a lot of work too, but it's still very exciting. Any tips for people looking to launch their own open source project? I know a lot of students here are just launch it. Okay, I like that. I've been telling students that.
05:10:41.464 - 05:10:58.432, Speaker B: Yeah, I have a note on that. Whenever your project is going to gain traction, you will get a fair deal of criticism powered through that. Some of the criticism is actually constructive. I think we're all learning from that. Some of it is brutal. Just ignore it and go forward. You can only improve from there.
05:10:58.432 - 05:11:28.612, Speaker B: So that's probably the only downside of working open source, but it's still worth it. Okay, great. Okay, I think now we can switch to questions from the audience. I see we've got a bunch of people lined up there. If even more people want to ask stuff, feel free to line up behind there. Is it on? Hello? Yeah, thank you for a great talk. So I'm thinking about for the conversational uses like a natural language query.
05:11:28.612 - 05:12:20.184, Speaker B: Could be anything. Like, for example, if people asking like, when did you go to the theater yesterday? Then the retrieval algorithm need to handle maybe the link couple, the event going to move theater and the time timestamp. It might be very hard and just naive like embedding concise maybe top case search cannot deal with that problem. So in that case, what is a good way? Should we have several retraval algorithms in the list and select what property one let LMT selected or even let LMT automatically generate retraval algorithm and if it works, then store into the skill list. Or we should make very structured database so we don't need to think about retraval algorithm. We can just use single general retraval algorithm to handle all various curling. Yeah, I think that's a really interesting question.
05:12:20.184 - 05:13:03.716, Speaker B: I think there's a lot of stuff to say on this one. I think the way to do retrieval I think is still like an open question. There was a great talk earlier about kind of like time related retrieval and I hadn't seen that repo before and I'm really excited to dive into that. I think probably if I had to give an answer off the top of my head, I'd say there's probably cases where you'd want to do different types of retrieval, so having some way to route between them, whether it's an agent or a classifier or something. To me, if you really want to bump up the performance, that seems promising. And then my favorite thing in Langchain, honestly, is this idea of the self query retriever. So basically you have a query that comes in and a question oftentimes, just as you pointed out, it doesn't contain only a semantic bit that you want to look up.
05:13:03.716 - 05:13:50.480, Speaker B: And vector stores are like the predominant way to look it up with this semantic bit. But questions don't only contain that semantic bit, they might also contain other pieces of metadata that you might want to filter on. So the example that we have in the docs is like, what is a movie about aliens in the year 1980? And so the year 1980 isn't purely a semantic thing, it's actually an explicit metadata filter that you might want to pass in to a vector store, most of which support metadata filters. And so there's some stuff you can do to kind of extract that metadata filter using LLMs or we use LLMs, there's maybe other ways to do it as well, and then you kind of like split it. So I think yeah, I don't know, open question, lots of really exciting stuff going on in the space there. Yeah, I think on the law index side, I mean, I think in general, I think this is a very open problem. I think how do you basically do different types of retrieval for different types of data and use cases? And so you could do the agent routing approach.
05:13:50.480 - 05:14:42.890, Speaker B: I think one general thing to think about is different kind of data tools will expose different API interfaces and so this idea of self querying is kind of like being able to use the vector store interface with a natural language string and a metadata filter the right way. I think the other thing that I do think that from the Llama index side, maybe the world will probably evolve into some mixture of unstructured and structured data. And so we do have tools that for instance, combine structured data using SQL query with actually joining that information with a vector store as well. And so if you can somehow not just run unstructured query but also a structured query that's maybe time based, you can somehow figure out interesting ways to join them together. So we have some stuff that we're exploring in Llama index, but that's basically thank you so much. Okay, next up. Hi, thank you for your talk.
05:14:42.890 - 05:15:17.340, Speaker B: So I've been working on creating tools using Langchain and Llama index. So I saw that you've recently added the chat agents and not recently, a few months ago. I just wanted to know if you have any tools in the pipeline for fine tuning these agents. Like right now I'm just using prompt engineering to get the agent to behave a certain way. But I would really like if there was any fine tuning tool or plugin that I could use with either of the two. Yeah. Yes, fine tuning.
05:15:17.340 - 05:15:56.780, Speaker B: Yes. It complements this agent kind of retrieval augmented like any type of application very well. I don't know how much we're going to invest personally in fine tuning, but having good interfaces to actually integrate this with stuff like OpenAI or kind of like fine tuning your own models with Qlora or something, I think that's something we're looking at. We have some existing tutorial repos to help deal with that. Not on the agent side right now, kind of more on the kind of like Texas SQL side, but in general yeah, it's like a bit that you can optimize to improve performance and reasoning over a certain yeah, it's interesting. Yeah. Similarly, I think we're thinking of Lang Smith as kind of this interface between fine tuning and your current application with respects to agents.
05:15:56.780 - 05:16:34.730, Speaker B: I think it's particularly interesting because they're not really reliable right now. So you'd think if you can fine tune, they'd become better. But it's also kind of difficult because it's much easier to fine tune, I would think, for an extraction task or a SQL task. I think it's a bit more clear what you should be fine tuning on once you get into agents. There's all types of variables at play, from the tools to the length of the conversation to the things. And so I think we released a blog post earlier this week when the OpenAI thing came out about how to kind of use linksmith to collect data sets, export those fine tune, et cetera. Hopefully we'll do one around agents, but we haven't so far.
05:16:34.730 - 05:17:26.952, Speaker B: Cool. I think we only have 1 minute left if you have a very quick question, otherwise hi, thanks for coming today. So there's literature out there that has examined that as the number of documents referred to in a rag increases, the amount of information they can sort of reference from a very large number of documents sort of decreases. So how do you think future development can address that problem? And do you think fine tuning models for information, fine tuning models specifically on the information you want to ask about is a possible alternative? I can take an initial stab at this. I do think generally these models will get better over time, so they'll just be better at dealing with long context. So I imagine that's like a temporary thing that hopefully assuming tech gets better, that will go away. But the other part is, yeah, I think there is value in fine tuning on top of your own kind of domain specific data and being able to reason over your own knowledge corpus.
05:17:26.952 - 05:17:54.470, Speaker B: We have some examples of that of embedding fine tuning, actually. So being able to adjust the embedding representations in the right data domain. But I think it's actually probably an interesting mix of that plus better or dynamic retrieval to make sure you're actually fetching the relevant context. And there's some stuff in the docs, but I know we're a little short on time. Yeah, I think there will always be data that you can't use to fine tune a model. For instance, it's like sensitive dating your company. And then there will always be data that you want to go through reg.
05:17:54.470 - 05:18:29.680, Speaker B: We know for a fact that if you fine tune a model, you get better performance than in context learning. So what we have to accomplish, I would say, as a field, is to make each one of this technique as easy to use as possible. I think in context learning, it works great, thanks to the two tools that they have on my right. And fine tuning is getting easier because of Qlora, and OpenAI is giving you a service right now. I think the library support out there is getting better and better, and in the real world, we're going to be using both at the same time. Okay, thanks a lot, everyone. Let's clap again for the speakers.
05:18:47.800 - 05:19:25.388, Speaker A: Thanks a lot for the great session on open source tools and ecosystems. So now we are going to move on to our next session. So, next session is also really exciting. It's on multi agent systems and economics. So first, let me introduce our speaker, Noam Brown. He's a research scientist at OpenAI, works on multi step reasoning, self play, and multi agent AI. And he's built AI to beat humans in non limited poker and also strategy game diplomacy.
05:19:25.388 - 05:19:26.290, Speaker A: Thank you.
05:19:33.400 - 05:20:00.904, Speaker B: Everybody. So I'm going to talk about learning to cooperate and compete via self play. This talk will be about our work on diplomacy. This was done at Meta, though I am now at OpenAI. Okay, so this Xkcd comic came out in 2012, and it shows a bunch of different categories for games. Games that are solved, games where computers can beat top humans. And there was this category games where computers still lose the top humans, and that had four games.
05:20:00.904 - 05:20:25.392, Speaker B: Go Areema poker and StarCraft. In 2015, my colleague David Wu actually made the first superhuman Arema bot. This is a game. It's a modification of chess that was designed to be hard for AI. In 2016, famously, AlphaGo beat top humans in Go. In 2017, my advisor and I at CMU made the first Superhuman. No Limit heads up, no Limit Texas Holden poker bot, that's two player.
05:20:25.392 - 05:20:56.200, Speaker B: We followed this up in 2019 with the first Superhuman multiplayer PokerBot. And also in 2019, DeepMind created AlphaStar, which was able to reach grandmaster level in StarCraft Two. And also OpenAI developed their dota two bot. So I think this shows the amount of incredible progress that there's been in AI over the past decade. And to us, after we had completed our work on poker, we're thinking about what to work on next. And it was clear that AI was progressing very rapidly. And so we had to be particularly ambitious with our next goal.
05:20:56.200 - 05:21:40.408, Speaker B: Around the same time. Also GBT Two came out in 2019 and it became clear that language modeling was progressing very rapidly as well. And so we decided to pick a new challenge that would be a combination of the progress that we've seen in strategic reasoning and the progress that we've seen in language modeling. And we decided to try to make an AI for the game diplomacy. So diplomacy is a popular strategy game developed in the 1950s. It was actually JFK and Kissinger's favorite game and it models the complex alliance structures that happened leading up to World War I. Now, what's really interesting about diplomacy is that unlike all those prior games that I mentioned that are purely adversarial, diplomacy involves both cooperation and competition.
05:21:40.408 - 05:22:28.524, Speaker B: And in particular it involves private natural language negotiation between all the players. And it places a very heavy emphasis on the negotiations between the players. So all the turns are done simultaneously. And so what players actually do is they're going to write down their moves on each turn, but before they finalize their orders, they're going to communicate with each player in private. So they'll pull somebody aside, go to a corner, make some kind of promises about what they would like to do this turn, what they would like the other person to do this turn, and then all the players write down their moves. And so you can promise somebody that you'll do something and then not follow through. And these alliances and negotiations are really important for the game because there is really no way to win without working with others.
05:22:28.524 - 05:23:28.220, Speaker B: And you can kind of see an example of what the dialogue might look like on the bottom left there. Now, diplomacy had existed as a challenge problem for a long time, going all the way back to the 80s, but it was never really a feasible challenge to try to address because the natural language component was so beyond AI capabilities, especially back in the 80s. Research started to pick up in 2019 with research from Mila, from DeepMind, from ourselves, at Meta and others. But even then the full natural language version of the game was considered out of reach now because diplomacy involves these complex negotiations and promises, but then people might not follow through on those promises. It has a reputation as a game that ruins friendships. So you can kind of think of like Settlers of Catan times a thousand. It's really difficult to be in an alliance with somebody for 5 hours and then have them betray you at the very end.
05:23:28.220 - 05:24:01.620, Speaker B: But if you talk to top diplomacy players, they view it differently. They say that diplomacy is a game about trust. It is a game about building trust in an environment that encourages you to not trust anyone. And that's why we saw this as a particularly interesting challenge. Could we build an AI that can build trust with a human player even in an environment where there is reason to be skeptical? If we could do that, then we considered that to be something that would be truly impressive. So that was our goal. Okay, how do we make an AI for the game? Diplomacy.
05:24:01.620 - 05:25:24.912, Speaker B: Now, I don't have a lot of time, so I'm going to try to focus on one aspect in particular, which is the contrast between two player zero sum games and the cooperative element of diplomacy. So if you look at all the previous games that AIS have beaten, humans, at, Go, Poker, StarCraft, even things like these, 3D Sumo Wrestling simulators, they've all been two player zero sum games and they've all used self play. Self play is this algorithm where the AI learns to play the game by playing against copies of itself? Now why does this work? Well, to motivate why this works, I want to first ask a question, which is between these two options, who do you think is the better poker player? Option one is somebody who over a large enough sample size, will win head to head against any other person alive in poker. So if they were to sit down at the table with one other person and play for, let's say, a million hands of poker, then that person would always end up winning. Option two is someone who makes more money playing poker in a year than anybody else. And I should point out that these aren't necessarily the same person, right? Like you can have somebody who is in the long run going to squeeze out a win against anybody else, but maybe isn't very good at maximizing their winnings against weaker players. While you could also have somebody who is really good at getting a lot of money out of weak players.
05:25:24.912 - 05:26:10.592, Speaker B: But when they're against somebody that's really strong loses by a little bit. So when we talk about two player zero sum games, we typically aim for the first option and that's called a minimax equilibrium. But I want to point out that that's not necessarily the definition of optimal. That's a choice that we're making. Okay? So when you look at things like AlphaGo, when you look at pokerbots, typically what they're computing is a minimax equilibrium. And that means a minimax equilibrium in a two player zero sum game means that you're playing optimally given that the other player is also playing optimally. And in particular in a balanced two player zero sling game, if you play the minimax equilibrium, you are guaranteed to not lose an expectation no matter what your opponent does.
05:26:10.592 - 05:26:55.244, Speaker B: And that is a really valuable quality to have. So to give you some intuition for how this can even be possible, let's think about rock, paper, scissors in rock, paper, scissors. Let's say your strategy is to throw rock every single round then your opponent could counter you by throwing paper every single round and you would lose. Your exploitability would be one because you would be losing one point on average each turn. You could also try something more clever like throwing paper on the first two rounds and then if they throwing rock on the first two rounds and if they throw paper trying to switch things up by throwing scissors but even then your exploitability would still be one because they could still counter you. And so the equilibrium, the minimax equilibrium in this game is to randomize. If you randomize equally between throwing rock, paper and scissors on each round then no matter what your opponent does you're going to be unexploitable.
05:26:55.244 - 05:27:46.000, Speaker B: You are guaranteed in the long run to not lose no matter what your opponent does. Now you might look at this and say like well this guarantees we won't lose but it also guarantees we are not going to win an expectation. And that's true. And this is why minimax equilibrium doesn't necessarily mean you're going to in a game like poker make as much money off of the other player as is possible. But typically if you play the minimax equilibrium in a more complicated game like poker it's actually very difficult to figure out how to even match the performance of a minimax equilibrium. And so on average you'll end up winning in practice because your opponent will make mistakes. And in fact if you look at poker strategy guides for example they will say this that the optimal strategy for poker is to allow your opponent to make mistakes, play minimax optimal and then allow your opponent to make mistakes and then you'll profit.
05:27:46.000 - 05:28:44.304, Speaker B: Okay, so how do we compute a minimax equilibrium? Well the answer is self play and I'm glossing over a lot of details here because there's a lot of complexity in how to do self play properly to guarantee that you converge to a minimax equilibrium. But the very high level is that you have an agent that starts by playing completely randomly, it doesn't know anything about the game and then it learns to get better by playing against copies of itself. So by playing against earlier copies of itself it will gradually improve. It will learn what actions are higher expected value and will play those more often. And in the long run if you do this self play properly you will converge to a minimax equilibrium. So what this means is that any two player zero sum game can be solved and by solved I mean you can compute a minimax equilibrium for it. Any two player zero sum game can be solved via self play given sufficient memory and compute and that is one of the key takeaways from this talk.
05:28:44.304 - 05:29:06.148, Speaker B: Any two player zero sum game any two player zero sum game can be solved with enough memory and compute. That's all you need. Memory and compute and a sound self play algorithm. Okay? And that's what you've seen in all these prior game AI. Breakthroughs in Go poker StarCraft dota. It's all been self play scaled. Okay? So I say that this is true for two player zero Susan games.
05:29:06.148 - 05:29:41.428, Speaker B: What about outside two player zero sum games? Well, it turns out that self play alone is not enough outside of two player zero sum games like in diplomacy. Because in addition to memory and compute, you also need human data. And to give you a very simple example of why this is the case, let's look at something called the ultimatum game. In the ultimatum game, Alice is given $100 and Alice must offer Bob between zero and $100. And then Bob decides whether to accept or reject that money. If Bob accepts, then Alice and Bob keep their money. If Bob rejects, then Alice and Bob get nothing.
05:29:41.428 - 05:30:19.052, Speaker B: So to give you an example, let's say Alice has $100, alice chooses to offer Bob $20. And if Bob accepts, then Bob gets 20, alice gets 80. They both walk away pretty happy. If Bob rejects, then they both get zero. Now I want you to imagine what would happen if you had an AI learn to play this game from scratch with no human data. The optimal strategy, the optimal game theoretic strategy is to offer Bob $1 and keep $99 for yourself. Now, what do you think would happen if the bot tried to offer that to an actual human player? They would probably reject.
05:30:19.052 - 05:31:09.328, Speaker B: And in fact, there's been extensive studies in economics on this question and they find that people tend to reject amounts less than about 20 or 30% of the total. Now, there's two ways we can approach this. We could either say that okay, well the humans aren't doing what game theory says, therefore the humans are broken and we should just keep computing minimax equilibria. Or we should say, well, maybe our approach is wrong and we should be accounting for the fact that humans are not going to play according to our game theoretic models. And the way we get around that is by incorporating human data. And I want to point out also, even if by some stretch of the imagination you tweaked self play so that it would converge to offering 20%, the amount that people reject ends up being culturally dependent. So in some cultures, people will reject anything less than 10%, whereas in some other cultures they will reject anything less than like 40 or 50%.
05:31:09.328 - 05:31:33.784, Speaker B: So the idea that self play is going to learn these cultural norms is just absurd. And the only way that you can really address this is by incorporating human data into the algorithm. And that's actually what we found in diplomacy. So in diplomacy, we developed a self play bot, a bot that learned to play the game completely from scratch. And in a two player zero. Sum version of the game, it actually ended up being superhuman. It won with an 86.5%
05:31:33.784 - 05:32:10.432, Speaker B: win rate. But then when we played it in a seven player version of the game, we found something interesting. Again, diplomacy is a seven player game. So anything if you're scoring 14%, then that is average performance. If you take one of these Dora bots, okay, so if you take Search Bot, so Search Bot is this bot that was developed to be in a very human like bot. Take one copy of SEARCHBOT and play it with six copies of Dora, where Dora is our bot trained through, purely through self play without any exposure to human data. The Search Bot agent ends up getting crushed.
05:32:10.432 - 05:32:57.620, Speaker B: It only scores 1% against the six Dora agents, where again, less than 14% means that you're losing. But if you take one Dora agent and play it against six human like Search Bot agents, dora only scores 11%, which means that Dora is also losing when it's playing against six human like bots. And what this illustrates is that if you develop a bot through self play in diplomacy, you end up converging is something that is self consistent. It's a meta game that is totally reasonable but very different from how humans play. It's as if the bot is developing its own language, which is great if you're playing with other bots that also speak that language. But if you then play in a game where you're the only one that speaks your weird robot language and everybody else is speaking English, then you're going to do very poorly. And that's kind of what we observe.
05:32:57.620 - 05:33:38.176, Speaker B: So how do we get around this? Well, what we use is an algorithm we call Pickle, but it's actually very similar to a lot of other algorithms that have existed for a very long time. Basically, you regularize self play towards a human like policy, an anchor policy that's learned through imitation learning. So you collect this giant data set of human games, train a model to imitate that human playstyle, and then when you do self play, you have a KL penalty for deviating from that policy. And this lambda parameter controls how much you're deviating. If you set lambda to zero, then you're doing self play from scratch. If you set lambda to infinity, then you're just doing behavioral cloning. You're not doing self play at all.
05:33:38.176 - 05:34:29.810, Speaker B: You're just modeling the human data. And by choosing an appropriate lambda, we can better model human players and develop a bot that is consistent with human play. Okay, so don't have a lot of time, so I'm going to get to results. We trained Cicero, an agent called Cicero for diplomacy on 50,000 human games of diplomacy. And we entered it anonymously in an online diplomacy league where it was not detected as an AI agent, even though over 40 games, even though it sent and received an average of 292 messages per game. And I should point out that this alone was a surprising result to people. In fact, we got messages like this where after we told the humans that they were playing with a bot the whole time, they were genuinely shocked that the bot was capable of this kind of sophisticated play.
05:34:29.810 - 05:35:08.952, Speaker B: Now, in terms of score, cicero plays in the top 10% of players and the second of 19 players who played five or more games. It's a high variance game. So that's why I'm saying players that played five or more games. But in general, it plays in the top 10% of players, even if you look at players that only played one game and achieved more than double the average human score. So this is a strong human level of performance, but I wouldn't say it's superhuman. Okay, so, to wrap up, first of all, I want to say this was a big team effort, and it was really a privilege to work with such a talented group of researchers. To recap sound, self play will compute a minimax equilibrium at any two player zero sum game, given sufficient memory and compute.
05:35:08.952 - 05:35:31.344, Speaker B: But that's not true outside of two player zero sum. The way to approach self play outside of two player zero sum games is with KL regularization towards a Human Imitation policy. I e. The algorithm that we call Pickle. At least that was what worked well for us. You can find out more details in our papers. And we also have our code and models for diplomacy open sourced at this URL.
05:35:31.344 - 05:35:33.850, Speaker B: And you can also use this QR code. Thank you.
05:35:41.340 - 05:35:54.670, Speaker A: We have 1 minute for a question from the audience. If there's any questions, and there is the mic over there. Otherwise, we'll move on to the okay, sorry.
05:35:58.580 - 05:36:27.240, Speaker B: Yeah. Question for you. Have you considered what such an agent might do in actual labor markets? In labor markets, yes. I think the important thing is that you need data for the situation. So we haven't had data on something like labor markets. But I think if you had data and you had a model of the interaction, that it would work quite well in that setting as well. Thank you so much for your awesome presentation.
05:36:27.240 - 05:37:07.572, Speaker B: How might this model be able to predict how humans react to changing environments if human players are changing their strategies? If humans are changing their strategy, we would have data on okay. It is true that the data that we have assumes that a stable distribution. And so if the players were to change their playstyle because, for example, we created this spot and started entering it into tournaments, then we would need additional data on how the players are changing their playstyles. And ideally, we would have enough data on how people on people in general that we would be able to anticipate people changing their playstyles in that way. That makes sense. Thank you so much. And I should also say, if anybody has further questions.
05:37:07.572 - 05:37:12.068, Speaker B: I'll be available at the break to answer questions. So how do you feel?
05:37:12.074 - 05:37:21.416, Speaker A: Like we don't have more time for questions. Yeah, thank you. Great. You can take right. So Sunam will be around to take additional questions. Great. Yeah.
05:37:21.416 - 05:37:50.180, Speaker A: Thank you. So now let's move on to our second keynote after summit from Mike Jordan. Mike is also someone who doesn't need introduction. Mike is a professor at UC Berkeley and, as everyone here knows, has been a true pioneer in the area of machine learning. So Mike is going to tell us an alternative view on AI, collaborative learning, incentives, and social welfare. Thanks, Mike.
05:37:53.880 - 05:38:16.004, Speaker B: All right. It's great to be here. Thanks, Don, for inviting me. It's always nice to give a talk where you can bicycle to the talk. Very carbon efficient. Okay, I'll go fast because I do have a lot to say. This talk is less about the what and the how, which is what most of us spend all of our time doing in AI.
05:38:16.004 - 05:38:49.600, Speaker B: What are we doing? It's more about the why. And why are we doing all this? I think we have to think about that from time to time. When you're young, you say, well, because it's cool, it's fun, and maybe you get a little older, you say, well, because I can make a lot of money doing it, or I can have a career. And those are absolutely great reasons up to age 25 or so. After that, you really should think about society and what's the legacy you're leaving and what are you doing. And don't just be naive about it that our technology will fix everything. So I know a lot of you are doing that, but I just want to highly encourage you to do yet more of it in this era that we're in.
05:38:49.600 - 05:39:27.216, Speaker B: And so the talk hopefully give you a little bit of fodder how to think about those issues and connect to some people outside of the computer science world. So let's just talk a little bit about the why, the original kind of why McCarthy already in the 1950s was saying well, for him, I think the why was a philosophical one. Can we put thought in a computer? How could we do that? That's an extremely interesting question. We don't really have thought in a computer. Arguably, at this point, we have some kind of cool data indexing and planning and so on. But thoughts hard. In the meantime, Douglas Inglebart there said, no.
05:39:27.216 - 05:39:50.824, Speaker B: Well, what computers are really all about is augmenting human intelligence. So search engines aren't smart themselves, but that's fine because they make all of us a whole lot smarter. They make society smarter. And I believe that that really was the main thing that happened in the last 40 years and not AI. AI arose because machine learning started to work really, really well, and we had lots of data. It wasn't thought in the computer. It was data analysis that triggered all of this.
05:39:50.824 - 05:40:37.024, Speaker B: And so I think taking that further, it's not about the individual AI. We tend to, like, act as if it is. We're replacing a human with an AI and see what happens. It's really about these great systems that are networked and they're planetary wide, and they incorporate all kinds of people with all kinds of desires and wishes and hopes and dreams and aspirations, and we should be thinking about all of that. How does data flow and value? How is it created in a system like that? We never had systems like that that would do data analysis as part of the economic interactions and structure. So how do we think about that sort of level of thing? And so when we start talking about decentralized and all that's kind of good, it gets in that flavor, but it's too technical. It just suggests that if it's decentralized, it'll be better, that people will know what to do with that.
05:40:37.024 - 05:41:15.748, Speaker B: Well, no, we've got to think more about the market mechanisms and make sure that it's fair to people and make sure it's economically viable and so on and so forth. Okay, so the 90 50s perspective kind of came running back, and I was kind of surprised to see it. I thought machine learning was just getting better, and that was kind of enough. We didn't have to return to that old perspective. But this idea of Frankenstein, of we created individual human that's as smart as us or more smart, I don't see the point in it. I mean, it's fun, it's exciting, it titillates, but it's not clear why we would do it. Yeah, you can make a story, but it's just not so clear to me why you would do it.
05:41:15.748 - 05:41:50.256, Speaker B: We got a lot of humans. We want to make all them happier, richer, more self aware, more participatory and all that. All right, so whatever your problem is, for me it's like creating markets to help music be more widespread and have jobs and stuff like that. That's kind of things. I get resonate too. And AI in the classical sense doesn't seem to be very connected to that. Okay, so a counterpoint in some ways is just totally obvious, which is that this whole field of technology let's not call it AI anymore about data analysis and networks and flows and connections and all that.
05:41:50.256 - 05:42:18.552, Speaker B: It's not about making an entity that's smarter than us in any sense. What it is, it's about creating new kinds of collectives. The old collectives are things like transportation systems that we collectively it collectively moves us around and makes us happier and more productive in some ways, or medical systems that makes us more safe, or entertainment systems. It's about the system. All right. But collectives are also markets, and they can be creative and do things we never designed. And those things are intelligent too.
05:42:18.552 - 05:42:55.796, Speaker B: So how do we build our technology to support more intelligence coming in ways we didn't even anticipate, but that it's at the collective level. So there probably are going to rise new forms of collectives we never thought about before in history. And I think that's what's really exciting in our era, not super intelligence. All right, so if you ask machine learning people, well, aren't you doing this kind of thing? They say, yeah, we do Federated learning decentralized this and that and all. Sure, you have a server and you have distributed people and all that, and sure, there's a little attention to privacy and all, but it's still not even close to what I envisage. There's no economic value being realized here. The players aren't agents.
05:42:55.796 - 05:43:25.996, Speaker B: They don't opt in or opt out. They don't get paid for their work. There's no economic model behind the Federated learning paradigm. So it's really just how do I compress data and how do I make sure the server gets as much data as possible to build something that they make a lot of money on and hopefully is of value to everybody? That's just not mature enough for a field, in my view. All right, so the real way to think about this is that it's not just data out there, like of the form, where was I this morning and what did I browse and all that. We're all living on our cell phones all the time. We create things on there.
05:43:25.996 - 05:43:56.456, Speaker B: We create music, we write things, novels, short stories, we interact with each other. All kinds of interesting things are happening there. And just to hoover that up and to take that as data and then not pay for it, to me, is just bad. It's wrong ethically, but more importantly, it's wrong economically. We're missing all kinds of ways to create new markets and incentivize people and bring more people in and so on and so forth. People value their data, and they don't want to just give it away, and they want to have it be valued in some sense. And we don't even know how to think about that.
05:43:56.456 - 05:44:27.792, Speaker B: I think we have very vague ideas about it. All right, so just to make this more concrete, one of the things I'm involved in the last ten years is a startup. It's not bigger than a startup, but it's called Unitedmasters.com. There's the CEO, Steve Stout. He's a legendary figure in the music world, the hip hop world. And this company I'm a scientific advisor for, it on the board is a three way market at the end of the day, and it's all based on data flows. All right? So if you look at the music that's being streamed today, there's huge amounts of music.
05:44:27.792 - 05:45:05.130, Speaker B: It seems like a huge success of computer science that everything is going all over the place and being streamed. Maybe it is, but 95% of the songs being listened to today were written in the last three months, and 95% of them were written by people you never heard of. Okay, so that's interesting. Who are all these people? Well, there are 16 to 20 year olds who make music just as a hobby, and they're really good at it, and people will listen to their music. And a lot of people who listen to music might listen to the latest coolest thing. They don't want to listen just to Beyonce and The Beatles and so on, all right? But all the money is going to Beyonce and The Beatles and so on. These kids are not getting paid hardly anything, really anything.
05:45:05.130 - 05:45:35.248, Speaker B: So no jobs are being created. And at age 20, they have to find something else to do with their life, even if they're really good at music. And people love their music anyway. United Masters is not just a platform. It's a three way market where if you sign on as a musician, your songs are then produced by the platform, and they are listened to by people you know who is listening to you. So if a lot of people listen to you in Columbus, Ohio, the venue owners there will see that you could go give a show there. And there's kind of data that flows there, but even more importantly, they're brands.
05:45:35.248 - 05:46:00.096, Speaker B: So Steve went to the national basketball association. He said, do you guys need music? Yeah, behind our clips we use music. How much do you pay for? Well, we pay Beyonce a huge amount of money for that. Well, what about having United Masters supply the music? And every time someone clicks and listen to a music, the artist gets paid. So that is living and working. Today there are 3 million musicians who've signed onto this, including some very famous ones, and they're all making money, real money. And it's a thriving three way market.
05:46:00.096 - 05:46:28.824, Speaker B: And so that means it created new kinds of ways for music to be used in various places. Adaptively, all done adaptively. So where's the AI behind this? What I really want to emphasize is just dumb it down. It's really the hard systems work of something like Mate would build together with hard economics work to make a market that actually works here. It doesn't fail together with all the data analysis to this, actually flow in real time and make it an actually viable place to go have your career. And it's working in the US. At the tune of 3 million people.
05:46:28.824 - 05:46:59.628, Speaker B: It could work in any country around the world. And real money is being made by lots and lots of people. So I want to aspire to things like this, and there's a lot of technical problems behind this that have got to be solved if this kind of thing is going to become a reality in lots and lots of fields and really, really work. Some of them are machine learning data analysis problems. Some of them are blockchain and all that, but most of them are ones I never hear my computer science colleagues ever talking about, including all my machine learning colleagues. All right? So in machine learning we're very good at finding optima. We're not so good at finding equilibria, zero sum games notwithstanding.
05:46:59.628 - 05:47:43.520, Speaker B: And we're not very good at thinking about dynamics of equilibria as equilibria shift because the world shifts in various ways, especially when data is part of the goods that's being used to form those equilibria. I don't have enough data. I'll get a better equilibrium if I get more data. How do you make that happen? All right, how do those equilibria rise? How do you explore exploit in multi way markets? We know how to explore exploit a single agent with array of options. But how do you worry about information asymmetries and contracts? And not just contracts of the blockchain kind, but contracts of the form where you try to incentivize someone to do something that's in their interest, maybe, but it's certainly in your own interest, and so on and so forth. Okay, so these are things I've been working on for the last ten years. And I want, in the last part of my talk report on a couple of examples of this just to hopefully inspire.
05:47:43.520 - 05:48:10.148, Speaker B: Some of these are sort of ready for software, but really more they're ready for thought experiments. How do we think about these problems? All right, so the first one I want to talk about is something we now call statistical contract theory. This is with my colleagues Stephen Bates, Michael Sclar, and Jake Soloff. A contract theory is a thing in economics. There's been Nobel Prizes for it. But statistical contract theory is new because the classical contract theory had no role for data. Interestingly.
05:48:10.148 - 05:48:35.056, Speaker B: All right, so what is contract theory? Well, it's part of the theory of incentives in economics mostly. You know about one branch of theory of incentives, which is auction theory that incentivizes people to come in and reveal their value for something, and auctions happen and revenue is maximized and so on. All right, that's one branch. There's another branch, which is asymmetric. Players at an auction are all symmetric. They all don't know anything. They have assumptions about each other.
05:48:35.056 - 05:49:05.784, Speaker B: Right? Now, you all have had experience of these asymmetries. So the airlines don't have a single price for all the seats on an airplane, right? Movie theaters do, airlines don't. Why is that? Well, in the airlines were all going bankrupt. It wasn't a good business model. And someone came and said, well, you guys need to learn about incentives and you learn about contract theory. And so contract theory is just the realization that many, many people have different willingness to pay. For example, from going from here to Paris tomorrow, some of you would be willing to pay $5,000.
05:49:05.784 - 05:49:36.496, Speaker B: You really got to get there. It's really important for some reason. Others just say I'll pay 100, but nothing more than that. There's a big gap between there and the airline would like to set a price way up near the 5000. They can get all that money, but they're going to have an empty airplane. So they bring it down, but they lose all that money and so on. So the answer obviously is to give a different price to everybody, right? But you can't just look at someone and say, I think you're someone who's willing to pay 5000 and you're a Berkeley professor, you're only willing to pay 100, okay? Because the Berkeley professor will start dressing like the CEO or vice versa.
05:49:36.496 - 05:50:01.340, Speaker B: So that you'll pretend to be something you're not. And that's what people do in the real world. So what contract theory does is it says we're not going to do price discrimination in that way. We're going to have a list of services and prices and it's a whole list. A contract is a menu of options, service price, service price, and everybody gets the same menu. No discrimination. What is in the menu? Well, things like they created this idea of business class.
05:50:01.340 - 05:50:30.790, Speaker B: It's a set of services. And for the younger people in the room, you may not know about this, but there's something called business class, where you get a little glass of red wine and you get to be first in line and you get a little bit bigger seat and you pay thousands of dollars more for that. And unbelievably, there are people that are really proud of themselves to be in business class and they really are happy to pay all that money. All right? In the meantime, a lot of younger people love to walk through business class and say, I'm going to be on the same airplane. I don't care about the glass of red wine, I'm only paying $100. Ha. I'm happy.
05:50:30.790 - 05:50:55.916, Speaker B: Well, everybody's happy. That's economics. You found a way to make more people happy and you filled the airplane, so the airline's happy. Okay, so I'm making a little bit light of this, but there's Nobel Prizes for this. There's various crossing curves where incentives line up in various ways, where you're not incentivized to lie, you're incentivized to take the option which actually fits closer to your actual value, but you're not doing an auction. All right? So that sounds pretty cool. And it is.
05:50:55.916 - 05:51:16.870, Speaker B: And people worked out all those crossing curves and all those points. But now here's the interesting thing. This was just people in a boardroom economist sitting down, writing down funny numbers, making things up. And they were smart, so they made up good numbers. But in the modern world, it should all be done adaptively. It should all be done based on data. It should all be based on done on human behavior, all right? But the theory doesn't have any role for that.
05:51:16.870 - 05:51:51.410, Speaker B: Okay? So we've been rolling this out in the domain of clinical trials. So as you all know, each country spends a huge amount on clinical trials, like vaccine trials, tens of millions of dollars a year for every disease. And this is handled by a regulatory agent agency who takes like 3000 people and runs a trial, and it costs a lot of money. So why do we do that? Well, because we don't know what drugs work. And there are these things called drug companies which are happy to supply lots of drugs to the market. And they will pretend that they work, but they themselves don't even really know. There's uncertainty here.
05:51:51.410 - 05:52:20.184, Speaker B: All right, so this is what I think is best thought of as the principal agent paradigm, which is the underlying paradigm. The game theory concept here is stackelberg games, sequential games, not Nash, not zero sum. And the inverse of a stackelberg game is contract theory, just like the inverse of a Nash game is auction theory. Hopefully that's helpful. All right, so it's a principal agent game. The agent knows some things that the principal doesn't know. The principal doesn't know how good that drug candidate is.
05:52:20.184 - 05:52:52.032, Speaker B: The agent may not entirely know, but they have some inside information. But they're not willing just to tell the FDA their inside information because they want to get a good deal. All right, so how do we fix this? Well, first of all, what is the FDA currently doing? They're being good statisticians. The whole goal is to put things on the market which work. All right, so you want the probability of approval to be high for things that work and low for things that don't work. So you make one of these little tables, like in an elementary statistics class. If it's a bad drug, meaning not that it harm people, they do test for that, but it doesn't work.
05:52:52.032 - 05:53:08.216, Speaker B: It doesn't help anybody. So that's theta is equal to zero, then your probability of approval is zero five. That's the false positive rate. All right, so the FDA's false positive is about zero one. But it's roughly something like this. If it's a good drug, theta is equal to one. They approve it with probability zero eight.
05:53:08.216 - 05:53:24.924, Speaker B: So that's the power. These are standard numbers. And is this a good protocol? Well, yeah, it's optimal. It's called the name and Pearson test. It's optimal from a statistical point of view, but that doesn't mean it's optimal from a full systems point of view. Consider case one. There's a small profit to be made.
05:53:24.924 - 05:53:43.648, Speaker B: It cost you 20 million to run the trial. That's your reservation price. And if you're approved, you make 200 million. So that would be for a small niche drug. All right, so now you can do the following counterfactual calculation. If theta is equal to zero counterfactually, no one really knows. But both the CEO and the FDA could do this calculation in their head.
05:53:43.648 - 05:54:07.480, Speaker B: If it were the case that theta is equal to zero the expected profit would be -10 million okay. The CEO looks at that and he or she says to the workers, there don't send any drugs up to the FDA, unless you're really quite convinced that they're good drugs because you did some internal testing or whatever, that's good. The FDA is now mostly receiving good candidates. They do their good testing. Everything is great. But the real world doesn't work like that all the time. In fact, most of the time it doesn't.
05:54:07.480 - 05:54:24.416, Speaker B: Most of the time there's a large profit to be made. It costs 20 million to run the trial. If you're approved, you get 2 billion. It's more like ibuprofen or something. And now you can both do the calculation counterfactually. If the drug didn't happen to be any good, the expected profit is 80 million. Right? Where's that coming from? Well, there's a false positive rate here.
05:54:24.416 - 05:54:46.730, Speaker B: It's not zero. And so these drug companies are now incentivized to send tons of candidates up there. The CEO does this and a lot of them get through the false positives and they go on the market for a couple of years and they make a ton of money, 80 million. And it doesn't hurt anybody. It just doesn't help anybody. And then after a couple of years, everybody realizes and there's a new drug that comes and displaces it. So this is what's really happening in every country.
05:54:46.730 - 05:55:13.920, Speaker B: All right, so how do you fix this? Well, this is exactly a principal agent problem. The principals, the FDA wants to get good statistical testing done. The agents know something and they're willing to help out, but they're just not going to tell them the answer. Okay, but what's new here? And so briefly, this is what statistical contract theory is going to look like. Here is a contract. You have an opt in protocol. The principle the FDA presents to the agent, the following opt in protocol.
05:55:13.920 - 05:55:41.880, Speaker B: The agent. First of all, if they opt in, they pay r a reservation price, say 20 million. Then they choose a payout function from a menu that we design. So this is like business class and economy class. There's a set of selections and they choose and it's going to be things like licensing duration, how much testing it has to undergo, and all kinds of other things that have economic consequences. All right, so that's classical contract theory. You would have an opt in, you would have a reservation, you would have a menu of options.
05:55:41.880 - 05:56:14.790, Speaker B: What's new is now you run a statistical trial. You gather data. All right? So statistical trial is run collecting data, z from a probability p, which depends on the true theta, which only nature knows, but nature will give us data. All right, so we now put that in. The agent receives a payoff f, which they picked the f, and now they get to be paid off f of Z. And the principal, the FDA receives a utility which depends on f of Z because that's what they have to pay out and also depends on the true state of nature, because after a few years go by. People realize whether the FDA is doing its good job or not.
05:56:14.790 - 05:56:49.388, Speaker B: All right? So that's our setup. But now we do the mathematics and we work out what's the optimal set of contracts, what false positive rate do you get? And the first thing you have to do in any of these economics kind of problems is work out the incentives. All right? So you define something known as incentive aligned. And I don't want to get into details on this slide. There's a paper on archive if you want to see it, but basically it's an expectation under the randomness from nature of Z of P at theta naught. Now, theta naught is not the true theta. There that's if the drug doesn't work.
05:56:49.388 - 05:57:27.864, Speaker B: So it's like a P value in statistics. If the null hypothesis is true, do something. So here's if the null hypothesis is true, the amount of money I'll lose is F of z minus r and that has got to be or the amount of money I will gain has got to be negative, okay? Otherwise people will be making money for free. You wouldn't want to do that. All right? So, long story short, there is a very natural notion of incentive alignment and what we've done is at the bottom of this slide is the key idea, is the key result. We've made a theorem which says that a contract is incentive aligned if and only if all the payoff functions are e values. All right? So in a longer talk I would get into eValues.
05:57:27.864 - 05:58:27.870, Speaker B: You've heard of P values and statistics. Those are notions of evidence that if the null were true, it's a probability of being at some point or further out that's a tail probability. And if it's a small number that says the null must not be true, e values are better than P values in many ways they're real numbers whose expectation, they're random variables, non negative, whose expectation is less than equal to one. And so if they turn out to be bigger than one, that's evidence against the null in general. These are non negative super Martingales, if you know what that means, that's the general family and Martin Gales have lots of nice properties, they have stopping conditions, you can put two of them together and all that p values don't have anyway, that's on the statistics side why us statisticians are starting to love eValues more and more. But the point of this talk is that it turns out our discovery is that eValues are in one to one correspondence with optimal payoff functions that are incentive aligned. So there's a link between economics mechanism design and statistics, a fundamental link.
05:58:27.870 - 05:59:01.224, Speaker B: All right, so we've applied, been applying this in a bunch of different kind of problems here's just one really quickly. We can now incentivize data sharing in Federated learning, which has kind of been an open problem. We got all these agents who have data in the federated setting we want to incentivize people to send in their data. And we want to deal with things like the free writing problem, that if two people next to each other have the same data, they're going to look at each other and only one of them wants to send it in, the other one won't send it in. This helped. This mitigates that problem. So we have some papers with Pranith Karimaretti and others on this if you want to see how this theory applies to that real world problem.
05:59:01.224 - 05:59:36.084, Speaker B: All right, my last five minutes, I'm going to talk about another project just to give you another kind of glimpse of this kind of research with all my these are my students at Berkeley, Steven's A. Postdoc. This is called prediction powered inference. And so, again, this is kind know, statistics meets big large scale computing meets some economics ideas. This has to do with calibration. When you have decentralized decision makers and they're all doing data analysis and they're all working with probabilities, those probabilities need to be calibrated. And probabilities from a machine learning system are not calibrated at all.
05:59:36.084 - 06:00:14.620, Speaker B: And that's a real problem in gluing together things and making decisions in decentralized settings. So this project aims at that problem and we do it in a pretty spectacular setting. As you all know, there's things like alpha fold that predict protein structure extremely well. It's one of the success stories of this predictive AI. All right? And there's so many of these predictions out now, they kind of are flooding the market, if you will. So there's hundreds of millions of amino acid sequences whose structures have been predicted very well by alpha fold, and there's only hundreds of thousands of crystallized ones. All right, so here's an example of how this is being used in science.
06:00:14.620 - 06:01:07.564, Speaker B: This was a paper in 2004 studying the relationship between intrinsic disorder protein, where the quantum effects are significant, and phosphorylation, which is an important part of how you annotate proteins, to decide to influence the kind of effect they have on other proteins. So they wanted to study, is there a relationship between these two ideas? And so in 2004, they tried this and they only had 10,000 structures in the protein data bank. And so they did their null hypothesis test and they weren't able to reject or accept. Okay, so it was inconclusive. 2022, someone redoes the same experiment, but now they have 200 million alpha fold predicted structures and they just use those instead of actual handlated hable data because there's so little handle without still. So they plug this into their ODS ratio calculation and they got a big ODS ratio. So they were able to say, yes, there is a relationship between these phosphorylation and intrinsic disorder.
06:01:07.564 - 06:01:55.596, Speaker B: Right? Is there really? Well, because this is not the real data, this is not the gold standard, but it's so accurate that it must be pretty good, right? Okay, so that's the last part of my talk is to convince you that this is really a problem. First of all, it's being used all over the world. You can pick up nature science on any day nowadays, and you'll see the uses of these kind of models in place of data, all right? Now, the problem is that these models do make errors, and depending on the specific hypothesis you're looking at, those errors may or may not be significant. For intrinsic disorder, they're actually pretty significant because that's kind of the quantum effects that alpha fold wasn't necessarily geared towards. All right? So here is some confidence intervals. As a statistician, this is important for me to always show confidence intervals, not just predictions, all right? So this is the ODS ratio. If it's bigger than one, we reject the null hypothesis.
06:01:55.596 - 06:02:21.448, Speaker B: Okay? The gold thing there is the imputed confidence interval using the alpha fold data. So you just take all the alpha fold data, you put it into ODS ratio, you do some kind of statistical mumbo jumbo, and out comes the confidence interval for the true ODS ratio, all right? And it's really tiny. I'm very, very confident. All right. The problem is that I'm confident. The ODS ratio is up around three, but in a long, money cargo firm, we found out what the true ODS ratio was. It was around two.
06:02:21.448 - 06:02:41.568, Speaker B: So you're highly confident you're completely wrong. All right? So you could now say, forget that, let's throw out the alpha volle data. Let's just use the protein data bank that we have now and see how well we do. And you get the gray confidence region. So it covers the truth. That's good provably. It does with high probability, but unfortunately, it covers one as well.
06:02:41.568 - 06:03:19.128, Speaker B: So you can't again make any conclusions that's problematic for science, but it's at least honest. Anyway, we have a new procedure we called prediction powdered inference, which fixes this problem. And it delivers the green region there, which also covers the truth, but is almost always much smaller than the one where you throw away the predictions. Okay? So the general picture is something like semi supervised learning, but it's very different. That just makes your predictor better. This is trying to give you honest confidence intervals, but still, you have some labeled data that some human labeled, and there's very little of that. And you have a huge amount of unlabeled data where there's no human label.
06:03:19.128 - 06:03:49.024, Speaker B: But there's a predictive engine like alpha fold that makes predictions, and you want to glue the two together to make a better confidence interval based on the huge amount of data. That's a true confidence interval. It really covers the truth with high probability. All right? So in the paper, you will see this being done mathematically. We're able to do this. Let me just show you a couple more examples. This is a gene expression problem where you're trying to decide whether a particular promoter actually gives you expression or not a very important problem in biology.
06:03:49.024 - 06:04:14.460, Speaker B: And you can see on the lower right over there, the gold interval is the imputed one. It's way small, but it's way off. And again, the green one is both covering the truth, and it's much smaller. And the classical one is useless. All right, here's one that I think is really important. This is a California census problem. It's estimating a logistic regression coefficient of income when predicting whether a person has private health insurance or not.
06:04:14.460 - 06:04:39.012, Speaker B: Kind of a standard thing you would predict in a public policy sort of situation. So it's using a boosting model and so on and so forth. Everything's being done correctly. If you use the computer vision model here, the imputed interval, you get out is that cold region there, which is completely off. And you're very sure of yourself. You'll tell your boss, yeah, it's over here for sure, but it's just dead wrong. That's surprising to a lot of us, I think, who are used to these models getting better and better and better and better.
06:04:39.012 - 06:04:52.548, Speaker B: Well, not always. And you can see the prediction powered here is, again, fixing that problem. So that's my last slide. I'm not going to go through this. I think I have 30 seconds left. So let me just say what this is doing. It's kind of, in some ways, classical statistics.
06:04:52.548 - 06:05:32.560, Speaker B: It says there's a bias, and that's a population parameter, and it's putting a confidence interval on the bias and then using that confidence interval on the bias to adjust all of the possible outputs of the neural network or whatever. So it's able to correct in a reasonable okay, so there's some mathematics. Okay, I'm going to leave this slide up here. This is a personal view on AI. I don't think of this as AI in the McCarthy sense at all. I think we're seeing an emerging field based on networks, data analysis flows, collectives of humans that's changing the world. And it's something like chemical engineering, which arose from chemistry and quantum chemistry.
06:05:32.560 - 06:06:06.704, Speaker B: Before 1940, there was really no chemical engineering. It arose as kind of a systems field, like, how do you do chemistry at scale in a field? All right. And electrical engineering arose. There was already Maxwell's equations. But how do you do it in a city? How do you make circuits? How do you make communication patterns? All that took 20 years to work out as an engineering discipline focused on the right goals. How do you bring electricity to a city? How do you make chemicals at scale? And the goals are important. So our field right now is not AI in terms of taking out a human and putting in a computer.
06:06:06.704 - 06:06:16.710, Speaker B: Our goal is to build data flows and markets and things that serve humans and make us all happier and better and safer. And we're not thinking about that enough. Thank you.
06:06:24.860 - 06:06:51.100, Speaker A: Thanks, Mike, for such an inspiring talk. So now we have a couple of minutes for questions from the audience. So I see a lack of diversity within this room and technology in general. And you mentioned that there's no economic paradigm in federated learning, but what is, if any, the economic paradigm for marginalized communities in technology that you have seen?
06:06:51.170 - 06:07:13.444, Speaker B: I don't have a great answer to that. First of all, all of us should be involved in this dialogue. Not just technical people, but it should be people in the communities, people in law schools, people in public policy, people throughout society, just like in the previous branches of engineering. It was not about technology being thrown out to the world. It was about a codevelopment all around the world of all kinds of people. I also go around the world. I go to China, I go to Europe, and I say, this is for everybody.
06:07:13.444 - 06:07:40.464, Speaker B: We're all in this together. And let me just though, return to my music example. Those are 16 to 20 year olds who suddenly have access to their data about their songs and are actually able to make money on it. And I can't think of a better model for technology than that. It's not about me giving you some shiny object that you play with and have fun with and I make money off of you. It's really empowering you to use technology to connect to people that you want to be connected to, and we just haven't done enough of that. Thank you, though.
06:07:40.464 - 06:08:13.640, Speaker B: Great question. Thank you so much for your talk. For the prediction powered inference that you discussed, how flexible are the models that you can train from that? Would they mostly work for linear and logistic regression? Like you said, they're completely flexible. Alpha fold, we use alpha fold, we use any neural network you want. It's the inference which is based on a particular statistic you're interested in. I was interested in there in a particular ODS ratio to compare one scientific hypothesis against another. All right.
06:08:13.640 - 06:08:26.990, Speaker B: And there there's limitations. You can't have any possible statistic, but that's part of statistical testing of any kind. We're just using this big assist from this box to help us to do science that makes sense. Thank you so much. Yeah, thank you.
06:08:31.700 - 06:08:40.000, Speaker A: As AI is shifting to more collective intelligence, how do you think evaluation should change for models that are specifically for ML and Econ?
06:08:40.420 - 06:08:54.256, Speaker B: Oh, that's a fantastic question. Again, I wish I could just answer it. I can't. It's a huge dialogue about how to evaluate. There's not going to be one evaluation. Different people will have different ways to evaluate. I do respect economics.
06:08:54.256 - 06:09:15.316, Speaker B: I've come to economics late. And so I'm a little bit of an advocate and evangelist because they think a lot about this. They don't just write down one criterion like Lee Square's Error and minimize it. They think about what are the utilities? Whose utilities we can't have one dominate. What if there are multiple utilities that conflict? What about pareto optimality, blah, blah, blah. And I've seen economists in businesses, I've spent time at Amazon. I just watch economists.
06:09:15.316 - 06:09:43.192, Speaker B: And their role is often to say, well, what do customers really want? How do we get at that? And so I think I have a lot of respect for their thinking process about how they try to do that and how they write it down. Of course, it's often very stylized. A utility function looks like this. And I come in, no one is a statistician, a data kind of person say, well, it doesn't look like that. Humans do something different. But we can put the two ideas together like in the previous talk and get a utility. It's more shape to how humans actually behave, right? But there's always going to be this is a kind of a language that we use to express our ourselves when we build a system.
06:09:43.192 - 06:10:10.130, Speaker B: And what we're going to need is people, everyday people, to learn to talk enough of this language so they can say, well, that system isn't working for me for this reason. And then you need to be able to adjust the system to actually accommodate that. Right now we're just very, very far from that. And so I think it's a hugely important problem. And so when I say collective also, as you hopefully saw in my talk, it's all about humans. It's not just the technology push. It's all about how do humans and technology come together to solve human problems.
06:10:10.130 - 06:10:32.708, Speaker B: Thank you. Hey, Michael. Great presentation. My name is Changran. I was a machine learning engineer. I work mainly on faster training algorithms and getting accurate higher. But I would say the presentation that you did is mind blowing to me.
06:10:32.708 - 06:10:56.040, Speaker B: And I guess my question is a little personal that is there any more materials and introductions and tools for me to learn more in this field? Great. Thank you for asking. I have the same trajectory as you. I spent most of my career just trying to make things faster and blow. And that's great and I've loved it and very important, and I will continue. But also I was often very interested in error bars. It wasn't just enough to be fast, it was fast and accurate.
06:10:56.040 - 06:11:19.088, Speaker B: And so Jan Stoich and I had a number of papers on fast, accurate error bars. So I still very much believed in that. But then that led me to this further expanding discipline. There are conferences, the EC conference, economics and Computation. I've been going to that. It's great people and a lot of them are working on these kind of problems. I gave a keynote there last year of something of this forum saying, hey, you guys are doing great, but you need to bring in a data analysis and machine learning.
06:11:19.088 - 06:11:40.750, Speaker B: And they already kind of were, but I think they're also getting excited about the possibilities here. So that's maybe just go to that conference and start looking at some of the people that go to that and look at their websites and all that and then they give talks. There's YouTubes. But yeah, you got to reach out a little bit. This is not the first thing you'll see when you hit Twitter. This is something else but you'll find it. Thank you.
06:11:41.200 - 06:11:43.230, Speaker A: We'll just take one last question.
06:11:45.440 - 06:12:08.156, Speaker B: Hi, Mike. My name is Eitan. Hi. So in statistical contract theory, you have this theta which comes from nature. But in reality when you want to design this contract, you don't actually know theta, you don't actually know the parameters which parameterize the humans. Now has there been theory about collecting this data in an online setting?
06:12:08.188 - 06:12:08.432, Speaker A: Right.
06:12:08.486 - 06:12:33.612, Speaker B: So you design this contract and you somehow need to modify this in an online setting. Yeah. So first of all, everything we're talking about here is done. We do the statistical analysis on top of that we don't know the data, but we can prove that it will work in a statistical sense with high probability. That's the first point. You don't know the data. Like you get the data once you have this out, this product, this pricing, right? Only then you can know.
06:12:33.612 - 06:13:01.752, Speaker B: So what we wanted, we have other papers that it wasn't just that paper but where we incentivize data providers to provide more data. Because for example, we get some data from multiple providers, we make some decision and we recognize that our AirBar is too big. So we need to get more data to start to make a real decision. So we got to incentivize the providers of data to provide more data. But data costs them something and it reveals something. They lose some things like privacy and all that. So we've got to design our incentives to do more of that kind of thing.
06:13:01.752 - 06:13:13.304, Speaker B: Ah, my colleague Nika Hakdelab here has a number of papers on this. On there was one that won a best paper prize at Newups this past year on on demand sampling. It gets it exactly that issue.
06:13:13.342 - 06:36:04.506, Speaker A: Thank you. Yeah, thanks everyone and thanks Mike for agree for amazing keynote. So now we are going to take a break. 20 minutes break. We'll be back, back at 330 for our final session of the day. Thank you. Hi everyone, thanks a lot for being here.
06:36:04.506 - 06:37:01.750, Speaker A: We are going to start our next session now. Could you please take your seats. So we are going to start our next session in 1 minute. Please take your seats. Okay, thanks. For people who are still standing, could you please take your seat. We are going to start the next session.
06:37:06.410 - 06:37:07.160, Speaker B: Thanks.
06:37:10.170 - 06:38:18.776, Speaker A: So again, if you are still talking, it'll be great if you can end the conversation now and take your seats. If you are in the room, great. Thanks everyone for being here. I know it's been a long day but a really exciting long day. So now we are going to enter the final session of our summit today. So, so far we've been focusing on a number of these really exciting topics on open source LM and decentralized training, machine learning infrastructure, open source LM tooling and ecosystem, and also from the multi agent economics perspective. So now for the last session of the summit, given that the summit will actually focus on the intersection of decentralization AI, the last session, we are going to be more forward looking and we are going to actually move more towards the decentralization side.
06:38:18.776 - 06:39:12.600, Speaker A: We are going to look at a number of really interesting and also challenging questions. One is actually how does decentralization technologies can help make better AI? And also we are going to look at how we can build a cooperative AI with decentralized trust. And also how these exciting AI LM technologies can help with some of the decentralization apps and technologies as well. So with that, we are going to start our part one for the last session on building AI machine Learning infrastructure services with decentralized trust and after. A professor from UCL is the chair for this part and as is going to introduce.
06:39:20.830 - 06:40:04.970, Speaker B: Thank you, everyone. So we have an exciting lineup today for the session of infrastructure and decentralized trust. Our very first speaker today is Pramad Visnavath. He's from Princeton and his primary flame nowadays is really to kind of think about how can I design and understand blockchains from a first principles thinking, which I really love. So thank you very much, Pramut. The floor is yours. All right.
06:40:04.970 - 06:40:41.572, Speaker B: Thank you, Arthur. Thank you, Don, for organizing this and inviting us. I'm interested in blockchains. I also like first principles thinking. And in this project we've been thinking about how to design stack, you could say from the network layer all the way to inference. That would be decentralized. The idea would be to have as few trust assumptions along the way and have AI service in a way that is provided by different parties with different participants.
06:40:41.572 - 06:41:18.100, Speaker B: Let me give a flavor. So the traditional way in which AI is being offered is the web two design, which is vertical integration of services. You have hosting infrastructure at one end together critically with billing and metering and payment and user interface. Perhaps through an API or a web interface, you have a chat window and you get that. But the key is that you have authentication username password, but billing and metering along the way. This is a very classic vertically integrated web Two design. The entire trust and billing metering, everything is managed by the provider.
06:41:18.100 - 06:41:44.984, Speaker B: Single point of trust. Of course, I'm here at the church, so I don't need to sort of speak to this. The goal really is that there is a high bar to entry. And unlike search, right in the very first session, there was a great point being made. There's only one search and everything gets centralized. It's very hard to have multiple types of searches. You can have a variety of AI models for every single node.
06:41:44.984 - 06:42:18.150, Speaker B: Every human really has its own individual AI. So there's a very huge need to have lots of models, one per human per task. And it really makes sense to think about them as decentralized as us, as a human society. Web Three is sort of the catch all phrase to make such a decentralization happen. So what are some desired features? So I try to write down sort of a list. The key is that whatever you need to have incentives. The incentives are the ones that tie the web of different parties to come together.
06:42:18.150 - 06:42:43.900, Speaker B: That's one side. The second is that you need to make sure that there is Byzantine resistance. You don't want to have arbitrarily bad players because you have as little bar to entry. So you need to have some permissionless aspect to it but still Byzantine resistance. And finally, and this is non negotiable is that you need to have Web Two like performance. There's no way you can have slowdown run on a blockchain. Ethereum would be like a 1986 computer.
06:42:43.900 - 06:43:32.936, Speaker B: So there's no way you'd be running this on Ethereum. And finally you would like to have few other things like privacy preserving and fairness and so forth. We'll get there. So how does this work? So you would like to have Web Two like service layer and decentralized take out the billing and infrastructure, billing and metering separately and finally have a proof system that provides robustness against disputes. A way to in which to objectively measure disputes. So these are the different participants to a decentralized platform where some people provide just pure storage, compute, some provide models, you could say the intellectual property, fine tuned or specialized models. And then there are some who maintain the system just to be able to measure who got what and to be able to validate disputes.
06:43:32.936 - 06:44:26.956, Speaker B: So what we've been doing in this project it's called Sakshi and I'll give you references at the end of this it's only a short talk is to have a layered design that separates out these different components and allows incentives to weave these components together. Even though I wrote it down like a stack, vertically integrated. Really what they are are that they are entirely paralyzed. The key is that the service layer feels like a Web Two like service except the billing and metering and authentication is taken out and the Byzantine resistance is provided by a proof layer. I just want to end this by providing what this Byzantine tolerance looks like. And the attack vectors are not all figured out but I want to give a flavor for the kinds of attack vectors and how you might want to provide resistance to that. So the first rule of thumb is that one should assume that things are appropriate.
06:44:26.956 - 06:44:52.136, Speaker B: That means it's an optimistic path, it's a normal path. Web two like service. You assume that things are being provided as they promise to be unless someone raises an alarm. And this alarm should be raised in some trust free way. So there is the alarm should not have sort of false alarms. And second is that they should be validated in a cryptographically appropriate way. Let me give an example and that may give a feel.
06:44:52.136 - 06:45:31.696, Speaker B: So here are a couple of here three examples of attacks and I'm going to focus. The most basic one is proof of inference. The AI service. You have a model and someone has a query, you get an inference. Did you actually work on the model? Did someone actually compute instead of chat GPT? Four, did they just do a GPT-3? And how you verify this and raise this dispute and verify it is what I would call proof of inference really a call to arms? Not all these proofs are set in stone. There could be multiple ways. Another one would be that if you post a model on such a website, what's preventing someone from just copying it and renaming it and also offering it as a service? So this would be like proof of ownership.
06:45:31.696 - 06:46:39.210, Speaker B: Or if you put a model, you develop your intellectual property to develop it and with your specialized data perhaps, and then someone fine tunes it for a specific application. What kind of ways in which you can say one model is a downstream version of another one, hence having access to economic value that generated from the model that you did, and that would be proof of fine tuning just to give you a flavor. So I'll just say one thing about proof of inference and then maybe we can go to discuss the details if as appropriate in the panel. Well, the natural thing is to have sort of sets of challengers and verifiers outside this service layer that would randomly check inferences and verifiers to verify whether whenever an alarm is raised. And the traditional method, of course, is that if a challenger raises an alarm, then the verifier can also check it and redo the compute and see if it's true. The trouble though is that this puts a lot of bar on the verifiers, whereas you really want to have as permissionless as many verifiers as possible. That was not good.
06:46:39.210 - 06:47:26.328, Speaker B: Yeah. And first thing, one of the things that we propose in Sakshi is to build a model bisection where you can have a specialized proof layer. So if you're verifying I just give an example, if it's a feed forward neural network, then there's a mismatch between endpoints for the same input and then you can try to do a bisection check at the middle layer and then so forth and so and divide it up. They can do a bisection. But if you have a graph or a specific network that induces a specific graph, then how do you do sort of a bisection? What's the fastest way? And this would be one example. Of how you would build an efficient proof of inference. So I want to just stop here.
06:47:26.328 - 06:48:03.430, Speaker B: So, our website is sakshi is available online. We have prototype working and basic models are available along with their proofs of inference. And I want to give a shout out. Several of my collaborators are here. This is really a multi university open source full spirit project and partly this is called to Arms. In this audience, this is joint work with several of my students at Princeton, collaborators at Qinghua University and University of Illinois Urbana Champagne along with startups, witness chain and icon layer. Thank you.
06:48:03.430 - 06:48:56.030, Speaker B: Thank you Ramot. So, we heard about proof of fine tuning. Next up we have Chao Young Hei. He is co founder and CTO at Fedml. So, Fedml is a company that's building distributed machine learning infrastructure to train and serve models. So, thank you very much. The floor is thank you for having me here.
06:48:56.030 - 06:49:32.020, Speaker B: So, today I would like to talk about decentralized infrastructure at Fennel. Actually we have done a lot of work in distributed AI platform. But today I would like to highlight only one product in our company. It's decentralized GPU cloud broker to unify distributed training, serving and collaborative learning. So, we truly believe the future of AI belongs to large scale collaboration. I think this already highlighted by professor Michael Jordan just now about data collaborations through the society. But the issue is that you can see the lab figure we only train in or reduced manner in a data centered environment.
06:49:32.020 - 06:50:20.580, Speaker B: We have federal learning go across device cross dino. But what if in the future we can unlock more data, more compute. So I truly believe that will enlarge larger scale AI. So, scalability definitely is a bot lag in such a vision. As you can see here, that the first challenge is that there's no such user to use ops to simplify the dispute training and also to maintain a large scale GPU cluster is very expensive, especially need to hire a lot of engineers and to lock in cloud price. And third is not inefficient to run in a heterogeneous environment. So, today we have deep speed macro like this modeling introduced pipeline python, but it's not efficient in a heterogeneous environment.
06:50:20.580 - 06:51:07.192, Speaker B: So our unique solution is try to unify such a scalable distributed AI platform that we hope to have a holistic design to optimize through these three layers ops distributed scalar to find more GPU resource in a heterogeneous multi cloud environment. And then you can run in a heterogeneous environment. So, how does this work? So, we just fully optimized such a simple function called phenomenal launch which takes any ML task and run anywhere. And then you can have more GPU availability to run in large scale settings. Especially you can provide some fault tolerance and failover features. So, it works like this way. First, you can run such a command to describe your features about how many GP resources you want to run.
06:51:07.192 - 06:51:32.700, Speaker B: And then we have a broker to bridge all the clouds, including individual contributors, and also AWS and some other cloud like fluid stack. And then we can launch our framework. For example, phenomenal train surf there. So with such a vision, it requires innovation from three research communities, like cloud computing. Of course, you have to scale up. So this is more like concept of sky computing. And then we need some Fortorins.
06:51:32.700 - 06:52:03.244, Speaker B: We need to install more agent to coordinate the fault torrents. And finally, you may argue that running a heroic environment is invasible. But actually, in the past five years in Ferry learning, community, people already tried a lot of heroes algorithm running a lied manner, so mathematically is already a well studied problem. So based on this setting, I want to highlight insight that Ferry learning. You can also view it in a cloud computing perspective. So if you look at fertile learning on GPU cluster, it's similar like the smartphone setting introduced by Google. Yeah.
06:52:03.244 - 06:52:23.264, Speaker B: So that's my introduction about technology. So we already trained this algorithm to do lung model scale up to decentralized matter. And this is a heroic environment. You can see the figure lens difference meaning the herogicity. So based on this, we can also scale up to smartphone for sure, but still in a single command. Just fan mail launch. Yeah.
06:52:23.264 - 06:52:48.728, Speaker B: So that's all about I hope you can remember today. Thank you for having me here. Thank you. Thank you so much. Xiaoyang our next speaker today is Ryan Zhao. He's CTO at Modulus Labs as well as CFO. So welcome.
06:52:48.728 - 06:53:34.260, Speaker B: And Ryan really loves combining AI and cryptography, which is tackling the problem of running deep learning on a blockchain, but where you don't have that much compute available. All righty, hello, everyone. Thank you so much, Arthur and Don, for hosting. See? How does this work? Cool. Okay, so hello, everyone. I'm here to talk to you a little bit about this exciting new subfield of basically zero knowledge cryptography, known as Zkml. So what is Zkml anyway? That's a great question.
06:53:34.260 - 06:54:15.552, Speaker B: Is it a buzzword? Kind of is is it a group of letters with no vowels in it? Also true. Is it machine learning with zero knowledge? What does that even mean? How can you not know anything about the AI algorithm that you're supposed to be running? Well, it's actually a little bit simpler than that. So really, there's two key properties of Zkml, and it's actually a little bit of a misnomer. It's more like validity ML or sort of verifiable ML. So in the setting of Zkml, there's basically a prover and a verifier. And it just so happens that, for example, if I'm the prover, then you and I, where you are the verifier, agree on some specific machine learning model beforehand. So we can agree on the architecture.
06:54:15.552 - 06:55:02.116, Speaker B: We can agree on, for example, the model weights. And basically, given those things, I can now produce a proof basically, of correct inference that I actually ran the machine learning model that we agreed upon on some input and got some output. And so this proof is cryptographically verifiable, so there's no way to fake it. And also it's very, very easy to basically verify. Additionally, it has some nice hiding properties such as allowing me to hide the actual model parameters so that I don't need to reveal those to you, but I can still give you a commitment to the model. Okay? So the question is, how do we actually do this? And basically we do this through this cryptographic primitive called zero knowledge snarks. So effectively, a snark allows me to prove to you that I have some knowledge of something.
06:55:02.116 - 06:56:11.256, Speaker B: So I know, for example, the thousandth iteration of Shaw on like the word cow or something like that. And so if you think of the knowledge of a computation trace as basically I did the computation, then effectively if I show you that I know the computational trace, there's no other way I possibly could have had that unless I ran the model itself. And so finally, if you think of AI algorithms as basically deterministic computation more or less for feed forward networks, then I can prove to you basically using this, that I actually ran the model. Okay? So the question is, of course, why? And the answer is, it turns out that if you apply this directly to the blockchain, if you use the blockchain as the verifier in this setting, it's effectively equivalent as you running all your models in a decentralized way on chain. So I think I'm close to being out of time, so I'll just give a real quick example, basically. For example, you can build a decentralized version of Kaggle. So currently in Kaggle, there's always the chance that basically, I guess, like, the contest operators will take your model and they'll tweak it slightly, or they'll run your model on a different test set such that you lose the contest even though you should have won.
06:56:11.256 - 06:56:44.550, Speaker B: So in the case of basically decentralized Kaggle, instead all the competitors train their models. They provide commitments to their models on chain ahead of time so no one can alter their models. And later, when the test set is published, everyone needs to provide a proof that they ran their model over that test set using the commitment that they already put on chain. And so in this way, no one can possibly basically mess with your model in any way. And in fact, you could even hide your model and never reveal it to the public if you want it for basically IP purposes. Okay, so here's another example. Here's another example.
06:56:44.550 - 06:57:07.660, Speaker B: Do we have more slides? Okay, yes. So a good question is, okay, great, why not zkml yesterday, right? Like, why didn't this happen earlier? And the answer is, it's just really hard. So it's very, very expensive. The latency is very large. And all the proof systems which exist nowadays are not very good at, basically machine learning operations. They're much more suited for general compute. So we're working on this.
06:57:07.660 - 06:57:12.430, Speaker B: This is a thing that we're building. I'll be up here for the panel. Thank you so much.
06:57:17.520 - 06:57:21.410, Speaker A: You all right.
06:57:21.480 - 06:57:59.222, Speaker B: So now that we had some very exciting talks, we're going to discuss together, and we're also very welcoming then questions from the audience afterwards. So Ryan yeah, please come on stage. Pramut, please come. Ben fielding, are you here? Ben? Very good. Thank you for having for being here and okay, great. You're also coming. All right, Ben, we haven't heard so much yet about you.
06:57:59.222 - 06:58:12.682, Speaker B: So Ben Fielding is from Jensen AI. Maybe you can give a small introduction about what you do at Jensen. Absolutely. Yeah. Hello. There we go. Yeah.
06:58:12.682 - 06:59:01.026, Speaker B: So Jensen is a decentralized machine learning compute network. Essentially, we incentivize the provisioning of machine learning capable hardware purely for training. So in contrast to some of the projects you've just seen, we're very, very low level. We provide essentially access to all of the world's machine learning capable compute hardware to people who need to train models. So if you have computational hardware that's capable of doing machine learning training, you can connect it up to Jensen, run some software, and start earning money for doing that training work. And if you need to do training work instead of using AWS or Google cloud or buying hardware in a data center, you can just send your model out to the Jensen network and it'll train on any device anywhere in the world or any kind of subset of the world's training devices. Some of the things you've seen up there could be done on top of Jensen.
06:59:01.026 - 06:59:34.514, Speaker B: So we see ourselves as basically sitting kind of just above the electricity and just providing access to that resource, changing the way kind of machine learning engineers and researchers see computational hardware as being this kind of infrastructure thing that they have to buy and use and to just a resource that's always available, a bit like electricity. It's kind of like in every home, it's in every data center. If you need a certain amount of it's almost like a commodity, you can just use as much as you need. Essentially exciting. And also, Congress on the recent 43 million series A. Thank you. So going on with our questions today.
06:59:34.514 - 07:00:48.774, Speaker B: So the first one and I think, Ryan, you really motivated the case for the need of this infrastructure quite well is the Kegel example. But maybe do you have any other examples that we can have? Why is that important to build this decentralized infrastructure? Yeah, so I can give kind of the Zkml, I guess, perspective on this, which is that in the case of basically verifiable inference, the outputs from your models are exactly what is valuable. The framework that I like to take around this is machine learning is really, really good at basically solving data driven, sort of not easily hard codeable problems. Right? So for example, a language model, and also basically ZK gives you the ability to verify that some particular algorithm was used correctly. So one cool example that I like to think about is basically the case in which you have language models as judges, right? Like as literal judges in courtrooms. Because currently you can look at a judge in a courtroom and say, I see who you are, I know where you got your law degree, I know all the cases that you've previously basically attended and how you ruled on each of them. But if you have a language model as that judge figure, you can't actually say that you have no idea where this thing was trained.
07:00:48.774 - 07:01:26.150, Speaker B: You don't even know if it was that language model that is giving you the sentence which could decide what happens for the rest of your life or set legal precedent. So in this case, each basically decision or ruling comes with a proof that it is the model that you think is being run. Actually being run to basically give you that. Perfect. Thank you. Turning to Pramut. So we kind of wonder how do we scale such systems and can you maybe share some of your experiences from your research there? Yeah, I think one of the focuses of Sakshi was to make sure that non negotiable is web two, like performance.
07:01:26.150 - 07:02:17.080, Speaker B: And sort of the design was done starting with this premise that your service should be like web two, and which is sort of counter to ZKS, which this is the optimistic path. So you assume it's okay and you make enough provisions around it for having ways in which you can have incentives to raise alarms, appropriate mechanisms to validate or resolve such alarms, and have a separate proof system around it. But the normal path is web two. So that was sort of the underlying thinking. It's a complementary aspect to ZK. It's meant where the transactions are relatively low value and so your normal paths are going fine and they integrate into a reputation system. Thank you.
07:02:17.080 - 07:03:03.542, Speaker B: Shaoyang, maybe one question for you. So we all have our smartphones in our pockets, right? So we do have some sort of potent hardware, maybe that hardware can at some point do some inference or already can. So wouldn't it be enough to just build kind of like a computer that everybody can plug at home and then we're good, we have kind of a decentralized system? Or can you explain why that is a naive thought, maybe? Yeah, this is a great question, actually. In smartphone setting, as you can see, this modeling people already present, like Google represent fair learning for language model. But actually at Fennel we also do some mobile ads and stable diffusion device training for private data. That's very promising. And in home setting, I think we already made it actually.
07:03:03.542 - 07:03:39.634, Speaker B: Which we can do fight turning on decentralized computes with the phenomena launch I just presented, people can directly plug in their individual GPUs. Maybe not GPU even. And for example, gamers may have GPU device. Maybe not that high performance, like a 100 h 100. But still we can train a medium sized model there. And even for seven B, we recently find with low rise possible. So this vision I think is really happening now but for pretraining we do see some bota lag, especially in memory setting but for the compute communication maybe that's not a problem.
07:03:39.634 - 07:04:39.474, Speaker B: So maybe in the future if at home we have more larger storage, I think it is feasible to train in a decentralized manner and especially for ops and compute libraries and also the scalar. I think if we can have more progress as the three innovations I mentioned in the slides, if we can have more progress there, definitely we can make it happen. So for pretraining you're suggesting that the memory you mean the Random access memory or the storage memory is a bottleneck? I think both. If you want to do decentralized training the data storage should be large because the model large, you have taken more data and also during the training in memory cost is huge so you may need some optimizers that can save the memory in running time. Perfect. We do have five more minutes so if you have any questions we're more than happy to take any questions from the audience if you like to. Yeah, ideally you could go to the microphone.
07:04:39.474 - 07:05:29.734, Speaker B: Exactly. Yeah. Thank you. How do you after the training of the models and when it comes to blockchain, a lot of times the data are stored on chain. How do you deal with the speed on chain, especially when it comes to training on large amount of data? Yeah, I can take that. I guess from our perspective the data doesn't necessarily need to be on chain. You need proofs on chain that prove that the data is available and kind of where the data is and that's something that we actually found reasonably difficult when we were setting out to do what we do.
07:05:29.734 - 07:06:35.050, Speaker B: There's quite a few storage networks that exist that will allow you to put proofs on chain and then point out to the data stored in decentralized storage somewhere. But if you have the use case that that data needs to be provably available to other participants in the chain, that's actually really hard to do and we ended up building it ourselves. So you can essentially use the principles from decentralized storage to create like an erasure coding system or something like that where the data sits on the participants of the chain but not actually on the chain and then you have hashes and proofs on the chain to that data and that's what we ended up building. But there's lots of work going on in decentralized storage and data availability and things. The kind of modular stack is getting a lot better, so I think things should get a lot easier very soon. What kind of platform are they stored in? When it's like, on chain but not on chain? What kind of platform do you mean? Like structure? Yeah, it doesn't really yeah, anything yeah, you can have like a committee, like a data availability committee. It's typically what I think is being used.
07:06:35.050 - 07:06:58.002, Speaker B: I mean, depending on the project, right? Exactly. Yeah. Thank you. Next question. Yeah, hi. So I work at NASA Ames and I lead some work where we're delivering to congressionally mandated AIML certification research plan. And a lot of work in certification is kind of where are the bounds where your model might perform good or bad.
07:06:58.002 - 07:07:49.102, Speaker B: But I'm starting to see that there might be some need for this proof of inference and things like that. Once I have a certified model. How do we start to prove that when you're delivering me a prediction, it's from the certified model and things like that. So just want to throw out this idea, see if you guys have other thoughts about how there might be intersections between some of these tools we're building to remove trust and then also how we certify these systems and then deploy provably certified systems into potentially real time safety critical systems. Pramut, do you want to take this? No. Okay, here we go. Yeah, so this is actually a pretty interesting case for, I guess, like, ZK snarks in the sense that the statement that you're proving is pretty flexible.
07:07:49.102 - 07:08:23.902, Speaker B: Right? So you don't just have to prove the statement. Yes, I ran this model on your input and I got this output. You can also have statements of the form. I ran this model on this pre committed validation set and over all the things in the validation set here was my F one score or over all the things in the validation set, including adversarial examples here was like the breakdown of accuracies. And so it would be really cool if you basically have these pretested certificates that are associated with specific model commitments which are later used to basically generate proofs of inference. Sounds very interesting. Great question.
07:08:23.902 - 07:09:10.502, Speaker B: Thank you. Next question, please. Thank you, guys. It seems zero knowledge proofs are quite interesting and it gets us in a safer way when it comes out to optimistic paths. But Professor Michael provoked us to think about these collectives in the earlier session and I was kind of curious to hear from you guys. How do you guys see this zero knowledge applied to more models that are looking in seeking equilibrium or dynamic workfields? I was curious to hear more about it. Maybe I can provide just a context about how much compute resource we need to do the DKP for machine learning.
07:09:10.502 - 07:09:45.938, Speaker B: Actually, recently we did a research about doing transformer and tried to prove the inference on chain so we can do offline computing, definitely for inference training. But we just started live transformer only two layers, for example. Then we try to prove on chain we find that even we leverage this video computing, it still takes a long time. So the latency of such proof on chain may be super high. So I think this is a contest, what I learned. Maybe the other panelists have more automated solution. All right, I'm sorry, we have no more time for further questions.
07:09:45.938 - 07:09:55.400, Speaker B: But yeah, thank you so much. And yeah, thank you. Thank you for the speakers. No, I mean, this was really pleasure. This was part one. Thank you.
07:10:14.840 - 07:10:24.390, Speaker A: Yeah. Thanks for the last session. The next session, Xin from flashbox is going to chair the session on cooperative AI with decentralized trust.
07:10:27.580 - 07:11:18.438, Speaker B: Nice. So on this panel, we're going to focus on how decentralized trust and specifically commitment devices, mechanism design, contract theory allow AIS to coordinate better, more robustly. And we'll begin with a talk from Gordon, who's the chief economist at Circle and advisor at Uniswap. Well, thanks so much for including me in this session. I will give a quick introduction to this panel discussion. So this session is on cooperative AI with decentralized trust. And so first of all, why is cooperative AI research important today? Well, most AI research and developments so far have focused on single agent or adversarial settings in which there are multiple agents.
07:11:18.438 - 07:12:02.494, Speaker B: That is zero sum games. I think the prior session noam presented a case in which there is multi agent cooperation in a diplomacy game. But those games are actually very frequent in the real world. So cooperation is often needed to achieve the Pareto optimal in games, which is a game thetagical concept. And one example of this is President's Dilemma, which I think many people have seen. This where you really need to know the payoffs of other players as well as being able to foresee what the other player would do to achieve an equilibrium in which no one's made better off without somebody made worse off. And there are many real world examples in which cooperative AI could play a role.
07:12:02.494 - 07:13:01.666, Speaker B: For instance, self driving technology in which there's interactions between different cars, algo pricing models, for instance, airplane pricing of Airfare. There's no coincidence why airfare oftentimes seem to be all bunched together at the same price and then things move together. Trading bots that could generate externalities in terms of causing flash crashes. Those are all examples of cooperative AI. Now, what are the main challenges to cooperative AI? I would argue is typical challenges in contract theory. So earlier, the presentation by Professor Jordan actually is music to my ear because I'm an economist. The typical issues of information, asymmetry in the form of hitting information, that is, adverse selection or hitting action in the form of moral hazard is at the core of information asymmetry challenges incomplete contracting in which not all states of the world are observed in advance is also a main challenge.
07:13:01.666 - 07:14:09.240, Speaker B: Now, AIS might also lack the human touch in cooperation, that is, passive knowledge and understanding of norms, ability to communicate, signal and commit, as well as ability to recall in repeated games. I'll skip the last part up here. Lastly, why do I think decentralization could aid cooperative AI development? Well, I think this is purely because blockchain is a very good mechanism of solving information. Asymmetries distributed ledger is a great way of sharing information without committing to one centralized ledger of sharing information. In addition, having smart contracts that could be programmed for signaling and programmed for commitment is a great way of reducing hidden action problems, that is, resolving moral hazard issues. Lastly, because many of the payoffs in real world scenarios are unknown or uncertain, having tokens that could represent those payoffs in a non specular way, hopefully that is one way of allowing incentives in situations in which payoffs are unknown. So I'll stop here and looking forward to the panel discussion next.
07:14:09.240 - 07:15:17.856, Speaker B: So hello. Okay. Yeah, so thanks Gordon, for the wonderful intro to the problem. And so let's first do a round of introductions and together with the introduction you can talk about in which way do you think decentralized trust or put more simply, programmable money. This kind of concept that is pretty relevant in the study of distributed decentralized ledgers can help AIS to quote him, more robustly. Sure, I can go first. I'm Matt Stevenson, I'm the head of crypto economics at wait, can you not hear me or you need a mic? I see.
07:15:17.856 - 07:15:49.908, Speaker B: I see. Okay. I'm the head of crypto economics at Pantera. I'm a PhD specializing in behavioral game theory. Former PhD, I'm a doctor now, I guess I should say that's a little confusing. Anyway, so I think I was also supposed to answer a question about waste credible commitments. So if you go back and read the game theory forefathers back in the day in the there was a lot of writing around commitment, but there was also a sort of suspicion around commitment, at least the stuff I've read, this sense that commitment was model mispecification in some sense, right? Like who was doing the committing.
07:15:49.908 - 07:16:30.896, Speaker B: And I think usually they'd use a lawyer or something and then the question is like, well, why aren't you modeling the lawyer? The lawyer should be part of a game and you should specify it as a principal agent or something like that. Right? But there was also the recognition that commitments were really powerful. And I do think it's exciting and interesting that in the blockchain world you do have a notion of commitment that really does feel pretty ignorable in terms of strategy. I mean, you can't do everything right. Censorship causes some issues, but in general you kind of can commit and not have to model the commitment or the person enforcing the commitment as much. And so it is this powerful tool. And I would say it gets added to the toolkit in terms of all applications.
07:16:30.896 - 07:17:55.760, Speaker B: I mean, why wouldn't you use it's a very powerful tool. But in terms of what it actually does, I think hard to know, right? It can be good, it can be bad and we'll talk about that a little bit more later. But at any rate, it definitely is and it's definitely important and powerful. So I think it's good that we're here talking about it can go next. You hear me? So, yeah, from my perspective, I'd be interested on how you can think about strategic behavior in economics, where when you have AI agents in the Internet who are participating in online auctions, in how you can have trust in an online environment where I think from the economic perspective, you always think about a regulator who can ensure that if you're committing to a mechanism, you're going to be punished if you deviate. But when you're running mechanisms over the internet, it's very challenging because sometimes it's very hard to audit. In fact, one problem that I kind of became very interested is the direction of credible algorithm design where, okay, how can you run an algorithm when you cannot really ensure that this algorithm designer is going to commit to follow what he's going to do? And there is impossibility results coming from economic theory, which you cannot really have algorithms with good properties.
07:17:55.760 - 07:19:46.108, Speaker B: And then I kind of got in this rabbit hole of combining cryptography and we kind of showed that you could get away of some of those impossibility results just using cryptography. And later on when you start combining more tools from the access to a blockchain which will be resistant to censorship, or when you have zero knowledge proof, you can even extend these properties even more. So I think there the blockchain provide an opportunity to actually really be designing algorithms that are more transparent for users and that can be audit. I think auditable is very important when you have especially AI engines. How can we ensure that anyone can really check and verify if these algorithms are behaving the way you want them to? Amazing answers, and I agree, it is fascinating because in the study of decentralization, we do see a lot of when you actually implement those credible commitment devices, you see that you can't implement the perfect commitment device. And in reality, especially in the blockchain world, we see this maximal extractable value phenomena, which is that the imperfect design of the commitment device actually affect the commitment games or the coordination games you play on top of it. And yeah, it'll be a fascinating problem to see when AIS actually learn how to use those commitments to work together, would they learn the exploit or employ some strategic behavior that resembles this kind of maximal extraditable value behavior to exploit the ensung designs of the devices? Matush I know you have been thinking about reputation systems a lot and reputation systems.
07:19:46.108 - 07:21:03.356, Speaker B: Specifically as in a way to help agents who are, let's say, maybe individually trained, maybe you cannot get any data on how the other agent has behaved before. And especially in those kind of zero shot coordination scenarios, commitment can help, but then for other scenarios, maybe reputation system can help. In which way do you think those kind of two methods the reputation is more on the macro level and versus the community vice or enabling AIS to have tokens more on the micro level. How do you see them interplay together? Yeah, so I think reputation is kind of how society is very important, how entities coordinate and I think that's going to probably become more important. I think one example would be in the area of fake news. I think when you have AI agents that can just create fake content very easily, how you can create entities that are reputable over the internet. So reputation I think is going to be very important in how we can design reputation entities.
07:21:03.356 - 07:22:18.156, Speaker B: And I think together that's the component of identity, like how you can have identity on these systems. I think one of the things that blockchain have solved is even without formal identity there are still ways to almost to implement some level of reputation through mechanisms such as staking a certain amount of value onto smart contracts that are just going to execute regardless on slashing functions. So that's one potential way of getting around reputation. Even without the full identity, of course identity would play in. I think that's an important aspect as well because otherwise you do have issues of civil attacks, which is also a common problem in decentralization. But I do think that using tokens, using blockchain mechanisms you could ensure there is some level of reputation associated with different bots or agents. Yeah, I definitely like the sort of staking slashing approach trying to work around the incentives versus the reputation.
07:22:18.156 - 07:22:46.976, Speaker B: I think reputation is super interesting, but I feel like you have this problem where reputation is uniquely saleable or transferable in a blockchain context, which just makes it hard to work with. Right. It's something especially with account abstraction and so on. Right? Like there's this idea that maybe you could build up a reputation and then you could sell that reputation when it's valuable enough that somebody could exploit it. Which means nobody should trust your reputation to begin with. These sorts of things just feel like they're circles a little bit. Obviously reputation winds up being a super important aspect of the way things actually function.
07:22:46.976 - 07:24:06.516, Speaker B: But in terms of the way you can work with it formally, I think it's one of the most challenging ones. But that's just my bias, I think. I guess maybe taking a step back because all the previous talks, many of them is more on decentralized infrastructure, right? So it's really not as specific games, specific interactions as those AI agents engage with each other, but more at the regulation then and I want to address to Gordon specific since you previously work in the traditional finance area so in which way do you foresee? If we allow or train AI agents to gain this ability to, say, use smart contracts to commit to coordinate with each other, what kind of interplay with policies do you foresee and then how can those two things be complement with each other? Right? Because policies can intervene with the training process whereas commitments are more microdable. And by the way, we're going to have questions at the end so for those of you who want to ask, please line up. That's a great question about how do policy also interact with everything. Well, first of all I think there's two separate decentralization going on. One is at the training level and that is perhaps adding in incentives for the data sharing infedited learning, et cetera.
07:24:06.516 - 07:25:27.910, Speaker B: But then separately there is at the inference level. Once you have the model at the inference level, how much cooperation is optimal? And I think one area in which policymakers and regulators would care a lot about is ensuring there's fairness, ensuring there's societal optimal outcome in terms of the level of cooperation. We take for granted that cooperation in AI is good but if you have too much cooperation in some contexts that is essentially collusion and collusion and antitrust type of behaviors is suboptimal for the consumer. So while it could be optimal for those who are running the programs, it could be actually pretty harmful. And this is why I think having too much centralization in modeling, centralization in terms of knowing all the payoffs could actually be quite harmful in that you don't introduce enough variability, you don't introduce enough heterogeneity in the outcomes. That you essentially have a centralized model that could squeeze perhaps too much rent, generate too much rent, for instance, in a pricing platform for airline tickets or a platform for any sort of merchandise, which we have already seen. Some examples of in economic literature have studied this.
07:25:27.910 - 07:26:04.610, Speaker B: So Gordon just made a great point. I want to make sure I alleyoup it here. Basically he was making the point, or one of his points was that cooperation and collusion are kind of just two sides of the same coin, right? Very important to keep in mind. Some of you, many of you probably already know it, right? But you often get these things presented where it's like there's a prisoners to limit it. Wouldn't it all be nice if we could cooperate and then the world would be great. But it's like cooperation is done well in mafia settings and stuff too, right? There's a lot of ways in which cooperation can sustain these things that we don't think are good. And maybe one of the weird sort of interesting ways to think about commitment is when you start to look at these models with commitment in them.
07:26:04.610 - 07:26:28.152, Speaker B: There's this literature called Endogenous Games and when you look at the endogenous games literature, the first thing that sort of sticks out at you is huge multiplicity, a lot of different things can happen and that might actually wind up being kind of a good thing. Right. The fact that we have some multiple equilibria that we can still sort of play with rather than a perfectly solved thing via command. Absolutely. Many open questions and we now actually would take one audience question.
07:26:28.206 - 07:26:29.130, Speaker A: Please ask.
07:26:31.260 - 07:27:18.120, Speaker B: Does that work? Cool. Hey, so the question I wanted to ask is actually about the compositionality of economic mechanisms. I think in a lot of the more low level, like cryptographic, primitive type systems, they're about what you can do and can't do. And when you move up into economics and you're incentivizing people to do things, the kinds of system level properties that emanate from those mechanisms aren't stable under composition. So I'm wondering how you guys think about designing mechanisms for incentivizing cooperation when those systems are layered together and potentially just either break or otherwise lump up in ways that are unintended. And how do you approach that kind of mechanism design? Yeah, that's a great question. I think a big challenge in game theory.
07:27:18.120 - 07:28:07.864, Speaker B: I don't have an answer. The theory is very limited of what you can get and you can prove. There is some interesting literature on Pride of Anarchy which you can prove composition properties, but as beyond that, I don't know much. And you can come up with examples where composition break and the cryptography is great because crypto gives you composition, but game theory is just so more complicated. Just to say a good word about game theory, there's a quote I like from Ken Benmore where he says all of social science is ultimately going to be game theory. We just know very, very little of it. I think that's kind of right.
07:28:07.864 - 07:28:42.656, Speaker B: I think it feels like we have the right tools. But Zargham, you're right to push at the sort of you might call it like construct validity, which is to say we have the idea of a strategic agent. Okay, well, what is a strategic agent? Is it a person? Is it I don't know. Can you negotiate with somebody's little finger? Obviously not, right? And those things really do start to bind when you try to formalize these things, I think, especially in an AI context. So it's exactly the right question. I think that's probably it's pushing at this boundary between theory and the application, I would say. All right, amazing tone to end the panel.
07:28:42.656 - 07:28:43.990, Speaker B: Thanks for.
07:29:03.280 - 07:29:22.870, Speaker A: Great. Thanks a lot for the great panel. So next we are going to have the final part of this session on how AI can help with decentralization and security. So first we are going to have Arthur Jevis, professor from UCL, to talk about his work in the space.
07:29:29.960 - 07:30:02.556, Speaker B: Thank you everyone. So, yeah, I am at UCL, but actually I'm mostly a prompt engineer I guess, as most of you also nowadays. So I want to talk briefly today about the dark forest of DeFi. We've heard in the previous panel about mev minor or maximal extractable value that's value that's extracted. And we can see that there's a lot of value being extracted. Obviously that depends a little bit on the market conditions. But roughly speaking, and I say roughly speaking, there's about a billion dollars per year being lost to DeFi attacks.
07:30:02.556 - 07:31:10.752, Speaker B: But don't ask me for any definition of what an attack is. At least the community thinks there are attacks and roughly half a billion are being extracted through, for example, arbitrage liquidation and sandwich attacks. So what can we do to secure smart contracts? What is kind of like the option to really defend against these and other malpractices in this space? And for quite a few years it was believed that, well, all we need is an audit, a manual audit, right? But that's not everything we can do, right? After a smart contract has been deployed, we can still do things, right? We can have an introduction detection system, we can search for vulnerabilities, right? We can do similarity detection among past attacks and new transactions, how similar do they look? And even once you have been hacked, you can still do something because many attackers that just leave money on the table, sometimes millions after an attack can still be recovered. And we've seen this in practice. So the three points here that I highlighted in bold is where we try to apply some AI techniques. And I'll briefly be discussing two. One is an audit.
07:31:10.752 - 07:31:50.370, Speaker B: So a pre order. So what if you have like a website where you can just copy paste your code and then after five minutes you get an email with, okay, here you have your automated smart contract pre audit that was generated by an LLM. That would be amazing, right? Well, it would be, but it doesn't exist yet. So what we have tried is to take kind of like a smart contract as a prompt engineer, right? Add a definition of a vulnerability. For example, a reentrancy is a very common bug in smart contracts. And then you feed this to some models such as Chat, GPT, Cloud, Llama or whatever your favorite model would be. And ideally these models then come back with some useful answer.
07:31:50.370 - 07:32:40.960, Speaker B: Well, these are the results that we've got and we have gone over quite a few vulnerabilities and I mean, all the data is actually public in the paper. If you look for do you still need a manual smart contract audit? But here you can see the results are rather not great. So we still have a lot of false positives. Obviously a random model is performing worse, but we have less true positives than a random model, which is not great. So there's still a lot of manual auditing here that's being needed, right? And this is what can be done before deployment. So what about after deployment, post deployment? What can we do? Well, we can actually, again, use an LLM. More specifically, we can pretrain a model where we take a set of unlabeled data.
07:32:40.960 - 07:33:45.856, Speaker B: So we take, for example, a set of blockchain transactions, and we let the model learn how does the transaction execution look like, and then we feed it a new transaction and ask it, how unlikely is that transaction, right? Or how unlikely is it that this kind of transaction would execute? And we've seen that if you specify a certain fixed parameter of an alarm threshold, that capable of detecting attacks. So we have a data set of, like, roughly 70 million transactions, of which 124, and we test on 124 transactions, then that are actually attacks. We find that 20 of these 120 attacks are ranked as most abnormal, which shows that this is actually a feasible mechanism. The advantages of training an LLM here is definitely that there are no engineered rules, right? You just throw data at us. You don't even need to label data for training, right? This is unsupervised learning. It allows us to attack new attacks that were not covered by known rules. So you don't have to engineer your rules.
07:33:45.856 - 07:34:37.480, Speaker B: You don't have to look up what is this new DeFi application and what are the features, and so on. So you can literally just train it by throwing data at it. But what I feel personally even more interesting is so attacks are very easy to detect because they move millions and millions of dollars, right? But such a detection mechanism, an anomaly detection mechanism, can detect sanity checks that are issued by attackers, like, as a test transaction before the actual attack happens. So it makes a differentiation not only in the mounts, but actually on the entire semantics of a transaction. All right, so here are just some overview of the results. So here you can see a table with actually a set of attacks that this model detected. I think the most significant one is the beanstalk, which resulted in about $180,000,000 of losses.
07:34:37.480 - 07:35:06.930, Speaker B: Yeah, that's the first unsubs learning detection mechanism. And yeah, we have an amazing throughput at least a batched throughput of about two transactions per second, 2000 transactions per second. And that's not bad, considering the roughly 1520 transactions per second maybe that we're seeing on, for example, Ethereum. So it's a scalable approach, and it's just the very first version, a few versions down the line, this can just get better. Thank you very much.
07:35:17.840 - 07:35:30.390, Speaker A: Thanks, adher. Next, Victor Feng, CEO from Anchin, is going to talk about their real world experience in applying AI in web3 security.
07:35:36.070 - 07:36:15.114, Speaker B: Thank you, everyone. So, yeah, today I'm going to talk to you about some of the real cases, how we leverage AI LLM to crack some of the use case in the real world. All right? And hey, thanks for inviting me back. So we're actually the first cohort of the Berkeley blockchain accelerator. I know right after this we have a great demo that coming up. So all the best with all this that are going to pitch awesome cool. So yeah, we are the leader the web Three security and compliance entrusted by more than 100 customers globally and including some of the biggest name in the industry.
07:36:15.114 - 07:36:56.326, Speaker B: For example, the US government SEC and IIS and enterprise Salesforce and repo. We're a global company making global impact. So two parts to the talk, one is let's dive straight into the status quo of the web Three security and great that Arthur actually mentioned some of the critical problem. So we're the primary investigator for Harmony for example. It's a big exploit and then we are the technology behind some other major exploit investigation. So it's a $4 billion problem. 70% of them are related to smart contract and 92% of the hack smart contract were audited.
07:36:56.326 - 07:37:21.202, Speaker B: Hopefully it's not by others that AI he developed. Okay, so we need to have a different solution. And also on the other side, the regulators are facing a resource problem. There's more than 10,000 crypto crime cases waiting for to be investigated. And there's your own stuff. So second part of it, how AI can disrupt this one? Start with a simple one. Right.
07:37:21.202 - 07:37:52.766, Speaker B: So one of the problem is tracing. Okay, well how do we trace from digital asset from one point A to point B? Turns out we can leverage the similar algorithm like Worldcraft Graph Search. Okay, turns out it's actually a great solution. We build 10 seconds. We can investigate that a lot faster. But it's actually not easy to build this kind of platform given there's billions of addresses, petabytes of data and also those EVM. We have to get down to the tracer level there to pull out all the smart contract function calls and all that.
07:37:52.766 - 07:38:27.750, Speaker B: Second lom is great. That OpenAI langchain great companies all here. Thanks a lot to making our job much easier. Our technology have been behind some of the regulators and investigators and all that. So yeah, this is a quick example how we can make non developer regulators and auditor much easier to interpret what those digital assets trying to do. Last one, we map the entire blockchain using supervised learning model. We use supervised learning model to kind of try to figure out whether it's exchange OTC or any other behavior.
07:38:27.750 - 07:38:41.120, Speaker B: Right. And also we do risk score at scale. So mapping out all the risk score for antimoney laundry purpose at scale. So that concludes the talk. Thanks a lot and happy to talk to you.
07:38:48.530 - 07:39:26.174, Speaker A: Thanks Victor. So next we are going to have the lightning talk session and the startup spotlights. And after that we'll have the accelerator demo day. So we are going to start the lightning talk in a second. I just wanted to briefly mention after this there will be the reception and there also be poster sessions. And also the Lightning Talk speakers will have their posters, will do the posters here as well, so you'll get a chance to talk with them in more detail. And also the startups that are in the startup Spotlights and Accelerated Demo Day, they will be doing the tabling over there as well.
07:39:26.174 - 07:39:35.200, Speaker A: You can see the signs for the company names, so you'll have time to talk with them in more detail as well. Great. Yeah. Thank you.
07:39:53.280 - 07:40:33.592, Speaker B: Okay, so my name is Chao Feng Shu. I'm the co founder of Fuzzland and I'm going to talk about how we democratize formal method using large language model. I guess a lot of you have questions when you are interacting with the smart contract, like would you get rockport or not? And it's really hard to answer even with LM because they are not accurate. And what we do is that we first take the user query and then turn it into the formal specification or invariance. And then we use formal methods or dynamic analysis to check whether it's going to be violated or not. And then we could answer the user question using those information. So here is an example.
07:40:33.592 - 07:41:26.024, Speaker B: Suppose we want to ask are you going to get blacklisted after you buy this anon token? And then our large language model that is fine tuned could turn it into the environments on the right. And then using our own formal message engine, we could answer it with the concrete transaction that could get you blacklisted. So here is the result. We asked a few folks to try our solution and they reported that over 67% of the time the result is as they expected. And if they could correct the LM, they could actually gain 99% to be accurate. We also performed auditing using this solution and we see that in less than 1 hour we can find 78% of the vulnerabilities that are reported by other companies. Thank you.
07:41:26.024 - 07:42:54.950, Speaker B: That's it. So hi everyone, I'm Rabimbo, I'm coming from University of Houston and here we are going to talk about who is smarter and empirical study of AI based smart contract creation. So a little bit context why we need it. A lot of you here are very interested in LLMs, very interested in Chat, GPT, Palm and all of these LLMs. And you already have probably used them to produce code and also produce smart contracts. How do you know that they are correct? What is correctness? What is the definition of correctness? So what we do here okay, I think this is the wrong slide, so I'm going to just go forward. So what we do here in that we just present a method where you can extract from a data corpus, which we have collected from GitHub and this is a smart contract data corpus from where we collect the solidity files and we also collect the associated test codes and then the unit test.
07:42:54.950 - 07:43:40.486, Speaker B: And then we actually generate a template based method where we just generate that and generate what are the outputs of that? The codes which we generate from this we evaluate. Are they correct? The correctness is based out of 21 parameters which we referred to previous work. So the experiment design is twofold. The first is that where we actually have the code generated based on templates. And on the second one, we have the portion where we actually guide the unit test methods to generate a better code. From our experiment and result, we can see that the study finds that it's very less amount of code which actually works. And you can read more about it in the paper.
07:43:40.486 - 07:44:19.810, Speaker B: Thank you. Hi, everyone. I'm Shrey. I work at Microsoft Research special Projects. Today I'll be talking about what a new formal framework for privacy evaluations could look like for LLMs today. A lot of the discourse on how we evaluate the privacy of an LLM is really focused on these Stochastic evaluations looking at benchmark data sets for toxicity. Truthfulness, whether it be toxigen bold, these benchmark data sets don't fully capture the sense of risks that each context may be worried about.
07:44:19.810 - 07:45:09.026, Speaker B: As an example, if I'm a clinician, none of the benchmark data sets today would capture the risks that I, as a clinician, might be worried about. In addition to that, the data mixtures used in pretraining supervised fine tuning or reward modeling are not Verifiable. And so we have to trust the developer that they're using the data that they claim to be using. The second assumption is that so long as we perform these evaluations, these models are actually safe to deploy. And we don't talk about currently in model cards or safety cards, the use of cryptographic techniques like watermarking provenance tracking, identity certificates. And so what I'm working on and trying to build out is this stack from both data collection through to the usage policy and what people are talking about today, like zero knowledge, machine learning, trustworthy, ML safety, AI safety. It really starts from one.
07:45:09.026 - 07:45:51.360, Speaker B: How do we verify a data mixture that was used in pretraining? We can't currently do Verifiable backpropagation, but we can verify a tested data mixture. We then can verify the annotations used in supervised fine tuning. Today, we currently don't know what data vendors are using what data, so we want to create some verification around that. Similarly so with alignment. Who's participating in the alignment for whether it be constitutional, AI reinforcement learning with community feedback or other. And finally, similarly so with Verifiable inferences for evaluations. And finally, verifying, if you're a clinician, verifying that you actually have a medical degree, so that when we release something like a medical model, we can appropriately license that model.
07:45:51.360 - 07:46:12.780, Speaker B: That's it for now. Thanks. Okay. Hi, everyone. I'm Chen Mae. I'm a graduate student at Berkeley. Today I'm going to talk about our recent work on decentralized coordination and communication free learning in matching markets.
07:46:12.780 - 07:47:11.496, Speaker B: So a lot of us are these days surrounded by these emerging applications in these online labor markets, dating markets or housing markets, where one can abstract out these applications as an interaction between two sides of the market. Namely one side consumers and agents which are seeking for certain services and the other side is constitute of firms which are providing certain services. However, once you think about these modern learning algorithms in these domains, there are two key characteristics one needs to think about. The first is that in these learning domains the players have to learn about their unknown preferences simply because the scale of these markets is very large. So players a priority do not know their preferences. And the second thing is special characteristics of these markets is the resource constraints imposed due to competition. If more than one agent is learning their preferences for a particular firm, then the firm also has a preference on the agents and therefore both of the agents cannot learn simultaneously.
07:47:11.496 - 07:48:17.878, Speaker B: So in this space, our question is to develop algorithms which converge to the stable match a notion of equilibrium in this setting in a decentralized manner. Our approach in this framework lies into providing this new algorithmic paradigm which is a novel combination of stochastic bandit and adversely bandit algorithm where the stochastic bandit algorithms deals with the uncertainty in the system and the adversarial bandit. Algorithm deals with the competitiveness in the system and we are able to achieve a very efficient algorithm in this domain which has a logarithmic and time horizon guarantee. With this I'll conclude my talk. If you are interested, feel free to visit my poster. Thank you. Hi everyone.
07:48:17.878 - 07:49:36.658, Speaker B: My name is Nico and I'm the co founder of Basket. At Basket we make network effect businesses in both gaming and as well as our own social media app, picnic and network effect businesses are very popular with teenagers and we have about one in four American teenagers or 150,000,000 monthly active users using our product. So I'll be speaking today a little bit about kind of our AI strategy and how we think about some of the recent technological trends that we're seeing. So you can see we have a lot of users. So in gaming we see a big shift in how cost allocation for developing and producing games looks. In the past we have been spending about 15% of a game's earnings as a function of microtransactions in salaries for a couple of roles for VFX GFX, kind of more artistic roles like that. By using lOMs in order to generate those 3D assets, we're seeing an enormous shift in kind of the cost of producing new games where what was previously 15% of a game's revenue every month is now reduced to 1%.
07:49:36.658 - 07:50:50.940, Speaker B: And similarly we see the ability for a lot more game production, game development, and overall for higher quality network effect games and multiplayer games to be produced as a result of this. So that's kind of a core part of our strategy. On the social side, what we see is that increasingly, social networks, especially video or Feedbased social networks, need to be a little more decentralized and need to allow people to build kind of mini applications on top of the main application, particularly for tasks that historically involved a lot of human attention, like moderation, image generation, text detection, things like that. So what we've rolled out is kind of a bot store, we call it the Pod store, where various LLMs can be deployed if they want to in order to help with tasks like moderation, image generation, et cetera. So, yeah, those are kind of the two impacts of the recent technological trends. And if anyone wants to contact me, you can do so at that email. Thank you.
07:50:50.940 - 07:51:41.960, Speaker B: Hi everyone. I'm Cartin, the co founder of Hyperoracle. So Hyperoracle is building a programmable. ZK oracle network. So Hyperoracle is empowering the smart contract with arbitrary computation like statistic computation, like normal distribution, and something more complicated like AI inferencing. And so also we will enrich the data source for smart contract like on chain historical data and off chain Internet data. So CK Oracle is a paradigm shifting in the Oracle industry compared with traditional Oracle we use today.
07:51:41.960 - 07:52:31.464, Speaker B: We call them staking based Oracle. That CK Oracle is ensure the security by the cryptography, so it's a way safer and faster, the more importantly, it's more decentralized. It's the first time we can have a fully decentralized Oracle network in the history. So here is the workflow for the comparison between the transitional Oracle and the ZK Oracle. In transitional Oracle, we will need to aggregate the computation and data, fetching itself into a centralized trusted entity to put them into a smart contract. But in the ZK Oracle, on the other hand, we use zero knowledge proof to ensure the integrity for the computation and data fetching so that we can have the smart contract to verify them directly from everyone. We propose the way to ensure the decentralization for the Oracle network that we call proof of Oracle work.
07:52:31.464 - 07:53:24.200, Speaker B: So essentially, developers need to put down the bounty for the computation job they want for their smart contract to compute with, and then Zkoraco compete with each other to finish the job first and then got the reward from the bounty, smart contract and Tk Oracle will sorry, yeah, it's too long. All right. Hi everyone. All right, good. I'm Ron Bodkin, I'm co founder of Chainml. We're an experienced team of AI professionals. My background, I sold my last AI company to Teradata, was responsible for applied AI in the Google Cloud CTO office, and most recently was at the Vector Institute.
07:53:24.200 - 07:54:23.112, Speaker B: We're really excited about how much you can now do with power of foundation Models LLMs, and we've built an open source framework called Council, which is really designed for that sweet spot between first generation frameworks that are about chaining together. Simple. Commands to do things like document retrieval and out of control agent frameworks like Auto GPT that are great for experiments but you could never deploy in an application. Instead we're talking about how do you set up teams of agents, a council to work together to do powerful planning and problem decomposition. So you can rely on open source and commercial models to build really useful capabilities into applications and in a way that people can oversee it. So with that we see examples like analytics questions where you might want to break down and not just think about what tools to use, but how to give instructions so they can achieve effective results. The kind of use cases we're seeing are things like self service analytics.
07:54:23.112 - 07:55:31.822, Speaker B: So giving people who are using an application a data scientist in the box to give them real insight when they talk to their data instead of just simple SQL queries or customer service that goes deeper like customized stack overflow answers to the questions for how to integrate with your application. So we see a ton of potential in these areas of next generation LLM applications and we're hiring, we'd love you to check out our open source and I'll be over there after this talk. We'd love to talk to you. Hey everyone, my name is Scott, I'm co founder and CTO of Zana blog. Before that blog I was engineering Committee chair at Uber, supervisor and mentor all the engineering design uber my co founder chief from Berkeley, she got PhD from Berkeley, I was a PM and databrick. So what's Zada block datablock is actually building data infra for WEBC. Databrick for WEBC we have 50 competitor last year, but we only won get founder racing this year.
07:55:31.822 - 07:56:13.246, Speaker B: And we triple our edge team since then, since two months ago. And we have the early employee from Confluent, Redshift, AWS, et cetera. So why we need a data infra? I always think the whole web C ecosystem is like Uber Facebook type of two C consumer tech company. So in this Uber we have 15 percentage engineer work on data infra. So business team like protocol and chance just focus on the business logic. So they can just build for example build a Robin Hood type of experience for transaction portfolio history or do the smart recommendation system or find who is your competitor and stay ahead of the competition curve. But in web3 there is only 0.1
07:56:13.246 - 07:56:53.910, Speaker B: percentage of company developer work on data infra. So it's definitely under development. So the first thing we do is that we actually unify all the data access patterns so you can build analytics, figure out what's all historical pattern and index the data. And whenever there is a new pattern served, like for example if you do Arbitrage, whenever there is new Arbitrage opportunity, you can just listen to the data and stream the data. So we basically build like a Facebook Uber type of data infra for whole web suite. But again, during last year's building the data infra, we're talking about the data level at same level as the Netflix data level. In that case, when we building this full terabytes of data, we figure out there is actually some patterns we can optimize.
07:56:53.910 - 07:57:25.908, Speaker B: Like majority of blockchain data is append only and then additionally, people only care about most recent data. They don't care about data like a year ago. So we did a further optimization on top of it. So if you are interested in more details, you can see me after the talk there. Thank you. Hi everybody, I'm Michelle Munson. I'm co founder of Alluvio.
07:57:25.908 - 07:58:18.568, Speaker B: We've created a decentralized platform targeting the world of video and hopefully a new video economy. Enabling projects like United Masters, like Mike Jordan talked about this afternoon, the Alluvio Content Fabric is an open and decentralized platform for streaming, storage and premium distribution of all types of content. You might wonder why would we go after the video problem? I mean, obviously there are mature video platforms, but they are all limited in many ways. Most importantly, video is at least 80% of the traffic on today's internet. The way we address, however and store manage all of the files and bytes of video is not content native. Also, media applications, when you look at the supply chain, are extremely complex. And the CDNS and media clouds really haven't changed much in the last 15 to 20 years.
07:58:18.568 - 07:58:58.596, Speaker B: We felt like it was time to create a content native protocol that would be able to address the future needs of all things video over the internet and to do it in a decentralized way. So I'm going to be over at the table afterwards, along with our co founder, we'll tell you more about the technology. One thing that I wanted to say to kind of like put this into context is obviously this is meant to address not only the technological, but also the content economic needs of all creators, big and small. And then finally, we've been lucky to be involved in some really interesting use cases. This summer we had the first feature film Warner Brothers Flash released on the Content Fabric. The first film ever to be released.
07:58:58.628 - 07:59:00.650, Speaker A: On a blockchain platform. Thank you.
07:59:18.030 - 08:00:29.938, Speaker B: I'm Gregory Roshu, professor of Computer Science at UIUC, also founder and CEO of Runtime Verification. Imagine this. Imagine that you can have Verifiable computing out of the box, correct by construction fast, instantaneously for your programming language or virtual machine, or your domain specific language, or for any AI ML model, even private ones. And all of this is just half of what Pi Squared does for us. The idea of Pi Squared, which is a proof of proof protocol, is to take any claim about any programming, any programming language that's very general and generate a mathematical proof for that claim using the K framework. And then take that mathematical proof and check it with a proof checker that is implemented a ridiculously small proof checker that is implemented as a Snark circuit that generates Indiana ZK proof. This way, going through mathematical cryptographic proof of mathematical proof, you get ZK proofs for anything that is true.
08:00:29.938 - 08:01:04.960, Speaker B: In particular, you get verifiable computing for any language. You plug and play your language with the same circuit. So you get interoperability also for free, not only correct by construction, for free, and you can do the same for formal verification claims properties. So Pi Squared smoothly combines verifiable computing and formal verification and leverages them into something that is more powerful than any of them, more than the sum of the parts. If you are interested in how to use this to prove private ML models, please look after me. If you are an investor, also look after me.
08:01:11.510 - 08:02:20.518, Speaker A: Great. Yeah. Thanks a lot for our wonderful speakers from the Lining Talk Session and the Startup Spotlight sessions. So this concludes the main contents of our really exciting Summit today. So next we are going to have an affiliate event from the Accelerator Demo Day, where you'll see some really exciting startups that actually has been part of the accelerator programs at Berkeley to actually do their pitches, where you can see some of the latest exciting technologies and products being built in the domain of Web Three. And again, so right after the Accelerator Demo Day, there will be reception and also there will be the posters from the Lining Talk Sessions and the tabling from the startup Spotlights as well as from companies from the Accelerator Demo Day. Great, thanks everyone.
08:02:20.518 - 08:03:08.120, Speaker A: And also as I mentioned, so the Summit is just the beginning of a conversation to help cross pollinate this diverse domain from AI, machine learning, security and privacy, game theory and economics, as well as decentralization technologies. And as I mentioned, we are going to teach a class this fall to dive deeper in this area on responsible gen, AI and decentralized intelligence. And you can learn more about others on the RDI website, RDI brick. And also all the slides and the videos from today's Summit will be online at RDI YouTube channel as well. Great, thanks everyone.
08:03:23.070 - 08:04:01.110, Speaker B: My name is Nick Friedland. I am the operations lead for the Berkeley Blockchain Accelerator, and I wanted to welcome you to the 2023 Berkeley Blockchain Accelerator demo day. Great. So a little bit about the BBX. We have been around since 2019. We've been providing education and other resources to students from various universities as well as entrepreneurs from around the world. Since 2019, we've actually hit a benchmark of 100 teams that have come through our cohort.
08:04:01.110 - 08:05:22.708, Speaker B: You'll be hearing from Blockchain at Berkeley at the end of the presentations. And then of course our mentors and sponsors who have been helping to educate our teams and have kept this program funded for the last five years. And then also I want to make one special thank you to the fellows and the fellow leads who have been an integral part of this program. See some of them in the back. They'll be coming on to speak at the end of the event. And they have been hands on with the teams. We've had fellows paired, they're basically been sourcing them from blockchain at Berkeley.
08:05:22.708 - 08:05:48.460, Speaker B: And we've had fellows paired with every team since the start of the program. They've been instrumental in helping us run this program. And last but not least, I'd like to introduce cohort Six. From this cohort, we have nine teams are going to be presenting in person today. Each team will be presenting for about three minutes. And I have a link here for our website, for the portfolio. The teams who weren't able to make it in person, they've recorded videos.
08:05:48.460 - 08:06:18.396, Speaker B: And those videos will be available online. Yeah. And without further ado, I'd like to bring on Cube Three. Hi, thank you very much. Super excited to be here. Wonderful people. My name is Inos Gravok.
08:06:18.396 - 08:07:05.388, Speaker B: I'm a co founder, CEO at Cube Free, here to talk about security. So I promise to make this painless. First, the problem. As many of you heard, a lot of money is being spent on security, but something is still missing, still have a gap. And until companies such as ours, and hopefully many others fill this gap, web Free can't really fully take off. So first, let me tell you why I think the gap is still there, why there are still problems and products to be created. Number one, you all know when you deploy a smart contract, if you're moving value around, or if getting hacked would be a bad day for you, we'll begin by auditing it, which is effectively manually looking for potential vulnerabilities in your contract.
08:07:05.388 - 08:07:35.588, Speaker B: Super important, but not enough, because ultimately you're looking for known vulnerabilities. And you're as good as your auditor is, as good as a human being is, right. So you need more than that. You need to build on top of that. You all have seen many companies that do monitoring, basically intrusion detection systems. Those are also important, but for the most part, by definition, they tell you that you got hacked after you got hacked. And at that point in time, it becomes about mitigation and remediation.
08:07:35.588 - 08:08:20.264, Speaker B: So while those things do play an important role, that's not enough. Today, we're introducing another way, the next logical step in security for Web free, which we call transaction security. Simply put, imagine if your smart contract was smart enough to inject threat intelligence, to evaluate every single transaction, every single wallet on smart contract, trying to do business with it, and evaluate for security before that contract is exploited. That's what we do as a company. Number one, we built an IDs to evaluate every contract, identify attackers before they go on exploiting, risk, scoring those attacks, such as the Euler hack, such as the Curve hack. Many, many others. We do that.
08:08:20.264 - 08:09:21.150, Speaker B: Of course using ML models by evaluating every single thing that is deployed on chain. The moment it's deployed on chain, so be it. A contract, a wallet, a transaction, et cetera, et cetera. Now, once you know who the bad guys are, what do you do about that? We built a fairly unique runtime security product that allows smart contracts after they've evaluated the risk to decline that individual transaction without pausing the contract. So what happens is your contract suddenly has the super skills to assess the risk, decline the risk, carry on processing the good transactions without interrupting your normal course of business. We think this is the next logical step towards Web Three security, especially if you're building and deploying smart contracts. Our clients seem to agree there's some select companies that we work with, and they agree that we fill a fairly important need.
08:09:21.150 - 08:10:09.590, Speaker B: We're backed by some of the best investors out there, including Patrick Chang at Dispersion Capital can see that Blockchain and many others. We've done this before. We've built and sold two cybersecurity companies, one to Comcast, one to Oracle. So we'd like to think that we have the necessary experience to add value to a very, very smart and rapidly growing cybersecurity industry in Web Three. And finally, I urge you to go to Q Three AI, try the products yourself, sign up for free, give it a go, and my contacts are there below as well. Love to chat with you here or afterwards on Telegram. Thank you very much.
08:10:09.590 - 08:11:15.360, Speaker B: Perfect. Hi there, how's it going? My name is Graham Fox. I'm the co founder and CEO of Societal, and today I'll be talking to you about our project, Societal, which is a home base for Internet native communities. So even though Web Three community tooling has been around for quite a few years, it still faces a number of challenges that prevent it from mass adoption. The first is the current gas fee model, which creates a barrier to entry for Web Two communities moving to Web Three tech. Second, the current blockchain infrastructure requires multiples of deployments of applications on different chains to get to market. This leaves team resources fragmented and leaving their operations fragmented as well.
08:11:15.360 - 08:12:12.980, Speaker B: Third, with the dispersion of all these applications on different chains, it leads to a poor user experience as there's no home base for this community tooling to live and reside. Well, let me introduce you to the solution. Societal, a ZK rollup designed to run the entire operations stack for Internet native communities, where we're going to build the application layer and the infrastructure layer. It will be the easiest platform to onboard Web Two communities into Web Three. We'll do this through a subscription based model via account abstraction so that every community member can interact on chain without paying gas fees. Second, Societal is a ZK roll up app chain, which means that we only have one code base to manage and deploy, and we'll use crosschain communication to integrate into other chains. And third, we'll use our infrastructure layer to offer a Community Tooling app store for third party community tooling to deploy on, integrate with our UI, and offer their services to the community on our network, providing that one home base for all community Tooling product and Web Three.
08:12:12.980 - 08:13:21.240, Speaker B: Looking at the current market opportunity, did you know that 76% of Internet users participate and engage with online communities? And currently Web Three communities have $24 billion of assets under management? Just imagine how large that will be when we onboard the next set of communities into Web Three. Looking at our competitors, they range from Mati all the way to Link Three, but none of these products have the platform or tech stack required to run communities, operations and reputation systems, to have an infrastructure layer for applications to deploy on, or have an easy onboarding of Web Two communities into Web Three. Looking at our roadmap, Societal currently has our testnet deployed and running with 20 design partners on it. We're launching our public incentivized testnet in Q Four this year, with our main net rolling out in Q Two of next year. Now, the Societal team has a diverse background experience in building and scaling startups for years with a focus in product engineering and business operations. We also have ten advisors that round out our team from governance, legal, and technical. And this will be the team that onboards the next set of communities into Web Three.
08:13:21.240 - 08:14:01.120, Speaker B: We are currently raising a $3 million seed round, so if you're interested in investing, please come and see me after. And if I can leave you with one thing we want to onboard the next billion people into crypto. It won't be through trading DFI applications, it won't be through privacy apps, but it will be through Community, and it will be on Societal. Thank you. Hello, I'm Daniel from 42. Move. Not really.
08:14:01.120 - 08:14:31.446, Speaker B: Sweet. Is everyone awake? I only have three minutes. Okay, 42. So we are zapper for the IBC. Essentially, we're trying to make DFI as simple as possible for everyone. We've spoken with a ton of Cosmos users, a ton of Ethereum users, and they all have the same problems with the interchain ecosystem. Cosmos users have trouble tracking their portfolios, they have trouble searching for yields, and they're losing their tokens along the way.
08:14:31.446 - 08:14:51.306, Speaker B: And Ethereum users don't know where to start in the intertrain ecosystem. And we're making this super easy for everyone. So intertrain yields, intertrain swaps, and more. We've just launched our Alpha product this week, so we got some Alpha users on. They love what we're doing. Essentially, connect your wallet. You can see your assets sitting across chains.
08:14:51.306 - 08:15:21.874, Speaker B: You can see your assets in the wallets in the validators liquidity pools, and you can find yields across the interchange super easily. So across different types of strategies, across we've already deployed across five different chains. And essentially, the goal is to find yields across any chain as quickly as possible. Maybe in the future. This is with real world assets, it most likely will be. But for now, we're really trying to get 100 intertrain users to love what we're doing. It's all with auto compounding.
08:15:21.874 - 08:15:49.722, Speaker B: We've also got amazing partnerships within Cosmos and the IBC that allow one click anything. So this is for swaps. Select a token, select the chain, et cetera, and we show you the best routes across any IBC chain. How to swap. We've done some research. So, essentially, on Ethereum, 12% of Total TVL sits in yield optimizers, wherein in Cosmos, only 1% currently sits in yield optimizers. And that's because no yield optimizers exist.
08:15:49.722 - 08:16:12.834, Speaker B: And Dex Aggregation, 13% of total swaps on Ethereum go through Dex aggregators and only 0.6% within Cosmos. These are some of our competitors, which you can look at later. We've got a bunch of really good revenue streams that actually make sense. I am the founder. I was at Binance Labs, Menlo Ventures, and I studied at Berkeley. I got a banking background.
08:16:12.834 - 08:16:34.538, Speaker B: We got an amazing engineering team, and we are currently raising a million at a 10 million valuation. We've got really good angels in the round. We just closed off a really good angel round, and we're almost fully committed at the million. It's just a small round to continue operations and yeah, launched our alpha. So come chat to us. We'll be over there on the left. My left.
08:16:34.538 - 08:17:30.030, Speaker B: Thank you. Thank you. Hello, everyone. My name is Baron Caster, and I am the co founder and CEO of Dmox Labs, where we are building Zero Knowledge privacy products. So, what does that mean? First off, please raise your hand if you have used Venmo in the past month. Keep your hand raised if you made every payment publicly. Wow.
08:17:30.030 - 08:17:57.986, Speaker B: All right, so we do have a few, but most people keep payments privately because privacy is really important for payments. Similarly, all of our people who are using Venmo are trusting Venmo. So please raise your hand if you have been blocked from a Venmo payment in the past. All right. There we go. More than I was expecting. That's because we're trusting these middlemen to make payments.
08:17:57.986 - 08:19:09.200, Speaker B: So, problems with peer to peer payments today are that using crypto rails, it's incredibly public, and going through traditional venues has gatekeepers that we rely on. So there's a new technology called Zero Knowledge Proofs that allow you to prove something is true without revealing the underlying information. And we're using this new cryptography to protect user privacy and peer to peer payments. So far, our major product is the Leo wallet, which is not in this slide deck there we go. Which is not just the light skinned wallet on top of existing chains, but it actually has a lot of really interesting infrastructure under the hood to make sure that all of your user information stays locally on your machine and is not pushed on chain for anyone to view. This is really important for holding any assets. And we know because we built a lot of assets that utilize this technology, such as NFTs and the NFT standard, which allow people to privately authenticate that they own assets without ever sharing that information with others.
08:19:09.200 - 08:19:55.754, Speaker B: So far in our journey, we've raised four and a half million dollars from some incredible investors, and we have a really strong and lean team. So far. We built the number one wallet on ZK privacy chains, starting with Alio, and we're expanding to others. We continue to build out more on the ZK infrastructure side of things to really make private payments a part of all of our futures. This is the team today, and you see our ZK NFTs, but we're very strong on the engineering front, and we don't have any Berkeley alum with us yet. That's something we're hoping to change in the future. So thank you so much.
08:19:55.754 - 08:21:01.150, Speaker B: And if you want to make private payments part of the future and help build with us, please follow along. That's it. Hey, everyone. Today I'm going to introduce you to Corner Market, the future of local business commerce. This is Marco. Marco saved years of his hard earned money to open up shop. But unfortunately, like many small businesses, marco has to close up shop.
08:21:01.150 - 08:21:35.172, Speaker B: Let me repeat that for you. Did you know that 50% of all new small businesses fail within their first three years of opening? We did the numbers. We identified three primary reasons why these small businesses often fail. One, lack of funds for marketing and ways for their loyalty programs. Creative ways to not only entice and excite customers, but also to retain them. Number two, access to affordable capital. Traditionally, when small business owners apply for loans, they get denied.
08:21:35.172 - 08:21:52.648, Speaker B: They don't have the credit, they don't have the history. In three, small business owners are getting charged 2.6 to as high as 3.7 per transaction. I don't know about you, but personally, for me, I think it's time for a change. Hi. My name is Daenerys.
08:21:52.648 - 08:22:32.104, Speaker B: I'm the CMO. Over here at Corner Market, we're an all in one web3 marketplace where we empower small businesses by providing them free marketing and loyalty programs to entice and incite those customers to not only know about their business, but to return to them. Two, community lending. No longer do small businesses have to get denied by going to the bank. They can simply offer their deals and their products on the Corner Market, app shoppers can purchase their deals through a digital gift card voucher. Now, that business has the capital upfront to not only scale by hiring staff, but also equipment to offer optimal customer service. Experience in three.
08:22:32.104 - 08:23:03.872, Speaker B: Corner Market has a sliding scale where we're able to charge business owners from zero to 2%, but we're able to charge as low as 0%. Now, eliminating those transaction fees that once hindered the progression of a small business. Now, I know what you're wondering. How does it work? It's very simple. As a shopper, you simply download the Corner Market app. GPS will show you businesses in your local area that take crypto as a form of payment. Step two, you simply decide what you want to purchase.
08:23:03.872 - 08:23:44.400, Speaker B: Choose from your favorite cryptos, whether that's Pepe, Shiba, Inu, or even XRP. Simply go to that business. When you go to that business, you redeem the Voucher funds, go directly to that merchant at point of sale. Those tokens, those cryptos that you chose at step two, automatically get converted into a stablecoin. We're built on the BNB and Polygon blockchain, and since inception, we've been getting recognized, we've been getting noticed. We're honored to be graduated from the Berkeley and XRP blockchain accelerators. We've amassed over a thousand test users, and we recently just won first place prize at the Polygon DevX Global Tour hackathon.
08:23:44.400 - 08:24:37.890, Speaker B: Now we know we're the team to get to bring real life crypto adoption to a reality. We've all successfully been a part of previous startups, from startup to acquisition. Today's, we're raising a seed round of a million dollars. The funds will be allocated to fuel our initial growth, helping us scale to 2000 merchants, 50,000 users worldwide, bringing in $2 million of monthly transaction value, and helping us launch in five additional countries, not including the three countries that we are currently already live in. Now, we all know three minutes is not enough time to really dive into what we currently do over here at Corner Market. So take out your phones, scan the QR code, and schedule a meeting with us where you can join us in supporting small businesses and families just like Marco, where we take their business from the corner to the market. Thank you.
08:24:37.890 - 08:25:36.516, Speaker B: It okay. Lovely. So, myself is this working? Other one. Okay. Got you. Hello? Working? All right, so myself and the rest of my team were still in college. We're a little bit risky.
08:25:36.516 - 08:26:19.092, Speaker B: And half an hour today, at half an hour ago, we released a completely new feature that we're going to share with you at the end of this demo. But first, who are we? So we're cryptic and we're building the Web Three super app. Our team is a four person team from Berkeley and Carnegie Mellon. We are CS students and design students. And together we realize that Web Three has a complexity problem. So many teams, many companies focus on very small niches, very small problems. So if you want to do something as basic as swapping two tokens, you have to navigate a gauntlet of steps.
08:26:19.092 - 08:26:55.420, Speaker B: And this creates a very fragmented user experience because no one has had the conviction or the courage to create a unified, seamless experience. And that's what we're doing. So, in a perfect world, you wouldn't have to download a new wallet for every blockchain. You wouldn't need a separate app for finance or social. You would only need one thing for all of these things, and that thing that app would be the Cryptic Super app. So you can access this wallet on any device, anytime, anywhere. And we do this using a feature that we built with some grants.
08:26:55.420 - 08:27:24.904, Speaker B: It's what you see on screen right here. It's called Cryptic Sync. All you have to do is scan a few QR codes and you can sync your wallet from your laptop, your phone. If you're first creating an account, all it takes is 5 seconds. You can use email, you can use biometrics traction, $75,000 in grants, including supporting Cryptic Sync, $50,000 in stored assets and 100 plus active users. And we're trying to build something that you never need to leave. So we are in the business of fulfillment.
08:27:24.904 - 08:27:51.268, Speaker B: If you have an intention, your intention may be to connect with a friend, to make a payment, to swap tokens. You should be able to and you are able to do all of those actions from a single place, from a single interface. The cryptic web wallet. Competitive advantages. This is a big vision. This is a very incredibly difficult task, right? It's a super app. It's trying to aggregate everything in web, three in one place.
08:27:51.268 - 08:28:19.912, Speaker B: So we don't want to rely on the Google or App Store release times. We need to move fast, we need to be quick, and we need to release what we want, when we want, and we can do that. Like I said, we just released a new feature half an hour ago, MultiChain Support. We support over ten different blockchains. We're open source. Every single line of 70,000 lines of code is open source. And we have integrated apps, including some of the biggest in the space, including Uniswap, Zero, X, Magic, Eden, et cetera.
08:28:19.912 - 08:28:36.100, Speaker B: And this is where you come into play if you want to join Cryptic, if you want to test out our feature that we just released half an hour ago, scan this QR code. It's a payment link. All you have to do is click a simple button and you'll have $5 of base, which we integrated just yesterday.
08:28:52.380 - 08:29:40.148, Speaker A: We are apostine the Future of Luxury Fashion despite being a $350,000,000,000 industry, luxury fashion is plagued by counterfeiting. US. Customs seized $3 billion worth of counterfeit goods last year alone, and many more have slipped through undetected, with customers investing more in luxury fashion than ever before. And the secondhand luxury fashion resale market seeing massive growth of 33% year over year, authentication has never been more important. The industry needs a trustless authentication solution. Current industry solutions are inefficient and fall short. They often rely on human labor and expertise, such as appraisers their patchwork and address the problem in bits and pieces.
08:29:40.148 - 08:30:12.580, Speaker A: They're not publicly accessible. This means that consumers cannot authenticate their own purchases and must rely on the paid services of the aforementioned experts. The hardware is insecure and can be cloned, which. Allows for forgeries and they often rely on centralized servers. This means if a server goes offline, it renders all authentication chips utterly useless. We offer one web, three powered trustless authentication solution that allows customers to authenticate their assets with one tap of their smartphone. No app download required.
08:30:12.580 - 08:30:48.392, Speaker A: Here's an overview of how our protocol works. First, proof of origin and vital data is stored across a decentralized file storage network. This data is then encrypted on an Asymmetrically encrypted NFD chip. This chip is then embedded in the garment. And now anyone with a smartphone can verify a product's authenticity with one tap. Since we were founded in January of 2022, we've signed contracts with 240 international designers and growing. We've built a highly engaged community of both luxury customers and designers, with 178,000 followers on Instagram alone.
08:30:48.392 - 08:31:17.640, Speaker A: And we've done 453,000 in gross revenue. Our team is uniquely positioned to solve this problem due to our rich backgrounds in both luxury goods and innovative tech products. Our CTO already has a track record of building multibillion dollar products such as MetaMask swaps, which you may be familiar with. We're excited to carry this spirit of innovation forward, and we're currently raising our Precede round. If you'd like to connect with us, please scan the QR code above or look for me after the talks. I'm pretty recognizable.
08:31:34.640 - 08:32:19.390, Speaker B: My name is Mikhail and I'm the founder of Hito. Hito is a crypto wallet for the future of money with traditional finances. We have beautiful not beautiful nice and okay front end, so average user can use banking. But for the crypto, the situation is opposite. We have beautiful back end built by Gigs, but we have a shitty front end. With crypto. It's not easy to use for average users, maybe because it was built by Gigs for Gigs only.
08:32:19.390 - 08:33:12.680, Speaker B: And also, it's not easy to make your crypto secure if you're an average user. That's why I think $4 billion last year was stolen. That's why we built Hito. Hito is a hardware crypto wallet which is tangible device which makes your crypto secure, and companion app which allow you to control your assets. So we built the hardware, the prototype. We built firmware back end mobile apps. And so we launched the presale campaign.
08:33:12.680 - 08:33:57.950, Speaker B: It was successful and we shipped 300 units to first customers. And they are tearing us down, basically asking for new features almost every day. But Hito, it's not only about hardware. For example, Visa has $70 trillion turnover per year and they make $40 billion on transaction fees. So we want to build the future of money around this beautiful hardware wallet. So we want to make money on transaction fees on exchange. That's our plan.
08:33:57.950 - 08:34:39.610, Speaker B: And our team has a rare combination of skills, hardware and software. For example, me, myself, I'm a technical founder and I spent three decades building different software and advising hardware startups. So we are raising $3 million to scale production and to build value added services around our beautiful hardware wallet. Let's build the future of money together. And if you are a developer, join us as well. Thank you so much.
08:34:58.400 - 08:35:25.252, Speaker A: Hi everyone. I'm co founder of Blocklist. Welcome everyone to the new modular with Blocklist. So Blocklist is the infrastructure platform to launch, integrate and secure network neutral applications. I know what you're thinking what does network neutral mean? So stay with me. I'll explain later. But let's talk about the problem.
08:35:25.252 - 08:36:19.872, Speaker A: First. Blockchain excels at Global Bookkeeping, but it's limited when it comes to general computing. So consequently, applications are limited by the layer one, layer two blockchain networks they live on, capping the potential to scale and new use cases to grow. Here comes the solution. The modular application architecture with Blockless allow decentralized applications to operate outside the constraints of layer one and layer two blockchains and make their own decisions about the workload and consensus. So network neutral means that applications don't have to sacrifice performance and usability to fit within the capabilities of layer one, layer two blockchains they live on. There are two core technologies involved in our solution.
08:36:19.872 - 08:37:18.920, Speaker A: First, randomized node orchestration. It's a very important networking mechanism that allows decentralized applications to randomize a selection of the most suited nodes to perform the workload. This ensures optional security against such as Sibyl Attack, even more robust than a layer 1. Second, dynamic consensus and verification allows developers to choose from PBFT, Raft, Cksnarc and more in just one click and assemble them for various modules of the application. And while all while natively integrate with any layer one and layer two blockchains. So, with these solution, we offer two product offerings. First is the B two B custom built solution that allows our client projects to leverage our technology to launch their specialized networks for their applications.
08:37:18.920 - 08:38:34.160, Speaker A: We also have the B two C offering the fully managed solution provided by our self servicing platform for indie developers and smaller projects to launch instantly on the Blockless decentralized network, their features and workloads. So, I also want to share a little bit about what our clients are building. For example, humans AI uses Blocklists to launch a decentralized AI platform. They set up their decentralized network to run large language models and AI libraries to ensure also the open source model providers are being fairly compensated. We also work with Agonlayer to enable actively validated services on their platform to run within the Blockless secure runtime leveraging. Say task orchestration dynamic consensus to make sure that permissionless innovation is safeguarded by the security and stability of the network. We launched the platform for open access two months ago and these are some initial traction, all achieved through organic growth with zero marketing, customer acquisition cost spin.
08:38:34.160 - 08:38:57.530, Speaker A: Well, there's an amazing team behind Blocklist. These are the engineers. I guess we don't have time for that, but come find us if you want to run a node for our decentralized network or build something or anything that requires trustless computation, but it's too heavy to run on chain. Come find us. Tell us more about what you're building. Thank you.
08:39:14.420 - 08:39:37.310, Speaker B: Don't need to. I'm hey, everyone. Nice to see everyone here. So, my name is Shaban. I'm a senior at UC Berkeley. I was going to be graduating soon. I've been with the program as the Co head on the student side of things with the Berkeley Blockchain Accelerator for about a year and a half at this point.
08:39:37.310 - 08:40:02.900, Speaker B: Hi, everyone. My name is Gurnor Shobana, and I served as the co leads for the Accelerator this past Cohort. You guys might remember us from the last Demo Day, and I think now that both of us are moving forward from the program, we'd like to use this opportunity in the stage to present everyone with the new co leads for the Berkeley Blockchain Accelerator. So if you guys could give a round of applause to Keishav and Bell.
08:40:07.720 - 08:40:24.140, Speaker A: First of all, thank you so much to all the teams and their hard work throughout the Accelerator program. And thank you, everyone, for being here. My name is Belle, and I am currently the co lead for the Blockchain Accelerator program. And I was also and I am still a part of the Blockchain at Berkeley Club here at Berkeley.
08:40:24.640 - 08:41:04.410, Speaker B: Hello, everyone. Thank you, Gurnora and Shabbant, for leading this amazing cohort. My name is Keshav. I'm co leading the Berkeley Blockchain Accelerator with Bell this year. I want to take this moment to give a particular shout out to our fellows from Blockchain at Berkeley who helped make this Cohort a smashing success. For context, Blockchain at Berkeley is an on campus organization that drives Blockchain innovation by empowering students to make an impact through practical education, consult enterprises, and conduct open source research. The organization will also be conducting its very first hackathon this year, later in November, to foster more Blockchain development among students.
08:41:05.660 - 08:41:22.510, Speaker A: So once again, thank you so much, everyone, for being here. All the Blockchain at Berkeley and also Blockchain Accelerator representatives will be available for any intros or questions on your right at these tables right here. So feel free to go and chat with them. Yeah, thank you so much, everyone. One.
