00:00:00.330 - 00:00:27.138, Speaker A: All right. Hello, everyone. Welcome to the afternoon session. For those of you who are coming in, please feel free to take a seat. We'll be talking about scalability and consensus. So the first speaker is going to be Dalia Malki, who is the chief research officer at Chainlink, and she is going to be talking about maximum extractable value protection on a Dag. So with that, it's great to have you and we very much look forward to hearing your thoughts.
00:00:27.138 - 00:00:27.880, Speaker A: Thank you.
00:00:28.410 - 00:01:31.798, Speaker B: Two slots that are the least favorite. One just before lunch and one right after lunch. I try to keep you awake and I'll talk about mev on a Dag. And what this talk does is puts two known advances or two recently known advances in this technology and puts them together. So what are these two advances? One is advances in the performance in the throughput of Byzantine fault tolerant or BFT Consensus methods, especially ones relying on a substrate which is causally and totally ordered and creates a direct cyclical graph transport, or a Dag. And the other one are mitigation approaches and strategies for protecting against mev. And the point is, and this is part of a larger agenda, is that when you look at the technology, it's really the weakest link or the slowest link that is going to determine the performance.
00:01:31.798 - 00:02:41.882, Speaker B: So it doesn't matter if you have this really fast consensus BFT Consensus engine, but there's some component of the system which slows you down, it won't help you. You're using the wrong technology to drive performance and you'd be like this poor guy trying to cart square instead of cartwheel up a heel. So let's start with recent advances in the performance, the throughput of BFT Consensus systems. How many people in the audience here are purely applied cryptography and don't know much about consensus and BFT? Not many. How many are the other way around? They know a lot about consensus and not about cryptography. And the rest are, oh, I just had lunch. What do you want from me? Okay, so let's do a very quick historical review of this quest, which is really a four decade long quest to bring up performance throughput of BFT Consensus engines.
00:02:41.882 - 00:04:01.270, Speaker B: So it started with a seminal work by Dwank Lynch Stockmire 94, who, instead of using randomization or other techniques to solve consensus, focused on the steady state where most of the time you could just have the leader drive consensus and then there's no problem with consensus. Everybody just listens to the leader. And the only problem is to keep the system consistent and safe between times when leaders are stable and drive progress. And this was the partial synchrony model, and they showed the first deterministic consensus under these settings and really broke new grounds a decade after that. The community has mostly focused on these group communication substrates that were building the transport as efficiently and as high throughput as possible. I actually gained my PhD during that decade, also writing a thesis about these group communication peer based systems that were using, first of all, reliable communication and then building gradually more and more semantics into it. Unfortunately, this decade kind of was killed, slaughtered by a paper by Chariton and a scheme called Causally or Understanding the Limitations of Causally and Totally Ordered Communication.
00:04:01.270 - 00:05:05.318, Speaker B: And after they wrote this paper, essentially the community pivoted away from causally totally ordered communication. They said, what the hell is it good for? And for about two decades after that, starting with the first practical Byzantine consensus solution by Castro and Liskov, the community focused on pure consensus protocols in this partial synchrony model. And Castro list of introduced a very practical protocol. But it did have quadratic communication complexity. It didn't bother us. In 1999, systems had four, maybe seven nodes, so N squared communication wasn't such a big deal. But both from a theoretical curiosity point of view and because people were starting to think about scaling systems, the quest has been, can we drive this quadratic communication complexity of BFT consensus down? And the answer was positive.
00:05:05.318 - 00:05:36.434, Speaker B: There are a lot of advances, a lot of steps. And in some sense this culminated in a work that I and my co authors introduced in 2019 called Hot Stuff, which linearized all the steps of the protocol. Linearized means you arrive at consensus. All you need to do is essentially pay no more than disseminating what the consensus is. Clearly, that's the minimum you can possibly hope for. If we're all agreeing on even a single bit one, at the very least we should all know that bit. So at the very least we have to pay linear communication.
00:05:36.434 - 00:06:33.718, Speaker B: Turns out that you can actually solve consensus with that complexity. So history done. But wait, no dag, no causally ordered. What's going on? So at this point, we scaled up as a community consensus as far as we can go. What do you do when you cannot scale up anymore? You scale out. And so this notion of solving consensus while having many, many parallel proposals going on simultaneously and scaling out reemerged and in a number of works over the past five, six years, starting with hashgraph and then through a number of systems as well as academic papers, people have shown and improved, have shown remarkable performance and very elegant designs. And through better engineering.
00:06:33.718 - 00:07:39.994, Speaker B: One of the most recent advances in the field was a system called Narwhal and Tusk, which built consensus over a dag or over a causally and reliably consistent communication substrate. And I'm encountering the same difficulty that others did. So this was the first part. This is okay recess advances, getting great performance and great throughput in consensus. The second advance is people have first of all flagged and realized that transaction transparency on a blockchain makes them vulnerable to arbitrage opportunities. So front running, sandwich attacks, all sorts of opportunities. When somebody sees a transaction being submitted and they know that it will drive the price of a certain digital asset up or down because everything is transparent.
00:07:39.994 - 00:08:58.994, Speaker B: They're able to front run it, they're able to sandwich it with all sorts of attacks and essentially extract value out of knowing which transactions are pending. And so they defined in a paper in 2020 called Flashboys Mev, the maximal extractable value as the profit that can be made through, including, excluding or reordering transactions within blocks. Now, the same crowd that introduced this threat has also worked on mechanisms that actually exploit it. And if you look at Nev Explorer, a tool that currently looks at how much value has been extracted, and this is a very cautious estimate, it only looks at ETH to dollar transactions in a very, very conservative way, estimates how much value was extracted. It's in the hundreds of millions of dollars accumulated until now and in the past month since ETH merge. If you look at the chart on the bottom, people are analyzing how many blocks on proof of stake Ethereum are actually being generated and crafted by mev block builders or block builders that are extracting value. And it's staggering.
00:08:58.994 - 00:09:53.142, Speaker B: Within less than a few weeks, less than a couple of months, it climbed up to close to 50%. So half of the blocks on Ethereum right now are subject to some kind of mev by these tools. I'm not going to try to form an opinion here. There's all sorts of religions on whether mev is good driving efficiency in a market or it is bad. But one thing is very clear it is the number one cause right now and the number one threat in proof of stake Ethereum that drives centralization. So only very powerful, very capable, computationally capable block producers compete for being able to extract value. And it's a market that is inefficient and drives through economy of scale centralization.
00:09:53.142 - 00:10:53.920, Speaker B: So if you don't like centralization, you probably want to look for ways to mitigate this. And there may be other reasons like stability, fairness, or other reasons you may care about it. I'm not going to opine either way. However, if you do want to protect against it, the question is what do you do? And so in this talk, I want to focus on one particular mechanism, which is blind ordering or committing to the ordering of transactions without seeing transactions in the clear, committing to a blind ordering, and only after that opening it. This is one of the most immediate mitigation strategies you can employ and it's pretty darn effective. But the question is, how do you implement it at the throughputs of BFT blockchains, BFT consensus mechanisms like the ones that are being developed on Dags. And so this is what this talk is about, putting the two of them together.
00:10:53.920 - 00:11:41.494, Speaker B: And so in the time that I have left, in the ten minutes, maybe less that I have left, I want to very, very quickly talk about a framework for embedding blind ordering in BFT consensus engines and then how to make it efficient to meet the throughputs of these recent advances. There's one, two, three. Let's see if I can do it in ten minutes. Stop me when I'm however long I get. How much do I have? Five minutes should work. Okay, so the framework is the idea is very simple. Let's use threshold mechanisms where a user will send a transaction encrypted transaction TX encrypted with TX key, such that it requires F plus one out of three f plus one to open it.
00:11:41.494 - 00:12:48.222, Speaker B: But you can always open it with two F plus one. So this is the dispersal phase. Then after transactions are committed to the total ordering blindly, then you can retrieve the shares of the transactions and open them such that you can reconstruct in a deterministic way and a guaranteed outcome. And in a consensus mechanism, we want to operate it view by view, so that the next view cannot even start and cannot run forward before all the transactions of the previous view, the ones that were committed already are opened. And this way we guarantee that clients can have the latest state to submit transactions based on. And what are the requirements from this framework? So whenever a user sends transactions and shares the key with the nodes in the system, it should be hiding. The transaction should not be known, it should be binding, such that as soon as it's delivered, the outcome has been determined.
00:12:48.222 - 00:13:24.266, Speaker B: So it's not subject to mev itself, because what an attacker can do is it can see what order has been committed. Maybe I shouldn't allow opening it. Maybe there's more than one way to open another transaction. No, that should be deterministic. And if the user was honest and was sharing correctly, then we should open the transactions of the user. What's important is also what is not a requirement, because threshold secret sharing mechanisms are well, well known in the literature. But what is not a requirement is we don't require that every node will be able to recover the transactions.
00:13:24.266 - 00:14:01.222, Speaker B: We have a consensus mechanism that takes care of progress. We don't require successfully reconstructing the secret. If the user shared incorrectly the key. For encrypting a transaction, it's fine to reject that transaction so long as the outcome is unique. Rejection is a perfectly legitimate outcome and we don't need to compute with a secret while keeping it a secret. As soon as the transaction is committed, we open it. So it's a much, much weaker problem than usual threshold cryptography mechanisms and to implement it, there are a number of sort of obvious candidates, but one that I particularly want to highlight.
00:14:01.222 - 00:14:56.838, Speaker B: One obvious candidate is threshold encryption. Through a setup, everybody shares the secret key between a public and private key pair, and then a user can encrypt the transaction such that F plus one are needed to decrypt it. The problem with threshold encryption is that it takes orders of hundreds of milliseconds I'm sorry, it takes orders of milliseconds, hundreds of microseconds or something like three milliseconds in a fairly small system to reconstruct a transaction. And that already caps the throughput at a few hundred transactions per second. So we're not going to be able to drive high throughput with threshold encryption. Another obvious candidate is secret sharing. But we need to employ verifiable secret sharing because every subset of retrieved chairs need to yield the same outcome.
00:14:56.838 - 00:16:03.662, Speaker B: And that, again, uses nontrivial that's my timer, nontrivial cryptography and takes time. So an approach that I want to underscore is one that was recently introduced by dispersed ledger called Avidm, where the idea is that you commit to a merkle tree of all the trees, and then after you reconstruct, you can regenerate the merkel tree and thereby guarantee that the sharing was correct. So you don't do it during disperse, you do it during reconstruct. So this works very fast and is very efficient. And there are also hybrid approaches. So putting all of this framework into a Dag based consensus is something that I won't have time to talk about. But let me just mention that we've introduced an extremely simple BFT consensus mechanism on Dag that there we go.
00:16:03.662 - 00:16:47.642, Speaker B: That cleanly separates between, okay, you have a reliable causally ordered communication substrate and you can build BFT consensus on top of it. And we call our algorithm fino with no requirements on the Dag. The Dag doesn't need to be layered. You don't have to wait for a particular number of predecessors before you can send another broadcast. You don't need to wait for consensus steps or timeouts or any kind of logic. It's completely separated. And on this scrolling to the end, you can efficiently employ this blind ordering on a Dag.
00:16:47.642 - 00:17:38.494, Speaker B: And what you can get as a result of this is all parts of the system are moving at the same throughput. You don't have any bottlenecks and you avoid any of the hard, costly crypto mechanisms like threshold encryption or VSS. You simply use merkel tree commitments and verify it after the fact. And my last sentence will be I think that in generally there are a lot of components now in systems, and making sure that they all combine and you're not bottlenecked at the slowest one is really generally a really good direction for the community to work. And I'm done. And I don't know if you allow questions or we don't have time for.
00:17:38.532 - 00:17:46.500, Speaker A: One question if someone is interested in coming up to the mic. Otherwise I can invite the next speaker to get ready.
00:17:47.830 - 00:18:02.998, Speaker C: Hi, Dahlia. So just the old question. Is the algorithm reliant on the DAC data structure nature at all or it doesn't really matter. It can be applied to a linear chain based data structure as well.
00:18:03.084 - 00:18:42.560, Speaker B: So the fast answer is the framework is something that you can generally embed in any, I don't know what linear means. But in any BFT consensus, the Dag makes it much, much simpler. You don't have to say what you're proposing. It's just there in the Dag. You don't have to worry about do everybody know what the committed decision is? Because everybody interprets the Dag locally, and everybody can agree and see on the Dag whether shares were retrieved and thereby enable the next view. So the Dag makes it very elegant. But the main thing that the Dag gives you is generally it's the recent advances, and it's the highest throughput consensus that there are around.
00:18:42.560 - 00:18:46.790, Speaker B: Does that answer? Okay, cool.
00:18:46.940 - 00:18:50.440, Speaker A: Thank you. All right, thank you very much. Let's thank the speaker again.
