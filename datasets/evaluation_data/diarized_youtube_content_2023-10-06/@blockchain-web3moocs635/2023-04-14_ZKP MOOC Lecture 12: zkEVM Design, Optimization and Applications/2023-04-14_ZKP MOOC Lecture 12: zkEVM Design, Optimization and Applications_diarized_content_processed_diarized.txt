00:00:00.650 - 00:00:51.950, Speaker A: Hello everyone, my name is Ye. I'm the co founder of Scroll. I do research about Ziki hardware acceleration and Ziki proof system. Today I'm going to talk about Ziki VM design optimization and applications. I will try to give a comprehensive overview of Ziki VM, explain what is it, how we are building it, some challenges we met, and some other interesting applications that use the key VM besides us before diving into more detail, just a quick introduction of us. So what is scroll? In short, scroll is a scaling solution for Ethereum. In other words, we are building a general purpose layer two platform that inherited security from Ethereum but can be much cheaper, faster with the highest throughput.
00:00:51.950 - 00:01:31.850, Speaker A: And more specifically, we are building an EVM equivalent Zikiro app. So firstly, we are building a Zikirap solution. It's considered to be the most secure scaling solution based on the proof. Second, we can support EVM inside our Zikiro app. It's not only a specific smart contract provision language like solidity, but we can achieve a deeper level of compatibility at the EVM bycode level. And it's not only specific language but we can support all the toolings around directly. So if you are like contract developer, the development experience on scroll will be exactly the same as Ethereum.
00:01:31.850 - 00:02:16.962, Speaker A: And in this lecture it will be divided into four parts. Firstly, I'm going to introduce some background and motivation of why we need ZKVM in the first place and why it becomes so popular in recent two years. Then I'm going to guide you through a complete journey of how to build a ZkVM from scratch, including the circuit optimization and the proof system. And after that I will go through some interesting research problems. We met when we are building our thekey VM. And finally I will introduce some other applications that can also leverage the keyvM. So let's start with the traditional diagram of layer one blockchain, especially for some ziki nerds who are not even familiar with blockchain.
00:02:16.962 - 00:03:18.970, Speaker A: Let me do a quick introduction of blockchain. So a blockchain network is consist of many nodes, yearly tens of thousand nodes for decentralization and they are interconnected by a p two p network and all the nodes are maintaining the same state shown here, which is a database like a shared ledger. So you can put your balance here or some code for a program here and then you use a data structure called Merkle tree to store all those information on your leaf. And then you get a state route which is like you just do hash and then you get a digest here to represent all your state and every node need to maintain the same database. And then they will also run the same software called ECM virtual machine to do some computation and update the state route. And blockchain is also called a word computer because anyone can use that to run any program in a decentralized fashion. And the program running on top of blockchain is called smart contract.
00:03:18.970 - 00:04:15.294, Speaker A: So the EVM will load this data from your storage to some computation and revise this tree and get a new state route. And if you are a user, you can send the transactions and it will be broadcasted in this p two p network. There will be some consensus algorithm to elect a proposer in each time slot. And this proposer will pack many transactions it received into a block and run EVM with a transaction as input and update this state route. And then it will submit this block. And after saying this block is submitted, everyone in the network will download this block and reexecute the transactions inside this block to reach some consensus about the state route. So the blocks are chained together like one after the other.
00:04:15.294 - 00:05:13.940, Speaker A: And the nodes need to do a testation which is re execute the transactions inside this block for all the blocks to reach the same state route, so that they are maintaining the same database always. And there are many benefits of the blockchain system. It's secure since your transaction will be executed tons of thousands of times by different nodes. And it's also decentralized because no one can censorship your transaction. If one node is rejecting a transaction, you can easily send to the other. But it's super expensive and super slow, and many inefficiency come from all the nodes doing the same computation, tens of thousand nodes, and also need to reach some consensus. It's very common for you to pay for over $10 as a gas fee on ECM, especially when congested, it's even much higher.
00:05:13.940 - 00:05:53.034, Speaker A: So that's a problem we want to solve. We want to solve the scalability problem. And so that's why we use zikiroap. Zikiroap is a scaling solution to solve the scalability problem of ECM. Let's take a look at the diagram of the k rob. So the idea is that instead of broadcasting all the transactions on a congested and expensive layer one p two p network, we have a separate layer two network, which can be more centralized with a much higher throughput. And this layer two network will post its state route in layer one smart contract.
00:05:53.034 - 00:06:56.290, Speaker A: And then all the users on layer two need to send transactions to this a bit more centralized node, and the node will process all those transactions and generate a very small and efficiently verifiable Ziki proof PI. And this PI is saying that all those untransactions are valid. And after executing all those untransactions, the state root will be updated to state root prime and then it will update the root PI in the smart contract. So the layer one only need to verify this very single proof instead of executing reexecuting all the transactions and then update the state route. So the magic here relies on the ornamental proof to compress a huge batch of transactions into a small but publicly verifiable proof. In this way, layer one can be scaled up massively. Imagine that if a layer one network, because it's so decentralized, it can process only ten transactions per second.
00:06:56.290 - 00:08:09.878, Speaker A: Now every transaction is verifying a proof, which is basically proving maybe once on the transaction are valid, so the network throughput can increase for order of magnitudes. And also the security is equivalent because the validity of Ziki proof only rely on math and crypto assumptions, and it's mathematically equivalent to the validity of all those untransactions. Although it sounds like so magic, like you can compress a large computation off chain to this proof, but it's non trivial to build such a Zikirap system for many reasons. One is that as we all know, if you want to generate proof for some computation, you firstly need to write all your program logic in the form of the circuit arithmetic circuit. It's very complicated and hard from previous lecture, you know that you can only use some addition multiplication and similar method assertions in your circuit to express very complicated logic, including for loop FLS and all your program syntax. It's just very complicated. And also one circuit is corresponding for one program.
00:08:09.878 - 00:09:16.990, Speaker A: So which means for different application developer you need to implement your own circuits. So that's really a pain point for the Kira team in the past years, because you are basically requiring a solidity developer to write a very complicated circuit to scale up their own application. And also even after this, you need to pass a very rigid security auditing, which just takes a fairly long development time. And worst deal. Even if for example you are like Uniswap swap application and there is a lending application, for example AVI, they all build their own Zikiro app. There is still no composability, which means you can't use one transaction to include transactions on Uniswap and AVI and include that in an atomic transaction. You don't have a compatibility, but that compatibility is super important in decentralized finance because you just can't prove for a dynamic sequence of circuits in one proof.
00:09:16.990 - 00:10:05.066, Speaker A: And also you might need different on chain verifier with different v keys. It just makes the system become more complicated or need some specialized system. So that's exactly the problem we want to solve. We want developer friendly because we don't want solidity developer to write the key circuits or even touch any mess. And also we want composability because we want the applications deployed on layer two network to have atomic composability and calling each other inside one transaction. So instead of proving for application specific logic, we are proving on the EVM level with a shared like global state similar to layer one. So to build a general proper Ziki Rob, we need to general proof for arbitrary type of EVM transactions.
00:10:05.066 - 00:10:37.602, Speaker A: So that's exactly called Ziki EVM. In short, Ziki EVM can prove a batch of EVM transactions are valid using a Ziki proof. And technically speaking it's a set of circuits. And using those circuits you can prove EVM is executing correctly so that you can handle any EVM transaction. So previously when you are thinking of application specifically, you get different circuits per smart contract or per application.
00:10:37.756 - 00:10:40.394, Speaker B: But now different smart contracts are part.
00:10:40.432 - 00:10:55.086, Speaker A: Of the input to your ZKM circuits and you are only approved on the virtual machine level. It sounds like magic, but the problem is that Ziki VM is extremely hard.
00:10:55.108 - 00:11:00.398, Speaker B: To build with a super large overhead. So basically, instead of developer to face.
00:11:00.484 - 00:11:02.254, Speaker A: The problem of building a circuit by.
00:11:02.292 - 00:11:05.186, Speaker B: Themselves, we take the hardest part to.
00:11:05.208 - 00:11:09.890, Speaker A: Build a general purpose virtual machine circuit. And it's super complicated.
00:11:10.630 - 00:11:12.914, Speaker B: It's very complicated already to implement some.
00:11:12.952 - 00:11:14.366, Speaker A: Functions inside the kit.
00:11:14.478 - 00:11:18.022, Speaker B: But even now, even not only building.
00:11:18.076 - 00:11:32.582, Speaker A: For program, but you are handling the virtual machine logic, you, you need to implement a lot of opcodes, you need to handle the workflow, you need to handle the arrow cases, and just a lot of VM specific logic.
00:11:32.726 - 00:11:35.434, Speaker B: And also, even if you really write.
00:11:35.472 - 00:11:48.750, Speaker A: Out a sound circuit two years ago, people think the proving overhead for Ziki VM will be huge. So for example, it might take you like one day to generate a Ziki VM proof for untransactions.
00:11:50.050 - 00:11:52.954, Speaker B: But luckily the efficiency of Ziki protocol.
00:11:53.002 - 00:11:55.666, Speaker A: Has been massively improved in the past two years.
00:11:55.768 - 00:11:57.586, Speaker B: And so that's the reason why in.
00:11:57.608 - 00:12:16.086, Speaker A: Recent two years the QM becomes so popular, it's become really possible. At least four major reasons why it becomes possible. So first is according to polynomial commitment. So on the one hand you can have a universal setup or transparent setup with different polynomial commitment schemes, but on.
00:12:16.108 - 00:12:19.942, Speaker B: The other hand, more importantly it enables.
00:12:20.006 - 00:13:00.466, Speaker A: Flexible optimization like lookup and some high degree custom case. So imagine that previously you don't have polynomial commitment, you only have growth 16. So your bilinear pairing requires that you have to use degree two constraints, which is less flexible. And so that's why with polynomial commitment scheme you can support more flexible optimization and it can make your circuit become one or two order of magnitude smaller. And so that's why it becomes more efficient. On the backend side, the proving algorithm, like when you are generating proof, you need to run some proving algorithm. And the proving algorithm is highly parallelizable.
00:13:00.466 - 00:13:50.918, Speaker A: So there is hardware acceleration through GPU effort, IPG effort and ASIC effort, making proverb become another one or two order of magnitude faster. And finally, there is a recursive proof, and it will further lower the cost of onchain verification. So basically you can aggregate multiple proof into one proof and make the verification becomes even smaller. So combining with all those four factors, the QM becomes finally possible with around like three order of magnitude efficiency improvement comparing with what we have two years ago. So that's where scroll began. And there are many different type of um, out there. Here I will use a standard proposed by Justin Drake, which I think is the most intuitive one.
00:13:50.918 - 00:14:00.954, Speaker A: And the first one is language level. So the idea is that EVM is not designed for ZK. And the writing circuits for EVM will.
00:14:00.992 - 00:14:03.546, Speaker B: Introduce a large overhead because EVM has.
00:14:03.568 - 00:14:05.686, Speaker A: So many Zkon friendly opcodes.
00:14:05.798 - 00:14:11.614, Speaker B: And so instead of building EVM inside ZK, inside the Ziki circuit, you build.
00:14:11.652 - 00:14:21.882, Speaker A: A totally new virtual machine which is more friendly for ZK, and it enable more ZK friendly hash function and all those kind of primitives.
00:14:22.026 - 00:14:23.950, Speaker B: But then there is a compatibility issue.
00:14:24.020 - 00:14:42.166, Speaker A: Because you are using a different virtual machine which is not EVM. Wonder why you are calling yourself EVM. Because for this new virtual machine you might need new programming languages. And so developers are not really deploying what they can do on ecum. So to solve this, you also need.
00:14:42.188 - 00:14:45.362, Speaker B: To write a compiler to compile EVM.
00:14:45.426 - 00:14:59.702, Speaker A: Language like solidity and u to your virtual machine bytecode. So combining this compiler and this new zeaky virtual machine, you still get some level of compatibility with your EVM language.
00:14:59.846 - 00:15:02.890, Speaker B: And this approach is adopted by Metalab and Starkware.
00:15:02.970 - 00:15:05.850, Speaker A: So that's why it's called language level Ziki vm.
00:15:05.930 - 00:15:09.562, Speaker B: Because basically you are using a compiler.
00:15:09.626 - 00:15:14.090, Speaker A: To force compile an EVM language, not to EVM but to another virtual machine.
00:15:14.170 - 00:15:16.310, Speaker B: And then prove for this more ziki.
00:15:16.330 - 00:15:18.130, Speaker A: Friendly virtual machine efficiently.
00:15:18.550 - 00:15:23.086, Speaker B: And the second level is called bytecode level and it can take your EVM.
00:15:23.118 - 00:15:27.234, Speaker A: Bytecode and prove the bytecode is executing correctly.
00:15:27.362 - 00:15:29.878, Speaker B: In this way all the toolings around.
00:15:29.964 - 00:15:32.994, Speaker A: EVM can be compatible with the keyvm directly.
00:15:33.042 - 00:15:38.358, Speaker B: It's not specifically using a compiler to support special language, but can support everything.
00:15:38.444 - 00:15:42.602, Speaker A: That EVM can support. It can inherit the EVM execution model.
00:15:42.736 - 00:15:44.406, Speaker B: But you can still make some trade offs.
00:15:44.438 - 00:15:55.022, Speaker A: For example like you can change the data structure of your storage with some wiki friendly alternatives, but the start doesn't influence on the execution layer, it only.
00:15:55.076 - 00:16:00.286, Speaker B: Influence how you store the data. But the trade off is that you.
00:16:00.308 - 00:16:08.926, Speaker A: Get a longer proving time because many primitives are not leaky friendly and you are proving at a lower level of compatibility with EVM.
00:16:09.118 - 00:16:12.786, Speaker B: And last classification is called consensus level.
00:16:12.888 - 00:16:22.934, Speaker A: Or sometimes people call that ECM equivalent ziki EVM. It basically need to maintain everything that ECM has. It's not only EVM, but also the.
00:16:22.972 - 00:16:29.174, Speaker B: Storage and even some other information in the block header. The trade off is that it has.
00:16:29.212 - 00:17:50.030, Speaker A: A much much longer proving time and it might introduce some potential DOS attack because you need to maintain the gas price to be the same and a lot of things like that. So it's still under exploration and we are in the middle here, but we are collaborating with the privacy and scaling exploration team at ECM foundation to help ECM to scale on the consensus level and doing more exploration there as much as we can. In this section I'm going to show you how to build a ziki Vm from scratch. I will talk about how we are choosing among different circuit optimization and proving algorithms. So let's start with the workflow of thermoproof. So if you want to join a proof for a program, you firstly need to express your program or computation using a number of arithmetic constraints and this requires some optimization. And the most commonly used one are r one cs which is linear combination and plunkish archimization which is custom gate lookup permutation or IR which is used in stark containing translation constraint and some boundary constraint.
00:17:50.030 - 00:18:38.866, Speaker A: So I will introduce and compare later. And on the backend side, after you get your constraint system you will run some proving algorithm to generate a proof and you have already learned so many different proving protocols. But here I only list the most commonly used one in industry. So which is a combination of polynomial IOP with polynomial commitment scheme. So here because we are proving for EVM execution so the program is EVM itself. And for us we are using Plunk's authorization to write our constraints and we are using Plunk IOP plus kilogy peronomic commitment scheme in the backend to generate the proof. I will explain why later.
00:18:38.866 - 00:19:18.960, Speaker A: So let's start with the most familiar RNCs optimization. And in RNCs you basically need to lay out all your variables in a long array and then you define your constraints over those variables. And the format is shown here. It's a linear combination times linear combination equal to some linear combination. So here are some examples. You can just add any coefficient to those variables and linear combine and the addition is almost free here. But in each constraint the degree need to be at most two.
00:19:18.960 - 00:20:20.786, Speaker A: And when you are proving for concrete instance, you need to fill up all those variables with concrete values and proving that I know a vector which is input vav bvc blah blah blah and that is fisify all those constraints. And eventually if prover can prove that the verifier know that this computation for this circuit is carried out correctly and differently. In Plunkish organization you are layout all the variables, not in an array, but in a two dimension table. So this is a table and you will fill up all the table with your values like input variable v variable b here. And you fill up this table with all those values. And then you can define different type of constraints. There are three type of constraints which you can write about.
00:20:20.786 - 00:20:53.520, Speaker A: The first is called custom gate. So basically you can define any ship within the table that's called a gate. So for example you can define this ship as your new gate. And then you can define arbitrary relationship between those values or like in that cells within those gates. So for example, you can define v three times vb three times v c three minus vb four equal to zero. And you can define basically arbitrary ship and.
00:20:55.330 - 00:20:57.066, Speaker B: The comparison with r one CSA.
00:20:57.098 - 00:21:01.966, Speaker A: That the degree can be higher because you can define arbitrary relationship for your.
00:21:01.988 - 00:21:03.854, Speaker B: Gate and it's also more customized.
00:21:03.902 - 00:21:41.098, Speaker A: For example, you can comprise three multiplications inside one constraint. And for example you can define this ship and define arbitrary relationship in your custom gate. And you can define even some very regular ship like this. And the second constraint you can define in Plunkey's organization is called permutation. So it defines equality among different cells. So it can be used to link different gates. Like for example like vb four equal to v six equal to equal to this cell equal to this cell.
00:21:41.098 - 00:22:37.806, Speaker A: Imagine that in previous slides if you define some custom gate here, custom gate here and if you want to link different gates together, and then you can just use one permutation to prove that the output of this gatekeeper is equivalent to the input of this gatekeeper, so that you can link different gates gather. And the last constraint you can define is called lookups. So basically what you can prove is that you can define a tuple and prove that this tuple belonging to a table. Like this tuple is like way seven, way b seven, we say seven, it belongs to this table here. So why this is super powerful. Let me show you some example here. So imagine that you want to prove for the range of vc seven is within zero to 15.
00:22:37.806 - 00:23:19.194, Speaker A: So imagine that you want to prove this in rncs. What you need to do is that firstly you need to decompose this value into four bit. And firstly prove that each bit is within the range of zero to one using this value times value minus one equal to zero. So that's already like four constraints. And then you prove that linear combination of those variables equal to vc seven. So it's very complicated and use many constraints. But here what you can do is that you can brute force all the possibilities within that range, like from zero to 15.
00:23:19.392 - 00:23:21.246, Speaker B: And then as far as you can.
00:23:21.268 - 00:24:26.398, Speaker A: Prove that vc seven is belonging to this column, then it's clear that Vc seven is in this range. So you can do range proof very efficiently. And similarly you can look up a tuple to prove some bitwise operations are correct. So for example, v seven xor vb seven equal to Vc seven. What you can do is that you can brute force all this XOR table and store all the possible entries here as unfixed column. And then you can prove that this tuple belong to this table instead of proving for this ignore relationship which needs a lot of constraints in r one cs, and it can really reduce your circuit size. And it's extremely helpful not only for some fixed brute force table, but also it's extremely helpful for doing some RaM operation which is read and write operation in the context of the key vm.
00:24:26.398 - 00:24:59.970, Speaker A: Because imagine that if you are building for a virtual machine circuit in a virtual machine it usually have memory which you write some data to this memory write to some places, and then later you will read from that position. So you basically need to prove that your read and write are consistent. So you can use a table which is generated on the fly as your ram table, and you just put all the entries in this table. And later when you are using this operand, you can look up into this table.
00:25:00.070 - 00:25:02.478, Speaker B: So that's why this lookup table can.
00:25:02.484 - 00:26:16.690, Speaker A: Be extremely helpful for some RaM operation. So that's why we need this lookup so much in the KVM. I will give more specific examples later, but as a summary, here are the constraints you can use in plunkish authorization so there is custom gate, there is permutation, and there is lookup tables and air for stark is quite similar. You can think of the authorization in stark as you can only define gate to be this kind of rectangular of your adjacent two rows, and then the relationship the constraints you are writing basically need to transform the first row to the second row. So you can imagine that each row represents some state in a virtual machine and then the next row is another new state and your constraints is defining that the transition between the first row to the second row is correct. Which actually plunkish is a more generalized format because your gate can be arbitrary not only like two json rows, but you can define any shape with different rotations. So I won't go into detail for stark.
00:26:16.690 - 00:27:23.126, Speaker A: Now the question is how we can choose the front end for our ziki vM. So here is our computation we need to prove if you look at the computation inside EVM, there are four challenges. One is that EVM word size is 256 bit. So every word, every variable using EVM is 256 bit. And this is extremely problematic for Ziki circuit, because Ziki circuit usually require the field to be at least some prime field or some scalar field of EPA curve. And usually we are using some curve, even if it's some large curve, the field is usually 254 bit and EVM word size is even larger than that. So basically every time you need to use a variable, you need very efficient range proof almost everywhere to limit that you are not using some value that auto of the range.
00:27:23.126 - 00:27:40.506, Speaker A: So that's one problem. And second problem is that EVM has so many Ziki on friendly opcodes. For example, it has catch Shard 256. It's just in general, many hash functions are very zkon friendly and you need a very large and complicated Ziki circuit.
00:27:40.538 - 00:27:41.882, Speaker B: To prove for those opcodes.
00:27:41.946 - 00:27:48.242, Speaker A: So sometimes you need to have separate circuits for those hash functions and link different circuits together.
00:27:48.376 - 00:27:55.566, Speaker B: And also for some zkon friendly, even some bitwise operations at this scale is very Zikyon friendly.
00:27:55.678 - 00:28:13.850, Speaker A: So you need to handle those Ziki unfriendly logic and sometimes like even outsource that to other circuits and connect them together. And third, as I mentioned, is that it's a virtual machine circuit, where you have this problem for read and write consistency so you need some kind of.
00:28:13.920 - 00:28:16.266, Speaker B: Efficient mapping to prove that what you.
00:28:16.288 - 00:29:07.290, Speaker A: Are reading is what you previously have written. And finally EVM has a dynamic execution trace. So for example for this transaction, EVM has a different execution order because this transaction needs for example add, add, push this sequence, but that transaction is different. So you need to handle those dynamic input in a dynamic way. So that's why we need some kind of efficient off selectors to open different constraints at different positions. You will see that later. So considering all those three factors like efficient range proof, efficient way to connect circuits, efficient read and write, we want our circuitization to enable lookup.
00:29:07.290 - 00:29:15.614, Speaker A: And considering the second one where you are handling a dynamic execution trace, we want some custom gate because we can.
00:29:15.652 - 00:29:22.362, Speaker B: Just abstract those opcode pattern and make that become some IR flower circuit and.
00:29:22.436 - 00:30:00.650, Speaker A: Efficiently open those IR at different positions. So combining with all those factors we decide to use Plancky seronization. Okay, now we already figure out that we are using plancky seronization and figure out the reason why we are using that comparing against rncs. Now let's write some concrete circuits and build a real the key vm from scratch. So to write the concrete circuits, let's take a closer look at what's the exact computation we need to prove and what's happening even behind the EVM.
00:30:00.990 - 00:30:03.258, Speaker B: So we start with some initial world state.
00:30:03.344 - 00:30:09.070, Speaker A: As I mentioned, you have a database, you have state root for that, and then you receive one transaction from this.
00:30:09.140 - 00:30:10.654, Speaker B: Address to that address.
00:30:10.852 - 00:31:03.410, Speaker A: And then after receiving the transaction the node will run this transaction over EVM. And EVM is a stack based virtual machine. It will load the contract bytecode you are running and load that to memory and execute this bytecode opcode by opcode with this transaction as your input. And then when you are executing this transaction, you will result in an execution trace, look like this. So this execution trace is composed of a sequence of opcodes that has been executed on EVM. So for example it contains push, push, store, call value, blah blah and finally return and at the same time because during execution you need to write to storage.
00:31:03.830 - 00:31:05.326, Speaker B: So you need to change the leaf.
00:31:05.358 - 00:32:00.130, Speaker A: Here and so you will end up with a different state root prime here. So here is the computation under node one, this node receiving a transaction. So now let's take a look, how does the EVM kick in during this process. So since the computation you need to prove as EVM. So you will use EVM as your computational spec for defining all your constraints. So for example, because EVM is executing all those opcodes, so you need to have constraints for all the operations happening inside EVM, including each opcode circuit for each opcode for this bytecode unrolling for storage, read and write, et cetera. So all the operations inside EVM you need write some constraints inside the Keyvm to prove that they are valid.
00:32:00.130 - 00:32:40.622, Speaker A: The public input for the Keyvm will be your old state and your new state and all those untransactions. So your statement will become that, the statement that after applying all those untransactions, your state root move from this one to state root prime. So verifier doesn't need to know the concrete execution trees and the concrete like what's happening during execution. It only need to know that this is a transaction input. After executing that, you get this. So that's what the verifier can see.
00:32:40.676 - 00:32:42.478, Speaker B: Verifier only need to update to a.
00:32:42.484 - 00:33:42.494, Speaker A: New state route, but ZigVM already takes all the execution process as witness and prove that. So your execution trace is actually your witness for Ziki Vm. So again, this spec defines constraints, but you need some kind of witness which satisfies those constraints. So that's your execution trace with those information as public input and defined those constraints. Basically what you are proving inside the logic inside EVM lick EvM, is that prove that I know execution trace which is uniquely unjured from this transaction. So basically I'm executing opcode by opcode and it's valid, which means this execution tree is valid, means all the opcode computation for each opcode is valid and read and write also consistent. For example, like if you have push.
00:33:42.542 - 00:33:46.920, Speaker B: Here, if you have some kind of.
00:33:47.850 - 00:34:43.234, Speaker A: Pop here, it must be some value which you have already pushed already. As I explained earlier, a transaction is valid, is equivalent to the execution trace is valid and unique, but the information here in the execution trace is not enough as witness. So the first thing you need to do is unroll this execution trace into a larger execution table. Because I'm using table here, because eventually all those values will be filled up in a plunkish optimization table and define constraints to constrain the relationship between those values. And so let's take a closer look at what the concrete witness you need for each step. So in each step there are mainly.
00:34:43.282 - 00:34:45.762, Speaker B: Three type of different type of witness.
00:34:45.906 - 00:34:55.114, Speaker A: One is called step context, one is called key switch, and one is called opcode specific witness. So what does step context mean?
00:34:55.232 - 00:35:01.190, Speaker B: So step context means when you are executing this step with your context variable.
00:35:01.270 - 00:35:39.974, Speaker A: So for example, it contains the code hash which indicates what's the code you are executing and there is a guess left which means after when you are executing this step, what's the remaining guess? And also contains some program counter stack pointer as a context for vm contact for executing this step. And there is a key switch. So key switch contain all the possible possibilities for opcodes and arrow cases for one step. So you can imagine that if you.
00:35:40.012 - 00:35:47.062, Speaker B: Have different type of possibilities for this step then you need to have variables.
00:35:47.126 - 00:36:01.166, Speaker A: Inside this area and to indicate that what this step is used for. So for example if this step is at, then you need to put this.
00:36:01.268 - 00:36:05.794, Speaker B: S add to be one and other variable to be zero so that other.
00:36:05.832 - 00:36:12.194, Speaker A: People know that this is for add because every time as far as this.
00:36:12.232 - 00:36:13.826, Speaker B: Value to be one, other value to.
00:36:13.848 - 00:36:19.382, Speaker A: Be zero, then it indicates that this function, this step is used for this.
00:36:19.436 - 00:36:21.574, Speaker B: Opcode and you can think of that.
00:36:21.612 - 00:36:34.630, Speaker A: As an efficient selector. Select the constraints you need to apply to this area and indicates what this area is used for. And the last thing is opcode specific witness.
00:36:34.790 - 00:36:38.070, Speaker B: So it contains all the necessary witness.
00:36:38.230 - 00:37:32.030, Speaker A: That is needed specifically to this step. So for example again if you are proving for ad, then you definitely need some kind of operand, right? Like a plus b equal to c because a, b and c are 256. So I'm using high low here to represent the highest half and lowest half. So you need to put your operand here and some other information that is needed as actual witness. For example, you might need some kind of carry for add because it might overflow and need some other variables. So this place is for opcode specific witness. Now let's take a look at some specific constraints and I will take add as an example to see what's the constraints we are defining for those different steps.
00:37:32.030 - 00:37:41.422, Speaker A: So for step context, because it's basically proving that the context variable are consistent. Like when you move from step one.
00:37:41.476 - 00:37:51.458, Speaker B: To step two, step two to step three, you are executing in the same context. So basically the step context you need.
00:37:51.464 - 00:37:58.978, Speaker A: To prove for two adjacent steps the variables are consistent. So here is your constraints.
00:37:59.074 - 00:38:00.726, Speaker B: So you need to write s add.
00:38:00.828 - 00:38:08.602, Speaker A: Times pc prime minus pc minus one equal to zero, which means if s add is one.
00:38:08.656 - 00:38:13.946, Speaker B: Because if s add is zero which means this step is not for add.
00:38:14.048 - 00:38:26.254, Speaker A: So only if this s add is one, which means this error is one, then this pc prime need to plus one. So which means if this step is.
00:38:26.292 - 00:38:28.206, Speaker B: Add, then your program counter need to.
00:38:28.228 - 00:38:36.020, Speaker A: Plus one and stack pointer. Similarly, if it's add then stack pointer need to plus one and the gas need to plus three.
00:38:36.550 - 00:38:43.250, Speaker B: So this is like the logic four step context. So basically after executing add, what's your.
00:38:43.320 - 00:38:51.320, Speaker A: Context variable will look like. And for key switch, as I mentioned, like keyswitch basically indicates that this area.
00:38:52.410 - 00:38:57.062, Speaker B: What is this step used for? So the idea here is that each.
00:38:57.116 - 00:39:19.646, Speaker A: Value need to be binary. It's either to be zero or one means on or off. And at most there will be only one variables, which is one, and other variables will all be zero. So you need to write all those constraints like this. Basically it's s add times one minus s add equal to zero, which means.
00:39:19.668 - 00:39:29.714, Speaker B: S add is binary, s more is also map binary. You write this for all the values in this area and then adding them.
00:39:29.752 - 00:39:41.000, Speaker A: Together, you get one, which means, okay, so previously you said, okay, it's all binary. And finally you said there will be only one value and exactly one value.
00:39:42.010 - 00:39:44.054, Speaker B: The value of that variable is one.
00:39:44.172 - 00:39:59.130, Speaker A: So this is how you constrain this key switch to guarantee that there will be only one variable set to be one. And for your opcode specific witness, I'm also using add as an example here.
00:39:59.200 - 00:40:01.078, Speaker B: So at you need to time at.
00:40:01.104 - 00:40:04.046, Speaker A: Add because it only works if you.
00:40:04.068 - 00:40:06.878, Speaker B: Are selected to be this area is.
00:40:06.884 - 00:40:23.602, Speaker A: Select to be add. And so if it's add, then like a plus b minus c equal to zero if there are some carry, and there are some carry. So this is add specific logic. Here is a more complete example for.
00:40:23.656 - 00:40:26.806, Speaker B: Add, but you will notice that there.
00:40:26.828 - 00:40:28.790, Speaker A: Is still one missing piece.
00:40:28.940 - 00:40:35.286, Speaker B: So what's missing here is that if you are doing add, then you are.
00:40:35.308 - 00:40:39.890, Speaker A: Basically popping two elements from your stack, which is a and b.
00:40:39.980 - 00:40:51.054, Speaker B: And then you push your c value back. So here you didn't prove that a and b value are what you previously have pushed to this stack. So you basically need to prove that.
00:40:51.172 - 00:40:56.330, Speaker A: This a and b are consistent with what you push previously.
00:40:56.410 - 00:40:56.702, Speaker B: Right?
00:40:56.756 - 00:41:00.010, Speaker A: So basically it's proving for read and write are consistent.
00:41:00.170 - 00:41:03.662, Speaker B: And to achieve this, we use a separate lookup table.
00:41:03.726 - 00:41:06.340, Speaker A: So imagine that there is a magic table here.
00:41:06.710 - 00:41:10.062, Speaker B: And assuming that this table has correctly.
00:41:10.126 - 00:41:15.286, Speaker A: Ordered all the entries, all the values you need to read and write in.
00:41:15.308 - 00:41:48.122, Speaker B: EVM circuits, and then you just have a magic table here. And inside EVM circuit, we only need to write a lookup constraints which you look up this value to this table and as far as it exists in this table, then it's cracked. So now the problem moves to the table. The question becomes how you are going to prove that this table has crack order. Because EVM circuit is pretty handwriting.
00:41:48.186 - 00:41:55.390, Speaker A: It's just lookup and if it exists, then it's consistent. But how you are actually constraining this table.
00:41:55.550 - 00:42:00.402, Speaker B: So what you do is that we will have a separate RAm circuit with.
00:42:00.456 - 00:42:05.718, Speaker A: Different constraints to constrain that this table is generated correctly with the correct order.
00:42:05.804 - 00:42:29.818, Speaker B: For example, this Ram circuit will check that two adjacent rows have like for example, the time step is increasing and the address will be has some order for this address and whether it's read or write. And for example, if it's read after write after read, then it's new to.
00:42:29.824 - 00:42:31.498, Speaker A: Prove that the value are consistent.
00:42:31.594 - 00:42:45.234, Speaker B: So this Ram circuit will be used to prove that all the tables, all the entries inside this Ram table are, is. I won't go into the detail here, but if you are interested, we have.
00:42:45.272 - 00:42:47.970, Speaker A: More specific explanations on our YouTube channel.
00:42:48.040 - 00:43:00.326, Speaker B: You can check out. But this is basically how you handle stack, memory and storage. So basically you have separate circuits to handle those circuits. And then inside EVM circuits you are.
00:43:00.348 - 00:43:04.070, Speaker A: Just hand waving and look up into that table.
00:43:05.130 - 00:44:04.406, Speaker B: And similarly, when you need to prove some really zicky unfriendly opcode like hash function, you just simply can't put all the witness within that tiny opcode specific area, right? Because hash is so complicated, it needs many rows more than just like two or three rows. So what you need to do is that, for example, you want to prove that this is input. This is the hash result of your input. And then you just put your value here and directly assume that it's cracked. And then inside EVM circuits, what you do is that you look up this tuple like input comma hash input and to a table. And then as far as this pair exists in that table, then it means this input and this hash output has the relationship you want. And then the magic become like how you kind of generate this hash lookup table.
00:44:04.406 - 00:44:09.066, Speaker B: So previously when we are proving for range proof, the lookup table is quite.
00:44:09.088 - 00:44:12.366, Speaker A: Simple because it's fixed even for some.
00:44:12.388 - 00:44:42.774, Speaker B: Bitwise operation, it's fixed table. But now, because you can't prove force, all the input for hash function and this lookup table has to be constructed in a way that is dynamic, so only proving for the pairs you want them to prove. So again, you just imagine that there exists such a magic lookup table which store all the possible entries like input and hash input pair and just store.
00:44:42.812 - 00:44:44.070, Speaker A: Many entries like that.
00:44:44.140 - 00:45:16.798, Speaker B: And all the EVM circuits will, when they need hash, they just look into this table as far as it exists, then it's cracked. Then again, how do you guarantee that the entries in this table are correct. So what you do is that similarly you have a hash circuit which you encode all the hash constraints inside these circuits. And then using these constraints you can prove that all the entries are cracked together. So you basically take all the input and hash output, all the minor instances.
00:45:16.894 - 00:45:19.054, Speaker A: And then use one circuit to prove.
00:45:19.102 - 00:45:21.346, Speaker B: That they are correct.
00:45:21.448 - 00:45:24.610, Speaker A: So you just separate that into a specific circuit.
00:45:27.850 - 00:45:36.226, Speaker B: Okay, now let's take a look at the overall architecture of the leak EVM circuits. So you firstly start with some EVM.
00:45:36.258 - 00:45:39.958, Speaker A: Circuit on the top to constrain the hosted machine.
00:45:40.054 - 00:45:45.398, Speaker B: It's divided into multiple steps and you have an execution trace which is your witness.
00:45:45.494 - 00:45:48.410, Speaker A: You unload this execution trace to specific.
00:45:48.480 - 00:45:50.326, Speaker B: Variables and fill up in the EVM.
00:45:50.358 - 00:45:53.834, Speaker A: Circuits and EVM circuit defining some your.
00:45:53.872 - 00:46:33.350, Speaker B: Constraints like constraining each step or look up into some other circuits. And then every time you met with some really hard opcodes, you just imagine that there is a magic table. So for example, there is a fixed table for bitwise operations, range check. And for example there is a catch hack table which stores all the input and output pair which is used in EVM. And there is a ram table for all the read and write entries. There is a bytecode table, there is a transaction table, there is a block context table. And you just imagine that all those table exists.
00:46:33.350 - 00:47:21.862, Speaker B: And every time EVM circuit met some problem, you just look up into this table and imagine that this table is correct. And then for each table you need to use separate circuits to prove that all the entries in this table are valid. So for fixed table, because it's fixed, it's preprocessed, you don't need to constrain that. But for catch table, for example, you need to have a catch up circuit proving that catch up is a hash function, that all the entries in catch up table are correct. And for RAM table because it contains some storage, memory and stack operations. So for storage operation you need a Merko potential tree circuit to constrain that storage is cracked. And in this MPT, if you are using the Merko potential trail like Ethereum.
00:47:21.926 - 00:47:23.594, Speaker A: Then you need to look up into.
00:47:23.632 - 00:47:45.726, Speaker B: Kajak table because constructing a Merkle train need hush. And you need to look up into that table and Ram circuit for stack and memory operations. And there is bico circuit proving for bico unrolling. And there is a transaction circuit block circuit. And for your signature even you can have an ECDs circuits to constrain the.
00:47:45.748 - 00:47:47.018, Speaker A: Signatures in that table.
00:47:47.114 - 00:48:16.682, Speaker B: And a transaction circuit can just look up this table. So basically you can imagine that the given circuits is a set of circuits, they are connected by lookup tables. Okay, so just a quick recap of where we are so far. We compared the RNCs with Plunkish authorization and explained the reason why Plunkish is a better fit for virtual machine circuit, mainly due to the support of custom.
00:48:16.736 - 00:48:18.902, Speaker A: Gate and lookup tables.
00:48:19.046 - 00:48:27.422, Speaker B: And then we explained how to write the constraints for our ZKVM and explain the architecture for the ZKVM, which is.
00:48:27.476 - 00:48:30.080, Speaker A: Like an EVM circuit on top and.
00:48:30.610 - 00:48:45.186, Speaker B: Many separate circuits for different functionalities and they are interconnected by this lookup tables. And now after the front end, let's move to the back end and see how you are actually generating proofs and.
00:48:45.208 - 00:48:47.270, Speaker A: How we are choosing different proof systems.
00:48:49.450 - 00:49:52.070, Speaker B: So again, here is all the circuits we need for the key EVM. So there is an EVM circuit for state translation ram circuit and a bunch of others like ECDSA circuit and all those other circuits. So in theory you need to generate proof for each circuit and verify all the proofs to know that you are executing a transaction correctly because you need all the functions, all the components to function as expected to know that one transaction is correct. But in practice it's too expensive to verify all those proofs. So instead we have an aggregation circuit to prove that all those proofs from those subsurcates are valid. And in the end you will only have one aggregation proof instead of many different proofs, which can massively reduce the verification overhead. And so accordingly, when you are considering the proof system, we can use a two layer architecture.
00:49:52.070 - 00:50:42.502, Speaker B: So basically the first layer you need to generate proof for EVM, for those kind of direct you are directly proving for EVM logic. And second layer is mainly targeting at the proofs from the first layer. But here you can use a different proof system. So let's take a look at the requirement for different layers and figure out what's the best proof system for each because you can basically compose different proof system together because they have different requirements and different targets. So let's take a look at the first layer. So the first layer need to handle some really large computation which is directly coming from your EVM. And so because you are directly proving for EVM logic, you need some very good support for custom gate and lookups.
00:50:42.502 - 00:51:29.606, Speaker B: You need to be really expressive because you are handling different type of computation inside EVM and more customized. So the proof system, ideally we want them to support custom gate and lookup. And second requirement is that you want some really hardware friendly prover to make proof faster by saying hardware friendly, I mean two things, one is highly parallel. So your prover, if you are using GPU which can basically make your proof like ten times or even 100 faster because it's so paralyzed and also low peak memory. So basically, for example, you can process your circuits like circuit by circuit, or you can have some way to reduce the maximum usage of your memory.
00:51:29.638 - 00:51:33.034, Speaker A: For example, in some cases if you.
00:51:33.072 - 00:52:26.646, Speaker B: Require very large ffT, then your memory requirement will become high. But if you are doing some multi exponentiation, you can do that like chunk by chunk and to reduce your peak memory. So we want some hardware friendly proverb because we want some efficiency. And third requirement is that the verification circuit should be small because eventually the aggregation circuit is proving for the verification algorithm from your previous proof system. So the verification circuit need to be small. And the last requirement is that ideally we want the proof system to have transparent setup or universal trustee setup because EVM is upgrading. And if you are using application specific trustee setup then you will have some trouble because basically every time you do some upgrade you need to run your setup.
00:52:26.646 - 00:53:19.850, Speaker B: So in the first layer it's very important that you at least have some universal trustee setup so that you don't need to run this ceremony again. And here are some promising candidates. So from polygon there is planky two, there is star key, there istark which basically all using also a plank like IOP, but use fry as a polynomial commitment scheme. And it has some really nice properties. For example like plunky two and those proof system use a smaller field called goldilocks which can make the computation on CPU become really fast because it's not eBay curve, you can use a smaller field which can fit into a smaller. So godilocks is like 64 bit which can fit into a cpu register so you can do computation much faster. And also you can support custom gate lookup.
00:53:19.850 - 00:54:26.974, Speaker B: And it's hardware friendly because it's fast and verification circuit is smaller and it has a transparent setup. And another promising category is halo two or halo two KDG version. So halo two is initially developed by Zcache team and initially Halo two is, they use a plunkish association on the front end with flexibility with various flexible support for custom gate and lookups. But halo two initially use an inner product argument with pasta curve which you can do folding because it's a cyclic ethereum curve, you can do recursion very efficiently. However, pasta curve is not directly supported on ethereum layer one and also because you are using inner product argument. So if you are very familiar, you know that it will have some kind of logarithmic verification. In the first place, EVM doesn't support pasta curve, so you can't use pre compile.
00:54:26.974 - 00:55:10.266, Speaker B: So verification will be very expensive. And second, because this logarithmic verification, it can make it even more expensive on ECM. So that's the reason why there is a derivative which we are using, and also like the PSE team from ECM foundation, they're using, it's called hello two kilogram version. So we basically replace the panomic committee part in hello two with kilogy. Because for kilogy we can have very small, small proof. And also because KDG we are using BN curve, it's supported on ECM, so it can be efficiently verified on ECM. And there are some other new candidates which are very promising.
00:55:10.266 - 00:56:11.886, Speaker B: Like for example there are some linear time prover which remove FFT to be more hardware friendly, and there are a bunch of new iops around that. And also another recent trend is like using, because Nova is famous for fast small threshold aggregation and recursion. So if you can use Nova to recursively prove for maybe a repeated catch or whatever, then you might be able to get some very efficient proverb. But currently, although some check based protocol or Nova initially in the paper they only support r one cs. So if they can support plankish authorization, which there are already recent work from geometry, like talking about that, how you can support degree two custom gate in Nova. And there are some new work supporting lookup forwarding in Nova. And also there is new work called supernova which can support like you can four different circuits.
00:56:11.886 - 00:56:14.226, Speaker B: So those are all very interesting line.
00:56:14.248 - 00:56:16.306, Speaker A: Of work which can be promising in.
00:56:16.328 - 00:56:18.482, Speaker B: The first layer to make your proverb efficient.
00:56:18.626 - 00:56:23.590, Speaker A: And also universal verification is smaller and support some optimization.
00:56:24.970 - 00:57:14.866, Speaker B: And now let's take a look at the second layer. So the second layer, because you generate an aggregated proof. So the second layer need to be very efficient, especially efficiently verifier bound EVM, because eventually you need to submit this proof on Ethereum. So there are some requirements. First requirement like as mentioned, like proof is efficiently verifiable on EVM you have smaller proof, you have low gas cost for verifying this proof. And second is that you need to consider the best practice from previous one, because you need to prove for verification circuit of the former layer and you need to prove that efficiently. And also ideally it should also be hardware friendly for efficiency and ideally transparent or universal strategy setup.
00:57:14.866 - 00:57:51.540, Speaker B: But transparent or universal setup is not a very harsh requirement for aggregation circuit. Especially if you have some very good design. The reason is that because aggregation circuit is only proving that proofs are valid and the proof verification logic is usually fixed. So even if you have some kind of growth 16, even growth 16 or some more like application specific trust tab because you are only proving for proof. So you can change your logic here, but without changing the logic here. So that's why I only write. Ideally you want this property.
00:57:51.540 - 00:58:55.974, Speaker B: So some promising candidates which can make your proof become really small is like growth 16 which is still remaining a very efficient prover in the literature. And another candidate might be plank with very few columns. So imagine that when I mentioned plankisharization, you can think of a table, and this table has many columns and the verification cost is linear to the column number. Also this table is configurable. You can configure this table to be really thin and really tall and then you have very fewer columns. So you can reduce your verification cost to be very small. And then with either KDG panomic commission scheme so that one row will have one opening proof or f long which sacrifice poorer time with verification efficiency and which is actually adopted by polygon Hermes to reduce a verification cost.
00:58:55.974 - 00:59:56.694, Speaker B: Or there might be catch up fry. So basically you are using fry but you are using catchk. Because EVM has catch pre compile and it can be verified efficiently. But mostly if people don't care about this trustee setup thing, they will go with KDG or flunk or go 16. So as far as what we choose, our first layer is using hello to KDG and our second layer is also hello to KDG. But the only difference is that like recall previous lectures from interactive proof to non interactive proof, you need some fear shamir to hash a previous transcript to a randomness. So because in aggregation circuit you need to prove the verification algorithm and so you want the hash used in this verification algorithm is efficiently provable.
00:59:56.694 - 01:00:33.714, Speaker B: So that's why we are using positive hash for FIA shamir in previous layer and then using catch hack in the second layer. Because eventually you need to verify this aggregation proof on layer one. Because the first layer because halo two can support custom gate and lookup. And also it has very good proverb performance because we have very fast gpu prover and we have a paper talking about that, you can check out that on s plus. And the verification circuit is also very small because it's kilog. So per column you only have very small proof. And also it's a universal setup.
01:00:33.714 - 01:01:34.582, Speaker B: And secondly, why we are choosing hello to kilogy is that because we have custom gate and lookup support, so we can prove the non native operations of KDG opening still very efficiently and then again we can reuse our gpu prover and also we can configure this because it's hello tool and it can support plankitization. We can configure this table to be really really thin and that verification is very very small. So it can be configured to be very small. So that's why we are using this. And this is one of the major differences which differentiates us from polygon Hermes. So they are basically using the first layer, they are using a stark and then they use multiple layer of stark to shrink the proof size and then finally they use iflonk to make on chain application even smaller. But here we are using directly using hello two kg.
01:01:34.582 - 01:02:16.214, Speaker B: Hello two kg which is basically a plank LP plus KDG. And also we are considering also having multiple layer of aggregation to even aggregate many aggregated proofs. So let's take a look at the concrete layout here I only list some status for EVM circuits. So EVM circuits has like over 100 columns, over 2000 custom case and 50 lookups. And the highest custom gate degree is nine. And for around 1 million gas it needs around two powers, 18 rows. And the more gas you want to prove, the more rows you will need.
01:02:16.252 - 01:02:18.246, Speaker A: Like for example if you need 2.
01:02:18.268 - 01:02:50.078, Speaker B: Million gas it's just double the number of rows. And then in the second layer you need to aggregate proofs like from un circuit Ram circuit all those circuits into one proof. But this aggregation circuit, the proof need to be submitted on layer one. So it's very same. So it only had like 23 columns, one cascade and seven lookup tables. And the highest custom gate degree is five. And for aggregating many circuits it takes like two powers 25 rows.
01:02:50.078 - 01:03:45.362, Speaker B: And just to notice here is that this is not finalized. So currently we are actually using two layer of aggregation. So one layer aggregation is like 23 columns, but right after this aggregation layer we have another layer which might only use like three columns or four or five columns to even reduce the verification cost further. And also this row number will be massively reduced with some optimizations from axiom and it's still working in progress. But we believe that this can be further reduced. And also if you are increasing the gas limit, the aggregation circuit won't be. The aggregation circuit size won't be increased because you only increase the real number.
01:03:45.362 - 01:04:38.882, Speaker B: But the proof verification won't be changed, it only influenced the pooing time in the first layer. And besides that we have done crazy optimizations for our proverb, mainly on GPU. So we have optimized the most computational heavy part inside prover on GPU, including MSM, which is like inner product over EPA curve, and NTT which is like FFT over finite field, and also caution kernel which is computing the caution polynomial. So if you are interested in this, again, we have some videos talking about more specifically into this GPU optimization poor optimization. You can check out some other videos. And also that's one optimization for different computational kernel. And also we pipeline and overlap CPU and GPU computations.
01:04:38.882 - 01:05:19.170, Speaker B: Like when you are doing things on cpu, you can also compute something on GPU at the same time to overlap to further reduce your latency. And also we have multicard implementation. We have done a bunch of crazy memory optimizations to reduce from like 1gb memory, which is our starting point to around like 300gb memory. And the performance is also great. Like for EVM circuits. Eventually the GPU prover takes around 30 seconds and for aggregation circuit it takes around like two minutes. It's actually even more reduced with our recent optimization including parallel synthesize and some other optimizations.
01:05:19.170 - 01:05:43.414, Speaker B: But this can show that the QVM is already practical. So for around 1 million gas, the first layer will takes two minutes. The second layer takes around three minutes. And for example, if you want to increase to 10 million gas, then the first layer will just be like 20 minutes and second layer still be three minutes. So it's not finalized. Again, we expect that to be within ten minutes in the future for one block.
01:05:43.542 - 01:05:56.666, Speaker A: But we just done many, many optimizations for that's in this section.
01:05:56.778 - 01:05:59.326, Speaker B: I'm going to talk about some interesting research problems.
01:05:59.428 - 01:06:14.580, Speaker A: We met when we are building our ZKVM. Still like I will talk from the front end circuit to the backhand approver. Now let's take a look some of the interesting problems on the circuit side.
01:06:15.290 - 01:06:23.970, Speaker B: So again, let's start with our EVM layout. So this is our EVM circuit layout. And you have spec compacts, k switch.
01:06:24.050 - 01:06:25.990, Speaker A: And opcode specific witness.
01:06:26.410 - 01:06:55.594, Speaker B: And as I said, evm word size is 256 bit. And it's problematic because for the variables inside your circuit, you need to be limit that into a smaller finite field. So you can't fit into 256 bit word into such a cell. So what we do here is that we decompose 256 bit into 32 limps.
01:06:55.722 - 01:06:57.986, Speaker A: And each limb is eight bit.
01:06:58.168 - 01:07:00.334, Speaker B: So that we can do efficient range.
01:07:00.382 - 01:07:02.030, Speaker A: Proof for each limp.
01:07:02.110 - 01:07:12.194, Speaker B: Like you can look up this eight bit limp into a two powers eight size table so that you know that it's eight bit size. So that you can do efficient range.
01:07:12.242 - 01:07:15.510, Speaker A: Proof for 256 bit word and also.
01:07:15.580 - 01:07:21.130, Speaker B: Fit this eight bit limp into smaller cell. So this is your value.
01:07:21.200 - 01:07:24.090, Speaker A: You decompose that into eight bin leaps.
01:07:25.710 - 01:08:27.146, Speaker B: But it's problematic because every time when you need this EVM word, you will need to bring and duplicate 32 variables. So what we do here is that we encode an EVM word using something called random linear combination or RLC. So this random linear combination is basically taking all those 32 numbers and then use a randomness to combine those numbers into one encoded value. And then every time you need to use this 256 bit word, you just use this encoded value instead and the proof for this constraint. So it's very convenient and efficient. But here are some problem. So because you are using a random number to linear combine 32 values, so those 32 values should be fixed and then theta should be derived from those numbers or fixed fixed values.
01:08:27.146 - 01:08:46.578, Speaker B: Because for example, if theta and a one to a 31 are decided at the same time, then you can adjust your limb number to change your encoded value. So it only makes sense if a zero to a 31 is already fixed and then you use a new randomness.
01:08:46.674 - 01:08:48.706, Speaker A: To linear combine those values.
01:08:48.898 - 01:08:54.038, Speaker B: But if you take a look at the witness, this is your encode value.
01:08:54.124 - 01:08:55.430, Speaker A: This is your limp.
01:08:56.090 - 01:09:10.234, Speaker B: It's all part of the witness, which means proverb need to fill up both parts by themselves at the same time. So that's a problem like you basically can't do that if roc and a.
01:09:10.272 - 01:09:12.862, Speaker A: Two to a 31 are generated at the same time.
01:09:12.996 - 01:10:30.226, Speaker B: So that's why we implement a feature called multifaceted prover, which is basically saying that you should synthesize a part of the witness, derive this randomness, and then keep synthesizing the rest of the witness so that you can derive randomness from previous values to this value. So at first, when we find this with the PSE team at ETM foundation, this is extremely helpful and it's standardized. The implementation is called challenge API and you can find out in the implementation it's extremely powerful and it can be used in many places. For example as I mentioned, like compress EvM word into one value and also it can encode dynamic lens input. So imagine that if you have catchack or you have any hash function, because you don't know what's the input size for this hash function. So you can lay out all those input trunks in some columns and then encode those values into one value and then use this as your input encoding which can efficiently represent a dynamic length of input. And thirdly, that you can do some lookup layout optimization.
01:10:30.226 - 01:11:38.810, Speaker B: So this is extremely, extremely helpful in our DKM context. So I can briefly explain what is this lookup layout optimization. So as I mentioned, you can look up a tuple to a table columns, right? Like for example, imagine that we have three value tuple, and the values in this tuple can come from any place in this table. So imagine that you have one three column table. So the first tuple is looking up this position, this position, this position to this table, and the next tuple is from this position, this position and this position also into this table. So different positions will introduce different tuples, which means every time you do a lookup, you will introduce one additional lookup arguments, which is very costly because almost like one lookup is equivalent to three custom gate plus some columns. So that's very costly.
01:11:38.810 - 01:13:03.778, Speaker B: One optimization you can do with this randomness is that you can allocate a new empty column and then you can use this randomness to linear combine those three value tuple into one value and put that into one cell in this column. And then if you have three values here, you also use this randomness to linear combine those three values into here. And then you get a table column which is full of your random linear combined tuples. And then you can also do the same linear combination for your table column and into one column. And then you only need to look up from this column to that column. So basically imagine that initially you have unlookup tables, unlookup arguments, because you have untupulse looking up into the same lookup table. And then after this optimization, you optimize that into uncasm gates, which is random linear combined tuples into a cell and then one lookup, because you look up this column into your lookup table, so you can efficiently make those different positions and then lay out that regularly in one table column and then look up that in your table.
01:13:03.778 - 01:14:32.478, Speaker B: So this is extremely powerful. But this randomness also has some problems, because as I mentioned, you need multi phase prover to derive this randomness. And it's especially problematic when you have many circuits. For example, there is EVM circuit, there is a catch hack circuit, there is ECDSA circuit, if one circuit need to stop to generate a witness, and then other circuits also need to stop if they need this randomness. Because for example, if you need to look up this value into catch up circuit, and then firstly, after synthesizing your EVM circuit, you get your randomness, you get this included value, and then bad guy can just put some kind of bad values which will satisfy this requirement because this randomness, he already knows there is a randomness, so he can change this limb number and get some other encoded version which is not exactly encoding the same number and then do something bad there. So if you have many circuits, this randomness will enforce you to half synthesize all the circuits. So you halfway synthesize EVM circuit, you halfly synthesize ECDSA circuit, you halfway synthesize the catch up circuit, blah blah blah, and then deriving the randomness at the same time, and then synthesizing this circuit, synthesize that circuit, synthesize that circuit.
01:14:32.478 - 01:15:20.466, Speaker B: So you need some kind of asynchronous but problematic way to generate your witness, which will introduce a very large overheading inside proverb. So that's the reason why in reality we actually put all those ECDs circuit catch up circuit EVM circuits into a super circuit, so that we can just have fewer stages, fewer physics, and then generate witness for the first half second half together. So that's the reason why we are seriously considering whether we can remove this randomness. And there are some ways, like for example for email word size optimization, you can divide that into high low, which is like just cut that into two half and only decompose when you need to do range proofs, but carry those.
01:15:20.488 - 01:15:22.354, Speaker A: Two values all the time, which can.
01:15:22.392 - 01:16:08.830, Speaker B: Still save some number. And for this dynamic lens input, you can just divide that into fixed trunk and do that dynamic times. But for lookup layout optimizations, that's still very important. We haven't figured out what's the best way to do this without RLC optimization. And another way to solve this lookup optimization is that if your circuit is layout very regularly, for example, for every tuple that need to look up into that table, you lay out that into the same position at the same step. Then you can directly use one lookup to off mat that without kind of recombining into a new column. But currently, to maximum the usage of each cells, we kind of allocate very freely.
01:16:08.830 - 01:16:53.798, Speaker B: So that's why we need this optimization. So this is still remaining an open problem, which is very interesting. And another interesting problem is about the circuit layout comparison. So as I mentioned earlier, the reason why we are using plunk case organization is that we can handle dynamic execution trace. So what does this mean? It means for one transaction, the execution trace can be push, push, add but maybe for the other transaction it can be add, pop, push, blah blah blah. In each step you need to prove for different opcodes. And so this makes your whole circuit become dynamic.
01:16:53.798 - 01:17:56.238, Speaker B: So recall that in RNCs if you want to write some constraints, you basically need to fix logic here, fixed logic here, and have one fixed circuit and then have some preprocessed PK and VK for your three matrix in ERNCs. But now because step one need to constrain for different opcodes and you basically need to enable different constraints in different positions. And so Plunkett is much more efficient in doing this because in RNCs the only way you can do this is that in each step you implement something like if else logic, you implement. If it's push, then they say the constraint. If it's pop, then they say the constraint. In each step you basically need to write out all the possibilities that can happen in this step, which is a VM logic, VM possibilities and then repeat that for every step. So which is a very gigantic overhead.
01:17:56.238 - 01:18:45.582, Speaker B: But using plunkish optimization, you can define some custom gate for each opcode. Like push, you can define opcode, you can define a custom gate for add, you can define some custom gate and only open or off those constraints at different steps. So like recall that previously there is a key switch. So basically if you set that to be one, then add constraint will be applied to this area. But if you set maybe multiplication to be one, then multiplication constraints will be applied to this area, which is super convenient, which means using this way it's very hard to standardize gates. You have to design custom gates for different opcodes. But we are kind of abusing the functionality of custom gates.
01:18:45.582 - 01:19:45.570, Speaker B: So for example, we have over 2000 custom gates. Because each opcode you might introduce more custom gates and each copcode you might introduce some custom gates. So we are thinking of whether there are some better way to lay out the whole custom gate. So one solution can be, again, it has some relationship with the previously mentioned, this regular layout. So if you can make maybe your operand to be within this area and then you can reuse maybe some kind of range checks across different steps, or you can reuse some basic mass gadget across different steps to reduce some custom gate numbers. Or another way which you can do is that you can define some basic, maybe 20 micro opcode. And then each opcode is consist of several micro opcodes and then those micro opcodes you will have some constraints for those macro opcodes.
01:19:45.570 - 01:20:36.790, Speaker B: And then the custom gate number is like how many micro opcode you will have and then each opcode will be mapped into some macro opcodes. And in this way you can also reduce the custom gate number. But this will also make your cell becomes like the usage percentage of your circuit will become smaller. Become smaller because previously we allocate cell by cell, every cell will be used directly for some function. But if you implement you want to reuse, then you need to layout in a way that can be reused, which means there will be more empty cells. So it doesn't mean like layout differently and regularly will be more efficient than our current design. But there are some open problems to be explored.
01:20:36.790 - 01:21:34.838, Speaker B: And another problem also in the circuit side is what I call a dynamic ZkvM problem. So imagine that in step one you are handling some really complicated opcodes like precompel. So what you do is that you just outsource that to another precompel circuit. And in step two you also outsource that to a hash circuit if you are handling hash. So there are two problems. One is that because hash pre compile and all those subsurcates are handling different circuits, so they have different lengths of row number and they are like for example hash circuit maybe needs many rows because it's very complicated, but maybe some smaller query compile needs fewer rows. And then there are some problem because eventually when you are proving you need to limit what's the maximum size of your circuit.
01:21:34.838 - 01:22:42.814, Speaker B: So for example we limit the real number to be two powers 18. And then like hash circuit might be easier to exceed this limit if you are not designed properly. And also currently in our design to connect different circuits more efficiently we pad them into the sim lens which will introduce some problem because we are paying for some additional costs even if we are not using that many hash. And also this again shows you will have the same low number, which means you will have some kind of maximum limit of hash you can use inside your circuit. Especially if, for example if the maximum row number is two powers 18 and hash circuit takes two powers like 16 rows, then you can only use four hashes in one circuit. So which is indeed very problematic. So we are thinking of whether there are some way which allow you to dynamically prove dynamic number of hashes which can exist this number and also like how to link different circuits together.
01:22:42.814 - 01:23:57.462, Speaker B: So the kind of reason why we need to pad them together is that also because of the randomness, because if you have independent circuits you need to derive randomness, like partial synthesize and derive randomness together. But if you pad them together and put that in a SIM circuit, it makes your circuit synthesis become faster and more convenient. So you have to solve the previous problem and then make this architecture become dynamic. And then think of a way to link dynamic size circuits together. So here I listed, I summarize all the bad influences, which is you might need a maximum of number of catch, because catch might need many rows and you have maximum number of rows limit, which means you have limited number of catch, which can be proven in one circuit. And also this can happen even not only for catch, but also for some opcode. Like if some opcode like this step three, for example, needs more rows than step two, then which means you can fit in less steps three opcodes in your circuit than step two, which might be some problem.
01:23:57.462 - 01:25:03.866, Speaker B: And you are also, because you pad to the same lens. Even if you are not using that, you are paying for the proving cost. So, can we make the QM become more dynamic like pay for what we really need and support a larger limit of catch and all those circuits? So one solution like people are coming up with is that maybe you can use recursive to recursively prove many catch. And then you can link those input output pairs to the specialized catch. And then in this way you can have dynamic length of subsurface, but still connect it to your main circuit. Next, I will go through some problems on the back end, which is quite interesting, especially for the audience of this class, because you learn so much about the theoretical proof system construction. So again, like recall that we have two layer of circuit and we have two layer of prover.
01:25:03.866 - 01:25:44.678, Speaker B: And in both layer the proving bottleneck is the same, because we are reusing the same halo two and kzg. And the most computational heavy part is multi exponentiation and FFT. So multi exponentiation, as I explained, is inner product on its really curve. And NTT is like FFT over finite field. And you can use GPU to massively parallelize those two components to make the core components become really, really fast. But then the problem moved to witness generation and data copy. So what does this mean? So, witness generation is when you are generating the value for the whole table.
01:25:44.678 - 01:26:28.460, Speaker B: So that is not highly parallelizable and that differs per circuit or per application. So the CPU part becomes bottleneck. And for the data copy part, what I mean here is that, for example, for FFT, for example, if you are doing like two powers, 26 size FFT, so GPU can compute FFT really, really fast. It can finish computing, for example, in 20 milliseconds. But copying this data from CPU to GPU takes much longer time than 20 milliseconds. So the data copy actually become even a larger bottleneck than computation itself. So we need some way which we can reduce this problem.
01:26:28.460 - 01:27:32.110, Speaker B: So that's one interesting thing we noticed during our benchmark, and secondly that we need a very large cpu memory. So initially when we implement our first EKVM, we need almost like one terabytes CPU memory. And then we currently will already optimize that to 300gb memory on CPU, and also optimize this GPU memory becomes like 8gb. Which means you can run our proverb on very cheap gpu, but still require a very expensive cpu. So the research problem is that whether you can have some hardware friendly prover, which by saying hardware friendly I mean both parallelizable and with a lower peak memory. And also you shouldn't ignore the witnessing generation because previously multi explanation and FFT takes 95% of your proving time. That's why people are all focusing on these two components acceleration.
01:27:32.110 - 01:28:21.770, Speaker B: But the problem is that even if you make this part becomes like ten times faster, then in your computation, you make the 95% of your computation become ten times faster, then you will automatic buy still by that smaller 5%, which is a long tail problem. But it can't be ignored. Especially with ASIC, you can make that even faster, but you will still be bottom map by this within generation. So you shouldn't ignore this within generation. And third, is like how to run on cheap machines and become more decentralized. So eventually I think this will evolve into a problem which needs software and hardware code design. Because basically when you are designing hardware, you also need to consider what's the best software.
01:28:21.770 - 01:29:13.082, Speaker B: Maybe you can even the one proverb is a linear time. It can be theoretically be very fast, but it's very super hardware unfriendly. Maybe it's not as practical as maybe the theoretical complexity is higher, but it's highly parallelizable in practice. And GPU can make that 100 times faster, so it can still beat that linear time prover construction. So we really need to take both into consideration. And finally, around prover there is some problem around recursive proof and how you can efficiently compose different proof systems. So as I mentioned earlier, there are different proof systems requirements for different layers.
01:29:13.082 - 01:29:49.830, Speaker B: So the first layer need to be very expressive for handling those EVM logic. And the second layer need to be very efficient in EVM. So how to combine gets a better combination. Maybe there are some better combinations than what we have so far. So there are two large directions. One is like people kind of prefer smaller and smaller field. For example this godilocks field which is like 64 bit, and even Mason prime which is like some Mason prime is like 32 bit which is a faster computation on CPU and on hardware like IPG and ASIC.
01:29:49.830 - 01:31:02.110, Speaker B: So should we move to that smaller field for faster speed and smaller memory? Or another way is like we stick to this iterative curve based construction. But there are some nice constructions recently like supernova Nova, which provide you, you can recursively prove dynamic number of circuits with a smaller recursive threshold, or you can use some cyclic curve to also fold your proofs and get a very small proof in the end. And the good thing about this easy curve based construction is that because your core computation is bottlenecked by this multi exponentiation, so it can be accelerated on GPU and ASIC for around like ten or even 100 times. So it can be very fast considering the best practice of hardware. So it's still an open problem. There are a lot more options to explore, especially from previous courses, and if you are interested, reach out to us and if you want to do related research. And we welcome this effort.
01:31:02.110 - 01:32:15.906, Speaker B: And another issue around the KVM is more about security. So both us and the PSE, the KVM, because we are collaborating on the same ZQM effort, we need over 30k lines of code. And it can't be bug free for quite a long time, because it's just a super complicated system with so many mathematical constraints, you definitely will miss something or do something wrong. So what's the best way to audit the keyword circuits? So in general this is a problem of how you can audit a VM circuit based on IR efficiently. So previously there are some work I know for them from Stanford, like there is Cersei which introduced some, like you can use some other IR and SMT server to kind of find box, but this is a VM circuit based on IR. And whether you can, and this is also plant optimization, whether you can audit that automatically. But currently we are still audit manually, line by line to figure out some bugs, but there are still some effort exploring, for example formification or some other way to audit your dicky VM circuits.
01:32:15.906 - 01:33:08.086, Speaker B: So maybe you can, it's feasible in some time, but we don't know. Or maybe you can start with some simple ones which you do for modification for some simpler opcodes, and then eventually extend that to your full ZKVM. But this is quite interesting and quite critical in reality, like how you kind of securely prove that thekivm has no bug. In the last section I'm going to quickly show you applications of EVM even beside the Kiro app. So again, like the first application and the most motivated application is still in the space of the Kiro app where we are building a layer two solution. We can prove intransactions on layer two are valid using Ziki VM and we verify this proof. We generate proof off chain, we verify this proof on layer one.
01:33:08.086 - 01:34:04.220, Speaker B: Smart contract. So that's the idea of Zikirap. So basically taking layer two transactions and general proof and submitting the proof another application is not only on layer two but also on layer one. I call that enshrined blockchain. So the idea is that if you take a look at the layer one diagram, why it's not so efficient? Because for each block most of the nodes need to do some attachation to kind of reexecute and then know the recent state they basically need to download and execute. But what if for each block there are some proof proving that all the transaction of that layer one block are valid and then all the nodes just need to verify that very small proof and it's enough instead of reexecuting all the transactions. And this happens on layer one instead of layer two.
01:34:04.220 - 01:35:20.510, Speaker B: So you can require that the proposer who is proposing this block to generate the given proof and then submit this on chain. But this is very ambitious because you are basically enshrining the whole layer one and each block will attach one proof and currently it's not feasible because for many reasons one is that as I mentioned, this requires the last consensus level or ECM equivalent of the KVM, which is very hard to build with a very large premium head and there might be some security considerations and also because the proving time is super long, it's unacceptable on layer one because for layer two it's quite safe because you can submit proof later because that only influence your finality on layer one. But layer two can provide some faster pre confirmation to users ahead of time. But for layer two, layer one, if you have some kind of like twelve second block time then you can't do this. So that's why it's still very ambitious dream and people are optimizing that, including us and the PSD team at easement foundation are working on this. But currently it's still an ambitious goal. And even more crazily you can do some kind of recursive proof.
01:35:20.510 - 01:36:00.160, Speaker B: I take this kind of dynamic figure from Ina. So basically the idea is that this proof is not only proving that all the transactions inside this block are valid, but also prove that the previous proof are valid. So the proof proves proof and the block. And then eventually you just need to get one proof to prove that the whole blockchain history is correct. So this is very ambitious to enshrine the whole blockchain into a very small proof. And one day you just hold one proof and saying that all the transactions in the history of ECM is valid. So that's pretty cool.
01:36:00.160 - 01:36:58.206, Speaker B: And also just to quickly distinguish that this layer one idea is different from layer two idea and also it's orthogonal, because imagine that layer one can generate proof for layer one block, but among layer one block, each transaction can be a proof of verification for layer two block. So basically like layer twos are enshrined into one transaction, one verification transaction to layer one, and then layer one can generate proof for the layer one block, which might consist of several verification proof from different layer twos. So that's how it's different. Like one is enshrined in layer one state, one is like based on layer one. I'm kind of doing scalability and making my verification transaction become one transaction on layer one. So another interesting application is called proof of exploit. And so the idea here is that if you think about what is ziki vm.
01:36:58.206 - 01:38:20.762, Speaker B: So Zikvm is basically proving that after applying one transaction, your state route moved from this to this root prime. So if you make your transaction also become part of your private input, what you can prove is that you can prove that I know a transaction that can change your state root to state root prime. Using this very fancy nature, you can prove that I know a bug which can change your balance. So for example, if your contract implementation has some bug which after I do a transaction, I can get your balance like here, leave here, become zero, and then I can prove that, okay, so after applying some transaction which I know I constructed, I can make your balance become from 100 east to zero, which means I can steal your fund from this solidity bug. And so you can use this proof to prove that I know a bug, I found a bug and then they can reward you with some kind of money so that you will review this bug. Because one common problem is that if you review this bug to the application, they may not reward you as much as you want. And also you can't really prove to application that without reviewing that you know a bug.
01:38:20.762 - 01:39:32.834, Speaker B: So this is very useful in term of proving that you know some transaction which makes this state transfer from this to that. And one last example is that you can do some attestation which is sometimes called Vicky Oracle. So imagine that in layer one blockchain in a smart contract you need some kind of history information and you just can't access that through layer. So how can you access the previous data? So one way that you can connect to some kind of other oracle like Chainlink, they provide some kind of node and then provide this, they just feed this data directly to you. So you are basically trusting chainlink or some other oracle network to provide the price fade for you. But another stronger way is that you can actually trustlessly read historical on chain data. Because historical data is like this block has this root and this state and you can prove that, you use a state proof which is proving for the storage memory and those stuff.
01:39:32.834 - 01:40:37.750, Speaker B: You can use this state proof to prove that hey, at this height your balance is this or the information you need is this because I can prove the merkle path and some other information and then I can take this data and do some computation. I want, like for example I want to perform like I want to compute the average price for uniswap in the past one month. So I can use the circuit is basically computing the average price and then you can take the price from previous block through the state proof of thekey EVM because you can prove it for the EVM state tree. And then you do some computation, generate proof and then you verify this proof on chain so that you know that this is your result, this is your proof. So basically doing a testation or oracle can be you read state from historical data, you do some computation and then you verify that. So this can be the key circuit, the KVM or the key virtual machine like arbitrary computation. You can define and exom is working on this.
01:40:37.750 - 01:41:24.246, Speaker B: It's basically a Ziki core processor where it may not be the KVM but it leveraged the state proof of the KVM and also many information of the block header. And then core processor means this doesn't necessarily need to be an EVM, it's just another processor and verified proof on chain. And finally just a small advertisement for us. So also a quick summary for what I have talked about previously. So we are building some really cool things at Scroll. Scroll is a general purpose scaling solution for Ethereum based on Zkoap and we are building a native ZKVM which is bytecode level compatible. Or you can call that EVM equivalent using very advanced circularization and very advanced proof system.
01:41:24.246 - 01:42:36.780, Speaker B: And we are also building fast proofer through hardware acceleration and we have like a GPU in production and also proofrecursion like we are using very advanced way to aggregate proofs and currently we are live on the testnet with a product level robust infrastructure and if you're a developer feel free to try out our testnet, give more feedback and deploy anything you want and experience that deploying on scroll is exactly the same as ECM with the best developer experience and also if there are a bunch of interesting problems to be solved as I mentioned they are like the K engineering problem they research problem for practical efficiency and also even besides the K if you are doing some other research around protocol design and mechanism design welcome like we are looking for protocol design and mechanism design people so thank you for listening and we have check out our website at scroll IO and also there is a hiring page or you can dm me on twitter if you are interested in joining or like working on the research together okay thank you.
