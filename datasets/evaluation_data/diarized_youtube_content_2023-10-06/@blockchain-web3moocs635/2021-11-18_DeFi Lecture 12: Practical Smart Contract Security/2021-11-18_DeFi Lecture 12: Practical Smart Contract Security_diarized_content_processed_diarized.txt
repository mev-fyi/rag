00:00:00.330 - 00:00:40.700, Speaker A: Hi, and welcome to practical smart contract security. My name is Sam and I'm a research partner at Paradigm. To give you an idea of what this lecture is going to be like, one question I get asked a lot is how do I learn about smart contract security? Now, there's a lot of ways to do it, but some of them are better than others. For example, while you could theoretically write your own protocol and then get hacked and then learn about smart contract security, that's not a very fun process to go through and I don't really recommend it. You can also try things like reading blog posts or doing security challenges. And while those are really great approaches, and I highly recommend them, I think nothing really beats talking to an expert. And so while what we have right now isn't exactly a two way conversation, I'm going to be trying to focus on that latter point.
00:00:40.700 - 00:01:27.046, Speaker A: The way I'm going to do it is I found six different case studies that I think illustrate some really interesting points about smart contract security. These are either really interesting bugs that I or other people have found, or just fun, interesting stories. You could watch these in any order you'd like, but I would recommend watching them in the order I've provided, because some later segments refer to information presented in earlier segments that you might not have the background knowledge on. With that said, let's hop into our first segment when safe code isn't welcome back. This is segment one when safe code isn't. So let's talk about safety. What exactly makes something safe, and who gets to decide whether something is safe? Well, as with most things, there's a lot of different perspectives on this.
00:01:27.046 - 00:02:13.720, Speaker A: For a user, safety might mean that the code protects them against user error. For example, what we see a lot with ERC 20 tokens is that users will send the tokens to the zero address or the token address itself. Obviously this is not ideal for a user, and so user safety might mean that the token contract does not allow you to do that. For a programmer, safety might actually mean something completely different, namely protection against malicious input. For example, if a hacker is trying to steal tokens or duplicate tokens, a safe contract might not allow them to do that by having various safeguards. So clearly there's the possibility for different people to have different definitions of what safety means, and when two people don't agree on the same definition, a lot of bad things can happen. One great example of this is the ERC 721 or the NFT standard.
00:02:13.720 - 00:02:57.602, Speaker A: Of course, when it was first released, it wasn't called the NFT standard. Rather, they were called ds. We can see in the earliest iteration of the spec we had two functions. We had the approve function, which allowed some other user to operate your deed, and then we had the take ownership function, which allowed you to take ownership of a deed only if you had already been approved for it. Of course, this doesn't look anything like the ERC 20 standard we're familiar with, or even the ERC 721 standard that we're now familiar with. And this is because after about a month the functions were renamed to better harmonize with ERC 20. So now we had the transfer function instead of the take ownership function, and we kept the approve function.
00:02:57.602 - 00:03:38.942, Speaker A: However, this still wasn't quite like ERC 20, because the transfer function allows both the current deed owner as well as the approved deed controller to transfer. In essence, this was like the transfer and transfer from functions combined. And so a little while later we split them out, and so we ended up with both the transfer and transfer from functions as well as the approve function. At this point, the ERC 721 spec looked roughly similar to the ERC 20 spec, but there was going to be one more big change. That big change was the safe transfer functions. To understand what the safe transfer functions are, we take a look at this first iteration of them. Here we have the unsafe transfer function, which behaves much like the current transfer functions.
00:03:38.942 - 00:04:37.154, Speaker A: That is to say, the caller is responsible for confirming that the recipient is able to process the deed if they're not. For example, if you type out the address or you sent it to the zero address, then the transfer would still succeed and nothing would happen. On the other hand, the transfer from function was now modified to check whether the recipient was able to accept the deed by calling a function on NFT received and expecting the magic return value of this shot three of ERC 721 on Mt received. So you can see how now the transfer from function is made safer, because if a user tries to transfer it to a contract which wasn't expecting it, the transfer would fail. However, now we have a slight divergence from the ERC 20 spec, where the transfer from function would only fail if you didn't have the correct balance or permissions. Now, the transfer from function could also fail if the recipient wasn't expecting it. As a result, one more change was made to rename transfer from into safe transfer and unsafe transfer into transfer.
00:04:37.154 - 00:05:19.186, Speaker A: This leaves us with the ERC 721 spec we're familiar with today, which includes the regular transfer from which does not perform any validation, as well as safe transfer from which performs the safe transfer. As I mentioned earlier, with the ERC 20 spec, a common mistake was to transfer tokens to the zero address or the token address itself. In other words, this wasn't exactly a solution in search of a problem. As a result, it wasn't surprising that future specs such as the ERC 1155 specification also included such safe transfer designs. So let's talk about safety. What do these safe transfers do? Well, in terms of user safety, they do a lot. They protect users from typos, they protect users from sending tokens to the wrong address.
00:05:19.186 - 00:06:07.950, Speaker A: However, in terms of programmer safety, it turns out these safe transfers are anything but, and in fact introduce new security risks. To understand why, we need to take a look at unsafe external calls. Now, threat modeling is very important, and on the blockchain, the threat model is significantly different than in traditional programming. This is because with traditional programming you typically assume that function calls are safe to make because you're either calling into your own code or a library that hopefully you trust that you've imported. On the other hand, on the blockchain, function calls are inherently unsafe because anyone can deploy to the blockchain. And when you call a function, you might be calling some untrusted code that an attacker has deployed. And the reason that unsafe external calls are so unsafe in the first place is because during an external call, the attackers full control over the control flow.
00:06:07.950 - 00:06:49.822, Speaker A: What they can do is they can modify the global ethereum state however they like, possibly in a way that you didn't anticipate because you didn't consider the possibility. One example would be to interact with your contract. Again, this is commonly known as preemptioncy, or to interact with other contracts that you interact with. This is something that's often not considered. All of this is to say that basically any external call to nontrusted contracts may be unsafe, unless you can verify that either the contract itself is trusted or the external call is safe. And so you might wonder, how exactly do we do that? Well, let's consider a hypothetical vulnerability. If this vulnerability is exploitable without needing an external call, then clearly the external call is redundant, and so it's not worth talking about.
00:06:49.822 - 00:07:59.094, Speaker A: Therefore, if the vulnerability needs the external call, the external call must be contributing something to the exploit, and the only thing it can contribute is its vantage point in the middle of function execution. In other words, when the external call occurs, the function being executed has already checked or changed some state variables, and then will proceed to check or change some state variables. And it's these state variables that you are able to manipulate in order to break the application in unexpected ways. To take a look at how a socalled safe transfer function might introduce unsafe behavior, I found two case studies for you. The first is the ENS name wrapper. This is an ERC 1155 token that's designed to wrap an ENS domain, and the purpose of this is to allow ENS domain owners to have finer permission management over the domain, for example, forbidding future subdomains from being minted, or forbidding the resolver from being changed. If you'd like to follow along or just take a look at the code for yourself, I provided the GitHub repository as well as the relevant commit in this slide, so feel free to take a look there if you want to pause the video and come back later.
00:07:59.094 - 00:08:34.560, Speaker A: That's totally cool. I'm going to assume that you're either back after pausing the video, or you just want to see the solution. In any case, the first step to reviewing code is always to identify the business logic. In this case, if you'd opened up GitHub, you probably would have seen this directory listing right off the bat. We can eliminate a lot of files and folders as unimportant. For example, the mocks folder and the test folder are very obviously not relevant to a main and deploy. Additionally, the bytes UtL file probably isn't very interesting unless we wanted to really get into the nitty gritty because it's a library function.
00:08:34.560 - 00:09:21.498, Speaker A: On the other hand, files like name wrapper sound very interesting because this is the name of the project and we can assume that there's going to be some interesting code in there. Additionally, ERC 1155 PU sounds interesting because it sounds like a modification to the ERC 1155 standard, and when people modify standards, they tend to break it in unexpected ways. Now that we've identified the business logic, it's time to figure out what the high level user stories are in this contract. There are a few for example, a user wraps the domain and receives a token in return. Or a user unwraps a token and gets the domain back. Or a user owns a token and can modify the domain as they like. Now that we understand how the contract works, it's time to start checking for vulnerabilities.
00:09:21.498 - 00:10:32.310, Speaker A: In this case, we know we're only focused on unsafe external calls, so let's start with that. Looking inside name wrapper, we can see some external calls being made, for example, here, here, and here. However, these are not unsafe external calls because we can verify that the ENS contract is trusted and won't be doing anything suspicious. On the other hand, if we look inside the ERC 1155 fuse file, we can see that, as expected, this file implements the ERC 1155 standard and explicitly makes an unsafe external call to the recipient of the token when it is minted. So now that we have an unsafe external call, the question is, is this call exploitable? Well, if we took a look at the wrap function, we can see that when we first enter wrap, we will immediately call underscore wrap, which will call underscore mint. And then once we've minted the token, we will receive the callback. And so the question is, what's been checked or changed since the start of the call? The answer is, well, we now have the token, and this token represents our ownership of the domain, which interestingly enough, we haven't actually verified that we own the domain yet because this ownership check occurs after the mint.
00:10:32.310 - 00:11:11.060, Speaker A: So now that we have token ownership, what can we do with it? Well, we can look for functions with the only token owner modifier. These include functions like unwrap burning fuses, setting subnodes, or setting the resolver. And while all these are pretty impactful, the one most impactful is likely unwrapping. This is because unwrapping will transfer the underlying ENS domain back to us, the owner of the token. And now that we have the ENS domain, we can do whatever we want with it. Especially we can actually bypass the permission check from earlier right here, because we now own the domain. And so of course, we're the rightful owners that are allowed to mint the tokens for it.
00:11:11.060 - 00:11:59.090, Speaker A: So we can see that through this safe transfer function, or I guess in this case the safe mint function, we actually were able to exploit it and obtain some very unsafe behavior, that is to say, the ability to take ownership of any domain that the ens name wrapper was approved to modify. The second example is the hashmasks NFT project. This is a limited supply NFT. There was a maximum of 16,384 of them to be minted, and anyone could mint them by purchasing them during the sale with a limit of 20 per transaction. Again, if you'd like to follow along, or simply look at this by yourself, the contract is given below. Or if you don't want to type that out, just search for the hashmasks token on ether scan. So, hopping right in, here's the mint NFT function.
00:11:59.090 - 00:12:35.470, Speaker A: We can see that it performs a few checks. For example, checking that the supply cap hasn't been reached, checking that the user is minting a correct number of nfts, checking that the user won't exceed the supply cap. After minting the nfts, and checking that the user is paying the right amount of money, these are all correct checks to be made. And then afterwards you can see that we just perform a very simple mint of all of the nfts requested. So at first glance, this might seem to be completely safe code. However, knowing what we know, we would want to check inside the safe mint function. And sure enough, there is an unsafe external call to the recipient of the token.
00:12:35.470 - 00:13:06.840, Speaker A: Given that there's an unsafe external call, how might we exploit this contract? Well, suppose that we begin minting with 20 nfts. And suppose that at the moment there are only 20 more nfts to be minted. In other words, once we mint these 20, the supply cap will be reached. Well, in this execution, the supply cap has not been reached and we are minting a valid number of nfts. And of course we'll be at the supply cap once we finish minting. And of course we'll have to pay the correct amount of ether. So all five checks pass.
00:13:06.840 - 00:13:36.574, Speaker A: After minting the first NFT, we're going to receive a callback. During this callback, we can choose to mint another 19 nfts. Notice that at the moment we still haven't exceeded the supply cap yet, because we've only minted one NFT. Furthermore, we are minting a valid number of nfts. 19 is between zero and 20, and total supply, which is only the previous total supply. Plus one, plus 19 still won't exceed the supply cap. And of course we'll send the correct amount of ether.
00:13:36.574 - 00:14:16.570, Speaker A: And so now we'll be able to mint another 19 nfts while the first batch of 20 still hasn't finished minting. After we finish minting the 19, we'll finish minting the remaining 19 in the first batch of 20. The end result is that we'll have minted an extra 19 nfts in this hashmath project, which is supposed to have a supply cap. As you can imagine, a project whose selling point is there's limited supply probably won't like it too much that the supply cap is now being breached. So some key takeaways from this case study. Just because a function is called safe doesn't mean it's safe. And in general, don't assume what a function does.
00:14:16.570 - 00:14:52.350, Speaker A: And in fact, even if you think you know what a function does, it might be worth checking it anyways. Because in the worst case, you've wasted some time checking a function, and in the best case, you've prevented a catastrophic bug from affecting your project. Finally, keep in mind that any external call may be unsafe unless you verify that either the contract being called is safe or that the call positioning itself is safe. And so in general, always consider the positioning of the call and what you can do with that vantage point. That's it for segment one. In our next segment, we'll be talking about uncovering a four year old bug. Welcome back.
00:14:52.350 - 00:15:32.202, Speaker A: This is segment two, uncovering a four year old bug. So there's this common belief that the longer a contract goes unhacked, the more secure it must be. And in general this is true because attackers are trying to steal money from contracts all the time. And if it hasn't been hacked yet, that must mean there's no low hanging fruit left. In this case, to find a bug in one of these contracts would require understanding the logic intimately, like the back of your hand. The question is then, how do you ensure that happens? How do you achieve maximum coverage when reviewing such a battle tested contract? Well, the answer seems obvious. Simply review the entire contract methodically with a fine tooth comb.
00:15:32.202 - 00:16:13.862, Speaker A: But that's a lot of effort and some of these contracts are really big, so you need to reduce the search space. But how exactly do you do that? Well, it helps to consider what exactly makes a vulnerability a vulnerability. When there's a vulnerability, the code of the contract remains the same, but the user input is what changes. In fact, it's that specially crafty user input that triggers the vulnerability in the first place. Therefore, one strategy might be to work forwards. Start from where the user input is processed, and then work your way forward and see how exactly you might tweak that input to trigger unexpected behavior. Another way to consider things is that a program has a vulnerability.
00:16:13.862 - 00:16:46.814, Speaker A: If it does a bad thing. For example, it sends ether to the wrong person, sends too much ether, self destruct or something else. And again, this bad thing is triggered by crafted user input, otherwise it wouldn't be a vulnerability. Therefore, another strategy might be to start looking where the bad things can happen. For example, take a look at where the ether transfers occur, and then work your way backwards from there and see how exactly you might craft your input such that you trigger that bad behavior. At this point, I'd like to introduce you to ambisafe. Ambisafe is an ERC 20 platform.
00:16:46.814 - 00:17:25.838, Speaker A: As a service, it means that they basically deploy ERC 20 tokens for you. Notably, every token they deploy is actually a token proxy, which refers to the core contract, and therefore a flaw in the core contract would affect every single token deployed on the platform. As usual. If you'd like to try this for yourself, I've provided a link to a specific token on the ambassador platform in the slide. Or you could try to google for the unibright token. Okay, so digging into the ambassador platform, we see that we start with the asset proxy, which is the ERC 20 proxy that we talked about earlier. It then refers to an asset with whitelist, which is the implementation for the proxy.
00:17:25.838 - 00:18:19.980, Speaker A: And finally there's the e token two contract, which is the core contract. Opening up the contract, the first thing I saw was the transfer function, which is the standard ERC 20 transfer. And immediately I observed a potential problem. That is to say the transfer function calls transfer with reference, which then calls perform transfer with reference on the return value of getasset. To me this looks like if I can change the return value of getasset, then I would be able to perform any arbitrary code here and then potentially do some bad things. So I checked Getasset and then I noticed that it returns the value from get version four. So I checked that and I noticed it returns either latest version, which is an only owner controlled variable, or it returns the value in user opt out version, which is a mapping that contains some data.
00:18:19.980 - 00:18:57.240, Speaker A: I checked how to write to that mapping and I noticed that it only writes into the mapping latest version. And if we check the comments, we see it says disagree with proposed upgrade and stick with the current asset implementation. So it's clear that there is no way to insert our own contract into user opt out version. It can only be used to insert values that the owner has already approved. The next thing I considered was maybe there was some way to spoof a call to perform transfer with reference. That way we could spoof the value of sender. However, there is an only proxy modifier, and sure enough, this modifier ensures that only the proxy can call its function.
00:18:57.240 - 00:19:36.018, Speaker A: Again, I tried the same thing with forward transfer from with reference, but there is another modifier which performs a similar check. And just to drive it home, on the core e token two platform contract we have proxy transfer from with reference. And the same thing occurs here. There's an only proxy modifier which ensures that only the proxy for the given symbol can call that function. So at this point, we've sort of exhausted all of the low hanging fruit and we found nothing. So it's time to dig a little deeper. Looking at the transfer function itself, we see it takes a couple of parameters, the from id to id, the value symbol reference, and then sender id.
00:19:36.018 - 00:20:26.690, Speaker A: We see it performs a couple of checks, for example, ensuring that you can't transfer to yourself, ensuring that the transfer value is positive, the sender has enough balance and the reference is correct. And we also see that it performs some checks for allowance. If the sender is not equal to the source of the tokens, it ensures there's enough allowance and adjusts the allowance if necessary. Finally, at the bottom here we see a call to proxy transfer event. This immediately caught my eye because the ERC 20 specification requires that the token contract itself emit the events. But right now we're not in the token contract. So how does it emit the events? Well, it calls emit transfer on the proxy get for the given symbol, and immediately I realized that if there was a way to replace the value of proxies at symbol, then we can emit a fake transfer event to any token we want.
00:20:26.690 - 00:21:26.594, Speaker A: However, recall from earlier that the proxy transfer from with reference function has this only proxy modifier, so we can't immediately replace the proxy with some other token, otherwise we'd never be able to call this function. So what we really need is some unsafe external call which gives us control flow in the middle of proxy transfer reference. That way we can swap out the proxy in the middle of execution. However, there was no obvious unsafe external call inside transfer, but there is the check sign modifier. And taking a closer look, we see that indeed the checksign modifier calls check signed, which calls the consume operation function on an arbitrary cosigner. So to summarize, there is an unsafe external call to a user specified cosigner. And in turning this call we can swap out the proxy from our contract to the victim contract, and now we can emit arbitrary transfer events.
00:21:26.594 - 00:22:27.180, Speaker A: However, given that ambisafe is a permissioned platform, you'd probably need to KYC to get listed on there, and attackers don't really like going through KYC processes, and so this would be at most a low severity exploit and we would want something better. So now it's time to look at some other pieces of business logic and see what else we can find. One thing that struck out to me was the recovery logic. The recovery logic basically allows you to grant access to your holder id to another user. Now, it's important to note at this point that ambisave doesn't operate on account addresses, but rather account ids. In other words, every address has one account id but every account id might have more than one address. And in fact, the way you end up with two accounts having the same id is you use this grant access function, which while it doesn't let you grant access to an account which already has an id, it does give the new account the same id as your id.
00:22:27.180 - 00:23:27.210, Speaker A: However, note that when two accounts share the same id, they're considered equivalent by the MCE platform. This means that what we can do is we can bypass this check here for allowances, because from id does equal sender id. And so if there was a way to figure out who was going to get tokens, then we could grant them access to our own id, and then we could bypass the allowance check, allowing us to take their tokens. But it seems kind of useless, because how can we know who's about to get tokens for the first time? Unless we can predict the future? As it turns out on the blockchain, we can't sort of predict the future. This is because the life of a transaction is somewhat different from the idealized version. In the idealized version, you click send in your wallet, you wait a bit, and then magically your transaction is mined and the state has been changed. However, in reality there's a few steps to it.
00:23:27.210 - 00:24:05.530, Speaker A: For example, when you click send, your transaction is signed locally by your wallet. Then it's sent to the Ethereum node that you're connected to. For example, if you're using metamask, that node might be infuria. After that, the node that receives the transaction broadcasts it to all the nodes it's connected to via the PTP network. This process repeats indefinitely for every node on the network, and during this moment, your transaction is in what is known as the mempool. After a while, one of the miners mining blocks on Ethereum will pick up your transaction. If you specified a high enough gas price, it will mine it, and only then does your transaction get included.
00:24:05.530 - 00:24:47.638, Speaker A: However, note that there's a brief period of time between this step and this step where your transaction is totally in the public. Anyone can see it, but it hasn't yet been included in a block. This means that anyone who's scanning the mempool can actually see what your transaction is and what it's going to do, and react accordingly. So the solution here is simple. What we can do is we can scan the mempool for transactions which, if executed, will result in someone's account receiving tokens on the ambassador platform for the first time. When we see this, we can front run that transaction by sending one of our own, with a higher gas price that grants them access to our account id. When this happens, they will share the same account id with us, and thus be considered to be us.
00:24:47.638 - 00:25:32.200, Speaker A: Essentially, this effectively backdoors their address, because we now control the same account id as they do. When we've backdoored enough addresses, then we can just steal all the tokens. So, to summarize, sometimes a bug will go unfound for years, and that doesn't necessarily mean that the contract is safe. And in general, if you want to find a bug like this, you need to really understand the implications of every line of code, and you need to be able to analyze the contract methodically to achieve maximum coverage. So that wraps up segment two, and I'll see you next time in segment three, the $20 million CTF. Welcome back. This is segment three, the $20 million CTF.
00:25:32.200 - 00:26:04.612, Speaker A: So let's talk about bugs. To be honest, most of the time when you find bugs in smart contract, the vulnerability isn't going to be that complicated. It's going to be missing access controls, a private function which turn out to be public integer overflow. Some very common logic errors like that. Very rarely will you see the sort of complex vulnerabilities that you do on something like a CTF, where the challenges are intentionally made much more difficult. So they're challenging. However, that's not to say that ever happens.
00:26:04.612 - 00:26:46.396, Speaker A: And sometimes the bugs can get very complicated. When you take several very mild bugs and combine them into something very complex and very impactful, that's known as exploit chaining. And let me tell you, when you pull it off, it's extremely satisfying. So with that context in mind, I'd like to introduce you to pickle finance. Pickle finance is a deFi protocol used for yield farming, which means that users can deposit stablecoins or other tokens into so called pickle jars, and then get P tokens back, which represent shares of the jar. Pickle strategists then use the tokens you deposit to generate returns. They do this by passing the tokens into the controller, then from the controller into one of many strategies.
00:26:46.396 - 00:27:25.826, Speaker A: The strategy then invests the tokens however it wants, and then returns are deposited back into the pickle jar, which increases the value of each individual share. As usual. If you'd like to follow along, you can do so at this commit hash on this repository. So, like I said, the controller sends tokens to the strategy. The strategy invests them in other protocols. However, sometimes protocols like to reward their users, even if it's retroactive in fact, we saw that with comp and with uni from compound and Uniswap respectively. And so there needs to be a way for these strategists to recover the airdrop tokens so that they're not locked forever, because they do get very valuable.
00:27:25.826 - 00:28:10.610, Speaker A: The solution chosen here was to designate the primary asset, the asset that's being deposited into the strategy as want, and then lock that asset so that the controller can't withdraw it. However, the controller is free to retrieve any other tokens. So that looks something like this, where you have this withdraw function which allows only the controller to withdraw any asset that isn't wont out of the strategy. Seems simple, but here's where the first bug lies. Because what happens if the protocol tokenizes their deposits? For example, if you deposit die on compound, you'll receive CDI in return, and if you deposit Dion Ave, you'll receive ADA in return. And CDI and Adar are just regular ERC 20 tokens that you can transfer around. They're tokenized deposits.
00:28:10.610 - 00:28:48.190, Speaker A: But note that CDI and Adar aren't the want token that would be die. And so these tokens aren't protected from the controller withdrawal. Now if you try to report this bug, you might get maybe a low severity grading at best, because typically we assume that the controller is not malicious. Because if the controller is malicious, then we have much bigger problems on our hands than the controller choosing to withdraw some tokenized deposits. Nevertheless, it's a bug, even though low severity. Speaking of the controller, it can do other things too. You can set fees on the controller, and you can add or remove strategies from the controller.
00:28:48.190 - 00:29:24.538, Speaker A: And notably, the controller also implements one other function, which is to allow two users, allow users to swap between two pickle jars. The exact function is rather long and complex, so I've only included the first part of it. But suffice to say that this function allows you to swap from one jar to another. Here's the from jar, and here's the two jar. And then here's how many shares of the from jar you want to swap the minimum shares of the two jar you're expecting. And then these two fields. This is an array of addresses which you presumably will use to convert from the underlying tokens in the from jar to the underlying tokens in the two jar.
00:29:24.538 - 00:30:10.954, Speaker A: And then this is an array of data to pass to the targets. Now you'll see here that the targets must be whitelisted, and we'll get to why that is later. But already we notice that although I haven't included it. There's no validation that from jar to jar are legitimate pickle jars. Now there's this theory in CS called duck typing. It says something along the lines of if it walks like a duck and quacks like a duck, it's probably a duck, and it's used to sort of represent the idea that something can meet an interface if it approximately behaves the same. But on the blockchain duct, typing doesn't really work because it's totally possible to make an evil contract that totally walks like a duck and quacks like a duck, but also hacks your protocol and then steals all your money.
00:30:10.954 - 00:31:18.930, Speaker A: So it's a pretty big issue that the controller just didn't verify the jars were legit, because now malicious user can specify their own jar and then do all sorts of nasty hijinks. Fortunately or unfortunately, depending how you look at it, the severity for this bug would also be low or informational, because really the only thing you could do with the malicious jar is to force the strategy to deleverage itself. And the details for that bug is left as an exercise to the reader. Going back to why the targets were whitelisted, it's because that every time you execute a target, you execute it using the delegate call opcode. This is to allow the controller to be more flexible at the potential risk of a security vulnerability. Because of course, for those who don't remember, the delegate call opcode allows you to execute the code in the target in the context of the current contract, which means that you're basically running the code as the same contract with access to the same balance and making calls as the current contract. As you can imagine, if the target contract was malicious, this is instantly game over.
00:31:18.930 - 00:31:59.434, Speaker A: Your contract will be hacked. Therefore it's very smart of the Picodevs to be whitelisting the target. However, whitelisting only goes so far, because if the whitelisted contracts are broken, then you'll still be able to pwn the contract using delegate call. And so that brings us to one of the whitelisted contracts, the curve proxy logic. Now, the curve proxy logic is meant to allow users to swap between curve lp tokens and the underlying tokens. Specifically, you can both burn and mint. Unfortunately, the curve pool's interface changes slightly between different pools, so the proxy supported every pool in a generic manner.
00:31:59.434 - 00:33:10.914, Speaker A: Instead of hard coding support for individual pools, this is how that works. We have this add liquidity function, which takes in a few different parameters, and then it does some logic. So starting from the bottom we see that it calls the curve variable, which is user controlled with some call data. This call data is constructed here, and it's made of curve function sig, which is user controlled liquidity, which is made of an array of a user controlled size with a user controlled value at an offset, and underlying amount is retrieved from a user controlled address. Suffice to say, this is slightly more user controlled data than you would like inside a proxy implementation. And the long and short of it is that basically a user can use this to call any arbitrary function with at most one arbitrary parameter on any arbitrary contract, which is very nasty. But if you reported this bug, I think you would probably get maximum, maybe a medium severity report because you can only specify one parameter.
00:33:10.914 - 00:34:22.382, Speaker A: And theoretically the controller itself can't do too much excluding stuff like withdrawing from strategies and the like. But it can't actually steal tokens because again, for example, if you wanted to transfer tokens to yourself, you would need two parameters, the token, the destination address, and the amount, right? If you wanted to extend and approve, you would need two parameters. You're sort of limited on what you can do due to this one arbitrary parameter limit. So now that we have three bugs, how do we turn these three low medium severity vulnerabilities into one critical severity vulnerability that lets us take a lot of money? Well, if we look at our toolbox, we have bug number one, which is that the controller can withdraw arbitrary or withdraw c tokens from strategies. These tokenized deposits bug number two, which is that the controller doesn't verify jars, and bug number three, which is that the curved proxy logic can call any function on any contract with one arbitrary parameter. And with all of these three bugs, we can actually chain them together and come up with this pretty slick exploit, which is to first create some contracts pretending to be the pickle jars. This exploits bug number two.
00:34:22.382 - 00:35:14.638, Speaker A: We then execute swap exact jar for jar and specify the curve logic, which is the buggy logic. We can exploit bug three and bug one together to force the controller to call withdrawall on the compound strategy to bring all the CI to the controller. Now we have a bunch of CDI in the controller, but no way to get it out except in the middle of executing swap exact jar for jar, the controller will transfer the assets for us into our fake jar. Now I've slightly simplified this a bit. You can see the full exploit at this URL which Bantek has kindly uploaded to GitHub. But suffice to say, this bug would allow you to chain three low medium severity vulnerabilities into a critical severity vulnerability that cost pickle $20 million, approximately. So some takeaways here.
00:35:14.638 - 00:35:47.046, Speaker A: The first one is input validation. As it turns out, duct typing doesn't really work on the blockchain because again, anyone can upload contracts in the blockchain, they can do anything they want. Attackers are going to upload contracts that pretend to be one thing and then do another thing. So the best thing to do here is to make sure that you validate every input, especially contract inputs, against the chain of trust that you know is secure. And my go to example for how to do this properly is the auger code base. This is a function from the auger main contract. You can see here.
00:35:47.046 - 00:36:27.438, Speaker A: This function takes in two parameters, the universe and the sender. My bad. And it immediately validates first that the universe is trusted. Right? And so this is obviously validating against some internal registry of trusted universes. But once the universe is validated, then they can check the universe to see if the market is valid, the market being the sender. And so you can see through this chain of trust, it eventually validates that the current sender is legit and the universe provided is legit, because if any one of them wasn't, this entire check would fail. The other takeaway is that complexity breeds insecurity.
00:36:27.438 - 00:37:17.830, Speaker A: This hack wouldn't have happened if Pico didn't try to handle the curve proxy logic generically by allowing the user to specify all of those different variables. If they had just provided five or six different functions for all the various types of curve pools, then this exploit would have been impossible. And so sometimes if you're trying to be really smart about things, you might end up accidentally shooting yourself in the foot. Also, just because you find a low severity bug doesn't mean that it actually is only low severity. Go back to the code base, take another look, see what you can do with the new powers that the bug grants you. It's possible for many low sovereignty bugs to come together and form one really critical severity bug. And with that we're finished segment three, and so I'll see you next time in segment four, how to optimize responsibly.
00:37:17.830 - 00:38:02.050, Speaker A: And welcome back. This is segment four, how to optimize responsibly now optimizations it makes sense to want to optimize code because the cost of unoptimized code really stacks up over time. Every user calling your function will pay the price of your lack of optimizations for every call, basically until your contract is not used at all, which may be until the end of time. And also just in general, users don't really like it when they have to spend $30 just to transfer some tokens around. And so you can see why it makes sense to want to optimize, especially in these two cases. The first case being when your contract is simple but heavily used. For example, the Safemath library.
00:38:02.050 - 00:39:04.278, Speaker A: Every single gas in that contract is being spread across the dozens of new contracts being deployed every day using that library. So every gas counts, the other case being extremely complex contracts, which, even if not often used, if it uses millions of gas per call, then cutting down 100,000 gas would be huge in terms of gas savings. Now, unfortunately, when it comes to optimizations, there's one strategy that most people tend to prefer, which is to drop down to assembly. And in a way it makes sense, because when you're in assembly, you don't have to deal with all of that compiler generated boilerplate, except for some of the compiler generated boilerplate for stacking manipulation. But in general, you can cut away all that other stuff. But it's usually very important to keep in mind security when you're trying to optimize like this, because compilers will do things that seem OD at first, but are actually meant to address some very specific edge cases and to understand what those may be. Let's take a look at the ox exchange version two.
00:39:04.278 - 00:39:39.902, Speaker A: Now, oxchange is a popular order book based Dex. The way it works is that makers will sign orders and publish them off chain. Makers will also approve the exchange contract to spend their tokens. That way, when the takers browse through the orders and find one they like, they can broadcast that order on chain as well as their offer, and the exchange is able to swap assets between the maker and the taker. Now, crucially, the exchange contract must not allow fake orders. Otherwise the taker can just lie about what the maker's order is. For example, if I was a malicious taker and I could just lie, I would say, well, yeah, that person there wants to sell me one eth for a dollar.
00:39:39.902 - 00:40:19.658, Speaker A: Here's my dollar, give me the ETH, which is obviously not great for the project. As usual. If you want to follow along, the commit hash is given here for this particular repository, and of course pause the video if you like, et cetera, et cetera. Okay, so, diving in, we'll find that the ox exchange supports seven types of signatures, but two of them are always invalid. One of them is the default invalid, the other one is intentionally invalid. One of them is pre signed, which is basically a signature to say actually wait, just kidding. The maker has approved the signature elsewhere.
00:40:19.658 - 00:41:30.150, Speaker A: So we really only care about these two, two, three which are signatures that are generated off chain using an actual private key, and then four and five which are signatures that indicate you should check an external smart contract for approval. Now these last two are particularly interesting because to check an external smart contract for approval, you necessarily must make an external call, and with external calls come the risk of unsafe external calls or re entry. So to avoid that situation, Ox made the use of the static call instruction and we can take a look at how they did that here. So here is the is valid wallet signature function. We can see that it uses the static call opcode here and it calls the wallet address passing in all the gas and specifying these input and these output memory areas. The input is given as the memory starting at CD start and for length of m load call data, which is just loading the length of the call data array. The call data array itself is constructed here and consists of the is valid signature selector followed by the hash followed by the signature.
00:41:30.150 - 00:42:23.714, Speaker A: The static call is expecting an output size of 32 bytes and it's going to put the output back over CD start. Finally, if the call is not successful, if it returns zero, then the signature validation will revert. Otherwise it will be considered valid if the call returned true, in other words, returned a nonzero value. Now at this point we need to take a break and understand how memory in ethereum works. For those coming from AC C or any sort of low level language, you'll be surprised to find out that the EVM has no concepts of pages or malloc. In fact, there is one single linear memory area, and whenever you try to read or write outside of your memory area, you automatically expand your memory. And this memory expansion is accompanied by a gas cost proportional to the amount of newly allocated memory.
00:42:23.714 - 00:43:23.000, Speaker A: This means that it's very easy to accidentally allocate some new memory and use up a lot of gas, which means that one common optimization is to reuse memory. And sure enough, if we look back, we'll see that Xerox has chosen to reuse the memory starting at CD start in order to prevent additional allocations. We should also talk about how exactly the call opcodes work in Ethereum. So nowadays when we want to get some return data from a contract, we'll use the return data copy opcode. This opcode allows you to copy starting from some offset in the return data for some length into some offset in your main memory. However, when the evam was first designed this opcode didn't exist, and only static return sizes were allowed. Which is why every single co op code requires both the well, it requires the target address, obviously, but it requires the input data address as well as the input data length, and then the output data address and output data length.
00:43:23.000 - 00:44:12.280, Speaker A: If I can just. There we go. But this raises some questions. Like, for example, what happens if the contract returns more than the length you were expecting? And the answer to that is, all of the data up to the length you have specified to the opcode is copied into your memory, and then extra data is simply thrown away. So in other words, if you were expecting 32 bytes, and the contractor returns 34 bytes, you'll get the first 32 and the rest is just gone. Now the second question is, what happens if the contract returns less than the specified length? And in this case, all of the data that was returned is written into memory, but extra memory is left as is. In other words, if you are expecting 64 bytes of data, and the contract only returns 32, the first 32 bytes will be overwritten with the return data.
00:44:12.280 - 00:45:11.816, Speaker A: The second 32 bytes will be left as is, and if the value was nonzero before the call, it will still be nonzero after the call. Some other questions you might have include what happens if the contract reverts? And in that case, a zero is pushed onto the stack. Likewise, if it doesn't revert, a one is pushed onto the stack. Simple enough. But what happens if the contract has no code in the first place? Well, in that case, the EVM pretends like the contract's code is made of an infinite series of zeros, and the opcode is the stop opcode, which means that the call instruction will pretend like it executes the first opcode, which is stop, and then stops execution. This obviously succeeds, so one is pushed and there's no return data. So knowing what we know about how the co op code works and how the co op code responds to empty accounts, we can see that there's actually a pretty glaring flaw in this implementation.
00:45:11.816 - 00:46:17.684, Speaker A: Specifically, there's no validation that wallet address is actually a wallet address. It could be any address. And if wallet address is actually an eOa, which stands for externally owned account, it just means that it's someone's actual wallet, not a smart contract. Then we'll see that first of all, success will always be equal to one, because calling an empty account always succeeds. But more importantly, there will be no return data. And so at CD start there will be no changes, so it will maintain the same data as it did before, which is the call data here, right? And we know from the way that call data is constructed that the first four bytes is definitely nonzero, because it's the selector for is valid signature, which means that after the call succeeds, because again, empty account means always succeeds, it will unload from CD, start a value which is again clearly nonzero, and it will be interpreted as true. All this is to really sum it up very succinctly.
00:46:17.684 - 00:47:50.008, Speaker A: If you take any address, any EOA, and you specify a signature of ox four, it will immediately verify no matter what data it's checking. And again, this is very bad, because as the malicious taker, you can now spoof an order from any EoA. You could say oh yeah, this EOA totally submitted an order to sell all of their ETH for one penny, and the exchange would happily accept it because it's a valid signature, or according to the code again, so interestingly enough, the compiler, if it supported static call at the time, which it didn't, which is why also why Ox had to drop down into assembly, would have prevented this, because the compiler, first of all checks if the contract being called has code using the external code size opcode, and it also checks if there's enough return data using the return data size opcode. And so if the compiler supported staticall and Ox used the staticall high level staticall function, then this wouldn't have been a problem. But because they dropped into assembly and didn't realize that the compiler did all these things for security reasons, this bug was introduced onto the second example I have for you. I'd like to talk about the ENS registry. So ENS, the Ethereum name service, kind of like DNS, but on Ethereum.
00:47:50.008 - 00:48:36.072, Speaker A: And because it's on Ethereum, it obviously supports, among other tlds, the ETH TLD. Now the registry itself is a very low level component. It stores the owner of the domain, the resolver for the domain, and the TTL for that domain. And because it's so low level, it's heavily optimized for gas, and as a result was written in LLL, which is kind of like an assembly language, you'll see. So here you can see the definition for a structure. We define the resolver field to be at offset plus oxo from whatever the relative start point is, the owner field to be plus ox 20, and the TTL field to be plus ox 40. Here's how the only node owner function is defined.
00:48:36.072 - 00:49:27.962, Speaker A: We say that if the caller is not equal to the owner of the node, then we'll jump to an invalid location, causing execution to revert. And to get the owner of a node, we simply load from storage, from raw storage. By the way, whatever value the node is plus owner, which again we've defined to be ox 20. And then just to take a look at how setting and node owners work, we can see it begins by checking that you are the owner. And if you are, it just calls set owner, followed by some events. Now, you might be wondering, what exactly is node, what value is it? Because back here we were just adding to it some offset. And the answer is that the node is actually the name hash of the ENS domain.
00:49:27.962 - 00:50:27.300, Speaker A: And so we can see here the definition for a name hash. The name hash of the empty array is all zeros, and the name hash of a list of symbols labels. So for example, foo bar eth would translate into a list of foo bar and eaves. So the definition of name hash is recursive, and it's the shaw three of the name hash of the remaining labels concatenated with the sha three of the current label. And so if we go back to the storage layout, this is again, remember that we're dealing with relative offsets. This is the layout for one node, right? At offset zero is resolver, offset 20 is the owner, offset 40 is CTL. But notice that what we can actually do is we can place a second node next to it.
00:50:27.300 - 00:51:24.542, Speaker A: Now, this absolute storage slot at x can be both interpreted as the owner for node one or the resolver for node two. These are equivalent, right? They're the same slot. Normally this wouldn't be an issue, because it's impossible to figure out what node two is, right? It's very easy to go from a string like Vitalik eth into node one. It's very hard to go from some arbitrary hash node two back into a label. But in this case, it turns out it doesn't really matter, because we don't actually need to know what label node two resolves to. We just need to know what node two is. This is because if we take a look at this example, suppose we currently own node one, so we control the owner field, right? We're the owner.
00:51:24.542 - 00:52:21.142, Speaker A: If we set TTL to any value, for example, our own address, we've now changed the owner for node two. We now own node two without even knowing what label it corresponds to. From here, we set the TTL of node two, and then this would in effect give us ownership over node three, right. If we set the TTL for node three, it would give us ownership to node four and so on. So at this point, we now own node two, node three and node four. Suppose then we sell ownership of node one to someone else, right? So now we no longer own node one, and as a result, we'd likely no longer own node two either, because the new owner might change the TTL, which would change the owner to something that isn't us. But no matters, we currently own node three and node four.
00:52:21.142 - 00:52:55.040, Speaker A: And so what we can do is we can take node three and set the resolver. When we set the resolver, we'll also set the owner for node two and the TTL for node one, so we can take ownership of node two. That way. From here we set the resolver of node two, which gives ownership in node one. And just like that, we've activated a backdoor. We've set and reclaimed the ownership of node one, even though we never really knew what the label for node two or node three or node four are. By the way, this also works in the other way.
00:52:55.040 - 00:53:46.526, Speaker A: And so you can see that, like I said, when we set the resolver for one node, we also set the owner for another. And then of course, setting the TTL for one node sets the owner for the one after it. Crucially, this breaks in variant, where changing the value of one entry in a mapping shouldn't affect the value of another in a predictable manner. But unfortunately, in this case it did. And this is what allows users to set backdoors in the NS registry by claiming ownership of nodes going forwards or backwards, and then reclaiming them going the other direction again. The solidity compiler would have addressed this if the ENS registry was written in solidity. This is because solidity implements mappings by computing the raw storage slot as sha three of the key concatenated with the storage slot of the mapping itself.
00:53:46.526 - 00:54:44.590, Speaker A: This means that even if you knew the key, which in this case is a node, and you knew the slot, you would have to break shot three in order to figure out what key you should use to find the next subsequent slot. Right? Like if you had, if we, if we just redraw this diagram real quick with the way solidity does it. Even if you knew node one, this goes through a shaw three before coming out with the actual slot. And so now that you have the next slot, you'd have to somehow break Shaw three in order to figure out what the other node is that you can take ownership of. And breaking Shaw three is kind of hard. So we assume that it's basically impossible. So to summarize this segment hand rolling optimizations is usually fine, especially if you stay out of assembly.
00:54:44.590 - 00:55:23.342, Speaker A: But if you want to drop down into assembly, really make sure you know all the quirks of the EVM first, because a lot of the time compilers will do things that seem really strange. But the compilers do it for a reason, and it may not be immediately obvious what that reason is until you get bit by it when someone exploits your contract. And that's it for segment four. So I will see you next time in segment five, crosschain complications. Welcome back. This is segment five, crosschain complications. Okay, so as we've seen so far, securing a protocol on a single chain is hard.
00:55:23.342 - 00:56:16.858, Speaker A: And it goes to follow that securing a protocol that rests on multiple chains is even harder. And this is especially true when the two chains can't even natively communicate with each other. With that introduction, let me show you atomic loans atomic loans is a decentralized loan platform that's built on both bitcoin and ethereum. The high level gift is that you lock bitcoins. You get stablecoins easy, except due to the lack of native cross chain communication. There is a really complex state machine that's implemented, and it's actually so complex that atomic loans provides an agent that you can run which helps you navigate the bitcoin ethereum interactions. So how exactly does it work? Well, in step one, Alice the borrower and Bob the lender agreed to some set of terms, for example, what the collateral ratio is, what stablecoins to pay, stuff like that.
00:56:16.858 - 00:57:12.398, Speaker A: These terms can be agreed on anywhere, but you'll probably be agreeing to them on the Ethereum blockchain, where there is a smart contract to help facilitate this. After both parties have agreed, Alice and Bob initialize the loan using the same smart contract and commit some secrets. Alice commits a one which allows Bob to withdraw her collateral if the loan expires without being repaid, while Bob commits b one which allows Alice to rekilling her collateral if she repays the loan. Alice and Bob also commit various a two b two secrets which are used for liquidations but are relatively unimportant in this example. Note that by commit, I mean that Alice and Bob will submit to the chain hashed versions of the secret. So they're not revealing the secret, they're just making it possible to prove that they're giving the correct secret after the fact. After the loan is initialized, Bob will lock their principal on ethereum by sending it to the smart contract.
00:57:12.398 - 00:57:53.610, Speaker A: Meanwhile, Alice will lock her collateral on bitcoin by sending it to a p two sh address. For those who aren't familiar, p two sh stands for pay to script hash, and this basically is a type of address whose address itself is the hash of a script, a bitcoin script. In this case, the script allows the funds in the address to be moved only if one of these is true. Either Alice signs the transaction and provides the b one secret, or Bob signs the transaction and provides the a one secret. And also, the liquidation period is over. You can see that in this way, both Alice and Bob need the other party's secret in order to spend the collateral. After everything is locked properly, Bob confirms it and then unlocks the loan on Eth.
00:57:53.610 - 00:59:00.782, Speaker A: Alice can withdraw the loan, but only if she reveals a one, and this is verified by comparing the hashed version of a one to the previously committed hashed version of a one. Now, Alice can use the loan however she likes, and when she's done, she will repay the loan, at which point Bob can claim the repayment, but only if he reveals a one, therefore allowing Alice to reclaim her collateral. So, in a system like this, it's usually good to start with some goals. What exactly is it do you want to do or exploit? In this case, there's a few different goals we can set. For example, as a third party, it'd be cool to steal either the collateral on ETH or bitcoin, either the locked loans on ETH or the locked collateral on bitcoin. Alternatively, as a participant, it'd be cool if you could either steal the loans on ETH by not locking the bitcoin, or steal the collateral on bitcoin without actually providing the loan. In this case study, we're only going to be looking at the third option, which is to take a loan without locking the bitcoin.
00:59:00.782 - 00:59:47.030, Speaker A: And to do that, there's really two options we have. The first one is to never actually lock any bitcoin in the first place, and the second one is to lock, but somehow obtain the b one secret so we can immediately unlock it. Let's take a brief aside to talk about how bitcoin transactions work. Instead of Ethereum, where account balances are just the number attached to the account, bitcoin tracks balances using what's known as unspent transaction outputs. Basically, the way bitcoin transactions work is at a very high level. Each transaction is combination of previously unspent outputs and a set of newly new outputs. So you take your old outputs, you combine them together to form enough bitcoin for your transaction, and you split them apart in a new way.
00:59:47.030 - 01:00:21.198, Speaker A: Some of them might go back to you as a refund, the rest goes out as a new output to your recipient. In this way, each transaction will spend some inputs and generate some outputs. The way that atomic loans checks that you've submitted the correct amount of collateral is like this. First it checks the balance of your collateral address, the p two sh address. This is given in refundable balance and feasible balance. It then checks the unspent transactions for both addresses. This is given in refundable, unspent and feasible unspent.
01:00:21.198 - 01:01:18.670, Speaker A: Finally, it checks three different requirements. The first requirement, the collateral requirement, if I can draw that line a bit straighter, this requirement basically says, is there enough balance in both addresses to fulfill the required amount? The second and third requirements are similar. They both say, does the first unspent output in each wallet contain enough confirmations, in this case, nonzero amount? Now, at first glance, for someone coming from Ethereum, this might look totally fine. You have verified both that there's enough balance and the balance can't be reworked. However, it's important to keep in mind that we're not dealing with Ethereum, we're dealing with bitcoin. And there are some quirks to bitcoin that might not be immediately obvious. The first is that get balance, this RPC call actually returns the balance of zero conf transactions.
01:01:18.670 - 01:02:29.680, Speaker A: What this means is that the moment you send a transaction on the bitcoin blockchain, it's considered to be part of the balance, even if it hasn't actually been mined in the block yet. The second quirk is that we're not necessarily a quirk, but the RPC getunspent transactions will return, obviously a list of transactions. And so you can actually see that here where the code is checking for the first of the list of unspent transaction outputs. And so with these two quirks in mind, we can actually notice a very obvious attack here. The attack is simply to first to Bob's wallet, or sorry, not to Bob's wallet, to the collateral wallet transfer, say 0.1 bitcoin, and then wait for it to receive one confirmation. Now you see that we've immediately met these two criteria here, because our first unspent output, which is this output, has now gotten greater than one confirmation, greater than zero confirmations.
01:02:29.680 - 01:03:14.560, Speaker A: The next step is to submit to the mempool another transaction. This one will have a value of 0.99 btC, and this will be zero conf because we just submitted it. Now, we note that the moment this transaction is processed by the node that the agent is connected to, the balance will now be equal to one BTC. Furthermore, these requirements are still met for the confirmations, which means that the loan can be approved. But at this point, this second output still hasn't been confirmed yet. And so the moment our loan is approved, what we can do is we can replace by fee this transaction with another transaction, sending the bitcoin to us.
01:03:14.560 - 01:04:15.962, Speaker A: Alice. And for those who aren't aware, replace by fee is similar to how on Ethereum, you can replace a transaction with another transaction that has a higher gas price if it uses the same nuns. And so you can see in this way, we've basically found a vulnerability which allows us to take out a loan without locking up essentially any collateral at all, because we only need to lock up a very minimal amount. Okay, so that takes care of the how do we get a loan without locking up bitcoin at all? Question what about getting a loan locking up bitcoin and then unlocking it without repaying the loan? So to do that, we'll need to get the b one secret from the agent. And the best way to start is simply look through the agent for where the b one secret is published. And it turns out there's two cases. Either the loan payment is accepted, or the loan itself is canceled.
01:04:15.962 - 01:05:12.704, Speaker A: Now, we don't really care about the first because that implies we just repaid the loan, which means we now no longer have the loan, but the loan being canceled. Sounds interesting. The way that works is that if the loan has been approved but not withdrawn for 22 hours, then the agent will automatically cancel the loan. Right. And canceling the loan involves publishing the b one secret so that alice can now withdraw her collateral. But if we take a look at how the withdrawal function works itself, we'll notice that while the loan can't be inactive, and while it has to be funded and approved and not withdrawn, there's no requirement here that the withdrawal expiration has been met. So again, if we look back, we'll see that the loan is cancelable.
01:05:12.704 - 01:06:12.922, Speaker A: Or in other words, the loan can transition from the withdrawal state to the cancel state after 22 hours. But here, there's no enforcement that the loan cannot be withdrawn after being transferred into the cancel state. This means that what we can do is we can act like a front runner again, wait until Bob is about to publish the transaction to cancel the loan, observe the secret in the mempool, and then quickly withdraw our collateral on bitcoin, while at the same time withdrawing our loan on the Ethereum chain. So what this example goes to show is that with multiple chains, we get multiple layers of complications. For example, each chain has its own nuances that for someone who's just started developing on it, aren't immediately obvious. So on bitcoin, it might be very confusing for someone coming from Ethereum to find out about zero confirmation transactions. While on Ethereum, the concept of bitcoin is very popular.
01:06:12.922 - 01:07:11.730, Speaker A: But for or, sorry. While on Ethereum, the concept of front running is very popular, but on bitcoin, that's not really a thing that happens. Additionally, building a crosschain state machine is hard, especially ensuring well defined transitions between all the states, especially when you start getting into the dozens of states and the multiples of dozens of transitions. And with that, we end segment five, and I will see you in segment six, escaping the dark Forest welcome back to the 6th and final segment, escaping the dark forest. For those who aren't familiar, escaping the dark Forest is an unfinished sequel to Dan and George's Ethereum is the dark forest that I wrote in around September of 2020. It's a dramatic retelling of the time that several white hat hackers and myself got together to rescue about 25,000 Ethereum, or about $9.6 million at the time, worth of vulnerable funds.
01:07:11.730 - 01:07:42.134, Speaker A: Although we tried to stay close to the truth, there wasn't much room for technical details. And so in this segment, I'm going to be going over some of the details that I didn't get to mention in the blog post. So to start, as we all know, the story starts out with me looking through the contracts. Here's the contract, or at least a snippet of it. It's about 2.2 thousand lines long, which is quite a lot of code. And to manually go through all this code would be quite the daunting challenge.
01:07:42.134 - 01:08:35.070, Speaker A: And so I employed some of the strategies I mentioned in previous segments. Specifically, I started out by looking for places where ether would be transferred out, because that's obviously where all of the really important stuff is happening. And if I could somehow break this function, then I would be able to steal ether from the contract. So then I searched the contract for every usage of transfer eth. There was this one here, but unfortunately this transfers to a hard coded address, and so this function would be useless in terms of exploits. The second usage here transfers the ETH to another hard coded, or rather to an address that we don't control. This is the bond token contract, and so in terms of an exploit, this function would also be not very useful however, this third function here transfers eth to the sender.
01:08:35.070 - 01:09:51.526, Speaker A: So taking a closer look, we see that this function will take a bond group id and some amount of bond tokens, and then it will first ensure that the bond has already matured, and then if it has matured, it will burn the requested amount of bond tokens from each bond in the bond group and then finally return some ether to you. So this is a pretty standard burn function, and this implies that in order to trigger this transfer eth call, we would need both a valid bond group as well as enough bond tokens for each bond in the bond group. So then the next question I had was how do we get a valid bond group? To answer that, I basically searched for usages of this bond group list variable and checked for which functions would write to it. As it turns out, there was only one function register new bond group. We can see that there's no modifiers on this function, which means that we can call it. We see it takes in an array of bond ids as well as maturity, and we have to pass the validation in assert bond group before we can actually create the new bond group here. So the next question is obviously what sort of assertions are being made? And you'll notice that I've sniffed out a lot of code here.
01:09:51.526 - 01:10:25.130, Speaker A: In fact, this code is rather complex. I still haven't bothered understanding it to this day. But that's because immediately you'll notice that it's possible to pass in an empty array for bond ids. And if you do that, then you bypass all of this validation num of breakpoints remains at zero, which means that this array is empty. All of this is bypassed because your bond ids is empty. And then all of this is bypassed because your rate breakpoints is empty. What I'm basically getting at is if you submit an empty array for bond ids, you pass all the validation.
01:10:25.130 - 01:11:07.966, Speaker A: And then if we go back, we'll notice that if you have an empty array of bond ids, you also don't need to burn any bond tokens, which means that you can basically withdraw all of the eth you want because there's no validation being performed. Okay, so obviously this is kind of bad. It puts every single way of ETH in the contract at risk. So my next step was to find out whose contract it was. Unfortunately in this case there weren't any comments in the contract that would lead me to any hints about who the owner was. Usually there is a copyright header or a website link or something, but in this case there was none of that. I tried googling the contract address and there were also no results for that.
01:11:07.966 - 01:11:54.778, Speaker A: Typically what you might find is that in the announcement blog post, the protocol would like to post the address of the contract to say, hey, this is where we deployed it. But that wasn't here. This time I tried googling the contract name and from there I found this blog post, which led me to the protocol lean finance, if that's how you pronounce it. But then I ran into another problem. The entirety of the team was anonymous, and so I couldn't be sure whether the admin in the Telegram group was a core dev or maybe just some social media manager that they hired from elsewhere. Obviously, I wanted to make sure I was reporting this bug to the right person, given that there was about $10 million at risk and anyone could exploit it. I noticed on their website that they had been audited by both consensus and certic.
01:11:54.778 - 01:12:47.706, Speaker A: So I tried contacting consensus first because I'd worked with them in the past. From there, it didn't take me long to get in touch with Alex Wade, who is an auditor at consensus. I briefed him on the situation and then we discussed possible solutions. One of the solutions we considered was to simply publish an announcement, being extremely vague, but just telling users that they should withdraw. However, this is obviously not great, because it immediately tips off the attackers that something has gone wrong, and it might take users anywhere from hours to days to see the message. Another option was to use the exploit to rescue the funds ourselves. However, as we talked about earlier, there are such things as front running bots, and in fact there are some advanced front running bots which not only monitor the mempool for arbitrage opportunities, but in fact handle generic transactions.
01:12:47.706 - 01:13:40.240, Speaker A: In other words, they'll try every transaction, see if executing it will give them the money, and if so, front run you. This meant that if we try to exploit the bug ourselves, we would likely get front run and then lose all the money, which would be very unfortunate. In ethereum is the dark forest, Dan talks about how he worked with Scott to collaborate on a solution to try to beat the frontrunners, but ultimately it was not successful. I had also been in touch with Scott at that time, and we discussed various solutions for how we might trick the front running bots. So I pinged him again, asking whether he'd be interested in giving this another shot. Scott then suggested that we contact Tina, who had been independently working on collaborating with miners for some sort of private relay system. Meanwhile, lean still hadn't responded to our attempts to contact them, so we decided to loop in certic as well.
01:13:40.240 - 01:14:17.978, Speaker A: There was a brief search for anyone on certic. Then we eventually were introduced to Giorgios from the certic team. In fact, here that I want to take a brief aside to talk about identity verification. So say you're in possession of a bug that puts a couple million dollars at risk and you need to contact some people. How do you know whether the person you're contacting is really the person they claim to be? You could just ask them, but then you have to sort of trust them for it. And with so much money on the line, maybe trusting someone's word isn't enough. There's two approaches I found work really great.
01:14:17.978 - 01:15:13.146, Speaker A: The first approach is good for verifying someone's professional identity, and that's to simply send a message to their work email. This works because although it's very easy to spoof an email from a fake sender, it's much harder to intercept an email as the fake receiver. You would have to either perform some sort of DNS hijacking attack against the domain or hack the person's inbox directly, both of which would be hard to do on the spot. On the other hand, spoofing an email is as easy as sending an SMTP request to whichever server you use and faking the from header if you want. You could also ask for a reply just to ensure that they do have access to the mailbox and can't send a message. But I think generally speaking, if you need to verify someone's identity on the spot and they don't have time to prepare for it, then just sending a code to the email is probably good enough. Sometimes you need to verify contract ownership though.
01:15:13.146 - 01:16:32.738, Speaker A: So in that case, what I like to do is just get a signature from the contract employer, because even if they're not the current owners of the contract, it shows that they at least were involved in the process at some point. When I'm requesting signatures, I always like to make sure that the message I ask them to sign contains all of the relevant information regarding the operation, whether it be rescuing funds, whether it be sending its transaction, whether it be receiving funds. This way it's guaranteed that the signatory is aware of all the details because they had to sign the message and they had to have seen it. So back from the brief aside, we finally got in touch with the anonymous developers and again we had to perform a round of identity verifications, even more so this time because they were anonymous and so we had no way of asserting their identity with any sort of public roots of trust. Instead, we had Alex and Giorgios verify their identity by having them confirm access to the emails they used during the audit, which meant that even if they weren't the people in charge of the protocol now, they were at least the people who requested the audit. Once we confirmed that we were speaking to the legitimate people, we proposed our solutions. So we said, look, you can either urge people to withdraw, but that's a bad idea because it'll tip off hackers.
01:16:32.738 - 01:17:09.134, Speaker A: You can try the exploit ourselves, but if we do that, there's a very high chance that we're going to get front run. And in fact, people have been front run for less. But we have a third option, which is we can try contacting a mining pool and doing a private transaction, which will shield us from a majority of the frontrunners. And fortunately, lean agreed to go with option number three. So from here, Tina contacted Spark Pool's co founder, Xiaoping, who offered to help. And again, we performed some more identity verification just to make sure that they were who they said they were. For what it's worth, Tina was already very familiar with Xiaoping.
01:17:09.134 - 01:17:52.126, Speaker A: But out of paranoia, I insisted that we do some more identity verification, because although I had known Tina, Scott, and Alex for quite a while now, I personally did not know Xiaoping or Giorgio's from certic, and so better say sorry. It turns out that by a stroke of good fortune, Sparkwool had already been in the middle of working on a service just like this. They weren't finished, but they had started. And so my understanding is that Xiaoping told the development team that there was now an urgent need for this to be finished, and they need to get it done as soon as possible. And the team finished the rest of the product in around 2 hours, which was extremely impressive. A very fast, very fast development pace. So kudos to that.
01:17:52.126 - 01:19:12.650, Speaker A: While the spark pool team was working on that, Scott and I were working on the final rescue payload. It's here that I want to take another brief aside to talk about liabilities. So you see all of these tweets and all these articles about when rescues go right and you save hundreds of millions of dollars and everyone's happy, but what happens if a rescue goes wrong? Whose fault is it? How do you deal with that? During the heat of the moment, Scott and I weren't really sure what to do with it. We knew that we probably didn't want to be the ones pressing the button in case something did go wrong, given the uncertainties around both Sparkpool's newly minted service as well as all of the frontrunners. So our solution was to have lean push the button that actually transferred the ether out of the contract into their account. Another question we were considering was what sort of hacks obligations does temporarily receiving $10 million create? We could, in our exploit payload, withdraw all of the ether to our own address and then transfer them to an address that link controlled. But then we would have potentially incurred an income of $10 million for a brief moment, and it was in the middle of the night, so we didn't really have the luxury of consulting a tax attorney.
01:19:12.650 - 01:19:47.734, Speaker A: Ultimately, our solution was to mint some potentially worthless SBT and LBT tokens. These are the tokens that lean platform uses, send those to lean, and then have them withdraw the ether directly. Again, I'm not a tax professional, so it's hard to say whether this actually would have shielded us from any tax liabilities. But this was something on our minds as we were developing the payload. In any case, Scott and I finished working on the payload. We decided to split the transactions across two accounts. So, in our first account, note the block number 27.
01:19:47.734 - 01:20:30.930, Speaker A: Ten. We sent a transaction to create a new bond group with an empty bond ids array and a valid maturity. Then in the second account, in the same block, sequentially. Note that this is position five and this is position six. We called the exchange bond group function, exchanging our dummy bond group with no bond ids to a valid bond group with some bond ids, which resulted in us being minted free SBT and LBT tokens. Finally, we transferred the two tokens, the LBT and the SBT, to the lien controller address. And from there, they withdrew the vulnerable funds.
01:20:30.930 - 01:21:08.990, Speaker A: So, some takeaways from this incident. There's a lot more to rescuing funds than just finding the bug. In fact, finding the bug is maybe one of the easier parts. Other considerations include, again, identity verification. How do you make sure you're talking to the right person and you didn't just make the situation a lot worse? By leaking the details to a black hat? Also, liabilities. So if something goes wrong, who's at fault? There are no good Samaritan laws on the blockchain, so you need to figure this out yourself. And then, of course, taxes one of the two inevitable things in life.
01:21:08.990 - 01:22:03.162, Speaker A: Other things that weren't included in this particular case, but might be relevant, include how do you patch something quick enough? How do you pause the protocol if funds can't be rescued using the exploit? And also although at the time private relays were not a thing, nowadays they are very much democratized in the form of flashpots and other private relay system services. So if you ever find yourself in this situation, you can just use flashpots instead of having to reach out to the CTO of a major mining pool. And that concludes segment six, escaping the dark forest. In this last and final segment, I'm just going to wrap up this lecture with some quick conclusions, so I'll see you there. And welcome back to the final segment, the conclusions. This is going to be a nice and short one. So as you've seen, security in DeFi is hard, but fortunately we're learning from our past mistakes.
01:22:03.162 - 01:22:56.754, Speaker A: And so stuff like using safe transfer inappropriately, or not realizing certain quirks about the EVM, or not implementing cross chain state machines correctly, these are things that we've made mistakes on in the past and in the future we will have learned from, and we won't make that same mistake again. But also, hopefully you've seen that in defi security, finding the bug is only the first step, and there's often lots more to follow, like coordinating with the project or verifying identities to make sure you're not leaking critical bugs to the wrong people, or even stuff like liability in general. I hope you found this lecture interesting. It's a bit of a new approach for me, teaching in this way, and hopefully it was fun for you if you found it exciting. If you liked what you saw, feel free to reach out. I'd love to answer any questions you have. If you're interested in getting into security, I'd love to help you get started.
01:22:56.754 - 01:23:04.990, Speaker A: Feel free to send me an email on that email given, or reach on a telegram on that handle, and with that, thanks for watching. Bye.
