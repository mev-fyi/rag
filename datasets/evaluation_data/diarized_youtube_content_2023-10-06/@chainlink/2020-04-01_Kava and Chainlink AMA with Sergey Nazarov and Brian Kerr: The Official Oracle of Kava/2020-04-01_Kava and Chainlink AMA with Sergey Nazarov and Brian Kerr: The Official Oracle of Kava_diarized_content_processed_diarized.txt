00:00:00.170 - 00:00:24.590, Speaker A: Want to thank the Kava chainlink and blockchain community for joining us today. In addition, special thanks to Brian Kerr, the CEO of Kave. Sergey is obviously here with us, co founder of Chainlink, and of course, Sarah Austin, who's the senior content manager for Kava. She'll be kind of co moderating with me as well, so definitely want to welcome her. And thanks to her, she also helped me with a ton of organization, so this couldn't happen without her. So thank you very much, Sarah, for joining us.
00:00:24.660 - 00:00:26.886, Speaker B: You got it. Thank you.
00:00:27.068 - 00:01:00.990, Speaker A: Awesome. So the overall kind of goal of this Q and A is really to allow the community to kind of learn more about the integration, sort of explore both teams and what makes this integration unique and beneficial to both the teams and eventual users of all the products. So a brief agenda for the first kind of 30 minutes, we're going to sort of do a fireside chat structure where questions are going to be asked by myself and Sarah. We also have a few community questions we've gathered. If we have time to get to those, we certainly will. And we'll obviously allow Sergey and Brian an opportunity to respond. If for some reason you do have a question that wasn't addressed, there's a lot of great links that are going to be at the bottom of this video.
00:01:00.990 - 00:01:25.158, Speaker A: So if you want to hop into Twitter, you want to hop into Telegram or anything like that and get those questions asked, I'm sure both teams will be more than happy to get answers for you. Excellent. So to go ahead and start, I'll start with the first question here, and that's really for both of you. We'll start with Brian. So would you mind introducing yourself, maybe giving us a little bit of your background, what your team is building, what Kava is really about?
00:01:25.324 - 00:01:57.266, Speaker C: Yeah, absolutely. So my name is Brian Kerr. I'm the CEO and co founder of Kava Labs. My background comes actually more from the gaming side of things. People find their way into crypto either through poker, online gambling, or video games, or they studied cryptography or distributed systems in school. I came from the camp of gaming, where just digital currencies and digitally native things are just part of everyday life. If you're playing World of Warcraft or Warcraft three.
00:01:57.266 - 00:02:43.914, Speaker C: And I actually took that passion and I made a company called Fanatic and Fanatic Gear, which became one of the largest esports companies in the space. So as twitch streaming and broadcasting of video games became big, we got to ride a really nice wave at fanatic, growing that business to over 100 people. Since 2015 to 2017. So it was like, very cool, very cool experience. But through that, I actually got into crypto as a bunch of my fellow co founders in the space in gaming, started moving into sort of moving digital assets from video games as, like, nfts into different blockchain applications and services. So that was like my deep dive. And at the time when I experienced blockchain, I was like, wow, this is so exciting.
00:02:43.914 - 00:03:46.070, Speaker C: This is really cool stuff. I think I need to spend the rest of my life, or at least the next, for five years to foreseeable future focused on this, because it's going to be game changing things. So at that time, I stepped down as CEO of that company and started cobble Labs. My co founders and I, we all came together thinking cross chain payments was going to be the big thing for crypto, as we saw ripple and a bunch of banks working on those sorts of solutions at the time, in the early days. And we quickly realized that payments was going to be small within crypto for quite some time. And that led us to what we're doing today, where we can take our cross chain expertise and apply that to cross chain settlement and movement of assets. And this is what allows us to do what we're doing at Kava today, which is we've built a cross chain lending platform that can serve as a foundational piece, kind of, to connect all blockchains providing defi services to them, where any user can deposit their crypto, receive a loan in the form of stablecoins.
00:03:47.530 - 00:04:17.310, Speaker B: Right. And just to follow on with what we're currently building, Brian, getting to the Chainlink partnership and why this is so powerful, and I have witnessed you firsthand and the diligence that you've done meticulously choosing from all of these other oracle providers. Why did you choose Chainlink for this integration? And then, Sergey, I'd love to hear from you, followed by Brian's answer on why you say this integration is powerful.
00:04:18.290 - 00:05:23.890, Speaker C: Yeah, absolutely. We spent a lot of time thinking about was there's, of course, many different solutions out in the space, and everything ranges from you can build your own to there's XY and Z providers. But it really came down to a few key things that we needed. One is we're building a DeFi platform that's going to be in control of a lot of people's money. And because one of our main value propositions is creating something trustless that doesn't have a single point of failure, that requirement alone just weed out all centralized oracle providers like we couldn't just pipe in an API and use that. We needed something that would be civil, resistant and could really serve kind of our core use case of deFi, again, being completely trustless, not having a single point of failure is incredibly important. Then the other criteria that we looked at were things like having high quality data.
00:05:23.890 - 00:07:02.894, Speaker C: What was the quality of data that we could bring in using the different solutions? Were they using free APIs or open APIs, or were they premium ones? What was sort of the quality of how did they scrub data? What was the aggregation methods? What was being done there? And then lastly, we had to look at who are actually running these nodes that end up being the oracles at the end of the day. Are these trustworthy entities? Are these folks that are really running high quality software? Are they running kubernetes setups so they can't get ddos? What's really going on there? And who's running? Do they know what they're doing? And the thing that checked, boxed all of these options really was Chainlink, because they use premium APIs, they have really high quality data. And how they bring that on chain was sort of top above all the rest. And then in terms of the Sybil resistance, we could have worked with a third party blockchain to bring that data onto kava, but we'd either be dependent on technologies that don't exist yet, like IBC, or we would have to bridge ourselves to something like Ethereum, where we're relying on the security model of Ethereum rather than for our own use case. And that just wasn't a great fit for us. So what we opted for was to run the oracle software on our blockchain, where we actually could have high frequent updates very fast, very regularly, and we could bring in all of the oracles. So all of the Chainlink oracles can move over, run nodes on kava and post price data onto the kava blockchain directly.
00:07:02.894 - 00:07:56.280, Speaker C: And this is, I would say it's kind of like a next gen solution in terms of what we're able to accomplish here. We get faster price data than what's available for DeFi applications on other blockchains, and we still get that civil resistance by having multiple oracles, all pricing data or all bringing that data onto the kava blockchain. And then it definitely was helpful, and I'm sorry this is going a little long winded, but it was definitely helpful when we were doing our due diligence into who are these node providers? When we looked at the Chainlink oracles, we saw that, oh, actually a lot of these are already validators that we've met in person, that we've validated their hardware setups because they were running nodes on the kava blockchain already as validators. So when we saw that they were also chainlink nodes, we had already done our security audits of their setups, and we had a lot of confidence that they were high quality.
00:07:57.290 - 00:08:11.180, Speaker B: Yeah, certainly that security and frequency of price data is powerful. Sergey would love to hear from you on why you think this groundbreaking partnership with kava is so.
00:08:13.390 - 00:09:23.454, Speaker D: I mean, I think that between USDX and some of the things that kava is going to be building in the Defi sphere, there's a lot of exciting things that can get built in that environment, in a scalable environment, the way that they're approaching it. I think we did have a good amount of very well informed conversations about kind of what is the right oracle mechanism? What are the right properties of it, generally speaking, I think the framework that I would suggest that people view oracle mechanisms through and think about what is the quality of an oracle mechanism, is that they start to really kind of consider a few different pieces of what actually makes up an oracle. So those pieces, they basically begin at the quality of data and whether an oracle mechanism can get high quality data at all. And then they move on to things like the quality of the oracle mechanism itself and its ability to provide updates. I know from the conversations that we had with Brian, some of the things that he found was that. And that we were already aware of. We just don't tend to publicize these differences or say too many negative things or something.
00:09:23.454 - 00:10:36.806, Speaker D: But I think some of the differences that he found that we were already aware of was that there's actually two versions of how to solve the problem with oracles. There's the easy version and there's the more real version. The easy version is you make some kind of piece of oracle software, either centralized or some other system, maybe it has dynamic membership where anybody can join, and you don't really enforce any quality of node operators the way that we do. And then because you don't enforce any quality of node operators, and even for other technical reasons, many of the oracle mechanisms out there, they don't have the capacity to use credentialed APIs. So credentialed APIs are the password protected premium APIs. Some of the folks that I know Brian looked at and that we've seen in the past and now and so on, they take on the version of this problem that kind of says, I have a piece of software, it can transport some data in some relatively fragile way through something I coded and run centrally, or something that runs on some collection of laptops, or something that has dynamic membership where I don't really exactly know who's doing what. And I can't enforce that.
00:10:36.806 - 00:11:37.958, Speaker D: These people are getting data from high quality sources because my system can't even use credentials. So one of the first questions I think that's worthwhile for evaluating an oracle mechanism is can this oracle mechanism pull from high quality data sources? Now that's a very important point, because if it can't, then people are relegated to using either free APIs or trial plans that don't require a password. And these are notoriously bad. They drop responses, they don't have accurate data. Nobody in the real financial world would use a free or a trial plan to trigger any significant or probably any amount of value. So any system that doesn't integrate concepts of what is the quality of data and where is the data coming from is a system that in my opinion hasn't fully thought through everything. And that was, I think, an important decision point from our conversations with Brian.
00:11:37.958 - 00:12:26.070, Speaker D: And it's part of a framework for evaluating any oracle. We, for example, we don't pull data from exchange APIs, we only go to data aggregators because with data aggregators and those are password protected and credentialed. And this is why when we've been building chainlink for years now, we've decided to approach the hard version of the oracle problem, not the easy version. And I think what educated and more informed consumers of an oracle mechanism like Brian and his team care about is the solution to the hard version of the Oracle problem that actually provides real guarantees, real security and access to high quality data. So I think the first checkbox that Brian mentioned was quality of data. There's a nuance there around. You need to have a credentialed API capacity.
00:12:26.070 - 00:13:29.360, Speaker D: You need to ideally also have integrations with high quality data aggregators the way we do. And then you also need to know what data aggregators are good because data aggregators, they provide market coverage. So one of the things that I sometimes see people do with oracle mechanisms is they say, I am going to build an oracle mechanism and I'm going to build a data aggregation system or a data company at the same time. This is a very difficult proposition and is usually born out of people not fully looking at the full depth of the problem. We ourselves do not make a data company. We do not have a data company that aggregates and provides market coverage of APIs to define what a global price is for a certain pair, whether it's USDX or whether it's bitcoin or whatever it is. We rely on the very competent, relatively large community of high quality data aggregators who have 10, 15, 20 person teams, even though we on our team have 1015 years of data experience.
00:13:29.360 - 00:14:26.450, Speaker D: These teams are run by people from Reuters, from Bloomberg, and many of them actually feed data cryptocurrency price data into Reuters in Bloomberg because the data business is actually a very large resale business there. So the reality is that if you don't architect out, this is the sense in which an Oracle mechanism is actually different than a blockchain. Sometimes when people approach making oracles, they're like, let's take the blockchain concepts and apply them to oracles and we'll be just fine. The Oracle is actually in a different category of problems where you're trying to make systems that can't be relied upon to the same degree that a smart contract or a blockchain can, and you're trying to get them to that point. So on the data level that we're discussing here with Brian, the first checkbox is quality of data, which basically requires credentialed APIs and high quality node operators.
00:14:27.030 - 00:15:19.540, Speaker C: Brian, I think if you don't mind, I just like to highlight how important that is because if we have on our deFi system people's collateral, millions of dollars of bitcoin, of XRP, of link, whatever is in the CDP, that's at risk. And basically if prices get posted that aren't accurate, those assets can be liquidated and people lose them right away. One, it would be game over for a developer like us if we had a problem like that. But two, it's like we have to be very serious about what solutions we choose and we have to be very scrupulous in terms of how we select where data is coming from because this is real people's money and it's a lot of it. So it's very critical that we get.
00:15:20.390 - 00:16:36.950, Speaker D: Yeah, absolutely. One of the biggest factors, kind of risk and attack factors that I've seen is that people take a snapshot of what the exchange the volume is for a pair at the day when they make their oracle, they don't use a data aggregator because they either don't know about them at all or they don't think they need them, which is usually a big mistake. And then the volume shifts from the exchanges where the volume was on the day they made the oracle mechanism to two months later, the volume is on other exchanges, new exchanges or exchanges with some kind of incentive they created. And now the oracle mechanism they have. Even if it's some kind of system that actually delivers a value on chain, you arrive at a situation where just from an architectural decision, you have a huge attack vector, where all somebody needs to do is manipulate a thinly traded market. And this is why using any one exchange as a data source for a price, where in an environment where volume can massively shift, literally on an hourly basis, because there's so little lock in for users in the cryptocurrency exchange world, that design decision alone creates a risk that we've already seen exploited a few times. So I think it's very thoughtful that you guys have thought about that risk.
00:16:36.950 - 00:17:28.182, Speaker D: I think the quality of node operators is very important, and I know that you guys have done diligence on that. And we do actually have a lot of overlap between our chain link operators and our kava, and the kava operators. And I think what that will result in is a more integrated kind of approach to the security of how information is broadcast onto kava. Because what we actually do for all the nodes, for all the chains that we go live on is we focus on having a full node setup, so that the infrastructure that a chainlink node operator relies on is not some third party service to broadcast. It is actually running a full node of this environment into which they need to broadcast. So the chainlink software is running the full node of the environment they need to broadcast, that is running. And therefore they're like a full participant, and they have the ability to provide much better uptime and reliability guarantees.
00:17:28.326 - 00:17:49.520, Speaker C: Yeah, and they also have, because they hold a bunch of kava and they're staking it based on kind of them being remaining a good actor. They have that skin in the game where on our blockchain we actually have slashing conditions where if things, they become a bad actor, they actually have capital at risk for doing the wrong things.
00:17:50.150 - 00:19:30.686, Speaker D: Yeah, that's absolutely right. I think the blockchains that have an approach where you have staking for proper behavior on the chain for providing data is the right approach, and then we're going to have that same behavior for staking, link for delivery of data, which we'll have on kava through a bridge, and then people will be able to stake, link and get paid and link for providing data there. So I think the staking approach is another place where we're very aligned. The other positive property that I see is that because of some of the scalability in kava, the frequency of updates can be higher, and the lower fee environment allows node operators to put larger and larger amounts of data that then, as I understand it, other cosmos chains can come and get from the cowboy environment at relatively or pretty much no cost. And I think the capacity to put relatively frequent updates is something that we've been doing our best to do in all the environments we work in, and the more scalable environments will allow more, and therefore more accurate data. This is actually another parameter of evaluating an oracle mechanism, where when we ourselves look at other oracle mechanisms, or we try to reason through what are they doing or what are they not doing? Sometimes we see strange things where certain oracle mechanisms, if somebody actually digs into them, they could find that it's been a week since one of them has updated a price and they have a stale price on their main net public environment. And maybe that's because of costs or any other number of reasons of how they run their oracle.
00:19:30.686 - 00:20:56.414, Speaker D: But I think, yeah, data quality is very important. Quality of node operators is important, which is why dynamic membership is something we've stepped away from. We've stepped much more towards. Here's the reputation of the node operator, and I think we're even going to probably move to an environment where not only here's the reputation of the node operator, but here's the reputation of the data provider that the node operator is either representing or in many cases. Now we have node operators that are also data providers themselves. And so I think those are just some of the high level checkboxes and kind of points about how do you evaluate a good oracle mechanism that in our conversations with Brian we've hit on, we've kind of shown what the right questions to ask are, quality of data, quality of node operators, why those node operators can guarantee certain outcomes, whether that's reputation or stake or both, and then also the ability to give good frequency of updates, and that there's actually a track record of doing that, that an oracle mechanism connected to a good node operator has been giving good frequency of updates, which is something, if people are really evaluating an oracle mechanism. The frequency of updates and the accuracy of updates in terms of data quality are two of the things I think they need to look at.
00:20:56.414 - 00:21:21.238, Speaker D: Now, the frequency of updates doesn't need to be too high, it needs to be high enough for the use case, but it can't be so low that the staleness of prices begins to affect a use case's dynamics. And I think in our conversations with Brian we found that we can meet all the requirements on data quality using aggregators, quality of node operators, and the frequency of updates dimension as well.
00:21:21.404 - 00:22:00.146, Speaker B: That's right. Yeah. It's so profound and important to pay attention to that. And I think that we basically just answered the question that I had, which is just to learn about how this came about, the challenges that we're trying to solve with this integration. So I think we can probably skip that question. And then, Rory, I don't know if you wanted to ask Brian about why he selected Chainlink as the Kava official oracle, unless you already think that that's covered, I could move on to the next question.
00:22:00.328 - 00:22:14.360, Speaker A: Yeah, sure. I think we're happy to touch on that for just a few moments and just kind of summarize. But Brian, could you maybe just explain just a little bit or touch briefly on kind of the process that you use to sort of evaluate an oracle solution for you?
00:22:15.050 - 00:23:32.218, Speaker C: Yeah, so, as I talked about before, it was pretty much like the quality of the data. Where's the data coming from? What are the methods of how they're getting that data? What are the quality of the node providers who are actually running these things? Are they trustworthy? After doing very intense audits, do they really pass the test? And then last is a solution for us, a civil resistant solution that we're not having a single point of failure that we can know is going to be robust, and we can maintain our trustless properties as a DeFi platform. Just maybe pointing on, not after this question, but going back to what Sergey mentioned, what was really profound about this integration for us was we started with our own problem. We said, okay, we have this oracle problem. But then once we talked with the Chainlink team and we designed this solution set to bring the Chainlink oracles onto the kava blockchain, have them post prices there. We then realized, well, shoot, we're this cosmos blockchain with IBC and all the bridges we're building to other blockchains, like we're building a bridge to binance chain right now. We can then be this hub that we have the data already on our blockchain.
00:23:32.218 - 00:24:26.702, Speaker C: We have blocks that go very fast, so we can have very frequent updates of that data, and now we can spread that data to all of these other blockchains that exist within the cosmos ecosystem. And there are things like binance chain, which has a huge amount of value and a lot of assets on it. There's a number of other blockchains, even OKX is building their chain now. Here the Cosmos hub itself, Atom is a cosmos blockchain. And actually, people don't know this, but there's over 80 different projects like these being developed on the Cosmos SDK. And today, developers in all of those teams and all those developers have no access to high quality data. So now with Chainlink and Kava, together, we can be the distributor and the place that they can get high quality data to build whatever application or service that they want to from, and it'll be trustless and reliable.
00:24:26.702 - 00:24:37.700, Speaker C: So I'm really excited that we came up with this. Kind of. Incidentally, we just started with our own problem, but now it became a solution set that kind of the entire crypto industry can use.
00:24:41.770 - 00:25:11.930, Speaker D: I think it's absolutely super exciting. I think the whole goal for us at Chainlink is to get people to build more smart contracts, like what you're doing with USDX, what you're doing with your other Defi contracts. And I think it's very exciting to see more people using all these data sets and kind of being able to build things in a high quality, reliable way. So I think in that sense, it's a great partnership, and we're really looking forward to having all the people on Cosmos be able to build great, high quality defi applications.
00:25:12.090 - 00:25:25.602, Speaker C: I think that we've had some early conversations with some teams, and they're waiting for it. We have to finish building what we're building together, but as soon as it's done, they're ready and they're ready to consume because they have a need for. Sure.
00:25:25.736 - 00:25:33.318, Speaker B: And on that note, I have a community question that does relate. I don't know. Do we have time for one community question?
00:25:33.484 - 00:25:34.774, Speaker A: Sure, I think we do.
00:25:34.892 - 00:25:51.680, Speaker B: Okay, great. So on that note, what are some dfs? Can you tell us the word on the street? Perhaps some use cases that might be possible now because of this kava and Chainlink partnership. And what are you excited about?
00:25:52.610 - 00:26:33.814, Speaker C: Sure. So today is the ability to deposit crypto asset collateral. Binance is our first integration. So BNB deposits and then withdrawing a loan of USDX off of that. That would not be possible. So our primary use case and application, our first product on the kava blockchain, that would not be possible without the chain link solution. And I guess fast forward after we unlock that for a number of assets, kind of the top 20 major cryptocurrencies, I think we've already had a number of teams start developing applications that kind of go on top of the kava D five stack.
00:26:33.814 - 00:27:06.260, Speaker C: So as an example, within Ethereum, there's a project called compound, and they're kind of like a money market solution for Ethereum users. Now, with accurate price data, we can also expand kava and what it's doing into applications like that on top. So we can take our stablecoin, we can take the price data that's being posted on kava, and now things like compound, but for kava and the cosmos ecosystem can now be built. So I'm really excited to see that come about.
00:27:09.210 - 00:27:12.630, Speaker B: Yeah, I'm excited. Roy, do you want to do another community question?
00:27:12.780 - 00:27:30.300, Speaker A: Yeah, I think we have time for just one more. This is kind of to both Sergey and Brian is kind of what's sort of the next steps in this process look like. So we've got know, kind of initial integration going. What's kind of the next steps? What can people sort of expect to see as time progresses here?
00:27:31.310 - 00:28:15.100, Speaker C: Yeah, so in the immediate future, we have two testnets rolling out for kava, and in those testnets we'll have the initial implementation of bringing the chain link oracles over to kava. We'll have those nodes actually run on the testnets themselves. We'll do some rigorous testing to make sure there's no bugs or anything, because the first time this has ever really been done with a cosmos SDK. So we have to kind of be careful and take it pretty seriously. And they'll go through a number of security audits by third parties. And then from there, we believe in about May, kind of mid May, late May, we should be able to move this to mainnet and have everything live and ready to.
00:28:17.150 - 00:28:18.090, Speaker B: Plastic.
00:28:22.350 - 00:29:15.898, Speaker D: From our side. We're kind of just here to make sure that all our users, Kava has what they need, they're supported, that all the pairs that they plan to put out, high quality data providers are involved. All of that is thought through. All the node operators that are getting put on those initial oracle networks that we know they're good node operators with good performance, good performance history. And we're kind of just there also to provide some amount of technical support, just make sure that the integration goes smoothly, just like we do with all our defi, DAP and other mean. Our goal is to really make it so that folks like Kava and people who make defi applications are able to succeed without having to hit up against this oracle problem. So our success is really defined by how many great things people are now able to build using high quality oracles.
00:29:15.898 - 00:30:15.922, Speaker D: And I'm always very excited when I see highly competent teams like Brian's and other great folks kind of coming into the space to build great defi applications like USDX and others. And the other example he mentioned, because I think those applications are going to end up redefining what the space is about. It's going to take the space from. I made a token and I sold you a token that can kind of seed a lot of value into the space that can then be used in all these different financial products. And then the space can become about financial products and insurance and all these other exciting use cases. And I'm always very excited to see really smart, competent people like Brian and his team building these things out and turning them into something that isn't just on paper, but that's real. And we're kind of here to make sure they have all the high quality data and technical support they need to do that for themselves, their community.
00:30:15.922 - 00:30:33.960, Speaker D: And in this case, it's very exciting that many different cosmos developers would be able to get high quality data. I think that's a big one as well. And they'll build exciting things. I think the spacehold will grow in a very significant way.
00:30:34.890 - 00:30:35.960, Speaker B: That's right.
00:30:37.210 - 00:30:55.546, Speaker A: Awesome. Well, thank you, everybody, kind of for joining us. That really sort of wraps up our Q A for today. I want to give a special thank you again to Brian, Sergey and Sarah for joining us and helping us put this together. As I mentioned earlier, Sarah did a lot of work to help me put this together. So special shout out to her. Really appreciate everything that she's done.
00:30:55.546 - 00:31:08.538, Speaker A: Along with the rest of the kava team, we are looking to do more of these types of things in the future. So again, make sure you subscribe to the YouTube channel and look forward to doing this again in the near future. Thank you again, everybody, for joining us and hope everybody has a great day. Stay safe.
00:31:08.714 - 00:31:09.630, Speaker B: Thank you.
00:31:09.780 - 00:31:11.514, Speaker C: Thank you very. Thank you, Sarah.
00:31:11.642 - 00:31:15.158, Speaker D: Thank you, guys. Speak soon. Bye.
