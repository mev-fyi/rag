00:00:00.810 - 00:00:45.770, Speaker A: Compared with existing protocols, we've improved latency considerably because all of these blocks are broadcast in parallel. Thank you. Thank you. I'm excited to be here. So here at Smartcon for the past two days, we heard a lot of exciting presentations, talks, demos on services, systems that are already in production and are launched, and we heard about launches. And those are great and really exciting. This is the present, and I want to talk today about the future.
00:00:45.770 - 00:01:42.120, Speaker A: So I want to shed some light into the work that goes behind the scenes into creating these technologies and the thought that we're putting in Chainlink labs and in the community into creating and meeting future needs. I want to talk about recent innovation that comes from our Chainlink labs. Now, why the hell would I do that? Why would I share future ideas, future results here? Well, there are three reasons and three main messages that I would like you to take away from this talk. One, we have a strong emphasis on technology. We at Chainlink, we bring these advances purely through technical merit. And we as a field and we in the community are changing the world through technical merit. This is what this is all about.
00:01:42.120 - 00:02:13.680, Speaker A: The financial systems, the existing systems that are somewhat slow and arcane compared with the systems we are building. They're like a horse and a wagon. And we are the Ferrari. So that's number one. Number two, this field is moving very quickly. We are just at the beginning, not at the end, like a good racing car. Every year there are new advances, and the only way to stay on top of it at the cutting edge is to participate and contribute.
00:02:13.680 - 00:02:47.180, Speaker A: And we have a team of research at Shannon Collab, and we have collaboration across the industry and with academia on these. So this is number two. And number three, we're committed to openly talking and sharing these advances. We blog, we post tutorials, white papers. We're here to talk about them. We welcome collaboration and we welcome scrutiny. We want to make sure that we are building the most reliable, the most secure systems that we can through this openness.
00:02:47.180 - 00:03:27.808, Speaker A: So if I scroll back. There we go. If I scroll back to my title slide, you see a list of names that I will not read to you. But these results and the work that we are doing is part of broad and wide collaborations with our partners in the ecosystem and in the scientific world. So we saw this slide come up in a number of presentations here at Smartcon. Chainlink is a decentralized computing platform for building verifiable applications. And I want to double click on this word, decentralized.
00:03:27.808 - 00:04:31.296, Speaker A: What does that mean? Well, in a nutshell, this is what we do, we bring a network of nodes run by distinct node operators that may not know each other, may not trust each other. We bring them to form consensus and act in coordination. So take price feeds, we take a network of nodes. Each one of them gets information about prices from their sources, and together, jointly, they form agreement on a report that they post to chain again and again. Take CCIP, we take a network of nodes, and they carry messages and sequence them from one system, could be a chain or a real world system, and post them into the chain. This is the method that powers and drives all of our technology in this space, not just chainlink. This is the method at the core that powers all blockchains.
00:04:31.296 - 00:05:10.400, Speaker A: And it really started this disruption. So this is so important that I want to step away from a particular product or service and really abstract it. I'm going to do it very hand wavily, very briefly, so that I don't scare anybody, but still give you an idea. So, in abstract terms, we have this model where we have a network of nodes. They have input, they have requests coming in, and then jointly they form a sequence of output composed of these requests. So we usually refer to them as blocks. So it's a sequence or a chain of blocks.
00:05:10.400 - 00:05:54.992, Speaker A: Each one of these blocks carries some of these requests. This is the most abstract definition. It doesn't care about what these requests are about or what do you do with them. And the challenge is to do this by a network where some nodes may suffer outages, crashes, some of them may even act adversarially, and still the network is able to overcome and tolerate these failures. And likewise, the network could have adversarial periods with unpredictable delays. And still the network as a whole is able to form a joint, coordinated output. So this is a model that we refer to as partial synchrony.
00:05:54.992 - 00:07:09.404, Speaker A: So how hard can it be? Well, it's pretty damn hard. In fact, in many, many settings, it is provably impossible to solve it. So, for example, we know that you absolutely have to incentivize and secure your network of node operators such that no more than a certain threshold, a third would actually suffer failures at any moment in time. And we have a lot of experience operating such networks and making sure we incentivize and secure them to operate in this way. And once you do that, the network is able to remain consistent and to have consistency of this output, of this sequenced output, even against arbitrary network delays. So this is the partial synchrony model that we are developing methods in and to, and experts in the field of distributed systems have been looking for solutions that would perform well and would scale literally for more than four decades. This problem has been formulated for the first time in the early eighty s, and by scaling and performance, I mostly think about two things.
00:07:09.404 - 00:07:32.624, Speaker A: Look at the left side of the slide here, communication and latency. We want communication to be efficient. We don't want too many messages, and we want the latency to be low. We want outputs to go quickly. Now there's some lower bounds and some known impossibilities. We're not going to be able to circumvent them. So in the worst case, we want to match them.
00:07:32.624 - 00:08:29.908, Speaker A: We want to be optimal in a rigorous sense, and there's a fourth dimension that is desirable. We don't want complex solutions, and the reason we want simple and developer friendly solution is because this is the way we can gain confidence in an implementation built and maintained in code. So, as I said, we have four decades of scientific work in this domain, and therefore, I'm really excited to be able to tell you about two advances that we've been able to bring in the past year from Chainley clubs. And I think the title of my talk in the program here at Smartcon is breakthroughs. I don't know about breakthroughs, but I would definitely say two recent advances. One, hot stuff. Two is the most efficient and fast known solution to date in this model.
00:08:29.908 - 00:09:24.470, Speaker A: Partial synchrony. And if that's not enough, I will also briefly mention bobca chain, which takes hot stuff two and enhances it with additional parallelism to increase throughput even more. Okay, so I've got 21 minutes to go over two of these advances. Let's get started. To appreciate these advances, I'm going to give you a very, very brief and somewhat skewed historical context. Just go through two or three important milestones that set up the state for it. So the first practical solution for the partial synchrony model that is known was given in the late ninety s, and that has become the golden standard and really the key skeleton or framework for all solution in this field.
00:09:24.470 - 00:10:14.736, Speaker A: And what that framework gives you is one really, really nice property. These solutions can move as fast as the network. So as soon as messages are carried by the networks, the decisions and the output can be done. Whatever speed the network is operating in, you don't have to wait for some predetermined deadlines. So that's the really nice property that these solutions have and that makes them practical. However, in terms of the communication complexity and the nature of these protocols, they were really designed for small networks. They're not really ready to scale for very large networks.
00:10:14.736 - 00:10:48.748, Speaker A: So they have a lot of messages, a lot of steps. Only the experts can understand some of these protocols, and sometimes even they get them wrong. A lot of fundamental flaws have been exposed over the years in these protocols. And here I have on this slide a picture by Harvard professor Mickins, who said, and drew this picture, this is the worst moment in my career. And this was the moment he looked at some of these protocols. At the same time, we have a different solution in our field. We have the bitcoin core method.
00:10:48.748 - 00:11:20.084, Speaker A: This is also the method that powers the bitcoin network and forms consensus on an immutable history of transfers. It is very simple. Couldn't get simpler than that. Every once in a while, a node wakes up, posts another block that extends the chain, and everybody agrees on the longest chain that has been posted so far. So simplicity is there. Communication couldn't be more efficient. However, it is very, very slow.
00:11:20.084 - 00:12:22.910, Speaker A: So in order for that to be, for this sample method to be consistent and secure, it relies on proof of work, consumes a lot of energy, and has to work very slowly. So we kind of have this conundrum, very fast, responsive, move in network speed, but complex protocols with a lot of messages, or very slow but very simple and friendly protocol. And it would be ideal if we can marry these two extremes and get the best of all worlds. In order to do that, I want to explain to you what is the crux of the difficulty in this partial synchrony framework. I want to give you a really brief explanation. So in order to keep me kind of brief, there's this jewish parable that somebody came to a rabbi and said, rabbi, rabbi, can you explain to me the whole Bible? But I don't have a lot of time. Do it really, really quickly.
00:12:22.910 - 00:12:49.364, Speaker A: And the rabbi said, sure, I can do that standing on one leg. Here it is for you in one sentence, love thy neighbor as if it was yourself. That's it. You can go home. Okay, so I don't know if I can do it that briefly, but what I'm going to do is I'm going to try to explain the crux of the difficulty in partial synchrony protocol. Standing on one leg, let's see how far I could go. Worst case, I'll change legs.
00:12:49.364 - 00:13:43.252, Speaker A: Okay. All right. So the crux of the difficulty is the following. All of these protocols, the basic framework, work by every period of time, electing a leader or agreeing on a leader, letting the leader have a small period of time where the leader can propose the next block in the sequence and everybody accepts it. That sounds really efficient and really fast. But in order for this to be consistent, you need to manage the case where the leader is bad and the next leader needs to go to the nodes in the network, collect information, and make sure that the next proposals and the next leader forms the next block is consistent with whatever may have been committed before by the previous leader. Okay, so that's the crux of the difficulty.
00:13:43.252 - 00:14:17.430, Speaker A: And the basic idea that has been with us for two decades is a two step solution. The current leader will do a lock before finalized two step protocol. So all the nodes need to become locked before any decision is finalized, and then the next leader asks around the nodes if anybody is locked. Then it adopts the lock and continues. That's the crux of the difficulty. That's what you have to solve. If you walk away right now, you have that.
00:14:17.430 - 00:15:06.900, Speaker A: Thank you. And the problem is that since the solutions that we've had for that protocol, for lock before finalize, that was complex. And it was complex both logically collecting these messages, forming a description of all these messages, sending them to other nodes, convincing them that this makes the next decision safe and consistent. That has been complex in code, complex to understand. Macon has had his worst moment looking at these protocols, and it's also complex in the mathematical sense. It sends a lot of messages. Quadratic, super quadratic messages works for small size networks, but doesn't scale.
00:15:06.900 - 00:15:49.132, Speaker A: So this was the state of art until about 2016, when people started looking at scaling these protocols. And I was fortunate to be part of a team that developed a solution called hotstuff. And hotstuff managed to close the gap between the simplicity and the efficiency of protocols like Nakamoto consensus, like bitcoins, and these partial synchrony mechanisms. And the idea was really simple. It took us three, four years, but it's really, really simple. So, instead of doing lock before finalize once, you do it twice. So a leader will ask everybody to lock on the locks before finalizing a decision.
00:15:49.132 - 00:16:18.910, Speaker A: And what that guarantees is nodes will become locked only at the second round. In the first round, they just remember what they hold, what they know. And now the next leader can ask around any node. If it gets the lock, then it has to extend it. And if not, that's okay, because that means no node reached the second round using that core idea. Just lock twice instead of once. Two rounds of locking instead of once.
00:16:18.910 - 00:17:12.248, Speaker A: You can then linearize the protocol. You don't have to use more than linear communication, the leader just goes to the network once and comes back, and the protocol starts looking really like a blockchain and not like a very complex protocol. So, on the left, we have the hot stuff measures. What you get is a protocol that has both the efficiency of communication and the speed, and at the same time, is remarkably simple. And so it has been adopted quite broadly in the blockchain field. And if you look at the QR code on the screen here, it even has a cartoon song to accompany it. So all of this is great, but it still left us with a little bit of a conundrum.
00:17:12.248 - 00:17:38.852, Speaker A: So now we have three alternatives, and neither one of them is perfect. We have a fast solution, which is not very efficient. We have an efficient solution, which is really slow, and then we have an efficient solution, which is almost fast. But we had to add this extra step of locking. So now we have three step core protocols and not two. So, time to break new grounds. Not before.
00:17:38.852 - 00:18:17.808, Speaker A: I actually broke a few other things. So those of you who know me know that earlier this winter, I suffered a spectacular skiing accident. Broke two wrists. Not recommended, couldn't type, couldn't do a lot of things. Thank God for electric toothbrushes. So I was a little bored, and I looked back at this dilemma that we have, and really, for the life of me, I don't want to come across as a hero or say, hey, we made lemonade out of lemon. But really just being bored a little bit, I looked at them, and I couldn't understand why we needed the second step, the second locking step to begin with.
00:18:17.808 - 00:19:01.596, Speaker A: So it was with us for a little over four years, and then with my colleague, Professor Nayak at Duke University, we said, look, the leader already has all the information it needs in the first round. If it gets a lock from the immediately preceding leader, it has to adopt it. It continues, and it knows that it has the most recent lock. If it doesn't hear from anybody about the lock, then remember, every leader is given enough time to try to make progress. Otherwise, these protocols would not have liveness. So already, if you didn't get the lock already, you waited enough time, and the leader has all the information it needs from all the nodes in the system. So it's as simple as that.
00:19:01.596 - 00:19:43.048, Speaker A: And so, with hot stuff, we introduced a protocol that with hot stuff stew, we introduced a protocol that takes hot stuff, just throws away the extra step and replaces it with a simple if. And then we get the best of all worlds two step protocol. Simple protocol with simple leader replacement and fast and efficient. And here's the same thing in tabular form, listing the evolution of protocols, starting from the first practical consensus protocol, which was fast but not very efficient. A very different solution. Nakamoto consensus. It is efficient, but it's really, really slow.
00:19:43.048 - 00:20:35.900, Speaker A: Then gradually improving through hot stuff first. And now hot stuff two, which checks all of these boxes. All right, are we done yet? No, we're not. So we're never satisfied in science. So we looked at all of these protocols, including hot stuff, too, and they all have this kind of core sequentiality nature to them. In all of them, there's a leader that has to, one after another, one by one, add blocks to a sequence. So we fundamentally lock bottlenecks at the speed that these leaders can propose blocks.
00:20:35.900 - 00:21:26.792, Speaker A: And so this led us to improve the throughput even further by allowing parallel blocks to be injected and proposed, and then all of them become finalized. So now the throughput is greatly increased. And this is work with my colleagues at chain inclus, Krisa and Ted, and it's called Babca chain. So, very briefly, the idea is, take your favorite consensus protocol, which by now, I hope is hot stuff, too. And this is these crowned blocks in the figure behind me. So these are blocks that are sent by leaders and form a sequenced backbone. But in addition to that, there are all these other squares that form a dag.
00:21:26.792 - 00:22:02.548, Speaker A: So a dag is a very scary word. Direct cyclic graph. All it means is that everybody can propose blocks. These blocks can reference each other, and this forms a partial ordering or a direct acyclic graph. And leader blocks are also referencing these blocks. And every time one of these leader blocks with a crown gets finalized, all of its causal history gets finalized with it. And any deterministic ordering among these blocks holds because they already inherit the consistency and the finality from the leader blocks.
00:22:02.548 - 00:22:54.612, Speaker A: So that's in a crux, the idea of DAG based protocols, nothing more than that. And we were not the first ones to do it. Of course, there are other projects and systems built out there with this idea. But once again, we were able to considerably push the envelope here and considerably improve state of art. So if you look at existing DAG based protocols until roughly 2023, now, today, the fastest one, the best one, the most efficient one out there would take, at a minimum, four layers of blocks in order to finalize even a single block. And these blocks have different roles. Some of them are proposals by the leaders, some of them are votes by the other nodes.
00:22:54.612 - 00:24:08.320, Speaker A: And you have these specialized layers. So if I want to send a block. I have to wait until a layer that allows me to propose a block. So there's a lot of latency involved, and even more so if you look at a sequence of four blocks that are needed to finalize each one of these blocks in existing solutions is itself a full distributed protocol with multiple rounds and messages. So we looked at that and introduced a new primitive, a new broadcast primitive, which is based on the observation that these broadcast protocols already have inside them, something very similar to this two step lock before finalize principle that I just explained to you on standing on one leg ten minutes ago. So all we did is peek inside this protocol and say, hey, we can expose this in a new API and build consensus with it. And so we've introduced Babka chain, which is based on Babka Byzantine broadcast with complete adopt.
00:24:08.320 - 00:25:00.320, Speaker A: And it's a broadcast built layer by layer, just like this. Very nice babka cake on the picture here. So at the innermost layer, you simply broadcast at a best effort, but you add references. Every broadcast, every block can reference other blocks. On top of this, you add consistency, which is one round that guarantees that a leader cannot send two different blocks to different nodes. And on top of this, we add this API that allow everybody to pick inside the consistent broadcast and indicate whether they have been locked or not. And that's all you need in order to form a protocol that uses bobca in a single message, a single block in this dag, to implement the entire finality.
00:25:00.320 - 00:25:48.130, Speaker A: And so looking back at the existing solution and how we improve on them, so look back at this sequence of four blocks. So the first thing is we get rid of is vote. We don't need blocks in a dag to express votes, because internally babka already has this lock before finalized, which is used instead of voting. So we get rid of one of these blocks in the chain, one of these layers. Secondly, once you get rid of votes, you don't need specialized layers. You can broadcast proposals. Every node can broadcast a proposal whenever they want, and immediately the next leader, Bavca block can reference it and finalize it.
00:25:48.130 - 00:26:38.764, Speaker A: So that means you can finalize every block in the bhavka chain dag in just two, in a sequence of two blocks. And finally, we observe that only the leader messages need to be Babka broadcast. Everybody else can just use best effort broadcast, just a simple one network latency, and then they inherit the consistency and the finality from the babka blocks that refer to them. So in the end, we have a two block mechanism to finalize any block in the babka chain. The first one is just a single network trip, just best effort broadcast. The second one is babka. So this is what it looks like in our dug.
00:26:38.764 - 00:27:15.280, Speaker A: This is the babka chain dug. All of these thin blocks are just best effort broadcasts. Just nodes are broadcasting them. And then leader blocks use BABCA to finalize, form a backbone that references these best effort broadcasts. And then the whole causal history becomes finalized right away. Compared with existing protocols, we've improved latency considerably. We have excellent throughput because all of these blocks are broadcast in parallel.
00:27:15.280 - 00:27:49.010, Speaker A: All of them have utility. As soon as a node broadcasts a new block, it very quickly becomes finalized and ordered and finalized into this total ordering of blocks. And we didn't make things more complicated. On the contrary, we did that by simplifying the protocol. So we end up with a very simple protocol to build and maintain. Cool. So I want to wrap up with going back to the first slide that I started with, but this time, just for variety, I'll read it from right to left instead of left to right.
00:27:49.010 - 00:28:29.868, Speaker A: So the takeaway from both of these advances are, first of all, we are committed to openly sharing it here. I shared it today and go read our papers, our white papers, our blog posts. We do everything we can to explain it. We really invite scrutiny, and we invite you to use these protocols and help us test and robustify them. So that's number three. Number two, as Steve said in the fire chat yesterday, this field is moving very fast. The only way to stay in the bleeding edge of this industry is to be part of it and to innovate and to contribute to it.
00:28:29.868 - 00:29:00.610, Speaker A: And we have a team of experts that are participating and collaborating and contributing to advance the field. And finally, number one, this is a very technical field. I'm sure everybody here loves this technology because of its technical advances. So hopefully this talk was fun for you. And if you missed any detail here and there, please, we invite you to go and read the papers and engage with us and collaborate. So with this. Thank you very much.
