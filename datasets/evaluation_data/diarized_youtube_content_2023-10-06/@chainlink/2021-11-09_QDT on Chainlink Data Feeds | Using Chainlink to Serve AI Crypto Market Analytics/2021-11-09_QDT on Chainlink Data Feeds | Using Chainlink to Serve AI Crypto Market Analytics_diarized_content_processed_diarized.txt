00:00:00.650 - 00:00:20.554, Speaker A: I'll do a mini intro. Hey, everybody. My name is Andy boy, and we're live on chainlink, YouTube. We're going to get started properly. Properly in a minute with QDT. I've got Sean and Rajiv here, but we were just talking a little bit about the hackathon, the chainlink hackathon that's going on. And Sean asked, like, well, what's a cool example of how somebody uses interesting data? Because this is a data feeds episode.
00:00:20.554 - 00:00:57.182, Speaker A: We're going to be learning about QDT and hopefully explain why developers who are in the hackathon or not in the hackathon would want to use that data feed. And so we're trying to think of some other really interesting use cases for data. Another one that I saw last year, Sean and Rajiv. I'll talk to both of you. I don't mind. A couple of guys made an insurance contract for a parametric insurance contract for shipping, aquatic shipping. And the idea is, when water levels are optimum, smooth sailing, no problem.
00:00:57.182 - 00:01:15.138, Speaker A: But when the water levels rise, then to a certain level, it triggers the insurance contract. And so they had to pull rainfall data, or when the water levels are too low, then it's dangerous for shipping, and there's too much volatility. And I don't know. I don't know how waterways work.
00:01:15.224 - 00:01:15.522, Speaker B: Right.
00:01:15.576 - 00:01:24.934, Speaker C: Anyway, they pulled this data channels, I would presume, through channels. And so if the water level is low, then you've got more stuff you got to navigate or be concerned about.
00:01:25.052 - 00:01:45.790, Speaker A: Yeah, exactly. So they made an insurance contract for that, and they pulled that. I think it was rainfall data. Maybe it was actual water level data from somewhere as well. And then the insurance contract just automatically triggers. If the water levels are low, pay out your insurance. It's a really cool use case, and I think those guys are actually building a product.
00:01:45.940 - 00:01:50.078, Speaker C: Hey, Andy, the people who watch this, do they usually watch it live or after the fact?
00:01:50.244 - 00:01:56.226, Speaker A: Most watch after, but the people who watch it live, they're delightful. They're wonderful. They're here.
00:01:56.408 - 00:01:57.682, Speaker C: Hey, what's up?
00:01:57.816 - 00:01:59.490, Speaker B: Hello, guys. Everyone.
00:01:59.640 - 00:02:16.920, Speaker A: And they're from all over the world. We got from Bolivia, from probably somewhere in the russian area, and folks from the states as well. Sean. Hey, Rajiv, before we get started, why don't you guys just talk a little bit about your background and who you are. Rajiv, why don't you get started?
00:02:17.290 - 00:02:39.246, Speaker B: Hi, I'm Rajiv Chandrasekran. I'm one of the founders of QDT. We are a machine learning platform company. We built a machine learning solution that uses a lot of freely available as well as paid data to predict financial instruments. That's what we are here to talk about. My background is in algorithm development. I've been a self styled quant for almost 20 years.
00:02:39.246 - 00:02:48.546, Speaker B: Background is in electrical engineering and algorithms, but I've been doing quants and algorithms for the last 20 years, like I said, and founded QDT about five years ago.
00:02:48.728 - 00:02:56.066, Speaker A: Awesome. Great to meet you. And now we got everybody helloing and gming. Let's get a little GM in the comments right here.
00:02:56.248 - 00:02:56.834, Speaker C: Good morning.
00:02:56.872 - 00:02:59.880, Speaker A: Get it going. And then, Sean, tell us a little bit about yourself.
00:03:00.250 - 00:03:27.998, Speaker C: Yeah, my name is Sean Bachelorld from Vancouver, Canada. Did my degree at McGill in economics, spent most of my time as an engineer and a finance guy, and then got bored working for the bank in the financial investment world. And so I wanted to try my hat at a startup and found these guys and the rest is history.
00:03:28.164 - 00:03:29.902, Speaker A: Nothing but fun.
00:03:30.036 - 00:03:49.554, Speaker C: So I'm the product owner here at QDT. So I spend a lot of time working with our customers. I spend a lot of time working with our developers and kind of liaising between the two. And then as a byproduct of that, spend a fair amount of time architecting the solution and helping to guide and contribute to the development in whatever way I can.
00:03:49.672 - 00:04:22.858, Speaker A: Awesome. Well, we're going to get into that and we're going to get into a little bit of what is machine learning? How does it work in this context? In this context, how do you predict financial markets? How do you teach a model how to do that as well as kind of the ux? We'll have a look at that as well. We got a bunch of people in here. I'm very excited about this. We're going to learn about data feeds and we're going to learn about these sort of predictive analytics. So let's get started on Chainlink Live. Welcome to Chainlink Live data feeds.
00:04:22.858 - 00:04:42.406, Speaker A: My name is Andy Boyen. I work for Chainlink Labs and I'm here talking to Rajiv and Sean from QDT. We're going to learn about their datafeeds product, how they're integrated with Chainlink, and what they're providing to the whole blockchain and smart contract space. Let's get started. Explain what is QDT? How does it stand out? What's kind of the mark that it.
00:04:42.428 - 00:05:32.022, Speaker B: Makes QDT is we are an AI machine learning company. We've built a purpose built platform where the platform has two major components. We've integrated with literally hundreds of thousands of data sources live data sources from financial markets, macroeconomics, weather, news, anything that we think can predict financial instruments. And on top of this data, we've actually built an automated machine learning solution in order to predict any financial instruments. And we provide the results of those predictions as signals that can be used by investors or traders. And specifically in this case, we're going to talk about some of the models we built for bitcoin, for example, that we're going to provide through the chain link data feeds. And that's, in summary, what we do.
00:05:32.022 - 00:05:46.374, Speaker B: We have a machine learning platform to predict any financial instrument, and then it can also be used by developers to build their own models. But we've also curated some models ourselves that we offer through this API.
00:05:46.502 - 00:06:21.240, Speaker A: Cool. Now, you guys just put out a blog post yesterday, kind of showcasing, was it yesterday or this morning, very recently, about the bitcoin model you guys were going to talk about a little bit. We don't have to get into it right now, right now, but we will definitely include a link to that down in the description because it is a cool kind of way that lays out what it predicts, how it predicts, and that sort of thing. So there is a written component to this if you guys want to follow along, viewers as well. Oh, I wanted to say to the viewers, we can see your comments, we can see your questions. So if you do have questions, feel free to drop them. We will get to the ones that we can in this time.
00:06:21.240 - 00:06:38.762, Speaker A: So we're really talking about models here, predictive models. Can you talk a little bit about how those models are built? When it comes to getting an output, a prediction, a signal of some kind, those are those outcomes. So what are those inputs? How do you build those? Yeah.
00:06:38.816 - 00:07:35.678, Speaker B: So let me first introduce the high level concepts of how we build those models, and Sean can give some specific examples. So the way we build the models is every data stream we get, we convert that into a synchronized time series. We time sync all the data and index them by time so that we have literally hundreds of thousands of time series. The objective of the machine learning now is to predict one of those time series components, say that's bitcoin price, using everything else as a potential input. The beauty of our system is we've automated that process fully, because what you typically are faced with in machine learning is how do I pick my data inputs? How do I test them? So we have an automated way of picking the best predictors, and then we literally build an ensemble of hundreds or thousands of models. So essentially we build many, many small models and ensemble them. So you may have a model for bitcoin that uses technical data.
00:07:35.678 - 00:08:07.458, Speaker B: You may have a model for bitcoin that uses macroeconomic data, something that uses financial bonds and indexes. And you mentioned shipping data. A lot of times for some financial instruments, we found shipping data correlates with how prices are moving. So we use any data and also news. For example, we monitor live news feeds from more than 170 news sources. We use that area as well. And then we build many, many small models, and then we ensemble them in order to build a composite prediction.
00:08:07.458 - 00:08:18.460, Speaker B: That's the general gist. And then I can talk specifics of model, of how the models, each particular model is built, if there are questions there as well. But I'll let Pandora to Sean to kind of give you elaborate on.
00:08:19.230 - 00:09:08.026, Speaker C: Oh, well, to expand on that in terms of the application and in terms of the use cases that can be derived from the model outputs, it's really varied. There's a ton of different options. As Rajiv mentioned, you can predict essentially any financial instrument. So the ones that we've been focusing on have primarily been the ones that we have customers for. So that's mainly in the enterprise space, predicting commodities that are used to input or for the inputs for production and manufacturing. But in addition to that, we've also been focused on financial instruments like stocks and indexes, like the S and P 500 and Tesla, Facebook, all the favorites. But our team is pretty passionate about crypto, so we've been building models for that on the side.
00:09:08.026 - 00:09:38.930, Speaker C: And then when we brought those to Chainlink, everybody got pretty excited about it. So that's what basically prompted the chainlink node. And from there, we've been focusing on building out our bitcoin models. So we have a lot of different models. And then, as Rajiv mentioned, there's some aggregation that we do that optimizes the combination of those models, the combination of those signals into one unified output, and we can get more into the details thereafter.
00:09:39.750 - 00:10:19.586, Speaker A: Well, I hope the audience doesn't mind if I get a little into the weeds, because I'm curious about this. And then we've got kind of a question about this, too, which is kind of about variable selection. Essentially, when you identify variables or various other feeds to use as input, how do you select mean? I know that was a topic a number of years ago in machine learning is if there's human people selecting those, then you've just got bias in your model. Right? So, Dr. Squelch here, Sasquelch, sorry, I mispronounced, is there a theory behind the identification strategy, or using best fit, or a best fit using out of sample data, how do you go back and iterate on those? And then I have a follow up as well.
00:10:19.688 - 00:10:47.786, Speaker B: Yeah, great question. Yes, you're absolutely right. In the beginning, one of the things we do in our data is we organize the data in a meaningful way. We have categorized the data using categories, subcategories, dictionaries and so on. So even for human is building hand curated models, they have a very easy way of selecting the predictors they think would make fit the best model. What we currently use, though, is the human curated models are great, but as you said, introduces bias. So we let the machine choose.
00:10:47.786 - 00:11:35.590, Speaker B: The way we let the machine choose is, as you mentioned, outer sample prediction. So if you want to make a prediction for the next, say, ten days or 20 days, and we have the history as of today, and we break that into a test set and a validation set, then test set is in the, the training set is in the past. So let's say we use two years of history, and then we see how well the model performs in predicting the one year in the future that it hasn't seen. But we actually know the answers because we actually know today. In the holdout set, we validate the performance on the holdout set, and then we test various. Now, the metrics we use to see how good the prediction is in the hold outset depends on the use case. It could be as simple as how accurate is it in predicting the directional accuracy or the magnitude of the moves.
00:11:35.590 - 00:12:02.850, Speaker B: Or it could be more complex financial metrics like shop ratios or information ratios. But the general principle is the same. We look at the data available as of today, for example, break that into two sets, a training set in the past and a validation set in the future. Train it on the training set, see how well it predicts the future. And then we run through all the hyperparameter tunings and algorithm selection to see the best fit. And that's what goes into the model.
00:12:03.000 - 00:12:30.394, Speaker A: Okay, so crypto in general as a marketplace is a little different than traditional markets, where there is pretty significant volatility. And so if I'm training my data on the past two weeks, and then yesterday bitcoin jumps up by 15% or 20%, these events do happen or drop. How do you take those sorts of swings into account? How does a model deal with that?
00:12:30.512 - 00:13:18.860, Speaker B: Yeah, so the one thing I should mention is our models retrain every night, so we collect at a periodic, at least once a day. In terms of crypto, we can do it multiple times a day. So essentially we are constantly streaming all the data from all the live sources, from all the crypto markets, and also all the external data, the news data or volatility data. And then we retrain the models every day, because every day, essentially we redo this process, essentially of breaking it up into a test and validation set. So essentially you have the latest information incorporated into your models. So what you get as a prediction? Yes. If there's a dramatic event that happens from today, tomorrow, the model will change tomorrow, and what you see in the API will be a new model update reflecting the new information that's available as of maybe today.
00:13:18.860 - 00:13:19.946, Speaker B: Yeah.
00:13:19.968 - 00:14:03.238, Speaker C: The other thing I'll add to that is that the volatility also provides opportunity insofar as by participating in the up moves and down moves. If you have an accurate model, it's going to be a more profitable market than something that doesn't move as much. So that can work for you or against you. But the volatility contributes to making the modeling process a little bit more challenging. One of the ways that we address sort of the changing market dynamics, or the evolving market dynamics, is through one of the parameters that we use, which is something called n forget forget, which is essentially like the memory of the model. And so the lower the n forget value, the shorter the memory. The longer the n forget value, the longer the memory.
00:14:03.238 - 00:15:17.650, Speaker C: So part of the modeling process, part of the machine learning process, is for the platform to assess these sort of higher level correlations. Rajiv can probably explain this in more accurate terms, but you do kind of an initial sweep of the data, assess those base correlations, and then the machine learning will track those correlations over time. Now, obviously, the correlations a year ago or two years ago are going to be very different than the correlations today, because different cryptocurrencies have come into fashion, and the relationships between those cryptocurrencies, or all coins and bitcoin are all very different. Plus, you've got new data that's available either via glass node, which often doesn't have a ton of historical data, or sentiment data, for instance, from lunar crush that we're pulling in, so that sentiment data doesn't go back as far. But more importantly, the relationships between all of these things are constantly changing. And so when you build a model with a longer end forget, it has a longer memory and the signal behavior is a little bit more stable. With a shorter end forget, it picks up a little bit more on that volatility and is slightly more reactive.
00:15:17.650 - 00:15:51.562, Speaker C: And then that has implications into how the strategy behaves over time. So typically, a longer end forget, will have a more durable and robust performance. But period over period performance might be less consistent. And then with a shorter end, forget. This is getting technical, but a shorter end forget. You might have, let's say, the last month or so that's performed really well, or maybe the last three to six months that's performed really well. Previous to that, that model hadn't performed well at all.
00:15:51.562 - 00:16:09.170, Speaker C: So then it comes down to how do you aggregate these signals together? What criteria do you use? So which performance metrics over what time period? And then how do you establish a weighting to the signals that you think or the machine learning thinks is going to have the best aggregate output?
00:16:09.830 - 00:16:39.180, Speaker A: That's a really interesting idea. I'm familiar with factor analysis. And so you do correlations between all of your factors and see kind of what works out and inter item correlations, all that. But if you do that at the beginning and then you keep back testing for that over time, you can say, well, that mattered at the time, but it doesn't matter now. Or that didn't matter at the time, but now it matters now. That's a really cool, interesting way to approach that. I do see your questions audience, and they're cool, but I have the microphone, so I'm going to ask my questions.
00:16:39.180 - 00:16:58.770, Speaker A: How do models like this, this might be a more general machine learning question. How do models like this account for error? There are so many items and there's so many inner item correlations, and there's so much testing. Seems like error would just creep into these models in a way that creates a lot of noise. How do you guys deal with that?
00:16:58.920 - 00:17:32.774, Speaker B: Yeah, so the way to deal with noise traditionally has been the ensemble models, traditionally, which have become commonplace in machine learning now of whether they're bagging models or boosting models are one way to reduce noise, which is you build many models and you average them out. So on an average, the noise cancels out. Right. That's the general idea of ensembling. And then you can then go to the next step, take multiple ensemble models and stack them as well. So general principles stay the same. The challenge, of course, is that the number of ways of choosing these ensemble models are pretty much almost infinite.
00:17:32.822 - 00:17:32.986, Speaker A: Right.
00:17:33.008 - 00:18:05.190, Speaker B: So there's an infinite space of models to pick from. So how do you do that more efficiently? So that's where some of our core ip comes in. We have incorporated an intelligent bayesian search approach to pick the best approach. So we look for the in the search space looking for a minimum, we want to search more often in the space, which is likely to result in a better minimum. And that's what we have incorporated. So the bayesian search that we have incorporated does two things. It automatically picks the best algorithm set.
00:18:05.190 - 00:18:46.186, Speaker B: So it might be a neural net, it might be a CNN, or it might be a booster tree, it might be SVM. It automatically selects the best algorithm at any given time and within the algorithm, it also optimizes the hyperparameters for that particular set in order to minimize the search time, because we are limited by search time and compute time. And then the way to minimize noise, honestly, is similar to other approaches, which is you build an ensemble of many models and then you average them. Wisdom of crowds, or whatever you want to call it. So the approach of bagging, boosting, ensembling, stacking, it's all an approach to make sure that you average.
00:18:46.238 - 00:19:08.682, Speaker A: Good song you got right there. Ensembling, bagging, boosting, stacking. That's a daft punk song. I love it. Sorry. Yeah, there's a lot of questions about sort of individual sort of indicators, and I'll get to one of them here, but then it pairs into another one. So let me mush these two together.
00:19:08.682 - 00:19:46.280, Speaker A: The first question is, how do you deal with rumors in markets? I know you talked about news data. Is there social media data? Like, how do you guys deal with rumor? That's a really interesting indicator. Does it fit in? And then a follow up, Dr. Sasquatch asks about liquidity concerns, but I'm going to focus on the end of this is for the outcome. Are you predicting price targets or confidence intervals? Or how does that work? And how do you select, like, if you're doing rumor, does that impact a confidence interval with. Or something like that? So kind of those two questions together. One, how do you deal with some of these more intangible inputs? And then what are the outputs for those?
00:19:46.650 - 00:21:04.766, Speaker B: Yeah, so if the rumors and any news item is part of a tweet, we analyze tweets, we analyze news items, and we convert them using natural language processing into standard metrics, whether they are sentiment, relevance, accuracy, and then number of repeats, number of times a news item is repeated, all of those standard metrics we capture. And then really the model is, if the rumor has similar, I would say, parameters, mathematical parameters as to compared to a past rumor. So the machine learning will pick it up because it'll say that last time we saw a rumor which had similar characteristics, the number of repeats, number of people texting it the sentiment or the relevance to what we're doing, and then it'll use that as a basis to predict what might happen with the current rumor. Like I said, it's used in conjunction with all the other data sets. So this is just one component of it. So you may have one component where the model is predicting the rumor has certain impact, and how it's weighted is really determined by this cross validation test we do. Right, which is say, hey, if you build a model using this, how accurate is it in predicting the future price? And everything is based on whether it's accurately able to predict these validation sets.
00:21:04.798 - 00:21:25.320, Speaker A: It hasn't seen how predictive is the coffin emoji in tweets. Well, you were going to show us a screen as well. So when you get through your add on, I want to show the UX as well. We've been going for a while already. I want to keep this going, but show people.
00:21:25.770 - 00:22:17.960, Speaker C: Okay, I'll be quick. So, one example of how rumors might be objectively measured is in the sentiment data. So we pull in lunar crush data, and for each target variable regime, do you remember how many columns there are that lunar crush tracks? 400, 600 or something like that. 600 columns, something like that. So there's basically 4500, 600 individual metrics that they're tracking for every target variable. And what we've done is experimented with taking the entire cryptocurrency market and all of those metrics and throwing that in a model. We've experimented with taking, let's say, the top 100 target variable, the top 100 altcoins by market cap, top 15, and then bitcoin on its own.
00:22:17.960 - 00:22:46.434, Speaker C: When you ask about rumor, the rumor theoretically should be captured within the sentiment data, because rumor, how do you define rumor? It's kind of a subjective thing. What's rumor? What's not, but what is objective and what can be measured is how much traffic is it generating, how much interest is it generating, and what's the sort of projected outcome from that?
00:22:46.552 - 00:22:50.574, Speaker A: Yeah. Rumors can be true or not. Rumors can be positive or negative.
00:22:50.702 - 00:22:52.338, Speaker C: Sometimes bad rumors are good.
00:22:52.424 - 00:23:33.786, Speaker A: Yeah, it's a really interesting idea. My background is communication science, and natural language processing is part of that. And so thinking about these things, like rumor, humans, we have detection algorithms in here that we can kind of figure out those things, but it's hard to put those into a predictive model. Or it was. Maybe you guys are working on that, and it sounds like you have. As for the second part of this question is, how are liquidity concerns or other inputs? What are the outputs, what do they look like? Are they confidence intervals? Can we see that ux you showed me at the beginning? I would love for you to share a screen and then talk a little bit about what that is. What are the outcomes? Because in the end, you guys are selling an API and a data feed.
00:23:33.786 - 00:23:46.258, Speaker A: So I don't want to give you guys a chance to show that as well. One sec, the audience. Thank you. What killer questions. I had questions. I'm not asking them because you guys got great ones. So awesome discussion happening.
00:23:46.258 - 00:23:50.358, Speaker A: Very pleased. Thank you all so much for this. Let's keep it going, yeah, so in.
00:23:50.364 - 00:24:42.950, Speaker B: Terms of the prediction, we actually are predicting the future price of bitcoin in this API for the next n number of days. I think we go up to 15 or 20 days. What we're also providing you with, in addition to the prediction is some standard metrics on all the models that go into predicting that final output. So, like I said, it's an ensemble, so there may be 100 models in it or 50 models. We give you all the metrics for that model, for each individual model. So whether it's information ratio, shop ratio, we also provide trading strategies, which is what Sean is showing here is it's great that the model is providing a signal, but how you actually trade it really determine your actual return. Right? Are you going to buy every time the signal is green, are you going to buy a fraction based on the confidence interval? So we simulate some standard trading strategies, and that's what we offer you as well as part of the API.
00:24:44.570 - 00:25:03.050, Speaker A: This is cool. This is in your blog post that you guys just put out yesterday. So, audience, you guys can find this and you can kind of scroll down, learn a little bit more about that, how they came to these as well. Sean, was there an interface that you were showing me, or was that just an example that I was excited?
00:25:03.950 - 00:25:31.910, Speaker C: Is there is. Let me find it. I got a lot of tabs open. Let me start at the beginning. So these are all of the models that we've built for bitcoin. There are 170 bitcoin models here. And these models are all built using different combinations of algorithms, data, and various parameters, machine learning parameters.
00:25:31.910 - 00:26:04.514, Speaker C: And then from that pool of many models from that hopper, we then pull out the models that have the best performance. So I can toggle over to that project folder. And you see in here, we've only got twelve models. And the reason why I pulled out these twelve models is because they have certain behavioral characteristics that I think combine well. So this approach is a manual approach. We also have an automated approach. We can get into that in more detail.
00:26:04.514 - 00:26:33.130, Speaker C: And what you're going to see in one, two, three, in three tabs is the automated approach. So if I dive into bitcoin experiments, which was that large folder of 130 models, these are all of the models. We call this the matrix. These are all the models. On the y axis, we've got the prediction signal. And on the x axis, each of these columns is a different time frame. So 123-4567 days into the future.
00:26:33.130 - 00:26:58.278, Speaker C: So each of these blocks is its own individual model. And then what we've done is we've grouped them into one aggregate average output. So within each column, we are generating the average signal with no intelligence, just the straight average, and then plotting that on the chart. And that's what gives us this aggregate signal output.
00:26:58.394 - 00:27:21.802, Speaker B: All right, let me add that, sean, we can also see you asked about confidence intervals. You can see kind of the gray lines that tells you what the min and max predictions are. And that's based on the one Sigma or two Sigma confidence intervals of what the price is likely to be. But the prediction we're giving you is the average. The API is the average. But we have every individual model also available. So you can compute those confidence intervals as well, if you want.
00:27:21.856 - 00:27:48.846, Speaker C: That's right. And what you'll notice is not very many of these models are red right now. So that's a good sign, because this was updated today. Now, if I go to the curated group dashboard, these are those twelve models. See, there aren't as many. All of them happen to be green today. What I can do that's kind of interesting is I can change the y axis of the matrix to equity curve.
00:27:48.846 - 00:28:03.800, Speaker C: Now, equity curve looks like this. This is an example of a single model. Here you've got bitcoin price. Here's the historical trading signal. So every day we get a value. I'll zoom in on this. Every day we get a value.
00:28:03.800 - 00:28:52.434, Speaker C: And that trading signal every day is associated with this prediction curve. So this model is predicting six days into the future, and it's saying that six days from now, price is going to be higher than it is. And you can see that throughout this whole period, it was basically saying the price was going to be higher. Now, the model isn't perfect. Right? If it was perfect, it'd be holy grail, and we'd already be so stinking rich, we wouldn't have to be here. But it's better than it is worse. The point is that you can see at the bottom here, this is a subplot of the equity curve.
00:28:52.434 - 00:29:47.142, Speaker C: So what the strategy simulates is when the signal is green, you buy, you simulate a buy, and when the signal is red, you simulate moving into cash. So in that way, it participates. If the model is good and it predicts an up and the price of bitcoin goes up, the value of the equity curve at the bottom gets higher or net position gets higher. And then when you predict a down move, like here, for instance, it steps out into cash, and therefore the value is preserved. So in this way, you're participating when price goes up and you're stepping out when price goes down. So what you're looking for is less drawdown. So you can see that if I contrast the profile of bitcoin price itself, which is quite volatile against this price, you can see it stepped out here, it stepped out here, it stepped out there, right.
00:29:47.142 - 00:29:49.730, Speaker C: But it still participated in the up moves.
00:29:49.890 - 00:29:52.810, Speaker B: And it beats the long buy and hold strategy.
00:29:53.710 - 00:30:28.200, Speaker C: Exactly. That's the crux, is it needs to beat the buy and hold strategy. So right now, this equity curve is 1614. I know that's really small, but buy and hold over the same period is about 800. So this is almost doubled, the buy and hold strategy and the way that I'm going to illustrate that is in this updated dashboard. So this automates the process of aggregating the signals, and it does that by applying various filters, stuff like that.
00:30:29.050 - 00:31:15.054, Speaker B: And this is the one that's available as an API through Chainlink. So essentially, we tell you this give you aggregated signal as well as all the individual model metrics and a simulated strategy. If you were to trade this, how would you make it now, what would you do? What would that give you? And then this is the simplest way of trading, which is if you have a green signal, you buy a red signal, you hold, or you have a fractional, or we have simulated fractional positions as well. But of course, everyone can use these signals in different ways. You can leverage up, leverage down, or you can use these to essentially create your own trading strategy and simulate and back test them. Right? But here are some once we've been back tested.
00:31:15.102 - 00:32:01.294, Speaker A: So now I'm imagining the applications for smart contract developers now that this data is available through Chainlink to smart contract developers. This is awesome. The audience, the comments over here are all like, cool, live demo. This is really great. And everything we covered before, like, oh, I see the length of time, all that back testing stuff in your models, too, so laid out very well, makes a lot of sense. But now the next question is, okay, it is available via chain link oracles. What are smart contract developers going to do with this data? Is it building just automated treasury management platforms for wrapped bitcoin on Ethereum or another l one in the future? Is there the possibility of building an automated strategy for a couple of different models? You said you have your twelve best performing models.
00:32:01.294 - 00:32:11.400, Speaker A: Could I do three against three against three against three and build a couple of strategies? What are some of those potential applications as people, especially the builders who are watching today, might want to think?
00:32:12.410 - 00:32:46.514, Speaker B: I mean, I'll take that. And the answer is really all of that. We've given you a handful of classical strategies, right? So balancing your portfolio using traditional, what I would call classic methods like Markowbitz portfolio balancing. But you can think about using these signals in many different ways. You can take three best models and create your own trading signal, and then have a smart contract that says, I'm going to execute an automated trading or automated portfolio management solution. If certain conditions are met. Perhaps you want to incorporate things like volatility and rumors also into your model.
00:32:46.514 - 00:33:23.978, Speaker B: But the simplest way to think about this, these signals, you have a family of models, and Sean showed twelve. But depending on any given history, we may have several dozen models. You look at the history of all the signals from all the models, you can simulate a hypothetical trading strategy using those models, and you incorporate that into a smart contract. You maybe want to rebalance your positions based on a smart contract. And right now we are only showing bitcoin. But I know eventually we will have signals for a lot of other leading cryptos as well. So you may want to balance your portfolio.
00:33:23.978 - 00:33:32.000, Speaker B: You can use these signals eventually to balance your portfolio across multiple crypto positions, possibly even other solutions as well.
00:33:34.870 - 00:34:09.942, Speaker C: Let me expand on that, because Rajiv just hit on a really important thing. So the question about liquidity was asked earlier. Okay. One of the major value propositions of, say, like a synthetics or mirror protocol is that they can add liquidity to the market. So as far as I understand it, these are blockchain based smart contracts that execute trades on the blockchain, but then are also executing a corresponding trade in the real world. Everybody's familiar with that. So, like, synthetics is one example.
00:34:09.942 - 00:35:12.106, Speaker C: Right now, that's on the execution side, on the data upload side. If we're talking about the blockchain, we've got chainlink. So Chainlink is doing the heavy lifting to get off chain data on chain. Now, ultimately, where we would want to take this is we have a machine learning engine in the middle of these two things. So you've got chainlink on one end, bringing the data in, and then this machine learning engine that sits in the middle, and then the trading execution happening via synthetics to address those liquidity issues. And Rajiv just mentioned that we have multiple target variables. Well, right now, if you were to take our bitcoin models and our ethereum models, as well as, let's say, our s and P 500, crude gold, gold, you name it, across various different industry sectors, the trading integrations that we would be required in order to do the actual execution, there would be multiple endpoints because we'd have to go somewhere.
00:35:12.106 - 00:35:40.534, Speaker C: For the S and P 500, we have to go somewhere else. For bitcoin, we have to go somewhere else. So what synthetics would allow us to do is have one unified endpoint that translates all of that into the various exchanges where those contracts would actually need to be executed. So in our minds, this is like the ultimate sort of possibility of where this could go. So you address the liquidity issues, and then on top of that, you address the endpoint execution issues.
00:35:40.652 - 00:36:06.034, Speaker A: So, to clarify, synthetix doesn't execute a trade in the real world. It does it on the trading platform, but it does it with an asset that is synthetically bonded to the price of gold or whatever it happens to be. So it's an equivalent match there. There's another question. We're getting up on time, but this is kind of. I like to think about these things. Nugget asks about.
00:36:06.034 - 00:36:33.514, Speaker A: Crypto doesn't have valuation models like earnings or things like that. And so price is really kind of that end goal. Is there an opportunity? This is, like, far in the future. I know you guys are busy. I'm not going to get it on your roadmap products. Guys are going to stab me. But is there an opportunity to look at other things, such as hash rate? If you can predict hash rate better than average, then you can move your miners where they want to go, or fees, like if you can tell which network is going to be gaining fees or.
00:36:33.514 - 00:36:53.280, Speaker A: Somebody asked Sasquatch asked about liquidity. If there's going to be a liquidity crunch somewhere, then that's an opportunity for an LP to get in early and make sure there's an opportunity there. So it seems like there's all these potential other areas where prediction with a lot of the data that you guys already have, could be really valuable in a smart contract use case.
00:36:54.470 - 00:37:20.950, Speaker B: Yeah, absolutely. And I think all of those use cases are, doable because we do have a lot of that data available in our platform. And then, like you said, we are just starting off with the leading cryptos because that's what the market is demanding. But absolutely predicting hash rates, predicting. There are other ways to use these signals. All of the use cases above would be potential target use cases for us.
00:37:21.100 - 00:37:37.840, Speaker A: So if you're a developer and you're looking for that sort of stuff, if you want to be the market and demand it from QDT, where can they get a hold of you? This has been a fantastic conversation. We're going to wrap up. We're on time, but where can people reach out to you if they want to learn more, if they want to join a community or anything like that?
00:37:38.290 - 00:38:01.670, Speaker B: Yeah, so I would ask everyone to follow us on LinkedIn and Twitter and ask any question you want, please post it there and then we'll try to answer it. If you have any requests like the ones you just mentioned, if you want models for hash rates or predictions for those, there's a use case for it. We will do it. So I think that's what we hoping you guys can help us refine this product as we go along. You want to keep adding to this data feed?
00:38:02.250 - 00:38:30.526, Speaker C: Yes. And if you want to contact us directly, you can get us at info at qdt AI. And the website is ww dot qdt AI. So if you have questions or thoughts or comments related to some of those other project ideas like hash rate fees, liquidity, crunch oriented stuff, yeah, feel free to message us at info at qdt AI. Or if you have questions related to the Chainlink API as well, you can reach us there too.
00:38:30.708 - 00:38:47.074, Speaker A: Awesome. Thank you so much guys for coming on and talking through this. This was invigorating and our audience, the comments are echoing that as well. I'm sorry if I didn't get to all your questions. We ran up on time. Everyone, thank you so much for your questions. Sean and Rajiv, I'm going to take it from here.
00:38:47.074 - 00:38:48.454, Speaker A: It's nice to talk to you guys.
00:38:48.572 - 00:38:51.800, Speaker C: Yeah, thank you. Thank you for having us. Take care.
00:38:52.410 - 00:39:23.694, Speaker A: All right, thank you so much for joining us. What a great session of Chainlink Live. This has been about QDT's data feeds. Again, if you want to know more, all of their links are down below in the description, so click there. I'll make sure to have a link to their recent blog post as well. While you're down there clicking on stuff like and subscribe, please like this video, it helps share it with more people, especially developers who want to figure out a way to use Chainlink and use data feeds like QDT in their smart contracts. And subscribe to get updates when we do more of these videos.
00:39:23.694 - 00:39:55.030, Speaker A: Lots of stimulating discussions with all kinds of projects, data feeds, projects, nfts, defi projects, all sorts of stuff. Please stick around and hang out. You can follow Chainlink and smart underscore contract on twitter for notifications about everything going on Chainlink and me if you want to. I'm at Andy boy and you can follow me if you feel like it for the occasional dad joke and frequent good morning Gm tweet, you know, because GM, I think that's all I got, guys. I got more of these next week, so come back and hang out with me. We'll do some more of these. And of course, there's hackathon stuff going on like crazy.
00:39:55.030 - 00:39:56.546, Speaker A: There's still time to join the hackathon.
00:39:56.578 - 00:39:56.966, Speaker B: How about this?
00:39:56.988 - 00:40:17.100, Speaker A: Here's a hackathon project. Sign up for the hackathon, integrate QD, bitcoin, price feed, and create a model that does something, I don't know, does something with Defi. You can do that right now. Go see if you could win a prize or whatever. Just have fun building it, whatever it happens to be. All right, everyone, thank you again for your great comments and for tuning in. I'm Andy boy, and I'll catch you next time on Chainlink Live.
