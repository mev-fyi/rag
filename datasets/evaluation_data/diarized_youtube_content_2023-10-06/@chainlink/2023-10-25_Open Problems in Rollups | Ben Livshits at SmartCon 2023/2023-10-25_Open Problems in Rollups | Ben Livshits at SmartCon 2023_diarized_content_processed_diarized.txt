00:00:04.490 - 00:00:27.574, Speaker A: So great to be here. Thanks. So this is an infrastructure talk. I'll talk about roll ups, just a couple of words about myself before we get started. So I run the research team at Metalabs ZK sync. I'm also a professor at Imperial College London, teaching computer science. So for the past couple of decades, I've been working on the intersection of programming languages and security.
00:00:27.574 - 00:01:44.990, Speaker A: And so lately, the last four or five years, it's been blockchain related topics, things related to defi fee mechanisms, effectiveness of audits, analysis tools, things of that sort. Okay, so for this talk, the TLDR is fairly simple. So I'll talk about some of the latest developments in the rollup space, and I think there's been quite a lot, but at the same time, it feels like we still have a number of open questions, things we don't really have much consensus about when it comes to roll up design, and I'll talk about some of that in the next 15 minutes or so. So a few things to get us started. First of all, especially given the market phase that we're in, there's been quite a lot of consolidation around Ethereum in the blockchain space. And despite some of the sort of disadvantages of Ethereum, despite some of the concerns around things like high fees and things of that sort, the roadmap of Ethereum has resonated with a number of people. And so as a result of that, people have gravitated towards roll up as the execution layer, de facto execution layer when it comes to Ethereum.
00:01:44.990 - 00:02:22.170, Speaker A: And some of the reasons sort of behind that are things like open standards, community participation, sort of good positive forces of that sort that drive people towards Ethereum. So that's one thing. The other thing is that EVM compatibility has become a much stronger sort of de facto standard as well. And that's not necessarily because EVM is sort of the greatest vm in the world. Hardly. That is the case. There are a number of well known shortcomings to the EVM, but at the same time, there is a number of growing number of developers who know how to write solidity code, wiper code, things of that sort.
00:02:22.170 - 00:03:13.100, Speaker A: And so the plan that Ethereum has put forward when it comes to scaling itself up as a chain which involves roll up, is now seemingly actually taking hold. So when it comes to roll ups, you probably have seen some of these things before. But people typically distinguish between two types of roll ups, generally speaking. So optimistic roll ups and some examples of that are projects like optimism and arbitrum, as well as zero knowledge Zksync, which is us, polygon, Scroll, Stargare, as well as a few others. So in the first case people rely on fraud proofs, in the second case people rely on zero knowledge or validity proof based roll ups. And we are in the second camp. But I'll try to give you a relatively unbiased kind of summary of things, if you will.
00:03:13.100 - 00:04:03.862, Speaker A: So that's one point then, I think, is that there is also a degree of modularity that's also settling in in this space, which is to say in addition to the execution layer itself, there are various other bits and pieces that are emerging. So people are proposing approaches to shared sequencing as part of the roll up landscape. People are also approaching rather proposing approaches to sort of shared data availability solutions and some examples of data projects like Eigenda for instance, as well as Celeste. And maybe you'll see some examples of that in later slides as well. So the ecosystem is growing, in other words. And this is also supported by some of the numbers. So there's the website, there's this website called l to beat.
00:04:03.862 - 00:05:01.574, Speaker A: And then if you go and look at the numbers over the last year or so, give or take, you'll see that there is quite a lot of growth by at least a couple of metrics. And the first one is basically transaction volume. This is the upper portion of the slide that's been growing quite steadily compared to main net traffic. This is an aggregation of all the roll ups that have been deployed over the last year, and their number has been growing of course, as well. So consequently we see a lot more traffic. And this factor over here, the scaling factor five x or so, is like how much more is actually getting now settled on roll ups? How many more transactions are getting settled on roll ups compared to mainet? And chances are that number will only keep growing, right? So that means that the demand for roll ups will keep increasing and so on and so forth. And the second metric at the bottom of the slide is the TVL.
00:05:01.574 - 00:06:00.714, Speaker A: And this is arguably maybe not the best metric and one that not everybody necessarily relies on. But I think it's still useful as a sort of baseline, as something to compare to, because again, over the last year or so, you see quite a lot of growth in the roll up space, essentially users coming into this and the interest is growing and increasing and so on and so forth. So for the rest of this presentation, I was hoping to hit quite a number of topics. So I'll go pretty fast and try to at least mention pretty much all of these points that I have over here. So it's a bit of a list and there is a bit of a focus as well on the zero knowledge side of the roll up sort of space as well. So let's start with decentralization, right? So that's one of the reasons we are in this space in the first place. And at the same time, if you look at the de facto deployment of current roll ups, they're mostly centralized.
00:06:00.714 - 00:07:27.974, Speaker A: There are a couple of small exceptions, but for the most part, while we are on the path of progressive decentralization, at the moment, we're still relatively centralized. Of course, the question is, well, how do we progressively decentralize while retaining most of the benefits, and the benefits are throughput and latency that the centralized solution generally will give us? Maybe not the highest level of availability, but at the same time, how do we move towards decentralization without losing sort of the upside that we actually wanted to preserve? And so you can also ask somewhat more philosophical questions like, well, how much decentralization is actually good enough? Like what's the right number? Or how do we measure this? Right? So one simplistic way to look at it is to say, let's look at Ethereum itself, right? The main net. So if you look at the number of validators, the number of validators on Ethereum at this point is actually quite staggering. It's gotten to something like 900k nodes or so. Well, is that necessary? Is that good enough? Right. Like what is the right number? So is 100 nodes, which is typical. For example, if you deploy a tendermint chain, tendermint based chain, is that also decentralized? Is it not enough to have ten nodes? Is it actually necessary to try and climb to these kinds of staggering numbers? And I think some of these answers are somewhat elusive.
00:07:27.974 - 00:08:36.882, Speaker A: By the way, there are some academic papers sort of focusing on these subjects, and if you are interested in that, just kind of go and follow up on what's at the bottom of this slide. So here's another way to look at this. There is a project called Project Sunshine that looks at Ethereum's decentralization and measures it across a range of different dimensions, tries to be a lot more comprehensive. Just know the sheer number of nodes and you can look at things like the consensus client diversity, execution client diversity, things like geolocation, geodecentralization, things of that sort. And so this is Ethereum itself. And so even Ethereum itself is not actually all that geodistributed, perhaps, as these people would like you, I mean, would like it to be, let's say, right? So the question is, how do we actually get an increase on these metrics without necessarily having that high number of nodes, which almost necessarily means that we, for example, reduce our latency or increase our latency. And this is just the decentralization point.
00:08:36.882 - 00:09:50.314, Speaker A: If we sort of look at decentralization not only from the standpoint of censorship resistance, but also like high availability, is there a chance to actually create roll ups that give us the kind of reliability numbers or high availability numbers that are common in web two systems? Right? So people talk about five nines and systems that are built on top of the cloud, for instance. So is that actually in the cards, is that viable? And if so, is that viable if you allow for permissionless participation, right? So decentralized does not necessarily mean permissionless in all cases, and so does it have to be permissionless? And then there is another paper that talks about some of these decentralization measurements at the bottom of the slide as well, sequencers. So let's switch over to that. So people have spent quite a lot of time working on sequencer designs, right? Essentially ordering transactions that come into the roll up. And there are interesting set of interactions between sequencers and minor extractable values, MeV. And people focus both on preventing the MEV as well as embracing the MEV. And I probably don't have too much time to talk about that.
00:09:50.314 - 00:10:54.380, Speaker A: But at the same time, from a practical standpoint, we haven't really seen all that much deployed out of the academic or semi academic proposals that have emerged. So, so far, it seems like the default is to mostly embrace the MEV that's there on main net and sort of go with the program, so to speak, at least for the L2. And maybe the situation will be a little bit different when it comes to hyperchains or layer three, as they're called. There are, of course, approaches that are there on baseline ethereums related to what's called PBS or epbs that attempt to decentralize things further. There are the approaches that relate to sort of making the operations of relays like flashbots more transparent by adding support for things like SGX and so on and so forth. And the question is, well, should we basically do the same on the roll up or should we try to invent something else? Yeah, and so these are some of the questions I think, already posed. So let me just continue.
00:10:54.380 - 00:11:52.160, Speaker A: So the other thing that's different in our system, at least when it comes to zero knowledge, is that in addition to sequences, we also have provers, right? And that's where actually a lot of the heavy lifting goes. The majority of the heavy lifting goes into the proverb. And so there's a bunch of novel designs that people are looking at that look at increasing the speed of hardware, rather proof production, reducing hardware requirements, coming up with more interesting novel hardware, increasing the parallelism. So there's a number of proposal as well as experimental design, as well as people designing fancy hardware. We recently released a new prover system, a new proof system called Bootjoom for Zksync. And the goal here is to allow for proof production on much smaller machines such as laptops. And we hope that this will take hold and allow people to experiment with this a lot more aggressively than they have been able to thus far.
00:11:52.160 - 00:13:35.250, Speaker A: But I think there is a long term question in terms of like, well, we have so much progress on the prover side, so many people working on new proof systems. At the same time, a number of companies, such as the ones shown on the right hand side of this slide, trying to produce hardware and what is the right software, hardware code design, what is the right sort of mix and what kind of long term outcomes can we hope will take place? And why, if we are starting to rely on custom design hardware, is it not going to be a winner take all kind of outcome the way we've seen it, and some other domains that rely on asics, some of that we've seen in bitcoin mining as well. So in other words, how do we basically make the proven network decentralized? And there have been some number of designs that focus on that so far. This is still very early on and it's really quite hard to tell what's going to be sort of the default, if anything. Yeah, so I mentioned this before. So basically, how do we avoid a winner take all possibility? How do we basically have flexible pricing? How do we make sure that proven markets remain reasonably decentralized? And what's the right notion of fairness? These are some of the open questions, and then when it comes to tokenomics as well. So we've seen a number of designs that have emerged in the last three to four years which basically connect a POS based system to stake based governance and stake based protocol participation, which is all good and well, but I think that's not necessarily the best design for all roll ups, especially zero knowledge based roll ups.
00:13:35.250 - 00:14:11.926, Speaker A: When it comes to governance. I think this leaves a bunch of open questions. We haven't really seen that many kind of successful governance schemes that rely on like that provide rather l one governance or l two governance just yet. So we have to worry about governance takeover attacks. And so in a sense, I think the simple designs that we've seen proposed, I'm not sure that that's going to necessarily stand the test of time. So how do you create aligned incentives? Right. So what are the outcomes we want? So one of the things we want is speed, which means low latency, high throughput.
00:14:11.926 - 00:15:29.750, Speaker A: We want to provide some measure of censorship resistance, maybe induction in MeV, as I kind of mentioned before, high network availability. And so how do we basically provide these kinds of quality with proper token designs? Right. How do we provide aligned incentives here? Also, we are in the race to the bottom when it comes to fees. How do we make sure that there is actually innovation happening and people who provide provers, both software as well as hardware, are actually benefiting from this kind of thing, while at the same time maintaining low latency and relatively low cost. And the other thing is that this is sort of a comment when it comes to designing complex distributed systems. I mean, zero knowledge roll ups, people say it's just math, but the reality is that it's a lot more complex than that because you have ultimately complex systems that are being, that are sort of in daily operation, right? And so you have to still worry about all the usual concerns, availability, implementation bugs, recovery, infrastructure vulnerabilities and so on and so forth. And so it helps to focus on things like reducing the toasted computing base, for example, when it comes to building systems like that, and not just looking at them as just math effectively.
00:15:29.750 - 00:16:20.550, Speaker A: So specifically, people have focused on some bugs that have emerged in the last, even six months to a year when it comes to zero knowledge proof production. So there are several projects that focus on, for example, finding missing constraints in zero knowledge proofs. And there are some tools that people have put together in the last several months alone. There are specific projects that rely on zero knowledge, like for example, Pendura, as well as a number of zero knowledge ZKe evms that have gone through comprehensive audits. And some interesting bugs have been identified as well. So denied is still young, in other words. I mean, we don't quite know as a community how to build the system at the level of robustness that we ultimately require.
00:16:20.550 - 00:17:01.058, Speaker A: Yeah. So when it comes to complex circuit generation, like how do we get things right? That's sort of the question. And then last but not least is the question of privacy. By and large, modern roll ups are considered to be sort of an acceleration facility, if you will. Right? So basically something that does execution on the site off chain without too much regard for privacy, although there are some exceptions. There is the adstack roll up that focuses on privacy first and foremost. But then there are also sort of a number of other solutions and designs, and some of them apply to the layer one.
00:17:01.058 - 00:18:31.818, Speaker A: So they run on main net projects like for example, a railgun, as well as Cape from espresso systems. And so the question is, well, what do we do about that? So do we basically take those projects and just superimpose them on the L2 and leave it as is, or do we do something special at the level of the roll up itself? And I think when it comes to the roll up itself, especially if proof production becomes a bit more affordable, then I think there is quite a lot of room for interesting client side computation, and we are just kind of on the precipice of some of that. We haven't quite explored these possibilities just yet, and if the fees remain relatively low, then this combination of client side computation combined with low fees is actually a very interesting mixture. So, just to summarize, I've touched on a bunch of topics related to roll up design, and as I mentioned in the beginning of this talk, we know how to do certain things pretty well. We've deployed a number of roll ups, so ZK sync error has been operational for a number of months now with a growing transaction volume, we're very happy to say. But at the same time, when it comes to designs, there is still quite a lot of soul searching, and not everybody has committed to specific sort of architectures just yet. I think we are on the precipice of some of that, and certainly in the next year we'll see a lot more companies and a lot more projects committing to sort of specific designs and specific outcomes.
00:18:31.818 - 00:18:37.500, Speaker A: Thanks very much. Close.
