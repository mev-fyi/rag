00:00:10.210 - 00:00:36.480, Speaker A: Welcome to Chainlink Research Reports, a new series featuring research across the web, three industry and academic fields with scholarship that expands our understanding of decentralized technologies. Today we have the pleasure of learning from Dr. Julia Fonti from Carnegie Mellon University presenting her research type titled Squirrel automating attack discovery on blockchain incentive mechanisms with deep reinforcement learning. Here is Julia. Here you go.
00:00:37.330 - 00:00:52.434, Speaker B: Hey. Hi, everyone. Hi. Thank you so much for the introduction. Thanks for joining. It's a pleasure to be here. So today I'm going to talk to you about some work on using deep reinforcement learning to identify attacks on blockchain incentive mechanisms.
00:00:52.434 - 00:01:47.480, Speaker B: And this was joint work with a number of fantastic collaborators who are pictured here, some of whom are partnering with Chainlink as well. So most blockchains, or at least most public blockchains, use incentive mechanisms, typically to incentivize participation, good behavior and so forth. These incentives are what convince people to actually participate in these systems. However, these incentive mechanisms can be susceptible to attacks. So one of the best known is called selfish mining. So it was proposed years ago, but has spawned a lot of follow up work in the academic community and a lot of discussion in the digital sphere about the implications of it. So this includes both follow ups on selfish mining and other types of incentive attacks like front running, which we've seen in the last couple of years.
00:01:47.480 - 00:03:14.690, Speaker B: So what do we do about these incentive attacks? Well, today there's a cottage industry, both of new protocols that are being proposed and new incentive mechanisms that people design to try to incentivize participation and good behavior. And at the same time, when people propose these new protocols, often it comes with some amount of theoretical analysis. In the ideal case, that guarantees the security or the incentive compatibility of these mechanisms. Now, the challenge is that this analysis is typically difficult and is done in a very restrained setting, if at all. So, for example, it might assume that you have two parties, one of which is honest and the other is adversarial. So it makes a lot of simplifying assumptions that may not actually hold in practice. So the goal of this talk was to try to understand, can we come up with a more general way of identifying attacks on incentive mechanisms so something that isn't specific to each individual protocol? Because some protocol designers may not have the ability or background to do this kind of incentive analysis theoretically, and even when we can do it theoretically, often it's difficult to do it for more realistic settings.
00:03:14.690 - 00:04:11.646, Speaker B: So our goal with this talk was, or with this work was to develop a framework for using deep reinforcement learning to analyze in an automated way incentive mechanisms that are proposed in blockchain mechanisms. So in this talk, I'm going to talk about some of our results in this space. So we're going to start with a sanity check that shows that reinforcement learning can recover known optimal results for a simple case for bitcoin incentives. Then we're going to talk about four different case studies. So one is we show that we can beat state of the art selfish mining strategies when the hash power is stochastic rather than fixed. A second is that we can identify a problem with the so called rushing adversary model in multi agent games. So these first two are actually, the second one in particular is kind of subtle.
00:04:11.646 - 00:05:00.814, Speaker B: So I won't spend time on this during the talk, but we can chat about it afterwards if you're interested. The third is that we observe no benefit to selfish mining with three or more competing parties. And this might help to explain why selfish mining hasn't actually been observed in practice, even though it's received a ton of attention from the academic community. And fourth, we identify a new attack on the Casper FFG protocol, which was proposed several years ago when miners choose to collude with voters. So in this talk, I'm going to focus on case studies three and four, just for the sake of time. But these are the four kind of main results that we talk about in the paper which appeared at ndss. So just a quick background.
00:05:00.814 - 00:05:45.170, Speaker B: I think this is probably most of you on this webinar have seen this before, but just for completeness. So we're all on the same page. In bitcoin, we have an incentive mechanism that works as follows. I'm going to focus mainly on block rewards here. So we'll think of our set of miners here as the blue node and the red node. And miners are incentivized to participate in bitcoin by being offered tokens in exchange for producing blocks. So if the blue node produces the next block, the blue node will get some amount of reward, and if the red node produces the block after that they get some reward and so forth.
00:05:45.170 - 00:06:48.710, Speaker B: And of course, because of bitcoin's consensus mechanism, because of the longest chain mechanism, if the blue node now produces another block that forks off of the main chain like this, because it's not the longest chain, this second blue block does not get rewarded. And as another reminder, selfish mining is an attack on this incentive mechanism that works as follows. So let's assume that this red node here is malicious. What the red node is going to do is to allow the blue node to accumulate some rewards and build up some blocks. And when the red node manages to produce a block, he's going to keep it to himself. He's not going to publish it yet, so he's going to build up his own private chain of red blocks while the blue node continues working and thinks that he's accumulating rewards. Now, at some point, the attacker is going to decide to release all of its red blocks.
00:06:48.710 - 00:07:37.254, Speaker B: And because of the longest chain rule, now this second blue block is no longer valid. So now the longest chain looks like this circled piece. So the blue node actually ends up losing some of the reward that it thought it had accumulated. And the idea here with selfish mining is that by doing this kind of a strategy, the red node can actually accumulate a higher fraction of total rewards than it would get if it had just been behaving honestly. So this paper came out in 2014. It spawned a bunch of follow up work, as I mentioned, and was viewed as a very important paper in the space. So, currently, the state of the art for analyzing this type of attack is to model the system as what's called a Markov decision process.
00:07:37.254 - 00:08:30.822, Speaker B: So basically, this is a randomized model that models an agent trying to maximize their own rewards in a randomized environment. And these mdps can be solved exactly with solvers like policy iteration or value iteration. And in fact, that's what was done in a 2016 paper called optimal selfish mining strategies in bitcoin. So basically, they found the best possible selfish mining strategy, which is slightly different from the one that was proposed in the original selfish mining paper. Okay, but what's the problem here? Well, there's two problems. So the core of the problem is that in order to use these techniques, we need to represent our agent's policy as a matrix. And the dimension of this matrix grows very fast in the state space and the action space.
00:08:30.822 - 00:09:19.590, Speaker B: So here, the state space, intuitively, is like some representation of the current state of the blockchain. So that's growing. And the action space is the set of actions that the adversary can take, like choosing where on the blockchain to build and when to release their blocks. So there's two main problems here. One is that this approach is not good at handling large or continuous state spaces, and the other is that it can't handle non stationary settings where you have multiple competing agents. So basically, this is only good for modeling a single adversary competing against some number of honest parties. All right, so this is where deep reinforcement learning can be really useful.
00:09:19.590 - 00:10:46.290, Speaker B: It doesn't have these strict requirements, so we can actually do things like model much larger state spaces and or model multiple agents that are competing against one another. So this is the technique that was used to train, like alphago, for example, if you've heard of that. So what's the idea? The idea is that now our agent is taking as input the current state of the system, and it is outputting a policy based on the input state that is governed by a deep neural network. And the parameters of these deep neural networks are being learned over time. So given an input space, the agent chooses some action to take, which is governed by this neural network, inputs the action to the environment, and the environment outputs some reward, which is sent back to the agent, and the agent uses this reward feedback along with the current state to take its next step. And it's learning these parameters for its policy network over time. So the main idea here is that now, instead of having to represent the relevant data as this gigantic matrix like before, we can represent it much more compactly through this deep neural network.
00:10:46.290 - 00:11:34.754, Speaker B: So that's good, because it reduces the dimensionality of the problem. And at the same time, we don't require a full specification of state space transitions in order to be able to act. Okay, so, very briefly, the idea here behind squirrel is that we have all of the traditional components of deep reinforcement learning. So we have a number of agents up to k agents, and each agent is initialized with some parameters, like the amount of hash power they have or the variance of that hash power. The agent also has some reinforcement learning algorithms. So there are different options that we can choose here. And we also need to model the environment here.
00:11:34.754 - 00:12:27.058, Speaker B: The environment is governed by our blockchain protocol, which consists of the combination of our consensus mechanism. So in the case of bitcoin, that's Nakamoto consensus, plus the incentive mechanism, which in the case of bitcoin is the block reward scheme that we mentioned earlier. So the main innovation in this work is really not on the reinforcement learning side. We weren't designing new reinforcement learning algorithms. So I won't dive too much into this, but this is the general setup. And the idea is that now, once you have all of these pieces instantiated, you can simulate games or you can simulate the evolution of the blockchain in order to help these agents learn how to act in order to maximize their incentive rewards. So, I'll quickly go through some of the results that I mentioned.
00:12:27.058 - 00:13:06.586, Speaker B: In particular, I want to start with a sanity check, which is bitcoin selfish mining. So here we wanted to compare what this RL agent can do compared to the optimal strategy that was derived from that MDP algorithm that I mentioned earlier. So here we're going to compare a few different baselines. So the first is honest mining, where everyone follows protocol. The second is the first selfish mining paper. The first one that I mentioned, this is the first time selfish mining had been proposed. And the second baseline I'm going to call OSM, which stands for optimal selfish mining.
00:13:06.586 - 00:13:54.858, Speaker B: And this is the version of selfish mining that was solved for using the Markov decision processes that I mentioned earlier. So this plot shows you, on the horizontal axis, we're looking at a two player setting where one player is adversarial and the other is honest. So here on the horizontal axis, we see the fraction of the attacker's hash power. So this ranges between zero and a half. On the vertical axis, we have the relative reward of the attacker. So what fraction of the total block rewards does our attacker get? And if our attacker were behaving honestly, then they should get the same fraction of rewards as their hash power. So, like, if we look at the curve with the red stars, this is the honest baseline.
00:13:54.858 - 00:15:08.760, Speaker B: So this is basically you get as much reward as your hash power. And what we can see is that by using these selfish mining strategies, the attacker can actually get much more, a much higher fraction of the relative reward than they're supposed to get. And in fact, the orange triangles, the triangles here represent the reward obtained by the optimal selfish mining strategy. And we see that squirrel, our reinforcement learning framework, gets within statistical error of the optimal selfish mining strategy. So this is just a sanity check that in the easiest case, we're able to recover known optimal results. Now, I'm going to talk about very quickly about two other results that we got on cases where we don't know the optimal solution. First, one is asking, is selfish mining actually profitable when you have multiple strategic agents that are playing against each other? And notice that this cannot be solved with mdps, with Markov decision processes, because it's a non stationary environment, because you have multiple agents that are competing against one another.
00:15:08.760 - 00:16:17.706, Speaker B: So here I'm showing you the attacker's hash power on the horizontal axis and the relative reward minus the attacker's hash power on the vertical axis. So here, the honest strategy gets you zero across the board. And if any of these curves goes above zero, it means that they're doing better than the honest strategy. And what we notice is that as you move from one to two to three strategic agents, the benefits of selfish mining are actually decreasing to the point where with three strategic agents, you don't see any benefit at all. Now, this doesn't mean anything conclusively, because with reinforcement learning, one of the downsides is that you don't get any theoretical guarantees. These are all computational things, empirical observations. But if we look, we looked more closely at what was going on here, and here I'm showing you the relative reward of each of the three agents over time.
00:16:17.706 - 00:17:16.238, Speaker B: And I want you to focus here on iteration 100. And here we see that our third agent around here deviates from the honest protocol and almost immediately their profit decreases. Whereas agent, the second agent here, their profit increases. So they're able to capitalize on the fact that this second agent deviated from the honest protocol, which suggests that honest mining may actually be a Nash equilibrium for three or more strategic agents. This is kind of interesting because selfish mining has not been observed in practice in the wild. And so this may help to explain, in part, why that's the case, because it seems like it doesn't actually help when you have multiple parties playing against one another. Okay, you know what? For the sake of time, I was supposed to keep this to 15 minutes, so I'm not going to talk about this second case study.
00:17:16.238 - 00:18:57.600, Speaker B: But the high level idea is that we looked at a finalization protocol that was proposed for ethereum, initially called Casper the friendly finality gadget. And we found that even though this finality protocol was initially shown to have honest behavior as a Nash equilibrium, we found that if you combine the adversary's action space with selfish mining and the choice of where to vote or what blocks to finalize, you can actually do much better than you're supposed to be able to do. So, in short, we get a plot that looks like this, where your relative reward fraction, by using reinforcement learning, is this curve with the red pluses, and it's able to achieve much higher rewards than you should be getting with honest behavior. Which suggests that when we think about these kinds of incentive mechanisms, we need to think not only about the mechanism in isolation, but also how it combines and how it composes with other incentive mechanisms in the system. So the take home message here is that deep reinforcement learning can be a useful tool for analyzing complex incentive mechanisms, especially when they have a large or continuous state space and or a non stationary environment. And more generally, I think we need to think about the composability of different incentive mechanisms. So analyzing one in isolation may not be enough.
00:18:57.600 - 00:19:06.740, Speaker B: So, thanks so much for your time. Here's a link to the GitHub repository for some of these experiments, and I'd be happy to take questions.
00:19:11.110 - 00:19:50.238, Speaker C: All right, thank you so much, Julia. That was a really fascinating presentation on some fascinating work, which bridges two kind of incredible technologies, both blockchain technology and reinforcement learning, both at the, especially reinforcement learning, being at the cutting edge of kind of machine learning and deep learning. I just have a couple of clarifying questions for you at first, and then some broader questions, if you wouldn't mind answering them. So, can you perhaps explain, first of all, for the audience members that aren't that familiar with what equilibria are?
00:19:50.404 - 00:21:02.630, Speaker B: So the notion of an ash equilibrium is relevant when we're thinking about different parties that are playing some kind of strategic game where everyone's trying to gain an advantage. So this is a very important concept in game theory. And the idea of a Nash equilibrium says that if everybody adopts a certain policy, then no single party gains an advantage by deviating from that policy. So in this case, what we would like the ideal situation is that, for example, for bitcoin, ideally, it would be the case that everyone's best strategy is to play the honest protocol rules, meaning they mine exactly as the protocol specifies. However, what was interesting about the original selfish mining paper is that it showed that's not the case, that actually an attacker can do better than they would by honest mining if they adopt this selfish mining strategy. So that showed that honest behavior among two parties in the bitcoin incentive mechanism is not an ash equilibrium.
00:21:03.690 - 00:21:22.394, Speaker C: Before your approach, could you perhaps explain to the audience a little bit about what other incentive mechanism analyses have been done in the past? So, in other words, how were incentive mechanisms assessed before the approach that you proposed here with reinforcement learning? That would be really helpful.
00:21:22.522 - 00:22:08.910, Speaker B: Sure. Yeah. So this is kind of what I was mentioning about the Markov decision processes. So there's a few different approaches that people take. One is to just stare at the protocol, think really hard, and come up with an attack, and then you can simulate, if you have an attack in mind, you can simulate it and show that it does better than honest behavior. So that's the kind of thing that people have done for more complex protocols that are difficult to analyze. The slightly more sophisticated approach is to use these Markov decision processes that I mentioned, which are just a probabilistic model of how these blockchain systems evolve.
00:22:08.910 - 00:22:46.426, Speaker B: And under these probabilistic models, we can actually solve exactly for the adversary's best strategy for playing this game. And so that had been the main approach for studying a lot of these incentive mechanisms. For example, people have tried applying it to Ethereum's state space. But the challenge is that these tools, these prior tools, weren't really scalable. So once you start looking at more complicated protocols like Ethereum's, which has uncle blocks and so forth, it gets harder and harder to actually solve for the.
00:22:46.448 - 00:23:48.880, Speaker C: Optimal strategy as we kind of move forward into the future. There's a lot of talk about digital currencies becoming more, let's say, mainstream. So China is introducing a digital yuan. There's talk amongst people in the Federal Reserve about introducing a digital dollar. And eventually there might be a central bank digital currency or CBDC. That will be something that many people will tend to. So I was wondering, outside of the kind of permissionless blockchain space that you're analyzing, do you see an application for this research in a more permissioned blockchain space, such as central bank digital currencies? And if so, what would you say that that would be?
00:23:49.410 - 00:24:43.680, Speaker B: Yeah, that's a great question. The kinds of attacks that we were focusing on in this paper I think are probably more applicable to permissionless currencies. However, I do think that incentive mechanisms will play an important role even in permissioned, more centralized currencies, like you're saying. But I think they'll probably look a little different. So, for example, I think one main difference is that there may be more of an emphasis on deterring bad behavior rather than encouraging good behavior, because in some sense, the participants in these digital currencies are already incentivized to participate. But you want to be able to deter or detect misbehavior. That becomes a lot more important, I think.
00:24:43.680 - 00:25:25.340, Speaker B: So there's been like a really interesting line of work out of the Vishwanath group at the University of Illinois on detecting misbehavior in byzantine, fault tolerant protocols. And so the idea know right now we can't guarantee that everyone's going to behave, but if someone misbehaves, we want to be able to know exactly who did it. And right now, some protocols are more amenable to that than others. And so I think that direction of trying to understand how much accountability can you bake into a protocol is a really interesting incentive related question.
00:25:26.030 - 00:25:40.880, Speaker C: I see. So something along the lines of, like, if there were systems being built to try to deter criminal behavior or something like that, you can apply this analysis tool to those mechanisms basically, right?
00:25:41.330 - 00:26:19.950, Speaker B: Potentially, yeah. I think there's kind of two main requirements that you need to be able to use deep reinforcement learning. One is that you need a well defined but usually large state space. So blockchains fit that very well. So there's only so many different configurations that a blockchain can take, but that number is really huge, but it's well structured. And the other requirement is that you need to have a well defined feedback or reward mechanism. And that's also pretty well defined.
00:26:19.950 - 00:26:44.870, Speaker B: That's very well defined in the block reward setting, because we have explicit rules about if you propose this block and it's on the longest chain, then you get this many tokens in return. So figuring out how to formulate those two components in the centralized setting, I think will require a little bit of thought and modeling, but it should be doable.
00:26:46.170 - 00:26:47.446, Speaker A: Great. Thank you.
00:26:47.628 - 00:27:25.054, Speaker C: So just one more kind of big picture question. I noticed a lot of your research is focused on kind of the intersection of machine learning and blockchain cryptocurrencies. And I was wondering, in general, what do you see will be the kind of role, or the roles that tools like deep reinforcement learning, deep learning and deep learning, because these tend to work better at scale.
00:27:25.102 - 00:27:25.266, Speaker B: Right.
00:27:25.288 - 00:27:50.060, Speaker C: For a wide variety of problems. What role do you foresee them playing in the kind of future of cryptocurrency and blockchain? Just more generally? Because I would say that from what I've seen, your research is probably some of the research that I've seen which employs these techniques the most and is some of the most original and unique in that sense.
00:27:50.750 - 00:28:47.870, Speaker B: Yeah, that's a really interesting question. So I think machine learning in general will probably have kind of two different types of roles in the digital currency space. One is going to be very similar to the way that it's used in traditional financial spheres, like predicting markets or trying to detect fraud, these kinds of things. And that's more related to the downstream applications of these systems. And so in that sense, I believe those kinds of tools will probably look very similar to the ones that are being developed today. And I'm not sure that a ton of new innovation would be required just for the cryptocurrency space. The other class of problems that I think are interesting are the ones that arise precisely because of how cryptocurrencies are implemented.
00:28:47.870 - 00:29:08.306, Speaker B: So, for example, analyzing these incentive mechanisms is a good example of that. For permissionless blockchains, you have to have some kind of incentive mechanism to make the system viable. But then that raises the question that if this incentive mechanism is poorly designed, your whole system could come crashing down.
00:29:08.408 - 00:29:09.060, Speaker A: Right.
00:29:09.990 - 00:29:32.540, Speaker B: So I think those are the kinds of questions where there may be some really interesting roles for machine learning to play, either in testing the security of existing protocols or potentially even in designing new ones. I think it's a really open space and there's a lot of interesting questions to be asked.
00:29:34.430 - 00:29:35.660, Speaker A: Great. Thank you.
00:29:36.510 - 00:29:43.310, Speaker C: Well, thank you so much for coming today, Julia. It was a real pleasure listening to you.
00:29:43.380 - 00:30:00.420, Speaker A: Jason. I actually have a couple of questions. Can I sneak in? Yeah. While I have Julia. Fantastic presentation. So just on the topic of the paper, you're talking about Nakamoto consensus, would this model work for other consensus models like the newer ones, avalanche consensus, for example?
00:30:01.110 - 00:30:31.680, Speaker B: Great question. So we haven't tried it on avalanche, but we did try it on other consensus mechanisms. So we looked at the Ethereum protocol, we looked at ghost, which is another consensus mechanism. We looked at fruit chains. So it does apply to multiple different types of consensus mechanisms. I think it would be interesting to try it out on something like avalanche. We just haven't done that yet.
00:30:32.690 - 00:31:00.070, Speaker A: Follow up question, and please correct me if I don't understand the space well enough, but what I thought is that you said for this analysis is k equals three. So basically there's three participants to do this testing. When that number scales to hundreds of dozens and thousands, is there any reason to expect change in incentive behaviors in that modeling? If it's one person against thousands or ten against thousands, is there any indicators about that?
00:31:00.220 - 00:31:32.690, Speaker B: Yeah. Great question. So typically, as the number of parties increases, it gets harder and harder to launch attacks because each party has less resource, fewer resources available to them. So we would expect to see this. If we take the trends for small k, let's say under ten, I would expect to see those trends continue as the number of participants gets large. But to really know, I guess you'd need some kind of theoretical analysis.
00:31:33.030 - 00:31:39.410, Speaker A: And then that doesn't account for collusion either. Once you get into the many more, that kind of changes the game theory there.
00:31:39.480 - 00:32:07.100, Speaker B: Oh, yeah. We actually did a couple of experiments that I didn't mention here on colluding parties and whether they can learn to collude with one another. And we were able to observe that as well. If they're completely colluding, you can treat them as one party. So that's one simplification that you can make. But I think where things get interesting is if they're sharing, like, partial information but not everything, then what can they do?
00:32:08.350 - 00:32:10.854, Speaker A: Are those analyses in this paper that you presented?
00:32:10.902 - 00:32:11.174, Speaker B: Yeah.
00:32:11.232 - 00:32:23.860, Speaker A: Okay, cool. Well, I will have the link to that paper, definitely down in the YouTube description so that anybody else can find that. Jason, thank you for your questions. Julia, thank you so much for your time. Where should people follow you to find out more about your research that's coming.
00:32:26.310 - 00:32:28.834, Speaker B: Sorry, you caught me off guard with that one.
00:32:28.952 - 00:32:47.862, Speaker A: Sorry. I have your website, your homepage down there in case people want to follow. Does that work? Okay, academics. We got to get you used to promoting your work as well. Thank you so much for coming on. Jason and Julia, I very much appreciate your time. I'm going to take it from here and tell everyone to please, like and subscribe to this channel.
00:32:47.862 - 00:33:05.050, Speaker A: We'll be doing Chainlink research reports regularly, every couple of weeks or every month or so. So please check back into this series along with all our other content that's available on the Chainlink official YouTube channel. I appreciate your time very much. Again, my name is Andy Boyan. Thank you for joining us on Chainlink Research reports.
