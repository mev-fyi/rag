00:00:05.370 - 00:00:43.510, Speaker A: Hello and welcome to the panel on Privacy and Snarks, organized by the Smart Contract Research Forum, or Scurf, as part of the Smart contract summit. I'm Eugene Leventhal, the operations lead here at Scurf. This is actually part one of the panel as the conversation was just too good to cut short. You'll be able to read more about the panelists and to watch the rest of the video for free@smartcontractresearch.org. T six, two, two. And with that, I'll pass it on to our moderator for this panel, Scurf's research lead, Lucas Newsy.
00:00:43.590 - 00:01:48.922, Speaker B: Thank you, Gene, and welcome, everyone, to what's going to be a really fun discussion on the state of the industry when it comes to ZK roll ups and the low level protocols that enable them. And to have this conversation. We have two of what I consider the top researchers in the space doing work in this area. Ariel Gabizan, who has been doing research in this area for quite some time now at aztec protocol, is building a really interesting ZK roll up solution, as well as Alex Vlasov, who has also been researching this area for quite some time and has been working with ZK sync in their various approaches to roll ups. Gentlemen, welcome to the panel. Really a pleasure to have you on board. Maybe to kick things off, you guys can give a little bit of an introduction of yourselves and your projects.
00:01:48.922 - 00:02:02.930, Speaker B: And we do have some questions to go over, but it'll be great to hear from you guys. The evolution of your research, what you've been working on in the areas that you focus on, maybe starting with you, Ariel?
00:02:05.290 - 00:03:01.126, Speaker C: Yeah, sure. Basically, I spent a lot of years in academia doing math, computer science. And once, basically, I read the bitcoin white paper, I felt that I should sort of get into this world. And the best way for me to do it seemed to be through these zero knowledge proofs, since they were relatively math heavy with the sort of math I liked and was comfortable with. So that led to me working with Eli Ben Sasson back in Technion University in Israel in the early days of Starks, before they were called Starks. And then I sort of left academia to work on Zcash. The first trusted, first sort of commercial use of starks was there.
00:03:01.126 - 00:03:19.850, Speaker C: And that was really. Yeah. And then sort of a chance meeting with Zach Williamson led to this universal ZK snark called Plonk, which sort of, I think, really put universal snarks.
00:03:21.730 - 00:03:22.046, Speaker D: Sort.
00:03:22.068 - 00:03:40.180, Speaker C: Of in the practical, made them very practical. And then that led to me moving from protocol labs, where I was working, to aztec to focus on this work with Zach and the aztec team.
00:03:42.390 - 00:03:44.930, Speaker B: Excellent. How about you, Alex?
00:03:46.310 - 00:04:43.800, Speaker D: Well, regarding the way in the snarks themselves, I was participating in ETH. Waterloo, I think was actually the first one from ETh Global series, which continued for a few years after. And at this hackathon, there was the talk, well, first, I think, by Vitalik on snarks, and then by elements of someone starx. So this is when it first caught my attention, in general, for what's possible to do with the snarks at that point in time. Then I was working on plasmas in different flavors. Then at some point, we decided to form a metal labs to bring the best from two worlds and kind of marry the roll ups and Ziggy snarks to make the Ziggy roll up released years ago. So now we are continuing to develop it to bring in the version 2.0,
00:04:43.800 - 00:05:43.482, Speaker D: and it should be able to run user defined smart contracts, which should be compilable from solidity and sync, which is our developed programming language. Also, not specifically for circuits, just normal programming. In theory, we should be able to even build rust and run it in our architecture, but it's a little bit off topic. So, yeah, along the way, I participated in improving Snarx. As for us, it's more like a tool than something like pristine and fundamental. So for this purpose, we published a small paper about the transparent polynomial commitments, which used fry, and then which led into the expanded paper called the redshift. This was roughly my way into here.
00:05:43.482 - 00:06:07.780, Speaker D: Now we're in the final stages to bring Ziki sync 2.0 with smart contracts to life with basically all snark related work done. Now we just have to check the code, run the tests, and be able to release it so people can try a compiler, see how the VM was designed. Actually, we have to write a lot of manuals, but it's on a good track.
00:06:08.790 - 00:07:54.100, Speaker B: Yeah, super interesting background, and I think it ties things very nicely. I do remember when Vitalik first described what we now call, you know, essentially applying this technology, which the industry had predominantly seen as an approach to privacy, with projects like Zcash applied to scalability. Right. Enabling transactions to be processed outside of the blockchain, and essentially borrowing some ideas even from plasma at a time when more research was coming out on the feasibility of plasma that, I would say, disappointed a lot of people. So, to set the stage a little bit, it would be interesting to talk about the commonality and the theme of the projects that you guys are working on, because when people think about Zksnarks, they still have this anchor towards privacy, whereas because of the research that you guys have produced over the years, that theme has shift quite a bit, right, from privacy to scalability. And now with recursive snarks, with the ability to program contracts using these systems and still carrying some of the benefits that snarks intrinsically have. So maybe to set the stage a little bit, Ariel, could you talk about recursive snarks and plonk? And what is it that enables these contracts to be generalized and really just granting the tools, the developers, to build these contracts using this technology?
00:07:58.070 - 00:09:41.750, Speaker C: Um, well, so I, maybe one thing to mention is like, why are recursive snarks relevant or essential to programmability? And I guess one part of that is if eventually you have one sort of verification contract that is constant, like say this is the contract that verifies the roll up. So this contract is fixed, it's not dependent on a specific program. So in this sense, recursion helps you, because whatever your sort of actual circuit is, the final program you're running is just a verifier. So sort of what, what is the relation between plonk and recursion? So the first thing to notice is that in principle, any ZK snark gives you recursion, because ZK snarks allow you to prove statements about any program. So you can always prove a statement about the snark verification program in particular. So recursion was never impossible. Theoretically, it was really a question of constants.
00:09:41.750 - 00:10:48.300, Speaker C: And the concrete constants when writing your program, your snark verification program, due to basically field mismatches, were just very large. So one reason why sort of plonk made recursion more simpler is basically shaving off a few constant factors in terms of representing constraints compared to the r one cs representation. And these sort of factors shaved off exactly, sort of made writing the recursive verification program more practical without the need for any special tricks like elliptic curve cycles. Yeah, so I'm saying a lot of things there that maybe need explaining, but that's sort of the mid to high level.
00:10:49.390 - 00:11:17.442, Speaker B: Yeah, no, absolutely. And I think there's part of the priming that I see exists when people reason about ZK rollups. I think it's exactly that. Right. I think there's a lot of priming in terms of applicability. I think there's this idea that a ZK snark is application specific. Its role is to simply verify transactions.
00:11:17.442 - 00:12:29.180, Speaker B: And the reason why I think there's been a lot of this priming is that in order for you to, under previous systems, bootstrap an application, it requires creating this string through a process that must generate a random number. Right. A random string. And what your research has shown over the years is that this is abstracted away through shaving some of those parameters so that you can apply this technology to broader programs without the requirements of actually regenerating these strings, which increase kind of the potential attack vectors of these systems. Right. So it really just enabled a way for you to reuse that SRS for plethora of applications that can then be granted the benefits of this technology. Is this a correct.
00:12:31.950 - 00:13:29.022, Speaker C: Mean? So that is another huge issue. Yeah. That universal snarks like plonk don't require a setup per circuit. Yeah, I was referring to this, I guess, other more low level issue that to actually write the snark verifier as a program, you need to sort of simulate these field operations in a non native field. And that was extremely inefficient before plunk. But actually, maybe you're right that actually you're right that a much more basic issue if you want recursion and user defined circuits. Yeah.
00:13:29.022 - 00:13:41.970, Speaker C: So the universality is actually a much larger and more basic point. Yeah. Otherwise every user would need to run their own trusted setup.
00:13:43.510 - 00:14:55.030, Speaker B: Yeah. I think there's been a lot of really interesting content. I think even the initial powers of Tao ceremony from Zcash that you were one of the architects of, I think there's a lot of mysticism around what that actually entails. Right. And how that is performed. But the basic idea is that that string must be secure enough or random enough so that you have guarantees around the provers and verifiers of these systems. Alex, you've worked on this scheme called redshift, which was a really interesting piece of research, and it basically introduced an interactive oracle proof system that used polynomial commitments to essentially circumvent the need for a trusted srs.
00:14:55.030 - 00:15:27.960, Speaker B: Could you talk a little bit about redshift? What motivated that research? Was it the security and more universal angle that motivated that research? And also, if you could also talk about the evolution of that research and what led you guys into plonk as that research evolved and maybe the trade offs associated with that research?
00:15:30.010 - 00:16:48.590, Speaker D: Well, first of all, I should mention that for redshift and its history, there was no particular decision to do such research. The historical state at this time was that there was a deep fry paper by Elie Benson and his colleagues, which basically used Fri proximity testing protocol in some particular way, which allows them to have more efficient stark. And here by Stark, I mean a proof system and the way of arithmetization, which is called air in the original paper. And basically, what this paper was doing is very close to what the Planck paper was doing in its original form, with just a polynomial commitments, which are catech commitments. Basically, the prober commits to the set of polynomials and then draws a random point. Then make a standard trick of cautioning. Basically, if your polynomial is at this value, at this point, then you can perform a division formal division operation, and that the result will also be a polynomial.
00:16:48.590 - 00:18:02.354, Speaker D: And then they used the fry to prove that this result of the division is indeed very close to polynomial, which was sufficient for all the purposes of stark as a proof system. But it was never called a polynomial commitment, even while it was doing basically the same as Katik commitment itself, just in a different way. I read this paper, and then after half a year or something like this, there was a plunk paper, which I seen was covering largely the same details in the same format. And only then I just randomly, in a train, actually had an idea, well, this is basically the same, and why no one tried to do it. And then we spent two weeks trying to convince ourselves that there was no some trivial mistake and overlook that this is very much the same idea, which was never covered. So then we made a very small paper on archive, which was basically talking about the polynomial commitments, which was strict commitments to only a single polynomial, and used fry as a proximity testing protocol in a mode which is called unique decoding radius. A little bit simpler.
00:18:02.354 - 00:18:43.362, Speaker D: It was testing that something which prover gave you is close to the polynomial. It's like very close to polynomial. Let's call it this way for simplicity, for now. And then we started to, which led to larger proof sizes, which was a problem of the original starks back then. When they were introduced, they required very small radius for testing of proximity, which led to large approach. And then we decided to go beyond this, the same achievement, which was done in the deepry paper, to go beyond unique decoding radius. And this was done in a redshift paper.
00:18:43.362 - 00:19:57.210, Speaker D: And the main trick, or problem with going beyond decoding radius, was that now the proverb is able to commit not just to a single polynomial, but some set of polynomials. The set is still small compared to the field size. So it wasn't the end of the day for someness, but it would be okay for witness polynomials. So the proverb can give you something which is like maybe one polynomial out of ten, which satisfies the verification relationship was still fine for witness polynomials, but it would be a problem for setup polynomials, which were present in all the proof systems. Aziz and Starks and others, which are like four uniform circuits. Let's not go that deep into this direction. So we had to find a way how to actually pinpoint a single polynomial out of the set at the setup time, which was done in Bratchet, which basically brought the best from the starks world, and basically transferred it to all the protocols which require polynomial commitments.
00:19:57.210 - 00:20:28.440, Speaker D: That's why it was demonstrated on plunk itself, because it was first an original inspiration. And then the idea was that you can do the same, you can run the same proofs by just swapping the commitment scheme and maybe adding some factors in the soundness. So that's why the ratchet was made in a form which basically taken the plunk and kind of changed it a little.
00:20:32.850 - 00:20:33.600, Speaker C: Actually.
00:20:35.330 - 00:20:36.240, Speaker B: Go ahead.
00:20:36.610 - 00:20:59.080, Speaker C: Yeah. So, actually, Alex, what do you think really, about the non preprocessing starks versus doing redshift? If you had to choose between these two, what would you choose?
00:21:00.490 - 00:22:22.750, Speaker D: Well, I have a preference for one with the preprocessing for a reason that it can be much simpler to write and design by people who cannot just take the polynomials and write the polynomial protocol by hand in a piece of paper, by basically linking all the witness values by themselves in a very large set of witness polynomials. The problem with protocols, which are like starks without preprocessing, is that your set of constraints is so much nontrivial that, first of all, it's difficult to design without mistakes. And the second, after you design it, it's unlikely that you will find another person who would be able to review it. But if your proof system is more like, more universal in a sense, that the preprocessing written once does everything for you. And when you design the circuit, you only write a code, which is much simpler. So you do not manually take care which variable is located in what polynomial, at which index, and you basically treat them like polynomial, like values and variables in any convoluted programming language. It eliminates a lot of mistakes and it actually becomes auditable and understandable by the wider audience.
00:22:22.750 - 00:23:19.330, Speaker D: That's why I think that there are more people which are able to write a long circuit or maybe understand, to understand sprouts or sapling zcash circuits than people who would be able to write a stark to, for example, do the Peterson commitment. And Peterson commitment was actually quite trivial for whatever people in startware did for the Cairo programming language. I can guess how it works. I didn't look in details and didn't try to reverse it, but I can guess how it works. But I think it would be nontrivial for anyone to try to try to write an external manual to how it's actually implemented. Even so, you will see a full set of constraints in the verifier if it's implemented as a solidity contract.
00:23:23.520 - 00:24:32.820, Speaker B: Fascinating. It's interesting to hear about the reasoning behind some of these trade offs, right? I think if the research around even more recently with plonk and its newer iterations and all its predecessors showcases is that there are some non trivial trade offs at play, right? When it comes to usability proving time verification, time proof sizes, even complexity, what other trade offs fueled your research as you're thinking about, for example, shifting the commitment scheme in this circumstance for efficiency purposes? What are the trade offs that influence your research, especially as you're trying to build the system for ethereum applications?
00:24:35.240 - 00:26:00.464, Speaker D: Well, I think one should actually separate two parts here. One is designing the final system in the sense that what circuits you write, which is the functions of the circuits, whether those circuits require some recursive verification, because they kind of prove different substations of one larger logical statement, and so they require recursive verification. And another trade off is purely regarding the proof systems and concrete implementations of the recursive approach. For example, if you take the fractal paper, a direct comparison was done there. So the fractal paper is also transparent however you want with the preprocessing. And what they did is they presented the size of the circuit which does a verification, a recursive verification of single statement if you use one hash or another hash to construct the mercury when you do the proofs. So if you use a standard non algebraic hash, then your verification circuit is very large, because non algebraic hashes take a lot of constraints to implement, which was partially solved by the plonk with lookup tables, we can cover it a little bit later.
00:26:00.464 - 00:26:51.744, Speaker D: And another example they did, and they used the rescue hash as a hash to construct the Merkel trees, which led to much smaller circuits to implement the verifier, but which also led to the much larger proving time because the hash itself was very slow to compute. So this would be, I would say, I think more to the question which you asked originally in the sense of different trade offs in a similar direction. You can view how many constraints does one recursive aggregation in plonk take. And it also depends a lot. For example, if your verification keys are all the same, it's one number. If your verification keys are all different, it's completely different number. Like I think five times larger, if I remember correctly, roughly at least.
00:26:51.744 - 00:27:30.990, Speaker D: And this even would not be a complete verification, it would be aggregation, which is also a little bit different procedure. And il can cover it in details, I think much better than I do. So for this, we should separate this talk, and first maybe talk about the proof systems and trade offs coming from proof systems themselves. And then you can try to talk about how you can actually try to implement a programmability kind of separately, which is unlikely to depend on how you, on what proof system you take if you have the recursion. It's another field of optimizations, I would say.
00:27:33.200 - 00:27:50.550, Speaker B: Yeah, that's a very interesting framing. I think that's exactly right. You start with the proof system, and then recursion is applied and you have all the interesting benefits of it. But Ariel, were you going to say something?
00:27:55.800 - 00:27:57.270, Speaker C: What was I going to say?
00:27:58.600 - 00:28:07.960, Speaker B: I guess. Do you agree with this framing? Is this the correct way to reason about the trade offs of these systems?
00:28:11.520 - 00:28:12.764, Speaker C: What is that way?
00:28:12.802 - 00:29:01.450, Speaker D: Again, it was more about that. You can separate this larger question made by Lucas into one is like, what are trade offs in proof systems and their particular implementations, versus a design area of how you actually implement programmability? And from my perspective, those are like completely two different tasks. One is optimize how you implement proof systems and recursion, like particular hash function choices, commitment schemes choices. Maybe something else we can talk about this. And another one is how you actually make a circuit, which, for example, does the CK roll up and maybe verify user defined circuits and contracts. I would say like two completely different deals for me at.
00:29:07.040 - 00:30:04.864, Speaker C: You know, one thing just, I think about you, Lucas, originally, in terms of trade offs of polynomial commitment schemes, I think right now, the KZG, the Verucha Goldberg one is clearly the winner. I think the only reason to use anything else is if sort of carrying friendly curves will at some point be crushed, and then there'll be no choice but to use something else. Yeah, I don't know. I think. Yeah, there were a lot of parts of this question. Yeah, maybe. Let's go to the next question.
00:30:04.864 - 00:30:08.130, Speaker C: There's a lot of parts, so I'm not sure what to.
00:30:08.900 - 00:31:02.050, Speaker B: It's a load of questions exactly. But you did mention something interesting with regards to pairing based cryptography that we've had a couple of discussions in the past, and you, of course, a huge topic of your research is security. Ethereum uses a variance of bn 256 as a proving curve or as a curve at the base layer. Eth two, moving to BLS twelve, three, eight, one. What is your take on the security of these curves? And maybe of pairing based cryptography more generally? Do you think there will be a scenario in the short term? And here's another loaded question, where these curves need to change or setups need to change?
00:31:04.260 - 00:31:19.350, Speaker C: Yeah, I don't know. It's like asking, will bitcoin hit $1 million? Or let's say will bitcoin hit $100,000 within three months, two years, five years? Never.
00:31:21.900 - 00:31:24.090, Speaker B: Is it a matter of when? Not if.
00:31:26.060 - 00:32:26.076, Speaker C: Well, there is a question of if. It's really hard to say. You've got these number field sieve attacks that are what makes us go to larger curves. And it's really tricky to say. You occasionally have a new paper with some improvement, and then it's sort of like, how fast are these improvements going to go? Are they going to get stuck? It's really hard to say. I tried to understand practically how safe is bn two five four for the next few years? Yeah, honestly, it's really hard to say. It's really hard to say.
00:32:26.076 - 00:34:09.864, Speaker C: If like, okay, the current attacks need a little extra trick to make it dangerous to use bn two five four, or there's going to be like ten years until this trick comes. It's really hard to say. One annoying thing is that when you've got the pollard's row algorithm, so the proof runtime is very well analyzed, but with these number field sieve attacks, they're saying, okay, this is the runtime under these number of, sort of theoretic conjectures. And then you're sort of like, okay, so should I update my code according to the more pessimistic, like pessimistic in terms of, say, the user, not the attacker? Should I update my curve according to the more pessimistic estimate sort of versions of these conjectures? Or should I actually wait till there's a. Right, so these curves have like 200, 254 bits that we're using right now in Ethereum. So if you look at Zcash, for example, the minute there was a paper saying, okay, there is potentially an attack on this curve, immediately they moved. What should be your approach? The minute someone writes a paper where this maybe is not safe anymore, do you move then? Or do you say, well, show me an actual attack on you say, okay, shot 256 is not safe anymore.
00:34:09.864 - 00:34:40.724, Speaker C: It has what is 80 rounds. Show me at least an attack of the break 70 of the 80 rounds. So equivalent. Show me an actual discrete log computation in a field that is getting close to what we're using, and then I'll switch my curve or. It seems that what people are actually doing is the minute there's a paper saying, okay, this may be dangerous, they immediately move. They don't want more evidence. Honestly, it's a bit of a rant.
00:34:40.724 - 00:35:09.150, Speaker C: When zcash, when we moved so quickly, I was like, oh, we're sort of giving them too much credit, legitimizing it too much. Yeah. But on the other hand, when I talked to some of these experts in the last few months. Yeah. I don't know, it seems like they have a lot of tricks up their sleeves. Yeah. There's sort of a long way of.
00:35:09.150 - 00:35:26.500, Speaker C: I don't really know. It really is like the price of bitcoin. How long will it take to hit the next. Going to be soon. Is it going to be never? Is it going to be really slowly?
00:35:28.600 - 00:36:06.768, Speaker B: No one knows. Yeah. It's an interesting area of research, and I think it's exactly what you said. Right. It's really a matter of making these design decisions on a proactive or reactive basis, on the basis of how legitimate or how concerned from a practical perspective, some of these attacks are. Claus Schnorr recently said that he broke RSA. Is the industry completely moving away from RSA? It's not really happening.
00:36:06.768 - 00:36:51.920, Speaker B: So it fits into those tradeoffs. But is this something that, as you're designing these systems that you guys take into consideration? Do you have kind of contingency plans if this were to happen? How proactive is that design? Because I would imagine it would be a nontrivial shift if parent base curves were to be, as you mentioned, at 70 rounds, be proven susceptible to some of these attacks, or is that not part of those research decisions or design decisions?
00:36:52.740 - 00:37:25.740, Speaker C: Well, I think the nice thing is, yeah. Since sort of the polynomial commitment scheme is an independent component, it shouldn't be too tricky to move to a system that doesn't rely on pairing based curves. Yeah, you'll take the efficiency hit, but it shouldn't be too tricky.
00:37:27.120 - 00:37:41.250, Speaker B: Do you have a favorite alternative to bn two five, six that maybe e two researchers should consider or even the client developers should be looking out for?
00:37:45.380 - 00:37:58.020, Speaker C: Just from the top of my head, I would maybe switch to these discrete algorithm based systems. So something similar to what zcash is doing with Halo.
00:37:59.400 - 00:38:23.428, Speaker A: Thanks for tuning in to the first half of the panel on privacy and snarks. If you want to see the rest of the panel, if you want to learn more about our panelists, or if you want to be part of our long tail discussion on the topic, check out smartcontractresearch.org t six two two. Thanks again and enjoy the rest of the summit.
00:38:23.604 - 00:38:24.330, Speaker D: You.
00:38:24.660 - 00:38:29.740, Speaker B: Thank you so much to scurf and Ariel for being part of that great discussion about snarks.
