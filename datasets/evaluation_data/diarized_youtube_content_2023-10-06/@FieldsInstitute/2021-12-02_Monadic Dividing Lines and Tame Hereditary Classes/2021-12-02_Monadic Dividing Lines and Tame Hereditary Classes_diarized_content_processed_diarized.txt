00:00:15.640 - 00:00:19.814, Speaker A: Okay, I guess we should start.
00:00:19.894 - 00:00:20.554, Speaker B: Yeah.
00:00:22.614 - 00:00:55.512, Speaker A: So welcome everybody back. Our first speaker of the afternoon is Samuel Branfield. He will speak about monadic dividing lines and tame hereditary classes. And just for the people online, if you're not seeing a lot of people today, it's because the heating has gone from the fields. So actually the people you're seeing now, Samuel, are very brave to be here just to listen to you. So all the speakers should be happy to, to having this, such a crowd.
00:00:55.648 - 00:00:57.360, Speaker B: I appreciate the live audience.
00:00:57.552 - 00:00:58.524, Speaker A: Go on.
00:00:59.384 - 00:01:32.104, Speaker B: Okay, so, yeah, I'm going to be talking about sort of monadic analogs of stability in nip and their use in analyzing hereditary classes. And I should say right now, the main results I'm going to talk about is joint work with Chris Laskowski. So. Okay, it's just some. All right. The definition is to begin with. So we'll say that a first order theory is either mandically stable or manatically nip, if any expansion of the theory by any number of unary predicates remains stable or nip.
00:01:32.104 - 00:02:11.324, Speaker B: So, a quick note, this is not stability or nip. For the full monadic second order theory, we're not allowed to quantify over the predicates, just we add the predicates and then once we've added them, it's normal first order logic. But still, this turns out to be an extremely strict condition. So some examples of. Some examples and non examples. So, for monadically stable theories, this includes refining equivalence relations and mutually algebraic theories, which maybe at least loosely, are like a generalization of bounded degree graphs. Some monetically nip theories include the rational order and various tree like structures.
00:02:11.324 - 00:03:07.984, Speaker B: And then some theories that are not maniatically nip include essentially anything algebraic. So, in particular, if you have a non unary function, for example, vector spaces, it's ruled out theories with cross cutting equivalence relations and the generic permutation, by which I mean the for, say, limit of all finite structures equipped with two linear orders. And these in some sense are sort of fairly typical or prototypical examples, or non examples, rather. Okay, and then a small note that, okay, monetic stability in monadic nip are sort of not the only monadic properties one can consider. At the very least, there's also monadic NFCP, which in fact turns out to be the same thing as mutual algebraicity, and then a stronger and closely related notion called cellularity. And these also show up quite a bit in the combinatorics of hereditary classes, but there would be a lot to say about them. And so I'll just solve that problem by saying almost nothing about them in this talk.
00:03:07.984 - 00:03:42.336, Speaker B: So I'll really be focusing on monadic stability and nip. Okay, now, hereditary classes. So for me, hereditary class, I'm thinking about relational structures. And then hereditary just means it's closed undertaking substructures in the model theoretic sense. For combinatorialists, this is sort of the analog of induced subgraph, where I'm not allowed to forget edges. And then if you're thinking about hereditary classes, in particular finite structures, there are various measures of tameness you might want to consider. For example, there's a growth rate of the class where you just count how many structures of size n there are for each n.
00:03:42.336 - 00:04:56.064, Speaker B: And you do this, say, asymptotically, there are various notions of width coming from structural graph theory, things like tree width and its generalizations that tend to measure, I would say, like how far structures in your class are from being very simply decomposable. There are questions about algorithmic tractability. So if you take an algorithm that's very hard in general, does it become simple when you restrict yourself to structures from your class? And, okay, so on and so forth, you could keep going, but at least these will be ones that will be mentioned in today. Okay, and then at least some of this should maybe sound a little bit familiar from more classical sort of concerns with complete theories. Yeah, I mean, the growth rate is really just a spectrum function. But now, in this context of hereditary classes of finite structures, and just like with sort of the classic analysis of the spectrum function, really what we're after is some sort of structure theory for our hereditary classes that roughly says how we can build up the structures from very simple structures in a controlled way. And then these, proving sort of these tameness measures is really just a series of test questions for whether or not our structure theory is sort of satisfactory or as powerful as we'd like it to be.
00:04:56.064 - 00:05:36.352, Speaker B: Okay. And then sort of another just terminological note, we'll say that hereditary class has some model theoretic property if the theory of the class has that property, which, I mean, you take the common theory, so the partial theory of all the substructures or all the structures in the class. So note, this is different from considering the universal theory of the class. And I don't know how much changes if you sort of consider the universal theory of the class instead. Okay, and then before we get to the sort of results, just some applications of them. So the first application I'll mention is two monotone graph classes. So these are graph classes.
00:05:36.352 - 00:06:29.114, Speaker B: These are hereditary graph classes, but they're closed under the stronger property of taking graph theoretic subgraph, where you are allowed to forget edges. So here, tameness seems to correspond to sparsity. And in the past ten or maybe a little bit more years, it seems sort of the very general and seemingly crucial dividing line that's gotten a lot of attention called nowhere denseness. So there's many, many equivalent combinatorial characterizations of nowhere denseness, and there's a lot of algorithmic applications. Maybe one of the high points is that first order model checking, which is the algorithmic problem where I give you, say, a graph, in this case in a first order sentence, and I ask you whether the graph satisfies. The sentence is tractable on nowhere dense classes, and otherwise very, very hard if you're in a monotone graph class that isn't nowhere dense. So it's really the dividing line for this algorithmic tractability question.
00:06:29.114 - 00:07:24.724, Speaker B: Okay? And it turns out that nowhere dense classes correspond to both stability and maniatic stability, as well as nip and manatic nip. So for monotone graph classes, all these notions collapse, and they're equivalent to nowhere denseness. In the past couple years, there's been this notion of structural graph theory called twin width. And it seems that bounded twin width really gives a crucial dividing line when you're looking at hereditary classes of ordered graphs. So here again, we have that first order model checking is tractable if you have bounded twin width, and intractable otherwise. And we also get a dichotomy in growth rates if your graph class does have bounded twin width. If your ordered graph class is bounded twin width, then it has in most exponential growth, whereas otherwise it has at least factorial growth.
00:07:24.724 - 00:08:21.982, Speaker B: And for ordered graph classes, it turns out that bounded twin width corresponds to both nip and monadic nip. So these notions at least collapse. And then another result is for growth rates of homogeneous structures, by which I mean, you know, I take a, take a for se limit, I look at its age, by which I mean its class of finite substructures, and I consider the growth rate, as we said before, where you just look at the number of structures of size n, and it turns out that, okay, well, again, theres a pretty big dichotomy between structures with sub exponential growth rate and growth rate at least phi to the n, where phi is the golden ratio. And this essentially corresponds to whether or not your homogeneous structure is manatically stable. Furthermore, for the mandarin stable case, there's a very nice structure theory due to Lachlan. That lets us say a lot more about what happens there. And in particular say there's a gap between polynomial growth and quite a bit faster.
00:08:21.982 - 00:09:02.984, Speaker B: That corresponds to whether or not your structure is, is cellular, which is this other property I mentioned a little bit earlier. Then, expanding on this last result about homogeneous structures, here's a conjecture. Maniac stability more or less corresponds to whether or not your structure has sub exponential growth rate. And this says that manatic nip should correspond to whether or not your structure has at most exponential growth rate. We'll come back to this conjecture later. Towards the end of the talk, maybe another quick note about these applications, at least for the last result. In one of the two papers about twin width.
00:09:02.984 - 00:09:45.644, Speaker B: This was proven simultaneously by two groups. One more model theoretically, and one more combinatorially. But okay, for one of these papers, in the last result, the model theory really is absolutely central to the analysis. It's not like the analysis is sort of done combinatorially, and then after the fact, you realize that, oh, you know, it also is equivalent to maniatic stability or mannequin IP or whatever sort of. The notion of model theoretic independence really plays a crucial role in, in these proofs here. This sort of was done combinatorially initially, and then later on realized that it was equivalent to these model theoretic notions. But now the model theory is really is playing an increasingly important role around these notions.
00:09:45.644 - 00:10:27.524, Speaker B: In particular, there's a significant program of trying to generalize the nice properties of nowhere dense monotone graph classes to either manatically stable or manatically nip hereditary graph classes. So what happens in this more general setting once you remove this monotonicity condition? Okay, so now on to monadic stability. And I want to start with, well, a non example. In an example. So first I want to show fairly concretely that vector spaces are not maniatically nip. And in particular, there's going to be a specific configuration that witnesses this, which I'll call coding an infinite grid. So, ok, let's start with the vector space.
00:10:27.524 - 00:10:40.962, Speaker B: Let's take an infinite linearly independent set and divide it into two infinite pieces, a and b, and then I'm going to let c be their sum set. So now I have the property that if I look at this formula, could.
00:10:40.978 - 00:10:48.134, Speaker A: You go back and forth once with this slide? I think you're erasing. Yeah, now that's okay. You can go back.
00:10:49.514 - 00:10:50.082, Speaker B: Sorry.
00:10:50.178 - 00:10:51.034, Speaker A: Oh, this is the slide.
00:10:51.114 - 00:11:03.006, Speaker B: Good, good. Sorry. Yes, yes. Yeah. Okay. Yeah, it's a very crudely drawn slide if that's causing issues. Okay, right.
00:11:03.006 - 00:11:51.862, Speaker B: So, right, we have these two linearly infinitely, a and b and c is their sum set. And then we have this formula, in this case, just x plus y equals z, which has a nice property. If I force x to be from a and I force y to be from b, then there's a unique point that can be right from c that I can plug in for z to satisfy this formula. So really you can think about this as sort of a is one axis of my grid, b is another axis of my grid, and then there's a unique point from C that's coordinator by this pair a and b, where phi is sort of the coordinator formula. And I can fill in the rest of my grid with other points, right? The rest of c fills in the rest of my grid. So now once I have this, I can define the edge relation of a bipartite graph between a and b. And I'm going to do this.
00:11:51.862 - 00:12:36.366, Speaker B: Well, first I'm going to expand by a new predicate d, which will be a subset of c. And in this case, the formula would just be that there exists some point in d such that x plus y is equal to the given point. So if d is all of c, then I just get the complete bipartite graph, because c is exactly sort of the set of solutions to this. But by taking d to be an appropriate subset of c, I can code whatever bipartite graph I want. And so this is certainly not manatically stable. And it really comes down to the fact that I code these infinite grids. Okay? On the other hand, here's a very simple manatically stable theory, the theory of an infinite equivalence relation.
00:12:36.366 - 00:13:46.454, Speaker B: I'm not going to show that it's mandically stable, but I'm going to talk about sort of the structure theory that you get once you have this. So, okay, for an equivalent relation with infinitely many infinite classes, I can just nicely partition any model into these blobs, where each blob, in this case is a class of the equivalence relation. And this has the nice property that the behavior between the blobs is very, very regular. More formally, what I get is I get a congruence, by which I mean, if I look at any tuple, I can intersect that, right? I can consider the intersections of that tuple with each blob when I get a bunch of subtuples. And the type of the full tuple is just determined by the type of all of these subtuples. So again, this says that somehow my blobs are behaving really regularly with respect to each other, and this is what I'll call a linear decomposition later. Now, one thing that's maybe not quite so nice about this is that these blobs can get arbitrarily large, and maybe I want to control how big these are getting, as well as the behavior between them.
00:13:46.454 - 00:14:33.792, Speaker B: So I can do that by doing a little bit of Lowenheim Scholem, I can turn these blobs into countable models at the cost of turning this line I have into a tree of height three. And that's rather unintelligibly drawn here. I drew this and I thought I might try to explain it. And then I figured it was maybe best not to. But you can maybe imagine how I get a tree from these equivalence relations. And then if I have refining equivalence relations, my tree just gets taller and taller, but I still have the nice, same nice tree structure, and I still sort of keep this congruence property where the pieces of my tree behave really nicely with respect to each other. Okay, so monadic stability was studied by Baldwin and Chella in the early to mid eighties, and they proved the following theorem.
00:14:33.792 - 00:15:08.364, Speaker B: So, okay, the following equivalent t is manatically stable. T is stable, and monetically in ip a forbidding configuration. T is stable and does not code an infinite grid in the sense that we saw with vector spaces. Models of t admit a nice decomposition into these trees of countable sub models, like we sort of saw at the equivalence relations. And the last condition is a condition on non forking independence. So t is stable. And if you have two independent sets and you try to add any new element, you can either add it to one set or the other while maintaining independence.
00:15:08.364 - 00:16:03.526, Speaker B: So, okay, a few words about this theorem. Maybe the first thing to say is that only the first two conditions refer to unary expansions of t, and the last three conditions all refer to t itself. Yeah, so even though maniatic stability was initially defined in terms of expansions of the theory, it really does get reflected just in the first order theory itself. And you can characterize it solely in terms of the first order theory t without saying anything about expansions. The other thing to say is that this last condition on non forking independence is really what drives the rest of the proof. So, in particular, if this condition fails, then you can take a witness to its failure, and you can play similar games to what we did with vector spaces. But instead of using linear independence, you use, you build this grid using non forking independence.
00:16:03.526 - 00:16:33.982, Speaker B: But it's very very similar. On the other hand, if this condition holds, it turns out that this condition is equivalent to the following. So now if you look at forking dependence instead, it should be trivial, by which I mean, if you take, right, if two sets are dependent on each other, then there's already singletons in each set that are dependent on each other. So the behavior of dependence really comes down to the behavior on singletons. And furthermore, forking dependence should be transitive. So, okay, we're in a stable theory. Forking dependence is automatically symmetric.
00:16:33.982 - 00:17:17.831, Speaker B: So therefore, forking dependence gives us an equivalence relation on singletons. And we can use this to build the treaty composition, sort of similar to how we did it with a literal equivalence relation in the example. So, right, this is really driving both the non structure of getting an infinite grid and the structure of building these, these tree decompositions. Okay, so soon after the work with Baldwin, Shalah went on and analyzed monadic nip, and he isolated this condition that I'll call the fs dichotomy. So it's the exact same condition as we saw before. Um, that was characterizing mimatic stability, right. The exact same independence condition.
00:17:17.831 - 00:17:36.083, Speaker B: Except now, instead of forking independence, it refers to the independence coming from finite satisfiability. Yeah, we're going to be working sort of outside of the stable. Right. The stable relevance of forking in general won't be so well behaved. So we're going to switch to finite satisfiability to give us our, our independence relation.
00:17:37.484 - 00:17:37.812, Speaker A: Okay.
00:17:37.828 - 00:18:27.364, Speaker B: And then Shella proved the following. So, I mean, among other things, you proved the following two results. So if t does not have the fs dichotomy, well, then, like in the maniatically stable case, we'll get an infinite grid, but we'll only get it in a manatic expansion. Maybe not in the theory t itself, but this is still good enough to show that, okay, if you don't have the fs dichotomy, then you're not manatically an ip. On the other hand, if you do have the fs dichotomy, then again, like in the Mandarin stable case, we'll get nice decompositions, but now we'll get these linear decompositions, which is sort of like the one step version of the tree decomposition. And we'll come back to these linear decompositions in a little bit more detail in a couple of slides. Okay, so now I want to mention sort of the main theorem joint with Laskowski.
00:18:27.364 - 00:19:00.920, Speaker B: So, okay, this is really like a Baldwin Schloss style theorem for manatic and ip. So the following are equivalent. T is magnetically nip. T does not code an infinite grid on singletons in a unary expansion. T itself does not code an infinite grid on tuples. So, okay, I mean, again, really a forbidding configuration in t, right? Behavior of independence in t t has this fs dichotomy that tells you how finite satisfiability behaves. Again, models of t admit nice decompositions, although sort of like in the Mandarin stable case.
00:19:00.920 - 00:20:05.204, Speaker B: This is really only helpful for sort of large infinite models of T. And then lastly, a condition on the behavior of indiscernibles. So your theory is DP minimal and indiscernible trivial. So I'll come back to these last two points, including some definitions in the next slide. But okay, really, the first five of these conditions all correspond pretty closely to the five conditions from the Baldwin Shalott theorem, and they're sort of the natural analogs once you're working outside of the stable case. This last condition on indiscernibility is sort of new with respect to the Baldwin Shallot characterization, and we should thank Pierre Simone for suggesting that this characterization ought to hold. So, okay, from Shella's results, we still need to show what two implies, three, which turns out to be not all that significant, you know, not all that big a step, although a very, very useful step in terms of applications, that five implies one, that if we had these decompositions, then we are mandically an IP.
00:20:05.204 - 00:21:20.474, Speaker B: Right? Shall I prove in the converse? And then lastly, the equivalence of all these other five characterizations with with the 6th. So, okay, a few words about sort of what all this means and how it's done. Okay, so given a model M and a bunch of a sequence of subsets from the monster model, we'll say that these form an MfS sequence if the ith set is independent over all the previous ones with independence in this sense of finite satisfiability in M. So this is sort of like the analog of Morley sequences in this setting, except, I mean, one key difference is that these don't have to be indiscernible. And then when I say linear decomposition, what I mean is a partition of your model into a sequence of subsets, and then some other model M coming from somewhere in the monster model, such that your sequence of subsets is an MfS sequence. So for a simple example of this, you can consider a model N is a model of DLo. I can take my partition to just be each AI is just a single point taken in increasing order, and then my model M, you know, I go to my monster model and I pick a model that just sits entirely above n inside the monster model, and then that gives me my, my MfS sequence.
00:21:20.474 - 00:22:12.044, Speaker B: Okay? And again, just like in the mandarin stable case, I have sort of quite tight control over how the pieces of a linear decomposition interact with each other. I sort of get this congruence condition again, except I also have to sort of consider the order that these, these subtuples come in. Right? So order is going to play a role because I've left the stable setting. Okay. And then coming to this, this indiscernibility characterization. Okay, so DP minimality is a fairly standard notion, roughly. It says that if you have an indiscernible sequence and you consider any new point that that point can make, at most one cut in the indiscernible sequence, and then indiscernible triviality says that whenever you have an indiscernible sequence and it remains indiscernible over each singleton from some set, then it remains indiscernible over the entire set.
00:22:12.044 - 00:23:03.168, Speaker B: And. Right, the name is meant to sort of evoke triviality of non forking that we saw back in the mandically stable case. Sorry, triviality of forking dependence that we saw back in the manatically stable case, which is really sort of the analog of, and maybe just a word about these indiscernibility conditions. It's easy to see that the DLO satisfies these two conditions. And what manmadically nip theory is having linear decompositions tells you is that very, very coarsely at least, their models look like models of Dlo. And so it's perhaps believable that you could somehow lift these properties as well to models of a mandatory nip theory. Okay, so maybe a few more ingredients about, you know, what goes into this proof.
00:23:03.168 - 00:23:42.776, Speaker B: Maybe the main, sort of the main ingredients is just the manipulation of these MfS sequences and how they relate to indiscernible sequences in particular. For this result, this five implies one step. So if you have decompositions, then you are manatically an IP. This comes down to a type counting argument. So really there's a 7th characterization of manatically an IP in terms of type counting. And this type counting is really extremely similar to various width notions coming from finite structural graph theory. But, okay, it turns out that if you're mandatory nip, you have just an absolute bound over all models of Beth two on this width.
00:23:42.776 - 00:24:28.454, Speaker B: So an absolute cardinal bound, whereas, okay, if you're not mandically an IP you can use increasingly large instances of the independence property to show that there's no bound whatsoever. And then maybe one more small thing to mention is that sort of key in all of this is a lemma that lets us write a very general lemma that lets us step up bounds from quantifier free types to bounds on complete types. And I think this could be useful in many other places as well. Okay, so once we have all that, some applications. So, okay, let t be a complete relational theory. And I'm going to also assume QE. Qe, if not absolutely necessary, is at least very, very helpful in bringing results down to the finite.
00:24:28.454 - 00:25:13.954, Speaker B: I'm pretty sure I can at least get away with, say, model completeness here. I'm not sure how much less I could get away with, but, okay, suppose we do have QE and we're not monetically nip. Well, then, conjecturally, at least in the homogeneous setting, our growth rate should be super exponential. This is what, this is what the conjecture I mentioned before said, and this confirms this in quite a strong way. Not only is it super exponential, if you're not mandatory nip, your growth rate is at least in this, right in the neighborhood of a factorial. Okay, so this is sort of one non structure result, and then here's another result. So again, if you think back to the application slide, we saw several instances where manatic stability and stability collapsed, or mnatic nip and nip collapsed for these hereditary classes.
00:25:13.954 - 00:25:41.402, Speaker B: And this says that, okay, well, this actually always happens. Again, assuming QE. So, okay, your age is nip if and only if the theory itself is manatically nip. If and only if the age is manatically nip. So again, this is mostly a non structured theorem. The interesting direction is that if your age is not manatically nip, then in fact it's not even nip, and you get the same result for stability. So both of these results, again, they're really non structured results.
00:25:41.402 - 00:25:59.706, Speaker B: And what lies behind both of them is the fact that if your theory is not mandatory an ip, then you code this grid on tuples in the theory itself without having to pass to a unary expansion. Okay. And then to close a few questions.
00:25:59.890 - 00:26:10.566, Speaker C: Sorry, can I ask a question about that? Is that like tight for the first one? Do you have an example of something that is not? What's the worst example you have?
00:26:10.670 - 00:26:32.502, Speaker B: Yeah, so I know that you can't get up to, like, you can't bring it up to n factorial. So like these sort of Krushovsky type constructions give you just a little bit below n factorial. Yeah, but I think you should be able to get, you should be able to get like a concrete k. Like you should be able to get maybe n over two factorial or something. Oh, okay. Okay. And in fact, probably even a little bit better.
00:26:32.502 - 00:26:38.430, Speaker B: But this is almost certainly not optimal. But you can't get n factorial. Yeah.
00:26:38.462 - 00:26:39.274, Speaker C: Okay, great.
00:26:42.414 - 00:27:38.074, Speaker B: Okay, so some questions to close with. So, okay, very broadly, right, and back in this previous slide, we saw that we had, these were really non structured results, it looks like doing sort of the structure theory, proving that things that are mandically nip or well behaved is quite a bit harder. So, you know, broadly, just sort of develop the structure theory for monetically nip. Maybe a little bit more specifically, what we have is we have this, right? In the manatically stable case we have these tree decompositions, and the mandarin IP case we have these linear decompositions that give you a lot of information about the structure of a very large infinite models. But can these somehow be refined to be useful for doing finite combinatorics? Okay, now that's a very sort of broad and vague question. So some more specific questions, just maybe balance that a bit. And these will actually all be about maniatic stability, which we understand relatively well compared to maniatic nip.
00:27:38.074 - 00:28:43.706, Speaker B: So, okay, right. We saw this collapse between stability and maniatic stability if we assumed quantifier elimination. So the first question is just, does this hold in general, and maybe more broadly, what is the role of quantifier elimination in bringing results down to the finite? How necessary is it really? And then these last two questions sort of come back to this sparsity program around nowhere denseness that I briefly mentioned. So, okay, I mean, like I said, nowhere denseness. There's been a lot of work done, particularly on the algorithmic side. And so it'd be very nice to be able to lift results about nowhere dense classes to monadically stable classes. And so what this is asking is, okay, well, sort of, do manatically stable classes all have some sort of nowhere against skeleton from which we can recover the monadically stable class? In particular, is every mandically stable class actually just definable in some manatic expansion of a nowhere dense class? So for some concrete applications, this would immediately imply that mimatically stable theories have low vc density.
00:28:43.706 - 00:29:29.754, Speaker B: And maybe ideally, you could somehow lift the result about first order model checking being the dividing line from maybe, let's say not being the dividing line, but being tractable from nowhere dense classes to maniatically stable classes, if you knew this. Okay, and then lastly, there's another sort of related to this nowhere denseness notion. There's a sort of, the other main dividing line in the sparsity program is what's called bounded expansion. This is a notion that is stronger than nowhere denseness. And my question is just sort of what is the right model theoretic and the formalization of this notion. So it should be some property that's stronger than magnetic stability, because nowhere denseness corresponds to maniac stability. But I don't know what it is.
00:29:29.754 - 00:29:36.714, Speaker B: Okay, so that's it. I have a slide with some references, but I'll go back to you.
00:29:38.454 - 00:29:46.714, Speaker A: Thanks, Samuel. Are there questions?
00:29:53.814 - 00:29:59.434, Speaker C: So what is bounded expansion mean? Is it simple to tell us?
00:29:59.934 - 00:30:33.652, Speaker B: Yeah, so, okay, maybe nowhere denseness means that if you look at your, I mean, very roughly, if you look at your graph and you, you're allowed to sort of clump things together. You can clump vertices together at distance sort of at most, fix a distance D clump together vertices that are distance at most d apart. And you get sort of the induced graph after you've pumped these all together. For each d, there should be a bound on the size of cliques that you get is what nowhere denseness says. Bounded expansion says that. Well, when you do this, you don't get anything that's, there's a bound on sort of how dense the results can be. So sort of the worst case in terms of dense is of course that you get a clique.
00:30:33.652 - 00:30:37.036, Speaker B: And this is saying something a little bit stronger, that it gives you a.
00:30:37.060 - 00:30:44.024, Speaker C: Bound on the density. Okay, yeah, I see. Okay, thanks.
00:30:48.404 - 00:30:49.664, Speaker A: Further questions.
00:30:51.804 - 00:30:53.944, Speaker D: May I ask a very naive question?
00:30:54.444 - 00:30:55.184, Speaker B: So.
00:30:58.874 - 00:31:15.402, Speaker D: Example, if you have something that is like monetically stable, and moreover it is, let's say strongly minimal, then the same as saying that it needs to be a trivial structure somehow.
00:31:15.578 - 00:31:17.094, Speaker B: So it looks to be what?
00:31:17.594 - 00:31:23.050, Speaker D: Trivial in the sense of geometric stability theory?
00:31:23.202 - 00:31:43.744, Speaker B: Yes. I mean, this, this going back here. Right. This last condition is equivalent to one of the two pieces of this forking characterization is that forking dependence needs to be trivial. I see. So yeah, this is sort of why all the algebra is getting ruled out. I see.
00:31:44.564 - 00:32:00.686, Speaker D: So in some way, can we try to think of monadic nap, even though we do not have a geometric nap theory yet or whatever, but can I think of monadic Niv as somehow like the analog of trivial in the sense.
00:32:00.710 - 00:32:01.486, Speaker B: Yeah, so I would say.
00:32:01.590 - 00:32:01.854, Speaker A: Right.
00:32:01.894 - 00:32:18.954, Speaker B: Triviality is certainly sort of one of the two pieces. I mean, again, going back to this. This indiscernible triviality is really like the triviality of dependence. But, okay, dependence coming from finite satisfiability. And then on top of that, DP minimality. So it's sort of like triviality plus DP minimality.
00:32:25.754 - 00:33:15.114, Speaker A: Last question, someone. If not, let's thank Samuel again. And we resume, I guess, directly in three minutes, just to be on schedule. So at 35 past. Hi, Leonardo. Hello.
