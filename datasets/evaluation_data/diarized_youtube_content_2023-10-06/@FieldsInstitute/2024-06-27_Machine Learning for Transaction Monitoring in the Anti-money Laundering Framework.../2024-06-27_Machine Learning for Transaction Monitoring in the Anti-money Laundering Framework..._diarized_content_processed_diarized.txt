00:00:00.600 - 00:00:27.640, Speaker A: We have another keynote speaker, the second keynote speaker for the day, and she's Professor Yen. And please, you can introduce yourself. Thank you. Thank you. Professor Gurachino. Good afternoon, everyone. My name is Yun Chang Nguyen, and I am an associate professor at York University.
00:00:27.640 - 00:02:14.189, Speaker A: So it's an honor and a pleasure to have the opportunity to present our work in financial technology here at this workshop. So I've been doing research in wireless networking, network security, cybersecurity, online social networks, and the past seven to eight years, we got into financial technology because of the interest from some graduate students. We've been working on fraud detection, financial crimes detection, stock price, currency exchange rate prediction, and obviously, anti money laundering. So in 2020, 2020, we were fortunate to receive a grant from Scotiabank on some projects with Scotiabank, those are not related to this talk because they are under a non disclosure agreement. But during the process of working with Scotiabank, we have learned a lot about money laundering, the process of money laundering, and the framework of anti money laundering. And also in 2022, we created a new course titled Financial Crimes and Technological Countermeasures offered by Lausanne School of Engineering. And during that process, we had the opportunity to work with an AML expert, Doctor Faizo Saeed.
00:02:14.189 - 00:04:06.478, Speaker A: So, again, the talk today that I will present is less technical than most of the presentation here at the workshop because I would like to focus on the lessons that we've learned from the experts and apply those lessons to machine learning in AMLA and time money laundering, with a focus on promoting robust research methodology and reliable reporting of results. So, to start, this is the outline of the talk. I will talk a little bit about money laundering process and the anti money laundering framework. And then we'll talk about the current rule based transaction monitoring system, the advantages of machine learning for AML, and the challenges related to data for training AML models. And that gave us the opportunity to talk about how to apply AI and machine learning to solve these challenges. And we'll talk about an example deep learning model for AML transaction monitoring my master's students developed in her master thesis, and I summarized the talk at the end. So what is money laundering? Money laundering is a process of moving money through many layers in order to hide the origin of the sources, and after that, integrating the money into the legitimate financial system.
00:04:06.478 - 00:04:58.242, Speaker A: So money laundering process has three stages. The first stage is placement. Basically, the dirty money is introduced into the financial system, and then in the second stage, layering the money will go through several stages or several layers in order to hide the source of the money. And in the last stage, now the money becomes clean, it will be introduced to the financial system. Officially, one example is money laundering through a casino. So assume I have $5,000 dirty money, I can bring it to a casino and then buy some chips. And then I can play a few games, lose a few hundred.
00:04:58.242 - 00:06:02.380, Speaker A: And then I came back. I come back and then exchange those chips, cash out those chips to get cash and get a receipt from the casino. Now I can bring, let's say, $4,800 and the receipt to the bank deposited as a source of winning from gambling rather than from annuity source. So money laundering is usually considered as a victimless crime. With these statistics, 2% to 3% global gdp, or 2 trillion. It sounds like a victimless crime, but in effect, laundry money laundered. The process is from criminal activities such as tax evasion, corruption, thefts, illegal gambling, trade in trucks and arms, and even more brutal or murderous, that could be human trafficking and terrorism.
00:06:02.380 - 00:07:21.000, Speaker A: So, for example, in 2018, there's a statistic that about 700,000 victims are exported annually. And globally there were about 40 million victims of human trafficking. So that's why there's a very stringent anti money laundering framework, especially in highly democratic countries like Canada, US, Australia, UK, for example. So what is the AML framework? It's basically a set of laws, regulations and procedures to prevent money laundering through many processes, such as know your customer and customer due diligence in order to verify the identity of a customer. To assess the risk of the customer. Another process is to monitor transactions finance transaction of these customers, to assess the risks of customers and transactions on a continuous basis. For example, whether the transactions come from high risk countries like Russia, Iran for example.
00:07:21.000 - 00:08:08.990, Speaker A: And if the bank detects high risk activities, they are responsible for closing or restricting these high risk customers accounts. And if there are suspicious cases related to money laundering, terrorist financing, they have the obligation to report them to the regulators. In Canada, that could be Fintrac. In the US that could be Fincent, and in Australia, abstract. And in the UK it could be fce. And those reports, in Canada we call them string. The US we call them SAR, suspicious activities report.
00:08:08.990 - 00:09:28.620, Speaker A: So what are the consequences of not following this framework defined by the regulators and the law? These are some of the penalties of non compliance. And you can say it's in the millions of dollars, but that's not it. That could be in the billions of dollars as well. And you can see some serial offenders like Standard Chartered bank in the UK in 2012, and then 2019, UBS 2009, and then again 2019. So you can see that the cost of non compliance, most of these related to money laundering, are working with sanctioned countries such as Russia or Iran, for example. All right, so in order to be compliant, as I mentioned earlier, one of the process in the framework is to monitor the transaction. And currently, banks use something called rule based transaction monitoring.
00:09:28.620 - 00:11:08.850, Speaker A: So basically they generate alerts when customer transactions breach a predetermined set of conditions in a given look back period, for example, for 24 hours or week or month, for example. So what can be the rules in this case, for example, the party are named on a watch list, such as a sanction list, for example, the russian official government, for now, iranian official government officials, or on the list of politically exposed person, basically those who held or currently hold high position in different governments because they would be very vulnerable to bribes, corruption. So that's another source of whitelist. Another example would be cash transactions over $10,000 in 24 hours here in Canada and the US. So another rule would be to watch out for something called smurfing. So basically, to evade this $10,000 limit, instead of depositing 95,000, they would divide this into ten deposits, and each is 9500 each. So because of the high cost of compliance, the thresholds for the alert generation logic are set to very highly concerted values.
00:11:08.850 - 00:11:38.248, Speaker A: The goal is not to. To miss any money laundering transaction. And because of this, there are very high numbers of false positives. So that's a trade off between having very strict or very low false negative. And now we have false, high false positive, and the rate is about 90%. 90%. So basically, 100 alerts, only about five would be suspicious.
00:11:38.248 - 00:12:24.354, Speaker A: Transaction 95 to be normal transactions, legitimate transactions. So this is real data. In a survey by the Bank Policy Institute in the US, they surveyed the largest banks in the US in 2017. So during that year, banks, those banks, reviewed about 16 million alerts, and from that they would consider about 640,000 were suspicious. And they filed reports with Fincen. So you can see it's about 4% of all the alerts generated. Now, there's another fact that I don't put here because there's no more space on the slide.
00:12:24.354 - 00:13:20.810, Speaker A: So among these 640,000 reports sent to FinCEN, Tencent investigated them and they would send back, they would ask for more feedback from the banks in order to determine whether those transactions were actually money laundering or not. And they asked an average of 4% of these cases. So basically only 4% of those cases would get feedback from FinCEN for more investigation. So basically, it's 4% of 4%. It's basically 0.16% of all of those alerts would be considered suspicious for penalties or for criminal charges. So, and because of the high number of alerts, there are three stages of AML investigation.
00:13:20.810 - 00:13:54.250, Speaker A: So imagine 95% false positive rate. So in the first stage, we call it rapid disposition of alerts. They will investigate two separate false positive and suspicious alerts quickly and dispose of those false are positive. The rest would be sent to the second stage. Now these analysts would have more experience. They would carry out detailed investigation to determine the nature of the alert. And again they would eliminate more post positives.
00:13:54.250 - 00:14:56.700, Speaker A: The rest would be sent to the last stage to validate these cases. They have to do more investigation, again to ensure those are actually money laundering transaction and to ensure compliance as well. And if those considered as suspicious will be reported to a MLK, to the ML reporting officer, money laundering reporting officer, and they will be recommended for the filing of SDR SAR to the regulator. So the rule based transaction monitoring have many advantages. So first, it provides a comprehensive coverage of known risk type and scenario. And the underlying rule here is known to if we don't know, then we cannot build into the rules. And it's easy to understand when customers or accounts or transactions violate those rules in order for the investigation.
00:14:56.700 - 00:15:58.286, Speaker A: And compared with machine learning, it's easy to explain why an alert was raised. Good machine learning, usually we get a black box output. The disadvantages would be very high number of false positive. The alert logic can get messy when the number of rules and scenario increases and these rule based system cannot detect new or emerging or complex money laundering patterns. So compared with rule based system, machine learning can give many benefits, such as they can reduce the number of false positives they can learn from past investigations in order to adjust the triggers accordingly. Dynamically, they can use data to identify new or complex money laundering patterns. But it's very hard to explain why an alert was raised with a machine learning system.
00:15:58.286 - 00:16:56.330, Speaker A: That's why we need explainable AI in order to have machine learning adopted by the industry. It's very important. The coverage of AML scenarios may not be 100% overlap with the rule based output because the data may not contain the rare events or very rare events for training. Machine learning requires high quality and country specific data for training for that country, and it requires proper methodology and performance matrix as we are going to talk talk about next. So in order to develop a machine learning model, the two components that are closely related would be learning algorithm and training data. Because depending on the training data, available, we can choose a learning algorithm, for example, data with no labels. We have to use some unsupervised learning, for example.
00:16:56.330 - 00:17:54.350, Speaker A: And then after we choose learning algorithm and we choose the training data, then we can adjust the learning parameters, for example the threshold, in order to optimize the performance of the algorithm for that training data. So I'm going to talk about the data aspect first, then we go into the learning algorithm. So there are many challenges related to data that we need for training AML models. When we talk about AML in this context, I refer to transaction monitoring, because there could be another aspect of AML which is risk assessment, but that's not the focus of today's talk. So there are three main challenges. One is the lack of publicly available real world data set. I mean, in this case real data sets.
00:17:54.350 - 00:18:54.500, Speaker A: And the second is the inherent nature of AML data. And the third one is how to handle AML data properly with the proper performance metric as well. So the lack of publicly available real data sets is valid because banks want to protect confidentiality and privacy of their customers. So to date, we have been able to find only three data sets. One is a financial data set from a checked bank, but it has no labels, so we cannot use, let's say supervised learning with this set. And not only that, it's very small, so deep learning is out question as well. The second one is the elliptic data set, but this one is mostly for bitcoin transactions, so it may not apply to banking transactions.
00:18:54.500 - 00:19:51.870, Speaker A: And the last one is a data set from an italian bank. Unfortunately, it's not publicly available. It can be available upon request and review by the authors, and it requires a non disclosure agreement. Now, even if there was a real data set, there would be a danger of having the anonymized data reviewed. It called the anonymization attacks. So that's why it's very hard to do research in AML, we don't have real data for many reasons. So now synthetic data come to rescue, but only recently, and we'll say recently, I mean after 2021, with some flaws.
00:19:51.870 - 00:21:01.180, Speaker A: So before 2021 there was this paysim and Aml sim by IBM. After 2021, there have been many other datasets available. So in this data set, ML stands for money laundering. So DP here stands for data production s synthetic, and d here data set. So these data set are synthetic data, and they were created based on two approaches. One is scenario based simulation, and the other one based on simulation based on real data. So with scenario based simulation, we identify possible customer behavior or interactions and possible patterns of transactions.
00:21:01.180 - 00:22:06.430, Speaker A: The terminology is typologies in the banking sector and then based on the identified behaviors, patterns of transaction. A simulator will simulate these behavior and transaction, and usually in these papers they use agent based simulation. And this diagram here shows some typology used in this simulator. AML word basically, this indicates from one source the money is distributed to other several sources. Same thing here. In this case, the money goes through several layers, several hops, in order to hide the original source of the money. So, scenario based simulation requires the domain expertise.
00:22:06.430 - 00:23:01.992, Speaker A: We have to understand what are the patterns of money laundering. And it's very labor intensive. For example, in this data set, the authors identified 28 typologies from existing data sets, from reading so many papers and from interviews with eight AML specialists. And you can see this very labor intensive. And they also added more detail to the data set, like geographic locations, high risk countries and high risk payment types, for example. Now, with all of these efforts, that data is applicable only to the UK, because these authors are from the UK. So if we want to train this for canadian bands money laundering detection, it may not work.
00:23:01.992 - 00:23:51.820, Speaker A: We need our data set specific to the canadian AML regulation. So with database simulation. So this is the second approach. We start with a real data set and then a simulator. We'll learn the distributions of the features and the attributes of the data set, like the graph, for example. And then it will generate synthetic data with the distributions closely following those of the real data. For example, given this graph, it can learn about, let's say the distribution of in degrees out, degrees of nodes, the weights of the edges of, and then it tried to simulate the distribution in the synthetic data.
00:23:51.820 - 00:24:50.170, Speaker A: So for the paysim dataset, the real data came from mobile payments transaction in some african countries. So you can see that it cannot be applied to, let's say, money laundering. In Canada, for example, with the synthetic AML data set, the customer and the transaction data are from a bank in Denmark. So with database simulation, the coverage depends on the real data. So if the real data misses the rare scenario, then that cannot be simulated in the synthetic data. And again, and because of that reason, the data set may not be comprehensive, it may not cover all these scenario money laundering, and it may reveal the underlying patterns of the real data. And this is a privacy concern as well.
00:24:50.170 - 00:25:44.540, Speaker A: So in this case, before we go into the solution to that, these are the very short limitation of existing data set AML SIM. They do not simulate many money laundering topologies, so they can be used only for scalability. Explainability or other technical performance. This data set has a very high money laundering ratio. It's not typical of a real case, because in real case, as we said earlier, it's about 0.16% of money laundering in a data set. AML word, they use very generic money launching patterns, and this one applicable to only the UK.
00:25:44.540 - 00:26:58.850, Speaker A: Now, for the Paysim dataset, it simulates mobile payments transaction in Africa, may not be applicable to other countries. And for this one, it's interesting because the real world data, the real data set were alerts, basically the set of SAR sent to the regulator. So that means that set of data contains the false positive and the true positive, and the false positive may not be representative of other legitimate transactions. So that's the disadvantage of synthetic AML data. So now we have an opportunity to use AIH to generate synthetic data. So we can start with a real data set, and then we use a machine learning technique, such as generated adversarial network GaN or VAE variational autoencoder. And these techniques will learn the distributions of the data by themselves, and then they generate synthetic data using the following those distributions.
00:26:58.850 - 00:28:03.190, Speaker A: So now, as we said earlier, if we just use, if we let the machine learning technique simulate the underlying distribution, we may reveal some patterns of the real data, and we want to avoid that. And to avoid that, we can use something called privacy preserving methods or techniques. So in this case, I can use one example, which is a model called differential privacy, Gan DPGan. So it use gan. So with Gan, there are two components in a GAN network. A generator here will learn the distribution of the real data, and try to generate samples as realistic as possible. And then that sample will be sent to the discriminator.
00:28:03.190 - 00:29:20.240, Speaker A: Now, the discriminator will compare the synthetic sample with the real sample. It will tell the generator whether that is a true or false, real or no. And based on that feedback from the discriminator, the generator will improve the process to create the data as closely related to the synthetic data, as closely related to the real data as possible, so they are trained in parallel. Want to generate data, want to evaluate whether the new data is as close as possible to the real data. Now, in this process, if we add some noise on the gradients to the discriminator, so basically we try to distort some of the distributions in the real data. By that way, we can hide the underlying distributions of the real data. Now, the disadvantage of this is the data may not be as realistic as we want, but the advantage is we can hide or we can distort some of the underlying distributions of the data to hide the data underlying data.
00:29:20.240 - 00:30:28.750, Speaker A: Now we go to the next challenge related to AML data, which is the inherent nature of AML data. So AML data are extremely imbalanced. So if we look at, take a look at these data sets, you can see that the glass distribution, ranging from smallest, is about one over 773. Basically one for every one money laundering transaction, we could have about 773 legitimate transactions, and it can go up to 1000, almost 1200. The only exception here is the bitcoin data set. But again, because this is bitcoin, bitcoins for this data set, they didn't have strong regulations. So there would be a lot of scam and frauds with these bitcoin data transactions.
00:30:28.750 - 00:31:36.690, Speaker A: And another reason could be that the authors did not include all the legitimate bitcoin transaction in their data set either. All right, so now if we have such extreme imbalance in data, what could be the problems? So first, a model, a machine learning model does not learn sufficiently about the minority class, or we call it the rare event sometimes. And that would cause a high misclassification rate for the minority class. So it requires additional data handling and care, or model handling as well, as we'll see later. So that's one disadvantage. The second one is the accuracy metric no longer works. So assume that we have a data set with one over 1000 class distribution and a very naive algorithm, right, for every sample, just return the label of the majority glass.
00:31:36.690 - 00:32:34.000, Speaker A: So what would be the accuracy of this naive algorithm? 99.99%. Right? 99.99%. So that's why the accuracy metric no longer works for extremely imbalanced data. And while we're talking about matrix, another metric that does not work for AML data either is f one score. Right? So this is the definition of f one score, I think all of us know here. Now, this problem is specific to the AML domain. Why? Because in AML, as we see from own, those penalties, the cost of a false negative, false false negative means a money laundering transaction misclassified as legitimate.
00:32:34.000 - 00:33:22.952, Speaker A: The cost of a false negative is much higher than the cost of a false positive. That's why we have those 4% 96% false alert rates. However, f one score favors a balance of recall and precision. Basically, it favors a balance of false negative and false alert rates. So that's why it doesn't work for AML data either. So we will see a very low s one score for AML machine learning model. And then those challenges are compounded, because now we have a lack of benchmark data sets.
00:33:22.952 - 00:34:20.130, Speaker A: We don't have good data set. And then on top of that, there's a lack of reliable baseline models for comparison. And when I say there's a lack of baseline model because we did a review of existing paper, Nazanin and myself, my master's student, and the majority of the papers we surveyed at that time did not use proper data handling or metric, for example. So Nazalin wanted to compare her model with existing model using the paysim dataset, because she used a paysim dataset. And these are the models that used the paysim dataset. And we copy here these numbers that we copy from their papers, their published numbers. And you can see here the accuracy.
00:34:20.130 - 00:34:52.868, Speaker A: Remember we said that our naive trivial algorithm achieved 99.99% accuracy. In this case, you can see we have 88%, 81%. We don't know how to interpret these numbers. F one score. Again, we should not have a high f one score because the cost of false negative is much higher than cost of false positive. Only one paper would give us the four negative rates.
00:34:52.868 - 00:35:26.880, Speaker A: Basically, for every 100 money laundering transaction, about 2.57 would be undetected. So this is a good number in the right direction. So for that reason, we cannot do any comparison with these models. We don't know whether our outer model is better or worse than this because we cannot interpret the accuracy or f one score results. We are doing better than 2.7. We'll see later, at least for this model.
00:35:26.880 - 00:36:35.510, Speaker A: All right, so now we're talking about how to handle imbalanced data. For training machine learning model, we can use resampling under sampling by removing samples from the majority class or oversampling by duplicate random samples from the minority class. Under sampling this way may cause loss information, and over sampling this way may cause overfeeding. So another approach is to generate synthetic positive samples, or we call it data augmentation. In order to instead of oversampling, we can use data augmentation. And there are potential machine learning techniques for this. As I mentioned earlier, again, VAE can be used to create additional samples, money laundering sample and we can also handle the glass imbalance problem at the model level as well.
00:36:35.510 - 00:37:37.960, Speaker A: So in the machine learning model, we can have something called cost sensitive learning. Basically, we penalize wrong prediction of the minority class with a higher cost than the penalty for wrong prediction of majority class. Or we can use ensemble learning. Basically combine different machine learning models and take the aggregate result from them. And in terms of metrics so far, what we have been focused on is false negative rate and false alert rate and false negative rate is related to recall, which is one minus. Recall is one minus false negative rate, and false alert rate is one minus precision, because recall and precision are used more frequently in the machine learning community. All right, so now we're talking about learning algorithms, and I'll go through them very quickly.
00:37:37.960 - 00:38:52.542, Speaker A: So the traditional models used for AML would be supervised and unsupervised models. And these are the surveys where you can get more information about how to use these implement AML model one caution though, because as I mentioned earlier, they may not use proper data handling or metric. And deep learning has also been used for money laundering, but mostly in the context of fraud detection. Deep learning methods such as autoencoder transformer and GAN have been used in this context. Now, fraud detection is a bit different from AML. Why? Because fraud detection, with fraud detection, a missed fraud transaction like a false negative, is the cost of that transaction. So let's say we have credit card fraud detection.
00:38:52.542 - 00:40:05.510, Speaker A: If we miss that fraud transaction, what do we lose? We lose the amount of that transaction. So let's say if we lose $1,000 on that transaction and we miss 100 transactions like that, how much do we lose? $100,000. Now imagine with AML, if we missed 100 monthly long term transactions, that could be millions of dollars in fine. So with fraud detection, the threshold for adjusting the number of false positive or false negative is different from the threshold for adjusting the number of false negative for AML, because again, the cost of missing fraud transaction is different from the cost of missing money laundering transaction, much lower. And graph based machine learning have also been used as well. And why graphs? Because money laundering is most of the time a group activity. It's a team effort with money laundering.
00:40:05.510 - 00:41:07.714, Speaker A: And graphs can capture the relationships between those members, the transaction, the account. That's why graphs is a very powerful tool in machine learning for AML. And these are the typical methods such as clustering, graph neural networks and graph convolutional networks that have been used for AML. So now we talk about our proposed graph convolutional network model that my master's student, Nazanin Bashi Najat, developed during her master's thesis. She graduated last year. So the objective is to develop a deep learning model for AML transaction monitoring. And given a set of transactions, we classify them into normal or suspicious transactions.
00:41:07.714 - 00:42:11.070, Speaker A: And the goal is to reduce the number of false negatives, ideally to zero, and to keep the false alarm rates as low as possible, under 90%. And we use, like I said we use graph and GCN. So these are the design issues. In order to capture the relationships among customers, accounts and transactions, we use not two back embedding to handle the glass imbalance problem. We use over sampling, using smoke and under sampling, using numerous under sampling. And the metric would be false negative rate, or equivalently recall and false alert rate as equivalent precision. And in order to achieve near zero false negative rate, and minimizing the false alert rate, we fine tune the learning threshold, as we'll talk about later.
00:42:11.070 - 00:42:42.010, Speaker A: And so this is the training process. We do oversampling under sampling, and then from the sample we generate embedding vectors using node two vec, go through the GCN and provide the output for the classification. Okay, so I'm going to skip this one. I will come back later if you have questions about this. The graph complement itself. So now I would like to present the experimental result. It's more interesting.
00:42:42.010 - 00:43:16.290, Speaker A: So these are some settings. I don't have all of them, just some. The most important setting. So basically, the class distribution of this data set is about one over 1000. And these are the features, most important features. And these are the parameters for node two, Vec and for the GCN. All right, so now here we show how glass distributions affect the performance of the model.
00:43:16.290 - 00:44:15.188, Speaker A: There are many numbers, but I would like to focus on this column, basically the number of false negatives. And this one here is the false alert rate. Remember, we tried to reduce this number to zero, and we try to minimize this number, but again, we can afford to go up to, let's say 90%, right? And you can see that the glass diffusion here is one over 1000. And it increases until we have a balance ratio. And the number of false negative has been reduced from 26, gradually going down until we get to the balanced data set with zero. Now, the false alert rate, obviously it will suffer because the gain from this will cause the loss from this one. So the false alert rate will go up from 20% to around 64%.
00:44:15.188 - 00:44:48.130, Speaker A: But again, 64% here is still better than 90%, industry standard. Our goal is to have this number as small as possible. So in this case, we achieved zero. Now, in practice, we cannot always get zero with the machine learning model, but in this case, we happen to get zero with this number here. Now, on this side here, I put two numbers. This is the f one score. So as you can see, the f one score for this case, 83%.
00:44:48.130 - 00:45:18.494, Speaker A: And this one is 53%. It's very low for the hour scenario, because again, we don't want to balance the false negative rate and the false alert rate. We don't want to balance that. We don't want to balance precision and recall. Right? Recall in this case 100% precision is only about 36%. We don't want to balance this. And this is a threshold fine tuning.
00:45:18.494 - 00:46:14.660, Speaker A: So basically a threshold of 50% is basically a random classifier. In this case we vary the threshold and we achieve the threshold around 0.32. This is where when we get the lowest number of force negative and we achieve the around 64% of force alert rate. Okay, so, and these are the parameter setting for node two vec. In terms of number of walk, we choose 15 because it gave us the best performance number of walk lengths, 32 would be the best in this case. So again, these numbers depend on the data set that we use for training, right? So for our case it's 15 and 22. Another data set may have different values.
00:46:14.660 - 00:47:09.664, Speaker A: And we also compare GCN with the traditional machine learning methods. So again, we cannot compare with existing work because they don't give us valid data to compare. In this case, we compare with random forest logistic regression and SVM. And in all cases the false negative rate of hours is better, much lower, and the false alert rate as well, better than the traditional machine learning method. So basically the cost of using deep learning pays off in this case. So in our future work, we'll fine tuning and evaluating the model using better data sets. Because like in order to validate the performance of the model and I, we were thinking of using these from our review.
00:47:09.664 - 00:48:08.370, Speaker A: These would be the best model for our case. We can improve DCN, for example, using fast GCN to improve the speed, combining this model with rule based transaction monitoring and study more metrics for comparing with the baseline. All right, so far the talks have been to identify the challenges with machine learning for AML transaction monitoring. That could be a lack of publicity. Available real data sets and synthetic data have many limitations. AML data are extremely imbalanced, so that requires proper metadata methodologies at the data level and model level as well. We also require proper evaluation metrics.
00:48:08.370 - 00:49:03.466, Speaker A: And then there's also a lack of benchmark data set and reliable baseline for comparisons. On the other hand, we have many opportunities provided by AI and machine learning. They can be used to generate synthetic data. We can apply privacy preserving techniques in the process in order to protect the privacy of the users. We can use AI and machine learning to augment the data in order to generate more sample positives for training. And they can be used to complement rule based transaction monitoring, deep learning and graph based learning have shown potential to reduce and false alert rays and to detect new or complex machine learning patterns and it's very important. Explainable.
00:49:03.466 - 00:50:08.098, Speaker A: AI is critical to ensure adoption of machine learning for AML compliance. And again, AML should be complement, it cannot be replaced rule based transaction, at least for a near future. So I would like to thank my master's student Nazanin Bashir Nejad who completed her master's thesis for this topic. My current PhD student Sharam Garimani who continue to work on the topic AnsERC and Scotiabank, York University and Lausanne School of Engineering for funding this research, funding the students. I also would like to thank field Institute for organizing this workshop and the organizer of the workshop as well. And finally, we appreciate the researchers. Their papers and data sets are the basis of our work and also the blocker as well.
00:50:08.098 - 00:50:14.850, Speaker A: From there I copy and pasted those diagrams. Very nice diagrams. And again thank you for the audience for your attention.
