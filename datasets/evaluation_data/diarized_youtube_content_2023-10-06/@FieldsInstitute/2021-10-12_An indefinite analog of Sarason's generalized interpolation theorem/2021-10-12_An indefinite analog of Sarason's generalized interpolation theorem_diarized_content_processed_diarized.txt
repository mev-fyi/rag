00:00:02.520 - 00:00:03.224, Speaker A: Okay.
00:00:03.326 - 00:00:04.094, Speaker B: Good morning.
00:00:04.214 - 00:00:32.502, Speaker A: Good afternoon. Good evening, everybody. Welcome to the colloquium of the week. I'm very honored to introduce you Professor James Rovniak, who's going to talk about an indefinite analog of Sarasohn generalized interpolation theorem. Before starting the talk, please let me emphasize that you please keep your questions up to the end of the lecture. Then we have your minutes to pose the question. Question.
00:00:32.502 - 00:00:36.914, Speaker A: Thank you in advance, Professor Rovniak, please.
00:00:39.014 - 00:00:39.954, Speaker B: Okay.
00:00:44.494 - 00:00:44.950, Speaker A: Okay.
00:00:44.982 - 00:02:02.030, Speaker B: Well, thank you very much. So I'll begin by fixing notation. If C is an analytic function which is defined and bounded by one on the unit disk, I will write H of C for the Hilbert space with reproducing kernel one minus c of z c bar of w over one minus zw bar. The space H of C is characterized as the space of all functions f of z and the Hardy space h two, such that the supremum of the norm squared of f plus cg minus the norm squared of g is finite, the supremum being taken over all g of c in h two. The value of the supremum, then, is the square of the norm of f of z in h of C. I would normally use the traditional h of b notation for these spaces, but throughout this talk, b will always denote a finite blaschka product. Besides kernel functions, h of C contains all difference quotients f of z minus f of w over z minus w, c of z minus c of w over z minus w.
00:02:02.030 - 00:03:03.904, Speaker B: Whenever f is in the space and w is a point in D, I will denote by t star the difference quotient transformation f of z into f of z minus f of zero over z. So t star is the adjoint of an operator. T itself is multiplication by z adjusted by a constant multiple of c of Z. The constant is a linear functional of f of z given by inner product with c of z minus c of zero over z. When c is an inner function, h of C is contained isometrically in h two as the orthogonal complement of ch two, and in this case, t is the compression of multiplication by z to h of cub. I will also speak of functions of t for any h infinity function f. The nauseen Feuer's functional calculus defines f of t as the strong limit.
00:03:03.904 - 00:03:55.454, Speaker B: As r increases to one, the sum fj r to the j t to the j. The fjs here are the tailored coefficients of f of Z. This has nice properties. It's an algebra homomorphism and obeys this bound. The same formula can be used for any completely non unitary operator on a Hilbert space. In particular, if S is multiplication by z on h two, f of s is multiplication by f of z. When c is an inner function, t is the compression of s to h of c, and it can be shown that f of t is the compression of f of s to h of C, and this is the form of the functional calculus used by Saracen.
00:03:55.454 - 00:04:57.334, Speaker B: So here is Saracen's generalized interpolation theorem. Let c be an inner function. If r is a bounded operator on h of C that commutes with t, then there's a function f in h infinity such that r is f of t. Moreover, f can be chosen such that the h infinity norm of f is the operator bound of R. In particular, when r is a contraction, we can choose f to be a sure function. The sure class S naught is a set of analytic functions that are defined and bounded by one on the unit disk. The generalized sure class S kappa is a set of quotients f over b, where f is a sure function, b is a Blaschka product of degree kappa, and f and b have no common zeros, and pairs of this type will play a prominent role in the talk.
00:04:57.334 - 00:06:17.994, Speaker B: So in this talk I will replace the condition that r is a contraction by the condition that one minus r r has kappa negative squares, which I'll denote in this way. Kappa is always a non negative integer. The meaning of this condition is that the negative spectrum of the self adjoint operator one minus r r consists of a finite number of eigenvalues of total multiplicity kappa. So I'll begin by showing that in Saracen's theorem, if one minus rr has kappa negative squares, then r satisfies an operator identity b of t r equals f of t, where b is a Blaschka product of degree kappa and f is a sure function. This will use an abstract interpolation method known as Newdelman's problem. The rest of the talk then will be a study of the operator identity b. The identity makes sense on more general h of c spaces, and that's how I will consider it.
00:06:17.994 - 00:07:29.734, Speaker B: To proceed, I will need some preliminaries on spaces h of c, especially the difference quotient identity and root subspaces for the operators t and t. The main result will show that the operator identity determines r on a subspace of co dimension at most kappa, and on this subspace r is given by an explicit formula depending on the Blaschke product and the Schur function b. So here is Newgemann's problem. I start with a complex vector space v and a linear mapping a on v into itself. We define a dual mapping a prime on the dual space d prime into itself. By this relation, the left side is the action of x prime on ax, the right side the action of a prime x prime on x. I also choose a subspace d of the dual space which is invariant under a prime.
00:07:29.734 - 00:08:04.114, Speaker B: And here's the problem. We let b and c be fixed vectors given vectors, and we choose a non negative integer kappa. And we want to find a sure function f and a Blaschka product b of degree kappa such that b of ab equals f of ac. Okay, so that's a formal relation. It's interpreted as follows. We expand. Think of expanding b of a and f of a as power series in a.
00:08:04.114 - 00:08:54.314, Speaker B: So the bj's here are the coefficients of b of z. The fjs are the coefficients of f of z. Apply that to the vectors b and c, and then apply some linear functional in the set d to both sides. The condition then is that these series converge and they're equal for all x prime and d. Now, in order to show that such functions actually exist, we need a strong hypothesis. And the strong hypothesis is this. Namely, we assume that for all linear functionals in our distinguished set d, these sums are finite and they obey a bound of this nature.
00:08:54.314 - 00:09:36.972, Speaker B: So we assume that that is satisfied, and then we have the following abstract theorem. The problem admits a solution. If the kernel on d cross d, defined in this way k of x prime y prime is equal to such a sum that kernel should have kappa negative squares. Conversely, if a solution exists, then the kernel has at most kappa negative squares. I'll define this notion for kernels. I defined it for self adjoint operators earlier. For kernels, we assume that the kernel is hermitian.
00:09:36.972 - 00:10:39.424, Speaker B: That is, it obeys a symmetry relation like this that makes the matrices down below be self adjoint. So a hermitian kernel on a product space Omega cross omega has kappa negative squares. If among all matrices of this form, depending on arbitrary points w one through wn omega, the maximum number of negative eigenvalues counting multiplicity is kappa. This theorem appears in my paper with Daniel Alpe and ad Dyksman. It's based on a kind of commutant lifting theorem which was pioneered by Ball and Helton. The particular theorem that we use appears in a paper by Arcinia, Azizof, Dijksma and Marcontini. I should add that the case kappa equals zero has both.
00:10:39.424 - 00:11:26.644, Speaker B: A simpler statement and proof. And that's given in my book with Marvin Rosenblum. Here's an instructive example. Picnib and linen interpolation let z one through zn be distinct points in d w one through wn. Any complex numbers define the pic matrix p in the usual way. If p has kappa negative squares, then there's a sure function f and obliged to product b of degree kappa such that b of zj wj equals f of zj for all j. Conversely, if such f and b exist, then p has at most kappa negative squares.
00:11:26.644 - 00:12:02.156, Speaker B: Now I want to point out one disturbing feature of this relation. It may occur that for some j, b of zj is zero. If that happens, f of zj has to be zero. So wj is zero over zero, which makes no sense. So what's going on here is that interpolation can fail at such a point. And since b has degree kappa, interpolation can fail for up to kappa points. And this is actually common.
00:12:02.156 - 00:12:40.344, Speaker B: It's typical for indefinite interpolation. There are always dimensions where you are not entitled to know what you would like to know. This follows from the abstract theorem. By choosing the vector space to be cn and a is the diagonal matrix z one through zn. These are the fixed vectors for d take all linear functionals. If you work through the theorem, there's straightforward calculations. You come up with precisely this statement.
00:12:40.344 - 00:13:20.254, Speaker B: Okay, but I'm more interested in the Saracen theorem here. So, and this is my theorem a. Assume c is inner. Let r be an operator on h of c that commutes with t, and in addition, one minus r r has Kappa negative squares. Then there exists a sure function f and Blaschka product b of degree Kappa such that this operator identity is satisfied. R satisfies an operator identity. And conversely, if such f and b exist, then one minus rr will have at most Kappa negative squares.
00:13:20.254 - 00:14:05.598, Speaker B: I allow kappa equals zero in this statement. In this case, notice that for any r not zero that commutes with t, then we can apply theorem a to r over the norm of r. That's a contraction. For a contraction, Kappa equals zero. And if you work through that statement, in that case, you recover Saracen's original theorem. Okay, let me just sketch how this follows from the abstract theorem. For the fixed linear mapping, we choose t itself acting on h of c into itself the fixed vectors.
00:14:05.598 - 00:14:55.684, Speaker B: We take the kernel function at the origin, and then we take what r does to the kernel function at the origin. For the distinguished set of linear functionals, I choose all continuous linear functionals on h of c. So to apply the theorem, we have to show that d is admissible. And what that means is that we have these inequalities. And the way we do that is by computing these sums. And that's easy in this case, because linear functionals are given by an inner product with some fixed vector. So in this first sum, I have the inner product t to the j kernel function at the origin, inner product with some fixed vector in h of c.
00:14:55.684 - 00:15:58.544, Speaker B: Just bring the t to the j over to the right as t star to the j that will shift k j steps. Then, when we take the inner product with the kernel function at the origin that picks off the j Taylor coefficient, take the modulus squared, add those, that's the h two norm of kick. But since c is an inner function, that's the same as the h of c norm. Okay, in the other case, it's the same, except we have r in here, but r commutes with t, so we can slip it off to the left and then bring it over as r star. And now it's exactly as before, but just with k replaced by r k. So the admissibility condition now just says this. And that is satisfied because r is a bounded operator.
00:15:58.544 - 00:16:54.094, Speaker B: Okay, a similar calculation shows that this fearsome looking kernel reduces to something very simple. Namely, it's just an inner product, one minus r r k with hook. And since we assume that one minus r r star has Kappa negative squares, that implies that the kernel has kappa negative squares. So the conditions are met. And then the abstract theorem produces a sure function f and blaschka product b of degree kappa that satisfy this relation. And we set this up so that b is rc. So what this tells us is that b of t r agrees with f on this one function, the kernel function at the origin.
00:16:54.094 - 00:17:57.294, Speaker B: But the kernel function at the origin is cyclic for t. And from that it's easily shown that b agrees with f on full space. And that's theorem a. Okay, so what do we have here? What's been accomplished? What remains to do? Well, by theorem a, in Saracen's theorem, if one minus r r has Kappa negative squares, then r satisfies an operator identity b of t r equals f of t for some pair f and b. And the question is, but what actually does this tell us about r? There's nothing here that says v of t is invertible if it has a kernel. This is a problem. Well, it turns out that this question is conveniently discussed by methods that apply to more general h of C spaces, that is c not necessarily inner.
00:17:57.294 - 00:18:47.826, Speaker B: And it's natural to consider the question in more general h of C spaces, and that's exactly what I'm going to do. So here's the problem. I'll let r be a bounded operator on some space h of c. C might be inner, but not necessarily inner. I assume that r commutes with t and satisfies an operator identity of this form for some sure function f, and blush the product b of the greek kappa. And I want to know what can we conclude about r in this case? Well, this is the topic now for the rest of the talk. Okay, so I need preliminaries on spaces hc.
00:18:47.826 - 00:19:36.584, Speaker B: So now I'm just going to talk about h of c spaces for a while. Recall that h of C is the Hilbert space with reproducing kernel one minus c of z c bar of w over one minus cw bar. It's well known that the difference quotient inequality holds in every space h of c. That's this inequality norm h of z minus h of zero over z squared less than or equal to the norm of h squared minus absolute h of zero squared. And in a large class of spaces, the difference quotient inequality is always an equality. The theorem the difference quotient identity. So that's this relation, but with the equality sign that holds for every h of z in the space if and only if c does not belong to h of C.
00:19:36.584 - 00:20:25.114, Speaker B: O'Sherri at Saracen showed that c does not belong to h of c if and only if c is an extreme point of the unit ball in h infinity. And there are a number of equivalent conditions, other equivalent conditions and viewpoints on the whole subject. These are discussed in survey by Balin Balatnikov. This was published in the volume operator theory that was edited by Daniel Elpe, published by Springer. Now, I definitely want the difference quotient identity to hold. And so from now on I'm only going to be talking about spaceship. Such a c does not belong to h of C, or equivalently I'm assuming the extreme point case.
00:20:25.114 - 00:21:31.924, Speaker B: Why do I want the difference quotient identities? Well, it turns out that the difference quotient identity facilitates calculations involving this operator. This is the main object of interest here. Now, to know anything about this operator, we have to know something about inner products of this type. So how do we form inner products where one term is c of z minus c of zero over z? And quite often these involve inner products where the h is replaced by a difference quotient either this type or this type. Well, it turns out that there are two useful formulas that occur in the original square summable power series book, which I'll advertise here. Problem 88. You can forget the exact terms here, but notice we have an inner product of such a type with here now, h of z minus h of alpha over z minus alpha.
00:21:31.924 - 00:22:21.194, Speaker B: And actually notice also that when beta is zero, that's the case that interests us. When beta is zero, this term drops out. So it simplifies quite a bit. Then problem 89 is the same thing, but with the difference quotient of a different type. These identities appear also in my book with Alpe, Dixma and de snow by a different method. I want to write them down again in the case beta equals zero, because that's the only case that actually comes up here. I've just written down the two previous identities with beta equals zero, and I replaced the alpha by w.
00:22:21.194 - 00:22:58.540, Speaker B: Again, you don't have to know these formulas, just that they exist. I want to show an example of how this is used. Results that I'm going to state involve a lot of elementary calculations. And those two identities just come up from time to time. And here's one example. Suppose we have a space h of c. C does not belong to h of c, and I have a point w in the unit disk, which is not a zero of c.
00:22:58.540 - 00:23:54.084, Speaker B: Then I claim t minus w is invertible and its inverse is given by this formula. This formula is equivalent to a formula that Saracen gives in his book in a very different notation and by different methods. Here's how we can prove it using those two formulas. I want to show that the right hand side defines an operator which is both a left inverse and a right inverse to t minus w. So choose some h of z in the space and let k of z be that expression. Then by algebra, this is a linear combination of these two types of difference quotients. Okay, so now I apply t minus w to k of z by the formula for t.
00:23:54.084 - 00:24:28.466, Speaker B: That's z minus w, k of z minus c of z, inner product of k of z with c of z minus c of zero over z. Well, so then I just substitute this linear combination for k of z in the inner product. Those two formulas on the previous slide, evaluate both of them. You write it down, it looks bad, but you do the algebra and it just simplifies to h and c. So that shows it's the right inverse. The left inverse is similar. Actually, even easier.
00:24:28.466 - 00:25:07.304, Speaker B: You just write it down, then it drops out. You don't even need those two formulas. Okay, this is standard terminology. If a is a Hilbert space operator alpha an eigenvalue, the geometric multiplicity is the dimension of the kernel of a minus alpha. Any non zero vector f which is annihilated by some power of a minus alpha is called a root vector. The set of all root vectors plus zero is a subspace. If that subspace has finite dimension, we call that the outbreak multiplicity of a.
00:25:07.304 - 00:26:02.224, Speaker B: Okay, the eigenvalues and eigenfunctions for t and t were studied by Saracen. They're discussed in the books by Firkin and Mashregi, and in the inner case also Garcia, Mashrage and Ross. Also in the inner case, the root subspaces are described in Nikolsky's treatise on the shift operator. Suppose we're given a space agent C c, not an agent c. That is the extreme point case. Then the spectrum of t in the open unit disk consists of isolated eigenvalues which occur at the zeros of c. If alpha is a zero of c and d, then alpha is an eigenvalue for t, and the kernel of t minus alpha is the span of c of z over z minus alpha.
00:26:02.224 - 00:26:59.414, Speaker B: Alpha bar is an eigenvalue for t star and kernel of t minus t star minus alpha bar is one over one minus alpha bar z. I use brackets to denote linear span brackets around any set of linear vectors as the linear span. Okay, so we can say more. Suppose again that alpha is a zero c, and now let n be its order. Then we define n functions qj of z q j of z is c of z over z minus alpha to the j j running from one to n. So then t minus alpha q one is zero, t minus alpha, qj is qj one four j between two and n. So in particular q one through qn are root vectors.
00:26:59.414 - 00:27:48.314, Speaker B: Let r, sub k be the kernel of t minus alpha, the k. Then r, sub k is the span of the first k of these root vectors. For k between one and nice for k bigger than n, you get nothing more and it's rn. So therefore the geometric multiplicity of alpha is one, the algebraic multiplicity n, and rn is the root subspace. I want to show a consequence of this that will be useful to us later. Suppose I have an h infinity function f, and suppose that f annihilates one of these root vectors. Then I say f has a zero at alpha of order at least k.
00:27:48.314 - 00:28:38.034, Speaker B: And the way we prove this is we start by expanding f of z as a Taylor series about the point alpha. When we get to the point z minus alpha to the k, lump the rest of it into the function g of z. F of t is the same expression, but with z replaced by t, because the functional calculus is an algebra homomorphism. Okay, so I want to apply f of t to q, sub k. Well, f of t applied to q k is zero by assumption. So I get zero f of alpha applied to q, sub k is f of alpha qk. Then f prime of alpha t minus alpha qk.
00:28:38.034 - 00:29:29.824, Speaker B: Well, that's now qk minus one, and so on down the line. But the q's are linearly independent. That means that all these coefficients are zero, and that says that f has a zero at alpha of order, at least k. Proposition two is an exactly parallel statement for t. Now I define pj of z as z to the j minus one over one minus alpha bar z to the j. T star minus alpha bar p one is zero, t star minus alpha bar pj is pj minus one for j between two and n. I define now rk Tilta to be the kernel of t star minus alpha bar to the k.
00:29:29.824 - 00:30:29.954, Speaker B: Then that's the span of p one through pk for k between one and n, and for k bigger than n, it's rn Tilta. And then we get the same conclusions about the geometric multiplicity and algebraic multiplicity. Now the functions p one through pn have a different representation, which turns out to be useful. Namely pj of z is r of alpha star to the power j minus one, acting on the kernel function at alpha for j between one and n. And here r of alpha is the difference quotient operator. That takes a difference quotient at the point alpha. And what is going to be useful to us later is the information that this provides about the orthogonal complement of this kernel.
00:30:29.954 - 00:31:25.574, Speaker B: For something to be orthogonal to this kernel, it has to be orthogonal to all of these functions. But I can bring over the r star to the j minus one, bring it over to the left, and then the instruction is you form j minus one difference quotients at the point alpha. When you do that and enter product with a kernel function at alpha, that gives you the Taylor coefficient. So all those have to be zero. And that tells us what this kernel looks like. I need one other property of these kernels. Let's suppose now that we have our distinct zeros of c alpha one through alpha r, I pick arbitrary positive integers m one through mister.
00:31:25.574 - 00:32:18.104, Speaker B: And now I look at the powers t star minus alpha 1 bar to the m one, t star minus alpha r bar to the mister, and I multiply those together, and I want to know the kernel of the product. Everything on the left commutes, and so that will certainly include the sum of the kernels of the individual factors. Now, connectivity alone is not enough to conclude that you have equality. Equality depends on the special nature of these kernels, which, by the way, consist of rational functions. The proof proceeds by induction on the number of factors. We look at the ways in which we can increase the number of factors by one. We could add a new zero.
00:32:18.104 - 00:33:10.566, Speaker B: The only other way then, would be to increase one of the exponents to mj plus one. In that case, it turns out you have two subcases depending on whether mj plus one exceeds the order of alpha j as a zero of c or not. So you write this down and you discover that in each case there's a judicious partial fraction decomposition that verifies the equality. Okay, well, those are the preliminaries. And so now we're ready to look at the operator identity, and I'll restate the problem. Let r be a bounded operator on a space h of c c, not in h of c. That is the extreme point case.
00:33:10.566 - 00:33:58.464, Speaker B: Assume that r commutes with t and satisfies the operator equality. B of t. R equals f of t, where f is a short function and b is a Blaschke product of degree Kappa. I want to know what's the form of R? Is r even uniquely determined? Well, here's an easy case. Let's suppose that the zeros of b do not overlap with the zeros of C. Well, then what is b of T? Well, b of T will be a product of linear fractional factors that look like this, and the beta is a zero of B. But if it's a zero of B, then it's not a zero of C.
00:33:58.464 - 00:34:57.354, Speaker B: If it's not a zero of C, then t minus beta is invertible. And, well, if t minus beta is invertible, then b is invertible. So we can just multiply through by b of t inverse, and that's the end of the story. Okay, but in general, the zeros of B will overlap with the zeros of C. So I'll let alpha one through alpha r be the distinction points at which that occurs. I also need the orders of the alphas, both as zeros of b and zeros of C. So m one through mister will be the orders of the alphas as zeros of b n one through nr, the orders of the zeros of the alphas as zeros of C.
00:34:57.354 - 00:35:54.604, Speaker B: Then I factor b of z into three parts. So think of this as three boxes here. And now, b of z is a product of linear fractional factors. So I pull out these factors one at a time, and I look at them, and depending what I find, I throw them into box one, box two, or box three. So first of all, if the zero, if the factor that I pulls out has a beta, which is not a zero of C, they all go into b one. Then if I pull out one of these factors and it is a zero of C, well, that goes into b naught. And I keep doing that, but I stop when I reach the order k, sub j.
00:35:54.604 - 00:36:36.760, Speaker B: It might be that b zero of z has too many zeros. So I throw all of those factors into b zero, but only up to k, sub j, which is the minimum of mj and nj. So this means that when I've disposed of all of those, there still might be some linear fractional factors in B. Well, they're going to have to look like this, and they get thrown into b two of z. Okay, and then here's the theorem. Let r be a bounded operator on h of c. Extreme point case.
00:36:36.760 - 00:37:40.234, Speaker B: Assume that r commutes with t satisfies this identity where f is a sure function, b is a Blaschka product of degree kappa. So we factor b one b into b one b zero b two as just described. Then I define k as the subspace of all functions h of z and h of c that vanish at each alpha j, along with derivatives up to the order kj minus one. Then I assert that has co dimension. That's a subspace. It has co dimension at most kappa, and it's invariant under both t and r. And the restriction of r to k is a function of the restriction of t to k, and explicitly is given as b one of tk inverse g of tk, b two of tk inverse.
00:37:40.234 - 00:38:25.786, Speaker B: So the b one and b two, that's as before. What is the g? Well, the g is f over b naught, and the proof will show that f over b naught is a short function. And the proof also shows that b two of tk is invertible. B two of t itself is not invertible unless it's trivial, but restricted to k it is invertible. Okay, so I'll show some steps in the proof here. First, that f over b naught actually is a sure function. We're starting off with this operator identity.
00:38:25.786 - 00:39:15.934, Speaker B: And I have a factorization. So that tells me be one of t, be not of t, b two of t r equals f of t. So all factors on the left commute and b one of t is invertible by what we were, I was discussing before. So I'm going to move the b one of t over to the right, get that out of the way at the start. And the remaining factors I can write in any order, and I write them with the b naught of t on the right hand side, because now we focus on b naught of t. So b naught of t, well, you just go back to b naught of z and replace z by t. So that's this.
00:39:15.934 - 00:39:58.644, Speaker B: And so now we can see clearly what the kernel of b naught of t is. Namely, it's the kernel of t minus alpha j to the kj. And that kernel is the span of q one through qj. So in particular, b naught of t annihilates q, sub j. And now from this equality, if b zero of t annihilates q, sub j, then f of t annihilates q, sub j. And we had a corollary about that. The corollary said, but then f has a zero at alpha j of order, at least kj.
00:39:58.644 - 00:41:02.774, Speaker B: Okay, now, by Schwartz's lemma standard function theory, we can divide out that factor b naught of z is the product of all such factors. And so we see that f over b naught is a sure function. Okay, so now where does that take us? Well, this is the same left hand side that I had before. On the right hand side, I replaced f of t by g of t, b naught of t. I cannot cancel the b zero of t. What the equation tells us is that the restriction of r b two of t to the range of b naught of t is the same as the restriction of b one of t inverse g of t to the range of b naught of t. So what is that range? Well, the closure of the range is the orthogonal complement of the kernel of the adjoint.
00:41:02.774 - 00:41:52.932, Speaker B: The kernel of the adjoint we saw was the kernel of this product. And in proposition four then tells us that that kernel is the sum of the individual kernels. And then proposition three told us what was the orthogonal complement of the individual kernels. Namely, it was these functions satisfying these conditions. So when you put all that together, the closure of the range of b naught of t is the subspace k that I described in the statement of the theorem. So from this formula, it's clear that k is invariant under t. It's essentially the range of b naught of t.
00:41:52.932 - 00:42:44.474, Speaker B: And we also see that the co dimension is at most k. And what we have thus far then, is that r b two of t restricted decay is b one of t inverse g of t restricted decay. And so from this it's a short step to replace t by t restricted decay. And then what remains is to move the b two of tk over to the right hand side. Well, we can do that. If we could show that b two of tk was invertible. And that's actually kind of a neat argument, but it requires some careful bookkeeping, and I'm going to omit that.
00:42:44.474 - 00:44:01.966, Speaker B: Okay, but then, granting that, granting that, then we obtain the formula that was in the statement of the theorem. So theorem B follows. Okay, so finally then we can apply, we can go back and apply this to the original Saracen theorem. The original problem that I was concerned with, and so what I'm going to do here actually is, okay, I'm going to apply the theorem to the case of an inner function, and more precisely, I'm going to restate the theorem with the same conclusion but a different hypothesis. The hypothesis now is r is an operator on h of c and c inner. So I'm specializing to c inner. Again, r commutes with t and satisfies this condition.
00:44:01.966 - 00:44:44.852, Speaker B: One minus r r star has kappa negative squares. Well, now we can apply theorem a by theorem a. There's a short function f and a Blaschka product b of degree kappa such that b of t r is f of t. Okay, so that's the operator identity that we had in theorem b. We factor b in the same way. I define k as the subspace of all functions h and h of c, which vanish at each alpha j along with the derivatives up to the order kj minus one. Then code dimension of k is at most kappa.
00:44:44.852 - 00:45:35.584, Speaker B: K is invariant under t and r. And the restriction of r to k is a function of the restriction of t decay given by the same formula. Okay, and that actually is the final result and the end of the talk. I want to just close with one remark. It's evident from the statement of the theorem that in general, k is not c. In other words, we cannot know r completely. And as I mentioned before, this is expected behavior when you're doing indefinite interpolation.
00:45:35.584 - 00:46:26.814, Speaker B: And we saw that in the Pyck nemenlin interpolation. If in that case b of zj was zero, there was a one dimensional space that you could know nothing about. So we expected this. But now, what is unusual here is that here is an example of indefinite interpolation where that does not occur, where you know it completely. Namely, when C is a singular inner function, has no zeros. Well, then automatically b of t will look like this, and beta will not be a zero of C because C has no zeros. So these are now always invertible.
00:46:26.814 - 00:46:46.274, Speaker B: So then in every case, R is completely known. K is all of H of C, and R is bft, inverse fft. Okay. And that is the end of the talk. And I thank you for your attention.
00:46:48.374 - 00:46:59.354, Speaker A: Thank you indeed. Let's thank Jamie first. Any question or comments?
00:47:03.914 - 00:48:00.854, Speaker B: Jim Bill Ross, what happens if the zeros of B are exactly equal to the zeros of C? Okay, let me think. The zeros of B are exactly equal to the zeros of C. Well, this certainly occurs, for example, with interpolation. Yeah. So I'm thinking something like C of Z is a power of z z to the n b of z. Let me go back to my theorem here. Let's go to this version.
00:48:06.514 - 00:48:09.014, Speaker A: You have just one factor, b naught.
00:48:09.474 - 00:48:56.938, Speaker B: Yeah. Right. You have b not then I think you don't know anything because you have no b one and b two. The range of. Let's go back to theorem B. So, Bill, we were saying the zeros of b are with us. What about orders? I'm not phrasing my question.
00:48:56.938 - 00:49:47.564, Speaker B: Right. I'm sorry if I'm disturbing the apple card here, but. No, no, that's okay. I mean, let's say they're all. Let's suppose the zeros are all simple or something like that. Well, I mean, if the zeros are simple, you just have these. You just have a single condition just looking at the functions that vanish at these points.
00:49:47.564 - 00:50:57.396, Speaker B: And now it depends on what C is. I mean, if C is, let's say, twelve dimensional, and, you know your Kappa is three, you're going to cut out three dimensions. There would be nine dimensions where you would know something, but there would be three dimensions where you could not. And somewhere in my notes, I have an example, I think, where the kappa is one, and indeed, r is not uniquely determined. But there's a one dimensional space in which there's a parameter that can be assigned arbitrarily, that there would be infinitely many r's. There would be one parameter family of r's that would satisfy the condition. So, in other words, the indefiniteness here represents a part of the problem where you just cannot know.
00:50:57.396 - 00:51:14.034, Speaker B: The answer is like the Pycnev and linear interpolation. If one of those. If b of Zj is zero for some j, that wj could be anything. There's a one dimensional space where you don't know what people were thinking. Okay.
00:51:18.614 - 00:51:38.346, Speaker A: Jamie, one small question by me. The operator identities, like the one we see here in t, or in b, or any other one. At the very beginning, you said that we considered these identities in the weak sense or weak operator topology. Does it make sense to consider other.
00:51:38.410 - 00:52:24.484, Speaker B: Ways like it could? It certainly could. I think this. For me, this kind of goes back to the Newdemand problem, where we imposed a strong condition in order to guarantee that a solution exists. And I think I and my co authors, Daniel Elpe and Dijksmith, feel that the conclusion is probably true more generally, but we don't know what that more general condition would be. Yes, it's certainly possible that this could be weakened.
00:52:31.664 - 00:52:44.064, Speaker A: Any further questions or comments? If not, let's thank Jimmy again. Thank you very much.
