00:00:23.320 - 00:00:27.354, Speaker A: From the field, start from the field site. We're good to start whenever you're ready.
00:00:31.454 - 00:01:47.174, Speaker B: Okay, so welcome, everyone. We're very lucky to have Matt Johnson here from Google. And Matt and I have actually known each other since 2008. We were in the same PhD cohort at MIT, and at the time he was doing research on bayesian models and MCMC algorithms, but he was also working a lot on the side on functional programming. And at the time, I was very skeptical that that was going to turn into anything useful, but he went on to prove me wrong. So he did a postdoc with Ryan Adams at Harvard, where he was one of the main creators of Autograd, which was one of the first automatic differentiation frameworks for deep learning, probably the cleanest one, and it's what we use to teach Autodef in 413 and 20 516. And then after that, he became a research scientist at Google and created Jax, which is the.
00:01:47.174 - 00:02:20.624, Speaker B: The new framework that is gradually winning over deep learning researchers. And if you haven't used it, I think you really should. Because whenever my students rewrite their code in jacks instead of tensorflow and pytorch, they come back a couple of weeks later and say, hey, everything is just so much cleaner and it runs faster and fewer bugs, and everything just works better. So Matt is going to tell us all about jacks.
00:02:22.124 - 00:02:49.110, Speaker A: All right, thanks, Roger. That very generous intro. I will share a window. So I was sort of the delay getting started. I think the packets from my computer ran into some kind of a blockade at the border, some kind of trucker issue, but they seem to be getting through now. I did. So I have, like, a live demo prepared, and that's going to be my excuse if something goes wrong with the live demo that, like, the packets are still getting blocked.
00:02:49.110 - 00:03:38.910, Speaker A: Okay, hopefully you can see my screen with a slide here. As Roger said, this talk is about jacks. This is sort of a jacks like, intro and whirlwind tour. But if you've seen a talk like this before, there's definitely some new stuff in here also, so keep your eyes peeled. So, to motivate Jax, one way we like to start is to say, how would you write implement deep learning from scratch in python? So, as Roger was saying in grad school, what we had was numpy. And it turns out that you can actually express the computations involved in deep learning pretty well with numpy. So here's like a simple, fully connected neural network that just loops over weights and bias parameters and, you know, applies matrix multiplication and you know, element wise addition and non linearities.
00:03:38.910 - 00:04:17.814, Speaker A: And this is, you know, obviously a trivial, trivial toy example for a neural network. But idea is that, you know, just with Numpy, you can already start to express, you know, neural network kinds of computations. You can also express loss functions. So here's, you know, little squared error loss function. So, you know, if we could do this in, when we were in grad school, what was missing from this picture? Why didn't, why weren't we happy? Why have we been building new tools for the last ten years? Some of the things that are missing are things like accelerator hardware. So we need to be able to project these computations onto accelerators to be able to make these models bigger and more efficient. We also need to train these models.
00:04:17.814 - 00:05:24.950, Speaker A: We need some kind of automatic differentiation. And once we start to make our models especially big or maybe do fancier things in our architectures or in our optimizers, um, we might want to start asking for things like, uh, optimized compilation of our architecture or training algorithm, um, with end to end optimizations like optimizing memory layouts and this sort of thing, we might also start to ask for something like uh, hey, I want to be able to, um, do automatic vectorized batching to, uh, uh, make good use of my, of my hardware. I'll say more about that later if that's not clear what that means. Um, and then we may also want to scale up and parallelize over lots of accelerators using batch data parallelism, but maybe also fancier things like model parallelism or spatial data parallelism. How can we do all these things? These are missing from this picture, and that is what Jax is for. The hypothesis driving Jax is that numpy style programming model lets us represent a lot of the computations we want to do, but the missing pieces can be filled in with what we think of as function transformations. So the most familiar function transformation would be like auto diff.
00:05:24.950 - 00:06:03.594, Speaker A: So I want to transform this function into a new one that evaluates its gradient. But these other things that I mentioned, like end to end optimized compilation or automatic vectorized batching, or even scaling up to bigger supercomputers. With Jax, we cast all those things as function transformations and give them to you at your fingertips. So to show you what I mean by all that, I have a demo. Actually, I have two demo tabs. Someone pipe up if you can't see a demo and one. The difference between these two tabs is that one is running on a GPU machine and one is running on a TPU machine.
00:06:03.594 - 00:06:41.902, Speaker A: This is the TPU one and this is the GPU one. And if I knew how to make zoom poles, I would have made a poll so you could tell me which one you wanted to see. One of the reasons that I have these two demos open is that the exact same demo code runs on CPU machines or GPU machines. And so that's one of the people's favorite aspects about jacks, is that you can write sort of like hardware agnostic code to a large extent. You can even run things on CPU if you want to prototype locally before running on the big metal. I'm going to sort of arbitrarily choose and just run this on this TPU machine. This is just showing us like the cpu devices available.
00:06:41.902 - 00:07:00.750, Speaker A: There are like eight cores of this cloud cpu, maybe? No, you know what? Maybe I'll run on the GPU machine. Let's do GPU machine here. It's an eight GPU machine. We're going to start by programming just the first one. I think these are V 100s. Let's check if we do Nvidia smi. Yeah, somewhere it says here.
00:07:00.750 - 00:07:24.592, Speaker A: Yeah, V 100s. Okay, so we have like eight V 100 in this cloud machine. That's how we're going to run this demo. Great, so let's start with using Jack. So I'm just going to import Jax numpy as JNP. Jack provides this numpy API for expressing numerical computations. I'm going to make a big 5000 by 5000 flow through two matrix, and then you can just use Jackson PI to do numpy like operations on it.
00:07:24.592 - 00:08:09.102, Speaker A: So you can apply cosine, you can do a matrix multiply, you can do fancy indexing, all kinds of stuff. You can use Matplotlib and plot slices of your arrays or something like this. So it's really just regular old numpy if you like, but running on a GPU, how could you tell? Maybe I'm lying and this is just regular numpy and we're not running on a GPU. One way you could tell is that it's pretty quick. So here is doing 5000 by 5000 matrix multiply. This is a little micro benchmarking thing. You don't usually have to write this, but this block, until ready is making sure that we're performing the correct timing because Jack's execution is asynchronous by default on some backends, like on GPU.
00:08:09.102 - 00:08:54.368, Speaker A: To make sure that we're actually waiting for this matrix multiply and finish, add this little block until ready. But here we're timing it and we're doing a 5000 by 5000 matrix multiply in 20 milliseconds. If you import the ordinary numpy as NP and you do that same matrix multiply, it's slower because cpu's aren't as fast as GPU's, at least at matrix multiplication. And so maybe it takes 200 to 300 milliseconds to do that same thing. So one way you can tell you're running on accelerators is just by things being performance, but otherwise it's just sort of the familiar numpy world. So what else does Jax have? That's sort of the Jax numpy piece that I mentioned. What about these function transformations? So the most familiar, most, maybe most important function transformation is automatic differentiation.
00:08:54.368 - 00:09:20.852, Speaker A: Jax has a lot of auto diff features. I'm just going to talk about. I'm going to focus on one, which is this reverse mode autodiff that we can access using grad. So here's a simple python function. You can see it has python control flow and all that kind of fancy stuff. And with grad, so think of Jax's function transformations as higher order functions in Python. So you give them a function like a Python callable and it gives you back a Python callable.
00:09:20.852 - 00:09:58.054, Speaker A: So here we're applying grad to f, we're giving it a Python callable and it's giving us back the value of this expression. Grad of f is itself a Python callable. If you give it a callable that evaluates some mathematical function, it will give you back a new callable that evaluates the gradient of that mathematical function. So here we can see we're just doing grad of f x at negative x. And you know, we're exercising both sides of that control flow. One interesting thing about having a sort of functional API to autodiff is it actually makes higher order auto diff much easier, including in this sort of trivial way where you can just keep applying grad to its own output. And in this case we're just differentiating this function many times.
00:09:58.054 - 00:10:30.068, Speaker A: No problem. So already with accelerator back to numpy and reverse mode auditive, we can do some deep learning. So here's that trivial toy neural network that I showed on the first slide, um, using Jackson numpy and sort of squared error loss function. Uh, so let's define that. And then this is just some uninteresting code to initialize some random data and totally, uh, random parameters, um, just so we can run it on something. So now that we have, uh, our parameters and uh, you know, a batch of random data we can evaluate. Our loss function is great.
00:10:30.068 - 00:10:59.262, Speaker A: That's just, you know, random, random neural network trying to predict random data. And now we can try to optimize the parameters of our own neural network against this loss function. So here we're just using grad of loss. That's that loss function we defined above. It differentiates with respect to its first argument by default. So we're getting the gradients of the parameters, and then this is just a simple loop that's going to go through and take a small step in the negative gradient direction. If we run that and then we print the new loss value, indeed, the loss has gone down, which is what we'd hope to do with gradient descent.
00:10:59.262 - 00:11:43.488, Speaker A: So this is actually like already a prototype for doing some deep learning, accelerator backed, numpy and reverse mode auto diff. So I mentioned grad is just scratching the surface as Jax's auto diff features. And as Roger said, I sort of got into this by working on Autodiff. And so Jax has a lot of really fancy autodiff, if you're into that kind of thing. I know Roger uses forward mode, I think, in his class on neural network training dynamics, but it's very good at composing up forward and reverse mode and doing very higher order, very high order differentiation, this sort of thing. If you want to learn more, we have some documentation, but actually we've only documented like maybe 25% of Jax's audited features. So yeah, take a look.
00:11:43.488 - 00:12:08.274, Speaker A: If you're into auto diff, Jax is a good place, good place to be. Okay, so reverse mode autodift numpy on accelerators in grad school, we would have been really excited about that. But Jax offers a bit more. I want to show you another function transformation called JIT, which is for end to end optimized compilation. So let's take a look at what that does. It's a higher order function, just like grad. So you give it a function, it gives you back a new function.
00:12:08.274 - 00:12:43.202, Speaker A: So in this case, we're just going to jit our loss function and get this lost Jit back. And if we run that loss JIT, we get this, we apply it to our parameters in our batch, we get some number out that's the same number as loss applied to those same inputs. The jitted version has the same input output behavior. That's the idea. But if it's faster, as you might guess, so in this case, maybe got twice as fast. So what JIT is doing is it's staging out the computation of this function. It's staging out to an optimizing compiler, this XLA optimizing compiler.
00:12:43.202 - 00:13:43.304, Speaker A: And that means that we're getting rid of all the python overheads that may have been involved in the abstractions inside your function. It's actually optimizing things like memory layout. It's doing fusion optimizations. So the idea is that if you like a package like numpy or Tensorflow, the sort of standard model of computation there was, that this library, you download this library, it comes pre packaged with a bunch of optimized kernels for running things quickly on GPU. And then you write Python code that orchestrates and composes those kernels. If you didn't have a way of building your own, and if you like, you can think of git as a means of composition to build your own compiled end to end optimized kernels just from python code. So in this case, by jitting our loss function, we're building, if you like, specialized compiled code for our neural network architecture and the loss evaluation and all that, which is why it's getting faster.
00:13:43.304 - 00:14:37.550, Speaker A: We can also, so it gets really interesting when you start composing things want to have an end to end optimized version of our gradient of our loss function. We just do Jit of grad of loss instead of grad of loss like before. And now if you run this thing, we are indeed making further progress on the loss, but now a bit more, a bit more quickly. So there are limitations. Why not use JIT everywhere? If you start to use JIT, it places some limitations on the functions that you apply JIT to. In particular, because the purpose of JIT is to delay computation instead of running it eagerly as you run these Python calls, we're trying to delay it, build a representation of that computation, and then hand it to XLA for optimized compilation and then execution. But that means that some Python operations aren't supported, for example, data dependent control flow.
00:14:37.550 - 00:15:22.868, Speaker A: Here's that function that we had earlier. We were able to apply grad to this function, no problem. But if we try to apply JIt to it, we get an error, and this is error. We try to make it as descriptive as possible. It says, while tracing this function f, the value in this control flow was not available because it depended on the value of the argument x to our function. If x greater than zero, jit doesn't like that, and so it raises an error here. This means that jit does have some constraints, but oftentimes you can sort of use jit on the biggest functions you can, and then you can sort of have arbitrarily flexible python dynamically orchestrating those jitted computations as blobs.
00:15:22.868 - 00:15:44.928, Speaker A: So that's jit, sort of the performance sledgehammer in Jax. What else? So here's another function transformation called vmap. This one is inevitably everyone's favorite. If you haven't tried Jax yet, it'll be your favorite once you try it. But it is hard to explain, I find. So vmap, it stands for vectorized map. And here's what it does.
00:15:44.928 - 00:16:20.922, Speaker A: It computes maps. So here's a simple toy function f. It just checks that its input is a scalar. Just imagine this function was written assuming that the input will always be a scalar, and then it'll square its input to produce its output. So what if you had not just a single input to this function that you wanted to apply, but you had a whole array of inputs? Well, we could sort of loop over application of the function f and unpack the array input, apply the function f to each input separately, and then concatenate the results back together. That's what this loop is doing. Vmap semantically does the same thing.
00:16:20.922 - 00:16:55.456, Speaker A: So if we apply vmap to f, that gives us a new function back that is like a mapped version of that function. So we can apply it to this whole stack of x's at once to get a whole stack of outputs. The interesting thing is how Vmap carries out this evaluation. So instead of having a sort of loop of a bunch of separate applications of your function, it will actually sort of plumb a batch dimension through your code, adding it, if you like, to all the primitive operations. So all the primitive operations get a new batch dimension added to them automatically. So in this trivial example. So I've imported this make jackspur.
00:16:55.456 - 00:17:39.901, Speaker A: This is a handy utility to see what Jax is doing behind the scenes. So in this case you can see that we've sort of, instead of applying to scalars, we've made a jax function that applies to a whole stack of inputs at once. And then we have this operator that applies to all the array elements at once to square all the elements. So it's batched up for you as if you'd written this by hand to plumb through a batch dimension. Here's another interesting example. If you have an l one distance function, and you know, if you wrote this in numpy, you might say like, oh, I'm going to assume that there are batch dimensions coming in on the left most sides of my inputs or something like this, then you have bookkeeping troubles that are, that are complicated. Imagine you have a function that just doesn't accept any batch inputs.
00:17:39.901 - 00:18:29.768, Speaker A: It just takes two vectors because you don't remember which axis is which and computes the l one distance between those two vectors. But what if you have a whole batch of inputs? So let's say you have a batch of 103 dimensional vectors and you want to get all the pairwise distances between them. Are you going to write doubly nested for loop, like for vector I and vector j, then compute the l one distance and save it in a matrix? No. What you can do instead is you can apply vmap twice. So we're going to batch this function twice. So even though this function was written only expecting to take scalar inputs by applying vmap twice, in this pairwise distances function, we can apply our, our l one distance function and get our 100 by 100 matrix of pairwise distances. And again, the magic is in how vmap does this.
00:18:29.768 - 00:19:05.002, Speaker A: Instead of looping over this function many times it's plumbed a batch dimension. So you can see the details aren't too important. But this is sort of doing the broadcasting and element wise operations and reduce sum operations. It's doing all that for you so you don't have to worry about it. Where this becomes really powerful is if you need to add a batch dimension to code that you didn't write, maybe because some other library author wrote it and they forgot to add a batch dimension. And you want to just like, you know, have an extra batch of things. For example, you're doing meta learning, or you want to study how your optimizer behaves on a whole batch of different initializations.
00:19:05.002 - 00:20:00.224, Speaker A: Maybe you want to like vmap some optimizer code that you didn't write, but also you can Vmap code that autodiff wrote for you. So for example, if you need to get per example gradients, which comes up in applications like differential privacy, then you can vmap a grad function to get efficient per example gradients. And then, you know, why not jit the whole thing for performance? And this is just using our simple loss function defined above. And this is sort of showing us that we're getting the right shape. This is sort of the initial, this is the gradient on an example by example basis for our batch size of our batches, size 128. Um, this is sort of the, the gradient for each example, for those, uh, those parameters. So vmap uh, gives us a way to sort of make efficient use of hardware by batching up all of our um, operations in a, in a vectorized way, so we can, you know, uh, get good performance even when we want to like evaluate maps.
00:20:00.224 - 00:20:36.652, Speaker A: Um, another transformation, it's very similar, um, is PmAP. So PMAP also gives us a way to evaluate maps. But instead of giving us a way to evaluate maps in a hardware vectorized way, it's a way to parallelize over multiple accelerators. So I mentioned that we have these eight gpu's sitting here. We've only been running things on one of them so far. What if we want to program eight gpu's? PMap is one way to do it. So for example, if we want to generate a 5000 by 5000 matrix, not just one, but maybe one on each of our eight accelerators, we can pmap this function, which generates random matrix.
00:20:36.652 - 00:21:12.956, Speaker A: And this comes back to us. This always takes a second. The first time it's initializing nickel or something. So now we have logically a batch of eight or eight gpu's, 5000 by 5000 matrices. And then if we wanted to do a parallel matrix matrix multiply on all of them, we can pmap the JNP dot function and run that with no problem. So how can we tell that it's actually running in parallel on all these gpu's? Let's do a little timing benchmark and we can see that we can run that parallel matrix, matrix multiply in about 20 milliseconds. That's about the same time as it took one GPU to do one of these multiplies.
00:21:12.956 - 00:21:48.314, Speaker A: So we are indeed running eight of these in parallel at once, just by using Pmap. So PMAP is a very simple way to express parallel computations. Usually when we're programming many gpu's, we don't want to just compute pure maps, we want to do something else. For example, if we're doing neural network training, we may want to do an all reduced sum on our gradients to make sure that we're p mapping over our examples for data parallelism. And we want to do an already sum to compute our summed gradients or summed loss function. PMap offers a way to do that as well. It turns out that if you map an access, you can associate with it an access name.
00:21:48.314 - 00:22:22.272, Speaker A: So in this case we're mapping over an axis, but we're saying the axis name is going to be this string, sort of. Any hashable value will do. Then we can do some operations like in this case we're doing an already sum over that named axis. Um, and so this is just a trivial function to normalize its input. It takes this vector, input x, and it'll divide x element wise by the allreduce sum. So this is actually, you know, running. It's normalizing a trivial, you know, length eight vector on eight gpu's, which is a bit, I think, of, uh, uh, overzealous use of GPU resources, but it works.
00:22:22.272 - 00:23:00.086, Speaker A: Um, great. So, uh, gradjit, vmap, pmap, those are the bread and butter things in jax. Something that I wanted to show to you all for the first time showing in this kind of demo is this PjIt function, which is a new transformation. It's another way to parallelize things. You could think of it as an alternative to Pmap, but where pmap is much more explicit about where the parallelization is happening or where the communication is happening. Pjit gives us a way to say, compiler, take the wheel. So like, I know I want to parallelize this function, but I don't want to have to write it in sort of PMAp style or something like this.
00:23:00.086 - 00:23:47.184, Speaker A: XLA as a compiler is actually able to look at a function, and then if you provide some annotations on the inputs and outputs of the computation to say how you want to shard up these inputs, it can actually shard the computation as well over many accelerators, like many GPU's in this case, or in my other tab, many TPU cores. And this is a way to get, this is another way to get parallelism. So for this example, I'm going to run that same sort of simple neural network that we had before, except I'm going to make it a bit bigger. I made this bigger because I was sort of thinking in the mode of large transformers for like large language models. So I just made the hidden unit size really big. So we're just going to have two layers, one set of hidden. It's pretty big, and a large batch size.
00:23:47.184 - 00:24:22.380, Speaker A: Apparently the transformer, MLP's and transformer models for large language models are significantly bigger than this still. Anyway, so let's build our model again. We have these eight gpu's and we want to program them with PGIT. Here's what we're going to do. So conceptually, the way PGIT thinks about physical resources, like many accelerators, is in terms of a mesh. So physical device mesh. We're going to make this more interesting, but to start off with, we're just going to say, ok, I'm going to take, I'm going to make a mesh of all eight of my GPU devices.
00:24:22.380 - 00:24:47.036, Speaker A: Good, got to import, got to import, forgot to execute that cell. Ok, so I can tell it's a live demos. I make mistakes sometimes. Ok, so to start off with, we're going to make a trivial mesh. There's nothing interesting going on in this mesh. It's just like all eight of our devices with a single axis that we're going to name the x axis of our mesh. And with PGIT, the way it works is you annotate the inputs to a function.
00:24:47.036 - 00:25:38.684, Speaker A: So in this case for a neural network loss function, we want to annotate how we want to shard the input data and input parameters to our function. So I'm going to start off with just data parallelism. And so the sharding annotations I'm going to use are to say I'm not going to do any sharding over parameters, but I want to split up my data so that if my data is like batch by number of input features, I want to shard it along that batch dimension. So I want to associate that leading access of my data with my physical resources access that I've called Mesh X. We'll define those and then I'll run this little PGA is new and experimental. So if you see this warning, I've written these little helper functions to shard the data. These are just pgetting the identity function, this is applying sharding to our data.
00:25:38.684 - 00:26:14.250, Speaker A: So it's like putting data on the processors in the right way, and then we're going to take our loss function we've been working with this whole time. I'm going to say my inputs, remember it took two inputs, the parameters and the data. I'm going to annotate those inputs with the parameter sharding and the data sharding and the loss output. That's just a scalar, I don't need that to be charted at all. So I'll just write access resources none, and then we'll run this computation. So this is just a little micro benchmark. So in this block we'll run this PG to computation once to warm it up, and then we'll time it to see after sort of compilation warm up how long it takes.
00:26:14.250 - 00:26:38.280, Speaker A: So it took, you know, 34 milliseconds or something. Run it again. Yeah, 34, 35 milliseconds, great. So that was with data parallelism in our network. But PGP makes it really easy to experiment with different kinds of parallelism. So what if you say, okay, I'm using all eight of my GPU's for data parallelism. What if I wanted to use like four? What if I wanted to think of it as like sharding my data in two ways, doing a combination of data parallelism and model parallelism.
00:26:38.280 - 00:27:23.958, Speaker A: So I want to shard my input batch data four ways and then shard maybe that large hidden L2 ways over two gpu's. That makes it pretty easy to do. Let's see if I can do it without making any typos. So what we're going to do is we're going to adapt our device mesh to, instead of thinking of it as just a single line of eight processors, I'm going to reshape this to be a four by two mesh and just make up some names, Mesh X and Mesh Y to name the physical axes of our device. Now we need to take the logical computation and map it onto this physical device mesh. And the way we're going to do it to get some model parallelism is I'm going to write parameter sharding. So these are the weight bias pairs.
00:27:23.958 - 00:28:11.490, Speaker A: This like input annotation corresponds to the weight bias pairs in our parameters list. So the way I'm going to write this is, let's say I want to use that second device access. And this is saying the weights of my first hidden layer are going to map from an uncharted axis, just my input features, to those hidden layer features, which I'm going to shard over that second mesh axis, the mesh y there. And then the second layer in the neural network is going to map, take in that charted hidden layer and then map it back to something that's uncharted. So no more model parallels in there. If I've done that right, that's our parameter sharding annotation. No errors yet.
00:28:11.490 - 00:28:51.680, Speaker A: So let's run it before we got like 34 milliseconds. I don't remember. Okay, so on this HTTP setup now we're doing a combination of model parallelism and data parallelism, and it happened to run faster. I guess the interesting thing here is that PGA makes it really easy to revise what your parallelism strategy is. So for example, if we wanted to go back to let's measure that pure data parallelism thing again, I'll just make my mesh eight by one and run it again. And yeah, that was like 34, 35 milliseconds. So it's really easy to sort of experiment with like, oh, should I do all data parallelism? Should I have model parallelism? If you have a convolutional network, you can have spatial data parallelism, and PGA will handle all that for you.
00:28:51.680 - 00:29:22.890, Speaker A: You could write all these things with pmap as well. But for example, spatial data parallelism and convolutions, you need to write something. That's tricky. You need to take all the edges of your charted spatial inputs and swap them between processors. PJit does all that for you by asking the XLA compiler to do it. PJiT is the sort of thing that's underlying a lot of our like large model kind of work and like our ML perf entries and sort of thing. It's really good for neural networks because that's where a lot of the attention is and that's where the compiler is optimized.
00:29:22.890 - 00:30:12.044, Speaker A: So things like transformer models, like really, really great at. We find that, you know, neural network people tend to really like pjets, and then people who are coding things that are more out of the mainstream, like physics simulators or something like this, those folks tend to prefer using PMAP to control exactly what parallelism and what communication is happening more explicitly themselves. Great. So I'm going to for a demo that's Jack's whirlwind tour, I want to tell you a little bit about how Jax is being used. There are a lot of projects that have adopted Jax. So the work on vision transformers and on nerfs, so neural radiance fields, these amazing demos, a lot of that code now is using JAx. There's also things that are a bit different.
00:30:12.044 - 00:30:58.558, Speaker A: So for example, this is a project called Jax molecular dynamics, or JAXMD. And these folks love to use Jax and then use accelerator hardware to do simulations. So one thing they simulated was like a slow mo bullet firing at the Jax logo just for fun. But when they're more serious, they do things like simulating material properties of photovoltaic cells or something like this, so they can optimize the material properties through a simulation sort of thing. JaX is also used for other kinds of simulations, like computational fluid dynamics. This is a recently released package. This is another interesting hybrid computation where they want to use neural networks to speed up PDE solutions in computational fluid dynamics.
00:30:58.558 - 00:31:29.042, Speaker A: And so having a system that's good in neural networks, but also good more generally at things like scientific computing, is a really great win for these folks. It's also used for like bracs. This is a physics engine that thanks to Jax, it sort of can run a lot of different configurations efficiently using PMAP and VMAP and this sort of thing. And it's all differentiable. Of course. Another big project is alphafold. The alphafold code is all in Jackson.
00:31:29.042 - 00:32:24.154, Speaker A: You can check it out, if you haven't already on GitHub. Some of the recent large language model research, for example from DeepMind, uses Jax under the hood. So you can check out those papers and a paper that I collaborated on with some folks from Toronto. This is a really fun application of Jax with neural odes and it actually involves some new, at least new to us, auto diff additions to Jax to do very high order differentiation to regularize ode neural ods. The greatest thing about this tweet, the reason I linked to this tweet is there's a great GIF animation that Jesse Betancourt made to go with this, but I can't show the, I can't show the animation because it's, you know, trademarked. So anyway, click on this tweet if you want a great meme about this project. So that's sort of what Jax is.
00:32:24.154 - 00:33:01.228, Speaker A: I like to give a little bit about how Jax works under the hood because it's actually quite simple conceptually. And I think understanding how it works explains why we think of it as the system for function transformations and even an extensible system for function transformations. So there's sort of a two step process for how it works. And the first step has to do with tracing a Python function, like the Python code you write to Jax's internal representation. So here's a simple example. Python function. This is actually basically how the log two function is written in Jax numpy.
00:33:01.228 - 00:34:01.692, Speaker A: If you open up Jax numpy, you'll see a function definition like this. The idea is that we have this simple composition of these lacks functions, lacks acts like the library. You can think of those library functions as corresponding directly to the primitives in the Jax language. So in this case, lax doesn't have a primitive for base two logarithm, but we have base e logarithms, so we can compute a base c logarithm. So Jax has primitive operations that we know how to transform. And when you say something like JIT, this log two function, what happens is we replace the input argument with a special tracer object that represents not the value at which you want to evaluate it, but just sort of like more abstract information, like just what the shape and the element type of the array is, because we don't actually perform any computations on this thing yet while Python's running. So we drop in this tracer object and then we just observe as this tracer is running through your python code.
00:34:01.692 - 00:35:05.346, Speaker A: Python is evaluating as it normally would. We just record what operations are being applied to the input, what primitives are applied to the input to finally produce the output. So this is actually why Jax has a requirement for all of its transformations, that your code is functionally pure, because we want the function to just compute an output value as a function of its inputs and not have something else like a side effect involved, because our very simple tracing mechanism isn't compatible with recording side effects. So once we get this kind of like functional trace now Python is, in principle at least, out of the picture. I guess this also explains why you can see some errors with something like JIT. Because if we had Python control flow that switches on just the sort of abstract information that's carried along with our tracer, that depends on the shape of the input, that's no problem. And JIT will actually respecialize your function if that sort of abstract information changes.
00:35:05.346 - 00:35:42.894, Speaker A: But this is why we have an error. If we have value dependent control flow. Because we drop in a tracer and record the operations being applied to that tracer input, we don't carry a value along. We're delaying that computation to be run later. This is the error that we saw earlier. I guess one interesting detail aside here is that what I'm telling you here is a bit of a simplification, because grad somehow works with Python control flow. And so transformations injects some of them we can make them work with Python control flow, but some of them we don't because we want to make them stage out computations.
00:35:42.894 - 00:36:48.602, Speaker A: That was all. Step one, we use a simple tracing mechanism to build this program representation of your Python code applied to an abstract input, like all arrays of a particular shape and element type. Then how do we transform it? So if we want to do something like automatic differentiation, we have this programmer which represents the composition of primitives we have for every primitive and for every transformation we have a rule that tells us how to transform the application of that primitive. So for example, for logarithm, we understand that, you know, to take the, um, to differentiate a logarithm involves, you know, division, uh, and, you know, here's the quotient rule, if you like, spelled out as the, um, differentiation rule for, for the divide primitive. So what a transformation looks like is then just a simple interpreter which walks this jackspur and applies, you know, as it, as it walks this jackspur. Um, it applies for every primitive application in the jackspur. It applies this rule that it has for this transformation.
00:36:48.602 - 00:37:44.038, Speaker A: So that's all a transformation is. And what's really special about having transformations that look like this is that this is just another python function, this transformation, this Jacksburg interpreter. And so if we wanted to, for example, do higher order differentiation, the way we would differentiate, the way we do higher order differentiation is we just take this python function, which is an interpreter of a jackspur, and we drop in tracer objects on its input and we'd record what Jackspur it is representing and then we could transform that. Jasper again, this is sort of the secret to Jack's composing transformations is that it all comes down to building intermediate representations and then writing interpreters. And these interpreters are really lightweight and easy to write and they themselves can be sort of traced and transformed again. So you like the lifecycle of jacks, like transformation or using code jacks, is that we have Python functions. We can trace them into this jackspare representation.
00:37:44.038 - 00:38:23.574, Speaker A: And then when we write a transformation, that is an interpreter that interprets the jackspur applied to inputs, but then that's again a Python function. So we can go around this loop again to transform it again. And of course we can, you know, we have to exit this loop somehow so we can evaluate Python functions or compile jackspurs. And as I mentioned, there's actually a little trick to this where you can handle python control flow and things like automatic differentiation. But that's a story for another time. So that's it that I have in terms of slides. Since we have ten minutes, let me make sure I'll spend 1 second plugging some extra work.
00:38:23.574 - 00:39:14.502, Speaker A: So there's a lot that Jax is very much an ongoing project. And you know, if you're, if you're interested in pieces or interested in applying to something new, please like come to our GitHub issue tracker and reach out and talk to us about it. I thought I'd mention very briefly, you know, some, a project that sort of has grown out of working on Jax. It's this. So Dex is a really interesting, sorry, Jax is a really interesting project because it's like just let's stay in ordinary Python and try to give Python programmers more superpowers for compilation or differentiation or batching, that sort of thing. But in a way that's a constraint that we have where we can't go beyond Python. And so one way we think of this is that Jax is sort of an embedded domain specific language in Python, like a numerical language.
00:39:14.502 - 00:39:57.778, Speaker A: And as we try to add more fancy features to it at first you're really happy because things are just getting more superpowers in Python. But then as you add more and more complicated features, it gets harder and harder. You start running into the limitations of Python more and more. For example, if you want to do, hey, I actually do want to stage out value dependent Python control flow, or I want to work with data structures that aren't just arrays. Your happiness might start to go down as you demand more and more language features. The benefits of having a standalone language, which isn't just Python, maybe you start to look more and more attractive as you make more and more demands of things. Like, at first you're like, oh, I have to learn a new language.
00:39:57.778 - 00:40:57.144, Speaker A: That's going to be really hard. But the benefits start to mount, for example, as people start pushing Jax's limits, especially in things like scientific computing domains. And so Dax is a research project within our Jax research project that grew out of asking the question like, if we did sort of start from scratch and design a new language, what would it look like? Um, you know, can we do something that's, that's, uh, better than, than Jack's in a few key ways? Um, so this is the syntax. I won't go into the, uh, details, but it's very much motivated on thinking about, you know, how can we do something that's fundamentally, you know, bigger than Jax? What would it look like? Um, I mentioned this is a research project itself. Uh, in some ways it's, it's already informing things we do with jacks to do with, um, uh, you know, supporting data structures beyond just rectangular, um, arrays of floats, this sort of thing. So if you're interested in like an even more boundary pushing direction, Dex is a cool project to take a look at. Great, I think I'll stop there.
00:40:57.144 - 00:41:42.360, Speaker A: I'll go back to this nice ending slide, and if there are questions, I'm happy to take them. Yeah, there's a question in chat, I can just answer that one. Unless, is that a good process? Go for it. All right, someone in chat asked, does Jax perform any optimizations like fusion on jackspers? Were they all handed off to XLA? They're all done by XLA. We basically don't do any optimizations on jackspers. There's a few things that you could call optimization. So, for example, things to do with, like in automatic differentiation, you might know, for example, that some gradient values are definitely zero.
00:41:42.360 - 00:42:45.820, Speaker A: And that's the sort of thing that the auto diff system can keep track of and propagate. And we just want to make sure that we're not instantiating arrays of zeros, we can sort of maintain that things are symbolically zero. You could think of that as an optimization that a compiler could potentially do, but because at the autodiff level we, you know, it's much easier for us to track what, what things are zero versus non zero, that's the kind of an optimization you could say that Jax performs. So I guess basically we hand everything off to XLA and rely on it for optimizations. But sort of unless there are optimizations where it just fits squarely into the wheelhouse of what Jax is doing, because we have so much more information than XLA does about things like automatic differentiation. There's another question about is the VMaP transformation done on Jackspers or HLL? It's on Jackspur's if you like. So all of our rules for VMAp are just in pure Python, and they look very much like the rules that I flash up on screen, where we just have a set of jacks primitives in Jackspurs and we have a rule for each one.
00:42:45.820 - 00:43:26.254, Speaker A: It's just a python function somewhere for how to do batching. So basically all the transformations you can think of as done at the jackspur level. Someone asked, can we use Tensorflow calculation performed in jaks? Yes. It's not an especially well trodden path, but there's something called Jax two TF, which is sort of an experimental Jax sub module. And if you look in our documentation for Jax two TF, you'll see a lot of documentation on it. And there are ways to bridge between Tensorflow and Jacks in that module. You can actually also use jacks with Pytorch.
00:43:26.254 - 00:44:18.446, Speaker A: I don't think we have anything in our documentation, but in a way, these systems, for example on GPU, are just dealing with arrays, on GPU backed arrays. And why can't I just hand it off from one system to another to do some computations and then hand it back? There are ways to do that with Pytorch as well as with Tensorflow, but I guess the best way to find out about them might be just to ask on our issue tracker, because I'm not sure how documented they are. Wow. Radford Neil asked me a question. Are Jack's computations reproducible? That's a great question. One, on identical systems, different systems, this sort of thing, there are a lot of ways that computations might not be reproducible, and a lot of different versions of reproducibility, we might ask for. So one might be about pseudo random number generation.
00:44:18.446 - 00:45:16.090, Speaker A: Like do the random numbers change if we switch from CPU to GPU or something like this? And actually for that particular question about pseudo randomness, the answer is no. Are pseudorandom number generators all done in software, as it were? Like we just implement it by hand. It's based on this parallel random numbers, as easy as one, two, three, work this like tree fry hash based pseudo random number generation. And so our random, the random bits we produce, and like the random integers we produce, if you like, those are the same across backends with floating point numbers. It becomes much trickier because different backends might have different floating point optimizations inside their numerical kernels. Like the order in which a reduction happens could affect floating point reproducibility. So when floats are involved, when you ask about different backends, it's not quite reproducible.
00:45:16.090 - 00:45:52.124, Speaker A: There sort of like best effort. There are ways to switch off some optimizations to get things more reproducible. So for example, GPU's can have non deterministic reductions of floating point numbers. So like run to run even on GPU might not see determinism. I guess one other example for non determinism is just the compiler. We have an optimizing compiler, and if you update jacks, maybe some new optimization is added that reorders floats on GPU or something like this, and that could cause non determinism. So as to whether things are reproducible, it's a very complicated, nuanced answer.
00:45:52.124 - 00:46:29.710, Speaker A: I would say that the random number generator tries to be as reproducible as possible for a given. If you've installed Jax and you don't upgrade it, and you're running on the same back end multiple times. If that backend is TPU or CPU, I would expect to always get the exact same runs across processes on the same system. If you upgrade jacks, that might change, though it's pretty stable. And then across runs on GPU, I would expect that to be the least stable. There's probably factors that I haven't thought of there, but I think the functional programming nature gives us a lot of tools to be more reproducible. So we're as reproducible as possible.
00:46:29.710 - 00:47:11.034, Speaker A: But accelerators and floating point and things make that sort of hard. Do we have anything similar to Jack's? But for c or rust? Now that I know of, in some ways we take advantage of python being very dynamic and being able to overload the heck out of it. So our tracing mechanism is based on euro code that thought it was getting some array objects and we're overloading those things and stuffing in traces and sort of thing. The kind of dynamic, flexible polymorphism that we expect from something like Python. We're taking great advantage of that. That said, maybe we could do something similar to supulse and rust. Yeah, I guess there's maybe more to say about that, but hard.
00:47:11.034 - 00:47:48.724, Speaker A: Is it possible to PMap PJIt over multiple GPU nodes? Yes, it is. So as I mentioned, these are the things we use. So that the DeepMind language model stuff that I mentioned, that was all done using PMap, they did in the paper. Whereas a lot of our ML perf submissions and other sort of large scale work is done effectively with PJIT, there is a way to do it over many nodes. The complication there comes in that you're running many python processes. This is a multi host programming model that I haven't talked about, but Pjet and PMap work with that multi host. If you think of MPI, that's what I mean by multi host running many processes, but you can perform collective computations with them.
00:47:48.724 - 00:48:55.324, Speaker A: Um, difficulty jitting, value dependent control flow limitation of using Python to write a DSL. Um, yes, that is, uh, a primary difficulty about our tracing mechanism. Um, but there's some, you know, you asked about value dependent control flow. Um, it has to do with tracing, but also has to do with the semantics of, you know, what the underlying language can do, like Python control flow structures, early exit from loops and exceptions, and all this stuff don't map perfectly onto the HLO language that we're targeting and that maybe the compilers and even the hardware ultimately can represent well. So I'd say that some of it is about just surface mechanism, about dealing with the Python front end. But there are also elements where the arbitrary control flow in python might not be efficient to execute on a ultimately on hardware, whereas the abstractions that we provide for control flow sort of higher order combinators, you can execute more efficiently on hardware. So it's a little bit fundamental also.
00:48:55.324 - 00:49:06.944, Speaker A: And Dex is to some extent about gaining flexibility here. I think we're out of time. I had to answer some questions, but I think there are two left in chat.
00:49:07.444 - 00:49:13.414, Speaker B: Why don't we take just these two? Um, okay. We can run a few minutes into the readings.
00:49:13.714 - 00:49:39.634, Speaker A: All right. Um, cool. Yeah. So is it always good practice to jit as much as possible? Um. How can we tell which ones? Yeah, it's basically, um, sometimes we talk about the jittocratic oath, which is like, do no harm, make it no slower. And so like, you should use JIt. And usually the reason not to use JIt is just that, um, it would like you have some python dynamism, control flow or something that JIt doesn't support.
00:49:39.634 - 00:50:22.858, Speaker A: Oftentimes people do neural network research, they jit their whole computation, so that at the end of the day, their training loop might just be a loop dispatching a single big jitted computation. So the entire entire update set might be jitted, or jitted. One thing that might prevent you also from from jitting big blocks is compilation times. So I was talking a lot about execution times being fast, and JIt won't do any harm there. But because XLA is a super optimizing compiler, if you stage out bigger and bigger computations, it'll take longer and longer, and the time it takes is super linear in the size of the programming stage app. So some people who write really complex programs, they jit the whole thing and it'll eventually be very fast to run. But XLA will take a long time compiling it.
00:50:22.858 - 00:50:59.202, Speaker A: That's something we're working on. But I'd say that's probably the number one reason why if you could jit a function, you wouldn't would be compilation times. Are there official benchmarks comparing Jax versus Tensorflow? No, not really. I mean, benchmarks are so hard to do well, and it's kind of a moving target. Also, Jax and Tensorflow are like a lot of Tensorflow code is moving to use XLA, so there's a lot of convergence ultimately. But yeah, I guess the best benchmarks are the ones that you care about. And so if you can run benchmarks or find benchmarks for a domain you care about, like a particular domain, about simulation or neural networks, take a look at those.
00:50:59.202 - 00:51:48.910, Speaker A: In terms of really official benchmarks, there's a very well done benchmark competition called MLPeRf that I mentioned a couple times about sort of like using. Usually it's about, the main entries of interest are about using supercomputers and running benchmarks on them with different frameworks. And Jax, I think as of, I guess it was the 2019 or 2020 year, Jack set the record for training transformer like super fast, and they retired the benchmark after that. So I guess I wouldn't be worried about performance in JAX. Usually concerns are more like, if you can't get your code, because there's a lot of dynamism or something like this, that's where Jacks might start to fall down. But if you can get your code like a big transformer, Jax is going to be the fastest thing that you can do, typically. All right, thanks.
00:51:48.910 - 00:51:57.474, Speaker A: Those are the two questions, so.
