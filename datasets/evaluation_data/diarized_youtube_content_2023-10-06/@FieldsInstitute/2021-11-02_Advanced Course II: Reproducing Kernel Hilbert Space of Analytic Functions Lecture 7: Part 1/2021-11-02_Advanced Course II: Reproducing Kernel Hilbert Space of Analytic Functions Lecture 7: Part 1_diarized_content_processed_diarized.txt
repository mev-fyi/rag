00:00:05.720 - 00:01:15.078, Speaker A: Thank you symmetra. We continue our chapter on interpolation on Rkhs. We studied one result, but in that result we assumed that there is a solution, I mean just to frame refresh our memory, let me recall that result. So recall as usual, h is an Rkhs and on a set x on x, we consider a set of finite points x, one up to xm in x. And the crucial point here is that we assume so. This is assumption. There is a g.
00:01:15.078 - 00:02:57.134, Speaker A: Well there is a g in h such that g at point x e is equal to something given, I mean it's called lambda. I should add here lambda one up to lambda and arbitrary in c. I mean no restriction on that. So existence is a part of as important hypothesis here. Then, when we consider the projection of g onto the subspace created by kernel function at point x, one up to xn and we called it pf of g, then Pfog is there. We have two things here. The first one is uniqueness function of and the other thing minimum norm in edge, which does the same job such that this function pfg at point xi interpolates the same points I equal to one up to n.
00:02:57.134 - 00:03:54.614, Speaker A: So if a solution is given, we know how to find the best one. We just make the projection. But now the question remains when the solution exists. And for that we need, we have a new set of results for a matrix a, say an n by n matrix. The set of point x such that ax is equal to zero is usually denoted by, I mean said to be the kernel of a. But here we have kernel for another purpose. So the authors have used the notion null space.
00:03:54.614 - 00:05:28.780, Speaker A: So n of a null space of a is equal to this. But if at some other text you see the kernel do not mix the two concepts. And here is our first proposition. The proposition is easy to prove, but inside the proof there is an identity which by itself is very important and it will be needed several times today and in some other demonstration later on. So here is the proposition. So the same assumption, x asset h is an rkhs on x x one up to xn in x distinct point. And here is a ten if omega, and in the book it's written omega.
00:05:28.780 - 00:06:31.452, Speaker A: I prefer to call it vector alpha. But anyhow, when the book is written omega, the vector alpha alpha one alpha two up to alpha. Nice is a vector in the null space of, or in null space of the gramion of k, xi, xj, I and j, both of them from one up to nice. Then so you see up to here it's just linear algebra. You have a matrix n by n, and alpha is in the null space. It means that if you can call this g g times alpha bar is equal to zero. I mean, very, very simple thing to verify.
00:06:31.452 - 00:08:19.554, Speaker A: But look at the result. Then the function f equal to some alpha j k at point x j is identically zero. It means as an element of space h. So there is a simple consequence of that. So consequently, if we have two points, I mean, call them omega one, or as I prefer. Well, let's stick to the notion of the book. Omega one equal to alpha one up to alpha n and omega two beta one up to b n, such that this matrix, the Grammy and matrix kx I xj times omega one is equal to k xi xj time omega two.
00:08:19.554 - 00:09:29.286, Speaker A: Then look, omega one is not necessarily equal to omega two. They can be two different vectors in Cn. But when they applied, I mean, when g acts on them, they create the same vector g times w one equals to g times w two. Then these two vectors create the same function. The sum alpha j k xj kxj is equal to the sum beta j kx. So the important thing here is to note that, as I said, these two vectors are not the same. The coefficients could be different, but at the end, when you do the summation, they create the same thing.
00:09:29.286 - 00:09:52.658, Speaker A: You have seen this before. I mean, an example which I have mentioned repeatedly. Let me in the chat. I think that the null space is better than merging kernel because no. And zero is yes. I mean, Sheldon says that the null space is a better terminology because node means zero. I agree with that.
00:09:52.658 - 00:10:18.194, Speaker A: But for operator theory, people, I mean, they. I mean, they use kernel for the nodal space of an operator. I don't know what is more common. Sheldon, in linear algebra. Could you clarify, please? In linear algebra, do they say null or do they say kernel?
00:10:18.574 - 00:10:43.824, Speaker B: I think we see both, probably about equal. Maybe even kernel is a little more common. But as you said, it's ambiguous. Kernel has a totally different meaning when we're talking about an integral operator. So that's very confusing, and it doesn't convey any sense of what it means. Whereas null, null, everybody knows null means zero, so it really fits much better. So I'm trying to get people to use null space all the time.
00:10:43.984 - 00:11:04.376, Speaker A: Sure. That's very good comment. Thank you. But maybe, as you said, equal, equal. That's the influence of operator theory, people that repeatedly they use kernel. And as far as I know, kernel means at least three different things. Two of them we have seen here.
00:11:04.376 - 00:12:15.034, Speaker A: There is one more, I mean, which is related to the kernel function, the kernel that they use in integral formulas. But still, it's not 100% the same thing. Oh yeah, I agree. Null space is better. So about this proposition, it's kind of a long proposition, but the second part, I mean, I mean, if this equality holds, then we have this identity is immediate from the previous one, because you simply see that k acting on w one minus w two is equal to zero. Therefore here, if instead of alpha j, you write alpha j minus beta j, then the function is identically zero, which gives us this one. So we can have, for one function, we can have different coefficients, and we have seen this before, for example, in sub four or three, one of them when f was f equal to f one.
00:12:15.034 - 00:12:51.274, Speaker A: So for example, for the zero function, we can say that zero times k at .0 plus zero k at. .1 and we also saw that k zero is equal to k to one. So here is another representation. The coefficients are one and minus one for the other 10 zero. So here, alpha one and alpha two make a vector, call it w one and one minus one.
00:12:51.274 - 00:13:39.210, Speaker A: Another vector w two, one minus one. These are not the same. But if you make the two by two matrix with x one equal to zero, x two equal to one, it's very immediate to see that this identity holds. So in other words, that two by two matrix acting on w one gives the same acting on w two, and the result will be zero zero. It's very easy to verify. So even though we already know, based on other things, that this identity hold, the proposition also tells us that, I mean, this holds. So this is not something new for us.
00:13:39.210 - 00:14:23.846, Speaker A: We can have different representation. But why is this true? Why? If there is something in the null space, the function it creates is identically zero. And this is based on an identity, as I said, which is somehow very important. And I believe the identity by itself should be mentioned as a lemmov, because we use it many, many times. So, proof. Let alpha one up to alpha n b in c. Arbitrary, I mean, not the one which is in the statement of theorem.
00:14:23.846 - 00:16:01.674, Speaker A: And consider and let f to be the function j from one up to n alpha j kxj and now compute the normal f in h squared. This is not something difficult. Normal f is f in a product with f in H. And we can use the formula j from one up to n alpha j k xj. We have seen this formula before, but I want to write it differently, the way it will be beneficial for my application. Kxi in h make the expansion I from one up to n j from one up to n, alpha I bar alpha j, k xj and kxi in h. And the last one we have seen up to here before, alpha I alpha j and this is k at point xi xj.
00:16:01.674 - 00:17:10.868, Speaker A: You have seen up to here before. But now I want to write this differently. I want to write this as the inner product. The product usually is the curly one, the inner product in cn a watt of the matrix k xix ij from up to n acting on the vector alpha one up to alpha n. Again, scalar product with alpha one up to alpha in c. Simple words, if I mean using g could be misleading because g is for any combination of xi and xj and with any n. But still please allow me to write g.
00:17:10.868 - 00:18:19.124, Speaker A: This is g alpha bar and alpha bar in c. And when g is in this matrix. So I write this again. This is the important formula that I will need. If f is equal to the sum j from one up to n, alpha j k xj and I call alpha to be the vector alpha, one up to alpha n and g be the n by n matrix k xix one to n. Then the normal f in h squared is the inner product of g, alpha and alpha in Cn. That's the formula in it.
00:18:19.124 - 00:18:56.284, Speaker A: And the proposition comes immediately from this formula. Because in the proposition we assume that omega which is in alpha bar is, is in the null space of g. This is our assumption. And therefore g times l acting alpha bar is equal to zero. So it gives normal f in h equal to zero. So f is identically zero in h. End of story.
00:18:56.284 - 00:19:50.726, Speaker A: Immediately comes from this yellow formula. If we pay attention to the formula, you will see that I will, I will use a couple of more times in what follows. So knowing this, now we go for our theorem, which is the main theorem of this section. It's called interpolation theorem. The equation Rkhs and difference between this and the one we started at the very beginning. Indeed, it was a recall from last week. We want to remove this assumption.
00:19:50.726 - 00:21:23.570, Speaker A: We want to know when this is, when there is an a g such that it holds, and for that, here is the result, the same setting as before. X is a set h and r khs on x and finite set x one x n in x distinct and omega. Yeah, well, doesn't give a name. Lambda one and lambda and some points in C doesn't give a name for the time being to this. And then the result then there exists. So see, it's an existence theorem. Then there exists a g in h that interpolates this point such that g point xi is equal to lambda I I from one up to n.
00:21:23.570 - 00:22:28.414, Speaker A: You see, up to here, it's a question about rths. You are looking for a function in that space. But now look at the characterization if and only if. Now we move to linear algebra. The solution exists if and only if the vector lambda one up to lambda n. Now call it v. If and only if the vector v lambda one is in the range of the matrix g belongs, I write, is in the range of kxix.
00:22:28.414 - 00:24:10.188, Speaker A: So the existence is equivalent to being in groups in the range of the question which was in rkhs is moved to another question in linear algebra, which is, I mean, usually easy to verify. An n by n matrix is given, we can calculate it easily and then verify if this vector is in the range or not. Moreover, based on our previous result, then we know the solution exists. Moreover, in this case, if we choose omega equal to alpha, one alpha n such that is g. Well, no need to mention from one to n, this g times omega gives v. In other words, omega is a solution of that question in linear algebra. If omega is any solution up to here, look, the omega, we know omega exists, but it's not necessarily unique.
00:24:10.188 - 00:25:41.002, Speaker A: We saw in the previous proposition. Yet, if we choose omega such that then h equal to the sum j from one up to n alpha jkxj is the unique. Omega is not unique, but h is unique. Unique function of minimal norm which interpolates well, let me add in age, it's clear, but it's still to emphasize in age such that h at point x j is equal to lambda. The one we were looking for satisfies the same thing that we wanted, but with two more properties. It's unique, minimal norm and even we can calculate the norm and the norm in h. Again, this is something which happens in the, in the, in our rkhs.
00:25:41.002 - 00:26:23.244, Speaker A: We have seen several examples before. It can be an integral formula, it can be a series. But whatever it is, the theorem says that it's equal to the inner product of v and w in cn. Again, another connection to linear algebra. So in other words, it's a precious theorem, this one. It connects an interpolation theorem to a question in linear algebra. And moreover, when we know the answer is positive, it gives us a function h which is unique, it has minimal norm.
00:26:23.244 - 00:27:01.844, Speaker A: And also, without going into detail of calculation in h, we are able to calculate its norm just as an inner product in C. Well, wonderful result. So let me highlight what we have here, we have uniqueness, we have minimal norm. Oh, that's not good. It's too much color. Maybe yellow is better. We have uniqueness.
00:27:01.844 - 00:28:26.284, Speaker A: We have minimal norm. And also we have a formula for the norm. So now why, let's see why we have this bridge. We have, I mean, everything, all the tools needed to, to prove our result. First, I mean, it's an even only if for one part, assume that g in h such that g of x j equal to lambda j exists. Then we saw before, it was last week that if we have a solution when we project, then h defined to be the projection of g into the finite space f. Recall that f is span of kxi x one up to kxn.
00:28:26.284 - 00:29:24.504, Speaker A: It's a finite dimensional space. We saw last week that if we have a solution and we project the, the projection is still a solution. So p of f, which is the h which is the projection, still is the solution we have. Then g is also an interpolating function. What does this mean? Means two things. First, because of the projection, there are constant, let's call them beta one up to beta n such that h is equal to the sum j from one up to n, beta j k x j. This is because we project.
00:29:24.504 - 00:31:31.464, Speaker A: And second, for all I equal to one up to n, we know that h at point xi equal to g at point xi is equal to g at point xi, which is equal to lambda. Now put these two together, this one and this one. This means that sum j from one up to n beta j kxj at point xi is equal to h at point x I, which is lambda I for I from one up to n. One more time, I write the same, the same thing. So this is equivalent to, say sum j from one up to n beta j k capital k. Now xi x is equal to lambda I, and the matrix form of this, this is equal to k x I x j. Now, a matrix a from one up to a and j from one up to n, times the vector beta one up to beta n is equal to the vector lambda one lambda.
00:31:31.464 - 00:32:23.068, Speaker A: That's the same thing. The ice row is what written here, that's the ice row. So we see that this implies that lambda one up to lambda n is in the range of the matrix k x one k xj. That's one direction. And it's almost reversible. This because, I mean, it's almost reversible. The thing, the things I wrote here, we can go backward.
00:32:23.068 - 00:33:22.414, Speaker A: Now, assume that lambda one up to the vector lambda and lambda and is in the range and let me write g to avoid that big notation. Then pick any, then there is any solution you want. There is alpha one up to alpha n such that g or k xix times alpha one up to alpha n is equal to lambda one. That's the meaning of being in the range. Now expand. That's why I said this is reversible. Instead of this form, it's alpha instead of beta.
00:33:22.414 - 00:34:25.984, Speaker A: Write it in this way. Hence sum precisely what is here j from one up to n. Okay, xi xj times alpha I, which I write it here is equal to lambda I. This is true for I equal one up to n. That's the end row of this identity and put it in the function form is equal to sum j from one up to n alpha I. This is alpha j alpha j kxj. This function at point xi is equal to lambda I.
00:34:25.984 - 00:35:24.684, Speaker A: That's the same thing but written a bit differently. And then it implies that the function h j from one up to n alpha j kxj interpolates. So we are, yeah, we are done. That's, that's kind of the end of the proof because the second part we have already seen another proposition. The thing that h equals t is unique, minimal norm we have already seen and the only thing which we need to verify is this useful identity. Why the normal age is given by this. And that is why I said keep this identity, I will use it again.
00:35:24.684 - 00:36:11.626, Speaker A: Normal f is equal to g alpha scalar product alpha. And what was alpha? Alpha is the solution of g alpha. Maybe, maybe I added somewhere here. It was g alpha equal to lambda. It should have been written somewhere in the proof. Yeah, g alpha is equal to the lambda. So I use this identity to arrive at the formula which is suggested here.
00:36:11.626 - 00:37:02.204, Speaker A: Normal h squared. So let me add it here at the end. Finally, norm of h squared is equal to by the formula norm of g alpha scalar product with alpha in cn we have seen this before and what was alpha? Alpha was a vector such that g alpha is equal to the lambda. Very easy if we know the formula before. That is why I suggested this identity. It's better to be mentioned as a lemma because we use it several times. The rest uniqueness and minimal we have seen before.
00:37:02.204 - 00:37:46.214, Speaker A: So the solution here, I repeat, h equal to the sum j from one up to n alpha jkxj is unique. Alpha is not necessarily unique, but h is unique. It's a minimal norm. And the norm is given by this inner product in C. Kind of trivial corollary for what we have seen the same assumption as before. I do not repeat the assumption. So the same assumption as in theorem.
00:37:46.214 - 00:38:54.644, Speaker A: But we, we add that also assume that another extra assumption, g k xix I j from one up to n is invertible. We add this. So invertibility is an extra assumption. Here, what is the consequence? Consequence is that here, when we have g of alpha equal to lambda in, in this, either in this or here, we need v to be in the range. But when it's invertible, the range is all of Cn. It's always in the range. It has always a solution.
00:38:54.644 - 00:40:52.004, Speaker A: So invertibility here guarantees the solution. Then, for any set of points, lambda one up to lambda n in C, there is an interpolate, there is an interpolating function. Always there is a solution. And we know how to get the minimal one. And a unique, a unique interpolating function of minimal norm is given by the formula is given by g equal some alpha jk xj, where g alpha is equal to lambda. Here we know what is g, what is vector alpha, what is vector lambda. It's a kind of trivial corollary after our theorem, but it also highlights the fact that there is a merit to go for kernels for which g is always invertible.
00:40:52.004 - 00:41:59.744, Speaker A: And that's the content of our next section. Strictly positive kernels in before, if a is an n by n matrix, we saw the meaning of a being positive. That was equivalent to say that the inner product of ax and x in Cn is always bigger than or equal to zero. That's the definition. And a bigger than zero, it means that a is bigger than or equal to zero. And moreover, a ax in a product with x is zero if and only if x is equal to zero. That's the meaning of being strictly positive.
00:41:59.744 - 00:42:34.646, Speaker A: In other words, no trivial combination gives us the zero. And being strictly positive. Again, from linear algebra, there is another way to characterize them. It's equal to equivalent to being positive and invertible. Again, another way to say the same thing. It's not. Something really different is being positive.
00:42:34.646 - 00:43:13.614, Speaker A: And all eigenvalues are strictly positive. That's equivalent to be invertible. And don't forget that we cannot remove a bigger than or equal to zero. For example, I mean, say for example, all eigenvalues. If all eigenvalues are positive, then our matrix is positive. This is not true. We need to keep this all the time.
00:43:13.614 - 00:44:36.064, Speaker A: Or at least say that a is a self adjoint. There are simple examples to say that why it's needed to keep either self adjoint or being positive, plus something more to give us a strict positivity. And now, why strict positivity is important is this theorem which combined with our previous color theory, we will see why we ended up with this theorem. Theorem x, z and k from x times x. To see a kernel, then the following are equivalent. The following are equivalent. First, k is a strictly positive.
00:44:36.064 - 00:46:07.954, Speaker A: I mentioned what's the meaning of a matrix? To be strictly positive, it would be better to hear here to add what's the meaning of k being strictly positive. But I believe everybody knows this means that I mean all possible n by n matrices for all n, one, two, three, all of them are strictly positive. That is the meaning of a kernel being strictly positive. So it's clear what it means. Two equivalent to for any n and any set of distinct points, and any set of distinct points x, one up to x n. In x, the kernel functions are linearly independent. K at point x, one up to k at point x n are linearly independent.
00:46:07.954 - 00:47:50.402, Speaker A: Three again, this is a theorem which so we show somehow the connection between linear algebra and function theory being linearly independent. Well, you can say that it's a question in function spaces also, even though it has a flavor of linear algebra, because we talk about linear independence. Look at the next one, number three, and also number four. For any n and any set of distinct points x monoplex n in x and any set of scalars, no, no restriction. So any, any alpha one up to alpha n in c. So any set of scalars that are not all equal to zero. That's the usual assumption.
00:47:50.402 - 00:49:23.416, Speaker A: For linear independence. There exists nf in h with the combination alpha one f at point x, one alpha n f at point x, and not equal. In other words, there is no linear dependence between elements of f evaluated. Element of h evaluated at point x, one up to x n. Recalling again the soil of space is a good idea. And finally, something from analysis that it happens in other contexts. Two, for any n and for any x one up to x n, there exists g, one up to gn n function in h such that gi at point xj is equal to delta ij.
00:49:23.416 - 00:50:51.604, Speaker A: The Kronecker notation, or put it this way, explicitly, one or zero if I is equal to j, and if not. So this is the interconnection between at least four concepts. And at the end of the day, all of them are different features for strict positivity. So in some spaces, for example, number three is equal to to to verify, and in some others, number four, before giving the proof. Consider examples our age to be either hardest space h two or Bergman space a two, or Dirichlet space d that we saw in chapter one. Do you agree immediately that number four, either condition four is valid for all of them in harder space in a two and also in the dirt space. They are defined on the open unit disk.
00:50:51.604 - 00:52:35.446, Speaker A: And if I give you endpoints distinct on the open unit disk, for any z one up to zn on the open unit disk distinct, you can easily find functions there is such that. What is your candidate? We have seen in one occasion this candidate before. Recall that these are function spaces of on the open unit disk, holomorphic ones, and therefore polynomials are there. Polynomials are in h two, are in the Bergman space a two and also in the durst space. And the candidate is gi of z to be z minus z one, z minus z two, and we just remove z minus z I. So we go z minus z I minus one. So this already satisfies the property that gi at point z one z two up to zn, except that I is equal to zero.
00:52:35.446 - 00:53:53.486, Speaker A: And if you want to normalize that point z I just add z I minus z one z I minus z minus one one z I minus normalization. So this function satisfies the property that we want. So it is very easy to verify for these three spaces and many more, that we have number four. And again in h two, in h two, and in d, our kernel is strictly positive. Let me add here so the kernels, they are different kernels. The kernels of h two, a two and d are strictly positive. However, in the subwoofer level space, either number two or number three, number two is easier.
00:53:53.486 - 00:54:59.354, Speaker A: We don't have number two does not work. We have k at .0 minus k at .1 is equal to zero, so it's linearly dependent. And therefore k, the kernel of this space, is not strictly positive. So, and I mean all examples that we have seen up to now, it's easier to verify one of these conditions to conclude that if the kernel is strictly positive or is not strictly positive, well, it's 1455. I put the proof for after the break.
00:54:59.354 - 00:55:04.254, Speaker A: So let's have a ten minute break and then we continue.
