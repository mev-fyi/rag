00:00:26.274 - 00:01:30.664, Speaker A: Sumitra, did you start? Yes, I have started. Okay, thank you. The proof of this theorem, even though it's an important theorem, the proof is not that difficult. It's very straightforward proof. We show that one is equivalent to two, two, two is equivalent to three, and then one implies four and four implies three. That's, that's the way it's, it's shown in the book to show that one is equivalent to two. Again, that's another point when our formula that we saw before is important.
00:01:30.664 - 00:03:32.524, Speaker A: So we call the formula that if f is sum j from one up to n alpha jk xj, then the norm of f squared is the matrix k xix acting on alpha one up to alpha, and then inner product with alpha one up to alpha. It's very precious formula in CN. And when we look at the statement one and statement two, we see that this identity immediately implies that one is equal to two, because two means that two means that k x one up to kxn are independent. And this means that f is not equal to zero if the vector alpha is not zero. And this is equivalent to say that is not zero if alpha is not based on the identity. Immediately, you see, this is number one. So one is equal to two.
00:03:32.524 - 00:04:36.294, Speaker A: And the reason two is equivalent to three is based on this formula. For any f is in h, what is the inner product of f and alpha I k x I? This is equal to a from one up to n. Here you obtain alpha I bar, which is okay, no problem with that. But if you want to have alpha I on the right side, simply here, replace alpha I by alpha I bar. That's okay. To have bar here, no problem. F in a product with kxi, which is the sum alpha I bar f at point xi.
00:04:36.294 - 00:07:05.692, Speaker A: And this is the identity which shows that two is equivalent to three, because, I mean, it's based on the meaning of being. Instead of saying two is equivalent to three, we can say not two equivalent to not three. In other words, if k x one up to xk are dependent, then there is a combination here. If dependent, and only if there is alpha one alpha n, not all zero, such that sum alpha I k x I is equal to zero and based on the identity. Yeah, this is equivalent to say that for all f in h, well, such that for all f in h, some alpha bar f of xi equal to zero. And this is not two equivalent to not three. So in other words, the notation we use in logic hands, two is equivalent to three, that four implies three is also trivial.
00:07:05.692 - 00:08:29.864, Speaker A: Four implies three kind of trivial because assume without laws of generality, alpha one is not equal to zero. Because in three we assume that not all of them are zero, one of them is not zero. Assume that alpha one is not zero and then put or consider your function f to be g one. Then what do we obtain? We obtain alpha one f of x one plus alpha two fan is equal to g one x one alpha two g one at point x two and g are chosen such that this is equal to zero, except when the two indices coincide, which is equal to one and this is alpha one not equal to zero. So we have a function, a candidate such that the combination is not equal to zero. And so we have, we have three, so four implies three, and finally y one implies four. This is based on previous corollary.
00:08:29.864 - 00:09:59.250, Speaker A: One means that for any n and for any x one up to xn, kxi kxj is strictly positive. In particular, this is invertible and the corollary says that there is a solution. There is always an interpolating function. This is the point I used previous corollary and so this ends ends the proof of number. This is one implies four. There's always more details. Always interpolating function means that doesn't matter what is your lambda one up to lambda n.
00:09:59.250 - 00:11:01.514, Speaker A: So this means that there is g one such that is equal to 10 zero. Again we apply it for g two. There is g two such that g two. Whatever you want to put on the right side is okay, because always so I know I put zero 10 to zero. And finally there is gn such that zero one. That's the end of. So in short, in short, we can say that there is gi such that gixj is equal to delta ij.
00:11:01.514 - 00:12:35.644, Speaker A: I mean, this is briefly speaking, so all the time we have a solution. And that's precisely the content of the force statement partition of unity. You have for sure not seen this before in other contexts. We have this in geometry, in manifold theory, even in these studies, g one up to gm are called partition of unity gm it satisfies. The above property are called or this set is called a partition. If you know g one up to gn, then you immediately have the answer to any interpolation problem and even the minimal solution that you are looking for. Because if g one up to gn are given, and you want a function h such that h at point xi to be lambda I, then we can simply take h to be lambda one, g one plus lambda two, g two plus lambda ngn.
00:12:35.644 - 00:13:51.090, Speaker A: By the defining properties of g, we immediately see that we have this property. And also because each of them g one g two gn are linear combination of x one up to x k x one up to k xn. So h is also in the span of k x one up to kxn. Interpolating in this span, therefore, h is the unique solution of minimal norm. So the ultimate answer that we were looking from the beginning, uniqueness and of minimal norm being of minimal norm is important in complex analysis. The solution to the extremal problem, immediately you plan the answer, just obtain g one up to gn, and then do this combination. Then you are done.
00:13:51.090 - 00:15:00.728, Speaker A: That's, that's the final answer. It's the good thing about partition of unit. A small section here for best. Lisa square approximation approximate as I said here, sometimes the solution does not exist. I mean, if x one up to x n in x distinct as usual given, and also lambda one up to lambda n in C, this one is also given. Sometimes there is no f in h such that f of xe is equal to lambda. So interpolation problem has no solution.
00:15:00.728 - 00:16:21.842, Speaker A: But still we can ask this question. Consider the quantity I think it's called j of j of in the book j of is equal to the sum I from one up to n f at point xi minus lambda I absolute value squared. For f in h, the ideal is to have f such that each component on the right side is zero. So the sum is zero and j of f is equal to zero. That's the ideal solution. In other words, interpolation as a solution if and only if j, if and only if there is an f in h such that j of f equal to zero. That's the meaning of interpolation.
00:16:21.842 - 00:17:23.992, Speaker A: But if not, give me an f such that j of f is minimal, is not zero, but it's as close as possible to zero. That's the meaning of least square, this mini square approximation. So we are looking for an f with minimal error. And the theorem says that it's kind of straightforward. Theorem again, the idea is the projection that we saw before trm same assumption x. Set h r k h s on x spawn up to xn in x distinct. This is as before and v.
00:17:23.992 - 00:18:01.084, Speaker A: I mean the vector lambda one to lambda n in c n. And for simplicity, I mean write it q. Here q is the matrix k xi j. We call this before g for gramion, but in the book is written q. The nominal notation really doesn't matter. Then there is. Here is ten.
00:18:01.084 - 00:20:24.404, Speaker A: There is an existence a vector in Cn call it alpha one up to alpha n c n such that v minus q applied to w before, when the solution exists, v was equal to q times w. It was a solution. But here v minus qw is not zero, but it's in the null space. In other words, I mean, I'll explain a little bit better. We decompose v as a projection on the null space of q and the range of q. If we set g two using these alphas, alpha one k x one plus alpha n k x, then g minimizes the parameter g that we define it all. Then mini y, mini myses j, and also g is unique function of minimal norm.
00:20:24.404 - 00:21:13.716, Speaker A: With this property. Again, attention to the uniqueness. I presented the function g which is a solution which minimizes j, the quantity j which was defined here. But I don't say the solution is unique. Generally there are other g's which minimizes two. However, if we are looking for a function of minimal norm, then it is unique and it's given by the formula which is mentioned here. You see the importance of minimal norm.
00:21:13.716 - 00:23:16.004, Speaker A: Again, true, it's as before. As before, when the solution exists, the solution is not necessarily unique. We can have many interpolating functions, but if you are looking for a function of minimal norm, then it becomes unique and well, not that difficult for any f in h, we see that we have a vector f. This is a vector for which the interpolation has a solution. It's the function f which is already given and therefore this is in the range of q by the very definition of interpolation and by the theorem we saw before. Why it becomes like that. Then there is so there is a vector w such that w in Cn such that qw is our lambda and our lambda here is f of x, one up to f of x.
00:23:16.004 - 00:24:43.182, Speaker A: This is based on the characterization we had in the previous main theorem, that interpolation theorem has a solution if and only if our vector lambda is in the range of q. And here is trivially the case because f is already given and so we can write j of which is sum j from one up to nice f of x j minus lambda j squared to be some j from one up to n. Well, the sum is not needed. This is q w minus v squared. In Cn, v is the vector lambda one up to n, and we immediately see that this is minimized. Look, v is fixed. W is changing because our function f is changing.
00:24:43.182 - 00:26:05.344, Speaker A: When the function f changes, this w here changes too. And so we have different combination here with v fix and w changing. And when is this? The minimal one. We saw that j of f is minimal if and only if q. W is the projection of v on the range of q, and it's the projection is the minimal one. So in a sense here it's then you think, I mean, it's kind of trivial that we obtain another way to look at this. And noting that q is self adjoint, q is equal to q star, we can write, we can, q is equal to q star.
00:26:05.344 - 00:27:19.164, Speaker A: Then the range of q is n of q star per in the general case. But here is equal to n of q, because q is equal to q star. And so our space c n can be written as n of q per plus orthogonal sum of n of q and n of q purpose and instead of n of q purp, we can write r of q. So c n is equal to r of q plus n of q. And this is an orthogonal projection, orthogonal summation. So any vector which is given over there, any v, any v which is given, we can project it on r of q and put this part in the garbage. And this gives us our best least mean square approximation.
00:27:19.164 - 00:28:16.064, Speaker A: That's the solution we are looking for. The rest, we have already seen that g given like that, is the minimizer and sub minimal minimal norm. We have already seen. The only observation which is needed is precisely this one. And for any v projection is possible, and it gives us the best, the best approximation. Well, the last section is probably the most important section of this chapter. The elements of h of k.
00:28:16.064 - 00:29:48.774, Speaker A: Why is this important h of k? It's elements of functions on the set x, not all the functions. So it is true that if f is in h of k, then f is a function defined on x, which is values on C. That's by the I mean main definition. But on the other hand, if f is given a function given on x which is values on C, we need some extra assumption. And here is what we want to study, such that under this extra assumption we can conclude that f is in h. Very valuable result if we can get it. We will see one of them here, because in some spaces like h of b, dobrange, Rovniac space, space, we start with the kernel k k of z, and w is given by one minus p w bar one minus w bar z over that.
00:29:48.774 - 00:30:32.126, Speaker A: That's a kernel. Well, it's an exercise. Suppose that we can show that k is bigger than or equal to zero. We pass this story so we have a space which we can denote it by h of k, or sometimes by h of b. H is more common good. Now, what are the elements of h of b? And it's a very difficult question and still a clear answer in a sense that we have the answer for Hardy space or for Bierschel space or Beckmann space we do not have for hp. That's the main problem with hp.
00:30:32.126 - 00:31:15.884, Speaker A: We do not have access to many elements of them. Some of them we know, for example, the kernel functions we know, we know under some conditions polynomials are there, and under some conditions not all polynomials are there. Some of them are there, not all. So these things we know. But characterizing all the elements and have a clear criteria like the one we have for the Hardy space, no, still we don't have, and that makes it difficult to prove theorems for hp. And there are some other examples too. So it is important to, to have results like this.
00:31:15.884 - 00:32:08.684, Speaker A: A function is given, we verify the property, some properties that we are looking for, and then this is a guarantee that the function stays in the space. This is what we want to do. And to do this we need the general notion of convergence of the net. We briefly discussed this before, just as a reminder. A general net x the index set I is a directed set. For us, it's usually subsets of a bigger set, not all of them, some of them. And the relation here is the inclusion.
00:32:08.684 - 00:33:06.618, Speaker A: But the property, the important property that we need I to possess is that if I one and I two is in I, then there is an element I such that it's bigger than both. This is the crucial property that most of the time we use when we are dealing with a sequence on the integers one, two, three. This is true trivial. Any two integers, I mean, take the bigger one. And when we are dealing with subsets, usually it's closed under the union I one is a subset, I two is a subset. For I takes the union of I one and I two. So in that case it's also what we want to have.
00:33:06.618 - 00:34:50.014, Speaker A: It's trivial, but in abstract setting, the the Moore Smith convergence that we mentioned last week, the crucial property that we need to have is this one. Any two elements have something which is bigger than both of them. And what does convergence mean? What do we mean by limb x yota and limit on yota is equal to x. This means that I mean the epsilon delta definition that we have before it's written like this here, for every epsilon there exists a delta zero, which depends of course on epsilon, and it may depend on many more parameters, but at least to the epsilon here, such that if our index is bigger than or equal to zero, then the distance is small. We are dealing with complex number here, or at most with norms. I mean, either this or this is less than or equal to absolute. The distance between x, UTa and x is b for all parameters or for all values of Uta, which is bigger than fixed one.
00:34:50.014 - 00:36:06.084, Speaker A: It needs a bit of practice. It's not the same as sequences, major differences between them. And if you're not familiar with this, many mistakes easily can be, can be made. And for example, one of this, which is a common one, is that for sequences, when we have limb x n equal to x and any epsilon given there is an n zero as above, such that n bigger than this index, then the distance between xn and x, either absolute value or norm, is less than epsilon. What is not counted here? I mean, the indices for which this is not true is a finite number of indices is at most from one up to n zero minus one. At least here the indices which are not taken care of, the rest. For the rest it's true.
00:36:06.084 - 00:37:04.784, Speaker A: However, this is not the case here. This is not the case here. If yota is bigger than or equal to yota zero, then okay, that works. But what are the indices for which this is not true? Which indices are not counted here, not necessarily finite number of them. It could be infinitely made and that's important to keep in mind. Okay, so for this we have a to start, we have a proposition to make us ready for this type of convergence. Same setting as before.
00:37:04.784 - 00:38:29.860, Speaker A: H is an rkhs on x, g is in h, and for each finite set in x, we let G index f be the projection of G on Hf. Again, recall hf is the spanish kx, one up to kx, the finite set x, one up to x. Because I didn't mention x one up to x n. It's better to say kx is in F. F is a finite set. This is better. So for each finite set we project G onto Hof.
00:38:29.860 - 00:39:42.256, Speaker A: So we have now we have the net gof again, and that G index f and f is on the set of all finite subset of G. And here is the result ten. This net converges to G. See, it's not a sequence you can imagine even in the form of trees. But be careful. Even if we imagine finite set trees means stuff like this, or here, this is a finite set. This is another finite set.
00:39:42.256 - 00:40:24.214, Speaker A: This one is the union pulse. And it can, I mean, have many branches like it can be very, very complicated. And a good thing about imaging like this, imagine like trees, is to see that when you consider if this is your f zero, all the f which are bigger than f zero s means this one. I mean, all the one which are above. Those which are not counted can be infinitely many. None of these are bigger than or equal to f zero. And it can be many, many, many of them.
00:40:24.214 - 00:41:21.980, Speaker A: Contrary to what we have in a, in a sequence. Well, still we have the limit of gf as a net equal to g. And the proof is a good exercise of doing limits with nets. And here is the way we do it. We saw before that the kernel functions are dense. Before that, if we consider kx not for finite x in x, all of them. And span, well, span, it depends which book you study.
00:41:21.980 - 00:42:28.834, Speaker A: Sometimes span means just linear combination. Sometimes it means linear combination. And with closure, well, to, to make it more transparent, I put the closure here. We saw this before that the linear combination of kernel functions is dense in this space. So it about maybe three weeks ago. And therefore, given, therefore, given epsilon bigger than zero, there x x one up to x n. And also there is in x scholars, alpha one up to alpha n in C C, such that the norm of g minus some I from one up to n alpha I k x y in h is less than s.
00:42:28.834 - 00:43:41.610, Speaker A: That's the meaning of being dense. True. And look, I don't say that this combination sum I from one up to n alpha akxy is the projection of g onto f. F is the finite set x one up to xn, and hof is the, and let's call it f zero spanner kx one up to kxn. Here, closure is not needed. Is finite dimensional more space. Okay, I don't say that the combination I obtain is the projection, but I know that the projection is the best one.
00:43:41.610 - 00:45:00.774, Speaker A: I mean, if I consider g index f zero, which means the projection of g onto this space, then I know that the norm of g minus g of f zero is less than or equal to the norm of g minus. Anything in that space, sum I from one up to n beta I k x I for any beta I. That's the meaning of, I mean, hidden in the meaning of projection. In particular, in particular, norm of g minus gf zero is less than or equal to put alpha I, which is less than x. So the choice here is a good choice if I replace it by the optimal one. Still, it's a good one. G minus even better.
00:45:00.774 - 00:46:45.064, Speaker A: But it really doesn't matter. The good thing is that it's less than epsilon for f zero. Now, what happens if we consider a bigger set here, less than or equal means that subset, if we consider a finite set which is which has more element than f zero. Well, clearly hf zero is a subspace of hf because we have more elements in f. And what we can say about gf g index f is the projection on Hf G. And since it's a projection, it means that, I mean, assume that f is equal to, say, x one up to xn, and these are the elements of if zero a little bit more, say y one up to ym, it's bigger. The projection means that the distance between g and g f in h is less than the distance between g and any other element in that space.
00:46:45.064 - 00:48:17.116, Speaker A: Any other element means that the sum j from one up to n beta jkxj and k from one up to m eta k yk. This is an arbitrary element of h index f for any combination, no restriction on beta and no restriction on eta. This is always smaller. In particular, we can take all eta k to be zero and choose beta k such that this combination gives us gf zero. In particular, take eta k, all equal to zero, and beta j such that the sum j from one up to n beta j k xj is equal to j index f zero. This is possible by the definition of g f zero. Therefore, g minus g f is a smaller than gift minus g index fc.
00:48:17.116 - 00:49:49.844, Speaker A: And we know this is a small. Therefore, for any f bigger than or equal to f zero, g minus gf is less than epsilon. That's precisely the meaning of Lim gf equal voltage. Well before our next break, let me recall you the relation or partial ordering that we have in the set of self adjoint matrices. So we consider matrices a such that a star equal to a and b star equal to b. I mean, these matrices, in this setting, we saw the meaning of a bigger than or equal to zero. You can say that this is equivalent to say that all the eigenvalues are zero, or if you wish, inner product away, x and x equal to zero.
00:49:49.844 - 00:50:48.174, Speaker A: And then definition, I mean, a is less than or equal to b. Here it means that the matrix B minus a is positive. If B minus a is positive, we say that a is less than or equal to b and it has all the properties of a partial ordering in the family of self adjoint matrices means that it's transitive and a is less than or equal to a. And also you can multiply a positive scale. This is something that I will, I will need here. Okay, let's have another break and then we come back to this after the break.
