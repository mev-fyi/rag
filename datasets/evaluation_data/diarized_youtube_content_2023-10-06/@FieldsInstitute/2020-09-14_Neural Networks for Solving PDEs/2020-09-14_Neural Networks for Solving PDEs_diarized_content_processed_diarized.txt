00:00:01.280 - 00:00:45.386, Speaker A: Hi everyone. So this video we will discuss how we can use neural networks to solve partial differential equations. And specifically we will discuss how we can define the underlying loss function in such a way that when we minimize this loss function, the obtained neural network is going to be close to the true PDE solution. So before I continue, this is based on joint work with Remk of van der Meer and Ks Oesterle at the CWI in Amsterdam. So to start, let us look into the neural network architecture that we're going to be using. We're going to be using a feed forward neural network, meaning that all of the nodes in each layer are connected to all of the nodes in the previous layer. And for the first layer, this means that our input x is going to be multiplied by our weight matrix w and then pass through a nonlinearity, some function f.
00:00:45.386 - 00:01:20.754, Speaker A: This is going to be the output from our first layer. And this in turn is going to get again multiplied by some weight matrix w, pass through, and linearity and so on, until we reach the final layer, in this case capital l. So I denote by theta all of our parameters in our neural network. So all of the parameters that we can tune to minimize the loss function. And f is a nonlinear activation function that we use. So the optimization problem is then given by our goal is to minimize this loss function. So find some configuration of the weights theta such that this loss function is as small as possible.
00:01:20.754 - 00:02:11.604, Speaker A: So the PDE that we consider, we will consider a general form of partial differential equation where we have the solution of the PDE u defined on x, which is our domain. And what we have is that on the interior of our domain, so on omega, our u is going to be satisfying some differential operators curly n and on the boundary of the domain. So d of omega is going to be satisfying some other differential operators on the boundary of the domain. So there are different approaches for tackling this kind of problem. The first one I want to briefly mention is a supervised approach. So in this case, what you do is we assume that we have access to some other solver of the PDE. And basically what we do is we create a data set of solutions.
00:02:11.604 - 00:03:08.824, Speaker A: So we take points in our domain, so in our x, and for each of those points we compute the solution U of x using the solver that we have. And then our goal will be to train a neural network such that it's going to be learning this mapping from our domain onto our solutions of the PDE head of U. So essentially, in this case, what we're doing is we are finding the weight configuration theta such that the loss function between the neural network. So U of theta of X is very close to the true PDE solution U of X. So we do this on our train datasets, so on this generated data set that we generated using another solver. And then we hope that this will additionally generalize the new inputs x. Now, the downside of this supervised approach is that we assume that we have access to another solver, and in certain cases it might not be the case, or this could be very computationally expensive.
00:03:08.824 - 00:03:49.836, Speaker A: So the question is, can we do in another way? The answer is yes, and we can do an unsupervised approach. So in this case we do the same thing. We select some interior and boundary co location points xy and xb. So xy are the collocation points on the interior and xb are the collocation points on the boundary of our domain. And then we solve the PDE by minimizing a single loss function which includes all of the constraints. So all of the differential operators that our PDE had to satisfy, we include them all in the loss function. So on the interior domain, this means that this guy, so this curly n of r points on the interior domain nrpde solution, this guy should be small.
00:03:49.836 - 00:04:28.830, Speaker A: In particular, for the true solution, this sets phi zero and the same for the boundary. So this guy should be small. For the true solution, it was equal to zero. And we do this both summed over all of the co location points that we have. And in this particular case I also worked with the l two norm. So in this case, what we hope is that if we have minimized this l of theta, then this hopefully implies that our u of theta of x is going to be close to the true solution of the pde u of x. And a good thing here is that once again, no knowledge of the actual solution is required.
00:04:28.830 - 00:05:53.554, Speaker A: We only need to know the differential operators on the interior, so the scurry n and the scurry b on the boundary. Now the question is, why would this work? So why does minimizing this loss function, why is it going to give us a solution of the PDE? Well, we have a result for this. So basically, if we observe that our loss that I had in the previous slide is basically a Monte Carlo approximation of the following loss functional, where instead of the summations, we have these integral terms over the interior and over the boundary. So what we can then prove for this particular loss functional is the following result that we have in our paper which says that if rpde is well posed, and if we let the loss function be defined in this way, then for any epsilon greater than zero, there exists a delta greater than zero such that a sufficiently small loss functional is going to imply solution, the neural network solution, to be close to the true solution of the PDE. So this difference between u and heads of u is going to be smaller than some epsilon. And this is basically good, because what does this say? Well, in general, neural networks are universal approximators, so they can, if they are sufficiently large, they can approximate any function. But in practice, the capacity is limited and there might be certain functions which they are not fully accurately representing.
00:05:53.554 - 00:06:41.928, Speaker A: So the loss is not exactly zero. And the previous theorem that shows that this is in general not an issue, since as long as our loss sufficiently small, this is also going to imply that our true PDE solution is going to be approximately given by our neural network solution. The one thing that I do want to mention here is that I say approximately given. And of course, this guy can still be relatively far apart from each other. And this is also the problem that we're going to see later on in the video, since we are not necessarily able to always get this loss up to a certain delta small. But before that first, and one more thing. So why neural networks? Well, neural networks are in general, like I said, universal functional approximators.
00:06:41.928 - 00:07:42.804, Speaker A: So the hope is that they can be used to accurately represent any kind of complicated PDE. In particular, there's this very interesting result from this work by Siriano and Spilio bulos, where they use this kind of thing known as the infinite width limit of a neural network. So as a bit of background, the infinite width limit of neural network is a very useful limit, since basically in this limit, the resulting network output is shown to be given by something that I here defined as a simple object. Now, what necessarily, I mean with simple, I'm not really defining here, but essentially, for example, there's this work on the neural tangent kernel, and there they say that as the infinite, the network width tends to infinity, the neural network output is going to become close to a linear model. So a simple object. So this limit is also useful to derive some insight into the PDE approximation. So why are our neural networks actually useful for solving PDE's? And so this result basically says the following.
00:07:42.804 - 00:09:04.348, Speaker A: So if we let ru be a one layer neural network with n hidden units in that layer, then under certain conditions, there exists a configuration. So weight configuration, and corresponding neural network, u of theta, such that this last functional is going to be tending to zero as n tends to infinity. So basically what this says is if we make our neural network sufficiently wide, then this implies that our l of u is going to be also approaching zero. And this in turn implies, as we saw from the previous theorem, that this would then mean that our u, so our neural network u is going to be close to our true PdE solution u hat. So this is good, because basically if we are able to set up a neural network which is sufficiently wide so has a sufficiently large number of nodes per layer, then we're going to be able to probably find a neural network configuration u of theta such that our loss function is going to be very small, which in turn is going to imply that our neural network solution is close to the true PDE solution. Alright, so now that we've had some intuition, let's get into the actual approach that we're going to be using. So remember, again, we are considering a general PDE of the following form.
00:09:04.348 - 00:10:05.374, Speaker A: So this curly n is a differential operator on the interior of our domain, and this Curly B is a differential operator on the boundary of our domain. And then we follow basically this work of Raisi, Perdicaris, Cornea, Dacis, and again Siriano and Spiliopoulos, where the authors propose to minimize this Monte Carlo approximation to the loss functional where basically we set up a number of NL interior collocation points and a number nb of boundary collocation points. And then we try to minimize the differential operators on both of these interior and the boundary where this u is our neural network with parameters theta. So our goal is to find the weight configuration of the neural network theta such that this loss function is minimized. And using what we saw in the previous slides, this is then hopefully going to imply that our u theta is indeed close to our true solution. You had and the good news is that as these papers show, for a certain class of PDE's, this all works very well. But this is where the bad comes in.
00:10:05.374 - 00:10:51.550, Speaker A: It doesn't always work very well. So before I show those examples where it doesn't, let me briefly mention the neural network setup that we are considering in this work. So we are considering a neural network with four hidden layers and 20 neurons per layer. So in total this gives us quite a lot of degrees of freedom. The activation function that we set is the hyperbolic tangent, and then we train the model with a quasi Newton algorithm, the L BFGs method. Since we found that this outperformed first order methods such as Adam, and last but not least, we used the l two norm to minimize the loss functionals. All right, so I said that it works, this minimization of the loss function, but it doesn't always work.
00:10:51.550 - 00:11:37.340, Speaker A: So let's look at an example where it doesn't always work. So if we consider the Laplace equation in two dimensions, so d is our dimensionality, and it is given by two on the interior, we need to satisfy this equation, and on the boundary, our u satisfies some function g of x, where g of x, we define it to be a relatively complex boundary. In particular, we let this omega determine kind of the frequency of the boundary. So this omega is a multiple of PI, and then this g of x is this sine times the exponential. And we want the boundary condition to be satisfying this function. So what we then see is if we set our w sufficiently high. So in this particular example, I set it to ten times PI, and we try to optimize our last function.
00:11:37.340 - 00:12:31.522, Speaker A: So this function that we had, l of theta, then we get that the l two error of the approximation. So how close our PDE solution is to our neural network approximation is very far. It's quite a large number, 2.44. And if we look at the picture here below, we also see the same thing. We see that in the boundary, our solution is not at all satisfying the boundary condition that we had, which is this high frequency condition. So more specifically, what we found is that as we increase the frequency, so if we put the frequency sufficiently high, and in particular higher than four times PI, then this default method, where they, where we minimize the loss function of the interior loss plus the loss function over the boundary, it didn't really work as well as we wanted it to. So some understanding into what is actually happening in this case.
00:12:31.522 - 00:13:11.304, Speaker A: Well, as I said, this default method results in an inaccurate approximation. However, it does have a relatively small loss. So if we look at this picture, look only on the orange line, which is the default approximation. So where we are simply minimizing li plus lb, what we see is that the interior loss. So this picture on the left, this guy, our loss is sufficiently small, it's ten to the minus three. However, our loss on the boundary, so this picture, it's not as small, it's ten to the ten to the minus one. So first of all, we observe a difference in the magnitudes.
00:13:11.304 - 00:14:02.344, Speaker A: And second of all, we observe a relatively high boundary loss, which might be the reason why we are seeing that the solution doesn't fit the true solution on the boundary. What exactly does this mean? Well, so first of all, the thing that it means is that the value of the loss is not fully representative of the accuracy of the solution. So, remember that I said that we had this result where a small loss would imply also that our true solution is close to the PDE solution. This still holds. However, we're not always able to get our loss as small as we need. So we're not always able to get this delta sufficiently small. And if in cases that we aren't, this loss function can still imply a relatively high discrepancy between the neural network approximated PDE and the true PDE.
00:14:02.344 - 00:15:02.984, Speaker A: So the underlying thing that, the underlying reason for this problem is that essentially our setup is a multi objective optimization problem. So we are not only, we don't only want to minimize the sum of the interior, the boundary loss, we actually want to minimize them both. However, given that the neural network has unlimited capacity, since we're always using a finite number of nodes, it has a certain limited capacity, it may not always be possible to minimize both at the same time. So in particular, the neural network might decide to focus more on the interior loss, or it might decide to focus more only on the boundary loss. So, since, and so in a previous objective that we used, we used an equal weighting. So we said that our objective function was l one, sorry, l interior plus l boundary, and this might not be the ideal solution. So it might not be best to equal, to give equal weight to both the interior and the boundary.
00:15:02.984 - 00:16:06.034, Speaker A: And we might need to be more careful in how we define our single objective loss, which is essentially a simplification of our actual multi objective loss. So what we propose in this work is to define a certain weighting, where the weighting is defined by this parameter lambda, where we give this lambda weight to the interior differential operator and one lambda to the boundary differential operator. So if we define this weighting with lambda, then if we again look back into our problem. So we set this omega so the frequency of our boundary to be relatively high, in particular tan PI. And then we search over all the possible values of lambda that we can have, and we select the best one, where with best one I define the one that gives the lowest loss to the closest solution to the true PD. If we search for this one, then we find that this particular lapta is going to be the best one. And note that this is very different from the original weighting, where we basically had that lapta was one two.
00:16:06.034 - 00:16:45.484, Speaker A: And in this case, what we find is that the l two error of the approximation is also as small as we would want it to be. In particular, it's just 1.5 310 to the minus two. So for this particular weighting of the loss function, where we essentially are giving a lot less weight. So remember, our loss was this lambda times our interior, plus one minus lambda times our boundary loss. So if we select this particular lambda, we are essentially giving a lot less weight to the interior and a lot more way to the boundary. In this case, our neural network is indeed also able to fit the boundary as we would want it to be fit.
00:16:45.484 - 00:17:39.994, Speaker A: So what is happening here? Okay, so remember that the orange line was the default. So the one where lambda was one, two, and then the blue one is the optimal one. So that's the lambda as we saw in the previous slide, if we search over all of the values of labda and find the best one. So what we then see is that for the default one, we had ten to the minus three interior loss and ten to the minus one boundary loss. And for the optimal one, we basically approximately have ten one interior, and let's say ten to the minus five boundary. So from these expressions, it might not necessarily be fully clear which one is going to be performing better or worse. And you might actually think that the optimal one is going to be performing worse, since it has a whole lot higher interior loss.
00:17:39.994 - 00:18:49.726, Speaker A: But this isn't actually the case. So the default one with lab test one two is actually giving a worse solution than the optimal one, even though in terms of the loss, it might not be that this is what we think. What does this mean and what does this imply for our laptop? So, the first thing it means is that our loss function value is not necessarily representative of the goodness of fit of the actual PDE approximation. So even though a small loss would imply also a sufficiently close neural network to the true PDE solution, it doesn't necessarily mean that we that if this loss is small, it's also going to always mean that this is sufficiently small or good. It can actually also mean that this is not the case, as we saw in the previous slide. So, the question that we then have to choose or answer is, how do we select the lambda? So, lambda determines the weighting that we give to the interior and to the boundary. And the question is how to choose it.
00:18:49.726 - 00:19:36.884, Speaker A: Well, one thing is we can iterate over all the possible lambdas that we can have. But as we saw that, I mean, obviously that is very computationally expensive, and that's not necessarily something that we wanted to do. So our aim would then be to find a loss function. So, to define a laptop in such a way that the solution of the neural network which minimizes this loss function is as close as possible to the true solution. So the true solution of the partial differential equation. And what we can note here is that if the value of lambda is changed such that the interior loss becomes more important, the neural network can sacrifice some accuracy on the boundary to reduce the interior loss and the other way around. So in this way, without actually changing the total loss value, so l remains the same by changing the weighting.
00:19:36.884 - 00:20:35.952, Speaker A: So by changing this lobster, we can give different weights to the interior and to the boundary loss, which then results in different solutions, which all still have the same loss functional. So this is exactly the point. The loss functional is not necessarily as informative about the goodness of fit of the true pd solution. So then how to select this value of lambda? Well, we're going to use this concept of epsilon closeness, which basically means that for the solution to our neural network and true solution of the PDE, we want their derivatives to be close together, in particular, close enough with this value of epsilon. One very quick thing that I do want to mention here is that this notion of epsilon closeness doesn't necessarily hold for all the PDE's it depends on the particular one. But let's assume that for the one we are considering, it is sufficient to define it like this. Now remember again that as I said before, now the same loss can be obtained with different values of lambda.
00:20:35.952 - 00:21:09.484, Speaker A: So in particular, for a large lambda, small loss can be obtained by minimizing only the interior forgetting about the boundary. And for small lambda, a small loss can be obtained by minimizing only the boundary. So the value of l is going to be similar in these cases. But our values of lambda can determine different kinds of solutions. So the question that we have to ask ourselves is, well, suppose we have a fixed total loss l. How do we select lambda in such a way that this absilence. So the closeness in terms of our true solution of our neural network to our true solution is as small as possible.
00:21:09.484 - 00:21:56.130, Speaker A: So if we then consider a linear partial differential equation just for the sake of the derivation, what we can then obtain. So if we set our linear PDE to be in the following way. So f is going to be this differential operator and g our boundary is going to be this differential operator. And we assume that this is what we have to satisfy on the boundary on the interior, and this is what we have to satisfy on the boundary. In this case, we can derive bounds on the interior loss and bounds on the boundary loss and in particular, these are the bounds that we can derive. And what these show is exactly why the loss is not always informative. Specifically, if we have a high value of the loss, or let's say, if we have a low value of the loss.
00:21:56.130 - 00:22:25.290, Speaker A: So if, let's say this interior loss, l I, is low. Well, what this can either mean is that epsilon is low. And that's good, because that would mean that our solution is actually close to the true PD solution. But what it can also mean is that this derivative is just small. So this derivative that we have defined that our interior, that our PDE should satisfy on the interior, is small. And in this case, this epsilon could still be large, but our loss is still going to be small. And the same thing basically happens for the boundary.
00:22:25.290 - 00:23:15.780, Speaker A: So if our loss is small, this could either mean that indeed our epsilon is small, or it could also mean that the magnitude of the derivatives is small. So if the derivatives are large, basically what this means is that the solution can still be accurate while having large, large values of the loss. So large errors l. But if the derivatives are small, the loss has to be made very small, because we really want to say that we also have to obtain a very small epsilon. Otherwise our solution is not going to be close to the true PDE. So if we then say that we want to optimize our lambda in such a way that our epsilon is as small as possible for a fixed loss value l, then after what some calculations, one can obtain the following weighting of our loss. So the following value for lambda.
00:23:15.780 - 00:24:26.720, Speaker A: Basically, this shows that if we remember again that our lambda is defined as lambda times the interior loss plus one minus lambda times times the boundary loss. What this says is that if our interior derivatives are relatively large, so if this guy is relatively large, then our lambda is going to be very small, and our neural network is going to place a lot of attention on minimizing the boundary loss, which is exactly what we want. Since in this case, we really want to make sure that we are focusing our attention also on making this epsilon for the boundary small. If, on the other hand, our boundary derivatives, so these guys, and these guys are a lot larger than our interior derivatives, then basically this means that our lambda is going to be large and we're going to be focusing a lot on the interior loss. And this is again exactly what we want. Since then, we would really want to minimize this epsilon for the interior loss, such that our PDE solution is going to be close to the true one. So this essentially gives us some insight into how we should choose the lambda.
00:24:26.720 - 00:25:24.874, Speaker A: So, to summarize, it basically tells us that we should choose lambda in such a way that we are trading off the magnitudes of the derivatives of our PDE. So essentially, if we have a very high interior derivatives, then we're going to be putting more value on the boundary loss and the other way around. So, does this actually work? Well, yes. The good news is, obviously it works if we consider a Poisson equation with an oscillating boundary. So a boundary which goes like this, then the original method, so the one where our labs that was chosen to be one half isn't really performing well. So again, we're not really fitting the boundary as well as we should be. But our method, so the one where we choose lambda according to this magnitude of the derivatives, it actually works pretty well because we are indeed able to fit the boundary quite nicely.
00:25:24.874 - 00:26:05.574, Speaker A: So, to finish off, I want to finish off with some questions. So, we looked in this work on trying to select the optimal lab time. However, this is not the only parameter that we have control over. For example, we also have control over the amount of data points that we use. So a question that one could ask is, well, if we increase the number of data points, what's going to happen? How is it going to affect our solution? We could also change the network size. Remember that I said if the number of nodes in the layer goes to infinity, then we are actually able to obtain a loss which goes to zero. So it might also be that instead of choosing this lambda in such a way, it might also be interesting to increase the network size.
00:26:05.574 - 00:27:14.798, Speaker A: Obviously, the downside here is that this also increases the computational costs, which already can be very high. But in general, the network size also affects how well we are able to find a solution to the PDE. So, related to these two questions, one could ask, okay, if we generate a large amount of data points and use an over parameterized neural network, are we then not able to satisfy both constraints easily? And is this not an alternative to the choice of, or at the same time, if we, instead of choosing labta, could we focus on choosing the collocation points in some kind of smart way? So, for example, giving more weights to locations where we're making bigger errors. So, to conclude the things that we have seen, basically we discussed the unsupervised setting where we are trying to use a neural network to learn a PDE solution. We set up the loss function in such a way that it encodes the PDE objective. So what we must minimize to what must be minimized to obtain a solution to the partial differential equation. So we observed, first of all, that a low loss does not necessarily mean an accurate solution.
00:27:14.798 - 00:27:56.140, Speaker A: So we did have this result where a low loss implied that our neural network was going to be close to our true solution. However, this notion of closeness was a bit vague. And in particular, we saw that for particular values of the loss, we could obtain solutions which are indeed almost the same as our true solution. Or we could obtain solutions which are actually very far off the true solution of the PDE. And in particular, we saw that if we use this equal weight between the boundary and the interior loss, the solution that we found was often not good enough. It wasn't satisfying, for example, the boundary conditions. So then we propose to use a different weight.
00:27:56.140 - 00:29:14.724, Speaker A: So, to choose a lambda in such a way that our boundary and our interior loss is more balanced. And we motivated the way we choose this lambda, indeed by the magnitudes of the derivatives on the interior and on the boundary, since if the magnitudes on the interior, for example, are a lot higher than on the boundary, our neural network should be focusing a lot more of its attention on minimizing the boundary loss, because this is already small. All right, so to conclude, I want to mention some other interesting literature for if you're interested in reading more about this, since our paper is one of many, many, many interesting works. So first of all, there's this paper, machine learning for semilinear PDE's, where they try out different network architectures and make conclusions on what is working, what is not working. Then there is a paper solving high dimensional PDE's using deep learning, where instead of minimizing the loss function that we were minimizing, they try to solve the backwards stochastic differential equation that relates to the PDE using a neural network. This paper is also pretty interesting because there they give a proof that neural networks are actually able to overcome the curse of dimensionality. So why are neural networks actually useful for solving high dimensional PDE's? It is because you can provably show that they can overcome this curse of dimensionality.
00:29:14.724 - 00:29:31.674, Speaker A: And then these two papers are the ones that we build upon. So these are the ones that use the loss function with the equal weighting. And this is our paper where we propose a loss function with a different weight and give some intuition into how to choose this weight, primary lambda. So that's it. Thank you so much, bye.
