00:00:00.240 - 00:00:57.754, Speaker A: Great pleasure to introduce Professor Martin Herrer from Imperial College and called Polytechnique de Lausanne. Martin has a PhD from the University of Geneva under supervision of champier Ekman, and prior to that, head positions at Warwick and NYU. He's a knight commander of the British Empire. So, Sir Martin Fellow of the Royal Society and also a recipient of many awards, including the 2021 Breakthrough Prize in mathematics and of course, the 2014 Fields Medal. And I don't have any personal anecdote with Martin except for heaven beer with him at the sin and redemption after his Fields Medal symposium lecture in 2017, which is, of course, one of the perks of being at the Fields Institute, of great relevance for me, but of no significance to anybody else. So I'm looking forward to a second memorable event with Martin.
00:00:57.812 - 00:02:07.390, Speaker B: Thank you. Okay, thanks a lot. Thanks a lot for the very kind introduction and thanks to, well, and Theodore for organizing this event. So what I want to talk about today, so, it's based on joint work with some with my former postdoc Tommaso Rosati, who's now at Warwick, and some with my current PhD student Wen Hao Zhao, at. But because that's the specific result I want to talk about, is basically just an excuse to give you a bit more of an overview. And so I should start by like, okay, what is this about? So the motivation. So what is, okay, the title of the talk.
00:02:07.390 - 00:02:29.670, Speaker B: So it's about singular stochastic PD's. Why are they interesting? So what makes them singular is that they're going to be, you know, there's going to be noise terms. So somewhat probability talk, even though it's going to be much more analytical. And the noise is very rough. And so that makes the solutions very rough as well. And that kind of makes them singular. Right.
00:02:29.670 - 00:03:25.490, Speaker B: But why are they interesting? I guess there's two motivations. So, motivations, like in Greg's talk, they come from physics, and there's motivation, on the one hand from statistical mechanics and on the other hand from quantum field theory. And in both cases, what you really want to do is you have some energy functional, so you have some space of fields, whatever, phi, and you have some energy functional or action functional, depending on whether you're in the statistical physics or the qft side s of phi. And the kind of thing that you can think of, the simplest one would be like the h one norm square, right, integral, radiant phi square. So that's the canonical example. And almost everything else is sort of perturbations of that or variance on that theme. And then.
00:03:25.490 - 00:04:18.220, Speaker B: So this should be something like integral gradient phi square plus stuff. And the interesting case is when stuff is not quadratic. Um, and what you then really want to do is you want to sort of build the measure mu of d phi, which would be something proportional to e to the minus s of phi, d phi, right, where something like a Gibbs measure in statistical physics, where this would be Lebeg measure. And this is sort of the density with respect to Lebec measure. And obviously, there's no Lebeg measure on the space of fields. And even if there was a Lebeg measure on the space of fields, the fields in question certainly wouldn't have finite h, one norm. And so this wouldn't have any meaning anyway.
00:04:18.220 - 00:05:23.370, Speaker B: And so what you do is you do your favorite way of approximating this, and then what you really want to show is not just that your favorite approximation is a limit, but also that things are sort of canonical. As you know, maybe my favorite approximation is different from your favorite approximation, but we should actually agree on what this guy means. So that's the kind of thing you want to prove. And so there's various ways of doing this. And there was. So this goes back basically to the sixties and seventies, I guess one of the first non trivial construction was actually by your supervisor, Ed Nelson, who constructed this. The simplest non trivial case is the one where here you put a five to the four, and you're in spatial dimension two, which is somehow the lowest non trivial dimension.
00:05:23.370 - 00:06:20.360, Speaker B: And at Nelson, I think in 64 or something like that, or at least sometime in the sixties, kind of built that measure in this particular case. And there was a lot of development on that particular model around that time. So the other interesting action functionals, I'm going to talk about other interesting action functionals later. So the technique for building this measure that I'm interested in is something that was originally proposed actually by Parisi and Wu in the eighties. And what he was saying is that, well, you can look at some kind of PDE or stochastic PDE. So basically a noisy gradient flow. So you think of phi, you can look at the PDE time derivative of phi is minus gradient s of phi.
00:06:20.360 - 00:06:51.660, Speaker B: So that gives you trying to kind of find the minima of this energy or action functional, but then you add noise to it that gives you some random process. And if you add the noise in a way that's compatible with. Right. If you take a gradient, the gradient is always with respect to some metric. Um, and you take the noise, which has covariance. That's like the inverse metric. So it has to be compatible with your notion of gradient.
00:06:51.660 - 00:07:57.160, Speaker B: Then at least formally, say if you're an RN instead of a space of fields, then you get a stochastic process, which is such that you can start with any initial data. You run it for long enough, the law then if it's time, t converges to that measure. Okay, so that's true on if, if Phi were finite dimensional, that would be true. If s is smooth enough and sort of close enough in all directions and stuff like that, right? And so you want to do that. So the idea is you write down formally a sort of destocastic PD, which you guess should have this measure as its invariant measure. And then you make sense of the PD, you show that it has an invariant measure, and then that's your construction. And in the cases where you, then you want maybe a reality check, right? The reality check would be take a case where you already had a construction previously and prove that the two constructions give you the same thing.
00:07:57.160 - 00:08:56.496, Speaker B: And then you're pretty confident that in the cases where you didn't have a construction before, what you did is sort of reasonable. Right? So that's kind of the idea. And so, for example, if you do this, for this example here, right, what's the gradient flow that goes with this? So that would be the kind of phi four gradient flow you would get vt, phi Laplace and phi, right? So that's, say if I take the l two metric, it's the metric for my gradient. If I take the l two functional derivative of that guy, it's just laplacian phi, or maybe two laplacian phi, that will put a half in front. And then the five four term gives you a cube here. And then you have to put noise. So I call it psi and the covariance of that guy, right? So you think of this as being something that depends on t.
00:08:56.496 - 00:10:04.170, Speaker B: And x is delta function in t and delta function in x. And so why should it be delta function in x? It's because, you know, so what does this really mean? It really means, you know, this is a random distribution. You test your random distribution against the test function phi and against some other test function psi. And then this is really an integral of phi of x, t psi of x, t dx dt, which you should think of phi of x, t psi of y, delta of t minus s, tau, x minus y, and so on, right? So that's sort of the rigorous way of writing this. And now here you see, that's just the l two scalar product and that l two is sort of the same l two as the l two gradient that we chose. So it's in that sense that the noise is compatible with the gradient that you've chosen to derive the deterministic part of your PDU. Okay.
00:10:04.170 - 00:11:12.716, Speaker B: Yes, well, so here you can do that, you can do that on a lattice, and then you would get exactly the same equation, and you just get a discrete Laplacian here. And here you would just get like iid white noises on every lattice point. Of course, that's how you would want, like if you want to prove. So by now there is a proof of this fact, which is that in the seventies, people have. So after Nelson constructed that guy in 2d does the works by Glim Jaffe and Federer and so on, we constructed that guy in three dimension. So if you want to, you can give a meaning to the solution of this equation in two and three dimensions, and you can prove that it actually has the invariant measure they constructed at the time, the measure they constructed at the time as its invariant measure. And the way you would prove that is precisely like this.
00:11:12.716 - 00:12:23.370, Speaker B: Right? So you do it on a lattice and you sort of pass to the limit and. Yeah, now the, so another interesting example is young mills, and I'm not going to, not going to write down the equation, it has a vaguely similar structure. So the structure of the equation. So here, now your field is not scalar, but it's, you should think of the a as being a one form, and it's not a normal one. So a normal one form takes a tangent vector, spits out the number, that one takes a tangent vector, and spits out an element of the li algebra, of some compact li group that you fixed in advance. But then the equation is something somewhat similar in the sense that there's a quadratic term, which is how a bilinear term, which depends on the first derivatives of a, and there's a trilinear term, which depends system a itself. And then you would add white noise.
00:12:23.370 - 00:13:28.926, Speaker B: Okay, so the structure of the equation is this. And the exact meaning of these products is the thing that defines your mills, that one has additional complications, which is the young mills equation has a big large symmetry group. And so it's in some sense much less obvious that this measure would have any sort of meaning. And so, one thing that we've done, so in the case of young males in two dimensions, again, people have constructed that measure, but the way they did it for young males is that one can, it has some sort of integrability property in two dimensions. So you can just guess everything. Like, there's explicit formulas for everything, and you don't have it in three dimensions. So in three dimensions, there's basically no construction, though there are results by Balaman that sort of go in that direction.
00:13:28.926 - 00:14:17.546, Speaker B: But there's no actual construction of a probability measure that is any viable candidate for the young mills measure. And so one recent result we had is we actually constructed this process. We gave a meaning to this equation, to solutions to this equation. And, you know, then what's missing for the construction of the young mill's measure in three dimensions, which is still an open problem we haven't solved, that is that the solutions we constructed are just local in time. So we know that for some class of initial data, you get a local solution. There's some, well, positiveness result. It has.
00:14:17.546 - 00:15:04.590, Speaker B: It is, everything indicates that it's the right process in the sense that in two dimensions, one can actually prove that it has the existing young mills measure as its invariant measure. That's a recent result by Harsh and Ilyach Schevirech. And in three dimension, it satisfies a form of gauge invariance, which is sort of this symmetry property of the young mills equations that I mentioned. So there's lots of indications that tell you that it's sort of the right object. But if you don't know that it has global solutions in time, that it might somehow just blow up in finite time, and that tells you nothing. And so you want some kind of techniques that give you control that's global in time. And so that's what I wanted to talk about today.
00:15:04.590 - 00:15:50.342, Speaker B: And the problem here, right? In principle, you think, well, these are just parabolic PD's. I mean, parabolic PD's are sort of the simplest type of PD's that you can possibly imagine. You know everything about them. The problem here is that the noise is very singular, right? Because of these delta functions. So the delta functions sort of show up in the covariance of the noise. So in some sense, it has the regularity of like a square root of a delta function, but in space time, which is still pretty irregular. And so in particular solutions, for example, in this case, as soon as you're in dimension two or more, solutions aren't function valued.
00:15:50.342 - 00:16:24.442, Speaker B: So it's not clear what that cube means. And, well, the same here. And so it's even less clear what that guy means. So it's not like Navier Stokes, for example, has this sort of structure. But in the case of Navier Stokes, you can put the derivative outside, you can write it as like a divergence of u tensor u. Here you cannot actually just being function valued wouldn't even be enough to give a meaning to that equation. Okay, so that's sort of the problem.
00:16:24.442 - 00:17:19.146, Speaker B: And that's why already, to just give a meaning to these equations, you need to work. So by now, we have a very kind of general theory that gives you meaning to these kind of equations and tells you that you regularize it in pretty much any way you want. You add suitable counter terms that actually have diverging constant in front of them, and then you can pass to the limit and you get a finite limit, and the limit doesn't really depend on how you regularize things, and so on. Okay, so we have a pretty good solution theory for these type of equations, but it's very delicate. So it relies on kind of approximations, taking limits, adding diverging counter terms to them. Right. And so now you want to get some kind of a priority control.
00:17:19.146 - 00:18:56.080, Speaker B: You cannot just, you cannot just out of the box just apply the standard techniques. Right? So what's the standard techniques to get a priority control for parabolic PDE's? So, well, maybe the simplest technique is the one. If the right hand side were global eliphas, then sure, you can somehow just do your pick iteration. You have a fixed time horizon over which the peak iteration converges, which doesn't depend on initial data, and then you can just iterate, right? So that's if you have global Lipschitz continuity data. Of course, here usually doesn't work because you tend to have polynomial type non linear, and they are not locally globally elliptic. The other sort of technique that you have is, which is much more promising, is if you have something like a gradient flow, you have a gradient flow, or even better, if you have some sort of contracting, or if you want, it's like the gradient flow for a convex functional, then you know that well, if you take any ode which is x dot equal minus gradient v of x, if you look at the time derivative of v of x itself, well, then you get minus gradient v of x squared, which is always negative. And so you have an a probability control which is v, you know that v always decreases.
00:18:56.080 - 00:19:38.814, Speaker B: So that's sort of promising, because we derive these equations by starting from a gradient flow and then adding some noise, right? So the problem, of course, is that these action functions, also the thing of which we took the gradient is not going to be finite for the solutions to any of these equations. So I already told you that for 5.4, the interesting dimensions are two and three. The solutions are not even function valued. So they're certainly not in h one. They're not even in l two s. So you're kind of pretty far off from that.
00:19:38.814 - 00:20:27.410, Speaker B: And so you can't just try to find what's the time derivative of your action functional. Then the other kind of technique that you have is things like maximum principle. So maximum principle, or maybe more generally, sort of like geometry. I mean, we've heard in Tobias Kolding's talk yesterday, I mean, like in the curvature flow, things don't cross. So that's like a form of maximum principle. So in scalar equations, you often, like in the first one, you can expect to have some form of maximum principle. Like if you have some super solutions, subsolutions, then the real solution gets squashed in between.
00:20:27.410 - 00:21:28.026, Speaker B: So then maybe that can be helpful. And the final thing is, if you have something like conservation laws. So that sort of. So the example for that would be Navier Stokes. So if you take Navier Stokes, then the equation there is, well, you have this lower projector, and you have u gradu, and say if you take the stochastic Navier Stokes and you add some random force into this, then the Navier Stokes nonlinearity, you build a, you know, it doesn't push you inwards, but if you look at the energy of your velocity field. So if you apply. So if you forget about the noise, if you take deterministic navy associated, the time derivative of the l two norm square, which is the energy of your velocity field, this term just drops out.
00:21:28.026 - 00:22:14.928, Speaker B: And then you have the Laplacian, which gives you something that decays. So that's typically the kind of tools that you have in the deterministic case, at least. And so, ideally, our motivation is we would like to have a power rebounds for young mills. So for five, four, there's been quite a lot of work in the last six, seven years. And so, in pate, it started off with a paper by Weber and Mora. And then there are several. There's a whole series of papers.
00:22:14.928 - 00:23:11.380, Speaker B: There's a more recent one by Chandra Weber, and I don't remember the third. So there was a PhD student of Hendrik as well on the paper, and there are several other papers. So by now, we have very good control the solutions to this equation. But that's the one which has the best properties you can think of in terms of global bounds. Say, if it weren't noisy. So if this guy weren't noisy, then this equation here, you can start with any initial data say, any function valued initial data at all, and you have a bound on the solution at time one, which is completely independent of the initial data. So it's like if you take x dot equal minus x cubed on r, so the solution to that OD, it reaches one in time one wherever you start from.
00:23:11.380 - 00:24:03.192, Speaker B: So that one has the same kind of property. And so you have extremely good control on the solutions. So, in particular, there's sort of so good control on the solutions that you can use that to get, for example, tail bounds on this PI four measure that are much better than what people have obtained back in the seventies. So you can get better than gaussian tail bounds, things like that. So in the case of young mills, well, first, in terms of structure, it looks more like Navier Stokes, but that term looks more like Navier Stokes. This term looks a little bit like that, but it doesn't. It pushes inwards, but in some sense, it doesn't push inwards in all directions.
00:24:03.192 - 00:24:48.004, Speaker B: And that's due to this large symmetry group, again, in young mills. And so, right now, we have basically no result. The only result that we have that gives global bounds for young mills is in 2d. But that's cheating in some sense, because there we have the guess of what the invariant measure should be. You can basically prove that it is invariant. And once you know that it has an invariant measure, well, then the solutions have to be global, because it sounds a little bit like a circular argument that you can make that rigorous. So in 2D, you can get global solutions, but it's not because you can actually bounce solutions to the PD's, just because you know what the environment measures.
00:24:48.004 - 00:25:14.690, Speaker B: Yeah, probably. No, no. I mean, no, no, no. This, these dots, it just means arbitrary trilinear form. You know, there's a trilinear thing and it has the correct sign, but it's just that it has, like, Lee brackets in it. And you can have things where the lead bracket is zero. And so that gives you directions in which it doesn't push it.
00:25:14.690 - 00:25:42.620, Speaker B: So. Yeah, okay. And so for young mills, hormones, we have no technique. And so the result that we have is for Navier Stokes. So in this case, we think of Navier Stokes in some sense as a toy model for these kind of. For young mills, which is the equation we're really interested in. Of course, Navier Stokes is super interesting by itself as a model.
00:25:42.620 - 00:26:47.706, Speaker B: It's maybe not that. If you view Navier Stokes as an equation of fluid dynamics, then putting here things like space time, white noise is not necessarily a very natural thing to do. Okay, so the motivation for looking at Navier Stokes with something like space time white noise or very irregular noise is just to, you know, develop tools to get a priority bounds for solutions to very singular parabolic equations. So now, okay, so maybe let me, we don't have a lot of time, so let me first actually state the result that we have. But it's not really super important what the precise result is. But basically the result, I'm not even going to write it down. The result is that we, and all of these are infinite volume.
00:26:47.706 - 00:27:22.584, Speaker B: So you take Navier Stokes in two dimensions on the two dimensional torus. So we don't worry about boundary conditions and stuff like that. Okay, so the easiest case that you can think of, and you take noise here, which is either exactly space time wide noise. In the case of exactly space time wide noise, Navier Stoke is sort of special. So again, you know what the invariant measure is? So you could get global solutions by cheating. And so we take a noise which is the regularity of space time white noise, but it's not exactly space time white noise. And so we don't know what the invariant measures.
00:27:22.584 - 00:28:25.000, Speaker B: Okay, so we're not allowed to use this or even in a hidden way in our proof. But basically you should think of this as space time white noise again. And then on the one hand, in the result with Tommaso, we show that you have global solutions in time, but with really bad a priori bounds. So we have some kind of pathwise a priori bound, but it goes like e to the e to the t. Actually even e to the e to the something that's t times somehow the size of the noise over the interval zero t or something like that. So it grows like really crazy fast. And then in the result with when how we need to put a small parameter in front of the noise, or at least we don't need the noise itself to be small, but we need to be able to decompose it into something small that has the regularity of space time white noise, plus possibly something large, which has better regularity.
00:28:25.000 - 00:29:58.050, Speaker B: So in some sense the irregular part should be small enough. And then we have a result that if alpha is small enough, you have not just global solutions in time, but you have uniform in time bounds far for small enough. And so in particular, you have an invariant measure and you even get tail bounds on the invariant measures that go maybe like exponential of square root of some normal view or something on the solution. So you take this, you want the bounden, so you have a bound of the form, basically expectation of u of t, you know, smaller than constant. And of course, it depends on, depends on the initial data as well. Actually, what we can show there is that you have some kind of Japan of function. So you have some norm here that's sort of crafted in the right way, so that if you take something like this, you have a bound which goes like constant times e to the minus constant t times the same sort of norm for u of zero, and then plus another constant, you have that kind of bounden, right? So that has to be distributional norms.
00:29:58.050 - 00:30:58.010, Speaker B: So this is indeed the solution is just about not functional. It actually the solution here for any fixed time, it looks exactly like if you didn't have the nonlinearity to just take heat equation plus space time white noise at any fixed time, the solution looks exactly like the gaussian free field that Greg just talked about in his previous lecture, which in two dimensions is just about another function. And then in three dimensions, every time you go up one in dimension, you lose one half in terms of regularity. So it becomes kind of worse and worse. Okay, so that's the result that we have. And so maybe, let me just do a small aside on, you know, so again, we have basically we use in Navier Stokes, we have this conservation law, and we want to kind of use that. And so let me just make a small aside on scalings for this.
00:30:58.010 - 00:32:04.774, Speaker B: So say you take any kind of norm on functions or distributions on the torus, and for most norms, you would have some kind of scaling exponents that's associated to it, which is you say that the norm has scaling exponent alpha. If when you take the norm of u of lambda dot and that's bounded by the normal view times lambda to the alpha. Because here I'm sort of cheating because you can't rescale the torus. But okay, forget about that. Think of it as being on the whole space, for example, and I think of it as being lambda small. And so, of course it means you look at small scales and with small scales, you don't really see the fact that the torus is bounded anyway. Okay, so that's what it means for a norm to have scaling exponent alpha, right? So the l infinity norm scaling exponent zero, because you rescale everything, then infinity norm doesn't change l.
00:32:04.774 - 00:33:19.230, Speaker B: One norm would have scaling exponent minus dimension your space, right? If you rescale, you rescale by lambda, you sort of zoom in. That means that you sort of stretch your function out. Then the integral is going to actually become bigger by, well, in your change of variables, you get a lambda to the minus d. So in general, the Lp norm gives you a scaling exponent, which is like minus d over p. And so one thing that it's kind of nice if you want, if you want to sort of depict this, most norms have somehow some integrability exponent and some regularity exponent, right? So the integrability exponent is like the p in the LP norm. And the regularity exponent is like the alpha in the c alpha norm. So typically a norm tells you that there's so and so many derivatives that they're in some LP and then maybe there's like additional exponent that give you kind of finer control and stuff like that, right? But typically, by and large, once you have these two exponents, that gives you a pretty good handle on what your norm does.
00:33:19.230 - 00:34:31.673, Speaker B: And so if I call say, s the regularity exponent and p for the put a one over p here. Okay, then so zero. So here would have the LP spaces. And the way these scaling exponents go is that they have, they are constant on sort of lines like that, that have slope dimensional space. That's what you, and the sort of sublef embeddings and so on, they tell you that if you have control over some norm here, then you have control over all the norms that are below a thing like that that follows the scaling slope in one direction and flat in the other direction. Okay? That's what the sort of typical inclusions tell you. And then if you take any of these kind of parabolic PD's, if you look at the nonlinearity, you can ask yourself for which kind of norms is the non linearity of about the same strength as the linear part at small scales.
00:34:31.673 - 00:35:24.244, Speaker B: Okay? And that's very easy to guess because the way you do it, you say, well, if a is in some space that has scaling exponent alpha, then the laplacian of a has exponent alpha minus two, and then the derivative of a is alpha minus one. And if I multiply by a, well, basically, at least in the low regularities, these exponents actually tend to sort of add up when you multiply. So this balances out. If alpha minus two is equal to two alpha minus one. So alpha minus two equal to two alpha minus one. So that gives you alpha equal minus one. Okay, so, and it's the same for Navier Stokes because that's sort of the same in terms of these power countings.
00:35:24.244 - 00:36:39.448, Speaker B: That's the same in terms of non linearity, it turns out that even for the five four, it's actually the same, but it's not the same equation. Right? So it's now alpha minus two is equal to three alpha, but that happens to also give you minus one. Okay, which is a bit of a coincidence, maybe not so much, because both terms show up and they really have the same strength in young mills. Okay, so here, what does this tell you? So it tells you that if you take the line that starts here, this line at exponent minus one, then basically if you take an initial data, so forget about the noise, still so still completely deterministic. If you take an initial data that lies below here, which has sort of worse scaling exponent, then you cannot close the pic iteration, whereas if you take initial data that lies above that line, you can close the peak iteration. And if you're on the line, then it sort of depends on details. So in most cases, what you can show is that somewhere on the critical line here, well, stuff above, somewhere work and stuff below maybe doesn't work.
00:36:39.448 - 00:37:22.790, Speaker B: So typically this is sort of like the worst space, the largest space with the scaling exponent. And typically you can show. So, for example, for Navier Stokes, there's a result by Albert Koch from, I know, maybe ten years ago or something that shows that in that critical space here, Navier Stokes is really, you cannot solve Navier Stokes in the sense that smooth functions are dense in that space. And the solution map just doesn't extend continuously to the space. And in fact, it becomes arbitrarily big in arbitrarily small neighborhoods of the origin. It really badly doesn't extend to the space. So that's called norm inflation.
00:37:22.790 - 00:38:23.000, Speaker B: And you can show the same thing for like five, four. In fact, in the case of phi four, things already go slightly wrong a bit above, like at minus two thirds here. There's a little regime here in which you end up having problems because, not just because of scaling, but at some point because of integrability of the right hand side. Okay, so it's, it's not universally true that if you're above that line, you have a solution theory. If you're below, you don't. But it is sort of universally true that if you're above the line, even if you're kind of in this bad region, you put a little bit of extra conditions or something, and you do actually have a solution theory. Or maybe like for generic random initial data, that sort of regularity, you would have a solution theory or these kind of things so it's still somehow much better behaved than if you're below the line.
00:38:23.000 - 00:38:57.970, Speaker B: And so now if you have a conservation law, right? So conservation law typically has itself some kind of scaling exponent. So like l two norm from Navier Stokes. So you can. So that gives you a point somewhere on that diagram, right? So the l two norm for Navier Stokes is here. In the case of young mills, you have this energy functional that sort of decreases. That's more like an h one norm, which is a bit better. So that's somewhere above here.
00:38:57.970 - 00:39:44.482, Speaker B: And in the case of Navier Stokes in 2D, you also have the h one norm. So it turns out that in 2d, Navier Stokes both the l two norm and dh one norm of the velocity field of preserve. So you actually have two conservations also like both of these points. And now whether conservation law is useful or not basically depends on whether that point lies below or above the critical line, right? Because if it lies above the critical line, you know you have some kind of a priority bound that tells you that thing is going to be finite. And if you know that it's finite, you can extend your solution further. But you calibrations and everything goes fine. If it's below that critical line, then you're in trouble.
00:39:44.482 - 00:40:39.352, Speaker B: So that's why Navier Stokes 3D is still a million dollar problem to get global web positives. Because if you do in dimension three, you take the line of slope three that starts at minus one here, but at zero, it's at a third. But the conservation law you have is the l two norm, which is at a half here, okay? And so it's below the line. That's why it's sort of useless to get your global lipositiveness results. Okay? And in the case of 2D, Navier Stokes, on the one hand, you're sort of just born a line, because if you take the slope, the line of slope two, then it intersects here exactly at a half, which is the l two norm in principle. For the deterministic theory, that's much better because you have the h one norm as well. And so that's why Navier Stokes 2D is considered super easy.
00:40:39.352 - 00:41:16.650, Speaker B: It's because you have this other conservation law. So if you didn't have that, it would be okay, but it would be kind of critical in two D. And so now what happens when you add this noise? So when you add the noise, it tells you that whatever you do, your solution is not going to be better in terms of regularity than the solution to the heat equation plus noise, right? And that one. So what's the regularity of that? Well, basically in dimension two, it's sort of zero. In dimension three, it's minus one, two and so on. Right. So it's what I mentioned before.
00:41:16.650 - 00:42:04.356, Speaker B: And that somehow doesn't, that's something which is very homogeneous in space because the white noise, everything's translation invariant and it's random. So the integrability exponent doesn't really play any role in terms of what space you belong to. And so that means that you have a line here which is a horizontal line to somewhere near zero, and your solution is not, whatever you do, it's not going to be better in terms of regularity than that. So you're always going to be below that green line in terms of regularity. Right. So now basically all these equations in dimensions less than four are such that at least the green line is above this point here. Right.
00:42:04.356 - 00:42:41.210, Speaker B: If your dimension three, the green line would be at minus one two, it's still above this. But the problem is now your conservation laws become like this conservation law becomes really kind of useless because it's way above that green line. So you have essentially no chance of being able to use that because the regularity of your solution is going to be way too low to use that. Right. This one is sort of borderline. Right. So that's why in some sense, this navier stokes 2D with the spacetime white noise is kind of interesting because this all sort of crosses at the same point.
00:42:41.210 - 00:43:11.424, Speaker B: And so that's why we can just, we can sort of deal with it. But just about. Right. It's because we can use, we can only just about make use of the l two conservation law, even though solutions are not in l two. But we really cannot use the h one conservation law at all. Okay, so I meant to. Yeah.
00:43:11.424 - 00:44:03.488, Speaker B: Okay. So I meant to show, to show you an actual calculation to see how one actually deals with that. But I have sort of minus two minutes. So maybe just give you a very quick idea. Sort of like the general idea is always try to somehow decompose your solution into some small high frequency bit that you have pretty good control over because it's mostly determined by what the noise does and not so much about what the nonlinearity does, and then a remaining bit, which is kind of large but smoother. Okay. And then the hope is that you can write things in such a way that you can get something which is close to like an effective evolution on what happens on the larger scales.
00:44:03.488 - 00:45:05.712, Speaker B: Okay. So it's really sort of in the spirit if you want, of Wilson's randomization group where you say, well, you should really you have these physical systems where you have stuff going on at very small scales, which appears to be very singular when you look at what happens at large scales. And so you should really sort of look at a whole bunch of kind of effective equations at every scale that are sort of compatible with each other. And you should at the end of the day look at only sort of analyze the effective dynamic at scales of order, one which you're really kind of interested in. So you're essentially trying to do that so you can't quite implement Wilson's RG in a way that's useful here. But still at least you can use that as a sort of guiding principle to figure out what's going on. And then, yeah, I think that maybe, I don't think I have time to say more than that.
00:45:05.712 - 00:45:06.920, Speaker B: Okay, thank you very much.
