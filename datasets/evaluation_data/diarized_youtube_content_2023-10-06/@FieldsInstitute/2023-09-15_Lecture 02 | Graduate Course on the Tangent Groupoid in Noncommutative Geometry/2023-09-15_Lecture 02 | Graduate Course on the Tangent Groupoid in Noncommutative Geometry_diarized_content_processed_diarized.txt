00:00:00.600 - 00:00:38.934, Speaker A: Okay. If you are taking the class for credit, please let me know. I feel I ought to know who you are and I ought to write your names down on a piece of paper because then it will seem more official. So please email me. So, for example here, Higson, I do this every single class. I try to collect emails from the students. And what happens is, you know, there's maybe 20 people in the room, and three of them send me an email.
00:00:38.934 - 00:01:06.834, Speaker A: So it's going to be real simple. If you email me, if you're in this category and you email me before Saturday, this Saturday, then you'll get credit. If you don't email me before Saturday, no credit, it's over. No more, no second chances, no nothing. Okay. Email me before Saturday or just it's over. Okay.
00:01:06.834 - 00:01:40.854, Speaker A: And as I said last time, I'm making some notes, and if you want the notes, then you need the URL. I'm hoping that I'll actually make finished the first couple of lectures in a couple of days. I was a little distracted yesterday. Same thing. Email me. Same email address. There's no negative consequences if you don't email me, except you don't get the notes.
00:01:40.854 - 00:02:29.154, Speaker A: So simple. Okay, any questions? That was the business part of the lecture. Okay. And we'll get to Richard Feynman in a little while. So we're going to do something a little more chewy and substantial, nutritious, than we did last time. Last time was just sort of messing around. Now we are going to do something interesting, and what we're going to do revolves around these things, which are really easy.
00:02:29.154 - 00:03:16.626, Speaker A: Let me give two versions of these. Maybe I'll call it tea. So these are necessary parts of structure. The linear maps given from, as you see, from smooth, compactly supported functions to smooth, compactly supported functions. I guess you could define what I'm about to define for any t, but only t's, not zero, are the interesting ones. It's really easy there. So that's not the substantial and interesting thing.
00:03:16.626 - 00:04:01.302, Speaker A: Yeah. So what you do is you're supposed to imagine in all of this entire lecture, probably the entire class, you're supposed to imagine that t is a really little number, like one half or something, something less than one. And so what you're doing with this operation is, let's just say it once. So we know you're taking a function which is given by a certain graph, and you're just stretching it by a factor of two, one over one half. So the functions get stretched out and their derivatives, for example, get smaller after you apply r of t, if t is a little number, which you're always supposed to think that it is. Okay. Now we don't.
00:04:01.302 - 00:04:37.384, Speaker A: What we're doing here, of course, is stretching out around the. .0 so if you're, if you're to the right of zero, you get stretched out to the right, and if you're to the left of zero, you get stretched out to the left. But we don't want to consider zero is too special. We don't want to, okay, I'm not responsible. Sorry, Heath. We don't want to make, we don't have zero, you know, making errors and thinking it's very important.
00:04:37.384 - 00:05:38.884, Speaker A: So we'll just also consider rescalings around arbitrary points, same sort of thing. So now t is a non zero number, and omega is just some point in rn. I wrote some notes a while ago with this notation with omegas and t's, and I just got stuck with it. So these parameter points I'll call Omega. And so it's just exactly the same thing, stretching the graphs of functions. But now the stretching is located around the point omega, which, algebraically speaking, means you do this. Here's just a little warning.
00:05:38.884 - 00:06:54.284, Speaker A: These operators are not supposed to be composed unless omega, unless the omegas are equal. Bad things happen if you write down something like this. Yeah, just don't do that. Why is that? Well, suppose you make the following definition run out of space. I'll just do it over here. Apart from defining rescaling operators, you could also define translation operators in the usual way. Then it's a fact that if you compose a translation with a rescaling, well, there's an interesting formula that we'll actually take advantage of.
00:06:54.284 - 00:07:46.244, Speaker A: But here's what it is. If I didn't make a mistake, of course this will work if t is not zero and also t is not one. So what it means is that these rescaling operators aren't closed under composition. You could easily move one of the r omegas over, because the inverse of r omega t is just r omega one over t. So there you go. What does this say? Well, it says that if you're, suppose t is really, really small, like I said it should be, then you're stretching functions really, really a lot. And if you stretch functions really, really a lot, they become more and more like constant functions.
00:07:46.244 - 00:08:20.788, Speaker A: And when they become constant functions, they become translation invariant. Right. If t is really, really small, there's not much difference between r omega t and r omega plus t over one minus t times z. And so what this is saying is the more you stretch functions, the more translation invariant they become. Nothing particularly surprising there. But for the moment I'm only telling you this because it shows you that these don't form like some sort of ax plus b group of motions of the plane. That's not what's going on.
00:08:20.788 - 00:09:20.894, Speaker A: If you put in the R's together with the t's, they do constitute a group, but the R's alone don't constitute a group of automorphisms of this vector space or of transformations of the underlying one. Okay, why am I entertaining you with this stuff? Well, here's the point. Suppose you have a linear partial differential operator like we saw before, and maybe it doesn't have constant coefficients. So here's a linear partial differential operator of order m or less. They all look like this if they're acting on scalar functions. Alpha is a multi index, of course. So what happens the simplest, well, let's just do the general case.
00:09:20.894 - 00:10:13.924, Speaker A: What happens if you rescale this differential operator in this sense here? Well, the first thing you do to a function is the opposite of what I was talking about before. You take a function and you scrunch it inwards so that the graph becomes more compressed. That's what happens when you apply the inverse operators. So you take the function, you push it in, so it becomes more compressed because it's more compressed. The derivatives of this function are going to be bigger, it's going up and down more quickly, and then after you've taken the derivatives, you move the thing out again. Oh, and let's not forget the multiplications here, what this thing turns out to be. I think it's clear that it's going to be something like what I'm about to tell you on the basis of my accordion like motions.
00:10:13.924 - 00:10:50.624, Speaker A: It's just this guy here, what am I calling it? T. Almost running out of space. So let me try this again, just so it'll all fit. Sorry. I'm going to write down exactly the same thing with a bit more room. And let me start off like this. T to the minus alpha and then a alpha omega plus t, x minus omega.
00:10:50.624 - 00:11:53.340, Speaker A: And this is almost an extremely interesting formula. It's not quite what we want, but this is, this is not quite the summit which is mentioned up there, but it's interesting nonetheless. Suppose you take t so the m times r omega t. Just what I was writing down before. Now, something interesting. Well, it's clear what's going to happen. You Just MULtiPLY that thing by m, and the point is that all of these exponents are now positive or at least non negative.
00:11:53.340 - 00:12:52.210, Speaker A: So there's no singularity as t approaches zero. So we're going to get This a of omega plus t of x minus omega d to the alpha. And now what you see is that the thing on the right hand side makes sense when t is equal to zero. When t is equal to zero, something rather beautiful happens, which is that these coefficient functions just become coefficients numbers because they're just evaluation of a at the point that should have been a alpha at the point omega. And the various terms of this operator may or may not be zero, depending on whether alpha has length number of derivatives equal to m or less than m. So what you get at t is equal to zero is just the highest part of the operator. You drop all the lower degree terms and, and then you freeze the coefficients.
00:12:52.210 - 00:13:31.222, Speaker A: That's kind of nice. And if you know about differential operators, you know that that's a good thing to do. That's effectively producing out of the operator the so called principal symbol. Okay, so this construction makes sense when m is equal to zero. Let me write that something down here. I think we don't need these big formulas anymore. Yeah, I guess that's a nice way of thinking about it.
00:13:31.222 - 00:14:22.674, Speaker A: Yeah, you take the, you drop all the lower order deep, the alphas, derivatives and also the lower. Yeah, yeah, I can. Right. So if we do the following thing, I'm just repeating now what I just said. If we define as we can d omega zero, just to be this omega. And now I'm just going to keep the terms of, of highest order like that. Then the collection of all of these operators together, the ones we already defined, plus the ones I've just now defined.
00:14:22.674 - 00:15:02.794, Speaker A: Ah, let me be, now I need to, let's do the following thing. Don't like my notation, but I've painted myself into a corner. Let's do the following thing. Oh no, I didn't. Okay, good, great, fine. But I do want to introduce this notation which is just t to the m times r omega td. So this is when t is equal to zero.
00:15:02.794 - 00:16:40.144, Speaker A: This is when t is not zero. Then we obtain in some obvious naive sense, a smooth family of operators, smooth in alti and smooth in omega. And for the record, let's make an official definition of what we mean by it. Smooth family. Retain an example of one of the things I'm about to tell you about. If you have a collection of operators parameterized by some, I don't know, new or something, all of them, they're just operators on the same space of test functions. So maybe the, the news vary over some u, which is an open set, and some other euclidean space are klmk like that.
00:16:40.144 - 00:17:19.255, Speaker A: And each. I'm only going to be interested in good operators, which are these ones that we were discussing is properly supported. That's kind of what you need to get into the Cc infinity anyway. And. Oh, continuous. I haven't told you what continuous means, I don't think, because I forgot to mention it on, uh, Tuesday's lecture. So I'll come back and tell you in a footnote what I mean by continuous.
00:17:19.255 - 00:18:19.564, Speaker A: Anyway, such a thing is a smooth family writing very big today, running out of space. So go back over here. They can't. Ah, it's your fault. Yeah, I don't know about the oscar, okay, if. And so I didn't finish the definition, but let's just do it. I mean, if there is, uh, properly supported, same thing, properly supported continuous function, a continuous operator.
00:18:19.564 - 00:19:12.684, Speaker A: Let's just call it a not a new. So it goes from test functions on u times rn. The test functions on the same, such that the obvious thing holds a of a function f in this space evaluated at point u x is equal to a u. I guess I should call it, called it nu over there. Let's just stick with nu, a nu of f value a to the x. Okay. It's the obvious thing.
00:19:12.684 - 00:19:53.442, Speaker A: The problem. Well, if this thing exists, which it doesn't necessarily do, then we say it's a smooth family. Did I. Oh, it just means that the formula for the operator is a smooth. Whatever it is, it's a smooth function in this new. So, for example, if you take a new, and you evaluate it on a function f, and then you evaluate the resulting function at x that should be smooth in both nu and x. And I'm saying just a little bit more than that because I'm asking that this whole thing be properly supported and continuous just because.
00:19:53.442 - 00:20:29.432, Speaker A: Why not all of the interesting operators for this class have that property, so why not throw it in? F is a function on R. No, f. Sorry. So f here is a function in this space. Sorry. So I should restrict it to nu times rn. Or how maybe I will call it like this.
00:20:29.432 - 00:21:19.246, Speaker A: Sorry. Let's call it f nu of x. So f is in cc infinity, mu times rn. And what I mean by f nu at the point x is just f of u x. Happy now? You guys are a bunch of copying. Anyway, what's your question? Is this sort of like a nice way of saying that the math? Yeah, because this is, we're approaching this summit that Feynman speaks of over there. So I thought I should just make official the definitions.
00:21:19.246 - 00:21:37.534, Speaker A: We're going to be talking about smooth families. Smooth families. So now you know. Exactly, thanks to the combined efforts of everyone in this room, what a smooth family is. It's not a big deal. And if you just guessed the definition, that would probably be good enough. But, you know, here it is, for the record.
00:21:37.534 - 00:22:18.528, Speaker A: Yeah, it's not a big deal. But here I'm insisting that the family be smooth in the parameter nu, in this precise sense that I've written down. If I were to change smooth to continuous, that would almost be good enough for everything, but not quite. So I do want the family to be smooth and new, just to make my life easy. There going to be a lot of. We're going to be applying again and again and again at various points. In effect, Taylor's theorem, if you have a function of x and it vanishes at zero, then it's x times another function of x.
00:22:18.528 - 00:22:58.520, Speaker A: We're going to be using that. That's obviously not true for continuous functions. So I almost have nailed everything down, uncharacteristically, precisely for me. Except I still have to tell you what continuous means. It means the least possible thing it could mean in this context. I don't mean, you know, continuous in the l two norm or anything like that. Despite this being the semester on sea star algebras and von Neumann algebras and so on, these operators, we're not there yet in the world of sea Star.
00:22:58.520 - 00:24:15.292, Speaker A: We will. There will be sea star algebras. But my objective today is just to build an algebra, not a sea star algebra. Anyway, what does continuous mean? If you have a bunch of functions which have a common compact support, or to have supports which are commonly supported in a given compact set k. And if the derivatives of any order in any direction of the FNS converge to sum f, derivatives of sum f in the uniform norm, then after you apply the operator, which is supposed to be a continuous operator. Let me just call it. Let's call it.
00:24:15.292 - 00:25:08.520, Speaker A: I should use a different letter, not a, because you might think that a referred to the a that's up there. Let's just call it b or something. What I should like is the same thing for b. I'm imagining that the operator b is properly supported. And what that means is that if you apply b to all of these functions fn, which are all supported inside of k, then all of the BFNs will all be supported, not necessarily inside of k, but inside of some bigger compact set. So it makes sense to choose the supremum norm because all of the functions are supported on some compact set. In particular, they're all compactly supported.
00:25:08.520 - 00:25:35.674, Speaker A: So there is some finite supremum norm. And that's what I mean by continuous. This is a property that, for example, differential operators have. Every differential operator, even if it's of order 92 is continuous in this sense. So it's no big deal. It's a very weak notion of continuity. So properly supported operator.
00:25:35.674 - 00:26:47.854, Speaker A: Yeah, it's the usual topology on test functions, and we're applying it in a very simple situation where b is compatible with the filtration of this by those canonical fresher subspaces is continuous. If this is true whenever, I guess if, whenever you have a compact set and sequence of functions. Yeah, yeah. If you don't have proper support, then okay. We'd have to negotiate what the right definition is. Any questions? Comments? Criticisms? It's okay, Heath. Well, he's fallen asleep.
00:26:47.854 - 00:27:14.074, Speaker A: No, he's still awake. All right, good. So we're almost at the summit. Yeah, let's just get to the summit. So I want to give a definition and let me just see. Okay. It might take a little bit of space.
00:27:14.074 - 00:27:53.564, Speaker A: Let's see how it goes. The definition might start here and end over there. So here it is a family of operators. So there'll be a bunch of operators. Spell it out. Continuous, as just discussed. Properly supported.
00:27:53.564 - 00:28:40.454, Speaker A: I guess I should have said properly supported and continuous, not the other way around. And they'll all be operators on our favorite space. And like the operators that we built out of a differential operator, they're going to be parameterized initially just by non zero t's. And I'm going to give actually a whole integer family of definitions. So there's one for each m. Let's stick m into the picture. We'll call this a scaling family and smooth family.
00:28:40.454 - 00:30:05.374, Speaker A: Heavens above. Smooth family of continuous in the sense we just described, properly supported operators parameterized by their non zero numbers. Is a scaling family a water mix? Let me call it, well, call it order m, maybe more precise to say, degree n if. And now the definition of the rest of the definition involves an auxiliary family, which I shall now construct. So what I'm going to do is what I just did with differential operators, which is to get a two parameter family out of the one parameter family by rescaling like this. So omega is any point and t is still, for the moment, any nonzero number. Dot, dot, dot.
00:30:05.374 - 00:30:40.196, Speaker A: So this is almost what we did to d. Except here there's a t in the picture. And the t in the picture is supposed to remind you that we did a little fiddle in the world of differential operators, which is we multiplied them by t to the m. So an example of a scaling family is going to be t to the m times d. That's going to be a scaling family. And then we take that family and we conjugate it by r omega t, like I'm doing here. And what we get is a family which now makes sense when t is equal.
00:30:40.196 - 00:31:47.582, Speaker A: Wherever it was, it makes sense when t is equal to zero. So the condition on the family which we've seen holds here is that this family extends to a smooth, properly supported family, smooth, properly supported continuous family, or smooth family of properly supported continuous operators. A omega t, just like before. So all of these fellows glued together into one single operator like we were talking about before. And now what's going on is that the omegas still range over rn, but the t's now range over all of r. So we have to define operators at t is equal to zero because they weren't given to us initially. And we define them by, if you like, some sort of extension by continuity process.
00:31:47.582 - 00:32:21.150, Speaker A: If these guys exist, then they're clearly unique by extension by continuity. I'm not quite done yet with the definition, but this is the first half. So this is a property that we observed that differential operators have. You have a differential operator d voltr m, and if you define d, sub t to be t to the m times d, then you get a collection of operators. So you're not supposed to do that when t is equal to zero. That would be kind of stupid. Zero to the m times t and you get a collection of operators and then it has this extension.
00:32:21.150 - 00:32:59.812, Speaker A: So, so far so good. Okay. Such that however, there's one extra piece. So it's not quite any family that we're interested in. It's families which have an additional symmetry which I'm now trying to write down. So suppose we take this family which is built using rescalings, and apply a rescaling to the family. What's supposed to happen is that what you get here is omega to the minus order minus m.
00:32:59.812 - 00:33:55.634, Speaker A: This is the only place where m enters into the picture. Times r omega lambda t plus a smoothing operator. Thank you very much. So somewhere, I'm sure you're right. I just don't see. Yeah, yeah, right. Good.
00:33:55.634 - 00:34:52.574, Speaker A: Thank you. I was too close to it to see it. My apologies. Okay. For every lambda which is positive, any t which is real, and any omega which is, as you just correctly said, a vector point in rn that is the end of, let's put it up here, the end of the definition. And because I added an extra clause at the end, let's just check the case of differential operators again, and then we'll pause and admire our handiwork. So suppose I have a differential operator like we had before, given by one of those formulas of order m, and I write dt to be t to the md.
00:34:52.574 - 00:36:06.904, Speaker A: Okay, that's what we're supposed to be considering these one parameter families. And now suppose we define d omega t just like we're supposed to on the, yeah, here, over here to the, to the right. Let's just check this relation involving this funny smoothing operator term, see what happens. Well, r omega t of dt, r omega t inverse. I should have called that one of those, two of those t's lambdas, and I should have called this fine mess of this here I'm trying to check this strange transformation law. And in the case where dt is obtained from a single differential operator, and of course, d omega t is obtained from this conjugation formula. So this is just t to the minus t to the plus m, excuse me, times r of omega.
00:36:06.904 - 00:36:48.132, Speaker A: So what I'm doing is I'm applying to d r omega lambda, having already first applied r omega t, but r omega lambda times r omega t, you're allowed to make that composition because it's the same omega. And what you just get is r omega lambda t. So this is just r omega lambda t d r omega lambda t inverse. These guys compose nicely. These scaling operators compose nicely if the omegas match like they do here. All right. And so I could write this as t to the minus lambda times lambda t, sorry, t to the minus n lambda to the minus n.
00:36:48.132 - 00:37:11.104, Speaker A: My heavens. Times lambda t to the m. I'm just fiddling with real numbers. Not very well. I could write it like that. I think I got it right. And what you see in the bottom line there is just the definition of d omega lambda t.
00:37:11.104 - 00:37:41.484, Speaker A: So that's it. That's the end of the calculation. This law is satisfied and we checked. It's a minus sign, not a plus sign. I got it right, and it's satisfied on the nose. You don't even need the smoothing operator. Zero is a smoothing operator, so it's satisfied for the world's simplest smoothing operator.
00:37:41.484 - 00:38:45.422, Speaker A: If t is equal to zero, it's still satisfied on the nose. If t is equal to zero, you have to use the rest of the definition of dt, which we wrote down earlier. We saw what, excuse me, the rest of the definition of d omega t, which you wrote down earlier. We saw that d omega zero was what you get by freezing coefficients and evaluating and dropping the lower order terms. So I'm claiming that if you take an operator which is homogeneous of order m, no lower order terms, and it has constant coefficients, and if you rescale it, nothing happens to it except for some scalar factor changes by some scalar factor, which we already know because we did the general computation of what happens when you rescale any differential operator by any r omega lambda. And we saw what happens. It depends on the.
00:38:45.422 - 00:39:10.154, Speaker A: It was annoying. It depends on the. On the degree of the terms in the operator. There was that when we did the calculation, there was a t to the m minus or t to the minus alpha. But if all of the alphas have the same length, which they do in d omega zero, then the transformation law is very simple and this works. So let me thank you very much for mentioning it. I'm not going to go through the calculation.
00:39:10.154 - 00:39:42.914, Speaker A: I'll just say that it's also true that r omega lambda of d omega zero, r of omega lambda, inverse is still exactly equal to v of minus zero. Well, zero times lambda is just zero. So this would be the rest of the verification. Yep. Thank you for keeping me on me honest. Of course, one has to check this, I guess you don't actually have to check it because it would follow by continuity from the previous calculation. But anyway, it's easy enough to check.
00:39:42.914 - 00:40:21.298, Speaker A: Here's the definition. Here's why we make the definition, and here is Feynman's comment on the definition. So Feynman wrote some lecture notes. He was speaking to. He was teaching the employees of some corporation, Raytheon or something, how to do statistical mechanics. I mean, those were the days when, you know, Nobel Prize winning physicists would go to talk to the engineers at Raytheon and teach them anything. I haven't personally done that myself.
00:40:21.298 - 00:41:01.892, Speaker A: I don't know, echo it if you've been down to embraer or what's it, Canada and taught the engineers or anything. Yeah. Anyway, he was speaking, and on page one of the lecture notes, on page one of the lecture notes, he gives the fundamental law of statistical mechanics, gibbs law. And then he writes this, which this quotation has always stuck in my mind. So right at the very beginning, there is the fundamental law. For us, it's the fundamental definition, and then everything else you can go in two directions. You can apply the fundamental law or you can try and understand its derivation.
00:41:01.892 - 00:41:38.194, Speaker A: And that's, roughly speaking here we can build up a theory around this definition, or we can try to carry this definition to new contexts and apply it. And. Yeah, so I'm off by quite a few pages, about 15 or 16 pages in my notes. Didn't manage to put this on page one, but anyway, here it is. So we're going to spend the entire rest of the semester exploring the consequences of writing down this definition. Well, we have a whole semester to do it, so there's no rush. But let's, let's see.
00:41:38.194 - 00:42:17.174, Speaker A: Any questions? First of all, any more mistakes that have to be corrected? Yeah, there's a group oid and that's what's going to come. Like I said last time, by the time you've seen me write down for the hundredth time, D omega, t. And there's Omega, you're just going to get sick of all of these omegas. And you'll ask again, isn't there some way of getting rid of all of these omegas? Because after all, everything just comes from a one parameter family of operators. And then we make a two parameter family. And then we say something about the two parameter family. It seems a little cumbersome.
00:42:17.174 - 00:43:04.034, Speaker A: And the answer is, yeah, you can avoid all of this cumbersome stuff with omegas by constructing the appropriate symmetry group oid, but I want to make you all angry first. And so you just, you know, you insist on being taught what is the tangent groupoid? That's the, that's the plan. Multiplication between bars. If you add new megas. Yeah, the, okay, just, I will not be drawn into this. Yeah, we'll get to it. Okay, I'm trying to do something really basic.
00:43:04.034 - 00:44:14.466, Speaker A: It's just a bunch of operators with formulas for differential operators. You can just calculate things. Let's do another calculation. Yeah, let's do another calculation. It's kind of funny. So I'll work here just on the line, but here's something that we considered last time, more or less, at least when t is equal to one, namely the logarithm function. I want to scale the logarithm function here by t or one over t, and I want to stick in a cutoff function like we saw before, that makes this into a properly supported operator.
00:44:14.466 - 00:44:55.850, Speaker A: So this guy here, oh, maybe I'll just finish the definition. So this is an operator on cc infinity, just r. And this sigma function is like, this is zero and it's equal to one in the neighborhood of zero. You'll see that that's important in a moment. So that's a family of operators, and as t varies, but is not zero. It's certainly the case that this is a smooth family of operators dividing by zero. Okay, we have to discuss that.
00:44:55.850 - 00:45:35.314, Speaker A: But as long as we stay away from zero, this is a smooth family of operators. Each of these functions for any non zero t is a locally integrable function, and you can certainly build a convolution operator out of it, and you're in business. In fact, it's compactly supported locally integrable functions, which sends smooth, compactly supported functions into themselves, and it's continuous in the sense that I described before, etc. Etcetera. This is an example of one of these, and it has this magical property that it extends. First of all, I guess it's written up there, and it has this transformation law. Let's check it out, because it's kind of interesting.
00:45:35.314 - 00:47:16.104, Speaker A: So it means that you have to study, oops, not quite that, but you have to study this. And rather than carry these omegas around all the time, let's immediately observe a couple of things. First of all, this operator, which is rescaling around omega, of course, is just the translated version by omega of rescaling around zero, which is what I called r lambda before, and our operators at, oops, our operators at. If we rescale them, which is what we're supposed to be doing, then so at by itself is translation invariant. It's a convolution operator. It commutes with all of the t omegas. And even after you rescale by r omega, it's still translation invariant omega two.
00:47:16.104 - 00:48:21.364, Speaker A: That's because if you conjugate a translation by a dilation or rescaling, what you get is just translation by Lambda Z, like that. So there's a simple algebra of these rescaling and translation operators, which tells you right away that the rescaled operator is translation invariant. So in other words, it doesn't really matter where you which point around which you do the rescaling. So you'll always get exactly the same thing. So the answer to this question is that you'll always get, first of all, r lambda a c r lambda inverse. You can forget about the omegas. That's special to the fact that this is a we didn't do the groupoid thing and get rid of the omegas.
00:48:21.364 - 00:48:57.754, Speaker A: We just observed that by translation invariance, we don't have to worry about the omegas. Yeah, probably. Ah, because I have teeth on this side. Yes. Thank you. Happy now? I mean, t's in the A's and T's, well, not so far, because I just, you know, I could have stuck any translation invariant operator in here. And what I've written so far is correct.
00:48:57.754 - 00:49:31.038, Speaker A: But indeed, you'll see that I do want to choose the same teeth. The law of the definition tells me I have to consider this. And then by a little bit of algebra, that means I have to consider this. I'm just following the fundamental law on fundamental definition, on the whatever side that is, left or right hand board, the other extremity. And now we have to do the calculation just to see what happens. So I just simplified the calculation. We don't have to calculate a whole bunch of operators now depending on t and omega, we just have to calculate operators depending on t.
00:49:31.038 - 00:49:54.034, Speaker A: That's all I did, because it's translation invariant. And so life is a little easier than it was five minutes ago for me. Not, not for you, because I think I managed. You don't look happy anymore. You were happy a moment ago and now you don't look happy anymore. No matter. Your happiness is not my concern.
00:49:54.034 - 00:51:14.284, Speaker A: All right, so what is this thing? So we are taking a certain, not just translation invariant operator, but a certain convolution operator given by the function that you see at the very top. So it's a type of integral operator, and now we're conjugating it by rescaling. Maybe I'll do it this way. Let's do it over here and then we'll fill this in. If I have an operator which is a convolution, an integral operator, a smoothing operator doesn't have to be smoothing, just given by any integral kernel here. Maybe we're working in RN. If I trans, I'll just do it with r lambdas.
00:51:14.284 - 00:52:04.394, Speaker A: If I rescale this operator in this way, this is again an operator given by an integral kernel. And all you do is you take the previous integral. All you do pretty much is you take the previous integral kernel and you stick in lambdas. So the integral kernel pretty much is just, we're calling it, well, here we're calling it lambda k lambda x k lambda y, except when we define this operator as an integral operator. Of course we have to choose a measure on Rn in order to make sense of the integration. And the measure is not invariant under these rescalings. So there's also going to be a Radon nickedom derivative which can comes in, which in this case is just lambda to the end.
00:52:04.394 - 00:53:09.024, Speaker A: So that's what happens with integral operators when you transform them. Basically all you do is you rescale the integral kernel, except you have to keep in mind that the measure has to be rescaled as well. So what happens here is you get convolutional with one over, well, not one over, but t first of all times one over t. And that brings to mind a small typo, which I'll go fix at the top here. I'll log and so what do I do? I take it was log x over t before, but now I multiply by t. So it's just going to be log x and same thing here, sigma x. So it's actually one and the same operator all the time.
00:53:09.024 - 00:53:54.926, Speaker A: So for sure there's a smooth extension. It's a constant family and it extends to a constant family. No problem with the first clause here, which is this extension clause, because we did a little bit of preparatory work by, we pre squashed the integral kernel so that when we stretched it out again, everything was just fine. We got back to what we started with and we multiplied by one over t and everything's just fine. Worked out just great. Perfect. Now what? Ah, yeah, we have to check this funny condition.
00:53:54.926 - 00:54:42.774, Speaker A: This is kind of interesting. Okay, so for this I have to tell you what m is. So this is indeed a scaling family and its order is minus one. So what do I have to do? I have to take r lambda times a t, which is the same thing as a omega t times. Oh, let me be a little bit careful. Let's do this. Rt at p inverse, r lambda inverse.
00:54:42.774 - 00:55:26.934, Speaker A: I have to show that this thing is equal to one over. So t to the minus minus one. In other words, just t times. Let's get this right. Now I've written down this in such a confusing way that even I'm confused now. Um, yeah, few times a month. All right, let's give it a go.
00:55:26.934 - 00:56:12.828, Speaker A: Let's keep that there. I guess maybe we'll do this. Ah, thank you very much. Yes, that's good. So after we do this rescaling by lambda, what we're going to get is convolution by. I won't write convolution by. I'll just write down the functions.
00:56:12.828 - 00:56:41.454, Speaker A: Let me just check that this is going to go the way I should like it to go. Although. Oh damn, I didn't even write it down. Okay, fine. Calculation ends after the first line, so. All right, good. I'm interested in understanding this fellow versus this one.
00:56:41.454 - 00:57:28.364, Speaker A: And the first thing you can say about these two guys is that they are not the same. But the left hand side is. What? Well, it's log of lambda times sigma of lambda X plus log of x times sigma of lambda X. That's a little bit better than it was before. And this is log of lambda, which is a number times sigma of lambda X Plus. And here's what I'll do. I'll just add in and subtract out sigma of x.
00:57:28.364 - 00:58:20.176, Speaker A: So let me do this thing first. Log subtracting out first, sigma of lambda of x minus sigma of x plus log of x. So that's supposed to be just algebra. It's okay. I'm glad you're there. It's like, like having a, it's like a mathematical equivalent of a spell checker. You know, if you look at this whole thing, this is a smooth, compactly supported function, the logarithm.
00:58:20.176 - 00:58:42.284, Speaker A: This is sigma of lambda X. For sure. That's smooth. The logarithm function is not smooth at x is equal to zero. But look, you're multiplying it by a function which is identically one in a neighborhood of zero minus another function which is identically one in a neighborhood of zero. So the difference is identically zero in a neighborhood of zero. And so the product of it with log x is a smooth function and everything's compactly supported.
00:58:42.284 - 00:59:40.254, Speaker A: So this thing here is when you turn it into an operator, a smoothing operator, and that verifies the fundamental law which is right here. Still, it's kind of nice. And it's necessary in this particular case to throw in the smoothing operator correction term, otherwise it clearly wouldn't be correct. In fact, the only operators which are exactly, I don't know, let's call it covariant or something, for which the smoothing operator term is not there. The only operators for which this happens are the differential operators. That's not too hard to show. And so you always have to have this extra smoothing operator correction term if you want to leave the world of differential operators.
00:59:40.254 - 01:00:12.318, Speaker A: That's just a remark that we don't need right now. So, all right, it's kind of interesting. I won't do it because you have x minus x y, and that goes to the derivative that. Exactly. Yeah, it's funny you should mention that. I was just about to say I won't do it. But, but here's another example to do by yourself.
01:00:12.318 - 01:01:16.834, Speaker A: Figure out some way to do it. I guess now we're getting used to the definition. Maybe I can just put a couple of other examples here, which I won't go through, I'll just tell you what they are again from using convolutions. So, one thing you can do is just mimic exactly what you see up there. But using the sine function. The sine function, it's almost easier than the logarithm function because you don't have to worry about this log lambda term. That's not going to be there when you do this calculation.
01:01:16.834 - 01:02:04.564, Speaker A: But the sigma of lambda x minus sigma term is still going to be there. So this is a scaling family unit over here of order or degree. Which one is it? Minus one. It's the same as the example we just considered. These are the only two, fundamentally the only two interesting examples in order, minus one on the line. And apropos of what Jamie was saying, if instead of the sine function, you just put here one over x, I guess one over x over t, which would be t over x times sigma. Oh, sorry, I missed it out here, didn't I? X over t.
01:02:04.564 - 01:02:46.136, Speaker A: So these are basically what Jamie was referring to, the Hilbert transforms. To stay within our world of properly supported operators, I cut off the Hilbert transform by a bump function like you see here. And so I kept the business part of the Hilbert transform and I removed some kind of stuff. This is something about zero. If you adjust the constants a little bit, you stick in a pie on an. I like that. Then something kind of interesting happens.
01:02:46.136 - 01:03:55.424, Speaker A: The reason for the constants is that at times at is the identity modulo smoothing operators. And as for this fellow here, at times t dx, this family is the identity family modular smoothing operators. So if you want to invert the differential operator in the world, trying to live in and get comfortable with, this is the way to do it. And here's another interesting operator that'll enter into the picture later on. Okay, so we had a few examples. It's worthwhile going through them with, you know, sort of educate, actually do it yourself, then you feel much better about it. All right, and now let us proceed.
01:03:55.424 - 01:05:12.394, Speaker A: So we wrote down the definition at the summit, and we stopped for a little while to admire the view and pick out some particular little daisies growing at the summit. And now we're going to go back to the beginning and somehow see what this definition is all about. So we're going to do, what's it called? The climb up now for the next several lectures, not in the next half hour. Oh, and I forgot to say a critical thing, which is to give credit where credit's due. This definition here is due to Eric van Erp and Bob Young. No, just van Jankin. There's an important earlier paper by Clair de Boer and Georges Scandalis, but I think it would be hard to detect the connection between that and this definition, whereas this is a straight translation of what's in the paper into a slightly different language.
01:05:12.394 - 01:05:43.654, Speaker A: Okay. Is for a year. Yeah. 17. That's approximately right. Yeah. Early or late? Pre pandemic.
01:05:43.654 - 01:07:09.258, Speaker A: Yeah. Let me give one definition. I guess what I'm going to do is a little bit of the downhill first, because it's just a nice easy definition you can make. So rather than do the slog, let's just enjoy ourselves at the summit for a bit longer. If you have a properly supported continuous, in the sense that we discussed, operator, just one operator, no families for the moment, then let's say that this operator is scale above if of order, let's say m. If there's a scaling family from which this operator comes. If there is a scaling family, that's a whole bunch of operators.
01:07:09.258 - 01:08:50.624, Speaker A: Remember, parameterized by the non zero real numbers, and the condition is that a is equal to a one. So the operator extends to a scaling family. Okay, so for example, DDX any differential operator, this construction, any of these operators here for t is equal to one. Any of the operators we were talking about on Tuesday are all scalable operators. There's a little lemma which goes with this, which is that if a is scalable of order m, if there is a scaling family, of course, of order order m. Anyways, if it's scalable of order m, then it's also scalable of order m plus one, and therefore m plus two, m plus three, and so on. In order to prove this, I have to construct a scaling family of order m plus one from a family of order m, and that's easy.
01:08:50.624 - 01:10:14.298, Speaker A: If at is a scaling family of order m, then t times at just the scalar multiplication is order n plus one. If you have two scale ing families like we have over there exactly on the summit, and they both have the same order m, then you can just add them and that's going to be a scaling family two. And if you have a scaling family of order m one, and you compose it point wise over t, so to speak, with a scaling family of order m two, then of course you'll get a scaling family of order m one plus m two. But you can't add a scaling family of order two to a scaling family of order minus two. That doesn't work. That just produces nonsense. So the scaling families don't form an algebra, but the scale abo operators do form or constitute the set of all scalable.
01:10:14.298 - 01:10:59.564, Speaker A: Scalable operators is a filtered algebra with an increasing filtration. I guess it's a z. So it could be either important. No. Well, if m is not an integer, then the integers have the pleasant property that they form a group. If you take the half, I don't know, integers plus the square root of two, then an integer plus a square root of two plus another integer plus the square root of two is an integer plus two square roots of two. And it just gets messy.
01:10:59.564 - 01:11:33.110, Speaker A: But no, it's not important that it's an integer. Yeah, you could, you could do that. If you take all of the operators of order integer plus one over PI, that's obviously a module over the algebra that I'm talking about here. So you could do things like that, or you could just knock yourself out and just talk about operators with real order. That would be fine. Then it would be r filtered. Yeah, but we're trying to do the easiest possible thing right now.
01:11:33.110 - 01:12:20.656, Speaker A: And so this is the minimal thing that you can reasonably do. You would end up in Eckhart's world of scalable operators of non integer order. A non scalable operator. Yeah. For example, we'll see in just a moment that every scalable operator is pseudo local. So anything which is not pseudo local is not going to be in the collection. But that's certainly not enough.
01:12:20.656 - 01:13:07.944, Speaker A: What's important if we're looking at convolution operators here, what's important is the nature of the singularity at zero. And the nature of the singularities that you see here is that they're all sort of homogeneous of various different orders. The logarithm functions not exactly homogeneous, but it's homogeneous up to a constant, like we see here. But if you built some locally integrable function which was smooth everywhere except at zero, but it didn't blow up like a power of one over x, or like a logarithm, it blew up like some crazy function. I mean, it should still be locally integrable, then you would have a non scalable operator. Oh, then it wouldn't be pseudo local. Yeah, I was trying, but I think, yeah, I think my example is a bit more realistic.
01:13:07.944 - 01:13:47.894, Speaker A: Yeah, it's sort of, I mean, what these guys did, those guys did is they restored some degree of algebraicity to the theory of the sorts of operators that we want to be concerned with. The operators which turn out to be parametrics of things like the Laplace operator. And yeah, it does have a very algebraic flavor to it. But to make the thing algebraic, you kind of have to stick in families. It's algebraic. When you talk about families, it's not really quite so algebraic when you talk about the individual operators. That's the funny way that it is.
01:13:47.894 - 01:15:06.550, Speaker A: Okay, I spent too much time rabbiting on there, so let me just say something about this. So the big theorem of Bob and Eric is that this is exactly the algebra of pseudo differential operators. Well, there's, within the world of pseudo differential operators, as I mentioned last time, there's a subclass called the classical operators, which we'll discuss in due course. And of course, all of the operators that we're going to get from this construction are going to be properly supported because that's what we threw in pseudo differential operators on RN. So they with, it wasn't terribly stressful to make this definition. There it is. We could have made it on, put it on page one of the notes without, you know, really pushing ourselves too much.
01:15:06.550 - 01:15:55.124, Speaker A: And then you've somehow captured in a few words what is a pseudo differential operator in a language which doesn't involve the Fourier transform and which has the potential, therefore, to venture into places where the Fourier transform can't go. That's the philosophy of this whole thing. Okay. Yeah, I don't have time to do more. Well, I can maybe do one thing just as a sort of hint. I've decided what to do with the remaining ten minutes. Any questions? First of all, before we go, keep fineman up there.
01:15:55.124 - 01:17:59.880, Speaker A: Let's just draw some little pictures to end today's lecture based on the following idea we've spoken about properly supported operators. Let's just quickly define the support of an operator. So when you define the support of anything, it's the complement of the largest open set such that. And the open set is going to be somehow, this is in rn times rn. Let's give the largest open set a name, u, so that if you have the support of Phi across the support of psi in U and you also have that, then you have that phi, a psi composition of three operators, like we argued about last time, is equal to zero. So it's the set of places where the operator isn't. And if the operator is one of these integral operators that we had a while ago given by some, let's say, a smoothing kernel k of x y.
01:17:59.880 - 01:18:22.700, Speaker A: I'm talking exactly about the support of the function k of x y. That's what I'm talking about. And I'm just talking about that set in a way which doesn't require me to say there is a k of xy to begin with. That's what I'm doing now. I can draw pictures. We're looking at properly supported operators. So we're looking at operators.
01:18:22.700 - 01:19:04.168, Speaker A: Think about the condition of proper support. It means the support of the operator looks something like this. I drew two parallel lines around the diagonal here, and that's not exactly what proper support is about. The two boundary lines could be curving outwards so that this support region could be getting fatter as you go off to infinity. But roughly speaking, this is what the support is. Let's say this was the support of a scalable operator a. And imagine that we took this operator and we rescaled it now by a factor of lambda in the way we were more or less discussing.
01:19:04.168 - 01:19:59.644, Speaker A: Except I want to do it the other way around. Namely, I'm always thinking of lambda as a little number and I'll put the inverse on the left and the operator on the right. This is the opposite to what we were doing before, or if you like, it's what we were doing before, but for a big lambda instead of a little lambda. And what it means that instead of the operator getting stretched out, the coefficients getting stretched out to be more and more slowly varying functions like we saw when we manipulated differential operators, I'm squashing them in and so I'm making the support smaller. And what happens is the support now becomes smaller. Not quite, because, well, actually what I've said is correct. Let's just do it one more time.
01:19:59.644 - 01:21:06.754, Speaker A: Let's do it again. And now we're going to look at the support of r squared lambda a r, sorry, to the minus two, the support gets smaller and smaller again. Okay, each time you apply this operation of rescaling the way I'm doing it, with the inverse on the left, the support of the operator gets smaller and smaller and smaller. Okay, now if I have a scalable operator, when I compare these two guys, I'm able to conclude from the definition of scaling families that these two operators up to some lambda to the plus or minus order, whichever it is, these two operators are actually equal modulo or smoothing operator. You have a scalable operator a and you perform this transformation and you compare these two operators. That's the same thing as comparing our lambda inverse to a. The difference is a smoothing operator.
01:21:06.754 - 01:22:00.460, Speaker A: So the part of this operator which lives near the edges of this support, which doesn't appear in this picture here, that has to be a part which is a smoothing operator part, if you like. If you write down the formula for whatever this integral kernel is, it has some singularities along the diagonal, maybe like we saw in our examples, there can't be any singularities anywhere else except on the diagonal. If you know about the Schwarz kernels theorem, then this operator has a Schwarz kernel because it's continuous in the sense we described. And the Schwarz kernel has to be a c infinity function, except on the diagonal. I'm saying that all of the operators we're considering are given by some k of xy, which is some kind of generalized function, a distribution. And all of these k of x y's are always necessarily c infinity functions everywhere except on the diagonal. That is to say, the restriction of these distributions to the complement of the diagonal.
01:22:00.460 - 01:22:41.616, Speaker A: You're allowed to restrict a distribution to an open set. You always get a smooth function. And another way of saying that is that each one of these operators is pseudo local. Not only that, but this is something we'll pursue next time. If I want to know what is a, well, there's a formula for a which only involves this plus whatever the formula for the smoothing operator is. So that smoothing operator, which I told you before, is like a small error. We're imagining that we're trading off accuracy for convenience by introducing smoothing operators which we pretend are really close to zero.
01:22:41.616 - 01:23:31.054, Speaker A: In fact, these smoothing operators are very important because you can reconstitute the entire operator a at least away from the exact diagonal from that smoothing operator error term, because what the operator looks like out here near the edges is determined by the smoothing operator that appears in the definition of scaling family. And that's true also here, and it's also true in the next one. So everywhere except on the diagonal, the formula for the operator is Schwarz kernel is determined by the smoothing operator. So it's not so irrelevant after all. It determines everything. And so in principle, we should be able to write down a formula for any scalable operator in terms of the moving operator error term that we were imagining throwing away, which appeared in the definition of scaling family. And then we can examine that formula and try and understand what kind of an operator we're dealing with.
01:23:31.054 - 01:24:07.682, Speaker A: And as a first example, which we'll do next time, the I'll prove that if you have a scaling operator whose order is minus k for every k, it has arbitrarily negative order. Right? Every operator which has scaling, every scalable operator which has order minus k also has order minus clay plus one number bigger. But it's possible that you might have an operator which has order minus k. Also order minus k minus one. Also order minus k minus two. Also order minus k minus three, and so on, so on. It's possible that you might have an operator of order, so to speak, minus infinity.
01:24:07.682 - 01:24:44.710, Speaker A: I'll prove next time, just to get us started, that such an operator is necessarily a smoothing operator. So inside of this algebra, this filtered algebra, it's not true that the intersection of all of the different filtration levels is zero. In fact, the intersection of all of the filtration levels is the algebra of smoothing, properly supported smoothing operators. That's an interesting start. If you're a sea star algebra person, you see something like this, the sister algebra of compact operators sitting as an ideal inside of this algebra. Okay, but I was rapiding on too much. No time to start that now.
01:24:44.710 - 01:25:17.554, Speaker A: So let's just stop. If you're registered in this class in some fashion, at some location anywhere in the world, and you want credit from me, then you need to send me an email before whatever I said. The weekend, Saturday, Friday. I can't remember what I said now. Certainly Saturday was the latest. Yeah. And if you would like to receive notes when they become, you know, when I've removed all of the obscene words and so on from them, then please also send me an email and I am higson.edu.
01:25:17.554 - 01:25:30.534, Speaker A: That's it. Thank you. Yes. Yeah, yeah, I'm always sort of thinking of that. Yeah.
