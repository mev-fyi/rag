00:00:01.320 - 00:00:48.280, Speaker A: Welcome, everyone, to the second quantitative and computational Biology seminar series, or seminar, of this year. So we're very happy to have Martin Smith, or Martin Smith, who did his PhD in genomics and computational biology at the University of Queensland, and he focused on evolutionary comparative genomics. And now he is. He's joined the department of biochemistry and molecular medicine at the University of Montreal, where we are now, and the St. Justin University Hospital Research center with me. He's my neighbor when I'm at the office, which is not frequently these days, and he works on genomics and bioinformatics. So he's going to talk to us today about the uncharted genome and mapping it at the single molecule resolution.
00:00:48.280 - 00:00:58.214, Speaker A: And as usual, please use your chat to ask any questions, and I'll open up the floor for questions at the end of this talk. Thanks.
00:00:59.994 - 00:01:40.734, Speaker B: Thanks, Morgan. So I've got four little stories here today to talk to you about. First of all, thanks, Morgan, for inviting me. It's always appreciated to kind of spread my research gospel in the new environment. But yes, so four little stories. Half of the first two stories will be on looking at uncovering hidden regulatory networks in the human genome and characterizing the function of the non coding genome. And then the last two stories are going to be related to genomic technologies, in particular looking at nanopore sequencing for rna and gene expression.
00:01:40.734 - 00:03:15.644, Speaker B: So, no further ado. All these fouries boil down to my, my main interest in research, which is trying to understand biological complexity, especially in higher eukaryotes. So one of the common things in the evolution of complexity is that the complexity is kind of defined as the number of cells and organisms has this pretty much scales pretty well with complexity at the biological level. And it wasn't until completion of the human genome project, you know, over about 1520 years ago now that we realized that the amount of proteins in the human genome, the amount of genes, if you will, stays pretty much stable throughout evolution, suggesting that the number of genes doesn't actually contribute to complexity. And instead, what, what was noticed is that the proportion of an organism's genome that encompasses non coding regions scales almost perfectly with complexity. So this non coding genomic DNA is probably involved in the regulation of these developmental programs and regulation of gene expression in these more sophisticated biological systems. So what's a genome made up of? If we go and dissect this genomic dark matter bit more, we know that less than 2% of our DNA codes for proteins.
00:03:15.644 - 00:03:45.706, Speaker B: In fact, it's more along 1.2%, I believe. And we also know that about 10% of our genome is conserved throughout vertebrates at the level of sequence. So these are regions that are less variable than by chance. So they've, they're undergoing purifying selection for sequence composition. So those sequences are important in the genetic makeup of an organism. Now, it wasn't until recently that the remaining 80% was shown to be biochemically active.
00:03:45.706 - 00:05:05.574, Speaker B: In fact, this is largely due to the advent of high throughput sequencing technologies that have kind of evolved from the huge efforts that were put into sequencing the human genome. We now know that over 80% of our genome is biochemically active, mostly through the production of non coding rna. So transcription of DNA into rna, and if you look at the present reference databases of genes or gene annotations in the human genome, you'll notice that the level of amount of protein coding genes that are discovered as we sequence more and more samples stays about the same at around 20,000. Yet the amount of new non coding rna's, long non coding rna's in particular, increases almost steadily with each new database or gen code release. We have hundreds of new long non coding rna's that stem from these non coding regions. So a recent study from some of my colleagues in Sydney showed that a lot of this transcription in the human genome occurs at really low levels. In fact, if you look on the top here in this slide, you have a typical rna sequencing experiment in yellow, where you have the reads that are mapped to positions on a chromosome and all the known genes in white.
00:05:05.574 - 00:05:59.426, Speaker B: And what they did is they applied a targeted sequencing approach. So basically enriching for or depleting highly abundant rna's, which allows you to see less abundant stuff. Much like looking at a night sky in the country versus the city. And what they found is that most of the chromosome, in this case chromosome 21, is actually transcribed at very low levels, almost ubiquitously across the whole chromosome. So we're missing a lot of information. And some of these less abundant transcripts, you might think that they're spuriously produced and that they're very low abundance, but in fact they're very highly, some of them at least are very highly expressed, but in a very small number of cells. So on the left here, you can see these fluorescence images of mouse brains and you can see that some of these non coding rna's are super highly expressed, but in very concise cellular populations.
00:05:59.426 - 00:07:21.764, Speaker B: And if you were to sequence a brain sample from mouse, you would basically put the cells in a blender and then you'd get very few of those long non coding rna's because they're kind of dissipated throughout the rest of the sample. Now, single cell sequencing technologies have recently allowed us to kind of zoom into individual cellular populations a bit more and really uncover the complexity of transcriptional and diversity of transcriptional products in human cells. But another interesting result that has come from many, many studies looking at genetic association studies. When looking at mutations that are associated with human diseases, we know that over 90% of these mutations fall outside of protein coding regions. So the vast majority of genetic variation that is associated with disease predisposition occurs in non coding regions. So what are they doing? Are they functioning through non coding rna's or other regulatory machinery? This is just another example here showing in white here is a region of the human genome that we targeted for sequencing, where there's a mutation that's associated with major depressive disorder. And what I'm showing here is actually the entire known gene annotation for this locus, for this region.
00:07:21.764 - 00:07:42.548, Speaker B: I've lost my cursor, it seems. Apologies. But on the right there, you can see there's a little link. I don't know if you can see that little green bit there, and that is the only gene that's known from this locus. But if we. Oh, there we go. If we do some targeted sequencing here, the whole region lights up.
00:07:42.548 - 00:08:41.594, Speaker B: So these are all non coding rna's that stem from this region that's associated with a neuropsychiatric disorder. And yet we're typically missing it because we're just doing bulk sequencing and not really looking, not really paying much attention to these non coding regions. So we've shown a bit more than a decade ago now that a lot of these non coding RNA's are functionally important, so they're not all super lowly expressed. In fact, this one here was the most abundantly expressed in melanoma patients or melanoma samples. So the most differentially expressed gene wasn't actually a protein coding gene, but was a long known coding rna. We showed different experiments that if you modulate the activity or the abundance of this non coding rna, you get very different effects on the cells. Moreover, this rna seems to fold very concise structures, so you can predict what the structures are in RNA.
00:08:41.594 - 00:10:07.144, Speaker B: And this is what led me to the main topic of my thesis, was essentially to try to understand how these long known encoding RNA's function, a unifying feature of all non coding RNA's, is their propensity to fold into these structures, these higher order structures which often govern their function. Here I have a little example of a predicted rna structure that's conserved throughout evolution. And this one happens to overlap mutation associated with breast cancer predisposition. So the mutation here is in white. And if you look at the 3d structure of this, that additional base pairing in this stem loop of the rna structure causes the nucleotides to kind of sequester into the loop and increases its stability by 25%, potentially preventing these atoms from interacting with other molecules in the cell. So I hypothesized that these long encoding RNA's function through the formation of distinct secondary and tertiary structures, and much like domains and proteins, the capacity to bring these different structures together might govern the function of these molecules. And if we can predict that, then we can use this information to understand what these molecules are doing.
00:10:07.144 - 00:11:00.456, Speaker B: The way I set out to investigate this was by looking at sequence conservation. So here's an example of a multiple genome alignment between a dozen or so mammals. And when looking for regions that are conserved throughout evolution, you typically look at mutation patterns in this region. Here we have lots of these base substitutions of mutations, which would suggest that this region isn't under purifying selection. So the mutations are just kind of occurring willy nilly, and that the region is probably not conserved at the level of sequence. However, if you overlay the structure, secondary structure of an rna on top of this, what you can find is often in regions that function through their structures. The mutations aren't random, and in fact, they are compensatory.
00:11:00.456 - 00:11:35.460, Speaker B: So they will mutate in order to preserve a higher order structure. So, for instance, an a would pair with a t or u in RNA, but that a could mutate to a g and g also pairs with u in rna and vice versa. The u could mutate to a c. And this covariation of mutations happens in these regions. And you can measure this. And, and this is what I set out to do using this kind of evolutionary evidence for covariation and hence conservation of structure. So the way that this is done, there's a few good algorithms for this.
00:11:35.460 - 00:12:27.910, Speaker B: One of them is used by the most commonly used tools. It's called RNA life folds. And the way it works is it takes a multiple sequence alignment, and for each pair of columns in the alignment, it measures the amount of covariation, or synonymous base mutations that would maintain a pairing. It does this for the whole alignment. It then produces a consensus sequence, and it folds that consensus sequence based on the patterns of mutation to maximize the base pairing potential. And you can essentially use this to predict the most optimal common secondary structure for a set of aligned sequences. Now, we have lots of these alignments in public databases, but the question is, how do we measure the significance of these predictions? And I use two tools to look at this.
00:12:27.910 - 00:13:22.442, Speaker B: One of them is called RNA Z. It uses a SVM classifier that's trained on evolutionary conservation and thermodynamic stability parameters of known RNA structures. And you feed it in multiple sequence alignment, and it'll tell you whether or not you have a conserved rna structure in that alignment. Another tool that we use and I helped develop was this tool called CSS. And CCC essentially works by randomizing the alignments. So it'll keep all the dinucleotide content and the phylogenetic distance of the different sequences, but it'll shuffle those nucleotides within the alignments to generate a background null model. And by running the null model many, many times in many different iterations, we can then use a background distribution to calculate a z score and make a prediction.
00:13:22.442 - 00:14:45.084, Speaker B: So I established this nice benchmark to try to calibrate the parameters for these different classifiers and predictors. We use known rna structure alignments. These are rna molecules like the ribosome trnas that have well known rna structures and are aligned in a manner to bring out the structural similarities and not necessarily the sequence similarities. Now, in a genome wide screen, you're typically taking a sliding window approach and chunking up the alignments into smaller windows kind of randomly. So we applied this methodology to this set of positive controls to establish the optimal parameters and baseline sensitivity and specificity metrics. We generated the predictions for a whole chunk of a genome and then looked at the distribution of the scores and used these distributions to really tune the optimal parameters for genome wide screen to try to maximize the sensitivity. We then calibrated this using a randomized background model similar to what CCC uses to measure the specificity and establish a threshold at which we could say with a certain degree of confidence that we indeed have a conserved secondary structure in this region of the genome.
00:14:45.084 - 00:16:15.704, Speaker B: What we found when we applied this to the whole genome and we looked at, I believe it was 36, 32 mammals at the time, excuse me, I forget, but we found that about 20% of the human genome is conserved at the level of rna structure throughout evolution, and that the majority of these regions fall outside of known sequence conserved regions, suggesting that there's a lot of regions in the human genome that are actually functioning through rna structure. And this kind of makes sense when you consider that a lot of the genome is transcribed into rna. So if these rna's are functional, they probably have conserved structures. Now, keeping in mind that when we benchmark this screen, we were only picking up about 40% of the known rna structures in the database. So this is likely to be an underestimate of the real proportion of the genome that functions through some sort of rna secondary structure. One of the things we did with these, we actually have, I think, 4 million predictions, is it's a lot of data. So the next question we had was, are these actually functional, are these real predictions? And how can we look for similarities in the genome? And one of the ways we did this is by taking these, taking the alignments and the predictions that came from those alignments and creating a probabilistic model called the covariance model that includes both the sequence and the secondary structure alignment information.
00:16:15.704 - 00:16:56.394, Speaker B: And we performed a scan across the genome for similarity, or homology, to conserve structures. And we found a lot of interesting stuff. So we took a handful of these alignments and scanned the genome, and some of them only gave us a few hits, whereas others gave us thousands of hits, and some interesting results, too. For instance, some chromosomes seem to be enriched for certain structural motifs and some of them overlap repetitive regions. So that kind of gets a bit complex. We have tens of thousands of these ancient transposons or viral particles that are inserted in the genome. They seem to have structures that are important for the function.
00:16:56.394 - 00:17:58.660, Speaker B: So these are all predictions and computational screens. They're not biologically informative per se. So we wanted to really add another layer of support to some of these predictions. And to do this, we also wanted to be able to process data in a way that could allow us to systematically annotate the function of these long non coding rna's, looking at their secondary structures. So a study, we published a study a few years ago showing how you can do this using biochemical data. The idea was to, instead of starting from an evolutionary approach, we could start from a biochemical approach using these masses of genomic data that are being generated. And fortunately, the timing of one of these data releases was very fortuitous because we had access to a very large dataset of rna immunoprecipitation data.
00:17:58.660 - 00:19:08.380, Speaker B: So this is data where you basically use antibodies against known proteins and pull down those proteins with antibodies and sequence all the rna's that are bound to the proteins. So you get an idea of which transcripts are associating with the proteins in the cell, and therefore you can associate a given sequence to a given biological function. So there was a huge data set that was released and we wanted to mine this data to see if it was, if there were structures, rna structures, that were enriched in the binding of the proteins. So, to do that, we needed to come up with a way to process very, very large amounts of data, and we wanted to do this using an unsupervised clustering approach. And I'll talk a bit about the back end to that now. So, the first step is to predict structures from the sequencing data, and then the next step is to compare the structures between them and search for commonalities. Now, rna structure prediction is kind of just that.
00:19:08.380 - 00:20:06.502, Speaker B: It's a prediction, it's not necessarily the biological reality. Now, if you've ever done rna structure prediction, you know that it's a global optimization problem where you try to get the most amount of base pairs for a given sequence, but the problem is that the biological sequence is actually not always emitted by the optimal solution. So, this is an example here of a biologically, a real biological rna structure, a tRNA, a transfer rna. And you can see here in this dot plot that the bottom quadrant is a, is the globally optimized structure or the minimum free energy structure. So each dot here represents a base pairing in the string of nucleotides. On the top quadrant, what we have is a Boltzmann distribution of the suboptimal base pairings. So, not all the, not the most optimal base pairings, but rather a distribution of other probable base pairings.
00:20:06.502 - 00:21:18.318, Speaker B: So, for instance, some of these bases have a very small dot or very small probability of being the most optimal solution. But when you look at this data, more often than not, the real biological structure is in the suboptimal space and not in the algorithmically optimized space. So we wanted to leverage this information when we compare multiple rna structures together. We came up with an algorithm that is essentially heuristic for optimizing the sequence structure similarity of two strings of nucleotides. So, if you're familiar with, you know, sequence alignment, you have your basic swift Waterman, or in this case, Needleman Wunsch algorithm, dynamic programming matrix that will find the optimal path between an optimal alignment between two sequences. What we did is we used a probabilistic alignment approach where we generate Boltzmann distribution of the sequence alignment. So we have kind of a, a stochastic backtracking strategy where we can iterate through multiple paths through this matrix to look at the probability of different alignments happening.
00:21:18.318 - 00:22:48.928, Speaker B: And we complemented that to a weighted scoring function where the base pairing probabilities were used to assign weights to the match scores. So essentially, backtracking through this matrix, we're going to warp it towards alignments that optimize the common base pairings between the two sequences. So if you use this approach, you actually see that in this case, these two structures seem quite different if you look at their basic predictions, but their suboptimal space actually has a common structure between them and this algorithm should be able to pick it up. So we benchmark this using the same controls that I talked about earlier, which is the known rna structure alignments that are generated from biological data. And we created this kind of this binary truth table where each family has a one in the matrix if it's within the same family, and if it's not, it has a zero. So we randomly sampled this RFAM database to generate these positive controls, and then we compared different rna structure comparison algorithms to each other to comparatively benchmark our tool called dot aligner. What we found is, the first time I generated this RoC curve, I thought there was an error in my r code because it was just a perfect 90 degree angle.
00:22:48.928 - 00:23:40.674, Speaker B: At least it seemed like it. But it turns out that the dotteliner is actually quite accurate at saying whether or not two sequences or two rna sequences are the same or not, or same rna structure family, rather. And surprisingly, an interesting thing is that this gray one, this gray algorithm, which is the basic pairwise sequence alignment, performs quite well at saying whether or not two rna structures are the same. And that's because most of the time in these databases, you have two sequences that are very similar to each other. For instance, you might have a human and a chimp rna that will be almost identical. So we had to tune the benchmarking and function of the similarity of the sequences. And that's where we found that our algorithm had a slightly higher edge in the lower sequence similarity threshold.
00:23:40.674 - 00:24:34.274, Speaker B: So between 50 and 70 or 80%, our algorithm was actually quite good at maximizing the sequence and structure similarities, whereas the needleman Wunsch kind of struggles a bit at lower sequence similarity thresholds. Now there's another tool called Karna, which is actually very robust and does a really good job of comparing two rna structures, but it's super slow. So as I mentioned, we wanted to develop a heuristic that would allow us to process thousands of sequences very quickly from these high throughput sequencing datasets. So we couldn't use condon, it's just way too slow to do all versus all pairwise comparisons. So now we had this, we had this algorithm that's good at generating comparisons between two sequences. It's pretty fast and it's pretty accurate. So we can generate these distance matrices, which are gold for machine learning.
00:24:34.274 - 00:25:36.934, Speaker B: We wanted to apply an unsupervised clustering approach to this. So, hierarchical clustering, we explored it a bit and it just wasn't, it wasn't good enough. You have to apply a fixed threshold, typically in these hierarchical clustering strategies, and using a fixed threshold just didn't work. So we stumbled upon, we explored a few options and we stumbled upon density based clustering as a really good solution. In particular, this optics algorithm, which you might be aware of, which essentially if you give it a distance matrix, it converts that distance matrix into a coordinate system, and then it measures the distance between two points, finding the closest distance and moving on, and then sorting the points based on their distance to each other. And then you can cluster similar data or entries in your data set by looking at the lulls in these reachability plots, as they're called. So the different colors will be delineated by peaks.
00:25:36.934 - 00:26:31.134, Speaker B: So we benchmark this strategy on our dotted liner algorithm using the RFM dataset, as you might have guessed, to try to find which parameters would be the best. And we came up with a pretty good threshold. So, not perfect, but still pretty good. And the way we tested this was by throwing in known RNA structures with randomized sequences from the same structures. And on the bottom here, you can see that most of the clusters are actual known rna structures and they group together quite well. So we benchmarked the clustering algorithm and now we wanted to apply it to biological data. So, as I mentioned earlier, we downloaded this rna immunoprecipitation data called eclip for cross link immunoprecipitation.
00:26:31.134 - 00:27:15.374, Speaker B: We filtered it to get pretty high confidence peaks or regions that have lots of reads that map to the genome. And then we took those sequences and submitted them to this dot aligner, all versus all pairwise comparison, followed by optics clustering. And this is what we got in the results. We also spiked in a few positive controls, because it's good to have that sanity check. So, known RNA's, rna structure families, just a handful of them, to see if they were going to get picked up. And you can see here there's not many clusters that get detected in this data, but there's still nonetheless some colored regions which are indicative of clusters. So one of the clusters is known.
00:27:15.374 - 00:27:48.020, Speaker B: So this is basically just our spike in one of our spike ins, where it's a snow RNA structure, and we picked those out quite well. And then we had some other clusters that were kind of mixed. So we had known rna structures, controls that were clustering with novel sequences of the same rna structure. So this particular cluster has a enriched for. It's enriched for DKC one protein binding. And DKC one is a protein that interacts with. With snow RNA.
00:27:48.020 - 00:28:21.964, Speaker B: So it's a known interactor with these snow RNA structures. And you can see here the snow RNA structure that came out of this is. That's a clear snow RNA structure. And we picked up three new snows that weren't known, weren't in the date, weren't annotated in our data set, and we also picked up a new one that wasn't. It's not even annotated in the human genome. So, essentially showing that we can use this approach to identify new rna structures in the human genome. And finally, there's, of course, there's a few new structures that weren't previously known.
00:28:21.964 - 00:29:20.198, Speaker B: So this one here is characterized by a domination of UPF one protein binding. So most of the sequences in this cluster were associated with UPF one, and that's the structure on the top left there. So a new rna structure that interacts with that protein that wasn't previously known. Then the next step from this is that you can actually take that alignment, take your cluster, align these sequences together, and you generate one of these probabilistic models called the covariance model, and then scan the genome for evidence of other hits in the genome. We did this for this new cluster, and we found thousands of hits in the human genome, which always raises a flag about false positives. But then, when we looked a bit closer, it turns out that upf one is known to interact with another protein called Stalfin. And Stauffen is a protein that's also known to interact with repetitive regions in the human genome, or repetitive rna's.
00:29:20.198 - 00:30:34.016, Speaker B: So this made sense to us that it's kind of a validation that, indeed these hits were enriched in the repeat in question, so it wasn't just spurious hits happening elsewhere. So, going back to that initial slide I presented earlier, we want to then map this out a bit wider. So we have a student in the lab, Vanda, whose this is a PhD project, is essentially generate these genome wide maps of rna structures and try to cluster them into functional, or get some functional information out of these regulatory maps. So the idea is that we have these conserved structures. We know that some of them are interacting with proteins, and we can annotate them and build this kind of index for the human genome. But the next step is to overlay that with known transcriptional products, so known loan non cornase and new loan on cornase. For instance, if we sequence a patient here, look at the transcriptome and identify a new long coding rna, which happens in almost every transcriptome data set, if you look properly, well, we'll be able to say, well, this lung according, this protein, because it has this structure in it.
00:30:34.016 - 00:31:14.850, Speaker B: So it's probably going to be involved in this biological pathway. That's our global objective now. Now, when I was doing my postdoc, I started to dabble in high throughput rna sequencing data, and it turns out it's really, really messy data. So typically people just take your, your sequencing reads, map them to the human genome and don't worry about it more than that, and then they just overlay that with known transcripts and get your gene abundance that way. But the truth is that the reference data sets are poorly characterized. So there's a lot of long encoding RNA's, for instance, that aren't actually in the data set. So you'll be missing those and vice versa.
00:31:14.850 - 00:32:38.594, Speaker B: You might be quantifying genes that aren't actually in your data set just because there's a bit of overlap in there splicing or their transcriptional products. So, to highlight this problem, this is an example of a synthetic gene that we generated molecules from and put those into a sequencing reaction or sequencing experiments using classical short read sequencing or illumina sequencing, which is what 90% of people are using nowadays for rna sequencing. And you can see here four different replicates of this synthetic standard. When you assemble agnostically using gold standard tools, the transcripts that come out of this experiment, you'll notice there's a false positive here that pops up. So this splicing pattern isn't actually in our biological data set, but the algorithms pump it out because they're inferring what the structure of the transcripts are with incomplete information. So long read sequencing, which has really come out and developed in the past five years or so, is the solution to this. The same sample here, we sequenced it with long reads or nanopore sequencing, and we get the complete transcript structure very easily without any guesswork computationally.
00:32:38.594 - 00:33:30.250, Speaker B: So if you're not familiar with nanopore sequencing, it's fundamentally different to illumina sequencing or short resequencing, in that it actually sequences individual molecules. And it does so by transiting the molecules through a protein nanopore that's embedded in a membrane, and you pass an electric current through the nanopore and the nucleotide, or the chain of nucleotides will stall or reduce the current. And you get these electric current traces called, we call them squiggles. And you can use those squiggles to infer what the biological sequence was. So this is what the data looks like from an anapore sequencer. If you look at the raw data that's generated from it up here, the spikes in current are when the pores open. So there's a lot of current going through.
00:33:30.250 - 00:34:19.010, Speaker B: And then when a molecule enters the pore, the current drops, and then you get these characteristic disruptions in the signal. If you zoom into a single molecule here, this is what it kind of looks like. And already you can see some of the features of the actual biological molecule. For instance, these little stall regions here are either adapters or segments of the rna that are known to be poly A or homopolymers that basically have no variation in the nucleotide composition. So the current is flatter. So to analyze this kind of data, the algorithms that are used in the first part is a segmentation algorithm that's going to look at the signal data and process it into chunks, kind of like a stepping algorithm. Next you have a probabilistic deconvolution of the raw signal.
00:34:19.010 - 00:35:09.220, Speaker B: This used to be done initially with hidden Markov models, and it eventually evolves into, as you're probably familiar with, these more sophisticated neural networks and deep learning models, which are currently what are used to predict what the nucleotide sequence is from the raw current. The problem that has stigmatized this technology is the error rate. And the error rate comes from the probabilistic deconvolution of the signal. So if your model isn't sufficiently complex or trained enough, you're going to get lots of noise in the output. And the noise stems from, from two aspects, the probabilistic aspect and also the stochasticity of the sampling. So as the molecule goes through the pore, there's time variations. Each nucleotide won't stay for the exact same amount of time.
00:35:09.220 - 00:35:59.864, Speaker B: There's going to be some variation there that's a bit stochastic. For instance, there could be a snag between molecules, or you could have other reasons, like a cosmic ray or something that will come in and, you know, change the current. But nonetheless, the main contributor to sequence variation is or error is the probabilistic modeling. So this is what the data looks like on the right here, the purple dots that you see here, each one of these lines is a read from the sequencer, and the purple dots are actually insertions and deletions. Those are the, the most common error you get where the segmentor is going to kind of skip a nucleotide because there's only a few data points, or it'll treat the same nucleotide as two because there's more data points. So that's currently, I think, where the biggest bottleneck is in this technology. But nonetheless, you can use it to do some really cool stuff.
00:35:59.864 - 00:37:00.744, Speaker B: We showed here that you can use it to characterize full length antibody sequences from in single cells, and you can also use it to track lymphocyte clones across different tissues by looking at their unique antibody or t cell receptor composition. I won't talk about this data too much because it's more biological, but in this work, the biggest challenge we had was that error rate is a bit confounding. We actually had to use the accurate reads from the short read sequencing to then go and fish out the long reads from the nanopore sequencing. And in doing so, we actually lose a lot of the data because of that error rate. So these reads only have about 15 or so nucleotides that tell you what cell it came from. And with a 10% error rate, you're going to be missing one or two of those. And essentially you're losing about two thirds of your data, possibly more, due to that error rate.
00:37:00.744 - 00:38:30.660, Speaker B: Now, in the lab, we kind of got burned with the, using the sequencing the nucleotide sequences in nanopore sequencing, I spent a lot of time developing algorithms for mapping these really noisy reads to the genome. And then one day to the next, there's a software update or a chemistry update, and then the technology goes from 20% error rate to 2% error rate, and then your algorithm is basically useless. So we decided in the lab that we're no longer going to be looking at sequence, or at least we're not going to be developing algorithms for sequence analysis, but rather for signal analysis to process this kind of data. And here's an example of a signal that you get from this single cell data, where if you're able to zoom into the region of interest and look at the raw signal directly, you can bypass a lot of those errors that stem from the conversion to nucleotides, because the raw data itself is actually, there's no errors in it. You're measuring atomic differences and there's a bit of noise in it, but there's no error per se. So to do this, we needed to develop some tools to help us process this kind of this raw signal data from nanopore sequencers. And we came up with this package called Squigglekit that essentially allows you to manage the thousands of files that are generated, plot the data easily, chop it up, or segment it into regions of interest.
00:38:30.660 - 00:39:38.114, Speaker B: So in this case here, we wanted to pull out the regions that were, that had the single cell barcode information and also do some kind of motif analysis. So we came up with this motif seq tool that essentially does a dynamic time warping algorithm, or a dynamic programming algorithm on the raw signal. And the idea was that you give it a sequence of interest and then it will convert that sequence into an expected electric current, and then compare the expected electric current to the observed electric current and find the optimal position within your raw data for that motif. So essentially, it's kind of like a control f or a blast for signal searching. So that package was really, it's more biologist friendly, I'd say, to help people kind of process their data. But playing around with the raw data, we notice there's a lot of features in the raw data that they're just strikingly obvious and easy to process. So, in particular here, I'll draw your attention to what you get when you do some rna sequencing with nanopore.
00:39:38.114 - 00:40:21.948, Speaker B: And that is, if you look at these current plots, at the bottom here, you have the barcodes that come out of the rna sequencing are strikingly obvious. The current profile is completely different to the rest of the molecule, and that's because I'm talking here about direct rna sequencing. So not DNA or cdna sequencing, where you take your rna and convert it into DNA, but rather straight rna measuring molecules directly at the rna level. And to do that, in this protocol, you have to add a DNA barcode to the end of your rna. So the current from the DNA is actually quite different from the current to the rna. And that just pops out when you look at the raw data. We came up with a way to.
00:40:21.948 - 00:41:28.390, Speaker B: We played around with some deep learning on this data, and we found that 1d convolutional neural networks weren't that great at classifying the signals. And my colleague Tanzel came up with a really clever idea, and that is to convert the signal into images using. In this case, we tested a few different approaches, but the one that worked the best for us was called a Grammy and angular summation fields, which essentially looks at the difference in the current and plots it in a 2d bitmap. So seeing this in the data, we thought, well, you know, what? We could easily come up with a new way to barcode libraries. So the idea here would be that you want to pool multiple samples on a single sequencing run and save on costs or increase the amount of biological samples that you can get out of one flow cell. And the idea was that we could very easily design barcodes that would have unique signal or unique images that come out of them. So that's what we did.
00:41:28.390 - 00:42:10.476, Speaker B: We hacked the protocol. The company actually didn't have a protocol to do barcoding on their technology yet, so we hacked it to include these different barcode adapters. We started with four, and all we did was shuffle the sequence in the DNA barcode, and we got very unique signals that came out of those four different barcodes. And then we converted them into these Grammy and angular summation fields or gas images. And if anybody can see a pattern in this data, I will buy you a beer, because I've looked at hundreds of these visually, by eye and tried to find patterns. And you think you find some and then you realize that it's not actual. They're not the same barcode.
00:42:10.476 - 00:43:13.814, Speaker B: For instance, in this case, the four different barcodes are on each row. And you can see that you might think that, you know, this guy and this guy are very similar to each other, but in reality, they're not so. Perfect challenge for deep learning algorithm. We tried a few different neural network architectures and we settled for this resnet. So a recursive neural net, or sorry, residual neural net with 20 embedded layers, and we got some ridiculously accurate data on the testing and validation data set. And then we did some independent biological replicates and we found that that accuracy kind of dropped a bit because the models were so overfitted to the data that, you know, if you. You could probably tell which song was playing in the lab based on how accurate these neural nets are when you did the sequencing, but nonetheless, on independent technical and biological replicates.
00:43:13.814 - 00:43:58.182, Speaker B: We got really good results with this neural net classifier. In fact, here, rep one is a bit lower, but it was actually done with a completely different chemistry. And we trained on a new chemistry and tested on older chemistry, and it still worked pretty good. So we're able to get really good classification accuracy using this raw data. Now, to put it in context currently, for DNA sequencing, the demultiplexing that's done uses the sequence. So you do the base calling, and then it looks at the sequence and it does a sequence alignment or maybe even an hmm to try to classify those barcodes. And we're getting, you get about at best 80% classification recovery.
00:43:58.182 - 00:44:46.004, Speaker B: So 20% of your data is going to be lost. It's more around 30%, actually, on a typical data set. So using this raw data, we show that we can get, you know, with a reasonable accuracy of 95%, we can get over 90% recovery, which is fantastic. In fact, in terms of the accuracy, the reads that were misclassified more often than not weren't actual classification errors per se, but were artifacts of the data. So if you look at these raw plots, you can actually see here, for instance, in these two replicates here, there's a spike of current between the two barcodes. The barcodes are in color here, and this spike of current is actually the pore that's completely open and another molecule goes through afterwards. But it happens so quickly that the segmentation algorithm didn't register it.
00:44:46.004 - 00:45:29.784, Speaker B: And it didn't register it because once in a while you get this spike in current and it'll artificially split a molecule into two for whatever reason. You know, we're talking about these nanoscale amp meters that are used. So there's, they're not perfect, but nonetheless, the classifier isn't necessarily misclassifying these reads, but rather the reads are, contain multiple barcodes or artifacts. For instance, like down here on the bottom left side, you can see there's a clear signal aberration in the raw data, which is probably just a stall. Molecule probably got stuck and then eventually passed through. All right, so just wrapping up now, a few more slides. The last story I'm going to talk about is, again with nanopore sequencing.
00:45:29.784 - 00:46:40.484, Speaker B: And it has to do with the capacity of nanopore sequencing to pick up base modifications of nucleotides. So, for instance, DNA methylation is probably the most well known. It's known to be involved in many biological processes, studied frequently in cancer. And it's basically, it shows you the epigenetic state of the nucleus of a cell, for instance. Now you can pick up DNA and RNA modifications with nano four sequencing because the different molecules, the chemical compounds that are added to the normal nucleotides, will generate a unique electrical signature when they go through the nanopore. So this is a well known feature of nanopore sequencing, probably understudied, but very, very popular at the moment, especially for RNA. So because you can sequence native rna molecules with this technology, it offers us the possibility to look at rna modifications like we could just not do previously with classical biochemical methods.
00:46:40.484 - 00:47:45.214, Speaker B: Now, the reason it's interesting to study rna modifications is that there's over 120 that are currently known to occur. I think DNA has about twelve known modifications, but RNA has over 100. And on this pie chart here, you can see in red are all the known rna modifications that are associated with human diseases. Now, the ones that are highlighted in greens are the ones, the only ones that we could accurately, or I wouldn't even say accurately, the only ones that we could measure using conventional methods before nanopore sequencing. So we're essentially opening up this pandora's box of epitranscriptomics research using nanopore sequencing. And hopefully we can help understand how these different rna modifications are associated with human diseases and just normal biological processes in the cell. So we came up with a strategy to study these RNA modifications, combining molecular experiments with machine learning.
00:47:45.214 - 00:48:37.426, Speaker B: And essentially what we did is we, we generated about a 10,000 long nucleotide sequence that encompassed every single possible five mer, and I think 85% of all six mers. So five and six mers are about the size of the protein nanopore. So you're going to be encompassing most of the current states using this kind of sequence diversity. We chopped the ten kb sequence up into four smaller sequences of different lengths so we can split them on a gel. And these got cloned into bacterial vectors here. And the vectors have a rna transcription start site at the beginning of them. So you put in these sequences in the vector and then you can transcribe rna molecules from them in the system.
00:48:37.426 - 00:50:05.484, Speaker B: And the idea was that we could generate in vitro rna libraries that had normal bases, normal nucleotides in them, and also substitute one of those modifications of one of those nucleotides for a modified nucleotide, so that we're generating a library that has 100% modified, in this case, a residues, and then we could, you know, compare both data sets and generate a classifier for rna modifications. We first tested two modifications known rna modifications. One is called pseudouridine and the other is m six, a or n six methyl adenosine generated nanopore sequencing libraries and sequenced these rna molecules on the nanopore sequencer. And what you can see here is, unsurprisingly, the normal library generated a nice typical output, whereas the m six a library, even though it generated 44,000 reads, we had much less reads that aligned to the genome or the reference sequence here and a much higher error rate, which was expected. Pseudouridine, on the other hand, almost no reads aligned to the reference, even though we generated 15,000 reads. So if you zoom in here, you can see there's a much higher error rate for the modified ones. Now, the reason that the modified reads aren't aligning as well isn't that they're not generating data.
00:50:05.484 - 00:50:49.274, Speaker B: It's just that the base colors cannot, aren't trained to emit nucleotide sequences from those current traces. So you're essentially getting garbage in the base culling step. But if you look at the raw data, you can imagine, and indeed does, it looks perfect, like you couldn't distinguish it by eye. So we dissected this data. We focused on M six a initially, and we dissected this data. Looking at these, you can see it here, very conspicuous differences in mutational patterns. So we posited that the mutational patterns you get from a given base color can serve as a fingerprint or a signature for those modifications.
00:50:49.274 - 00:51:35.156, Speaker B: And you can see here, for instance, there's lots of adenosines here. So in the m six a modified, we're getting different nucleotides than the a. So we decompose this data down into the different features of it. We looked at base quality, mismatching, insertions, deletions, and we even looked at the raw current levels and we compared normal to normal replicates and modified to modified replicates, very little variation between them. And when you compare normal to modified, then you get a nice spread. And if you're, you know, if you've done machine learning, you see that spread and it just speaks out as a perfect application of a SVM support vector machine classifier. So that's what we set out to do.
00:51:35.156 - 00:52:31.884, Speaker B: And specifically, we focused on a known m six A motif. So this is a region motif that's known to bind the m six A modifier protein. And we compared the known motifs with an a in the middle to the known or control motif that essentially has the same sequence features but doesn't have the a in the middle. And for both of these motifs, we compared modified and unmodified. And here you can see, for instance, when there's no a, as you would expect, there's, the signal is very much indiscernible from modified and unmodified because there's no modified basis. Whereas here you can draw almost a perfect line, diagonal line, down the middle, and you would get pretty good classifier. So we mixed and matched a few of these different features into an SVM that we trained, and we found that combining more features generally was beneficial, as you would expect.
00:52:31.884 - 00:53:32.964, Speaker B: However, when we looked at the same combination of features and compared that to the control, we found that more wasn't always the best. In fact, we got a lot of false positives, and we included all the features, but rather just the quality. And the mismatches were enough to give us a really nice AUC of, like, I think, 0.9. And that's what we used. We ended up testing this on some biological data where we took yeast strains and a normal yeast strain, or wild type yeast strain and a yeastrain that had a knockout gene for that DNA methylation enzyme. So we'd expect essentially the knockout to be just like our training data, the 100% normal data set. Of course, the knockouts worked, and as expected regions that had known modifications, our tool could predict that they were present in the wild type, but absent in the knockout.
00:53:32.964 - 00:54:51.008, Speaker B: So, last few slides here, just to wrap up. The enormous scale of technological advancement that's happened in genome sequencing over the past few decades is really dwarfing the technological advancement that we've seen in computers over the past century. So the question is, how are we going to adapt and merge these two technologies together to really benefit biomedical research and clinical applications? You know, we've got sequences now that fit on mobile phones, nanopore have one of these, and still it's been a better prototype for the past two, three years, but you can still, you can use these highly compact sequences on mobile devices. So, in the lab now moving forward, what we're interested in is trying to use machine learning approaches on nanoport data that's generated in real time to help make decisions faster in the clinic. Right now, sequencing clinical genomic applications take weeks, if not months, to give results back to clinicians. And we want to leverage the capacity of nanopore sequencers to do this in minutes or hours. One of the things that we've been working on is actually a real time classifier for signal data.
00:54:51.008 - 00:55:49.374, Speaker B: So the idea is you give it a biological sample of interest, and while you're generating the data, you can compare it to existing data sets and find a match. We've showed that you can very easily discriminate between tissues in a human body, or in this case, it was mouse, in a matter of minutes. So 50,000 reads is the equivalent of about five or ten minutes of sequencing. So the problem and the challenge with dealing with this real time data now isn't necessarily the classification, the alignment, the bioinformatics aspect of it, but rather processing the data quickly enough. A typical nanopore sequencer called the dominion generates something like 1.5 million data points per second, and the write speed of that is about the same as a USB stick. So if flash memory struggles to keep up with a small nine and poor sequencer.
00:55:49.374 - 00:56:34.674, Speaker B: The challenge is really going to be at the data acquisition step, and we've played around with this a bit. Last year, we wrote this, we actually modified an algorithm for sequence alignment so that it could run on your mobile phone. And the rationale for this was that we're generating so much data, we want to be able to process it quickly, so we have to be able to distribute it on lightweight systems. We can't use HPC for this stuff. It has to be light, it has to be cheap, and has to be multiplexer or parallel. So we actually been, we developed this prototype for this embedded system array, or a small compute module based of these rock 64 chips. These are like Raspberry PI's, if you heard of them, very small compute modules.
00:56:34.674 - 00:57:12.194, Speaker B: But the idea was that that acts as an initial sieve to filter out the data, do your basic bioinformatics sequence alignment and base calling, so on and so forth. So with that, a big thanks to people in my current lab and also my colleagues in my lab in Sydney before moving here, who are involved in a lot of the nanopore sequencing work that I talked about, and of course, my collaborators. Sorry, that's popping out of the screen. But the other international collaborators. And if you have any questions. I'm sorry, we're a bit short for time, but I'd be more than happy to take any questions.
00:57:13.054 - 00:57:34.526, Speaker A: Exactly. So I was just going to say, if you have any questions, we probably have time for one question. If you want to use your chat or unmute yourself, I'll give you a couple moments, and if not, I think I have a question. So that will wrap it up. Well, maybe I'll start. So, I guess, I mean, great talk. Thank you.
00:57:34.526 - 00:58:07.216, Speaker A: I don't think about rna sequencing as much as DNA sequencing, so it's always interesting to hear about your work. And this idea where you could do these long reads fast or faster means that we could do sequential sequencing a lot easier, and so we'd have kind of now this temporal slice of how things are evolving. And do you see that as also being a potential large application, or is that. Maybe I misread that last part of the talk?
00:58:07.360 - 00:58:48.794, Speaker B: No, no, absolutely. Generating data on the fly is something that's kind of. People don't quite grasp it yet. I think it's definitely not percolating into the clinical realm experimentally. It might not be as important because you can just regrow your cells or something, but I think for patients, it's quite interesting, the whole COVID pandemic thing has really helped show that you can use this real time sequencing not necessarily to process data in real time, but to generate data much quicker than you would with an existing NGS platform. And you can also decentralize. You can do it in your basement if you wanted to.
00:58:48.794 - 00:59:08.574, Speaker B: The sequencers actually only cost about $1,000 for a starter pack, and you get enough reagents to do a few experiments. So, yeah, that's something that's gonna definitely, I think, gonna gain more traction in the genomics communities. What kind of stuff can you do now that you can sequence things in real time?
00:59:09.354 - 00:59:45.804, Speaker A: Yeah. Cause I think, like, I was thinking of COVID for example, or things that are changing intrahost, looking at expression levels over time, will become much more accessible. And I think that that has a lot of applications for different diseases that we see kind of disease trajectories, and we want to understand how things are evolving. So, yeah, I think that's super interesting. I'll just open it up because we're right at time. So if somebody has one question, please let me know, and then otherwise, as Martin said, he's available if you have any questions.
00:59:47.784 - 00:59:48.432, Speaker B: Great.
00:59:48.568 - 01:00:13.994, Speaker A: Okay, so, thank you again so much for talking today. I thought. I mean, I'm fascinated by her work. So, thanks again to Martin. And our next talk will be the first Wednesday in December, and it'll be given by Daniel Reeves, who's from the Fred Hutch center. So I will send out the announcement of the title and the abstract and hope to see you in one month. So thanks very much, everyone.
01:00:14.534 - 01:00:15.774, Speaker B: Thanks, Morgan. Bye, everybody.
