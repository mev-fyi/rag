00:00:00.200 - 00:01:46.872, Speaker A: So I'm very glad to have a special session of the fields mathematics for public health colloquium on these beautiful sunny summer days in Ontario. Ethnicity part of the places, I'm saying I'm very glad to have today's speaker come from the department of Mathematics and Statistics of Brock University, Torah Ramazi. I met him, I heard him long time ago for from my colleague and from Universal Alberta, where doctor Ramadi has spent his postdoc fellowship there, I think with Mark Louise. And before that doctor Mali has very colorful training, starting with bachelor degree in electrical engineering for University of Tehran, Iran, followed by a Master of Science degree in system control and robotics from Roy Institute of Technology, Sweden and then a PhD degree from again aerial serious assistance and control from the University of Groningen. As I said, he is currently a faculty at the Broker university. I'm very glad we have the pleasure of engaging his expertise of AI incorporated into mathematic modeling of infectious disease modeling. So poorah with your time, thank you.
00:01:46.888 - 00:02:04.524, Speaker B: Very much for the introduction. I'll just be sharing my screen. I guess it's visible. Yes. Okay, thank you. Okay. Hello everyone.
00:02:04.524 - 00:03:34.904, Speaker B: In this talk I'm going to talk about mathematical modeling of disease spread and the use of machine learning tools. So I would like to start with a little perspective of what are the two main approaches used often for modeling disease spread. On the left part we have the mechanistic modeling approach when we want to model, for example, the spread of COVID And for example, we have their ordinary differential equations such as the well known sir model. So what happens in this approach is that usually a set of variables are considered and the dynamics between these variables are explicitly modeled. Now these models, because exactly that were explicitly modeling them, they are quite insightful and they have proven to actually predict the near future quite well. However, they are based on our prior understanding of the work, right? So all these equations that we write down, they are based on some prior knowledge. So a little shortcoming with them is that they are only as good as that prior understanding.
00:03:34.904 - 00:04:19.104, Speaker B: So if that prior understanding is up to a certain extent correct, or if it's wrong, the same applies to the results. And also they do not extend easily to situations with unknown dynamics. So for example, if you have a process going on, you don't know how the interaction between the variables are happening. Well, it's really difficult to write down equations for them. And also if you have too many variables, then you need to make simplifications. You cannot really write down all the interactions between the variables. For example, one thing that happens that I will address later is the issue with estimating the transmission rate in SIr models.
00:04:19.104 - 00:04:58.184, Speaker B: Often they're assumed to be constant, whereas in reality, of course, they change over time. Now, on the right column I have listed data driven modeling. They can be ranging from very simple models to extremely complicated but deep learning. But here, for example, I have listed the LSTM model. They are often fully based on data without any expert knowledge. So they have proven sometimes to be exceptionally accurate in predicting the future. I would say sometimes, not always.
00:04:58.184 - 00:05:40.552, Speaker B: And well, they come up with some shortcomings. Of course, they provide often close to zero insight because they bear a black box design, and most often they are not interpretable. And of course they are not causal. I don't want to get in into the details of causality here. It's all different topic, but these are some of the shortcomings. Now, I have met with great mathematicians working on the left side and with great data scientists working on the right side. I myself are.
00:05:40.552 - 00:06:48.514, Speaker B: I would consider myself as bilingual that can interact with both sides, but I understand sometimes the extremes of each and the concern. So it often happens that mathematicians, they would not consider using any data driven modeling because exactly of the shortcomings that they have listed there. They even say, how can we even trust these models? They're only based on data. And then I also see comments from the other side, some extremes in the data driven modeling experts. They say, why should we ever use any mechanistic model where we can learn everything from data? In this talk? Because it's mainly math theme, I will see things from the left column, from the mathematical approach. I'm assuming that we are looking at the work from the lens of a mathematician. And I want to explain, first of all, why should we be using data driven models? And secondly, how to use them to improve our modeling.
00:06:48.514 - 00:07:36.976, Speaker B: So the overview of my talk would be, first, to show the power of machine learning approaches. I will explain the data approach results and a package that we have made and some concluding remarks. And then I will talk about how to bridge mechanistic and machine learning models at the end of my talk in the second part. Okay, so I will use a case study COVID-19 I don't need to explain much about it, just a little update. Up to today we have around 540 million confirmed cases and 6.3 million deaths updated today, the case study that I will be focusing on is the US country. So by today, 1 million deaths, 86 million cases.
00:07:36.976 - 00:08:25.004, Speaker B: And these are the graphs of the daily death and confirmed cases. So the first part what is the goal? I want to predict the number of COVID-19 deaths and confirm cases in the US national wise in the future, five to ten weeks. So it's kind of long range predictions. Okay. And, well, what data we have collected for that is mainly from March 1 to November 17, 2020. Johns Hopkins University data repository Google reports for community mobility and daily summaries data set. And if you're interested, a county level version of the data set is available at this link.
00:08:25.004 - 00:09:08.960, Speaker B: Good. Okay. A summary of the data, number of daily cases, daily death. We're using these covariates or features, number of daily tests, and we try to provide an average of the whole country, temperature and precipitation as a weather covariate. We also included mobility data collected from Google, including parks, transit stations, residences, workplaces, groceries, pharmacies and retail shops. Okay, so what is the approach we took here? First of all, we. We wanted to exploit the historical values of the data sets.
00:09:08.960 - 00:09:57.836, Speaker B: Okay. So in order to predict the number of death or confirmed cases at day t plus r, imagine that we are at day t or week t. We want to predict our weeks in the future. That is the forecast horizon. We will be using not only the data at time t, but also we will range back to, for h h, previous values, which we call history length. Okay, so how do we construct it? A little figure here, a single data instance with history length, one that is basically no history. So imagine that we are at this day 14 February 2020, the way we do it.
00:09:57.836 - 00:10:45.598, Speaker B: And the forecast horizon is two. So imagine two days in the future. So the data instance that I will be using is that I will include all these training, all these features, say the temporal id, the spatial covariates, the temporal covariates, I will put them here in my historical data. But then instead of the target variable, which was the number of COVID deaths at day 14, I will replace it with day 16. Okay? So I'm forcing the model to learn from the information at time t, the target at time t plus two. Now, if I want to argument this with some historical values, what I will do is that I will add this temperature at the previous day. So we'll just add it right here.
00:10:45.598 - 00:11:36.028, Speaker B: And then I will ask the model not only use temperature at time t, but also t minus one, and I can go back like here. I think we tried, like five, up to five previous historical values. Okay, now, data splitting two main points first. Okay, these may sound trivial to machine learning experts, especially the first point, but I have seen that in mathematics, often the first one is not taken seriously. A testing data set is needed to evaluate the model. Okay, if we want to talk about, if we want to compare the models, it's not the best idea to compare them on a training data set that this same data set that used for calibrating the parameters, it's good to put a separate data set to test them. Point number one.
00:11:36.028 - 00:12:09.510, Speaker B: Point number two, this task is a time series prediction. So a random split of train tests or a k fold cross validation may not be used. Okay. And the reason is we want to mimic the situation in the real world. In the real world, you never get to see the future instances. You can only use the data for the passing instances. So if you do a k fold cross validation, I won't be surprised if you will obtain misleadingly high performance values.
00:12:09.510 - 00:12:41.962, Speaker B: And this is why in some applications I see that people even report like an accuracy of hundred percent. Well, it's just because that there's information leakage from the test data set. So we perform. So what is the solution? Well, we perform a temporal partitioning. So the test data is the single final instance. Say we want to predict day 14, we'll put it there. We additionally added now this validation training, you don't need to do that, but this is our approach that we tried here.
00:12:41.962 - 00:13:14.924, Speaker B: So we also had a validation data set and that was the single pre final in Stan. Okay. The last, like day 13 and for the training, all the remaining ones could be used, except for the fact that the gap data should be avoided. That I will shortly explain. Now, this result, you see that we're not. So if you want to predict seven weeks in the future, we're not taking all of that and train a single model to predict it. What we're doing that is that we will train a single model for every instance in the test data set.
00:13:14.924 - 00:13:50.586, Speaker B: And this last fold partitioning forecaster, which we call it Lafo patho, this is the model that we'll be using. It's more an approach rather than a model because you can fit it with any model. Here, a little description of this data splitting that I was talking about. So if you have all these weeks, imagine you want to predict week 32. I guess the image quality is not high here enough, but I guess it is understandable. Hopefully we have week 32, we want to predict this week. And imagine that we are here at week 23.
00:13:50.586 - 00:14:31.894, Speaker B: The history length is three, so I'm allowed to use the previous past two weeks. So this is the box that I will be using. So the task is here. Whatever model you use, you can use all this information and then you want to predict up to nine weeks in the future. Okay? So point number one, if you want to do this, if this is the reality, you cannot use any of these instance, the red instances here. Why? Because, well, they're using some information about the future, right? Like none of these weeks will be, should be used. And this is what we call a gap data set.
00:14:31.894 - 00:15:23.964, Speaker B: Point number two, we're using a validation data set as a single instance. So the yellow one is here, so we can use exactly up to here, and then all the previous ones can be used at train. Now, this gap issue will not happen if you're using like a, consecutively reproducing models like an LSTM where you predict date t plus one based on t, and then t plus two based on t plus one, and so on. But it happens. It is an issue for models that the target value is sometime in the future. Another illustration, if you have your data set as a table where the temporal ids are listed here, say day one up to 27, and then 28, 29, this is the gap based on the forecast horizon. You should not use this.
00:15:23.964 - 00:16:24.774, Speaker B: All the remaining previous ones can be used as the train and all the after ones can be used as the test. Okay, so what model did we use? We used, I would say probably the most simplest model that one can ever use, or at least one of the simplest ones, the k, nearest neighbor. What does it do? For those of you who are not familiar with it, it will just look at the training data set and find them the most similar instance or instances to the one that you're asking for in the test. And then it will average out the target values to get the result for the test. So it's just simply averaging the similar instances in your train to get that one in the output. How many would you average? Well, that is determined by the k here, this k nearest neighbor. So number of parameters, one.
00:16:24.774 - 00:16:57.848, Speaker B: Okay. And, well, we use the MRMR to rank the features. And again, to emphasize a different model, setting was allowed to be obtained for predicting each testing data instance. So, meaning that a different k could be used for each instance in the test data set. Here are the results. Okay, so predicting the number of deaths for over seven weeks. And in the x axis here you see predicting the future, five, six, seven up to ten weeks.
00:16:57.848 - 00:18:01.834, Speaker B: And on the y axis we have the error, the prediction error, mean average percentage error, or map. Now, the purple one is our model, the laffopatho. And the other ones are all the models in the CDC website of predicting us. Now you see, overall it's doing better, but it's doing better than what like this is. I would say it's a, it's very, like, thoughtful with like, considerable result in the sense that by just averaging out, this model is performing better than lstms than probably, like, we see the list of some, like, ScRR model. I mean, this is not an LSTM with like, or a CNN model with hundreds of neurons or 20 dimensional SIr model. No, it just has a single model and look at the performance.
00:18:01.834 - 00:19:03.496, Speaker B: Even if we forget about the comparison, it's getting like an average error of 20 or 10% predicting two months in the future, two and a half months in the future. And for the cases, well, the error were worse, but still it's overall outperforming the other models. Well, is it guaranteed that this will work always for every data set? For every week? No, of course not. And that's a shortcoming with many of the data driven models. But at least this is what we have for these weeks. So, and also you may ask, okay, this is a time series prediction, for example, that the blue one is the reported one and the orange one is the lathopafo. So if we were here at August 1 and we were asked to make a future five week predictions, then we could predict the number of deaths with this error.
00:19:03.496 - 00:19:58.844, Speaker B: And at that same time we could predict that the number of deaths at ten weeks in the future would be this much. Okay, people often say that, well, you know, these models are incapable of capturing periodicity. You see here that it's for future predicting, the future, two weeks daily forecast, it can still capture that periodicity, which many of the sir models cannot. And, well, I know, like, again, based on my interactions with some mathematical biologists, experts, they may say, well, it's just a try and error, right? So for this model, it worked, for this data set it worked. But you just tried out this partitioning, temporal partitioning. It's not necessarily an intuition behind it, okay, for answering that, we tried out this thing. So, so we believe that our model is benefiting from several components.
00:19:58.844 - 00:20:37.444, Speaker B: So each time we took out that component out of the training, and then we tested the results. So first we said instead of a single model for every week, let's just try out a single model for the whole data set. Second, we said instead of the eleven features, let's just use the number of death. Third, we said instead of this last fold partitioning, temporal partitioning, let's use 30% of the train data set, was used as the validation. And then last, let's use cross valid. One of them was like cross validation. Here, here is the result.
00:20:37.444 - 00:21:50.214, Speaker B: So like you see, the Laffopatho overall has the lowest error, and if you change any of these ingredients of the modeling, the error will just get worse. Okay, so I would say that this is supporting the fact of the way this modeling approach was used. And we later extended this coding. All the codes are available online and we put it into an automated spatial temporal python package called the ST predict, which you can easily install using this single line command. The package is quite automated in the sense that you can use a single command to do pre processing data, choosing from different models, like even predefined models, user defined models, and then it will automatically do all the predictions. So what did we learn from this first part? Complex machine learning models are not necessarily the best first choice in predicting a disease spread. Well, this is not, I guess, a bullet point for math, but for more for the data scientists.
00:21:50.214 - 00:22:22.236, Speaker B: But also, even if anyone in general want to try out the machine learning model, it's not suggested that you run into a super complicated deep learning model. No. First, try out simple models. Okay. And second, it is more about the training than the choice of the models. Okay, so, and by that I would suggest that avoid cross validation for spatial temporal predictions. This is a must practice for train tests, but it is suggested only for train validation.
00:22:22.236 - 00:22:51.400, Speaker B: If you want to do any train validation, avoid random splitting. For the same reason, omit the gap data. If it happens, consider developing a separate model for each of the instances in the testing data set and exploit the historical values. Okay. Some tips that you can consider when you want to use the machine learning model. Okay. In the next part.
00:22:51.400 - 00:23:59.124, Speaker B: So I hope I have provided some convincing, or a little convincing at least argument for why should you consider using machine learning approaches. I try to exhibit the potential power in these models, not super complicated, but just a very simple 1 second. But yes, I'm also a mathematician, I understand why we use mechanistic models because of their great insights and many other great advantages they have. But can we somehow bridge between the two models and exploit the good of the two world? And this is what I want to talk in the second part of my talk. So the goal is similar to the previous one, just that here we focus on the daily cases. One of the reasons why we in the previous work we focused on the long range predictions was that, well, for the short range, sir, models did a pretty good job like predicting the future one or two weeks. But they fell for the long range predictions.
00:23:59.124 - 00:24:43.434, Speaker B: So if we want to predict like, say, four weeks in the future, for example, or five here, we know that, sir, models can be used, but, you know, they require this transmission rate beta to be known. And you can certainly make assumptions on how beta will be in the future. For example, you can say that it's called as a piecewise linear, or based on the height, based on the previous number of cases, I. I expect it to go like this and that. Or even you can assume that it's fixed. It's a fixed parameter, which is the case in most literature, in most of the work in the literature. But we all know that it changes over time.
00:24:43.434 - 00:26:30.628, Speaker B: So rather than making this hypothesis on how beta is used, can we learn or estimate it from data from non pharmaceutical policies? Okay, so, of course, one of the main things, main factors that will affect this beta will be the policies that are enforced in the past and in the future. Can we use them to somehow estimate beta to then feed it to this sir model and then get, make a prediction now? And how can we use these policies? This is one of the places that I would say it's really difficult if you want to make a mechanistic model for modeling the pharmaceutical, non pharmaceutical policies and how they affect the SIO models. Of course, there have been nice work on this, but, like, capturing the exact way people interact and abide by non pharmaceutical policies, and then relating it to beta. That is that if you want to do that with a mechanistic model, it definitely comes with certain simplifications. Okay, the data we use is similar to the previous one, except that here, instead of mobility, we didn't use past mobility, we just used a certain number of policies. School closing, workplace closing, cancel public events, restrictions on gatherings, closed, public transport, stay at home requirements, internal movement, international travels. These are listed by c one to c eight.
00:26:30.628 - 00:27:16.296, Speaker B: And there were also some, like, public information campaigns, testing policies, contact tracing, facial coverings and protection of elderly people. There were other, like, you may ask, like why you didn't use, for example, h seven or other. So there were also vaccinations policies that we didn't include in this work. But there's a follow up paper that we have integrated vaccinations as well. So this is the data set that we use. You can see the timeline also here for different of the different policies. What model did we use? It was an Sciar model.
00:27:16.296 - 00:27:55.212, Speaker B: Okay. So susceptible, exposed, infected. So you can see an asymptomatic and removed. You can see the equations here, like we considered beta to also be a function of time, but we didn't write down the explicit equation for it. We have the SIA here. It's, it's kind of a standard way of an extension to the SIr model, nothing special. And the parameters, well, we have here that the transmission rate, I will discuss about it.
00:27:55.212 - 00:28:22.252, Speaker B: We don't want to dedicate a fixed value to it. Total population. We use that at theta e relative turns transmissibility of exposed individuals. We got these values from the literature, 0.1 and similar one for asymptomatic 0.5. Incubation period 14 days. Population of asymptomatic infections.
00:28:22.252 - 00:28:57.744, Speaker B: Sorry, proportion 0.7. We use mu t, which is the death rate of symptomatic infected individuals, which we can easily get from the timelines and Rira recovery rates, which we got from as one over 14. Okay, so I had a graph for beta here, by the way. This is. I will get back to this slide soon. But this is the beta value, as you can see. You can see that it's quite fluctuating.
00:28:57.744 - 00:29:54.686, Speaker B: And I wouldn't consider a piecewise linear model to be a good fit for this also. Well, even if it does do a good job in making predictions, you still have to make assumptions, like how it will change in the future. It's quite fluctuating. Now, the question is, how can I estimate beta? Well, first I need a training data set to calibrate the value of beta. And for that, what I will do is that we will use the inverse method, which was developed previously by Professor Wang Hao Wang and colleagues. We just discretize the dynamics. And then, based on the equations that be here, is, by the way, the accumulated death, we can obtain beta at time k minus one as a function of other variables.
00:29:54.686 - 00:30:40.784, Speaker B: You see, it's based on s at time k. But it doesn't matter because we're not making predictions here. We just want to obtain the value of beta. So by doing this, we can construct a training data set, okay, where we can have the policies, and then we can have the beta. We can the beta value for the training data set, for the calibration data set, and then we can feed the model there. So, after making this data set, we will use a generalized boosted model. You can, if you're not familiar with it, just think of it as an extension to decision trees, a more stochastic version of it.
00:30:40.784 - 00:31:33.904, Speaker B: And how do we fit beta? Well, the policies in the training data, and by that, I mean these policies that I listed here. We will use them in the time series as the inputs to the model and the obtained beta that we explained. We just saw how to get it from this inverse method. We will use that as the output and then we will feed the GBM, and then this GBM with its learned parameters, it will be used to make predictions based on the future policies. Based on the policies. So the role of this GBM is to estimate the beta value and then we will feed it back to the, to the, to our mechanistic model for making predictions. And here are the results.
00:31:33.904 - 00:32:14.550, Speaker B: So here we used the policies listed here and they're the same as what I just showed. 126 days were used for the training. The graph on the left is the transmission rate, how we obtain that from the training data set. So it's from April 4 to December 1, 2020. The blue is the beta obtained from. From the inverse method. The red one is the GBM obtained from the training data set.
00:32:14.550 - 00:33:09.898, Speaker B: And the yellow is the predictions on the test. The mean absolute percentage error of the beta value, not the confirmed cases or death, was here 40%. So you see that it's not really a perfect feet. We can definitely try to improve the feet by, for example, considering different training techniques or different models. We wanted to also to try to keep it as a hypothesis free as possible, but at least it's a good starting point. And then you see that also, even though with this, that like this level of error that we have in the feeding, when you feed it to the mechanistic model, it does a fair job in predicting the future. This is the future, 35 days.
00:33:09.898 - 00:33:34.584, Speaker B: The yellow one. This is again daily. It's not like weekly average or. It's not that. So then what we did was we shifted the, the test data set like one week or a few days in the future. And you see this is. It's more or less capturing the fluctuations, but as it goes further in the future, it becomes a more poor feat.
00:33:34.584 - 00:34:02.582, Speaker B: And then you can see that it is. It was able to capture the rays, right? It's going off. It's not like just monotonically going down. And this is because how well it captured this raised in the beta value. Again, please note that the model is not given this blue part, it's estimating this blue part. The yellow is just an estimation from the policies. Just imagine just from the.
00:34:02.582 - 00:34:27.990, Speaker B: And the policies. The accuracy in the policies is this. It's not like super accurate. Look, you're getting from this data, you want to predict the beta value up to 35 in the future. It's definitely not a straightforward task. But still the up and downs that the model was able to capture. Like you can see here, the yellow you see, like it's more or less capturing this blue peak.
00:34:27.990 - 00:35:17.724, Speaker B: This helped the model to predict that here it needs to race. And this is more visible here. When we increase it to December 2020, you can see that it is more or less capturing the race here. And the map here was just 5%. Now, of course, if we would feed the model with mobility data, as we did in the previous work, and by mobility here, I mean like future mobility, then we would get a better feat. But since we wanted to make real predictions, in reality, we never know how mobile people will be in the future. What we know is that what are the policies that will be used in the future.
00:35:17.724 - 00:36:12.798, Speaker B: So the task was to base just on the policies to predict the transmission rate this mobility could be. It can help in the sense that saying that if we knew the mobility, then how the predictions could be improved and we can do a relative influence of the policy variables, and these are the results. Like you see, the most influential was the restrictions on gatherings. Next was testing policies and so on. Now, here, I just want to point, though, that we definitely cannot make any causal analysis here. Yes, the. Sir, the mechanistic models are more or less causal.
00:36:12.798 - 00:37:34.556, Speaker B: They are, again, based on our previous understanding of the diseases spread, which is a causal understanding. But the machine learning part is definitely not causal. So we cannot really make conclusions that, you know, the most important variable in mitigating the disease spread is restrictions on gatherings, even though this, this practice is again widely used in the literature. Like a data driven model is obtained and then immediately after the results, like sensitivity analysis or something like that is done to say, like, okay, the most influential covariate is x, but for doing, for making such rigorous conclusions, you really need to have a causal model, which is not the case here. And I'm happy to talk about that if it comes to the, in the questions, but it wasn't the scope of this work. So what did we learn from this part? Well, mechanistic models are absolutely intuitive most of the time, and they are powerful in short term predictions. Why are they powerful in short term predictions? Because they typically assume that the parameters are time invariant.
00:37:34.556 - 00:38:21.564, Speaker B: And that is a valid assumption for a short period of time. Okay. But they typically fail in predicting the mid to long range for the future. Okay. And the reason is exactly as what I said, like making assumptions on the variables such as data. Now, machine learning models are powerful in capturing non intuitive patterns, patterns that may not be easily either caught by eye or they can be easy. For example, the task of face recognition, right? It is intuitive, but writing a mechanistic model for that is very difficult, if not closely impossible.
00:38:21.564 - 00:39:43.014, Speaker B: So they are capable in capturing patterns, but they are a black box design. And that really causes the biologists, or mathematical biologists, to often question these models. And the reason is they are merely based on data. Of course there are extensions where human expert knowledge is also incorporated, but the common trend there is to if you want to get better predictions, most of the time you will just let the model to decide which variables to choose, which model to choose, which feature ordering to choose, and all of that only based on data. Now, a proper mixture of the two can definitely be the key to improving our mechanistic models. How? Well, by using the mechanistic models for those parts that the dynamics are well understood, like what we did here in the Sierra model, and then the machine learning parts for those that are still not well understood, or for the time varying parameters of the model from data. I would like to acknowledge the many colleagues that I was grateful to work with.
00:39:43.014 - 00:40:04.764, Speaker B: Professor Russ Rainer, Arjahra Tian, Professor Luis Marc, Louisa, Zainab, Maliki Aryad Naya, Mitadi Kanguna and Roberto Vega, Hal Wang, Shunan Wang and David Wishart from different universities. Thanks. That's all. And I'm happy to answer questions.
