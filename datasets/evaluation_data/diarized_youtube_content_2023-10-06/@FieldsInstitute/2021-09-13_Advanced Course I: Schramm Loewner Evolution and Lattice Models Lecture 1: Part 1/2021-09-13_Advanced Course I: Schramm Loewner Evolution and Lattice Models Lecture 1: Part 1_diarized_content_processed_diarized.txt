00:00:06.040 - 00:01:15.084, Speaker A: Hello everyone, and welcome to the first meeting of the first course, the first advanced topics course in the program on analytic function spaces of analytic functions. And the topic of the course will be SRAM Luvner, evolution and lites models. And what I will do now, I will first quickly talk about what this course will be all about. Then I will talk about some admin things, and then I will delve into more detailed introduction. Okay, so what is the subject of this course? In recent years, outstanding progress was made in rigorous understanding of various physics predictions related to critical lites models. I will explain all these terms during the course, of course. And one of the main tools in this development was Schram Levin Revolution.
00:01:15.084 - 00:02:07.170, Speaker A: This was invented by Adet Schramm in 1998. And the idea of the course is to introduce you to these developments and to introduce you to some techniques. So the topics will include, as I wrote here, definition and geometric properties of shambles evolution, and I will provide all the necessary background and geometric function theory. So for some of you, this would be absolutely trivial. Then we'll talk about lightest models, percolation ease, imports and self avoiding render work. And actually, as a part of extended introduction, we'll start with these properties. Then we'll talk about proofs of the existence of scaling limits and relations to shramlevnir evaluation.
00:02:07.170 - 00:02:49.808, Speaker A: And then we'll discuss some recent progress, such as rate of convergence of critical interfaces. I will use three main textbooks. So one of them is written by Dim Billev, conformal maps and geometry. Another is probably classics. By now it's conformal invariant processes on the plane by Greg Lowther, and finally, schramble of Nervalusian, another great book by anti campany. So if that is something non understandable in what I am saying, please consult these books. And again, I will be more than willing to discuss things now about admin things.
00:02:49.808 - 00:03:35.724, Speaker A: So this course, well, it's kind of dual. First of all, it's part of the fields program. And why am I teaching this course here? There are, if you look at the syllabus, there are no analytic function spaces. So function spaces of analytic functions. The reason is the following. As you will see, many methods and ideas which prove to be very important in the field turned out that they came exactly from your field, from spaces of analytic functions. More than that, most of the people who work there now, they have probabilistic background.
00:03:35.724 - 00:04:26.206, Speaker A: And again, my main reason for teaching this course here is to advertise this field as something which has lots of interesting open problems. Some of them are probably extremely difficult but some of them are low hanging fruits, especially for people with your background. And I hope to attract you to this field. And, well, second role of this course. Of course, it's a normal graded course at University of Toronto. And those of you who take it for credit, well, they will get a grade, and the grade will be wholly based on your final project. Somewhere in November, we'll start discussing possible topics for the project.
00:04:26.206 - 00:05:13.842, Speaker A: And again, that's a separate thing. Well, since it's part of our online program, the course will be run online. But for those of you who are in Toronto, I plan to be on campus, well, at least one day of the week, and I'll be happy to meet and discuss whatever is interesting about the course, maybe additional topics and similar things. Okay, so that was a formal admin introduction and about now format of the lectures. So there will be, this is a three hour lecture. But of course. Well, I can talk for 3 hours.
00:05:13.842 - 00:05:46.462, Speaker A: You probably cannot listen for 3 hours. I understand this. So we'll take ten minute breaks every 15 minutes. So I'll try to be punctual here. I'll stop at 1050, 1150, and finally I will stop at 1250 and I'm able to reach out. I'm connected here through two devices, so if you have any questions, ask and chat or actually shout them out. So I want to maintain an atmosphere of small topics.
00:05:46.462 - 00:06:23.406, Speaker A: Course, whatever the number of participants would be. Any questions about admin things? Okay, if there are none, let me turn to less formal transaction. Okay. One necessary disclosure before we do anything, it's zoom. So things can go wrong. Especially, as I said, I'm connected with two devices and I'm currently connected from home. So Internet can go down.
00:06:23.406 - 00:07:08.304, Speaker A: Zoom can die on one of both devices. I apologize in advance. I know that it will happen at least once during this term. Again, my apologies in advance. And since the lectures are recorded, I hope that at least some part will remain. Okay, anyways, with all this, let me move to the continue the introduction and let me do some start with some very hand waving introduction first. And I will start with something which is again very familiar to some of you and maybe to most of you.
00:07:08.304 - 00:07:44.244, Speaker A: So let's start with random walk. So what do I mean by random walk? Well, you have a light. And for me, lysis would be some subset of the plane, probably infinite regular lights on the plane. I will mostly talk about examples. So one of my favorite examples would be hexagonal lattice. So you see the faces are hexagons. And we start random walk on the edges.
00:07:44.244 - 00:08:24.436, Speaker A: Now what we do, we put this infinite lattice on the plane, and then we intersect this lattice with some fixed domain in the plane. So here the domain is very easy, it's disk. And we make lattice of the size delta. The smaller the delta, the finer you approximate the lattice. And what you start doing, you start a random walk on this lattice. And of course, I have technical difficulties at the very beginning. So you start this random walk on the lattice.
00:08:24.436 - 00:09:04.924, Speaker A: It goes somewhere around, and I run it till it hits the lights, until it hits the boundary of the domain, and then I stop it. Okay, so what I generated, now I generated a random curve. Okay? So like most of you, I'm not a probabilist. When I hear random curve, well, when I used to hear random curve, when I started doing this stuff, I would tune out and would just say, okay, well, random curve, whatever it is. So let me give you an analytic definition of random curve. Well, it's very simple. It's just a probability measure on the space of curves.
00:09:04.924 - 00:09:56.044, Speaker A: So you look at all the curves from the point w naught to the boundary, all the curves, not necessary curves on the boundary, on the boundaries of the faces. So not necessary edges of the slightest. You introduce some sort of topologies, I will actually talk, ignore them about topology on the space of the curves later on in the course. And then you introduce, just you introduce probability measure. So the probability measure, let me remind you, this is a measure which is non negative and has total mass one. Okay? So when I say random curve, you should, again, those of you who are probabilists, you can just think of random curves. But those of you who are analysts, as I expect most of you are, should just think about probability measures on space of all curves.
00:09:56.044 - 00:10:44.534, Speaker A: And, well, it's kind of obvious that it's not rotational invariant. This measures that I just introduced because, well, if you rotate by a small angle, of course the lattice will change and the curves which will in the support of our measure, well, they just wouldn't be there. So, okay, so this is not rotational, but as delta goes to zero, this law actually converges to something which is very, well, rotationally invariant. And this is the law on the trajectories of two dimensional brownian motion. Again, this is some object which I will define later. Rigorously, this is not a course for probabilities. So I will explain these things too.
00:10:44.534 - 00:11:50.898, Speaker A: And amazingly, this is rotational invariant. So, if you look at random curves which start at some point w naught inside of the dynamic, if the dynamic itself is rotational invariant, then this law is rotational variant, meaning that if I rotate the domain, the measure doesn't change more than that is true. And this is an amazing theorem of Paul levy. So, okay, since this is the first time I put a portrait of somebody out in this course, let me say something about it. So what I like to about online format as opposite to in person format, well, I dislike everything, I must admit. But one thing I like is that you can see portraits of people who did this stuff. And for me, additional bonus is that, well, lots of this stuff was done recently, so I will be able to show you portraits of some of my close friends.
00:11:50.898 - 00:12:45.240, Speaker A: And, well, I'll try not to show embarrassing portions. No promises. And so, including people who are listening, some of the people who are listening now. But anyway, Paul Pierre Levy definitely didn't know him, and he had this amazing theory that two dimensional brownian motion is conformally environmental. Again, this is a hand waving part of the course, so I will not say exactly what it means, but let me try to explain what it means in terms of our understanding of conformal invariance. So suppose that you have two simply connected domains, omega and Omega prime. And suppose that you have conformal map, which maps omega and the point w naught inside to domain omega prime.
00:12:45.240 - 00:13:32.284, Speaker A: This point w naught prime inside side, and it's conformal. Then brownian motion generates two probability measures, some curves here, one measure in domain omega, one measure in domain omega prime. Livy theorem. Well, it tells more, but from our purpose, for our purposes, it says that this measure is conformal invariant means. And when you push forward measure here, you get measure here. Okay, so that's what conformal invariance means. And this would what conformal invariance would mean for critical lightest models of statistical physics.
00:13:32.284 - 00:14:30.172, Speaker A: And one of the reasons I like to show this easy example is that one of the proofs of this, which I will actually show later in this course, is based on the following. Well, I call it observation by. But it's probably one of his most famous theorems, although, again, he proved lots of amazing theorems. This is probably the easiest result that he proved, but probably the most cited result of his. And the result is the following. Suppose that you start bran in motion at some point z, sorry, at some point w, naught inside omega. And this is so it's parameterized by t, and this capital t is the time you hit the boundary.
00:14:30.172 - 00:16:13.770, Speaker A: So the usual notation is that this is the smallest time such that your brownian motion escapes omega. And suppose that you have f, which is continuous on the boundary and e would be a solution of the corresponding Dirichlet problem. So the harmonic function which has boundary values equal to f, then what Kakutini theorem says is that value of u at the point w naught is just expected value of f at this exit point. So what happens is that you start at w naught, you run brownian motion, you hit a random point on the boundary, and you take the value of f at this point, then you average. Turns out that you have the solution of the Dirichlet problem. Okay, now, what is the restatement feature like, and hopefully you would like, is that this random point, so what is the random point? Again, it's just a measure, a probability measure on the boundary. What Kakutini theorem says that Brownian, this random point, is distributed according to harmonic measure.
00:16:13.770 - 00:17:18.000, Speaker A: So this random point is actually the harmonic measure which is conformally invariant. Again, I will define harmonic measure later, but I expect that most of the people here who come from functional spaces part would know what it is. So the reason for the invariance of the whole curve comes from invariance of just one point for an observable. Well, it's a random point, so we should talk about measures here. But it's still amazing that just from one point being invariant, you get invariance of the whole curve, and that's, that would be in the heart of what happens next. That environs of one observable and here absorbable is this exit point would lead to invariants of the whole process. When I say that val would lead, it's usually a lengthy process.
00:17:18.000 - 00:18:05.504, Speaker A: You need to prove lots of things. But there are now some axiomatic theories for this. But anyway, I promise to hand wave. Let me continue doing this. So, as I mentioned here, so this will be the recurring theme, the conformal invariance in one observer leads to conformal invariants of random, of. Okay, another technical thing about the course I will, during this lecture, I will insert a few of them, is that, of course, all these notes would be available online, together with the recording after the lecture. If you don't understand my handwriting, have questions about it, you're more than welcome to ask, either now or later in the mail.
00:18:05.504 - 00:18:57.122, Speaker A: I would be more than happy to correct it if you find mistakes here, which I hope you do. I make mistakes. Sometimes even I do that on purpose, but this is probably too advanced course, for me to do it on purpose too often. You're more than welcome to tell me, and again, I really want you to tell me that. Okay, so now let me move somewhat closer to the real topic of the course. Let me move to examples which are really related to what this course is all about. So, brownian motion and random book was kind of motivation, something you probably heard about.
00:18:57.122 - 00:19:36.566, Speaker A: And now this is the real development. And well, you know, in russian mythology we have three strong heroes. So here the three strong heroes of the field, or maybe three musketeers, whatever you like. So, Greg Lawler, Adade Schramm and van der Linde Werner. So Adet unfortunately is no longer with us. He died in hiking incident 13 years ago. Okay.
00:19:36.566 - 00:20:24.324, Speaker A: And so, well, let me talk about their work. So, the first thing I want to talk about is a model proposed by Greg Lawler. And this is called loop erased random walk. So, let me try to explain what this is. And again, I will run it on the hexagonal lattice, but it can be done on any lattice, even on the graph. Let me try to explain what it is. Instead of writing long formal definition, I will try to just show you an example.
00:20:24.324 - 00:21:01.100, Speaker A: So, you start at some point inside the domain and you start doing random walk, just normal random walk. So this probability one thought, you move in any direction, then you do the same, then you do the same, then you do the same, and then you need to move back. That's what you threw a three sided coin, and that's what it told you, to move back. Okay. When you moved back, you created a loop. And what this process does, it erases a loop. So let me raise this.
00:21:01.100 - 00:21:25.218, Speaker A: So we moved back, we erase the loop. Let's continue moving. So we are moving, hop. We created a loop. Here, the loop is created, we need and we keep going. So here, I created a longer loop and I still need to erase it. Then the moment I create a loop, I erase it.
00:21:25.218 - 00:22:07.084, Speaker A: So, I still ran a standard random walk, but the moment I see a loop. So I just. Well, I wanted to show you a real simulation, but then I thought that, well, it would be easier just to draw hope. And the moment I exited the domain, I stopped this process. So this is the same as random walk, but in the process of running it, I just erased the loop. So, amazingly, of course, the exit point is exactly the same as the exit point of random walk. So it's distributed according to harmonic measure, discrete harmonic measure in this case.
00:22:07.084 - 00:23:17.624, Speaker A: So the model, if you run it on a lattice of size delta was proposed, is called loop raised random walk delta. It was proposed by Greg Lawler. And what the Lawler Schrammen Werner proved, they proved an amazing theory that when delta goes to zero. This process converges to conformly invariant loan simple curves. Okay, so let me emphasize what convergence I talk about. Remember that this thing just generates a probability measure on space of all, in this case, simple curves going from w naught to the boundary of omega. Now, when delta goes to zero, this measures have weak limit, and this limit turns out to be conformally invariant, and it lives on simple terms.
00:23:17.624 - 00:24:21.012, Speaker A: So this is what they proved is. Well, amazingly, this was not the first theorem about convergence to this Schramlovner evolution, but in a sense, this was probably the most natural result, the first, most natural result in this direction. I will explain what I mean later. Okay, so it's conformally invariant in exactly the same sense as brownian motion, meaning that if you have two domains and you run loop raised random, walk here, loop raised random, walk here, go to the limit, conformally maps this domain to that domain, it's the same process. Again, this is much harder theorem than this theorem. Another theorem that they proved is also very interesting. This concerns uniform spanning trees.
00:24:21.012 - 00:25:14.684, Speaker A: So for uniform spanning trees, let me do a slightly different setup. So we have a domain and we have two points at the boundary a and b. And what we do here, we look at the intersection of lattice. In this case, it's square lattice, it's our domain, and we look at trees which contain all the vertices. This is called spanning tree. Now, there are many such trees. So you have lattice, you can span this lattice in many possible ways.
00:25:14.684 - 00:26:06.174, Speaker A: You take one of them, uniform electron, and then what you do, you add to this tree, this little thing, a and b, so that they are kind of part of this tree. And now you draw a curve, this black curve goes around this tree. So it tries to trace this tree. And so you see, this curve is very different from the previous one. So previous curve here, this was a simple curve. This one also looks simple. But you can imagine that when delta goes to zero, this curve would become, well, it would cover everything against a random curve.
00:26:06.174 - 00:27:21.144, Speaker A: And what they proved in the same paper, as delta converges to zero, this curve converges to confirm the invariant law, as in curves on space filling curves in omega, which go from this fixed a to fixed b. Okay, so again, this was very handwriting, I know. So I will give all the rigorous definitions later on in the course, but just wanted to show you an example of such. Now, the third example is actually chronologically. The first, it's a theorem of Stas Smirnov about critical percolation. One feature of these two results about loop raised random walk and uniform spanning trees is that they walk on absolutely any reasonable lattice. Again, I will talk about what any reasonable lattice means later on in the course.
00:27:21.144 - 00:28:26.974, Speaker A: But this proof for critical percolation, amazingly, it still works just on one lattice, on this hexagonal lattice we started with. Physicists know that it should work on any lattice, but so far, well, there were some mathematical generalization, mathematically rigorous generalization of this, but two very small class of lattices. So phi, essentially, it's hexagonal lattice plus epsilon. So what is percolation? This would be very important for this course. So let me spend some time defining it, although this is very easy to define. So what, what do I mean by that? By percolation, you just look at all the hexagonal lattice, and you look at each hexagon and you color it at random, either blue or yellow. That's it.
00:28:26.974 - 00:29:11.334, Speaker A: So, in a sense, this is the total disorder, because again, you, your colorings of hexagons are totally independent of each other. By the way, blue and yellow colors apparently are very important here. These are the colors of swedish flag. And Sweden is the country where this theorem was proven. So, I was told that this should be used every time we discuss percolation, although there are the same thing. Okay, so this is percolation. Very simple model, total chaos.
00:29:11.334 - 00:29:55.404, Speaker A: Now, let us try to do it in the domain, so that we can both set up it in a way that is similar to what we did before. So, let's take a simple, simply connected domain, omega. Let's take two boundary points, a and b. And then it will color all the things here, from a to b, yellow, all the things at the bottom, again from a to b, blue. So this is fixed. And now we color everything in the middle at random. So, there is a fancy name for this.
00:29:55.404 - 00:30:34.164, Speaker A: This is called douche and boundary conditions. And we'll encounter them a lot. So, we have two boundary points. On one side, you do something, one thing, on the other side, you do another thing. And then, just from topology, there is unique interface, which I drew here in red from point a to point b. The interface is the following. On the left side you have yellow finger, on the right side you have blue.
00:30:34.164 - 00:31:09.024, Speaker A: And then it just goes between them. Now, why is it called exploration process? Let me try to explain it to you. The reason is the following. That to draw it, you really don't have to know the configuration, since things are totally independent of each other. What you can do, you can just move on your hexagonal lattice. And when you encounter a new hexagon, like you do here. What you do is the following.
00:31:09.024 - 00:32:17.764, Speaker A: Okay, so you went here, you always have yellow on the left, blue on the right, and then you encounter an undiscovered hexagon, the hexagon which you don't yet know how to color. What you do, you simply toss a coin, and if the hexagon is blue, you turn left, right, because you need to be blue on the right, yellow on the left. If the hexagon is yellow, it on the right. That's all. So that's why it's called explorer process, again, because exploration process, because essentially you just explore the configuration without fully revealing. This is actually important for applications, because, well, again, explorer percolation is, has lots of physics applications, which I try not to talk about too much. So what you do, you create a random curve from point a to point b, which we called perc delta.
00:32:17.764 - 00:33:11.152, Speaker A: Again, as you see, it's easy to define on hexagonal lattice, but the same process can be defined on any reasonable lattice. Nothing is so special about. And then, this was the first theorem on convergence to sram l evolution, the theorem of Stasi Mernow. As I mentioned, the theorem is the, as delta goes to zero, ercalation of delta converges to a conformally invariant law. But it leaves some very interesting curves. The curves that you get in the end are not simple. They are not space feelings, they are self touching.
00:33:11.152 - 00:33:56.976, Speaker A: So what I mean by this is that you are allowed to, the curve is allowed more than that. It does touch itself always, but it's not allowed to cross itself. So this is not allowed. Okay, so these were three examples, and the reason I was talking about them, that they illustrate three phases of valve framley of revolution, the thing which is the topic of the course. So let me try to quickly explain again, handwriting. I'll do it in much more detail. I will spend probably a few weeks rigorously defining this.
00:33:56.976 - 00:35:07.458, Speaker A: But let me try to now explain how to describe this limiting process. So how do we describe this limiting process? So we have the three examples which converge to is a simple curves, space filling curves, or self touching curves, and it's supposed to be conformity invariant. So you should be able to describe it at one domain and then push it to the any other domain. So you pick economical domain. And for a case like example one, when we talk for a curve from interior point w, naught to the boundary, the logical domain, the natural domain, is unit disk. And I always denote unit disk by this blackboard d and volcano point is of course zero. So what we will do we will eventually define this random curves going from zero to the boundary in the disk.
00:35:07.458 - 00:36:01.824, Speaker A: And then the measure in any other domain would be just the image for the examples like two and three, where you have curves between two boundary points or two boundary primaries. The natural domain, of course, is upper half plane. Again, blackboard bolt h would mean upper half plane, and your curve would go from zero to infinity. Now, another important feature of all this process, it's a random curve. Usually you define curve by parameterization, but here parameterization shouldn't matter at all. I am interested in the curve in the trajectory. I am not interested in how we parameterize it.
00:36:01.824 - 00:36:53.454, Speaker A: And that actually will turn out to be very important because there will be one good canonical parameterization. And, well, we'll use conformal map to describe the scars, right? We want to describe something conformally invariant. So why don't use conformal math? So what we do is the following. We run our curve up to time t gains. At this moment, I am not fixing parameterization of curve. I'm just saying, okay, some parameterization. We run it up to time t, and then we unzip the curve.
00:36:53.454 - 00:37:56.644, Speaker A: So what we do, we map from this domain, complement of this curve in the upper half plane back to upper half plane, but we just not just do it, we do it in this hydrodynamic normalization just to fix one conformal map. And this map, well, what is hydrodynamic normalization at infinity, it behaves like z, and the second term is constant over z. And it's easy to see that this constant a of t would go to infinity as t goes to infinity. That's real. So, well, and you start, well, when t is equal to zero, you start at zero. So a of zero is equal to zero. So you have increasing function going from zero to infinity.
00:37:56.644 - 00:38:17.004, Speaker A: You can repairametrize it. You can repairametrize the curve. So again, the parameterization didn't matter. So now we will fix the parameterization. We'll take the parameterization size. That parameterization that a of t is equal to t, again, doesn't look too natural now, but it will. I promise you, it will be natural.
00:38:17.004 - 00:39:18.894, Speaker A: And then lambda of t would be the image of the tip and the Gt. Again, it's somewhat non trivial that this image exists. This is Carcelo Ethereum. Remember, conformal map, originally just defined in the domain, but you can actually extend it to the boundary by carthedo thermic. Okay, and then the chordal form of Leo equation, this is, this was invented by Charles Leevener is this that time derivative of this conformal map Gt is actually equal to gt two divided by gt of z minus lambda of t. Again, we will derive this equation. So this is always true.
00:39:18.894 - 00:40:18.624, Speaker A: If you have a curve and you run this process, then the map Gt would satisfy this odor. This equation itself, even without being s l revolution has an amazing history. So it was invented in 1923 by Charles Leonard to solve Bieber Bach conjecture. I realize I don't have a picture of Bieber Bach. Okay, I will try to correct it later. But anyway, so what is Bberbach conjecture? Biber Bach conjecture is this statement that if you have univalent function in d, which is normalized, so f of zero is equal to zero. If prime of zero is equal to one, this would be an extremely important class for us in about two weeks.
00:40:18.624 - 00:41:34.816, Speaker A: And so you look at such univalent functions in the union disk, and then Bieber Bach conjecture states that absolute value of f is less or equal than n. The conjecture is from 1910. And this whole machinery of Ljovnir equation was invented to solve it. And actually, it helped solve it for n equal to three back then. Fast forward to 1994, 61 years later, Bieberbach contraction was solved by Louis de branch using a few other tricks in his head, not all of which I understood, but this was luyush. And so, even before introduction of Schramlovner evaluation, this was one of the great technical tool, actually in geometric function theory. Well, this is at least very close to function spaces of analytic functions.
00:41:34.816 - 00:42:32.726, Speaker A: And again, I hope most of the people who are here from the program, they heard this amazing story. There are quite a few details there about how the branch came to this proof and all this, but let me skip it for now. Now, one remark here, you can kind of restore the curve gamma of t from lambda. So if I give you this equation and I give you a continuous function lambda, of t, then you can restore the curve. So this equation, you just solve this equation, you can always do it, and it will always produce you, not always a curve. So let me show you what it can produce. So one of the things it can produce is, of course, valve.
00:42:32.726 - 00:42:58.628, Speaker A: That's simple curve. But it can also produce something like this self touching curve. It's okay. It's actually part of the picture. Remember, some of our conjectures, some of our theorems dealt exactly with this curves. It can produce space filling curves. I don't know how to draw a space filling curve.
00:42:58.628 - 00:43:37.524, Speaker A: So something specific. But unfortunately, unfortunately for some, very nice continuous lambda t, not differentiable, but even older lambda, you can produce something like this. You can produce things which are not curves. So let me try to describe this outstanding set. So I drew a little circle here. Then I have thing which spirals on this, on the circle. Then I have thing which spirals out.
00:43:37.524 - 00:44:26.220, Speaker A: So would I be able to get it out? Yes. So this object is also described by the Slovner equation for some continuous lambda. And actually I will prove it for you later in the course. This is theorem of Charles Pomerenke. Okay, so, but again, things are usually, most of the time, if you have a nice curve lambda, you would produce something nice here. And then there was this great observation by Adet Schramm. So what he did was the following.
00:44:26.220 - 00:45:09.394, Speaker A: Assume that gamma is a random curve. So again, random curve probability measure on space of curves which satisfies two conditions which again were conjectured in physics for critical lightest models like the ones we discussed. First is conformal invariance. So, just as we described. So it's supposed to be defined in every domain. And when you make domains between themselves, the measure is invariant. But the second is the main Markov property.
00:45:09.394 - 00:46:20.044, Speaker A: That's the following. Suppose that you run your curve up to some time t capital, and you look at this domain, and then you can run your random curve in this domain, right? So it's new domain. So domain Markov property tells you the following, that when you run this conformal invariant curve in the domain omega without the beginning of the curve, it's supposed to be the same law as if you would continue your curve gamma in the original domain. So this is very easy to see for actually exploration process for calculation. So just let me try to explain what it is. Suppose that you stop here somewhere. So you run your exploration process and you run it up to some point here.
00:46:20.044 - 00:47:08.284, Speaker A: Then what would happen next is the same. It doesn't. Well, percolation didn't know anything about the domain, right? You just threw your coins, tossed your coin for each side. So of course what would happen next is the same whether you continue in the original domain or do it in the domain where you cut some part. If you think about it, this is actually not easy to prove. For this to raise random work. Again, not that it's anything complicated, but it's a non trivial state.
00:47:08.284 - 00:48:16.064, Speaker A: Anyway, let me return to this. So suppose that you have these two conditions. Then, as he proved, gamma is generated by a very special lambda. So your function, which is used in your OD, the image of the tip of the curve, it should be random, right? It's a random curve. So the function also should be random, and this function should be brownian motion. Run with some speed, kappa and kappa can be valve, any positive speed, and amazingly, for different kappa, you would produce totally different objects. And, well, this object is now called shramlevna revolution.
00:48:16.064 - 00:49:10.914, Speaker A: Of course, Adet never called it. He called it stache ljovna revolution, the same abbreviation, sli. And he would strongly object when in his presence or not in his presence, people would call it. Fortunately, he's no longer with us, so he's no longer objecting. And what the theorem I mentioned before all about is that actually loop raised random walk on any lattice, can we register SLa two? Percolation, can we register SLa six? And uniform spinning three converges to sle eight. Now, remember that there were some differences there. So, loop raised random walk is naturally defined from a point inside the domain to the boundary.
00:49:10.914 - 00:50:21.344, Speaker A: This corresponds to slightly different version of Leonard evolution, which is called radial evaluation. So this would converge to radial evaluation. These two would converge to cordal, the honor violations. And let me mention first, showing a picture of Stefan Rode, who is one of the co authors of this theorem of Rod and Shram. So, what, at that short, is that if you have this conformity invariant law, it should be driven by brownian motion. But does it exist? Suppose that you run brownian motion, you plug it in the equation, and then does it have a solution? It does. And this is theorem of rodeshram, which I will discuss after a ten minute break.
00:50:21.344 - 00:50:24.084, Speaker A: So I will stop the recording. Now.
