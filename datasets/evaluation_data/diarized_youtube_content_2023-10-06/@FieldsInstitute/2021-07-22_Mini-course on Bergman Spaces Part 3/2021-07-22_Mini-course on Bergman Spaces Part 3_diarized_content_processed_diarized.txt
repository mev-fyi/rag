00:00:00.200 - 00:00:07.354, Speaker A: Asked of the Bergman space talks by Stefan Richter at the University of Tennessee. Stefan?
00:00:09.174 - 00:00:36.726, Speaker B: All right, good morning, everybody. So, let me start by trying to share my slides. I made a beginner's mistake this time. I had some. I remembered something early this morning that I had actually wanted to put into the, the slides, into the talk. And so I made some changes. And so, as a result, well, first of all, I probably won't be able to finish all my slides.
00:00:36.726 - 00:01:05.494, Speaker B: That's okay. I'll just stop when the time's up. But hopefully I don't have too many typos in there. And I need you guys to keep me honest and make sure that everything I say is still true. All right, so let's. Today we want to talk about Bergmann inner functions, and let's take a look back at the Hardy spaces. So if p is between one and infinity and we have an invariant subs, we always just worry about the boundary? Well, not always, but we do worry about the boundary space situation mostly.
00:01:05.494 - 00:01:47.232, Speaker B: If we take an invariant subspace in the Hardy class HP, then it's of the type phi times hp for some inner function phi. All right, so how does the proof of this go? So, first of all, you look at Berling's paper. He does a proof for p equals two. You look at this m minus c m. So m is your invariant subspace. Yesterday, I talked about that m minus c m is not equal to zero when m is not zero. And so we take a function of norm one in there, and then we use the fact that when n is positive z to the n, phi is in, in z m, and so it's orthogonal to phi.
00:01:47.232 - 00:02:16.266, Speaker B: And so that gives us zero. And when I write out in terms of that l two integral, then I see that the integral of z to the n phi squared is zero. And so that means the Fourier coefficients of phi squared are zero for all n. I can take complex conjugates of this. It's still zero, and except for n equals zero. And that means that phi squared has to be one almost everywhere. So phi is a classical inner function.
00:02:16.266 - 00:03:07.182, Speaker B: And then once you know that, you finish the proof and show that m is equal to phi times h two. And then in Berling's paper, he says a similar argument shows the result for other values of p. Now, I'm not going to argue with what Berling meant by that, but what I would do if, if I were to prove this for other values of p is, let's say we take p equals one, then the spaces get smaller as the index increases. So I take an invariant subspace in h one. I intersect it with h two, and now I have an invariant that turns out to be close in h two, and it's invariant. And so I have it that's equal to five times h two. Now I have my inner function, and then I work and show that that actually works.
00:03:07.182 - 00:03:41.318, Speaker B: And so that proves it's nice because it works. And, you know, I don't know whether that's what Boerling had in mind. It doesn't look like a similar proof to me. It just looks like we're using the Hilbert space situation. Okay, so now let's go to the Bergmann space. So if we are in p two again, it's perfectly natural now to look at m minus cm and take a unit vector in there. And as before, we get now that the integral z to the n phi squared da is equal to zero for all positive values of n.
00:03:41.318 - 00:04:25.006, Speaker B: So that looks sort of very similar to what we had before. But now if we're in l one a, then we know there are l 10 sets, l one a, zero sets that are not zero sets for l two a. So when I look at the zero based invariant subspace inside of l one a, then that's non zero. But when I take the intersection with l two a, then I get zero. So right off the bat, as I go from p equals two to other values of p, what I did there before, which worked kind of nicely, is never going to work. Well, I let that not deter me much. I still use this as my definition for an LPA in a function.
00:04:25.006 - 00:05:36.686, Speaker B: And we'll see in a while that is a good definition. So a function in LPA is called lpainer if it has norm one, and if its integral with z to the n and phi to the p is equal to zero for all n bigger than zero. So of course I can take complex conjugates of this, and I also get zero, but I don't get zero when I take something of the type integral z bar to the k, z to the nice times phi to the p, or at least I don't know. And also, while I did know that for p equals two, for every invariant subspace, there's going to be lots of these, or at least for every invariant subspace, there's at least one such function over here. I don't know anything about the existence. So when p is, say, three over two, and I take an invariant subspace with, with lots of zeros, well, I don't know that I can take two over pth roots, and I don't know that any such functions will exit or many such functions exist. Of course, if I have a function over here in the two situation that has no zeros, then I can take some sort of a pth root or something, and then I get a function that satisfies this.
00:05:36.686 - 00:06:17.730, Speaker B: So I know there are some of these, but I don't know that there are a lot of them. Okay, so for that I consider an extremal problem. So I take an invariant subspace in Lp, and now there's always a little tedious extra argument that one has to do if all the functions in the invariant subspace have a zero at the origin. So I will, in my proofs, I will always assume that that's not the case. So there's always a function in the invariant subspace that's not zero at the origin. Okay, now I consider this extreme old problem. I'm trying to maximize the value of the absolute value of f of zero, given that the norm of f is equal to one.
00:06:17.730 - 00:07:14.926, Speaker B: Or look at this ratio here, and that's obviously the same as the infemum of its reciprocal. So the supremum of this ratio here is the same as when I take one over the infemum of the reciprocal. And so I have two extremum problem here, a supremum problem and an infemum problem, but they're really the same problems. A function will solve this first problem if the function solves the second problem, because that's the exact same problem, except just the reciprocals. Okay, well, now this has the problem that anything that solves this, then a multiple of it will also solve it. So I want to do a normalization, and so I do the normalization. So I take now functions of norm one, and I maximize the real part of these functions.
00:07:14.926 - 00:08:15.580, Speaker B: And then that's the same as taking the one over the infimum of the norms, given that the value at zero should be one. Okay, so then I can say that either both of these problems have a solution or neither, because, you know, when I get one solution, I get the other. And now, because I did different normalizations here, either the solution to both problems exists and is unique, or that's the case for neither of the two problems. So if I have a unique solution to the soup problem, I have a unique solution to the inf problem. And if I have a unique solution to the inf problem, I have a unique solution to the soup problem. And so any solution to the soup problem is what I'll now call an extremal function for m. All right, so now I've written down the inf extrema problem here, because with this one, it's very nice to see that the set I'm taking the inf over h and m h of zero is equal to one.
00:08:15.580 - 00:09:08.234, Speaker B: That's a convex set. And so in a strictly convex reflexive banner space, convex sets have a unique function where the infimum is attained. So for p between one and infinity, extremal functions exist and are unique. If p is equal to one, then l one a is still strictly convex, so uniqueness still holds. But apparently existence for general invariant subspaces is an open question. And for p between zero and one, I don't know at all in all generality. But fortunately, even in these exceptional cases here for the invariant subspaces that we might be interested, in most cases, extreme malfunctions exist and uniqueness is known.
00:09:08.234 - 00:09:56.384, Speaker B: So when m is cyclically generated invariant subspace, the smallest invariant subspace that contains the function f, or a zero based invariant subspace. So for example, let me show the existence of the zero based invariant subspaces. So let's say we want to achieve the infemorm here. So we take a sequence of functions hn, which is one at the origin and is an m, and their norms converge to the infimum. Then the ball in the spaces is a normal family. So there'll be a locally uniformly convergent subsequence that converges to some analytic function. And then by Fatu's lemma, that function will be lp integrable.
00:09:56.384 - 00:11:04.662, Speaker B: And because it's a limit of functions that have all these zeros at the point lambda n, since locally uniform limit, the limit will also have those zeros. So the limit function is in LPA and it has the right zeros, and so it's in I of lambda. So the existence really doesn't depend on pie for these zero based invariant subspaces. Okay, so, and it's now an exercise to prove that for classical inner functions in the hp situation, if you know the invariant subspace is phi times hp, then the phi is the unique solution, the extremal function for the invariant subspace. All right, and in the Bergmann space, one can show that if you start out with an LPA invariant subspace and you have an extremal function, then it's an LPA inner function. So that means, remember the integral z to the n phi to the p is zero for n bigger than zero. If p is equal to two, then functions to this extremal problem.
00:11:04.662 - 00:11:42.464, Speaker B: Solutions to this extremal problem have to be in m minus z m. Even when this space is infinite dimensional, solutions to the extremal problem are going to be in there. And I just showed you that there's a unique solution, so there's lots of solutions. Perhaps in here, if the index is infinite, but only one will solve the extrema problem. All right, so proof for p equals two is. So, for example, I take a function in m. It's one at that point, and then I write it as a function in m minus z, m plus a function in zm.
00:11:42.464 - 00:12:34.712, Speaker B: And then I look at the pythagorean theorem that tells me that the norm of f squared is greater than or equal to the norm of the term in m minus zm. And in this case, of course, when it's like this, the function zm is zero at the origin. So f of zero and g of zero take the same value, and so the f has a larger norm. So when I'm looking for the infemum of the norms over the functions that have absolute value, one that or a one at that point, then the g is a better candidate than the f. And so extremal functions will have to be an m minus cm. If p is not equal to two, then one uses a little variational argument, and that starts out as follows. So you look, you take a complex number, a, that's less than one.
00:12:34.712 - 00:13:02.974, Speaker B: So one plus a times z to the n quantity, absolute value to the p. I can write as one plus a z to the n to the p over two quantity squared. And this is analytic. So I can, and the binomial series converges. So, I write down the first couple of terms of the binomial series. Now use the the absolute value squared, and write that out. So I get one plus p times the real part of a z to the n plus big o of absolute a squared.
00:13:02.974 - 00:13:40.454, Speaker B: And then I go back to what I'm trying to prove here. So, I'm trying to prove that if I have a solution to the extremal problem, it's lpa inner. And I ran out of space on this slide. But here I have a function, one plus a times z to the n times phi, that takes the same value as phi at the origin, and phi minimizes the value at the origin. So this expression is positive. And now I write down the integrals for this, and I use this formula. And, you know, I play around with the, with the a, and I get that the five will satisfy the LPA in air condition.
00:13:40.454 - 00:14:00.936, Speaker B: So that's a fairly simple variational argument there. All right. So that means that now every invariant subspace contains some LPA inner function. If I have a solution. So in all pay cases p between one and infinity, I have that. All right. Now you can calculate some of these.
00:14:00.936 - 00:14:52.852, Speaker B: So, if p is between zero and infinity, and we look at the invariant subspace of all functions that vanish at one point, then Ozipenkyo and Stessen and Duhrhen, Kavinson, Shapiro and Sandberg have calculated what these functions are. So they're not very simple, but they're also not very complicated. So we're just looking at one at 10 here. So there's a zero at the point lambda, the blaschke factor here, lambda minus z over one minus lambda bar z. So that's zero at z equals lambda, like that function should be. And then I guess the only thing I want you to take away here is that this extra factor is never equal to zero on the disk, because one minus lambda times this expression will have positive real part. And so one plus something with positive real part has positive real part, so that's never equal to zero.
00:14:52.852 - 00:15:43.828, Speaker B: And then the constant here is something that depends on lambda and on p. And anyway, so for p is equal to two, it turns out these functions are always, you see that two over p there cancels out, and they are always just rational functions. And in fact, they are linear combinations of reproducing kernels. And even if you take invariant subspaces of the type where you take finitely many zeros, and you look at all the functions that are zero at those finitely many zeros, then the extremal functions will be linear combinations of reproducing kernels and the constant function one. And so there'll be analytic in the neighborhood of the disk. So you have some smoothness, at least for those functions with finitely many zeros. Now, Sandberg shows that that's also true for other values of p.
00:15:43.828 - 00:16:52.280, Speaker B: So for any value of p, if you take a finite subset of the disk and you calculate the unique extremal function for this invariant subspace, then it's analytic in the neighborhood of the closed unit disk. All right? And now here's the first big theorem about the extremal functions for some of the invariant subspaces. So, if you take a invariant subspaces of LPA, then either if the invariant subspace is a zero based invariant subspace, or it's cyclically generated by some LPA inner function, then it turns out that you can divide out the LPA inner function f over phi for any function f in the invariant subspace m. If f is in m, then f over phi is in LPA, and it satisfies this contractive inequality. And perhaps easier to, to visualize what's going on is that when you take the functions in M, you multiply them by phi, then you stay in LPA. There's also this inclusion here that hp is always contained in m over phi. I'll show that to you in a minute.
00:16:52.280 - 00:17:52.524, Speaker B: That's actually somewhat trivial. So anyway, so you have this nice inclusion that if you take a invariant subspace of one of these types here, then it lies between Hp and LPA. Let's contrast that with the situation in Hp and the Dirichlet space. So when we're in Hp, of course, when we take the L invariant subspace of phi over Hp, we take this ratio here, we always get equality, and we have an isometric division property there. So there are isometric divisors, the functions phi. If you take the Dirichlet space, then it turned out that if you take these Dirgeley inner functions, so to speak, they're contractive multipliers, and you get this inclusion of spaces that the Dirichlet space is contained in m over phi and is contained in h two. So the inequalities and inclusions are just the opposite to what they are in the Bergmann situation.
00:17:52.524 - 00:18:45.674, Speaker B: All right. So, as a corollary, let me prove, or indicate the proofs of a few things. Now, so these LPA inner functions are actually characterized by various different conditions. So the following four conditions are equivalent. Again, I'm assuming 50 is positive, and that the LPA, that the function is LPA inner, it's an extremal function for the invariant subspace it generates. It satisfies this contractive divisor property for all functions in this bracket phi. Or it satisfies this contractive multiplier property that it multiplies hp into Lp, so that the Bergmann Lp norm of phi times f is less than the hp norm of f.
00:18:45.674 - 00:19:21.736, Speaker B: Or you could say, another way of saying is, if you think of this as an integral, is that phi to the p DA is a traditional Carlasson measure for the Hardy space. All right, so the implication one implies three. That was the previous theorem that LPA inner function have this contractive divisor property. So I'm going to say something about that in a minute. Let me prove that three implies two. So for any function, absolute value phi of zero is less than or equal to its norm. And the norm was equal to one is what we assumed.
00:19:21.736 - 00:20:07.294, Speaker B: If we now take a function f in the bracket phi of norm one, then it's when I divide two by five. So, I'm assuming part three here that I am in the space, and so that satisfies that its value at zero is less than its norm. Then I'm assuming three. I have the contractive property, and I'm assuming that the norm is equal to one. So I get f of zero is less than or equal to one, and so f is less than or equal to phi of zero. So again, the value at phi of zero is larger than the value of f at zero. So phi is a better candidate for the extremo problem to maximize the value at zero among the functions of norm one.
00:20:07.294 - 00:20:48.566, Speaker B: So three implies two, and then two implies one. Was the earlier theorem already? And so, let's connect four to this. So, we want to look at f times phi in the p integral. So, here's f to the p times phi to the p. F to the p is subharmonic. It's smaller than its least harmonic majorant, which is the Poisson integral of f to the p. All right, well, now we use what I mentioned earlier, that the sweep of this measure f to the p da is actually the integral against Lebec measure.
00:20:48.566 - 00:21:26.926, Speaker B: Or maybe I forgot to mention that earlier. But in any case, that's the property that it's an LPA in a function that when you integrate against harmonic functions, you get the same as if you integrate the back measure against the boundary values of that harmonic function. And so that's f to the p. And so that's the hp norm of f. So you get the inequality that I have here. And then the implication four implies one is another variation, a variational argument with this one plus a times z to the n to the p, just like what I did earlier. So I'll skip that.
00:21:26.926 - 00:22:32.970, Speaker B: So that's nice. So, we have four conditions that are equivalent to being lp and f functions. The proof. So I didn't say earlier, this theorem was proven by Heidenmam in the case p equals two, and then by Durin, Havinson, Shapiro, and Sandberg for the other cases of p. And what they did is a beautiful connection to the harmonic or biharmonic function in the disk. I don't have enough space here or time to do all the details, but what's important there is that when you look at the function phi to the p minus one, then the fact that it's Bergman in means that it annihilates all the continuous harmonic functions in the disk that are continuous up to the boundary. And then if you know that, then it turns out that phi to the p minus one is the Laplacian of some function u that is positive in the disk and zero and has normal derivative zero on the boundary.
00:22:32.970 - 00:23:36.080, Speaker B: And the way that works is. So you would start out, you would say, well, so we start out with phi to the p minus one, and then we take the green potential of this function and then we get a function u whose laplacian is equal to phi and the u is equal to zero on the boundary. And now we use the fact that phi to the p minus one is orthogonal to the harmonic functions. And that tells us then, after some calculations, that the normal derivative of the u will also have to be zero on the boundary. And then once you have u and its normal derivative are both zero on the boundary, then you know that u is equal to the bilaplasia. The, the biharmonic function integrated against the bilaplacian of u and the bilaplasian of u is the same as phi to the p. And so I get this form, u is the integral of the biharmonic function against the Laplacian of phi to the p.
00:23:36.080 - 00:24:05.306, Speaker B: I know that went a little quick here, but it turns out the biharmonic function is known and is positive in the disc. Phi to the p is subharmonic. So the Laplacian is positive. So the u is positive and it satisfies its zero and its partial derivative of zero on the boundary. Now of course, im assuming its smooth on the boundary. So, you know, in the case when its not, one has to do a little bit of extra work. But I did tell you that for, at least for finite zero sets, that is the case.
00:24:05.306 - 00:24:34.342, Speaker B: And now if I look at, I want to first prove that q times phi has larger norm than q for a polynomial q. So I look at the lp norm of q, phi to the p, and subtract the lp norm of q. So I write that down as integrals. I factor out the q to the p. Now phi to the p minus one is Laplacian of u. Now, I use Green's theorem. I can put the Laplacian on the other side.
00:24:34.342 - 00:25:13.910, Speaker B: So I have u times Laplacian of q to the p. And then I get boundary terms, but the boundary terms are zero because of the properties of u. So I get this expression, u is positive Laplacian of the subharmonic function is positive again. And so the expression is positive. And so, multiplication by phi expands the norm. And that tells me that, for example, if I apply that to functions f in bracket phi, so say, for example, f of polynomial q times phi, then here the phi cancels out, and I get q times phi over here, that's just the expansive multiplier property. So I get it for dense set in bracket phi.
00:25:13.910 - 00:26:28.268, Speaker B: And so I get this property by taking limits for all functions in bracket phi. And then to do it for the zero based invariant subspaces, it's a little extra work, but it can be done. All right, so it's a good theorem. So, I stated here for the third time, I wanted to now say what Alexandra Alleman, Carl Sandberg and I, what we proved to improve this theorem a little bit by extending their range of invariant subspaces that it applies to. And in fact, what we showed is that if you take any invariant subspace with LPA inner function or extremal function phi, and if it's either a zero based invariant subspace, or if it's a cyclic subspace, and now not just generated by the phi, but generated by any function, then it's in fact generated by the, by the extremal function. So, before, in the theorem up there, we just had only for the extremal functions, you could take this invariant subspace. And so that's an important step that, in fact, it works for all of these type of invariant subspaces.
00:26:28.268 - 00:27:24.544, Speaker B: So for all of the cyclic invariant subspaces, or zero based subspaces, they're generated by their extremal function. If p is equal to two, we actually know that the analogous statement holds for invariant subspace of index bigger than one. But in the other cases, where p is not two, we have no clue how to even start to prove something similar. And in this talk, I won't pursue any of this. All right, now, as an analog, as a corollary to this statement, we get an inner outer factorization for functions in LPA. So what I can do here is I can start out with a function in LPA, and I can look at its the, I can look at bracket g, and I look at the extremal function for that, that bracket g. And then by the previous theorem, I know that bracket g equals bracket phi.
00:27:24.544 - 00:28:16.634, Speaker B: And by the hindrance theorem, or by the jorn, Havinson, Shapiro and Sandberg theorem, I know the contractive divisor property. Now, for all the functions in there, because these brackets spaces are the same. And so when I, in particular, when I take the function f to be the function g that's in here. Then that's in the space. And this function h g over phi is in LPA, and it satisfies that its norm is smaller than that. And it is easy to check that this inequality, the fourth inequality that I have here, is satisfied just by this inequality and the function phi. Of course, phi times h is equal to g, so, satisfied, two satisfied.
00:28:16.634 - 00:28:52.390, Speaker B: And let me show you that three is satisfied also. So, since phi is in bracket g, there exists a sequence of polynomials such that q and g converges to phi. And then when I look at qnh minus one in the p norm, I multiply and divide by phi. So I get g is equal to phi times h. So I get this expression here, and I have the contractive divisor property, and I know that Qnf converges to phi, so that goes to zero. So the Qnh converge to one. And that means that h will have to be a cyclic function.
00:28:52.390 - 00:29:44.894, Speaker B: The invariant subspace generated by h is the whole thing. All right, so that looks like it'd be nice theorem already, if I only have the first three conditions. And the fourth condition is a little technical, but the reason I have that is because the fourth condition determines the phi and the h uniquely up to a constant factor of modulus one, while Borito and Hindman proved that when you just have the first three conditions, then they're not uniquely satisfied. In fact, there are Lp inner functions, no LPA outer functions, that have a LPA inner factorization or the other. I forget. Okay, so anyway, so it's not uniquely satisfied. All right, now, the proof of our theorem here, in the case when.
00:29:44.894 - 00:30:18.014, Speaker B: When p is not equal to two is again, actually Hilbert space proof. So what we do is we look at. So we prove this for p equals two at first. And like I said here, we have a better result. And then for p not equal to two, we take the extremal function phi and we look at phi to the p, and then we look at the l two space with measure phi to the p DA. And then we do a Hilbert space proof. And then we're somehow, with all the structure that we already have in the Lp, we're able to go back to the Lp situation.
00:30:18.014 - 00:30:47.494, Speaker B: So everybody always tries to use the Hilbert space approach first. And it turned out that Shimoran has a very simple approach to Hilbert space situation. And so I want to explain that now. So, in fact, that works in somewhat more generality. So let me review. So I know there's going to be a mini course on reproducing kernel Hilbert spaces. But I need a few of those facts already.
00:30:47.494 - 00:31:22.342, Speaker B: So let's assume that h is a Hilbert function space on some set x. By that I mean all the functions in h are functions on x and and point evaluations. At each point is a continuous linear functional. Then I can represent evaluation at each point by a function in the space by the Reese representation theorem for the Hilbert space. And that's when I evaluate that function at each point at k, sub z of w. That's the reproducing kernel for the space. All right, here's another definition.
00:31:22.342 - 00:32:21.882, Speaker B: If you take a set and the function of two variables on x cross x complex valued is called positive definite. If whenever you look at this expression, you get something positive. So you take arbitrary collection of points in the set and you look at Uzi and uzj, and you multiply this with some arbitrarily chosen complex numbers, a one through an, and get something positive. So if you look at it carefully, if you form the n by n matrix with entries u zi of zj, then what this condition is saying that this matrix is positive semi definite. So for every n, the matrix that you get by evaluating at all these points is a positive semi definite matrix. That's a positive definite curl. So the important example is that if you take a reproducing kernel Hilbert space, its reproducing kernel is positive definite.
00:32:21.882 - 00:32:50.464, Speaker B: Well, it's obvious that linear combination, finite linear combinations of reproducing kernels have positive norm. And if you write out what that means is it's the inner product with itself. So I can take the sums out finite sums summation AI aj bar inner product of Kzi with kzj. And that's just Kzi evaluated at KzJ. So that's the u of z I z j. It's exactly this expression. So, reproducing kernels are always positive definite.
00:32:50.464 - 00:33:40.656, Speaker B: And it's a theorem of Moore and Aaron John that says the converse is true. Also that if you have a positive definite function, then there's a unique Helbert function space with reproducing kernel U. And I'm actually not going to use that, but remembering that is sometimes helpful. So for example, if you take a reproducing kernel for a reproducing kernel Hilbert space, you take the square of kz at w you. That's the inner product of kz with kw, the evaluation quantity squared. I use the Cauchy Schwarz inequality, and then I evaluate again norm Kz squared is Kz in a product with Kz, and same for W. So I get this inequality for every reproducing kernel, but every reproducing, every positive definite function is a reproducing kernel.
00:33:40.656 - 00:34:25.714, Speaker B: So this inequality also holds for positive definite functions. And I don't need to know what these norms are in our products are in the middle. All right. One other thing about reproducing kernel Hilbert spaces is that finite linear combinations of reproducing kernels are dense in the space. And that's easy to see, because if you had a function that was orthogonal to all these finite linear combinations, then it has to be orthogonal to each reproducing kernel, and so it would have to be zero at each point, and so it has to be the zero function. And so finite linear combinations of reproducing kernels are dense. And one more thing, if you take a subspace of a reproducing kernel Hilbert space, then that subspace is another reproducing kernel Hilbert space.
00:34:25.714 - 00:34:48.669, Speaker B: It's a Hilbert space and it's a Hilbert function space. And so evaluations are going. So it has a reproducing kernel. And the reproducing kernel is given by the projection of the original reproducing kernel onto the subspace. And it's easy to check. The projection, of course, is in the space. So for each z, PMKz is a function in the space.
00:34:48.669 - 00:35:08.244, Speaker B: And when you take any function in m, then you take the inner product with f. With PmKz, then, because the f is in m, you don't need the projection here. So you evaluate. So. So that's the reproducing kernel for the subspace. All right. And now here's Sir Moran's absolutely amazing theorem.
00:35:08.244 - 00:35:59.794, Speaker B: Or it's a version of it. The version that I stated here is not the full generality, but it's a version that's best for my presentation. So, let's take a arbitrary reproducing kernel Hilbert space on the unit disk that actually consists of holomorphic functions. And suppose that multiplication by z is a bounded operator that z times the space is closed in z. So that's going to be equivalent to saying that z is bounded below, multiplication by z is bounded below. And I assume that I have this index one property here that the dimension of m minus k minus zk is one. So, for example, if I take an index one invariant subspace inside of the Bergmann space, then taking that to be equal to the k, this will be satisfied because we know the closeness and boundedness and all that.
00:35:59.794 - 00:37:07.030, Speaker B: All right, so then we take the corresponding inner function, the extremal function, for that invariant subspace, as norm one. And then the theorem says that we can know the form of this reproducing kernel by saying it's of the type, the phi, this inner function here minus some positive definite u times z bar w divided by the term that shows up in the Bergman kernel if and only if this curious inequality on the vectors in the space is satisfied. Now, it's been 20 years since Shimon proved this theorem, and I know how to prove the equivalence of these conditions to go from one to the other. But how to come up with either one of them, I still have no idea. So, I mean, even if you say, well, let's take the Bergmann space, in fact, let's do that. So if I take k to be the Bergman's base, then this extremal function for the whole Bergman space is just the constant function one, the Bergman kernel. Of course, we know what it is.
00:37:07.030 - 00:37:59.226, Speaker B: It's one over one minus cw z bar w quantity squared. And so I should take the u to be zero. And so the Bergmann kernel is of this type with a positive definite function u, which is identically equal to zero. And so that means the functions in the Bergmann space satisfy this inequality here. Of course, you can verify that by more elementary ways also, because, you know, you have a power series and so on, but it's not that obvious, actually, to verify that. Okay, so let's assume this for a while, and let's prove the theorem of that I talked about earlier. So, first of all, I just said that if we take an index, one invariant subspace of the Bergmann space, then these first three hypotheses are satisfied.
00:37:59.226 - 00:38:42.854, Speaker B: We've just proven that every vector's f and g in the Bergmann space satisfy this inequality, because the Holbergman kernel has this property. So, for any invariant subspace for all f and g in the invariant subspace, that inequality will be satisfied. So that means the reproducing kernel for the invariant subspace will have this form for sum u and the corresponding inner function. So, remember, the kernel for the invariant subspace is the projection onto the invariant substance of the original kernel. And by that theorem, then that looks like this. And over here, I factored out the phi of z bar times phi of W. And so when I do that, I have to divide that term by that.
00:38:42.854 - 00:39:23.314, Speaker B: And I claim that once I know this, then these other two things that, namely that bracket phi is equal to m and the contractor's divisor property are actually totally straightforward. And so let's prove that. So first of all, I want to prove that bracket phi is equal to m, and so phi is contained in M. So bracket phi is contained in M. That's the trivial direction. I have to prove that m is in bracket phi. We know that linear combinations of reproducing kernels are dense in M, so I only need to prove that every reproducing kernel for m is in there.
00:39:23.314 - 00:40:04.694, Speaker B: Now, reproducing kernels, I have a formula for them up here. And so first of all, if I pick a point z where phi of z is zero, then this is identically equal to zero. Actually, I factored it out here, so there might be a problem. But if you look at it here, if you plug in z and w both equal to z, then on the left hand side, this needs to be positive, and that will force the u to be zero at that point z as well. And so if phi of z is zero, then the u is identically equal to zero and everything inside is zero. And then clearly the pmkz is in bracket m. So we'll always just assume that phi of z is not zero.
00:40:04.694 - 00:40:50.498, Speaker B: So if phi of z is not zero, then I'm able to divide through over here at least by the phi of z. This expression here tells me this, since this is an analytic function in w, that means I could solve here for the u. The u sub z is an analytic function in W. And when I divide by phi, at least I get a meromorphic function of w. But this calculation that I did over here, when I look at the z itself, this meromorphic function has to have absolute value less than or equal to one, because this whole expression is positive. And so that means that expression is bounded. So any possible singularities you have are removable singularities.
00:40:50.498 - 00:41:25.674, Speaker B: So this expression here for fixed z as a function of w is actually an h infinity function, and the denominator is benign. The phi of z is fixed. So the expression over here is just an h infinity multiple of the function phi. And clearly that's contained in bracket phi. Any h infinity multiple is contained in there. So I get that the reproducing kernel pmkz is in bracket phi, and that shows this totally easily. And similarly for the contractive divisor property, I have this on the next slide.
00:41:25.674 - 00:42:28.618, Speaker B: That's sort of a general fact about reproducing kernel Hilbert spaces. If you have two reproducing kernel Hilbert spaces, one with reproducing kernel k and one with reproducing kernel s and a function on the set x that these spaces act, then multiplication by this function is a contraction from one space into the other if and only if. This expression here, the difference of kz w minus f of z bar f of w s z w is positive definite function. If you're at all familiar with reproducing kernel Hilbert spaces, you've almost certainly seen this theorem for the case where s is equal to k. So multiplication by f is a contraction operator on h of k if and only if. Now you can factor out the k one f of z bar f of w times k of z is positive definite. But this more general version holds also and if, for example, if you take f to be identically one, then multiplying by f is just inclusion mapping.
00:42:28.618 - 00:43:14.284, Speaker B: And so then it says the inclusion from Hs into HK is a contraction if the difference of the two kernels is positive definite. And that's a theorem that Aurangan proved in his 1950 paper. Okay, so I want to prove that I have a contractive divisor property. So that's saying that multiplication by one over phi is a contractive operator from m to l to a. And so by this I just have to prove that the difference of the reproducing kernel for l two a and the reproducing kernel for m times the one over phi, that this gives me something positive definite. And I have a form for the Kz that's the Bergman kernel. I have a form for the PMKZ.
00:43:14.284 - 00:44:06.292, Speaker B: With Chemorrin's theorem, I do the calculation, I get this expression here, and the UZW is positive definite. When I conjugate it by just a function like this, that's easy to see, it's also positive definite. And then it's a general theorem called the Sho product theorem that says that the point wise product of two positive definite functions is positive definite. So this is positive definite. And so that proves then that the contractive divisor property. So with the reproducing kernel helmet space approach, I have no need for biharmonic functions or complicated approximations that I might need to do, at least in the case of p two, to prove this, if I understand siborins theorem. Okay, so I go back and I try to prove Sinorin's theorem now in my remaining time.
00:44:06.292 - 00:44:49.520, Speaker B: So I've just restated the theorem here, and I will prove one direction of the theorem. So I will prove the direction to go from this estimate on the functions in the space to the form of the reproducing kernel. And then once you see that proof the other direction, if you know your reproducing kernel, Hilbert space theory is pretty much the same, just reversing the steps. So the first thing I want to observe is that I can define an operator l, and it's because it's a left inverse of multiplication by z. I call it l by looking at the following. So phi is a unit vector. Phi is this inner function.
00:44:49.520 - 00:45:29.648, Speaker B: So when I look at the inner product of h with phi and multiplied by the function phi, that gives me the projection of h onto phi. And now phi is the vector that spans this one dimensional case a space. So when I subtract the projection of h onto that space from h, I end up in zk. So I have a function that's in zk, so I can divide by z and I end up in k. And by the closed graph theorem, then I get a bounded linear operator l. When I look at z times l applied to h, of course the denominator cancels out. I get h minus this projection term.
00:45:29.648 - 00:46:02.006, Speaker B: And of course h itself is equal to h minus this projection term plus the projection term. And those two terms are orthogonal to one another. So h has no larger than this. So z times l is a contraction operator. All right, so that gets used in the first step here. So I'm assuming now that I have this, this inequality and I apply it instead of I apply it to a function f and I apply it to a function of the type l times g. So when I do l times g again lg here.
00:46:02.006 - 00:46:22.270, Speaker B: And so I've now written it. I'm sorry, I'm not doing lg. I do lh. Lg is equal to lh. So I get something that looks like this, lh plus zf. So I have this row operation applied to the column vector and the norm squared. And over here I have the f squared and I have z times g.
00:46:22.270 - 00:47:13.994, Speaker B: Well, g is lh, so that's zlh. And I just showed you that the norm of zlh is less than the norm of h. And so I get that this row operator has norm less than or equal to the square root of two. All right? So an operator on a Hilbert space has norm less than or equal to square root of two if and only if x star or xx star has norm less, is bounded by two times the identity in the order on operators on positive operators on Hilbert space. So what I get is that two minus the operator lmz, the row operator times its air joint l m z, the column operator. That's a positive operator. And so when I multiply that out, I get two minus two times the identity, minus ll mz mz, that's a positive operator.
00:47:13.994 - 00:47:46.392, Speaker B: And now I define my u to be a applied to the reproducing kernel. Anytime I take a positive operator, apply it to a reproducing kernel, it's easy to check. I get something that's positive definite. Remember, the reproducing kernel was positive definite. So applying positive operator to it is positive definite. So I have a u, and now I want to tell me, I need to figure out what this is, so I know what two times that is going to be. I also know what Mz mz star applied to the reproducing kernel.
00:47:46.392 - 00:48:25.430, Speaker B: We know Mz applied to the reproducing kernel is z bar times. The reproducing kernel multiplied by z, you get, when you evaluate it at w, you get w times z star at ZW. All right, now the operator l was given on the previous slide, and you can do a simple calculation. It tells you that when you apply ll to the reproducing kernel, you also get something that you can calculate in terms of the expressions that we already have. It's this. And now I can go back. So the U was defined to be a applied to SZ in a product with SW.
00:48:25.430 - 00:48:55.546, Speaker B: A was this operator, two minus this stuff. And I plug in two times Sc. SW is two Sc at w ll with SC. SW was this expression, and mz mz applied to scw bar Sc. And so when you do all of that, that's what you get. Now you can take the common denominator, and you can sort this expression. Now, so there's one expression doesn't have an sc in it.
00:48:55.546 - 00:49:39.918, Speaker B: The other three do. And I can factor out the Sc. And I notice it's minus one, minus wz bar quantity squared times sc. And so I have this, and I solve that for Sc, and I get the form that Sir Morgan had first, reproducing kernel. So that's the proof of how you go from the estimate in the space to the reproducing kernel. And going backwards is that, well, as I said, when you know you're reproducing kernel properties is you now assume that the u of Z w is positive definite. You then get that this operator, two l star minus mz star is a positive operator.
00:49:39.918 - 00:49:59.554, Speaker B: And then you, starting from there, you can just reverse all the steps. Okay, so this is the proof of Schmorin's theorem. And I'll end here. I had two more slides, but as I said earlier, since I added something in the beginning, I'll just stop here.
00:50:02.254 - 00:50:36.286, Speaker A: So, thank you so much, Stefan, for three terrific lectures. We are open for business. For questions, either put them in the chat or just shout them out. I can't keep track of almost 100 people, so. All right. I don't hear any.
00:50:36.430 - 00:51:17.686, Speaker B: Do we know all invariants or space of index one? Do we know. So, okay, so I sort of glossed over this thing. So we have some information about what these extremal functions are, if you have that. But we only had formulas for. For these extremal functions when we have the one point extremal problems and 1.0 set. So, for example, when you want to know what the Bergman extremal functions for two points are, I think there was a student of Hocken Heidenmams that calculated that.
00:51:17.686 - 00:52:01.184, Speaker B: And that gets to be very complicated. You can explicitly get some reasonable form when you have repeated zero of higher order. Or if you take a limit of that, you get one extreme of functional certain invariant subspaces generated by singular inner functions with point masses on the boundary. But the real setback is that even though we have some information about these extrema functions, we don't know what these phis are. And so when you ask me, do we know what these invariant subspaces are, the answer is no, because we don't know what the phis are. So they're all of the type bracket phi, but we don't really know what they are, except in special cases.
00:52:04.204 - 00:52:07.548, Speaker A: Stefan, there's a thing in the chat.
00:52:07.676 - 00:52:16.244, Speaker B: Yeah, I'll try to locate the file again to share the. The file.
00:52:19.864 - 00:52:34.928, Speaker A: All right, while Stefan looks for a file, does anybody have anything else they want to add or comment on? All right, so I have a question.
00:52:35.016 - 00:52:36.592, Speaker C: Bill, can you hear me?
00:52:36.728 - 00:52:38.240, Speaker A: I can, Joe.
00:52:38.352 - 00:53:06.774, Speaker C: Oh, yeah. Let me ask an unfair question, Stefan, going back to the point where you get this ratio involving the phis to be an h infinity function on the disk. If you assume that function, let's say, is univalent, can it tell you anything about the phis? Can you unwrap anything from an assumption.
00:53:07.254 - 00:53:09.434, Speaker B: By that over here?
00:53:09.734 - 00:53:10.606, Speaker C: No, no, no.
00:53:10.670 - 00:53:11.014, Speaker B: They.
00:53:11.094 - 00:53:30.714, Speaker C: It's down here at the bottom, I think. Yeah, down at the bottom. Z bar w u z w divided by Phi bar z Phi W. If you assume that for Z fixed, that is univalent in W. Can you say anything at all about the properties of Phi?
00:53:33.094 - 00:54:02.024, Speaker B: I don't know. Okay. Actually, now that you bring it up. There is a conjecture that's sort of related to that that had on the slides that I didn't do. There's a question of hidden moms, whether something is univalent or even star like. And if you want to read over the last two slides, maybe you get a little closer to that. But that is a little related to what you're asking me, but I don't know the answer.
00:54:06.964 - 00:54:21.068, Speaker A: All right. Any other comments or questions? Stefan shared the file again, so if anybody came in late, you can see the file.
00:54:21.236 - 00:54:21.580, Speaker B: All right.
00:54:21.612 - 00:54:25.824, Speaker A: So let's thank Stefan again for three terrific talks.
00:54:29.604 - 00:54:30.004, Speaker B: All right.
00:54:30.044 - 00:54:31.324, Speaker A: So give us five minutes.
