00:00:00.200 - 00:00:13.954, Speaker A: Program. We start with the mini course of our beloved colleague Andreas Hartman from Bordeaux, who is going to talk on interpolation and sampling. Please, Andreas.
00:00:14.854 - 00:00:57.254, Speaker B: Okay, thank you very much. I hope that you can hear me well everywhere. First of all, first, of course, I have to thank the organizers to put together this very impressive program, this focus program with many different topics, which I like a lot. And unfortunately I cannot attend all the talks and all the courses. But I think this is a very good idea, very nice idea also for young people to get introduced in different topics. So I also would like to say that this course will be quite elementary. I was told to do so.
00:00:57.254 - 00:01:37.884, Speaker B: So I see many colleagues, of course, who already know all the stuff. But it should be understandable, of course, if you have any question, you can interrupt me and ask questions. So, this course will be divided into three parts, if I manage to click on the next slide. So today I will give some motivations, basic examples, and also some general results, which make maybe some connection to some other courses. I saw the course on frames. Maybe there will also be something on re spaces. I think there's a course on re spaces.
00:01:37.884 - 00:02:26.484, Speaker B: So I will establish some relations between interpolation in particular and respaces. Tomorrow I will go, and in fact, in quite some detail concerning the quite old interpolation problem in the Hardy spaces, which is quite instructive for a more general situation. I've seen that Michelle Herz is also attending some point. I will cite some of his results. And the last course will be devoted to debar methods. And I think people who do not know too much about interpolation, maybe they're not very acquainted with such methods. And they can be sometimes very powerful.
00:02:26.484 - 00:03:00.218, Speaker B: And so I decided to. To show how one can solve the interpolation problem using Dibar methods. And I think after my talk, there will be a talk by Joachim Ortega Sela. And in fact, I will present a method he gave with Bo Benson, in of course, more general setting. So, as you see also from the already, from this description, I will more insist on interpolation. And I will from time to time also speak about sampling. This is a very broad subject, and there are many, many things to say.
00:03:00.218 - 00:03:56.954, Speaker B: And so, well, we have to make some decisions at certain points. So let's go to the initial situation. And Bill just told me he will talk about Lagrange interpolation tomorrow or later in his courses. So I remind this situation, because it's very instructive for what will come later. So the classical interpolation problem, which is quite old, I gave some historical approach in the abstract. I'm not completely sure how old this problem is, really. So if we are given a set, finite set of points in the complex plane, in my graphics here, it's in the real line, but of course the real line is in the complex plane.
00:03:56.954 - 00:04:52.694, Speaker B: So you are interested in finding a polynomial which takes given values v one till the end in the given points lambda n. Okay, for this problem, there's a solution which you will see in a moment. There's a well known fact. Of course, if you want to solve this problem for arbitrary values v k, then you have a condition on the degree which should be big enough. So this raises a certain number of questions which we might ask also later in a more general setting. And the solution to this problem, as you might know, is of course, the Lagrange interpolation formula, which I recall here. And so this is a very constructive formula also for what will come later.
00:04:52.694 - 00:05:44.548, Speaker B: So indeed, what you are constructing, you construct a polynomial which has the right degree which vanishes in all the interpolation nodes except one. So of course, then you have to multiply all the z minus lambda j except the lambda k. And in order to normalize it, you divide by lambda k minus lambda j. So this product is a function which vanishes in all the nodes except one, where it is one. And this, we will see, is a general philosophy in interpolation problems. And then, of course, it's clear these conditions on the polynomial, that the formula which interpolates the values as it's given here. Okay, so from this observation, of course, you can ask different questions.
00:05:44.548 - 00:06:47.604, Speaker B: So first, does it make sense to consider a more general setting? So, going to infinitely many points. But this raises immediately, of course, the question by which functions do we wish to interpolate? We cannot any longer interpolate by polynomials because, well, of course you can interpolate very specific values in infinitely many points, but this problem is maybe not so interesting. The next question in this connection is, of course, which values we would like to interpolate. So, in the previous situation, this was quite clear. The set of functions were the polynomials, but in marginal situations, which are the answers that we can give. And the first course is. Maybe the examples I would like to discuss should show that there are situations where the answers to these questions can be natural, but of course, in motion settings, the answer is maybe more complicated.
00:06:47.604 - 00:07:38.924, Speaker B: Here is a very silly example which I want to give. So, just start with the sequence one over narrative for integers different from zero. And we want to interpolate by functions from the class of holomorphic functions, functions which are holomorphic and the unit disk. Then if I take for instance, a value sequence given by these alternating values minus one to the n. Of course, I cannot interpolate such oscillating values by a function which is polymorphic in the point where the sequence converges. It's the principle of isolated singularities, isolated zeros. Excuse me.
00:07:38.924 - 00:08:47.543, Speaker B: On the other hand, of course, I could take the value sequence one over n, and this can be interpolated by the function f of z, which is equal to z and which is of course, in the space x. So we already see the questions two and three. Well, we have to think about carefully if you want to solve some reasonable problems. Problems I should also mention that again, this is effect from the separation of zeros, isolated zeros. The sequence one over n is a uniqueness sequence for the space x holomorphic functions on the unit disk. So let's go further to a less trivial situation for which I don't want to go into details, of course, today, but I want to give some observations to get maybe some feeling. And the space I would like to consider is h infinity, the space of uniformly bounded functions.
00:08:47.543 - 00:09:46.314, Speaker B: Analytic functions. I haven't written analytic functions here. Of course, these functions have to be analytic and bounded in the unidisk, and I pick a sequence of distinct points in d. And so, in this situation. So I fixed the answer to the first questions, which functions I would like to interpolate with in this situation. The natural answer to the second question is the space of uniformly bounded sequences. Of course, in this situation, if you look at the trace of the function of the space x on the sequence, it's just the sequence is f of lambda n, and since f is bounded, well, f of lambda n is uniformly bounded by some by the norm of f.
00:09:46.314 - 00:10:48.902, Speaker B: And so this is null infinity. So the sequence space here seems to be quite natural. And of course, the much more interesting question is when can I interpolate? So when can I find for arbitrary sequences which are bounded, a function f in the hardy space of bounded analytic functions in the unit disk. Interpolating these values so it's like the interpolation problem we saw earlier. If we can solve this problem, then we will use the notation lambda is an interpolating sequence of h infinity. Okay, this slide is maybe much information on this, so I would like to start discussing some partial answers to this problem. So suppose lambda is interpolating.
00:10:48.902 - 00:11:38.368, Speaker B: So we already know that if the function f is in h infinity, so uniformly bounded on the unit disk, the sequence of values f lambda n is uniformly bound. So we have disembedding. But we want to know when can I interpolate? The first observation is in order that this problem has a solution. Necessarily, the sequence lambda n has to go to the boundary of the unit disk. Again, this is a consequence of isolated zeros. So if you have a function, if you were able to interpolate, for example, the sequence taking zero on lambda two n and one on lambda two n plus one. So on odd points, you take one, on even points, you take the value zero.
00:11:38.368 - 00:12:33.044, Speaker B: If the sequence tends to value inside the unit disk, then of course you cannot find an analytic function alternating between the values zero and one. So this is a quite easy observation. The next observation is already maybe a little bit more intricate. And this observation tells us that in fact we can solve the interpolate if we can solve if a sequence is interpolating. So this is my assumption on this slide. So in particular, this means that this restriction operator is of course well defined, because as I already said, bounded functions take bounded values. Now, since lambda is interpolating, I know that this operator is onto.
00:12:33.044 - 00:13:39.088, Speaker B: Then we know from the open mapping theorem that whenever we are given a value sequence, then not only we find a function interpolating this value sequence, but we can also control the norm of the interpolating function. This is just a consequence of the open mapping theorem and which will have some consequence, which I explain in a few seconds. And this is separation. So given a point lambda in the unit disk, we can define the moebius map. So this is an automorphism on the unidisc. Okay, this maps unidisc to itself injectively, of course. And with such a mobius map in mind, we can associate a metric, so called pseudo hyperbolic metric.
00:13:39.088 - 00:14:51.054, Speaker B: And this is just defined by the evaluation of phi lambda in z. So the distance between two points lambda and z is the modulus of phi lambda z. I realized that I will explain a little bit later how this, let's say metric is constructed, but maybe I can just try to draw a picture here. So this is unidisc, okay, so if I take a disc with some radius close to zero, so it has some radius, but the closer I get to the circle, the smaller my discs will be. Okay, okay, this is not a very nice picture, but let's say that if you fix the disk or neighborhood in this pseudo hyperbolic metric with some fixed radius, then if your lambda goes to the boundary, then the corresponding euclidean radius will go to zero. It's compared to the distance to the boundary. I will come back to this a little bit later.
00:14:51.054 - 00:16:04.604, Speaker B: It's just to maybe recall that this defines a distance and it has some particular behavior. Nice result in this connection is the Schwarz lemma, which is not very hard to prove. It's essentially the maximum modulus principle. This lemma claims that if you take a function in h infinity, which is of norm less or equal to one, then this function is one lipschitz, which means in this metric o, which means if you take two points lambda z, then you apply f to both points. Then the pseudohyperbolic distance between the images is less than the pseudo hyperbolic distance of the original points. Okay, this is the Schwarz lemma, and you can just prove it by using the maximum modulus principle, I think. And with this in mind, we can show a separation result, which we'll discuss a little bit longer tomorrow.
00:16:04.604 - 00:16:51.762, Speaker B: So now, since if you take an interpolating sequence, of course, then you can find a function interpolating, in particular the value one over c in lambda k and zero in all the other points. Well, in particular in the point lambda n n different from k. By the open mapping theorem, we just learned that the function f can be controlled by c times the norm of v. But since v k is one over c in one point, and everywhere else its norm is one over k, it's one over c. Excuse me. And so the norm of f can be chosen less or equal to one. And now I can show, well, this is very easy.
00:16:51.762 - 00:17:31.852, Speaker B: I use the one lipschitzness of the function f. So the distance of the images is less than the distance of the points. Now you plug in the values of f lambda n f lambda k in this formula here. But since f lambda n is zero, so one of the, maybe the z is zero. Okay, so you just get lambda, and the lambda was one over c here. So this is equal to one over c, and one over c was a fixed constant. And so at the end of the day, you see that the points are separated in this metric.
00:17:31.852 - 00:18:24.510, Speaker B: So you see, if you choose a space, if you choose a target space, the sequence you want to interpolate, then you can get conditions on the sequence lambda and the distribution. Maybe I will not go into details of the argument, which is here on the bottom of the slide, because I will repeat it more or less tomorrow. But we have already more or less all the keys. In the preceding observation, you can pick f vanishing in all the points except lambda k. And since it vanishes in all the points this function has a blaschke factor, which is zero on all the points of the sequence except at k. And then there is another function, fk, which has the same norm as fast. Okay.
00:18:24.510 - 00:18:44.774, Speaker B: Fk is not necessarily outer. So this is for the experts. Someone will tell me, is fk outer or not? Well, we don't care. It can have singular or even plushka parts. It's not very important. What is important, that the norm of fk is also wrong. And then you can run the argument exactly in the same way, in the same fashion.
00:18:44.774 - 00:19:35.934, Speaker B: And at the end, you will have shown that the blaschko product, vanishing in all the points except lambda kinetic, is uniformly bounded from below. And this is what we will call the Carlson condition, which will characterize interpolation. And this I will prove tomorrow. Okay, these are just some observations to get some feeling about these interpolation problems. Here is another example, which is also quite easy. Maybe I should erase my annotations here. Okay, let's start with the function of square space, of square summary functions on the interval minus PI PI.
00:19:35.934 - 00:20:08.210, Speaker B: And I make this maybe strange renormalization. And I must admit, I'm not sure whether this is the right observation that I should have chosen here, because maybe that my constants are not completely consistent. And I apologize for this. Okay, why do I put a square root here? It's because I consider this on the real line. And then if you use Fourier transform, then maybe you will get an isometry. And this depends on the normalization. Okay.
00:20:08.210 - 00:20:46.734, Speaker B: But it's not very important, this normalization. So I take the space, and of course, you all know that l two minus PI, PI there is the trigonometric basis. So, which is e to the int and n runs through the integers. Okay, again, I have this stupid normalization constant, and I have to take a four square root. But don't worry about the square root here. And it's well known that e is an orthonormal basis for y. So with this normalization, it's really an orthonormal basis for the space y.
00:20:46.734 - 00:21:35.996, Speaker B: This means that every function in the space can be developed in the spaces, of course. And the norm is exactly given by the coefficients of this, of the sequence. Now we switch to the Fourier transform. Okay, so I take functions f. Well, this you can do for functions in l two on the real line. Of course, this is the definition of free transforms. So, with this normalization, and there's an important theorem, which is the Paley Wiener theorem, which claims that three transforms of functions in l two of minus PI PI.
00:21:35.996 - 00:22:18.850, Speaker B: So in this band limited, well, in this finite interval, well, this defines the so called paleo vena space of entire functions. And this Palo venous space is very important, of course, in signal theory. In a sense, you can say that Palluiner space characterizes the sound. If you have stationary sound, then in a sense, if the frequency spectrum of the sound is finite, then it's a function in the Palomina space. So this is a very important space. And I record the definition here. Of course, free transforms of l two functions are in l two.
00:22:18.850 - 00:22:54.514, Speaker B: So this condition is clear. Bailey Wiener's theorem tells you that these functions are anti. But you can prove this more or less by hands and these functions of exponential type. I recall what this meant by exponential type. It means essentially that functions cannot grow more fast than e to the PI mod z. So if I'm here in pw PI, so the type is bounded by PI. So the functions grow at most like e to the PI z.
00:22:54.514 - 00:23:48.764, Speaker B: In fact, you can say a little bit more, you can say that in fact it's the imaginary part of these functions, which is bounded by p times. No, it's the modulus of the functions are bounded by e to the imaginary part p times the modulus of the imaginary part of z. Sorry for this. Okay, so we can now look at another interesting example. So we consider this Paleo venous space, which I recall is the space of free transforms of l two functions on minus PI PI. And since e was an orthonormal basis in y, well, fe will be an orthonormal basis in the Pallia venous space. So let's try to compute the free transform of these exponential basis.
00:23:48.764 - 00:24:25.274, Speaker B: And so here I take different normalization in order to get the constants. Nice. So you can compute free transform of this exponential function. And at the end you will end up with the cardinal sine function. And here we make an observation. So if you take lambda j and to be equal to j. So now I fix sequence in the complex plane, which is exactly the integers.
00:24:25.274 - 00:25:12.016, Speaker B: Well, they are in the complex plane and this cardinal sine function, well, it, it does exactly what did earlier these Lagrange polynomials. It is one in one point and zero in all the other points. And from this it seems clear how we should be able to construct later the interpolation operator. In particular, if f is a function in the palomina space, since the kn's are also on orthonormal basis, maybe up to some normalization constant, I can develop f on this basis. Okay. And since this is an orthonormal basis. The coefficients have to be in l two.
00:25:12.016 - 00:26:37.214, Speaker B: And I can also show that for every an which is this sum will converge to a function of the palominar space, because kn is an orthonormal basis. And as we saw earlier in the first example, Lagrange interpolation. If I compute f at lambda k, well, I will get exactly a k. So this means that if we take the palo vena space, then the set z of the integers is an interpolating sequence, and the appropriate space of value sequences is little l two. Let me summarize this a little bit. So in a sense we have proved this theorem more or less that the integers are an interpolated sequence for l two, but in fact we know a little bit more. Not only is it, is it an interpolating sequence operator, we have seen that the trace of the Spalovina space on the is in l two and the restriction operator is n two.
00:26:37.214 - 00:27:18.418, Speaker B: So it's an interpretive sequence. But the interpolation problem has also the unique solution. So you cannot have different solutions to a given sequence a. In such a situation, we will call the sequence a complete interpolating sequence. So it's not only interpolating, it's also uniqueness. So this interpolation problem has a unique solution, and then we will call such a sequence a complete interpolating sequence. So in fact, maybe I will not give completely, while it's quite clear what is written here.
00:27:18.418 - 00:28:24.462, Speaker B: So by the fact that it's an orthonormal basis and the reason, the argument I gave earlier, the norm of the function is determined by the values in the points. And in fact we have now met already our first sampling sequence also. Okay, sampling means that you can reconstruct the function from discrete values and you have a control on the norm. I will give more formal definitions later. It's just an observation. At this point I'm also tempted to give more general result, which I will not prove there's a perturbation result, because now one could be very eager to know, but what are general interpolating sequences for the Paleo venous space? Well, this is a difficult question, but there's some kind of easier result which tells you that if you perturbate this sequence, then you can stay complete interpolating. And this is a result due to Ingham and cadets.
00:28:24.462 - 00:29:19.448, Speaker B: It's one part which was proven in the forties and the other 164, I think. And this tells us that if you perturbate delta n should be real values. If you perturbate by uniformly by some this way that so the soup of the perturbations should be strictly less than one quarter, then you're still complete interpolating. And if you choose, and if delta n is such that the soup is less or equal to one fourth, then there are sequences which are no longer interpolating. It's not true that for every sequence with this condition, it's not interpolating. Okay, but there are sequences such that this is no longer interpolating. If you are less ambitious for smaller constants than one fourth, proving the theorem is not too difficult.
00:29:19.448 - 00:30:03.360, Speaker B: You can do it in one page. Some more comments on the interpolation of the Paleo venous space. Paleo venous space I don't know if the course on model spaces was before, I think, or does it come later? It was before. So paleo Vena spaces are special incidences of model spaces. And for model spaces, there is some general approach, but the conditions are difficult to check. Okay, I will not be too long on this for now. Okay, so I will now talk about some more channel results.
00:30:03.360 - 00:30:59.524, Speaker B: But let me maybe say as an intermediate summary, what we have seen in these two examples is that in certain situations, the question which functions we use. So Lagrange was polynomials, but we can also decide to take function spaces. So we have looked at spaces of uniformly bounded functions, and we have looked also at a space which is a Hilbert space. And so for these two settings, so spaces of uniformly bounded functions, the answer to the question which are the appropriate traces, the sequence we would want to interpolate. There are some natural answers. The hardy space of uniformly bounded functions, it was l infinity. And in the Silbert space, well, it was a Hilbert space of sequences for sequence Hilbert space.
00:30:59.524 - 00:31:58.864, Speaker B: Okay, I want now to make this second observation a little bit more general. So, if you take the Hilbert space, so what should be the trace space in such a situation? And so I would like to talk about reproducing kernel Hilbert spaces. They are also on the schedule, I think, of this big semester here. So I pick a domain and the complex plane. But of course I could be even more general and consider cn, but I stick to the. I feel more comfortable in the complex plane, but okay, you can do more things. And in what follows, these domains will be essentially the unit disk, the upper half plane, or the complex plane, the entire complex plane.
00:31:58.864 - 00:32:50.704, Speaker B: Now, I take age space of polymorphic functions. So on omega, and I will assume that point evaluation is bounded. And then we call this reproducing kernel Hilbert space and by the representation theorem. Of course, this point evaluation function can be represented by a function which is in the space. And we can recover the value of a function f in point lambda just by taking the scalar product of f against the reproducing kernel. And a very easy observation is the norm of this reproducing kernel is very easily calculated because it's just k lambda against k lambda, which is k lambda in lambda. Okay, this makes computation somehow easy.
00:32:50.704 - 00:34:05.228, Speaker B: Okay, so we pick a sequence. So the lambda ends are distinct points, discrete and distinct points in omega. And in order to get the trace space, I make the following simple observation. If I want to interpolate sequence v, which is an l, so I have to find a function f, and interpolating all these values so vn is equal to f lambda n, and f, I can recover the value by integrating against the reproducing kernel. And by the kosher Schwarzenegger inequality, I get this inequality, this estimate, and so, vn over the norm of the reproducing kernel has to be bounded. And let's say for the moment, since the tray space should be Hilbert space, natural gas for the tray space is to pick now the tray space as the l two space with this weight. Okay? And so this will be our natural trace space.
00:34:05.228 - 00:34:59.464, Speaker B: And we'll see another argument why this is the right trace space to pick. Okay, so in the functions uniformly bounded, there's a natural trace. And for reproducing kernel Hilbert spaces, there's also a natural trace. Okay, so this will be my definition. So if I take repressing Colonel Hubert space on the domain omega, the sequence lambda will be called interpolating. If the trace is equal to exactly this space of sequences, l two sequences with this weight. And we will write that lambda is an interpolating sequence for the space age.
00:34:59.464 - 00:36:05.154, Speaker B: Okay, here is first observation. So, since I've written an equality here, this means that I have two embeddings. I know that the trace of h is embedded into the sequence space, and then I was embedding in the other direction. And the embedding of the trace into this l two space can be reformulated as a kind of Carlson embedding result. Okay? In particular, if we choose the measure delta lambda n with the right weight, then the embedding in this direction. Okay, this embedding, this is essentially the Carlson embedding property. Okay? So I should also say something maybe about the other embedding.
00:36:05.154 - 00:37:14.134, Speaker B: Well, in fact, some, in some situations, it makes sense to consider as interpolation only this embedding here, because when you interpolate, you're given a sequence and you want to find a function interpolating these values. So one could think that the real definition of interpolation is this. And as a matter of fact, this makes sense. And for certain spaces, we have to distinguish notion of universal interpolation, which means that you have equality, and simple interpolation, which means that you have just this embedding. And for example, in the deer clay space, you have a difference between both notions. But I don't want to go into details of this here. Okay, so here are some simple examples where you can get explicitly the trace space.
00:37:14.134 - 00:37:53.214, Speaker B: So in the Hardy space, you can show that, of course, the reproducing kernel is just the sega kernel. And so if you compute the norm of the Sigel kernel, then you have to compute k lambda of lambda. And this is exactly one minus lambda, one over one minus lambda squared. And since we divide by the norm squared, we get this weight Bergman space. Okay, I haven't given the right name here. It's not h two, excuse me. It should be Bergman.
00:37:53.214 - 00:38:43.238, Speaker B: I should correct this. Okay, this is a Bergman space. So, the Bergmann space, of course, is defined by integration condition with respect to planar measure on the unit disk or on other domains. But for convenience, I've given here the formula with Taylor coefficients, because then, for the exact same reasons as before, I can compute easily the rebreasing kernel. And then I get also the corresponding trace space. So, with a square here and weight and the fox space we will see after tomorrow. Well, you can write it also as, as Taylor, Taylor series with some conditions on the Taylor coefficients.
00:38:43.238 - 00:39:29.374, Speaker B: And then again, you can find quite easily the replacing kernel and the corresponding trace. All right, so let's go further in the connection, which I would like to establish between interpolation and resequences. Excuse me. Okay. I would like to recall the definition of resequence. So, resequence is a sequence of elements which I, in general suppose normalized. You can do more general things.
00:39:29.374 - 00:40:33.616, Speaker B: And if you expand, well, if you write linear combinations in this basis, in this sequence, then you want that the norm is comparable to the norm of the coefficients, the two norm of the coefficients. This is some kind of Pythagoras theorem. A special case is when big c is equal to one, then everything is equal here. And this is just Pythagoras theorem, and you have an orthonormal basis. Okay? So I will use the notation, this notation here to summarize this condition, style. Also, I can say, of course, this resequence condition doesn't require that the un's form a complete system in the Hilbert space. If it's complete, I will call it rebate.
00:40:33.616 - 00:41:22.398, Speaker B: And res sequence is, of course, always a re spaces in its back. All right, so another definition I will need to use is the biorthogonal family. And the biorthogonal family for a given sequence, un is just a sequence which satisfies this Kronecker relation. So, un against phi k is equal to one when n is equal to k and zero elsewhere else. And as you will see later, the phi n will be the replacement of this lagrange interpolating polynomial. Okay, here's an observation. So this doesn't always exist.
00:41:22.398 - 00:42:00.814, Speaker B: We have to impose some conditions. In particular, the UN's. Well, un doesn't have to be. It shouldn't be in the span of all the other functions. And then by Han Banach, we can say that such a sequence will exist. Here, I recall a certain number of conditions you should know about these sequences. So, a resequence, in particular, it's an isomorphic image of an orthonormal sequence on orthonormal basis.
00:42:00.814 - 00:42:42.342, Speaker B: I mean, onto. Okay, this is important. A resequence, you can show. It's the orthonormal image of an orthonormal sequence. It's also, you can show that a resequence is an unconditional sequence on unconditional basis in h zero. What, what do I mean by unconditional basis? It means that you can write, you can sum the sum in any order, and you always get convergence and same value. Okay? Unconditionality is like unconditionality on sequences of numbers.
00:42:42.342 - 00:43:39.472, Speaker B: Okay? So resequence is an unconditional basis. So you can sum in whatever order you want, and you get always the same results. I will come back maybe later to these conditions, which we'll use a little bit later. Okay, now if I have. So now we want to establish the, the relation between interpolation and biorthogenal systems. So if I take, again a ripple synchronic Hilbert space, I take a sequence in omega, I consider the system of repressing kernels normalized and the associate b orthogonal system, provided it exists. Then, since it's b orthogonal, I have this condition, delta n j.
00:43:39.472 - 00:44:33.170, Speaker B: So phi n vanishes on all the points, like, well, I say the plaschop product, but also like this simple Lagrange interpolation polynomial. And with this in mind, in fact, I can write at least if I take a function f which doesn't vanish infinitely many points, I can write the sum, because then the sum will be finite. This is a constant. I divide by a constant f against k. Lambda n is f lambda n, and then I multiply by this function phi n, which satisfies this condition. So it vanishes in all other points except lambda n, and it takes some value on lambda n. And I can realize that big f and little f take the same values on lambda.
00:44:33.170 - 00:45:51.744, Speaker B: Okay, it's a fact that if the sequence of re producing kernels is a rebasis in its span, then the corresponding b orthogonal family is also re spaces in the same span. And this can be shown just by saying that, well, if one is re spaces, it's isomorphic range of an orthonormal basis. And for phi n you have to choose just van. Okay, this is just direct verification. And so the theorem, which I'm not sure, but I can give the whole proof because I'm running a little bit out of time. But the theorem, and you can get it from the, from the slides, I think they will be maybe made available. So if you have a representing current Hubble space on omega and a sequence of discrete sequence of distinct points in omega, then the sequence is interpolating if and only if the sequence of reproducing kernels is resequence.
00:45:51.744 - 00:46:38.646, Speaker B: And maybe I will just try to prove one direction of this application. So let lambda be an interpolating sequence. So then we know that the restriction is well defined into the space and onto. Okay, that was my definition I gave earlier. Let's take h zero to be the space generated by the sequence, and then I take n to be the space of functions which vanish on lambda. And then I think it's clear that n is exactly the kernel of r. R is zero.
00:46:38.646 - 00:47:26.072, Speaker B: This means that f vanishes on lambda. Okay, so this is n, and then r is of course isomorphic from the orthonor complement of n onto the space. And of course we have this relation. So this means the, the rr, the restriction operator is an isomorphism of this subspace h zero onto this weighted l two space. And now let's see if I can deduce that I have a resequence. So I pick un norm one sequence in this space. And how do I do this? I let un in n to be the norm of k lambda n, and it should be zero in all the other points.
00:47:26.072 - 00:48:19.104, Speaker B: Okay, this j should be a k and then I. So I get a sequence in the target space, and I pull back to the space h zero, and I will call this pn. And then I can introduce an interpolation operator exactly as before. So I take the value vk and I multiply by something, which takes exactly the value one, I think, in the point and zero elsewhere. Okay, so the interpolation condition is clear, because phi k vanishes everywhere else except in v, k ventures normal points lambda and different from k. And in k, this quotient is equal to one. So this is an interpolation operator.
00:48:19.104 - 00:49:26.404, Speaker B: But for the moment, we don't know if it converges. So the idea here is to say that in fact, this interpolation operator is the inverse of the restriction operator. And now you see, if you take this sum, it's exactly by definition, the norm of v in the target space. And since the operator r is an isomorphism, this is comparable to this function. And this function, the norm, is given as an expansion on phi k. And then setting these coefficients ak, we see that while the norms are comparable, so we see that phi k is a resequence, because phi k satisfies this norm equivalence from the v spaces. And since it's a biothrom eligible represents, well, we conclude that k is a sequence.
00:49:26.404 - 00:50:22.096, Speaker B: Okay, I see I'm running out of time, so I will finish this year. I think these slides will be made available for the audience, and so we'll get also the other implication for this proof. And, well, there are some remarks, and maybe again what I wanted to mention the examples we have discussed. So, for p equals infinity, and for pquells two, we get some natural conditions on trace spaces. And this defining trace spaces is not clear. There was notion introduced by the Leningrad school in the seventies, which is called free interpolation. And this means just the trace is free from analyticity.
00:50:22.096 - 00:50:52.364, Speaker B: So if you can interpolate the sequence, then you can multiply it by a uniformly bounded sequence. And it's still Interpol label. And so you don't need to have any intuition on the trace space. Okay, I will not go much more in the slides, and maybe I should stop here. And thank you very much for your attention and to keep the schedule. Okay, thank you.
00:50:52.864 - 00:51:15.094, Speaker A: Thanks, Andreas. That's thanks to speaking. Is there any question or remark for Andreas? Since I do not see everybody either, please type in the chat or shout it out. There is a question for you, Andreas, in the chat. Can you see it?
00:51:16.674 - 00:51:59.066, Speaker B: Yeah, I can see it. So, of course. Well, that's a question. So, does there always exist a reproducing kernel Hilbert space. Well, at least for interpolation problems, if you don't have a reproducing kernel Hilbert space, then the problem doesn't make sense, because what does it mean that point evaluation is not bounded. You can construct exotic examples where point evaluations are not bounded, even in function spaces. But these are.
00:51:59.066 - 00:52:22.934, Speaker B: You have to do some artificial tricks. Okay, this is not exactly the answer. So for the domains I consider, let's say if you have a domain you can always define, for example, Bergman space, which is replacing Colonel Hilbert space.
00:52:24.914 - 00:52:35.170, Speaker A: Is it true to say that sometimes we have Banach spaces of analytic functions for which evaluations are continuous, and then we proceed pretty much similarly?
00:52:35.362 - 00:53:01.774, Speaker B: Yeah, of course. So I haven't opened about more general situation. Maybe I will mention this tomorrow. For example, the Hardy space hp. So in the Hardy space Hp, you proceed in a similar way, because the. The pee situation behaves more or less like the hill version situation. If.
00:53:03.714 - 00:53:23.134, Speaker A: Any further question or thoughts for. For Andreas? If not, let's thank him again very much. Thank you, Andreas. And we'll come back tomorrow. We have.
