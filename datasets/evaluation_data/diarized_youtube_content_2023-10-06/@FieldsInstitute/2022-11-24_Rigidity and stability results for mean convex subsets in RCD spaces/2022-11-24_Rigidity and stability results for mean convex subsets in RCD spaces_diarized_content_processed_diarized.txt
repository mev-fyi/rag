00:00:00.280 - 00:00:46.944, Speaker A: Speak about lower mean curvature bounds for subsets and RCD spaces. Yeah, thank you for the introduction. And yeah, thanks to the organizers to give me the opportunity to speak here. I'm sorry that I cannot be in Toronto, so I was there last week, but unfortunately I had to go back. But yeah, I still can do the seminar here. And so I will speak about lower, as the title says about lower mean curvature bounds for subsets in RCD spaces. And so the plan of the talk will be roughly like this.
00:00:46.944 - 00:01:57.474, Speaker A: So, I mean, first I will review some results about romanian manifolds with lower Ricci curvature and boundary. Then I will explain how we can define mean curvature bounds for subsets in RCD spaces. And then finally I will explain about uniform convergence and stability of mean curvature bounds. Okay, so let's start. So I will start with some motivational slides. So we all know, I mean, when you have a remaining manifold with empty boundary and the lower Ricci curvature bound, we can prove very nice results or theorems are proven. So for instance, we have the Bony Myers estimate that tells us that the under our positive lower Ricci bound, the diameter is bound from above and we have equality in this estimate if and only if we have a sphere.
00:01:57.474 - 00:03:20.574, Speaker A: Another theorem in this class is the Chiga Komal splitting theorem. Okay, so the question now is, can we generalize or can we obtain similar theorems in the context of remaining manifolds with boundary? And especially we want to consider not just convex boundary manifolds with convex boundary. So we want to consider remaining manifolds with non empty boundary and some additional curvature assumption for the boundary that is weaker than convexity. And for that we will consider the mean curvature of the boundary. And that is just to remind you, this is the trace of the second fundamental form of the boundary. Okay? And then if we now assume, in addition to lower Ricci bounds, a lower bound for the mean curvature. Indeed, there are also similar, one can obtain similar results like in the case of like in the non, like in the non boundary case.
00:03:20.574 - 00:04:32.102, Speaker A: And yeah, results in this context. Many or results in this context were proven by, by casue. And I think that was in 1984. And he proved, for instance, this theorem. So if you have non negative so imagine you have a remaining manifold with boundary, a smooth boundary, and now assume you also have non negative Richie curvature and the mean curvature is bounded from below by a positive constant h. Then what Casuel proved is that you get an upper bound for this expression that you can see here that is, that is more better understood in the following way. So the infimum here this is nothing else than the distance to the complex or the distance to the boundary, so that the minimal distance of each point to the boundary.
00:04:32.102 - 00:05:41.398, Speaker A: And then the estimate says that this function has a uniform upper bound that only depends on, on the dimension and the mean curvature. And here in this estimate, one also has a rigidity statement. So we have equality if and only if the manifold with boundary is isometric to a closed ball in our clean space. And this estimate, geometrically can be understood as an, as a bound for the radius of balls that you can pack into your manifold with boundary. So we also speak about inscribed radius bounds. Okay. Other results in this direction that were also proven by casue that generalize the splitting results are the following.
00:05:41.398 - 00:06:29.530, Speaker A: So if we have non negative Ricci curvature and non negative mean curvature, and we know the boundary is disconnected and also compact, then casually prove that the space splits. And more precisely, there's another remaining manifold with non negative Richie curvature such that m is isometric to the interval times n. So this is in the compact situation. And what you can see is that, I mean, there's no, a priori number on the number of boundary components is a priori not fixed. But the result tells you that, I mean, there can be, there can be only two. Hi Christian.
00:06:29.562 - 00:06:38.414, Speaker B: Sorry Christian. Yeah, so I'm confused about this statement on the splitting. Don't you need some sort of line or long jurisic or anything like that?
00:06:38.594 - 00:07:17.478, Speaker A: No, no, no, this is, this works without any assumption. Yeah. Let me say one more comment. So this is, this theorem is in the, but thanks for the question. The theorem is in the spirit of similar results for, or not similar results, but related results for the scalar curvature. So there's this famous result by Sherman Jao Komo that you cannot, that any non positive, how is it called? Any non negative, any remaining metric of non negative curvature on the toru?
00:07:17.646 - 00:07:23.142, Speaker B: Yeah, the one on the torus, you mean curvature is negative on the torus, then it's flat.
00:07:23.318 - 00:07:28.794, Speaker A: Yeah, yeah. And this, this is somehow the mean curvature analog of this, of this theory.
00:07:29.914 - 00:07:31.234, Speaker B: Okay, thanks for this comment.
00:07:31.274 - 00:08:13.934, Speaker A: Thanks. Yeah, you're welcome. Yeah. And so this is the compact situation, and the non compact theorem is the following, where you now in fact need a line, or more precisely array. So you have a remaining manifold with non negative rigid curvature and non negative mean curvature. And if you know there exists a geodesic ray with initial point in the boundary, then you also split and you split off a half line. And here, the interesting thing is that you do not need to assume compactness.
00:08:13.934 - 00:08:39.494, Speaker A: So this works. Without that, the boundary is compact. Okay, so this is for motivational purposes. And now. Yeah, the goal in my talk is. So, we want to generalize this results to the non smooth setting of RCD spaces. RCD metric metal spaces.
00:08:39.494 - 00:08:57.352, Speaker A: And here, I have to say. So these theorems were already generalized, let's say, to the setting of Bakri Emery weighted remaining manifolds. This was done by Sakurai, for instance.
00:08:57.448 - 00:09:03.724, Speaker C: So sorry, can you go back to the previous serum with array? What is.
00:09:06.064 - 00:09:08.164, Speaker A: Yeah, what is the question?
00:09:14.104 - 00:09:56.414, Speaker C: This seems wrong to me. Wait a minute. I mean, even if you have. Even if you have non negative sectional curvature and convex, not just mean convex boundary, then just existence of array doesn't imply any splitting you can have. I don't like, you know, take for, you know, y equal to x squared and something in two and above the parabola, like.
00:10:00.554 - 00:10:01.334, Speaker A: Yeah.
00:10:02.874 - 00:10:09.054, Speaker C: That has rishiker, which is zero. Convex boundary. There is a. Ray doesn't split.
00:10:10.794 - 00:10:12.054, Speaker A: Yeah, you're right.
00:10:12.954 - 00:10:24.134, Speaker C: Something is wrong. There should be some more assumptions here that. It cannot be just like this. I'm not sure what they should be.
00:10:24.754 - 00:10:26.894, Speaker D: Is the main curvature reversed, though?
00:10:29.274 - 00:10:37.334, Speaker C: No, I'm pretty sure it just means convex. And what I was suggesting is just to take a convex boundary. Right. So that should be.
00:10:38.154 - 00:10:39.814, Speaker A: Yeah, okay. You're right.
00:10:40.834 - 00:10:44.534, Speaker C: I feel like there is some assumption missing, but I don't know what it is.
00:10:44.944 - 00:10:48.084, Speaker D: I feel like the mean is just in the other direction than what?
00:10:48.624 - 00:10:49.648, Speaker C: No, that.
00:10:49.776 - 00:10:52.312, Speaker B: Maybe it's compactness, like in the statement above.
00:10:52.368 - 00:10:52.584, Speaker A: Right?
00:10:52.624 - 00:11:06.296, Speaker B: You cannot make with a compact. You know, I think in your example, vital. It's really important that you have the graph, you know, a full function. Otherwise, if you insist, maybe on a compound. Well, but no, this thing is not compact.
00:11:06.360 - 00:11:07.560, Speaker C: Right? I mean, exactly.
00:11:07.592 - 00:11:09.244, Speaker B: That's what I'm saying. That's what I'm saying.
00:11:12.424 - 00:11:19.124, Speaker C: I'm not sure what. What one needs to assume. In addition, here, assumption is missing in this assumption. That's all.
00:11:20.184 - 00:11:21.404, Speaker A: Yeah, you're right.
00:11:22.464 - 00:11:25.584, Speaker C: I don't know what I mean.
00:11:25.624 - 00:11:31.004, Speaker A: Maybe it's. Yeah, thanks. So, probably it's compactness of the boundary.
00:11:32.504 - 00:11:37.124, Speaker C: I didn't think about boundary. Oh, compactness of the boundary.
00:11:37.584 - 00:11:52.300, Speaker A: Yeah. Yeah. I think I would need to. I need to check how the proof works again. So, one needs to use the maximum principle at one point and.
00:11:52.452 - 00:11:57.464, Speaker C: Yeah, okay. If there is a compactness of the boundary, then yes. Yeah, that should be true.
00:11:58.564 - 00:12:11.924, Speaker A: Yeah. So, I mean, it's definitely true if the boundary is compact. If the boundary is not compact, one. You're right. One needs one more assumption. Yeah, thanks. Thank you.
00:12:11.924 - 00:13:32.910, Speaker A: Okay, so let's continue. So, I mean, my main motivation here is I want to prove almost rigidity statements in the spirit as we have for, let's say, as we like, we can prove for remaining many folds by, by contradiction arguments. And so for that it's, yeah, so this is the motivation why we would like to generalize this theorems to a non smooth setting. But so this of course raises a lot of questions that I will try to address later in the talk. So, but let's first think, what would be the right setup here? So let me briefly recall again, synthetic Ritchie curvature bounds, or let me just jump that slide because I guess since last week, I mean, we all know now synthetic, we know the curvature dimension condition and the remaining curvature dimension condition. So I skipped that. But let me recall two things from the calculus of RCG spaces.
00:13:32.910 - 00:14:44.462, Speaker A: So first thing is, so we have an inner product, so we can pair to sobelev functions such that we can make sense of an inner product between their gradients. And the second thing is with this inner product we can also make sense of the distributional Laplace operator. And more precisely, we say that. So given an open subset in x, we say that f is in the domain of the distributional Laplace operator, if they exist, or rather functional, such that we have this identity here for all Lipschitz functions with compact support in omega. I apologize here, this is very, I mean, this is so I don't explain anything here, so I will not explain what's the random function. And I also will not say how this, how one should understand how this is understood. So especially, I think when we are in the n equal infinity case, one needs to assume something better here for the test functions.
00:14:44.462 - 00:15:59.628, Speaker A: But I hope this is sufficient as an explanation. And let me also say the first time when this was used in the RCD context, I guess was in a paper by Nicola. Okay, so then let me go to the next slide. So I also apologize for the crappy pictures. So first, unfortunately the RCD space is not quite the right setup, because I mean in general, if we have a remaining manifold with boundary that just has a lower bound on the mean curvature, this will not be an RcDKN space in general, or even an Rcdk infinity space. Because in general then I mean, this is not so hard to see. But one reason why this is the case is that the relative entropy is not semi convex.
00:15:59.628 - 00:17:02.106, Speaker A: And this is illustrated by this picture that you can see here. So if you have. So the picture is not quite what we want to look at, because here the boundary is even non convex, and so here the boundary is not real. Or let's say it's not really mean convex also, or okay, it has a lower mean curvature bound, but it's in the plane. So it's not quite what we should imagine. But you can see that the non convexity of the boundary forces geodesics to touch the boundary in a very bad way, or in a bad way for the entropy. So if you want to transport these two measures that you can see here onto each other, then the t midpoints in the Wasserstein space will be very concentrated in the region where the geodesics touch the boundary, and it in this region the entropy will explode.
00:17:02.106 - 00:18:08.300, Speaker A: So therefore you cannot expect entropy convexity. Therefore, what I suggest to consider is we look at pairs. The pair is first in RCD space X together with an open subset in our space. And for this open subset we want to define what it means to have synthetic lower mean curvature, what it means to have mean curvature bounded from below, or more precisely for the boundary. But I will not distinguish. So I will say that for omega, or also for the boundary of omega. Okay, so this is what we want to do in the following so how can we define a lower mean curvature bound for such subsets? So therefore let's consider such omega inside an RCD space, and we assume that the complement of this omega is not empty.
00:18:08.300 - 00:19:01.514, Speaker A: And if this is true, then we can define the distance function to the complement of omega. So this is given by this definition that you see here. And this distance function is always one dipschitz. So this is clear. And because of that it's also a Sobelef function. And what we also know, and in RcD setting, this I guess was first proved or used by Cavaletti and Mondino, is that this distance function to the boundary, if we restrict it to omega, is in the domain of the distributional Laplace operator. And in fact it the distributional applausion of this function is assigned rather measure.
00:19:01.514 - 00:20:01.436, Speaker A: Okay, so let's keep this in mind and let's go back to the smooth setting for a moment. So we have the following lemma. So let's consider a remaining manifold with non negative richie curvature and some omega that has smooth boundary. And then the following is true that the mean curvature of the boundary of this omega is bounded from below by h if and only if. The distributional applausion of the distance function to the complement of omega satisfies this estimate here in the sense of measures. And in particular you can see that if h is zero, then this exactly means that the distance function to the boundary is subharmonic. So this can be further generalized.
00:20:01.436 - 00:20:53.564, Speaker A: So I mean, non negativity of the Ricci curvature doesn't really play a role. So more generally, if the Ricci curvature is bound from below by k, a similar statement. This is true with a more complicated formula here. So where you can see we reuse this functions s that depend on two parameters, Kappa and he, and it's defined here with this generalized sine function. That is a solution of this ode that you see in the last line. Okay, but most of the time it will be sufficient for us to consider non negative Ricci curvature. So you don't need to memorize this formula.
00:20:53.564 - 00:21:46.954, Speaker A: Or I mean it will appear again, but I mean, it doesn't matter so much. But what we this lemma is now very interesting because I mean, we start here with a smooth, with omega that has a smooth boundary. But this estimate for the distributional applussion now makes sense without any smoothness assumption. And we saw a second before that we can our distance functions for such omegas in fact have a distributional Laplace operator. So it's well defined. So therefore it makes sense to to make the following definition, and it's perfectly meaningful in RCD context. Now.
00:21:46.954 - 00:23:22.234, Speaker A: So that is, we say omega has a subset in an RcDkN space where omega, where the complement is non empty, has mean curvature bounded from below by h if and only if this estimate for the distributional Laplacian holds in the sense of measures. This is our definition for mean curvature that we want to use in the following let me make a few comments. First comment is there is a different notion of mean curvature that I introduced some time ago using 1d localization and similarly also cavaletti Mondino in the context of lorentzian length spaces, and independently. Also with Annie great Bosch and Robert McKenna and Eric Wolger, we refined this notion that I gave before, and this uses 1D localization, and it's also very useful. But it requires a little bit of regularity for omega. And under this regularity one can prove that this notion of mean curve, hr wire and delocalization is in fact equivalent to this Laplace estimate. Okay.
00:23:22.234 - 00:24:13.824, Speaker A: Second remark is that so do we actually have such sets in RCD spaces? And the answer is yes. And there are many examples. But one particular example is given by Mondina Simola, who have the following very nice result. They prove that if we have a subset omega that has a locally minimizing parameter. So the parameter here is kind of the functional analytic notion of boundary. Then this omega has non negative mean curvature in the sense of the definition here. So they in fact exactly prove this estimate here.
00:24:13.824 - 00:25:44.222, Speaker A: But let me say this laplacian estimate. I mean, it already appears in the paper that I wrote together with Borchen, McCann and vulgar. So, I mean, that this is related to mean curvature was already known to us. And then the final remark here. So the relation to the intrinsic boundary of this notion is an open question, or more precisely, a conjecture that I think the first time appears in the paper by Boeing and Simola is that the intrinsic boundary, let's say, in the sense of Chile and the Philippis mean convect or, sorry, the intrinsic boundary in RcDKN space in the sense of chilean nephilipus, then the intrinsic boundary is mean convex. Okay, so these are the definition and remarks about it. And with this definition, I'm, now I was able, or I could prove this generalizations of these results that you saw before where.
00:25:44.222 - 00:25:59.224, Speaker A: Okay, here still we have this statement where I just assume we have array question. Yeah.
00:26:01.364 - 00:26:43.534, Speaker C: So maybe I missed that you said something. Let's say you are in a smooth manifold and then you have some domain such as the distance to the, but without, say, smoothness assumptions on this domain such as distance to the complementary is. Well, has this laplacian bound. Can you then say that it's in some sense? I mean, it doesn't have to be when smoosh, right?
00:26:52.074 - 00:27:24.454, Speaker A: Yeah. So, I mean, so first of all, this is, this is a condition, I just assume now for sets, of course. I mean, but the result of Mondino and Simona, before, I mean, tells you that if you have a set that is locally parameter minimizing that even in a remaining manifold fault, then it will be okay in the remaining many fold. Okay, yours move automatically.
00:27:25.794 - 00:27:52.496, Speaker C: I'm sort of asking about the converse. Well, maybe it's not. It should be true. Yeah, I was just wondering that if you can go the other way, that if you have this condition, then this in some sense should be sort of mean convex in reasonable sense, right? Maybe by smooth approximation. Yeah.
00:27:52.520 - 00:27:54.680, Speaker A: Okay. I mean, I'm not quite sure.
00:27:54.712 - 00:27:56.524, Speaker C: I'm not sure how to say it. Yes.
00:27:57.024 - 00:28:14.144, Speaker A: Yeah. What. What you mean then with mean convex? Because, I mean, I make this definition because if the omega is not smooth, then I cannot. So what is a priori mean convexity for non smooth sets?
00:28:15.324 - 00:28:31.836, Speaker C: Right. That's not completely clear, but it should be maybe approximatable by. If you. If you step inside, then it should become smooth and have actual mean convexity. But anyway.
00:28:31.860 - 00:28:44.754, Speaker A: Okay. Yes, sorry, thanks. I understand your question also. Yeah, you're right. There's this notion of. Of, let's say, mean convexity for very faults. I guess I mean.
00:28:47.974 - 00:28:50.114, Speaker C: Relatable to all those notions.
00:28:50.494 - 00:29:08.916, Speaker A: Yeah, it's a good question. So, I mean, maybe. Yeah, I didn't think about it, but it's a good point. Thanks. Okay. Yeah. So, this is the theorem I could prove.
00:29:08.916 - 00:29:27.164, Speaker A: So, I mean, in the second case, in the non compact case, as vitaly mentioned, one should probably. Or one should add another assumption, otherwise it's wrong. So, probably compactness of the boundary. Sounds reasonable.
00:29:27.244 - 00:29:34.884, Speaker C: Well, compactness will certainly be enough, right? I mean, if it. Because double it, then you'll have a line and.
00:29:36.744 - 00:29:45.688, Speaker A: No, no, but I mean, you cannot double it. I mean, it's. When you double it, mean convexity doesn't give you a space with non.
00:29:45.856 - 00:29:55.496, Speaker C: Oh, sorry. In RCD, I was thinking about the smooth case. Right. In smooth case, you can double. Right, I was talking about whatever you mentioned.
00:29:55.560 - 00:30:10.388, Speaker A: No, no, but even in the smooth case, if you double. If you double it. I mean, the double of a thing with just mean convex boundary is not a generalized. It's not a space.
00:30:10.436 - 00:30:13.076, Speaker C: Oh, right. Yeah. You need actual convexity. Right.
00:30:13.180 - 00:30:47.334, Speaker A: Yeah, yeah. Okay. Okay. But you can forget about the non compact case, because from now on, we only will consider compact spaces. So, only the first statement will be actually the one we keep in mind now. So, that means if we have mean convex boundary, omega is connected, the boundary is disconnected, then it splits off. Then omega splits with such an interval.
00:30:47.334 - 00:31:14.694, Speaker A: Okay? So then we come to the second part of the talk that is about stability. And so, this is kind of. Yeah, so, this was something I was trying to understand. So. And what I present here is, it's one. One way you can think about it. So, I mean, but maybe there are other ways you can do it.
00:31:14.694 - 00:31:44.494, Speaker A: But let me try to convince you why. The way I think about this might be the. The reasonable or the best way to think about it. So, let's look at constants here, k, h, d and n. And we define the space of all. And we define here x, k, and d is the space of all RCD spaces, RcDKN spaces with diameter bound from above by d. And we assume these spaces are normalized.
00:31:44.494 - 00:32:25.000, Speaker A: So they are always compact and have finite measures. So we can do this normalization. Then also we define the space y kndh. And this is now what I said before. The space of pairs where we have an RCD space and a subset omega inside of the rcd space that has mean curvature bound from below by h. Now we want to define a topology on this second family y. And this is, and the way how we do it is as follows.
00:32:25.000 - 00:33:30.994, Speaker A: So we pick two rcd, yeah, or we pick two rcd spaces, x zero and x one. And since they are compact, they always have finite grammar faustof distance to each other. So we fix this distance that we call it small d here, and then we always, and then we can find two r grommethaust of epsilon isometries that allow us to estimate the grommethorst of distance where r here is bigger than d. So this we can always do. And then in the second step, look at, we look at two functions, continuous functions, and for simplicity, non negative on x zero and x one. And what we define now is this quantity as psi here. So this is supposed to generalize kind of the uniform distance between two functions that live on the same space.
00:33:30.994 - 00:34:45.244, Speaker A: And to do that, we look at this modulus of the difference between f and g y where x is in x zero and y is in x one, such that x mapped with the grammar faustor of isometry. Psi has at most distance two r from y. And we can do the same thing for phi in the other direction. Now we define the uniform distance between f and g in the following. We take the maximum between these two quantities s psi and s phi. And then we take the infimum with respect to all pairs psi and phi that are two r Kronov Hausdorff isometries between our spaces where r here is bigger than the kromm of Halstorf distance between x zero and x one. So I hope this makes sense.
00:34:45.244 - 00:35:21.294, Speaker A: And so this is just the distance that matrizes. That is a distance for uniform convergence. And so, let me list some properties. So first it's symmetric. Then I mean the sum of this uniform or this d infinity distance with the kom of HaasDof distance is zero if and only if x zero and x one are isometric and f equals g. And we also have a triangle inequality. So this is not so difficult to prove.
00:35:22.234 - 00:35:34.564, Speaker E: So on the previous slide, in this last definition, I mean, so the f and the G have to be reversed in one of these things under the max, don't they? Or maybe I don't understand.
00:35:36.704 - 00:35:38.344, Speaker A: Yeah, yeah, yeah.
00:35:38.384 - 00:35:41.048, Speaker E: The phi and the psi are going in opposite directions, right?
00:35:41.216 - 00:35:41.992, Speaker A: Yeah.
00:35:42.168 - 00:36:21.134, Speaker E: Okay, what it means to be a two rhoma, or rather an r gram of Hausdorff isometry is that you're an isometry up to a distort up to an error r, and you cover the target up to an error R. Right. So the constraint on the s psi is somehow has to do with the sets points that you miss or something. I'm not sure. I mean, what is the restriction that the dx one of psi of Xy is less than two r? Is that a restriction if psi is an r? Grammar of isometry.
00:36:21.684 - 00:36:23.124, Speaker A: Yeah, thanks for the question.
00:36:23.244 - 00:36:29.304, Speaker E: Restriction. It's a restriction on the pairs of points that you look at. Okay, I see. All right, thanks.
00:36:30.604 - 00:36:53.872, Speaker A: Yeah, you. You already said it. So, I mean, the two are. Karma. Fausto vasometries are also called epsilon gromov horstophysometries. I mean, they, the images are epsilon nets in your target. And then, so this constraint here on the points x and y is actually not empty.
00:36:53.872 - 00:38:23.294, Speaker A: So, I mean, we find these points and so we can make sense of this supremo. Okay, other, more questions. Okay, so we have these properties, and now assume we have a sequence of Gromov Hausdorff space, sorry, of RcD spaces xi that converge to measured Gromov Hausdof sends to an RcDk in space X. And so this distance is now defined exactly in a way such that it's somehow. So we can formulate, how do I say? We can give a version, another version? But it's exactly the Gromov Ozella Ascali theorem that, that I guess many are familiar with. For instance, if we have a sequence of one Lipschitz functions on our space xi, then this sequence of functions f I sub converge with respect to this d infinity distance to a von Leibsch function f on the, on the limit space. So it's a reformulation of theorem.
00:38:24.074 - 00:38:26.094, Speaker E: I guess the sequence better be bounded.
00:38:28.194 - 00:38:37.494, Speaker A: Yes, I mean, that the spaces are bounded, and so the sequence is.
00:38:37.794 - 00:38:41.134, Speaker E: But you don't want a sequence of constants that goes to infinity. I mean.
00:38:42.454 - 00:38:54.154, Speaker A: Yes. Yeah, true.
00:38:58.454 - 00:39:03.474, Speaker D: I mean, it's usually stated as xi to a constant to a compact metric space.
00:39:04.774 - 00:39:07.518, Speaker A: Sorry, can you say that again?
00:39:07.606 - 00:39:11.404, Speaker D: It's usually stated as xi mapped to a compact metric space. Space.
00:39:13.264 - 00:39:14.804, Speaker A: I see, yeah.
00:39:16.824 - 00:39:19.044, Speaker D: So whatever compact set in r.
00:39:20.904 - 00:39:42.890, Speaker A: Yeah, yeah, thanks. Yeah. So, yeah, that is true, but it will not play a role for what comes next. So therefore, I didn't think about it. Sorry. So what we. So, because of this theorem, we call this now uniform convergence in the Arcella Asculi sense.
00:39:42.890 - 00:41:06.894, Speaker A: So this is how, at least how I call it in my paper. Maybe this is also like it was named before. And so the relation to this omegas that I showed you before is now the following. So, if we have such omega I's subsets in our spaces x I, such that the complements are non empty, then we find in the limit space x an open subset omega that has non empty complements, such that the distance function to the complement of omega I converge to the distance function to the complement of omega with respect to this d infinity distance, or with respect to uniform convergence in the sailor a sense. Ok, so this is good, because it already preserves this structure of pairs that we saw before. And next thing is also, what is the relation to mean curvature. So let me give you before that, let me introduce this new distance script d here.
00:41:06.894 - 00:41:57.858, Speaker A: So here you see, this should be actually the uniform distance between the distance functions. Sorry. So this script d is simply the sum of the measured Kommerf Hausdorff distance between x zero and x one, and the uniform distance between the distance function to the complements of omega zero and omega one. I hope that is clear. And then let's continue. So then we have the following theorem. So, if we have such a sequence of pairs xi omega I that converges to x comma omega with respect to this d distance, then the following is true.
00:41:57.858 - 00:42:49.386, Speaker A: If we know that this pair x I omega I was in this class of pairs, where the omega, sorry, where the omegas have a lower mean curvature bound, that is h, then also the limit satisfies this. So this is the important theorem here. So that mean curvature bounds are preserved under this convergence that I just defined. And the sketch of the proof is, I mean, the proof is actually fairly simple. So, I mean, this uniform convergence of the distance functions implies w twelve strong convergence of the distance functions. So, I will not explain what it is, but it's kind of. So we'll have convergence along the sequence.
00:42:49.386 - 00:43:59.934, Speaker A: And this, and the second point is that under this convergence, the inequality that we have for the Laplace operator is preserved. And also, let me say, this is not something that, let's say this is something that was observed before in other papers. I proved it independently from these other papers, but I think it appears in some form already in a paper by Ambrosio in Honda, and also in a paper, in the paper by Poi, Neva and Simola. So they also use this stability of the Laplacian, okay, but together with the stability and karma of Azela theorem, you have the following nice compactness theorem. So that is this space of pairs of RCD spaces together with subsets that have mean curvature bounds. This is compact with respect to this distance. D.
00:43:59.934 - 00:44:43.474, Speaker A: And again, I mean here, let me maybe write again. So this here is not, this here should be the uniform distance between the distance functions. No? Yeah. Okay. Please interrupt me if I'm over time. So this is now nice, because we have a compactness result, which brings us pretty close to what we are actually looking for. That is an almost rigidity statement.
00:44:43.474 - 00:45:36.756, Speaker A: So the problem now is the following, that this notion, this notion of convergence is very nice. It preserves the mean curvature and everything, and it has a compactness statement. But what happens is the following. Maybe I didn't have the right picture. Do I have the right picture? No. Yeah. So what happens is, so, before I come to the next definition, is that this in the limit, even if you start with omega I's that are connected in the limit, it might happen that the omega is not connected anymore.
00:45:36.756 - 00:46:29.282, Speaker A: And if you go back to this theorems that I showed you, the rigidity theorems, connectedness of omega was crucial because we want to apply a maximum principle. Without connectness, the theorems are simply false. So therefore, to have an almost rigidity statement, and using the rigidity statements you saw before, we need to add in a condition that guarantees that we do not lose connectedness in the limit. And the definition I propose is the following. So this is what is already appears in the literature, and it's called c uniformity. So, if we have a geodesic metric space x and omega, that is a subset, we call it c uniform. If the following is true.
00:46:29.282 - 00:47:34.114, Speaker A: And just let me explain it. So it means that between any two points in our set omega, we, so x and y, we refined a rectifiable curve that connects these two points. But also this curve satisfies this estimates here, which essentially means that not just the curve is in omega, but this blue banana that you see here is in omega quantitative way. I mean, this is given in a quantitative way because we have this constant c here. And so this is a nice condition, because first, I mean, it's easy to check. I mean, we can see with our eyes if something is c uniform. And the second thing is also that it, it's preserved under this.
00:47:34.114 - 00:48:27.654, Speaker A: It's preserved under this convergence that I defined. So that is, if we have a sequence xi omega I, that converges to x omega, then the limit and the omega is c uniform. Then also the limit is c uniform. And this is now sufficient to, to state and to prove a theorem that we are looking for. That is an almost rigidity theorem under lower mean curvature bounds, and it's in the one such theorem is this here. So we have, let's consider, let's have constants d, c small d n, and so on. And then, for all epsilon, there exists the data such that the following is true.
00:48:27.654 - 00:50:15.324, Speaker A: If we have a pair x omega in this class that almost satisfies in this class, y almost satisfying the curvature bounds in the rigidity case, and such that omega is c uniform and also omega. The complement of omega is a disjoint union of sets where the number of components is k bigger or equal than two. And we also assume that they all have a definite distance from each other. Then the following is true that, first of all, the k is actually just two. And second, we find a space or a pair set omega infinity such that, and another space y such that the omega infinity here is isometric to this product here, where we here have isometry not with respect to the distance on x, but with respect to the induced intrinsic distance in each of these sets. And x omega is infinite d distance here to the pair z omega infinity. And that this is true, you can now easily check by a contradiction argument, because we have all the ingredients that allow us to apply the rigidity theorems that you saw before.
00:50:15.324 - 00:50:35.014, Speaker A: Okay, so I'm almost done. Let me give you one more point that I want to mention here. So this is this picture here. So we have this motion.
00:50:35.434 - 00:50:43.944, Speaker E: So we're running over time, Christian, and we have to move to the other building for Nicolas colloquium. So can you wrap it up in about a minute or less, do you think?
00:50:44.284 - 00:51:27.602, Speaker A: Okay, then, yeah. In less than a minute. Yes. So the problem here is this condition c uniformity, if you think about it, I mean, it's actually too strong. I mean, way too strong, because it rules out this example that you see here. This example is perfectly fine with respect to our convergence notion here, but it's not allowed under c uniformity. And something I try to do in the future is to replace the sea uniformity by a weaker condition or by a different condition that also would allow examples like this here.
00:51:27.602 - 00:51:37.314, Speaker A: Okay. And that's everything what I wanted to say. And thanks. And, yeah, thanks for all the questions.
