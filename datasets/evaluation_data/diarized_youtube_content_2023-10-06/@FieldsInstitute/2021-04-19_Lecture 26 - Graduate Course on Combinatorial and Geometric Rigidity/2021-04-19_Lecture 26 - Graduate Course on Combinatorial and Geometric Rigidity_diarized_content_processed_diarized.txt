00:00:00.160 - 00:00:42.286, Speaker A: We spent a lot of time in the course thinking about the generic situation. And a natural way to weaken the idea of generic is to allow your framework to exhibit some symmetry. So first of all, we have a graph that has some, some non trivial automorphism group, and then the framework realizes the graph in a symmetric way. But we're always going to assume basically that it's, the framework is as generic as possible given that it has some symmetry. And the symmetries will be simple, simple things. It'll be like reflection or rotation symmetry for some finite, or the cyclic rotation group. And so you'd be able to very easily see the particular symmetry and then you'll be as generic as possible within that class.
00:00:42.286 - 00:01:43.734, Speaker A: So if you want to get technical, we will basically think about symmetric as up to some field extension of the rationals rather than the rationals. And so you'll adjoin the coordinates of the matrices that define the symmetry group to q and then have generic over for that. But we'll just say it's as generic as possible given that it is has some symmetry as we go. Okay, so I think on Wednesday Daniel will do much more general cases than this, but I'm going to basically, and a lot of what I say won't change, but I just, for simplicity, I'm going to suppose that my group is just cyclic and I don't need to worry too much. And the particular theorems, I'm going to say I've sort of been basically focused on, I think it's reflection symmetry group. So it's an order two cyclic group and maybe small rotation symmetry as well. Okay, so we start with our graph as usual, g equals ve and we have a, want to define the automorphism group.
00:01:43.734 - 00:02:40.702, Speaker A: So an automorphism of a graph is a permutation PI from the vertex set to the vertex set, which respects edges. So if I j is an edge of of e, then PI j should be an edge of e for this permutation. And the group of all of the automorphisms is the automorphism group or g. And so our graph is gamma symmetric, so symmetric with respect to this gamma if there exists a group action theta, which maps the group into the automorphism group. So we map our abstract group into the automorphism group of this graph. And I might forget to say this, but this sentence is important. So most of the time I would like to assume that our actions are nice, so they're free on the vertices, and quite often it is easier to assume them to be free on the edges as well.
00:02:40.702 - 00:03:26.290, Speaker A: But what I want, is there to be no vertex that's fixed by my automorphism? No vertex fixed by theta. So for example, later on we're going to really think of our groups instead of as abstract objects. We're going to think of them as geometric objects. So we have say a reflection in the plane. I really don't want any vertex lie on the reflection, so I want a vertex and it's copy the other, the other vertex in the orbit for v. We don't want to have one single vertex that maps to itself because this is going to throw off ours roughly the degrees of freedom kind of counting that you have in rigidity. We're going to set up matrices, rigidity matrices, and it will throw off the number of rows and columns you want.
00:03:26.290 - 00:04:46.942, Speaker A: I do allow fixed edges like, like this, but because there's no fixed vertices, obviously I can't have a fixed edge like that. Okay, okay, so suppose our group, and this is how I get the sort of geometric group. We think of our group as a subgroup of the orthogonal group by supposing it acts on Rd via some homomorphism tau, which sends the group into the orthogonal subgroup and into the orthogonal group. Okay, and then, so being formal, but briefly, basically because I'm going to drop the bracketed part here in a moment, but our framework GP, we call it gamma symmetric, so it's symmetric with respect to this, this group gamma if the following equation happens, and obviously it depends on our choice of theta and tau. So you could say it's gamma symmetric with respect to theta and Taurus. If so, we look at all elements gamma in our group and all vertices I, we take the point p of I and apply tau, which was our homomorphism, and gamma our abstract elements. So tau of gamma is basically think of as a matrix.
00:04:46.942 - 00:05:42.564, Speaker A: Apply that matrix to PI, you get the same as if you apply the group elements first. So I apply, I take my graph, my vertex I in the graph and apply the automorphism, the group action feature of the group element to I and then realize that via the map. So the sort of, the way the order I apply these symmetry things doesn't matter in some sense. Okay, so this is, this is my nice, well it's not my definition, but this is the nice definition of gamma symmetric. And so we're going to think about when gamma symmetric frameworks are rigid and when they're not. But there's sort of a distinction into the literature that we split now into two parts. There's sort of two natural rigidity questions you can ask about a gamma symmetric framework.
00:05:42.564 - 00:06:20.154, Speaker A: So they are the incidental symmetry question and the force symmetry question. So the incidental symmetry question is just the usual rigidity question we ask. So I'll give you a framework. Is it infinitesimally rigid? Is it rigid? Is it globally rigid even? But in the incidental symmetry case, it just happens that the GP I give you and ask you the question rigidity question about happens to be gamma symmetric. In the fourth symmetric context, it's different. So now I'm not asking about rigidity anymore. I'm asking about forced symmetric rigidity.
00:06:20.154 - 00:06:57.610, Speaker A: So here I'm given a gamma symmetric frame of GP, and it has to remain gamma symmetric, so I'm not allowed to move it in a way that breaks the symmetry. So I'm asking, does there exist a motion which preserves the symmetry? And if there's no symmetry preserving motion, then I say it's for symmetrically rigid. So this incidental versus forced is sort of a big distinction. And people tend to study one or the, or the other. And at least when you're doing individual results, you have to focus on which one you're interested in. And then, as I say. So Daniel on Wednesday will talk a lot about force symmetry.
00:06:57.610 - 00:07:28.304, Speaker A: So I will talk some about both of them, but I will not say as much about forced as I would because I am deferring to Daniel's lecture for some of that. I have to work out how to get to the chat because I can see there's a question. Does this basically say that every embedding of the graph equivalent to an automorphism obtained by theta can be obtained by a trivial motion? Does Watt basically say this?
00:07:29.364 - 00:07:32.664, Speaker B: Oh, sorry. So I was asking about that equation up there.
00:07:34.524 - 00:07:38.184, Speaker A: So embedding, you mean the p's? Yeah. Okay.
00:07:39.164 - 00:08:05.064, Speaker B: Yeah. So I was just wondering. So the right side basically says that you are relabeling the vertices and then embedding that. And then the left side says that you're actually taking the original embedding? And then is it saying that there's a trivial motion such that the two like for every relabeling of the vertices, and embedding of that you can get it from a trivial motion.
00:08:06.054 - 00:09:14.760, Speaker A: So if we look over here. So on the right hand side, imagine my group is the cyclic group on four elements, and my graph has four vertices, 1234. Then on the right hand side, I'm taking some group element in c four which commutes among these numbers the vertices one, two, three and four, and then all of them, I apply the realizations. So I may have moved I may have, instead of having p one, I may have p of two, etcetera, but I've just sort of relabeled them so that the p applies to them in a different order. Over here I have some matrix, some, let's say we're in the plane. So now it's a cosine little two by two rotation matrix that we've fourfold symmetry and whatever the element is. So if it's, say again, it's vertex one, then I put vertex one at some position, say at one one, and then I apply the matrix to that vector and move it to somewhere.
00:09:14.760 - 00:09:27.404, Speaker A: And I do this for all of the things. And so it should be that picking the same gammas and the same eyes gives me the same result in these two things.
00:09:28.704 - 00:09:30.160, Speaker B: Okay, I see. Thanks.
00:09:30.232 - 00:09:31.080, Speaker A: That makes sense.
00:09:31.192 - 00:09:33.848, Speaker C: Yeah, I think the answer then is yes.
00:09:33.976 - 00:09:34.384, Speaker A: Right?
00:09:34.464 - 00:09:41.324, Speaker C: I mean, there is a rigid motion that achieves the relabeling.
00:09:43.184 - 00:09:51.672, Speaker A: Yes. So there is a particular rotation. Yeah.
00:09:51.848 - 00:09:52.232, Speaker C: Cool.
00:09:52.288 - 00:09:54.484, Speaker B: Okay. Yeah, that's what I was wondering. Thanks.
00:10:00.904 - 00:10:33.870, Speaker A: Okay, so let's focus on 2d for this next sentence. So we've got this incidental question and the force question. So we're going to think of the incidental symmetry question because we're going to think of 2d was think of having a minimally rigid graph on the plane. We know. In other words, we could think of it as a two free tight graph. Which of these graphs, okay, it has to be gamma. It has to have the right automorphism group.
00:10:33.870 - 00:11:24.126, Speaker A: So it's not all two free tight graphs, it's just symmetric ones. Which of them remain minimally rigid when instead of realizing this graph of non trivial automorphism group in a symmetric way rather than a generic way, which ones remain minimally rigid? And so this gamma generic, I was rambling about this at the start. This is what I mean. So it satisfies the symmetry group gamma, but otherwise it's as generic as possible. Okay, so let me do an example to show you what I mean by the force symmetry. That's the one that's a bit harder to get your head around, I think, if you haven't seen it before. Okay, so I'm going to take cyclic group on two elements and I'm going to interpret it as the reflection in the y axis.
00:11:24.126 - 00:12:24.556, Speaker A: This is a two dimensional example as usual. And so I've realized a four cycle in this symmetric way where u and u are in the same orbit and v and v dash are in the same orbit. So if you like, I have two fixed edges, four vertices and two non fixed edges, the edges, okay, they change the orientation by the symmetry, but it doesn't change, change their locations. So we know that generically, the four cycle has a four dimensional space of infinitesimal motions. It has two translations, one rotation and one non trivial motion. See that? The particular symmetric realization that I've shown has the same for four dimensional space of infinitesimal motions. If I apply a translation orthogonal to the reflection line, then clearly I break the symmetry, because u gets closer to here, but u gets further away.
00:12:24.556 - 00:13:01.640, Speaker A: So rather than becoming a nice symmetric pair like that, it becomes this one and one over here. So it's not symmetric anymore. Similarly, if I apply the rotation and I'm sort of imagining the origin is there where I'm going to do my rotation around. If I apply a rotation, then u heads this way and u heads this way, so again I'm going to break the reflection symmetry. So since I'm interested in force symmetric rigidity, both those isometries destroy the symmetry. They're anti symmetric in a sense. They don't preserve the symmetry, so they're not allowed.
00:13:01.640 - 00:13:36.014, Speaker A: So two of my four dimensional space are non symmetric and not allowed. This translation is fine. This is a trivial motion. So this time, v, u, u dash and v all head up by the the same amount. So it's still true that u and u are nice and symmetric in the same orbit, and v and v dash are. So this vertical translation is a symmetric trivial motion. Okay, so I've partitioned the three isometries into the two that destroy the symmetry and one that preserves the symmetry.
00:13:36.014 - 00:14:36.694, Speaker A: So there's my free isometries, and then there's the one non trivial motion of the four cycle that you can see, but that non trivial motion will break the symmetry, right? So imagine what happens over here. You can just move everything down and keep these the same. If you look over here, what you can do then you think you can quite relatively easily convince yourself that the only way to move this in a non trivial, not an isometry way is to break the symmetry. So sort of, I guess you can have a sort of any outy motion, say, but it's still, it's going to break the symmetry. So what did we say? That we had a four dimensional space of motions. Two of them destroy the symmetry, so they get discarded. One of them is trivial.
00:14:36.694 - 00:15:28.984, Speaker A: So there's one sort of non trivial if you forget the symmetry motion, but that non trivial motion destroys the symmetry as well. So the only way to move this, this framework which preserves the reflection symmetry, is a trivial motion. And hence this is for symmetrically rigid, even though it's very clearly not rigid in general. It's just if I didn't have the symmetry constraint, then it wouldn't move. Okay, so let's, let's go back to some definitions. So what do we mean by these motions? I tried to give you some intuition there, but an infinitesimal, infinitesimal motion u of a gamma symmetric framework. The infinitesimal motion is called gamma symmetric if the velocity vectors exhibit the same symmetry.
00:15:28.984 - 00:16:26.528, Speaker A: So again, we look at every vertex and we look at every group element. And what we want is if we, we change the indices for the infinitesimal motion, it's the same thing as applying the, the matrix to the vector of the infinitesimal motion. So we change the indices in the vertices, we're applying the group elements to abstractly, or we apply the representations of the group as the orthogonal group submatrices to each of the velocity vectors themselves. And we should get the same thing. So it really does look like it's moving the framework symmetrically where you put the little velocity vector arrows. And we're going to say that the framework is for symmetrically infinitesimally rigid if every gamma symmetric infinitesimal motion is trivial. Okay, so like in our example, we only had one gamma symmetric infinitesimal motion.
00:16:26.528 - 00:17:11.504, Speaker A: It was the translation along the reflection line, so it was a trivial motion. Okay, so there'll be much more on this on Wednesday, but we're going to understand for symmetric rigidity by looking at representatives of the vertex orbit. So here's our picture again. And so u and u are in the same orbit, so we're going to denote those by one vertex. Sorry, there shouldn't be a dot there. V and v dash are in the same orbit by the symmetry, so they're one vertex, there is an edge between u and v and an edge between u and v dash, so that's the same edge orbit. So there's one edge between them here and there's also an edge between u and u.
00:17:11.504 - 00:17:47.366, Speaker A: So we have to denote that somehow we choose to add a loop on u and there's an edge between v and v, which we choose to add as a loop on v. So on the left we have the covering graph, the thing that we realize as a framework with the symmetry, and on the right we have a quotient graph. Okay, so going from left to right that seemed relatively straightforward. We really were just quotienting out by the symmetry. So we took our vertex set and quotient. It by the size of the group. We put things in the same orbit together and combined them.
00:17:47.366 - 00:18:14.564, Speaker A: And then the edges sort of naturally went across as either edges between different orbits where we could just have nice simple edges. It could have been that we had, it was possible that we had a uu and a v and we had a uv and a U v, say. So there would be, when u and u come together, it could have had two edges to, to v. That's okay. And we had u to u. So there was a loop on u. That's all.
00:18:14.564 - 00:18:59.708, Speaker A: Okay. But going back the other way is not yet well defined. So it's not yet clear how to, how to go from this thing up, because the smaller quotient graph, first of all, it needs to include some, some information from the group. Okay, so how are we going to do that? So to make the, the map from the quotient graph to the covering graph well defined, we're going to first of all, orient the edges of the quotient graph and then also label them with elements of a group. So in particular, in this example that we want u and v, the group was ordered two. So actually the directions in this case don't matter. So that the directions are going to determine as we're going to do.
00:18:59.708 - 00:19:34.958, Speaker A: Walker, we're going to walk around the graph, and as we traverse the graph, if we follow an edge from its tail to its head, follow the arrow in the right direction, then we're going to take the group element we put on the edge. If we go against the direction, we take the inverse of the group element. So since the reflection group is c two, so that the order two group, the inversion doesn't really do anything. Everything's its own inverse. So I haven't labeled the, I haven't directed the edges here. But you can, it would just be any arbitrary orientation you like. And in fact, you can typically put any orientation you like.
00:19:34.958 - 00:20:30.006, Speaker A: And then you just have to be careful which, whether you take, as you go this way, you take the group element, which was the identity. But as you go against the direction, you would take the inverse of the inverse of the identity as the identity of the inverse of minus one is minus one, if you think of the group like this as multiplicative. Okay, so now what do we do? What does it mean to label the group element? So when I label this one here, I've labeled with the, the identity. What that means is in the covering graph. So I put my u, two copies of u down, two copies of v down, and that's clear. As I go from here to here, then u will go to v. So if I take this as a representative of that orbit and this is the copy, and this is a representative of that orbit and this is the copy, then the representatives are joined together by the identity edge and so is the copy.
00:20:30.006 - 00:21:14.936, Speaker A: Whereas had it been a minus one, let me do a different color. Had the gain on the simple edge been a minus one, I'd have gone from representative to the other copy, and I'd have gone from representative to the other copy. So the choice of the one or the minus one would have chosen whether I picked the green pair of edges here or the red pair of edges here. Okay? And similarly, when I take a loop, so say here at vertex u, I've got this loop with a gain of minus one. That means I go from the representative u to the copy and I go. And it, had it been a one, instead, again, let's change color. Had it been a one instead, I'd have gone from u to itself.
00:21:14.936 - 00:22:04.742, Speaker A: I'd have put a loop. So hence, because we want the covering graph to think about the rigidity properties, we're really not going to ever have loops that have a gain that isn't, that is the identity, we're always going to take the non identity, some non identity element for the loops. Okay, so as I say, directions didn't matter here, but they will matter more generally. It's only because it was an order two group that we didn't have to worry about the directions. So you can imagine, say, if you had a fourfold symmetry, then the direction would tell you where to send the edge in the covering graph. Okay? So let's be slightly more formal. So the quotient graph, the gamma symmetric graph, which is the first thing we did.
00:22:04.742 - 00:22:47.714, Speaker A: So it's a multi graph you obtained by quotienting out the group from the vertex set. So you get the set of vertex orbits and quoting out the group from the edge and get the set of edge orbits. Okay? So then when we have this quotient graph, we can form the gamma gain graph. It's called often in the literature, gain graphs and voltage graphs and group labeled quotient graphs. So there's several names, but I'm sticking to gain graph because it's the most common in the rigidity literature. But these objects are more general than rigidity, so they have other names as well. So the gamma gain graph of g is a pair, g zero psi.
00:22:47.714 - 00:24:00.154, Speaker A: So g zero is the quotient graph. So v zero is the vertex orbits and e zero is the edge orbits the quotient graph with respect to gamma, obviously. And we assign some orientation to the edges and then we define a gain function or gain assignment psi which assigns a group element to every edge. Okay, so then if there's an edge orbit gamma e which connects two vertices gamma I to gamma j. So we can write that as group element gamma times I and say, and then gamma composed with alpha times j for some unique element of the group. So there's a unique way to do this and we're going to orient gamma E from gamma I to gamma j and assign it this unique group element alpha as the gain we put on that edge. Okay, so then pause for just a moment.
00:24:00.344 - 00:24:19.594, Speaker C: Yeah, still trying to absorb the definition and write it down at the same time. So how do we label, I'm sorry, I missed how we labeled the edges. We use the.
00:24:20.534 - 00:24:23.954, Speaker A: So it's easier in the pictures. I think so.
00:24:26.714 - 00:24:33.374, Speaker C: Yeah, I got the picture part. I'm still trying to, I'm just trying to match it up with the notation.
00:24:36.034 - 00:24:46.334, Speaker A: So I guess you're meaning this bit. And I did try to be a bit brief rather than go into a great amount of detail. So maybe I was too quick in this. So.
00:24:49.594 - 00:24:56.194, Speaker C: You just fix a unique alpha.
00:24:57.694 - 00:25:04.154, Speaker A: So maybe it's, maybe it's easier to do something I don't usually like to do and have edges done like that.
00:25:05.734 - 00:25:06.474, Speaker C: Ah.
00:25:08.294 - 00:25:11.754, Speaker A: So this is the first vertex in the edge and this is the second.
00:25:12.134 - 00:25:13.074, Speaker C: I see.
00:25:17.434 - 00:25:19.674, Speaker A: So I like ij rather than braces.
00:25:19.754 - 00:25:39.734, Speaker C: I see, I see you'd like to put them next to each other but which I agree makes sense. But here maybe just in this one instance. Uh huh. Uh, yes. That clears up maybe my confusion. You orient it from gamma I to gamma j and assign it this gain. Okay.
00:25:39.734 - 00:25:43.334, Speaker C: Right, thanks.
00:25:44.594 - 00:26:44.338, Speaker A: Is anyone else want to ask any questions at this point? Okay, so this is kind of technical, this next part, but it's going to be, it's going to be important in the combinatorial characterizations. So the aim as usual in two dimensional rigidity is we would like to say something like a lamanthum. We'd like to say rigid if and only if two free tight, say minimally rigid if an, only if two free tight. And we'd like to say similar things with symmetry. It's going to depend not just on counting the number of edges and vertices in one of these quotient graphs, but also on the particular sparsity conditions of the gain graphs. And so the group elements are going to have some relevance and so that's that's what's happening here. So for any closed walk c.
00:26:44.338 - 00:27:31.244, Speaker A: So a walk, you start at a vertex, traverse edges, vertices, edges, vertices, you can repeat things if you like. And you get back to the, it's closed, so you get back to the same vertex. So I start at v one and finish at v one. A closed walk in one of these gain graphs, g zero of psi, so v one up to vk are all elements of v zero and e one up to ek in e zero. This closed walk is called balanced. If psi of the walk is equal to the identity. So what's psi of the walk? Well, you take for every edge on the walk, for e one, e two up to e k, you take the gain element we labeled on that edge, and you take the product of all of those things, except we have to pay attention to the direction.
00:27:31.244 - 00:28:26.826, Speaker A: So if the, if we follow the edge ei directed from vi to vi plus one, and we're following the nice direction of it, sine EI is just one. So you just ignore it, you just take the group element, but if you go against the direction sine EI is minus one, and so you take the inverse in the group. And so you, for example, you might have a, let's just do a very simple case where our walk is a cycle and it's a directed cycle and you have the identity, identity, identity. Then here it's just the product of the identity with itself four times and you do get one. And this is nicely balanced. But if just one of these was a minus one instead. So I'm still thinking in my simple version of multiplicative c two, then here you'd have the product of one, one, one and minus one, and you get minus one and it would not be equal to one, it would be equal to minus one.
00:28:26.826 - 00:29:17.786, Speaker A: So we would say it was unbalanced if it was a minus one here. And if you have a, you could have obviously a more complicated cycle or more complicated walk that had lots and lots of, in a more complicated group. But it still all boils down to the same thing. You take the product of the gains respecting the direction and just so taking inverse as if it's the wrong direction. Okay, so that's, that's a closed walk. Now take a subset of edges, and for that subset of edges f, take the induced subgraph of the the gain graph of g zero and say that that subgraph, that subset of edges is balanced. If every closed walk in this subgraph induced by the edge set balanced.
00:29:17.786 - 00:29:54.614, Speaker A: So if there's a if there's even one walk that's unbalanced, the set is called unbalanced. Okay, so this balanced condition is going to be the difference between counts, that we just ignore the symmetry and counts where the symmetry matters in some sense. So our gain graph g, zero psi is going to be triple. So before we had it's kl type and Kl sparse. Now we've got Akkk L m. So it's Klm gain sparse. And we're always going to have that.
00:29:54.614 - 00:30:48.714, Speaker A: Kl and m are all natural numbers. Or, well, k should certainly not be zero and m should be at least zero and l should be at least m. But for the definition of the graphs that's not so important. But for the particular rigidity thing we're interested in, that's always, certainly the case that zero is at most m is at most l. So the Klm gain sparse means that if you take a balanced subset of your edge set, then that number of edges in that set is at most k times the number of vertices induced by f minus l. So the, obviously the k is just going to be the scalar. In both cases the l is the constant for balanced and then the second, sorry, the third letter m is the constant for unbalanced.
00:30:48.714 - 00:31:37.366, Speaker A: So the balanced and unbalanced difference changes us from an l to an m. So for example, if l and m are the same, so it's l is equal to m, then Klm gain sparse is exactly KL sparse because the gains have, if l and m are the same, then it's just for all subgraphs and we're not doing anything other than what we did for kl sparse graphs. But when l and m differ, then we have to check which of our subsets of edges are balanced and which armed. Okay, so now we take our KLM gain sparse condition and we can form the usual klm gain tight condition by saying your KLM gains sparse. And the top count works. So the top count saying e zero is equal to kv, zero minus m. So we always take the last one here.
00:31:37.366 - 00:32:21.282, Speaker A: Remember the, the m is at most l condition here. Okay, so here it turns out that in the plane very often the nice class of gain tight graphs you want is two three, one. So remember I said if L equaled m then so here that would be two three tight. And that's just the natural non symmetric case. But the symmetry very often is going to give us that the m should be a one. And so our balance subgraphs should be two three sparse and our unbalanced subgraphs where m is one should be two one spars. Okay, so this is the natural class of graphs to think about.
00:32:21.282 - 00:32:54.316, Speaker A: And I think this is going to be the class that Daniel goes into a bit of detail on Wednesday. But let me just show you a few examples. Okay, so. Oh, I'm nowhere near halfway through, so I'm going slower than usual again. So a single loop, single vertex with a loop an identity element on the gain is not in this class. Why? Because it's the identity. So it's balanced, which means we have to be two free sparse, but we have one edge which is greater than two times one minus three.
00:32:54.316 - 00:33:34.292, Speaker A: So that's just not allowed. So this is not two free, one gain sparse or tight, because I put the identity on the loop. If I put the non identity element on the loop, then I'm okay. And I can form even a bigger subgraph like this one on two vertices with two loops and one normal edge like we had earlier and here every cycle, certainly because the cycles are just the two loops which are both definitely unbalanced because they have the non identity element. And any more complicated walk you want to, to do, I mean, there's not much option here. All of the, the closed walks are unbalanced. So we can nicely check the, the condition here.
00:33:34.292 - 00:33:42.060, Speaker A: And this one is two free, one gain tied. And obviously the, the, why is the.
00:33:42.092 - 00:33:46.916, Speaker C: First one not, um, two three gain tight? It looks.
00:33:46.980 - 00:33:51.244, Speaker A: Oh my. So it's the first one it has, it's one loop.
00:33:51.404 - 00:33:51.764, Speaker C: Yeah.
00:33:51.804 - 00:34:05.548, Speaker A: And one vertex and one loop. So it's got e is one and it's got 2 volts. And we want to compare that to the two v minus free condition and obviously one. Wait, one is greater than minus three.
00:34:05.596 - 00:34:07.500, Speaker C: It's not always balanced.
00:34:07.692 - 00:34:25.714, Speaker A: Yes, because it's the identity. Here I'm in the balanced case, so I'm in the l and then my l is a free. So that's the, that's really a distinction between balanced and unbalanced sets. To know whether to check the two v minus three or the two v minus one condition.
00:34:26.414 - 00:34:27.714, Speaker C: Got it. Okay.
00:34:29.054 - 00:34:44.834, Speaker A: Yeah, that's important. So thanks for asking. Hopefully that's, that's clear to everybody. Okay. And then this last one. So here you, I'm showing you are allowed parallel edges. You can have multi graphs as well as looped simple graphs.
00:34:44.834 - 00:35:21.114, Speaker A: So here, any closed walk that starts and ends at the same vertex will go along both of the edges. So it will be a one times minus one, so it will be unbalanced. And so here you have two edges, but we're comparing it to two times two minus one, which is free. So this is sparse, but it's not tight. It doesn't have the 2 volts minus one count to be two three one gain type, but it is sparse. So you can have parallel edges and loops in your subgraph. There's a sort of distinction.
00:35:21.114 - 00:35:55.054, Speaker A: I shouldn't go into this, I don't think I've got time. But you can also think about triple edges. And there you have to be slightly careful because it's fine triple edge, as long as it's unbalanced, as long as the whole thing is unbalanced, and any pair of edges within it is unbalanced, it satisfies the count. But I've been talking mostly about c two, which is an order two group. So I would really have to have the same gain on two of the edges. So I can't actually make it possible for the green pair to be, to be unbalanced. That must be balanced.
00:35:55.054 - 00:36:51.336, Speaker A: And the point is, it would give a pair of parallel edges in the covering graph. So that's bad. Okay, so these klm gain sparse graphs don't always, but they very often give us nice matriids. So when l and m are the same, we just get down to the one, one sparse graph. So the cycle matriid, or they're both zero and it's the bicycle matrix, the bicycle matriid. But the case where the l and m are different, the only possible case in 1D, because we always want to have this m at most two k minus one. To be a nice matriarch, you get this signed graph frame matriarch kind of object, and you can build a load of the naturally occurring two dimensional matrix from these.
00:36:51.336 - 00:37:27.524, Speaker A: So for example, you could think about frameworks on the sphere with symmetry, and one such group is the inversion symmetry group, where you get, this is the KLM. In the KLM gain sparse, or tie to, I've just put after an m to mean the matriid induced by this sparsity counting condition. I'm being vague. So you can have symmetry on the sphere. If you go to 232, you can have periodic fixed lattice rigidity. Periodic things will be mentioned a bit on Friday. If you go to 231, then I think this is going to be the focus of what Daniel talks about on Wednesday.
00:37:27.524 - 00:38:34.318, Speaker A: Maybe, maybe he'll focus a bit more on this case where we have to go into the sort of dihedral groups, but this one is an exception because it's not in general a matride. So there's something kind of strange happens there, that there are rigidity characterizations for this with this count, but they have to have additional conditions on top of being just the sparse count conditions, the gain sparse conditions to give a matroid and to have a chance of characterizing a rigidity condition. Okay, so I'm speeding up a little bit because I want to make sure I get to the right point to set up Daniel's talk, hopefully nicely. But I want to focus now on the incidental symmetry problem rather than the force problem. So Daniel will talk about the force problem and tell you some theorems. Next time I guess you might do some proofs. So again, we had two problems, a force and incidental problem.
00:38:34.318 - 00:39:17.494, Speaker A: But even within incidental, there are really two subproblems. So the first is to characterize minimally rigid gamma symmetric, but otherwise generic frameworks in r two for various groups. That's a sensible problem. There's also what sounds to be the same problem at first, but you quickly see it's not, is to characterize infinitesimal rigidity for gamma symmetric frameworks in r two for various groups. So problem one is to characterize minimal rigidity, and problem two is to characterize infinitesimal rigidity. So usually if you characterize minimal rigidity, you can then just add some edges in and you get all the infinitesimally rigid things. But the symmetry condition is going to stop that always being true.
00:39:17.494 - 00:39:54.464, Speaker A: So the, the point being that sometimes these minimally rigid ones don't exist. So sometimes you, the, there's an incompatibility between the count you need for minimal rigidity and the order of the group. So it's probably an example is better than me saying words to explain this. So here's the same reflection group. It's reflection in the plane through this line. So here's an example of a, a rigid object in the plane, a rigid graph with reflection symmetry. So notice that this has, this is one more edge than the two free type count.
00:39:54.464 - 00:40:51.324, Speaker A: So this has one too many edges to have a chance of being minimally rigid, there must be a dependence in the rows of the rigidity matrix. There must be one edge I can take away and still have the full rank. But the problem is, edges are in pairs in an order two group, because there's no fixed edge here. So if I wanted to say take this one away, then if I just do that, then now I'm not symmetric anymore. So to maintain the fact I need to be a CS reflection symmetric graph, I have to also take that one away and now I've gone from one too many to one too few to have a chance of being rigid. So it wasn't possible for that graph to have a spanning gamma symmetric minimally rigid subgraph. So I couldn't try and characterize this thing for it, but I could do infinitesimal rigidity.
00:40:51.324 - 00:41:36.664, Speaker A: So this problem too is more general, but they're distinct. So there's a little incompatibility here I wanted to try and explain. So that's what I'm saying here. So with the two edges that I took away, which were those two, it's infinitesimally rigid, even though it's at this symmetric realization cs symmetric realization. But to maintain the symmetry I have to remove two edges. And so in general, our count and the order of the group might not be compatible in this way. That allows you to have a graph on a certain number of vertices with two n minus n with two n minus three edges and be symmetric with that, that group.
00:41:36.664 - 00:42:39.614, Speaker A: So in particular, it's even true that some groups have no minimally rigid frameworks in r two with a free group action. And if you're willing to work harder and go into a little bit of the representation theory stuff, you can even make the same conclusion true where you allow there to be fixed edges or vertices. Okay, so in particular it's not the main result of that paper, but there's this paper by Connolly, Fowler, guest, Schultz and Whiteley in 2009 that shows every cyclic group of order greater than three has no minimally rigid thing. So these things can't be minimal in the sense that they're rigid and the rows are independent. It can't be a nice 2 volts minus three graph where all the rows are linearly independent. It will go from being flexible to jump up to having a load more edges added in and having some dependencies as well as being just rigid. Okay, so let me speed on.
00:42:39.614 - 00:43:59.104, Speaker A: So problem one was the minimally rigid case. So I think there are some results in this direction by Bernd Schultz and his PhD thesis, which basically say for order two groups, whether it's half turn or reflection, and for order three rotation, then you can give a characterization of minimal rigidity. But I want to focus on subproblem two. Okay, so the nice thing about subproblem two is there's a paper of Schulz and Tanegawa that I'm going to tell you a little bit of the details of in whatever time I have. But basically what you can do is you can block diagonalize the rigidity matrix according to the symmetry in some sense. So this goes back to engineers Kangway and guest, and again to a paper of burns. So I don't have time to go into the representation theory, but in some sense, we're going to apply Scher's lemma and have some invertible matrices t and s, where you times t times rigidity matrix times s, and you turn it into a nice block diagonal form and look at the individual blocks.
00:43:59.104 - 00:45:00.884, Speaker A: I thought I'd first give this very quick diversion, which is when I thought I wasn't going to fill up the time, actually, to a paper of John Owen and Steve Power from maybe 2010 or 2011. So it relates to the sort of counting that chemists and engineers do in rigidity. So there's this Maxwell Calladine, Calladine Maxwell counting rule, which is basically a linear algebra statement. But it's a nice one, even though I didn't really focus on it earlier on in the course. But you can prove it quite easily with linear algebra, from what Ode did say. So it says that the number of non trivial, independent infinitesimal motions, the dimension of the subspace of infinitesimal motions, non trivial ones minus the number of independent equilibrium stresses, is 2 volts e minus three. And similarly, you can say a three dimensional version, the motions minus stresses in terms of the freebie minus six count.
00:45:00.884 - 00:46:21.382, Speaker A: And so for symmetry, you can get nice extended versions of these counts due to Fowler and guest, no one and power, where the, these numbers, ms, v and e are replaced by representations or character lists, actually, of the, for each symmetry group. So there's a nice diversion there if you want to look a little bit into the representation theory and get a sense for what's, what's going on. But I just say that as a diversion. But this is the, the general point I was sort of making earlier on is that what you, what can be done? And again, I'm skipping over the how is that you can come up with non singular matrices s and t. I don't think it's, it's clear how to come up with the particular matrices, but there they exist, these non singular matrices, which turn the rigidity matrix into this nice block diagonal form like this. And the subscript zero, one up to t, correspond to the irreducible representations of the group. So you have a sequence, well, a set of these t little rigidity matrices, which correspond to these representations.
00:46:21.382 - 00:47:16.144, Speaker A: So each of these, these ris has the number of edge orbits, rows and d times the number of vertex orbits, columns. So they're sort of natural matrices that model the quotient gain graphs that we had before. Okay? And so each of those matrices has their own rank. The kernel of those matrices are infinitesimal motions, but the infinitesimal motions of RGP split into the r zero is for the trivial representation. And so that has all the fully symmetric infinitesimal motions. And then each Ri has some infinitesimal motions. But I'm just going to say, and you have to sort of digest and look into this a bit for yourself after, if you're interested.
00:47:16.144 - 00:47:57.326, Speaker A: But the infinitesimal motions which are symmetric with respect to that representation. So you can naturally work out exactly how many infinitesimal motions each of these things have to work out what rank each of these blocks can be, and then see how to check the rank individually, block by block, to determine the rigidity of the whole thing. Okay, so let me just focus slightly for a minute on the trivial representation. Row zero, zero reals gp. This is what in some of the rigidity literature is called the orbit matrix. So this has this kind of form. But let me skip past it.
00:47:57.326 - 00:48:46.626, Speaker A: All I'm saying here really is that this is the usual entry where there's some group effect on the PI minus PJ, and under the, this is under j and under I, it's the inverse element of the same group that was over element over there. And if it's a loop, you have some more complicated entry, but it's only in the D columns for vertex I where the loop is at. So you have to be a bit more careful with, with what these entries are. But I mean, I don't want you to worry about the technical details because of time and because we're getting towards the end of the course as well. Okay, so let me do a, an example in a little bit more detail. So if I take the reflection group cs again, we've seen this already a bit. Cs is an order two group.
00:48:46.626 - 00:49:38.794, Speaker A: There are two real irreducible representations, row zero and row one. Row zero is the trivial representation. So this is block and row one has this block. So here, because there's only two, we have the fully symmetric infinitesimal motions in this kernel and the anti symmetric infinitesimal motions in this kernel. And we saw this for the reflection earlier with the example I did, that two of the isometries, one, one the rotation and one of the translations destroy the symmetry and one preserves it. So this block can have rank two, v zero minus one, and this block can have rank two v zero minus two. So these are the, the two maximum ranks we want, and you can characterize incidental symmetric rigidity that by characterizing the rank of this matrix.
00:49:38.794 - 00:50:37.120, Speaker A: So by characterizing the ranks of these two matrices. So we want to work out which gain graphs g zero psi, give us a 2 volts zero minus one rank here, and a two v zero minus two rank here. So I think Daniel will talk on Wednesday about a result which does that in the fourth symmetric case, which actually gives us the orbit matrix, which is the trivial representation block. And the results he'll explain, I believe, are due to Maelstein and Ferran in 2015 and Jordan, Kazanytsky and Tanegaro independently in 2016. So that part was known. And then Schultz and Tanegaurac analyzed the second block and combining those two results give us the following theorems. So I have a reflection symmetric, so it's realized with reflection symmetry, but otherwise generic framework.
00:50:37.120 - 00:52:31.996, Speaker A: In the plane, it's infinitesimally rigid, if and only if. So this GP was the covering graph realized the quotient gain graph, G zero Psi should have full rank in the top block in our little two x two matrix above, which means it should contain a spanning two three one gain sparse subgraph, which meets the two three one gain type count. And G psi should also contain a spanning two three two gain sparse subgraph, which meets the top count for there to make the rank of the second block maximal as well. So we need to have both these conditions as our characterization of when a reflection symmetric framework is infinitesimally rigid. Okay, so they also, in the same paper in the Schulz and Tanegauer paper in 2015, dealt with half turn rotation and threefold symmetry. It's, I think in one of the cases, it's not quite so straightforward, but they can, they can do it in both cases in Asia, as I believe, unpublished work, Shinichi Tanigawa has, I think he's published it in a japanese imperial, in Japanese, but it's not appeared in an english speaking journal yet, has extended this to odd order cyclic groups, where the size of the group is something like at most a thousand oops, that's the size of the group. And I don't know what f isn't supposed to be, but when it's even, then there are counterexamples to the.
00:52:31.996 - 00:53:32.584, Speaker A: So the counterexamples, meaning that if you just take whatever the these blocks are and you write down the game type things for each block, counterexamples to that being sufficient. So there are extensions of the result, I've just said, to odd order cyclic groups, but the even case is open and beyond cyclic is quite challenging, whereas in the fourth case, which Daniel will talk about, more groups are understood for wider ranges. But still there, again, there are some challenging and interesting questions that might interest some of you, but let me stop there. I am aware, again, that I went fast for the second half, so feel free to ask questions if you want to. I think I missed the thing about. What's this about gamma being less than or equal to 1000? Right. So.
00:53:32.584 - 00:54:04.232, Speaker A: So this, this theorem, where's it gone? I've gone past it. Yeah. This theorem that says a, a reflection symmetric framework is infinitesimally rigid. This is incidental rigidity, if and only if some nice counts. Nietzsche can give an analogous statement where Cs is replaced by any odd order cyclic group. But as I understand it, his proof relies on a computer. Computer to deal with the base graphs.
00:54:04.232 - 00:54:37.868, Speaker A: And so if the order of the group is sufficiently small, the computer program wins. But at some point, you get computational challenge. So I think he was able to go the order of the group up to about 1000. If you go higher, then his technique will work other than the base graphs, I believe, and you need to check that, which a computer can do. But at some point, I think maybe he, maybe he could push the computer further if he wanted to. But it's just sort of a proof that it will work for most of the way. Okay.
00:54:37.868 - 00:54:39.704, Speaker A: Okay, thanks.
00:54:40.004 - 00:54:42.744, Speaker C: I think maybe I missed. What is Cs?
00:54:45.644 - 00:55:08.474, Speaker A: Cs is just. It's actually, I'm using the. Without saying, I'm using. So circle two that I was using for CI was cyclic, and just using C for cyclic groups. But Cs is the fact that I'm taking this order two group, realizing it as a reflection. So I was just using Cs as short. It's the reflection, not the rotation case.
00:55:08.474 - 00:55:18.722, Speaker A: So this CS notation is the shown fleas. Point group notate for symmetry notation. I see.
00:55:18.778 - 00:55:19.354, Speaker C: Okay.
00:55:19.474 - 00:55:21.094, Speaker A: I did not explain that. Sorry.
00:55:22.274 - 00:55:25.154, Speaker C: That's why odd order cyclic group. I see. And even order.
