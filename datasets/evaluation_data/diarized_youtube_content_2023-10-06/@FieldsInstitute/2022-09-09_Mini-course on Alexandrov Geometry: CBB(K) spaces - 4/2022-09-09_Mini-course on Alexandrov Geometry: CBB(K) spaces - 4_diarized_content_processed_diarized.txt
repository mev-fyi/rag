00:00:00.160 - 00:00:08.954, Speaker A: Of Anton Petronian on Alexandra spaces CBBK spaces Anton, please.
00:00:09.534 - 00:01:00.474, Speaker B: Thank you. So I plan to talk about dimension, but maybe this last lecture. So maybe if you know what you, if you have some questions about Alexander spaces, maybe we can start with questions questions if you want. Or maybe think about this and ask questions whenever you're ready. So I'm going to talk about specific techniques that's available for finite dimensional Alexander spaces. So for spaces with Kotu bounded below. So first I need to introduce rank of a point.
00:01:00.474 - 00:02:40.440, Speaker B: So rank of a point is simply. I have assume I have a point p in my Alexander space and I'm interested. I want to find the maximum collection of points a zero, a one two two a n such that the comparison angle between every comparison angle aj is greater than PI over two for any I naught equal j. So the maximal number n is called rank of point p. So equivalently you can say that rank is the. You can work in the space of directions sigma p at the point and look for maximum number of directions that lies on the distance greater than PI over two from each other. So the only difference with this definition is that I'm taking geodesic directions.
00:02:40.440 - 00:03:22.382, Speaker B: And here I could take any direction. But of course any direction can be approximated by geodesic direction. Okay. And the important property of such collection of like, its unclear how to call it. Like people use term strainers. Like I'm sorry, maybe I should go back to. So the collection of points AI j a around point b might be sometimes called strainers.
00:03:22.382 - 00:04:11.394, Speaker B: I think more accurate term would be struts. But nobody use struts. So I mean, it doesn't matter. And there are variations of this definition. What I'm using is the first definition, but in the Braga, Gromov Perelman they used a different one, but they are pretty much the same. So what can I do with such collection of points? I can consider a map that maps point x to the collection of distance from a one to x, from a to x, and so on to an x. Notice that I forget about zero.
00:04:11.394 - 00:05:58.418, Speaker B: I didn't include no a zero here. So this way I get a map from my Alexander space to rm. And the first statement that this map, let's call it, I don't know, f f has a right, right inverse. So there is a map phi from rn back to l such that f composed with phi is equal to identity. So of course phi should be continuous. But you can also say that phi is holder continuous. Plus you may forgive, in point p you may assume that phi of, I don't know, like their p lies in the image of phi, right? So it's kind of like, notice that once we have this, once the theorem is proved, we have a homomorphic copy of euclidean ball in the Alexander space.
00:05:58.418 - 00:06:04.934, Speaker B: And that's quite a strong statement for a priori, just emetic space with certain properties.
00:06:05.914 - 00:06:12.476, Speaker A: Okay, so I'm going, sorry, phi is not defined on the. I mean, this is not on the.
00:06:12.500 - 00:08:21.440, Speaker B: Whole array, right, right. It's like. Right, the domain, domain of phi should be open and it should contain f of p, right? Okay, so we have a topological copy of n dimensional ball in my Alexander space. Once I have this collection of points, right, with certain angles. The proof is just, I mean, quite simple application of gradient flow, right? So let me indicate the proof quickly. So, I want to construct special functions, say h such that if I apply flow for h for like, for some small time t, then starting from p, I will get the like h is h depends on the point in the domain of f, right? For each point in domain, I can, let's call it x, right? I find h of x such that if I apply flow for this function for a small time, I will get, I can define it as to be like, I can define my map phi of x this way, right? And the construction of h is quite simple. So my point x is x one, x two, x n.
00:08:21.440 - 00:09:50.464, Speaker B: It has n coordinates. And I may take my function, distance function from AI and take its minimum with xi. So what happens on the, on the space that I have certain function that if I apply gradient flow to this function, the gradient flow moves the point to the level set distance function AI equal xi. And after it doesn't move anything. Okay, then I take a minimum of such functions for all I minimum by I, and I add the small coefficients, I add the distance function to my a zero, right? That's the point which I didn't use. So as a result, I get a function such that if I apply. So let me do the next slide, right? So here my gradient flow goes this way.
00:09:50.464 - 00:10:41.304, Speaker B: Here it goes this way. Here it goes this way. It stops at those points here. Without a last term, it doesn't move anything in this quadrangle, in this part, in this angle, right? And after adding this epsilon times distance a zero, I get little wind that moves it back to the point. So if I, I could choose right time. So that will return my, starting. With any point nearby, the gradient flow will move the point to the needed place.
00:10:41.304 - 00:12:52.944, Speaker B: So that's my technical tool, which makes possible to prove number of equalities of dimension for Alexander space. So what we, what type of theorem we have? We have linear dimension. It's a dimension of a maximum dimension of euclidean space, space that admits isometric embedding, isometric embedding into a tangent space for some point p in l, right. We have a topological dimension, okay? And we have housed off dimension, dimension. And moreover, the dimension is local, that everything could be exchanged to any open set. Instead of l, l can be exchanged to any open set in l, right? So let me say that in finite dimensional case, the equality of dimensions was proved by already in the paper of Buraga, Gromov, Perelman. And for infinite dimensional case, it was done by plaut, but the only one case was remaining open for quite a while.
00:12:52.944 - 00:13:25.744, Speaker B: It was that, it was that linear dimension equal infinity implies topological dimension equal infinity. And this was done only after introducing gradient flow technique. Right. Without gradient flow technique, it wasn't possible to prove. Right, so that's. Here we are. We.
00:13:25.744 - 00:13:55.130, Speaker B: I kind of. Right. And I also, that I should also say maybe in the previous slide, that's the same as maximal rank, right? The maximum might be. Right. Might take infinite, might be infinite, right. So not maximum, but maybe, say, supremum of rank. Right.
00:13:55.130 - 00:14:57.074, Speaker B: So, questions? Comments? Okay, so now for the finite dimensional case, if dimension of the space is finite, then you can make the statement a little better. Then there is an inverse function theorem in this case, and it says that, like. Okay, so in this case, we have inverse function theorem, and it's also not just. Right, inverse, it's inverse of f. Right. We have the same map, f. We assume that n is equal to rank.
00:14:57.074 - 00:15:44.738, Speaker B: Sorry. N is equal to the dimension of the space. And then f has, is a partially defined map. On Alexander's place, you have an inverse, also partially defined back to l. And phi is ellipse. So in particular, you get by Lipschitz map to the euclidean space. And this map is called distance chart.
00:15:44.738 - 00:15:45.474, Speaker B: Right.
00:15:48.014 - 00:15:52.886, Speaker A: I don't think people will know your notation. Maybe partially, which.
00:15:53.030 - 00:16:03.558, Speaker B: You mean partial. Partially defined, but I said partially defined function. Right? So, yeah, right. That's. There is this. I mean, we were actually. I hate this idea.
00:16:03.558 - 00:17:01.694, Speaker B: That function must be defined on whole space. Right. It's just, it's terrible, right? And it's to say, every time that function is partially defined, it. It's kind of. Yeah. And what we, what we use is this notation for a function that defined on the open set in the, in the like, like from a to b, right? That means that your function is defined on open set in a and points to b, right? Okay, so now for finite dimensional spaces, like for finite dimensional spaces, we have other good things, right? We have that space of directions, like. So if l say maybe.
00:17:03.474 - 00:17:06.694, Speaker A: Sorry, but I don't think you stated the inverse function theorem.
00:17:09.693 - 00:17:26.313, Speaker B: I just made it as an addition to the right inverse. I said in words that phi is actually inverse local universe plus its lipsticks.
00:17:29.053 - 00:17:30.593, Speaker A: Under the assumption of.
00:17:32.573 - 00:18:44.404, Speaker B: The dimension is finite and the point is, and this is m the same rate. So that's high strength, right? At the point of high strength, right, you get like a lot of equivalents, a lot of equivalences for the dimension like stuff, right? Which I don't want to list, like, all the statements, they are not important. What important is the. Right. Inverse function theorem and inverse function theorem. That's kind of working horses in the field. Once you know this, everything follows quite, quite simple in a simple way, right? So next thing I want to say is, like, I want to mention one property which was not available in the infinite dimensional Alexander space.
00:18:44.404 - 00:20:33.258, Speaker B: If dimension of l is m is finite, then for any point p in l, the space of directions of p has curvature bounded below by one and has dimension n minus one. So that's it makes like, once you have the statement, which is again, not hard to prove, but I mean, for infinite dimensional cases, you don't have this dimension reduction. And plus the space of direction is not an Alexander space, right? So once you have this, you open a gate to like, there is a standard way to prove things in Alexander geometry using induction on dimension. So if you try to adapt a proof in romanian geometry to Alexander settings, what you do in romanian geometry, there is a little part which you usually don't notice, something about linear algebra, linear, right. A linear algebra in the tangent space. And instead of this step, you apply induction. You take, you prove something for lower dimensional Alexander space and apply it to cone over sigma p, which is the tangent cone at the point, right.
00:20:33.258 - 00:21:32.694, Speaker B: And this way you kind of able to prove something, right? Okay, so, that's right. I. So, right. I think I finished with this part. Maybe some questions on there. Okay, so next thing I want to say, couple of, like, I want to list couple of constructions which might be used to make life better, right? One is controlled convexity. Controlled convexity.
00:21:32.694 - 00:22:32.346, Speaker B: So, first thing that you, like Alexander spaces, have a lot of semiconcave functions. For example, if your space has koh bounded below by zero, then the distance function squared from point p is to concave. But it's hard to get strictly concave functions. So to concave is weaker statement than concave, right? And natural question appears, do we have, is there a way to construct strictly concave function? And the construction was found by Perelman. And it goes like this. First of all. So start with.
00:22:32.346 - 00:23:27.792, Speaker B: First of all, let me prepare some function phi from a real line to real line. Okay? So it might, it has to be smooth. Infinity plus increasing, plus second derivative should be actually. Yeah, let me just maybe, let me make it stronger. Phi prime is nearly one and phi double prime is nearly minus infinity. So it's very negative increasing concave function. And now imagine that I take a descent function and apply, and take composition with phi.
00:23:27.792 - 00:24:33.034, Speaker B: What's happening? So I claim that it's very convex, right? Very convex in the following sense. So if I shoot a, if I choose a geodesic gamma and take composition. So let it be f, right. Take composition f with gamma. So it's unit speed geodesic, right? Then there is a high probability that this thing is negative, say minus one, less than minus one. So why. Well, I mean, you should just take calculations and roughly the only chance for this value to be like greater than minus one is that this angle, the angle, it runs from like this point p, right.
00:24:33.034 - 00:26:05.702, Speaker B: That's my gamma of t. Gamma of t. The angle is nearly, must be nearly PI over two, very close. So there is a little set of directions where if you shoot geodesic, then the derivative is almost positive, right? Otherwise it's very negative second derivative, right? And now you vary this. You, you take some of such functions. So you vary point p base point for the, for the distance function. And you can see the function, say sum of phi composed with distance function two PI for I and points could be chosen pretty much arbitrary, except that you, they, for any point, for any point x near the place, we want to construct a function in any direction, most of the point, like many points, should not be perpendicular to these directions.
00:26:05.702 - 00:27:50.336, Speaker B: And that's just generic collection of points works. And you may choose roughly cloud of points around anything, and after around any point, any given point, and then choose sufficiently concave function phi, so that this construction will produce concave functions once more, right? Like if you choose a geodesic and that, and you composed f, like let's say my function f, you take function f and compose with gamma. And the only chance for this to be non concave is that all these directions to all these points, p one, p 2d, should be almost orthogonal. And the almost could be arbitrary, like close to two PI over two, right. These angles should be sufficiently close to our PI over two. And it's rather easy to choose set of points that doesn't satisfy this condition, right? So this way you can improve roughly like you can start with the distance function and construct a strictly concave function with the gradient arbitrary close to your, to your distance function. So it has all the properties like up to first derivative.
00:27:50.336 - 00:30:06.284, Speaker B: It has properties that you want, right? Plus it's strictly convex concave. Okay, so another nice construction that might be used to improve things, right? It's smoothing of distance function, right? Instead of taking distance function to a point, which is a priori, very non smooth function on a non smooth space, you take another one, say distance twiddle of p, which is integral for q in the ball of radius epsilon of p, of distance function of q. So what's happening? Why it's better, right? So that's, you just take average, maybe you take average. So why, why this formula produce something better than just distance function? So first of all, in terms of convexity concavity of the function, it's pretty much the same thing, right? But in addition, for a distance function, it's the distance function. It's easy to prove that for infinite dimensional case, the distance function is differentiable at point. Sorry? Yeah, I assume x is a regular point, right? So that's a tangent space at x is just euclidean space of dimension and maximal dimension. So a distance function from p is differentiable at x, meaning that differential of distance function p at point x is linear.
00:30:06.284 - 00:31:10.414, Speaker B: This happens if a non leaf, the geodesic pq px is unique. And the last statement holds for almost all p. If you vary p that most of the time it holds. So when you take average here, right? When you take this average, your function is always like the differential of the function is always slipships. It might be not linear, right? It's always slipshits. And most of the time it's linear. So you take average of pretty much linear functions, and it must be linear, so that this distance function becomes differentiable at all points where it might be differentiable, where the tangent cone is euclidean.
00:31:10.414 - 00:32:01.626, Speaker B: So now I want to say one more construction. So I want to talk a bit, maybe some questions, because I just finished two more topics, right. And I would like to have, if you have questions, please ask, because I don't have an idea to whom I'm talking about and why. Right, yeah, I have.
00:32:01.770 - 00:32:11.494, Speaker C: How is this, how can this be used? Or what is the application of this smoothening, for instance?
00:32:11.834 - 00:33:05.618, Speaker B: So yeah, I will go to applications. Maybe I was planning to talk about applications when I go to DC calculus. That's next topic. But for example, if you want to like the, what I'm going to describe if you try to mimic something from romanian geometry, right. The Alexander space is very non smooth, right? But like almost all points have euclidean cone. And you can actually describe. Define a riemannian metric on this.
00:33:05.618 - 00:33:51.184, Speaker B: Like you may use distance chart. And using this distance chart, in this distance chart, you can construct riemannian matrix. Now, if you use standard distance chart, right, for usual distance functions, then this metric is not continuous even at smooth points. But if you use smooth chart, a smooth distance function, it becomes continuous at the, at the regular points. The regular points are points with tangent conucrydium. So maybe. Is it, is it an answer?
00:33:51.764 - 00:33:54.756, Speaker C: Yeah. Thanks. Thank you.
00:33:54.820 - 00:35:20.974, Speaker B: Questions? Maybe. Okay, so one, like one extra thing, which another kind of technique is Dc calculus in the. So dc stands for difference of concave functions. Difference of concave or convex doesn't matter functions. So there is a very simple but very powerful observation. Assume I have a distance chart, which I already described, right? F is a distant chart. And I assume h is a concave, like say like maybe semi concave function.
00:35:20.974 - 00:36:04.834, Speaker B: Then how to write it? So if I get f, then f inverse composed with h. It's a function on rn. Now to r, right? So I apply chart. So I get back from rn to Alexander space, and after that I apply f, right? Again, it's partially defined. This is a dc function. Dc function. So that's just a little game with convexity.
00:36:04.834 - 00:37:25.242, Speaker B: Approves it. And let me describe some color of this, right? So the, so that's actually due to permanent. First of all, the riemannian metric metric can be defined on the regular, in the regular points. And it is. It is bv has bounded variation in the distance chart. So bounded variation makes possible to take yet one more derivative. And so you can say that Christopher, symbols in Alexander space can be defined as measure valued kind of object, right? Unfortunately, you cannot do like it would be nice to do one more step and define curvature tensor.
00:37:25.242 - 00:38:09.416, Speaker B: But it doesn't work directly when you just apply. I mean, it's okay to take derivative of measure, right? But it's not okay to take product of two measures. If you apply directly formula for curvature, you will get product of measures. And so it doesn't work anymore. So other like. But we have some relatively recent work with ninolebidiv which goes around this problem and proves that existence of Keowich tensor on Alexander space as measure valued tensor. But it only works for smoothable Alexander space.
00:38:09.416 - 00:39:52.920, Speaker B: For those who admits approximation by the minion manifolds of the same dimension with lower Kochi bound. Okay, another thing you can talk about, Hessian, Hessian of semiconvex, semi concave functions. So you can actually, with this, with this technique, you can do some calculations which are available in riemannian geometry up to certain, like, up to certain extent, right? And in addition, you can make it a little bit better, right? You, if you apply the same thing using only smooth distance functions instead of standard distance functions, then you get DC zero, meaning that it's differentiable, in addition that it's DC is differentiable at all regular points. And the riemannian tensor is BV zero. So in additional to having bounded variation, it is continuous at all regular points and so on. So I think that's extended answer to your question. And at the same time it's like, yeah, I just introduced like, yeah, I described that there is this technique which is, I don't know, powerful, right? But I didn't describe.
00:39:52.920 - 00:41:04.098, Speaker B: Yeah, right. I said that there is one lemma right before. So this one, it's really like, once you have a formulation, it's really easy to prove, right? So you just kind of do like half a page of proof. But actually, when I saw it for the first time, it was very, for me, it was very surprising. And usually convexity does not survive in the chart, right? So, and maybe if no questions, I will say a little bit more about this part. It's also remarkable proof. So, if you read Riemann's lecture like the beginning of riemannian geometry, he defines metric tensor using some linear equations on distance functions.
00:41:04.098 - 00:42:13.614, Speaker B: Like he takes the gradient. So distance function, and using these gradients, he write equations for metric tensors. That's roughly n squares, n square equation, where n is dimension. And I didn't ever use it and I didn't ever saw it in applications of this statement, but that's the first time I saw it to be applied again by Perlman, right? So he just take the distance functions. So let me say it. So he knows that distance function is dc in chart. In chart, he knows that gradient of distant function by absolute value is one almost everywhere, okay? And this identity can be written using the metric tensor.
00:42:13.614 - 00:42:35.884, Speaker B: And this identity gives you some linear equation on metric tensor. And you have plenty of distance functions which sufficient to describe metric tensor completely. Okay, so any questions about this?
00:42:36.044 - 00:42:44.492, Speaker C: Because can you one, how do you get the metric tensor again from the distance?
00:42:44.628 - 00:43:01.488, Speaker B: So imagine that I, I have, like, I have, is it clear that distance function has gradient almost everywhere one. Right. Whenever it's defined.
00:43:01.536 - 00:43:02.124, Speaker C: Yeah.
00:43:02.464 - 00:43:30.608, Speaker B: Right. So now imagine that I have plenty of function, plenty of points for each point I have this. Let it be k. So it will not be confused with IJ. So, for each point, I know that your metric tensor has certain property rights. Like, if you apply it to. If you apply metric tensor to differential, it must.
00:43:30.608 - 00:43:43.216, Speaker B: You. It must give you one. Right, okay. And you have. For each point, you have different equation. Right. If you have an.
00:43:43.216 - 00:44:15.468, Speaker B: If you take sufficiently many points, there are enough equations to define the metric tensor at the point. Right? That's. That's the idea. And as I said, this is the idea to write. To use distance functions to extract back medic tensor is due to Riemann in his first lecture. So, yeah, that's. That's what's applied here.
00:44:15.468 - 00:44:21.344, Speaker B: More questions. I mean, I'm not sure. Did I answer the question or not?
00:44:23.884 - 00:44:25.260, Speaker C: Yeah, I think, right.
00:44:25.292 - 00:44:43.580, Speaker B: If you. If you have a differential, right? If you have a differential and you know that absolute value of differential is one. Right. It gives you linear equation for the. For the metric tensor. So if you have enough linear equations, you know the metric tensor. That's.
00:44:43.580 - 00:44:48.104, Speaker B: That's all. Okay. Yeah. So.
00:44:50.644 - 00:44:53.904, Speaker C: I mean, that. That is on a chart, I guess, then.
00:44:54.324 - 00:45:01.952, Speaker B: Right, right. In the chart. In the chart. In the distance chart. Right. And right. And you can make it a little better.
00:45:01.952 - 00:45:23.444, Speaker B: Right. So you. Not only DC, not only BV, but BV zero. Right. So it will be continuous. And. Yeah, I should say that first work to extend certain constructions from romanian geometry to Alexander spaces was done by Yukio Otso and Takashi Shuoya.
00:45:23.444 - 00:46:23.014, Speaker B: And after that, this DC calculus was invented by Peramman, and it gave much stronger results. Right, okay, so now I want to describe tight maps, right? It's. Maybe I shouldn't describe it at all, but I mean, maybe you. If you go. If you did go to talks of Alexander Lichak, he. When he described dimension, he pretty much did this, right? So when he made the construction of submanifold c one c half submanifold based on number of convex functions in the. In cad zero space.
00:46:23.014 - 00:47:30.484, Speaker B: And the same, exactly the same argument works to construct submanifold for collection of convex functions in Alexander spaces with Kauchi bounded below. So let me just describe it quickly. So, Rafi, you remember that we have, like, when I started to talk about rank, I had a point and collection of, like struts, right. A zero, a one, two to do am, right. And all these angles have to be bigger than PI over two bigger than PI over two. Okay, so instead of these functions, instead of distance functions, I can do better. I can exchange each AI to a cloud of points and apply the construction of strictly convex.
00:47:30.484 - 00:49:00.044, Speaker B: I would, I could construct a strictly convex function with almost the same gradient here as distance to a zero. So in this way I get collection of functions, say f zero, f one to do fn and one. The condition on angles imply that if I take differential of fi and plug in gradient of fj, that will be very negative, maybe very negative for I not equal j. So that's called tight collection of functions. And I see you might take functions of maximal like, I take a collection of that, realize the rank at the point p and what like. And I want to consider the map point x maps to x f zero of x. F one of x to the do f n of x, right? So again, I have a map to euclidean space of now dimension rn plus one.
00:49:00.044 - 00:50:24.864, Speaker B: Okay, so what's the how image look like? So that's my rn plus one. And I have a kind of, that's my image image of f. Okay, so now imagine that I want to take a larger set together with every point. I take all points that has all coordinates less than this one, less or equal than this one, and take union of all this, right? In other words, I take Minkowski sum with a negative octant of my set. So this way I get a new set that looks like this. And since the functions, since functions f I are concave, I get that the obtained set is convex. That's rather easy to check, right? So if I take, so what I need to check that the line segment between any two points lie inside of this set.
00:50:24.864 - 00:50:57.284, Speaker B: Why is it so. Well, let me look at the corresponding points right in my, in my space l, right. I have two points. I can connect them by geodesic. And convexity means that this geodesic, the image of this geodesic lies above the line segment. And since I add all points below right, this line segment must be inside. So I get a convex set.
00:50:57.284 - 00:52:40.564, Speaker B: Now further, there is this boundary part of this set. And for this boundary path, it's like each point on this boundary part is corresponds to maximum point for some linear combination of my functions fi. Roughly, if I take supporting plane, right, this supporting plane tells how with which coefficients I have to take the coordinates to make it like to make this function with such level set. And I take these coefficients in front of fi and this point will correspond to maximum point of this alpha one. Alpha zero f zero plus plus alpha one f one plus to the two alpha n fn, right? And this is unique point. So this is homeomorphic copy of family of maxima of such functions. And it turns out that if a point has rank exactly n, like n is a rank of rank of point p, it must lie on this maximal set.
00:52:40.564 - 00:53:16.414, Speaker B: So all points in the neighborhood of p will lie on sub manifold of dimension. Nice. All points of the same rank as p will lie on this submanifold. And it's easy to check that this submanifold is gain c one alpha. So it's by herder manifold, and remaining remaining set has rank at least n plus one.
00:53:17.184 - 00:53:17.616, Speaker C: So.
00:53:17.680 - 00:54:29.624, Speaker B: And that's another useful technique in Alexander geometry. You go dimension backward dimension on the rank of points, right? So you start with, you prove something, assuming that rank of points of all points in the space is maximum. Okay? Once you prove it, you just use it to reduce the rank of points to n minus one and just step by. And then you need to, like locally, you need to prove something for sub manifold. And you just reduce it one by one and prove it for whole space, right? Okay, so, okay. And by the way, it also gives you local stratification, right? You take a point, all it has around n. All points of the same rank lies on the sub manifold of the same of dimension n.
00:54:29.624 - 00:55:41.102, Speaker B: You remove the sub manifold. Everything else is dimension that has Ranker n plus one. And again, you can repeat the procedure and go around. Okay, so I have five more minutes. Since I told you about stratification, let me mention one more stratification of Alexander space, that is so called extremal substance, which is another and very natural stratification, right? That which could be defined the following way. That imagine that I, roughly you think of like you, the association is like you move in furniture inside of your room, right? Imagine that you have a piece of furniture and you can push it to any direction. If your piece of furniture stays in the center of the room, you can push it everywhere in the room.
00:55:41.102 - 00:57:01.304, Speaker B: But if it stands at the side, at the wall, you only can push it one way or another way, right? You cannot take it from the wall, right? Or if it stands in the corner, you cannot move it at all. Right? No matter how you push it, it will stay in the corner, right? And roughly, you think that now you have Alexander space, l and you think that you have a point in Alexander space. And you can apply gradient flow to this point for a distance function. So you can move points one way or another way. You can iterate the procedure and you get certain subset which accessible by this move. And this subset is called extremal subset. So for example, for a rectangle, we have each vertex of rectangle is extremal subset, each side is extremely subset, and the whole space is extrema subset plus of course, empty set.
00:57:01.304 - 00:58:38.756, Speaker B: If you would do pentagon, you would get just like empty set, whole space, and the boundary, that's kind of two extremal subsets and the three extremal subsets. And you can prove that again, that every extremal subset, if you remove the small extremal subset from it, that's manifold, right? You have a, that gives you stratification of your Alexander space into manifolds of different dimension. And it fits very well with one of the examples of Alexander space. If you have a manifold, right? And with isometric group actions, group action, and then say g, right, then you can pass to the quotient space Mg and every, for any subgroup, lambda in g. If you take fixed point of this subgroup and take its orbits, then it will correspond to extremal subset, right? So it's very well fits with natural stratification for the group action. And now I think I'm out of time. Right.
00:58:38.756 - 00:58:45.304, Speaker B: I have 1 minute for questions. Sorry for. Yeah, I plan to give more. Right.
00:58:47.124 - 00:58:50.864, Speaker A: Okay, thank you. Questions?
00:58:57.784 - 00:59:23.804, Speaker D: No, I have a good question. So, we know that the Alexandra spaces that arise from isometric group actions, they're all going to have space directions as a linear quotient of a sphere. So that's pretty restricted, at least it seems. So, I was wondering if maybe you could mention some special properties of these Alexandra spaces with respect to the general ones.
00:59:26.784 - 00:59:45.336, Speaker B: Well, you already mentioned one. Right. But yeah, of course, it's very special case. Right. But I'm not ready to answer. Right, right. But I mean, you gave one example.
00:59:45.336 - 01:00:21.244, Speaker B: Right? And of course, it's. It's also very smooth space, right? At around every point, exponential map is well defined in a little neighborhood, which is almost never happens in generic Alexander space, right. Typically, exponential map is not defined in a little neighborhood of any point. But that's what comes to my mind first. Right. But I'm sure there are many.
01:00:22.064 - 01:00:32.376, Speaker A: If I can add, you can also show that in this case, small metric spheres around points, Alexandrov, that is not something, you know in general.
01:00:32.480 - 01:00:34.564, Speaker B: Right, right. Yes.
01:00:36.144 - 01:00:38.912, Speaker D: Yeah, these are good examples.
01:00:38.968 - 01:00:44.454, Speaker B: Thank you. More questions?
01:00:48.714 - 01:01:06.402, Speaker C: Can you give an example how this technique that you showed before can be used with the maximal rank about the rank, and where the rank, all the points in the set have the same rank?
01:01:06.498 - 01:02:01.758, Speaker B: Just a slide before the last one. Sorry, the last. Like which one? That's this one. Yeah, yeah, yeah. So look, we have some number of concave functions, right? We take, we apply these functions and map the space, or piece of space, maybe convex subset in the space, to the euclidean space. And then we have this, have to add this negative quadrant and quadrant. And after that, I look at this subset at the boundary, right? First of all, it must be convex surface in euclidean space, right? So it's a nice manifold.
01:02:01.758 - 01:03:21.800, Speaker B: Convex surface in euclidean space is a submanifold, right? And second, it's. There is a homeomorphism between, on this subset, your functions like there is unique inverse, right? So you have a submanifold in the, in the Alexander space. Now if you move little bit from this, from this boundary, then these functions, kind of, for all these functions, there is a way to go up, there is a way to increase function, each function simultaneously. And what does it mean? That means that there is a direction, roughly, that goes at angle bigger than PI over two, from directions to all these functions, right? Imagine that they are just distance function. For second, it's not a big line, right? And that exactly means that there is one more. You can, to the collection of struts you have, you can add one more strut. So your rank is getting bigger.
01:03:21.800 - 01:03:56.334, Speaker B: So was it, was it your question? That's what you ask? That's what did you ask this? Yeah. What I explained that you, you have a point of rank m. Then together with this point, you get a submanifold that includes all points in its neighborhood of the same rank. And the rest of points has, have ranked at least n plus one. Right? So was it, that's what you wanted, or not? If not, tell me what.
01:03:57.794 - 01:04:14.186, Speaker C: No, that's, um. Then that's, that's already good. I was more specifically, how, what, what is the kind of result that you can prove using this, this method, or where we're at this point was used?
01:04:14.330 - 01:05:31.134, Speaker B: So you see that you often need to deal with singularities in Alexander space. When you try to prove something in Alexander geometry, it's better to assume first. Then your Alexander space is small, meaning that all points are regular, all tangent space are euclidean. So if you can prove it in this way, in this case, then most likely you can prove it for General Alexander space. But it doesn't mean that it's easy. And then this technique will come into like, you will need to apply this, this technique, right? So you kind of, you use the first part of proof to say that something is true for most, for most of Alexander space. And now you take first point when it doesn't work, right? Assume it has rank, rank n, and then you have this submanifold, and it makes.
01:05:31.134 - 01:05:48.540, Speaker B: Right. Your. Yeah, I. Somehow. I'm just bit tired. I don't have good example to show you how it works. Maybe, vitaly, maybe.
01:05:48.540 - 01:05:59.504, Speaker B: Do you. Oh, you on the phone. Okay, sorry. Maybe somebody has a good example in the audience. Right. It's rather standard technique.
01:06:00.354 - 01:06:01.338, Speaker C: Yes, thank you.
01:06:01.426 - 01:06:37.134, Speaker B: For example? For example. For example, the Morse theory of Perelman proved this way. If all points are euclidean, it's rather easy to prove. If not, you use this construction to improve, to get a bigger and bigger set weights where it holds. Right? That's. Yeah, but it's probably a bad example, because you cannot explain it in 1 second. Vitaly, do you see better example?
01:06:37.434 - 01:06:54.094, Speaker A: No, but I mean, I wanted to mention this particular example that this construction is used to prove by Pilmon, to prove that, topologically, are these stratified manifolds with multiple clinical singularities.
01:06:54.434 - 01:06:54.890, Speaker B: Right?
01:06:54.962 - 01:06:59.474, Speaker A: So it goes exactly. The proof goes exactly as Anton explained.
01:06:59.634 - 01:07:06.402, Speaker B: Right? By the way, that's the construction. That's the first step in this proof, right?
01:07:06.458 - 01:07:07.054, Speaker A: Right.
01:07:07.954 - 01:07:24.444, Speaker B: After that, you have work like peril money, and then you get the stability theorem and theory for Alexander space. So, more questions.
01:07:33.544 - 01:07:43.104, Speaker A: Okay. All right, then, if there are no more questions, let's thank Anton. Great lectures. Thank you very much.
