00:00:00.200 - 00:00:16.370, Speaker A: The last but not least, talk of the workshop. Introducing Guillermo Guillerm, Leal cardos, Opita, and who's going to present enhancing synthetic transaction data generation with graph clustering.
00:00:17.750 - 00:00:45.300, Speaker B: Thank you everyone. If you're here, I would just assume that it's for me. And thank you so much because it's the last presentation. My name is Guillermi, Lea Ocardo Zupita. Usually just go by PETA in english speaking countries. Today I'm presenting my undergrad thesis that I defended a few months ago on enhancing synthetic data transaction data generation with graph clustering. So this is a table of contents, just an outline of what we're going to see today.
00:00:45.300 - 00:01:15.766, Speaker B: First I'm going to state the problem summarizing tabular data by clustering. Then explain why use graphs to do that. Give an overview of the data that we have to work with. Give an idea of. Sorry, an overview of what the Eigen gap heuristic is. Some of you may already be familiar with it and finally understand the cluster ability and some thoughts of the original data. So, bank transaction data, account level credits and debits.
00:01:15.766 - 00:02:13.066, Speaker B: You can see on the right here an example of what that would look like. Basically just entries for individual timestamps of transactions that one person would do. So credit card debit, just withdrawal money from an ATM, simple stuff like that. It is highly useful to train fraud detection or to train product recommendation models, but it is also highly sensitive with personally identifiable information on spending habits. This paper here by Montoya states that as little as four credit card transactions should be enough to de anonymize an account. Because not a lot of people do. For example, pay something in the subway, then get an ice cream at a given given shop, and then get something at the show warmer place right next to it.
00:02:13.066 - 00:03:07.618, Speaker B: Right. Current work used to synthesize data so that there's no denonymization possible is pretty good. They use either gans like Doppelganger or DPGaN that we saw in the keynote yesterday, or transformers like banks former, that actually improves on what gans can do and add timestamps. But although they work to a certain extent, there's room for improvement. Here on the right you can see a graph of the distribution of the amounts in transactions from Rio data. In blue, maybe people in the back can't see that well, but in blue you can see the Rio transactions, while banks, former and doppelganger are in orange and green. As you can see, the distribution of values is not that great.
00:03:07.618 - 00:04:15.902, Speaker B: So we should be able to improve it. This graph is actually from Amy, who was a master's student at the lab I did my undergrad thesis on. My idea is to separate the real training data into clusters and generate new data based on each section individually. So we take the already state of the art models and feed only information from a given cluster, thus having less variability and potentially closer to reality in the final result. This is an unsupervised learning problem because we don't know exactly what kind of clusters any given dataset would have. Now, why use graphs to do this clustering? The advantages are we don't need to have any previous knowledge on the client behaviors. As I said, it is unsupervised learning and we have robustness against bad choices of summary statistics of transaction series, like the frequency of transactions or taking just the average amount of that was transacted, we lose information on the behavior of the spending habits.
00:04:15.902 - 00:04:52.574, Speaker B: But we do have a problem. How do we build edges between the nodes? To think about that, let's first look at the data that we have available. As we saw in the keynote yesterday, there are many open data sources for banking transactions and the ones available are quite old and unnecessarily representative of current trends. With that in mind, we used real open source transaction data from czech banks in the nineties, actually 1999 if I remember correctly. It's available on this URL. And these three. Yeah, it works.
00:04:52.574 - 00:05:45.960, Speaker B: These three are three of the main columns that we have in that data set, stating the types of operations, the amount that was transacted and the balance in the accounts after the transaction was done. So we have mixed columns, both categorical and numerical. So how to work with this data? There were about 4500 and accounts, which is not that much for training. Like larger models, it would not be not be enough. So what we did with what we have, we grouped transactions by accounts as time series and category categorical data, sorry. Was flipped into columns so that we would only have numerical data. Every account becomes a node in our graph.
00:05:45.960 - 00:06:40.612, Speaker B: Then we add edges between accounts using combinations of account features. So the amount transacted, the type of transaction that was done, and distance matrix between those two nodes, which we'll explore in a bit. Finally, we measured the cluster ability of the resulting graph using the Eigen gap heuristic. The original idea was to measure the ideal number of clusters using the Eigen gap heuristic and then actually cluster them to feed into the models. But that was not possible and we'll get to it in a second. Now creating nodes and edges with every node being in an account, we can calculate the distances between nodes using, for example, dynamic time warping or compression based algorithms, which we'll see in the next slide how it works. Once we have the distances, edges are added between the nodes if they're below a certain threshold.
00:06:40.612 - 00:07:48.760, Speaker B: In our case, we tested multiple thresholds. So, taking the median distance and adding or subtracting, subtracting a few standard deviations, and then testing all of it to see which one is best. Dynamic time warping is a technique used to measure the dissimilarity between two time series, and that may vary in time or speed. It is also expandable to multivariate time series by combining the distances between multiple columns. As you can see on the right, it differs from regular euclidean matching by not just taking the time stamp, but it also looks forwards and backwards a little bit to see which point would match the best. So if you have, for example, two sine waves that are slightly offset, it would still give a very, very low distance between them, even though if you go point by point, they are quite different, which is valuable for accounts if you want to see similar behaviors that are not exactly the same. Not everyone spends the same thing on the same day, but a lot of people pay about the same things in a similar order.
00:07:48.760 - 00:08:45.998, Speaker B: Now, compression based algorithms leverage the file compressing power of regular compression algorithms to measure how many times larger the compressed file of both stewards concatenated is. Then just summing the compression of those two individually. Here's the algorithm. It's a very simple idea to see if patterns repeat that we can just regularly compress. Now let's have a reminder of what the eigen gap heuristic is. So if we have n time series, we can create the adjacency matrix a, where every entry of a is one. If the distance between nodes I and j is less than a given threshold, with a, we can do the equations on.
00:08:45.998 - 00:09:25.526, Speaker B: Until you can see on the slides, I'm not going to repeat it. To find the normalized laplacian matrix l. With l, we can calculate the eigenvalues of l, and when ordering them by size. The first one, lambda zero, should be zero. The rest should be between zero and two. And the amazing thing is, the ideal number of clusters for the graph should be around the index of the first spike in the eigenvalues. It is kind of magic if you see the examples here.
00:09:25.526 - 00:10:31.850, Speaker B: Actually from Pierre's paper, you see a clear spike around four in a, around 50 in b. But if the number of clusters is too big, so 255,000 nodes. This is not possible. So, given our data set, which is small, we used only 500 accounts for a graph falls between a and b, so it should still be good. And although the original point of the heuristic is to yield the ideal number of clusters, we will change a little bit of to say it is a cluster ability metric. So if the ideal number of clusters is around one or around the total number of nodes, then we know that the clustering doesn't yield any value. This is what we got actually from the accounts, from the check bank dataset.
00:10:31.850 - 00:11:32.150, Speaker B: One thing that I didn't mention is that the graph needs to be connected. A needs to be fully connected for the Eigen gap heuristic to work. So all those n a's here are graphs that did not fully connect, but otherwise they were either very close to one or very close to the total number of accounts, 500. Therefore, the graph should not be clusterable. And here are some of the actually the graphs that you can see, very sharp spike right in the beginning and another one in the end. And this happened for both measurements of distance and for both for the three different types of feature selections. So, either using everything or just the amounts or just the balance here for compression based, the conclusion is training data appears to be unclusterable.
00:11:32.150 - 00:12:29.650, Speaker B: But this conclusion depends entirely on the original data set and how the tabular data was converted into a graph. So this could still be useful if you have access to proprietary data, as people in banking institutions would have here. Also, choosing different types of distance measurements could yield better values as well. Current and future work should focus exactly on that, similarity metrics and edge representations on this and other datasets, the cluster ability measure using eigen gap heuristic could also be used to determine the fidelity of synthetic transaction data. So if you have the original data and data that was created based on that, and you measure they having different clustering profiles, then you say, oh, okay, there's something wrong here. Maybe my model is generating just this specific type of account more than it should. Let me look into that.
00:12:29.650 - 00:12:54.470, Speaker B: That was my presentation. I'd like to thank the CMTE lab, Pierre, specifically Professor Yuri Larishin and Doctor Lucy from RBC for all their mentorship and help. That's the bibliography questions.
00:12:57.610 - 00:12:58.670, Speaker A: Final tuck.
00:13:01.050 - 00:13:01.990, Speaker C: Speechless.
00:13:02.490 - 00:13:34.080, Speaker A: Yes, everyone is super, super tired, so. Well, chair, I'll ask one. And of course gonna be data related. So when you're training again, right, you start from something and you try to do the right back then. So how do you verify that your synthetic data then is going to approach the original sufficiently close upon use. I may have missed that when you were. So it's because of this distance that you're calculating between the two matrices.
00:13:34.200 - 00:13:55.020, Speaker B: Yeah. So what I proposed is, before using gans, before using transformers, it would improve any given model. Right. Uh, to measure in the end if they're good or not. Uh, this is what Amy's work. Yeah, this is what Amy's work was for. To actually see if they.
00:13:55.020 - 00:14:03.124, Speaker B: They reflect the. The ideal. Uh, not the ideal, but the real data. It's just one of the measurements, like the distribution of amounts. Perfect.
00:14:03.172 - 00:14:08.560, Speaker A: So just using some distributional difference. Okay. And I have a question then, up here.
00:14:10.140 - 00:14:22.214, Speaker D: So the fact that the graph is unclusterable means that somehow those nodes are all the same, the edges have all the same values.
00:14:22.262 - 00:14:22.566, Speaker C: Right.
00:14:22.638 - 00:14:35.810, Speaker D: You don't find a situation in which there are less edges or less weighted edges. So this might depend on the way in which you are measuring this distance among these time series of the junctions.
00:14:36.160 - 00:14:36.784, Speaker B: For sure.
00:14:36.872 - 00:14:46.620, Speaker D: Do you think that using a different way, this might expose the differences existing between the nodes?
00:14:47.200 - 00:14:54.816, Speaker B: For sure, it could. We're just limited in time and computation power. We don't have access to the power. You?
00:14:54.928 - 00:14:57.900, Speaker A: Why not? It's free for everyone.
00:14:58.560 - 00:15:00.220, Speaker B: Well, I was in undergrad.
00:15:01.730 - 00:15:05.354, Speaker A: Tell Yuri to sponsor. I sponsor Peruvians, dude.
00:15:05.442 - 00:15:23.110, Speaker B: Okay, fair enough. Fair enough. That's up to CMTU later then. But, yeah, absolutely. The idea is still sound, but new metrics and new ways of connecting the nodes would absolutely be in future work.
00:15:25.290 - 00:15:44.200, Speaker C: Yep. Great talk. Just like, the quick question is, like, what was your input and what was your output for your generated model? I guess I'll just start with, I have a few questions. That's my output. Okay. The first question is, like, what was your input and what was the output of your generated process?
00:15:45.940 - 00:16:12.270, Speaker B: I didn't use any generative models. This would be before any generative models, but the inputs would be accounts like this. So just a list of tabular data? Yeah, it may be hard to see from the back, but it's tabular data with just transactions that happen. For every singular account, there's an account id for each row. That's the input, and the output should be something similar.
00:16:12.770 - 00:16:54.030, Speaker C: So is the goal is to just generate synthetic data that represents the actual spending of people, or is it a like, how would you ensure your data is, like, somewhat well represented for geographical or age? Like, high school students will spend differently than, like, say, people who are 40 or 50 or women and men might spend slightly differently, different geographic coverage, like depending on if they're in downtown city versus rural area, their spending habit is slightly different. Would you also thought about maybe combining those features to make something representative? Or was it just to generate something like the data you had?
00:16:54.650 - 00:17:33.870, Speaker B: So the original data did not have this information on age, location, and all that? It is from tech banks in the nineties, so not a lot of information, but otherwise, your question is very similar to Christian's. There are a few metrics to see if the synthetic data is close to the real one. One of them is using n grams to see if patterns of like three types of transactions that appear in the original data also appear in the synthetic ones. But it's usually a measure of distributions of amounts or types of transactions.
00:17:35.570 - 00:17:36.510, Speaker C: Thank you.
00:17:37.010 - 00:17:39.450, Speaker A: Perfect. So let's thank Glem again.
