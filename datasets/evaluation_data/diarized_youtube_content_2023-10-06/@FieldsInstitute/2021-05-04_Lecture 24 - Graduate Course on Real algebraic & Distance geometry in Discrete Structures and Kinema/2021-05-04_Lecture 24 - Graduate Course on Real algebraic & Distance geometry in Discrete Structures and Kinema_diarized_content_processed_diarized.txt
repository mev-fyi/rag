00:00:00.080 - 00:00:48.134, Speaker A: So I will talk about the sum squares, the polynomial optimization. So, basically, we will study how do we use the sum squares technique to approximate some polynomial optimization problems. And we will see how this. How can we solve the sum square program using the semi definite programming? So the. So I will be. So this material is from the book. Let's share my screen.
00:00:48.134 - 00:00:58.994, Speaker A: Share my screen. So you're seeing this. It's my screen.
00:00:59.414 - 00:01:00.194, Speaker B: Yeah.
00:01:02.174 - 00:01:47.964, Speaker A: So this is, this is the, this is the book by Pavlo and Parrillo, Gregory and Beckhurst and Thomas, by these people called semi definite optimization and convex algebraic geometry. So, so you can see there's, uh, these chapters. Uh, yeah, so they are, yeah. So the first. The first two chapters is very basic stuff. Chapter one, chapter two. And today I will mainly focus on chapter three.
00:01:47.964 - 00:02:12.113, Speaker A: So I'll be roughly. I will. I will be roughly following this. This chapter. Yeah, and so I'm not going to talk about the other chapters. Maybe a little bit. Okay, so you know that in the book, this book is.
00:02:12.113 - 00:02:59.518, Speaker A: So let's go back to the. To my. To my. Yeah, to my screen. So we consider the polynomials in n variables with real coefficient. So we say a multivariate polynomial px one xn, is non negative if it takes only non negative values. So that is the following.
00:02:59.518 - 00:04:02.454, Speaker A: We have a piece x one x x two xn is a negative for all x one xn in the real. So there are three types of questions. The first question is the decision question. So, given a polynomial px, how do we decide if this polynomial is a, is non negative? And the second is the certification problem. So we ask, it is possible to certify a negativity efficiently. So it means if a polynomial is non negative, you want to certify it to convince someone else that this polynomial is indeed and negative. And the third question is the computational complexity to decide polynomial and activity.
00:04:02.454 - 00:04:48.074, Speaker A: So these are the three type of questions we want to study. First, let's see some basic, some definitions. So a definition letter and Pn 2D be the set of. Actually, let me share my screen. I think I can share my screen, including my computer. One sec. I think this is better.
00:04:48.074 - 00:05:07.914, Speaker A: Okay.
00:05:09.954 - 00:05:11.134, Speaker B: Yeah, looks good.
00:05:11.434 - 00:05:59.594, Speaker A: Now I can. Now I can use this. I can. I can share the screen and then use. No, use the zoom whiteboard to draw something if I want. So, yeah, so the basic definition first let p n 2D be the set of non negative polynomials in n variables of degree less or equal to 2d. So when I say degree, I mean all the other summations of the.
00:05:59.594 - 00:06:55.698, Speaker A: Of the degrees for each variable, right? So if I have a variable have a monomial x two y one, then this degree is two plus one is three. Or if I, if I have a monomial x two y two, then the degree is two by two plus two is four. So this is the, so this p and 2D is the set of non negative polynomials in a variable of degree less or equal to 2d. So there are theorem that is p n 2d. This set of non negative polynomial is a proper convex form which, which is easy to see. So it means it's close, pointed and solid. Okay, so let's say, let's say an example.
00:06:55.698 - 00:08:22.958, Speaker A: How can we show that a polynomial is non negative? So we consider this a quadratic polynomial. P's px equals one half times x transpose ax plus two transpose x plus c. Well, it can be shown that this polynomial px, it's positive is the negative if and only if this matrix a b, b transpose C is positive semi definite. So, so the, so positive semi definitely means all these polynomials are, all the eigenvalues are negative. Or equivalently, you can see if a equivalent, a matrix, a symmetric matrix is part of same definitive if for all x x transpose times a times x is a negative. So the proof is, how do we prove this? Prove this claim. So Ab B transpose C is negative.
00:08:22.958 - 00:09:04.114, Speaker A: It implies x transpose one. Ab B transpose C times x one is a negative. So for all the x in Rn, and then we expand this, expand this quadratic term. So this is exactly px being negative. So this is one direction.
00:09:05.414 - 00:09:08.074, Speaker B: By this you mean positive semi definite, right?
00:09:08.614 - 00:09:09.406, Speaker A: Yeah. Yeah.
00:09:09.510 - 00:09:10.274, Speaker B: Okay.
00:09:10.734 - 00:10:11.084, Speaker A: Yeah. This, this matrix is greater means it's a positive semi definite. Okay, but x x transpose one, this is a scalar. So this is, yeah, yeah. Okay. And then for the other direction, so, so if I have a, yeah, so, so if I have a, uh, px positive, for px non negative, then I have x one. This one is a positive for all x in Rn.
00:10:11.084 - 00:10:45.624, Speaker A: And that means, uh, uh, well, that means that, that means actually this one. So x transpose zero times this matrix times x transpose zero. It's the negative. Why, why we have here, here we have one. Here we have zero. I did. This is one, but this is zero.
00:10:45.624 - 00:11:46.114, Speaker A: And that's because if this thing is, it's strictly negative. Then I can perturb this zero, right? I can perturb zero by actually small epsilon. And then this will be still be, uh, this will still be strictly negative. And therefore I can scale it, scale it, scale this, this epsilon, right? I can scale it. So it multiply a number, some number, some very large numbers. So it becomes one. So, so immediately I will have something, I have, I have something like one and x transpose here, and x one transport one times ab u transpose c times x transpose one is being strictly negative.
00:11:46.114 - 00:13:08.984, Speaker A: So this contradicts the first, this condition. So therefore x transpose zero times this matrix. This big matrix times x zero is positive. It's a negative for all x in Rn. Therefore, by the definition of, of the polygon semidetic matrix, I claim that ab b transpose C is positive semi definite matrix. So I proved this claim that for quadratic polynomials in n variables, it's being an active, it's equivalent exactly as a equivalent to an SDP optimization problem. Okay, so, so however, this is a very nice, so, not, not, it's not case for other polynomials.
00:13:08.984 - 00:14:26.014, Speaker A: For other polynomials, we may not have this nice property that negativity is equivalent to positive semi definite. So therefore, we introduce, we will introduce some squares. So the definition of some squares is such that px is equal to k equals, it's some squares, right? So it's equal to the qkx and k is from one to m. So each qk square is so qk squared, x is a square and then px is a sum square. So there's a notation of the sum squared polynomials in. So sigma sum n 2d denotes the set of some square polynomials in variables of degree less or equal, less than or equal to 2d. So we have this here.
00:14:26.014 - 00:15:56.284, Speaker A: But so we know from Hilbert theory that the negative polynomial is the set of the negative polynomial is equal to the set of some squared polynomial only in the following three cases. First for union, invariably polynomials, second for quadratic polynomials, and third for bivariate cortex. So, binary coordinates, I mean, there are two variables and the degree degree is four. Maximum degree is four. Okay, so, so for all the other cases, they are always, there always exist non negative polynomials that are not sum squares. So here's the example. So, muskine polynomial, which is x four, x four, y two plus plus x two plus x two, y four plus one minus three, x squared, y squared.
00:15:56.284 - 00:16:58.034, Speaker A: So we can verify this negativity of the polynomial using the geometric arithmetic mean. Right? So, so I have a, so three, a plus b plus c divided by n is greater or equal to the third root. The third is greater than or equal to a times b times c to the power of one third. Yeah, this is three. So generally this, you can replace this three by n. So that's, that shows that. And then multiply this by three, both sides by three.
00:16:58.034 - 00:18:25.684, Speaker A: And then by putting the right hand side to the left hand side, then we get that, that the mosquito polynomial is then negative. But this polynomial, Moskin polynomial, is not some squares, unfortunately. Um, so how do we prove it? So we use the so called Newton particle to prove this claim. So what is the Newton particle? The Newton, python is the, the Newton part of our polynomial is a complex hall of, of the points assigned to each monomial. So let's say this example, let's see, so for our masking polynomial, right? So the, you see this, this degree, the biggest, the first term is x four, y two. And second term is x two, y four x. Therefore, so we have a point here, we have a point x two, y four.
00:18:25.684 - 00:19:18.648, Speaker A: And then we have a point here, x four, x four, y two. And then, and then of course we have zero, zero. So then you draw the convex call. So you just connect all these nodes, all these points, and then all the other monomials of our, of the, of the, of the mosquito polynomial is inside this, the convex car, the, the blue, the blue triangle, right? So, so now we can see that. Let f be a sum square. If f is a sum squared. So f is equal to the sum of gj.
00:19:18.648 - 00:20:26.734, Speaker A: So gg is a, is a square. And then what is the Newton power of monomials? Uh, g of the monomial in, in the, what's new to, what's the Newton power? Top power, top of the squares. The is, uh, is, uh, this g g. This is g, right? Uh, so, so then it turns out, it turns out that this Newton point of f is two. The Newton poly top of s is two x, where x is the Newton part of the, of the squares g. The gg. So in this, in this picture, the blue triangle is the northern part of the, uh, moskin polynomial.
00:20:26.734 - 00:21:15.194, Speaker A: And if, if mosquito polynomial is some squares, then this blue triangle, sorry, the red triangle, the red triangle is the Newton polytope of the, of the squares. Of the, of the squares. Uh, and you can see this red, the red triangle is exactly one half of the blue triangle. So therefore, if muskin polynomial is some squares, then we can, certainly, we can write this as. Right, right. As this form. So mxy is equal to the sum of ax squared y plus b x squared plus cxy plus d.
00:21:15.194 - 00:22:00.184, Speaker A: And under this x x square y x y square xy and d are in this red triangle. And this is impossible since the must be polynomial x. Let's go to the must be polynomial. Here we have the coefficient of x square. Y squared is negative three. But here we get the coefficient of x squared. Y squared must be, must be some c squared, right? The sum of c squared.
00:22:00.184 - 00:22:30.892, Speaker A: So this c squared must be a positive on a negative. So that's a contradiction. Okay? That's the proof that why the mosquito polynomial is not a some square. So, so, okay, so so far, are there any questions? It's good. Okay. Okay, good. No question.
00:22:30.892 - 00:24:02.854, Speaker A: Then I will proceed. So now I will talk about how using semi definite programming to certify some squares, right? So we, instead of just using handwritten proof, so let's first see this example. We consider this unumeric polynomial. And suppose this and what, and we can write this px in this matrix form, right? Matrix product form, so, yeah, and this matrix is a symmetric matrix. So, so px is one x x squared plus times this symmetric matrix in the middle and one x x squared. So, so if, if, because for union various polynomial, some negativity is equivalent to some squares. Therefore, uh, if, if this, let's call this matrix, that's q, this, this big matrix as q.
00:24:02.854 - 00:24:58.244, Speaker A: Therefore, if, if this px is a nine negative, then there must exist the q, which is positive. Semi definitely. So, so now we, we match the coefficient, right? We spend this, expand this product, this product, and matching the coefficient. So q 22 is x four, right? So this is. Yeah, so q two is corresponding to x four. So which means q two is one. And for x three, so we have so two q and two two q one two is four.
00:24:58.244 - 00:25:45.144, Speaker A: And for x squared, so x squared is x times x. So which would be q eleven plus x two times one. So there are two of them. So q eleven plus two, q zero two is equal to six, right? So that's the, that's the coefficient, correspond to the x two x two term. And for x term, x is two, q 10 is equal to four. Yeah, and q zero, zero is equal to five. So now we have a SDP feasibility problem.
00:25:45.144 - 00:26:28.594, Speaker A: So, we want to find a q which is part of semi definite subset of these conditions. This condition one, all these conditions are the linear constraints hold. We can, then we can solve, can throw it into SDP solver like in Matlab. And then we get this solution. So, q can be refined as q. Q is this matrix and it's positive semi definite. So it can be written as v transpose times v, the v is this matrix.
00:26:28.594 - 00:27:37.244, Speaker A: So, so q can be, therefore, px can be written as a three sum squares, a sum of sum of three squares. So first term is x squared plus two, right? So, which means here, this, this two is correspond to x and one is the coefficient of x squared. So the first term is, the first square is the come from the first row. So two, which is x squared plus two, x plus zero, and second square comes from the second row, which will be square root x square root of x plus square root, and then the squared. So there is a two and then similarly for the third term. So this is the example for the univariate case, for univariate polynomial. Now let's see a multivariate example.
00:27:37.244 - 00:28:48.116, Speaker A: So let's say we have a bivariate polynomial pxy. This can be done similarly. So we want to verify, we want to see if this polynomial can be written as some squares. Then again, we first write this, write it as a matrix product form. And then, equating the coefficient, we equate the coefficient of the monomials and there are n plus d plus d, which is, which are 15 affine constraints. So that's another, and then that's, and then there's a equivalent to SDP problem. So now we see, how can we, how can we replace the negativity constraint as a sum squares? So let's see the application.
00:28:48.116 - 00:29:56.964, Speaker A: Right. So how do we apply this, some square technique to polynomial optimization? So first, let's say this, the following question. So we minimize px. This is, so px is polynomial with a unitary polynomial, and there's no, this is an unconstrained optimization problem. So no under constraints. So a simple observation is that a number gamma is a global lower bound of px if and only if px minus gamma is non negative. So this is, gamma is the lower bound of this optimal minimizer if and only if px minus gamma is the negative.
00:29:56.964 - 00:31:27.314, Speaker A: So now we can convert our minimization problem into a maximization problem. So instead we maximize gamma subject to px minus gamma being the negative and for all these, for all x in r. So this, this, so this problem is a, is a convex problem. This is the feasible set. The field set is defined by an infinite member of affine string because they are, there are infinite values of many values, infinitely many values of x, right? So each value of x in the real, and then we plug in x in and we get affine constraint for gamma, and there are even many of them. So this is the convex problem. So that means we can, we can convert this into a, some square program.
00:31:27.314 - 00:32:44.934, Speaker A: So as you can, as we see previously. A universe polynomial is negative is equivalent to cm, so px minus I is negative is equivalent to saying px minus gamma is some squared. So this is HTTP problem and which can be solved by stp solver. Okay, so let's say, let's say an example. So for example, px is x four -20 x squared plus x. And then we want to minimize px. So instead we maximize gamma such that x four x -20 x squared plus x minus gamma is nine negative for rx belongs to r to it and this, and then this is equal to some square problem which is equivalent to SDP.
00:32:44.934 - 00:33:49.584, Speaker A: So we max gamma such that this, this q, this one x squared, default this is, this one holds. And so q is the volume q is. So the top left is minus gamma because we have a minus gamma here. So this, the top left is minus gamma. And then, and then we have other constraints by equating the coefficients of the monomials. So, so q three equal to one, that means, so that means q three is the coefficient coefficient of x four, so x, so q three is one. And then there are other, there are some other constraints.
00:33:49.584 - 00:35:35.354, Speaker A: So in the end we got this, we will present this stp problem. So you maximize minus q zero, which is the top left element of q such that q is part of semi definite with this affine constraint. Okay, so this is a univariate varied example. Now let's look at the multivariate case. So the multivariate optimization, so, similar to the univariate we have this we can convert when we minimize the unimprivate polynomial, we can maximize gamma subset delta p x one delta xn minus gamma is the negative. And then there's a, there's a, we, we approximately approximate this problem by some square problem. So, so this, so instead we say px one delta x n minus gamma is a sum square.
00:35:35.354 - 00:37:06.824, Speaker A: So, which is obvious that obviously, obviously the p some square psos is less or equal to p p because psos is the, the sum square program, since some square implies non negativity, but negativity may not implies some square for multiple cases. So this, some squares. This one has a smaller feasible set than this problem. So since it has a smaller feasible set, the maximum value must be no greater than the previous than the first one. So we have this e party. But fortunately for many relatively small problem, we often have p s p some squares is equal to p star. In situation where p's os some square is strictly less than p star.
00:37:06.824 - 00:38:50.864, Speaker A: What we can, we can, we can actually produce stronger s to s, some square condition that will converge to, to p star. We will talk this, we will talk about this later. So, we will see how we, how we can solve a series of some square problem and such that this, it converges. It eventually converts to the optimal value of the, of the previous, our previous, uh, our polynomial optimization problem, right? Uh, yeah, so, uh, yeah, so, so uh, so now let's talk about this some square of rational functions. So let, so let qx be a sum squared. Qx equals sum over q I x squared. Then we have a qx times px equal to the sum of sjx squared.
00:38:50.864 - 00:40:16.314, Speaker A: So if qx times px is some, some squares. Uh, yeah, so, so yeah, uh, then uh, then qx times px is the summation of sgx and there, and therefore, uh, we can write px as a px equal to. We divide qx from both sides. So initially px equal to, you know, after some calculation. So, so we put qx here and then, so one over qs becomes two x divided by q x squared. And then since qx is the sum squares, so we replace this qx by some q I x squared. Eventually that we have qx times px is some squared.
00:40:16.314 - 00:42:18.554, Speaker A: So it's sum of rational squares. So yeah, so, so yeah, so if qx px is some square, then px is some rational squares. So what this used for, it used, it tells us that if px is not a sum square, we can multiply px by qx where qx is the sum squared. And then we can, if qx, if after multiplying px by multiplying qx, if this product qx times px is the sum squared, then we can actually write px as a, as the sum of a rational, some of sum squares of rational rational functions, right? So, so on. Or the other way, if, if we can write px as a, as a sum square of rational function, then we can multiply the common common, the biggest modified the denominators and we can modify the denominators of the rational functions. And then, so this qx, I e. The qx, right, qx is the can be divided by, by all these, all these rational, by the auditing denominators of the rational function, the rational rational squares.
00:42:18.554 - 00:43:33.154, Speaker A: So qs is the greatest GCD, the GCd of all these other denominators. So this is the famous Hilbert 17 problem, which is proved by atene such that every non negative polynomial has a representation as a some square of rational functions. That means for other nanic polynomials, we can find a q x such that qx is a sum square and the qx times tx is some square. So again, and then this can be solved by SDP. But of course we don't know the degree of qx. So. So the degree of qx could be very large.
00:43:33.154 - 00:44:28.634, Speaker A: So in practical way, we will, we actually, we actually, first we fix the size of the, of the degree. We give a degree bound of the monomial in Qx. And then we said have this some square problem. And then we increase the size the degree bound. So we have a qx larger and larger degree. And eventually, eventually, if px is negative, then for the degree bound, largely enough, this problem will be, will be feasible. This problem can be found by an SDP solver.
00:44:28.634 - 00:44:51.194, Speaker A: So this is, how can we certify. This is a way to certify the net negativity of a multivariate. Multivariate polynomial. Okay.
00:44:53.334 - 00:45:17.104, Speaker B: So, faye, just to sort of make a pause here, so essentially, if px is sum of squares, then. Sorry, if px is sum of squares, then what? So this is if and only if. But when we started out, we had one direction that was simpler.
00:45:17.724 - 00:45:25.916, Speaker A: Yeah, so, yeah, so we have, so suppose we have a px is the negative, right?
00:45:26.100 - 00:45:26.864, Speaker B: Yeah.
00:45:27.244 - 00:46:21.944, Speaker A: And then by the Hilbert 17 problem, we know px can be written as a some square rational functions, right? So, so, and then we just multiply p multiply by the GCd of the denominators of all the rational squares. Then, which is q x. Let's say it's q x. So, so by going this direction in reverse direction, we have a. We can have qx times px is the sum squares. So px is some rational function, but we can find the qx so that we q x, modify px is the sum of square polynomials instead of rational functions. Yeah.
00:46:21.944 - 00:46:38.742, Speaker A: So we cannot actually directly solve some square of a rational function. Right? So if px rational sum square of rational function, then you multiply by qs to make it a sum of square polynomials.
00:46:38.918 - 00:46:39.754, Speaker B: Okay?
00:46:40.974 - 00:46:42.194, Speaker A: That's the idea.
00:46:43.374 - 00:46:54.554, Speaker B: Okay. Now are you later on going to connect this to, you know, sort of giving certificates for the non existence of real zeros and things like that or existence of real zeros?
00:46:55.684 - 00:46:57.588, Speaker A: Yeah, yeah, yeah. I'm going to.
00:46:57.716 - 00:46:59.172, Speaker B: You're going to do that later? Okay.
00:46:59.228 - 00:47:03.224, Speaker A: Yeah. You feel.
00:47:06.564 - 00:47:13.744, Speaker B: Okay? No, go ahead. I just asked the question. You can continue in your own pace. Yeah, yeah, yeah.
00:47:14.484 - 00:47:53.134, Speaker A: So this is the. Yeah, this is. Yeah, this is. I just want to. This is certified a way to solidify the negativity of a multivariate polynomial. So multivariate polynomial, you just solve a sequence, a sequence of some squares or SDP relaxations. And then if we the size of SDP, it's large enough, then eventually we will find these some squares.
00:47:53.134 - 00:48:55.492, Speaker A: So, yeah, so this is the, so, to summarize, we talk about this certificate of a univariate polynomial, which is easy, which is just a simple some square problem, and the certificate of the negativity of a multivariate polynomial. This is a little bit tricky, but still, instead one SDP, you solve a sequence of SDP or some square problem. So this is a, this is a deficient activity. Now I want to talk about the negative onset and the constraint optimization. Before I was, it was, this is an constraint optimization. Now let's talk about the constraint optimization. So there are two parts.
00:48:55.492 - 00:49:48.544, Speaker A: First part, there's sufficient condition and necessary condition, and severe condition means. So we want to find a certain, sufficient condition to, for the negativity of px over or over set. So we consider this set. Set is, let's first consider set is set is defined by these equalities. So the problem is to decide if px is non negative on the set s. So a certificate condition for this is true or px is negative. Rs is the, is the following.
00:49:48.544 - 00:50:48.404, Speaker A: So px with some multipliers, so px plus some I equal one to m g x f I some squares. This is easy to see. We are just, if this is some square, and then on this s, then all this f I managed. So px will be negative. So this is, again, this is a, this is sufficient condition. And then we can, how do we actually solve this problem? Right? To solve this, of course, we have to fix the degree of lambda I x. So by fixing the degree of lambda x, so we have some square problem.
00:50:48.404 - 00:51:56.518, Speaker A: So SDP problem. So the unknowns are the coefficient of lambda x. And then we write this as a matrix product form, and then some of this stp is a visibility problem which unknowns from lambda x. So in a more algebraic cycle, this sufficient condition can be written as some square over quotient ring. So px is at some square quotient is ideal fix. So the ideal is this fix. It's ideal generated by all these strength fx.
00:51:56.518 - 00:53:38.964, Speaker A: So yeah, so now let's see, how can we use this question of operation can be done using the governor basis of the IPI, right? So let's see an example. So we wanna, we wanna and see how can we, how can we, how do I, how do we convert this to SDP? And how, how do we quotient this idea within quantum basis? So let's say we want to decide if p is ten minus x squared minus y is the next over f. This f is x squared plus y squared minus one is zero. So, so instead we want to rewrite this polynomial at ten minus x squared plus y as a matrix product form. And then we expand it. Now we can module, we do the modular operations. And this step, in this step, this, this can be done by gravity basis.
00:53:38.964 - 00:54:45.004, Speaker A: So basically you replace, so y square is replaced by one minus x squared, right? Because x squared plus y squared is one. So, so y square is replaced by one squared. And that's the only operation we need to do. Now, after this modular operation, so we have less monomials and then we equate the coefficient of monomials. So q eleven plus q three is a, it's ten, right? This is, this is the constant term. And q twelve is zero. Q 120 is the equation of x.
00:54:45.004 - 00:55:27.408, Speaker A: Here is no x. So this is zero. So q 13 is y and there's a y negative one here. So two q 13, negative one, etcetera. So, and then we solve this some square problem. So this, you want this matrix to be positive semidefinite. Then we get this, this, this we can, we have this expression for our polynomial.
00:55:27.408 - 00:57:01.814, Speaker A: So ten minus x squared plus y is equal to this, some squares over the ideal I. So this is this certified data. Ten minus x squared minus y is in effect indeed non negative over the constraint. So that's, so this quotient over, over this quotient, operation over ring is definitely more powerful than simply fixing the degree, right? So if we fix degree, then we have to know we only have a small subset of candidates. So, yeah, so this is how we do it over ring without, instead of fixing the, fixing the bound, yeah, instead of giving a degree bound of the monomials. Yeah. So this is the, now I want to talk about survey condition for inequalities.
00:57:01.814 - 00:58:22.594, Speaker A: So instead of inequalities, we now have a bunch of inequalities, which is in this form. So we have a bunch of g one x is negative, g one, g two gm negative. And of course sufficient condition for px in such that px can be written in this, this following form. Px equal to x zero x plus sum of x I x g I x, whereas zero, x and x, they are all some square polynomials. So you can see if this is some squares, then, then px is a certain negative, right? Yeah. X zero x is non negative as I x is non negative and gi x is non negative from the constraint. And of course this can be done similarly using the subscriber program or SDP problem program.
00:58:22.594 - 00:59:38.324, Speaker A: This is an example which I want to go through. So in fact, we can have more powerful expressions. In fact, instead of using this form, we can have more powerful form, which is we have a, we actually have a product of the constraint we want to px in this form. So where px equal to s zero x plus some of xi x. So under here is, this is gx. Here we have gi x times ggx. And basically we can have a two to the power of m many, put many many terms, because, which is the subsets, the number of subsets of the m e quantities.
00:59:38.324 - 01:02:16.894, Speaker A: Yeah, so, so, yes, so the, so far I talked about this sufficient condition, right, to solidify the negativity over constraint over, over some constraints. Now let's talk about the necessary condition, right? So what's the necessary condition? It means that actually, so if he says if px is the negative, then what condition do we have, right? So therefore, if there's a, if it doesn't satisfy this, if that doesn't satisfy this, the necessary condition, then we can show that this polynomial is not a negative one. And this next definition is usually a feasible feasibility problem. So, yeah, so therefore we want to actually use the necessary condition to show that this is not negative. We have to certify, we have to do this called infeasibility. So to take, because the necessary condition for p, for p being, for polynomial being negative, is a feasibility problem. So to disprove this claim, we have to use to do this infeasibility certificate.
01:02:16.894 - 01:03:20.958, Speaker A: That's why I call, I call, I call it, I call the nest recognition as a, this is, this might be nestled condition is called infeasibility certificates. Yeah. So let's say we have this first of all, linear equality. We consider the linear equation linear system x xb x equal b. Then x b is infeasible if only if there exists u subs that a transpose u is zero and b transpose u is negative one. Okay? And this, well, like the. So the one direction is easier.
01:03:20.958 - 01:04:15.484, Speaker A: If, if a transpose u is zero, b transpose u is negative one, then we have u transpose a. U is, is, uh, it's b, u transpose b. And uh, yeah, just from here I multiply this by u, by u transpose, multiplied by u transpose, and this becomes zero. This become a negative one. So zero is not a negative one. Yeah. So therefore you can say if this condition holds, then certainly xp is infeasible.
01:04:15.484 - 01:06:07.374, Speaker A: Okay, this is the, if this is the linear equality and for polynomials over c, we have this theorem called Hilbert non standard sets. Let fix fi fiz del the fmg be polynomial in complex variables z one j. Then fi is zero being zero. This is infeasible in Cn if and only if negative one is belongs to the ideal generated by f one delta fm. So and then for linear inequalities, we have this tax lemma which says if ax plus b equals zero, x plus d is negative. If this is infeasible, then this means if and only if we can, there exists lambda u is arbitrary. So that this condition is, this condition holds.
01:06:07.374 - 01:07:25.464, Speaker A: So when they actually right from, from this section to direct this direction from this section from this claim to this claim, it's easy, which is, this is eventually the weak value of the linear programming. And from the invisibility to this, this condition, this is a difficult condition. It's inventory equivalent to the strong derivative of linear programming. Okay, so, so these are the classic, these are classic results for infeasibility certificates. So we have to summarize. We have, I have nice if it is a beta certificate for linear equalities, for linear inequalities and for polynomials over complex. Yeah.
01:07:25.464 - 01:09:12.661, Speaker A: Uh, well now we want to, we have this, now we want to, uh, generalize this to rules, right? So this is classic result. These classic results actually can be generalized to, to handle the case of citizen polynomials, equations and the e equation over the or the real numbers, uh, which is called a theorem called the positivity standard size, which says if fi, this fi is zero. So we have a bunch of equalities, m equalities and a bunch of inequalities Gi x negative. This is infeasible in rn in real if and only if following the truth that there exists the fx and gx in Rx. Rx is the multivariate polynomial ring subset fx plus gx is negative one and f is in the ideal of f one generated by f one. The fm and gx is in the pre order of g one the gp. So what is the pre order? Pre order the definition pre order of a bunch of polynomial is following.
01:09:12.661 - 01:10:41.674, Speaker A: Uh, so see the point order of g one g two. Gm is consists of other polynomial g such as g is s zero plus the sum of the first term is si times gi. And then they have two indices sig plus times gi times gg and sig mgk times three, product of three gigg and gt and et cetera. So, and where is this coefficient s zero s I s I g s I j k. These are all sum squares, okay? So the s alpha, this equation s and this s are some squares. So, so the, the sum is finite. Uh, so they're with two to the power of m terms, uh, because we can say this I g k, they are the subsets of the, of gi g to gm, and there are two to the two power m subsets.
01:10:41.674 - 01:12:02.834, Speaker A: So the sum is, has the two to power m terms. So this is the pre order, this is the definition of pre order. So this, this infinite, this invisibility, if one solidify this invisibility, you see the beauty of this polynomial constraint. And then this equal to actually a, some square problem, right? Yeah. So let's say an example, let's say example f one, f g one g two. So f one is x squared plus x two squared minus 10 g one is three times x two minus x one. Uh, to, uh, uh, to the power three minus two being the negative g two is x one minus eight, x two, uh to the power three is being negative.
01:12:02.834 - 01:13:06.324, Speaker A: Now, now the bisector, so we want to find if this is infeasible. So, so by positive, positive, stellar says this system is infeasible if and only if there exists polynomial t one s zero s one s two s twelve in this, in our xy two such that this condition holds, right. So here we have this condition holes, this condition fx plus gx. And this is a, this is the pre order of g one g two, which is gx. And here, here is f one. T one is the ideal, the ideal of f one. It comes from the ideal of f one.
01:13:06.324 - 01:14:09.446, Speaker A: So here we have, this is f x and, yeah, and there are only how many terms, right? 1234, because we only have two equations, two inequality. So two to two is four. So four terms from this summation. Yeah. So now how do we actually, then this, how do we actually use this condition? How do we actually, to show that there exists such a polynomial, there exists this polynomial such that this condition holds. So first we need to, we need to bound the degree of other monomials by d. The degree bound is given by capital d.
01:14:09.446 - 01:14:59.316, Speaker A: So all these monomials, f one times t one degree is less or equal to d degree s zero degree s one g one. All these are 1234. This five times the degree of the monomials in this on the left hand side is bounded by the capital d. Now, we now, given a degree bound, we actually, we can. For each degree bound d, there's a corresponding some square std problem, right? Because this s zero x one x two x twelve. They are awesome squares. This is an underdog.
01:14:59.316 - 01:15:44.610, Speaker A: Unknowns are under. Yeah. And the number of unknowns, the number of unknowns, which is. Which is actually. Which is just the coefficient of f one s zero x one s two s twelve. These are the number of the unknowns are bounded by this degree, by degree d. So this is the corresponding std problem, or sn square problem.
01:15:44.610 - 01:16:38.614, Speaker A: So, for instance, when we left the grid bound b is equal to four, then we actually resolve this cross module some square and SDP. We did find this is feasible. So we get this t one s zero and s one s two x zero. So, therefore, we have certified that by solving. By solving this. By solving this stp problem, and given this degree bound d, we have shown that this is, this polynomial system is, in fact, infeasible. But, of course, now that if d doesn't.
01:16:38.614 - 01:16:56.554, Speaker A: If d is equal, four doesn't work, you may want to consider you increase. I increase. Increase the degree bound d. Right. So, yeah, if D doesn't work, then you try five. And then if five doesn't work, you try. Try six.
01:16:56.554 - 01:17:32.152, Speaker A: Each time you have a larger, larger SDP problem. Eventually, for the large enough you will get, you will find such a significant. Given that this polynomial system is indeed, indeed infeasible. This is because of the positive standard size that must exist somewhere. Something. Given something. Some.
01:17:32.152 - 01:17:57.802, Speaker A: Somewhere, for some degree, there might exist at this polynomial. So, therefore, for d large enough, eventually, you will be able to certify its feasibility. Okay. Yeah. So that's. I think that's what you said, mira, about the. So if it's better certification.
01:17:57.858 - 01:18:03.854, Speaker B: Yeah. Correct. Yeah. Um. Yeah, that's good.
01:18:04.554 - 01:18:40.554, Speaker A: So are there any. Any questions so far? Okay, uh, if. If, uh, if no. Uh, if there's no. There are no other questions, then I will proceed first. Yeah, so far, I define. I define this pre order.
01:18:40.554 - 01:19:05.230, Speaker A: You can see the pre order is quite large. It's a product of this g one. Little is a very. It's very large set. The, like, the elements in the preorder have very high degree. Right. Because you have so many, so many terms, and each term has some.
01:19:05.230 - 01:19:54.044, Speaker A: It's a product of our other polynomials. So the degree is very large. That means our sum square, or SDP, has a very large sign. This is not a. This means that in practice, this is not good. We don't want to have our problem to be too big. So now we have, so we define a different, we introduce a different definition, which called the quadratic module, which is much simpler, simpler than the pre order.
01:19:54.044 - 01:21:05.444, Speaker A: So this definition of a quadratic modular module g, one delta gm is defined other polynomial g such that g is equal to stro plus s one g one plus delta plus s smgm. So there's no, now there's no, there are no products of the genes. So here, of course, this s zero s one sm. These are some square polynomials. Now I wanna, now study this polynomial optimization over s. Uh, so, uh, so we consider this a, this, uh, this, uh, uh, this, this problem. And uh, yeah, so we actually, we want to solve this polynomial over polynomial optimization over overset s by a hierarchy of relaxation.
01:21:05.444 - 01:22:14.384, Speaker A: So let's consider this, this optimization problem. Consider p star is the minimizer of px. Px is, is a multivariate polynomial, or s s is a constraint. It's a set defined by these polynomial inequalities. So this is generally a non convex problem, which means it's hard to solve. Uh, yeah, because, uh, this polynomial could be this non convex, this constraint, this s could be non convex. So this is a general and non convex optimization problem.
01:22:14.384 - 01:24:01.674, Speaker A: So how do we actually, uh, how do we solve this? How do you actually treat this deal with this problem? Right. So the idea is the following. We, we actually, we want to call the hierarchy of relaxation. For each, at each step, we saw a convex optimization problem, and, and then each step we have a degree bound, and then we have multiply some convex automation problem, and then we increase the degree bound at each step. So by increasing the, this degree bound, we have larger, we have a better approximation of the original nankal, and eventually we have a sequence of a monotone, we have a monotone sequence of optimal values which will converge to the global, global, global optimal optim, global optimum point of the original non convex problem. So the idea is the following. So we said this minimize problem.
01:24:01.674 - 01:25:31.204, Speaker A: We consider this, this maximize problem. So we maximize gamma. So the px minus gamma is in the pre order of g one, g two gm, right? Uh, uh, pre order here. This is just, uh, this py is just a. Yeah. Uh, so py, so you said, uh, so py minus gamma should be, should be negative, right? So this is, and they also, or we can solve this problem, p two, it's the maximize, we maximize because the maximizer of gamma. So the px, my gamma is in the q module of electric module of g one g to gm.
01:25:31.204 - 01:26:28.284, Speaker A: So, yeah, this is the two. This one. The first one is called representation. The second one is called putin representation. And now we are given a degree bound d for the, for the sum squared polynomial in pre order or the quadratic module. And then we give it. We give it to b bound.
01:26:28.284 - 01:27:14.964, Speaker A: Then this. Then we can maximize over gamma via the, through the sum squares, and it's SDP problem. So. So, yeah, and then this here actually proved that as our degree d, degree d increases, then this p two. P two actually converges to p star. And I think similarly, p one also converted to p star.
01:27:15.704 - 01:28:01.104, Speaker B: So, yeah, at something like the deep level or after laser hierarchy or so's hierarchy, you're supposed to get polynomials of degree two times d or something like that. I mean, then you have to lose something. There's no free lunch, right? I mean, if you're convexifying, then whenever you do convex relaxation, somehow the complexity, you're increasing variables, right. You're adding r. Yeah. Right. But also, somehow the degree should go up.
01:28:02.004 - 01:28:03.624, Speaker A: Yeah, yeah.
01:28:04.364 - 01:28:05.824, Speaker B: Or something like that.
01:28:07.244 - 01:28:42.844, Speaker A: Yeah. So the grid for the sum squared. Right? So in this pre order or the creating module, there is a. There is this, you know, some squares, right. Of course, if we don't have the degree bound, then this is equivalent. But then in theory, in practice, we can only solve it for given degree bound. So, yeah, so we increase.
01:28:42.844 - 01:29:01.544, Speaker A: When you have the degree one larger, then this become. There are more monomials. Right? So, yeah, so then we, we have better approximation, but the computing computational complexity is much higher.
01:29:04.964 - 01:29:06.064, Speaker B: Okay, thanks.
01:29:06.444 - 01:29:35.904, Speaker A: Yeah, yeah, that's you. You do something when you increase the degree, but degree, the degree bound you get, then the sum squares here evolve, have. Have a larger. Have more monomials. Yeah. Therefore. So here, this is actually called the hierarchy, right.
01:29:35.904 - 01:30:34.494, Speaker A: But let's say hierarchy basically prove that you can actually approximate these. This global is a. You can find this global optimization of a polynomial optimization problem in this form by solving a sequence of this convex problem. And the px gamma is in the, in the truncated, in the truncated correct module. So the degree is called truncated because we actually. We bound the, we bound the degree of the sum squared term, I think, for preorder. Although.
01:30:34.494 - 01:31:05.608, Speaker A: Although he didn't prove the Schmidt and representation, but similarly, I think it's true. It's true. It's true also. So generally, you can solve it using both. And both the representations, and then eventually it will converge. But. But it could be arbitrary close.
01:31:05.608 - 01:32:38.576, Speaker A: Right? It could be arbitrary close. There's no guarantee that this will stop at some finite, finite degree, finite degree d. So you could, instead you could, you keep increasing degree bound d and then you are getting better, better approximation, but you may never have actually reached the global optimal. So that means, so now actually to study then to study this kind of problem, and we want to study this finite converters of hierarchies. So we want to know when under which condition is you get an exact approximation. So for some degree d, convex approximation, exact, exactly solves the convex optimization problem. Uh, so, so, uh, so, uh, yeah, so, uh, uh, so I say first we need a definition of the archimedean.
01:32:38.576 - 01:33:13.600, Speaker A: Uh, definition is the following. A quadratic module is Archimedean. If there exists, uh, r the natural number such that it's r minus the sum square, that's the sum of xi squared. This is in the kinetic module. So we see this model has this archimedean probability. If we satisfy this condition. Yeah, yeah.
01:33:13.600 - 01:34:07.914, Speaker A: Note, if, if this uh, module has its archimedean, then this s is compact. And because s is contained in the ball, in the ball, that this sum of x as I squared is less than or equal to n. So this s is compact. If it's actually compact, then we can prove lots of nice properties. So now we can study general problem of this polynomial minimization problem. F I. Minimize f I, such as hi x is zero.
01:34:07.914 - 01:35:22.474, Speaker A: There's m one equalities and there are three. There are m two inequalities, right? So minimize f I f x h I x is zero, gx is the negative. So and then we consider this a hierarchy of square relaxations. So we want to maximize gamma such that, yeah, so that it is, yeah, sorry, I think there's something missing there. Yeah. Q. Yeah.
01:35:22.474 - 01:36:45.116, Speaker A: Px minus r. Yeah, yeah. Fxffx minus r, right. Minus gamma. Yeah. So maximize gamma such that f minus gamma is in this h two k plus q kj. So where this h square bracket h square two k is the two case truncated ideal generated by h, right? Yeah, for truncated ideal, I mean all these idea that f subscript h is h one, h m one, and then hk, it's two.
01:36:45.116 - 01:37:48.924, Speaker A: K is the, uh, is the, it's all these, uh, h two k consists of other polynomials, uh, like p as p something that p x is equal to, uh, t one x h I x plus data plus t m x. Hm. And the degree degree p is less than or equal to two k. Yeah. Just truncated means all the, you just trunk is ideal, right? Ideal. You just, throughout all these polynomial that has degree larger than two k. And so is it.
01:37:50.064 - 01:37:58.484, Speaker B: Sorry, is there another version of this hierarchy where you bound the degree of the t I's?
01:38:00.984 - 01:38:09.104, Speaker A: Yeah. You bound degree of ti. Eventually you're bounding the ideal, right?
01:38:11.404 - 01:38:13.824, Speaker B: That's true. But they are slightly different, right?
01:38:14.324 - 01:38:40.764, Speaker A: Yeah, yeah, they're slightly different. Yeah. I think this is the version of bounding the degree of the ideal. Yeah, but previously I'm talking about I'm finding the degree of the sum squared. Yeah.
01:38:45.744 - 01:38:48.124, Speaker B: And it's the same definition for qk.
01:38:49.024 - 01:38:55.084, Speaker A: Yeah, qk is the same thing. It's the case truncated quadratic module generated by g. Okay.
01:38:59.154 - 01:39:00.254, Speaker B: Okay, thanks.
01:39:06.794 - 01:40:39.324, Speaker A: Actually. Yeah. And then, and then we have this theorem by, so that under this condition, Archimedes condition for this. So that means r minus some square I squared is in this chung tl, h two t plus qt for some tip in the natural numbers by being integers and the r is positive. And then if the, if the constraint qualification, strict complementarity and second other sufficiency condition holds and then at every global, global minimizer of the blue one. Right, the original non convoluted problem, then the hierarchy has finite converges. So it means for some finite, for some finite k, this, this some square problem exactly solves the, solves the non convex optimization problem.
01:40:40.984 - 01:40:43.764, Speaker B: So are there any bounds on the number of levels?
01:40:48.104 - 01:40:55.344, Speaker A: Yeah, yeah. There is a, I think for some particular problems, but it's not a general.
01:40:55.424 - 01:41:12.152, Speaker B: Condition on the original. Whatever. I guess I can't see properly here, but this original, the original problem. Okay. Yeah, yeah.
01:41:12.328 - 01:41:22.760, Speaker A: But for some, like zero, like zero one, like integer programming problem, there is actually a degree bound.
01:41:22.912 - 01:41:23.764, Speaker B: Okay.
01:41:24.104 - 01:41:40.274, Speaker A: Because let's see, I can, I can write a, I can write a, you know, if, if I have x being zero one, then this can be replaced as a square minus x equals zero.
01:41:40.814 - 01:41:42.194, Speaker B: That's right. Yeah.
01:41:42.774 - 01:42:12.524, Speaker A: And so suppose I have an integer programming problem. I maximize some, some fx such that x is zero one. Then I replace this, this one by a polynomial. By polynomials. Can I use this some square hierarchy? There is actually a degree bound for that. I think the CEO has a paper. Yeah.
01:42:12.524 - 01:42:45.772, Speaker A: Has paper. Let me just write on my. Yeah, so, so, yeah, Mercedes has a, has a paper, I think, for the boundary of the sum squared for the integer programming a problem.
01:42:45.948 - 01:42:46.784, Speaker B: Okay.
01:42:47.524 - 01:42:49.464, Speaker A: Yeah, but it is exponential.
01:42:50.284 - 01:43:04.894, Speaker B: It's exponential. But it has this, this property for it has finite convergence. That means some finite number of levels will actually be sufficient. And then you're saying that number of levels is exponential.
01:43:05.874 - 01:43:06.322, Speaker A: Yeah.
01:43:06.378 - 01:43:15.094, Speaker B: In the number of variables of the original problem for integer programming, you can always assume it's quadratic, right?
01:43:16.034 - 01:43:20.290, Speaker A: Yeah, yeah. For the number of variables.
01:43:20.402 - 01:43:22.346, Speaker B: Yeah, number of variables. Okay.
01:43:22.530 - 01:43:44.694, Speaker A: Yeah, actually parallel has a. I think. Lauren. Yeah. Monica Lawrence has a paper, has a paper improved this stunt. And then Pablo also has a paper using some group theory to improve this fund. This paper injector.
01:43:44.694 - 01:43:59.150, Speaker A: Yes. Yeah, what's the name of the paper? But yeah, anyway I can, I can find the paper later.
01:43:59.302 - 01:44:05.514, Speaker B: Yeah, maybe you can put it in the chat or something or send it by email and I'll send it to the class.
01:44:06.214 - 01:44:07.246, Speaker A: Yeah, yeah.
01:44:07.430 - 01:44:25.414, Speaker B: Okay. I mean that's a very important case of zero one programming. Yeah, yeah. So, okay, just continue please.
01:44:26.034 - 01:45:43.390, Speaker A: Yeah, so yeah, I want to explain this. Yeah. This and this condition, right. This constraint qualification. What's this? Constraint. So, so constraint qualification. It means what it means there's a subdifferential of our of Gx is linearly independent and near independent and stricter complex mentality.
01:45:43.390 - 01:46:43.844, Speaker A: Means what means lambda I lambda I g x. Whatever. Yeah. Lambda I'm as optimal solution. Lambda lambda lambda star g g x star. It is zero and three second of the summation condition. It means the high symmetrics.
01:46:43.844 - 01:48:03.614, Speaker A: It's repetitive. Definitely something like, like that, but yeah, but basically it says this, this, this property holds generically. So this property holds generically. That means for most of the problem, you see in practice, this will, you have this finite finite convergence property. Yeah, so yeah, yeah. If you all know this, there's a paper, right, there's paper by Zhang Moung. Yeah.
01:48:03.614 - 01:48:48.564, Speaker A: Optimum conditions and finite converters of racial hierarchy opt in our last year. Hi. Right, so yeah, I guess this is the. What I want to talk today.
