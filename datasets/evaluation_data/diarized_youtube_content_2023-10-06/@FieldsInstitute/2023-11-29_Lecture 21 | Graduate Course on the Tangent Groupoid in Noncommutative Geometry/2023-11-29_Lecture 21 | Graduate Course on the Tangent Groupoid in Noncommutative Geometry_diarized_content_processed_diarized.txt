00:00:00.720 - 00:00:55.256, Speaker A: So, yeah, for Andre's benefit, we were looking at pseudo differential operators on a symmetric space. And in this class, because we were following a approach to pseudo differential operators coming from non commutative geometry, in the work of Eric Vanop and Bob junk, we're calling them scalable. So scalable operators on Gmod K. It's a little better to think of the symmetric space this way, have the group acting on the right and dividing out by k on the left. Of course, it's the same symmetric space. And we were wrestling with the formulation of some theorem in K theory, which we'll get to today. I want to sort of hint at not just what the formulation is.
00:00:55.256 - 00:01:30.472, Speaker A: In fact, I want to make precise what the formulation is, but I want to also hint the proof of this theorem, because you'll see it ties together the definition of these things with the con casper of isomorphism. Everything in the whole class becomes neatly tied together. Not a moment too soon. In the penultimate lecture, everything becomes neatly tied together. Can you guys hear me out there in the Internet? All is good? Yep. Okay, good. Thank you.
00:01:30.472 - 00:02:06.484, Speaker A: We await the cinematographer. So the boards will not move. Okay, so what are these things? Well, one way, if you can think about them, is following. First of all, let's just talk about operators acting on scalar functions, not on sections of vector bundles. And I didn't write it down, but now I will. We're interested in operators which are g equivariant. I want to think, if you like, if these operators acting on the left and the group G is acting on the right, and these two things, these two actions should commute with one another.
00:02:06.484 - 00:02:26.120, Speaker A: Welcome. Thank you for coming. Because without you. Is that for me, the coffee? No. Okay. And so I want to think of them in the following way, as being given by certain distributions. That's how you supposed to write distributions.
00:02:26.120 - 00:03:55.864, Speaker A: Compactly spotted distributions. On this space came my g. So what's the relationship between these distributions and actual operators? You might also prefer to think of scalable operators as actual operators, like this. And the relationship between these two formulas is that if you take an a and you apply it to a function that I'll call f bar, and you evaluate this at the identity, I mean, the identity coset in k mod g, that's the same thing as evaluating lambda on f. And so what's going on here is that f is a smooth function on g, and f bar is the function on k mod g that you get just by averaging over k. K is the maximal compact subgroup of g, like that. How many parentheses with my.
00:03:55.864 - 00:05:22.074, Speaker A: So these guys are related by this formula. If you have an operator, you can get a distribution, and if you have a distribution, you can get an operator. And as long as the operator is continuous in this extremely minimal sense that we've been talking about, if you're interested in scalable operators, then you're not allowed to start with arbitrary distributions, because arbitrary distributions don't give these, this special class of scalable operators, pseudo differential operators. And so let's just figure out how to express the fact that some distribution lambda corresponds to an actual scalable operator. Oh, I missed off one little thing here. Let me go back and fix it up. If you want a distribution here to correspond to an operator, the distributions that you should study are those distributions which are right translation invariant by k, like that.
00:05:22.074 - 00:06:11.338, Speaker A: So once you have an a, you can build a distribution by this formula. I forgot to say that once you have a distribution, as long as it's right k invariant, you can rebuild the operator by the action of convolution by that distribution. Yeah. So here, if you have a function on, yeah, lambda is a distribution. Excuse me, my apologies. The way I've written it here, I'm sort of sloppily in identifying k invariant distributions on g with distributions on k mod g. So maybe the right way of thinking about it is to have a distribution on g which is both left and right.
00:06:11.338 - 00:06:49.690, Speaker A: K invert translation invariant. Okay, so going in one direction, convolution with the distribution gives a map from here to here. How does it do that? Well, I'm thinking of functions on k mod g as just functions on g, which are k translation invariant on the left. And then I just convolve, on the one hand, we can go from distributions to functions. On the other hand, you can go from distributions to functions. So there's no difference between operators on k mod g, which are g invariant, and convolution operators. Left convolution operators by distributions which are k times k invariant.
00:06:49.690 - 00:08:25.344, Speaker A: So the whole operator is packaged up neatly in a single distribution which is some generalized function of some sort on g. Now back to where we were. So, in order to explain or characterize among all possible lambdas, those lambdas that we're really interested in will follow the advice of Bob Junkin and Eric van eep. So we're interested in, these scalable operators have an order for us, an integer order, and we're interested in the order zero scalable operators. And in terms of this lambda picture of operators, what is it that distinguishes these fellows? Well, there is a smooth family of distributions, maybe lambda t. So this is a compactly supported distribution on the group Gt. We have this smooth family of groups somewhere over here.
00:08:25.344 - 00:09:55.264, Speaker A: Almost all of these Gt's are just copies of g, but g zero is this cut on motion group and normal bundle for k inside of g. Anyway, what we're interested in here are k by k bi invariant distributions on Gt. What does it mean for the family to be smooth? Well, if you take a function on this manifold big g, which is smooth, and you restrict it to each of the Gt's, then you'll get a family of functions Gft. And the condition of smoothness is that lambda of t evaluated on f of t should be a smooth function of t. That's what smooth means anyway, smooth families such that, well, we have this family of diffeomorphisms of g, big g to itself, which moves around the fibers little Gt, this rescaling or zoom family of automorphisms. And the condition is just this, I haven't finished yet. This would say that the lambda t's were scale invariant, and that's too much to ask.
00:09:55.264 - 00:11:00.340, Speaker A: But if you add a little kappa lambda t like this, then it's not too much to ask. So what is a kappa lambda t where this thing, I don't know how to make a kappa look like a kappa, not a k. I mean, basically the same letter. This is a smooth family of smooth, compactly supported densities on the fibers of G. That's what Eric and Bob say. That's just translated into this language. Yeah, many lambdas.
00:11:00.340 - 00:11:32.968, Speaker A: Of course, I think it's written correctly. This is a distribution on GT. This is a distribution on G lambda T. But you can move the distribution on Gt using this diffeomorphism alpha lambda, you can push it forward from gt to G lambda T. And when you do that, you don't get the same thing necessarily. You get the same thing up to this small error, except when t is equal to zero, then it's a little bit of a different story. Then lambda t is little lambda times t is little t.
00:11:32.968 - 00:12:18.548, Speaker A: And so we're saying the single distribution lambda sub zero should have some invariance property. That's it. That's all that's at stake. That's what Eric and Bob have to say about scalable operators. So they're just generalized functions on G which extend to these families which have satisfied this law. Here I took advantage of order zero, so there's no exponential term here, little lambda to the power order of the operator. There we go, and incidentally, this is smooth means smooth in t, but we happen to know because it's in.
00:12:18.548 - 00:12:50.884, Speaker A: I don't know if we proved it or I just proved in one of the exercises. This family is also smooth in lambda as well. That's a theorem that it's smooth in lambda. It's part of the definition that it's smooth in t. Is that a question? I'm just stretching. And here we are in the, in the, speaking the language of the French here with densities are not functions, because in this particular situation, it's more efficient. There are no half densities around.
00:12:50.884 - 00:13:51.580, Speaker A: We're just dealing with densities. It seems to be worthwhile to make the investment. So I hope you're happy, all you density fans. So we're interested in a marginally more complicated situation in which the operators don't act on. Yeah, and here, because densities are things, sorry, distributions, if you like, come with a measure. You can just integrate a distribution multiplied against a function without mentioning a measure. That's what distributions are.
00:13:51.580 - 00:14:33.344, Speaker A: So this kappa is not actually a function, it's a one density like we were talking about. Yeah, fine. Yeah. Then it's just a function. All right. And so we're interested in a small fancification of this in which we have equivariant vector bundles, hermitian vector bundles over the symmetric space ev one and ev two. The v's stand for finite dimensional representations of k.
00:14:33.344 - 00:15:28.670, Speaker A: This is bismuth speak, and the e sub v's stand for the corresponding vector bundle. That's what's going on here. So v one and v two are finite dimensional unitary representations of k. They determine vector bundles over Kmart over the symmetric space. And we're interested in operators like this, which are scalable order zero. What else? Oh, GI equivariant and Eric and Bob tell you what these are two they correspond to distributions, and where the distributions live is, well, you can think of them as living here. If you have.
00:15:28.670 - 00:16:47.204, Speaker A: Did I miss off a star? I think I did. If you have an element of this threefold tensor product, you can certainly get an operator from, by convolution from sections of this bundle to sections of this bundle. If you use the fact that the sections of one of these bundles are naturally the same thing as. See, while I'm here, I'm using compact exported functions, and the actual operators we're interested in are the scalable operators, and those are the ones for which there is such smooth family. I forgot to say what the relationship is between lambda of t and the individual operator distribution lambda relation is that this family starts at the point t is equal to one with lambda. And you make the obvious changes to this description and you'll get a similar description for scalable operators here. I'm not even going to write it down.
00:16:47.204 - 00:18:47.304, Speaker A: There should be a bunch of lambda T's and you just stick a t here and it should be a smooth family, blah, blah, blah. Okay? And actually this is still not quite what we're interested in, actually, again, or in fact in wrestling with what to call this thing. And this is the best I came up with scallop scalable operators from the vector bundle associated to v one to the vector bundle associated to v two. And this is the norm closure of the operators a, as above committed to writing this all out now. So its effect, we more or less proved this. We didn't quite prove this equivariant, non compact version, but all such operators extend to bounded Hilbert space operators. And so the collection of all scalable operators between sections of V one and sections V two has a norm, the operator norm, and we take the norm closure.
00:18:47.304 - 00:19:32.874, Speaker A: And these are the morphism spaces. As we discussed last time, it's not a sister algebra we're looking at, because the operators don't live on one Hilbert space. They map one Hilbert space to another. But no big deal. That's just a tiny thing they form as you vary. The v's a category, a C star category, scallop, scallop. And the objects in this category are just the finite dimensional unitary representations of k, and the morphisms are what I wrote down, and the adjoint is the obvious adjoint, et cetera, et cetera, et cetera.
00:19:32.874 - 00:20:24.230, Speaker A: What are the axioms for a C star category? It's a little more complicated than you might think. The main axiom is that there should be an involution operation, and the norm of a star a should be the norm of a squared. But back in the days, the early days of sea star algebra theory, some bright spark proved that every element of the form little a star, little a, is a positive element. It has only non negative numbers in its spectrum. Some bright spark proved that about C star algebras. If you have any abstract C star algebra just characterized by that C star norm identity, then the spectrum of a star, a little a star, little a, is contained in the non negative real numbers. Some bright spark proved that.
00:20:24.230 - 00:20:56.834, Speaker A: And so that meant that that could be dispensed with as an axiom in the description of what a C star algebra is it doesn't work out quite so well here. So you have to throw that in as an axiom, because otherwise there are stupid counterexamples. So it's almost exactly what you think it is, but not quite. You have to throw in that extra axiom. You wouldn't have to do it really here, because the morphism spaces are concrete spaces of operators. And then this extra axiom is automatic. But in general, you should do that.
00:20:56.834 - 00:21:32.928, Speaker A: All right, so we have this category. You can always do the following thing. You can take the direct sum, just let's not worry about its size for a moment. Take the direct sum of all possible versus. That's a huge, well, now it's a Hilbert space, infinite dimensional. And each time you have an operator like this, it maps that Hilbert space, that huge direct sum to itself, namely its zero. You can think of it as being zero on all of the other orthogonal summands except this one.
00:21:32.928 - 00:22:11.004, Speaker A: And it maps this particular summand into this particular summand. So each morphism in this category can be situated as an endomorphism of some huge Hilbert space, the direct sum of all of these l two kmg, comma, ev's. And then you can look at the sea star algebra generated by all of those. And that's the sea star algebra. It's a sort of a clumsy, clunky thing. And, you know, you have to worry a little bit about set theory and so on, but it's, that is just a sea star algebra. And the k theory of that sister algebra is almost by definition the k theory of this sea star category, which is what we're interested in.
00:22:11.004 - 00:22:41.874, Speaker A: So it's not. Yeah. Okay, good. And so the theorem, well, first of all, there is a natural function, the finite dimensional unitary representations of k, finite dimensional. I didn't put in a u unitary representations of. Kick into this thing. Scallop.
00:22:41.874 - 00:23:32.692, Speaker A: Yeah. What is it? Well, if you have a two finite, first of all, if you have a finite dimensional unitary representation, what are the objects of this category? They are the finite dimensional unitary representations. That's the same as the objects of this category. So what the functor is on objects is obviously just sends v to v. As for morphisms, if you have an intertwine of representations from v one to v two, it gives you a morphism, g equivariant morphism of vector bundles from ev one to ev two. And now you can compose with that g equivariant morphism of vector bundles, and you'll get an operator from sections of ev one to sections of ev two. That operator is a pseudo differential, excuse me, a scalable order zero operator.
00:23:32.692 - 00:24:14.708, Speaker A: So doing the obvious thing, to convert an intertwiner of k representations into an intertwiner of vector bundles, and then applying that intertwining action on vector bundles to sections, we get an operator like one of these a's. It's never a very complicated operator, is it? I mean, it just came from a morphism of finite dimensional vector spaces from v one to v two. So these are the world's most idiotic order zero scalable operators, but they still have a right to exist. And they are used right here in the definition of this arrow. So that's the natural function. And now everything is sea star. This is a sea star category too.
00:24:14.708 - 00:25:44.432, Speaker A: Finite dimensional unitary representations of k. And the theorem is that this induces an isomorphism in k theory. And to prove this theorem, one needs to complicate a contemplate a gigantic diagram, which we shall do, and this gigantic diagram will involve these families. That seems sensible. We should somehow use the definition of what a scalable operator is. The diagram here I'm going to make a new category, which I shall call scaling fam for scaling families. And how am I going to build that category? Well, an element.
00:25:44.432 - 00:26:33.324, Speaker A: I'm going to do the following thing. I'm going to look at these families that we're discussing here. These families can be composed, added and so on. They form a category enriched over vector spaces, a category in which the objects are just finite dimensional unitary representations of V again, and the morphisms are the vector spaces of all such families of distributions. And I'm going to complete those families of distributions in the obvious way, into Banach spaces. And the way I'm going to do it is I'm going to take the operator norm. All of these guys determine by convolution bounded operators from l two of k mod gt comma ev one into l two of k mod gt of ev two.
00:26:33.324 - 00:27:27.664, Speaker A: I can just take the operating room and then I'll take the supremum, because there's a non compact family of real numbers at play here. What I'm going to do is I'm going to take the supremum just over the unit interval like this, just to make sure we get an honest kosher finite norm. So what I'm doing is I start off with, if I wanted to find the morphisms in this category from v one to v two, do it over here. I start off with all families of this type. Now, lying in up here, this is what I should be pointing at, lying in here. Maybe I'll just temporarily stick in a t like that. I take all such families satisfying the obvious variation on this condition here.
00:27:27.664 - 00:28:48.800, Speaker A: Each lambda t gives an operator by convolution from one Hilbert space of l, two sections to another, and it has a norm. To attach a norm to a family, actually a semi norm to a family, I'm going to just take the supremum of all the norms of the convolution operators, lambda t between zero and one, because I don't care what happens towards plus infinity or minus infinity on the real line. That's what I'm talking about, and so on. If I have such a family, I can restrict that family to t is equal to zero or t is equal to one. If I restrict to t is equal to one, what I get as a quotient is just scallop, because scallop is exactly the set of all families which have an extension. On the other hand, if I restricted t is equal to zero at zero, we have what in the world of scalable operators we were calling the symbol families. These are operators, as you can see here.
00:28:48.800 - 00:29:35.032, Speaker A: These are operators on k mod g zero. But k mod g zero is a much easier space than k mod g, because g zero is a semi direct product of k with the tangent space of g mod k at the identity which we were calling p. And so if I divide out by k, we're just talking about operators on the vector space p. These operators are translation invariant operators and k equivariant operators, so they're easy to analyze using the Fourier transform, the usual Fourier transforms. So this is a much easier thing. It consists of really exactly the same thing as this, except when we're talking about the group g zero instead of the group g. And for g zero, it's that easy to understand what's going on.
00:29:35.032 - 00:30:56.694, Speaker A: We're talking about translation invariant scalable operators on a vector space. And this finite dimensional, this category of finite dimensional unitary representations, it maps into all of these things via the same natural function all the way along. All of the objects in all of these categories are just finite dimensional unitary representations of k and the morphisms, any k intertwiner of representations, as I was explaining, gives rise not just to a single scalable operator, but an obvious scaling family, et cetera, et cetera. And then that family has a symbol. So you have a big picture like this, and I want to show that the map on the right going downwards is an isomorphism in K theory. So the strategy for doing that is to understand what happens on the left and down here and down here. If we were able to show that these three morphisms are isomorphisms in K theory, then we're in business, I think you'll agree.
00:30:56.694 - 00:32:07.182, Speaker A: So that's what's going on. And so you have to draw your eyes away from this diagram, beautiful as it is for a moment, and study these sequences. Here's our, what do we call it? Scaling family category. Suppose I take a scaling family and I evaluate at one. Then we get the notorious category scallop as a quotient. And the people in c star categories do this. They talk about extensions of categories.
00:32:07.182 - 00:32:49.816, Speaker A: All the categories have the same objects, but the morphism spaces fit into short exact sequences. That's what they mean by extensions. And now what's the ideal? Well, suppose you have a scaling family, and suppose the operator you get by evaluating at t is just plain zero. What do we know about the rest of the scaling family? We know it consists entirely of smoothing operators. That mostly follows from that Eric Werner Bob Junkin relation. But a supplementary argument is needed when t is equal to zero, which we gave in the class. So we're actually talking about families of smoothing operators which are in the kernel g.
00:32:49.816 - 00:33:39.234, Speaker A: Ecovariant smoothing operators. Maybe I'll just call it smoothing, maybe smoothing op. And what do we know about these smoothing families? Well, first of all, they lie between zero and one. Secondly, the smoothing family is actually, the operator at t equals one is actually zero, because we're in the kernel of this map. So let me just indicate that in this way we're taking families of smoothing operators parameterized, well, initially by the whole real line, we're putting a norm which is just the supremum over zero to one, the interval from zero to one. And then we're taking a completion, but we're also only completing those operators whose families whose value at one is equal to zero. So this is another c star category.
00:33:39.234 - 00:34:28.494, Speaker A: Like I said last time, the people in C star categories don't care if their categories don't have identity morphisms, which this one doesn't. They don't care, they don't run into any trouble. So I guess we shouldn't worry about them. Okay, now what? And while we're at it, we could do exactly the same thing, but now we could evaluate at t equals one. Excuse me, zero. So that'll give us this symbol category, and same thing, there'll be some category of families of smoothing operators here indexed by the interval. But now the value at zero is of this smoothing family.
00:34:28.494 - 00:35:06.674, Speaker A: The operator at t zero is going to be zero. And now something rather interesting happens. This fellow here is an example. If it was a sea star algebra, it would be an example of a contractible sea star algebra. The identity morphism is homotopic to the zero morphism. Well, it's a category, so it's a contractible C star category. The identity functor is homotopic through functors to the zero function.
00:35:06.674 - 00:36:22.066, Speaker A: All of these functions are the identity on objects. It's not zero on objects, but the morphisms are all sent to zero. So this is an isomorphism in k theory by the property that we discussed before. And something very interesting happens here. We saw that the concasper of isomorphism could be expressed in terms of these continuous fields of C star gts, which is exactly what we're talking about here. This thing has zero k theory, if and only if the concept of isomorphism is an isomorphism, which it is. So indeed, both of the maps, two and three, the functus two and three, at the level of K theory, are isomorphisms, if you believe in the concept of isomorphism.
00:36:22.066 - 00:37:09.090, Speaker A: And by the way, they're both rather, this one here, excuse me, this one here is an isomorphism, if and only if, the concasporov conjecture is isomorphism, is an isomorphism. If the concrov, well, anyway, it is true, we're in sort of phantom never never land. If we're in a world in which the concept of isomorphism was not an isomorphism, which we're not. But if we were, then this map here would not induce an isomorphism. K theory. You can go backwards and finally, like a comment that just had to do with Cesar algebras, right? Yeah, it's just cat, they're just like sea star algebras. Do what I just said and stick them all together into a sea star algebra.
00:37:09.090 - 00:38:13.472, Speaker A: There's nothing, there's no substance to accept, being honest to mentioning sea star categories at all. I found it kind of peculiar that somehow you use this positive element elements, positive spectrum, but then you can also get an actual C star algebra, where you know that if you have to assume it, then you get something which is. Yeah, so it's true that if you forgot that funny extra axiom, you could build a mock c star category, which for which you couldn't build a C star algebra attached to it. And that's just. And would that be a disaster? Well, I don't know, but if you wanted to stay entirely within the world of categories closely related to sister algebras. Then you would add that extra axiom, and we're not anywhere near the danger zone here, because all of our categories are concrete categories of Hilda space operators. This construction, the fact that you actually get a cigarette and a cognitive assumption.
00:38:13.472 - 00:38:47.604, Speaker A: Yes, exactly. Exactly. Yeah. If you neglected some of the details, didn't cross the t's, dot the I's like your grandmother told you to, then you would run into some small technical difficulties and you'd get some Banach algebra, which is not necessarily a sea star algebra. But is that so bad? I mean, still a bona couch. All right, good. So there's only one other arrow to deal with, which is the one that's labeled one.
00:38:47.604 - 00:39:47.154, Speaker A: I'm trying to make the point that this whole argument is just like floating down the river, Raoul Bach style, as long as you do two things. First of all, you agree that you've somehow already conquered the conceptor of isomorphism. And secondly, you adopt this perspective on what are secretly pseudo differential operators, this perspective offered to us by Bob and Eric, which is what's exactly written on this board. If this is how you think of pseudo differential operators, scalable operators, and if you believe in Kon Kasparov, then this argument that I'm slowly walking you through, I mean, there's a lot of big categories with funny names that. Categories with big, funny names, I guess, is the right way of saying it. But there's nothing difficult going on here. It's just really lazy sailing, floating.
00:39:47.154 - 00:40:27.812, Speaker A: In fact, not sailing at all. Great. So let's just wrap things up here. I remembered what I did had a slightly different notation in the notes. Put little stars here to remind ourselves that these were c star categories. Thought that was very clever. But, okay, we're going to stick stars everywhere, so.
00:40:27.812 - 00:41:46.294, Speaker A: Won't do that now. So what is this thing? Well, these are, as I said, scalable g zero equivariant pseudo differential operators, and because they are g zero, equivariant on k mod g zero, because k mod g zero is the vector space p, and because the vector space p acts as translations as part of the g zero action on p, we can analyze this thing by the Fourier transform. Each one of these operators corresponds to some other operator on the Fourier transform. Instead of having the thing act on l two of p, you can have it act on l two of p star just by identifying l two of p and l two of p star by the Plancherelle theorem. And what you get is something very, very simple. You take the vector space p, or rather p star, and you compactify it by putting the sphere at infinity. So, for example, if p was two dimensional, like it is for sl two r, what you're putting at infinity is a boundary circle.
00:41:46.294 - 00:42:51.994, Speaker A: Okay, so it's a vector space, a two dimensional vector space, in that case, with a boundary circle added. So it's a disk. And what kind of functions are we talking about? Well, we got to get v one and v two into the picture. One way of saying it, I guess, to make it look like it did. Look, before we write it this way. So these are functions from p star into v two, tends to v one, or if you like, it's functions from p star into the finite dimensional vector space of morphisms from v one to v two, which are quivered. Yeah.
00:42:51.994 - 00:43:36.534, Speaker A: So this is what you get by taking p star and adding a boundary sphere. And the sphere you get, I'll just write it as sphere of p star. The sphere you get is the sphere you get by removing zero from p and then dividing by the action of positive scalar multiplication. That's the sort of abstract version of what this sphere is. Yeah, that's exactly what I was going to write. Radial compactification, if you like. Under Fourier transformation, the symbol operators act exactly like point wise multiplication by functions which have radial limits.
00:43:36.534 - 00:44:24.214, Speaker A: We already know how smoothing operators act. They act by c zero functions. That was our description of, built into our description of c star of g zero. All right, good. And write this fd rep, not frd red. This is something really simple. Here are the morphisms from v one to v two in the category of finite dimensional representations.
00:44:24.214 - 00:45:26.234, Speaker A: And here's what's going on in the symbol category. Once you apply the Fourier transform. What's the map? Well, it's v two tends to v one, goes to the constant function v two, tensor one. So v one, only the square brackets there. So we're just including intertwining operators as constant valued functions, the value being that intertwining operator. This is the map you would get by in the reverse direction, taking the space p star bar and just collapsing it down to a point. That map is a homotopy equivalence.
00:45:26.234 - 00:45:50.744, Speaker A: We take a euclidean vector space, well, it doesn't even have to be euclidean. We have a vector space, p star. We spherically compactify it. And now we have a disk ball. Inside of this ball, there is the origin, which is a fixed point for the k action. And now the point, which is this fixed point, is homotopy equivalent to p. K equivalently homotopy equivalent to p.
00:45:50.744 - 00:46:29.240, Speaker A: On the one hand, you just include the point in as the origin. On the other hand, you just collapse down to a point. And those are mutually homotopy inverse maps. So this guy here is not just a functor, it's a functor, which is a homotopy equivalence deformable through functors. Yeah, this functor and the obvious function in the other direction, their compositions are deformable through functors, in both cases to the identity. So for sure, it's sort of trivial and obvious for sure, this map is an isomorphism in k theory, because this k theory is a homotopy invariant function. It's really easy.
00:46:29.240 - 00:46:51.398, Speaker A: It's still floating down the river, we're still just relaxing like Raoul Bond taught us we should. Okay, so there's nothing going on. It's really nice. You know, you had to make a bit of an investment to understand C star categories. Not C star categories, scalable operators. You make no investment to understand C star categories. That's my point.
00:46:51.398 - 00:47:39.494, Speaker A: It's just, it's not even a subject. But, but scalable operators are a subject. And buried in this argument are all of the facts that we managed to prove that scalable operators of order zero are bounded, that the symbol of a scalable operator is well defined modulo smoothing operators. So if an operator is zero, then the whole scaling family has to be zero, two, or at least a smoothing family. Buried in this argument are all of the various steps we took along the way in the first part of the class, and also not so buried at all. Very evident in this argument is the fact that the concept isomorphism is playing a critical role. It's translated into the statement that this map three is an isomorphism in k theory.
00:47:39.494 - 00:48:41.936, Speaker A: Nice. So we have this available to us. Now we can consult Vincent Lafourg, who was able to prove the concast pro physimo just using his bare hands, just using analysis, no representation theory. And now we can ask ourselves, are there any representation theory representation theoretic consequences to this kind of statement? The value of this statement over the usual formulation of the con casper of isomorphism is that it's really easy. The k theory of finite dimensional representations is a free rebellion group with one generator for each irreducible representation of the group k. This map here takes those standard generators to generators in the k theory of the scaling operators, which are easy to understand. They're not weird Dirac type.
00:48:41.936 - 00:49:30.254, Speaker A: I don't know what operators, they're just identity operators. The k theory of this category, which is to say some group, abelian group made up out of idempotence, idempotent morphisms in this category. It's actually only made up not out of weird idempotence, built in some crazy way using spectral projections for Dirac operators, or I don't know what. No, it's built entirely just out of identity morphisms. There's nothing there inside of this k theory except identity morphisms, the easiest possible things. And yet the statement has some substance, because this theorem is equivalent to the concasperov isomorphism. So we have an easy, easy easy statement which we're entitled to believe has some real substance because it, it's equivalent to the concept of isomorphism.
00:49:30.254 - 00:49:58.070, Speaker A: So that's the thinking. So let's just see what happens. We need one more. How are we doing? Somewhat okay. Idea to move into the territory of representation. Sorry, you're saying it? Yep. If you know the concept of isomorphism, then you can float down the river like we just did and prove this theorem.
00:49:58.070 - 00:51:06.192, Speaker A: If you know this theorem, then you can float down the river in the opposite direction, oddly enough, and, and end up proving the concasper of isomorphism. No muscular effort is required in either direction. Yeah, well, let me show you. I have a sort of roundabout suggestion in that direction, but it goes through some representation theory, this result of voguen that we were talking about last time. So that's indeed exactly what I want to talk about. And mostly we're still going to. There's a lot of flotation in this selection.
00:51:06.192 - 00:52:13.018, Speaker A: Nothing very difficult is going to happen, except at the end, and there's going to be a statement which we only know I and my student Peter Debelle only know how to prove in the most lame, disrespectable, disreputable way. So we're floating down the river and sadly we're going to float directly towards a Niagara Falls. And then things are going to get a little awkward. But until we get to the falls, it's going to be a beautiful journey. Just relax, sit back in your barrel, enjoy the gentle ride along the river. Just don't look ahead too far. Okay, suppose I have unitary representation, and I don't need this extra clause right away, but I'm going to put it anyway.
00:52:13.018 - 00:53:07.764, Speaker A: Admissible. What does admissible mean? Admissible is what we talked about before. It means that each of the k isotypical subspaces of this unitary representation is finite dimensional, each representation of k, because with finite multiplicity inside of the given representation PI of g okay. Oh, actually, it's a little bit over optimistic. We are going to use admissibility almost immediately. So this is a construction that you can make for any lambda at all in the space, one of the spaces we were looking at. This doesn't actually use scalability.
00:53:07.764 - 00:54:37.464, Speaker A: Any operator mapping smooth sections of the vector bundle for v one continuously into smooth sections of the vector bundle for v two allows you to build an operator PI of lambda. We can extend the representation, if you like, from the group to certain distributions on the group, just like we can integrate a representation and extend it from the group to smooth, compactly supported functions on the group. We can do the same thing here for distributions. And where does this thing go? Well, it goes from not all of h PI, but just a little piece of h PI, this piece of h PI, because we should start with hb one. This is like an isotypical part of h PI to the same isotypical part, to the v two isotypical part. Like these fellows here are actually finite dimensional vector spaces. So I'm getting something very concrete and finite area out of this distribution.
00:54:37.464 - 00:55:44.164, Speaker A: And how does this go? Well, because we're talking about distributions, we need smooth functions in the picture and inside of every unitary representation space. Once on Hilbert space h PI, there's a space of so called smooth vectors. And those are the vectors with the property that the corresponding orbit map is a c infinity map from the c infinity manifold, which is the group, into the infinite dimensional c infinity manifold, which is the Hilbert space. So you can differentiate this in whatever direction you like as many times as you like. Okay. The representation theorists do this all the time because they want not just the Lie group to act on the representation space h PI. They want the Lie algebra to act on the representation space.
00:55:44.164 - 00:56:42.734, Speaker A: And you get the Li algebra to act by taking derivatives, as you know, and you need some hypothesis of smoothness before you can take derivatives. This thing, h infinity PI, is a module for the lie algebra of gene and also a representation of g. It's a fresh space. It's not a Hilbert space anymore. And it's an important fact, which is a consequence of admissibility, that the obvious inclusion map, which I'm writing down is actually an isomorphism. So every v isotypical vector on the right actually involves only smooth vectors inside of h PI. It's not obvious from what I've written that there are any smooth vectors in h PI at all.
00:56:42.734 - 00:57:22.600, Speaker A: So that's a little lemma that the smooth vectors are dense inside of h PI. Not only that, but the smooth vectors, which belong to a given isotypical subspace, are dense in that isotypical subspace. And since the isotypical subspace is a finite dimensional, that means that every isotypical subspace consists entirely of smooth vectors. That's what's going on. All right, so now we're in business. Say again, the arrow from h I. Yeah, this is just the inclusion of this, as it happens, dense subspace and into here.
00:57:22.600 - 00:58:22.618, Speaker A: Yeah, you see, it's easily possible to make a distribution act on smooth vectors by writing down a formula like this. We wanted to find PI of lambda evaluated on a smooth vector like this. That's supposed to be a little w. Another vector could be in the whole Hilbert space. So v here is in h infinity PI, but w could be anything at all. And now I've run out of space. This thing is just the following.
00:58:22.618 - 00:59:00.246, Speaker A: You take the distribution and you evaluated on integrated against this thing, w phi of g. So that's the distribution lambda evaluated on the c infinity function, which is this inner product function. Now you're in business. And what's the integral? Well, it's a distribution. You don't need to write down what the integral is, except maybe I should say, integrating. And with that, I think it's fairly clear what you're supposed to do. You have a lambda in this threefold product.
00:59:00.246 - 00:59:52.534, Speaker A: The first thing you do is you contract the v one against the v one. The next thing you do is you throw in the v two, which is up in the definition of lambda. And then as for the element which is a distribution, you convolve it against h PI according to this formula here. So each actual scalable operator has what you might call an actual multiplicity in any irreducible or indeed admissible representation of g. We get out of this actual pseudo differential operator lambda, this scalable operator lambda, a map between finite dimensional vector spaces. That's good, because it's very tangible and you have a, you know, hope to understand what's going on. And things are a little bit better than I've indicated so far in the following way.
00:59:52.534 - 01:01:25.416, Speaker A: If PI is a representation, if it has a slightly better analytical structure, it's actually related to the reduced dual practice. To a representation of the reduced sea star algebra. For example, PI might be an irreducible unitary representation in the reduced dual. Then instead of just applying this construction to actual scalable operators using distributions, you can apply it to any norm limit of such scalable operators. That's what scallop is made out of, and that gives you a pairing from h PI v one. So it's a bilinear map. So this is the next best thing to a homomorphism from the Seser algebra into the compact operators.
01:01:25.416 - 01:02:50.884, Speaker A: This is a functor from the category scallop for the category of finite dimensional Hilbert spaces, Fdrep if you like, for the trivial group, K trivial group. And so the level of k theory, you get a map from the k theory of scallop, which is what we've been talking about. So the k theory of finite dimensional vector spaces, that's just z. So there's a sort of multiplicity map attached to any representation, and it fits nicely with this Kon Kasparov like story. So here's the k theory of skeletal operators, multiplicity of PI, k theory of the category of finite dimensional Hilbert spaces. So that's just the integers. Here's the k theory of the category fd replication.
01:02:50.884 - 01:03:39.650, Speaker A: Here's the map which we were discussing before, which is an isomorphism. This thing is just the representation ring of k. So we get some functor like this, excuse me, not a functor, morphism of alien groups like this. And what does it do? It just sends an object, maybe it's an irreducible representation, to the dimension of h PI v. Okay, so that's easy to understand. There are no Dirac operators, no spinners, nothing to worry about. This map plays nicely with our concasperov functor.
01:03:39.650 - 01:04:18.924, Speaker A: And the composition is just given by some multiplicities. You take the representation PI, h PI if you like, and then you restrict it to k, and then you count all of the multiplicities with which irreducible representations of k occur in it. And that's what this map is here. It's just the multiplicity map. So there's no index theorem to worry about. It's just a multiplicity, not some weird thing with some strange, vile character like formula, dimension like formula. It's just this.
01:04:18.924 - 01:06:12.016, Speaker A: So now I want to take an enormous category, which is just the direct product of all of the categories of finite dimensional vector spaces, of multiple copies of the category of finite dimensional vector spaces, with itself indexed by these special representations that Vogen talked about and that we talked about last time, Vogen's temperic PIs. And inside of it. Let's build the following thing. Maybe we'll call it tempuric. It's a category, and it will be the direct sum of the same fellows like this. So an object in either one of these categories is just a bunch of finite dimensional vector spaces, one for each temperature representation, an infinite list of finite dimensional vector spaces, one for each temporary representation. That's what an object here is as well.
01:06:12.016 - 01:07:36.514, Speaker A: But the morphisms, instead of just being infinite, lists of morphisms, which they are in the direct product from finite, of, of finite dimensional vector spaces here, it's an infinite list of morphisms, almost all of which are just plain zero. That's what's going on. So the k theory, k theories of both of these categories are easy to understand, but it's the k theory of the temporary one that we're interested in. So the second category, this temperic thing, is one of these sea star categories without identity morphisms, which we've agreed. It's okay, it's okay for categories not to have identity morphisms. This is just a direct sum, not a direct product, over Vogen's PI empiric representations, copies of z. If you multiply together, it's supposed to be a map.
01:07:36.514 - 01:09:02.984, Speaker A: All of the functions we just finished constructing. What do we call a mult of PI mult sub PI. This is a functor from scallop into the direct product here. Okay? And this thing, actually, if punctu has image, I think how to say this, in the temporary category, and it induces an isomorphism in Cathy. So we're talking about Vogen's representations in some meaningful way. That's the good news. The bad news is we're going to use Vogen's theorem here, which says, but the temporary representations are in bijection with the irreducible representations of k.
01:09:02.984 - 01:10:23.794, Speaker A: So we're not proving Vogen's theorem, but at least we have a context in which we can, which allows us to situate Voguen's theorem in the world of Konan Kasparov. And then there'll be one more thing to say, and then over the waterfall we go, I should have said it at the very beginning, I guess it's. Yeah, stick his name somewhere and where to put him. He mentions him, but everything in this lecture is stuff that my student. It's not really my student. I borrowed him, my borrowed student Peter Debello and I were working on. So let's just think about how this goes, and we'll just use this picture over here.
01:10:23.794 - 01:11:23.554, Speaker A: Is it right? Yeah. Didn't say very explicitly where this thing starts. It's a functor from the category scallop into this direct product category, although I'm going to think of it as a functor into this direct sum category. Why does it go into the direct sum? Well, that's a fact from representation theory, which I'm not going to say much about. We know what this is. So initially, where we, what we're landing in here is the direct product. But I'm telling you, you're actually in the direct sum.
01:11:23.554 - 01:12:43.714, Speaker A: What this map does is very easy to understand, because this multiplicity functor is super easy to understand. And the elements to which we're applying the multiplicity functor are super easy to understand, too. This just sends an irreducible representation, v, for the direct sum of the multiplicities with which that representation, or rather its contragradient, lie inside of h PI. Oh, and the dimensions of those. No, that's supposed to be okay. So what Vogen says is that there's a notion of a way of ordering the representations of k, and there's a way of ordering the temporary representations, namely by their minimal k type. And then by virtue of the way we've ordered things, if you write down this map as a gigantic matrix, maybe I should say that this is a direct sum over all.
01:12:43.714 - 01:14:00.850, Speaker A: What's a good letter sigma in k hat? Copies of Z, which case maybe sigma there. What Vulcan's theorem says is that when you write down this map as a gigantic matrix is upper triangular and the ones down the diagonal for the obvious basis inside of this abelian group, because each minimal ktap occurs, each ktap occurs exactly once. There's a minimal K type in this, among these representations, and because it occurs with multiplicity one, and because every minimal K type does appear, when you write down the matrix of this, it's upper triangular and has ones on the diagonal, so it's invertible. And if the composition is invertible, you're done, because you already know that this is invertible. And this is pretty much to say that this composition, or rather this functor, is an isomorphism, is pretty much the same as saying Wogan's theorem. Not exactly, but pretty much. And I don't have much to say about the gap between the k theoretic statement and the actual statement of Wogan's theorem, because I didn't think about it now.
01:14:00.850 - 01:14:48.274, Speaker A: But let's end now. As the falls get closer, you can begin to hear them. What's that sound? Thundering sound. Is the weather going turning bad? Let's end with one more topic. So all of a sudden, we're doing representation theory in a way that's sort of intelligible to a representation theorist who maybe doesn't have, doesn't particularly care about Dirac operators. It's just multiplicities that we're still, that we're discussing here. And let's now take it to the edge.
01:14:48.274 - 01:16:07.206, Speaker A: In order to keep things as simple as possible, I'm going to assume right from the outset that g is a group with real rank one, like we were talking about last time. So what that means is that some maximal abelian subspace of p, any maximal abelian subspace of e, is one dimensional. But what it means is that when you look at the reduced dual of g, it consists possibly of some discrete series representations, the net most countable number of isolated points, and then families of representations, continuous families of representations, where the parameter lies in a real line in a one dimensional vector space, not a two dimensional vector space. So when you draw a picture of the unitary, excuse me, the reduced dual, also the unitary dual. But when you draw a picture of the reduced stool of one of these groups, the picture is one dimensional. That's what we're talking about. And what is the picture? Well, let's just think about it.
01:16:07.206 - 01:17:14.044, Speaker A: Here's the reduced C star algebra of G. And each time you have an irreducible representation of G, which lies in the reduced tool, you get an irreducible representation of the siesta algebra, and where you land up is in the compact operators on the Hilbert space. And now, as you vary PI around the reduced dual, you'll get a family of representations like this, a homomorphism, family of homomorphisms from C star of G into the compact operators, or, if you like, a single star homomorphism from the sea star algebra of G into functions, from the reduced dual into compact operators. And you can get pretty precise about what is involved there, especially in the real rank one situation. There are different types of representations. There are some representative, as we discussed last time, some representations which are discrete series and some representations which are principal series. And so you have to take them all into account.
01:17:14.044 - 01:17:56.828, Speaker A: If you're following the teachings of Harris Chandra, then the way to keep track organize the set of all components of the reduced dual is via mystical data called p sigma. And p stands for a parabolic group subgroup. And in the world of real rank one groups, p stands for one of either two things, the minimal parabolic that we talked about last time, or all of G. That's a legitimate parabolic subgroup. So when I write p, I mean one of two things, all of g or pman. And sigma means a certain special type of representation. It means, in the case of G, one of these discrete series representations.
01:17:56.828 - 01:18:40.854, Speaker A: And it means, in the case of p, actual parabolic subgroup, it means an irreducible representation of that little m group that we saw. For example, for sl two, r m was really little. It was just a two element group diagonal matrices with values plus or minus one. And this trick of gathering together representations into families produces a collection of star homomorphisms like the ones I'm now writing down. So this is a parameter space for the family of representations labeled by p sigma. And if the family of, if p is all of g, then we're not really talking about a family here at all. We're just talking about a single representation.
01:18:40.854 - 01:19:21.194, Speaker A: In that case, the vector space that I'm calling Ap star is just a single .00 dimensional vector space. Otherwise it's a one dimensional vector space. So these are locally compact spaces, and they're either points or they are the line. And here's the compact operators. It turns out that all of these representations in any given family, you can identify the underlying Hilbert spaces in a canonical way. And there's one other small detail, which is that it's very frequently the case that the representation, in the case of representations parameterized by a line, the representations at a point, t on the line, and nu on the line, I guess, is our favorite letter for this, and minus Nu are the same.
01:19:21.194 - 01:20:05.900, Speaker A: In fact, it's always the case that they are the same, almost always the case that they are the same. And so we make that identification. And that's what the seastor algebra looks like. It's like the Fourier transform. Suppose G was gl one. So this is not quite a connected group, but connectivity is not essential at this point. What is this saying? It's saying the c star algebra of the, of course, a one by one invertible matrix is just a non zero complex number.
01:20:05.900 - 01:20:31.974, Speaker A: It's saying that this sister algebra can be understood in the following way. In this case, the M group has two elements plus or minus one. It's a bit like sl two. R, it's plus or minus one. So there are, there are, and sigma, there are no discrete series. So sigma is a representation of the M group, two element group. So it's one of two things.
01:20:31.974 - 01:21:30.086, Speaker A: And what you get are two copies of the line, if you like, a copy that ought to be labeled even and a copy that ought to be labeled odd like that. The W groups don't exist in this situation. The Hilber spaces are one dimensional, and so you get a simpler thing like this. But that's what we're talking about. We have here a group, and if you want to do Fourier theory on this, group. Well, there's some continuous parameters like you see here, but also a discrete parameter which separates out these two summons, not very complicated to think about, to nail down this thing and then understand all of the intricacies surrounding this description, figure out exactly what the parameter sets are, what the sigmas are. That's not so easy, because the m group is not necessarily connected.
01:21:30.086 - 01:22:47.596, Speaker A: So figuring out its irreducible representations is not so simple. And then there's the discrete series to worry about, lots of things to worry about. But you get a picture like this and you might wonder, well, let's, first of all, maybe we'll make a, let's go back to the morphism spaces in the category smoothing, which looked like this thing here. Now that we know, if you believe me, exactly what the siesta algebra looks like, or approximately what the siesta algebra looks like. I didn't tell you how to label the parameters sigma. Now that we know that we have a Fourier picture for C star of G, we can turn C star of G into a category. Study all of these morphism spaces.
01:22:47.596 - 01:23:56.682, Speaker A: And what's going to be involved here? Well, this is what's going to be involved. There'll be a direct sum over some index set exactly like before, of c zero functions from a. Maybe I'll do it into what? Well, the compact operators, I'll write it this way. From h sigma of V one, h sigma. Oh, I forgot. Now we still have to do one more thing, which is, so this h sigma of V just means you take h sigma, I've got to say, but this is a representation of G, and you look at its isotypical space, it's a finite dimensional space. So I'm writing compact operators from one finite dimensional Hilbert space to another.
01:23:56.682 - 01:24:41.054, Speaker A: They're just matrices. And so you have a fairly concrete picture like this. It should be, should be another parenthesis in there somewhere. And actually, in this particular situation, once you fix v one and v two, this is actually a finite direct sum. Over here, where we're not specializing to v one and v two, it's an infinite direct sum, typically. But here it's actually a finite direct sum because these spaces, for all except finitely many sigmas, are all going to be zero. There you go.
01:24:41.054 - 01:25:36.744, Speaker A: Now you can ask what happens for scalable operators. After all, scalable operators have a very similar picture. If you have a scalable operator, it gives rise to a little morphism from this finite dimensional Hilbert space to this finite dimensional Hilbert space. And as the representation PI varies, we'll get a family of morphisms, just like I was discussing over here. And an inquiring mind might want to know what you get. And here's the answer, and then we'll stop. So let's take our friend scallop.
01:25:36.744 - 01:26:31.304, Speaker A: Scallop, take an operator in this space, and so to speak, evaluate it on an irreducible representation, PI, which lies in the reduced dual, and then we'll get a little morphism from the space that I'm calling here, h sigma v one, to the space h sigma v two, morphism between these multiplicity spaces. And let's take all of those morphisms and glue them together. What do you get? Well, you get something which looks like this same sum. It's going to be, again a finite sum. Everything in this space is a sort of multiplier of everything in those spaces up above. If you compose one of the operators above with one of the operators below, you get one of the operators above. So we need some kind of operators.
01:26:31.304 - 01:27:12.144, Speaker A: Now, the operation here is point wise multiplication, which are closed, which map this space into itself as like an ideal. And this is what it is. You take the same spherical radial compactifications as before, and that's it. Everything else is the same. I write compact operators, but, but these are finite dimensional Hilbert spaces. Sorry, I'm going to stress to fit all of this in. Oh, and then there are these still finite symmetry groups.
01:27:12.144 - 01:28:08.918, Speaker A: So if you take, we encountered something like this before. If you have a pseudo differential operator which is g invariant, scalable operator, which is g invariant, g zero invariant, on the symmetric spacing quotation marks for g zero, k mod g zero. In other words, you have a translation invariant k equivariant operator on p, the vector space p. Then under Fourier transform, the usual abelian Fourier transform, you can figure out what it is. It's multiplication by some function on the radial compactification of, well, not a star, but p star. And so we saw something very much like this before. But now we're dealing with this more complicated reductive group, not a motion group, and this is the counterpart for motion groups of, excuse me, for non motion groups, for reductive groups of what we saw before.
01:28:08.918 - 01:29:26.078, Speaker A: Sadly, we don't know how to prove this in general, but we do know how to prove it if g has real rank one. In fact, we know it's not true in general, it's not quite the right statement in general. Let me just say one thing about the proof, this space here that I've just written down again, it's a finite sum, and it includes the space above, because, of course, every smoothing operator is a scalable operator. This guy has to include this space and it's not much bigger. The space at the bottom is not much bigger than the space at the top, because the only difference between functions down here and functions up here is that the limiting values here at plus or minus infinity on the lines might be non zero, but those limiting values lie in a finite space of matrices. So this space here includes the space at the top as a finite co dimensional subspace. It's not much bigger downstairs than what you have above, like taking functions on an interval which vanish at both endpoints versus all functions on an interval.
01:29:26.078 - 01:30:33.546, Speaker A: The bigger space is only two dimensions bigger than the smaller space. It's really close. What they are in size that only works for lines. It doesn't work so well for boundaries in a situation where the boundary is a circle, because then you won't have this finite dimensional situation. So in real rank one, the dimension of scallop is finite. And, and what is this quotient? Well, we're taking order zero operators modulo the closure of smoothing operators. That's the same thing as order zero operators, modulo order minus one or less operators.
01:30:33.546 - 01:31:43.454, Speaker A: It's just the principal symbol, which tells you what the difference is, what the quotient is here. But these principal symbols are maps from. Well, there are two ways of looking at these principal symbols. One is not in the Fourier transform picture, so to speak, over there on the left, and the other is in the Fourier transforms picture. But back in the not Fourier transform picture, we have a pseudo differential operator. We want to know what is its principal symbol? Well, it's a function on the sphere bundle of the cotangent bundle just at one point, because once you know what it is at one point, you know what it is everywhere by equivariance. So we're looking at a function here from the sphere of p star is into.
01:31:43.454 - 01:32:30.154, Speaker A: Well, we have v one and v two in place. So just the compact operators v one to v two, and there's still some k equivariance. So this comes from the geometric picture of pseudo differential operators, of scalable operators, has nothing to do with Fourier transform. Now, this sphere at infinity of p is something you can calculate in real rank one. The group k actually acts transitively on the sphere at infinity by rotations. And the isotropy group is the famous m group that we had before maps k. Equivariant maps from the sphere into here are the same thing as m equivariant maps just from v one to v two.
01:32:30.154 - 01:33:16.804, Speaker A: So that's a certain finite dimensional space, and you can calculate how big it is using representation theory. On the other hand, you can calculate how big the quotient of this fellow by that fellow up there is using representation theory. And they turned out to have the same dimension. So you have two super spaces of that fellow up there which have the same. And then one can see rather easily that they're nested, and so they have to be equal because they have the same co dimension. It's not a great argument. You'd end up counting something and discounting dimensions, but that's the best we can do.
01:33:16.804 - 01:33:55.524, Speaker A: We know the top thing by virtue of. Well, we thank Harris Chandra, basically, for this formula at the top. A little subsidiary calculation tells you that the Fourier transform takes any scalable operator into a continuous function on this compactified space. That's always the case. Then a representation theory calculation, just an easy thing. Using the Pieter Weil theorem tells you that the co dimension of the c zero space up there in this c space is exactly equal to the dimension of this fellow here. And you're done for some stupid reason.
01:33:55.524 - 01:35:05.264, Speaker A: And so you have this picture, the last thing, let me just put it somewhere here, as a corollary. What comes out of this is a direct understanding of what is this category. This thing is also a category, c star category, where the objects are finite dimensional representations of v one and v two. And with this extremely direct picture of what the c star category is, you can just calculate K theory. And what you find is that the K theory of the skeletal operators, it's just the k theory of the temperate category, the temperature representations are exactly those which occur when the continuous parameter here is zero, as we discussed last time. And so a little bit of regular k theory allows you to prove this directly, which is more or less proving David Morgan's theorem. So here's Niagara Falls, and we're floating along, and we're feeling pretty good, because we've more or less proved Vogen's theorem in representation theory for real rank one groups.
01:35:05.264 - 01:35:25.338, Speaker A: Sadly, now the noise is getting louder and louder, but we tuned it out. Now, however, we cannot ignore it anymore. And over we go. We don't know what comes next. But we got close to representation theory before. Well, it's worse than. It's not.
01:35:25.338 - 01:35:40.166, Speaker A: Not a cliffhanger. I think the correct metaphor is. You know, we've been, oh, we're just falling like this. Yeah. And so you don't know what's going to happen. Maybe some. There'll be some, like a giant eagle will swoop down and save us somehow.
01:35:40.166 - 01:36:07.264, Speaker A: That'll be Jacob lying down, swooping down and saving us by figuring out what to do in general. Very good. We have one last lecture, the famous ultimate lecture as defined in the Oxford English dictionary. And we'll go back, we'll look at really where this all began. We won't talk about representation theory. We'll talk about contact manifolds and pseudo differential operators and contact manifolds. Just to round things out.
01:36:07.264 - 01:36:11.844, Speaker A: Got one lecture. It'll just be a quick, quick glimpse, thank you very much.
