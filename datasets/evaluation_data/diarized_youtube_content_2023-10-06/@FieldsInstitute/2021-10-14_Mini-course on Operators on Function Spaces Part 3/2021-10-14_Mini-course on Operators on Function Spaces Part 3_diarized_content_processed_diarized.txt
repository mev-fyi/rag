00:00:05.920 - 00:00:21.954, Speaker A: Okay, good morning, good afternoon, good evening, everybody. Welcome to the third day of week nine and we are ready for the third lecture by Bill Ross on Henkel and Hilbert transforms. Bill, please.
00:00:22.534 - 00:01:23.354, Speaker B: So once again, a final thanks to Javad for putting in all this work, the Fields Institute for doing all this tech support and gathering us all up. And finally, for all of you for attending or continuing to attend. I posted notes in the chat, but I'll do it again at the end for those of you who are just coming in now. And also there's going to be a fields paper on all this. So some of the main speakers are going contribute survey papers on what they talked about. So in this last lecture, I'm going to combine, I'm going to cover Hunkel operators and the Hilbert transform. But what I'm also going to do is I'm going to try to bring in a lot of the material we've been looking at so far to sort of go over it.
00:01:23.354 - 00:01:59.792, Speaker B: I'm the son of two teachers, so going over things is an important feature of it. And also I enjoy connections between things. Okay, so let's talk about Hunkel operators. Kind of the earliest example of a Hunkel operator is the Hilbert matrix. Interesting in itself. So Hilbert was looking at a finite version of this, sort of an n by n Hilbert matrix. So notice it's the reciprocal of the integers placed on cross diagonals.
00:01:59.792 - 00:02:41.336, Speaker B: So the one half, one third, one fourth, and so on. So he was trying to minimize the l two norm of a polynomial with integer coefficients on an interval. And the determinant of the Hilbert matrix was an important piece of this. So naturally you can consider the infinite Hilbert matrix, and you can think of this as operating on little L two. And you want to know, is it a bounded operator, what is its norm, its spectrum, everything like that. It's a self adjoint matrix. So you should be able to talk about its spectral decomposition.
00:02:41.336 - 00:03:27.768, Speaker B: And I'm happy to report that everything is kind of known about the Hilbert matrix on l two. So first of all, by Hilbert, it's a bounded operator on L two. Hilbert also obtained an upper bound for the norm of H. I think it's two PI. But Schur was able to show that the norm of the Hilbert matrix as an operator on l two is actually equal to PI. Magnus was able to show that the spectrum is the interval zero to PI. Well, that's somewhat to be expected, because the Hilbert matrix is a self adjoint operator.
00:03:27.768 - 00:04:50.674, Speaker B: So the interval should be contained in the real line and the spectrum of the Hilbert matrix is the empty set and sort of the one ring. To master them all is going to be this result of Marvin Rosenblum, which says that the Hilbert matrix is unitarily equivalent to multiplication by PI over cosh PI x on l two of the positive line. If you just look at the graph of PI cosh PI x, it starts at PI and it decreases asymptotically zero. And if you think about it, and you combined everything we talked about with multiplication operators, things like the norm is the essential sup? Norm. The spectrum is the essential range and all those other things, d implies c and b and a. Okay, this is an interesting little gem of Graham Bennett to prove the Hilbert matrix is bounded on l two. If you look at the Cesaro matrix, we covered this in the first lecture and we proved that the norm is two.
00:04:50.674 - 00:05:36.294, Speaker B: A little bit of work. You can show that the entries of the Hilbert matrix satisfy this in a which c and c star. So therefore the. And you can fashion a proof that the norm of the Hilbert matrix is at most four. Yeah, I know it's, the norm is exactly PI. I know this doesn't give you anything new, but it's just an interesting way you can associate the Hilbert matrix with the Cesaro operator. All right, so it's sort of a very natural question to ask.
00:05:36.294 - 00:06:31.044, Speaker B: Suppose I have a sequence of complex numbers. Now, I don't just have the reciprocals of the integers, I have any complex numbers and I place them in this cross diagonal pattern, right? Sort of the kind of like a toplitz matrix. But instead of going along the main diagonal, that goes cross the main diagonals. All right, so when is this operator a mounded operator on l two? Can I compute the norm? What is this operator? A compact operator. Okay, so let me mention a few gems. Here's an early one of chronicler from 1881. All right, so if you have a Hunkel matrix, notice that a Hunkel matrix is going to be determined by the first column.
00:06:31.044 - 00:07:16.284, Speaker B: All right? So this matrix has finite rank if and only if these coefficients. So you take off these the first column and you make those the power series coefficients if you going to be a finite rank if and only if that is a rational function. Now pay attention to the date. I'm not being very specific about bounded operators and things like that. This is 1881. So things like Hilbert spaces and the niceties of operator theory really didn't come about yet. So you sort of just think of this as a linear transformation on sequences.
00:07:16.284 - 00:08:06.986, Speaker B: All right, so we are going to come back to Kronecker's theorem through the lens of what we feel more comfortable with, which is bounded operators on Hilbert spaces. All right, just let me give you a whisper of the proof of this. If you look at this huncul matrix, notice that the first column. Compare that to this second column. And notice the second column is a backward shift of the first, right? You shift everything up one and you lose the first coefficient. And notice that a third column is the backward shift of the second. Again, you move everything up and the first coefficient is gone, and so on.
00:08:06.986 - 00:09:20.170, Speaker B: All right, so if this hunkel matrix has rank n, then the first n columns are going to be linearly dependent. And so there are constants, n plus one, complex constants, not all zero, such that this function identity is satisfied. And then from there, you can fashion a proof that f is going to be the quotient of two polynomials. I'm being sufficiently vague because I don't want to get bogged down into the details, but this is the germ of the proof of this thing. All right? So if we're going to understand when a Hunkel matrix is a bounded operator on l two, we actually don't get all that far from just looking at the matrix, which is why we're all attending this course on operators on functions bases. So we're going to need l two theory here. So, just as a little review and a re emphasis.
00:09:20.170 - 00:10:07.054, Speaker B: So if I have so h two, I want to think of this as the circle. So these are the l two functions with only positively indexed Fourier series. And you can also think of these as square summable power series. And you get analytic functions on the disk. An important space here is h 20 bars. So these are going to be the complex conjugates of h two functions whose zero Fourier coefficient is zero. So this is exactly the l two functions who with only negatively index Fourier coefficients.
00:10:07.054 - 00:11:04.084, Speaker B: All right, so notice that c to the n, these form an orthonormal basis for l two. So h 20 bar is actually h two perp, right? And then we have this nice orthogonal decomposition of l two in h two and h 20 perp. The Ries projection. We talked about this when we covered toplitz operators. So I have you give me a typical l two function, which I'm going to write as its Fourier series. The Ries projection takes this Fourier series and cuts off the negative part of it. An important orthogonal projection is going to be I minus p.
00:11:04.084 - 00:11:36.240, Speaker B: So I minus p is going to do just the opposite. It's going to take a Fourier series of this form and cut off its non negative Fourier coefficients. So you're going to have this part. All right. So I think we're all set to define a Hunkel operator. So a hungel operator is going to depend on a symbol. So for a essentially bounded function, phi on the circle, I'm going to define the Hunkel operator.
00:11:36.240 - 00:12:20.278, Speaker B: Now, this is a little bit different from what we've talked about so far. Hunkel operators go from different spaces. So it's going to go from the Hardy space H two to its orthogonal complement. So this is h 20 bar, and it's defined in a similar way to a tuplitz operator. So you take a function f in h two, you multiply it by phi. Right? Now, f has only positively indexed Fourier or non negatively indexed Fourier coefficients. I multiply by phi, which has both possibly positive and negative Fourier coefficients.
00:12:20.278 - 00:13:21.910, Speaker B: So this is going to have a phi f, although in h in l two, it's going to have both positive and negative Fourier coefficients. And now we apply the ortho complement of the or the I minus p of the projection, and we're going to cut off the non negative coefficients. Let me pause here to say that if phi is an h infinity, so it's an l infinity function, and it only has non negative Fourier coefficients. And I look at the corresponding Hunkel operator. So f is an h two, I multiply it by phi. So phi f is going to be in h two, right, because phi f is only going to have non negative Fourier series here. And so if I take I minus p, well, that's going to project onto the ortho complement of h two.
00:13:21.910 - 00:14:09.418, Speaker B: So it'll be zero. All right, so this is very much unlike with a tupelet operator. So you can have a non zero symbol, an h infinity symbol, for which the Hunkel operator is actually equal to the zero operator. And matter of fact of a little bit of work will show that hv is the zero operator if and only if phi is in h infinity. Okay, so this might seem like a technical annoyance, right, that I have. Hunkel operators are not uniquely dependent on their symbol, but it actually gives rise to really amazing approximation theory. All right, let me just talk about some very basic facts here.
00:14:09.418 - 00:15:07.708, Speaker B: So for an essentially bounded symbol, the Hunkel, the norm of the Hunkel operator is bounded above by the essential suit norm of phi. It usually is not equal to it. And that'll follow from just by looking at the definition of a Hunkel operator. And looking at what we did for toplitz operators now, I could perturb, as I mentioned before, I could perturb a symbol for a Hunkel operator by an h infinity function. And the Hunkel operator doesn't change. This norm is usually not going to be equal. Okay, so this is a very interesting little fact here.
00:15:07.708 - 00:15:50.064, Speaker B: B. All right, so as you can see, I'm very much enamored with the matrix representations of operators. It's me. I'd like to see an operator. So if I take an orthonormal basis for h two, this is the standard monomial basis for h two. So c is a parameter on the circle. And if I take c to the minus n for n equal to one to infinity, that's going to be an orthonormal basis for h 20 bar.
00:15:50.064 - 00:16:21.924, Speaker B: All right, let's figure out the matrix of hv, represented with respect to these two orthonormal bases. Well, here's going to be the mn coefficient. All right, so what do you do? You apply the Henkel operator. So you multiply by phi, and then you apply I minus p. This is the same song over and over again. You've seen me do this. So you move this to the other side.
00:16:21.924 - 00:17:16.744, Speaker B: Now, c to the minus n is an h 20 bar. So this projection has no effect. All right? Then you bring this c to the n over to the other side as its integral, as it's negative. And then you see that the matrix entry is the Fourier coefficient of minus m minus n. Now, give that a little bit of thought, and you're going to see that the Fourier matrix coefficient are going to be constant on the cross diagonals. So the matrix representation of this is actually going to be a Hunkel matrix. So these are these matrices of numbers where you have the same entries on the cross diagonals.
00:17:16.744 - 00:18:36.884, Speaker B: Okay? So, as with topelet matrices and matrices both doubly infinite toplets matrices and singly infinite topelets matrices, you want to know, is the converse true? So if you write down a Hunkel matrix, is it a bounded operator on l two? All right, so a theorem of Nahari states that if you have a Hunkel matrix and it defines a bounded operator on little l two, then there exists a bounded symbol on the circle phi, such that these coefficients alpha n are the Fourier coefficients of. Okay, so in other words, a Hunkel matrix and a Hunkel operator are the same thing. You just have to think of them. Right. I actually like to present a proof of Nahari's theorem, or at least an outline of it. This is not going to be Nahari's original proof. It's going to depend on a wonderful theorem of parrot.
00:18:36.884 - 00:19:42.164, Speaker B: So before I go on, just let me mention here that as sort of contained in Nahari's theorem is actually the norm of a Hunkel operator. So recall that I can perturb the symbol of a Hunkel operator with any h infinity function. So it stands to reason that the norm of this symbol should be the distance between the symbol and h infinity. Right? So this is the official definition of the distance. Again, we're thinking of h infinity as a subalgebra of l infinity. So, moreover, what's kind of exciting and interesting about this is that this extremal problem actually has a solution, right? So there's a phi, an ancient, for which the norm of the Hunkel operator is equal to this thing. So recall that the norm of a Hunkel operator is less than or equal to the essential suit norm of a symbol.
00:19:42.164 - 00:20:24.314, Speaker B: So we actually have a little bit more here. All right, so let me talk about, again, just give you the outline of how you can prove Nahari's theorem with Paret's theorem. So this is, again, not Nehari's original proof. So suppose your matrix, your Henkel matrix, is a bounded operator on little l two. All right? So we're going to rescale it and we're going to make it have norm one. So in other words, a is a contraction on little l two. So a theorem of parrot.
00:20:24.314 - 00:21:11.260, Speaker B: So a theorem of parrot, it has to do with matrices of operators. And when you can sort of place an operator in a matrix with sort of known pieces of it to be contractions, when you can do that and still get a contraction. So a version of this here is, we know this piece here is a contraction. So Parrott's theorem is going to say I can find a complex number so that I can add this column and still get a contraction. All right? So that was so much fun. And I still have a Hunkel matrix. So do it again.
00:21:11.260 - 00:22:07.926, Speaker B: So this operator is a contraction, this little piece, so I can find a complex number and add a column, and it'll still be a contraction. All right? So I won't do this more than twice. So anyway, you keep on going and doing that, and you actually get, I kind of transferred this thing to make it look more like a Hunkel matrix without any cost to anything. So anyway, you're going to get that this doubly infinite Hunkel matrix is a contraction. All right, so now we're going to bring in the norm of a multiplication operator on this. So I'm going to multiply my Hunkel, my doubly infinite hunckel matrix by this unitary operator on little l two. It's called the flip operator.
00:22:07.926 - 00:22:56.008, Speaker B: And it's going to take this Hunkel, you multiply this flip operator with times this Hunkel matrix, everything's kind of backwards. And you're going to get a topelet matrix, a doubly infinite topelets matrix. So notice the alpha zeros over along the main diagonal. The alpha one's here. Okay? So, and we all know from the, from the last talk that this is a multiplication operator. And if you're a doubly infinite toplets matrix, you have to be the entries, the matrix representation of some bounded multiplication operator. So there's some symbol, some essentially bounded symbol for which those entries are the Fourier coefficients.
00:22:56.008 - 00:23:38.288, Speaker B: Okay? So that's an outline of Nahari's theorem. Depends on Paret's theorem. By the way, I kind of learned this proof late in the game, so I googled parrot's theorem, and I came up with a novel about a little boy and a parrot. And the boy was very good at math, and he used his mathematical skills to go on all sorts of adventures. But anyway, it's not that parrot theorem. It's Stephen Parrot. All right, so let's revisit the Hilbert matrix.
00:23:38.288 - 00:24:29.588, Speaker B: So again, this is a special example of a Hungel operator. So we looked at these entries, the first column. All right? So the function that you get by making those the first column, the power series coefficients of a function. If I've done my calc two right, you get one over z times the logarithm of one over one minus z. So, first of all, that is not a rational function. So the Hunkel matrix is not a compact operator, but it is an un, it's an unbounded function. It's an h two square summable power series.
00:24:29.588 - 00:25:25.754, Speaker B: And if you were paying attention to Daniel Guerrella's talk, it actually, this is kind of the first example that you think of as an unbounded BMoA function, an analytic function of bounded mean oscillation. So you can also show that here is a phi. So this is an l infinity function on the circle, right? And a little bit of Fourier series will show that the projection of that function actually gives you f. That's not to be surprising. Gorilla mentioned Pfefferman's theorem. That if I take a essentially bounded function and I apply the projection, I don't necessarily get an h infinity function. I get a BMOa function.
00:25:25.754 - 00:26:08.594, Speaker B: So this is an example of this. All right, so there's a kind of a generalization of this observation. So I'm kind of piecing together Nahari's theorem and Pfefferman's theorem and reinterpreting it. All right, so if I look at a Hunkel matrix, this is going to be a bounded operator on l two if and only if. This function you get is in BMoa, and that's if and only if the f is the Rees projection of an essentially bounded function. So there's kind of a way of understanding Nahari's theorem and Pfefferman's theorem. I'm not doing it justice.
00:26:08.594 - 00:26:37.488, Speaker B: I covered the proof of Nahari's theorem just to show you the. The depth that's required to do that. But anyway. All right, let's revisit. So I kind of was a little bit vague on purpose in covering Kronecker's theorem because there was no Hilbert spaces involved in there. And I kind of want to make it up to you. So we were talking about Hunkel operators.
00:26:37.488 - 00:27:08.774, Speaker B: So they should be bounded operators with symbols and things like that. So here's a way you can kind of reinterpret Kronecker's theorem. So you have an essentially bounded function. You want to know, when is the corresponding Hunkel operator? When does it have finite rank? And that's if and only if I minus p is a rational function. And if you like finite blaschka products. And I certainly do. I can.
00:27:08.774 - 00:28:00.944, Speaker B: That's the same thing as there's a finite blotchka product that multiplies phi into h infinity. So I'll leave it to you to do a little exercise to show that rational function thing is the same as these things. But this is a more Hilbert space oriented thing. So, original Kronegar's theorem was not quite all right, there's a version of the Brown Helmos theorem here. So, do you remember the Brown Helmell's theorem was for toplitz operators, and it's which operators on h two are topelet's operators. And there was a nice operator identity that took place. So which operators, bounded operators from h two to h two perp are Hunkel operators.
00:28:00.944 - 00:28:36.366, Speaker B: And it's this theorem. So this is kind of a version of it. So a is a Hunkel operator if and only if this operator identity holds. So let me reorient you to all the little pieces here. So Mc is the bilateral shift. So, multiplication by the independent variable on l two, and Tc is the unilateral shift on h two. It's multiplication by the independent variable on h two.
00:28:36.366 - 00:29:40.140, Speaker B: And recall, this is the Hunkel operator version of this theorem for Toplitz. And if you think about it, and you work, a matter of fact, the proof I know uses matrices. So essentially, you show that a satisfies this identity if and only if its matrix representation is a Hunkel operator. And that should do it. Okay, so now I'm going to, I can't not talk about Hunkel operators without talking about h infinity plus c. All right? So I'm going to combine a lot of results here to kind of give the an efficient way of covering all this stuff. So we haven't, we've talked about bounded Hunkel operators, but what about compact ones? So Tupelet's operators on h two weren't worth talking about for compact operators, because it's just the zero operator.
00:29:40.140 - 00:30:34.666, Speaker B: But Hunkel operators are extremely interesting on h two. So here's the theorem. So hv is compact if and only if the symbol is in h infinity plus c. Notice that I can perturb a Henkel operator with any ancient affinity symbol. And a kind of a restatement of that is there's a psi in the continuous functions on the circle such that the Henkel operator can be represented with that continuous symbol. All right, here's another sort of amazing thing I'll start off with. This theorem of Saracen, is that this h infinity plus c turns out to be a closed subalgebra of l infinity.
00:30:34.666 - 00:31:21.768, Speaker B: Neither of those facts are obvious, right? So it's certainly h infinity, right, is a closed subalgebra of l infinity, as are the continuous functions. But when you add them, why is that closed? And even more, think about this. Take an f in h infinity and take a g, a continuous function g, right? And take f plus g and square that. So you foil that out and you get f squared plus two. F, g plus g squared. F squared will certainly belong to h infinity. G squared will certainly belong to the continuous function.
00:31:21.768 - 00:32:25.944, Speaker B: But what happens to fg, right? So f is an h infinity, but it may not be continuous. So fg may spoil the continuity, and then g may be a continuous function, but it's not necessarily analytic. So f times g will spoil the analyticity. Right? But anyway, Sauerson was able to actually show that this is actually a closed subalgebra of l infinity. All right, so since it's the closed subspace of l infinity, I can talk about the distance between an l infinity function and this space. And it turns out that this is the essential norm of the Hunkel operator. So do you remember the essential norm of an operator is the distance between Hunkel operator or any operator, and it's.
00:32:25.944 - 00:33:14.332, Speaker B: And the, and the algebra of compact operator. I'm going to be an algebra in this case, but it'll be the space of compact operators. So notice that when phi is in this, right? So when phi is in h infinity plus c, or in other words, the distance is equal to zero, then that means phi is in this. And you go back to part b here, and that'll show that hv is compact. And in other words, the essential norm would be zero. All right, so this is a very, a full picture of the compactness of Hunkel operators. And there's a lot more to say about the h infinity plus c, and I'm definitely not doing it its proper justice.
00:33:14.332 - 00:34:06.904, Speaker B: All right, so if you're kind of disappointed with the thin sliver of Hunkel operators I cover, I'll refer you to two books, one of Jonathan Partington and one of Vladimir Peller, that are thorough treatments of Hunkel operators. All right, so let's cover the Hilbert transform. I include the Hilbert transform because I use it a lot. It appears, in a way, when we talk about the Rees projection on different spaces besides l two. And it also belongs to a very important class of operators that still undergo study. Now, they're called the Calderon sigmund operators. So let's do it on the circle first.
00:34:06.904 - 00:35:12.588, Speaker B: So suppose I have a c two function u, and I'm going to do everything in polar coordinates, right? It's a real valued c two function, then it's harmonic. In other words, the Laplacian is equal to zero if and only if it's the, the real part of an analytic function. It's a very general theorem and it works on any simply connected domain. Matter of fact, it's a characterization of simple connectivity as a corollary to this. If I have a real valued harmonic function, then there's a harmonic conjugate v, right? Harmonic function b, such that u plus r iv is analytic. And this function v is called the harmonic conjugate. And I'll use the, without really being too fussy here, because the harmonic conjugate v is unique up to an additive constant.
00:35:12.588 - 00:35:59.248, Speaker B: So I'll use the, and no one is going to be too upset with me. All right, so here's the question. If I give you a certain type of harmonic function u, what type of harmonic function is v? What type of harmonic function is the harmonic conjugate v? Since they only differ, harmonic conjugates will only differ by a constant. And let's assume the constants belong to any space, any desirable space of functions you would like. This is a very natural question to ask. All right, so let's talk about a class of harmonic functions. So these are harmonic functions by the, they're the Poisson integrals of l two functions.
00:35:59.248 - 00:36:58.154, Speaker B: So if you have an l two function on the circle, you form the, what's called the Poisson integral. So you integrate it against this kernel. So notice that as a function of re to the it, this part here without the real part is actually an analytic function. So its real part will be harmonic, and then differentiating under the integral sign will show that u is a harmonic function. All right? And then Fatu's theorem will show that the radial boundary values of u are equal to f almost everywhere. So this solves what's known as the Dircel apex problem. So if you have a continuous function, continuous boundary data, can I find a continuous function on the closure of the disk that's harmonic on the interior that agrees with f on the boundary? So this is a version of that.
00:36:58.154 - 00:37:53.864, Speaker B: All right, so let's just sort of associate a little bit of Fourier series with this. So if you work out a little bit of Fourier series here, not Fourier series, geometric series, right? You can write this as this infinite series. All right, now put all that back into u. So instead of this. So the Poisson integral, right? I used a power series, right? Then I can switch the limit, the sum in the integral, because there's enough uniform convergence going around. And notice at some point I'm going to be integrating f of e to the it against e to the minus int, which is going to give me my Fourier coefficient. So here is the kind of a nice series version of that harmonic function.
00:37:53.864 - 00:38:50.530, Speaker B: All right? So if I, instead of, so the harmonic conjugate should just be the imaginary part. So it should be the Poisson integral, but what's called the conjugate Poisson integral, where I take the imaginary part of this. So again, for the same reason, this is going to be harmonic on the open unit disk, and for a slightly different reason, its boundary values will exist almost everywhere. So I want to focus on this function q. So let's look at the Fourier series of q. So if you take the imaginary part of this, use a little bit of geometric series, and you get something a little more complicated. It involves the signum function.
00:38:50.530 - 00:39:32.354, Speaker B: So signum of n is one if n is positive, it's minus one if n is negative, and it's zero if n is equal to zero. Plug all this back into v. So this will be the harmonic conjugate of u. So remember, f is in l two. All right, so replace this imaginary part with this series. There's enough uniform convergence going around. And notice, as before, I'm going to be integrating at some point f against e to the minus int, and that's going to give me the fourier coefficients.
00:39:32.354 - 00:40:33.922, Speaker B: You can see the relationship between those two things. All right, so this function q, this boundary function, is called the harmonic conjugate to f, and I can actually compute the. I'm giving you the Fourier series. So if you read some of the classical works of Pravalov and Rees and things like that, when they talk about the conjugate of a function of an l two function, they're talking about this Fourier series. All right, so now I want to look at the conjugation operator. So this is a partial isometry on l two, and I can identify its initial and final spaces here. So the kernel is c and the range is l two minus c.
00:40:33.922 - 00:41:15.492, Speaker B: So just a moment's thought here. Notice that the Fourier series of f is always missing the constant term because of this signum function. So that gives you the range part. And the kernel is obviously going to be when f is a constant. All right, the partial isometry part is going to come from just Parsifal's theorem. So the l two norm is the same as the, some of the squares of the Fourier series. All right, so that's a cool thing about the conjugation operator.
00:41:15.492 - 00:42:21.130, Speaker B: You can actually compute its adjoint and maybe don't go through all the details here, but you just kind of bring this to the other side and use Parcelfold's theorem. So the adjoint is minus q. Again, I'm jazzed up all about the matrix representation. So on l two, if I write the orthonormal basis for l two, it's a diagonal matrix. This is the conjugation operator given by this series. We all know about diagonal matrices, right? And I can quickly identify the spectrum, and the point spectrum is minus I zero and I. All right? And no talk about the conjugation operator would be complete without sort of a discussion of some classical theorems here.
00:42:21.130 - 00:43:38.154, Speaker B: So the oldest, from 1916 is due to Ries, which says that if you give me any Lipschitz function, the conjugation operator will take you to the Lipschitz function, as mentioned in previous talks, certainly in guerrilla's talk, that there's the theorem of Ries from 1928, which says it'll take a lp function to lp. As to be expected, there are problems at the boundaries. If you take an l infinity function, the conjugation operator will not take you to l infinity, it will take you to Bmo, and it takes the continuous functions not to the continuous functions, but to Vmo. And if you're wondering what happens to l one, it goes to a space called weak l one, which is not even in l one, but anyways, measurable functions which satisfy some weak inequality. All right, there's this. I love this theorem. It's by piccurides, which actually computes exactly the lp norm of the conjugation operator.
00:43:38.154 - 00:44:27.350, Speaker B: If you want to check that this formula makes sense, try it out for p is equal to two, right? So when p is equal to two, I had better get that. The norm of the conjugation operator is one because there's a partial isometry on l two. Let's see, plug in. P is equal to two, and I get PI over two times two, which is PI over two, and the tangent of PI over four is actually one. All right, and finally, there's the. You can also write the conjugation operator as a sort of a cauldron Sigmund operator, sort of a singular integral operator. All right, let me mention the Hilbert transform.
00:44:27.350 - 00:44:56.100, Speaker B: So now if we look at the same problem on the upper half plane. So now I look at l two functions on, on the line, and I look at these harmonic functions. So they're the real part of this kernel. So z is in the upper half plane. So this was an analytic function. So it's real part will be harmonic. This is a harmonic function.
00:44:56.100 - 00:45:38.092, Speaker B: And there's kind of a version of the Poisson kernel for the upper half plane. And so this thing is equal to f almost everywhere. So the harmonic conjugate should be when you take the imaginary part of this thing. So this limit will exist almost everywhere. And the almost everywhere limit is going to be what's called the Hilbert transform of f. All right, a couple things about the Hilbert transform. It's a singular integral operator.
00:45:38.092 - 00:46:21.194, Speaker B: So just as before, I can write it. So this is what people know as the Hilbert transform, is this cauldron Sigmund operator. And by the way, this works in rn. There are versions of this thing. This is a unitary operator on L two. I can compute its adjoint, just like in a very similar way for the adjoint of the conjugation operator on L two of the circle. Let's bring in stuff about multiplication operators that we've covered so far.
00:46:21.194 - 00:47:28.204, Speaker B: Notice that if I take the Fourier transform on L two, it's going to conjugate the Hilbert transform into this multiplication operator on l two. So it only has two values, I and minus I zero. But that's insignificant. So let's see now, you know all sorts of stuff about multiplication operators, right? So I know the spectrum of a multiplication operator is the essential range, right? And that's all the range of this function. I times this signum function is either I or minus I. And I can also pick out the eigenvalues, right, just by looking at what we discussed earlier with multiplication operators, right? There are two very significant sets for which this symbol takes on the value I and minus I. All right, again, one last stab at matrices.
00:47:28.204 - 00:48:26.034, Speaker B: I can identify the eigenspaces for the eigenvalues minus I at the top right, and I at the bottom here, right? So you can by hand, right, it's an integral. You can actually show that these two things form an orthonormal basis. Together, these form an orthonormal basis for l two. The first one, it will be an orthonormal basis for h two of the real line, and the second one will be its orthogonal complement. All right, so, and you get a, a diagonal matrix. So I have always, always impressed when you can give a full spectral decomposition of any normal operator, in this case, a unitary operator. All right, so we're coming to the close here.
00:48:26.034 - 00:49:04.548, Speaker B: So let me mention a few things. So again, apologies in advance. I know I probably didn't cover some of your favorite operators. And, you know, you have to make a choice somewhere. If I was given a month for these talks, I could have done a lot more, but three days is all I was given. Okay, so where can I learn more? Right, so there's a long list of references. So I have twelve pages of references at the end.
00:49:04.548 - 00:49:53.388, Speaker B: And I also have an upcoming book with Stefan Garcia and Javad ma Trehi. It's called operator theory by example. It has 20 chapters, and so it covers 20 of your favorite operators, including these. Okay, so again, these are all the references at the end. So what I'm going to do is I'm going to stop sharing, and then before I forget, I am going to post the notes again, in case you didn't get them. So these notes are the same as the lectures, but I corrected a few typos. And some of you are generous enough to.
00:49:53.388 - 00:50:10.404, Speaker B: To point out little inaccuracies. And I also took away all the pauses. So it's not 400 pages with all the pauses. It's only 100 pages. Okay, so that's it. And thanks again for listening.
00:50:11.384 - 00:50:22.094, Speaker A: Thanks a lot, Biel. Thank you for those wonderful talks. Any question or comments for Bill?
00:50:24.954 - 00:50:44.486, Speaker C: Hi, Bill. Bill, this is Sheldon. I want to thank you for a really, truly terrific set of talks. Thank you so much. Thank you. I wanted to make a comment about the Hilbert matrix, which is truly phenomenal. Again, this is the matrix you showed one over j plus k plus one in the jth row, kth column.
00:50:44.486 - 00:51:12.276, Speaker C: Of course, that's an infinite matrix. But if you chop it, pick a positive integer n. Chop off the upper left hand corner, you get an n by n square matrix. And so you can ask questions about that. One question is invertibility. And that Hilbert matrix, chopped off is often used by people writing linear algebra software to check because it's actually invertible for every. Nice but barely, barely, barely.
00:51:12.276 - 00:51:36.144, Speaker C: So if n is 20, for example, the smallest eigenvalue is on the order of ten to the -20 8th. And so people who are doing software think, oh, that's a round off error. It's really zero for the smallest eigenvalue. But that's not true. It's really invertible. But there's a more amazing fact that's not so well known. If you look at this n by n matrix, which is invertible.
00:51:36.144 - 00:51:50.670, Speaker C: All the entries in the inverse are integers. I don't know any connection to that with function theory but it's an absolutely startling fact that you don't expect. It's always amusing.
00:51:50.782 - 00:51:54.230, Speaker B: I am taking all this down, but. Okay, so they're all integers.
00:51:54.302 - 00:51:54.934, Speaker C: Yes.
00:51:55.094 - 00:51:55.918, Speaker B: Okay.
00:51:56.086 - 00:52:00.406, Speaker A: And Sheldon, any specific reference for this observation?
00:52:00.510 - 00:52:13.504, Speaker C: Oh, I can't remember now. Sorry. But I know it's true. I just tested on Mathematica during your 20 by 20 matrix. And the inverse has huge entries, but they're all integers.
00:52:14.164 - 00:52:18.900, Speaker B: Wow, thanks a lot for that. That is great.
00:52:19.092 - 00:52:27.788, Speaker A: Yeah. Indeed. The title of the book that bill mentioned by example shows why we are interested in such examples.
00:52:27.916 - 00:52:37.340, Speaker C: Oh, yeah, it's a great example. Even the five by five. Very interesting that they're all integers. Thank you again.
00:52:37.532 - 00:52:38.424, Speaker B: Thank you.
00:52:39.924 - 00:52:51.684, Speaker A: Any further comments or question for bill? Bill, I just wanted to thank you also for a fantastic series of talks. It was really, really nice.
00:52:51.844 - 00:53:08.694, Speaker B: Yeah. Thank you so much. I do appreciate that these talks are a little always nerve wracking to do because you have to make some choices. And I see people here who I know I didn't cover their favorite operator, but.
00:53:12.874 - 00:53:22.898, Speaker A: I wondered, Sheldon, can you give us any hint as to how it's proved that the entries of the inverse of that finite Hilbert matrix are always integers?
00:53:22.986 - 00:53:37.734, Speaker C: No. I went through it once, but it was long enough ago that I forget what tools were used. I don't think it's function theory, though. So somebody. That's a good homework. Somebody come up with a function theory proof?
00:53:39.634 - 00:54:06.314, Speaker A: In a sense, yes. Stampach, who indeed generalizes some of these results, made the connection to Jacobi functions, or Jacobi matrices and used hypergeometric functions to find the spectrum. The paper is available on archive and it's not published yet. It's a very new result.
00:54:09.934 - 00:54:19.674, Speaker B: More stuff to. More stuff to. So, thankfully, that last comment is coming from a co author of this book, so I don't have to come down too far.
00:54:20.974 - 00:54:25.438, Speaker C: If you Google Hilbert matrix inverse, you'll find some references. Sure.
00:54:25.486 - 00:54:29.874, Speaker B: Okay, I will. I will do that. Thanks, Sheldon. That's terrific.
00:54:31.454 - 00:54:34.514, Speaker A: Okay, so let's thank Bill again.
00:54:36.614 - 00:54:40.034, Speaker B: And thank you, Bill.
00:54:40.614 - 00:54:43.034, Speaker A: We resume in six minutes.
00:54:44.094 - 00:54:44.854, Speaker B: We just got a new.
