00:00:00.280 - 00:00:55.878, Speaker A: I'm planning to kind of go slowly. And in this first lecture, in fact, I mostly just want to kind of essentially unpack all of these words in a relatively precise way. And so maybe some of what I say, hopefully not too much by design, not too much, will be of review. But I want to kind of set the stage for what I plan to do in the next one, since I want to cover some ideas that we're using in some detail. So one thing is, if you do have a question, either just put it in the chat or preferably just shout it out. I'd much rather move more deliberately than push forward. If there's something confusing, just to set up some notation, which is the one that everybody has been using.
00:00:55.878 - 00:01:52.144, Speaker A: I mean, I'll think of these configurations of points, p, and I'll always just underline them and they'll always be like p one to PN. And we'll be looking really, in this lecture, I'm really interested in real configurations, but if they're whatever, complex as well, that won't actually come up so much. So, my graphs are the same kind that we've been looking at. Usually they're simple, they are finite, they have n vertices and edges. And really for convenience, I will usually just think of v as like one to n. There's no kind of real issue with respect to indexing the vertices. And also I'll just think that I have some fixed edge ordering.
00:01:52.144 - 00:02:45.344, Speaker A: So I want to keep things as simple as possible. And then of course, I'll just use this g comma p notation to be a framework. And because you've had excellent instruction so far on the topic of frameworks, I'm not going to really belabor what one is. So let's just recall two definitions. So one definition is that in dimension d, a framework, GP is globally rigid if equivalence implies congruence. So if GQ is equivalent to GP, right, then GQ is actually congruent to GP. And this is unconditional.
00:02:45.344 - 00:03:56.554, Speaker A: There's no open neighborhood of GP or something. So in other words, in congruent, this is what Bob would have said as well. So congruent just means related by a rigid motion, right? Say the d dimensional Euclidean group. Okay? And so Bob also talked about this, but we'll say that a framework is universally rigid in dimension D. If, well, for all bigger dimensions, big d greater than or equal to little d, GP is globally rigid as it is in dimension. Oops, I am sorry, big D. And I'll try to scribe these afterwards and distribute it so that you have some.
00:03:56.554 - 00:04:41.254, Speaker A: Some decent typewritten notes, and you don't have to live with my handwriting. I see my meqs, and so I know what students don't like, and they don't like my lowercase deltas, for example. But I think my english letters, whatever roman letters are. All right, so my remark here is that, notice that GP, as I put it up, in dimension, it's not changing, so it will be, in some sense, flat. Right. And for the purpose of, like, giving things coordinates, you can always just use an isometry of the bigger space to arrange that. The affine span of p is, like the first little d coordinates.
00:04:41.254 - 00:05:06.634, Speaker A: Right. Or even smaller, if it was flat to start with. Right. And so I will kind of think of. Kind of r to the little d is just, you know, being embedded in the standard way in r to the big d. They're euclidean spaces, but this doesn't really cause any technical problems either way. If it was embedded differently, it would still have the same metric.
00:05:06.634 - 00:05:22.414, Speaker A: Okay. And so I have some examples. So you should start with a picture. So maybe if I could throw this out, like, is maybe this one. This is meant to be completely flat. So is this universally rigid?
00:05:26.314 - 00:05:27.094, Speaker B: Yes.
00:05:27.954 - 00:05:37.854, Speaker A: Okay. So I hear. Yes, I see also will is nodding. So anybody wish to fill in why it's universally rigid?
00:05:40.184 - 00:05:42.524, Speaker B: Maybe the triangle inequality.
00:05:43.104 - 00:06:14.388, Speaker A: Yeah. Yeah, precisely. Thank you, Alex. So, basically, the triangle inequality says that if somehow. If these lengths add up to whatever this length here is, the only way to actually achieve that is for them all to be collinear in this euclidean space. On the other hand, I've got this example on the right, and this one. Well, the triangle inequality argument doesn't work.
00:06:14.388 - 00:07:01.270, Speaker A: Does this one feel sort of universally rigid? No. I mean, yeah. So these are the examples to kind of, I guess, keep in mind. So, to me, you know, somehow you could just, like, unfurl it a little bit, as freehand drawing is dangerous. Right. But you kind of want to think of this as, like, a flattening of a parallelogram that, you know, leans this way. Okay.
00:07:01.270 - 00:07:41.354, Speaker A: And this is very, um, I apologize, impressionistic. So, here are some more examples in D equals two. And I guess the one thing I would want to notice is that it is now, like, a little bit trickier to see what's going on. It will turn out that this one is universally rigid. This one is not even globally rigid, which you probably learned about from Hendrickson's theorem. Right. And the reason is that this is minimally, infinitesimally rigid, as I have drawn it.
00:07:41.354 - 00:08:56.556, Speaker A: So you might, you know, if you're very lucky, you'll get some lectures from Ileana Strano and maybe she'd talk about these pseudo triangulations. So there's a way of understanding this as being infinitesimally rigid as drawn, for what it's worth. So this one is, you are right, and this one is not, you are, but like kind of, it should be if I haven't drawn it too degenerately, it's globally rigid. So I've got some kind of zoo already in dimension two, there's something going on. And so, you know, these, um, these properties are like a tad subtle, let's say, and I'll, I'll come back to these examples in a bit. So, but before I do that, I want to, um, to kind of go through some, some, some kind of technical material because it's, it's, maybe it's how we do things in rigidity, but it's maybe different than what you've seen. Either you haven't seen it or, you know, algebraic geometry as an algebraic geometer.
00:08:56.556 - 00:10:39.154, Speaker A: And this is not how you think about some of this stuff. So, so I'm going to say that, you know, a set in r to the d, or for that matter c to the d, is algebraically if somehow it's the vanishing locus of a bunch of polynomials, right? And for reasons I don't want to get into too much, you might as well if all you care about is set theoreticness, right? The ideal would be finitely generated, so you can imagine it as even a finite set of polynomials. And there is a kind of useful topology that you can then place on r to the d or c to the d, which is just to say that the closed sets are the algebraic sets, okay? And so now if you're so, so please do stop me if you don't like some of the things I'm about to say. So this topology is very coarse, right? So notice that, you know, right, so closed in there's a risky topology, implies closed in the standard topology, but the converse is not true. And also somehow interesting things happen. So the open sets in this topology are all actually dense, they're all very, very large, and the closed sets are all nowhere dense. And this is also true in the standard topology.
00:10:39.154 - 00:12:02.884, Speaker A: And something sort of makes this interesting is that in this very coarse topology, like r to the d, the whole space is irreducible, right? So this, I'm going to say, is an easy exercise. And by irreducible I mean not the union of proper closed sets. Okay. And so, and this is somehow clearly not the case for the standard topology, right? Because in the standard topology, I don't know, like Richard is equal to maybe like, you know, minus infinity, zero, union zero, infinity, something like this, right? So this property of being irreducible is like not one that if you're used to, like just doing analysis or, you know, it's not one that's that common. But it's actually pretty useful here because there are so few closed sets running around. Okay. And so the reason I brought this up is that a, I'm gonna build on it to use some of these things later, but I want to discuss genericity in some detail to make sense out of the statements that I would like to make later.
00:12:02.884 - 00:13:44.994, Speaker A: And so let's say that some sort of property is generic, and this is going to be kind of like generic wong, right? So in our, in our rigidity literature, both things run around. So let's say it's generic one, if there's some kind of open dense subset of our big n dimensional space. And really I'm going to want to write, or indeed could replace rn with an irreducible algebraic set. We'll get to that later. So if somehow you've got like an open dense subset, and you might insist it's a risky open, you might not, on which the property is constant. Okay, so the example to keep in mind is like infinitesimal rigidity, you know, with respect to a fixed graph, right? And, you know, of course, defixed. So here kind of as p varies, right? So let's just say, you know, as p varies, right.
00:13:44.994 - 00:14:31.134, Speaker A: So does the rank of the rigidity matrix, right? But there's a big, there's an open set because the being less than the maximum is defined by the vanishing of some minors. There is an open set on which it's constant. And so as p varies, there's a risky open set on which all of them are infinitesimally rigid. If even one point is infinitesimally rigid, we'll get to that, right. And this is like a good notion of generic. It's the one that many people seem to want in applications. It's perturbation, stable and so forth.
00:14:31.134 - 00:15:20.624, Speaker A: Right. But this is like, somehow the crux of this is that it's a property that's being stated as generic, right. And also I had to do something. I had to, like, fix d, and also I had to fix g, or at least the number of vertices that g has. There has to be like a finite number of graphs involved, right? I am not free to look at all graphs of all numbers of vertices to get the statement. Then I'm intersecting infinitely many open sets probably don't get an open set anymore, for what it's worth. So I might, I might not, but, like, I'm not very safe.
00:15:20.624 - 00:16:23.874, Speaker A: Okay, so I could also talk about what it means for like a point in an irreducible algebraic set. And irreducible is sort of somehow important here. And I also had to bring in a new word, which is I had to talk about being defined over something. So I'm going to think that all my defining polynomials had, say, rational coefficients, but some countable subfield would be fine for this purpose. And we'll say that the point little x is generic if somehow, if some polynomial vanishes at little x over, also, say, defined over the rationals, right. Then also it vanishes at all the other points. Right? So in other words, x doesn't have any special relations on it that are not satisfied by every point of big x.
00:16:23.874 - 00:17:40.124, Speaker A: Okay, so this is a notion of genericity. I should say that it is not the same one that people in algebraic geometry use, because this is not even scheme theoretic. I'm talking about like actual points on the set. So it's some sort of quite strong notion that I'm imposing here. You know, I also don't want to make it sound like I'm claiming great algebraic geometric knowledge. So and so the fact is kind of that if your property in question is defined kind of algebraically, the way infinitesimal rigidity was in terms of the vanishing or not of some polynomials, then the property being generic would be the same as being constant over these generic points, which would be the same as just saying that, like any generic point has the property. So this is just a fact, and this is kind of the relation between the two notions for our purposes.
00:17:40.124 - 00:18:25.574, Speaker A: Right. And this is certainly true, I should say, you know, just compare, right, to sort of Asimov and Roth or something. That's exactly what they said about infinitesimal rigidity. Okay, so a little bit more. So let's like, first give an example. So what are the generic points of R? Well, I mean, they are just r. Take away the algebraic numbers and this is basically a good example.
00:18:25.574 - 00:19:10.924, Speaker A: So to keep in mind of how these generic points can sit and why they're not like a particularly nice set in some way like the algebraic numbers, if you've taken them away, they're still, like quite a bit left. They're also dense in the standard topology but they also don't contain any open set. Right. If you have a non algebraic number, you can't perturb it arbitrarily and remain non algebraic, for example. Right, so they are standard topology dense. They are, if, you know, some analysis, they're a residual set. People also call this, I think, comeager.
00:19:10.924 - 00:19:52.484, Speaker A: Um, right. But they're not open. Um, okay, so a fact that I sort of want to kind of get out of the way. I'm trying to not go too deeply down a rat hole. But if we were working over the complex numbers, they would be standard topology dense in any irreducible algebraic set. Um, but over r, this is not necessarily true because you have things like the Whitney umbrella or something. Right, so.
00:19:59.664 - 00:20:02.004, Speaker B: Can you explain that example a little more?
00:20:03.104 - 00:21:01.714, Speaker A: Yeah, sure. So there are real varieties, I forget the defining equation of the Whitney umbrella, but it's basically a surface that also contains part of, and all those points that are on the z axis are somehow non generic according to this definition, because they satisfy this equation which is not satisfied by the whole thing. So you have a ruled surface that when it folds in on itself, contains an axis or something like this, but otherwise the surface isn't hugging the axis. And so once you get far away from the, the main part of the surface, you're satisfying an equation which is not satisfied by the whole thing, namely x equals zero and y equals zero. Got it. So this can happen. So it's one of these things where the complex geometry is much more rigid and nicer.
00:21:01.714 - 00:22:05.962, Speaker A: Okay. But basically what I'm getting at here is that these generic points do actually exist. So this notion is not completely useless. So the reason I'm bringing this up now is just that when we talk about what these statements mean, I'm going to try to be at least a little specific. So now I'm going to sort of end, for the moment, end of technical stuff, and we'll talk about rigidity for a minute. So this is something that you've surely seen. So if I have a framework, an equilibrium stress is an assignment of edge weights, so that at every vertex, this vertex equilibrium condition holds.
00:22:05.962 - 00:23:16.114, Speaker A: And this notice is like a vector equation. So I have assigned edge weights omega ij to every edge ij of the framework. And then I sum up over, whoops, over the edge vectors, and this vanishing is equilibrium. And so here's some standard examples. A one dimensional example, this flattened triangle, here's like a two dimensional example. I've meant to draw this as a square, but notice that one thing that's pretty nice about this vertex equilibrium equation, is that it really refers to p, but not to d in any kind of meaningful way, as opposed to like, say, flexes, which have some dimension in which they live. And so a tool that we're going to use a lot around universal rigidity and also global rigidity, is the stress matrix.
00:23:16.114 - 00:23:57.644, Speaker A: And so I should say I did. Okay, so one way to think about defining the stress matrix is to think about, right, this equation here with p varying and one dimensional. So p I and p j are just numbers. And this would give you like a linear map. And so something that's important, and I think Tony is probably told you about it, and Bob has certainly told you about it, is that. So the rank of an equilibrium stress is just the rank. I should have underlined this in green of omega.
00:23:57.644 - 00:25:16.574, Speaker A: Okay, so I have done a thing which I know that people like, I hope, which is, I have put numbers into square arrays, right? So if I took this equilibrium stress and I numbered my vertices, say one. Let's just get this right, two, three. I numbered my columns, one, two, three my rows, one, two, three. Then I could just put the, you know, this is omega one, two. This is omega one, three. Now, notice I put minus the sum of them on the diagonal. And the reason I did that was just because I had minus PI here.
00:25:16.574 - 00:26:07.454, Speaker A: So I could just take this, move it over. I can even have a little fun with technology. This is so cool. So great. I could think of this instead as something like, right, I could do something like this. And, and that would give you the sum. Diagonal elements are minus the sum.
00:26:07.454 - 00:27:13.658, Speaker A: Okay. And then I, um, I kind of have done the same thing here. And I'm pretty sure that I did it right, because I checked a couple of times, I used this numbering here. All right? And so, so this is the mechanical procedure of going from a equilibrium stress to its stress matrix. Now, nobody has really yelled just yet, so my examples were complete graphs, right? So obviously I'm going to have to put zeros if my graph is not complete, right? So basically, you know, sort of. Spoiler, whatever. Let's call it big omega.
00:27:13.658 - 00:28:00.428, Speaker A: Ij if, you know, ij is. Oops, it's not an edge, just to be very clear. And so this leads us to, like, a fact. And, I don't know, is this like totally review? Like wave, if this is totally review, and I'm going way too slow. Okay, good. So let's say that I could say that I could start with a framework GP and pick some equilibrium stress of it. But it might also be the case that some other framework, GQ, somehow has the same stress.
00:28:00.428 - 00:28:57.304, Speaker A: And what I mean by this is that if I plugged in q to my equilibrium equation, and then I would also get vertex, you know, equilibrium at every vertex, right? And we'll say that like, q is a satisfier of the stress omega. Okay? And so a useful lemma, and this is one that you have probably seen in a different guise, right, is that if somehow q is an affine image, p. Right, then I should really say, I apologize, satisfy all stresses that the framework GP does, right? And does anybody want to shout out a proof of this.
00:29:08.024 - 00:29:10.524, Speaker C: Linearity of matrix multiplication?
00:29:12.784 - 00:29:39.002, Speaker A: Yeah, I mean, that's actually probably the fancier proof than the one I had in mind, but. Yeah, yeah, yeah. I mean, so you can simply. Yeah, I mean, so this is precisely the proof, or maybe so. So the point is somehow that you have zero equals. Well, let's write our equilibrium condition. Do it correctly.
00:29:39.002 - 00:32:36.518, Speaker A: J neighboring I pj minus p I, right? But then I'm going to say that if I replace it with l times the sum of j omega I j pj minus p I, where l is just any linear map, okay? But now I do what will suggest and I just pull it through. Okay, so this is true for any linear map, right? This will be true, right? But then this is just, this is good enough, right? Now take, you know, take l to be, sorry about screwing up my slides a little bit. I have to scroll off the top, right? So now, you know, take l as the linear part, the affine map a, such that qj is equal to apj for all j, right? So this equation is sort of nicely affine invariant, right? Translation would work even more easily. It just cancels out translation even easier. Okay, so now, one thing that I want to mention about this little lemma, right, which is it may have married, right? I mean, sort of one way to understand that is that this affine image could be defective. It doesn't even have to be non singular. This will still be true, right? And so you can imagine kind of picking up more equilibrium stresses if somehow you, like, flatten the thing out, right? Like as an example, like since you've been doing a lot of more combinatorial things, think k four is independent in dimension three.
00:32:36.518 - 00:34:13.104, Speaker A: It's dependent in dimension two, right? So if you take a stress free simplex in dimension three and like, just sort of project it onto xy, then you'll pick up for sure a stress just by linear algebra duality with fluxes okay, so another useful fact, and this is like something that will help us out in a minute. So let's just say that a different framework, Gq, satisfies a stress for GP if and only if the 1d coordinate projections of q are in the kernel of the stress matrix omega. So this is one place where we gain some information, um, about frameworks really, and what configurations they can, by the stresses they have, right? So, so the stresses constrained to some extent what p could be. Um, right. And here, what I mean by this is that if I took, you know, I usually think of vect points as like column vectors. So if I like transpose them and then I put a one, right? So I would end up with here, this is n, this is d plus one, right? And I'll call this matrix q hat. These are just homogeneous coordinates, right? And so then I have this matrix equation.
00:34:13.104 - 00:35:13.274, Speaker A: I don't know how to write like a zero matrix. This is a matrix equation. Okay? And this lets me do something that's going to be a bit of a theme through the rest of this discussion, especially next time, which is I can simply just define what it means for a matrix to be a stress matrix just of a graph. I don't really need this like stress omega of some framework gp. Right. I can simply look at matrices which are symmetric, you know, n by n, they have zeros on the non edges. Right? So what I mean by this is that omega ij equals zero if, you know, ij is not an edge.
00:35:13.274 - 00:35:45.354, Speaker A: Right? And I should also have this all ones vector in the kernel. Right? And so what's motivating this definition is really taking q to equal p in this matrix equation. Yep.
00:35:46.374 - 00:35:56.582, Speaker B: So the matrix q hat, it doesn't have anything to do with the lemma, does it? I mean, it's independent of what's written in the lemma.
00:35:56.758 - 00:35:59.214, Speaker A: I mean, it's a reformulation of the lemma.
00:35:59.294 - 00:36:01.594, Speaker B: Well, except for the last column.
00:36:04.814 - 00:36:29.044, Speaker A: Yeah. Yeah. So, so the matrix q hat, I mean, q is the, the same q. So I'm saying that the 1d coordinate projection should be in it. I put the homogeneous coordinates just because I wanted to connect it to this. So it's a slight tightening. So I know that all stress matrices have all ones in the kernel, just because if you remember how I defined them.
00:36:29.044 - 00:36:45.414, Speaker A: So I defined them that, like I just said, I'm going to take this equilibrium equation and apply it to one dimensional configurations and make it a linear map. And certainly if all the PIs are the same, I get zero on the right hand side.
00:36:49.354 - 00:36:52.374, Speaker B: Yep, that makes sense. I was just wondering about the statement.
00:36:52.674 - 00:37:03.094, Speaker A: Yeah, so the statement is. That's why really, I was a little bit sloppy. So, I mean, I said, that is. But I actually kind of packed in there. The all ones.
00:37:04.514 - 00:37:11.710, Speaker B: And the columns of Q hat, they needn't be independent basis vectors of the kernel.
00:37:11.862 - 00:37:13.594, Speaker A: That's correct. Okay.
00:37:16.294 - 00:37:22.114, Speaker C: Lewis, could you say a little bit more about what you mean by 1d coordinate projections?
00:37:22.454 - 00:37:55.210, Speaker A: Yeah. So these are all in some d dimensional space. Right. So if we're in r three, then there's like, you know, so PI. So you can think of, you know, PI is whatever, xi yi zi. Then I can let I range over one to n, so I could project my three dimensional point configuration onto the x axis, the y axis, the z axis, and I'll get a column vector.
00:37:55.402 - 00:37:56.882, Speaker C: I see, okay.
00:37:57.058 - 00:38:00.694, Speaker A: Right. So then the matrix equation is just kind of.
00:38:00.994 - 00:38:12.922, Speaker C: I see. So that would be the first chord, the first column of q hat would be, for instance, the projection onto the. Onto xi in this example. Okay, got it.
00:38:13.098 - 00:38:26.014, Speaker A: That's right. Yeah. These are good kinds of questions. I'm glad we're talking about them. So, yeah, so that is precisely the idea. Okay. Right.
00:38:26.014 - 00:39:25.354, Speaker A: And so we can start to sort of actually prove some. Some maybe like less obvious things. So, for example, if Gp is a framework in dimension d, and omega is an equilibrium stress of this framework, then, um, the rank of the stress matrix of this equilibrium stress, little omega, is at most n minus d minus one. Right. And so, so the proof of this is simply that matrix equation. Right, so omega, p hat equals zero. But now sort of, I defined, I should say a d dimensional framework, you know, so this has, you know, full affine span.
00:39:25.354 - 00:40:05.854, Speaker A: Right? And so then I have omega, p hat equals zero. The affine span assumption implies that p hat has independent columns, since it must have d plus one independent rows. So this is where I'm really just using homogeneous coordinates, but transposed. Why am I doing it transposed? I don't know. We just do that. It seems to make more sense. Yeah, I'm thinking of the stress matrix as the main actor.
00:40:05.854 - 00:41:00.318, Speaker A: So, another kind of nice lemma is that if little omega is an equilibrium stress of a framework, and the stress has rank n d minus one, in dimension d, t is finally spanning. So now I'm starting to say some words, and q is some other stress satisfier that, in fact, q is an affine image of pull. Right. Um. This shouldn't really be that surprising, right? It's essentially gale duality. Right? And so, so now I'll, I'll actually maybe attempt to argue this. So, notice that the columns of p hat span the kernel of the stress matrix this time.
00:41:00.318 - 00:41:56.162, Speaker A: So that was like the previous lemmas. And all of these hypotheses are getting me to. P hat has d plus one independent columns, right? You know, because P hat has d plus one independent columns. And, you know, omega p hat equals zero. Okay? Now, omega Q hat is also equal to zero. So the column span of Q hat lies inside of the column span of p hat. So therefore, I can find some d plus one by d plus one matrix a, so that p hat times a is equal to q hat, right? And now, I mean, a acts on homogeneous coordinates, but it preserves the all ones vector at the end.
00:41:56.162 - 00:42:38.784, Speaker A: So it's modeling an affine map. Right? And so, maybe, if you want to take away something right, from what we've done so far, it's that satisfying an equilibrium stress if the equilibrium stress has high rank. And also, the original framework had full affine span, is now giving you, like, a lot of control. Right? It's just that there's one up to a affine stress satisfier. Right. And that was maybe something you didn't know before you introduced the stress matrix.
00:42:40.684 - 00:42:47.340, Speaker B: Could, um. Could you say more about gale duality? And. Or is that, too.
00:42:47.372 - 00:42:54.020, Speaker A: Yeah, sure. Okay, so, so is the question of what is Gale duality? Or. I mean, oops. So I have.
00:42:54.132 - 00:42:56.476, Speaker B: Yeah. That and how it applies here.
00:42:56.660 - 00:45:06.948, Speaker A: Yeah, sure. So, so, Gale duality is a linear algebra statement. So Gale duality, right, is just the following kind of statement that, you know, suppose that, you know, a is. I'm going to set it up the way I'm using it. So I'm just going to say that it's n by nice of rank, say n minus r, right? And x is, let's say n by r, such that, you know, ax equals zero and x has independent columns. Then a subset of rows of x is linearly independent if and only if the complementary subset of columns of a is linearly independent, right? So, basically, I am relying upon essentially this statement. So, so, in other words, what I'm saying is that kind of morally, although it didn't, I didn't use it explicitly in my computation, I'll use this idea later, that somehow, because omega had high rank and also p had full affine span, omega is encoding, like, all of the affine relations on pie and vice versa.
00:45:06.948 - 00:45:16.344, Speaker A: For what it's worth, this is a consequence of Gail duality. And so Q may satisfy more, but it has to satisfy at least the same ones.
00:45:20.924 - 00:45:23.384, Speaker B: And the affine relations are on the rows.
00:45:24.324 - 00:45:28.332, Speaker A: Yeah, yeah. On the point set. Right. Because I've written the points as rows.
00:45:28.428 - 00:45:29.704, Speaker B: All right. Okay.
00:45:34.344 - 00:46:15.634, Speaker A: Right. Or alternatively, the linear relations on the columns. Pick your favorite dual statement. Right. But really, like, all I needed was this, was this fact that, you know, since I had, I was spanning the kernel of omega, I can then write the columns of q as linear combinations of columns of p, and all ones will still be there. So I'm going to, just for cleanliness, take away gale duality, although I'll probably bring it back later. For those of you who like matroids, Gale duality is duality for linear matriids.
00:46:15.634 - 00:47:21.654, Speaker A: But that is a nice linear algebra statement. And if you haven't sort of immediately thought of a proof, then I encourage you to spend a few minutes thinking about when it is a fun exercise. It's also a good linear algebra challenge problem if you're teaching linear algebra. Okay, so where does all this go? So there's a theorem, and this is an important theorem of Bob Connolly. So this is like a lecture. So, you know, but this is connally sufficient condition. Okay, so if you have a generic d dimensional framework, right? And here I'm talking about p as being a generic point.
00:47:21.654 - 00:47:39.194, Speaker A: That's the kind of upshot. But then this backs down onto one of these. There's a nice open condition with a stress matrix omega of this magic rank n d minus one. Then this framework is in fact globally rigid.
00:47:44.424 - 00:47:45.872, Speaker B: Can I ask another question?
00:47:46.008 - 00:47:46.764, Speaker A: Yep.
00:47:47.704 - 00:47:52.884, Speaker B: Generic. Here we can take as the algebraic geometry definition generic.
00:47:53.824 - 00:48:07.466, Speaker A: So you can, but I mean, the proof kind of goes through generic two and the algebraically definedness of this whole setup. Yes. So ultimately there's an open subset, and.
00:48:07.490 - 00:48:12.694, Speaker B: Also because we fixed d and g. Yes, that's right.
00:48:14.754 - 00:49:16.414, Speaker A: Or if you were only to fix n, it would still be fine. You could just take all the graphs and avoid their bad set. So the issue is that again, the rank of a generic stress matrix for a fixed framework is again an algebraically defined thing. And so it will hit its maximum, assuming you can. So the idea here, and if you want a detailed proof, I'm actually happy to give one, but I don't want to deal with it too much here. But at the end, if I have some time, maybe I'll give you a very, I'll give a detailed proof. Is that any equivalent framework to the one I started with? And so crucially, this is not necessarily generic equivalent framework because P because GP was generic would also have to satisfy the stress I started with.
00:49:16.414 - 00:50:16.888, Speaker A: And so, once we're at the end of what we talk about, we'll have, like, the technology to prove that statement. Okay? But, like, once you believe this, you're in quite good shape. Right? So I would say that, like, the crux of the proof all happens here. So, once you believe this, whatever q is, it's also like an affine image of pull. But if p is generic enough, and I'll give a condition, then it will turn out that an affine image that preserves all the distances would need to be a congruence. And the reason for that is just to think about how the squared distance works. The squared distance of Pj minus PI, right.
00:50:16.888 - 00:51:23.770, Speaker A: Is equal to pj minus PI interproducted with pj minus PI. Okay? So if you think about how it transforms under an affine, right, you get an equation of the form pj minus PI. Transpose a, transpose a. Whoops. PJ minus PI is equal to pj minus PI. PJ minus PI, right? And now, you would expect that. If you would expect this to be a non trivial, you would expect P to impart a non trivial constraint on what a can be, right.
00:51:23.770 - 00:51:42.240, Speaker A: If a transpose is not the identity. So you'd have, for all I j, an equation like this. Right, and so this is why I'm kind of delaying. A detailed proof is that we'll get kind of bogged down in this aspect of things. Yeah.
00:51:42.352 - 00:51:50.564, Speaker C: Louis, I don't understand. Why is the left side equal to the right side? Is it some assumption on the affine map?
00:51:52.384 - 00:52:00.216, Speaker A: No. Yeah, because, I mean, this is. The left side is actually equal to. I'm imagining that q is like an affine image, or really just a linear image of pull.
00:52:00.270 - 00:52:01.436, Speaker C: Uh huh. Okay.
00:52:01.620 - 00:52:30.264, Speaker A: Right. So then. So basically, then this is equal to actually qj minus qi dot qi. Right, but I. But I'm assuming that these are equal. This is the assumption, right? So I got out of the stress that q had to be related to p in this way. So therefore.
00:52:30.264 - 00:52:51.132, Speaker A: But the lengths, I'm assuming they are equal. Right, right. And now I want to say that, like, well, if p is not somehow quite degenerate, then the only thing I'm going to be able to do is take this linear part a and make it just an orthogonal matrix.
00:52:51.308 - 00:52:52.264, Speaker C: That's right.
00:52:57.424 - 00:53:09.844, Speaker A: Right. And so, and so. But basically, notice that genericity of p came in in a couple of places, right? So q doesn't need to be generic, but p does. If p is not generic. Then all bets are off.
00:53:12.384 - 00:53:19.084, Speaker C: Sorry. So I interrupted the statement at the end then. Is that a transpose a is what?
00:53:19.694 - 00:53:20.502, Speaker A: Identity.
00:53:20.638 - 00:53:23.470, Speaker C: It's the identity, yeah. Okay.
00:53:23.662 - 00:53:33.834, Speaker A: And that gives me. Right, so if a trans. Right, just sort of recall a transpose a equals ad, right. This is just saying, right, that a is orthogonal.
00:53:35.254 - 00:53:35.994, Speaker C: Right.
00:53:38.014 - 00:53:44.742, Speaker A: Right. And so that's how I got my global rigidity. Right. So I wanted to show that q is congruent to pie.
00:53:44.788 - 00:53:53.914, Speaker B: Is there any. When you look at what restrictions this puts, is there any geometric way to understand the non.
00:53:53.954 - 00:54:51.214, Speaker A: Excellent question. Yes, excellent question. I'm about to tell you there is a lot of geometric content here, but I'll give you at least one statement. The remark here is that at this point, either all the generic frameworks for some fixed graph and dimension d have a n minus d minus one rank stress matrix, or none do. And so this is like one half. So then there's a very important theorem. This is the Gortler Healy Thurston theorem, Thurston necessity theorem, which is that if you have a generic d dimensional framework that is globally rigid, then it does have a stress matrix like this.
00:54:51.214 - 00:55:48.698, Speaker A: So conceivably, that could fail. Conceivably, there could be graphs, you know, which are generically globally rigid. And I should say there is a technical condition I have forgotten, d dimensional framework with at least d plus two vertices. Right? So if you're too small in a load. If you're too small, then you need to be a simplex and you'll be globally rigid for reasons not relating to stresses. Okay? And so this gives you a complete characterization about global rigidity in some sense. You can say that a graph is generically globally rigid.
00:55:48.698 - 00:56:07.914, Speaker A: Either all of its generic frameworks are globally rigid, or none of them are. Or equivalently, there exists a is a risky open subset of configurations on which all of them are generically globally rigid. Or none of them are.
00:56:11.134 - 00:56:13.758, Speaker C: Lewis, could you just scroll back to the.
00:56:13.886 - 00:56:17.462, Speaker A: Gortler, Healy, Thurston, it's this theorem here.
00:56:17.558 - 00:56:19.294, Speaker C: Yeah, it's that one.
00:56:19.334 - 00:56:19.994, Speaker A: Perfect.
00:56:26.994 - 00:56:27.774, Speaker C: Thanks.
00:56:28.274 - 00:56:55.480, Speaker A: Okay. And so now notice that all of these examples. So to kind of. Okay, so these graphs are generically globally rigid, and dimensions one and two, respectively. Right. But. But one is.
00:56:55.480 - 00:57:15.088, Speaker A: Is kind of this kind of, um, framework, we decided, was universally rigid. Um. This one is not universally rigid. It is actually globally rigid. Not much you can do to it. I mean, if you perturb it, you know. Right.
00:57:15.088 - 00:57:31.048, Speaker A: This one is also universally rigid. This one is not. Am I still here? Can anybody hear me?
00:57:31.136 - 00:57:32.248, Speaker B: Yes. You are.
00:57:32.416 - 00:57:55.470, Speaker A: Okay, good. All right. Okay. So not universally rigid. Universal rigidity is therefore not a generic property. Right. So I have no hope to say that, like, every graph is either generically universally rigid or not.
00:57:55.470 - 00:58:26.104, Speaker A: Does not make sense. There are euclidean open sets of both types. Okay, so Alex asked about some geometric content. So does anybody mind if I, like, run over by, like, five minutes? Go ahead. Okay. I sincerely apologize for this. So let's say that I have a framework in dimension d, and it's affinely spanning.
00:58:26.104 - 00:59:07.734, Speaker A: We'll say that the edges of it are on iconic at infinity. Bob's terminology. This is Connolly's terminology. If there's a d by d matrix, so that these iJ edge vectors lie in the quadric, you know, the conic at infinity, it's the homogeneous quadric defined by the matrix. So an example is that, like, in dimension two, being on a conic at infinity is like having only two edge directions. Right. The intuition of the definition, right.
00:59:07.734 - 01:00:06.474, Speaker A: Is that if you used homogeneous coordinates, or at least this is how I want to think about it, then these vectors here are at infinity. So, okay. And so basically, to answer Alex's question, this is sort of the geometric content of the failure of the stress criterion to give you global rigidity. Okay, so another definition which will be relevant to universal rigidity is that a framework is super stable. And now I'm going to highlight if it has a positive, semi definite stress matrix, omega. Right. But again of the same maximum rank, and the edges are not on a conic at infinity.
01:00:06.474 - 01:00:23.040, Speaker A: Okay, so this is like a weirdo definition, but it will turn out to be a good one. So a theorem. And this is Connolly, 83 other variants.
01:00:23.112 - 01:00:27.144, Speaker C: Can you scroll back to the super stable definition just for a second?
01:00:27.264 - 01:00:29.364, Speaker A: Yep. I can.
01:00:33.424 - 01:00:35.044, Speaker C: Not on a conic.
01:00:37.024 - 01:00:40.404, Speaker A: Okay, so the conic at infinity thing is where everything goes wrong.
01:00:42.984 - 01:00:44.204, Speaker C: Okay, cool.
01:00:44.824 - 01:01:28.206, Speaker A: Right? And so that's basically giving one answer to Alex's question. What's the geometric content? The geometric content is at a conic, you know, conic at infinity, but you can do better. So. So there's a paper of Connelly, Gortler and Theron from a couple of years ago where we give an even more precise statement. So Connolly's theorem is a super stable framework is universally rigid, no genericity. The only amount of genericity you need is not on a conic. So that's nice.
01:01:28.206 - 01:02:06.654, Speaker A: And so this is Gortler and Thurston sort of reverse, which is that if you are generic and universally rigid at a dimension d, um, then you are super stable. And again, I was sloppy. You should also have at least d plus two vertices, so avoiding tiny. Um, and my remark here is that this, this requires genericity, otherwise you can come up with examples pretty easily.
01:02:08.364 - 01:02:14.092, Speaker B: And this is an, this is a new theorem. This doesn't follow from the previous Gortler Thurston theorem.
01:02:14.148 - 01:02:33.344, Speaker A: That's correct. And it needs like slightly different ideas from convex geometry. And so such a graph is necessarily GGR because it's generic and it has already the right kind of stress matrix. And so by their previous theorem it would be a GGR graph at least.
01:02:35.004 - 01:02:36.508, Speaker B: Can I ask another question?
01:02:36.676 - 01:02:38.196, Speaker A: Yeah, please do. Oops.
01:02:38.260 - 01:03:02.784, Speaker B: Okay, so the positive semi definiteness in super stability, can that be translated into, I mean, suggested by your example that one vertex was within the convex hole of other vertices, can they be translated into some statements on the, on the coordinates of points like that?
01:03:05.004 - 01:03:43.720, Speaker A: Sort of, but it's not going to be so nice. So I'm going to actually describe a situation in which something like that is true, but it's going to be a little bit, it can be a bit more subtle than that. Right. So in some sense, I don't know, a kind of synthetic story. I mean, I know a lot about, for what it's worth, like, I know a huge amount about examples like this and also examples like this. Like there is an issue that this winds. So there's some kind of winding that controls the signature of the stress of wheels.
01:03:43.720 - 01:04:09.444, Speaker A: So I understand wheels pretty well, and Kopovich and Milson did before me, but I was able to kind of replicate the results without hodge theory. So let's just say that I know a lot about wheels in dimension two, which is useful for origami. But in general, I don't have, like, I don't have, and I don't believe there is like a good synthetic description all the time.
01:04:09.944 - 01:04:13.524, Speaker B: Is this stuff about wheels in a paper somewhere?
01:04:13.864 - 01:04:34.132, Speaker A: I have never written it down. I mean, Kopovich and Milson did. I'll send you the reference. So, and I can get the same result kind of using whatever more rigidity theoretic methods. So. So let's see. So, yeah, so where was.
01:04:34.132 - 01:05:27.760, Speaker A: So, okay, so this is where I was basically planning to stop. So I've like wasted six minutes of your time. And so now I was just going to finish by saying where I, what I plan to do next time. So the spoiler. So next time. What I'm going to show is the following theorem, basically is that if g is ggr in dimension d, then there exists a generic pull and such that Gp is super stable. Okay? And to give the big picture of how to do this.
01:05:27.760 - 01:07:48.974, Speaker A: So I will deal with a special case, which is that g contains kg plus one, although the theorem is always true. Okay? And so the idea is that I'm going to describe sort of all the rank n minus d minus one, stress matrices, g, with some other nice conditions that g can have, right? And then I'm going to argue that the PSD ones are big, right? So I'm going to argue that somehow the PSD set is pretty big, and then I'm going to use the ght theorem to show, right, that somehow infinitesimally flexible frameworks can't account for a full dimensional subset. Okay, so there. So I'm embarrassingly ten minutes over. That's where I want to stop. And I can actually take some questions if I'm not keeping you from other meetings. Thanks, Luz.
01:07:48.974 - 01:08:32.374, Speaker A: And so I hope at this point it was at least somewhat clear why I'm going slowly, and also why, like, I started to bring up this like, somewhat detailed viewpoint on algebraic sets. So this is going to be my kind of task for next time, is to describe stress matrices and not frameworks until the very end. Okay. And I think I'm going to show my screen.
01:08:32.874 - 01:08:34.534, Speaker C: Thanks, Louis. This was nice.
01:08:34.874 - 01:08:43.123, Speaker B: Yeah, this was really, really nice. Can I ask a question about what kind of description you're going to give to the stress matrices?
01:08:43.783 - 01:09:08.084, Speaker A: Yeah, I can actually. So it's going to use rubber banded beddings. So I'm going to. And that's why I'm dealing with this KD plus one. So I can solve the general problem. In the general problem, what we do is we use these orthogonal representations, and this is just like a whole machine that I don't want to get into. And so, rubber band embeddings, I can actually give an account.
01:09:08.084 - 01:09:28.944, Speaker A: And so I'll use, you know, I'll use rubber band embeddings with kind of this KD plus one as the, like, outer frame to hold everything, right? So generalizations of touch drawings, right? So essentially I'll describe it as an open subset of an appropriately sized, you know, real space.
