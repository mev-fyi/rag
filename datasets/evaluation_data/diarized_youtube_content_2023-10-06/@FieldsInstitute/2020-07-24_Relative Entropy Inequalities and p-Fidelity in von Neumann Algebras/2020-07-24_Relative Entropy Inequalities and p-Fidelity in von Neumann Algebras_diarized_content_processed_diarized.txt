00:00:04.360 - 00:01:08.244, Speaker A: So, parts of this are joint work with Marius Younge, and I'd like to thank our funding sources and also thank the organizers of the conference. And then I'd like to define relative entropy. So, this is a quantity defined between two states, algebra. And in this case, I'll start by defining what we call the sandwiched Regni relative entropy, which is this DP rho relative to omega. Now, this is given essentially by the logarithm of a particular p norm in the tracial case. And when we say relative entropy without the p parameter, almost always this will refer to the limiting case in which we take p going to one. And in that case, it has this formula shown below in terms of a trace of involving operator logarithms.
00:01:08.244 - 00:01:55.724, Speaker A: Now, you may notice that this quantity is not always finite. In fact, if the support of rho is not contained in the support of omega, then it goes to infinity. And in that case, we will define it by another limit. It's generally okay that sometimes this quantity is finite that actually has some interpretation to it. Now, we can also define relative entropy from Tamita Takasaki modular theory. So, we can take these ket vectors for rho and omega, which are assumed to be cyclic and separating vectors. We can get these, for example, from a GNS construction.
00:01:55.724 - 00:03:13.676, Speaker A: And in that case, there is the tomita Takasaki modular operator s rho omega, which is defined, as shown, and given an operator acting on the Hilbert space vector corresponding to omega, this will sort of flip that operator to its adjoint and also change it to act on rho. Now, from the polar decomposition of s rho omega, we get this delta rho omega, which is known as the relative modular operator. And from this, we can define via an inner product, the relative entropy, without using any tracial or finite dimensional constructions. So this will work, for example, in type three lambda and in type three one. Now, relative entropy is commonly applied in physics. This is a quantity that's studied in quantum information, in condensed matter, and now even in quantum gravity. And so, studying relative entropy, if you can actually get fundamental inequalities or results on this, will tend to have repercussions for many different fields.
00:03:13.676 - 00:04:08.732, Speaker A: It will tend to have many different applications. So, in that regard, this is extremely well motivated. On the other hand, this is also known to be extremely hard. So many quantum information theorists will say that there is essentially one fundamental entropy inequality, and that nearly everything else is basically rewriting that in different ways. And so I'll explain exactly what this is, but first, I need to introduce the concept of a commuting square, or rather recall the concept of a commuting square. So, we take two algebras, s and t such, and we consider their intersection and their union. This will give us three conditional expectations to each of the subalgebras and to their intersection.
00:04:08.732 - 00:05:11.014, Speaker A: And I shouldn't have really said union. I should have said this is really a joint algebra, which is the algebraic closure of their union. We say that these are in commuting square if their conditional expectations commute, in which case the product of the conditional expectations of the two sub algebras is equal to the conditional expectation to their intersection. And in that case, we can represent this by this nice diagram, where any path down these arrows will essentially get you the same result if you end up to the same place. And when a commuting square exists, the relative entropies taken with respect to these subalgebras will obey an inequality known as strong subdivity. And so this is actually a form that was introduced by pets. This is a later form.
00:05:11.014 - 00:06:47.104, Speaker A: The original form of strong subdivity, or SSA, came from Lebenruskai in 1973. And this is essentially comparing the conditional expectations on the joint algebra and on the intersection algebra to the relative entropies with respect to conditional expectations on the subalgebras s and t. And so there's a very nice interpretation or intuition of SSA that actually comes from a branch of physics known as holography. And in holography, they associate entropies with minimal surfaces in sort of a geometric picture. And so you can think of the two subtracted terms, which are relative entropies containing es and et as sort of like minimal surfaces. And in this analogy, then, we would take these two minimal surfaces for s and t, and we could reassign pieces of them to get the union and intersection surfaces, but these would be non minimal. And that non minimality corresponds to the fact that the sum of these two entropies for the union and intersection is going to be at least as large as the sum of the entropies for the two subalgebras taken individually.
00:06:47.104 - 00:08:14.124, Speaker A: But now, one thing we wanted to know was, is there still a corresponding or analogous inequality without the commuting square condition? And the answer first is that if you drop the commuting square condition, then there is at least some state that violates SSA. And so this fundamental entropy inequality really does need this condition. But maybe there are some ways to weaken it, in which case we still get a useful entropy inequality that would be analogous to it. And so sort of the first idea or hint that maybe we could actually do something in this direction is actually a fairly simple lemma regarding chains of conditional expectations. And so here we have a conditional expectation and this completely positive trace preserving map phi, and we have a sum of relative entropies between the state and its conditional expectation of it, and between the state and the channel of that original state. And these will turn out to be larger than or equal to the relative entropy between the original state row and the conditional expectation of the channel of it. So it allows us to sort of combine things.
00:08:14.124 - 00:09:58.772, Speaker A: So if we take a large number n of copies of the sum of relative entropies with respect to, of a state with respect to two conditional expectations, then, and there should be a parentheses here, then this will be larger than or equal to relative entropy of the state with respect to having a long chain of these conditional expectations act on it. And what we would hope and is in certain senses true or known, is that as this chain lengthens, it will eventually converge to the intersection. Now, to make that more quantitative, we had to introduce another idea, which is that of what I call a cascading flatten. And this is a lemma proved for now two states, rho and omega, where omega satisfies this majorization condition. And in the sense of classical probabilities, what this would say is essentially that there's a partial ordering in which omega could be considered flatter than rho. And when that's the case, if we take the relative entropy of Rho with respect to a convex combination of a normalized identity matrix and omega, without increasing that relative entropy, we can replace the omega by a copy of Rho. And so this is easiest to visualize in the commutative case.
00:09:58.772 - 00:10:50.334, Speaker A: And in fact, it will reduce to the commutative case. There's a trick known as pinching, by which we can make these end up in the same basis. And what this does is it applies a procedure that, using transformations which do not increase relative entropy, converts omega into rho, or go in the other direction, uses non decreasing transformations to convert rho into omega. What this yields, ultimately, is an inequality that I call adjusted subjectivity. And this inequality does not need the commuting square condition. It does replace it by a couple of other conditions. First of all, we need a set of subalgebras that have a trivial intersection.
00:10:50.334 - 00:12:13.404, Speaker A: And when they have a trivial intersection, the conditional expectation to that trivial intersection will, in finite dimension, take all states to a multiple of the identity or to a normalized identity and then, given that we recover a sort of dimension dependent version of this subadditivity bound. Now, this involves a couple of constants, and one of them is a dimension. An ongoing effort is actually to try to replace that dimension constant by a constant in terms of the subalgebra index, and hopefully show that the intersection need not be trivial. However, for now, we have this one, and this one actually has some applications, improving modified logarithmic sobelev inequalities. So if you were here two talks ago, you saw a lot on that. These are estimates of relative entropy decay, and so this already has some potential physical consequences. It's also related to a notion called quasi factorization, which looks at systems that are close to being in tensor product with each other, but not quite.
00:12:13.404 - 00:13:16.894, Speaker A: But for now, I'm going to switch gears actually, and talk about a different set of relative entropy inequalities proved with Marius junge that relate channels, fidelities and recovery. And so what are these three things? First, p fidelity is a notion that we'll use here, and this is a measure of similarity between states. This is this function f p of rho and omega. And this was originally introduced by Liang and co authors in 2018. So this is a fairly new construction. It turns out that there are some connections between this relative entropy and typical measures of distance. So a combination of Pinsker's inequality and a result by Audenart and Isert shows that the relative entropy is bounded in terms of norm distances between the two states.
00:13:16.894 - 00:14:27.024, Speaker A: Then there's another inequality, the Fuchs van de Graaff inequality, which shows that the norm distance is also bounded in terms of the one fidelity, which reduces to the usual and older fidelity considered in quantum information. And so fidelity is sort of like a measure of similarity. Relative entropy is sort of like a measure of distance. Then a quantum channel is a completely positive and trace preserving map, at least in the tracial settings. This is a map between pre duals of von Neumann algebras. It is pre dual of von Neumann algebra is essentially a one norm space. It has a dual channel between the algebras themselves, and interpreted in terms of channels, that the structure of additivity is nearly equivalent to something that we call data processing for relative entropy, which says that the relative entropy is non increasing under the application of a channel to both states.
00:14:27.024 - 00:15:15.716, Speaker A: Now, this brings up a question which is what does it mean when this difference is large or small? So if we could say that for a given channel, it doesn't change the relative entropy very much, would this have some kind of interpretation. And it turns out the answer is yes. This is what leads to the idea of recovery maps. So this paper I'm citing is one of the more recent results. But this goes back, actually to pets. Pets introduced a very early recovery map known as the pets map, which is still very useful today. And what the pets map does, it takes sort of as parameters, a state omega and a channel phi, and that has an input state row.
00:15:15.716 - 00:16:21.974, Speaker A: And this map is guaranteed to invert the action of phi on omega. It's not guaranteed necessarily to do it on another state row, but you notice that it has an application of the adjoint channel. So it is going to the right space, and it's using these density matrix multiplications in this sandwiched construction to in some way reweight the debt, the state that it's given. Now. Later came what we call a rotated pets map, which, again, adds, actually more of these sort of sandwiching terms. And this was introduced largely because of its similarity to some constructions in complex interpolation. And quantum information theorists were seeing that complex interpolation often gives you nice results on information theory.
00:16:21.974 - 00:17:53.356, Speaker A: And so adding these extra sort of imaginary rotations made it more in line with the complex interpolation results. And then after this, what these authors introduced in this paper was what we call a universal recovery map, which integrates the rotated hets map over a particular measure. And the big result was what's called universal recovery. And what this says is that for a given state omega, if for all rho, we have the have this relative entropy difference being small, then what the right hand side is saying is that there's a bounded similarity, or really a bounded distance between the original state and its recovered version. And so a small relative entropy difference implies that we can very nearly invert the channel. The relative entropy difference is essentially an upper bound on one measure of dissimilarity, of the attempted inversion of the channel to the original state. So, with Marius, we have generalized this formula to a sort of sub harmonicity for recovery fidelity.
00:17:53.356 - 00:18:45.334, Speaker A: So we define this function fr the recovery of fidelity. This is really just taking the recovery map, putting in the density it's given, and then taking that fidelity with what we consider to be an original state. And so this is a theorem that has some structure that, again, is very reminiscent of complex interpolation on which it's based. And it gives a comparison in terms of these fidelities. Now, this is a generalization of the universal recovery map result. And that follows because the left hand side can actually be rewritten in terms of a relative entropy difference. I won't go into all the technical details, but you can essentially get back a result of this form as a special case of this.
00:18:45.334 - 00:20:42.766, Speaker A: But you can also get some other special cases that we think are interesting. So we can take these Hp spaces, which are the subspaces of a Hilbert space with p normalizable vectors, and from this we get a recovery map result applied to Gns vectors, in essence. And so we can define this particular recovery map, which takes a form in terms of modular operators, and for any value of p, as long as the states are normalizable in the p norm, we'll actually get a p recovery result that bounds the fidelity in terms of the relative entropy difference. And finally, we get some extra results on data processing type inequalities. So first, if we have a channel from l one spaces onto algebras m and n, then there is a data processing inequality for the p fidelity, which says that if you apply a channel to both states, they could only end up looking more similar at the end. Again, this is in line with what we saw with relative entropy and with trace distance, but now we've confirmed it for all of these p fidelities. We also derive another condition on the relative entropy saturation for data processing, which is that when it is saturated, there exists a completely positive l one isometry that maps both omega and rho to their output versions after the channel has acted on them.
00:20:42.766 - 00:21:14.734, Speaker A: And so this is sort of a more mathematical characterization of the saturation condition, whereas the recovery map formulation is sort of a more operational characterization. So, in conclusion, we have found new inequalities for non commuting conditional expectations. We've shown that there are many connections between the p fidelity distance and relative entropy, and we've used these to generalize the universal recovery result. So I'm actually pretty much at the end of my time.
