00:00:00.160 - 00:01:07.714, Speaker A: So the motivation for this talk is to say how can we exploit product structure and polynomial optimization problems? So by exploiting, what does it mean? So it means that we want some sort of computational tractability and like approximation. So because products of different polynomials usually have very high degrees, that's why by standard methods, it slowly becomes computationally intractable very quickly. And we also want provable approximation guarantees. So this is what we set out to hope to achieve. And now let's introduce the exact problem that we are studying. So here we are given d matrices, a one to ad d PSD matrices, and we understudied this following polynomial optimization problem on the domain, which could either be real or complex numbers. So the objective is to optimize the product of these PSD forms.
00:01:07.714 - 00:02:07.334, Speaker A: And here we introduce a normalization factor of the power of one over d, but is equivalent to a polynomial of high degree polynomial optimization problem, and we are optimizing it over the sphere. Okay, so it turns out that this problem is a high degree polynomial optimization problem. It's a degree 2d in n variables. And however, this problem has a compact representation in that it can be represented by this d matrices, a one to ad. So it can be represented in relatively low number of variables, polynomial space. So it turns out that when d is fixed, this problem has a polynomial time solution. But when d is, say, a constant multiple of n, this problem becomes np hard.
00:02:07.334 - 00:03:10.854, Speaker A: So in this talk, we'll be focusing on this problem, but the method of relaxation we introduced can also be generalized products of non negative forms. All right, so what are some applications or some instances in which this problem come up in? So one simple example is Cantorovich's inequality. It comes up in the analysis of gradient descent on quadratic forms. So this is where, given some positive definite matrix q, we're trying to maximize this object, this objective over the sphere. So it's x transpose qx times x transpose q inverse x. So this corresponds to our objective in the case where a one is q and a two is q inverse. And it turns out that this objective, but can tell which is inequality, upper bounds this quantity by the smallest and largest eigenvalues of q.
00:03:10.854 - 00:03:53.088, Speaker A: And this is one instance in which this product comes up. Another instance that comes up is where this a sub I's are rank one matrices. So they're say Vi transpose. So when these are rank one matrices, the objective becomes a product of linear forms. And this objective here is actually very closely related to permanence of PSD matrices. In particular, the PSD matrix, which is the gram matrix formed by these vectors vice. So I'll not go too much into it, but it turns out this objective is a lower bound on the permanent.
00:03:53.088 - 00:04:39.728, Speaker A: It can be used to approximate this permanent. And that's another work of Mien Pablo. The next special case, I guess, is when these AI's are diagnosed. And this has the interpretation of being a portfolio optimization. So, suppose you're given this rate of returns over a time period, and you want to maximize expected profit. This turns out to be sort of a geometric mean, because these rates compound and you want to. So here you can make the substitution y I equals to xi squared, and here you get the, this product of linear form, linear form in this ri.
00:04:39.728 - 00:05:27.472, Speaker A: So it's another special case, and of this objective. And there's a few more applications which I don't have time to go into today. And so, what are the contributions? Right, so, using, so, since this is optimization, it's a polynomial optimization problem. You can use like standard semi definite programming based, like relaxations or approximation algorithms, which is like the called the sum of squares. And this involves SDP with o of m plus d truth d variables or constraints. And the approximation factor that it gives is omega one over n. So since it's a maximization problem, you want the approximation factor.
00:05:27.472 - 00:05:57.754, Speaker A: The closer it is to one, the better. So, one over n is not as good. But for this particular problem, the product of PSD forms, we can show that this relaxation approximation that exploits this compact representation involves solving a much smaller size SDP, and it only has n squared d variables, constraints, and we can actually get a constant factor approximation for this problem.
00:05:59.254 - 00:05:59.654, Speaker B: Yeah.
00:05:59.694 - 00:06:57.584, Speaker A: So, um, and in addition, where we, we show that when d is a constant constant multiple of n, this problem is even empty, hard to approximate. And we also introduce some higher degree relaxations. So, which increases this number of variables or constraints, but it trades off the computation with approximation qualities. So it gets more, it becomes computationally harder, but the approximation also becomes better. And finally, we also show that, okay, so how good is our analysis of this approximation factor? Can this even be better for our SDP relaxation? To show that there are integrated gap instances, we show that our analysis, this constant here that we get is essentially tight. So what is this relaxation? So here, here it is. So here.
00:06:57.584 - 00:08:18.000, Speaker A: So, at first, we have this optimization problem, and there are two ways of looking at this relaxation. The first one is using a rank one relaxation by essentially writing this x aix to be the inner product of a with x x transpose. This is very standard and showing that, okay, so, x x transpose can be relaxed into this big matrix x, which is PSD and has trace one. Right? So it turns out that this is a, this rank one relaxation gives you the following SDP. And there's another way of looking at this, which involves using the AMGM inequality. So the second way is to use AMGM to bound this upper bound, this objective. So this can be upper bounded as the sum, or the arithmetic mean of sum of I one to d of the individual terms.
00:08:18.000 - 00:09:02.644, Speaker A: So, x AI x. All right, so, and it turns out that since we have some freedom to introduce extra variables, and this freedom is given by this alpha I's, so if we introduce this alpha I's, whose product is equals to one, the initial objective wouldn't change. But we are also able to optimize over these alpha is to find the best ones to give us the best in upper bound. And so this optimizing over these alpha is, can also be written as the following SDP. Here it turns out that these two are dual to each other. So two separate ways of looking at this relaxation. And.
00:09:02.644 - 00:10:02.176, Speaker A: Yeah, so the theorem that we have is that, okay, so let x star be the optimum of, say of this SDP here, and we have some constant, this gamma to be some constant, and this phi be some diagramma function. And this constant approximation factor that we get from here depends on the rank of the optimum solution. And it's given by this function over here. So it turns out that. So how does this relate with r? So even if r is really big, this expression is still lower bounded by a constant. But if r gets smaller, if you can show that the r is smaller, there's a low rank solution, then this approximation factor will improve. In particular, when r is one, if there's a rank one solution, it means that the problem is exact and similar.
00:10:02.176 - 00:10:52.970, Speaker A: And here the c of r also becomes one in this expression. So this is what we expect as well. All right, so just to give a short proof sketch, how do we do this? Right, so we start by defining a rounding algorithm. So given this optimum solution, we can produce a feasible solution given the SDP solution, produce a feasible solution by first sampling from the normal distribution and then normalizing the factor. So this is very standard as well. And the interesting thing comes in the analysis. So, in the analysis, we know that the optimum is lower bounded by this expectation, um, expectations over the randomness of the rounding algorithm.
00:10:52.970 - 00:11:37.984, Speaker A: And, and this can also be written in the form, this, this in this equivalent form. But we can also bring the expectation in using Jensen's inequality. So, which lets us analyze each term one by one. So, after some more calculations and a few more tricks, we can we obtain this, um, this constant factor that we should, that I described, like in the previous slide. And this is just a proof sketch. And so next. So this analysis is actually quite nice, and this leads to another application or interpretation of this result.
00:11:37.984 - 00:12:07.022, Speaker A: So here. So it's the. So it's called the convex hall of the image of a quadratic map. So, first we define a quadratic map, phi, which is a map that maps any x in kn to another vector in. Sorry, this, I think this should actually be r of d. I'm sorry, this is r of d. And this is done by these d quadratic forms that were provided with.
00:12:07.022 - 00:12:59.404, Speaker A: And the natural question is to ask, when is this image of the quadratic map convex? The image of the kn under this quadratic map, when is this convex? So it turns out that if you have only two quadratic forms, so the output is a two dimensional vector. This is true. This is related to the s lemma. And it's related, yeah, and rated lemma for the, like a complex case. But it's not true. In general, it's not always true that this is, this image is complex. So the next question, natural question is asked, well, how far is this image from its convex hall? There's many different ways of measuring this distance, but one way is to measure this relative entropy distance.
00:12:59.404 - 00:13:55.666, Speaker A: What's the relative entropy distance between? So it turns out that also this image, and it's convex, how? They are all cones. It's quite easy to see that they are both cones. So we take these cones and intersect them with the simplex. And from this intersection, you get some compact set of which you can measure the distance between them. It turns out that this rounding method that we have above actually gives us a way to upper bound the relative entropy distance. And it shows that, well, this relative entropy distance, upper bounded by the log of the same constant we have before, from before. And this result actually is proven by Bowery not for a larger constant, but our analysis, as well as the integrity gap that we executed, show that this is essentially asymptotically optimal.
00:13:55.666 - 00:15:15.594, Speaker A: So this constant over here that we get. So there's another way of interpreting the results. And so the next part of the talk, we talk about some higher order relaxations, like can we somehow trade off between computation and relaxation quality? So this, as we talked before, this relaxation can be interpreted as using the AMG inequality to find the best upper bound. But can we get a upper bound, a better bound with something better than the AMG inequality? So this exists, and first we start by defining this elementary symmetric polynomials, and this, and also Maclaurin's inequality, which is a kind of a generalization of AMG inequality. So here is the left hand side is the geometric mean, and there's a whole bunch of intermediate inequalities which involves the elementary symmetric polynomials. And at the right hand side you get an arithmetic mean. So we can, instead of using this EMGM inequality in our relaxation, we can use this intermediate inequality to construct these intermediate relaxations for this k between one and d.
00:15:15.594 - 00:15:59.594, Speaker A: So what we had before this octopus, we can call it opt of so's one. And we have, from using this strategy, we get of so's k for k from one to d. So what this does is of course, it trades off computation for accuracy. So next, end with an example. This example is called the equestriedrophone. So this is the picture you saw in the first slide. So, this is a polynomial in three variables in and of degree twelve.
00:15:59.594 - 00:17:11.814, Speaker A: And it also can be written as this product of PSD forms, because it's a square, it's a product of squares of linear forms. So what does this polynomial look like? It looks like this, it's a polynomial on the sphere. So on the sphere, it looks like this has its optimum on the vertices of the aquasahedron. And it turns out to be a very hard instance because of the high degree of symmetry. So in particular, if you want to just use our SDP based relaxation on this, the resulting rounding algorithm just tells you to sample a random point on the sphere. Because of the symmetry, it doesn't get any special information from just the SDP relaxation. But another question is, with this higher order relaxation, can we do better from this intermediate relaxations? We can also implement randomized rounding algorithms to obtain some feasible solutions from the relaxations.
00:17:11.814 - 00:17:56.694, Speaker A: So this randomized rounding algorithm tries to take in the solution to these intermediate relaxations and produce a feasible point. Okay, so how does this perform? So I show some numerical results. So, remember what this polymer looks like, and here's, here is like a projection of it onto the plane. So the contour line shows the optimal values of this polynomial. And you can see there's about twelve optimum points. And this is of course the equilibria plot. Okay, so, and the dots show the sample points from the rounding algorithm.
00:17:56.694 - 00:18:30.844, Speaker A: Here is when k goes to two. So you can see that when kx two running algorithm doesn't seem to be producing as much right. It seems to be just sampling random points on the sphere. What happens when we increase k? When you go to k equals to three? Rounding algorithm seems to be, the distribution from this rounding algorithm seems to be like, concentrated more towards this actual optimum. And this is true. When we increase k even more, k equals to six. And you see that.
00:18:30.844 - 00:19:35.622, Speaker A: And here's the comparison between, say, k two to six. You see, of course, the distribution concentrates towards the optimum as k increases. All right, so what are the takeaways of this talk? So, this product of PSD forms has really nice structure where in generalizes many problems. And using this nice product structure, we can write computationally efficient relaxations, and also we can prove good approximation guarantees for these relaxations. Of course, some interesting things to think about is, since our approximation factor depends on the rank, if you can show that in some instances, because of symmetry, you can guarantee a low rank solution, then it can also guarantee, like, a better approximation factor. Also, like, can we also generate these, like, intermediate relaxations for other, like, high degree optimization, polymer optimization problems? That's another thing to think about. All right, so that's all I have.
00:19:35.622 - 00:19:37.594, Speaker A: So any, any questions?
00:19:42.014 - 00:19:44.794, Speaker C: Thank you. Are there any questions or comments?
00:19:46.954 - 00:19:59.642, Speaker B: Hi, Chen yanga. I have a question. So you're, you're like, one way to generate your relaxation is with these, like, AMGM inequality, right?
00:19:59.818 - 00:20:00.570, Speaker A: Yeah.
00:20:00.722 - 00:20:14.400, Speaker B: And this reminds me of the, like a sage relaxations, like from Venkat or the songs relaxations. Like, I was wondering if there is a connection between your relaxation and sage. Yeah, yeah.
00:20:14.432 - 00:20:45.704, Speaker A: So that's an interesting question, because I think. Yeah, I've looked into it, but it doesn't appear there's like a immediate, like, connection. But, and the nice thing about this AMGM, like, it allows us to, like, really proof, like approximation guarantee. And that's, I think that's the main reason why we use this.
00:20:46.084 - 00:20:46.904, Speaker B: Yeah.
00:20:48.484 - 00:20:48.988, Speaker A: Okay.
00:20:49.036 - 00:20:50.108, Speaker B: Okay. Thank you.
00:20:50.276 - 00:20:51.024, Speaker A: Yeah.
00:20:58.124 - 00:21:02.984, Speaker C: So I. What do you mean by low rank guarantees of solution from symmetry?
00:21:03.284 - 00:21:36.782, Speaker A: Yeah, so I have a very concrete example, actually. So, so it's actually in the other, in this paper where. So just say, for example, where this. So where the a sub is. So in the permanent case, for example, if the, if this matrix m is circulant, then you can guarantee, like, that the solution is. Is of a low rank, like maybe rank one or two. So this is actually written in this paper.
00:21:36.782 - 00:21:43.630, Speaker A: So that's one particular example. So when this matrix is circulant, then.
00:21:43.702 - 00:21:57.142, Speaker C: Okay, but the kind of result you're hoping for is what? Something about the symmetry group. And based on the symmetry group, you would then guarantee a low rank solution of some sort. Something like this?
00:21:57.238 - 00:21:57.558, Speaker A: Yeah.
00:21:57.606 - 00:21:58.234, Speaker B: Yeah.
00:21:58.554 - 00:22:01.294, Speaker C: That's the kind of result you're looking for?
00:22:01.874 - 00:22:09.034, Speaker A: Yes. In particular, in the second case, this ace sub I's are simultaneously diagonalizable.
