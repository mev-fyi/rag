00:00:00.960 - 00:00:13.874, Speaker A: Okay, perfect. Hello, everyone. Welcome. And our first speaker today is Professor Stefan Garcia, who continue his mini course.
00:00:14.574 - 00:00:59.382, Speaker B: Please. Thanks, everyone, for joining us for this final lecture in this series of three lectures on model spaces. So, the first topic of the day is Alexandrov Clark theory, which is some of the most interesting and deepest material in the theory of model spaces. But I first wanted to go over a couple things, a few definitions from the last couple lectures. The model space script Ku is the orthogonal complement of the subspace, uh, two in the Hardy space, h two, where u is an inner function. Another important thing that we saw was the compressed shift operator to a model space. So that is the operator defined in this fashion.
00:00:59.382 - 00:01:50.400, Speaker B: So we'll usually drop the u there, and we'll just say azaz of f is the orthogonal projection of zf onto the model space Ku. So the idea is f starts in the model space, you multiply by z. Possibly that takes you out of the model space, and you project back. So that is the compression of the unilateral shift to script ku. The reproducing kernel for script ku is given by this function, which you've probably seen not only in my talk, but in several other talks where kernels that look like this appear in many, many different instances. So, another important thing was the conjugate linear operator defined in terms of boundary functions as follows. We say that f in the model space gets mapped to cf, which is defined by f bar z bar u.
00:01:50.400 - 00:02:58.504, Speaker B: And again, the surprising thing is, although that doesn't look like the boundary function of an analytic function, if f belongs to script ku, magically, that boundary function is the boundary function of a function in the model space. And lastly, we saw, and this is the ahearn Clark material from lecture two that for certain points on the boundary circle, the reproducing kernel k zeta, where you plug in zeta for lambda, actually belongs to the model space. Now, that isn't always going to be true, so it's only true for certain zeta depending on the inner function u. But if that happens, the kernel function for the boundary point zeta happens to be almost self conjugate. A multiple of k zeta will be a self conjugate function. So self conjugate with respect to this conjugation, c. So I want to remind everyone that at the end of the talk, updated slides for the first couple lectures, and the third lecture will be posted in the chat, along with a revised version of the book chapter that is associated to these series of lectures.
00:02:58.504 - 00:03:38.792, Speaker B: I've made some small modifications based on errors that have come up during the talks. So I want to start with a finite dimensional example. As always, let u be a finite Blaschke product of degree n, so n zeros, and we'll assume for convenience that it vanishes at the origin. That makes some of the computations a little bit simpler. So the compressed shift, as we saw, is this operator where you multiply by z and project back into the model space. So it turns out that the compressed shift is almost unitary in the sense that it barely misses. I equals a star a, and I equals a a star by a rank one operator.
00:03:38.792 - 00:04:14.770, Speaker B: So these numbers here are one. If a were a unitary operator, those would both be zero, of course. So we say that a is almost unitary, and it can be made into a unitary operator by adding a rank one perturbation. So by adding a rank one operator to az, we can actually get a unitary operator. And this is how that works. I first need this little notation. So if I start with f and g in the mod, sorry, g and h in the model space ku, I define the rank one operator g tensor h, as follows.
00:04:14.770 - 00:04:47.414, Speaker B: I say the g tensor h applied to f is f inner product with h times g. So it has rank one, because the only thing you can get out of the right hand side is a multiple of g. So the dimension of the range is one. So that's a shorthand for a rank one operator. And so what we can do is we can repair the almost unitary compressed shift by adding a rank one operator here. And in fact we can add a variety of rank one operators there. We have a whole one parameter family of possibilities.
00:04:47.414 - 00:05:23.610, Speaker B: For each alpha in the unit circle t. We can multiply this rank one here by alpha, and it turns out we will get a unitary operator. So this u alpha is now unitary. We've repaired that rank one defect, and it turns out to be c symmetric in the sense that it satisfies this equation here. And it also, because we're dealing with a unitary operator on an n dimensional space, linear algebra tells us we can diagonalize it, the spectral theorem in finite dimensions. And it turns out you can identify the eigenvalues and the eigenvectors. Very concretely.
00:05:23.610 - 00:06:08.360, Speaker B: The eigenvectors are these vectors here, which I'm calling e sub zeta of something. But more importantly, the eigenvalues, well, those just come from the level set of alpha. You take a look at the points that u maps to alpha, and you label them zeta one, zeta two up to zeta n. Or in other words, you take the inverse image of alpha under the n fold blaschka product u, and you get some points on the boundary circle. Now, for those zeta, you take a look at the boundary kernels here. So those are boundary kernels, because even though I've got an alpha here, remember, u maps the zetas to alpha. So this is, this alpha here is just really u applied to zeta j.
00:06:08.360 - 00:07:03.402, Speaker B: So this is a boundary kernel corresponding to the boundary point zeta j, and then times some constant, so that constant is there so that we get a self conjugate boundary kernel. So we modify the boundary kernel by this unimodular constant, and that's to get the phase right. And we divide by this thing to get the norm to be equal to one. So what we have here are the eigenvectors for the so called Clark operator, and the eigenvalues, their level sets, or they are the points that get mapped to alpha under the inner function u. And the eigenvectors are certain boundary kernels, which can be normalized to be c real, so each of them is fixed by c. Now we want to sort of move back into analysis world from linear algebra world. And the spectral theorem tells us that, okay, I've got a unitary operator, turns out to be cyclic.
00:07:03.402 - 00:07:48.174, Speaker B: Why cyclic? Because, well, the eigenvalues are distinct here, so it's going to be a cyclic unitary operator. So it has to be unitarily equivalent to a multiplication operator on a lebaic space. And it turns out you can identify the, it is the big space corresponding to this mean of point masses. You put one point mass at each of the zetas, and you divide by n to get a probability measure. And you have that, this Clark operator, this unitary operator that we got from the compressed shift, is unitarily equivalent to multiplication by z on a very, very specific space. We've got a probability measure defining this lebeic space, and it's a singular probability measure. It's singular with respect to the big measure.
00:07:48.174 - 00:08:30.470, Speaker B: So we want to generalize these ideas, and we first start with a result of Herglotz. Notice that if mu is a probability measure on t, and you take a look at this herglot's transform, you get an analytic function of the variable z, and also f of zero is equal to one. Because mu is a probability measure, it has total mass one. When you take the real part of this formula, you get a Poisson kernel here. And so the Poisson kernel is non negative. So what happens is, in fact, it's strictly positive once you fix z. What you have here is that the real part of this analytic function of z is greater than zero.
00:08:30.470 - 00:09:09.204, Speaker B: Now, the Herglatz theorem, which I'm going to post here, says that the converse holds. If you have an analytic function on the disk and you've normalized it. So at the origin, it has a value one, and its real part is positive on d, then it must have come via a probability measure. Through this formula, there's a probability measure mu on the circle, so that capital f is capital f, sub mu as defined here. So that's her glatz's theorem. And so what we're going to try and do is generalize that finite Blaschke product result to general inner function. So we're going to let u be an inner function that vanishes at the origin.
00:09:09.204 - 00:09:39.224, Speaker B: You might wonder why it vanished at the origin. Well, it makes that level set argument a little bit better. Otherwise, you have a Mobius transformation that shows up, and you're not looking at the level set of alpha, you're looking at something else. So that's why we normalize, so that this works out. Now, for each point alpha on the unit circle t, you take a look at this Mobius transformation of U. You basically plug u, the inner function, into a linear fractional transformation. And it's cooked up in such a way so that this works.
00:09:39.224 - 00:10:09.004, Speaker B: Capital f of zero is equal to one. And that's because u vanishes at the origin. And we take a look at the real part. Well, you get this Poisson kernel looking thing. It's not the Poisson kernel. It's got a u in there instead, but it's strictly positive. And what you do is say, ah, Herglatz's theorem tells me that this analytic function capital f, since it has value one at the origin and is strictly positive on the disk, has to come from probability measure on the circle.
00:10:09.004 - 00:10:52.084, Speaker B: So that gives us a probability measure sigma alpha. It depends on this parameter alpha that we picked. So we get this probability measure sigma alpha on the circle, so that my analytic function capital f comes as this herglass type integral from that probability measure. And so this family of probability measures parameterized by alpha, that's called the Clark measures corresponding to u. Now, you can do this not just for inner functions u. You can do this for functions of norm one in the in h infinity. And then you can get into Alexandrov Clark measures, which are a bit more general.
00:10:52.084 - 00:11:36.404, Speaker B: The theory becomes a little bit more complicated, but connects to composition operators and a bunch of other topics that we don't have time to go into. So I'm going to focus on the case where u is an inner function, because some special things happen there. So, I want to get to the fundamental result on Clark measures. This is the Alexandrov Clark theory, where you are focusing on inner functions as opposed to just arbitrary functions in the unit ball of h infinity. I need this notion of a carrier for a measure. So, a carrier for sigma sub alpha is going to be a Borel set c on the circle, because the measure lives on the circle. So that, well, sigma of a intersect, c is equal to sigma of a for every Borel set a.
00:11:36.404 - 00:12:21.834, Speaker B: So, in other words, the measure is determined entirely by what's going on in the carrier capital c. Now, carriers don't have to be unique, and they don't actually even have to be equal to the support of the measure, because you could take a look at the support of the measure, and then you can add on some extra stuff that doesn't really matter, right? And you'll still have this result. So you can tack on some extra stuff to the support, and you can still get a carrier that way. And it turns out we need to talk about carriers for this following theorem, which gives us the basic problem properties of these Clark measures. So, we start with an inner function that vanishes at the origin. Clark measures turn out to be singular with respect to normalized Lebaic measure. M is Lebaig measure here.
00:12:21.834 - 00:13:06.610, Speaker B: So the Clark measures are singular. We saw that in the finite Blaschke product case, where we had a singular measure, it was just n point masses averaged. The other important thing is that these measures are mutually singular. If alpha and beta are points on the unit circle, you take a look at those Clark measures that you got from the Herglatz theory. And if alpha and beta are different, then those two measures are singular with respect to the Baigue measure and singular with respect to each other. Now, there's the question of where do these measures live? And it turns out they kind of live on where you on the level set of alpha. In other words, you look at the points on the circle that the inner function u sends to alpha, at least in this sense.
00:13:06.610 - 00:13:40.806, Speaker B: And I think you can upgrade this to non tangential, but I have it as radial here. So you look at the point zeta so that u of zeta is equal to alpha in the non tangential or radial limiting sense. So that's where sigma alpha lives. So this is generalizing this level set idea, or the inverse image idea that we saw for finite blatch key products. And, in fact, we can characterize where the point masses of the Clark measure sigma alpha are. You've got a point mass at zeta on the unit circle, if and only if. Well, u of zeta is equal to alpha.
00:13:40.806 - 00:14:27.340, Speaker B: I believe someone is unmuted out there in crinkling paper, so I'm hoping they can mute themselves. U of zeta is equal to alpha, and u has a angular derivative in the sense of carata Dory at zeta. So in other words, that allows us to talk about u prime at zeta. And it turns out, in that case, you've got a point mass at zeta, and the mass assigned to the point zeta is one over the absolute value of the derivative of u at zeta. So these are the basic properties of these so called Clark measures. Again, you can upgrade this and go to the full Alexandrov Clark theory. It's a little bit more complicated if you're just looking at u in the unit ball of h infinity, you don't necessarily get singular with respect to the big measure, and things become a lot more interesting.
00:14:27.340 - 00:15:07.074, Speaker B: But we're focusing on this case. So let's go back to the Clark operators. If I've got an inner function u that vanishes at the origin, the compressed shift is a c symmetric contraction. So c symmetric, meaning it's compatible with the conjugation on the model space in this sense. Now, we saw that for each alpha, the Clark operator is a rank one unitary perturbation of a, the compressed shift, you modify it by this rank one operator, or this family of rank one operators, you get a family of unitary operators. Now, I'm going to note here that k sub zero, that's the reproducing kernel for the. .0
00:15:07.074 - 00:15:29.624, Speaker B: since the inner function vanishes at the origin, that turns out to be the constant function one. The conjugate of the function one turns out to be u divided by z. So we're going to need these functions at some point. Just keep those in mind. These are not sort of weird things. This is one, and this is u divided by z in this rank one operator here. So the big theorem of Clark is this.
00:15:29.624 - 00:15:55.258, Speaker B: You start with an inner function that vanishes at the origin. You get the corresponding family of Clark measures parameterized by alpha on the unit circle. Then, as we saw in the finite Blaschke product case, you get a cyclic unitary operator. Moreover, you can figure out what a cyclic vector is. It's the constant function one. Now you've got a cyclic unitary operator. Spectral theorem says, I can write that as a multiplication operator.
00:15:55.258 - 00:16:25.604, Speaker B: There's got to be a spectral measure behind this, and the spectral measure is, as one might hope, sigma sub alpha. And a carrier for that measure is essentially the level set of alpha. So it works out for every alpha, you get this rank one unitary perturbation. It's cyclic. You know what a cyclic vector is? It's one. And you know what its spectral measure is. You can talk about what's the point spectrum of a Clark operator? Well, the point spectrum is exactly what I said on the previous slide.
00:16:25.604 - 00:17:07.780, Speaker B: It's where the Clark operator has an atom, right? So it's essentially, you look for places where you have an angular derivative in the sense of caroteodori. Importantly, eigenvectors are boundary kernels for the point zeta. So you look, your point spectrum is the zeta on the unit circle so that you have an angular derivative in the sense of carata dory. The corresponding eigenvector is the corresponding boundary kernel for zeta. Pretty cool. Now we can talk about the map which does this conversion from this Clark operator to a multiplication operator. And it turns out that this transform, v sub alpha.
00:17:07.780 - 00:18:05.036, Speaker B: So that turns out to be a unitary operator from l two of the Clark measure onto the model space ku. So it's a natural map that moves l two of that Clark measure to the model space. Now, of course, these are going to be Hilbert spaces of the same dimensions, so obviously they're isomorphic, but we have a unitary operator that implements it. That's quite natural, because you can see here, this is almost like a boundary kernel type here, except we move part of it here, the part that doesn't depend on zeta outside. Anyone say, well, why is this legal? Well, remember, sigma only cares about certain zeta, right? Sigma is singular with respect to Lebeg measure. So what happens here, this is only focusing on the zeta that really matter as far as sigma is concerned. And for those zeta, what happens is that u actually has the value alpha at those zetas.
00:18:05.036 - 00:18:44.442, Speaker B: So, you know, when I combine these parts here, that's kind of like we've got a boundary kernel in the integral. And the point of this transform here, this unitary operator, is the one that implements the spectral theorem. So the Clark operator u is unitarily equivalent to a multiplication operator, which I've denoted z alpha. It's the multiplication operator on the corresponding Lebeg space. And I'm using zeta here instead of z because I'm operating on the unit circle. This is the operator on this Lebeic space l two of sigma alpha, which sends f to zeta times f. What we have here is a rarity.
00:18:44.442 - 00:19:25.876, Speaker B: We have an explicit spectral representation for an operator that originally lived on the model space. This originally lived on the model space, but the spectral theorem for normal operators tells us that I've got a cyclic operator here, so it has to be representable in this form. Usually you can't do this so concretely, but that's the beauty of this Clark Alexandrov theory, that you can do this all very, very explicitly. And again, this becomes much more interesting, much more complicated, once you move away from inner functions. But we're going to stick with inner functions here because that's what we're looking at. We're looking at model spaces, not sort of the more general theory. I'm just giving you a taste of the Alexander F.
00:19:25.876 - 00:19:55.362, Speaker B: Clark theory. So we'll start our example with a very, very simple case. We're going to start with z cubed. Then the model space corresponding to z cubed is the span of one z and z squared. So I've got a three dimensional model space. In order to set up what we saw in the previous slide, I've got to look at a couple things. I know that the constant function one is going to be my cyclic vector, and I'm going to need the conjugate of the function one.
00:19:55.362 - 00:20:40.184, Speaker B: So I look at 1 bar z bar u on the circle, and so that works out to be z squared. So one and z squared are conjugate functions in this model space. So I'm also, since I want to do some matrix computations here, I want to compute the compressed shift applied to these very, very nice basis vectors. So the compressed shift applied to one is take one multiply by z and project into the model space. Well, I don't need to project because z is already there. Same thing with when I apply the compressed shift to z, I multiply by z, I get z squared and project into the model space, but I don't have to because z squared is already there. Now, when I look at z squared, I multiply by z and I get z cubed and project into the model space.
00:20:40.184 - 00:21:09.456, Speaker B: Now, z cubed is orthogonal to the model space here. And in fact, the model space is, is the orthogonal complement of z cubed h two. So when I project, I get zero. So that'll help me build some matrices later on. Now, for alpha and t, I've got this parameterized family of unitary operators, U sub alpha. These are the Clark operators. And what happens here, we do a computation.
00:21:09.456 - 00:21:55.366, Speaker B: We've got this rank one operator here. We know what a does to the standard basis vectors, and this really only cares about what's going on with z squared. So this rank one operator is going to let z and is going to let one and z pass through it. And what it's going to do, it's going to take z squared and map it to a multiple of the constant function one. So the difference here is, well, we have changed the compressed shift by a rank one operator, and that rank one operator has chosen to fix this mistake here, because sending z squared to zero means that the compression isn't unitary. Right, and we're patching this up by replacing that zero with alpha. And that's what this Clark operator is doing.
00:21:55.366 - 00:22:47.936, Speaker B: So now we've got a unitary operator. And if you take a look at the matrix representations of the compressed shift and the Clark operator with respect to the natural orthonormal basis of our model space, namely the basis beta equals one z and z squared, we've got the compressed shift, which you can see is not unitary. And we can patch it up to make it unitary by putting that alpha up there. And so you can see the choice of what we put up here. There's a degree of freedom. Alpha can be any point on the unit circle, and this will still give me a unitary matrix. Now, turns out that the eigenvalues of the Clark operator, and in fact, you can compute the eigenvalues by the old fashioned linear algebra way here, the eigenvalues are going to be the solutions to z cubed equals alpha, because they're going to be the level set of alpha corresponding to this function u.
00:22:47.936 - 00:23:12.528, Speaker B: So you basically figure out what u is sending to alpha. In this case, it's the cube roots of alpha, and we're going to call them omega one, two and three. The corresponding eigenvectors are going to be the boundary kernels for omega one, two and three. Now, I called them Zeta in a previous slide. I'm sorry about that. Now, that's the boundary kernel. But it turns out I've got a polynomial divided by another polynomial.
00:23:12.528 - 00:23:57.054, Speaker B: And if I do this division, I can actually write it out more explicitly in this way. And the reason I do that is because we've realized this particular model space as the space of polynomials of degree two and less. So, this boundary kernel is indeed, despite its appearance, a polynomial of degree two. And these can be normalized to give us a c real orthonormal basis for the model space. So our probability measure, or our Clark measure is going to be this. It's going to be a point mass at each of those eigenvalues so those eigenvalues come from the cube roots of alpha, and so you basically take their average. So I get a probability measure, and it turns out to satisfy this.
00:23:57.054 - 00:24:38.548, Speaker B: Now, this requires a little bit of a computation, but it's something that would be good for the students to try and work through. When I plug in this singular measure here, what it's basically going to do is it's only going to care about zeta when it equals omega one, omega two, and omega three. And if you do the algebra here, this actually works out. The unitary operator from the Clark theorem is this. We basically have this boundary kernel type expression here, partly inside and partly outside. And you can take a look at what happens if I plug in an arbitrary function in l two of sigma alpha. Remember, sigma alpha is a singular probability measure.
00:24:38.548 - 00:25:09.882, Speaker B: Where does it live? It lives on only three points, omega one, omega two, and omega three. Those are the cube roots of alpha. Those are the points that you sends to alpha. So in this labaic space, it's a labaic space on only three points, right. So I'm going to write a function in this as a sum of three characteristic functions. The characteristic function of the first point, characteristic function of the second point, and the third point. I have a big space that only has three atoms, so it only consists of three points.
00:25:09.882 - 00:26:03.456, Speaker B: So what happens when I plug f into this formula again, realizing that despite all this fancy language, sigma is only living on three points? What happens is this transform of f becomes this. This term stays out in the front. And then what I basically have is, because I've got these one three attached to my atom, I get these one three, and I'm basically selecting those zeta. Well, I'm selecting three specific values of zeta, omega one, omega two, and omega three. So I get this thing, and that's supposedly, by that Clark theorem, an element of the model space. And in fact, it is, because it turns out each of these denominators is a factor of this expression out front. So when I cancel things, I cancel this denominator here, cancel this denominator here, et cetera.
00:26:03.456 - 00:26:48.202, Speaker B: I get a polynomial of degree two, and all of that magically works because the omegas are cube roots of alpha. So this is the Clark theory in a very, very simple finite dimensional case. But I think it's sort of instructive to do this computation, because at least we can get our hands dirty. Everything's very concrete, and it will help make the more general theory make some more sense. So I want to tell you a little bit about some more deeper results attached to this theory. So let's recall that this map sends the maps, the l two space for the Clark measure sigma alpha, onto the model space. This is the transformation that we've been looking before.
00:26:48.202 - 00:28:09.522, Speaker B: This is the unitary operator that implements the spectral representation of the Clark operators. So since v sub alpha of f belongs to the model space for every f in this labeague space, well, it's going to have non tangential limits almost everywhere because it lives in the hardy space, right? But we can say a little bit more because functions in the model space behave a little bit better than functions in h two. And so let's recall, there's an interesting result. I can't remember who it's due to, but it's a classical result that for any closed subset of the unit circle of measure zero, you can find a hardy space function, which can in fact be taken to be an inner function that fails to have non tangential limits anywhere on e. So if I have a closed set of measure zero, I can have a bad hardy space function on that set e, I just won't have non tangential limits there. Because hardy space functions, you have non tangential limits almost everywhere, but that almost everywhere can change depending on the function. And so there's this nice result of Polteratsky that says that if you pick any Clark operator in any Clark measure, rather, and you have a function in that corresponding l two space, then when you take a look at this expression here.
00:28:09.522 - 00:29:12.534, Speaker B: So v applied to f, that belongs to the model space, right? Because that was the whole point of this v operator. It maps capital l two of sigma alpha to the model space. Ku. So if I take a look at the radial limit here, and pretty sure this will work in the non tangential sense, it actually gets mapped to the correct thing for almost every zeta with respect to sigma alpha. So what we're dealing here with is very, very fine properties of these functions in model spaces, where we can be very specific on a very, very, very thin set of measure zero. We can talk quite a bit about how the boundary values of functions in those model spaces are working, because, remember, v sub alpha of f is a typical function in the model space, because v sub alpha is a unitary operator from l two of the Clark measure to the model space. So this tells us a lot about the fine structure of boundary values in model spaces.
00:29:12.534 - 00:29:52.406, Speaker B: Now, this is a really, really beautiful result of Alexandrov, also connected to these Clark measures. And again, a lot of this work was done by Alexandrov building off the general inner function case of Clark from back in the 1970s. The real sort of triumph of this theory is when you say, ah, a lot of this will carry over to you a function in the unit ball of h infinity. But I'm not going to go into it. But I certainly don't want to short change a lot of the work that has gone on there, because I'm only focusing on the inner function case here. And there's a really interesting result here. It's called the disintegration theorem.
00:29:52.406 - 00:30:38.478, Speaker B: It says that got an inner function, you know, probably, maybe I want u of zero equal to zero. But, you know, you know, I, let's ignore that for the moment. For f in capital l one, the usual l one, it turns out that the map that sends alpha to the integral of f with respect to the alpha, Clark measure is actually defined almost everywhere, the big almost everywhere. So for almost every t. So for almost every alpha, that map is actually well defined. It's not obvious that that should be well defined for a typical f in l one, right? I'm not saying that this f lives in some special l two space with respect to some measure. I'm saying that this just lives in l one.
00:30:38.478 - 00:31:11.054, Speaker B: And it turns out, once you fix your f, that map actually makes sense, and it is Lebeg integrable. So that's remarkable. And moreover, these Clark measures form a disintegration of the Baig measure in the following sense. If I compute these integrals here, which exist almost everywhere according to what we said here. So, for almost every alpha, this makes sense. Now, these are singular measures with respect to labague measure. But now, when I integrate all of this stuff, I actually just get the integral of f with respect to labague measure.
00:31:11.054 - 00:31:45.124, Speaker B: So, in other words, the big measure has been disintegrated, decomposed into this family of mutually singular measures, the sigma alpha. So the big measure has been taken apart and dissected into these Clark measures. It's a really remarkable thing. Here's a quick proof of the really easy case. This is a very difficult theorem, but you can prove it for special functions with a quick argument. So I'm going to balance things out and just show you this. For continuous functions, f, it's actually a much harder result in general.
00:31:45.124 - 00:32:31.124, Speaker B: Now, take the real part in the definition of sigma Alpha. And what are we going to do? We obtain this. So let p sub Z be a Poisson kernel, right? We have a Poisson kernel here. And so what happens when I compute this expression? I get something like this turns out it's a little bit of a computation, but I'm just going to look at what happens to Poisson kernels first. And I get this. Now, it turns out, well, that's basically a Poisson kernel looking thing anyway, so that's turns out to be one. And so one also happens to be the integral of the Poisson kernel for the point Z.
00:32:31.124 - 00:32:52.224, Speaker B: And so what happens? The theorem works for a Poisson kernel. That's the point here. The theorem works for a Poisson kernel, but it turns out that the span of the Poisson kernels is dense. So then we'll be done with that result there. So it's kind of an interesting thing. The real result is much harder. This is a sketch of a very simple case.
00:32:52.224 - 00:33:39.442, Speaker B: So what I want to deal with now is an explicit description of the model spaces and in fact the p, the p versions of them. So I'm going to do mostly h two stuff, but things will work in hp as well, and I'll generalize as we go along here. So what we're going to do is use the conjugation on our model space to get a cartesian decomposition, like a real and imaginary type decomposition with respect to c. So every function in the model space can be written as a plus ib, where a and b are. They're not real functions, they are c real. They're fixed by the conjugation C. So a and b are functions in the model spaces because they're linear combinations of functions in the model space.
00:33:39.442 - 00:34:24.998, Speaker B: Kuwait. But they've been cooked up, so they're fixed by C. So a and b are fixed by the conjugation C. And so that means if I want to get my hands on a typical function in a model space and understand what that looks like, or come up with some sort of description of it, all I really need to do is identify functions that are fixed by c, because this cartesian argument says, well, I can always reduce the case where I've got a linear combination of things that are c real. So in other words, I need to study functions that satisfy f is equal to fz bar times u. Or in other words, I can rewrite this as f over f bar equals z bar u. And again, this is on the circle.
00:34:24.998 - 00:35:16.302, Speaker B: This is working almost everywhere on the unit circle doing this conjugation stuff. Now, without loss of generality, I'm going to suppose that u of zeta is equal to zeta, and that's a normalization we can make for some zeta, the normalization we can make by multiplying u by a suitable constant. And so if u happens to have a adc angular derivative in the sense of caroteodori at zeta, well, then what happens? The corresponding boundary kernel belongs to the model space. Great. If not, at least it belongs to the Smirnoff class n plus, which you heard of in some other lectures from this week. That's the space of quotients of bounded analytic functions where the denominator is outer. If we don't have an adc at zeta, well, I can still write down k zeta.
00:35:16.302 - 00:35:52.952, Speaker B: It just doesn't belong to h two, but it certainly belongs to the Smirnov class. Now, there's a little bit of a computation here that is fun, playing with unimodular expressions on the boundary. You can verify that for a boundary kernel. As I've said before, boundary kernels are almost self conjugate. And in fact, because of this normalization, it turns out this boundary kernel is actually self conjugate. That was the point of this normalization saying that u of zeta is equal to zeta for some fixed zeta. And so what happens here is k zeta is equal to k zeta z barred times u.
00:35:52.952 - 00:36:34.534, Speaker B: Or in other words, k divided by k bar is equal to z bar u. Or in other words, these two things are the same here, f over f bar and k over k bar are the same on the circle, almost everywhere on the circle. And that tells us that f k is equal to its own complex conjugate. So in other words, it's a function that's real. But wait, that means it's an analytic function, a function in the Smirnoff class that's real on the boundary. So we've reduced the problem of talking about typical model space functions to functions that are c real, fixed by the conjugation C. And now we've reduced it to talking about functions in the Smirnov class that are real on the boundary.
00:36:34.534 - 00:37:08.888, Speaker B: So that's the first step here. F is equal to its own conjugate function in the model space if and only if f divided by the boundary kernel. K zeta is real on the circle. And so that motivates the study of so called real Smirnov functions. So we're going to look at Smirov functions again, quotients of bounded analytic functions with outer denominator. We're going to want to look at those that are real almost everywhere on the circle, and I'm going to call that r. So remember, r is a collection of analytic functions.
00:37:08.888 - 00:37:40.934, Speaker B: It is not real numbers that are positive. This is a set of analytic functions. It's going to cost us really nothing to look at these generalizations of model spaces. These are the typical invariant subspaces for the backward shift operator on hp for p in this range greater than or equal to one. And the reason is the proofs that what I'm going to show you follows essentially exactly the same. So we're going to do this for kp. And it turns out there's a really interesting characterization here.
00:37:40.934 - 00:38:22.706, Speaker B: This space kp is exactly the space of functions that look like this. You take real Smirnov functions a and b, plug them in here to make this thing that kind of looks like a complex number. But again, a and b are functions that happen to be real on the boundary times the boundary kernel. And you intersect that set with hp. That is exactly what that model space kp sub is. And moreover, that conjugation that was described originally in terms of screwing around with boundary functions and getting these weird f bar z bar u expressions that didn't look like they were analytic functions. Well, it turns out that the conjugation is exactly what you think it should be.
00:38:22.706 - 00:38:50.706, Speaker B: Morally, it should be a thing that changes an a plus bi to an a minus bi. And that's what the conjugation on these model spaces actually is in terms of real Smirnov functions. Because remember, a and b, those are SMirnov functions that are real on the boundary. So this suggests having a closer look at these things. There's some interesting results. You can do this very explicitly. HelSon a long time ago proved that if you've got a real SMirnov function, it basically looks like this.
00:38:50.706 - 00:39:26.124, Speaker B: You take two inner functions, psi one and psi two, so that the denominator is outer, and you plug them into this thing. And that is a typical real Smirnoff function. This is an if and only if result. And the proof is a Mobius transformation argument. This Mobius transformation maps the disk onto the upper half plane. So what that means if f is real on the boundary t inverse composed with f, well, it's a quotient of h infinity functions, because it's coming from the Smirnov class. Or it actually just is a quotient of h infinity functions.
00:39:26.124 - 00:40:20.886, Speaker B: And by construction, it's unimodular almost everywhere on the unit circle. So, factorization theory for the Smirnov class or the Nevan line class says that, well, it's got to be a quotient of inner functions. And so when you undo this and apply the Mobius transformation tau to both sides, you get a function of Helson's form. So it's an undergraduate level complex analysis argument there, aside from the definition of h infinity and stuff like that. So the upshot of this is that every function in this generalized model space, kpu, again, the typical backward shift invariant subspace of hp, can be expressed using the formula on the previous slide. I need a real Smirnoff function a, a real Smirnoff function b, and the boundary kernel. The boundary kernel involves the inner function u and the a, and the b can be described each using two inner functions.
00:40:20.886 - 00:41:15.844, Speaker B: So, in fact, every function in that backward shift invariant subspace can be written explicitly using five inner functions, u and four other inner functions. Now, recall that there was this whole big deal about pseudo continuation and pseudo continuable functions. Well, this actually tells us that pseudo continuation entirely arises from Schwarz reflections of inner functions. Inner functions have pseudo continuations via Schwarz reflections. You can reflect them through the circle, get a pseudo continuation to the exterior of the disk. Well, the pseudo continuable functions are the ones that live in the model spaces. So, since we have this explicit description of the functions of the model spaces, and you write them down algebraically using five inner functions, it turns out that all pseudo continuations come from inner functions and Schwarz reflection, which is kind of an interesting idea.
00:41:15.844 - 00:42:02.260, Speaker B: Now, I'm going to throw some stuff out there that's more function theoretic, because I think there's probably a lot of stuff that can be done in this direction, especially for some of the younger researchers and students, because it turns out that Helson's theorem, although it's really, really elegant, might pack things together a little bit too tightly. So what I want to do is take a look at sort of a factorization theory for real Smirnov functions. So I have a real Smirnoff function, little f. I break it up into its usual inner outer factorization. Now, it turns out, just by algebra, I can write this. That's just algebra. So the reason that one would write all this messy stuff here is because the first term happens to be a real Smirnoff function.
00:42:02.260 - 00:42:27.154, Speaker B: You can almost see like a, it's almost like a kerber type function with the inner factor of little f plugged in. So this first term is a real Smirnoff function. It's real on the boundary. It's in the Smirnoff class, and it has the same inner factor as f. So I can remove the inner factor of little f, and then I'm left with something else. The second factor is an outer function. It's an outer function.
00:42:27.154 - 00:42:58.884, Speaker B: And since this part is real on the boundary, since little f was supposedly real on the boundary, I've got an outer function that's real on the boundary. So I've got an outer function in r plus. So a real Smirnoff function that is outer. So I can always break a general real Smira function into two pieces. And so there's a factorization theorem that goes back. One of the first things I proved, Saracen, who was my advisor. And our result is this.
00:42:58.884 - 00:43:36.508, Speaker B: Every function in r factors in a locally uniformly convergent product in this fashion. So, I've got essentially the Kerber function from univalent function theory here, except there's a minus four thrown in. And I have an inner function plugged into the kerber function here. I've got a constant that's real, right? And then I've got a, you know, it's a. It's a Mobius transformation, the plus or minus I here there to make things work out. And so I have a product of Mobius transformations of inner functions. And so that is the general real Smirnov function.
00:43:36.508 - 00:44:24.634, Speaker B: So it's kind of interesting that now we have a decomposition theorem for real Smirnov functions. We know that real Smirnov functions can be used as the building blocks of typical functions in these model spaces, or more generally, backward shift invariant subspaces of hp. So it seems like there's a lot of stuff that could be done using this sort of machinery. Now, I want to make a side remark, because there were a lot of talks on Tuplitz operators and toplets kernels recently, and I wanted to point out that a lot of this stuff works out, you know, conjugations and real Smirnoff functions works out for Tuplitz kernels, too. So, refresher. The topelet's operator with symbol Phi is this operator of projection by Phi, and then, sorry, multiplication by Phi, and then projecting back into the hardy space. I mean, we saw a couple lectures on this in the previous days.
00:44:24.634 - 00:45:28.924, Speaker B: So, there's a nice theorem of Hayashi that goes back quite a bit and says this, that if the kernel of a topelets operator is non trivial, there's an equivalent symbol that gives you the same kernel. So there's an outer function in h two, so that the kernel of your given toplitz operator is equal to the kernel of this other one with sort of a standard symbol. And so there's a theorem that I proved a long time ago, but I have to admit, I had no idea how to write things. And I wrote things in a very cryptic and not understandable way. So I think a lot of people have never heard of this theorem because it's buried at the end of my first paper way back when. And it says that, okay, if capital f is outer in h two, then the kernel of this topelet operator, which is the standard form for a topelet's kernel, looks like this. A real Smirnoff function plus b plus I times another real Smirnoff function times that outer function f, and you intersect with h two, and the result holds for hp with p greater than or equal to one.
00:45:28.924 - 00:46:19.360, Speaker B: And in fact, the idea is exactly the same. These Tuplitz kernels carry a natural conjugation, and you use exactly the same ideas, as we saw in the previous few slides, to reduce things to the real Smirnov case. So I thought I would throw that out there, since we'd seen a lot of interest in Toplitz kernels. So I want to end on an interesting topic, and that's the quaternionic structure of two by two inner functions. I think this is another place where some young researchers can probably take these ideas and develop them and do some things with them. So, if u is an inner function, and we're going to let a, b, c, d belong to h infinity, bounded analytic functions on the disk, and I'm going to define this matrix valued analytic function capital theta. The minus sign is there for reasons we'll see later on.
00:46:19.360 - 00:47:10.824, Speaker B: So the theorem is that, well, theta is going to be unitary almost everywhere on the circle, and its determinant will be the inner function u. If and only if. The following three things hold. And these three things are a, b, c and d have to belong to the appropriate model space. The model space corresponding to zu, a and b have to satisfy this pythagorean type identity on the boundary modulus a squared plus modulus b squared has to equal one on the circle. And most interestingly, since we know a, b, c and d have to actually belong in the model space determined by zu, it turns out that a and d are conjugates in that model space, and c and b are conjugates in that model space. So this tells us that these two by two inner functions have a quaternionic structure.
00:47:10.824 - 00:47:46.444, Speaker B: To them. They look like these. This is the standard representation of quaternions of as complex as two by two matrices of complex numbers. So when you build the quaternions, this is one way of doing them. You take complex numbers z and w, throw them in a matrix like this, and you get an algebra that's isomorphic to the quaternions. So what's going on here? This is saying that two by two inner functions look like quaternions of unit modulus because they have the same structure. And the cross things here the a and the d have to be conjugates and the c and the b have to be conjugates.
00:47:46.444 - 00:48:43.830, Speaker B: One thing I've been curious about that I've never looked into, and again, this is something that some young researchers can do, is it seems like this is enough of a concrete foothold on two by two inner functions that you might actually be able to develop a factorization theory for two by two inner functions that is very explicit in terms of you might be able to say things in terms of the actual components of the two by two inner function, as opposed to doing things sort of at the more abstract level. There's enough hard function theory tools available now. I think that one might be able to say some things or do some things in this direction. And I'm going to push this off to the notes and the, because I think I got 1 minute left. I'd rather wrap things up and get to the references. But again, there's more stuff that's going to be in the slides that will be provided at the end. There's a lot more stuff in the book chapter that is going to be posted at the very end in the chat.
00:48:43.830 - 00:49:20.290, Speaker B: I want to get to the conclusion here, and first thing I want to say, there's a lot of things I omitted in these series of talks, partly because lack of time, but also partly I don't want to step on the toes of the other lecturers. So there's some sort of missing topics that you could argue should have been included here. So Debraj Rovniak, spaces are very, very close cousins of model spaces. Somebody else is going to be giving three lectures on that interpolation theory. That is another huge topic, I believe. There's a series of lectures on interpolation theory, re spaces and that sort of stuff. There's going to be a whole series of lectures on deeper properties of inner functions.
00:49:20.290 - 00:50:10.976, Speaker B: And there's also going to be a series of lectures on operators on function spaces. So you'll learn all about toplitz operators and Honkel operators, which I've bare, which I haven't even mentioned, Honkell operators, but their connection to model spaces is very strong. And then there'll be applications of the Alexandrov Clark theory to mathematical physics at the very end. Some of the deep work of Polteratsky and his collaborators are going to be talked about toward the end of these lectures. So if you want to find out more, I wrote a book with Javad Mushreggi and William Ross, who I believe are both here somewhere in virtual sense, and we wrote a whole book on model spaces. So if you find this topic interesting and you find that the notes that are provided at the end maybe don't meet your interest, you might pick up this book one way or another. I'm sure it probably is out there on the Internet somewhere, but I prefer if you bought it.
00:50:10.976 - 00:50:39.824, Speaker B: But if you don't, you know, everything that you could want to know is there. And if anything that you could know isn't there, it's in one of these other books. So these are books, not papers. I think these are great references. They touch on different topics and different aspects of model spaces. But if you have all of these books, then you would probably have everything you would want to know about model spaces and their various connections. Although I'm sure there are some references that I've forgotten here.
00:50:39.824 - 00:50:44.276, Speaker B: Right.
00:50:44.340 - 00:51:03.244, Speaker A: Thank you very much. And we have time for questions. So if you have a question, then please just go ahead and. Yeah, so there is a question in chat. How about the kernel of topless iterators defined in the upper half plane?
00:51:05.744 - 00:51:32.554, Speaker B: I have never looked at it. I don't know. I would suspect probably, I mean, I would imagine that, you know, real Smirnoff functions on the disk translate pretty easily to real Smirnov functions on the upper half plane via Mobius transformations. And I would expect that something similar works, but I don't know, something that's worth looking at, and I've never looked at it or thought about it.
00:51:36.054 - 00:51:36.994, Speaker A: Thank you.
00:51:38.574 - 00:52:09.414, Speaker B: I'll put the reference now that the slides and everything have been posted there. I'll put the references back up, since I'm sure some people probably want to see them. I believe that the issue is you can't post links or something successfully while I'm sharing, so hopefully you can still download from the links. But I wanted everyone at least, you know, if you're interested, take a screenshot of this, or, you know, it's going to be in the end of the third lecture notes. So there's a lot of books with a lot of information on this topic.
00:52:15.214 - 00:52:34.174, Speaker A: Other questions? If not, then let's thank our speaker again. Thank you very much. And we continue.
