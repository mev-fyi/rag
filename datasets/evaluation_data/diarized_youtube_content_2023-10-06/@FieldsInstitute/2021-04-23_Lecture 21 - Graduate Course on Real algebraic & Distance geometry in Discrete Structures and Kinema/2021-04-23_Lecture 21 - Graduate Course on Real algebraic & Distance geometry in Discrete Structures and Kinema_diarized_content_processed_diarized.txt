00:00:00.760 - 00:01:12.184, Speaker A: We already talked about the basics of homotopy continuation. In previous videos, we covered the extremely important parameter homotopy and also monodromy methods. Today we're talking about numerical irreducible decomposition, randomizing over determined systems, and how you can start to compute witness sets for positive dimensional components. So you have solution sets, which are curves are one dimensional, or solution sets which are surfaces, two dimensional complex manifolds in their smooth points, or three dimensional or four dimensional components. And how can we compute these things? Well, we can't compute infinitely many points, but we can compute a witness set. And so right now, what I showed is that setting up this square system will compute finitely many points which are called a witness set for the nine dimensional component. So we have this example where we consider five variable or five equations and ten unknowns.
00:01:12.184 - 00:02:01.034, Speaker A: It could have components of dimension six, seven, eight or nine because it's rank four. We're just hypothesizing this to be true. And so we're saying, okay, well, how do we look for a solution component which is nine dimensional? The answer is randomize the original five component functions into one random linear combination of them. That gives one polynomial equation. And for the remaining nine equations, take linear, random linear equations. Actually affine linear we should do. But either way works.
00:02:01.034 - 00:02:53.482, Speaker A: And those nine linear equations determine a line in c ten. That line will hit our nine dimensional component finitely many points. And we will compute those finitely many points. So here, solve this solve by homotopy continuation. Computing a witness set for the nine dimensional solution component. Okay, reducible decomposition. It'll help us to think about how do we compute witness sets.
00:02:53.482 - 00:03:49.466, Speaker A: So later we'll compute a witness set for the six, oops, not six, but six dimensional component. And we could compute a witness set for the seven dimensional component. For the 8th and for the 9th, we could compute witness sets for each of them. But we don't even need to try to compute witness sets for the five dimensional component. Since f was only had four component functions, we know that the most the dimension could drop is by four. So ten minus four is six and ten minus four equals six, which means that is the lowest we'll have to look for. Okay, okay.
00:03:49.466 - 00:05:13.750, Speaker A: And now here's the problem is that maybe I'll put the problem in red. Problem. So far we only dealt with f from rn to rn, so called square systems where the number of variables equals the number of equations. Okay, so we need to extend our methods to the case where the two numbers of variables and equations could be different in either direction. That is like, up here, we have more variables than equations, and hence our solution sets will have necessarily positive dimension. But in fact, in order to get to that situation, which we would call underdetermined deter. Oops, that's not an.
00:05:13.750 - 00:05:54.334, Speaker A: Mm. Uh, we first need to discuss overdetermined, where, let's say it would be ten reals to 14. Okay, so that brings us to overdetermined systems. Oh, and let me clear some room for myself. Okay, so here we are. In the case of an overdetermined system, we have five variables and eight equations. And can we use homotopy continuation? Well, not as we discussed so far.
00:05:54.334 - 00:06:43.166, Speaker A: It's not a square system. So, for example, the jacobian DF would be. What would be? It would be a matrix of size eight by five, at least how I set up the Jacobians. It could be five by eight if you do it, transpose. But. So this is not, as you know, we could maybe do Newton's method, where we're doing like, a least square solve or a pseudo inverse solve to invert the Jacobian for our Newton's method that we discussed in previous videos, in previous lecture. But it would be.
00:06:43.166 - 00:07:27.984, Speaker A: There's. It turns out there's a better way. And the better way is to sort of square up this system. So, okay, so, first of all, five minus eight is negative three. So sort of, I'd really naive dimension count would say that the dimension of an irreducible component could be, well, negative three, negative two, negative 101234. Okay. But, um, at least, I don't know how to make sense of these numbers, so we can kind of get rid of it.
00:07:27.984 - 00:08:11.414, Speaker A: But you see that we could have. These are called isolated solutions. That is, zero dimensional components. Okay? And so those are what we can compute using homotopic continuation. Everything about homotopic continuation was computing isolated solutions. Our start system had finitely many solutions, and we would follow those finitely many solutions to, at most, finitely many solutions of the target system. And so, you know, we're not going to magically get infinitely many solutions.
00:08:11.414 - 00:08:17.414, Speaker A: And so, you know, sort of the isolated solutions were what we were hoping for this whole time.
00:08:18.474 - 00:08:26.830, Speaker B: Okay, now, Alex, I think there's a question by Faye here. What is the definition of dimension?
00:08:27.022 - 00:09:25.674, Speaker A: Okay, great question. Let's maybe. Let's go back up to the warm up, or let's go over here, dimension. And also to define dimension, I'll also need to say what I mean by irreducible. So over r or c, or let's just. Let's just keep it over c over c and irreducible. Okay, so first of all, okay, let's see, let x be an algebraic set that is a subset of c to the n defined by polynomial equations.
00:09:25.674 - 00:11:05.834, Speaker A: Then it turns out that for almost, or let's say outside an algebraic subset of strictly lower dimension, or, I mean, I can't really say dimension until I've defined dimension. So, okay, outside of an algebraic subset, every point x in x has a neighborhood holomorphic to c to the d. For some d, a natural number, that is, x, being an algebraic set, is automatically a manifold at almost every point. So near every point, there's some neighborhood where every. Where the neighborhood of x at that point looks like c to the d. Okay, so this means it's a complex manifold. And the points where it's a complex manifold, we could say they're called x regular or x manifold points, or x smooth, something like this.
00:11:05.834 - 00:11:53.384, Speaker A: And basically, most of the points look like that. And the only points where it fails, so x minus the manifold points equals an algebraic subset. So it turns out that you can even write down the equations. They're the equations that the Jacobian has one less rank. So you can take the minors of the jacobian matrix of the equations defining x. And those minors, in addition to the equations defining x, will define the locus of points where it fails to be a manifold. I see.
00:11:53.384 - 00:11:56.316, Speaker A: Yeah, yeah, thanks. Okay.
00:11:56.460 - 00:11:59.424, Speaker B: And then also define it the way I put it on chat.
00:12:00.644 - 00:12:02.004, Speaker A: I can't see the chat.
00:12:02.164 - 00:12:14.584, Speaker B: Oh, okay. So if you just took a regular point on the variety, and then you just took its neighborhood in the ambient space and then intersected it with the variety and take the dimension.
00:12:18.244 - 00:12:24.684, Speaker A: Yeah. So then that neighborhood, I assume you mean like a ball and c to the end.
00:12:24.804 - 00:12:25.596, Speaker B: Yeah, yeah.
00:12:25.660 - 00:12:27.100, Speaker A: And intersecting.
00:12:27.212 - 00:12:29.116, Speaker B: I wrote r to the end, but I meant c to the end.
00:12:29.140 - 00:12:39.204, Speaker A: Yeah, yeah, that's like a. Yeah, it's a subspace topology sort of thing. And that would exactly be, that's what I would mean by a neighborhood of, of a point on x.
00:12:39.364 - 00:12:40.144, Speaker B: Okay.
00:12:40.564 - 00:12:44.468, Speaker A: Yeah. So what is a neighborhood? It means that you intersect it with a ball.
00:12:44.636 - 00:12:45.124, Speaker B: Yes.
00:12:45.204 - 00:13:52.856, Speaker A: Okay, so the, and then the other thing is, what's irreducible, then? Well, if X manifold, if the manifold points is connected, then x is irreducible. Okay. And so if you're, if you like algebra, you would not like my definitions here, but these are the definitions that make most sense for numerical computation because we're just dealing with the field c and its subfield r and floating point numbers. And so the, the, the geometric approach just makes a lot more sense and makes things clearer in computation over c. Now, over other fields, this doesn't make any sense at all because you don't have a manifold. Like a manifold is defined for R. We know what that means, and it's defined for c.
00:13:52.856 - 00:14:09.114, Speaker A: But you're going to have more difficulty defining geometric concepts like this over other fields. And so in algebra, where you discuss other fields, you want algebraic definitions of things like dimension and algebraic definitions of things like irreducible.
00:14:11.654 - 00:14:41.544, Speaker B: But so just, I mean, going back to what you were talking about before, because you're looking at, when you look at real zeros, you're also looking at, you know, the discriminants. So in fact, you have to add on the discriminants. That's why you can also get zero dimensional zero set, even though you have only four polynomials in ten variables. Right?
00:14:45.644 - 00:14:47.212, Speaker A: Yeah, think of it that way.
00:14:47.308 - 00:14:52.664, Speaker B: You could think of it that way instead of using the sum of squares argument that you gave.
00:14:55.464 - 00:14:59.084, Speaker A: Yeah, I'm not sure what it means to add on the discriminant.
00:14:59.664 - 00:15:05.324, Speaker B: In order to be a real zero, you also need the discriminants to be non negative.
00:15:07.904 - 00:15:10.044, Speaker A: The discriminants of what?
00:15:11.304 - 00:15:40.654, Speaker B: Of the system that you have. So in order to, you have a semi algebraic set whenever you have a real, I mean, you have additional polynomial conditions to have to isolate real zeros. So even though you only have four polynomials in ten variables, you could end up with a zero dimensional set of zeros, because you additionally need the discriminants to be non zero.
00:15:41.434 - 00:15:46.162, Speaker A: Okay, maybe this is sort of a topic that I, I'm unfamiliar with.
00:15:46.298 - 00:15:47.074, Speaker B: Okay.
00:15:47.234 - 00:15:58.826, Speaker A: Yeah. Because for me, like, a discriminant is usually associated to a certain property, and maybe you're talking about the property of being real valued.
00:15:59.010 - 00:16:15.554, Speaker B: Right. So if you, the simple ones that people are familiar with would be if you just had a single polynomial, a single quadratic, you would say, you know, b squared minus four ac has to be greater than or equal to zero. So you have to cut that out.
00:16:17.214 - 00:16:19.454, Speaker A: Oh, I see. Okay. Yeah, yeah.
00:16:19.614 - 00:16:42.494, Speaker B: So essentially you have these, this variety, which is defined by these four polynomials, plus you have all these other polynomials which you have to assert to be non zero. I mean. Sorry, non negative, maybe I kept saying non zero. That would be confusing. And so, so that could isolate your roots to a zero dimensional set.
00:16:43.714 - 00:16:51.394, Speaker A: Yeah, I see what you're saying. So it's sort of like one way to encode the property of being real valued is by looking at the discriminants.
00:16:51.434 - 00:16:54.654, Speaker B: Of these correct yeah.
00:16:55.914 - 00:18:01.490, Speaker A: Okay, so now we know what dimension means. We also talked about the fact that the, that the most of the points on an algebraic set will be manifold points. And that's good because it sort of plays a role in what we're going to do next. Okay, so for over determined systems, like, let's say this one right here, let let a be a random, let's see now I want five by eight matrix. Then Af is a map. Well, there's still five variables, but now we've squashed all eight component functions into linear combinations, and there's only five linear combinations of them. And so we have now a square.
00:18:01.490 - 00:19:03.534, Speaker A: Then Af is square. And so what do I mean by af? I mean 512345 by eight. I don't know if that's eight. And then f one, f two, f three, f four, f five, f six, f seven, f eight. So this is going to be 12345 component functions of, you know, this is af right here. Does my notation make sense? Questions on that. Okay, so what we've done is we've taken random linear combinations of the original eight component functions in such a way that we've produced only five output component functions.
00:19:03.534 - 00:19:46.704, Speaker A: And so now, you know, there was only five variables to begin with. So now we have five variables and five equations. And then the theorem is that, well, it's sort of easy to see that the zeros of our original system are contained in the zeros of our randomized system. Randomized system. Okay, and so that's good. We haven't lost any of the solution set that we were originally interested in. Right.
00:19:46.704 - 00:21:16.314, Speaker A: So this is our original solution set. Now also this v of Af minus v of f. So if we remove, what are the extra, these are the extra solutions we have introduced by randomization. Well, this set will be either empty or smooth of pure dimension. Five minus five in this case, equals zero. Okay, so later we'll randomize things to smaller numbers of equations, but let's write it as pure dimension. So what this means is that the extra solution components are nice, and they will be, we'll be able to identify them as extra.
00:21:16.314 - 00:22:59.678, Speaker A: And so this is what we want. And so, okay, so then what we do is computer the isolated solutions of af, and then we'll throw away any extra solutions. So by homotopy continuation, throw away any extra solutions after the fact by evaluating them against f, the original system. Okay, and so basically, this is how we would compute the isolated solutions of our original system, which, of course could have solutions of higher dimension, but we'll throw them away by a process that we'll describe later. Okay, questions so far. Pure dimension just means is that all the components have the full dimension. Well, here, let's see.
00:22:59.678 - 00:23:53.236, Speaker A: Okay, so basically we need to go to a more general situation, but. Okay, so if we have this and then we multiply by a random matrix of size k by n, then. So let's call this, let's call a. This was our a. F then is going to be from cn to ck, and then the extra components will be smooth of dimension n minus k. So before, in our example that we just did, we randomized so that k equals n, and then n minus n is zero. But we don't have to randomize all the way down.
00:23:53.236 - 00:24:31.684, Speaker A: We could randomize lower and then the extra components will be smooth. In particular, they don't intersect each other, so they're just like what you picture is just several extra components that are just completely disjoint, smooth components. And this is what's introduced by randomization. Okay, thanks. Yeah, so as long as we choose a random matrix a, because there's this risky open dense set of such matrices where this is true.
00:24:36.544 - 00:24:37.432, Speaker C: Alex.
00:24:37.608 - 00:24:38.328, Speaker A: Yes.
00:24:38.496 - 00:24:44.904, Speaker C: So we started with five variables and had eight equations, an over determined system, right?
00:24:45.024 - 00:24:45.328, Speaker A: Yeah.
00:24:45.376 - 00:25:47.324, Speaker C: And a much more naive approach to this might just be to say, well, let's just solve the first five equations, and then we potentially get a bunch of isolated solutions, because five equations ought to cut us down to a zero dimensional system, and then we could plug those into the last three equations to make sure that they solve the actual system. And if they don't, we could toss them away. That's the kind of much more naive approach to this. But it's the problem with that approach that the zero set of the five first equations that we would be looking at might actually be higher dimensional than zero dimensional. So then we would have trouble kind of throwing away isolated points. Is that what randomization is buying us?
00:25:48.304 - 00:26:33.934, Speaker A: Yeah, this is an excellent question. And in fact, the naive method that you just suggested turns out not to be so naive. I think there's a chapter in it in either of the textbooks, sort of working equation by equation. I'm not familiar with that method as much. But you know, to, yeah, to do this, to design a robust numerical algorithm, you can think about using several different approaches. And I think that is one approach that could be used to answer your second question about higher dimensional components. That will be a problem with both, with both approaches.
00:26:33.934 - 00:27:32.934, Speaker A: And so that sort of motivates the witness set and the numerical irreducible decomposition, which I plan to talk about next. Okay. Right. Because we, you know, of the original, I'll go back here for this original system, eight equations and five unknowns. It could still, it could be that it has components of higher dimension. And so, you know, when we compute the isolated solutions of the randomized system, it could be that, that one of those isolated solutions is also a solution of our original one, but it's a solution that lies on a two dimensional component, and it's not actually an isolated solution of our original system. Okay, so, and in fact, I mean, this was a problem even before when we just had a square system.
00:27:32.934 - 00:28:39.114, Speaker A: If we were just computing some system that had the number of variables equals the number of unknowns, we compute its isolated solutions with homotopy continuation. But some of those solutions might actually lie on a higher dimensional component, because again, this could have solution components of any of these dimensions. Okay, so let's talk about that then. This requires witness sets and numerical irreducible decomposition. And I'll sort of talk about them in the same section rather than separating it. And as always, I like to make it concrete. So let's take a polynomial, let's say r ten to r five, and let's say with rank of f equal to four.
00:28:39.114 - 00:30:07.642, Speaker A: So now I need to introduce, this is another, another concept I want to introduce. What is the rank of a polynomial system? So df is a five by ten matrix. The rank, the rank of df evaluated at a random point, let's say z in c ten, will be, will be generically. Generically. Well, I said a random point. So if we choose a random point, will be equal to a natural number, right, called, and we are going to call that natural number the rank of f. So in this situation, if I have a five component, five polynomials and ten variables, and I say that it has rank four, what I mean is that if I plug in a random point, the Jacobian only has rank four.
00:30:07.642 - 00:31:19.554, Speaker A: It, as opposed to, we would expect it would have rank five. And as an example of this, you can take the rotation group of three dimensional space. So, example, so three is defined by, you know, it's defined in several ways, but as defined by how many seven polynomial equations, and yet their rank is six in nine variables. So, so three is defi, is a group of three by three matrices. So there's nine entries of a three by three matrix. So there are nine variables. When we say that a transpose equals a inverse, then we're imposing polynomial equations.
00:31:19.554 - 00:32:23.694, Speaker A: And when we say that it has to be so three, we're imposing another equation, namely the determinant b one. And in total those are seven equations. So like 123456 and deter d equals one. So this is the 7th equation. And these are the six, sort of a visualization of the six equations from what is it like a transpose a equals the identity. All right, so compute a transpose a and look at it these six entries and require them to be 11100 respectively. Okay? And so here's an example where you naturally have a polynomial system whose rank is less than you would expect.
00:32:23.694 - 00:33:44.714, Speaker A: And so the dimension of an irreducible component then actually is only cut down by the rank. So even though we would might naturally say 56789, in actuality we don't need five. Because this system has rank four, we know that it does not have a five dimensional irreducible component. The lowest dimension it could have is a six dimensional component. Okay, so now how do we start getting its components? So what we'll do is we'll start here, start here, look for, look for x with dimx equals nine. Okay? So to find, so that what we'll do. So just so I have it on the screen, we had ten variables and five equations with rank of f is four.
00:33:44.714 - 00:34:36.999, Speaker A: And so what we're going to do is we're going to randomize with a random one by five matrix. So I'll write it. 12345 f one f two f three f four. F five equals zero gives one equation. And now we're going to say that. Okay, so, oh wait, I always get confused. Yeah.
00:34:36.999 - 00:35:56.344, Speaker A: Okay, so what we want to do is if we're looking for, look for dimx equals nine in c ten. So if we're looking for a nine dimensional component, what we should do is idea slice with a linear space of dimension one. And so, you know, the idea is like, let's say we had a parabola in the plane, take a random line. That line will hit the parabola in two points, right? And so if we have a, that's what we're going to do. We're only co dimension one nine dimensional and c ten. So if we slice with a random line, we should hit that variety in finitely many points, slice with the linear space of dimension one. It should intersect in finitely many points, namely the degree.
00:35:56.344 - 00:37:01.404, Speaker A: So that's the degree of this algebraic set. And so how do we get a linear space of dimension one in ten variables? Well, that means we need nine. So this is a nine by ten random matrix, let's call it l. And let's take our variables x one through x ten and 123-45-6789 equations gives nine equations. And these are nine linear equations. And together this is square square system. So, namely af equals zero and lx equals zero is, gives ten equations and ten unknowns.
00:37:01.404 - 00:38:14.994, Speaker A: Okay, I'm going to pause there for a moment. I'm making this little part about so three smaller, because I just, I really wanted to introduce the idea of the rank of a system. And so, so three is a good example for that. But now what we should be focusing on is this is what is a witness set? A witness set is the finitely many points. Well, really, they say that the original system f, the finitely many points w for witness set, which we can store in a computer, we can store finitely many floating point solutions. And then finally, we should also write down the linear equations that we used to slice. Okay? And so if we have this data, we have our original polynomials, we have our linear equations that we use to slice, and we have the resulting finitely many intersection points.
00:38:14.994 - 00:38:22.394, Speaker A: Then this data is called a witness set. In this case, we computed it for the nine dimensional component.
00:38:25.094 - 00:38:35.358, Speaker C: Alex, how do you know that these points in w actually satisfy, by way?
00:38:35.406 - 00:38:39.590, Speaker B: Well, I find it hard to hear you. Can you speak up?
00:38:39.702 - 00:38:46.198, Speaker C: How do you know that the equations, that the points in w satisfy the equations in f. Excellent point.
00:38:46.246 - 00:39:25.714, Speaker A: Because might not, because we randomized. Right. So we could have introduced extra new points. Exactly. And so here's that theorem from before, where, because we took a random linear combination, the only extra points could, could, the only thing they can be, are smooth of pure dimension nine, and they're smooth so they don't intersect each other. They don't intersect our original solution component. And so we, we just try to plug them into the original polynomials fast.
00:39:25.714 - 00:40:15.954, Speaker A: And if needed, because all of our solution points are close enough so that Newton iteration converges and we can refine them to any degree of accuracy. We can take our time when we plug them into the original polynomials f, and determine which of these finitely many solutions to this square, randomized and sliced system, which of them actually satisfy our polynomials f. Yeah, so that's a, that's a great question. That's, that's sort of why I talked about randomization. First is for this problem. Now notice the other problem that you mentioned will, where these, these, what if these points are on a higher dimensional component? Well, in this case, there is no higher dimension. We started with the high.
00:40:15.954 - 00:41:18.024, Speaker A: So, you know, this is, the idea is you start with the highest dimension. Now, once we have a witness set for the highest dimensional component. When we go and we search for points on the eight dimensional component, we can very quickly determine if any of those points actually lie on the nine dimensional because we have a witness set. So we do a parameter homotopy and we take our linear space that, that we started with here, and we just move it over to a linear space that contains our new points on the potential eight dimensional component. And the theorem is that it must go to that point like our witness set of the nine dimensional. When we perturb the linear space, one of those witness set points will go to the new point that we're trying to decide is whether it's on nine dimensional or not. So this is, this is why we started with nine.
00:41:18.024 - 00:42:17.224, Speaker A: And to make this really clear, let's continue and talk about how we would do the eight. Okay, so first make some more room for myself. Okay, so now we have a witness set for the nine dimensional component. So how about now find witness set for eight dimensional component. Well, this time, instead of a one by five random matrix, we'll take a two by five random matrix. So this is going to be af equals zero. And then instead of a nine by ten random matrix for l, we'll take, well, we already have two equations, so this gives us two equations.
00:42:17.224 - 00:43:08.154, Speaker A: So all we need is, you know, we're in c ten, so we need eight more equations to have a square system. So now we'll take a random eight by ten matrix l. So now lift is eight by ten, x maybe plus b equals zero. And this would again be random. What would this be? Eight by one. And so this would give us eight linear or eight affine linear equations together. This is a ten by ten square system of equations, you know, solve by homotopy continuation and store f.
00:43:08.154 - 00:44:20.564, Speaker A: Let's, let's call this, I don't know, l eight. Well, I'm just going to call it l again and store, you know, the original polynomials, the finitely many solutions in w and the linear space l. And this time, before, you know, before we take our solutions and put them in our set w, we should check them against f. We should see if they even satisfy f, plug them in. If they do satisfy f, then if we want to be sure that we didn't, you know, here we sliced with some two dimensional linear space cut out by eight equations and ten variables, and that two dimensional linear space could also be hitting our nine dimensional component as well as our eight dimensional component. And so, to ensure that this witness set doesn't contain points on the nine dimensional, we use our previous witness set and take a parameter homotopy from that linear space to our new one, and that will allow us to eliminate. Yeah.
00:44:20.564 - 00:45:27.894, Speaker A: Okay. So at this point, I feel like if you followed what we talked about so far, you now know how to continue and compute the entire numerical irreducible decomposition with maybe one caveat of. We need to separate these witness sets for the pure component into irreducible pieces. But, yeah, let me pause here. These two boxes, this box and this box, it sort of, it contains all the information we need. I think if you understand these, you can continue. Could you repeat what you said about using homotopic continuation to go from l for the nine dimensional one to l for the eight dimensional one? Sure.
00:45:27.894 - 00:47:18.684, Speaker A: Yeah. So this would be using parameter homotopy, which is a lot to wrap our minds around, but we talked about in a previous lecture a few weeks ago now, um, and what we do is, is construct a parameter homotopy where we perturb these linear equations towards, um, towards a linear space that can. So let's, okay, so let's say that here for this ten by ten system, we compute a point, um, z, special zs, or just a point z. And we want to decide should we include it in the witness set for the eight dimensional component? Or might it be a point on the nine dimensional component? Because we sliced with the two dimensional inner space, it will hit the eight dimensional component in finely many points, but it might also hit the nine dimensional component in a few points. So then what we do is we perturb this linear, this linear space from our nine dimensional witness set until it's a new linear space, l prime, containing z. And what the theorem is, is that these witness points, if we follow them, one of those witness set points must become the point z if z is on the nine dimensional component. And so in doing this, we can rule out we can throw away z if one of our nine dimensional witness sets, witness set points goes towards z as we make this perturbation, then we can throw it away.
00:47:18.684 - 00:48:17.074, Speaker A: In other words, it's not, we determined it's on the nine dimensional component, it's not on the eight, and so we won't include it in the witness set for the eight dimensional component. Okay, thank you. Okay. Sorry. Okay, well, I think that's basically the time for today. It was a little fast, but I feel like if you go to these, these books and if you also attend this workshop, what we talked about today will be a good introduction, allowing you to sort of get a full grasp on what these topics are we did randomize systems and we did talk about at least the first two steps to obtaining a numerically reducible decomposition using witness sets. And.
00:48:17.074 - 00:48:25.634, Speaker A: Yeah, that's basically what I had planned for today. So thank you for listening, and I'll take any other questions right now.
00:48:27.614 - 00:48:29.474, Speaker B: Are there any further questions?
00:48:30.094 - 00:48:53.156, Speaker C: So, Alex, just to make sure that I understand, at the end, after you've, you've figured out all the components, the components of dimension nine, the components of dimension eight, et cetera, and you go down now, you've got all the components. Their union is the numerical irreducible decomposition, and you've got the witness sets for each of the components. Is that the idea?
00:48:53.340 - 00:49:39.464, Speaker A: Yeah, I would. There's one more step, which is the witness set for the nine dimensional component would have to be separated into witness sets for each irreducible nine dimensional piece. Yes. And same for the eight dimensional, the seven dimensional, the six dimensional for this map, since they had rank four, we only had 6789. And then we'd be done. And to split the witness set into witness sets for each individual irreducible component, we would use something like the trace test, which, which I think we briefly discussed in a separate video, maybe the second, the second lecture.
