00:00:00.200 - 00:01:01.444, Speaker A: Thank you for coming to what is the second lecture after the delay while we had the winter school for the graduate course on rigidity theory. So the main aim of today's lecture is really to introduce infinitesimal rigidity, and we'll talk mainly about definitions and examples through some ideas in this regard. Just before we start, let me mention, for those of you who did attend the winter school, as I asked you to, the main thing relevant to this course was the sequence of lectures by Bill Jackson. You will have noticed that he went into a lot of detail on what's called abstract rigidity matroids. So I decided, because he went into so much detail and so well into that topic, that I'm really not going to focus on it much in this course. But I am happy to talk to anyone who wants to ask questions about that. But I'm not going to focus on that so much, but more on the actual generic rigidity matriarch.
00:01:01.444 - 00:01:47.550, Speaker A: And I'll tell you what those words mean as we go. The thing I wanted to say is that there is an outline of the course now on the webpage. This should be treated as very tentative, but basically all it says is in lecture x, a couple of topics that will be covered in that lecture. One of the purposes of putting this there is, for the benefit of anyone who's come to this, because they were more interested in Meera's graduate course, the companion graduate course, and real algebraic geometry. And she advised her students to come to some of the lectures for this course. So those students can pick and choose from that. And to all of you, if there's particular bits that are interesting or not, say that I think there are some experts in the audience, so you might be able to pick and choose.
00:01:47.550 - 00:02:38.358, Speaker A: It's a very tentative outline, particular already today and on Wednesday. The amount of material I've got to cover is quite a lot, so it may be already by Wednesday. I've had to shift some things back a little bit, so it will be updated pretty regularly, I think. Okay, so if we remember back to either to Bill's lectures in the winter school or to my first lecture, we introduced the idea of a framework. So just, just in very briefly, a framework is a combination of a graph and a map p, which realizes the vertices of the framework into Rd and then gives you lengths for the edges. And we want to constrain those lengths to not change in the framework. Okay, so I did mention what a continuous motion was, but let me write it.
00:02:38.358 - 00:03:36.602, Speaker A: And this is a slightly different formulation to what I did two or three weeks ago now. So a continuous motion in some sense, you can think of it as a family of continuous functions, and that's really what's happening here. But I've written it here as just as a continuous function, which applies to the interval minus one to one. But for every vertex of your graph and this map, phi, which I'm going to define by phi, sub t of v t, you should think of it as time. So the motion, as the framework moves in time, where t is something within minus one, one is phi t v for every vertex. And this function should, at time zero, be equal to the p in our framework, Gp. So we should start in some sense at Gp, and then as t varies, we look at what happens where Gp has become g phi t, and we want the edge lengths to be the same.
00:03:36.602 - 00:04:31.942, Speaker A: In these realizations, we want to continuously move from p to phi t as t changes, and the edge length should be the same. So the two frameworks, Gp and g phi, t, should be equivalent, and that should be true throughout the range. If you prefer, you can, you can write this within the interval zero to one. But I just decided to make it sort of two directional in this case. So whichever direction you head off from, from zero, either forward or backwards in time. Okay, so this motion, phi, this continuous motion, is trivial if, as well as being equivalent, these two frameworks are congruent as well. So remember that congruent meant that the distance between every pair of vertices was unchanged rather than equivalence, which was only for the lengths of edges and non pairs of vertices that didn't define edges could change in equivalent frameworks, but they can't in congruent ones.
00:04:31.942 - 00:05:36.888, Speaker A: And its equivalent to congruence is to say that Gp and g phi t are linked by an isometry at all points t and time in the motion. So if they're always congruent, then the motion is trivial. And if there is a non trivial motion, so a motion where the congruence property does fail at some point in time, then we say that the framework is flexible. So it's flexible if it admits and non trivial continuous motion. Okay, so in order to understand this, and actually there'll be a lecture, I think, next week, where we talk about the diff that these different notions of continuous rigidity, as are defined here, as I did three weeks ago, in terms of with any realization q sufficiently close to p, and so on, about why these are all equivalent. But for now, what we're going to do is linearize the problems. So instead of having quadratic edge lengths we're going to differentiate and get systems of linear equations and use linear algebra and study what's called infinitesimal rigidity.
00:05:36.888 - 00:06:29.328, Speaker A: So that's what I say here. So we will proceed by studying the first order version by differentiating the continuous motions. So in particular these continuous motions, we're going to assume that they're actually smooth motions and then we'll consider the derivative of our motion phi at time zero. So in other words, we're going to take d phi by dt and evaluate this differential at t equals zero. Okay, but let me, instead of working directly with differentiation, let me just define an infinitesimal motion from scratch in a way that hopefully you can see, and we will see in a few lectures time, how it all links up. So an infinitesimal motion u is an assignment of infinitesimal velocities to the vertices. So we're just going to have, you can, you can think of u as a vector in rd mod v.
00:06:29.328 - 00:07:30.804, Speaker A: So u as a vector in rd mod v where the component of u, the d tuple of entries in u which corresponds to vertex v, gives us a velocity vector at that vertex. Okay? And this assignment u satisfies this dot product condition. So it must be true that if we take for vertices vi and vj which are joined by an edge, we don't care about non edges. But if we have an edge between vi and vj, then for vertices vi and vj, the positions of vi and vj in the framework Gp, the difference there dot product with the velocities we've assigned the components of u would be assigned to vi and vj should be zero. Okay, and this has to be true for every edge in the graph, but non edges we don't care about. Okay, so just like we had isometries. So these were translations and rotations that your congruent.
00:07:30.804 - 00:08:40.678, Speaker A: The congruent frameworks were linked by isometries, they're infinitesimal versions of these. And so it's not so important for what we're going to do, but just to describe them precisely in linear algebraic terms, we say an infinitesimal motion u is trivial if you can write it as the composition of a matrix and a translation. So if we can write u is a times our framework point pv plus some translation b for all of our vertices. In particular a here should be a nice little skew symmetric matrix to model a rotation or a reflection. And then the framework is infinitesimally flexible if the only infinitesimal motions, sorry, it's infinitesimally flexible if there is an infinitesimal motion, which isn't just one of these trivial ones, which isn't just an isometry. So if it has a non trivial infinitesimal motion, and it's infinitesimally rigid if it has no non trivial infinitesimal motion. So the only motions, the only vectors u you can choose are infinitesimal versions of isometries.
00:08:40.678 - 00:09:18.604, Speaker A: You can always choose to just to translate everything say. So we can never say there's no motion, but infinitesimally rigid if the only ones are trivial. Okay, so it's maybe not so intuitive, at least right at the beginning, to see when infinitesimal motions exist or not. But really what we're going to do is turn it into a condition on matrices, and so we just look at rank of matrices. So we're not going to need to do too much. But I still want to give one example in a moment, to give you a little bit of intuition. But let me say first a link between infinitesimal motions and continuous motions.
00:09:18.604 - 00:10:12.142, Speaker A: So we said that infinitesimal motions come as derivatives of smooth motions. So you can use this, just the contrapositive to say that infinitesimally rigid frameworks are rigid, and this was first done by Gluck in the seventies. I believe the converse is not true, but again, in a few lectures time we'll see why it becomes true if we restrict to, say, generic framework. So again, remember from previously that a generic framework is one where the set of coordinates of p from my framework GP is algebraically independent over the rationals. So the generic is a really strong condition. We'll see in a moment that this result and many others are true when we talk about, in fact, not in a moment, but in about half an hour's time, I guess, is true for regular frameworks. Okay, so here's an example.
00:10:12.142 - 00:10:48.008, Speaker A: So what we've got here, I'm looking at two dimensions, and I should have said, again, I am watching the chat. So if anyone does have any questions, feel free to either shout out if you prefer, or just to type it in there, and I'll keep an eye on it throughout the lecture. So this example, I'm talking about two dimensions. It's a very basic example. Just first of all, imagine you have three vertices in a graph. You have the path of length two on these three vertices. And imagine I realize them in the framework, so that vertex one, vertex two and vertex three are collinear.
00:10:48.008 - 00:11:34.376, Speaker A: So I've just got this two edge graph and three vertices. But I've realized the three vertices in a line. Then one easy way to get an infinitesimal motion is to imagine you have zero as the velocity vector at vertex 10 is a velocity vector at vertex three and vertex two, you'd send it orthogonally away from the line through the three points. So then the dot product condition says we have to apply it to both of the edges. We say p one minus p two should be orthogonal to u one minus u two, but u one is zero. So we get this condition. But that's, that's for free by the choice we made for the, the, um, the, the velocity vector at vertex two.
00:11:34.376 - 00:12:42.906, Speaker A: And the same is true here. So what you can always do is if you have, say, an r two free vertices being collinear, you can always make an infinitesimal motion simply by taking one of those, the one in the middle and sending it off orthogonally away, infinitesimally, orthogonally away from the line through the three vertices. So this is a silly example. It's very definitely flexible in the plane because continuously as well, because this vertex is free to move around on a circle centered here. So there's lots and lots of flexibility to this in some sense, but we can embed it in a less trivial example. So if you imagine this example without this vertex, then we saw two or three weeks ago that this is an example of a rigid, in fact a minimally rigid framework in two dimensional space, even at the sort of special position I've drawn, but certainly generically. And if I add one more constraint, which is one more vertex in the middle of this line, and add this constraint, then continuously, this is a rigid object.
00:12:42.906 - 00:13:33.978, Speaker A: So this is continuously rigid. Sorry, that's bad handwriting. This is a continuously rigid framework in two dimensions, but it's not infinitesimally rigid because we can apply the motion we had here and just send this vertex up perpendicular to the line through those free vertices so you can, it's not hard to show the difference between infinitesimal and continuous rigidity in general. But as I said, and as we'll hopefully prove in a few lectures, time, the equivalence is true in the generic case. Okay, okay, so infinitesimal motions, we have this set of infinitesimal motions. It's just a set of vectors. It's a nice linear subspace of RDV.
00:13:33.978 - 00:14:29.094, Speaker A: And what it's given by is the linear equations, one for every edge in our definition, our dot product definition of infinitesimal motions. And what's more is that the, the matrix of coefficients of this system of equations gives us a nice matrix. It's called the, the rigidity matrix and we denote it as r of Gp. So it might be at some points that I put a sub d in here to tell you which dimension it's in, because the dimension will determine various properties of it. But for now at least, I've just said RGP. The, this rigidity matrix, it comes as the, the jacobian derivative matrix of the, of a certain map, which we'll see again towards the end of this, this lecture. And so it sort of naturally comes from differentiating the, the smooth motions in the definition of continuous rigidity.
00:14:29.094 - 00:14:54.518, Speaker A: Okay, so, but what is it? Let's do, let's write it down explicitly and do concrete example. It's got one row for every edge. It's got columns for every vertex. And, but for each vertex there's a d tuple of columns when GP lives in Rd. Okay, so the rows correspond to edges. And suppose I have an edge between vertices vi and vj. Then the edge vijay.
00:14:54.518 - 00:15:41.884, Speaker A: The relevant parts are the d tuple of columns for vi and the d tuple of columns for vj. And in those entries we have pvi minus Pvj and Pvj minus Pvi. And everywhere else in the row you just have zeros. So it's at least for large number of vertices. It's a very sparse matrix with well understood entries and wellness and pattern to the entries. And then we want to basically study properties of this matrix, the rank properties, independent, slimly independent, slimly independence kind of property. So just to be absolutely sure, to make sure you understand, suppose we take the complete graph on three vertices and realize that in r two, so you see here, I've just put positions p one, p two and p three for the vertices.
00:15:41.884 - 00:16:32.744, Speaker A: And I've ordered the edges e one, e two, e three. So here is the matrix for this one row. For each of the edges we have the tuple of columns for vertex one and vertex two and vertex three. And you can see the, the vector differences in the, in the matrix. And if you like to make it even more explicit, since we're in two reals, we could write PI as xi yi and then for example, p one minus p two would be x one minus x two, y one minus y two, and so on and so forth throughout. Okay, so this is hopefully easy to follow. Okay, so, and I guess this is sort of summarizing what I, what I've been been saying, that the, the vector u in rdv is an infinitesimal motion.
00:16:32.744 - 00:17:11.412, Speaker A: It satisfies this dot product condition if and only if it's a vector in the kernel of the rigidity matrix if and only if the rigidity matrix times u is equal to zero. Okay, so, and I guess you can sort of see that by putting a vector here and seeing what it has to satisfy. So you have your u components here. If you look along a row, say, and you have to get to zero. But I'll let you digest that if it wasn't sufficiently clear for yourself after the lecture. Okay, and we talked about trivial infinitesimal motions. These are the ones that correspond to isometries.
00:17:11.412 - 00:18:01.194, Speaker A: These are exactly the, the vectors that belong to the kernel when they're, when you have every possible edge. So if you take the complete graph on the vertex set of your given framework, g, then the things that are still in the kernel of that rigidity matrix, when I've thrown all the edges in, I can, then what's left will just be translations and rotations. They will be the infinitesimal motions in the kernel of the rigidity matrix. And I've put here, we need to be a little bit careful with p. So if p is generic, this is certainly true, but even if it satisfies weaker conditions, this will be true. We just don't want to have sort of unnecessary, say coincident points or collinearities, this sort of thing. Okay, so, yeah, and then the kernel of RKVP, so it's of the complete graph.
00:18:01.194 - 00:18:44.164, Speaker A: These are just the infinitesimal isometries. And these are, as we talked about in the first lecture, have dimension d plus one, choose two. So the kernel of the rigidity matrix has dimension, at least this always. So in the complete graph case, when it's generic, it's exactly that. When it's not complete, it may be less than this. Okay, so that observation basically tells us by rank nullity gives us an upper bound on the rank of the rigidity matrix, which is going to be really important for us because we're going to really care about when the rigidity matrix has full rank. Okay, so suppose I have a framework gp and rd on n vertices.
00:18:44.164 - 00:19:18.254, Speaker A: Then there's two cases because small graphs are a bit different. So if the number of vertices is small enough, so in this case at most d plus one, then every edge in the graph can be independent. So the rank can really be equal to the number of edges. So you can be as many as there are in the complete graph on n vertices, which is n, choose two. But if n is at least d plus two, then the rank has a different condition. It's the number of columns. And this is where I said rank nullity minus this dimension we talked about for the kernel that's always there.
00:19:18.254 - 00:19:58.424, Speaker A: So this is really the sort of counting condition where it comes from. So we want the number of edges to be at least d times the number of vertices minus d plus one. Choose two. Okay, and we'll see that as an exact counting condition. We'll derive that again later in this lecture. So one sort of, I guess it's a very minor thing, but just in case you wonder, is that I chose d plus one here. I could have made a different choice here, because if n equals d, or if n equals d plus one, then actually both conditions can be satisfied at the same time.
00:19:58.424 - 00:20:25.224, Speaker A: This n choose two is actually equal to the upper bound. So I could have said that. I could have done that, basically said if n is at least d, then we have the dn minus d plus one choose two condition. And if n was strictly less than d, then we have this one. But I think that being a complete graph is special and is simpler. So I pushed it into the first condition instead. So I do want to have the d plus two here.
00:20:25.224 - 00:21:22.820, Speaker A: Okay, so the small graph case, as I say, it is special, and we can really understand infinitesimal rigidity very easily in that case. So if n is at most d plus one, then the only chance you have of being infinitesimally rigid is if you actually have all these edges. And so the, the graph is, is complete. And then, so when we're going to infinitesimally rigid will be when this is an equality, and hence the graph has to be complete, I say, can only be because it might still be that it shows p in a weird way. But if it's generic, then this is exactly the case that you're infinitesimally rigid if you have at most d plus one vertices, then you're infinitesimally rigid if and only if you're complete. But in the interesting cases, obviously, when you have at least d plus two vertices. And so, and when that's true, then we go into the second option here.
00:21:22.820 - 00:21:57.434, Speaker A: So the, the framework Gp is infinitesimally rigid if the rank is equal to dn minus d plus one, choose two. Okay. And actually this is what I like to think of as the definition of infinitesimal rigidity. I don't particularly like personally to think about infinitesimal motions as assignments of velocity vectors. I like to think about the rank of this matrix, the linear independence and the linear dependencies in the rows of the rigidity matrix. So this is really what we'll focus on as the idea of infinitesimal rigidity. So, if you like, take this as the definition.
00:21:57.434 - 00:22:40.592, Speaker A: And the second part of it, I guess, is that the framework is infinitesimally flexible if the rank is less than this maximum. Okay. Okay. So as I keep saying, we also care about independence and dependencies. So we say that the framework GP is independent if the rigidity matrix has linearly independent rows. So if the rows of RGP are linearly independent, and then we can talk about minimal rigidity, in this case, minimal infinitesimal rigidity if the rows are independent and the rank is full. So, minimally rigid frameworks are ones which have the full rank condition, and the rows are linearly independent.
00:22:40.592 - 00:23:17.754, Speaker A: So this rank, RGP, is actually equal to the number of edges. And so already you can sort of see there e equals dn minus d plus one. Choose two. But we will state that more formally a bit later. Okay, so I don't want to go next. So this is a bit of an aside, I guess, but it's just continuing to give you more sort of background information about rigidity to try and increase your intuition before we start proving one of the most important theorems in the subject. Next time.
00:23:17.754 - 00:24:02.040, Speaker A: Okay, so I want to introduce now the rigidity map and the configuration space. So the rigidity map is just a way of assigning a list of the edge lengths. So the map has domain RDV and co domain is r to the e. And what it is is it just lists out the length of every edge. And I've took the squared lengths of all the edges in the framework. Okay, but if I do this, and this is the map that I talked about earlier, that if I take the jacobian derivative of you go through and you can do this for yourself, it's the partial derivative matrix. It turns out it is the rigidity matrix, except for one minor.
00:24:02.040 - 00:24:34.812, Speaker A: The thing that when you differentiate these twos, you always get two s. Instead of PI minus PJ, you'll get two times PI minus PJ. So I have to half to even that out. But as I said, what we're interested in is rank conditions and linearly independent conditions and linear dependencies. So scaling all the rows by two is not going to have any effect. Okay. But the main reason I wanted to introduce this rigidity map is because it gives me a simple definition of what I mean by regular.
00:24:34.812 - 00:25:26.554, Speaker A: And regular is a weakening of generic that works in a lot of cases. So I'm going to use generic throughout the course, because it will just always work for everything we want to present. But if you instead talk about frameworks which are regular rather than generic, most things still work, but occasionally things go a bit wrong. But what is it? So a framework is regular if GP is regular, if P is a regular point of this map, Fg. So what does that mean? It means that the, the rank of RGP is at least as big as the rank of RGX for all possible vectors in RDV. So it's regular if you can't choose some other realization X and have GX having bigger rank. So we've chosen one which is at the largest possible.
00:25:26.554 - 00:26:07.252, Speaker A: It's not quite enough to make things really nice. What we actually need is a little bit stronger. So what we actually assume, instead of a generic, a lot of the times, it suffices to assume that you're regular. But all edge induced subgraphs are regular rather than just the entire thing. Because, for example, the example I showed you with three colonial points, which has an infinitesimal motion you don't expect. So it has a rank less than you would expect in the rigidity matrix. You can sort of hide that in a larger thing which does have full rank, but having it as an edge induced subgraph, where it goes wrong, does create problems.
00:26:07.252 - 00:27:00.212, Speaker A: And so you have to be a bit careful. So we do. Typically, you can get away with saying regular for all edge induced subgraphs, but regular itself is not, sometimes is not sufficiently strong. Okay, so from this map, we can also define the configuration space. So this is the inverse image here. So in effect, what I'm doing here, I've taken the list of edge lengths in p, and I'm asking for the set of all vectors Q in RDV, such that GP, let's say that sign means equivalent to GQ. So I'm taking the set of all vectors in RDV, which have the same edge lengths in the framework GQ, as they do in the given framework GP.
00:27:00.212 - 00:27:34.882, Speaker A: So that set of all of all q is the configuration space of our framework. And studying this sort of more abstract object is one way you can proceed in rigidity. It's not the focus, as I say. So our focus is going to be on as elementary as we can. We have a matrix, and we want to understand its ranking, and we want to understand its rank using combinatorial ideas. Okay, but you can study the nature of this configuration space. You can prove when it's a manifold, talk about it as a variety and various things, if you prefer.
00:27:34.882 - 00:28:13.834, Speaker A: So some people study the topological nature of it, and again, it's really up to you if you prefer this viewpoint. But if you do, then quite often what people do is take the isometries. And instead of what I will do is more simple minded, where you have a, a framework and say, well, I want to get rid of the translation in this direction and this direction. So let's just pin this vertex in place. This is in two reals, this example. So by pinning the vertex in place, those translations can't exist. But if you want to do it a bit more algebraically, more formally, you could take the quotient of the configuration space by the Lie group of euclidean isometries.
00:28:13.834 - 00:28:55.790, Speaker A: By the euclidean group, okay? And you can study the, this quotient space. Okay, so I keep saying we want to talk about linear dependencies as well as independencies. So I want to just show you a bit of that. So really what we're just going to talk about is vectors in the co kernel of the rigidity matrix we saw, the vectors in the kernel were our infinitesimal motions. And now I want to talk about vectors in the co kernel. So these won't be used for all of the course, but occasionally we will come to them and they can be very important. So this might not be the standard way that they're defined, because I'm splitting the definition into two steps.
00:28:55.790 - 00:30:08.036, Speaker A: So first of all, a stress is simply an assignment of weights to the edges of the graph. So I'm just going to put use omega to some function that weights the edges. Arbitrary weighting is completely fine for a stress, but for an equilibrium stress, I'm going to put an equilibrium condition on the choices of weights for the edges. Okay, so an equilibrium stress is a stress omega such that this condition is satisfied, but it's satisfied for every vertex in your graph. So what's the condition? We take pvi minus pvj, we scale it by the weight, and we do that for a given vertex vi, we sum over all the edges in incident to I. So I have my vertex vi and then I sum over all j such that so maybe vi is adjacent to vertex one, three and seven. So I sum over all of those wi one, wi three wi seven and then times the pvis minus pvjs and it's an equilibrium stress if this sum gives me zero.
00:30:08.036 - 00:30:52.654, Speaker A: And so zero is a vector here. So it gives me zero in each coordinate, if you like. Okay, so let's see that I find equilibrium stresses a little bit non intuitive. So let's go through an example slowly. So I want to take the complete graph, k four and I'm going to do a two dimensional example. So I've chosen to realize k four as a framework by putting the first vertex at the origin, second vertex at 10, the third, oh, sorry, second vertex at zero one third is 10 and four for one one. So I've got a unit square on the axis in two reals, so you can see after I've just labeled the, the edge weights as omega one two, et cetera, going round.
00:30:52.654 - 00:31:31.170, Speaker A: So we can find out the stress just by solving linear equations. So what was the condition? So say around vertex, say around point p one, we look at, let's say first the x coordinates of this. So we take zero for its x coordinate of p 10 for p two. So that difference is zero. So we get nothing we can look at to p three, we get zero minus one, gives us minus one times omega one three, so that's minus omega one three. And we also have the edge on the diagonal here. Get rid of that.
00:31:31.170 - 00:31:56.786, Speaker A: And so that's zero minus one omega one four. So we get minus omega one three, minus omega one four must be zero. So that's our first equation that we got. And you can continue in this way. So say the y coordinate, we get zero minus one omega one two, we get zero minus zero omega one three. So that's nothing. Zero minus one omega one four, so that's minus omega one three, and we get zero.
00:31:56.786 - 00:32:54.312, Speaker A: So these two simple equations come from the omega being equilibrium stress, and that we've told you about this vertex now. So we could move to vertex two and we do the same thing. So at vertex two, you get zero minus zero omega one 20, minus one omega two three, which is that bit, you get zero minus one omega two four, which is that bit, and so on and so forth. So you get yourself a system of linear equations, and we know the solutions to the system, if they exist, will be the equilibrium stresses. So one way to solve this system, if you can just spot, is if you take ones for the ones around the outside. So that's putting a one there, there, there and there, and minus ones on the diagonal. So we get minus one here and here.
00:32:54.312 - 00:33:27.980, Speaker A: And so if you like, if I, instead, I could have told you in advance that four ones and two minus ones in that pattern gave you an equilibrium stress. And then you could go back and test this directly on those particular ones. But I wanted to show you, you could actually just, if I give you the coordinates, and it's a simple enough example, you can quite quickly solve the. And find an example yourself. Notice this was one solution. If you scale one and minus one, you also get a solution. Right? So this is a one dimensional space of solutions.
00:33:27.980 - 00:34:03.802, Speaker A: And I guess the other thing I wanted to point out here before I move on is, and I hope this example is clear, I know there's now a lot of color and scribbles everywhere, but what happens, say, if I take something away? So, let's suppose I looked at a different graph instead. I looked at k four minus an edge. So I look at this one instead. So now I don't have these ones anymore. So, down to this. And so now what happens? What did I take away? I took away omega two three, so I can look at the same equations I wrote out, but I now know omega two three doesn't exist. So that's zero.
00:34:03.802 - 00:34:33.749, Speaker A: So that goes, omega two three is zero, omega two three is zero. And anywhere else it might be. So now, you can quickly see that omega two three, being zero forces omega two four to be zero. So that's, this one is zero, forces omega twelve to be zero, so that's zero. Omega one three to be zero, and omega three four to be zero. That's that one. And then omega one four plus omega two four, which is now zero, forces this one to be zero as well.
00:34:33.749 - 00:35:19.358, Speaker A: So just by taking that one edge away, you can use the same equations to see that the. In fact, in that case, the only equilibrium stress would be Omari equals zero, which is another way of saying that these rows were linely independent, which, of course, we know, we know this framework was rigid. It has the right number of edges as well, so it's linearly, its rows are independent, linely independent. So we can sort of see both a dependent and independent example here. Okay, so where we say. So now, getting back just to the linear algebra. So, non zero equilibrium stresses are the rho dependencies in the rigidity matrix.
00:35:19.358 - 00:35:52.774, Speaker A: So this is what I said a few minutes ago. They're actually exactly the vectors in the co kernel of RGP. So we're looking now at Omega such that omega RGP is equal to zero. And I'm being a bit lazy, because before I said. Sorry, before I said this, so there I was thinking of u as a column vector, whereas here I'm thinking of omega as a row vector. So if my laziness with transposes is confusing at any point, please just shout. But hopefully it's not too bad.
00:35:52.774 - 00:36:38.900, Speaker A: Okay. And so here's the point I was saying about this one we found for k four was really not just one equilibrium stress, but it was really it a small family of them, because we can just scale an equilibrium stress by any real number, any non zero real number, and you'll get another good one. If it's by zero, you'll get to the zero stress, which we sort of want to exclude, really. We just want to think about the non zero ones. Okay, so here's a more complicated example, but I'm not going to say much at all about this particular example. So, the graph now is two copies of k four, which share a single edge. And so again, I'm thinking about this as an example in two dimensional euclidean space.
00:36:38.900 - 00:37:04.262, Speaker A: And so, one easy way to come up with an equilibrium stress for this framework is what I've drawn here. So I've put exactly the stress I showed you for the k four on the left, and then I've put zeros everywhere else. So we don't want it to be, I want it to be non zero. So somewhere it should be non zero. But you can have a load of zeros if you like. And that gives you an example, an equilibrium stress. But I could also do exactly the opposite.
00:37:04.262 - 00:38:12.374, Speaker A: Put zeros on all of these and put ones on here and minus ones on these. And so you can see these are two linearly independent vectors in the co kernel of the rigidity matrix. So this shows you a two dimensional space of stresses. I haven't really shown you that two is actually the maximum, but just tell you it is for this particular example. And so, for example, if you wanted a nowhere zero stress, a stress which was zero, which was non zero on every single edge, you could take some, some random linear combination of these two, if you like, and make sure that these are different, for example, so that they don't cancel out in the middle. Okay? So properties of stresses will be really important, but I just want to say one sort of elementary lemma about them, which is, again, just facts from basic linear algebra for now, and then we'll come back to them in future lectures, I think. So we can take the vector space of equilibrium stresses and call that the stressors s.
00:38:12.374 - 00:38:55.298, Speaker A: And we can take the vector space of infinitesimal motions. I don't want to use m because we will use m for matroid a lot. So, instead of motion, think of flex. And I used an f here. So we can express the rank of the rigidity matrix in terms of these spaces, s and f of our framework, the spaces of stresses and the spaces of motions. So the rank of the rigidity matrix is the number of edges minus the number of dimension of the space of stresses. So just looking at rows, or it's the dimension of the, sorry, it's the number of vertices, which is the dimension times v minus the size of the kernel, or the dimension of the space of infinitesimal motions.
00:38:55.298 - 00:39:59.414, Speaker A: So again, this is a sort of easy linear algebra observation. Okay, so I mentioned this in this next bit three weeks ago. Again, I'm still just building up sort of haphazardly a little bit, some different ideas to give you some more and more words that you're familiar with, and rigidity more and more sort of basic understanding before we really start proving things properly next time. But we mentioned previously that rigidity and global rigidity depend only on the graph. So rather than being a property of both g and p, it's a property only of g, as long as our framework is generic. So I definitely say without proof, the, we will prove the rigidity version of this. But the, the global rigidity version of this in arbitrary dimension is, is beyond the scope of this course, and it will take us a lot of effort to prove it as a corollary to a different result for the two dimensional case.
00:39:59.414 - 00:40:44.978, Speaker A: Okay, but let's assume it, and because it depends only on the graph, what we really want to do, what I like to do a lot is talk about combinatorial rigidity, which is really saying when a graph is rigid, or when a graph is globally rigid. So let's say a graph is rigid in d dimensions. If every generic framework GP in RD is rigid, and it turns out that this is exactly equivalent, we could replace the word every with some. So all you have to do is find one generic framework which is rigid. And we know that the generic frameworks maximize the rank of the rigidity matrix. So if we find one rigid framework, regardless of whether it's generic or not, we'll. The generic ones are all rigid.
00:40:44.978 - 00:41:23.374, Speaker A: So to find that a graph is rigid, you just have to find some framework of that graph which is rigid. The global rigidity, there's a similar definition. So the graph G is globally rigid in RD. If every, or again equivalently, if some generic framework in RD is globally rigid, so find a framework which is globally rigid. The thing I said about finding a rigid, but not necessarily a generically rigid one, is a bit more complicated in the global rigidity case. And let me not say anything more now, but it's a bit more, you have to be a bit more careful. It has to be globally rigid.
00:41:23.374 - 00:42:15.512, Speaker A: Plus something else, if it's not generic, to know all the generic ones are globally rigid, but we will come on to a result of Connolly and Whiteley, which tells you a nice result in that direction. That will be later on. Okay, so what's the purpose of the course? In fact, the purpose of large aspects of the research in rigidity theory over the last however many years and going forward, I think as well, is to understand necessary and sufficient conditions for a graph to be rigid or a graph to be globally rigid. So necessary conditions are typically much, much easier. And we seen a little bit of that already, and we will see more in the course. Sufficient conditions are harder. We'll be able to show it in certain dimensions, low dimensions, one and two in higher dimensions.
00:42:15.512 - 00:43:04.998, Speaker A: It seems to be a very hard problem and motivates a lot of the study that both will do in the course and that is done in the field of rigidity theory. Okay, so the next thing I want to do is introduce matroids a bit. So I'm going to assume you don't know anything about matroids, although I guess most of you were at the winter school, so saw Bill Jackson talk quite a bit about matroids and abstract rigidity matroids. But I just want to go very slowly through a couple of facts about matroids. Just definition and some notation really that just to give you the, what's needed for this course. I don't expect to use matriids all the time. I'll usually talk about graphs and so be matriid words used in, to help with the understanding.
00:43:04.998 - 00:43:48.704, Speaker A: So really we're going to make use of the language of matriarch theory without needing to be experts ourselves in the study of matroids. Okay, so what's the idea? The idea is we want to generalize the notion of linear independence in vector spaces. So this, you can define a matrix in a lot of ways, but let me define it and then I'll show you why it's, why it generalizes linear independence. So we take a matrix, m is a pair ei, so it's an ordered pair. E is just a finite set. You can get into infinite matriids, but I don't want to in this course. So we take a finite set e, and then I is really an independence condition.
00:43:48.704 - 00:44:36.470, Speaker A: So you take a family of subsets of e and they must satisfy some independence condition. So what's the condition? Well, the condition is three things. Firstly, that the empty set must be independent. Secondly, if you're independent, if there's a set that's independent, and you take any subset of it, then that subset must be independent. So this is clearly something that's true for linearly independent vectors, right? If I have a linear independent set of vectors and I take any subset of it, then obviously it's linearly independent as well. So that's a sort of natural property. And then the second property, maybe you have to think for a second about the linear algebra interpretation, but this should be relatively clear with a bit of thought that it's true as well.
00:44:36.470 - 00:45:17.586, Speaker A: You take two independent sets, a and b, which have different cardinalities. Let's suppose a is bigger than b. Then there must exist some element of the set a which is not in b, which I can adjoin to the set b and still be independent. Okay, so that one is the, I guess the complicated one. That one is usually sort of just a convention and two is somewhat obvious that it should be natural thing to happen. But free is what really makes matroids work, I guess. Okay, so our independent sets are the sets that satisfy some condition.
00:45:17.586 - 00:46:00.086, Speaker A: If I have an independent set that's as large as possible, if I add anything to it, it suddenly becomes dependent. Then I call it a basis. This is like a set of vectors, linearly independent vectors. If it's as big as possible, then it gives you a basis for some space and a subset of e which is not independent. So it's not, in our family of independent sets, we call that one dependent. And then we can also, instead of having maximal, we can have the idea of a minimal dependent set, which we'll call a circuit. So when we're going to, in a moment, we're going to talk about the rigidity matrix and there's really the idea here.
00:46:00.086 - 00:46:28.556, Speaker A: So we want bases are the ones with full rank. So they're going to be the rigid things, independent ones. Independent rows are the independent sets of some matriid and the dependent sets are vectors. In the co kernel, the ones are the ones we care about there. And the minimal dependent sets we'll call circuits and we really care about them as well. Okay, so just very briefly, the two motivating examples for matriids. We already talked about linear matriids being one of them.
00:46:28.556 - 00:47:03.842, Speaker A: So for me these are just row matriids and matrices. So really, I mean, vector matrioids, linear matriids are a bit about representability, but I don't want to worry about that. I just want to think about row matriids and matrices. So you take the set of rows of a matrix and if you like, take the row labels as you set e. And then we say the subset f of e is independent. If the rows of the matrix corresponding to that subset f of e are linearly independent. So this satisfies the three conditions, sort of by, by the way they were constructed.
00:47:03.842 - 00:47:52.190, Speaker A: The second kind of example is that you can get matriids from graphs in a nice way as well. So it's called cycle matriids or graphic matriids. So here the finite set e is the edge set of a graph, and the independence condition I is saying that a subset f of e is independent if the graph you get induced by that edge set f. So I take this edges of f, obviously all the vertices incident to any edge in f. Look inside the graph g and ask, does the graph have any cycles? It's independent if there are no cycles. So in other words, the bases are trees and the circuits are cycles in the graph. Okay, so I'm distinguishing between the words circuit and cycle.
00:47:52.190 - 00:48:36.670, Speaker A: But some people, I think, for example, we had a talk at the winter school, a short talk at the winter school where the cycles were used for circuits in the matrix. But I will distinguish between circuits in matroids and cycles in graphs. Again, if you go back to the definitions, you can check for yourself that the empty set is fine. If you have no edges in your set, then clearly there's no cycles. If you have something that has no cycles, any subset of it clearly has no cycles. And you can convince yourself with a minute of thought that the third condition is satisfied too. So these are natural examples of matroids and cycle.
00:48:36.670 - 00:48:43.074, Speaker A: Matriids will also come up in the one dimensional rigidity case. Tony?
00:48:43.494 - 00:48:57.304, Speaker B: Yes, sorry, this is will, could you just scroll back up tiny bit? When we said a tree, did we mean a spanning tree f as a basis? If it induces, so it doesn't have.
00:48:58.524 - 00:49:44.914, Speaker A: So it doesn't have to span the entire. So you could imagine I have a graph with lots of vertices, and then I take as they edge set this one. So it's not spanning the entire graph, but it's a basis itself in the sense that if I add any edge to it on the same vertex set, it goes outside it. So it sort of, I mean, I was a bit vague here because I didn't really write down what was going on. So it depends which matriid you're sort of thinking of. So there's matriid on a particular graph. So if you wanted on the entire graph, then yes, you're right that this should say spanning, because I could have added that edge in without violating independence.
00:49:44.914 - 00:49:49.034, Speaker A: So it wouldn't be a maximal independent set. So yeah, I think you're right.
00:49:50.974 - 00:49:51.754, Speaker B: Thanks.
00:49:52.254 - 00:50:13.438, Speaker A: Okay. Yeah. So, yes. Okay, so it's 51. So I'm already not going to finish everything I want to mention, I don't think. Let me just mention the rigidity matriid, because that would be a natural place to stop, I think. Okay, so we know the rigidity matrix.
00:50:13.438 - 00:50:47.216, Speaker A: That's what we've been talking about in this lecture. So this gives us a matriarch just exactly as a row matriarch. So we take a D dimensional frame of GP, its rigidity matrix R. GP defines the rigidity matriid by exactly what we just talked about. So we take the ground set e, and we say a set f of e is independent if the corresponding rows are linearly independent. So we're really just caring about linearly independence in the rigidity matrix. But there's a key thing.
00:50:47.216 - 00:51:31.984, Speaker A: I'm going to talk about the generic rigidity matrix here. I just said rigidity matriid, but I want it to be about generic frameworks. And if you take generic frameworks, say GP and GQ, then it's not hard to see that any two such frameworks on the same graph, G, have the same rigidity matrix. Since they're both generic, the linear independence or dependence conditions would be the same in both. Even though the realizations may be different because they're both generic, it will all turn out to be the same. So the generic rigidity matriid for any generic p or q, we're going to call RDG. And obviously, and I'm usually going to drop the g because the graph will usually be clear.
00:51:31.984 - 00:52:10.784, Speaker A: So I'm just going to have the rigidity matriarch script r sub d. That allows me to say independence, etcetera, as matriid properties. So the framework is independent, then the edge set must be independent, the rigidity matriarch. And we'll say that the graph is RD independent if its edge set is independent in RD. So I'll do this quite a bit. In particular, when we talk about circuits, I will use the matriarch terminology to define graphs. But really, what I mean is the edge set of the graph is independent or dependent, et cetera, in the matriid.
00:52:10.784 - 00:53:02.890, Speaker A: And so our aim of characterizing rigidity we can express as the aim of characterizing independence in this rigidity matriaroid, or if you like, characterizing the rank function of this rigidity matriarch. But I haven't defined what the rank function is yet. Okay, so yeah, I guess I have time, if no one has to, to rush off. So before we gave an upper bound on the rank of the rigidity matrix. And I mentioned that this gave us a necessary condition for the graph count. I think I wrote this at some point that if the rank was full and the rows were independent, we got to this condition. This is effectively our necessary condition, our Maxwell stock style necessary condition for, for rigidity.
00:53:02.890 - 00:53:42.364, Speaker A: But let me phrase it as a condition for independence. So we don't need the inequality, we'll just have a, don't need the equality, we'll just have an inequality. So we want to be independent in the rigidity matriarch, then we're going to have counts like this. So I think it's convenient to introduce some notation for this. So I've given a graph with vertex set v for any subset x of v. I'm going to use I sub g of x to denote the number of edges of g induced by x. So I take this vertex set x, all the edges in the graph that have both endpoints in the set x, count them all up, and the number of edges is igx.
00:53:42.364 - 00:54:41.140, Speaker A: And again, because I'm a bit lazy, I'll use iX if the G is clear and I don't need to keep writing the subscripts. Okay, so with this notation let me write the necessary condition. So first of all, in the simple case, if I have a subset of v which has at most d plus one vertices, then we get for free that this is true, because graphs in this course have no loops and no parallel edges. So I of x is at most mod x, choose two so that that bit's easy. But if I, when I want to talk about larger things with larger subsets of the vertex set of my graph, ones with at least d plus two vertices, then I get the other condition that we had earlier. So suppose the graph is rd independent. Then we get that I of x or I sub g of x is at most d times x minus d plus one.
00:54:41.140 - 00:55:27.726, Speaker A: Choose two. Okay, so this is our Maxwell type necessary condition, just written in terms of rd independence rather than rigidity, and with this function I of x rather than just counting the number of edges. So this is true for all subsets of your vertex set. And really what we care about is when you can go the other way. So when does a combinatorial condition like this allow us to imply an independence or dependence or a rank condition about the rigidity matriarch or about the rigidity matrix? If you want to be a bit, have it that way. And so we'll see next time, if we have time in the lecture. I will try to prove the entirety of the characterization when D equals two, but it might take a couple of lectures.
00:55:27.726 - 00:55:47.664, Speaker A: And then next week I think we will spend several lectures talking about special cases of what's known in dimension three and that sort of thing. So I think I should definitely stop there to give at least the last four minutes for if anyone has any questions. But thank you for attending. And so I'll stop.
00:55:52.764 - 00:56:01.504, Speaker B: Tony, could you just scroll up a little bit? I lost track a little bit when you were talking about RD and defining RD independence.
00:56:02.244 - 00:56:03.104, Speaker A: Okay.
00:56:05.044 - 00:56:08.864, Speaker B: So is this maybe down a little bit?
00:56:10.984 - 00:56:25.004, Speaker A: So, framework is independent. So we say that G is RD independent if its edge set is independent in the matrix, which means its edge set induces linearly independent rows in the rigidity matrix.
00:56:56.044 - 00:57:20.154, Speaker B: Maybe it was up a little bit further. So I understand that G is independent, then the whole edge set is independent in the rigidity matriid, but g is RD independent. If. What happens when was this? Seems like a.
00:57:21.014 - 00:57:51.560, Speaker A: So GP being independent was what I had earlier, before I introduced the rigidity matrix. That was just a statement about the, the rigidity matrix. And so this was just a sort of an implication that if it was true, then the rows are independent. So the edge set is surely independent in the matriid where we care about this row independence. And so this is really saying that the same thing in just, in different words. This is defining RD independence. It's just saying it's.
00:57:51.560 - 00:58:56.004, Speaker A: A graph is independent if its edge set is independent, because I want to talk about graphs rather than just edges, to talk about frameworks and rigidity rather than, as a purely matriarchal property, I want to stick to graphs and frameworks. Okay, there's a question. Will the lecture notes be available as a PDF? They, these handwritten ones I've been doing, I'd be happy to make them available. If onenote is a program that, that can be done from, I can, I can look into that almost straight away and see if it comes out in a, in a usable format. I have not got type notes, but there's a comment, a very helpful comment from William in the chat that he is making an overleaf doc with lecture notes. And so that version that he's been kindly making sounds like will be available. Thank you, William.
00:59:07.584 - 00:59:34.304, Speaker B: I guess I have a generic kind of general question. Tony. The matrix that we look at, the rigidity matrix, looks like it has very special structure. Yeah. What's known about that structure? Is there like a general thing? It's not positive definite. It doesn't like, I mean, what do we know about that matrix in general?
00:59:37.624 - 00:59:50.404, Speaker A: No, sorry. So the, it depends on the entries too much, I think. So we don't know anything about the eigenvalues in advance. In full generality at least.
00:59:51.304 - 00:59:52.844, Speaker B: Yeah, I think that makes sense.
00:59:54.144 - 01:00:54.258, Speaker A: I mean, we know some rank condition, upper bounds on the rank, like I said. But I mean, there was a, I guess this is still not answering your question, but Joseph Solomosi's talks last week, talked about these design matrices which gives you lower bounds that seem to be said were true for any realization, but that we've at least know it was at least in general position. But yeah, I think it's a nice pattern, but it seems to be too hard in general to give you a real nice structural result in that direction. So it's not the same thing. But Sebastian Chober, I think it is. He has some results on linking eigenvalues of the laplacian matrix of the graph to rigidity conditions. Oh, that's true.
01:00:54.258 - 01:01:19.142, Speaker A: Yeah. But it's not the same thing as the rigidity matrix, except maybe in one dimension, I guess. But his results are two dimensional rigidity based. So his results are roughly of the form. If the Laplacian has certain properties, then the graph is sufficiently connected and hence the, the rank is sufficiently high. The rigidity matrix. Yeah, exactly that.
01:01:19.142 - 01:01:49.850, Speaker A: Yeah. So, yeah, this is a good point. Thanks, Sean. I mean, the, the one dimensional rigidity matrix is effectively the, what is it? Do I want to say adjacency or incidence matrix? Sean? I think it's the incidence. Incidence, I think it's transposed technically because I think they usually do it the other way around. But I might be wrong. Yeah, it's the one minus one incidence matrix.
01:01:49.850 - 01:01:58.964, Speaker A: Transpose. Yeah. So in that case, you probably have a huge amount extra information, but in general, we know a lot less.
